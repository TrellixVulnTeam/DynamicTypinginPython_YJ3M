commit 9fce9c641e7f51b5931ebb8dc78858baa357adf5
Author: Zhichao Lu <lzc@google.com>
Date:   Tue Jun 5 13:31:32 2018 -0700

    Merged commit includes the following changes:
    199348852  by Zhichao Lu:
    
        Small typos fixes in VRD evaluation.
    
    --
    199315191  by Zhichao Lu:
    
        Change padding shapes when additional channels are available.
    
    --
    199309180  by Zhichao Lu:
    
        Adds minor fixes to the Object Detection API implementation.
    
    --
    199298605  by Zhichao Lu:
    
        Force num_readers to be 1 when only input file is not sharded.
    
    --
    199292952  by Zhichao Lu:
    
        Adds image-level labels parsing into TfExampleDetectionAndGTParser.
    
    --
    199259866  by Zhichao Lu:
    
        Visual Relationships Evaluation executable.
    
    --
    199208330  by Zhichao Lu:
    
        Infer train_config.batch_size as the effective batch size. Therefore we need to divide the effective batch size in trainer by train_config.replica_to_aggregate to get per worker batch size.
    
    --
    199207842  by Zhichao Lu:
    
        Internal change.
    
    --
    199204222  by Zhichao Lu:
    
        In case the image has more than three channels, we only take the first three channels for visualization.
    
    --
    199194388  by Zhichao Lu:
    
        Correcting protocols description: VOC 2007 -> VOC 2012.
    
    --
    199188290  by Zhichao Lu:
    
        Adds per-relationship APs and mAP computation to VRD evaluation.
    
    --
    199158801  by Zhichao Lu:
    
        If available, additional channels are merged with input image.
    
    --
    199099637  by Zhichao Lu:
    
        OpenImages Challenge metric support:
        -adding verified labels standard field for TFExample;
        -adding tfrecord creation functionality.
    
    --
    198957391  by Zhichao Lu:
    
        Allow tf record sharding when creating pets dataset.
    
    --
    198925184  by Zhichao Lu:
    
        Introduce moving average support for evaluation. Also adding the ability to override this configuration via config_util.
    
    --
    198918186  by Zhichao Lu:
    
        Handles the case where there are 0 box masks.
    
    --
    198809009  by Zhichao Lu:
    
        Plumb groundtruth weights into target assigner for Faster RCNN.
    
    --
    198759987  by Zhichao Lu:
    
        Fix object detection test broken by shape inference.
    
    --
    198668602  by Zhichao Lu:
    
        Adding a new input field in data_decoders/tf_example_decoder.py for storing additional channels.
    
    --
    198530013  by Zhichao Lu:
    
        An util for hierarchical expandion of boxes and labels of OID dataset.
    
    --
    198503124  by Zhichao Lu:
    
        Fix dimension mismatch error introduced by
        https://github.com/tensorflow/tensorflow/pull/18251, or cl/194031845.
        After above change, conv2d strictly checks for conv_dims + 2 == input_rank.
    
    --
    198445807  by Zhichao Lu:
    
        Enabling Object Detection Challenge 2018 metric in evaluator.py framework for
        running eval job.
        Renaming old OpenImages V2 metric.
    
    --
    198413950  by Zhichao Lu:
    
        Support generic configuration override using namespaced keys
    
        Useful for adding custom hyper-parameter tuning fields without having to add custom override methods to config_utils.py.
    
    --
    198106437  by Zhichao Lu:
    
        Enable fused batchnorm now that quantization is supported.
    
    --
    198048364  by Zhichao Lu:
    
        Add support for keypoints in tf sequence examples and some util ops.
    
    --
    198004736  by Zhichao Lu:
    
        Relax postprocessing unit tests that are based on assumption that tf.image.non_max_suppression are stable with respect to input.
    
    --
    197997513  by Zhichao Lu:
    
        More lenient validation for normalized box boundaries.
    
    --
    197940068  by Zhichao Lu:
    
        A couple of minor updates/fixes:
        - Updating input reader proto with option to use display_name when decoding data.
        - Updating visualization tool to specify whether using absolute or normalized box coordinates. Appropriate boxes will now appear in TB when using model_main.py
    
    --
    197920152  by Zhichao Lu:
    
        Add quantized training support in the new OD binaries and a config for SSD Mobilenet v1 quantized training that is TPU compatible.
    
    --
    197213563  by Zhichao Lu:
    
        Do not share batch_norm for classification and regression tower in weight shared box predictor.
    
    --
    197196757  by Zhichao Lu:
    
        Relax the box_predictor api to return box_prediction of shape [batch_size, num_anchors, code_size] in addition to [batch_size, num_anchors, (1|q), code_size].
    
    --
    196898361  by Zhichao Lu:
    
        Allow per-channel scalar value to pad input image with when using keep aspect ratio resizer (when pad_to_max_dimension=True).
    
        In Object Detection Pipeline, we pad image before normalization and this skews batch_norm statistics during training. The option to set per channel pad value lets us truly pad with zeros.
    
    --
    196592101  by Zhichao Lu:
    
        Fix bug regarding tfrecord shuffling in object_detection
    
    --
    196320138  by Zhichao Lu:
    
        Fix typo in exporting_models.md
    
    --
    
    PiperOrigin-RevId: 199348852

diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 1a618c64..3628a85e 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -56,15 +56,26 @@ def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
   else:
     height, width = spatial_image_shape  # pylint: disable=unpacking-non-sequence
 
+  num_additional_channels = 0
+  if fields.InputDataFields.image_additional_channels in dataset.output_shapes:
+    num_additional_channels = dataset.output_shapes[
+        fields.InputDataFields.image_additional_channels].dims[2].value
   padding_shapes = {
-      fields.InputDataFields.image: [height, width, 3],
+      # Additional channels are merged before batching.
+      fields.InputDataFields.image: [
+          height, width, 3 + num_additional_channels
+      ],
+      fields.InputDataFields.image_additional_channels: [
+          height, width, num_additional_channels
+      ],
       fields.InputDataFields.source_id: [],
       fields.InputDataFields.filename: [],
       fields.InputDataFields.key: [],
       fields.InputDataFields.groundtruth_difficult: [max_num_boxes],
       fields.InputDataFields.groundtruth_boxes: [max_num_boxes, 4],
-      fields.InputDataFields.groundtruth_instance_masks: [max_num_boxes, height,
-                                                          width],
+      fields.InputDataFields.groundtruth_instance_masks: [
+          max_num_boxes, height, width
+      ],
       fields.InputDataFields.groundtruth_is_crowd: [max_num_boxes],
       fields.InputDataFields.groundtruth_group_of: [max_num_boxes],
       fields.InputDataFields.groundtruth_area: [max_num_boxes],
@@ -74,7 +85,8 @@ def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
       fields.InputDataFields.groundtruth_label_scores: [max_num_boxes],
       fields.InputDataFields.true_image_shape: [3],
       fields.InputDataFields.multiclass_scores: [
-          max_num_boxes, num_classes + 1 if num_classes is not None else None],
+          max_num_boxes, num_classes + 1 if num_classes is not None else None
+      ],
   }
   # Determine whether groundtruth_classes are integers or one-hot encodings, and
   # apply batching appropriately.
@@ -90,7 +102,9 @@ def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
                      'rank 2 tensor (one-hot encodings)')
 
   if fields.InputDataFields.original_image in dataset.output_shapes:
-    padding_shapes[fields.InputDataFields.original_image] = [None, None, 3]
+    padding_shapes[fields.InputDataFields.original_image] = [
+        None, None, 3 + num_additional_channels
+    ]
   if fields.InputDataFields.groundtruth_keypoints in dataset.output_shapes:
     tensor_shape = dataset.output_shapes[fields.InputDataFields.
                                          groundtruth_keypoints]
@@ -108,9 +122,13 @@ def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
           for tensor_key, _ in dataset.output_shapes.items()}
 
 
-def build(input_reader_config, transform_input_data_fn=None,
-          batch_size=None, max_num_boxes=None, num_classes=None,
-          spatial_image_shape=None):
+def build(input_reader_config,
+          transform_input_data_fn=None,
+          batch_size=None,
+          max_num_boxes=None,
+          num_classes=None,
+          spatial_image_shape=None,
+          num_additional_channels=0):
   """Builds a tf.data.Dataset.
 
   Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all
@@ -128,6 +146,7 @@ def build(input_reader_config, transform_input_data_fn=None,
     spatial_image_shape: A list of two integers of the form [height, width]
       containing expected spatial shape of the image after applying
       transform_input_data_fn. If None, will use dynamic shapes.
+    num_additional_channels: Number of additional channels to use in the input.
 
   Returns:
     A tf.data.Dataset based on the input_reader_config.
@@ -152,7 +171,9 @@ def build(input_reader_config, transform_input_data_fn=None,
     decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=input_reader_config.load_instance_masks,
         instance_mask_type=input_reader_config.mask_type,
-        label_map_proto_file=label_map_proto_file)
+        label_map_proto_file=label_map_proto_file,
+        use_display_name=input_reader_config.use_display_name,
+        num_additional_channels=num_additional_channels)
 
     def process_fn(value):
       processed = decoder.decode(value)
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index 36bd3090..0f1360f5 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -30,49 +30,50 @@ from object_detection.utils import dataset_util
 
 class DatasetBuilderTest(tf.test.TestCase):
 
-  def create_tf_record(self):
+  def create_tf_record(self, has_additional_channels=False):
     path = os.path.join(self.get_temp_dir(), 'tfrecord')
     writer = tf.python_io.TFRecordWriter(path)
 
     image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    additional_channels_tensor = np.random.randint(
+        255, size=(4, 5, 1)).astype(np.uint8)
     flat_mask = (4 * 5) * [1.0]
     with self.test_session():
       encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
+      encoded_additional_channels_jpeg = tf.image.encode_jpeg(
+          tf.constant(additional_channels_tensor)).eval()
+    features = {
+        'image/encoded':
+            feature_pb2.Feature(
+                bytes_list=feature_pb2.BytesList(value=[encoded_jpeg])),
+        'image/format':
+            feature_pb2.Feature(
+                bytes_list=feature_pb2.BytesList(value=['jpeg'.encode('utf-8')])
+            ),
+        'image/height':
+            feature_pb2.Feature(int64_list=feature_pb2.Int64List(value=[4])),
+        'image/width':
+            feature_pb2.Feature(int64_list=feature_pb2.Int64List(value=[5])),
+        'image/object/bbox/xmin':
+            feature_pb2.Feature(float_list=feature_pb2.FloatList(value=[0.0])),
+        'image/object/bbox/xmax':
+            feature_pb2.Feature(float_list=feature_pb2.FloatList(value=[1.0])),
+        'image/object/bbox/ymin':
+            feature_pb2.Feature(float_list=feature_pb2.FloatList(value=[0.0])),
+        'image/object/bbox/ymax':
+            feature_pb2.Feature(float_list=feature_pb2.FloatList(value=[1.0])),
+        'image/object/class/label':
+            feature_pb2.Feature(int64_list=feature_pb2.Int64List(value=[2])),
+        'image/object/mask':
+            feature_pb2.Feature(
+                float_list=feature_pb2.FloatList(value=flat_mask)),
+    }
+    if has_additional_channels:
+      features['image/additional_channels/encoded'] = feature_pb2.Feature(
+          bytes_list=feature_pb2.BytesList(
+              value=[encoded_additional_channels_jpeg] * 2))
     example = example_pb2.Example(
-        features=feature_pb2.Features(
-            feature={
-                'image/encoded':
-                    feature_pb2.Feature(
-                        bytes_list=feature_pb2.BytesList(value=[encoded_jpeg])),
-                'image/format':
-                    feature_pb2.Feature(
-                        bytes_list=feature_pb2.BytesList(
-                            value=['jpeg'.encode('utf-8')])),
-                'image/height':
-                    feature_pb2.Feature(
-                        int64_list=feature_pb2.Int64List(value=[4])),
-                'image/width':
-                    feature_pb2.Feature(
-                        int64_list=feature_pb2.Int64List(value=[5])),
-                'image/object/bbox/xmin':
-                    feature_pb2.Feature(
-                        float_list=feature_pb2.FloatList(value=[0.0])),
-                'image/object/bbox/xmax':
-                    feature_pb2.Feature(
-                        float_list=feature_pb2.FloatList(value=[1.0])),
-                'image/object/bbox/ymin':
-                    feature_pb2.Feature(
-                        float_list=feature_pb2.FloatList(value=[0.0])),
-                'image/object/bbox/ymax':
-                    feature_pb2.Feature(
-                        float_list=feature_pb2.FloatList(value=[1.0])),
-                'image/object/class/label':
-                    feature_pb2.Feature(
-                        int64_list=feature_pb2.Int64List(value=[2])),
-                'image/object/mask':
-                    feature_pb2.Feature(
-                        float_list=feature_pb2.FloatList(value=flat_mask)),
-            }))
+        features=feature_pb2.Features(feature=features))
     writer.write(example.SerializeToString())
     writer.close()
 
@@ -218,6 +219,31 @@ class DatasetBuilderTest(tf.test.TestCase):
         [2, 2, 4, 5],
         output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
 
+  def test_build_tf_record_input_reader_with_additional_channels(self):
+    tf_record_path = self.create_tf_record(has_additional_channels=True)
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    tensor_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(
+            input_reader_proto, batch_size=2,
+            num_additional_channels=2)).get_next()
+
+    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    with sv.prepare_or_wait_for_session() as sess:
+      sv.start_queue_runners(sess)
+      output_dict = sess.run(tensor_dict)
+
+    self.assertEquals((2, 4, 5, 5),
+                      output_dict[fields.InputDataFields.image].shape)
+
   def test_raises_error_with_no_input_paths(self):
     input_reader_text_proto = """
       shuffle: false
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index c8cf63a4..3b3014f7 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -79,12 +79,17 @@ def build(image_resizer_config):
             keep_aspect_ratio_config.max_dimension):
       raise ValueError('min_dimension > max_dimension')
     method = _tf_resize_method(keep_aspect_ratio_config.resize_method)
+    per_channel_pad_value = (0, 0, 0)
+    if keep_aspect_ratio_config.per_channel_pad_value:
+      per_channel_pad_value = tuple(keep_aspect_ratio_config.
+                                    per_channel_pad_value)
     image_resizer_fn = functools.partial(
         preprocessor.resize_to_range,
         min_dimension=keep_aspect_ratio_config.min_dimension,
         max_dimension=keep_aspect_ratio_config.max_dimension,
         method=method,
-        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension)
+        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension,
+        per_channel_pad_value=per_channel_pad_value)
     if not keep_aspect_ratio_config.convert_to_grayscale:
       return image_resizer_fn
   elif image_resizer_oneof == 'fixed_shape_resizer':
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index 6744d95d..38f620e0 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -52,6 +52,9 @@ class ImageResizerBuilderTest(tf.test.TestCase):
         min_dimension: 10
         max_dimension: 20
         pad_to_max_dimension: true
+        per_channel_pad_value: 3
+        per_channel_pad_value: 4
+        per_channel_pad_value: 5
       }
     """
     input_shape = (50, 25, 3)
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
deleted file mode 100644
index 1ebdcb79..00000000
--- a/research/object_detection/builders/model_builder.py
+++ /dev/null
@@ -1,377 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""A function to build a DetectionModel from configuration."""
-from object_detection.builders import anchor_generator_builder
-from object_detection.builders import box_coder_builder
-from object_detection.builders import box_predictor_builder
-from object_detection.builders import hyperparams_builder
-from object_detection.builders import image_resizer_builder
-from object_detection.builders import losses_builder
-from object_detection.builders import matcher_builder
-from object_detection.builders import post_processing_builder
-from object_detection.builders import region_similarity_calculator_builder as sim_calc
-from object_detection.core import box_predictor
-from object_detection.meta_architectures import faster_rcnn_meta_arch
-from object_detection.meta_architectures import rfcn_meta_arch
-from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
-from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
-from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
-from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
-from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
-from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
-from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
-from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
-from object_detection.protos import model_pb2
-
-# A map of names to SSD feature extractors.
-SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'ssd_inception_v2': SSDInceptionV2FeatureExtractor,
-    'ssd_inception_v3': SSDInceptionV3FeatureExtractor,
-    'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,
-    'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,
-    'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
-    'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
-    'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
-    'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,
-}
-
-# A map of names to Faster R-CNN feature extractors.
-FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'faster_rcnn_nas':
-    frcnn_nas.FasterRCNNNASFeatureExtractor,
-    'faster_rcnn_pnas':
-    frcnn_pnas.FasterRCNNPNASFeatureExtractor,
-    'faster_rcnn_inception_resnet_v2':
-    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,
-    'faster_rcnn_inception_v2':
-    frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,
-    'faster_rcnn_resnet50':
-    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
-    'faster_rcnn_resnet101':
-    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,
-    'faster_rcnn_resnet152':
-    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,
-}
-
-
-def build(model_config, is_training, add_summaries=True,
-          add_background_class=True):
-  """Builds a DetectionModel based on the model config.
-
-  Args:
-    model_config: A model.proto object containing the config for the desired
-      DetectionModel.
-    is_training: True if this model is being built for training purposes.
-    add_summaries: Whether to add tensorflow summaries in the model graph.
-    add_background_class: Whether to add an implicit background class to one-hot
-      encodings of groundtruth labels. Set to false if using groundtruth labels
-      with an explicit background class or using multiclass scores instead of
-      truth in the case of distillation. Ignored in the case of faster_rcnn.
-  Returns:
-    DetectionModel based on the config.
-
-  Raises:
-    ValueError: On invalid meta architecture or model.
-  """
-  if not isinstance(model_config, model_pb2.DetectionModel):
-    raise ValueError('model_config not of type model_pb2.DetectionModel.')
-  meta_architecture = model_config.WhichOneof('model')
-  if meta_architecture == 'ssd':
-    return _build_ssd_model(model_config.ssd, is_training, add_summaries,
-                            add_background_class)
-  if meta_architecture == 'faster_rcnn':
-    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,
-                                    add_summaries)
-  raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))
-
-
-def _build_ssd_feature_extractor(feature_extractor_config, is_training,
-                                 reuse_weights=None):
-  """Builds a ssd_meta_arch.SSDFeatureExtractor based on config.
-
-  Args:
-    feature_extractor_config: A SSDFeatureExtractor proto config from ssd.proto.
-    is_training: True if this feature extractor is being built for training.
-    reuse_weights: if the feature extractor should reuse weights.
-
-  Returns:
-    ssd_meta_arch.SSDFeatureExtractor based on config.
-
-  Raises:
-    ValueError: On invalid feature extractor type.
-  """
-  feature_type = feature_extractor_config.type
-  depth_multiplier = feature_extractor_config.depth_multiplier
-  min_depth = feature_extractor_config.min_depth
-  pad_to_multiple = feature_extractor_config.pad_to_multiple
-  use_explicit_padding = feature_extractor_config.use_explicit_padding
-  use_depthwise = feature_extractor_config.use_depthwise
-  conv_hyperparams = hyperparams_builder.build(
-      feature_extractor_config.conv_hyperparams, is_training)
-  override_base_feature_extractor_hyperparams = (
-      feature_extractor_config.override_base_feature_extractor_hyperparams)
-
-  if feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP:
-    raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))
-
-  feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]
-  return feature_extractor_class(
-      is_training, depth_multiplier, min_depth, pad_to_multiple,
-      conv_hyperparams, reuse_weights, use_explicit_padding, use_depthwise,
-      override_base_feature_extractor_hyperparams)
-
-
-def _build_ssd_model(ssd_config, is_training, add_summaries,
-                     add_background_class=True):
-  """Builds an SSD detection model based on the model config.
-
-  Args:
-    ssd_config: A ssd.proto object containing the config for the desired
-      SSDMetaArch.
-    is_training: True if this model is being built for training purposes.
-    add_summaries: Whether to add tf summaries in the model.
-    add_background_class: Whether to add an implicit background class to one-hot
-      encodings of groundtruth labels. Set to false if using groundtruth labels
-      with an explicit background class or using multiclass scores instead of
-      truth in the case of distillation.
-  Returns:
-    SSDMetaArch based on the config.
-
-  Raises:
-    ValueError: If ssd_config.type is not recognized (i.e. not registered in
-      model_class_map).
-  """
-  num_classes = ssd_config.num_classes
-
-  # Feature extractor
-  feature_extractor = _build_ssd_feature_extractor(
-      feature_extractor_config=ssd_config.feature_extractor,
-      is_training=is_training)
-
-  box_coder = box_coder_builder.build(ssd_config.box_coder)
-  matcher = matcher_builder.build(ssd_config.matcher)
-  region_similarity_calculator = sim_calc.build(
-      ssd_config.similarity_calculator)
-  encode_background_as_zeros = ssd_config.encode_background_as_zeros
-  negative_class_weight = ssd_config.negative_class_weight
-  ssd_box_predictor = box_predictor_builder.build(hyperparams_builder.build,
-                                                  ssd_config.box_predictor,
-                                                  is_training, num_classes)
-  anchor_generator = anchor_generator_builder.build(
-      ssd_config.anchor_generator)
-  image_resizer_fn = image_resizer_builder.build(ssd_config.image_resizer)
-  non_max_suppression_fn, score_conversion_fn = post_processing_builder.build(
-      ssd_config.post_processing)
-  (classification_loss, localization_loss, classification_weight,
-   localization_weight, hard_example_miner,
-   random_example_sampler) = losses_builder.build(ssd_config.loss)
-  normalize_loss_by_num_matches = ssd_config.normalize_loss_by_num_matches
-  normalize_loc_loss_by_codesize = ssd_config.normalize_loc_loss_by_codesize
-
-  return ssd_meta_arch.SSDMetaArch(
-      is_training,
-      anchor_generator,
-      ssd_box_predictor,
-      box_coder,
-      feature_extractor,
-      matcher,
-      region_similarity_calculator,
-      encode_background_as_zeros,
-      negative_class_weight,
-      image_resizer_fn,
-      non_max_suppression_fn,
-      score_conversion_fn,
-      classification_loss,
-      localization_loss,
-      classification_weight,
-      localization_weight,
-      normalize_loss_by_num_matches,
-      hard_example_miner,
-      add_summaries=add_summaries,
-      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize,
-      freeze_batchnorm=ssd_config.freeze_batchnorm,
-      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update,
-      add_background_class=add_background_class,
-      random_example_sampler=random_example_sampler)
-
-
-def _build_faster_rcnn_feature_extractor(
-    feature_extractor_config, is_training, reuse_weights=None,
-    inplace_batchnorm_update=False):
-  """Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.
-
-  Args:
-    feature_extractor_config: A FasterRcnnFeatureExtractor proto config from
-      faster_rcnn.proto.
-    is_training: True if this feature extractor is being built for training.
-    reuse_weights: if the feature extractor should reuse weights.
-    inplace_batchnorm_update: Whether to update batch_norm inplace during
-      training. This is required for batch norm to work correctly on TPUs. When
-      this is false, user must add a control dependency on
-      tf.GraphKeys.UPDATE_OPS for train/loss op in order to update the batch
-      norm moving average parameters.
-
-  Returns:
-    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.
-
-  Raises:
-    ValueError: On invalid feature extractor type.
-  """
-  if inplace_batchnorm_update:
-    raise ValueError('inplace batchnorm updates not supported.')
-  feature_type = feature_extractor_config.type
-  first_stage_features_stride = (
-      feature_extractor_config.first_stage_features_stride)
-  batch_norm_trainable = feature_extractor_config.batch_norm_trainable
-
-  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:
-    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(
-        feature_type))
-  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[
-      feature_type]
-  return feature_extractor_class(
-      is_training, first_stage_features_stride,
-      batch_norm_trainable, reuse_weights)
-
-
-def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
-  """Builds a Faster R-CNN or R-FCN detection model based on the model config.
-
-  Builds R-FCN model if the second_stage_box_predictor in the config is of type
-  `rfcn_box_predictor` else builds a Faster R-CNN model.
-
-  Args:
-    frcnn_config: A faster_rcnn.proto object containing the config for the
-      desired FasterRCNNMetaArch or RFCNMetaArch.
-    is_training: True if this model is being built for training purposes.
-    add_summaries: Whether to add tf summaries in the model.
-
-  Returns:
-    FasterRCNNMetaArch based on the config.
-
-  Raises:
-    ValueError: If frcnn_config.type is not recognized (i.e. not registered in
-      model_class_map).
-  """
-  num_classes = frcnn_config.num_classes
-  image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)
-
-  feature_extractor = _build_faster_rcnn_feature_extractor(
-      frcnn_config.feature_extractor, is_training,
-      frcnn_config.inplace_batchnorm_update)
-
-  number_of_stages = frcnn_config.number_of_stages
-  first_stage_anchor_generator = anchor_generator_builder.build(
-      frcnn_config.first_stage_anchor_generator)
-
-  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate
-  first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(
-      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)
-  first_stage_box_predictor_kernel_size = (
-      frcnn_config.first_stage_box_predictor_kernel_size)
-  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth
-  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size
-  first_stage_positive_balance_fraction = (
-      frcnn_config.first_stage_positive_balance_fraction)
-  first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold
-  first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold
-  first_stage_max_proposals = frcnn_config.first_stage_max_proposals
-  first_stage_loc_loss_weight = (
-      frcnn_config.first_stage_localization_loss_weight)
-  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight
-
-  initial_crop_size = frcnn_config.initial_crop_size
-  maxpool_kernel_size = frcnn_config.maxpool_kernel_size
-  maxpool_stride = frcnn_config.maxpool_stride
-
-  second_stage_box_predictor = box_predictor_builder.build(
-      hyperparams_builder.build,
-      frcnn_config.second_stage_box_predictor,
-      is_training=is_training,
-      num_classes=num_classes)
-  second_stage_batch_size = frcnn_config.second_stage_batch_size
-  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction
-  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn
-  ) = post_processing_builder.build(frcnn_config.second_stage_post_processing)
-  second_stage_localization_loss_weight = (
-      frcnn_config.second_stage_localization_loss_weight)
-  second_stage_classification_loss = (
-      losses_builder.build_faster_rcnn_classification_loss(
-          frcnn_config.second_stage_classification_loss))
-  second_stage_classification_loss_weight = (
-      frcnn_config.second_stage_classification_loss_weight)
-  second_stage_mask_prediction_loss_weight = (
-      frcnn_config.second_stage_mask_prediction_loss_weight)
-
-  hard_example_miner = None
-  if frcnn_config.HasField('hard_example_miner'):
-    hard_example_miner = losses_builder.build_hard_example_miner(
-        frcnn_config.hard_example_miner,
-        second_stage_classification_loss_weight,
-        second_stage_localization_loss_weight)
-
-  common_kwargs = {
-      'is_training': is_training,
-      'num_classes': num_classes,
-      'image_resizer_fn': image_resizer_fn,
-      'feature_extractor': feature_extractor,
-      'number_of_stages': number_of_stages,
-      'first_stage_anchor_generator': first_stage_anchor_generator,
-      'first_stage_atrous_rate': first_stage_atrous_rate,
-      'first_stage_box_predictor_arg_scope_fn':
-      first_stage_box_predictor_arg_scope_fn,
-      'first_stage_box_predictor_kernel_size':
-      first_stage_box_predictor_kernel_size,
-      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,
-      'first_stage_minibatch_size': first_stage_minibatch_size,
-      'first_stage_positive_balance_fraction':
-      first_stage_positive_balance_fraction,
-      'first_stage_nms_score_threshold': first_stage_nms_score_threshold,
-      'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,
-      'first_stage_max_proposals': first_stage_max_proposals,
-      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,
-      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,
-      'second_stage_batch_size': second_stage_batch_size,
-      'second_stage_balance_fraction': second_stage_balance_fraction,
-      'second_stage_non_max_suppression_fn':
-      second_stage_non_max_suppression_fn,
-      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,
-      'second_stage_localization_loss_weight':
-      second_stage_localization_loss_weight,
-      'second_stage_classification_loss':
-      second_stage_classification_loss,
-      'second_stage_classification_loss_weight':
-      second_stage_classification_loss_weight,
-      'hard_example_miner': hard_example_miner,
-      'add_summaries': add_summaries}
-
-  if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):
-    return rfcn_meta_arch.RFCNMetaArch(
-        second_stage_rfcn_box_predictor=second_stage_box_predictor,
-        **common_kwargs)
-  else:
-    return faster_rcnn_meta_arch.FasterRCNNMetaArch(
-        initial_crop_size=initial_crop_size,
-        maxpool_kernel_size=maxpool_kernel_size,
-        maxpool_stride=maxpool_stride,
-        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,
-        second_stage_mask_prediction_loss_weight=(
-            second_stage_mask_prediction_loss_weight),
-        **common_kwargs)
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
deleted file mode 100644
index 225e1d50..00000000
--- a/research/object_detection/builders/model_builder_test.py
+++ /dev/null
@@ -1,1056 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""Tests for object_detection.models.model_builder."""
-
-import tensorflow as tf
-
-from google.protobuf import text_format
-from object_detection.builders import model_builder
-from object_detection.meta_architectures import faster_rcnn_meta_arch
-from object_detection.meta_architectures import rfcn_meta_arch
-from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
-from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
-from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
-from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
-from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
-from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
-from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
-from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
-from object_detection.protos import model_pb2
-
-FRCNN_RESNET_FEAT_MAPS = {
-    'faster_rcnn_resnet50':
-    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
-    'faster_rcnn_resnet101':
-    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,
-    'faster_rcnn_resnet152':
-    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor
-}
-
-SSD_RESNET_V1_FPN_FEAT_MAPS = {
-    'ssd_resnet50_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
-    'ssd_resnet101_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
-    'ssd_resnet152_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor
-}
-
-
-class ModelBuilderTest(tf.test.TestCase):
-
-  def create_model(self, model_config):
-    """Builds a DetectionModel based on the model config.
-
-    Args:
-      model_config: A model.proto object containing the config for the desired
-        DetectionModel.
-
-    Returns:
-      DetectionModel based on the config.
-    """
-    return model_builder.build(model_config, is_training=True)
-
-  def test_create_ssd_inception_v2_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_inception_v2'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-          override_base_feature_extractor_hyperparams: true
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDInceptionV2FeatureExtractor)
-
-  def test_create_ssd_inception_v3_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_inception_v3'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-          override_base_feature_extractor_hyperparams: true
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDInceptionV3FeatureExtractor)
-
-  def test_create_ssd_resnet_v1_fpn_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_resnet50_v1_fpn'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        encode_background_as_zeros: true
-        anchor_generator {
-          multiscale_anchor_generator {
-            aspect_ratios: [1.0, 2.0, 0.5]
-            scales_per_octave: 2
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          weight_shared_convolutional_box_predictor {
-            depth: 32
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                random_normal_initializer {
-                }
-              }
-            }
-            num_layers_before_predictor: 1
-          }
-        }
-        normalize_loss_by_num_matches: true
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_sigmoid_focal {
-              alpha: 0.25
-              gamma: 2.0
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-              delta: 0.1
-            }
-          }
-          classification_weight: 1.0
-          localization_weight: 1.0
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-
-    for extractor_type, extractor_class in SSD_RESNET_V1_FPN_FEAT_MAPS.items():
-      model_proto.ssd.feature_extractor.type = extractor_type
-      model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-      self.assertIsInstance(model._feature_extractor, extractor_class)
-
-  def test_create_ssd_mobilenet_v1_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        freeze_batchnorm: true
-        inplace_batchnorm_update: true
-        feature_extractor {
-          type: 'ssd_mobilenet_v1'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV1FeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-    self.assertTrue(model._freeze_batchnorm)
-    self.assertTrue(model._inplace_batchnorm_update)
-
-  def test_create_ssd_mobilenet_v2_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'ssd_mobilenet_v2'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 320
-            width: 320
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        normalize_loc_loss_by_codesize: true
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          SSDMobileNetV2FeatureExtractor)
-    self.assertTrue(model._normalize_loc_loss_by_codesize)
-
-  def test_create_embedded_ssd_mobilenet_v1_model_from_config(self):
-    model_text_proto = """
-      ssd {
-        feature_extractor {
-          type: 'embedded_ssd_mobilenet_v1'
-          conv_hyperparams {
-            regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-          }
-        }
-        box_coder {
-          faster_rcnn_box_coder {
-          }
-        }
-        matcher {
-          argmax_matcher {
-          }
-        }
-        similarity_calculator {
-          iou_similarity {
-          }
-        }
-        anchor_generator {
-          ssd_anchor_generator {
-            aspect_ratios: 1.0
-          }
-        }
-        image_resizer {
-          fixed_shape_resizer {
-            height: 256
-            width: 256
-          }
-        }
-        box_predictor {
-          convolutional_box_predictor {
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        loss {
-          classification_loss {
-            weighted_softmax {
-            }
-          }
-          localization_loss {
-            weighted_smooth_l1 {
-            }
-          }
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = self.create_model(model_proto)
-    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          EmbeddedSSDMobileNetV1FeatureExtractor)
-
-  def test_create_faster_rcnn_resnet_v1_models_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        inplace_batchnorm_update: true
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-      model_proto.faster_rcnn.feature_extractor.type = extractor_type
-      model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-      self.assertIsInstance(model._feature_extractor, extractor_class)
-
-  def test_create_faster_rcnn_resnet101_with_mask_prediction_enabled(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-            conv_hyperparams {
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-            predict_instance_masks: true
-          }
-        }
-        second_stage_mask_prediction_loss_weight: 3.0
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertAlmostEqual(model._second_stage_mask_loss_weight, 3.0)
-
-  def test_create_faster_rcnn_nas_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_nas'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_nas.FasterRCNNNASFeatureExtractor)
-
-  def test_create_faster_rcnn_pnas_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_pnas'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_pnas.FasterRCNNPNASFeatureExtractor)
-
-  def test_create_faster_rcnn_inception_resnet_v2_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_inception_resnet_v2'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 17
-        maxpool_kernel_size: 1
-        maxpool_stride: 1
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(
-        model._feature_extractor,
-        frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor)
-
-  def test_create_faster_rcnn_inception_v2_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_inception_v2'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
-    self.assertIsInstance(model._feature_extractor,
-                          frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor)
-
-  def test_create_faster_rcnn_model_from_config_with_example_miner(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        feature_extractor {
-          type: 'faster_rcnn_inception_resnet_v2'
-        }
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        second_stage_box_predictor {
-          mask_rcnn_box_predictor {
-            fc_hyperparams {
-              op: FC
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        hard_example_miner {
-          num_hard_examples: 10
-          iou_threshold: 0.99
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    model = model_builder.build(model_proto, is_training=True)
-    self.assertIsNotNone(model._hard_example_miner)
-
-  def test_create_rfcn_resnet_v1_model_from_config(self):
-    model_text_proto = """
-      faster_rcnn {
-        num_classes: 3
-        image_resizer {
-          keep_aspect_ratio_resizer {
-            min_dimension: 600
-            max_dimension: 1024
-          }
-        }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
-        first_stage_anchor_generator {
-          grid_anchor_generator {
-            scales: [0.25, 0.5, 1.0, 2.0]
-            aspect_ratios: [0.5, 1.0, 2.0]
-            height_stride: 16
-            width_stride: 16
-          }
-        }
-        first_stage_box_predictor_conv_hyperparams {
-          regularizer {
-            l2_regularizer {
-            }
-          }
-          initializer {
-            truncated_normal_initializer {
-            }
-          }
-        }
-        initial_crop_size: 14
-        maxpool_kernel_size: 2
-        maxpool_stride: 2
-        second_stage_box_predictor {
-          rfcn_box_predictor {
-            conv_hyperparams {
-              op: CONV
-              regularizer {
-                l2_regularizer {
-                }
-              }
-              initializer {
-                truncated_normal_initializer {
-                }
-              }
-            }
-          }
-        }
-        second_stage_post_processing {
-          batch_non_max_suppression {
-            score_threshold: 0.01
-            iou_threshold: 0.6
-            max_detections_per_class: 100
-            max_total_detections: 300
-          }
-          score_converter: SOFTMAX
-        }
-      }"""
-    model_proto = model_pb2.DetectionModel()
-    text_format.Merge(model_text_proto, model_proto)
-    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-      model_proto.faster_rcnn.feature_extractor.type = extractor_type
-      model = model_builder.build(model_proto, is_training=True)
-      self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)
-      self.assertIsInstance(model._feature_extractor, extractor_class)
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index 09d85ae0..a755ef68 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -778,7 +778,7 @@ def to_absolute_coordinates(boxlist,
                             height,
                             width,
                             check_range=True,
-                            maximum_normalized_coordinate=1.01,
+                            maximum_normalized_coordinate=1.1,
                             scope=None):
   """Converts normalized box coordinates to absolute pixel coordinates.
 
@@ -792,7 +792,7 @@ def to_absolute_coordinates(boxlist,
     width: Maximum value for width of absolute box coordinates.
     check_range: If True, checks if the coordinates are normalized or not.
     maximum_normalized_coordinate: Maximum coordinate value to be considered
-      as normalized, default to 1.01.
+      as normalized, default to 1.1.
     scope: name scope.
 
   Returns:
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index 4d74fc40..bb76cfd3 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -931,6 +931,21 @@ class CoordinatesConversionTest(tf.test.TestCase):
       out = sess.run(boxlist.get())
       self.assertAllClose(out, coordinates)
 
+  def test_to_absolute_coordinates_maximum_coordinate_check(self):
+    coordinates = tf.constant([[0, 0, 1.2, 1.2],
+                               [0.25, 0.25, 0.75, 0.75]], tf.float32)
+    img = tf.ones((128, 100, 100, 3))
+    boxlist = box_list.BoxList(coordinates)
+    absolute_boxlist = box_list_ops.to_absolute_coordinates(
+        boxlist,
+        tf.shape(img)[1],
+        tf.shape(img)[2],
+        maximum_normalized_coordinate=1.1)
+
+    with self.test_session() as sess:
+      with self.assertRaisesOpError('assertion failed'):
+        sess.run(absolute_boxlist.get())
+
 
 class BoxRefinementTest(tf.test.TestCase):
 
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index f68ec7eb..78d82423 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -79,10 +79,12 @@ class BoxPredictor(object):
 
     Returns:
       A dictionary containing at least the following tensors.
-        box_encodings: A list of float tensors of shape
-          [batch_size, num_anchors_i, q, code_size] representing the location of
-          the objects, where q is 1 or the number of classes. Each entry in the
-          list corresponds to a feature map in the input `image_features` list.
+        box_encodings: A list of float tensors. Each entry in the list
+          corresponds to a feature map in the input `image_features` list. All
+          tensors in the list have one of the two following shapes:
+          a. [batch_size, num_anchors_i, q, code_size] representing the location
+            of the objects, where q is 1 or the number of classes.
+          b. [batch_size, num_anchors_i, code_size].
         class_predictions_with_background: A list of float tensors of shape
           [batch_size, num_anchors_i, num_classes + 1] representing the class
           predictions for the proposals. Each entry in the list corresponds to a
@@ -120,10 +122,12 @@ class BoxPredictor(object):
 
     Returns:
       A dictionary containing at least the following tensors.
-        box_encodings: A list of float tensors of shape
-          [batch_size, num_anchors_i, q, code_size] representing the location of
-          the objects, where q is 1 or the number of classes. Each entry in the
-          list corresponds to a feature map in the input `image_features` list.
+        box_encodings: A list of float tensors. Each entry in the list
+          corresponds to a feature map in the input `image_features` list. All
+          tensors in the list have one of the two following shapes:
+          a. [batch_size, num_anchors_i, q, code_size] representing the location
+            of the objects, where q is 1 or the number of classes.
+          b. [batch_size, num_anchors_i, code_size].
         class_predictions_with_background: A list of float tensors of shape
           [batch_size, num_anchors_i, num_classes + 1] representing the class
           predictions for the proposals. Each entry in the list corresponds to a
@@ -765,6 +769,13 @@ class ConvolutionalBoxPredictor(BoxPredictor):
     }
 
 
+# TODO(rathodv): Replace with slim.arg_scope_func_key once its available
+# externally.
+def _arg_scope_func_key(op):
+  """Returns a key that can be used to index arg_scope dictionary."""
+  return getattr(op, '_key_op', str(op))
+
+
 # TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above
 # since they are very similar.
 class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
@@ -773,8 +784,12 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
   Defines the box predictor as defined in
   https://arxiv.org/abs/1708.02002. This class differs from
   ConvolutionalBoxPredictor in that it shares weights and biases while
-  predicting from different feature maps.  Separate multi-layer towers are
-  constructed for the box encoding and class predictors respectively.
+  predicting from different feature maps. However, batch_norm parameters are not
+  shared because the statistics of the activations vary among the different
+  feature maps.
+
+  Also note that separate multi-layer towers are constructed for the box
+  encoding and class predictors respectively.
   """
 
   def __init__(self,
@@ -833,14 +848,15 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
 
     Returns:
       box_encodings: A list of float tensors of shape
-        [batch_size, num_anchors_i, q, code_size] representing the location of
-        the objects, where q is 1 or the number of classes. Each entry in the
-        list corresponds to a feature map in the input `image_features` list.
+        [batch_size, num_anchors_i, code_size] representing the location of
+        the objects. Each entry in the list corresponds to a feature map in the
+        input `image_features` list.
       class_predictions_with_background: A list of float tensors of shape
         [batch_size, num_anchors_i, num_classes + 1] representing the class
         predictions for the proposals. Each entry in the list corresponds to a
         feature map in the input `image_features` list.
 
+
     Raises:
       ValueError: If the image feature maps do not have the same number of
         channels or if the num predictions per locations is differs between the
@@ -858,15 +874,18 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                        'channels, found: {}'.format(feature_channels))
     box_encodings_list = []
     class_predictions_list = []
-    for (image_feature, num_predictions_per_location) in zip(
-        image_features, num_predictions_per_location_list):
+    for feature_index, (image_feature,
+                        num_predictions_per_location) in enumerate(
+                            zip(image_features,
+                                num_predictions_per_location_list)):
       # Add a slot for the background class.
       with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
                              reuse=tf.AUTO_REUSE):
         num_class_slots = self.num_classes + 1
         box_encodings_net = image_feature
         class_predictions_net = image_feature
-        with slim.arg_scope(self._conv_hyperparams_fn()):
+        with slim.arg_scope(self._conv_hyperparams_fn()) as sc:
+          apply_batch_norm = _arg_scope_func_key(slim.batch_norm) in sc
           for i in range(self._num_layers_before_predictor):
             box_encodings_net = slim.conv2d(
                 box_encodings_net,
@@ -874,14 +893,22 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                 [self._kernel_size, self._kernel_size],
                 stride=1,
                 padding='SAME',
-                scope='BoxEncodingPredictionTower/conv2d_{}'.format(i))
+                activation_fn=None,
+                normalizer_fn=(tf.identity if apply_batch_norm else None),
+                scope='BoxPredictionTower/conv2d_{}'.format(i))
+            if apply_batch_norm:
+              box_encodings_net = slim.batch_norm(
+                  box_encodings_net,
+                  scope='BoxPredictionTower/conv2d_{}/BatchNorm/feature_{}'.
+                  format(i, feature_index))
+            box_encodings_net = tf.nn.relu6(box_encodings_net)
           box_encodings = slim.conv2d(
               box_encodings_net,
               num_predictions_per_location * self._box_code_size,
               [self._kernel_size, self._kernel_size],
               activation_fn=None, stride=1, padding='SAME',
               normalizer_fn=None,
-              scope='BoxEncodingPredictor')
+              scope='BoxPredictor')
 
           for i in range(self._num_layers_before_predictor):
             class_predictions_net = slim.conv2d(
@@ -890,7 +917,15 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                 [self._kernel_size, self._kernel_size],
                 stride=1,
                 padding='SAME',
+                activation_fn=None,
+                normalizer_fn=(tf.identity if apply_batch_norm else None),
                 scope='ClassPredictionTower/conv2d_{}'.format(i))
+            if apply_batch_norm:
+              class_predictions_net = slim.batch_norm(
+                  class_predictions_net,
+                  scope='ClassPredictionTower/conv2d_{}/BatchNorm/feature_{}'
+                  .format(i, feature_index))
+            class_predictions_net = tf.nn.relu6(class_predictions_net)
           if self._use_dropout:
             class_predictions_net = slim.dropout(
                 class_predictions_net, keep_prob=self._dropout_keep_prob)
@@ -912,7 +947,7 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                                        combined_feature_map_shape[1] *
                                        combined_feature_map_shape[2] *
                                        num_predictions_per_location,
-                                       1, self._box_code_size]))
+                                       self._box_code_size]))
           box_encodings_list.append(box_encodings)
           class_predictions_with_background = tf.reshape(
               class_predictions_with_background,
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index a98d2f3f..49680596 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -442,6 +442,24 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
     return hyperparams_builder.build(conv_hyperparams, is_training=True)
 
+  def _build_conv_arg_scope_no_batch_norm(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      activation: RELU_6
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        random_normal_initializer {
+          stddev: 0.01
+          mean: 0.0
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.build(conv_hyperparams, is_training=True)
+
   def test_get_boxes_for_five_aspect_ratios_per_location(self):
 
     def graph_fn(image_features):
@@ -463,7 +481,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, objectness_predictions) = self.execute(
         graph_fn, [image_features])
-    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 4])
     self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])
 
   def test_bias_predictions_to_background_with_sigmoid_score_conversion(self):
@@ -512,7 +530,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, class_predictions_with_background) = self.execute(
         graph_fn, [image_features])
-    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 4])
     self.assertAllEqual(class_predictions_with_background.shape,
                         [4, 320, num_classes_without_background+1])
 
@@ -543,11 +561,12 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, class_predictions_with_background) = self.execute(
         graph_fn, [image_features1, image_features2])
-    self.assertAllEqual(box_encodings.shape, [4, 640, 1, 4])
+    self.assertAllEqual(box_encodings.shape, [4, 640, 4])
     self.assertAllEqual(class_predictions_with_background.shape,
                         [4, 640, num_classes_without_background+1])
 
-  def test_predictions_from_multiple_feature_maps_share_weights(self):
+  def test_predictions_from_multiple_feature_maps_share_weights_not_batchnorm(
+      self):
     num_classes_without_background = 6
     def graph_fn(image_features1, image_features2):
       conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
@@ -574,26 +593,95 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       actual_variable_set = set(
           [var.op.name for var in tf.trainable_variables()])
     expected_variable_set = set([
+        # Box prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictionTower/conv2d_0/weights'),
+         'BoxPredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictionTower/conv2d_0/BatchNorm/beta'),
+         'BoxPredictionTower/conv2d_1/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictionTower/conv2d_1/weights'),
+         'BoxPredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictionTower/conv2d_1/BatchNorm/beta'),
+         'BoxPredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Box prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/biases'),
+        # Class prediction tower
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
          'ClassPredictionTower/conv2d_0/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'ClassPredictionTower/conv2d_0/BatchNorm/beta'),
+         'ClassPredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
          'ClassPredictionTower/conv2d_1/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'ClassPredictionTower/conv2d_1/BatchNorm/beta'),
+         'ClassPredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Class prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/biases')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_no_batchnorm_params_when_batchnorm_is_not_configured(self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams_fn=self._build_conv_arg_scope_no_batch_norm(),
+          depth=32,
+          num_layers_before_predictor=2,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictor/weights'),
+         'BoxPredictionTower/conv2d_1/biases'),
+        # Box prediction head
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-         'BoxEncodingPredictor/biases'),
+         'BoxPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/biases'),
+        # Class prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/biases'),
+        # Class prediction head
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
          'ClassPredictor/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -628,7 +716,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
            [tf.shape(box_encodings), tf.shape(objectness_predictions)],
            feed_dict={image_features:
                       np.random.rand(4, resolution, resolution, 64)})
-      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
+      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 4])
       self.assertAllEqual(objectness_predictions_shape,
                           [4, expected_num_anchors, 1])
 
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 70b601a5..0fcdfcc6 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -2128,7 +2128,8 @@ def resize_to_range(image,
                     max_dimension=None,
                     method=tf.image.ResizeMethod.BILINEAR,
                     align_corners=False,
-                    pad_to_max_dimension=False):
+                    pad_to_max_dimension=False,
+                    per_channel_pad_value=(0, 0, 0)):
   """Resizes an image so its dimensions are within the provided value.
 
   The output size can be described by two cases:
@@ -2153,6 +2154,8 @@ def resize_to_range(image,
       so the resulting image is of the spatial size
       [max_dimension, max_dimension]. If masks are included they are padded
       similarly.
+    per_channel_pad_value: A tuple of per-channel scalar value to use for
+      padding. By default pads zeros.
 
   Returns:
     Note that the position of the resized_image_shape changes based on whether
@@ -2181,8 +2184,20 @@ def resize_to_range(image,
         image, new_size[:-1], method=method, align_corners=align_corners)
 
     if pad_to_max_dimension:
-      new_image = tf.image.pad_to_bounding_box(
-          new_image, 0, 0, max_dimension, max_dimension)
+      channels = tf.unstack(new_image, axis=2)
+      if len(channels) != len(per_channel_pad_value):
+        raise ValueError('Number of channels must be equal to the length of '
+                         'per-channel pad value.')
+      new_image = tf.stack(
+          [
+              tf.pad(
+                  channels[i], [[0, max_dimension - new_size[0]],
+                                [0, max_dimension - new_size[1]]],
+                  constant_values=per_channel_pad_value[i])
+              for i in range(len(channels))
+          ],
+          axis=2)
+      new_image.set_shape([max_dimension, max_dimension, 3])
 
     result = [new_image]
     if masks is not None:
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 6af45d88..588a3f90 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -2316,6 +2316,46 @@ class PreprocessorTest(tf.test.TestCase):
                                               np.random.randn(*in_shape)})
         self.assertAllEqual(out_image_shape, expected_shape)
 
+  def testResizeToRangeWithPadToMaxDimensionReturnsCorrectShapes(self):
+    in_shape_list = [[60, 40, 3], [15, 30, 3], [15, 50, 3]]
+    min_dim = 50
+    max_dim = 100
+    expected_shape_list = [[100, 100, 3], [100, 100, 3], [100, 100, 3]]
+
+    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
+      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+      out_image, _ = preprocessor.resize_to_range(
+          in_image,
+          min_dimension=min_dim,
+          max_dimension=max_dim,
+          pad_to_max_dimension=True)
+      self.assertAllEqual(out_image.shape.as_list(), expected_shape)
+      out_image_shape = tf.shape(out_image)
+      with self.test_session() as sess:
+        out_image_shape = sess.run(
+            out_image_shape, feed_dict={in_image: np.random.randn(*in_shape)})
+        self.assertAllEqual(out_image_shape, expected_shape)
+
+  def testResizeToRangeWithPadToMaxDimensionReturnsCorrectTensor(self):
+    in_image_np = np.array([[[0, 1, 2]]], np.float32)
+    ex_image_np = np.array(
+        [[[0, 1, 2], [123.68, 116.779, 103.939]],
+         [[123.68, 116.779, 103.939], [123.68, 116.779, 103.939]]], np.float32)
+    min_dim = 1
+    max_dim = 2
+
+    in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+    out_image, _ = preprocessor.resize_to_range(
+        in_image,
+        min_dimension=min_dim,
+        max_dimension=max_dim,
+        pad_to_max_dimension=True,
+        per_channel_pad_value=(123.68, 116.779, 103.939))
+
+    with self.test_session() as sess:
+      out_image_np = sess.run(out_image, feed_dict={in_image: in_image_np})
+      self.assertAllClose(ex_image_np, out_image_np)
+
   def testResizeToRangeWithMasksPreservesStaticSpatialShape(self):
     """Tests image resizing, checking output sizes."""
     in_image_shape_list = [[60, 40, 3], [15, 30, 3]]
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index ddd7ad38..11282da6 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -34,6 +34,7 @@ class InputDataFields(object):
 
   Attributes:
     image: image.
+    image_additional_channels: additional channels.
     original_image: image in the original input size.
     key: unique key corresponding to image.
     source_id: source of the original image.
@@ -66,6 +67,7 @@ class InputDataFields(object):
     multiclass_scores: the label score per class for each box.
   """
   image = 'image'
+  image_additional_channels = 'image_additional_channels'
   original_image = 'original_image'
   key = 'key'
   source_id = 'source_id'
@@ -161,6 +163,8 @@ class TfExampleFields(object):
     height: height of image in pixels, e.g. 462
     width: width of image in pixels, e.g. 581
     source_id: original source of the image
+    image_class_text: image-level label in text format
+    image_class_label: image-level label in numerical format
     object_class_text: labels in text format, e.g. ["person", "cat"]
     object_class_label: labels in numbers, e.g. [16, 8]
     object_bbox_xmin: xmin coordinates of groundtruth box, e.g. 10, 30
@@ -195,6 +199,8 @@ class TfExampleFields(object):
   height = 'image/height'
   width = 'image/width'
   source_id = 'image/source_id'
+  image_class_text = 'image/class/text'
+  image_class_label = 'image/class/label'
   object_class_text = 'image/object/class/text'
   object_class_label = 'image/object/class/label'
   object_bbox_ymin = 'image/object/bbox/ymin'
diff --git a/research/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt b/research/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt
new file mode 100644
index 00000000..044f6d4c
--- /dev/null
+++ b/research/object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt
@@ -0,0 +1,2500 @@
+item {
+  name: "/m/061hd_"
+  id: 1
+  display_name: "Infant bed"
+}
+item {
+  name: "/m/06m11"
+  id: 2
+  display_name: "Rose"
+}
+item {
+  name: "/m/03120"
+  id: 3
+  display_name: "Flag"
+}
+item {
+  name: "/m/01kb5b"
+  id: 4
+  display_name: "Flashlight"
+}
+item {
+  name: "/m/0120dh"
+  id: 5
+  display_name: "Sea turtle"
+}
+item {
+  name: "/m/0dv5r"
+  id: 6
+  display_name: "Camera"
+}
+item {
+  name: "/m/0jbk"
+  id: 7
+  display_name: "Animal"
+}
+item {
+  name: "/m/0174n1"
+  id: 8
+  display_name: "Glove"
+}
+item {
+  name: "/m/09f_2"
+  id: 9
+  display_name: "Crocodile"
+}
+item {
+  name: "/m/01xq0k1"
+  id: 10
+  display_name: "Cattle"
+}
+item {
+  name: "/m/03jm5"
+  id: 11
+  display_name: "House"
+}
+item {
+  name: "/m/02g30s"
+  id: 12
+  display_name: "Guacamole"
+}
+item {
+  name: "/m/05z6w"
+  id: 13
+  display_name: "Penguin"
+}
+item {
+  name: "/m/01jfm_"
+  id: 14
+  display_name: "Vehicle registration plate"
+}
+item {
+  name: "/m/076lb9"
+  id: 15
+  display_name: "Training bench"
+}
+item {
+  name: "/m/0gj37"
+  id: 16
+  display_name: "Ladybug"
+}
+item {
+  name: "/m/0k0pj"
+  id: 17
+  display_name: "Human nose"
+}
+item {
+  name: "/m/0kpqd"
+  id: 18
+  display_name: "Watermelon"
+}
+item {
+  name: "/m/0l14j_"
+  id: 19
+  display_name: "Flute"
+}
+item {
+  name: "/m/0cyf8"
+  id: 20
+  display_name: "Butterfly"
+}
+item {
+  name: "/m/0174k2"
+  id: 21
+  display_name: "Washing machine"
+}
+item {
+  name: "/m/0dq75"
+  id: 22
+  display_name: "Raccoon"
+}
+item {
+  name: "/m/076bq"
+  id: 23
+  display_name: "Segway"
+}
+item {
+  name: "/m/07crc"
+  id: 24
+  display_name: "Taco"
+}
+item {
+  name: "/m/0d8zb"
+  id: 25
+  display_name: "Jellyfish"
+}
+item {
+  name: "/m/0fszt"
+  id: 26
+  display_name: "Cake"
+}
+item {
+  name: "/m/0k1tl"
+  id: 27
+  display_name: "Pen"
+}
+item {
+  name: "/m/020kz"
+  id: 28
+  display_name: "Cannon"
+}
+item {
+  name: "/m/09728"
+  id: 29
+  display_name: "Bread"
+}
+item {
+  name: "/m/07j7r"
+  id: 30
+  display_name: "Tree"
+}
+item {
+  name: "/m/0fbdv"
+  id: 31
+  display_name: "Shellfish"
+}
+item {
+  name: "/m/03ssj5"
+  id: 32
+  display_name: "Bed"
+}
+item {
+  name: "/m/03qrc"
+  id: 33
+  display_name: "Hamster"
+}
+item {
+  name: "/m/02dl1y"
+  id: 34
+  display_name: "Hat"
+}
+item {
+  name: "/m/01k6s3"
+  id: 35
+  display_name: "Toaster"
+}
+item {
+  name: "/m/02jfl0"
+  id: 36
+  display_name: "Sombrero"
+}
+item {
+  name: "/m/01krhy"
+  id: 37
+  display_name: "Tiara"
+}
+item {
+  name: "/m/04kkgm"
+  id: 38
+  display_name: "Bowl"
+}
+item {
+  name: "/m/0ft9s"
+  id: 39
+  display_name: "Dragonfly"
+}
+item {
+  name: "/m/0d_2m"
+  id: 40
+  display_name: "Moths and butterflies"
+}
+item {
+  name: "/m/0czz2"
+  id: 41
+  display_name: "Antelope"
+}
+item {
+  name: "/m/0f4s2w"
+  id: 42
+  display_name: "Vegetable"
+}
+item {
+  name: "/m/07dd4"
+  id: 43
+  display_name: "Torch"
+}
+item {
+  name: "/m/0cgh4"
+  id: 44
+  display_name: "Building"
+}
+item {
+  name: "/m/03bbps"
+  id: 45
+  display_name: "Power plugs and sockets"
+}
+item {
+  name: "/m/02pjr4"
+  id: 46
+  display_name: "Blender"
+}
+item {
+  name: "/m/04p0qw"
+  id: 47
+  display_name: "Billiard table"
+}
+item {
+  name: "/m/02pdsw"
+  id: 48
+  display_name: "Cutting board"
+}
+item {
+  name: "/m/01yx86"
+  id: 49
+  display_name: "Bronze sculpture"
+}
+item {
+  name: "/m/09dzg"
+  id: 50
+  display_name: "Turtle"
+}
+item {
+  name: "/m/0hkxq"
+  id: 51
+  display_name: "Broccoli"
+}
+item {
+  name: "/m/07dm6"
+  id: 52
+  display_name: "Tiger"
+}
+item {
+  name: "/m/054_l"
+  id: 53
+  display_name: "Mirror"
+}
+item {
+  name: "/m/01dws"
+  id: 54
+  display_name: "Bear"
+}
+item {
+  name: "/m/027pcv"
+  id: 55
+  display_name: "Zucchini"
+}
+item {
+  name: "/m/01d40f"
+  id: 56
+  display_name: "Dress"
+}
+item {
+  name: "/m/02rgn06"
+  id: 57
+  display_name: "Volleyball"
+}
+item {
+  name: "/m/0342h"
+  id: 58
+  display_name: "Guitar"
+}
+item {
+  name: "/m/06bt6"
+  id: 59
+  display_name: "Reptile"
+}
+item {
+  name: "/m/0323sq"
+  id: 60
+  display_name: "Golf cart"
+}
+item {
+  name: "/m/02zvsm"
+  id: 61
+  display_name: "Tart"
+}
+item {
+  name: "/m/02fq_6"
+  id: 62
+  display_name: "Fedora"
+}
+item {
+  name: "/m/01lrl"
+  id: 63
+  display_name: "Carnivore"
+}
+item {
+  name: "/m/0k4j"
+  id: 64
+  display_name: "Car"
+}
+item {
+  name: "/m/04h7h"
+  id: 65
+  display_name: "Lighthouse"
+}
+item {
+  name: "/m/07xyvk"
+  id: 66
+  display_name: "Coffeemaker"
+}
+item {
+  name: "/m/03y6mg"
+  id: 67
+  display_name: "Food processor"
+}
+item {
+  name: "/m/07r04"
+  id: 68
+  display_name: "Truck"
+}
+item {
+  name: "/m/03__z0"
+  id: 69
+  display_name: "Bookcase"
+}
+item {
+  name: "/m/019w40"
+  id: 70
+  display_name: "Surfboard"
+}
+item {
+  name: "/m/09j5n"
+  id: 71
+  display_name: "Footwear"
+}
+item {
+  name: "/m/0cvnqh"
+  id: 72
+  display_name: "Bench"
+}
+item {
+  name: "/m/01llwg"
+  id: 73
+  display_name: "Necklace"
+}
+item {
+  name: "/m/0c9ph5"
+  id: 74
+  display_name: "Flower"
+}
+item {
+  name: "/m/015x5n"
+  id: 75
+  display_name: "Radish"
+}
+item {
+  name: "/m/0gd2v"
+  id: 76
+  display_name: "Marine mammal"
+}
+item {
+  name: "/m/04v6l4"
+  id: 77
+  display_name: "Frying pan"
+}
+item {
+  name: "/m/02jz0l"
+  id: 78
+  display_name: "Tap"
+}
+item {
+  name: "/m/0dj6p"
+  id: 79
+  display_name: "Peach"
+}
+item {
+  name: "/m/04ctx"
+  id: 80
+  display_name: "Knife"
+}
+item {
+  name: "/m/080hkjn"
+  id: 81
+  display_name: "Handbag"
+}
+item {
+  name: "/m/01c648"
+  id: 82
+  display_name: "Laptop"
+}
+item {
+  name: "/m/01j61q"
+  id: 83
+  display_name: "Tent"
+}
+item {
+  name: "/m/012n7d"
+  id: 84
+  display_name: "Ambulance"
+}
+item {
+  name: "/m/025nd"
+  id: 85
+  display_name: "Christmas tree"
+}
+item {
+  name: "/m/09csl"
+  id: 86
+  display_name: "Eagle"
+}
+item {
+  name: "/m/01lcw4"
+  id: 87
+  display_name: "Limousine"
+}
+item {
+  name: "/m/0h8n5zk"
+  id: 88
+  display_name: "Kitchen & dining room table"
+}
+item {
+  name: "/m/0633h"
+  id: 89
+  display_name: "Polar bear"
+}
+item {
+  name: "/m/01fdzj"
+  id: 90
+  display_name: "Tower"
+}
+item {
+  name: "/m/01226z"
+  id: 91
+  display_name: "Football"
+}
+item {
+  name: "/m/0mw_6"
+  id: 92
+  display_name: "Willow"
+}
+item {
+  name: "/m/04hgtk"
+  id: 93
+  display_name: "Human head"
+}
+item {
+  name: "/m/02pv19"
+  id: 94
+  display_name: "Stop sign"
+}
+item {
+  name: "/m/09qck"
+  id: 95
+  display_name: "Banana"
+}
+item {
+  name: "/m/063rgb"
+  id: 96
+  display_name: "Mixer"
+}
+item {
+  name: "/m/0lt4_"
+  id: 97
+  display_name: "Binoculars"
+}
+item {
+  name: "/m/0270h"
+  id: 98
+  display_name: "Dessert"
+}
+item {
+  name: "/m/01h3n"
+  id: 99
+  display_name: "Bee"
+}
+item {
+  name: "/m/01mzpv"
+  id: 100
+  display_name: "Chair"
+}
+item {
+  name: "/m/04169hn"
+  id: 101
+  display_name: "Wood-burning stove"
+}
+item {
+  name: "/m/0fm3zh"
+  id: 102
+  display_name: "Flowerpot"
+}
+item {
+  name: "/m/0d20w4"
+  id: 103
+  display_name: "Beaker"
+}
+item {
+  name: "/m/0_cp5"
+  id: 104
+  display_name: "Oyster"
+}
+item {
+  name: "/m/01dy8n"
+  id: 105
+  display_name: "Woodpecker"
+}
+item {
+  name: "/m/03m5k"
+  id: 106
+  display_name: "Harp"
+}
+item {
+  name: "/m/03dnzn"
+  id: 107
+  display_name: "Bathtub"
+}
+item {
+  name: "/m/0h8mzrc"
+  id: 108
+  display_name: "Wall clock"
+}
+item {
+  name: "/m/0h8mhzd"
+  id: 109
+  display_name: "Sports uniform"
+}
+item {
+  name: "/m/03d443"
+  id: 110
+  display_name: "Rhinoceros"
+}
+item {
+  name: "/m/01gllr"
+  id: 111
+  display_name: "Beehive"
+}
+item {
+  name: "/m/0642b4"
+  id: 112
+  display_name: "Cupboard"
+}
+item {
+  name: "/m/09b5t"
+  id: 113
+  display_name: "Chicken"
+}
+item {
+  name: "/m/04yx4"
+  id: 114
+  display_name: "Man"
+}
+item {
+  name: "/m/01f8m5"
+  id: 115
+  display_name: "Blue jay"
+}
+item {
+  name: "/m/015x4r"
+  id: 116
+  display_name: "Cucumber"
+}
+item {
+  name: "/m/01j51"
+  id: 117
+  display_name: "Balloon"
+}
+item {
+  name: "/m/02zt3"
+  id: 118
+  display_name: "Kite"
+}
+item {
+  name: "/m/03tw93"
+  id: 119
+  display_name: "Fireplace"
+}
+item {
+  name: "/m/01jfsr"
+  id: 120
+  display_name: "Lantern"
+}
+item {
+  name: "/m/04ylt"
+  id: 121
+  display_name: "Missile"
+}
+item {
+  name: "/m/0bt_c3"
+  id: 122
+  display_name: "Book"
+}
+item {
+  name: "/m/0cmx8"
+  id: 123
+  display_name: "Spoon"
+}
+item {
+  name: "/m/0hqkz"
+  id: 124
+  display_name: "Grapefruit"
+}
+item {
+  name: "/m/071qp"
+  id: 125
+  display_name: "Squirrel"
+}
+item {
+  name: "/m/0cyhj_"
+  id: 126
+  display_name: "Orange"
+}
+item {
+  name: "/m/01xygc"
+  id: 127
+  display_name: "Coat"
+}
+item {
+  name: "/m/0420v5"
+  id: 128
+  display_name: "Punching bag"
+}
+item {
+  name: "/m/0898b"
+  id: 129
+  display_name: "Zebra"
+}
+item {
+  name: "/m/01knjb"
+  id: 130
+  display_name: "Billboard"
+}
+item {
+  name: "/m/0199g"
+  id: 131
+  display_name: "Bicycle"
+}
+item {
+  name: "/m/03c7gz"
+  id: 132
+  display_name: "Door handle"
+}
+item {
+  name: "/m/02x984l"
+  id: 133
+  display_name: "Mechanical fan"
+}
+item {
+  name: "/m/04zwwv"
+  id: 134
+  display_name: "Ring binder"
+}
+item {
+  name: "/m/04bcr3"
+  id: 135
+  display_name: "Table"
+}
+item {
+  name: "/m/0gv1x"
+  id: 136
+  display_name: "Parrot"
+}
+item {
+  name: "/m/01nq26"
+  id: 137
+  display_name: "Sock"
+}
+item {
+  name: "/m/02s195"
+  id: 138
+  display_name: "Vase"
+}
+item {
+  name: "/m/083kb"
+  id: 139
+  display_name: "Weapon"
+}
+item {
+  name: "/m/06nrc"
+  id: 140
+  display_name: "Shotgun"
+}
+item {
+  name: "/m/0jyfg"
+  id: 141
+  display_name: "Glasses"
+}
+item {
+  name: "/m/0nybt"
+  id: 142
+  display_name: "Seahorse"
+}
+item {
+  name: "/m/0176mf"
+  id: 143
+  display_name: "Belt"
+}
+item {
+  name: "/m/01rzcn"
+  id: 144
+  display_name: "Watercraft"
+}
+item {
+  name: "/m/0d4v4"
+  id: 145
+  display_name: "Window"
+}
+item {
+  name: "/m/03bk1"
+  id: 146
+  display_name: "Giraffe"
+}
+item {
+  name: "/m/096mb"
+  id: 147
+  display_name: "Lion"
+}
+item {
+  name: "/m/0h9mv"
+  id: 148
+  display_name: "Tire"
+}
+item {
+  name: "/m/07yv9"
+  id: 149
+  display_name: "Vehicle"
+}
+item {
+  name: "/m/0ph39"
+  id: 150
+  display_name: "Canoe"
+}
+item {
+  name: "/m/01rkbr"
+  id: 151
+  display_name: "Tie"
+}
+item {
+  name: "/m/0gjbg72"
+  id: 152
+  display_name: "Shelf"
+}
+item {
+  name: "/m/06z37_"
+  id: 153
+  display_name: "Picture frame"
+}
+item {
+  name: "/m/01m4t"
+  id: 154
+  display_name: "Printer"
+}
+item {
+  name: "/m/035r7c"
+  id: 155
+  display_name: "Human leg"
+}
+item {
+  name: "/m/019jd"
+  id: 156
+  display_name: "Boat"
+}
+item {
+  name: "/m/02tsc9"
+  id: 157
+  display_name: "Slow cooker"
+}
+item {
+  name: "/m/015wgc"
+  id: 158
+  display_name: "Croissant"
+}
+item {
+  name: "/m/0c06p"
+  id: 159
+  display_name: "Candle"
+}
+item {
+  name: "/m/01dwwc"
+  id: 160
+  display_name: "Pancake"
+}
+item {
+  name: "/m/034c16"
+  id: 161
+  display_name: "Pillow"
+}
+item {
+  name: "/m/0242l"
+  id: 162
+  display_name: "Coin"
+}
+item {
+  name: "/m/02lbcq"
+  id: 163
+  display_name: "Stretcher"
+}
+item {
+  name: "/m/03nfch"
+  id: 164
+  display_name: "Sandal"
+}
+item {
+  name: "/m/03bt1vf"
+  id: 165
+  display_name: "Woman"
+}
+item {
+  name: "/m/01lynh"
+  id: 166
+  display_name: "Stairs"
+}
+item {
+  name: "/m/03q5t"
+  id: 167
+  display_name: "Harpsichord"
+}
+item {
+  name: "/m/0fqt361"
+  id: 168
+  display_name: "Stool"
+}
+item {
+  name: "/m/01bjv"
+  id: 169
+  display_name: "Bus"
+}
+item {
+  name: "/m/01s55n"
+  id: 170
+  display_name: "Suitcase"
+}
+item {
+  name: "/m/0283dt1"
+  id: 171
+  display_name: "Human mouth"
+}
+item {
+  name: "/m/01z1kdw"
+  id: 172
+  display_name: "Juice"
+}
+item {
+  name: "/m/016m2d"
+  id: 173
+  display_name: "Skull"
+}
+item {
+  name: "/m/02dgv"
+  id: 174
+  display_name: "Door"
+}
+item {
+  name: "/m/07y_7"
+  id: 175
+  display_name: "Violin"
+}
+item {
+  name: "/m/01_5g"
+  id: 176
+  display_name: "Chopsticks"
+}
+item {
+  name: "/m/06_72j"
+  id: 177
+  display_name: "Digital clock"
+}
+item {
+  name: "/m/0ftb8"
+  id: 178
+  display_name: "Sunflower"
+}
+item {
+  name: "/m/0c29q"
+  id: 179
+  display_name: "Leopard"
+}
+item {
+  name: "/m/0jg57"
+  id: 180
+  display_name: "Bell pepper"
+}
+item {
+  name: "/m/02l8p9"
+  id: 181
+  display_name: "Harbor seal"
+}
+item {
+  name: "/m/078jl"
+  id: 182
+  display_name: "Snake"
+}
+item {
+  name: "/m/0llzx"
+  id: 183
+  display_name: "Sewing machine"
+}
+item {
+  name: "/m/0dbvp"
+  id: 184
+  display_name: "Goose"
+}
+item {
+  name: "/m/09ct_"
+  id: 185
+  display_name: "Helicopter"
+}
+item {
+  name: "/m/0dkzw"
+  id: 186
+  display_name: "Seat belt"
+}
+item {
+  name: "/m/02p5f1q"
+  id: 187
+  display_name: "Coffee cup"
+}
+item {
+  name: "/m/0fx9l"
+  id: 188
+  display_name: "Microwave oven"
+}
+item {
+  name: "/m/01b9xk"
+  id: 189
+  display_name: "Hot dog"
+}
+item {
+  name: "/m/0b3fp9"
+  id: 190
+  display_name: "Countertop"
+}
+item {
+  name: "/m/0h8n27j"
+  id: 191
+  display_name: "Serving tray"
+}
+item {
+  name: "/m/0h8n6f9"
+  id: 192
+  display_name: "Dog bed"
+}
+item {
+  name: "/m/01599"
+  id: 193
+  display_name: "Beer"
+}
+item {
+  name: "/m/017ftj"
+  id: 194
+  display_name: "Sunglasses"
+}
+item {
+  name: "/m/044r5d"
+  id: 195
+  display_name: "Golf ball"
+}
+item {
+  name: "/m/01dwsz"
+  id: 196
+  display_name: "Waffle"
+}
+item {
+  name: "/m/0cdl1"
+  id: 197
+  display_name: "Palm tree"
+}
+item {
+  name: "/m/07gql"
+  id: 198
+  display_name: "Trumpet"
+}
+item {
+  name: "/m/0hdln"
+  id: 199
+  display_name: "Ruler"
+}
+item {
+  name: "/m/0zvk5"
+  id: 200
+  display_name: "Helmet"
+}
+item {
+  name: "/m/012w5l"
+  id: 201
+  display_name: "Ladder"
+}
+item {
+  name: "/m/021sj1"
+  id: 202
+  display_name: "Office building"
+}
+item {
+  name: "/m/0bh9flk"
+  id: 203
+  display_name: "Tablet computer"
+}
+item {
+  name: "/m/09gtd"
+  id: 204
+  display_name: "Toilet paper"
+}
+item {
+  name: "/m/0jwn_"
+  id: 205
+  display_name: "Pomegranate"
+}
+item {
+  name: "/m/02wv6h6"
+  id: 206
+  display_name: "Skirt"
+}
+item {
+  name: "/m/02wv84t"
+  id: 207
+  display_name: "Gas stove"
+}
+item {
+  name: "/m/021mn"
+  id: 208
+  display_name: "Cookie"
+}
+item {
+  name: "/m/018p4k"
+  id: 209
+  display_name: "Cart"
+}
+item {
+  name: "/m/06j2d"
+  id: 210
+  display_name: "Raven"
+}
+item {
+  name: "/m/033cnk"
+  id: 211
+  display_name: "Egg"
+}
+item {
+  name: "/m/01j3zr"
+  id: 212
+  display_name: "Burrito"
+}
+item {
+  name: "/m/03fwl"
+  id: 213
+  display_name: "Goat"
+}
+item {
+  name: "/m/058qzx"
+  id: 214
+  display_name: "Kitchen knife"
+}
+item {
+  name: "/m/06_fw"
+  id: 215
+  display_name: "Skateboard"
+}
+item {
+  name: "/m/02x8cch"
+  id: 216
+  display_name: "Salt and pepper shakers"
+}
+item {
+  name: "/m/04g2r"
+  id: 217
+  display_name: "Lynx"
+}
+item {
+  name: "/m/01b638"
+  id: 218
+  display_name: "Boot"
+}
+item {
+  name: "/m/099ssp"
+  id: 219
+  display_name: "Platter"
+}
+item {
+  name: "/m/071p9"
+  id: 220
+  display_name: "Ski"
+}
+item {
+  name: "/m/01gkx_"
+  id: 221
+  display_name: "Swimwear"
+}
+item {
+  name: "/m/0b_rs"
+  id: 222
+  display_name: "Swimming pool"
+}
+item {
+  name: "/m/03v5tg"
+  id: 223
+  display_name: "Drinking straw"
+}
+item {
+  name: "/m/01j5ks"
+  id: 224
+  display_name: "Wrench"
+}
+item {
+  name: "/m/026t6"
+  id: 225
+  display_name: "Drum"
+}
+item {
+  name: "/m/0_k2"
+  id: 226
+  display_name: "Ant"
+}
+item {
+  name: "/m/039xj_"
+  id: 227
+  display_name: "Human ear"
+}
+item {
+  name: "/m/01b7fy"
+  id: 228
+  display_name: "Headphones"
+}
+item {
+  name: "/m/0220r2"
+  id: 229
+  display_name: "Fountain"
+}
+item {
+  name: "/m/015p6"
+  id: 230
+  display_name: "Bird"
+}
+item {
+  name: "/m/0fly7"
+  id: 231
+  display_name: "Jeans"
+}
+item {
+  name: "/m/07c52"
+  id: 232
+  display_name: "Television"
+}
+item {
+  name: "/m/0n28_"
+  id: 233
+  display_name: "Crab"
+}
+item {
+  name: "/m/0hg7b"
+  id: 234
+  display_name: "Microphone"
+}
+item {
+  name: "/m/019dx1"
+  id: 235
+  display_name: "Home appliance"
+}
+item {
+  name: "/m/04vv5k"
+  id: 236
+  display_name: "Snowplow"
+}
+item {
+  name: "/m/020jm"
+  id: 237
+  display_name: "Beetle"
+}
+item {
+  name: "/m/047v4b"
+  id: 238
+  display_name: "Artichoke"
+}
+item {
+  name: "/m/01xs3r"
+  id: 239
+  display_name: "Jet ski"
+}
+item {
+  name: "/m/03kt2w"
+  id: 240
+  display_name: "Stationary bicycle"
+}
+item {
+  name: "/m/03q69"
+  id: 241
+  display_name: "Human hair"
+}
+item {
+  name: "/m/01dxs"
+  id: 242
+  display_name: "Brown bear"
+}
+item {
+  name: "/m/01h8tj"
+  id: 243
+  display_name: "Starfish"
+}
+item {
+  name: "/m/0dt3t"
+  id: 244
+  display_name: "Fork"
+}
+item {
+  name: "/m/0cjq5"
+  id: 245
+  display_name: "Lobster"
+}
+item {
+  name: "/m/0h8lkj8"
+  id: 246
+  display_name: "Corded phone"
+}
+item {
+  name: "/m/0271t"
+  id: 247
+  display_name: "Drink"
+}
+item {
+  name: "/m/03q5c7"
+  id: 248
+  display_name: "Saucer"
+}
+item {
+  name: "/m/0fj52s"
+  id: 249
+  display_name: "Carrot"
+}
+item {
+  name: "/m/03vt0"
+  id: 250
+  display_name: "Insect"
+}
+item {
+  name: "/m/01x3z"
+  id: 251
+  display_name: "Clock"
+}
+item {
+  name: "/m/0d5gx"
+  id: 252
+  display_name: "Castle"
+}
+item {
+  name: "/m/0h8my_4"
+  id: 253
+  display_name: "Tennis racket"
+}
+item {
+  name: "/m/03ldnb"
+  id: 254
+  display_name: "Ceiling fan"
+}
+item {
+  name: "/m/0cjs7"
+  id: 255
+  display_name: "Asparagus"
+}
+item {
+  name: "/m/0449p"
+  id: 256
+  display_name: "Jaguar"
+}
+item {
+  name: "/m/04szw"
+  id: 257
+  display_name: "Musical instrument"
+}
+item {
+  name: "/m/07jdr"
+  id: 258
+  display_name: "Train"
+}
+item {
+  name: "/m/01yrx"
+  id: 259
+  display_name: "Cat"
+}
+item {
+  name: "/m/06c54"
+  id: 260
+  display_name: "Rifle"
+}
+item {
+  name: "/m/04h8sr"
+  id: 261
+  display_name: "Dumbbell"
+}
+item {
+  name: "/m/050k8"
+  id: 262
+  display_name: "Mobile phone"
+}
+item {
+  name: "/m/0pg52"
+  id: 263
+  display_name: "Taxi"
+}
+item {
+  name: "/m/02f9f_"
+  id: 264
+  display_name: "Shower"
+}
+item {
+  name: "/m/054fyh"
+  id: 265
+  display_name: "Pitcher"
+}
+item {
+  name: "/m/09k_b"
+  id: 266
+  display_name: "Lemon"
+}
+item {
+  name: "/m/03xxp"
+  id: 267
+  display_name: "Invertebrate"
+}
+item {
+  name: "/m/0jly1"
+  id: 268
+  display_name: "Turkey"
+}
+item {
+  name: "/m/06k2mb"
+  id: 269
+  display_name: "High heels"
+}
+item {
+  name: "/m/04yqq2"
+  id: 270
+  display_name: "Bust"
+}
+item {
+  name: "/m/0bwd_0j"
+  id: 271
+  display_name: "Elephant"
+}
+item {
+  name: "/m/02h19r"
+  id: 272
+  display_name: "Scarf"
+}
+item {
+  name: "/m/02zn6n"
+  id: 273
+  display_name: "Barrel"
+}
+item {
+  name: "/m/07c6l"
+  id: 274
+  display_name: "Trombone"
+}
+item {
+  name: "/m/05zsy"
+  id: 275
+  display_name: "Pumpkin"
+}
+item {
+  name: "/m/025dyy"
+  id: 276
+  display_name: "Box"
+}
+item {
+  name: "/m/07j87"
+  id: 277
+  display_name: "Tomato"
+}
+item {
+  name: "/m/09ld4"
+  id: 278
+  display_name: "Frog"
+}
+item {
+  name: "/m/01vbnl"
+  id: 279
+  display_name: "Bidet"
+}
+item {
+  name: "/m/0dzct"
+  id: 280
+  display_name: "Human face"
+}
+item {
+  name: "/m/03fp41"
+  id: 281
+  display_name: "Houseplant"
+}
+item {
+  name: "/m/0h2r6"
+  id: 282
+  display_name: "Van"
+}
+item {
+  name: "/m/0by6g"
+  id: 283
+  display_name: "Shark"
+}
+item {
+  name: "/m/0cxn2"
+  id: 284
+  display_name: "Ice cream"
+}
+item {
+  name: "/m/04tn4x"
+  id: 285
+  display_name: "Swim cap"
+}
+item {
+  name: "/m/0f6wt"
+  id: 286
+  display_name: "Falcon"
+}
+item {
+  name: "/m/05n4y"
+  id: 287
+  display_name: "Ostrich"
+}
+item {
+  name: "/m/0gxl3"
+  id: 288
+  display_name: "Handgun"
+}
+item {
+  name: "/m/02d9qx"
+  id: 289
+  display_name: "Whiteboard"
+}
+item {
+  name: "/m/04m9y"
+  id: 290
+  display_name: "Lizard"
+}
+item {
+  name: "/m/05z55"
+  id: 291
+  display_name: "Pasta"
+}
+item {
+  name: "/m/01x3jk"
+  id: 292
+  display_name: "Snowmobile"
+}
+item {
+  name: "/m/0h8l4fh"
+  id: 293
+  display_name: "Light bulb"
+}
+item {
+  name: "/m/031b6r"
+  id: 294
+  display_name: "Window blind"
+}
+item {
+  name: "/m/01tcjp"
+  id: 295
+  display_name: "Muffin"
+}
+item {
+  name: "/m/01f91_"
+  id: 296
+  display_name: "Pretzel"
+}
+item {
+  name: "/m/02522"
+  id: 297
+  display_name: "Computer monitor"
+}
+item {
+  name: "/m/0319l"
+  id: 298
+  display_name: "Horn"
+}
+item {
+  name: "/m/0c_jw"
+  id: 299
+  display_name: "Furniture"
+}
+item {
+  name: "/m/0l515"
+  id: 300
+  display_name: "Sandwich"
+}
+item {
+  name: "/m/0306r"
+  id: 301
+  display_name: "Fox"
+}
+item {
+  name: "/m/0crjs"
+  id: 302
+  display_name: "Convenience store"
+}
+item {
+  name: "/m/0ch_cf"
+  id: 303
+  display_name: "Fish"
+}
+item {
+  name: "/m/02xwb"
+  id: 304
+  display_name: "Fruit"
+}
+item {
+  name: "/m/01r546"
+  id: 305
+  display_name: "Earrings"
+}
+item {
+  name: "/m/03rszm"
+  id: 306
+  display_name: "Curtain"
+}
+item {
+  name: "/m/0388q"
+  id: 307
+  display_name: "Grape"
+}
+item {
+  name: "/m/03m3pdh"
+  id: 308
+  display_name: "Sofa bed"
+}
+item {
+  name: "/m/03k3r"
+  id: 309
+  display_name: "Horse"
+}
+item {
+  name: "/m/0hf58v5"
+  id: 310
+  display_name: "Luggage and bags"
+}
+item {
+  name: "/m/01y9k5"
+  id: 311
+  display_name: "Desk"
+}
+item {
+  name: "/m/05441v"
+  id: 312
+  display_name: "Crutch"
+}
+item {
+  name: "/m/03p3bw"
+  id: 313
+  display_name: "Bicycle helmet"
+}
+item {
+  name: "/m/0175cv"
+  id: 314
+  display_name: "Tick"
+}
+item {
+  name: "/m/0cmf2"
+  id: 315
+  display_name: "Airplane"
+}
+item {
+  name: "/m/0ccs93"
+  id: 316
+  display_name: "Canary"
+}
+item {
+  name: "/m/02d1br"
+  id: 317
+  display_name: "Spatula"
+}
+item {
+  name: "/m/0gjkl"
+  id: 318
+  display_name: "Watch"
+}
+item {
+  name: "/m/0jqgx"
+  id: 319
+  display_name: "Lily"
+}
+item {
+  name: "/m/0h99cwc"
+  id: 320
+  display_name: "Kitchen appliance"
+}
+item {
+  name: "/m/047j0r"
+  id: 321
+  display_name: "Filing cabinet"
+}
+item {
+  name: "/m/0k5j"
+  id: 322
+  display_name: "Aircraft"
+}
+item {
+  name: "/m/0h8n6ft"
+  id: 323
+  display_name: "Cake stand"
+}
+item {
+  name: "/m/0gm28"
+  id: 324
+  display_name: "Candy"
+}
+item {
+  name: "/m/0130jx"
+  id: 325
+  display_name: "Sink"
+}
+item {
+  name: "/m/04rmv"
+  id: 326
+  display_name: "Mouse"
+}
+item {
+  name: "/m/081qc"
+  id: 327
+  display_name: "Wine"
+}
+item {
+  name: "/m/0qmmr"
+  id: 328
+  display_name: "Wheelchair"
+}
+item {
+  name: "/m/03fj2"
+  id: 329
+  display_name: "Goldfish"
+}
+item {
+  name: "/m/040b_t"
+  id: 330
+  display_name: "Refrigerator"
+}
+item {
+  name: "/m/02y6n"
+  id: 331
+  display_name: "French fries"
+}
+item {
+  name: "/m/0fqfqc"
+  id: 332
+  display_name: "Drawer"
+}
+item {
+  name: "/m/030610"
+  id: 333
+  display_name: "Treadmill"
+}
+item {
+  name: "/m/07kng9"
+  id: 334
+  display_name: "Picnic basket"
+}
+item {
+  name: "/m/029b3"
+  id: 335
+  display_name: "Dice"
+}
+item {
+  name: "/m/0fbw6"
+  id: 336
+  display_name: "Cabbage"
+}
+item {
+  name: "/m/07qxg_"
+  id: 337
+  display_name: "Football helmet"
+}
+item {
+  name: "/m/068zj"
+  id: 338
+  display_name: "Pig"
+}
+item {
+  name: "/m/01g317"
+  id: 339
+  display_name: "Person"
+}
+item {
+  name: "/m/01bfm9"
+  id: 340
+  display_name: "Shorts"
+}
+item {
+  name: "/m/02068x"
+  id: 341
+  display_name: "Gondola"
+}
+item {
+  name: "/m/0fz0h"
+  id: 342
+  display_name: "Honeycomb"
+}
+item {
+  name: "/m/0jy4k"
+  id: 343
+  display_name: "Doughnut"
+}
+item {
+  name: "/m/05kyg_"
+  id: 344
+  display_name: "Chest of drawers"
+}
+item {
+  name: "/m/01prls"
+  id: 345
+  display_name: "Land vehicle"
+}
+item {
+  name: "/m/01h44"
+  id: 346
+  display_name: "Bat"
+}
+item {
+  name: "/m/08pbxl"
+  id: 347
+  display_name: "Monkey"
+}
+item {
+  name: "/m/02gzp"
+  id: 348
+  display_name: "Dagger"
+}
+item {
+  name: "/m/04brg2"
+  id: 349
+  display_name: "Tableware"
+}
+item {
+  name: "/m/031n1"
+  id: 350
+  display_name: "Human foot"
+}
+item {
+  name: "/m/02jvh9"
+  id: 351
+  display_name: "Mug"
+}
+item {
+  name: "/m/046dlr"
+  id: 352
+  display_name: "Alarm clock"
+}
+item {
+  name: "/m/0h8ntjv"
+  id: 353
+  display_name: "Pressure cooker"
+}
+item {
+  name: "/m/0k65p"
+  id: 354
+  display_name: "Human hand"
+}
+item {
+  name: "/m/011k07"
+  id: 355
+  display_name: "Tortoise"
+}
+item {
+  name: "/m/03grzl"
+  id: 356
+  display_name: "Baseball glove"
+}
+item {
+  name: "/m/06y5r"
+  id: 357
+  display_name: "Sword"
+}
+item {
+  name: "/m/061_f"
+  id: 358
+  display_name: "Pear"
+}
+item {
+  name: "/m/01cmb2"
+  id: 359
+  display_name: "Miniskirt"
+}
+item {
+  name: "/m/01mqdt"
+  id: 360
+  display_name: "Traffic sign"
+}
+item {
+  name: "/m/05r655"
+  id: 361
+  display_name: "Girl"
+}
+item {
+  name: "/m/02p3w7d"
+  id: 362
+  display_name: "Roller skates"
+}
+item {
+  name: "/m/029tx"
+  id: 363
+  display_name: "Dinosaur"
+}
+item {
+  name: "/m/04m6gz"
+  id: 364
+  display_name: "Porch"
+}
+item {
+  name: "/m/015h_t"
+  id: 365
+  display_name: "Human beard"
+}
+item {
+  name: "/m/06pcq"
+  id: 366
+  display_name: "Submarine sandwich"
+}
+item {
+  name: "/m/01bms0"
+  id: 367
+  display_name: "Screwdriver"
+}
+item {
+  name: "/m/07fbm7"
+  id: 368
+  display_name: "Strawberry"
+}
+item {
+  name: "/m/09tvcd"
+  id: 369
+  display_name: "Wine glass"
+}
+item {
+  name: "/m/06nwz"
+  id: 370
+  display_name: "Seafood"
+}
+item {
+  name: "/m/0dv9c"
+  id: 371
+  display_name: "Racket"
+}
+item {
+  name: "/m/083wq"
+  id: 372
+  display_name: "Wheel"
+}
+item {
+  name: "/m/0gd36"
+  id: 373
+  display_name: "Sea lion"
+}
+item {
+  name: "/m/0138tl"
+  id: 374
+  display_name: "Toy"
+}
+item {
+  name: "/m/07clx"
+  id: 375
+  display_name: "Tea"
+}
+item {
+  name: "/m/05ctyq"
+  id: 376
+  display_name: "Tennis ball"
+}
+item {
+  name: "/m/0bjyj5"
+  id: 377
+  display_name: "Waste container"
+}
+item {
+  name: "/m/0dbzx"
+  id: 378
+  display_name: "Mule"
+}
+item {
+  name: "/m/02ctlc"
+  id: 379
+  display_name: "Cricket ball"
+}
+item {
+  name: "/m/0fp6w"
+  id: 380
+  display_name: "Pineapple"
+}
+item {
+  name: "/m/0djtd"
+  id: 381
+  display_name: "Coconut"
+}
+item {
+  name: "/m/0167gd"
+  id: 382
+  display_name: "Doll"
+}
+item {
+  name: "/m/078n6m"
+  id: 383
+  display_name: "Coffee table"
+}
+item {
+  name: "/m/0152hh"
+  id: 384
+  display_name: "Snowman"
+}
+item {
+  name: "/m/04gth"
+  id: 385
+  display_name: "Lavender"
+}
+item {
+  name: "/m/0ll1f78"
+  id: 386
+  display_name: "Shrimp"
+}
+item {
+  name: "/m/0cffdh"
+  id: 387
+  display_name: "Maple"
+}
+item {
+  name: "/m/025rp__"
+  id: 388
+  display_name: "Cowboy hat"
+}
+item {
+  name: "/m/02_n6y"
+  id: 389
+  display_name: "Goggles"
+}
+item {
+  name: "/m/0wdt60w"
+  id: 390
+  display_name: "Rugby ball"
+}
+item {
+  name: "/m/0cydv"
+  id: 391
+  display_name: "Caterpillar"
+}
+item {
+  name: "/m/01n5jq"
+  id: 392
+  display_name: "Poster"
+}
+item {
+  name: "/m/09rvcxw"
+  id: 393
+  display_name: "Rocket"
+}
+item {
+  name: "/m/013y1f"
+  id: 394
+  display_name: "Organ"
+}
+item {
+  name: "/m/06ncr"
+  id: 395
+  display_name: "Saxophone"
+}
+item {
+  name: "/m/015qff"
+  id: 396
+  display_name: "Traffic light"
+}
+item {
+  name: "/m/024g6"
+  id: 397
+  display_name: "Cocktail"
+}
+item {
+  name: "/m/05gqfk"
+  id: 398
+  display_name: "Plastic bag"
+}
+item {
+  name: "/m/0dv77"
+  id: 399
+  display_name: "Squash"
+}
+item {
+  name: "/m/052sf"
+  id: 400
+  display_name: "Mushroom"
+}
+item {
+  name: "/m/0cdn1"
+  id: 401
+  display_name: "Hamburger"
+}
+item {
+  name: "/m/03jbxj"
+  id: 402
+  display_name: "Light switch"
+}
+item {
+  name: "/m/0cyfs"
+  id: 403
+  display_name: "Parachute"
+}
+item {
+  name: "/m/0kmg4"
+  id: 404
+  display_name: "Teddy bear"
+}
+item {
+  name: "/m/02cvgx"
+  id: 405
+  display_name: "Winter melon"
+}
+item {
+  name: "/m/09kx5"
+  id: 406
+  display_name: "Deer"
+}
+item {
+  name: "/m/057cc"
+  id: 407
+  display_name: "Musical keyboard"
+}
+item {
+  name: "/m/02pkr5"
+  id: 408
+  display_name: "Plumbing fixture"
+}
+item {
+  name: "/m/057p5t"
+  id: 409
+  display_name: "Scoreboard"
+}
+item {
+  name: "/m/03g8mr"
+  id: 410
+  display_name: "Baseball bat"
+}
+item {
+  name: "/m/0frqm"
+  id: 411
+  display_name: "Envelope"
+}
+item {
+  name: "/m/03m3vtv"
+  id: 412
+  display_name: "Adhesive tape"
+}
+item {
+  name: "/m/0584n8"
+  id: 413
+  display_name: "Briefcase"
+}
+item {
+  name: "/m/014y4n"
+  id: 414
+  display_name: "Paddle"
+}
+item {
+  name: "/m/01g3x7"
+  id: 415
+  display_name: "Bow and arrow"
+}
+item {
+  name: "/m/07cx4"
+  id: 416
+  display_name: "Telephone"
+}
+item {
+  name: "/m/07bgp"
+  id: 417
+  display_name: "Sheep"
+}
+item {
+  name: "/m/032b3c"
+  id: 418
+  display_name: "Jacket"
+}
+item {
+  name: "/m/01bl7v"
+  id: 419
+  display_name: "Boy"
+}
+item {
+  name: "/m/0663v"
+  id: 420
+  display_name: "Pizza"
+}
+item {
+  name: "/m/0cn6p"
+  id: 421
+  display_name: "Otter"
+}
+item {
+  name: "/m/02rdsp"
+  id: 422
+  display_name: "Office supplies"
+}
+item {
+  name: "/m/02crq1"
+  id: 423
+  display_name: "Couch"
+}
+item {
+  name: "/m/01xqw"
+  id: 424
+  display_name: "Cello"
+}
+item {
+  name: "/m/0cnyhnx"
+  id: 425
+  display_name: "Bull"
+}
+item {
+  name: "/m/01x_v"
+  id: 426
+  display_name: "Camel"
+}
+item {
+  name: "/m/018xm"
+  id: 427
+  display_name: "Ball"
+}
+item {
+  name: "/m/09ddx"
+  id: 428
+  display_name: "Duck"
+}
+item {
+  name: "/m/084zz"
+  id: 429
+  display_name: "Whale"
+}
+item {
+  name: "/m/01n4qj"
+  id: 430
+  display_name: "Shirt"
+}
+item {
+  name: "/m/07cmd"
+  id: 431
+  display_name: "Tank"
+}
+item {
+  name: "/m/04_sv"
+  id: 432
+  display_name: "Motorcycle"
+}
+item {
+  name: "/m/0mkg"
+  id: 433
+  display_name: "Accordion"
+}
+item {
+  name: "/m/09d5_"
+  id: 434
+  display_name: "Owl"
+}
+item {
+  name: "/m/0c568"
+  id: 435
+  display_name: "Porcupine"
+}
+item {
+  name: "/m/02wbtzl"
+  id: 436
+  display_name: "Sun hat"
+}
+item {
+  name: "/m/05bm6"
+  id: 437
+  display_name: "Nail"
+}
+item {
+  name: "/m/01lsmm"
+  id: 438
+  display_name: "Scissors"
+}
+item {
+  name: "/m/0dftk"
+  id: 439
+  display_name: "Swan"
+}
+item {
+  name: "/m/0dtln"
+  id: 440
+  display_name: "Lamp"
+}
+item {
+  name: "/m/0nl46"
+  id: 441
+  display_name: "Crown"
+}
+item {
+  name: "/m/05r5c"
+  id: 442
+  display_name: "Piano"
+}
+item {
+  name: "/m/06msq"
+  id: 443
+  display_name: "Sculpture"
+}
+item {
+  name: "/m/0cd4d"
+  id: 444
+  display_name: "Cheetah"
+}
+item {
+  name: "/m/05kms"
+  id: 445
+  display_name: "Oboe"
+}
+item {
+  name: "/m/02jnhm"
+  id: 446
+  display_name: "Tin can"
+}
+item {
+  name: "/m/0fldg"
+  id: 447
+  display_name: "Mango"
+}
+item {
+  name: "/m/073bxn"
+  id: 448
+  display_name: "Tripod"
+}
+item {
+  name: "/m/029bxz"
+  id: 449
+  display_name: "Oven"
+}
+item {
+  name: "/m/020lf"
+  id: 450
+  display_name: "Computer mouse"
+}
+item {
+  name: "/m/01btn"
+  id: 451
+  display_name: "Barge"
+}
+item {
+  name: "/m/02vqfm"
+  id: 452
+  display_name: "Coffee"
+}
+item {
+  name: "/m/06__v"
+  id: 453
+  display_name: "Snowboard"
+}
+item {
+  name: "/m/043nyj"
+  id: 454
+  display_name: "Common fig"
+}
+item {
+  name: "/m/0grw1"
+  id: 455
+  display_name: "Salad"
+}
+item {
+  name: "/m/03hl4l9"
+  id: 456
+  display_name: "Marine invertebrates"
+}
+item {
+  name: "/m/0hnnb"
+  id: 457
+  display_name: "Umbrella"
+}
+item {
+  name: "/m/04c0y"
+  id: 458
+  display_name: "Kangaroo"
+}
+item {
+  name: "/m/0dzf4"
+  id: 459
+  display_name: "Human arm"
+}
+item {
+  name: "/m/07v9_z"
+  id: 460
+  display_name: "Measuring cup"
+}
+item {
+  name: "/m/0f9_l"
+  id: 461
+  display_name: "Snail"
+}
+item {
+  name: "/m/0703r8"
+  id: 462
+  display_name: "Loveseat"
+}
+item {
+  name: "/m/01xyhv"
+  id: 463
+  display_name: "Suit"
+}
+item {
+  name: "/m/01fh4r"
+  id: 464
+  display_name: "Teapot"
+}
+item {
+  name: "/m/04dr76w"
+  id: 465
+  display_name: "Bottle"
+}
+item {
+  name: "/m/0pcr"
+  id: 466
+  display_name: "Alpaca"
+}
+item {
+  name: "/m/03s_tn"
+  id: 467
+  display_name: "Kettle"
+}
+item {
+  name: "/m/07mhn"
+  id: 468
+  display_name: "Trousers"
+}
+item {
+  name: "/m/01hrv5"
+  id: 469
+  display_name: "Popcorn"
+}
+item {
+  name: "/m/019h78"
+  id: 470
+  display_name: "Centipede"
+}
+item {
+  name: "/m/09kmb"
+  id: 471
+  display_name: "Spider"
+}
+item {
+  name: "/m/0h23m"
+  id: 472
+  display_name: "Sparrow"
+}
+item {
+  name: "/m/050gv4"
+  id: 473
+  display_name: "Plate"
+}
+item {
+  name: "/m/01fb_0"
+  id: 474
+  display_name: "Bagel"
+}
+item {
+  name: "/m/02w3_ws"
+  id: 475
+  display_name: "Personal care"
+}
+item {
+  name: "/m/014j1m"
+  id: 476
+  display_name: "Apple"
+}
+item {
+  name: "/m/01gmv2"
+  id: 477
+  display_name: "Brassiere"
+}
+item {
+  name: "/m/04y4h8h"
+  id: 478
+  display_name: "Bathroom cabinet"
+}
+item {
+  name: "/m/026qbn5"
+  id: 479
+  display_name: "Studio couch"
+}
+item {
+  name: "/m/01m2v"
+  id: 480
+  display_name: "Computer keyboard"
+}
+item {
+  name: "/m/05_5p_0"
+  id: 481
+  display_name: "Table tennis racket"
+}
+item {
+  name: "/m/07030"
+  id: 482
+  display_name: "Sushi"
+}
+item {
+  name: "/m/01s105"
+  id: 483
+  display_name: "Cabinetry"
+}
+item {
+  name: "/m/033rq4"
+  id: 484
+  display_name: "Street light"
+}
+item {
+  name: "/m/0162_1"
+  id: 485
+  display_name: "Towel"
+}
+item {
+  name: "/m/02z51p"
+  id: 486
+  display_name: "Nightstand"
+}
+item {
+  name: "/m/06mf6"
+  id: 487
+  display_name: "Rabbit"
+}
+item {
+  name: "/m/02hj4"
+  id: 488
+  display_name: "Dolphin"
+}
+item {
+  name: "/m/0bt9lr"
+  id: 489
+  display_name: "Dog"
+}
+item {
+  name: "/m/08hvt4"
+  id: 490
+  display_name: "Jug"
+}
+item {
+  name: "/m/084rd"
+  id: 491
+  display_name: "Wok"
+}
+item {
+  name: "/m/01pns0"
+  id: 492
+  display_name: "Fire hydrant"
+}
+item {
+  name: "/m/014sv8"
+  id: 493
+  display_name: "Human eye"
+}
+item {
+  name: "/m/079cl"
+  id: 494
+  display_name: "Skyscraper"
+}
+item {
+  name: "/m/01940j"
+  id: 495
+  display_name: "Backpack"
+}
+item {
+  name: "/m/05vtc"
+  id: 496
+  display_name: "Potato"
+}
+item {
+  name: "/m/02w3r3"
+  id: 497
+  display_name: "Paper towel"
+}
+item {
+  name: "/m/054xkw"
+  id: 498
+  display_name: "Lifejacket"
+}
+item {
+  name: "/m/01bqk0"
+  id: 499
+  display_name: "Bicycle wheel"
+}
+item {
+  name: "/m/09g1w"
+  id: 500
+  display_name: "Toilet"
+}
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 9f59c212..8480a14b 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -112,7 +112,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
                label_map_proto_file=None,
                use_display_name=False,
                dct_method='',
-               num_keypoints=0):
+               num_keypoints=0,
+               num_additional_channels=0):
     """Constructor sets keys_to_features and items_to_handlers.
 
     Args:
@@ -133,6 +134,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         are ['INTEGER_FAST', 'INTEGER_ACCURATE']. The hint may be ignored, for
         example, the jpeg library does not have that specific option.
       num_keypoints: the number of keypoints per object.
+      num_additional_channels: how many additional channels to use.
 
     Raises:
       ValueError: If `instance_mask_type` option is not one of
@@ -178,15 +180,28 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         'image/object/weight':
             tf.VarLenFeature(tf.float32),
     }
+    # We are checking `dct_method` instead of passing it directly in order to
+    # ensure TF version 1.6 compatibility.
     if dct_method:
       image = slim_example_decoder.Image(
           image_key='image/encoded',
           format_key='image/format',
           channels=3,
           dct_method=dct_method)
+      additional_channel_image = slim_example_decoder.Image(
+          image_key='image/additional_channels/encoded',
+          format_key='image/format',
+          channels=1,
+          repeated=True,
+          dct_method=dct_method)
     else:
       image = slim_example_decoder.Image(
           image_key='image/encoded', format_key='image/format', channels=3)
+      additional_channel_image = slim_example_decoder.Image(
+          image_key='image/additional_channels/encoded',
+          format_key='image/format',
+          channels=1,
+          repeated=True)
     self.items_to_handlers = {
         fields.InputDataFields.image:
             image,
@@ -211,6 +226,13 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         fields.InputDataFields.groundtruth_weights: (
             slim_example_decoder.Tensor('image/object/weight')),
     }
+    if num_additional_channels > 0:
+      self.keys_to_features[
+          'image/additional_channels/encoded'] = tf.FixedLenFeature(
+              (num_additional_channels,), tf.string)
+      self.items_to_handlers[
+          fields.InputDataFields.
+          image_additional_channels] = additional_channel_image
     self._num_keypoints = num_keypoints
     if num_keypoints > 0:
       self.keys_to_features['image/object/keypoint/x'] = (
@@ -294,6 +316,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         [None] indicating if the boxes enclose a crowd.
 
     Optional:
+      fields.InputDataFields.image_additional_channels - 3D uint8 tensor of
+        shape [None, None, num_additional_channels]. 1st dim is height; 2nd dim
+        is width; 3rd dim is the number of additional channels.
       fields.InputDataFields.groundtruth_difficult - 1D bool tensor of shape
         [None] indicating if the boxes represent `difficult` instances.
       fields.InputDataFields.groundtruth_group_of - 1D bool tensor of shape
@@ -316,6 +341,12 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.shape(
         tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]
 
+    if fields.InputDataFields.image_additional_channels in tensor_dict:
+      channels = tensor_dict[fields.InputDataFields.image_additional_channels]
+      channels = tf.squeeze(channels, axis=3)
+      channels = tf.transpose(channels, perm=[1, 2, 0])
+      tensor_dict[fields.InputDataFields.image_additional_channels] = channels
+
     def default_groundtruth_weights():
       return tf.ones(
           [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index affb6cca..b567b8c2 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -23,6 +23,7 @@ from tensorflow.core.example import example_pb2
 from tensorflow.core.example import feature_pb2
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
+from tensorflow.python.framework import test_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import lookup_ops
 from tensorflow.python.ops import parsing_ops
@@ -72,10 +73,41 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def _BytesFeatureFromList(self, ndarray):
     values = ndarray.flatten().tolist()
-    for i in range(len(values)):
-      values[i] = values[i].encode('utf-8')
     return feature_pb2.Feature(bytes_list=feature_pb2.BytesList(value=values))
 
+  def testDecodeAdditionalChannels(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+
+    additional_channel_tensor = np.random.randint(
+        256, size=(4, 5, 1)).astype(np.uint8)
+    encoded_additional_channel = self._EncodeImage(additional_channel_tensor)
+    decoded_additional_channel = self._DecodeImage(encoded_additional_channel)
+
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    self._BytesFeature(encoded_jpeg),
+                'image/additional_channels/encoded':
+                    self._BytesFeatureFromList(
+                        np.array([encoded_additional_channel] * 2)),
+                'image/format':
+                    self._BytesFeature('jpeg'),
+                'image/source_id':
+                    self._BytesFeature('image_id'),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        num_additional_channels=2)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+      self.assertAllEqual(
+          np.concatenate([decoded_additional_channel] * 2, axis=2),
+          tensor_dict[fields.InputDataFields.image_additional_channels])
+
   def testDecodeExampleWithBranchedBackupHandler(self):
     example1 = example_pb2.Example(
         features=feature_pb2.Features(
@@ -304,6 +336,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(
         2, tensor_dict[fields.InputDataFields.num_groundtruth_boxes])
 
+  @test_util.enable_c_shapes
   def testDecodeKeypoint(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -331,7 +364,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                          get_shape().as_list()), [None, 4])
     self.assertAllEqual((tensor_dict[fields.InputDataFields.
                                      groundtruth_keypoints].
-                         get_shape().as_list()), [None, 3, 2])
+                         get_shape().as_list()), [2, 3, 2])
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
 
@@ -376,6 +409,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllClose(tensor_dict[fields.InputDataFields.groundtruth_weights],
                         np.ones(2, dtype=np.float32))
 
+  @test_util.enable_c_shapes
   def testDecodeObjectLabel(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -391,7 +425,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
-                        [None])
+                        [2])
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
@@ -522,6 +556,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual([3, 1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  @test_util.enable_c_shapes
   def testDecodeObjectArea(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -536,13 +571,14 @@ class TfExampleDecoderTest(tf.test.TestCase):
     tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
 
     self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_area].
-                         get_shape().as_list()), [None])
+                         get_shape().as_list()), [2])
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
 
     self.assertAllEqual(object_area,
                         tensor_dict[fields.InputDataFields.groundtruth_area])
 
+  @test_util.enable_c_shapes
   def testDecodeObjectIsCrowd(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -558,7 +594,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_is_crowd].get_shape().as_list()),
-                        [None])
+                        [2])
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
 
@@ -566,6 +602,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                         tensor_dict[
                             fields.InputDataFields.groundtruth_is_crowd])
 
+  @test_util.enable_c_shapes
   def testDecodeObjectDifficult(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -581,7 +618,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_difficult].get_shape().as_list()),
-                        [None])
+                        [2])
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
 
@@ -589,6 +626,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                         tensor_dict[
                             fields.InputDataFields.groundtruth_difficult])
 
+  @test_util.enable_c_shapes
   def testDecodeObjectGroupOf(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -605,7 +643,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     self.assertAllEqual((tensor_dict[
         fields.InputDataFields.groundtruth_group_of].get_shape().as_list()),
-                        [None])
+                        [2])
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
 
@@ -637,6 +675,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         object_weights,
         tensor_dict[fields.InputDataFields.groundtruth_weights])
 
+  @test_util.enable_c_shapes
   def testDecodeInstanceSegmentation(self):
     num_instances = 4
     image_height = 5
@@ -673,11 +712,11 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
     self.assertAllEqual((
         tensor_dict[fields.InputDataFields.groundtruth_instance_masks].
-        get_shape().as_list()), [None, None, None])
+        get_shape().as_list()), [4, 5, 3])
 
     self.assertAllEqual((
         tensor_dict[fields.InputDataFields.groundtruth_classes].
-        get_shape().as_list()), [None])
+        get_shape().as_list()), [4])
 
     with self.test_session() as sess:
       tensor_dict = sess.run(tensor_dict)
diff --git a/research/object_detection/dataset_tools/create_oid_tf_record.py b/research/object_detection/dataset_tools/create_oid_tf_record.py
index 0df56a11..26d9699c 100644
--- a/research/object_detection/dataset_tools/create_oid_tf_record.py
+++ b/research/object_detection/dataset_tools/create_oid_tf_record.py
@@ -16,7 +16,8 @@ r"""Creates TFRecords of Open Images dataset for object detection.
 
 Example usage:
   python object_detection/dataset_tools/create_oid_tf_record.py \
-    --input_annotations_csv=/path/to/input/annotations-human-bbox.csv \
+    --input_box_annotations_csv=/path/to/input/annotations-human-bbox.csv \
+    --input_image_label_annotations_csv=/path/to/input/annotations-label.csv \
     --input_images_directory=/path/to/input/image_pixels_directory \
     --input_label_map=/path/to/input/labels_bbox_545.labelmap \
     --output_tf_record_path_prefix=/path/to/output/prefix.tfrecord
@@ -27,7 +28,9 @@ https://github.com/openimages/dataset
 
 This script will include every image found in the input_images_directory in the
 output TFRecord, even if the image has no corresponding bounding box annotations
-in the input_annotations_csv.
+in the input_annotations_csv. If input_image_label_annotations_csv is specified,
+it will add image-level labels as well. Note that the information of whether a
+label is positivelly or negativelly verified is NOT added to tfrecord.
 """
 from __future__ import absolute_import
 from __future__ import division
@@ -40,13 +43,16 @@ import pandas as pd
 import tensorflow as tf
 
 from object_detection.dataset_tools import oid_tfrecord_creation
+from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import label_map_util
 
-tf.flags.DEFINE_string('input_annotations_csv', None,
+tf.flags.DEFINE_string('input_box_annotations_csv', None,
                        'Path to CSV containing image bounding box annotations')
 tf.flags.DEFINE_string('input_images_directory', None,
                        'Directory containing the image pixels '
                        'downloaded from the OpenImages GitHub repository.')
+tf.flags.DEFINE_string('input_image_label_annotations_csv', None,
+                       'Path to CSV containing image-level labels annotations')
 tf.flags.DEFINE_string('input_label_map', None, 'Path to the label map proto')
 tf.flags.DEFINE_string(
     'output_tf_record_path_prefix', None,
@@ -61,7 +67,7 @@ def main(_):
   tf.logging.set_verbosity(tf.logging.INFO)
 
   required_flags = [
-      'input_annotations_csv', 'input_images_directory', 'input_label_map',
+      'input_box_annotations_csv', 'input_images_directory', 'input_label_map',
       'output_tf_record_path_prefix'
   ]
   for flag_name in required_flags:
@@ -69,17 +75,24 @@ def main(_):
       raise ValueError('Flag --{} is required'.format(flag_name))
 
   label_map = label_map_util.get_label_map_dict(FLAGS.input_label_map)
-  all_annotations = pd.read_csv(FLAGS.input_annotations_csv)
+  all_box_annotations = pd.read_csv(FLAGS.input_box_annotations_csv)
+  if FLAGS.input_image_label_annotations_csv:
+    all_label_annotations = pd.read_csv(FLAGS.input_image_label_annotations_csv)
+    all_label_annotations.rename(
+        columns={'Confidence': 'ConfidenceImageLabel'}, inplace=True)
+  else:
+    all_label_annotations = None
   all_images = tf.gfile.Glob(
       os.path.join(FLAGS.input_images_directory, '*.jpg'))
   all_image_ids = [os.path.splitext(os.path.basename(v))[0] for v in all_images]
   all_image_ids = pd.DataFrame({'ImageID': all_image_ids})
-  all_annotations = pd.concat([all_annotations, all_image_ids])
+  all_annotations = pd.concat(
+      [all_box_annotations, all_image_ids, all_label_annotations])
 
   tf.logging.log(tf.logging.INFO, 'Found %d images...', len(all_image_ids))
 
   with contextlib2.ExitStack() as tf_record_close_stack:
-    output_tfrecords = oid_tfrecord_creation.open_sharded_output_tfrecords(
+    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
         tf_record_close_stack, FLAGS.output_tf_record_path_prefix,
         FLAGS.num_shards)
 
diff --git a/research/object_detection/dataset_tools/create_pet_tf_record.py b/research/object_detection/dataset_tools/create_pet_tf_record.py
index c8e476a3..9b3b55c6 100644
--- a/research/object_detection/dataset_tools/create_pet_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pet_tf_record.py
@@ -33,11 +33,13 @@ import os
 import random
 import re
 
+import contextlib2
 from lxml import etree
 import numpy as np
 import PIL.Image
 import tensorflow as tf
 
+from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 
@@ -52,6 +54,8 @@ flags.DEFINE_boolean('faces_only', True, 'If True, generates bounding boxes '
                      'in the latter case, the resulting files are much larger.')
 flags.DEFINE_string('mask_type', 'png', 'How to represent instance '
                     'segmentation masks. Options are "png" or "numerical".')
+flags.DEFINE_integer('num_shards', 10, 'Number of TFRecord shards')
+
 FLAGS = flags.FLAGS
 
 
@@ -208,6 +212,7 @@ def dict_to_tf_example(data,
 
 
 def create_tf_record(output_filename,
+                     num_shards,
                      label_map_dict,
                      annotations_dir,
                      image_dir,
@@ -218,6 +223,7 @@ def create_tf_record(output_filename,
 
   Args:
     output_filename: Path to where output file is saved.
+    num_shards: Number of shards for output file.
     label_map_dict: The label map dictionary.
     annotations_dir: Directory where annotation files are stored.
     image_dir: Directory where image files are stored.
@@ -227,34 +233,36 @@ def create_tf_record(output_filename,
     mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to
       smaller file sizes.
   """
-  writer = tf.python_io.TFRecordWriter(output_filename)
-  for idx, example in enumerate(examples):
-    if idx % 100 == 0:
-      logging.info('On image %d of %d', idx, len(examples))
-    xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')
-    mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')
-
-    if not os.path.exists(xml_path):
-      logging.warning('Could not find %s, ignoring example.', xml_path)
-      continue
-    with tf.gfile.GFile(xml_path, 'r') as fid:
-      xml_str = fid.read()
-    xml = etree.fromstring(xml_str)
-    data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']
-
-    try:
-      tf_example = dict_to_tf_example(
-          data,
-          mask_path,
-          label_map_dict,
-          image_dir,
-          faces_only=faces_only,
-          mask_type=mask_type)
-      writer.write(tf_example.SerializeToString())
-    except ValueError:
-      logging.warning('Invalid example: %s, ignoring.', xml_path)
-
-  writer.close()
+  with contextlib2.ExitStack() as tf_record_close_stack:
+    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
+        tf_record_close_stack, output_filename, num_shards)
+    for idx, example in enumerate(examples):
+      if idx % 100 == 0:
+        logging.info('On image %d of %d', idx, len(examples))
+      xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')
+      mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')
+
+      if not os.path.exists(xml_path):
+        logging.warning('Could not find %s, ignoring example.', xml_path)
+        continue
+      with tf.gfile.GFile(xml_path, 'r') as fid:
+        xml_str = fid.read()
+      xml = etree.fromstring(xml_str)
+      data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']
+
+      try:
+        tf_example = dict_to_tf_example(
+            data,
+            mask_path,
+            label_map_dict,
+            image_dir,
+            faces_only=faces_only,
+            mask_type=mask_type)
+        if tf_example:
+          shard_idx = idx % num_shards
+          output_tfrecords[shard_idx].write(tf_example.SerializeToString())
+      except ValueError:
+        logging.warning('Invalid example: %s, ignoring.', xml_path)
 
 
 # TODO(derekjchow): Add test for pet/PASCAL main files.
@@ -279,15 +287,16 @@ def main(_):
   logging.info('%d training and %d validation examples.',
                len(train_examples), len(val_examples))
 
-  train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record')
-  val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record')
-  if FLAGS.faces_only:
+  train_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_train.record')
+  val_output_path = os.path.join(FLAGS.output_dir, 'pet_faces_val.record')
+  if not FLAGS.faces_only:
     train_output_path = os.path.join(FLAGS.output_dir,
-                                     'pet_train_with_masks.record')
+                                     'pets_fullbody_with_masks_train.record')
     val_output_path = os.path.join(FLAGS.output_dir,
-                                   'pet_val_with_masks.record')
+                                   'pets_fullbody_with_masks_val.record')
   create_tf_record(
       train_output_path,
+      FLAGS.num_shards,
       label_map_dict,
       annotations_dir,
       image_dir,
@@ -296,6 +305,7 @@ def main(_):
       mask_type=FLAGS.mask_type)
   create_tf_record(
       val_output_path,
+      FLAGS.num_shards,
       label_map_dict,
       annotations_dir,
       image_dir,
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
new file mode 100644
index 00000000..6c00ac42
--- /dev/null
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -0,0 +1,172 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""A class and executable to expand hierarchically image-level labels and boxes.
+
+Example usage:
+    ./hierarchical_labels_expansion <path to JSON hierarchy> <input csv file>
+    <output csv file> [optional]labels_file
+"""
+
+import json
+import sys
+
+
+def _update_dict(initial_dict, update):
+  """Updates dictionary with update content.
+
+  Args:
+   initial_dict: initial dictionary.
+   update: updated dictionary.
+  """
+
+  for key, value_list in update.iteritems():
+    if key in initial_dict:
+      initial_dict[key].extend(value_list)
+    else:
+      initial_dict[key] = value_list
+
+
+def _build_plain_hierarchy(hierarchy, skip_root=False):
+  """Expands tree hierarchy representation to parent-child dictionary.
+
+  Args:
+   hierarchy: labels hierarchy as JSON file.
+   skip_root: if true skips root from the processing (done for the case when all
+     classes under hierarchy are collected under virtual node).
+
+  Returns:
+    keyed_parent - dictionary of parent - all its children nodes.
+    keyed_child  - dictionary of children - all its parent nodes
+    children - all children of the current node.
+  """
+  all_children = []
+  all_keyed_parent = {}
+  all_keyed_child = {}
+  if 'Subcategory' in hierarchy:
+    for node in hierarchy['Subcategory']:
+      keyed_parent, keyed_child, children = _build_plain_hierarchy(node)
+      # Update is not done through dict.update() since some children have multi-
+      # ple parents in the hiearchy.
+      _update_dict(all_keyed_parent, keyed_parent)
+      _update_dict(all_keyed_child, keyed_child)
+      all_children.extend(children)
+
+  if not skip_root:
+    all_keyed_parent[hierarchy['LabelName']] = all_children
+    all_children = [hierarchy['LabelName']] + all_children
+    for child, _ in all_keyed_child.iteritems():
+      all_keyed_child[child].append(hierarchy['LabelName'])
+    all_keyed_child[hierarchy['LabelName']] = []
+
+  return all_keyed_parent, all_keyed_child, all_children
+
+
+class OIDHierarchicalLabelsExpansion(object):
+  """ Main class to perform labels hierachical expansion."""
+
+  def __init__(self, hierarchy):
+    """Constructor.
+
+    Args:
+      hierarchy: labels hierarchy as JSON file.
+    """
+
+    self._hierarchy_keyed_parent, self._hierarchy_keyed_child, _ = (
+        _build_plain_hierarchy(hierarchy, skip_root=True))
+
+  def expand_boxes_from_csv(self, csv_row):
+    """Expands a row containing bounding boxes from CSV file.
+
+    Args:
+      csv_row: a single row of Open Images released groundtruth file.
+
+    Returns:
+      a list of strings (including the initial row) corresponding to the ground
+      truth expanded to multiple annotation for evaluation with Open Images
+      Challenge 2018 metric.
+    """
+    # Row header is expected to be exactly:
+    # ImageID,Source,LabelName,Confidence,XMin,XMax,YMin,YMax,IsOccluded,
+    # IsTruncated,IsGroupOf,IsDepiction,IsInside
+    cvs_row_splited = csv_row.split(',')
+    assert len(cvs_row_splited) == 13
+    result = [csv_row]
+    assert cvs_row_splited[2] in self._hierarchy_keyed_child
+    parent_nodes = self._hierarchy_keyed_child[cvs_row_splited[2]]
+    for parent_node in parent_nodes:
+      cvs_row_splited[2] = parent_node
+      result.append(','.join(cvs_row_splited))
+    return result
+
+  def expand_labels_from_csv(self, csv_row):
+    """Expands a row containing bounding boxes from CSV file.
+
+    Args:
+      csv_row: a single row of Open Images released groundtruth file.
+
+    Returns:
+      a list of strings (including the initial row) corresponding to the ground
+      truth expanded to multiple annotation for evaluation with Open Images
+      Challenge 2018 metric.
+    """
+    # Row header is expected to be exactly:
+    # ImageID,Source,LabelName,Confidence
+    cvs_row_splited = csv_row.split(',')
+    assert len(cvs_row_splited) == 4
+    result = [csv_row]
+    if int(cvs_row_splited[3]) == 1:
+      assert cvs_row_splited[2] in self._hierarchy_keyed_child
+      parent_nodes = self._hierarchy_keyed_child[cvs_row_splited[2]]
+      for parent_node in parent_nodes:
+        cvs_row_splited[2] = parent_node
+        result.append(','.join(cvs_row_splited))
+    else:
+      assert cvs_row_splited[2] in self._hierarchy_keyed_parent
+      child_nodes = self._hierarchy_keyed_parent[cvs_row_splited[2]]
+      for child_node in child_nodes:
+        cvs_row_splited[2] = child_node
+        result.append(','.join(cvs_row_splited))
+    return result
+
+
+def main(argv):
+
+  if len(argv) < 4:
+    print """Missing arguments. \n
+             Usage: ./hierarchical_labels_expansion <path to JSON hierarchy>
+             <input csv file> <output csv file> [optional]labels_file"""
+    return
+  with open(argv[1]) as f:
+    hierarchy = json.load(f)
+  expansion_generator = OIDHierarchicalLabelsExpansion(hierarchy)
+  labels_file = False
+  if len(argv) > 4 and argv[4] == 'labels_file':
+    labels_file = True
+  with open(argv[2], 'r') as source:
+    with open(argv[3], 'w') as target:
+      header_skipped = False
+      for line in source:
+        if not header_skipped:
+          header_skipped = True
+          continue
+        if labels_file:
+          expanded_lines = expansion_generator.expand_labels_from_csv(line)
+        else:
+          expanded_lines = expansion_generator.expand_boxes_from_csv(line)
+        target.writelines(expanded_lines)
+
+
+if __name__ == '__main__':
+  main(sys.argv)
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py
new file mode 100644
index 00000000..cd62b9cf
--- /dev/null
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py
@@ -0,0 +1,88 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for the OpenImages label expansion (OIDHierarchicalLabelsExpansion)."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from object_detection.dataset_tools import oid_hierarchical_labels_expansion
+
+
+def create_test_data():
+  hierarchy = {
+      'LabelName':
+          'a',
+      'Subcategory': [{
+          'LabelName': 'b'
+      }, {
+          'LabelName': 'c',
+          'Subcategory': [{
+              'LabelName': 'd'
+          }, {
+              'LabelName': 'e'
+          }]
+      }, {
+          'LabelName': 'f',
+          'Subcategory': [{
+              'LabelName': 'd'
+          },]
+      }]
+  }
+  bbox_rows = [
+      '123,xclick,b,1,0.1,0.2,0.1,0.2,1,1,0,0,0',
+      '123,xclick,d,1,0.2,0.3,0.1,0.2,1,1,0,0,0'
+  ]
+  label_rows = [
+      '123,verification,b,0', '123,verification,c,0', '124,verification,d,1'
+  ]
+  return hierarchy, bbox_rows, label_rows
+
+
+class HierarchicalLabelsExpansionTest(tf.test.TestCase):
+
+  def test_bbox_expansion(self):
+    hierarchy, bbox_rows, _ = create_test_data()
+    expansion_generator = (
+        oid_hierarchical_labels_expansion.OIDHierarchicalLabelsExpansion(
+            hierarchy))
+    all_result_rows = []
+    for row in bbox_rows:
+      all_result_rows.extend(expansion_generator.expand_boxes_from_csv(row))
+    self.assertItemsEqual([
+        '123,xclick,b,1,0.1,0.2,0.1,0.2,1,1,0,0,0',
+        '123,xclick,d,1,0.2,0.3,0.1,0.2,1,1,0,0,0',
+        '123,xclick,f,1,0.2,0.3,0.1,0.2,1,1,0,0,0',
+        '123,xclick,c,1,0.2,0.3,0.1,0.2,1,1,0,0,0'
+    ], all_result_rows)
+
+  def test_labels_expansion(self):
+    hierarchy, _, label_rows = create_test_data()
+    expansion_generator = (
+        oid_hierarchical_labels_expansion.OIDHierarchicalLabelsExpansion(
+            hierarchy))
+    all_result_rows = []
+    for row in label_rows:
+      all_result_rows.extend(expansion_generator.expand_labels_from_csv(row))
+    self.assertItemsEqual([
+        '123,verification,b,0', '123,verification,c,0', '123,verification,d,0',
+        '123,verification,e,0', '124,verification,d,1', '124,verification,f,1',
+        '124,verification,c,1'
+    ], all_result_rows)
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation.py b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
index ccf6ec8e..70628098 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
@@ -41,24 +41,31 @@ def tf_example_from_annotations_data_frame(annotations_data_frame, label_map,
 
   filtered_data_frame = annotations_data_frame[
       annotations_data_frame.LabelName.isin(label_map)]
-
+  filtered_data_frame_boxes = filtered_data_frame[
+      ~filtered_data_frame.YMin.isnull()]
+  filtered_data_frame_labels = filtered_data_frame[
+      filtered_data_frame.YMin.isnull()]
   image_id = annotations_data_frame.ImageID.iloc[0]
 
   feature_map = {
       standard_fields.TfExampleFields.object_bbox_ymin:
-          dataset_util.float_list_feature(filtered_data_frame.YMin.as_matrix()),
+          dataset_util.float_list_feature(
+              filtered_data_frame_boxes.YMin.as_matrix()),
       standard_fields.TfExampleFields.object_bbox_xmin:
-          dataset_util.float_list_feature(filtered_data_frame.XMin.as_matrix()),
+          dataset_util.float_list_feature(
+              filtered_data_frame_boxes.XMin.as_matrix()),
       standard_fields.TfExampleFields.object_bbox_ymax:
-          dataset_util.float_list_feature(filtered_data_frame.YMax.as_matrix()),
+          dataset_util.float_list_feature(
+              filtered_data_frame_boxes.YMax.as_matrix()),
       standard_fields.TfExampleFields.object_bbox_xmax:
-          dataset_util.float_list_feature(filtered_data_frame.XMax.as_matrix()),
+          dataset_util.float_list_feature(
+              filtered_data_frame_boxes.XMax.as_matrix()),
       standard_fields.TfExampleFields.object_class_text:
           dataset_util.bytes_list_feature(
-              filtered_data_frame.LabelName.as_matrix()),
+              filtered_data_frame_boxes.LabelName.as_matrix()),
       standard_fields.TfExampleFields.object_class_label:
           dataset_util.int64_list_feature(
-              filtered_data_frame.LabelName.map(lambda x: label_map[x])
+              filtered_data_frame_boxes.LabelName.map(lambda x: label_map[x])
               .as_matrix()),
       standard_fields.TfExampleFields.filename:
           dataset_util.bytes_feature('{}.jpg'.format(image_id)),
@@ -71,43 +78,29 @@ def tf_example_from_annotations_data_frame(annotations_data_frame, label_map,
   if 'IsGroupOf' in filtered_data_frame.columns:
     feature_map[standard_fields.TfExampleFields.
                 object_group_of] = dataset_util.int64_list_feature(
-                    filtered_data_frame.IsGroupOf.as_matrix().astype(int))
+                    filtered_data_frame_boxes.IsGroupOf.as_matrix().astype(int))
   if 'IsOccluded' in filtered_data_frame.columns:
     feature_map[standard_fields.TfExampleFields.
                 object_occluded] = dataset_util.int64_list_feature(
-                    filtered_data_frame.IsOccluded.as_matrix().astype(int))
+                    filtered_data_frame_boxes.IsOccluded.as_matrix().astype(
+                        int))
   if 'IsTruncated' in filtered_data_frame.columns:
     feature_map[standard_fields.TfExampleFields.
                 object_truncated] = dataset_util.int64_list_feature(
-                    filtered_data_frame.IsTruncated.as_matrix().astype(int))
+                    filtered_data_frame_boxes.IsTruncated.as_matrix().astype(
+                        int))
   if 'IsDepiction' in filtered_data_frame.columns:
     feature_map[standard_fields.TfExampleFields.
                 object_depiction] = dataset_util.int64_list_feature(
-                    filtered_data_frame.IsDepiction.as_matrix().astype(int))
+                    filtered_data_frame_boxes.IsDepiction.as_matrix().astype(
+                        int))
 
+  if 'ConfidenceImageLabel' in filtered_data_frame_labels.columns:
+    feature_map[standard_fields.TfExampleFields.
+                image_class_label] = dataset_util.int64_list_feature(
+                    filtered_data_frame_labels.LabelName.map(
+                        lambda x: label_map[x]).as_matrix())
+    feature_map[standard_fields.TfExampleFields.
+                image_class_text] = dataset_util.bytes_list_feature(
+                    filtered_data_frame_labels.LabelName.as_matrix()),
   return tf.train.Example(features=tf.train.Features(feature=feature_map))
-
-
-def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):
-  """Opens all TFRecord shards for writing and adds them to an exit stack.
-
-  Args:
-    exit_stack: A context2.ExitStack used to automatically closed the TFRecords
-      opened in this function.
-    base_path: The base path for all shards
-    num_shards: The number of shards
-
-  Returns:
-    The list of opened TFRecords. Position k in the list corresponds to shard k.
-  """
-  tf_record_output_filenames = [
-      '{}-{:05d}-of-{:05d}'.format(base_path, idx, num_shards)
-      for idx in range(num_shards)
-  ]
-
-  tfrecords = [
-      exit_stack.enter_context(tf.python_io.TFRecordWriter(file_name))
-      for file_name in tf_record_output_filenames
-  ]
-
-  return tfrecords
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
index 383af8a8..44ef8521 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
@@ -14,8 +14,6 @@
 # ==============================================================================
 """Tests for oid_tfrecord_creation.py."""
 
-import os
-import contextlib2
 import pandas as pd
 import tensorflow as tf
 
@@ -24,16 +22,17 @@ from object_detection.dataset_tools import oid_tfrecord_creation
 
 def create_test_data():
   data = {
-      'ImageID': ['i1', 'i1', 'i1', 'i1', 'i2', 'i2'],
-      'LabelName': ['a', 'a', 'b', 'b', 'b', 'c'],
-      'YMin': [0.3, 0.6, 0.8, 0.1, 0.0, 0.0],
-      'XMin': [0.1, 0.3, 0.7, 0.0, 0.1, 0.1],
-      'XMax': [0.2, 0.3, 0.8, 0.5, 0.9, 0.9],
-      'YMax': [0.3, 0.6, 1, 0.8, 0.8, 0.8],
-      'IsOccluded': [0, 1, 1, 0, 0, 0],
-      'IsTruncated': [0, 0, 0, 1, 0, 0],
-      'IsGroupOf': [0, 0, 0, 0, 0, 1],
-      'IsDepiction': [1, 0, 0, 0, 0, 0],
+      'ImageID': ['i1', 'i1', 'i1', 'i1', 'i1', 'i2', 'i2'],
+      'LabelName': ['a', 'a', 'b', 'b', 'c', 'b', 'c'],
+      'YMin': [0.3, 0.6, 0.8, 0.1, None, 0.0, 0.0],
+      'XMin': [0.1, 0.3, 0.7, 0.0, None, 0.1, 0.1],
+      'XMax': [0.2, 0.3, 0.8, 0.5, None, 0.9, 0.9],
+      'YMax': [0.3, 0.6, 1, 0.8, None, 0.8, 0.8],
+      'IsOccluded': [0, 1, 1, 0, None, 0, 0],
+      'IsTruncated': [0, 0, 0, 1, None, 0, 0],
+      'IsGroupOf': [0, 0, 0, 0, None, 0, 1],
+      'IsDepiction': [1, 0, 0, 0, None, 0, 0],
+      'ConfidenceImageLabel': [None, None, None, None, 0, None, None],
   }
   df = pd.DataFrame(data=data)
   label_map = {'a': 0, 'b': 1, 'c': 2}
@@ -47,7 +46,8 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
 
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i1'], label_map, 'encoded_image_test')
-    self.assertProtoEquals("""
+    self.assertProtoEquals(
+        """
         features {
           feature {
             key: "image/encoded"
@@ -87,7 +87,13 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
             value { int64_list { value: [0, 1, 1, 0] } } }
           feature {
             key: "image/object/truncated"
-            value { int64_list { value: [0, 0, 0, 1] } } } }
+            value { int64_list { value: [0, 0, 0, 1] } } }
+          feature {
+            key: "image/class/label"
+            value { int64_list { value: [2] } } }
+          feature {
+            key: "image/class/text"
+            value { bytes_list { value: ["c"] } } } }
     """, tf_example)
 
   def test_no_attributes(self):
@@ -97,6 +103,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
     del df['IsGroupOf']
     del df['IsOccluded']
     del df['IsTruncated']
+    del df['ConfidenceImageLabel']
 
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i2'], label_map, 'encoded_image_test')
@@ -138,7 +145,8 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
 
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i1'], label_map, 'encoded_image_test')
-    self.assertProtoEquals("""
+    self.assertProtoEquals(
+        """
         features {
           feature {
             key: "image/encoded"
@@ -178,26 +186,15 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
             value { int64_list { value: [0, 1] } } }
           feature {
             key: "image/object/truncated"
-            value { int64_list { value: [0, 0] } } } }
+            value { int64_list { value: [0, 0] } } }
+          feature {
+            key: "image/class/label"
+            value { int64_list { } } }
+          feature {
+            key: "image/class/text"
+            value { bytes_list { } } } }
     """, tf_example)
 
 
-class OpenOutputTfrecordsTests(tf.test.TestCase):
-
-  def test_sharded_tfrecord_writes(self):
-    with contextlib2.ExitStack() as tf_record_close_stack:
-      output_tfrecords = oid_tfrecord_creation.open_sharded_output_tfrecords(
-          tf_record_close_stack,
-          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), 10)
-      for idx in range(10):
-        output_tfrecords[idx].write('test_{}'.format(idx))
-
-    for idx in range(10):
-      tf_record_path = '{}-{:05d}-of-00010'.format(
-          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), idx)
-      records = list(tf.python_io.tf_record_iterator(tf_record_path))
-      self.assertAllEqual(records, ['test_{}'.format(idx)])
-
-
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util.py b/research/object_detection/dataset_tools/tf_record_creation_util.py
new file mode 100644
index 00000000..e8da2291
--- /dev/null
+++ b/research/object_detection/dataset_tools/tf_record_creation_util.py
@@ -0,0 +1,46 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Utilities for creating TFRecords of TF examples for the Open Images dataset.
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):
+  """Opens all TFRecord shards for writing and adds them to an exit stack.
+
+  Args:
+    exit_stack: A context2.ExitStack used to automatically closed the TFRecords
+      opened in this function.
+    base_path: The base path for all shards
+    num_shards: The number of shards
+
+  Returns:
+    The list of opened TFRecords. Position k in the list corresponds to shard k.
+  """
+  tf_record_output_filenames = [
+      '{}-{:05d}-of-{:05d}'.format(base_path, idx, num_shards)
+      for idx in range(num_shards)
+  ]
+
+  tfrecords = [
+      exit_stack.enter_context(tf.python_io.TFRecordWriter(file_name))
+      for file_name in tf_record_output_filenames
+  ]
+
+  return tfrecords
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util_test.py b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
new file mode 100644
index 00000000..f1231f8b
--- /dev/null
+++ b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
@@ -0,0 +1,42 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for tf_record_creation_util.py."""
+
+import os
+import contextlib2
+import tensorflow as tf
+
+from object_detection.dataset_tools import tf_record_creation_util
+
+
+class OpenOutputTfrecordsTests(tf.test.TestCase):
+
+  def test_sharded_tfrecord_writes(self):
+    with contextlib2.ExitStack() as tf_record_close_stack:
+      output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
+          tf_record_close_stack,
+          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), 10)
+      for idx in range(10):
+        output_tfrecords[idx].write('test_{}'.format(idx))
+
+    for idx in range(10):
+      tf_record_path = '{}-{:05d}-of-00010'.format(
+          os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), idx)
+      records = list(tf.python_io.tf_record_iterator(tf_record_path))
+      self.assertAllEqual(records, ['test_{}'.format(idx)])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/evaluator.py b/research/object_detection/evaluator.py
index 9be4e544..4e661354 100644
--- a/research/object_detection/evaluator.py
+++ b/research/object_detection/evaluator.py
@@ -39,12 +39,14 @@ EVAL_METRICS_CLASS_DICT = {
         object_detection_evaluation.PascalInstanceSegmentationEvaluator,
     'weighted_pascal_voc_instance_segmentation_metrics':
         object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,
-    'open_images_detection_metrics':
+    'open_images_V2_detection_metrics':
         object_detection_evaluation.OpenImagesDetectionEvaluator,
     'coco_detection_metrics':
         coco_evaluation.CocoDetectionEvaluator,
     'coco_mask_metrics':
         coco_evaluation.CocoMaskEvaluator,
+    'oid_challenge_object_detection_metrics':
+        object_detection_evaluation.OpenImagesDetectionChallengeEvaluator,
 }
 
 EVAL_DEFAULT_METRIC = 'pascal_voc_detection_metrics'
diff --git a/research/object_detection/g3doc/evaluation_protocols.md b/research/object_detection/g3doc/evaluation_protocols.md
index f72b72f5..ec960058 100644
--- a/research/object_detection/g3doc/evaluation_protocols.md
+++ b/research/object_detection/g3doc/evaluation_protocols.md
@@ -4,12 +4,14 @@ The Tensorflow Object Detection API currently supports three evaluation protocol
 that can be configured in `EvalConfig` by setting `metrics_set` to the
 corresponding value.
 
-## PASCAL VOC 2007 detection metric
+## PASCAL VOC 2010 detection metric
 
 `EvalConfig.metrics_set='pascal_voc_detection_metrics'`
 
-The commonly used mAP metric for evaluating the quality of object detectors, computed according to the protocol of the PASCAL VOC Challenge 2007.
-The protocol is available [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf).
+The commonly used mAP metric for evaluating the quality of object detectors,
+computed according to the protocol of the PASCAL VOC Challenge 2010-2012. The
+protocol is available
+[here](http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf).
 
 ## Weighted PASCAL VOC detection metric
 
@@ -20,33 +22,36 @@ precision when treating all classes as a single class. In comparison,
 PASCAL metrics computes the mean average precision as the mean of the
 per-class average precisions.
 
-For example, the test set consists of two classes, "cat" and "dog", and there are ten times more boxes of "cat" than those of "dog".
-According to PASCAL VOC 2007 metric, performance on each of the two classes would contribute equally towards the final mAP value,
-while for the Weighted PASCAL VOC metric the final mAP value will be influenced by frequency of each class.
+For example, the test set consists of two classes, "cat" and "dog", and there
+are ten times more boxes of "cat" than those of "dog". According to PASCAL VOC
+2010 metric, performance on each of the two classes would contribute equally
+towards the final mAP value, while for the Weighted PASCAL VOC metric the final
+mAP value will be influenced by frequency of each class.
 
-## PASCAL VOC 2007 instance segmentation metric
+## PASCAL VOC 2010 instance segmentation metric
 
 `EvalConfig.metrics_set='pascal_voc_instance_segmentation_metrics'`
 
-Similar to pascal voc 2007 detection metric, but computes the intersection over
+Similar to Pascal VOC 2010 detection metric, but computes the intersection over
 union based on the object masks instead of object boxes.
 
 ## Weighted PASCAL VOC instance segmentation metric
 
 `EvalConfig.metrics_set='weighted_pascal_voc_instance_segmentation_metrics'`
 
-Similar to the weighted pascal voc 2007 detection metric, but computes the
+Similar to the weighted pascal voc 2010 detection metric, but computes the
 intersection over union based on the object masks instead of object boxes.
 
-## Open Images detection metric {#open-images}
+## Open Images V2 detection metric
 
-`EvalConfig.metrics_set='open_images_metrics'`
+`EvalConfig.metrics_set='open_images_V2_detection_metrics'`
 
-This metric is defined originally for evaluating detector performance on [Open Images V2 dataset](https://github.com/openimages/dataset)
-and is fairly similar to the PASCAL VOC 2007 metric mentioned above.
-It computes interpolated average precision (AP) for each class and averages it among all classes (mAP).
+This metric is defined originally for evaluating detector performance on [Open
+Images V2 dataset](https://github.com/openimages/dataset) and is fairly similar
+to the PASCAL VOC 2010 metric mentioned above. It computes interpolated average
+precision (AP) for each class and averages it among all classes (mAP).
 
-The difference to the PASCAL VOC 2007 metric is the following: Open Images
+The difference to the PASCAL VOC 2010 metric is the following: Open Images
 annotations contain `group-of` ground-truth boxes (see [Open Images data
 description](https://github.com/openimages/dataset#annotations-human-bboxcsv)),
 that are treated differently for the purpose of deciding whether detections are
@@ -61,7 +66,7 @@ such that:
     box is greater than the IoU threshold (default value 0.5). \
     Illustration of handling non-group-of boxes: \
     ![alt
-    groupof_case_eval](img/nongroupof_case_eval.png "illustration of handling non-group-of boxes: yellow box - ground truth bounding box; green box - true positive; red box - false positives."){width="500" height="270"}
+    groupof_case_eval](img/nongroupof_case_eval.png "illustration of handling non-group-of boxes: yellow box - ground truth bounding box; green box - true positive; red box - false positives.")
 
     *   yellow box - ground-truth box;
     *   green box - true positive;
@@ -80,7 +85,7 @@ ground-truth box such that:
     ground-truth box. \
     Illustration of handling `group-of` boxes: \
     ![alt
-    groupof_case_eval](img/groupof_case_eval.png "illustration of handling group-of boxes: yellow box - ground truth bounding box; grey boxes - two detections of cars, that are ignored; red box - false positive."){width="500" height="270"}
+    groupof_case_eval](img/groupof_case_eval.png "illustration of handling group-of boxes: yellow box - ground truth bounding box; grey boxes - two detections of cars, that are ignored; red box - false positive.")
 
     *   yellow box - ground-truth box;
     *   grey boxes - two detections on cars, that are ignored;
@@ -105,3 +110,20 @@ other kind of car is annotated as "car" (for example, a sedan). Given this
 convention, the evaluation software treats all classes independently, ignoring
 the hierarchy. To achieve high performance values, object detectors should
 output bounding-boxes labelled in the same manner.
+
+## OID Challenge Object Detection Metric 2018
+
+`EvalConfig.metrics_set='oid_challenge_object_detection_metrics'`
+
+The metric for the OID Challenge Object Detection Metric 2018, Object Detection
+track. The description is provided on the [Open Images Challenge
+website](https://storage.googleapis.com/openimages/web/challenge.html).
+
+## OID Challenge Visual Relationship Detection Metric 2018
+
+The metric for the OID Challenge Visual Relationship Detection Metric 2018, Visual
+Relationship Detection track. The description is provided on the [Open Images
+Challenge
+website](https://storage.googleapis.com/openimages/web/challenge.html). Note:
+this is currently a stand-alone metric, that can be used only through the
+`metrics/oid_vrd_challenge_evaluation.py` util.
diff --git a/research/object_detection/g3doc/oid_inference_and_evaluation.md b/research/object_detection/g3doc/oid_inference_and_evaluation.md
index 164fdc2c..93aedf29 100644
--- a/research/object_detection/g3doc/oid_inference_and_evaluation.md
+++ b/research/object_detection/g3doc/oid_inference_and_evaluation.md
@@ -93,7 +93,7 @@ mkdir ${SPLIT}_tfrecords
 
 PYTHONPATH=$PYTHONPATH:$(readlink -f ..) \
 python -m object_detection/dataset_tools/create_oid_tf_record \
-  --input_annotations_csv 2017_07/$SPLIT/annotations-human-bbox.csv \
+  --input_box_annotations_csv 2017_07/$SPLIT/annotations-human-bbox.csv \
   --input_images_directory raw_images_${SPLIT} \
   --input_label_map ../object_detection/data/oid_bbox_trainable_label_map.pbtxt \
   --output_tf_record_path_prefix ${SPLIT}_tfrecords/$SPLIT.tfrecord \
@@ -214,7 +214,7 @@ tf_record_input_reader: { input_path: '${SPLIT}_detections.tfrecord@${NUM_SHARDS
 " > ${SPLIT}_eval_metrics/${SPLIT}_input_config.pbtxt
 
 echo "
-metrics_set: 'open_images_metrics'
+metrics_set: 'open_images_V2_detection_metrics'
 " > ${SPLIT}_eval_metrics/${SPLIT}_eval_config.pbtxt
 ```
 
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 87957f07..c9b3ae25 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -56,12 +56,15 @@ def transform_input_data(tensor_dict,
   """A single function that is responsible for all input data transformations.
 
   Data transformation functions are applied in the following order.
-  1. data_augmentation_fn (optional): applied on tensor_dict.
-  2. model_preprocess_fn: applied only on image tensor in tensor_dict.
-  3. image_resizer_fn: applied on original image and instance mask tensor in
+  1. If key fields.InputDataFields.image_additional_channels is present in
+     tensor_dict, the additional channels will be merged into
+     fields.InputDataFields.image.
+  2. data_augmentation_fn (optional): applied on tensor_dict.
+  3. model_preprocess_fn: applied only on image tensor in tensor_dict.
+  4. image_resizer_fn: applied on original image and instance mask tensor in
      tensor_dict.
-  4. one_hot_encoding: applied to classes tensor in tensor_dict.
-  5. merge_multiple_boxes (optional): when groundtruth boxes are exactly the
+  5. one_hot_encoding: applied to classes tensor in tensor_dict.
+  6. merge_multiple_boxes (optional): when groundtruth boxes are exactly the
      same they can be merged into a single box with an associated k-hot class
      label.
 
@@ -88,6 +91,11 @@ def transform_input_data(tensor_dict,
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
     after applying all the transformations.
   """
+  if fields.InputDataFields.image_additional_channels in tensor_dict:
+    channels = tensor_dict[fields.InputDataFields.image_additional_channels]
+    tensor_dict[fields.InputDataFields.image] = tf.concat(
+        [tensor_dict[fields.InputDataFields.image], channels], axis=2)
+
   if retain_original_image:
     tensor_dict[fields.InputDataFields.original_image] = tf.cast(
         tensor_dict[fields.InputDataFields.image], tf.uint8)
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index c4c463cf..83266335 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -398,6 +398,33 @@ def _fake_image_resizer_fn(image, mask):
 
 class DataTransformationFnTest(tf.test.TestCase):
 
+  def test_combine_additional_channels_if_present(self):
+    image = np.random.rand(4, 4, 3).astype(np.float32)
+    additional_channels = np.random.rand(4, 4, 2).astype(np.float32)
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(image),
+        fields.InputDataFields.image_additional_channels:
+            tf.constant(additional_channels),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([1, 1], np.int32))
+    }
+
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=1)
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllEqual(transformed_inputs[fields.InputDataFields.image].dtype,
+                        tf.float32)
+    self.assertAllEqual(transformed_inputs[fields.InputDataFields.image].shape,
+                        [4, 4, 5])
+    self.assertAllClose(transformed_inputs[fields.InputDataFields.image],
+                        np.concatenate((image, additional_channels), axis=2))
+
   def test_returns_correct_class_label_encodings(self):
     tensor_dict = {
         fields.InputDataFields.image:
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index eb72ea5e..6315d4df 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -1199,7 +1199,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if self._is_training:
       proposal_boxes = tf.stop_gradient(proposal_boxes)
       if not self._hard_example_miner:
-        (groundtruth_boxlists, groundtruth_classes_with_background_list,
+        (groundtruth_boxlists, groundtruth_classes_with_background_list, _,
          _) = self._format_groundtruth_data(true_image_shapes)
         (proposal_boxes, proposal_scores,
          num_proposals) = self._unpad_proposals_and_sample_box_classifier_batch(
@@ -1358,9 +1358,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
         resized_masks_list.append(resized_mask)
 
       groundtruth_masks_list = resized_masks_list
+    groundtruth_weights_list = None
+    if self.groundtruth_has_field(fields.BoxListFields.weights):
+      groundtruth_weights_list = self.groundtruth_lists(
+          fields.BoxListFields.weights)
 
     return (groundtruth_boxlists, groundtruth_classes_with_background_list,
-            groundtruth_masks_list)
+            groundtruth_masks_list, groundtruth_weights_list)
 
   def _sample_box_classifier_minibatch(self,
                                        proposal_boxlist,
@@ -1586,14 +1590,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     with tf.name_scope(scope, 'Loss', prediction_dict.values()):
       (groundtruth_boxlists, groundtruth_classes_with_background_list,
-       groundtruth_masks_list) = self._format_groundtruth_data(
-           true_image_shapes)
+       groundtruth_masks_list, groundtruth_weights_list
+      ) = self._format_groundtruth_data(true_image_shapes)
       loss_dict = self._loss_rpn(
           prediction_dict['rpn_box_encodings'],
           prediction_dict['rpn_objectness_predictions_with_background'],
-          prediction_dict['anchors'],
-          groundtruth_boxlists,
-          groundtruth_classes_with_background_list)
+          prediction_dict['anchors'], groundtruth_boxlists,
+          groundtruth_classes_with_background_list, groundtruth_weights_list)
       if self._number_of_stages > 1:
         loss_dict.update(
             self._loss_box_classifier(
@@ -1603,18 +1606,17 @@ class FasterRCNNMetaArch(model.DetectionModel):
                 prediction_dict['num_proposals'],
                 groundtruth_boxlists,
                 groundtruth_classes_with_background_list,
+                groundtruth_weights_list,
                 prediction_dict['image_shape'],
                 prediction_dict.get('mask_predictions'),
                 groundtruth_masks_list,
             ))
     return loss_dict
 
-  def _loss_rpn(self,
-                rpn_box_encodings,
-                rpn_objectness_predictions_with_background,
-                anchors,
-                groundtruth_boxlists,
-                groundtruth_classes_with_background_list):
+  def _loss_rpn(self, rpn_box_encodings,
+                rpn_objectness_predictions_with_background, anchors,
+                groundtruth_boxlists, groundtruth_classes_with_background_list,
+                groundtruth_weights_list):
     """Computes scalar RPN loss tensors.
 
     Uses self._proposal_target_assigner to obtain regression and classification
@@ -1637,6 +1639,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       groundtruth_classes_with_background_list: A list of 2-D one-hot
         (or k-hot) tensors of shape [num_boxes, num_classes+1] containing the
         class targets with the 0th index assumed to map to the background class.
+      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape
+        [num_boxes] containing weights for groundtruth boxes.
 
     Returns:
       a dictionary mapping loss keys (`first_stage_localization_loss`,
@@ -1647,7 +1651,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       (batch_cls_targets, batch_cls_weights, batch_reg_targets,
        batch_reg_weights, _) = target_assigner.batch_assign_targets(
            self._proposal_target_assigner, box_list.BoxList(anchors),
-           groundtruth_boxlists, len(groundtruth_boxlists)*[None])
+           groundtruth_boxlists,
+           len(groundtruth_boxlists) * [None], groundtruth_weights_list)
       batch_cls_targets = tf.squeeze(batch_cls_targets, axis=2)
 
       def _minibatch_subsample_fn(inputs):
@@ -1695,6 +1700,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                            num_proposals,
                            groundtruth_boxlists,
                            groundtruth_classes_with_background_list,
+                           groundtruth_weights_list,
                            image_shape,
                            prediction_masks=None,
                            groundtruth_masks_list=None):
@@ -1731,6 +1737,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       groundtruth_classes_with_background_list: a list of 2-D one-hot
         (or k-hot) tensors of shape [num_boxes, num_classes + 1] containing the
         class targets with the 0th index assumed to map to the background class.
+      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape
+        [num_boxes] containing weights for groundtruth boxes.
       image_shape: a 1-D tensor of shape [4] representing the image shape.
       prediction_masks: an optional 4-D tensor with shape [total_num_proposals,
         num_classes, mask_height, mask_width] containing the instance masks for
@@ -1765,7 +1773,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       (batch_cls_targets_with_background, batch_cls_weights, batch_reg_targets,
        batch_reg_weights, _) = target_assigner.batch_assign_targets(
            self._detector_target_assigner, proposal_boxlists,
-           groundtruth_boxlists, groundtruth_classes_with_background_list)
+           groundtruth_boxlists, groundtruth_classes_with_background_list,
+           groundtruth_weights_list)
 
       class_predictions_with_background = tf.reshape(
           class_predictions_with_background,
@@ -1847,8 +1856,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
             unmatched_cls_target=tf.zeros(image_shape[1:3], dtype=tf.float32))
         (batch_mask_targets, _, _,
          batch_mask_target_weights, _) = target_assigner.batch_assign_targets(
-             mask_target_assigner, proposal_boxlists,
-             groundtruth_boxlists, groundtruth_masks_list)
+             mask_target_assigner, proposal_boxlists, groundtruth_boxlists,
+             groundtruth_masks_list, groundtruth_weights_list)
 
         # Pad the prediction_masks with to add zeros for background class to be
         # consistent with class predictions.
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 306075f5..00c70540 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -26,6 +26,7 @@ from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.protos import box_predictor_pb2
 from object_detection.protos import hyperparams_pb2
 from object_detection.protos import post_processing_pb2
+from object_detection.utils import test_utils
 
 slim = tf.contrib.slim
 BOX_CODE_SIZE = 4
@@ -650,8 +651,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
     with self.test_session() as sess:
       proposals_out = sess.run(proposals)
-      self.assertAllClose(proposals_out['detection_boxes'],
-                          expected_proposal_boxes)
+      for image_idx in range(batch_size):
+        self.assertTrue(
+            test_utils.first_rows_close_as_set(
+                proposals_out['detection_boxes'][image_idx].tolist(),
+                expected_proposal_boxes[image_idx]))
       self.assertAllClose(proposals_out['detection_scores'],
                           expected_proposal_scores)
       self.assertAllEqual(proposals_out['num_detections'],
@@ -810,7 +814,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
   def test_loss_full(self):
     model = self._build_model(
         is_training=True, number_of_stages=2, second_stage_batch_size=6)
-    batch_size = 2
+    batch_size = 3
     anchors = tf.constant(
         [[0, 0, 16, 16],
          [0, 16, 16, 32],
@@ -822,42 +826,44 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
          BOX_CODE_SIZE], dtype=tf.float32)
     # use different numbers for the objectness category to break ties in
     # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant([
-        [[-10, 13],
-         [10, -10],
-         [10, -11],
-         [-10, 12]],
-        [[10, -10],
-         [-10, 13],
-         [-10, 12],
-         [10, -11]]], dtype=tf.float32)
+    rpn_objectness_predictions_with_background = tf.constant(
+        [[[-10, 13], [10, -10], [10, -11], [-10, 12]], [[10, -10], [-10, 13], [
+            -10, 12
+        ], [10, -11]], [[10, -10], [-10, 13], [-10, 12], [10, -11]]],
+        dtype=tf.float32)
     image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
 
-    num_proposals = tf.constant([6, 6], dtype=tf.int32)
+    num_proposals = tf.constant([6, 6, 6], dtype=tf.int32)
     proposal_boxes = tf.constant(
-        2 * [[[0, 0, 16, 16],
-              [0, 16, 16, 32],
-              [16, 0, 32, 16],
-              [16, 16, 32, 32],
-              [0, 0, 16, 16],
-              [0, 16, 16, 32]]], dtype=tf.float32)
+        3 * [[[0, 0, 16, 16], [0, 16, 16, 32], [16, 0, 32, 16],
+              [16, 16, 32, 32], [0, 0, 16, 16], [0, 16, 16, 32]]],
+        dtype=tf.float32)
     refined_box_encodings = tf.zeros(
         (batch_size * model.max_num_proposals,
          model.num_classes,
          BOX_CODE_SIZE), dtype=tf.float32)
     class_predictions_with_background = tf.constant(
-        [[-10, 10, -10],  # first image
-         [10, -10, -10],
-         [10, -10, -10],
-         [-10, -10, 10],
-         [-10, 10, -10],
-         [10, -10, -10],
-         [10, -10, -10],  # second image
-         [-10, 10, -10],
-         [-10, 10, -10],
-         [10, -10, -10],
-         [10, -10, -10],
-         [-10, 10, -10]], dtype=tf.float32)
+        [
+            [-10, 10, -10],  # first image
+            [10, -10, -10],
+            [10, -10, -10],
+            [-10, -10, 10],
+            [-10, 10, -10],
+            [10, -10, -10],
+            [10, -10, -10],  # second image
+            [-10, 10, -10],
+            [-10, 10, -10],
+            [10, -10, -10],
+            [10, -10, -10],
+            [-10, 10, -10],
+            [10, -10, -10],  # third image
+            [-10, 10, -10],
+            [-10, 10, -10],
+            [10, -10, -10],
+            [10, -10, -10],
+            [-10, 10, -10]
+        ],
+        dtype=tf.float32)
 
     mask_predictions_logits = 20 * tf.ones((batch_size *
                                             model.max_num_proposals,
@@ -867,18 +873,29 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
     groundtruth_boxes_list = [
         tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
-        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
-                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]
+        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32),
+        tf.constant([[0, .5, .5, 1], [.5, 0, 1, 1]], dtype=tf.float32)
+    ]
+    groundtruth_classes_list = [
+        tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
+        tf.constant([[1, 0], [1, 0]], dtype=tf.float32),
+        tf.constant([[1, 0], [0, 1]], dtype=tf.float32)
+    ]
 
     # Set all elements of groundtruth mask to 1.0. In this case all proposal
     # crops of the groundtruth masks should return a mask that covers the entire
     # proposal. Thus, if mask_predictions_logits element values are all greater
     # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((2, 32, 32)),
-                                                   dtype=tf.float32),
-                              tf.convert_to_tensor(np.ones((2, 32, 32)),
-                                                   dtype=tf.float32)]
+    groundtruth_masks_list = [
+        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
+        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
+        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32)
+    ]
+    groundtruth_weights_list = [
+        tf.constant([1, 1], dtype=tf.float32),
+        tf.constant([1, 1], dtype=tf.float32),
+        tf.constant([1, 0], dtype=tf.float32)
+    ]
     prediction_dict = {
         'rpn_box_encodings': rpn_box_encodings,
         'rpn_objectness_predictions_with_background':
@@ -892,9 +909,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'mask_predictions': mask_predictions_logits
     }
     _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list,
-                              groundtruth_masks_list)
+    model.provide_groundtruth(
+        groundtruth_boxes_list,
+        groundtruth_classes_list,
+        groundtruth_masks_list,
+        groundtruth_weights_list=groundtruth_weights_list)
     loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 3bd0491f..ffbe9148 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -402,8 +402,9 @@ class SSDMetaArch(model.DetectionModel):
               im_width=image_shape[2]))
       prediction_dict = self._box_predictor.predict(
           feature_maps, self._anchor_generator.num_anchors_per_location())
-      box_encodings = tf.squeeze(
-          tf.concat(prediction_dict['box_encodings'], axis=1), axis=2)
+      box_encodings = tf.concat(prediction_dict['box_encodings'], axis=1)
+      if box_encodings.shape.ndims == 4 and box_encodings.shape[2] == 1:
+        box_encodings = tf.squeeze(box_encodings, axis=2)
       class_predictions_with_background = tf.concat(
           prediction_dict['class_predictions_with_background'], axis=1)
       predictions_dict = {
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 25581dd1..b1b62a3c 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -243,21 +243,24 @@ class SsdMetaArchTest(test_case.TestCase):
                     (batch_size, None, None, 3),
                     (None, None, None, 3)]
 
-    expected_boxes = np.array([[[0, 0, .5, .5],
-                                [0, .5, .5, 1],
-                                [.5, 0, 1, .5],
-                                [0, 0, 0, 0],   # pruned prediction
-                                [0, 0, 0, 0]],  # padding
-                               [[0, 0, .5, .5],
-                                [0, .5, .5, 1],
-                                [.5, 0, 1, .5],
-                                [0, 0, 0, 0],  # pruned prediction
-                                [0, 0, 0, 0]]  # padding
-                              ])
-    expected_scores = np.array([[0, 0, 0, 0, 0],
-                                [0, 0, 0, 0, 0]])
-    expected_classes = np.array([[0, 0, 0, 0, 0],
-                                 [0, 0, 0, 0, 0]])
+    expected_boxes = [
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0],  # pruned prediction
+            [0, 0, 0, 0]
+        ],  # padding
+        [
+            [0, 0, .5, .5],
+            [0, .5, .5, 1],
+            [.5, 0, 1, .5],
+            [0, 0, 0, 0],  # pruned prediction
+            [0, 0, 0, 0]
+        ]
+    ]  # padding
+    expected_scores = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
+    expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
 
     for input_shape in input_shapes:
@@ -282,7 +285,11 @@ class SsdMetaArchTest(test_case.TestCase):
                                       input_placeholder:
                                       np.random.uniform(
                                           size=(batch_size, 2, 2, 3))})
-      self.assertAllClose(detections_out['detection_boxes'], expected_boxes)
+      for image_idx in range(batch_size):
+        self.assertTrue(
+            test_utils.first_rows_close_as_set(
+                detections_out['detection_boxes'][image_idx].tolist(),
+                expected_boxes[image_idx]))
       self.assertAllClose(detections_out['detection_scores'], expected_scores)
       self.assertAllClose(detections_out['detection_classes'], expected_classes)
       self.assertAllClose(detections_out['num_detections'],
@@ -429,7 +436,7 @@ class SsdMetaArchTest(test_case.TestCase):
 
   def test_restore_map_for_detection_ckpt(self):
     model, _, _, _ = self._create_model()
-    model.predict(tf.constant(np.array([[[0, 0], [1, 1]], [[1, 0], [0, 1]]],
+    model.predict(tf.constant(np.array([[[[0, 0], [1, 1]], [[1, 0], [0, 1]]]],
                                        dtype=np.float32)),
                   true_image_shapes=None)
     init_op = tf.global_variables_initializer()
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
new file mode 100644
index 00000000..3c8bb54a
--- /dev/null
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
@@ -0,0 +1,151 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Runs evaluation using OpenImages groundtruth and predictions.
+
+Example usage:
+  python third_party/tensorflow_models/object_detection/\
+  metrics/oid_vrd_challenge_evaluation.py \
+    --input_annotations_boxes=/path/to/input/annotations-human-bbox.csv \
+    --input_annotations_labels=/path/to/input/annotations-label.csv \
+    --input_class_labelmap=/path/to/input/class_labelmap.pbtxt \
+    --input_relationship_labelmap=/path/to/input/relationship_labelmap.pbtxt \
+    --input_predictions=/path/to/input/predictions.csv \
+    --output_metrics=/path/to/output/metric.csv \
+
+CSVs with bounding box annotations and image label (including the image URLs)
+can be downloaded from the Open Images Challenge website:
+https://storage.googleapis.com/openimages/web/challenge.html
+The format of the input csv and the metrics itself are described on the
+challenge website.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import pandas as pd
+from google.protobuf import text_format
+
+from object_detection.metrics import oid_vrd_challenge_evaluation_utils as utils
+from object_detection.protos import string_int_label_map_pb2
+from object_detection.utils import vrd_evaluation
+
+
+def _load_labelmap(labelmap_path):
+  """Loads labelmap from the labelmap path.
+
+  Args:
+    labelmap_path: Path to the labelmap.
+
+  Returns:
+    A dictionary mapping class name to class numerical id.
+  """
+
+  label_map = string_int_label_map_pb2.StringIntLabelMap()
+  with open(labelmap_path, 'r') as fid:
+    label_map_string = fid.read()
+    text_format.Merge(label_map_string, label_map)
+  labelmap_dict = {}
+  for item in label_map.item:
+    labelmap_dict[item.name] = item.id
+  return labelmap_dict
+
+
+def _swap_labelmap_dict(labelmap_dict):
+  """Swaps keys and labels in labelmap.
+
+  Args:
+    labelmap_dict: Input dictionary.
+
+  Returns:
+    A dictionary mapping class name to class numerical id.
+  """
+  return dict((v, k) for k, v in labelmap_dict.iteritems())
+
+
+def main(parsed_args):
+  all_box_annotations = pd.read_csv(parsed_args.input_annotations_boxes)
+  all_label_annotations = pd.read_csv(parsed_args.input_annotations_labels)
+  all_annotations = pd.concat([all_box_annotations, all_label_annotations])
+
+  class_label_map = _load_labelmap(parsed_args.input_class_labelmap)
+  relationship_label_map = _load_labelmap(
+      parsed_args.input_relationship_labelmap)
+
+  relation_evaluator = vrd_evaluation.VRDRelationDetectionEvaluator()
+  phrase_evaluator = vrd_evaluation.VRDPhraseDetectionEvaluator()
+
+  for _, groundtruth in enumerate(all_annotations.groupby('ImageID')):
+    image_id, image_groundtruth = groundtruth
+    groundtruth_dictionary = utils.build_groundtruth_vrd_dictionary(
+        image_groundtruth, class_label_map, relationship_label_map)
+
+    relation_evaluator.add_single_ground_truth_image_info(
+        image_id, groundtruth_dictionary)
+    phrase_evaluator.add_single_ground_truth_image_info(image_id,
+                                                        groundtruth_dictionary)
+
+  all_predictions = pd.read_csv(parsed_args.input_predictions)
+  for _, prediction_data in enumerate(all_predictions.groupby('ImageID')):
+    image_id, image_predictions = prediction_data
+    prediction_dictionary = utils.build_predictions_vrd_dictionary(
+        image_predictions, class_label_map, relationship_label_map)
+
+    relation_evaluator.add_single_detected_image_info(image_id,
+                                                      prediction_dictionary)
+    phrase_evaluator.add_single_detected_image_info(image_id,
+                                                    prediction_dictionary)
+
+  relation_metrics = relation_evaluator.evaluate()
+  phrase_metrics = phrase_evaluator.evaluate()
+
+  with open(parsed_args.output_metrics, 'w') as fid:
+    utils.write_csv(fid, relation_metrics)
+    utils.write_csv(fid, phrase_metrics)
+
+
+if __name__ == '__main__':
+
+  parser = argparse.ArgumentParser(
+      description=
+      'Evaluate Open Images Visual Relationship Detection predictions.')
+  parser.add_argument(
+      '--input_annotations_boxes',
+      required=True,
+      help='File with groundtruth vrd annotations.')
+  parser.add_argument(
+      '--input_annotations_labels',
+      required=True,
+      help='File with groundtruth labels annotations')
+  parser.add_argument(
+      '--input_predictions',
+      required=True,
+      help="""File with detection predictions; NOTE: no postprocessing is
+      applied in the evaluation script.""")
+  parser.add_argument(
+      '--input_class_labelmap',
+      required=True,
+      help="""OpenImages Challenge labelmap; note: it is expected to include
+      attributes.""")
+  parser.add_argument(
+      '--input_relationship_labelmap',
+      required=True,
+      help="""OpenImages Challenge relationship labelmap.""")
+  parser.add_argument(
+      '--output_metrics', required=True, help='Output file with csv metrics')
+
+  args = parser.parse_args()
+  main(args)
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py
new file mode 100644
index 00000000..8c834775
--- /dev/null
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py
@@ -0,0 +1,133 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Converts data from CSV format to the VRDDetectionEvaluator format."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import csv
+import numpy as np
+from object_detection.core import standard_fields
+from object_detection.utils import vrd_evaluation
+
+
+def build_groundtruth_vrd_dictionary(data, class_label_map,
+                                     relationship_label_map):
+  """Builds a groundtruth dictionary from groundtruth data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the groundtruth data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+    relationship_label_map: Relationship type labelmap from string name to an
+      integer.
+
+  Returns:
+    A dictionary with keys suitable for passing to
+    VRDDetectionEvaluator.add_single_ground_truth_image_info:
+        standard_fields.InputDataFields.groundtruth_boxes: A numpy array
+          of structures with the shape [M, 1], representing M tuples, each tuple
+          containing the same number of named bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max] (see
+          datatype vrd_box_data_type, single_box_data_type above).
+        standard_fields.InputDataFields.groundtruth_classes: A numpy array of
+          structures shape [M, 1], representing  the class labels of the
+          corresponding bounding boxes and possibly additional classes (see
+          datatype label_data_type above).
+        standard_fields.InputDataFields.verified_labels: numpy array
+          of shape [K] containing verified labels.
+  """
+  data_boxes = data[data.LabelName.isnull()]
+  data_labels = data[data.LabelName1.isnull()]
+
+  boxes = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.vrd_box_data_type)
+  boxes['subject'] = data_boxes[['YMin1', 'XMin1', 'YMax1',
+                                 'XMax1']].as_matrix()
+  boxes['object'] = data_boxes[['YMin2', 'XMin2', 'YMax2', 'XMax2']].as_matrix()
+
+  labels = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.label_data_type)
+  labels['subject'] = data_boxes['LabelName1'].map(lambda x: class_label_map[x])
+  labels['object'] = data_boxes['LabelName2'].map(lambda x: class_label_map[x])
+  labels['relation'] = data_boxes['RelationshipLabel'].map(
+      lambda x: relationship_label_map[x])
+
+  return {
+      standard_fields.InputDataFields.groundtruth_boxes:
+          boxes,
+      standard_fields.InputDataFields.groundtruth_classes:
+          labels,
+      standard_fields.InputDataFields.verified_labels:
+          data_labels['LabelName'].map(lambda x: class_label_map[x]),
+  }
+
+
+def build_predictions_vrd_dictionary(data, class_label_map,
+                                     relationship_label_map):
+  """Builds a predictions dictionary from predictions data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the predictions data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+    relationship_label_map: Relationship type labelmap from string name to an
+      integer.
+
+  Returns:
+    Dictionary with keys suitable for passing to
+    VRDDetectionEvaluator.add_single_detected_image_info:
+        standard_fields.DetectionResultFields.detection_boxes: A numpy array of
+          structures with shape [N, 1], representing N tuples, each tuple
+          containing the same number of named bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max] (as an example
+          see datatype vrd_box_data_type, single_box_data_type above).
+        standard_fields.DetectionResultFields.detection_scores: float32 numpy
+          array of shape [N] containing detection scores for the boxes.
+        standard_fields.DetectionResultFields.detection_classes: A numpy array
+          of structures shape [N, 1], representing the class labels of the
+          corresponding bounding boxes and possibly additional classes (see
+          datatype label_data_type above).
+  """
+  data_boxes = data
+
+  boxes = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.vrd_box_data_type)
+  boxes['subject'] = data_boxes[['YMin1', 'XMin1', 'YMax1',
+                                 'XMax1']].as_matrix()
+  boxes['object'] = data_boxes[['YMin2', 'XMin2', 'YMax2', 'XMax2']].as_matrix()
+
+  labels = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.label_data_type)
+  labels['subject'] = data_boxes['LabelName1'].map(lambda x: class_label_map[x])
+  labels['object'] = data_boxes['LabelName2'].map(lambda x: class_label_map[x])
+  labels['relation'] = data_boxes['RelationshipLabel'].map(
+      lambda x: relationship_label_map[x])
+
+  return {
+      standard_fields.DetectionResultFields.detection_boxes:
+          boxes,
+      standard_fields.DetectionResultFields.detection_classes:
+          labels,
+      standard_fields.DetectionResultFields.detection_scores:
+          data_boxes['Score'].as_matrix()
+  }
+
+
+def write_csv(fid, metrics):
+  """Writes metrics key-value pairs to CSV file.
+
+  Args:
+    fid: File identifier of an opened file.
+    metrics: A dictionary with metrics to be written.
+  """
+  metrics_writer = csv.writer(fid, delimiter=',')
+  for metric_name, metric_value in metrics.items():
+    metrics_writer.writerow([metric_name, str(metric_value)])
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
new file mode 100644
index 00000000..49ce0898
--- /dev/null
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
@@ -0,0 +1,149 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for oid_vrd_challenge_evaluation_utils."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+from object_detection.core import standard_fields
+from object_detection.metrics import oid_vrd_challenge_evaluation_utils as utils
+from object_detection.utils import vrd_evaluation
+
+
+class OidVrdChallengeEvaluationUtilsTest(tf.test.TestCase):
+
+  def testBuildGroundtruthDictionary(self):
+    np_data = pd.DataFrame(
+        [[
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/083vt', 0.0, 0.3, 0.5, 0.6,
+            0.0, 0.3, 0.5, 0.6, 'is', None, None
+        ], [
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/02gy9n', 0.0, 0.3, 0.5, 0.6,
+            0.1, 0.2, 0.3, 0.4, 'under', None, None
+        ], [
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/083vt', 0.0, 0.1, 0.2, 0.3,
+            0.0, 0.1, 0.2, 0.3, 'is', None, None
+        ], [
+            'fe58ec1b06db2bb7', '/m/083vt', '/m/04bcr3', 0.1, 0.2, 0.3, 0.4,
+            0.5, 0.6, 0.7, 0.8, 'at', None, None
+        ], [
+            'fe58ec1b06db2bb7', None, None, None, None, None, None, None, None,
+            None, None, None, '/m/04bcr3', 1.0
+        ], [
+            'fe58ec1b06db2bb7', None, None, None, None, None, None, None, None,
+            None, None, None, '/m/083vt', 0.0
+        ], [
+            'fe58ec1b06db2bb7', None, None, None, None, None, None, None, None,
+            None, None, None, '/m/02gy9n', 0.0
+        ]],
+        columns=[
+            'ImageID', 'LabelName1', 'LabelName2', 'XMin1', 'XMax1', 'YMin1',
+            'YMax1', 'XMin2', 'XMax2', 'YMin2', 'YMax2', 'RelationshipLabel',
+            'LabelName', 'Confidence'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    relationship_label_map = {'is': 1, 'under': 2, 'at': 3}
+    groundtruth_dictionary = utils.build_groundtruth_vrd_dictionary(
+        np_data, class_label_map, relationship_label_map)
+
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_boxes in
+                    groundtruth_dictionary)
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_classes in
+                    groundtruth_dictionary)
+    self.assertTrue(standard_fields.InputDataFields.verified_labels in
+                    groundtruth_dictionary)
+
+    self.assertAllEqual(
+        np.array(
+            [(1, 2, 1), (1, 3, 2), (1, 2, 1), (2, 1, 3)],
+            dtype=vrd_evaluation.label_data_type), groundtruth_dictionary[
+                standard_fields.InputDataFields.groundtruth_classes])
+    expected_vrd_data = np.array(
+        [
+            ([0.5, 0.0, 0.6, 0.3], [0.5, 0.0, 0.6, 0.3]),
+            ([0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2]),
+            ([0.2, 0.0, 0.3, 0.1], [0.2, 0.0, 0.3, 0.1]),
+            ([0.3, 0.1, 0.4, 0.2], [0.7, 0.5, 0.8, 0.6]),
+        ],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    for field in expected_vrd_data.dtype.fields:
+      self.assertNDArrayNear(
+          expected_vrd_data[field], groundtruth_dictionary[
+              standard_fields.InputDataFields.groundtruth_boxes][field], 1e-5)
+    self.assertAllEqual(
+        np.array([1, 2, 3]),
+        groundtruth_dictionary[standard_fields.InputDataFields.verified_labels])
+
+  def testBuildPredictionDictionary(self):
+    np_data = pd.DataFrame(
+        [[
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/083vt', 0.0, 0.3, 0.5, 0.6,
+            0.0, 0.3, 0.5, 0.6, 'is', 0.1
+        ], [
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/02gy9n', 0.0, 0.3, 0.5, 0.6,
+            0.1, 0.2, 0.3, 0.4, 'under', 0.2
+        ], [
+            'fe58ec1b06db2bb7', '/m/04bcr3', '/m/083vt', 0.0, 0.1, 0.2, 0.3,
+            0.0, 0.1, 0.2, 0.3, 'is', 0.3
+        ], [
+            'fe58ec1b06db2bb7', '/m/083vt', '/m/04bcr3', 0.1, 0.2, 0.3, 0.4,
+            0.5, 0.6, 0.7, 0.8, 'at', 0.4
+        ]],
+        columns=[
+            'ImageID', 'LabelName1', 'LabelName2', 'XMin1', 'XMax1', 'YMin1',
+            'YMax1', 'XMin2', 'XMax2', 'YMin2', 'YMax2', 'RelationshipLabel',
+            'Score'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    relationship_label_map = {'is': 1, 'under': 2, 'at': 3}
+    prediction_dictionary = utils.build_predictions_vrd_dictionary(
+        np_data, class_label_map, relationship_label_map)
+
+    self.assertTrue(standard_fields.DetectionResultFields.detection_boxes in
+                    prediction_dictionary)
+    self.assertTrue(standard_fields.DetectionResultFields.detection_classes in
+                    prediction_dictionary)
+    self.assertTrue(standard_fields.DetectionResultFields.detection_scores in
+                    prediction_dictionary)
+
+    self.assertAllEqual(
+        np.array(
+            [(1, 2, 1), (1, 3, 2), (1, 2, 1), (2, 1, 3)],
+            dtype=vrd_evaluation.label_data_type), prediction_dictionary[
+                standard_fields.DetectionResultFields.detection_classes])
+    expected_vrd_data = np.array(
+        [
+            ([0.5, 0.0, 0.6, 0.3], [0.5, 0.0, 0.6, 0.3]),
+            ([0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2]),
+            ([0.2, 0.0, 0.3, 0.1], [0.2, 0.0, 0.3, 0.1]),
+            ([0.3, 0.1, 0.4, 0.2], [0.7, 0.5, 0.8, 0.6]),
+        ],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    for field in expected_vrd_data.dtype.fields:
+      self.assertNDArrayNear(
+          expected_vrd_data[field], prediction_dictionary[
+              standard_fields.DetectionResultFields.detection_boxes][field],
+          1e-5)
+    self.assertNDArrayNear(
+        np.array([0.1, 0.2, 0.3, 0.4]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_scores], 1e-5)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/tf_example_parser.py b/research/object_detection/metrics/tf_example_parser.py
index 5b8ab7af..22a28e8a 100644
--- a/research/object_detection/metrics/tf_example_parser.py
+++ b/research/object_detection/metrics/tf_example_parser.py
@@ -113,7 +113,9 @@ class TfExampleDetectionAndGTParser(data_parser.DataToNumpyParser):
         fields.InputDataFields.groundtruth_difficult:
             Int64Parser(fields.TfExampleFields.object_difficult),
         fields.InputDataFields.groundtruth_group_of:
-            Int64Parser(fields.TfExampleFields.object_group_of)
+            Int64Parser(fields.TfExampleFields.object_group_of),
+        fields.InputDataFields.verified_labels:
+            Int64Parser(fields.TfExampleFields.image_class_label),
     }
 
   def parse(self, tf_example):
diff --git a/research/object_detection/metrics/tf_example_parser_test.py b/research/object_detection/metrics/tf_example_parser_test.py
index 6d9ce748..99a64d5f 100644
--- a/research/object_detection/metrics/tf_example_parser_test.py
+++ b/research/object_detection/metrics/tf_example_parser_test.py
@@ -44,6 +44,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     object_class_label = [1, 1, 2]
     object_difficult = [1, 0, 0]
     object_group_of = [0, 0, 1]
+    verified_labels = [1, 2, 3, 4]
     detection_class_label = [2, 1]
     detection_score = [0.5, 0.3]
     features = {
@@ -113,10 +114,19 @@ class TfExampleDecoderTest(tf.test.TestCase):
     example = tf.train.Example(features=tf.train.Features(feature=features))
     results_dict = parser.parse(example)
     self.assertIsNotNone(results_dict)
-    np_testing.assert_almost_equal(
+    np_testing.assert_equal(
         object_group_of,
         results_dict[fields.InputDataFields.groundtruth_group_of])
 
+    features[fields.TfExampleFields.image_class_label] = (
+        self._Int64Feature(verified_labels))
+
+    example = tf.train.Example(features=tf.train.Features(feature=features))
+    results_dict = parser.parse(example)
+    self.assertIsNotNone(results_dict)
+    np_testing.assert_equal(
+        verified_labels, results_dict[fields.InputDataFields.verified_labels])
+
   def testParseString(self):
     string_val = 'abc'
     features = {'string': self._BytesFeature(string_val)}
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 8a5de22d..973ca259 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -25,6 +25,7 @@ import tensorflow as tf
 
 from object_detection import eval_util
 from object_detection import inputs
+from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.builders import optimizer_builder
 from object_detection.core import standard_fields as fields
@@ -289,7 +290,11 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       total_loss = tf.add_n(losses, name='total_loss')
       losses_dict['Loss/total_loss'] = total_loss
 
-    if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:
+      if 'graph_rewriter_config' in configs:
+        graph_rewriter_fn = graph_rewriter_builder.build(
+            configs['graph_rewriter_config'], is_training=is_training)
+        graph_rewriter_fn()
+
       # TODO(rathodv): Stop creating optimizer summary vars in EVAL mode once we
       # can write learning rate summaries on TPU without host calls.
       global_step = tf.train.get_or_create_global_step()
@@ -333,6 +338,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       }
 
     eval_metric_ops = None
+    scaffold = None
     if mode == tf.estimator.ModeKeys.EVAL:
       class_agnostic = (fields.DetectionResultFields.detection_classes
                         not in detections)
@@ -360,7 +366,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
         detection_and_groundtruth = (
             vis_utils.draw_side_by_side_evaluation_image(
                 eval_dict, category_index, max_boxes_to_draw=20,
-                min_score_thresh=0.2))
+                min_score_thresh=0.2,
+                use_normalized_coordinates=False))
         img_summary = tf.summary.image('Detections_Left_Groundtruth_Right',
                                        detection_and_groundtruth)
 
@@ -382,6 +389,16 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
             img_summary, tf.no_op())
       eval_metric_ops = {str(k): v for k, v in eval_metric_ops.iteritems()}
 
+      if eval_config.use_moving_averages:
+        variable_averages = tf.train.ExponentialMovingAverage(0.0)
+        variables_to_restore = variable_averages.variables_to_restore()
+        keep_checkpoint_every_n_hours = (
+            train_config.keep_checkpoint_every_n_hours)
+        saver = tf.train.Saver(
+            variables_to_restore,
+            keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
+        scaffold = tf.train.Scaffold(saver=saver)
+
     if use_tpu:
       return tf.contrib.tpu.TPUEstimatorSpec(
           mode=mode,
@@ -398,7 +415,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
           loss=total_loss,
           train_op=train_op,
           eval_metric_ops=eval_metric_ops,
-          export_outputs=export_outputs)
+          export_outputs=export_outputs,
+          scaffold=scaffold)
 
   return model_fn
 
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index bff4abbb..f0cad235 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -147,23 +147,19 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         with (slim.arg_scope(self._conv_hyperparams_fn())
               if self._override_base_feature_extractor_hyperparams
               else context_manager.IdentityContextManager()):
-        # TODO(skligys): Enable fused batch norm once quantization supports it.
-          with slim.arg_scope([slim.batch_norm], fused=False):
-            _, image_features = mobilenet_v1.mobilenet_v1_base(
-                ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-                final_endpoint='Conv2d_13_pointwise',
-                min_depth=self._min_depth,
-                depth_multiplier=self._depth_multiplier,
-                use_explicit_padding=self._use_explicit_padding,
-                scope=scope)
-      with slim.arg_scope(self._conv_hyperparams_fn()):
-        # TODO(skligys): Enable fused batch norm once quantization supports it.
-        with slim.arg_scope([slim.batch_norm], fused=False):
-          feature_maps = feature_map_generators.multi_resolution_feature_maps(
-              feature_map_layout=feature_map_layout,
-              depth_multiplier=self._depth_multiplier,
+          _, image_features = mobilenet_v1.mobilenet_v1_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='Conv2d_13_pointwise',
               min_depth=self._min_depth,
-              insert_1x1_conv=True,
-              image_features=image_features)
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
deleted file mode 100644
index 2c72eeb4..00000000
--- a/research/object_detection/models/feature_map_generators.py
+++ /dev/null
@@ -1,225 +0,0 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-"""Functions to generate a list of feature maps based on image features.
-
-Provides several feature map generators that can be used to build object
-detection feature extractors.
-
-Object detection feature extractors usually are built by stacking two components
-- A base feature extractor such as Inception V3 and a feature map generator.
-Feature map generators build on the base feature extractors and produce a list
-of final feature maps.
-"""
-import collections
-import tensorflow as tf
-from object_detection.utils import ops
-slim = tf.contrib.slim
-
-
-def get_depth_fn(depth_multiplier, min_depth):
-  """Builds a callable to compute depth (output channels) of conv filters.
-
-  Args:
-    depth_multiplier: a multiplier for the nominal depth.
-    min_depth: a lower bound on the depth of filters.
-
-  Returns:
-    A callable that takes in a nominal depth and returns the depth to use.
-  """
-  def multiply_depth(depth):
-    new_depth = int(depth * depth_multiplier)
-    return max(new_depth, min_depth)
-  return multiply_depth
-
-
-def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
-                                  min_depth, insert_1x1_conv, image_features):
-  """Generates multi resolution feature maps from input image features.
-
-  Generates multi-scale feature maps for detection as in the SSD papers by
-  Liu et al: https://arxiv.org/pdf/1512.02325v2.pdf, See Sec 2.1.
-
-  More specifically, it performs the following two tasks:
-  1) If a layer name is provided in the configuration, returns that layer as a
-     feature map.
-  2) If a layer name is left as an empty string, constructs a new feature map
-     based on the spatial shape and depth configuration. Note that the current
-     implementation only supports generating new layers using convolution of
-     stride 2 resulting in a spatial resolution reduction by a factor of 2.
-     By default convolution kernel size is set to 3, and it can be customized
-     by caller.
-
-  An example of the configuration for Inception V3:
-  {
-    'from_layer': ['Mixed_5d', 'Mixed_6e', 'Mixed_7c', '', '', ''],
-    'layer_depth': [-1, -1, -1, 512, 256, 128]
-  }
-
-  Args:
-    feature_map_layout: Dictionary of specifications for the feature map
-      layouts in the following format (Inception V2/V3 respectively):
-      {
-        'from_layer': ['Mixed_3c', 'Mixed_4c', 'Mixed_5c', '', '', ''],
-        'layer_depth': [-1, -1, -1, 512, 256, 128]
-      }
-      or
-      {
-        'from_layer': ['Mixed_5d', 'Mixed_6e', 'Mixed_7c', '', '', '', ''],
-        'layer_depth': [-1, -1, -1, 512, 256, 128]
-      }
-      If 'from_layer' is specified, the specified feature map is directly used
-      as a box predictor layer, and the layer_depth is directly infered from the
-      feature map (instead of using the provided 'layer_depth' parameter). In
-      this case, our convention is to set 'layer_depth' to -1 for clarity.
-      Otherwise, if 'from_layer' is an empty string, then the box predictor
-      layer will be built from the previous layer using convolution operations.
-      Note that the current implementation only supports generating new layers
-      using convolutions of stride 2 (resulting in a spatial resolution
-      reduction by a factor of 2), and will be extended to a more flexible
-      design. Convolution kernel size is set to 3 by default, and can be
-      customized by 'conv_kernel_size' parameter (similarily, 'conv_kernel_size'
-      should be set to -1 if 'from_layer' is specified). The created convolution
-      operation will be a normal 2D convolution by default, and a depthwise
-      convolution followed by 1x1 convolution if 'use_depthwise' is set to True.
-    depth_multiplier: Depth multiplier for convolutional layers.
-    min_depth: Minimum depth for convolutional layers.
-    insert_1x1_conv: A boolean indicating whether an additional 1x1 convolution
-      should be inserted before shrinking the feature map.
-    image_features: A dictionary of handles to activation tensors from the
-      base feature extractor.
-
-  Returns:
-    feature_maps: an OrderedDict mapping keys (feature map names) to
-      tensors where each tensor has shape [batch, height_i, width_i, depth_i].
-
-  Raises:
-    ValueError: if the number entries in 'from_layer' and
-      'layer_depth' do not match.
-    ValueError: if the generated layer does not have the same resolution
-      as specified.
-  """
-  depth_fn = get_depth_fn(depth_multiplier, min_depth)
-
-  feature_map_keys = []
-  feature_maps = []
-  base_from_layer = ''
-  use_explicit_padding = False
-  if 'use_explicit_padding' in feature_map_layout:
-    use_explicit_padding = feature_map_layout['use_explicit_padding']
-  use_depthwise = False
-  if 'use_depthwise' in feature_map_layout:
-    use_depthwise = feature_map_layout['use_depthwise']
-  for index, from_layer in enumerate(feature_map_layout['from_layer']):
-    layer_depth = feature_map_layout['layer_depth'][index]
-    conv_kernel_size = 3
-    if 'conv_kernel_size' in feature_map_layout:
-      conv_kernel_size = feature_map_layout['conv_kernel_size'][index]
-    if from_layer:
-      feature_map = image_features[from_layer]
-      base_from_layer = from_layer
-      feature_map_keys.append(from_layer)
-    else:
-      pre_layer = feature_maps[-1]
-      intermediate_layer = pre_layer
-      if insert_1x1_conv:
-        layer_name = '{}_1_Conv2d_{}_1x1_{}'.format(
-            base_from_layer, index, depth_fn(layer_depth / 2))
-        intermediate_layer = slim.conv2d(
-            pre_layer,
-            depth_fn(layer_depth / 2), [1, 1],
-            padding='SAME',
-            stride=1,
-            scope=layer_name)
-      layer_name = '{}_2_Conv2d_{}_{}x{}_s2_{}'.format(
-          base_from_layer, index, conv_kernel_size, conv_kernel_size,
-          depth_fn(layer_depth))
-      stride = 2
-      padding = 'SAME'
-      if use_explicit_padding:
-        padding = 'VALID'
-        intermediate_layer = ops.fixed_padding(
-            intermediate_layer, conv_kernel_size)
-      if use_depthwise:
-        feature_map = slim.separable_conv2d(
-            intermediate_layer,
-            None, [conv_kernel_size, conv_kernel_size],
-            depth_multiplier=1,
-            padding=padding,
-            stride=stride,
-            scope=layer_name + '_depthwise')
-        feature_map = slim.conv2d(
-            feature_map,
-            depth_fn(layer_depth), [1, 1],
-            padding='SAME',
-            stride=1,
-            scope=layer_name)
-      else:
-        feature_map = slim.conv2d(
-            intermediate_layer,
-            depth_fn(layer_depth), [conv_kernel_size, conv_kernel_size],
-            padding=padding,
-            stride=stride,
-            scope=layer_name)
-      feature_map_keys.append(layer_name)
-    feature_maps.append(feature_map)
-  return collections.OrderedDict(
-      [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])
-
-
-def fpn_top_down_feature_maps(image_features, depth, scope=None):
-  """Generates `top-down` feature maps for Feature Pyramid Networks.
-
-  See https://arxiv.org/abs/1612.03144 for details.
-
-  Args:
-    image_features: list of tuples of (tensor_name, image_feature_tensor).
-      Spatial resolutions of succesive tensors must reduce exactly by a factor
-      of 2.
-    depth: depth of output feature maps.
-    scope: A scope name to wrap this op under.
-
-  Returns:
-    feature_maps: an OrderedDict mapping keys (feature map names) to
-      tensors where each tensor has shape [batch, height_i, width_i, depth_i].
-  """
-  with tf.name_scope(scope, 'top_down'):
-    num_levels = len(image_features)
-    output_feature_maps_list = []
-    output_feature_map_keys = []
-    with slim.arg_scope(
-        [slim.conv2d], padding='SAME', stride=1):
-      top_down = slim.conv2d(
-          image_features[-1][1],
-          depth, [1, 1], activation_fn=None, normalizer_fn=None,
-          scope='projection_%d' % num_levels)
-      output_feature_maps_list.append(top_down)
-      output_feature_map_keys.append(
-          'top_down_%s' % image_features[-1][0])
-
-      for level in reversed(range(num_levels - 1)):
-        top_down = ops.nearest_neighbor_upsampling(top_down, 2)
-        residual = slim.conv2d(
-            image_features[level][1], depth, [1, 1],
-            activation_fn=None, normalizer_fn=None,
-            scope='projection_%d' % (level + 1))
-        top_down += residual
-        output_feature_maps_list.append(slim.conv2d(
-            top_down,
-            depth, [3, 3],
-            scope='smoothing_%d' % (level + 1)))
-        output_feature_map_keys.append('top_down_%s' % image_features[level][0])
-      return collections.OrderedDict(
-          reversed(zip(output_feature_map_keys, output_feature_maps_list)))
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index b1aaf390..aada1111 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -110,23 +110,19 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         with (slim.arg_scope(self._conv_hyperparams_fn())
               if self._override_base_feature_extractor_hyperparams
               else context_manager.IdentityContextManager()):
-        # TODO(skligys): Enable fused batch norm once quantization supports it.
-          with slim.arg_scope([slim.batch_norm], fused=False):
-            _, image_features = mobilenet_v1.mobilenet_v1_base(
-                ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-                final_endpoint='Conv2d_13_pointwise',
-                min_depth=self._min_depth,
-                depth_multiplier=self._depth_multiplier,
-                use_explicit_padding=self._use_explicit_padding,
-                scope=scope)
-      with slim.arg_scope(self._conv_hyperparams_fn()):
-        # TODO(skligys): Enable fused batch norm once quantization supports it.
-        with slim.arg_scope([slim.batch_norm], fused=False):
-          feature_maps = feature_map_generators.multi_resolution_feature_maps(
-              feature_map_layout=feature_map_layout,
-              depth_multiplier=self._depth_multiplier,
+          _, image_features = mobilenet_v1.mobilenet_v1_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='Conv2d_13_pointwise',
               min_depth=self._min_depth,
-              insert_1x1_conv=True,
-              image_features=image_features)
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index 72305133..d3a9542b 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -148,7 +148,7 @@ class SsdMobilenetV1FeatureExtractorTest(
     self.check_feature_extractor_variables_under_scope(
         depth_multiplier, pad_to_multiple, scope_name)
 
-  def test_nofused_batchnorm(self):
+  def test_has_fused_batchnorm(self):
     image_height = 40
     image_width = 40
     depth_multiplier = 1
@@ -159,8 +159,8 @@ class SsdMobilenetV1FeatureExtractorTest(
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
     _ = feature_extractor.extract_features(preprocessed_image)
-    self.assertFalse(any(op.type == 'FusedBatchNorm'
-                         for op in tf.get_default_graph().get_operations()))
+    self.assertTrue(any(op.type == 'FusedBatchNorm'
+                        for op in tf.get_default_graph().get_operations()))
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
index a5f39534..014b93a8 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
@@ -112,24 +112,18 @@ class SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         with (slim.arg_scope(self._conv_hyperparams_fn())
               if self._override_base_feature_extractor_hyperparams else
               context_manager.IdentityContextManager()):
-          # TODO(b/68150321): Enable fused batch norm once quantization
-          # supports it.
-          with slim.arg_scope([slim.batch_norm], fused=False):
-            _, image_features = mobilenet_v2.mobilenet_base(
-                ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-                final_endpoint='layer_19',
-                depth_multiplier=self._depth_multiplier,
-                use_explicit_padding=self._use_explicit_padding,
-                scope=scope)
+          _, image_features = mobilenet_v2.mobilenet_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='layer_19',
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
         with slim.arg_scope(self._conv_hyperparams_fn()):
-          # TODO(b/68150321): Enable fused batch norm once quantization
-          # supports it.
-          with slim.arg_scope([slim.batch_norm], fused=False):
-            feature_maps = feature_map_generators.multi_resolution_feature_maps(
-                feature_map_layout=feature_map_layout,
-                depth_multiplier=self._depth_multiplier,
-                min_depth=self._min_depth,
-                insert_1x1_conv=True,
-                image_features=image_features)
+          feature_maps = feature_map_generators.multi_resolution_feature_maps(
+              feature_map_layout=feature_map_layout,
+              depth_multiplier=self._depth_multiplier,
+              min_depth=self._min_depth,
+              insert_1x1_conv=True,
+              image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
index 74e16044..0b374749 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
@@ -135,7 +135,7 @@ class SsdMobilenetV2FeatureExtractorTest(
     self.check_feature_extractor_variables_under_scope(
         depth_multiplier, pad_to_multiple, scope_name)
 
-  def test_nofused_batchnorm(self):
+  def test_has_fused_batchnorm(self):
     image_height = 40
     image_width = 40
     depth_multiplier = 1
@@ -146,8 +146,8 @@ class SsdMobilenetV2FeatureExtractorTest(
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(image_placeholder)
     _ = feature_extractor.extract_features(preprocessed_image)
-    self.assertFalse(any(op.type == 'FusedBatchNorm'
-                         for op in tf.get_default_graph().get_operations()))
+    self.assertTrue(any(op.type == 'FusedBatchNorm'
+                        for op in tf.get_default_graph().get_operations()))
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index e431fc9d..f8b3f660 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -37,6 +37,10 @@ message KeepAspectRatioResizer {
 
   // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
   optional bool convert_to_grayscale = 5 [default = false];
+
+  // Per-channel pad value. This is only used when pad_to_max_dimension is True.
+  // If unspecified, a default pad value of 0 is applied to all channels.
+  repeated float per_channel_pad_value = 6;
 }
 
 // Configuration proto for image resizer that resizes to a fixed shape.
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index 3fcae2d0..a0000460 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -69,6 +69,10 @@ message InputReader {
   // Type of instance mask.
   optional InstanceMaskType mask_type = 10 [default = NUMERICAL_MASKS];
 
+  // Whether to use the display name when decoding examples. This is only used
+  // when mapping class text strings to integers.
+  optional bool use_display_name = 17 [default = false];
+
   oneof input_reader {
     TFRecordInputReader tf_record_input_reader = 8;
     ExternalInputReader external_input_reader = 9;
diff --git a/research/object_detection/trainer.py b/research/object_detection/trainer.py
index 268ecb1a..caa7187d 100644
--- a/research/object_detection/trainer.py
+++ b/research/object_detection/trainer.py
@@ -235,6 +235,9 @@ def train(create_tensor_dict_fn,
       built (before optimization). This is helpful to perform additional changes
       to the training graph such as adding FakeQuant ops. The function should
       modify the default graph.
+
+  Raises:
+    ValueError: If both num_clones > 1 and train_config.sync_replicas is true.
   """
 
   detection_model = create_model_fn()
@@ -256,9 +259,16 @@ def train(create_tensor_dict_fn,
     with tf.device(deploy_config.variables_device()):
       global_step = slim.create_global_step()
 
+    if num_clones != 1 and train_config.sync_replicas:
+      raise ValueError('In Synchronous SGD mode num_clones must ',
+                       'be 1. Found num_clones: {}'.format(num_clones))
+    batch_size = train_config.batch_size // num_clones
+    if train_config.sync_replicas:
+      batch_size //= train_config.replicas_to_aggregate
+
     with tf.device(deploy_config.inputs_device()):
       input_queue = create_input_queue(
-          train_config.batch_size // num_clones, create_tensor_dict_fn,
+          batch_size, create_tensor_dict_fn,
           train_config.batch_queue_capacity,
           train_config.num_batch_queue_threads,
           train_config.prefetch_queue_capacity, data_augmentation_options)
@@ -377,7 +387,8 @@ def train(create_tensor_dict_fn,
               train_config.load_all_detection_checkpoint_vars))
       available_var_map = (variables_helper.
                            get_variables_available_in_checkpoint(
-                               var_map, train_config.fine_tune_checkpoint))
+                               var_map, train_config.fine_tune_checkpoint,
+                               include_global_step=False))
       init_saver = tf.train.Saver(available_var_map)
       def initializer_fn(sess):
         init_saver.restore(sess, train_config.fine_tune_checkpoint)
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 944f50bd..d7ca1e98 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -278,6 +278,19 @@ def get_learning_rate_type(optimizer_config):
   return optimizer_config.learning_rate.WhichOneof("learning_rate")
 
 
+def _is_generic_key(key):
+  """Determines whether the key starts with a generic config dictionary key."""
+  for prefix in [
+      "graph_rewriter_config",
+      "model",
+      "train_input_config",
+      "train_input_config",
+      "train_config"]:
+    if key.startswith(prefix + "."):
+      return True
+  return False
+
+
 def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   """Updates `configs` dictionary based on supplied parameters.
 
@@ -287,6 +300,16 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   experiment, one can use a single base config file, and update particular
   values.
 
+  There are two types of field overrides:
+  1. Strategy-based overrides, which update multiple relevant configuration
+  options. For example, updating `learning_rate` will update both the warmup and
+  final learning rates.
+  2. Generic key/value, which update a specific parameter based on namespaced
+  configuration keys. For example,
+  `model.ssd.loss.hard_example_miner.max_negatives_per_positive` will update the
+  hard example miner configuration for an SSD model config. Generic overrides
+  are automatically detected based on the namespaced keys.
+
   Args:
     configs: Dictionary of configuration objects. See outputs from
       get_configs_from_pipeline_file() or get_configs_from_multiple_files().
@@ -302,44 +325,42 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   if hparams:
     kwargs.update(hparams.values())
   for key, value in kwargs.items():
+    tf.logging.info("Maybe overwriting %s: %s", key, value)
     # pylint: disable=g-explicit-bool-comparison
     if value == "" or value is None:
       continue
     # pylint: enable=g-explicit-bool-comparison
     if key == "learning_rate":
       _update_initial_learning_rate(configs, value)
-      tf.logging.info("Overwriting learning rate: %f", value)
-    if key == "batch_size":
+    elif key == "batch_size":
       _update_batch_size(configs, value)
-      tf.logging.info("Overwriting batch size: %d", value)
-    if key == "momentum_optimizer_value":
+    elif key == "momentum_optimizer_value":
       _update_momentum_optimizer_value(configs, value)
-      tf.logging.info("Overwriting momentum optimizer value: %f", value)
-    if key == "classification_localization_weight_ratio":
+    elif key == "classification_localization_weight_ratio":
       # Localization weight is fixed to 1.0.
       _update_classification_localization_weight_ratio(configs, value)
-    if key == "focal_loss_gamma":
+    elif key == "focal_loss_gamma":
       _update_focal_loss_gamma(configs, value)
-    if key == "focal_loss_alpha":
+    elif key == "focal_loss_alpha":
       _update_focal_loss_alpha(configs, value)
-    if key == "train_steps":
+    elif key == "train_steps":
       _update_train_steps(configs, value)
-      tf.logging.info("Overwriting train steps: %d", value)
-    if key == "eval_steps":
+    elif key == "eval_steps":
       _update_eval_steps(configs, value)
-      tf.logging.info("Overwriting eval steps: %d", value)
-    if key == "train_input_path":
+    elif key == "train_input_path":
       _update_input_path(configs["train_input_config"], value)
-      tf.logging.info("Overwriting train input path: %s", value)
-    if key == "eval_input_path":
+    elif key == "eval_input_path":
       _update_input_path(configs["eval_input_config"], value)
-      tf.logging.info("Overwriting eval input path: %s", value)
-    if key == "label_map_path":
+    elif key == "label_map_path":
       _update_label_map_path(configs, value)
-      tf.logging.info("Overwriting label map path: %s", value)
-    if key == "mask_type":
+    elif key == "mask_type":
       _update_mask_type(configs, value)
-      tf.logging.info("Overwritten mask type: %s", value)
+    elif key == "eval_with_moving_averages":
+      _update_use_moving_averages(configs, value)
+    elif _is_generic_key(key):
+      _update_generic(configs, key, value)
+    else:
+      tf.logging.info("Ignoring config override key: %s", key)
   return configs
 
 
@@ -411,6 +432,38 @@ def _update_batch_size(configs, batch_size):
   configs["train_config"].batch_size = max(1, int(round(batch_size)))
 
 
+def _validate_message_has_field(message, field):
+  if not message.HasField(field):
+    raise ValueError("Expecting message to have field %s" % field)
+
+
+def _update_generic(configs, key, value):
+  """Update a pipeline configuration parameter based on a generic key/value.
+
+  Args:
+    configs: Dictionary of pipeline configuration protos.
+    key: A string key, dot-delimited to represent the argument key.
+      e.g. "model.ssd.train_config.batch_size"
+    value: A value to set the argument to. The type of the value must match the
+      type for the protocol buffer. Note that setting the wrong type will
+      result in a TypeError.
+      e.g. 42
+
+  Raises:
+    ValueError if the message key does not match the existing proto fields.
+    TypeError the value type doesn't match the protobuf field type.
+  """
+  fields = key.split(".")
+  first_field = fields.pop(0)
+  last_field = fields.pop()
+  message = configs[first_field]
+  for field in fields:
+    _validate_message_has_field(message, field)
+    message = getattr(message, field)
+  _validate_message_has_field(message, last_field)
+  setattr(message, last_field, value)
+
+
 def _update_momentum_optimizer_value(configs, momentum):
   """Updates `configs` to reflect the new momentum value.
 
@@ -587,3 +640,17 @@ def _update_mask_type(configs, mask_type):
   """
   configs["train_input_config"].mask_type = mask_type
   configs["eval_input_config"].mask_type = mask_type
+
+
+def _update_use_moving_averages(configs, use_moving_averages):
+  """Updates the eval config option to use or not use moving averages.
+
+  The configs dictionary is updated in place, and hence not returned.
+
+  Args:
+    configs: Dictionary of configuration objects. See outputs from
+      get_configs_from_pipeline_file() or get_configs_from_multiple_files().
+    use_moving_averages: Boolean indicating whether moving average variables
+      should be loaded during evaluation.
+  """
+  configs["eval_config"].use_moving_averages = use_moving_averages
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index 2d3dd2e0..f528d453 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -69,6 +69,11 @@ def _update_optimizer_with_cosine_decay_learning_rate(
 
 class ConfigUtilTest(tf.test.TestCase):
 
+  def _create_and_load_test_configs(self, pipeline_config):
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    _write_config(pipeline_config, pipeline_config_path)
+    return config_util.get_configs_from_pipeline_file(pipeline_config_path)
+
   def test_get_configs_from_pipeline_file(self):
     """Test that proto configs can be read from pipeline config file."""
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
@@ -307,6 +312,34 @@ class ConfigUtilTest(tf.test.TestCase):
     new_batch_size = configs["train_config"].batch_size
     self.assertEqual(1, new_batch_size)  # Clipped to 1.0.
 
+  def testOverwriteBatchSizeWithKeyValue(self):
+    """Tests that batch size is overwritten based on key/value."""
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.train_config.batch_size = 2
+    configs = self._create_and_load_test_configs(pipeline_config)
+    hparams = tf.contrib.training.HParams(**{"train_config.batch_size": 10})
+    configs = config_util.merge_external_params_with_configs(configs, hparams)
+    new_batch_size = configs["train_config"].batch_size
+    self.assertEqual(10, new_batch_size)
+
+  def testKeyValueOverrideBadKey(self):
+    """Tests that overwriting with a bad key causes an exception."""
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    configs = self._create_and_load_test_configs(pipeline_config)
+    hparams = tf.contrib.training.HParams(**{"train_config.no_such_field": 10})
+    with self.assertRaises(ValueError):
+      config_util.merge_external_params_with_configs(configs, hparams)
+
+  def testOverwriteBatchSizeWithBadValueType(self):
+    """Tests that overwriting with a bad valuye type causes an exception."""
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.train_config.batch_size = 2
+    configs = self._create_and_load_test_configs(pipeline_config)
+    # Type should be an integer, but we're passing a string "10".
+    hparams = tf.contrib.training.HParams(**{"train_config.batch_size": "10"})
+    with self.assertRaises(TypeError):
+      config_util.merge_external_params_with_configs(configs, hparams)
+
   def testNewMomentumOptimizerValue(self):
     """Tests that new momentum value is updated appropriately."""
     original_momentum_value = 0.4
@@ -501,6 +534,19 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(new_mask_type, configs["train_input_config"].mask_type)
     self.assertEqual(new_mask_type, configs["eval_input_config"].mask_type)
 
+  def testUseMovingAverageForEval(self):
+    use_moving_averages_orig = False
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = use_moving_averages_orig
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, eval_with_moving_averages=True)
+    self.assertEqual(True, configs["eval_config"].use_moving_averages)
+
   def  test_get_image_resizer_config(self):
     """Tests that number of classes can be retrieved."""
     model_config = model_pb2.DetectionModel()
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index 628ce1b8..071dfcd8 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -117,13 +117,17 @@ def read_dataset(file_read_func, decode_func, input_files, config):
     A tf.data.Dataset based on config.
   """
   # Shard, shuffle, and read files.
-  filenames = tf.concat([tf.matching_files(pattern) for pattern in input_files],
-                        0)
-  filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)
+  filenames = tf.gfile.Glob(input_files)
+  num_readers = config.num_readers
+  if num_readers > len(filenames):
+    num_readers = len(filenames)
+    tf.logging.warning('num_readers has been reduced to %d to match input file '
+                       'shards.' % num_readers)
+  filename_dataset = tf.data.Dataset.from_tensor_slices(tf.unstack(filenames))
   if config.shuffle:
     filename_dataset = filename_dataset.shuffle(
         config.filenames_shuffle_buffer_size)
-  elif config.num_readers > 1:
+  elif num_readers > 1:
     tf.logging.warning('`shuffle` is false, but the input data stream is '
                        'still slightly shuffled since `num_readers` > 1.')
 
@@ -131,8 +135,10 @@ def read_dataset(file_read_func, decode_func, input_files, config):
 
   records_dataset = filename_dataset.apply(
       tf.contrib.data.parallel_interleave(
-          file_read_func, cycle_length=config.num_readers,
-          block_length=config.read_block_length, sloppy=config.shuffle))
+          file_read_func,
+          cycle_length=num_readers,
+          block_length=config.read_block_length,
+          sloppy=config.shuffle))
   if config.shuffle:
     records_dataset = records_dataset.shuffle(config.shuffle_buffer_size)
   tensor_dataset = records_dataset.map(
diff --git a/research/object_detection/utils/dataset_util_test.py b/research/object_detection/utils/dataset_util_test.py
index b0cd7a03..a74d4dca 100644
--- a/research/object_detection/utils/dataset_util_test.py
+++ b/research/object_detection/utils/dataset_util_test.py
@@ -16,6 +16,7 @@
 """Tests for object_detection.utils.dataset_util."""
 
 import os
+import numpy as np
 import tensorflow as tf
 
 from object_detection.protos import input_reader_pb2
@@ -32,6 +33,13 @@ class DatasetUtilTest(tf.test.TestCase):
       with tf.gfile.Open(path, 'wb') as f:
         f.write('\n'.join([str(i + 1), str((i + 1) * 10)]))
 
+    self._shuffle_path_template = os.path.join(self.get_temp_dir(),
+                                               'shuffle_%s.txt')
+    for i in range(2):
+      path = self._shuffle_path_template % i
+      with tf.gfile.Open(path, 'wb') as f:
+        f.write('\n'.join([str(i)] * 5))
+
   def _get_dataset_next(self, files, config, batch_size):
     def decode_func(value):
       return [tf.string_to_number(value, out_type=tf.int32)]
@@ -78,6 +86,43 @@ class DatasetUtilTest(tf.test.TestCase):
                           [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
                             30, 4, 40, 5, 50]])
 
+  def test_reduce_num_reader(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 10
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '*'], config,
+                                  batch_size=20)
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data),
+                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
+                            30, 4, 40, 5, 50]])
+
+  def test_enable_shuffle(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = True
+
+    data = self._get_dataset_next(
+        [self._shuffle_path_template % '*'], config, batch_size=10)
+    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
+
+    with self.test_session() as sess:
+      self.assertTrue(
+          np.any(np.not_equal(sess.run(data), expected_non_shuffle_output)))
+
+  def test_disable_shuffle_(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next(
+        [self._shuffle_path_template % '*'], config, batch_size=10)
+    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
+
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data), [expected_non_shuffle_output])
+
   def test_read_dataset_single_epoch(self):
     config = input_reader_pb2.InputReader()
     config.num_epochs = 1
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 785b5a2a..662f7e0d 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -318,8 +318,9 @@ def retain_groundtruth(tensor_dict, valid_indices):
   Args:
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
-      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_keypoints
+      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
       fields.InputDataFields.groundtruth_area
       fields.InputDataFields.groundtruth_label_types
@@ -347,6 +348,7 @@ def retain_groundtruth(tensor_dict, valid_indices):
     for key in tensor_dict:
       if key in [fields.InputDataFields.groundtruth_boxes,
                  fields.InputDataFields.groundtruth_classes,
+                 fields.InputDataFields.groundtruth_keypoints,
                  fields.InputDataFields.groundtruth_instance_masks]:
         valid_dict[key] = tf.gather(tensor_dict[key], valid_indices)
       # Input decoder returns empty tensor when these fields are not provided.
@@ -374,6 +376,8 @@ def retain_groundtruth_with_positive_classes(tensor_dict):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_keypoints
+      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
       fields.InputDataFields.groundtruth_area
       fields.InputDataFields.groundtruth_label_types
@@ -413,6 +417,8 @@ def filter_groundtruth_with_crowd_boxes(tensor_dict):
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_keypoints
+      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
       fields.InputDataFields.groundtruth_area
       fields.InputDataFields.groundtruth_label_types
@@ -435,8 +441,9 @@ def filter_groundtruth_with_nan_box_coordinates(tensor_dict):
   Args:
     tensor_dict: a dictionary of following groundtruth tensors -
       fields.InputDataFields.groundtruth_boxes
-      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_classes
+      fields.InputDataFields.groundtruth_keypoints
+      fields.InputDataFields.groundtruth_instance_masks
       fields.InputDataFields.groundtruth_is_crowd
       fields.InputDataFields.groundtruth_area
       fields.InputDataFields.groundtruth_label_types
@@ -703,23 +710,30 @@ def reframe_box_masks_to_image_masks(box_masks, boxes, image_height,
     A tf.float32 tensor of size [num_masks, image_height, image_width].
   """
   # TODO(rathodv): Make this a public function.
-  def transform_boxes_relative_to_boxes(boxes, reference_boxes):
-    boxes = tf.reshape(boxes, [-1, 2, 2])
-    min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)
-    max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)
-    transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)
-    return tf.reshape(transformed_boxes, [-1, 4])
-
-  box_masks = tf.expand_dims(box_masks, axis=3)
-  num_boxes = tf.shape(box_masks)[0]
-  unit_boxes = tf.concat(
-      [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)
-  reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)
-  image_masks = tf.image.crop_and_resize(image=box_masks,
-                                         boxes=reverse_boxes,
-                                         box_ind=tf.range(num_boxes),
-                                         crop_size=[image_height, image_width],
-                                         extrapolation_value=0.0)
+  def reframe_box_masks_to_image_masks_default():
+    """The default function when there are more than 0 box masks."""
+    def transform_boxes_relative_to_boxes(boxes, reference_boxes):
+      boxes = tf.reshape(boxes, [-1, 2, 2])
+      min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)
+      max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)
+      transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)
+      return tf.reshape(transformed_boxes, [-1, 4])
+
+    box_masks_expanded = tf.expand_dims(box_masks, axis=3)
+    num_boxes = tf.shape(box_masks_expanded)[0]
+    unit_boxes = tf.concat(
+        [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)
+    reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)
+    return tf.image.crop_and_resize(
+        image=box_masks_expanded,
+        boxes=reverse_boxes,
+        box_ind=tf.range(num_boxes),
+        crop_size=[image_height, image_width],
+        extrapolation_value=0.0)
+  image_masks = tf.cond(
+      tf.shape(box_masks)[0] > 0,
+      reframe_box_masks_to_image_masks_default,
+      lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32))
   return tf.squeeze(image_masks, axis=3)
 
 
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index a236bb40..e7865e27 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -1100,6 +1100,16 @@ class ReframeBoxMasksToImageMasksTest(tf.test.TestCase):
       np_image_masks = sess.run(image_masks)
       self.assertAllClose(np_image_masks, np_expected_image_masks)
 
+  def testZeroBoxMasks(self):
+    box_masks = tf.zeros([0, 3, 3], dtype=tf.float32)
+    boxes = tf.zeros([0, 4], dtype=tf.float32)
+    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                       image_height=4,
+                                                       image_width=4)
+    with self.test_session() as sess:
+      np_image_masks = sess.run(image_masks)
+      self.assertAllEqual(np_image_masks.shape, np.array([0, 4, 4]))
+
   def testMaskIsCenteredInImageWhenBoxIsCentered(self):
     box_masks = tf.constant([[[1, 1],
                               [1, 1]]], dtype=tf.float32)
diff --git a/research/object_detection/utils/per_image_vrd_evaluation.py b/research/object_detection/utils/per_image_vrd_evaluation.py
index fee65f12..41682101 100644
--- a/research/object_detection/utils/per_image_vrd_evaluation.py
+++ b/research/object_detection/utils/per_image_vrd_evaluation.py
@@ -67,16 +67,18 @@ class PerImageVRDEvaluation(object):
       tp_fp_labels: A single boolean numpy array of shape [N,], representing N
           True/False positive label, one label per tuple. The labels are sorted
           so that the order of the labels matches the order of the scores.
+      result_mapping: A numpy array with shape [N,] with original index of each
+          entry.
     """
 
-    scores, tp_fp_labels = self._compute_tp_fp(
+    scores, tp_fp_labels, result_mapping = self._compute_tp_fp(
         detected_box_tuples=detected_box_tuples,
         detected_scores=detected_scores,
         detected_class_tuples=detected_class_tuples,
         groundtruth_box_tuples=groundtruth_box_tuples,
         groundtruth_class_tuples=groundtruth_class_tuples)
 
-    return scores, tp_fp_labels
+    return scores, tp_fp_labels, result_mapping
 
   def _compute_tp_fp(self, detected_box_tuples, detected_scores,
                      detected_class_tuples, groundtruth_box_tuples,
@@ -107,33 +109,46 @@ class PerImageVRDEvaluation(object):
       tp_fp_labels: A single boolean numpy array of shape [N,], representing N
           True/False positive label, one label per tuple. The labels are sorted
           so that the order of the labels matches the order of the scores.
-
+      result_mapping: A numpy array with shape [N,] with original index of each
+          entry.
     """
     unique_gt_tuples = np.unique(
         np.concatenate((groundtruth_class_tuples, detected_class_tuples)))
     result_scores = []
     result_tp_fp_labels = []
+    result_mapping = []
 
     for unique_tuple in unique_gt_tuples:
       detections_selector = (detected_class_tuples == unique_tuple)
       gt_selector = (groundtruth_class_tuples == unique_tuple)
-      scores, tp_fp_labels = self._compute_tp_fp_for_single_class(
-          detected_box_tuples=detected_box_tuples[detections_selector],
-          detected_scores=detected_scores[detections_selector],
+
+      selector_mapping = np.where(detections_selector)[0]
+
+      detection_scores_per_tuple = detected_scores[detections_selector]
+      detection_box_per_tuple = detected_box_tuples[detections_selector]
+
+      sorted_indices = np.argsort(detection_scores_per_tuple)
+      sorted_indices = sorted_indices[::-1]
+
+      tp_fp_labels = self._compute_tp_fp_for_single_class(
+          detected_box_tuples=detection_box_per_tuple[sorted_indices],
           groundtruth_box_tuples=groundtruth_box_tuples[gt_selector])
-      result_scores.append(scores)
+      result_scores.append(detection_scores_per_tuple[sorted_indices])
       result_tp_fp_labels.append(tp_fp_labels)
+      result_mapping.append(selector_mapping[sorted_indices])
 
     result_scores = np.concatenate(result_scores)
     result_tp_fp_labels = np.concatenate(result_tp_fp_labels)
+    result_mapping = np.concatenate(result_mapping)
 
     sorted_indices = np.argsort(result_scores)
     sorted_indices = sorted_indices[::-1]
 
-    return result_scores[sorted_indices], result_tp_fp_labels[sorted_indices]
+    return result_scores[sorted_indices], result_tp_fp_labels[
+        sorted_indices], result_mapping[sorted_indices]
 
-  def _get_overlaps_and_scores_relation_tuples(
-      self, detected_box_tuples, detected_scores, groundtruth_box_tuples):
+  def _get_overlaps_and_scores_relation_tuples(self, detected_box_tuples,
+                                               groundtruth_box_tuples):
     """Computes overlaps and scores between detected and groundtruth tuples.
 
     Both detections and groundtruth boxes have the same class tuples.
@@ -143,8 +158,6 @@ class PerImageVRDEvaluation(object):
           representing N tuples, each tuple containing the same number of named
           bounding boxes.
           Each box is of the format [y_min, x_min, y_max, x_max]
-      detected_scores: A float numpy array of shape [N,], representing
-          the confidence scores of the detected N object instances.
       groundtruth_box_tuples: A float numpy array of structures with the shape
           [M,], representing M tuples, each tuple containing the same number
           of named bounding boxes.
@@ -153,7 +166,6 @@ class PerImageVRDEvaluation(object):
     Returns:
       result_iou: A float numpy array of size
         [num_detected_tuples, num_gt_box_tuples].
-      scores: The score of the detected boxlist.
     """
 
     result_iou = np.ones(
@@ -161,46 +173,35 @@ class PerImageVRDEvaluation(object):
         dtype=float)
     for field in detected_box_tuples.dtype.fields:
       detected_boxlist_field = np_box_list.BoxList(detected_box_tuples[field])
-      detected_boxlist_field.add_field('scores', detected_scores)
-      detected_boxlist_field = np_box_list_ops.sort_by_field(
-          detected_boxlist_field, 'scores')
       gt_boxlist_field = np_box_list.BoxList(groundtruth_box_tuples[field])
       iou_field = np_box_list_ops.iou(detected_boxlist_field, gt_boxlist_field)
       result_iou = np.minimum(iou_field, result_iou)
-    scores = detected_boxlist_field.get_field('scores')
-    return result_iou, scores
+    return result_iou
 
   def _compute_tp_fp_for_single_class(self, detected_box_tuples,
-                                      detected_scores, groundtruth_box_tuples):
+                                      groundtruth_box_tuples):
     """Labels boxes detected with the same class from the same image as tp/fp.
 
+    Detection boxes are expected to be already sorted by score.
     Args:
       detected_box_tuples: A numpy array of structures with shape [N,],
           representing N tuples, each tuple containing the same number of named
           bounding boxes.
           Each box is of the format [y_min, x_min, y_max, x_max]
-      detected_scores: A float numpy array of shape [N,], representing
-          the confidence scores of the detected N object instances.
       groundtruth_box_tuples: A float numpy array of structures with the shape
           [M,], representing M tuples, each tuple containing the same number
           of named bounding boxes.
           Each box is of the format [y_min, x_min, y_max, x_max]
 
     Returns:
-      Two arrays of the same size, containing true/false for N boxes that were
-      evaluated as being true positives or false positives;
-
-      scores: A numpy array representing the detection scores.
       tp_fp_labels: a boolean numpy array indicating whether a detection is a
           true positive.
     """
     if detected_box_tuples.size == 0:
-      return np.array([], dtype=float), np.array([], dtype=bool)
+      return np.array([], dtype=bool)
 
-    min_iou, scores = self._get_overlaps_and_scores_relation_tuples(
-        detected_box_tuples=detected_box_tuples,
-        detected_scores=detected_scores,
-        groundtruth_box_tuples=groundtruth_box_tuples)
+    min_iou = self._get_overlaps_and_scores_relation_tuples(
+        detected_box_tuples, groundtruth_box_tuples)
 
     num_detected_tuples = detected_box_tuples.shape[0]
     tp_fp_labels = np.zeros(num_detected_tuples, dtype=bool)
@@ -215,4 +216,4 @@ class PerImageVRDEvaluation(object):
             tp_fp_labels[i] = True
             is_gt_tuple_detected[gt_id] = True
 
-    return scores, tp_fp_labels
+    return tp_fp_labels
diff --git a/research/object_detection/utils/per_image_vrd_evaluation_test.py b/research/object_detection/utils/per_image_vrd_evaluation_test.py
index 28e2bdf9..b81c5631 100644
--- a/research/object_detection/utils/per_image_vrd_evaluation_test.py
+++ b/research/object_detection/utils/per_image_vrd_evaluation_test.py
@@ -28,31 +28,25 @@ class SingleClassPerImageVrdEvaluationTest(tf.test.TestCase):
     box_data_type = np.dtype([('subject', 'f4', (4,)), ('object', 'f4', (4,))])
 
     self.detected_box_tuples = np.array(
-        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1.1, 1], [1, 1, 2, 2]),
+        [([0, 0, 1.1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2]),
          ([1, 1, 2, 2], [0, 0, 1.1, 1])],
         dtype=box_data_type)
-    self.detected_scores = np.array([0.2, 0.8, 0.1], dtype=float)
+    self.detected_scores = np.array([0.8, 0.2, 0.1], dtype=float)
     self.groundtruth_box_tuples = np.array(
         [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=box_data_type)
 
   def test_tp_fp_eval(self):
-    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
-        self.detected_box_tuples, self.detected_scores,
-        self.groundtruth_box_tuples)
-    expected_scores = np.array([0.8, 0.2, 0.1], dtype=float)
+    tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_box_tuples, self.groundtruth_box_tuples)
     expected_tp_fp_labels = np.array([True, False, False], dtype=bool)
-    self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
   def test_tp_fp_eval_empty_gt(self):
     box_data_type = np.dtype([('subject', 'f4', (4,)), ('object', 'f4', (4,))])
 
-    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
-        self.detected_box_tuples, self.detected_scores,
-        np.array([], dtype=box_data_type))
-    expected_scores = np.array([0.8, 0.2, 0.1], dtype=float)
+    tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_box_tuples, np.array([], dtype=box_data_type))
     expected_tp_fp_labels = np.array([False, False, False], dtype=bool)
-    self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
 
@@ -82,16 +76,18 @@ class MultiClassPerImageVrdEvaluationTest(tf.test.TestCase):
         [(1, 2, 3), (1, 7, 3), (1, 4, 5)], dtype=label_data_type)
 
   def test_tp_fp_eval(self):
-    scores, tp_fp_labels = self.eval.compute_detection_tp_fp(
+    scores, tp_fp_labels, mapping = self.eval.compute_detection_tp_fp(
         self.detected_box_tuples, self.detected_scores,
         self.detected_class_tuples, self.groundtruth_box_tuples,
         self.groundtruth_class_tuples)
 
     expected_scores = np.array([0.8, 0.5, 0.2, 0.1], dtype=float)
     expected_tp_fp_labels = np.array([True, True, False, False], dtype=bool)
+    expected_mapping = np.array([1, 3, 0, 2])
 
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+    self.assertTrue(np.allclose(expected_mapping, mapping))
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/utils/test_utils.py b/research/object_detection/utils/test_utils.py
index 919bde96..2e50419c 100644
--- a/research/object_detection/utils/test_utils.py
+++ b/research/object_detection/utils/test_utils.py
@@ -138,3 +138,36 @@ def create_random_boxes(num_boxes, max_height, max_width):
   boxes[:, 3] = np.maximum(x_1, x_2)
 
   return boxes.astype(np.float32)
+
+
+def first_rows_close_as_set(a, b, k=None, rtol=1e-6, atol=1e-6):
+  """Checks if first K entries of two lists are close, up to permutation.
+
+  Inputs to this assert are lists of items which can be compared via
+  numpy.allclose(...) and can be sorted.
+
+  Args:
+    a: list of items which can be compared via numpy.allclose(...) and are
+      sortable.
+    b: list of items which can be compared via numpy.allclose(...) and are
+      sortable.
+    k: a non-negative integer.  If not provided, k is set to be len(a).
+    rtol: relative tolerance.
+    atol: absolute tolerance.
+
+  Returns:
+    boolean, True if input lists a and b have the same length and
+    the first k entries of the inputs satisfy numpy.allclose() after
+    sorting entries.
+  """
+  if not isinstance(a, list) or not isinstance(b, list) or len(a) != len(b):
+    return False
+  if not k:
+    k = len(a)
+  k = min(k, len(a))
+  a_sorted = sorted(a[:k])
+  b_sorted = sorted(b[:k])
+  return all([
+      np.allclose(entry_a, entry_b, rtol, atol)
+      for (entry_a, entry_b) in zip(a_sorted, b_sorted)
+  ])
diff --git a/research/object_detection/utils/test_utils_test.py b/research/object_detection/utils/test_utils_test.py
index 1a4799c6..ea173c7a 100644
--- a/research/object_detection/utils/test_utils_test.py
+++ b/research/object_detection/utils/test_utils_test.py
@@ -68,6 +68,22 @@ class TestUtilsTest(tf.test.TestCase):
     self.assertTrue(boxes[:, 2].max() <= max_height)
     self.assertTrue(boxes[:, 3].max() <= max_width)
 
+  def test_first_rows_close_as_set(self):
+    a = [1, 2, 3, 0, 0]
+    b = [3, 2, 1, 0, 0]
+    k = 3
+    self.assertTrue(test_utils.first_rows_close_as_set(a, b, k))
+
+    a = [[1, 2], [1, 4], [0, 0]]
+    b = [[1, 4 + 1e-9], [1, 2], [0, 0]]
+    k = 2
+    self.assertTrue(test_utils.first_rows_close_as_set(a, b, k))
+
+    a = [[1, 2], [1, 4], [0, 0]]
+    b = [[1, 4 + 1e-9], [2, 2], [0, 0]]
+    k = 2
+    self.assertFalse(test_utils.first_rows_close_as_set(a, b, k))
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 47a78672..79e18250 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -315,11 +315,13 @@ def draw_bounding_boxes_on_image_tensors(images,
                                          instance_masks=None,
                                          keypoints=None,
                                          max_boxes_to_draw=20,
-                                         min_score_thresh=0.2):
+                                         min_score_thresh=0.2,
+                                         use_normalized_coordinates=True):
   """Draws bounding boxes, masks, and keypoints on batch of image tensors.
 
   Args:
-    images: A 4D uint8 image tensor of shape [N, H, W, C].
+    images: A 4D uint8 image tensor of shape [N, H, W, C]. If C > 3, additional
+      channels will be ignored.
     boxes: [N, max_detections, 4] float32 tensor of detection boxes.
     classes: [N, max_detections] int tensor of detection classes. Note that
       classes are 1-indexed.
@@ -332,12 +334,17 @@ def draw_bounding_boxes_on_image_tensors(images,
       with keypoints.
     max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.
     min_score_thresh: Minimum score threshold for visualization. Default 0.2.
+    use_normalized_coordinates: Whether to assume boxes and kepoints are in
+      normalized coordinates (as opposed to absolute coordiantes).
+      Default is True.
 
   Returns:
     4D image tensor of type uint8, with boxes drawn on top.
   """
+  # Additional channels are being ignored.
+  images = images[:, :, :, 0:3]
   visualization_keyword_args = {
-      'use_normalized_coordinates': True,
+      'use_normalized_coordinates': use_normalized_coordinates,
       'max_boxes_to_draw': max_boxes_to_draw,
       'min_score_thresh': min_score_thresh,
       'agnostic_mode': False,
@@ -382,7 +389,8 @@ def draw_bounding_boxes_on_image_tensors(images,
 def draw_side_by_side_evaluation_image(eval_dict,
                                        category_index,
                                        max_boxes_to_draw=20,
-                                       min_score_thresh=0.2):
+                                       min_score_thresh=0.2,
+                                       use_normalized_coordinates=True):
   """Creates a side-by-side image with detections and groundtruth.
 
   Bounding boxes (and instance masks, if available) are visualized on both
@@ -394,6 +402,9 @@ def draw_side_by_side_evaluation_image(eval_dict,
     category_index: A category index (dictionary) produced from a labelmap.
     max_boxes_to_draw: The maximum number of boxes to draw for detections.
     min_score_thresh: The minimum score threshold for showing detections.
+    use_normalized_coordinates: Whether to assume boxes and kepoints are in
+      normalized coordinates (as opposed to absolute coordiantes).
+      Default is True.
 
   Returns:
     A [1, H, 2 * W, C] uint8 tensor. The subimage on the left corresponds to
@@ -425,7 +436,8 @@ def draw_side_by_side_evaluation_image(eval_dict,
       instance_masks=instance_masks,
       keypoints=keypoints,
       max_boxes_to_draw=max_boxes_to_draw,
-      min_score_thresh=min_score_thresh)
+      min_score_thresh=min_score_thresh,
+      use_normalized_coordinates=use_normalized_coordinates)
   images_with_groundtruth = draw_bounding_boxes_on_image_tensors(
       eval_dict[input_data_fields.original_image],
       tf.expand_dims(eval_dict[input_data_fields.groundtruth_boxes], axis=0),
@@ -439,7 +451,8 @@ def draw_side_by_side_evaluation_image(eval_dict,
       instance_masks=groundtruth_instance_masks,
       keypoints=None,
       max_boxes_to_draw=None,
-      min_score_thresh=0.0)
+      min_score_thresh=0.0,
+      use_normalized_coordinates=use_normalized_coordinates)
   return tf.concat([images_with_detections, images_with_groundtruth], axis=2)
 
 
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index b0f6b581..87bbacad 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -48,6 +48,9 @@ class VisualizationUtilsTest(tf.test.TestCase):
     image = np.concatenate((imu, imd), axis=0)
     return image
 
+  def create_test_image_with_five_channels(self):
+    return np.full([100, 200, 5], 255, dtype=np.uint8)
+
   def test_draw_bounding_box_on_image(self):
     test_image = self.create_colorful_test_image()
     test_image = Image.fromarray(test_image)
@@ -144,6 +147,32 @@ class VisualizationUtilsTest(tf.test.TestCase):
           image_pil = Image.fromarray(images_with_boxes_np[i, ...])
           image_pil.save(output_file)
 
+  def test_draw_bounding_boxes_on_image_tensors_with_additional_channels(self):
+    """Tests the case where input image tensor has more than 3 channels."""
+    category_index = {1: {'id': 1, 'name': 'dog'}}
+    image_np = self.create_test_image_with_five_channels()
+    images_np = np.stack((image_np, image_np), axis=0)
+
+    with tf.Graph().as_default():
+      images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
+      boxes = tf.constant(0, dtype=tf.float32, shape=[2, 0, 4])
+      classes = tf.constant(0, dtype=tf.int64, shape=[2, 0])
+      scores = tf.constant(0, dtype=tf.float32, shape=[2, 0])
+      images_with_boxes = (
+          visualization_utils.draw_bounding_boxes_on_image_tensors(
+              images_tensor,
+              boxes,
+              classes,
+              scores,
+              category_index,
+              min_score_thresh=0.2))
+
+      with self.test_session() as sess:
+        sess.run(tf.global_variables_initializer())
+
+        final_images_np = sess.run(images_with_boxes)
+        self.assertEqual((2, 100, 200, 3), final_images_np.shape)
+
   def test_draw_keypoints_on_image(self):
     test_image = self.create_colorful_test_image()
     test_image = Image.fromarray(test_image)
diff --git a/research/object_detection/utils/vrd_evaluation.py b/research/object_detection/utils/vrd_evaluation.py
new file mode 100644
index 00000000..06931101
--- /dev/null
+++ b/research/object_detection/utils/vrd_evaluation.py
@@ -0,0 +1,572 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Evaluator class for Visual Relations Detection.
+
+VRDDetectionEvaluator is a class which manages ground truth information of a
+visual relations detection (vrd) dataset, and computes frequently used detection
+metrics such as Precision, Recall, Recall@k, of the provided vrd detection
+results.
+It supports the following operations:
+1) Adding ground truth information of images sequentially.
+2) Adding detection results of images sequentially.
+3) Evaluating detection metrics on already inserted detection results.
+
+Note1: groundtruth should be inserted before evaluation.
+Note2: This module operates on numpy boxes and box lists.
+"""
+
+from abc import abstractmethod
+import collections
+import logging
+import numpy as np
+
+from object_detection.core import standard_fields
+from object_detection.utils import metrics
+from object_detection.utils import object_detection_evaluation
+from object_detection.utils import per_image_vrd_evaluation
+
+# Below standard input numpy datatypes are defined:
+# box_data_type - datatype of the groundtruth visual relations box annotations;
+# this datatype consists of two named boxes: subject bounding box and object
+# bounding box. Each box is of the format [y_min, x_min, y_max, x_max], each
+# coordinate being of type float32.
+# label_data_type - corresponding datatype of the visual relations label
+# annotaions; it consists of three numerical class labels: subject class label,
+# object class label and relation class label, each class label being of type
+# int32.
+vrd_box_data_type = np.dtype([('subject', 'f4', (4,)), ('object', 'f4', (4,))])
+single_box_data_type = np.dtype([('box', 'f4', (4,))])
+label_data_type = np.dtype([('subject', 'i4'), ('object', 'i4'), ('relation',
+                                                                  'i4')])
+
+
+class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
+  """A class to evaluate VRD detections.
+
+  This class serves as a base class for VRD evaluation in two settings:
+  - phrase detection
+  - relation detection.
+  """
+
+  def __init__(self, matching_iou_threshold=0.5, metric_prefix=None):
+    """Constructor.
+
+    Args:
+      matching_iou_threshold: IOU threshold to use for matching groundtruth
+        boxes to detection boxes.
+      metric_prefix: (optional) string prefix for metric name; if None, no
+        prefix is used.
+
+    """
+    super(VRDDetectionEvaluator, self).__init__([])
+    self._matching_iou_threshold = matching_iou_threshold
+    self._evaluation = _VRDDetectionEvaluation(
+        matching_iou_threshold=self._matching_iou_threshold)
+    self._image_ids = set([])
+    self._metric_prefix = (metric_prefix + '_') if metric_prefix else ''
+    self._evaluatable_labels = {}
+    self._negative_labels = {}
+
+  @abstractmethod
+  def _process_groundtruth_boxes(self, groundtruth_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    Phrase detection and Relation detection subclasses re-implement this method
+    depending on the task.
+
+    Args:
+      groundtruth_box_tuples:  A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max] (see
+        datatype vrd_box_data_type, single_box_data_type above).
+    """
+    raise NotImplementedError(
+        '_process_groundtruth_boxes method should be implemented in subclasses'
+        'of VRDDetectionEvaluator.')
+
+  @abstractmethod
+  def _process_detection_boxes(self, detections_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    Phrase detection and Relation detection subclasses re-implement this method
+    depending on the task.
+
+    Args:
+      detections_box_tuples:  A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max] (see
+        datatype vrd_box_data_type, single_box_data_type above).
+    """
+    raise NotImplementedError(
+        '_process_detection_boxes method should be implemented in subclasses'
+        'of VRDDetectionEvaluator.')
+
+  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
+    """Adds groundtruth for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      groundtruth_dict: A dictionary containing -
+        standard_fields.InputDataFields.groundtruth_boxes: A numpy array
+          of structures with the shape [M, 1], representing M tuples, each tuple
+          containing the same number of named bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max] (see
+          datatype vrd_box_data_type, single_box_data_type above).
+        standard_fields.InputDataFields.groundtruth_classes: A numpy array of
+          structures shape [M, 1], representing  the class labels of the
+          corresponding bounding boxes and possibly additional classes (see
+          datatype label_data_type above).
+        standard_fields.InputDataFields.verified_labels: numpy array
+          of shape [K] containing verified labels.
+    Raises:
+      ValueError: On adding groundtruth for an image more than once.
+    """
+    if image_id in self._image_ids:
+      raise ValueError('Image with id {} already added.'.format(image_id))
+
+    groundtruth_class_tuples = (
+        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes])
+    groundtruth_box_tuples = (
+        groundtruth_dict[standard_fields.InputDataFields.groundtruth_boxes])
+
+    self._evaluation.add_single_ground_truth_image_info(
+        image_key=image_id,
+        groundtruth_box_tuples=self._process_groundtruth_boxes(
+            groundtruth_box_tuples),
+        groundtruth_class_tuples=groundtruth_class_tuples)
+    self._image_ids.update([image_id])
+    all_classes = []
+    for field in groundtruth_box_tuples.dtype.fields:
+      all_classes.append(groundtruth_class_tuples[field])
+    groudtruth_positive_classes = np.unique(np.concatenate(all_classes))
+    verified_labels = groundtruth_dict.get(
+        standard_fields.InputDataFields.verified_labels, np.array(
+            [], dtype=int))
+    self._evaluatable_labels[image_id] = np.unique(
+        np.concatenate((verified_labels, groudtruth_positive_classes)))
+
+    self._negative_labels[image_id] = np.setdiff1d(verified_labels,
+                                                   groudtruth_positive_classes)
+
+  def add_single_detected_image_info(self, image_id, detections_dict):
+    """Adds detections for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      detections_dict: A dictionary containing -
+        standard_fields.DetectionResultFields.detection_boxes: A numpy array of
+          structures with shape [N, 1], representing N tuples, each tuple
+          containing the same number of named bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max] (as an example
+          see datatype vrd_box_data_type, single_box_data_type above).
+        standard_fields.DetectionResultFields.detection_scores: float32 numpy
+          array of shape [N] containing detection scores for the boxes.
+        standard_fields.DetectionResultFields.detection_classes: A numpy array
+          of structures shape [N, 1], representing the class labels of the
+          corresponding bounding boxes and possibly additional classes (see
+          datatype label_data_type above).
+    """
+    num_detections = detections_dict[
+        standard_fields.DetectionResultFields.detection_boxes].shape[0]
+    detection_class_tuples = detections_dict[
+        standard_fields.DetectionResultFields.detection_classes]
+    detection_box_tuples = detections_dict[
+        standard_fields.DetectionResultFields.detection_boxes]
+    selector = np.ones(num_detections, dtype=bool)
+
+    # Only check boxable labels
+    for field in detection_box_tuples.dtype.fields:
+      # Verify if one of the labels is negative (this is sure FP)
+      selector |= np.isin(detection_class_tuples[field],
+                          self._negative_labels[image_id])
+      # Verify if all labels are verified
+      selector |= np.isin(detection_class_tuples[field],
+                          self._evaluatable_labels[image_id])
+
+    self._evaluation.add_single_detected_image_info(
+        image_key=image_id,
+        detected_box_tuples=self._process_detection_boxes(
+            detection_box_tuples[selector]),
+        detected_scores=detections_dict[
+            standard_fields.DetectionResultFields.detection_scores][selector],
+        detected_class_tuples=detection_class_tuples[selector])
+
+  def evaluate(self, relationships=None):
+    """Compute evaluation result.
+
+    Args:
+      relationships: A dictionary of numerical label-text label mapping; if
+        specified, returns per-relationship AP.
+
+    Returns:
+      A dictionary of metrics with the following fields -
+
+      summary_metrics:
+        'weightedAP@<matching_iou_threshold>IOU' : weighted average precision
+        at the specified IOU threshold.
+        'AP@<matching_iou_threshold>IOU/<relationship>' : AP per relationship.
+        'mAP@<matching_iou_threshold>IOU': mean average precision at the
+        specified IOU threshold.
+        'Recall@50@<matching_iou_threshold>IOU': recall@50 at the specified IOU
+        threshold.
+        'Recall@100@<matching_iou_threshold>IOU': recall@100 at the specified
+        IOU threshold.
+      if relationships is specified, returns <relationship> in AP metrics as
+      readable names, otherwise the names correspond to class numbers.
+    """
+    (weighted_average_precision, mean_average_precision, average_precisions, _,
+     _, recall_50, recall_100, _, _) = (
+         self._evaluation.evaluate())
+
+    vrd_metrics = {
+        (self._metric_prefix + 'weightedAP@{}IOU'.format(
+            self._matching_iou_threshold)):
+            weighted_average_precision,
+        self._metric_prefix + 'mAP@{}IOU'.format(self._matching_iou_threshold):
+            mean_average_precision,
+        self._metric_prefix + 'Recall@50@{}IOU'.format(
+            self._matching_iou_threshold):
+            recall_50,
+        self._metric_prefix + 'Recall@100@{}IOU'.format(
+            self._matching_iou_threshold):
+            recall_100,
+    }
+    if relationships:
+      for key, average_precision in average_precisions.iteritems():
+        vrd_metrics[self._metric_prefix + 'AP@{}IOU/{}'.format(
+            self._matching_iou_threshold,
+            relationships[key])] = average_precision
+    else:
+      for key, average_precision in average_precisions.iteritems():
+        vrd_metrics[self._metric_prefix + 'AP@{}IOU/{}'.format(
+            self._matching_iou_threshold, key)] = average_precision
+
+    return vrd_metrics
+
+  def clear(self):
+    """Clears the state to prepare for a fresh evaluation."""
+    self._evaluation = _VRDDetectionEvaluation(
+        matching_iou_threshold=self._matching_iou_threshold)
+    self._image_ids.clear()
+    self._negative_labels.clear()
+    self._evaluatable_labels.clear()
+
+
+class VRDRelationDetectionEvaluator(VRDDetectionEvaluator):
+  """A class to evaluate VRD detections in relations setting.
+
+  Expected groundtruth box datatype is vrd_box_data_type, expected groudtruth
+  labels datatype is label_data_type.
+  Expected detection box datatype is vrd_box_data_type, expected detection
+  labels
+  datatype is label_data_type.
+  """
+
+  def __init__(self, matching_iou_threshold=0.5):
+    super(VRDRelationDetectionEvaluator, self).__init__(
+        matching_iou_threshold=matching_iou_threshold,
+        metric_prefix='VRDMetric_Relationships')
+
+  def _process_groundtruth_boxes(self, groundtruth_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    Args:
+      groundtruth_box_tuples: A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max].
+
+    Returns:
+      Unchanged input.
+    """
+
+    return groundtruth_box_tuples
+
+  def _process_detection_boxes(self, detections_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    Phrase detection and Relation detection subclasses re-implement this method
+    depending on the task.
+
+    Args:
+      detections_box_tuples:  A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max] (see
+        datatype vrd_box_data_type, single_box_data_type above).
+    Returns:
+      Unchanged input.
+    """
+    return detections_box_tuples
+
+
+class VRDPhraseDetectionEvaluator(VRDDetectionEvaluator):
+  """A class to evaluate VRD detections in phrase setting.
+
+  Expected groundtruth box datatype is vrd_box_data_type, expected groudtruth
+  labels datatype is label_data_type.
+  Expected detection box datatype is single_box_data_type, expected detection
+  labels datatype is label_data_type.
+  """
+
+  def __init__(self, matching_iou_threshold=0.5):
+    super(VRDPhraseDetectionEvaluator, self).__init__(
+        matching_iou_threshold=matching_iou_threshold,
+        metric_prefix='VRDMetric_Phrases')
+
+  def _process_groundtruth_boxes(self, groundtruth_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    In case of phrase evaluation task, evaluation expects exactly one bounding
+    box containing all objects in the phrase. This bounding box is computed
+    as an enclosing box of all groundtruth boxes of a phrase.
+
+    Args:
+      groundtruth_box_tuples: A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max]. See
+        vrd_box_data_type for an example of structure.
+
+    Returns:
+      result: A numpy array of structures with the shape [M, 1], each
+        structure containing exactly one named bounding box. i-th output
+        structure corresponds to the result of processing i-th input structure,
+        where the named bounding box is computed as an enclosing bounding box
+        of all bounding boxes of the i-th input structure.
+    """
+    first_box_key = groundtruth_box_tuples.dtype.fields.keys()[0]
+    miny = groundtruth_box_tuples[first_box_key][:, 0]
+    minx = groundtruth_box_tuples[first_box_key][:, 1]
+    maxy = groundtruth_box_tuples[first_box_key][:, 2]
+    maxx = groundtruth_box_tuples[first_box_key][:, 3]
+    for fields in groundtruth_box_tuples.dtype.fields:
+      miny = np.minimum(groundtruth_box_tuples[fields][:, 0], miny)
+      minx = np.minimum(groundtruth_box_tuples[fields][:, 1], minx)
+      maxy = np.maximum(groundtruth_box_tuples[fields][:, 2], maxy)
+      maxx = np.maximum(groundtruth_box_tuples[fields][:, 3], maxx)
+    data_result = []
+    for i in range(groundtruth_box_tuples.shape[0]):
+      data_result.append(([miny[i], minx[i], maxy[i], maxx[i]],))
+    result = np.array(data_result, dtype=[('box', 'f4', (4,))])
+    return result
+
+  def _process_detection_boxes(self, detections_box_tuples):
+    """Pre-processes boxes before adding them to the VRDDetectionEvaluation.
+
+    In case of phrase evaluation task, evaluation expects exactly one bounding
+    box containing all objects in the phrase. This bounding box is computed
+    as an enclosing box of all groundtruth boxes of a phrase.
+
+    Args:
+      detections_box_tuples: A numpy array of structures with the shape
+        [M, 1], each structure containing the same number of named bounding
+        boxes. Each box is of the format [y_min, x_min, y_max, x_max]. See
+        vrd_box_data_type for an example of this structure.
+
+    Returns:
+      result: A numpy array of structures with the shape [M, 1], each
+        structure containing exactly one named bounding box. i-th output
+        structure corresponds to the result of processing i-th input structure,
+        where the named bounding box is computed as an enclosing bounding box
+        of all bounding boxes of the i-th input structure.
+    """
+    first_box_key = detections_box_tuples.dtype.fields.keys()[0]
+    miny = detections_box_tuples[first_box_key][:, 0]
+    minx = detections_box_tuples[first_box_key][:, 1]
+    maxy = detections_box_tuples[first_box_key][:, 2]
+    maxx = detections_box_tuples[first_box_key][:, 3]
+    for fields in detections_box_tuples.dtype.fields:
+      miny = np.minimum(detections_box_tuples[fields][:, 0], miny)
+      minx = np.minimum(detections_box_tuples[fields][:, 1], minx)
+      maxy = np.maximum(detections_box_tuples[fields][:, 2], maxy)
+      maxx = np.maximum(detections_box_tuples[fields][:, 3], maxx)
+    data_result = []
+    for i in range(detections_box_tuples.shape[0]):
+      data_result.append(([miny[i], minx[i], maxy[i], maxx[i]],))
+    result = np.array(data_result, dtype=[('box', 'f4', (4,))])
+    return result
+
+
+VRDDetectionEvalMetrics = collections.namedtuple('VRDDetectionEvalMetrics', [
+    'weighted_average_precision', 'mean_average_precision',
+    'average_precisions', 'precisions', 'recalls', 'recall_50', 'recall_100',
+    'median_rank_50', 'median_rank_100'
+])
+
+
+class _VRDDetectionEvaluation(object):
+  """Performs metric computation for the VRD task. This class is internal.
+  """
+
+  def __init__(self, matching_iou_threshold=0.5):
+    """Constructor.
+
+    Args:
+      matching_iou_threshold: IOU threshold to use for matching groundtruth
+        boxes to detection boxes.
+    """
+    self._per_image_eval = per_image_vrd_evaluation.PerImageVRDEvaluation(
+        matching_iou_threshold=matching_iou_threshold)
+
+    self._groundtruth_box_tuples = {}
+    self._groundtruth_class_tuples = {}
+    self._num_gt_instances = 0
+    self._num_gt_imgs = 0
+    self._num_gt_instances_per_relationship = {}
+
+    self.clear_detections()
+
+  def clear_detections(self):
+    """Clears detections."""
+    self._detection_keys = set()
+    self._scores = []
+    self._relation_field_values = []
+    self._tp_fp_labels = []
+    self._average_precisions = {}
+    self._precisions = []
+    self._recalls = []
+
+  def add_single_ground_truth_image_info(
+      self, image_key, groundtruth_box_tuples, groundtruth_class_tuples):
+    """Adds groundtruth for a single image to be used for evaluation.
+
+    Args:
+      image_key: A unique string/integer identifier for the image.
+      groundtruth_box_tuples: A numpy array of structures with the shape
+          [M, 1], representing M tuples, each tuple containing the same number
+          of named bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max].
+      groundtruth_class_tuples: A numpy array of structures shape [M, 1],
+          representing  the class labels of the corresponding bounding boxes and
+          possibly additional classes.
+    """
+    if image_key in self._groundtruth_box_tuples:
+      logging.warn(
+          'image %s has already been added to the ground truth database.',
+          image_key)
+      return
+
+    self._groundtruth_box_tuples[image_key] = groundtruth_box_tuples
+    self._groundtruth_class_tuples[image_key] = groundtruth_class_tuples
+
+    self._update_groundtruth_statistics(groundtruth_class_tuples)
+
+  def add_single_detected_image_info(self, image_key, detected_box_tuples,
+                                     detected_scores, detected_class_tuples):
+    """Adds detections for a single image to be used for evaluation.
+
+    Args:
+      image_key: A unique string/integer identifier for the image.
+      detected_box_tuples: A numpy array of structures with shape [N, 1],
+          representing N tuples, each tuple containing the same number of named
+          bounding boxes.
+          Each box is of the format [y_min, x_min, y_max, x_max].
+      detected_scores: A float numpy array of shape [N, 1], representing
+          the confidence scores of the detected N object instances.
+      detected_class_tuples: A numpy array of structures shape [N, 1],
+          representing the class labels of the corresponding bounding boxes and
+          possibly additional classes.
+    """
+    self._detection_keys.add(image_key)
+    if image_key in self._groundtruth_box_tuples:
+      groundtruth_box_tuples = self._groundtruth_box_tuples[image_key]
+      groundtruth_class_tuples = self._groundtruth_class_tuples[image_key]
+    else:
+      groundtruth_box_tuples = np.empty(shape=[0, 4], dtype=float)
+      groundtruth_class_tuples = np.array([], dtype=int)
+
+    scores, tp_fp_labels, mapping = (
+        self._per_image_eval.compute_detection_tp_fp(
+            detected_box_tuples=detected_box_tuples,
+            detected_scores=detected_scores,
+            detected_class_tuples=detected_class_tuples,
+            groundtruth_box_tuples=groundtruth_box_tuples,
+            groundtruth_class_tuples=groundtruth_class_tuples))
+
+    self._scores += [scores]
+    self._tp_fp_labels += [tp_fp_labels]
+    self._relation_field_values += [detected_class_tuples[mapping]['relation']]
+
+  def _update_groundtruth_statistics(self, groundtruth_class_tuples):
+    """Updates grouth truth statistics.
+
+    Args:
+      groundtruth_class_tuples: A numpy array of structures shape [M, 1],
+          representing  the class labels of the corresponding bounding boxes and
+          possibly additional classes.
+    """
+    self._num_gt_instances += groundtruth_class_tuples.shape[0]
+    self._num_gt_imgs += 1
+    for relation_field_value in np.unique(groundtruth_class_tuples['relation']):
+      if relation_field_value not in self._num_gt_instances_per_relationship:
+        self._num_gt_instances_per_relationship[relation_field_value] = 0
+      self._num_gt_instances_per_relationship[relation_field_value] += np.sum(
+          groundtruth_class_tuples['relation'] == relation_field_value)
+
+  def evaluate(self):
+    """Computes evaluation result.
+
+    Returns:
+      A named tuple with the following fields -
+        average_precision: a float number corresponding to average precision.
+        precisions: an array of precisions.
+        recalls: an array of recalls.
+        recall@50: recall computed on 50 top-scoring samples.
+        recall@100: recall computed on 100 top-scoring samples.
+        median_rank@50: median rank computed on 50 top-scoring samples.
+        median_rank@100: median rank computed on 100 top-scoring samples.
+    """
+    if self._num_gt_instances == 0:
+      logging.warn('No ground truth instances')
+
+    if not self._scores:
+      scores = np.array([], dtype=float)
+      tp_fp_labels = np.array([], dtype=bool)
+    else:
+      scores = np.concatenate(self._scores)
+      tp_fp_labels = np.concatenate(self._tp_fp_labels)
+      relation_field_values = np.concatenate(self._relation_field_values)
+
+    for relation_field_value, _ in (
+        self._num_gt_instances_per_relationship.iteritems()):
+      precisions, recalls = metrics.compute_precision_recall(
+          scores[relation_field_values == relation_field_value],
+          tp_fp_labels[relation_field_values == relation_field_value],
+          self._num_gt_instances_per_relationship[relation_field_value])
+      self._average_precisions[
+          relation_field_value] = metrics.compute_average_precision(
+              precisions, recalls)
+
+    self._mean_average_precision = np.mean(self._average_precisions.values())
+
+    self._precisions, self._recalls = metrics.compute_precision_recall(
+        scores, tp_fp_labels, self._num_gt_instances)
+    self._weighted_average_precision = metrics.compute_average_precision(
+        self._precisions, self._recalls)
+
+    self._recall_50 = (
+        metrics.compute_recall_at_k(self._tp_fp_labels, self._num_gt_instances,
+                                    50))
+    self._median_rank_50 = (
+        metrics.compute_median_rank_at_k(self._tp_fp_labels, 50))
+    self._recall_100 = (
+        metrics.compute_recall_at_k(self._tp_fp_labels, self._num_gt_instances,
+                                    100))
+    self._median_rank_100 = (
+        metrics.compute_median_rank_at_k(self._tp_fp_labels, 100))
+
+    return VRDDetectionEvalMetrics(
+        self._weighted_average_precision, self._mean_average_precision,
+        self._average_precisions, self._precisions, self._recalls,
+        self._recall_50, self._recall_100, self._median_rank_50,
+        self._median_rank_100)
diff --git a/research/object_detection/utils/vrd_evaluation_test.py b/research/object_detection/utils/vrd_evaluation_test.py
new file mode 100644
index 00000000..52ce9683
--- /dev/null
+++ b/research/object_detection/utils/vrd_evaluation_test.py
@@ -0,0 +1,255 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for tensorflow_models.object_detection.utils.vrd_evaluation."""
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.core import standard_fields
+from object_detection.utils import vrd_evaluation
+
+
+class VRDRelationDetectionEvaluatorTest(tf.test.TestCase):
+
+  def test_vrdrelation_evaluator(self):
+    self.vrd_eval = vrd_evaluation.VRDRelationDetectionEvaluator()
+
+    image_key1 = 'img1'
+    groundtruth_box_tuples1 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples1 = np.array(
+        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    groundtruth_verified_labels1 = np.array([1, 2, 3, 4, 5], dtype=int)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key1, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples1,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples1,
+            standard_fields.InputDataFields.verified_labels:
+                groundtruth_verified_labels1
+        })
+
+    image_key2 = 'img2'
+    groundtruth_box_tuples2 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples2 = np.array(
+        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key2, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples2,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples2,
+        })
+
+    image_key3 = 'img3'
+    groundtruth_box_tuples3 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples3 = np.array(
+        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key3, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples3,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples3,
+        })
+
+    image_key = 'img1'
+    detected_box_tuples = np.array(
+        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    detected_class_tuples = np.array(
+        [(1, 2, 5), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)
+    detected_scores = np.array([0.7, 0.8], dtype=float)
+    self.vrd_eval.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_box_tuples,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_tuples
+        })
+
+    metrics = self.vrd_eval.evaluate()
+
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_weightedAP@0.5IOU'],
+                           0.25)
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_mAP@0.5IOU'],
+                           0.1666666666666666)
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_AP@0.5IOU/3'],
+                           0.3333333333333333)
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_AP@0.5IOU/4'], 0)
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_Recall@50@0.5IOU'],
+                           0.25)
+    self.assertAlmostEqual(metrics['VRDMetric_Relationships_Recall@100@0.5IOU'],
+                           0.25)
+    self.vrd_eval.clear()
+    self.assertFalse(self.vrd_eval._image_ids)
+
+
+class VRDPhraseDetectionEvaluatorTest(tf.test.TestCase):
+
+  def test_vrdphrase_evaluator(self):
+    self.vrd_eval = vrd_evaluation.VRDPhraseDetectionEvaluator()
+
+    image_key1 = 'img1'
+    groundtruth_box_tuples1 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples1 = np.array(
+        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    groundtruth_verified_labels1 = np.array([1, 2, 3, 4, 5], dtype=int)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key1, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples1,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples1,
+            standard_fields.InputDataFields.verified_labels:
+                groundtruth_verified_labels1
+        })
+
+    image_key2 = 'img2'
+    groundtruth_box_tuples2 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples2 = np.array(
+        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key2, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples2,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples2,
+        })
+
+    image_key3 = 'img3'
+    groundtruth_box_tuples3 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples3 = np.array(
+        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key3, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_box_tuples3,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_tuples3,
+        })
+
+    image_key = 'img1'
+    detected_box_tuples = np.array(
+        [([0, 0.3, 0.5, 0.5], [0.3, 0.3, 1.0, 1.0]),
+         ([0, 0, 1.2, 1.2], [0.0, 0.0, 2.0, 2.0])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    detected_class_tuples = np.array(
+        [(1, 2, 5), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)
+    detected_scores = np.array([0.7, 0.8], dtype=float)
+    self.vrd_eval.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_box_tuples,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_tuples
+        })
+
+    metrics = self.vrd_eval.evaluate()
+
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_weightedAP@0.5IOU'], 0.25)
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_mAP@0.5IOU'],
+                           0.1666666666666666)
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_AP@0.5IOU/3'],
+                           0.3333333333333333)
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_AP@0.5IOU/4'], 0)
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_Recall@50@0.5IOU'], 0.25)
+    self.assertAlmostEqual(metrics['VRDMetric_Phrases_Recall@100@0.5IOU'], 0.25)
+
+    self.vrd_eval.clear()
+    self.assertFalse(self.vrd_eval._image_ids)
+
+
+class VRDDetectionEvaluationTest(tf.test.TestCase):
+
+  def setUp(self):
+
+    self.vrd_eval = vrd_evaluation._VRDDetectionEvaluation(
+        matching_iou_threshold=0.5)
+
+    image_key1 = 'img1'
+    groundtruth_box_tuples1 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples1 = np.array(
+        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key1, groundtruth_box_tuples1, groundtruth_class_tuples1)
+
+    image_key2 = 'img2'
+    groundtruth_box_tuples2 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples2 = np.array(
+        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key2, groundtruth_box_tuples2, groundtruth_class_tuples2)
+
+    image_key3 = 'img3'
+    groundtruth_box_tuples3 = np.array(
+        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)
+    groundtruth_class_tuples3 = np.array(
+        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)
+    self.vrd_eval.add_single_ground_truth_image_info(
+        image_key3, groundtruth_box_tuples3, groundtruth_class_tuples3)
+
+    image_key = 'img1'
+    detected_box_tuples = np.array(
+        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2])],
+        dtype=vrd_evaluation.vrd_box_data_type)
+    detected_class_tuples = np.array(
+        [(1, 2, 3), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)
+    detected_scores = np.array([0.7, 0.8], dtype=float)
+    self.vrd_eval.add_single_detected_image_info(
+        image_key, detected_box_tuples, detected_scores, detected_class_tuples)
+
+    metrics = self.vrd_eval.evaluate()
+    expected_weighted_average_precision = 0.25
+    expected_mean_average_precision = 0.16666666666666
+    expected_precision = np.array([1., 0.5], dtype=float)
+    expected_recall = np.array([0.25, 0.25], dtype=float)
+    expected_recall_50 = 0.25
+    expected_recall_100 = 0.25
+    expected_median_rank_50 = 0
+    expected_median_rank_100 = 0
+
+    self.assertAlmostEqual(expected_weighted_average_precision,
+                           metrics.weighted_average_precision)
+    self.assertAlmostEqual(expected_mean_average_precision,
+                           metrics.mean_average_precision)
+    self.assertAlmostEqual(expected_mean_average_precision,
+                           metrics.mean_average_precision)
+
+    self.assertAllClose(expected_precision, metrics.precisions)
+    self.assertAllClose(expected_recall, metrics.recalls)
+    self.assertAlmostEqual(expected_recall_50, metrics.recall_50)
+    self.assertAlmostEqual(expected_recall_100, metrics.recall_100)
+    self.assertAlmostEqual(expected_median_rank_50, metrics.median_rank_50)
+    self.assertAlmostEqual(expected_median_rank_100, metrics.median_rank_100)
+
+
+if __name__ == '__main__':
+  tf.test.main()
