commit d10d0027b76cdc3d50e2101200622f6f67601881
Author: Pengchong Jin <pengchong@google.com>
Date:   Thu Apr 9 18:59:29 2020 -0700

    Save the checkpoint at step 0 and run evaluation.
    
    PiperOrigin-RevId: 305806096

diff --git a/official/modeling/training/distributed_executor.py b/official/modeling/training/distributed_executor.py
index d4dfd259..e44178c8 100644
--- a/official/modeling/training/distributed_executor.py
+++ b/official/modeling/training/distributed_executor.py
@@ -421,6 +421,19 @@ class DistributedExecutor(object):
       self.global_train_step = model.optimizer.iterations
       test_step = self._create_test_step(strategy, model, metric=eval_metric)
 
+    # Step-0 operations
+    _save_checkpoint(
+        checkpoint, model_dir, checkpoint_name.format(step=current_step))
+    if test_step:
+      eval_iterator = self._get_input_iterator(eval_input_fn, strategy)
+      eval_metric_result = self._run_evaluation(
+          test_step, current_step, eval_metric, eval_iterator)
+      logging.info(
+          'Step: %s evalation metric = %s.', current_step, eval_metric_result)
+      test_summary_writer(
+          metrics=eval_metric_result, step=optimizer.iterations)
+      eval_metric.reset_states()
+
     logging.info('Training started')
     last_save_checkpoint_step = current_step
     while current_step < total_steps:
