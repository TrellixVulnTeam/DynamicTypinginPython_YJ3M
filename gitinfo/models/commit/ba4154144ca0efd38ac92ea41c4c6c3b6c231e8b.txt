commit ba4154144ca0efd38ac92ea41c4c6c3b6c231e8b
Author: Haoyu Zhang <haoyuzhang@google.com>
Date:   Fri May 31 12:47:17 2019 -0700

    Fix various lint errors (#6934)
    
    * Fix various lint errors
    
    * Fix logging format

diff --git a/official/recommendation/ncf_common.py b/official/recommendation/ncf_common.py
index 355b5afd..f04e4a8b 100644
--- a/official/recommendation/ncf_common.py
+++ b/official/recommendation/ncf_common.py
@@ -143,7 +143,7 @@ def get_distribution_strategy(params):
         "coordinator": tpu_cluster_resolver.cluster_spec()
                        .as_dict()["coordinator"]
     }
-    os.environ['TF_CONFIG'] = json.dumps(tf_config_env)
+    os.environ["TF_CONFIG"] = json.dumps(tf_config_env)
 
     distribution = tf.distribute.experimental.TPUStrategy(
         tpu_cluster_resolver, steps_per_run=100)
@@ -320,20 +320,20 @@ def define_ncf_flags():
       name="clone_model_in_keras_dist_strat",
       default=True,
       help=flags_core.help_wrap(
-          'If False, then the experimental code path is used that doesn\'t '
+          "If False, then the experimental code path is used that does not "
           "clone models for distribution."))
 
   flags.DEFINE_bool(
       name="early_stopping",
       default=False,
       help=flags_core.help_wrap(
-          'If True, we stop the training when it reaches hr_threshold'))
+          "If True, we stop the training when it reaches hr_threshold"))
 
   flags.DEFINE_bool(
       name="keras_use_ctl",
       default=False,
       help=flags_core.help_wrap(
-          'If True, we use a custom training loop for keras.'))
+          "If True, we use a custom training loop for keras."))
 
 def convert_to_softmax_logits(logits):
   '''Convert the logits returned by the base model to softmax logits.
diff --git a/official/recommendation/ncf_keras_main.py b/official/recommendation/ncf_keras_main.py
index c3fae450..e491aac5 100644
--- a/official/recommendation/ncf_keras_main.py
+++ b/official/recommendation/ncf_keras_main.py
@@ -171,21 +171,21 @@ class CustomEarlyStopping(tf.keras.callbacks.Callback):
 
   def on_train_end(self, logs=None):
     if self.stopped_epoch > 0:
-      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))
+      print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))
 
   def get_monitor_value(self, logs):
     logs = logs or {}
     monitor_value = logs.get(self.monitor)
     if monitor_value is None:
-      logging.warning('Early stopping conditioned on metric `%s` '
-                      'which is not available. Available metrics are: %s',
-                      self.monitor, ','.join(list(logs.keys())))
+      logging.warning("Early stopping conditioned on metric `%s` "
+                      "which is not available. Available metrics are: %s",
+                      self.monitor, ",".join(list(logs.keys())))
     return monitor_value
 
 
 def _get_keras_model(params):
   """Constructs and returns the model."""
-  batch_size = params['batch_size']
+  batch_size = params["batch_size"]
 
   # The input layers are of shape (1, batch_size), to match the size of the
   # input data. The first dimension is needed because the input data are
@@ -240,7 +240,7 @@ def run_ncf(_):
 
   params = ncf_common.parse_flags(FLAGS)
 
-  if params['keras_use_ctl'] and int(tf.__version__.split('.')[0]) == 1:
+  if params["keras_use_ctl"] and int(tf.__version__.split(".")[0]) == 1:
     logging.error(
         "Custom training loop only works with tensorflow 2.0 and above.")
     return
@@ -248,7 +248,7 @@ def run_ncf(_):
   # ncf_common rounds eval_batch_size (this is needed due to a reshape during
   # eval). This carries over that rounding to batch_size as well. This is the
   # per device batch size
-  params['batch_size'] = params['eval_batch_size']
+  params["batch_size"] = params["eval_batch_size"]
   batch_size = params["batch_size"]
 
   num_users, num_items, num_train_steps, num_eval_steps, producer = (
@@ -285,7 +285,7 @@ def run_ncf(_):
         beta_2=params["beta2"],
         epsilon=params["epsilon"])
 
-  if params['keras_use_ctl']:
+  if params["keras_use_ctl"]:
     loss_object = tf.losses.SparseCategoricalCrossentropy(
         reduction=tf.keras.losses.Reduction.SUM,
         from_logits=True)
@@ -352,8 +352,8 @@ def run_ncf(_):
         time_callback.on_batch_begin(step+epoch*num_train_steps)
         train_loss += train_step()
         time_callback.on_batch_end(step+epoch*num_train_steps)
-      logging.info("Done training epoch {}, epoch loss={}.".format(
-          epoch+1, train_loss/num_train_steps))
+      logging.info("Done training epoch %s, epoch loss=%s.",
+          epoch+1, train_loss/num_train_steps)
       eval_input_iterator.initialize()
       hr_sum = 0
       hr_count = 0
@@ -361,8 +361,7 @@ def run_ncf(_):
         step_hr_sum, step_hr_count = eval_step()
         hr_sum += step_hr_sum
         hr_count += step_hr_count
-      logging.info("Done eval epoch {}, hr={}.".format(epoch+1,
-                                                       hr_sum/hr_count))
+      logging.info("Done eval epoch %s, hr=%s.", epoch+1, hr_sum/hr_count)
 
       if (FLAGS.early_stopping and
           float(hr_sum/hr_count) > params["hr_threshold"]):
@@ -399,7 +398,7 @@ def run_ncf(_):
 
     if history and history.history:
       train_history = history.history
-      train_loss = train_history['loss'][-1]
+      train_loss = train_history["loss"][-1]
 
   stats = build_stats(train_loss, eval_results, time_callback)
   return stats
@@ -408,28 +407,29 @@ def run_ncf(_):
 def build_stats(loss, eval_result, time_callback):
   """Normalizes and returns dictionary of stats.
 
-    Args:
-      loss: The final loss at training time.
-      eval_output: Output of the eval step. Assumes first value is eval_loss and
-        second value is accuracy_top_1.
-      time_callback: Time tracking callback likely used during keras.fit.
-    Returns:
-      Dictionary of normalized results.
+  Args:
+    loss: The final loss at training time.
+    eval_result: Output of the eval step. Assumes first value is eval_loss and
+      second value is accuracy_top_1.
+    time_callback: Time tracking callback likely used during keras.fit.
+
+  Returns:
+    Dictionary of normalized results.
   """
   stats = {}
   if loss:
-    stats['loss'] = loss
+    stats["loss"] = loss
 
   if eval_result:
-    stats['eval_loss'] = eval_result[0]
-    stats['eval_hit_rate'] = eval_result[1]
+    stats["eval_loss"] = eval_result[0]
+    stats["eval_hit_rate"] = eval_result[1]
 
   if time_callback:
     timestamp_log = time_callback.timestamp_log
-    stats['step_timestamp_log'] = timestamp_log
-    stats['train_finish_time'] = time_callback.train_finish_time
+    stats["step_timestamp_log"] = timestamp_log
+    stats["train_finish_time"] = time_callback.train_finish_time
     if len(timestamp_log) > 1:
-      stats['avg_exp_per_second'] = (
+      stats["avg_exp_per_second"] = (
           time_callback.batch_size * time_callback.log_steps *
           (len(time_callback.timestamp_log)-1) /
           (timestamp_log[-1].timestamp - timestamp_log[0].timestamp))
diff --git a/official/transformer/transformer_main.py b/official/transformer/transformer_main.py
index e0e030b1..342d3145 100644
--- a/official/transformer/transformer_main.py
+++ b/official/transformer/transformer_main.py
@@ -583,7 +583,7 @@ def run_transformer(flags_obj):
   params["static_batch"] = flags_obj.static_batch or params["use_tpu"]
   params["allow_ffn_pad"] = not params["use_tpu"]
 
-  params["max_length"] = flags_obj.max_length or params['max_length']
+  params["max_length"] = flags_obj.max_length or params["max_length"]
 
   params["use_synthetic_data"] = flags_obj.use_synthetic_data
 
diff --git a/official/transformer/v2/data_pipeline.py b/official/transformer/v2/data_pipeline.py
index 5f77f2f3..a83eca66 100644
--- a/official/transformer/v2/data_pipeline.py
+++ b/official/transformer/v2/data_pipeline.py
@@ -164,7 +164,7 @@ def _batch_examples(dataset, batch_size, max_length):
     """Return int64 bucket id for this example, calculated based on length."""
     seq_length = _get_example_length((example_input, example_target))
 
-    # TODO: investigate whether removing code branching improves performance.
+    # TODO(xunkai): investigate if removing code branching improves performance.
     conditions_c = tf.logical_and(
         tf.less_equal(buckets_min, seq_length),
         tf.less(seq_length, buckets_max))
diff --git a/official/transformer/v2/metrics.py b/official/transformer/v2/metrics.py
index 46f4c70a..5ea5bfbd 100644
--- a/official/transformer/v2/metrics.py
+++ b/official/transformer/v2/metrics.py
@@ -160,12 +160,13 @@ class MetricLayer(tf.keras.layers.Layer):
 
   def call(self, inputs):
     logits, targets = inputs[0], inputs[1]
-    # TODO(guptapriya): Remove this check when underlying issue to create metrics
-    # with dist strat in cross replica context is fixed.
-    if tf.distribute.has_strategy() and not tf.distribute.in_cross_replica_context():
-     for mean, fn in self.metric_mean_fns:
-       m = mean(*fn(logits, targets))
-       self.add_metric(m)
+    # TODO(guptapriya): Remove this check when underlying issue to create
+    # metrics with dist strat in cross replica context is fixed.
+    if (tf.distribute.has_strategy() and
+        not tf.distribute.in_cross_replica_context()):
+      for mean, fn in self.metric_mean_fns:
+        m = mean(*fn(logits, targets))
+        self.add_metric(m)
     return logits
 
 
diff --git a/official/transformer/v2/transformer_benchmark.py b/official/transformer/v2/transformer_benchmark.py
index d36731bc..147b9259 100644
--- a/official/transformer/v2/transformer_benchmark.py
+++ b/official/transformer/v2/transformer_benchmark.py
@@ -225,6 +225,7 @@ class TransformerBaseKerasAccuracy(TransformerBenchmark):
                                    bleu_min=27,
                                    bleu_max=28)
 
+
 class TransformerBigKerasAccuracy(TransformerBenchmark):
   """Benchmark accuracy tests for Transformer Big model w/ Keras."""
 
@@ -351,7 +352,7 @@ class TransformerKerasBenchmark(TransformerBenchmark):
     FLAGS.batch_size = self.batch_per_gpu * 8
     FLAGS.model_dir = self._get_model_dir('benchmark_8_gpu_static_batch')
     FLAGS.static_batch = True
-    FLAGS.max_length = 64    
+    FLAGS.max_length = 64
     self._run_and_report_benchmark(total_batch_size=FLAGS.batch_size,
                                    log_steps=FLAGS.log_steps)
 
diff --git a/official/transformer/v2/transformer_main.py b/official/transformer/v2/transformer_main.py
index be457701..49b2742e 100644
--- a/official/transformer/v2/transformer_main.py
+++ b/official/transformer/v2/transformer_main.py
@@ -127,7 +127,7 @@ class TransformerTask(object):
 
     model.summary()
 
-    # TODO(guptapriya): Figure out a way to structure input that works in both 
+    # TODO(guptapriya): Figure out a way to structure input that works in both
     # distributed and non distributed cases.
     train_ds = data_pipeline.train_input_fn(params)
     if not self.distribution_strategy:
