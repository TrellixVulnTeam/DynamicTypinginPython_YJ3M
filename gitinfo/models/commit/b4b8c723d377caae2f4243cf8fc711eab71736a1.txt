commit b4b8c723d377caae2f4243cf8fc711eab71736a1
Author: Yash Katariya <yashkatariya@google.com>
Date:   Fri Apr 12 15:51:33 2019 -0700

    Update README.md (#6569)
    
    * Update README.md
    
    * Update README.md
    
    * Update README.md

diff --git a/official/transformer/README.md b/official/transformer/README.md
index d494fe48..81a23b5c 100644
--- a/official/transformer/README.md
+++ b/official/transformer/README.md
@@ -1,5 +1,5 @@
 # Transformer Translation Model
-This is an implementation of the Transformer translation model as described in the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. Based on the code provided by the authors: [Transformer code](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) from [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor).
+This is an implementation of the Transformer translation model as described in the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. Based on the code provided by the authors: [Transformer code](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) from [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor). Also, check out the [tutorial](https://www.tensorflow.org/alpha/tutorials/sequences/transformer) on Transformer in TF 2.0.
 
 Transformer is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.
 
