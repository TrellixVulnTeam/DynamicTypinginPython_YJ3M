commit ba87e2c6afd383a962f805b290efb0218068d096
Author: Mark Sandler <sandler@google.com>
Date:   Tue Nov 12 14:36:15 2019 -0800

    Merged commit includes the following changes: (#7797)
    
    279978375  by Sergio Guadarrama:
    
        Pass s=2 to the expanded_conv block so it can apply residual correctly in case of fused convolutions.  (Before it was relying on channel mismatch only)
    
    --
    279788358  by Sergio Guadarrama:
    
        Update README to add mobilenet-edgetpu details
    
    --
    279774392  by Sergio Guadarrama:
    
        Adds MobilenetV3-EdgeTpu definition.
    
    --
    278917344  by Sergio Guadarrama:
    
        Create visualwakewords dataset using slim scripts instead of custom scripts.
    
    --
    277940048  by Sergio Guadarrama:
    
        Internal changes to tf.contrib symbols
    
    --
    
    PiperOrigin-RevId: 279978375

diff --git a/research/slim/BUILD b/research/slim/BUILD
index 90c06788..7d65341a 100644
--- a/research/slim/BUILD
+++ b/research/slim/BUILD
@@ -39,6 +39,7 @@ py_binary(
     python_version = "PY2",
     deps = [
         # "//numpy",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -49,6 +50,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//numpy",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -68,44 +70,40 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//numpy",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
 
-py_binary(
-    name = "download_and_convert_data",
-    srcs = ["download_and_convert_data.py"],
-    python_version = "PY2",
+py_library(
+    name = "download_and_convert_visualwakewords_lib",
+    srcs = ["datasets/download_and_convert_visualwakewords_lib.py"],
     deps = [
-        ":download_and_convert_cifar10",
-        ":download_and_convert_flowers",
-        ":download_and_convert_mnist",
+        ":dataset_utils",
+        "//third_party/py/PIL:pil",
+        "//third_party/py/contextlib2",
         # "//tensorflow",
     ],
 )
 
-sh_binary(
-    name = "download_mscoco",
-    srcs = ["datasets/download_mscoco.sh"],
-)
-
-py_binary(
-    name = "build_visualwakewords_data",
-    srcs = ["datasets/build_visualwakewords_data.py"],
-    python_version = "PY2",
+py_library(
+    name = "download_and_convert_visualwakewords",
+    srcs = ["datasets/download_and_convert_visualwakewords.py"],
     deps = [
-        ":build_visualwakewords_data_lib",
+        ":download_and_convert_visualwakewords_lib",
         # "//tensorflow",
     ],
 )
 
-py_library(
-    name = "build_visualwakewords_data_lib",
-    srcs = ["datasets/build_visualwakewords_data_lib.py"],
+py_binary(
+    name = "download_and_convert_data",
+    srcs = ["download_and_convert_data.py"],
+    python_version = "PY2",
     deps = [
-        ":dataset_utils",
-        "//third_party/py/PIL:pil",
-        "//third_party/py/contextlib2",
+        ":download_and_convert_cifar10",
+        ":download_and_convert_flowers",
+        ":download_and_convert_mnist",
+        ":download_and_convert_visualwakewords",
         # "//tensorflow",
     ],
 )
@@ -116,6 +114,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -125,6 +124,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -133,7 +133,9 @@ py_library(
     srcs = ["datasets/imagenet.py"],
     deps = [
         ":dataset_utils",
+        "//third_party/py/six",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -143,6 +145,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -152,6 +155,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -172,6 +176,7 @@ py_library(
     srcs = ["deployment/model_deploy.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -184,6 +189,9 @@ py_test(
         ":model_deploy",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -192,6 +200,7 @@ py_library(
     srcs = ["preprocessing/cifarnet_preprocessing.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -209,6 +218,7 @@ py_library(
     srcs = ["preprocessing/lenet_preprocessing.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -217,6 +227,7 @@ py_library(
     srcs = ["preprocessing/vgg_preprocessing.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -228,7 +239,7 @@ py_library(
         ":inception_preprocessing",
         ":lenet_preprocessing",
         ":vgg_preprocessing",
-        # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -261,6 +272,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -273,6 +285,7 @@ py_test(
     deps = [
         ":alexnet",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -281,6 +294,7 @@ py_library(
     srcs = ["nets/cifarnet.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -289,7 +303,11 @@ py_library(
     srcs = ["nets/cyclegan.py"],
     deps = [
         # "//numpy",
+        "//third_party/py/six",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
+        # "//tensorflow/contrib/util:util_py",
     ],
 )
 
@@ -309,7 +327,9 @@ py_library(
     name = "dcgan",
     srcs = ["nets/dcgan.py"],
     deps = [
+        "//third_party/py/six",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -321,6 +341,7 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":dcgan",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -333,6 +354,7 @@ py_library(
         ":i3d_utils",
         ":s3dg",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -354,7 +376,10 @@ py_library(
     srcs = ["nets/i3d_utils.py"],
     srcs_version = "PY2AND3",
     deps = [
+        # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
     ],
 )
 
@@ -377,6 +402,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -387,6 +413,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -397,6 +424,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -407,6 +435,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -417,6 +446,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -426,6 +456,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -440,6 +471,7 @@ py_test(
         ":inception",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -454,6 +486,7 @@ py_test(
         ":inception",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -468,6 +501,7 @@ py_test(
         ":inception",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -481,6 +515,7 @@ py_test(
     deps = [
         ":inception",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -494,6 +529,7 @@ py_test(
     deps = [
         ":inception",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -502,6 +538,7 @@ py_library(
     srcs = ["nets/lenet.py"],
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -511,6 +548,8 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/layers:layers_py",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -523,6 +562,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -533,6 +573,8 @@ py_library(
     deps = [
         ":mobilenet_common",
         # "//tensorflow",
+        # "//tensorflow/contrib/layers:layers_py",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -542,7 +584,9 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet_common",
+        # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -553,7 +597,9 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet",
+        ":mobilenet_common",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -563,6 +609,7 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet",
+        "//third_party/py/absl/testing:absltest",
         # "//tensorflow",
     ],
 )
@@ -587,6 +634,7 @@ py_test(
         ":mobilenet_v1",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -599,6 +647,8 @@ py_binary(
         ":mobilenet_v1",
         ":preprocessing_factory",
         # "//tensorflow",
+        # "//tensorflow/contrib/quantize:quantize_graph",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -611,6 +661,8 @@ py_binary(
         ":mobilenet_v1",
         ":preprocessing_factory",
         # "//tensorflow",
+        # "//tensorflow/contrib/quantize:quantize_graph",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -620,6 +672,8 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -630,6 +684,10 @@ py_library(
     deps = [
         ":nasnet_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
+        # "//tensorflow/contrib/slim",
+        # "//tensorflow/contrib/training:training_py",
     ],
 )
 
@@ -655,6 +713,7 @@ py_test(
     deps = [
         ":nasnet",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -666,6 +725,9 @@ py_library(
         ":nasnet",
         ":nasnet_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/slim",
+        # "//tensorflow/contrib/training:training_py",
     ],
 )
 
@@ -679,6 +741,7 @@ py_test(
     deps = [
         ":pnasnet",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -688,6 +751,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -700,6 +764,7 @@ py_test(
     deps = [
         ":overfeat",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -709,6 +774,8 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
     ],
 )
 
@@ -720,6 +787,7 @@ py_test(
     deps = [
         ":pix2pix",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
     ],
 )
 
@@ -729,6 +797,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -739,6 +808,7 @@ py_library(
     deps = [
         ":resnet_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -755,6 +825,7 @@ py_test(
         ":resnet_v1",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -765,6 +836,7 @@ py_library(
     deps = [
         ":resnet_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -780,6 +852,7 @@ py_test(
         ":resnet_v2",
         # "//numpy",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -790,6 +863,8 @@ py_library(
     deps = [
         ":i3d_utils",
         # "//tensorflow",
+        # "//tensorflow/contrib/framework:framework_py",
+        # "//tensorflow/contrib/layers:layers_py",
     ],
 )
 
@@ -812,6 +887,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -824,6 +900,7 @@ py_test(
     deps = [
         ":vgg",
         # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -832,7 +909,7 @@ py_library(
     srcs = ["nets/nets_factory.py"],
     deps = [
         ":nets",
-        # "//tensorflow",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -858,6 +935,8 @@ py_library(
         ":nets_factory",
         ":preprocessing_factory",
         # "//tensorflow",
+        # "//tensorflow/contrib/quantize:quantize_graph",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -880,6 +959,8 @@ py_library(
         ":nets_factory",
         ":preprocessing_factory",
         # "//tensorflow",
+        # "//tensorflow/contrib/quantize:quantize_graph",
+        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -908,6 +989,8 @@ py_library(
         ":dataset_factory",
         ":nets_factory",
         # "//tensorflow",
+        # "//tensorflow/contrib/quantize:quantize_graph",
+        # "//tensorflow/contrib/slim",
         # "//tensorflow/python:platform",
     ],
 )
diff --git a/research/slim/README.md b/research/slim/README.md
index ec6a4259..3018dba0 100644
--- a/research/slim/README.md
+++ b/research/slim/README.md
@@ -131,15 +131,11 @@ These represent the training and validation data, sharded over 5 files each.
 You will also find the `$DATA_DIR/labels.txt` file which contains the mapping
 from integer labels to class names.
 
-You can use the same script to create the mnist and cifar10 datasets.
-However, for ImageNet, you have to follow the instructions
+You can use the same script to create the mnist, cifar10 and visualwakewords
+datasets. However, for ImageNet, you have to follow the instructions
 [here](https://github.com/tensorflow/models/blob/master/research/inception/README.md#getting-started).
-Note that you first have to sign up for an account at image-net.org.
-Also, the download can take several hours, and could use up to 500GB.
-For the visualwakewords dataset, you need to download the MSCOCO dataset [here](https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_mscoco.sh)
-and build TFRecords with the following instructions
-[here](https://github.com/tensorflow/models/blob/master/research/slim/datasets/build_visualwakewords_data.py).
-
+Note that you first have to sign up for an account at image-net.org. Also, the
+download can take several hours, and could use up to 500GB.
 
 ## Creating a TF-Slim Dataset Descriptor.
 
@@ -149,12 +145,12 @@ which stores pointers to the data file, as well as various other pieces of
 metadata, such as the class labels, the train/test split, and how to parse the
 TFExample protos. We have included the TF-Slim Dataset descriptors
 for
-[Cifar10](https://github.com/tensorflow/models/blob/master/research/slim/datasets/cifar10.py),
-[ImageNet](https://github.com/tensorflow/models/blob/master/research/slim/datasets/imagenet.py),
 [Flowers](https://github.com/tensorflow/models/blob/master/research/slim/datasets/flowers.py),
-[VisualWakeWords](https://github.com/tensorflow/models/blob/master/research/slim/datasets/visualwakewords.py),
+[Cifar10](https://github.com/tensorflow/models/blob/master/research/slim/datasets/cifar10.py),
+[MNIST](https://github.com/tensorflow/models/blob/master/research/slim/datasets/mnist.py),
+[ImageNet](https://github.com/tensorflow/models/blob/master/research/slim/datasets/imagenet.py)
 and
-[MNIST](https://github.com/tensorflow/models/blob/master/research/slim/datasets/mnist.py).
+[VisualWakeWords](https://github.com/tensorflow/models/blob/master/research/slim/datasets/visualwakewords.py),
 An example of how to load data using a TF-Slim dataset descriptor using a
 TF-Slim
 [DatasetDataProvider](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py)
diff --git a/research/slim/datasets/build_visualwakewords_data.py b/research/slim/datasets/build_visualwakewords_data.py
deleted file mode 100644
index 0d69e99c..00000000
--- a/research/slim/datasets/build_visualwakewords_data.py
+++ /dev/null
@@ -1,149 +0,0 @@
-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-r"""Build Visual WakeWords Dataset with images and labels for person/not-person.
-
-This script generates the Visual WakeWords dataset annotations from
-the raw COCO dataset and converts them to TFRecord.
-Visual WakeWords Dataset derives from the COCO dataset to design tiny models
-classifying two classes, such as person/not-person. The COCO annotations
-are filtered to two classes: foreground_class_of_interest and background
-( for e.g. person and not-person). Bounding boxes for small objects
-with area less than 5% of the image area are filtered out.
-
-The resulting annotations file has the following fields, where
-the image and categories fields are same as COCO dataset, while the annotation
-field corresponds to the foreground_class_of_interest/background class and
-bounding boxes for the foreground_class_of_interest class.
-
-  images{"id", "width", "height", "file_name", "license", "flickr_url",
-  "coco_url", "date_captured",}
-
-  annotations{
-  "image_id", object[{"category_id", "area", "bbox" : [x,y,width,height],}]
-  "count",
-  "label"
-  }
-
-  categories[{
-  "id", "name", "supercategory",
-  }]
-
-
-The TFRecord file contains the following features:
-{ image/height, image/width, image/source_id, image/encoded,
-  image/class/label_text, image/class/label,
-  image/object/class/text,
-  image/object/bbox/ymin, image/object/bbox/xmin, image/object/bbox/ymax,
-  image/object/bbox/xmax, image/object/area
-  image/filename, image/format, image/key/sha256}
-For classification models, you need the image/encoded and image/class/label.
-Please note that this tool creates sharded output files.
-
-Example usage:
-Add folder tensorflow/models/research/slim to your PYTHONPATH,
-and from this folder, run the following commands:
-
-    bash download_mscoco.sh path-to-mscoco-dataset
-    TRAIN_IMAGE_DIR="path-to-mscoco-dataset/train2014"
-    VAL_IMAGE_DIR="path-to-mscoco-dataset/val2014"
-
-    TRAIN_ANNOTATIONS_FILE="path-to-mscoco-dataset/annotations/instances_train2014.json"
-    VAL_ANNOTATIONS_FILE="path-to-mscoco-dataset/annotations/instances_val2014.json"
-
-    python datasets/build_visualwakewords_data.py --logtostderr \
-      --train_image_dir="${TRAIN_IMAGE_DIR}" \
-      --val_image_dir="${VAL_IMAGE_DIR}" \
-      --train_annotations_file="${TRAIN_ANNOTATIONS_FILE}" \
-      --val_annotations_file="${VAL_ANNOTATIONS_FILE}" \
-      --output_dir="${OUTPUT_DIR}" \
-      --small_object_area_threshold=0.005 \
-      --foreground_class_of_interest='person'
-"""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import os
-import tensorflow as tf
-from datasets import build_visualwakewords_data_lib
-
-flags = tf.app.flags
-tf.flags.DEFINE_string('train_image_dir', '', 'Training image directory.')
-tf.flags.DEFINE_string('val_image_dir', '', 'Validation image directory.')
-tf.flags.DEFINE_string('train_annotations_file', '',
-                       'Training annotations JSON file.')
-tf.flags.DEFINE_string('val_annotations_file', '',
-                       'Validation annotations JSON file.')
-tf.flags.DEFINE_string('output_dir', '/tmp/', 'Output data directory.')
-tf.flags.DEFINE_float(
-    'small_object_area_threshold', 0.005,
-    'Threshold of fraction of image area below which small'
-    'objects are filtered')
-tf.flags.DEFINE_string(
-    'foreground_class_of_interest', 'person',
-    'Build a binary classifier based on the presence or absence'
-    'of this object in the scene (default is person/not-person)')
-
-FLAGS = flags.FLAGS
-
-tf.logging.set_verbosity(tf.logging.INFO)
-
-
-def main(unused_argv):
-  # Path to COCO dataset images and annotations
-  assert FLAGS.train_image_dir, '`train_image_dir` missing.'
-  assert FLAGS.val_image_dir, '`val_image_dir` missing.'
-  assert FLAGS.train_annotations_file, '`train_annotations_file` missing.'
-  assert FLAGS.val_annotations_file, '`val_annotations_file` missing.'
-  visualwakewords_annotations_train = os.path.join(
-      FLAGS.output_dir, 'instances_visualwakewords_train2014.json')
-  visualwakewords_annotations_val = os.path.join(
-      FLAGS.output_dir, 'instances_visualwakewords_val2014.json')
-  visualwakewords_labels_filename = os.path.join(FLAGS.output_dir,
-                                                 'labels.txt')
-  small_object_area_threshold = FLAGS.small_object_area_threshold
-  foreground_class_of_interest = FLAGS.foreground_class_of_interest
-  # Create the Visual WakeWords annotations from COCO annotations
-  if not tf.gfile.IsDirectory(FLAGS.output_dir):
-    tf.gfile.MakeDirs(FLAGS.output_dir)
-  build_visualwakewords_data_lib.create_visual_wakeword_annotations(
-      FLAGS.train_annotations_file, visualwakewords_annotations_train,
-      small_object_area_threshold, foreground_class_of_interest,
-      visualwakewords_labels_filename)
-  build_visualwakewords_data_lib.create_visual_wakeword_annotations(
-      FLAGS.val_annotations_file, visualwakewords_annotations_val,
-      small_object_area_threshold, foreground_class_of_interest,
-      visualwakewords_labels_filename)
-
-  # Create the TF Records for Visual WakeWords Dataset
-  if not tf.gfile.IsDirectory(FLAGS.output_dir):
-    tf.gfile.MakeDirs(FLAGS.output_dir)
-  train_output_path = os.path.join(FLAGS.output_dir, 'train.record')
-  val_output_path = os.path.join(FLAGS.output_dir, 'val.record')
-  build_visualwakewords_data_lib.create_tf_record_for_visualwakewords_dataset(
-      visualwakewords_annotations_train,
-      FLAGS.train_image_dir,
-      train_output_path,
-      num_shards=100)
-  build_visualwakewords_data_lib.create_tf_record_for_visualwakewords_dataset(
-      visualwakewords_annotations_val,
-      FLAGS.val_image_dir,
-      val_output_path,
-      num_shards=10)
-
-
-if __name__ == '__main__':
-  tf.app.run()
diff --git a/research/slim/datasets/build_visualwakewords_data_lib.py b/research/slim/datasets/build_visualwakewords_data_lib.py
deleted file mode 100644
index 8ff3635a..00000000
--- a/research/slim/datasets/build_visualwakewords_data_lib.py
+++ /dev/null
@@ -1,338 +0,0 @@
-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-r"""Generate Visual Wakewords Dataset.
-
-    Helper functions to generate the Visual WakeWords dataset. It filters raw
-    COCO annotations file to Visual WakeWords Dataset annotations.
-    The resulting annotations and COCO images are then
-    converted to TF records.
-    See build_visualwakewords_data.py for the sample usage.
-"""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import hashlib
-import io
-import json
-import os
-import contextlib2
-
-import PIL.Image
-
-import tensorflow as tf
-
-from datasets import dataset_utils
-
-tf.logging.set_verbosity(tf.logging.INFO)
-
-
-def create_visual_wakeword_annotations(annotations_file,
-                                       visualwakewords_annotations_path,
-                                       small_object_area_threshold,
-                                       foreground_class_of_interest,
-                                       visualwakewords_labels_filename):
-  """Generate visual wakewords annotations file.
-
-  Loads COCO annotation json files and filters to person/not-person
-  class (or user-specified class) to generate visual wakewords annotations file.
-  Each image is assigned a label 1 or 0. The label 1 is assigned as long
-  as it has at least one foreground_class_of_interest (e.g. person)
-  bounding box greater than 5% of the image area.
-
-  Args:
-    annotations_file: JSON file containing COCO bounding box annotations
-    visualwakewords_annotations_path: output path to annotations file
-    small_object_area_threshold: threshold on fraction of image area below which
-      small object bounding boxes are filtered
-    foreground_class_of_interest: category from COCO dataset that is filtered by
-      the visual wakewords dataset
-    visualwakewords_labels_filename: The filename to write the visual wakewords
-      label file
-  """
-  # default object of interest is person
-  foreground_class_of_interest_id = 1
-  with tf.gfile.GFile(annotations_file, 'r') as fid:
-    groundtruth_data = json.load(fid)
-    images = groundtruth_data['images']
-    # Create category index
-    category_index = {}
-    for category in groundtruth_data['categories']:
-      if category['name'] == foreground_class_of_interest:
-        foreground_class_of_interest_id = category['id']
-        category_index[category['id']] = category
-
-    # Create annotations index
-    annotations_index = {}
-    annotations_index_filtered = {}
-    if 'annotations' in groundtruth_data:
-      tf.logging.info(
-          'Found groundtruth annotations. Building annotations index.')
-      for annotation in groundtruth_data['annotations']:
-        image_id = annotation['image_id']
-        if image_id not in annotations_index:
-          annotations_index[image_id] = []
-          annotations_index_filtered[image_id] = []
-        annotations_index[image_id].append(annotation)
-      missing_annotation_count = 0
-      for image in images:
-        image_id = image['id']
-        if image_id not in annotations_index:
-          missing_annotation_count += 1
-          annotations_index[image_id] = []
-          annotations_index_filtered[image_id] = []
-      tf.logging.info('%d images are missing annotations.',
-                      missing_annotation_count)
-    # Create filtered annotations index
-    for idx, image in enumerate(images):
-      if idx % 100 == 0:
-        tf.logging.info('On image %d of %d', idx, len(images))
-      annotations_list = annotations_index[image['id']]
-      annotations_list_filtered = _filter_annotations_list(
-          annotations_list, image, small_object_area_threshold,
-          foreground_class_of_interest_id)
-      annotations_index_filtered[image['id']].append(annotations_list_filtered)
-    # Output Visual WakeWords annotations and labels
-    labels_to_class_names = {0: 'background', 1: foreground_class_of_interest}
-    with open(visualwakewords_labels_filename, 'w') as fp:
-      for label in labels_to_class_names:
-        fp.write(str(label) + ':' + str(labels_to_class_names[label]) + '\n')
-    with open(visualwakewords_annotations_path, 'w') as fp:
-      json.dump(
-          {
-              'images': images,
-              'annotations': annotations_index_filtered,
-              'categories': category_index
-          }, fp)
-
-
-def _filter_annotations_list(annotations_list, image,
-                             small_object_area_threshold,
-                             foreground_class_of_interest_id):
-  """Filters COCO annotations_list to visual wakewords annotations_list.
-
-  Each image is assigned a label 1 or 0. The label 1 is assigned as long
-  as it has at least one foreground_class_of_interest (e.g. person)
-  bounding box greater than 5% of the image area.
-
-  Args:
-    annotations_list: list of dicts with keys: [ u'id', u'image_id',
-    u'category_id', u'segmentation', u'area', u'bbox' : [x,y,width,height],
-      u'iscrowd']. Notice that bounding box coordinates in the official COCO
-      dataset are given as [x, y, width, height] tuples using absolute
-      coordinates where x, y represent the top-left (0-indexed) corner.
-    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',
-      u'width', u'date_captured', u'flickr_url', u'id']
-    small_object_area_threshold: threshold on fraction of image area below which
-      small objects are filtered
-    foreground_class_of_interest_id: category of COCO dataset which visual
-      wakewords filters
-
-  Returns:
-    filtered_annotations_list: list of dicts with keys: [ u'image_id',
-    u'label', u'category_id', u'count',
-    u'object':[{"category_id", "area", "bbox" : [x,y,width,height],}]
-  """
-  category_ids = []
-  area = []
-  flag_small_object = []
-  num_ann = 0
-  image_height = image['height']
-  image_width = image['width']
-  image_area = image_height * image_width
-  bbox = []
-  # count of filtered object
-  count = 0
-  for object_annotations in annotations_list:
-    (x, y, width, height) = tuple(object_annotations['bbox'])
-    category_id = int(object_annotations['category_id'])
-    category_ids.append(category_id)
-    obj_area = object_annotations['area']
-    normalized_object_area = obj_area / image_area
-    # Filter small object bounding boxes
-    if category_id == foreground_class_of_interest_id:
-      if normalized_object_area < small_object_area_threshold:
-        flag_small_object.append(True)
-      else:
-        flag_small_object.append(False)
-        bbox.append({
-            u'bbox': [x, y, width, height],
-            u'area': obj_area,
-            u'category_id': category_id
-        })
-        count = count + 1
-    area.append(obj_area)
-    num_ann = num_ann + 1
-  # Filtered annotations_list with two classes corresponding to
-  # foreground_class_of_interest_id (e.g. person) and
-  # background (e.g. not-person)
-  if (foreground_class_of_interest_id in category_ids) and (
-      False in flag_small_object):
-    return {
-        u'image_id': image['id'],
-        u'label': 1,
-        u'object': bbox,
-        u'count': count
-    }
-  else:
-    return {u'image_id': image['id'], u'label': 0, u'object': [], u'count': 0}
-
-
-def create_tf_record_for_visualwakewords_dataset(annotations_file, image_dir,
-                                                 output_path, num_shards):
-  """Loads Visual WakeWords annotations/images and converts to tf.Record format.
-
-  Args:
-    annotations_file: JSON file containing bounding box annotations.
-    image_dir: Directory containing the image files.
-    output_path: Path to output tf.Record file.
-    num_shards: number of output file shards.
-  """
-  with contextlib2.ExitStack() as tf_record_close_stack, \
-      tf.gfile.GFile(annotations_file, 'r') as fid:
-    output_tfrecords = dataset_utils.open_sharded_output_tfrecords(
-        tf_record_close_stack, output_path, num_shards)
-    groundtruth_data = json.load(fid)
-    images = groundtruth_data['images']
-
-    category_index = {}
-    for category in groundtruth_data['categories'].values():
-      # if not background class
-      if category['id'] != 0:
-        category_index[category['id']] = category
-
-    annotations_index = {}
-    if 'annotations' in groundtruth_data:
-      tf.logging.info(
-          'Found groundtruth annotations. Building annotations index.')
-      for annotation in groundtruth_data['annotations'].values():
-        image_id = annotation[0]['image_id']
-        if image_id not in annotations_index:
-          annotations_index[image_id] = []
-        annotations_index[image_id].append(annotation[0])
-    missing_annotation_count = 0
-    for image in images:
-      image_id = image['id']
-      if image_id not in annotations_index:
-        missing_annotation_count += 1
-        annotations_index[image_id] = []
-    tf.logging.info('%d images are missing annotations.',
-                    missing_annotation_count)
-
-    total_num_annotations_skipped = 0
-    for idx, image in enumerate(images):
-      if idx % 100 == 0:
-        tf.logging.info('On image %d of %d', idx, len(images))
-      annotations_list = annotations_index[image['id']]
-      _, tf_example, num_annotations_skipped = _create_tf_example(
-          image, annotations_list[0], image_dir)
-      total_num_annotations_skipped += num_annotations_skipped
-      shard_idx = idx % num_shards
-      output_tfrecords[shard_idx].write(tf_example.SerializeToString())
-    tf.logging.info('Finished writing, skipped %d annotations.',
-                    total_num_annotations_skipped)
-
-
-def _create_tf_example(image, annotations_list, image_dir):
-  """Converts image and annotations to a tf.Example proto.
-
-  Args:
-    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',
-      u'width', u'date_captured', u'flickr_url', u'id']
-    annotations_list:
-      list of dicts with keys: [u'image_id', u'bbox', u'label',
-      object[{"category_id", "area", "bbox" : [x,y,width,height],}]]. Notice
-        that bounding box coordinates in the COCO dataset are given as [x, y,
-        width, height] tuples using absolute coordinates where x, y represent
-        the top-left (0-indexed) corner. This function converts to the format
-        that can be used by the Tensorflow Object Detection API (which is [ymin,
-        xmin, ymax, xmax] with coordinates normalized relative to image size).
-    image_dir: directory containing the image files.
-
-  Returns:
-    example: The converted tf.Example
-    num_annotations_skipped: Number of (invalid) annotations that were ignored.
-
-  Raises:
-    ValueError: if the image pointed to by data['filename'] is not a valid JPEG
-  """
-  image_height = image['height']
-  image_width = image['width']
-  filename = image['file_name']
-  image_id = image['id']
-
-  full_path = os.path.join(image_dir, filename)
-  with tf.gfile.GFile(full_path, 'rb') as fid:
-    encoded_jpg = fid.read()
-  encoded_jpg_io = io.BytesIO(encoded_jpg)
-  image = PIL.Image.open(encoded_jpg_io)
-  key = hashlib.sha256(encoded_jpg).hexdigest()
-
-  xmin = []
-  xmax = []
-  ymin = []
-  ymax = []
-  category_ids = []
-  area = []
-  num_annotations_skipped = 0
-  label = annotations_list['label']
-  for object_annotations in annotations_list['object']:
-    (x, y, width, height) = tuple(object_annotations['bbox'])
-    if width <= 0 or height <= 0:
-      num_annotations_skipped += 1
-      continue
-    if x + width > image_width or y + height > image_height:
-      num_annotations_skipped += 1
-      continue
-    xmin.append(float(x) / image_width)
-    xmax.append(float(x + width) / image_width)
-    ymin.append(float(y) / image_height)
-    ymax.append(float(y + height) / image_height)
-    category_id = int(object_annotations['category_id'])
-    category_ids.append(category_id)
-    area.append(object_annotations['area'])
-
-  feature_dict = {
-      'image/height':
-          dataset_utils.int64_feature(image_height),
-      'image/width':
-          dataset_utils.int64_feature(image_width),
-      'image/filename':
-          dataset_utils.bytes_feature(filename.encode('utf8')),
-      'image/source_id':
-          dataset_utils.bytes_feature(str(image_id).encode('utf8')),
-      'image/key/sha256':
-          dataset_utils.bytes_feature(key.encode('utf8')),
-      'image/encoded':
-          dataset_utils.bytes_feature(encoded_jpg),
-      'image/format':
-          dataset_utils.bytes_feature('jpeg'.encode('utf8')),
-      'image/class/label':
-          dataset_utils.int64_feature(label),
-      'image/object/bbox/xmin':
-          dataset_utils.float_list_feature(xmin),
-      'image/object/bbox/xmax':
-          dataset_utils.float_list_feature(xmax),
-      'image/object/bbox/ymin':
-          dataset_utils.float_list_feature(ymin),
-      'image/object/bbox/ymax':
-          dataset_utils.float_list_feature(ymax),
-      'image/object/class/label':
-          dataset_utils.int64_feature(label),
-      'image/object/area':
-          dataset_utils.float_list_feature(area),
-  }
-  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
-  return key, example, num_annotations_skipped
diff --git a/research/slim/datasets/cifar10.py b/research/slim/datasets/cifar10.py
index e0d29684..a28e0096 100644
--- a/research/slim/datasets/cifar10.py
+++ b/research/slim/datasets/cifar10.py
@@ -24,10 +24,11 @@ from __future__ import print_function
 
 import os
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 _FILE_PATTERN = 'cifar10_%s.tfrecord'
 
diff --git a/research/slim/datasets/dataset_utils.py b/research/slim/datasets/dataset_utils.py
index da793fdb..47e27d19 100644
--- a/research/slim/datasets/dataset_utils.py
+++ b/research/slim/datasets/dataset_utils.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 import os
 import sys
 import tarfile
+import zipfile
 
 from six.moves import urllib
 import tensorflow as tf
@@ -101,28 +102,68 @@ def image_to_tfexample(image_data, image_format, height, width, class_id):
   }))
 
 
-def download_and_uncompress_tarball(tarball_url, dataset_dir):
-  """Downloads the `tarball_url` and uncompresses it locally.
+def download_url(url, dataset_dir):
+  """Downloads the tarball or zip file from url into filepath.
 
   Args:
-    tarball_url: The URL of a tarball file.
+    url: The URL of a tarball or zip file.
     dataset_dir: The directory where the temporary files are stored.
+
+  Returns:
+    filepath: path where the file is downloaded.
   """
-  filename = tarball_url.split('/')[-1]
+  filename = url.split('/')[-1]
   filepath = os.path.join(dataset_dir, filename)
 
   def _progress(count, block_size, total_size):
     sys.stdout.write('\r>> Downloading %s %.1f%%' % (
         filename, float(count * block_size) / float(total_size) * 100.0))
     sys.stdout.flush()
-  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)
+
+  filepath, _ = urllib.request.urlretrieve(url, filepath, _progress)
   print()
   statinfo = os.stat(filepath)
   print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')
+  return filepath
+
+
+def download_and_uncompress_tarball(tarball_url, dataset_dir):
+  """Downloads the `tarball_url` and uncompresses it locally.
+
+  Args:
+    tarball_url: The URL of a tarball file.
+    dataset_dir: The directory where the temporary files are stored.
+  """
+  filepath = download_url(tarball_url, dataset_dir)
   tarfile.open(filepath, 'r:gz').extractall(dataset_dir)
 
 
-def write_label_file(labels_to_class_names, dataset_dir,
+def download_and_uncompress_zipfile(zip_url, dataset_dir):
+  """Downloads the `zip_url` and uncompresses it locally.
+
+  Args:
+    zip_url: The URL of a zip file.
+    dataset_dir: The directory where the temporary files are stored.
+  """
+  filename = zip_url.split('/')[-1]
+  filepath = os.path.join(dataset_dir, filename)
+
+  if tf.gfile.Exists(filepath):
+    print('File {filename} has been already downloaded at {filepath}. '
+          'Unzipping it....'.format(filename=filename, filepath=filepath))
+  else:
+    filepath = download_url(zip_url, dataset_dir)
+
+  with zipfile.ZipFile(filepath, 'r') as zip_file:
+    for member in zip_file.namelist():
+      memberpath = os.path.join(dataset_dir, member)
+      # extract only if file doesn't exist
+      if not (os.path.exists(memberpath) or os.path.isfile(memberpath)):
+        zip_file.extract(member, dataset_dir)
+
+
+def write_label_file(labels_to_class_names,
+                     dataset_dir,
                      filename=LABELS_FILENAME):
   """Writes a file with the list of class names.
 
diff --git a/research/slim/datasets/download_and_convert_visualwakewords.py b/research/slim/datasets/download_and_convert_visualwakewords.py
new file mode 100644
index 00000000..dc6aa69f
--- /dev/null
+++ b/research/slim/datasets/download_and_convert_visualwakewords.py
@@ -0,0 +1,158 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Downloads and converts VisualWakewords data to TFRecords of TF-Example protos.
+
+This module downloads the COCO dataset, uncompresses it, derives the
+VisualWakeWords dataset to create two TFRecord datasets: one for
+train and one for test. Each TFRecord dataset is comprised of a set of
+TF-Example protocol buffers, each of which contain a single image and label.
+
+The script should take several minutes to run.
+Please note that this tool creates sharded output files.
+
+VisualWakeWords dataset is used to design tiny models classifying two classes,
+such as person/not-person. The two steps to generate the VisualWakeWords
+dataset from the COCO dataset are given below:
+
+1. Use COCO annotations to create VisualWakeWords annotations:
+
+Note: A bounding box is 'valid' if it has the foreground_class_of_interest
+(e.g. person) and it's area is greater than 0.5% of the image area.
+
+The resulting annotations file has the following fields, where 'images' are
+the same as COCO dataset. 'categories' only contains information about the
+foreground_class_of_interest (e.g. person) and 'annotations' maps an image to
+objects (a list of valid bounding boxes) and label (value is 1 if it has
+atleast one valid bounding box, otherwise 0)
+
+  images[{
+  "id", "width", "height", "file_name", "flickr_url", "coco_url",
+  "license", "date_captured",
+  }]
+
+  categories{
+  "id": {"id", "name", "supercategory"}
+  }
+
+  annotations{
+  "image_id": {"objects":[{"area", "bbox" : [x,y,width,height]}], "label"}
+  }
+
+2. Use VisualWakeWords annotations to create TFRecords:
+
+The resulting TFRecord file contains the following features:
+{ image/height, image/width, image/source_id, image/encoded,
+  image/class/label_text, image/class/label,
+  image/object/class/text,
+  image/object/bbox/ymin, image/object/bbox/xmin, image/object/bbox/ymax,
+  image/object/bbox/xmax, image/object/area
+  image/filename, image/format, image/key/sha256}
+For classification models, you need the image/encoded and image/class/label.
+
+Example usage:
+Run download_and_convert_data.py in the parent directory as follows:
+
+    python download_and_convert_visualwakewords.py --logtostderr \
+      --dataset_name=visualwakewords \
+      --dataset_dir="${DATASET_DIR}" \
+      --small_object_area_threshold=0.005 \
+      --foreground_class_of_interest='person'
+
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import tensorflow as tf
+from datasets import download_and_convert_visualwakewords_lib
+
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
+
+tf.compat.v1.app.flags.DEFINE_string(
+    'coco_dirname', 'coco_dataset',
+    'A subdirectory in visualwakewords dataset directory'
+    'containing the coco dataset')
+
+FLAGS = tf.compat.v1.app.flags.FLAGS
+
+
+def run(dataset_dir, small_object_area_threshold, foreground_class_of_interest):
+  """Runs the download and conversion operation.
+
+  Args:
+    dataset_dir: The dataset directory where the dataset is stored.
+    small_object_area_threshold: Threshold of fraction of image area below which
+      small objects are filtered
+    foreground_class_of_interest: Build a binary classifier based on the
+      presence or absence of this object in the image.
+  """
+  # 1. Download the coco dataset into a subdirectory under the visualwakewords
+  #    dataset directory
+  coco_dir = os.path.join(dataset_dir, FLAGS.coco_dirname)
+
+  if not tf.gfile.IsDirectory(coco_dir):
+    tf.gfile.MakeDirs(coco_dir)
+
+  download_and_convert_visualwakewords_lib.download_coco_dataset(coco_dir)
+
+  # Path to COCO annotations
+  train_annotations_file = os.path.join(coco_dir, 'annotations',
+                                        'instances_train2014.json')
+  val_annotations_file = os.path.join(coco_dir, 'annotations',
+                                      'instances_val2014.json')
+  train_image_dir = os.path.join(coco_dir, 'train2014')
+  val_image_dir = os.path.join(coco_dir, 'val2014')
+
+  # Path to VisualWakeWords annotations
+  visualwakewords_annotations_train = os.path.join(
+      dataset_dir, 'instances_visualwakewords_train2014.json')
+  visualwakewords_annotations_val = os.path.join(
+      dataset_dir, 'instances_visualwakewords_val2014.json')
+  visualwakewords_labels_filename = os.path.join(dataset_dir, 'labels.txt')
+  train_output_path = os.path.join(dataset_dir, 'train.record')
+  val_output_path = os.path.join(dataset_dir, 'val.record')
+
+  # 2. Create a labels file
+  tf.logging.info('Creating a labels file...')
+  download_and_convert_visualwakewords_lib.create_labels_file(
+      foreground_class_of_interest, visualwakewords_labels_filename)
+
+  # 3. Use COCO annotations to create VisualWakeWords annotations
+  tf.logging.info('Creating train VisualWakeWords annotations...')
+  download_and_convert_visualwakewords_lib.create_visual_wakeword_annotations(
+      train_annotations_file, visualwakewords_annotations_train,
+      small_object_area_threshold, foreground_class_of_interest)
+  tf.logging.info('Creating validation VisualWakeWords annotations...')
+  download_and_convert_visualwakewords_lib.create_visual_wakeword_annotations(
+      val_annotations_file, visualwakewords_annotations_val,
+      small_object_area_threshold, foreground_class_of_interest)
+
+  # 4. Use VisualWakeWords annotations to create the TFRecords
+  tf.logging.info('Creating train TFRecords for VisualWakeWords dataset...')
+  download_and_convert_visualwakewords_lib.create_tf_record_for_visualwakewords_dataset(
+      visualwakewords_annotations_train,
+      train_image_dir,
+      train_output_path,
+      num_shards=100)
+
+  tf.logging.info(
+      'Creating validation TFRecords for VisualWakeWords dataset...')
+  download_and_convert_visualwakewords_lib.create_tf_record_for_visualwakewords_dataset(
+      visualwakewords_annotations_val,
+      val_image_dir,
+      val_output_path,
+      num_shards=10)
diff --git a/research/slim/datasets/download_and_convert_visualwakewords_lib.py b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
new file mode 100644
index 00000000..4c3d2004
--- /dev/null
+++ b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
@@ -0,0 +1,286 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Helper functions to generate the Visual WakeWords dataset.
+
+    It filters raw COCO annotations file to Visual WakeWords Dataset
+    annotations. The resulting annotations and COCO images are then converted
+    to TF records.
+    See download_and_convert_visualwakewords.py for the sample usage.
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import hashlib
+import io
+import json
+import os
+import contextlib2
+
+import PIL.Image
+
+import tensorflow as tf
+
+from datasets import dataset_utils
+
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
+
+tf.compat.v1.app.flags.DEFINE_string(
+    'coco_train_url',
+    'http://images.cocodataset.org/zips/train2014.zip',
+    'Link to zip file containing coco training data')
+tf.compat.v1.app.flags.DEFINE_string(
+    'coco_validation_url',
+    'http://images.cocodataset.org/zips/val2014.zip',
+    'Link to zip file containing coco validation data')
+tf.compat.v1.app.flags.DEFINE_string(
+    'coco_annotations_url',
+    'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
+    'Link to zip file containing coco annotation data')
+
+FLAGS = tf.compat.v1.app.flags.FLAGS
+
+
+def download_coco_dataset(dataset_dir):
+  """Download the coco dataset.
+
+  Args:
+    dataset_dir: Path where coco dataset should be downloaded.
+  """
+  dataset_utils.download_and_uncompress_zipfile(FLAGS.coco_train_url,
+                                                dataset_dir)
+  dataset_utils.download_and_uncompress_zipfile(FLAGS.coco_validation_url,
+                                                dataset_dir)
+  dataset_utils.download_and_uncompress_zipfile(FLAGS.coco_annotations_url,
+                                                dataset_dir)
+
+
+def create_labels_file(foreground_class_of_interest,
+                       visualwakewords_labels_file):
+  """Generate visualwakewords labels file.
+
+  Args:
+    foreground_class_of_interest: category from COCO dataset that is filtered by
+      the visualwakewords dataset
+    visualwakewords_labels_file: output visualwakewords label file
+  """
+  labels_to_class_names = {0: 'background', 1: foreground_class_of_interest}
+  with open(visualwakewords_labels_file, 'w') as fp:
+    for label in labels_to_class_names:
+      fp.write(str(label) + ':' + str(labels_to_class_names[label]) + '\n')
+
+
+def create_visual_wakeword_annotations(annotations_file,
+                                       visualwakewords_annotations_file,
+                                       small_object_area_threshold,
+                                       foreground_class_of_interest):
+  """Generate visual wakewords annotations file.
+
+  Loads COCO annotation json files to generate visualwakewords annotations file.
+
+  Args:
+    annotations_file: JSON file containing COCO bounding box annotations
+    visualwakewords_annotations_file: path to output annotations file
+    small_object_area_threshold: threshold on fraction of image area below which
+      small object bounding boxes are filtered
+    foreground_class_of_interest: category from COCO dataset that is filtered by
+      the visual wakewords dataset
+  """
+  # default object of interest is person
+  foreground_class_of_interest_id = 1
+  with tf.gfile.GFile(annotations_file, 'r') as fid:
+    groundtruth_data = json.load(fid)
+    images = groundtruth_data['images']
+    # Create category index
+    category_index = {}
+    for category in groundtruth_data['categories']:
+      if category['name'] == foreground_class_of_interest:
+        foreground_class_of_interest_id = category['id']
+        category_index[category['id']] = category
+    # Create annotations index, a map of image_id to it's annotations
+    tf.logging.info('Building annotations index...')
+    annotations_index = collections.defaultdict(
+        lambda: collections.defaultdict(list))
+    # structure is { "image_id": {"objects" : [list of the image annotations]}}
+    for annotation in groundtruth_data['annotations']:
+      annotations_index[annotation['image_id']]['objects'].append(annotation)
+    missing_annotation_count = len(images) - len(annotations_index)
+    tf.logging.info('%d images are missing annotations.',
+                    missing_annotation_count)
+    # Create filtered annotations index
+    annotations_index_filtered = {}
+    for idx, image in enumerate(images):
+      if idx % 100 == 0:
+        tf.logging.info('On image %d of %d', idx, len(images))
+      annotations = annotations_index[image['id']]
+      annotations_filtered = _filter_annotations(
+          annotations, image, small_object_area_threshold,
+          foreground_class_of_interest_id)
+      annotations_index_filtered[image['id']] = annotations_filtered
+
+    with open(visualwakewords_annotations_file, 'w') as fp:
+      json.dump(
+          {
+              'images': images,
+              'annotations': annotations_index_filtered,
+              'categories': category_index
+          }, fp)
+
+
+def _filter_annotations(annotations, image, small_object_area_threshold,
+                        foreground_class_of_interest_id):
+  """Filters COCO annotations to visual wakewords annotations.
+
+  Args:
+    annotations: dicts with keys: {
+      u'objects': [{u'id', u'image_id', u'category_id', u'segmentation',
+                  u'area', u'bbox' : [x,y,width,height], u'iscrowd'}] } Notice
+                    that bounding box coordinates in the official COCO dataset
+                    are given as [x, y, width, height] tuples using absolute
+                    coordinates where x, y represent the top-left (0-indexed)
+                    corner.
+    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',
+      u'width', u'date_captured', u'flickr_url', u'id']
+    small_object_area_threshold: threshold on fraction of image area below which
+      small objects are filtered
+    foreground_class_of_interest_id: category of COCO dataset which visual
+      wakewords filters
+
+  Returns:
+    annotations_filtered: dict with keys: {
+      u'objects': [{"area", "bbox" : [x,y,width,height]}],
+      u'label',
+      }
+  """
+  objects = []
+  image_area = image['height'] * image['width']
+  for annotation in annotations['objects']:
+    normalized_object_area = annotation['area'] / image_area
+    category_id = int(annotation['category_id'])
+    # Filter valid bounding boxes
+    if category_id == foreground_class_of_interest_id and \
+        normalized_object_area > small_object_area_threshold:
+      objects.append({
+          u'area': annotation['area'],
+          u'bbox': annotation['bbox'],
+      })
+  label = 1 if objects else 0
+  return {
+      'objects': objects,
+      'label': label,
+  }
+
+
+def create_tf_record_for_visualwakewords_dataset(annotations_file, image_dir,
+                                                 output_path, num_shards):
+  """Loads Visual WakeWords annotations/images and converts to tf.Record format.
+
+  Args:
+    annotations_file: JSON file containing bounding box annotations.
+    image_dir: Directory containing the image files.
+    output_path: Path to output tf.Record file.
+    num_shards: number of output file shards.
+  """
+  with contextlib2.ExitStack() as tf_record_close_stack, \
+      tf.gfile.GFile(annotations_file, 'r') as fid:
+    output_tfrecords = dataset_utils.open_sharded_output_tfrecords(
+        tf_record_close_stack, output_path, num_shards)
+    groundtruth_data = json.load(fid)
+    images = groundtruth_data['images']
+    annotations_index = groundtruth_data['annotations']
+    annotations_index = {int(k): v for k, v in annotations_index.iteritems()}
+    # convert 'unicode' key to 'int' key after we parse the json file
+
+    for idx, image in enumerate(images):
+      if idx % 100 == 0:
+        tf.logging.info('On image %d of %d', idx, len(images))
+      annotations = annotations_index[image['id']]
+      tf_example = _create_tf_example(image, annotations, image_dir)
+      shard_idx = idx % num_shards
+      output_tfrecords[shard_idx].write(tf_example.SerializeToString())
+
+
+def _create_tf_example(image, annotations, image_dir):
+  """Converts image and annotations to a tf.Example proto.
+
+  Args:
+    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',
+      u'width', u'date_captured', u'flickr_url', u'id']
+    annotations: dict with objects (a list of image annotations) and a label.
+      {u'objects':[{"area", "bbox" : [x,y,width,height}], u'label'}. Notice
+      that bounding box coordinates in the COCO dataset are given as[x, y,
+      width, height] tuples using absolute coordinates where x, y represent
+      the top-left (0-indexed) corner. This function also converts to the format
+      that can be used by the Tensorflow Object Detection API (which is [ymin,
+      xmin, ymax, xmax] with coordinates normalized relative to image size).
+    image_dir: directory containing the image files.
+  Returns:
+    tf_example: The converted tf.Example
+
+  Raises:
+    ValueError: if the image pointed to by data['filename'] is not a valid JPEG
+  """
+  image_height = image['height']
+  image_width = image['width']
+  filename = image['file_name']
+  image_id = image['id']
+
+  full_path = os.path.join(image_dir, filename)
+  with tf.gfile.GFile(full_path, 'rb') as fid:
+    encoded_jpg = fid.read()
+  encoded_jpg_io = io.BytesIO(encoded_jpg)
+  image = PIL.Image.open(encoded_jpg_io)
+  key = hashlib.sha256(encoded_jpg).hexdigest()
+
+  xmin, xmax, ymin, ymax, area = [], [], [], [], []
+  for obj in annotations['objects']:
+    (x, y, width, height) = tuple(obj['bbox'])
+    xmin.append(float(x) / image_width)
+    xmax.append(float(x + width) / image_width)
+    ymin.append(float(y) / image_height)
+    ymax.append(float(y + height) / image_height)
+    area.append(obj['area'])
+
+  feature_dict = {
+      'image/height':
+          dataset_utils.int64_feature(image_height),
+      'image/width':
+          dataset_utils.int64_feature(image_width),
+      'image/filename':
+          dataset_utils.bytes_feature(filename.encode('utf8')),
+      'image/source_id':
+          dataset_utils.bytes_feature(str(image_id).encode('utf8')),
+      'image/key/sha256':
+          dataset_utils.bytes_feature(key.encode('utf8')),
+      'image/encoded':
+          dataset_utils.bytes_feature(encoded_jpg),
+      'image/format':
+          dataset_utils.bytes_feature('jpeg'.encode('utf8')),
+      'image/class/label':
+          dataset_utils.int64_feature(annotations['label']),
+      'image/object/bbox/xmin':
+          dataset_utils.float_list_feature(xmin),
+      'image/object/bbox/xmax':
+          dataset_utils.float_list_feature(xmax),
+      'image/object/bbox/ymin':
+          dataset_utils.float_list_feature(ymin),
+      'image/object/bbox/ymax':
+          dataset_utils.float_list_feature(ymax),
+      'image/object/area':
+          dataset_utils.float_list_feature(area),
+  }
+  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
+  return example
diff --git a/research/slim/datasets/download_mscoco.sh b/research/slim/datasets/download_mscoco.sh
deleted file mode 100755
index 333d787f..00000000
--- a/research/slim/datasets/download_mscoco.sh
+++ /dev/null
@@ -1,76 +0,0 @@
-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-# Script to download the COCO dataset. See
-# http://cocodataset.org/#overview for an overview of the dataset.
-#
-# usage:
-#  bash datasets/download_mscoco.sh path-to-COCO-dataset
-#
-set -e
-
-if [ -z "$1" ]; then
-  echo "usage download_mscoco.sh [data dir]"
-  exit
-fi
-
-if [ "$(uname)" == "Darwin" ]; then
-  UNZIP="tar -xf"
-else
-  UNZIP="unzip -nq"
-fi
-
-# Create the output directories.
-OUTPUT_DIR="${1%/}"
-SCRATCH_DIR="${OUTPUT_DIR}/raw-data"
-mkdir -p "${OUTPUT_DIR}"
-mkdir -p "${SCRATCH_DIR}"
-CURRENT_DIR=$(pwd)
-
-# Helper function to download and unpack a .zip file.
-function download_and_unzip() {
-  local BASE_URL=${1}
-  local FILENAME=${2}
-
-  if [ ! -f ${FILENAME} ]; then
-    echo "Downloading ${FILENAME} to $(pwd)"
-    wget -nd -c "${BASE_URL}/${FILENAME}"
-  else
-    echo "Skipping download of ${FILENAME}"
-  fi
-  echo "Unzipping ${FILENAME}"
-  ${UNZIP} ${FILENAME}
-}
-
-cd ${SCRATCH_DIR}
-
-# Download the images.
-BASE_IMAGE_URL="http://images.cocodataset.org/zips"
-
-TRAIN_IMAGE_FILE="train2014.zip"
-download_and_unzip ${BASE_IMAGE_URL} ${TRAIN_IMAGE_FILE}
-TRAIN_IMAGE_DIR="${SCRATCH_DIR}/train2014"
-
-VAL_IMAGE_FILE="val2014.zip"
-download_and_unzip ${BASE_IMAGE_URL} ${VAL_IMAGE_FILE}
-VAL_IMAGE_DIR="${SCRATCH_DIR}/val2014"
-
-
-# Download the annotations.
-BASE_INSTANCES_URL="http://images.cocodataset.org/annotations"
-INSTANCES_FILE="annotations_trainval2014.zip"
-download_and_unzip ${BASE_INSTANCES_URL} ${INSTANCES_FILE}
-
-
diff --git a/research/slim/datasets/flowers.py b/research/slim/datasets/flowers.py
index a2e4b2e0..afbfc13b 100644
--- a/research/slim/datasets/flowers.py
+++ b/research/slim/datasets/flowers.py
@@ -24,10 +24,11 @@ from __future__ import print_function
 
 import os
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 _FILE_PATTERN = 'flowers_%s_*.tfrecord'
 
diff --git a/research/slim/datasets/imagenet.py b/research/slim/datasets/imagenet.py
index a505953a..9b648a2d 100644
--- a/research/slim/datasets/imagenet.py
+++ b/research/slim/datasets/imagenet.py
@@ -35,10 +35,11 @@ from __future__ import print_function
 import os
 from six.moves import urllib
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 # TODO(nsilberman): Add tfrecord file type once the script is updated.
 _FILE_PATTERN = '%s-*'
diff --git a/research/slim/datasets/mnist.py b/research/slim/datasets/mnist.py
index b1c81eb7..faa6799f 100644
--- a/research/slim/datasets/mnist.py
+++ b/research/slim/datasets/mnist.py
@@ -24,10 +24,11 @@ from __future__ import print_function
 
 import os
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 _FILE_PATTERN = 'mnist_%s.tfrecord'
 
diff --git a/research/slim/datasets/visualwakewords.py b/research/slim/datasets/visualwakewords.py
index 39b0ae22..41d4e673 100644
--- a/research/slim/datasets/visualwakewords.py
+++ b/research/slim/datasets/visualwakewords.py
@@ -29,11 +29,12 @@ from __future__ import print_function
 
 import os
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_utils
 
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 _FILE_PATTERN = '%s.record-*'
 
@@ -47,7 +48,6 @@ _ITEMS_TO_DESCRIPTIONS = {
     'image': 'A color image of varying height and width.',
     'label': 'The label id of the image, an integer in {0, 1}',
     'object/bbox': 'A list of bounding boxes.',
-    'object/label': 'A list of labels, all objects belong to the same class.',
 }
 
 _NUM_CLASSES = 2
@@ -99,8 +99,6 @@ def get_split(split_name, dataset_dir, file_pattern=None, reader=None):
           tf.VarLenFeature(dtype=tf.float32),
       'image/object/bbox/ymax':
           tf.VarLenFeature(dtype=tf.float32),
-      'image/object/class/label':
-          tf.VarLenFeature(dtype=tf.int64),
   }
 
   items_to_handlers = {
@@ -111,8 +109,6 @@ def get_split(split_name, dataset_dir, file_pattern=None, reader=None):
       'object/bbox':
           slim.tfexample_decoder.BoundingBox(['ymin', 'xmin', 'ymax', 'xmax'],
                                              'image/object/bbox/'),
-      'object/label':
-          slim.tfexample_decoder.Tensor('image/object/class/label'),
   }
 
   decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,
diff --git a/research/slim/deployment/model_deploy.py b/research/slim/deployment/model_deploy.py
index 1918d510..ad5005a4 100644
--- a/research/slim/deployment/model_deploy.py
+++ b/research/slim/deployment/model_deploy.py
@@ -102,8 +102,9 @@ from __future__ import print_function
 import collections
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 __all__ = ['create_clones',
diff --git a/research/slim/deployment/model_deploy_test.py b/research/slim/deployment/model_deploy_test.py
index 780b691d..c9a1c07c 100644
--- a/research/slim/deployment/model_deploy_test.py
+++ b/research/slim/deployment/model_deploy_test.py
@@ -20,10 +20,13 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import slim as contrib_slim
 
 from deployment import model_deploy
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class DeploymentConfigTest(tf.test.TestCase):
@@ -508,9 +511,8 @@ class DeployTest(tf.test.TestCase):
 
       with tf.Session() as sess:
         sess.run(tf.global_variables_initializer())
-        moving_mean = tf.contrib.framework.get_variables_by_name(
-            'moving_mean')[0]
-        moving_variance = tf.contrib.framework.get_variables_by_name(
+        moving_mean = contrib_framework.get_variables_by_name('moving_mean')[0]
+        moving_variance = contrib_framework.get_variables_by_name(
             'moving_variance')[0]
         initial_loss = sess.run(model.total_loss)
         initial_mean, initial_variance = sess.run([moving_mean,
@@ -537,8 +539,8 @@ class DeployTest(tf.test.TestCase):
       # clone function creates a fully_connected layer with a regularizer loss.
       def ModelFn():
         inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)
-        reg = tf.contrib.layers.l2_regularizer(0.001)
-        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)
+        reg = contrib_layers.l2_regularizer(0.001)
+        contrib_layers.fully_connected(inputs, 30, weights_regularizer=reg)
 
       model = model_deploy.deploy(
           deploy_config, ModelFn,
@@ -556,8 +558,8 @@ class DeployTest(tf.test.TestCase):
       # clone function creates a fully_connected layer with a regularizer loss.
       def ModelFn():
         inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)
-        reg = tf.contrib.layers.l2_regularizer(0.001)
-        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)
+        reg = contrib_layers.l2_regularizer(0.001)
+        contrib_layers.fully_connected(inputs, 30, weights_regularizer=reg)
 
       # No optimizer here, it's an eval.
       model = model_deploy.deploy(deploy_config, ModelFn)
diff --git a/research/slim/download_and_convert_data.py b/research/slim/download_and_convert_data.py
index 924a3a46..e935780c 100644
--- a/research/slim/download_and_convert_data.py
+++ b/research/slim/download_and_convert_data.py
@@ -18,16 +18,21 @@ Usage:
 ```shell
 
 $ python download_and_convert_data.py \
-    --dataset_name=mnist \
-    --dataset_dir=/tmp/mnist
+    --dataset_name=flowers \
+    --dataset_dir=/tmp/flowers
 
 $ python download_and_convert_data.py \
     --dataset_name=cifar10 \
     --dataset_dir=/tmp/cifar10
 
 $ python download_and_convert_data.py \
-    --dataset_name=flowers \
-    --dataset_dir=/tmp/flowers
+    --dataset_name=mnist \
+    --dataset_dir=/tmp/mnist
+
+$ python download_and_convert_data.py \
+    --dataset_name=visualwakewords \
+    --dataset_dir=/tmp/visualwakewords
+
 ```
 """
 from __future__ import absolute_import
@@ -39,19 +44,31 @@ import tensorflow as tf
 from datasets import download_and_convert_cifar10
 from datasets import download_and_convert_flowers
 from datasets import download_and_convert_mnist
+from datasets import download_and_convert_visualwakewords
 
-FLAGS = tf.app.flags.FLAGS
+FLAGS = tf.compat.v1.app.flags.FLAGS
 
-tf.app.flags.DEFINE_string(
+tf.compat.v1.app.flags.DEFINE_string(
     'dataset_name',
     None,
-    'The name of the dataset to convert, one of "cifar10", "flowers", "mnist".')
+    'The name of the dataset to convert, one of "flowers", "cifar10", "mnist", "visualwakewords"'
+    )
 
-tf.app.flags.DEFINE_string(
+tf.compat.v1.app.flags.DEFINE_string(
     'dataset_dir',
     None,
     'The directory where the output TFRecords and temporary files are saved.')
 
+tf.flags.DEFINE_float(
+    'small_object_area_threshold', 0.005,
+    'For --dataset_name=visualwakewords only. Threshold of fraction of image '
+    'area below which small objects are filtered')
+
+tf.flags.DEFINE_string(
+    'foreground_class_of_interest', 'person',
+    'For --dataset_name=visualwakewords only. Build a binary classifier based '
+    'on the presence or absence of this object in the image.')
+
 
 def main(_):
   if not FLAGS.dataset_name:
@@ -59,15 +76,19 @@ def main(_):
   if not FLAGS.dataset_dir:
     raise ValueError('You must supply the dataset directory with --dataset_dir')
 
-  if FLAGS.dataset_name == 'cifar10':
-    download_and_convert_cifar10.run(FLAGS.dataset_dir)
-  elif FLAGS.dataset_name == 'flowers':
+  if FLAGS.dataset_name == 'flowers':
     download_and_convert_flowers.run(FLAGS.dataset_dir)
+  elif FLAGS.dataset_name == 'cifar10':
+    download_and_convert_cifar10.run(FLAGS.dataset_dir)
   elif FLAGS.dataset_name == 'mnist':
     download_and_convert_mnist.run(FLAGS.dataset_dir)
+  elif FLAGS.dataset_name == 'visualwakewords':
+    download_and_convert_visualwakewords.run(
+        FLAGS.dataset_dir, FLAGS.small_object_area_threshold,
+        FLAGS.foreground_class_of_interest)
   else:
     raise ValueError(
         'dataset_name [%s] was not recognized.' % FLAGS.dataset_name)
 
 if __name__ == '__main__':
-  tf.app.run()
+  tf.compat.v1.app.run()
diff --git a/research/slim/eval_image_classifier.py b/research/slim/eval_image_classifier.py
index 7f830bdc..12ea9c0f 100644
--- a/research/slim/eval_image_classifier.py
+++ b/research/slim/eval_image_classifier.py
@@ -20,12 +20,14 @@ from __future__ import print_function
 
 import math
 import tensorflow as tf
+from tensorflow.contrib import quantize as contrib_quantize
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import nets_factory
 from preprocessing import preprocessing_factory
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 tf.app.flags.DEFINE_integer(
     'batch_size', 100, 'The number of samples in each batch.')
@@ -146,7 +148,7 @@ def main(_):
     logits, _ = network_fn(images)
 
     if FLAGS.quantize:
-      tf.contrib.quantize.create_eval_graph()
+      contrib_quantize.create_eval_graph()
 
     if FLAGS.moving_average_decay:
       variable_averages = tf.train.ExponentialMovingAverage(
diff --git a/research/slim/export_inference_graph.py b/research/slim/export_inference_graph.py
index 431dc99c..ba5fb1de 100644
--- a/research/slim/export_inference_graph.py
+++ b/research/slim/export_inference_graph.py
@@ -58,13 +58,15 @@ from __future__ import print_function
 import os
 
 import tensorflow as tf
+from tensorflow.contrib import quantize as contrib_quantize
+from tensorflow.contrib import slim as contrib_slim
 
 from tensorflow.python.platform import gfile
 from datasets import dataset_factory
 from nets import nets_factory
 
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 tf.app.flags.DEFINE_string(
     'model_name', 'inception_v3', 'The name of the architecture to save.')
@@ -144,7 +146,7 @@ def main(_):
     network_fn(placeholder)
 
     if FLAGS.quantize:
-      tf.contrib.quantize.create_eval_graph()
+      contrib_quantize.create_eval_graph()
 
     graph_def = graph.as_graph_def()
     if FLAGS.write_text_graphdef:
diff --git a/research/slim/nets/alexnet.py b/research/slim/nets/alexnet.py
index e860a6cb..a2ea6ee6 100644
--- a/research/slim/nets/alexnet.py
+++ b/research/slim/nets/alexnet.py
@@ -37,8 +37,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 
 
diff --git a/research/slim/nets/alexnet_test.py b/research/slim/nets/alexnet_test.py
index 6f85d33f..15f003a2 100644
--- a/research/slim/nets/alexnet_test.py
+++ b/research/slim/nets/alexnet_test.py
@@ -18,10 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import alexnet
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class AlexnetV2Test(tf.test.TestCase):
diff --git a/research/slim/nets/cifarnet.py b/research/slim/nets/cifarnet.py
index 97ed944b..2e151c88 100644
--- a/research/slim/nets/cifarnet.py
+++ b/research/slim/nets/cifarnet.py
@@ -19,8 +19,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)
 
diff --git a/research/slim/nets/cyclegan.py b/research/slim/nets/cyclegan.py
index 4f9936fa..dc87bd0b 100644
--- a/research/slim/nets/cyclegan.py
+++ b/research/slim/nets/cyclegan.py
@@ -20,8 +20,11 @@ from __future__ import print_function
 import numpy as np
 from six.moves import xrange  # pylint: disable=redefined-builtin
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import util as contrib_util
 
-layers = tf.contrib.layers
+layers = contrib_layers
 
 
 def cyclegan_arg_scope(instance_norm_center=True,
@@ -54,7 +57,7 @@ def cyclegan_arg_scope(instance_norm_center=True,
   if weight_decay and weight_decay > 0.0:
     weights_regularizer = layers.l2_regularizer(weight_decay)
 
-  with tf.contrib.framework.arg_scope(
+  with contrib_framework.arg_scope(
       [layers.conv2d],
       normalizer_fn=layers.instance_norm,
       normalizer_params=instance_norm_params,
@@ -124,7 +127,7 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
 
 def _dynamic_or_static_shape(tensor):
   shape = tf.shape(tensor)
-  static_shape = tf.contrib.util.constant_value(shape)
+  static_shape = contrib_util.constant_value(shape)
   return static_shape if static_shape is not None else shape
 
 
@@ -196,7 +199,7 @@ def cyclegan_generator_resnet(images,
       dtype=np.int32)
   spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])
 
-  with tf.contrib.framework.arg_scope(arg_scope_fn()):
+  with contrib_framework.arg_scope(arg_scope_fn()):
 
     ###########
     # Encoder #
@@ -208,12 +211,11 @@ def cyclegan_generator_resnet(images,
       end_points['encoder_0'] = net
 
     with tf.variable_scope('encoder'):
-      with tf.contrib.framework.arg_scope(
-          [layers.conv2d],
-          kernel_size=kernel_size,
-          stride=2,
-          activation_fn=tf.nn.relu,
-          padding='VALID'):
+      with contrib_framework.arg_scope([layers.conv2d],
+                                       kernel_size=kernel_size,
+                                       stride=2,
+                                       activation_fn=tf.nn.relu,
+                                       padding='VALID'):
 
         net = tf.pad(net, paddings, 'REFLECT')
         net = layers.conv2d(net, num_filters * 2)
@@ -226,12 +228,11 @@ def cyclegan_generator_resnet(images,
     # Residual Blocks #
     ###################
     with tf.variable_scope('residual_blocks'):
-      with tf.contrib.framework.arg_scope(
-          [layers.conv2d],
-          kernel_size=kernel_size,
-          stride=1,
-          activation_fn=tf.nn.relu,
-          padding='VALID'):
+      with contrib_framework.arg_scope([layers.conv2d],
+                                       kernel_size=kernel_size,
+                                       stride=1,
+                                       activation_fn=tf.nn.relu,
+                                       padding='VALID'):
         for block_id in xrange(num_resnet_blocks):
           with tf.variable_scope('block_{}'.format(block_id)):
             res_net = tf.pad(net, paddings, 'REFLECT')
@@ -248,11 +249,10 @@ def cyclegan_generator_resnet(images,
     ###########
     with tf.variable_scope('decoder'):
 
-      with tf.contrib.framework.arg_scope(
-          [layers.conv2d],
-          kernel_size=kernel_size,
-          stride=1,
-          activation_fn=tf.nn.relu):
+      with contrib_framework.arg_scope([layers.conv2d],
+                                       kernel_size=kernel_size,
+                                       stride=1,
+                                       activation_fn=tf.nn.relu):
 
         with tf.variable_scope('decoder1'):
           net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])
diff --git a/research/slim/nets/dcgan.py b/research/slim/nets/dcgan.py
index b13ba1e6..d1461f58 100644
--- a/research/slim/nets/dcgan.py
+++ b/research/slim/nets/dcgan.py
@@ -21,8 +21,9 @@ from math import log
 
 from six.moves import xrange  # pylint: disable=redefined-builtin
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def _validate_image_inputs(inputs):
diff --git a/research/slim/nets/i3d.py b/research/slim/nets/i3d.py
index 97fe4f2e..08a9fa43 100644
--- a/research/slim/nets/i3d.py
+++ b/research/slim/nets/i3d.py
@@ -25,11 +25,12 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import i3d_utils
 from nets import s3dg
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 conv3d_spatiotemporal = i3d_utils.conv3d_spatiotemporal
 
diff --git a/research/slim/nets/i3d_utils.py b/research/slim/nets/i3d_utils.py
index 2b9911e8..c843bb50 100644
--- a/research/slim/nets/i3d_utils.py
+++ b/research/slim/nets/i3d_utils.py
@@ -20,12 +20,14 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
 
 
 # Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to
 # more update-to-date tf.contrib.* API.
-add_arg_scope = tf.contrib.framework.add_arg_scope
-layers = tf.contrib.layers
+add_arg_scope = contrib_framework.add_arg_scope
+layers = contrib_layers
 
 
 def center_initializer():
diff --git a/research/slim/nets/inception_resnet_v2.py b/research/slim/nets/inception_resnet_v2.py
index f14c08ef..33c19163 100644
--- a/research/slim/nets/inception_resnet_v2.py
+++ b/research/slim/nets/inception_resnet_v2.py
@@ -26,8 +26,9 @@ from __future__ import print_function
 
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
diff --git a/research/slim/nets/inception_resnet_v2_test.py b/research/slim/nets/inception_resnet_v2_test.py
index 833ada5e..7ee633b2 100644
--- a/research/slim/nets/inception_resnet_v2_test.py
+++ b/research/slim/nets/inception_resnet_v2_test.py
@@ -18,6 +18,7 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception
 
@@ -310,7 +311,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     inputs = tf.placeholder(tf.float32, (1, height, width, 3))
-    with tf.contrib.slim.arg_scope(inception.inception_resnet_v2_arg_scope()):
+    with contrib_slim.arg_scope(inception.inception_resnet_v2_arg_scope()):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
     self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
@@ -319,7 +320,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     inputs = tf.placeholder(tf.float32, (1, height, width, 3))
-    with tf.contrib.slim.arg_scope(
+    with contrib_slim.arg_scope(
         inception.inception_resnet_v2_arg_scope(batch_norm_scale=True)):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
diff --git a/research/slim/nets/inception_utils.py b/research/slim/nets/inception_utils.py
index a8c8fc14..b9cedd38 100644
--- a/research/slim/nets/inception_utils.py
+++ b/research/slim/nets/inception_utils.py
@@ -25,8 +25,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def inception_arg_scope(weight_decay=0.00004,
diff --git a/research/slim/nets/inception_v1.py b/research/slim/nets/inception_v1.py
index 83bb55d9..f17ebb1a 100644
--- a/research/slim/nets/inception_v1.py
+++ b/research/slim/nets/inception_v1.py
@@ -19,10 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 
 
diff --git a/research/slim/nets/inception_v1_test.py b/research/slim/nets/inception_v1_test.py
index 5155d8f3..617265d7 100644
--- a/research/slim/nets/inception_v1_test.py
+++ b/research/slim/nets/inception_v1_test.py
@@ -20,10 +20,11 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class InceptionV1Test(tf.test.TestCase):
diff --git a/research/slim/nets/inception_v2.py b/research/slim/nets/inception_v2.py
index a391a6e1..2bae3fea 100644
--- a/research/slim/nets/inception_v2.py
+++ b/research/slim/nets/inception_v2.py
@@ -19,10 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 
 
diff --git a/research/slim/nets/inception_v2_test.py b/research/slim/nets/inception_v2_test.py
index 67def352..52b2f345 100644
--- a/research/slim/nets/inception_v2_test.py
+++ b/research/slim/nets/inception_v2_test.py
@@ -20,10 +20,11 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class InceptionV2Test(tf.test.TestCase):
diff --git a/research/slim/nets/inception_v3.py b/research/slim/nets/inception_v3.py
index 12217791..92896b5a 100644
--- a/research/slim/nets/inception_v3.py
+++ b/research/slim/nets/inception_v3.py
@@ -19,10 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 
 
diff --git a/research/slim/nets/inception_v3_test.py b/research/slim/nets/inception_v3_test.py
index 3d81aab3..86499b33 100644
--- a/research/slim/nets/inception_v3_test.py
+++ b/research/slim/nets/inception_v3_test.py
@@ -20,10 +20,11 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class InceptionV3Test(tf.test.TestCase):
diff --git a/research/slim/nets/inception_v4.py b/research/slim/nets/inception_v4.py
index bab406a8..c5cb7f36 100644
--- a/research/slim/nets/inception_v4.py
+++ b/research/slim/nets/inception_v4.py
@@ -25,10 +25,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def block_inception_a(inputs, scope=None, reuse=None):
diff --git a/research/slim/nets/inception_v4_test.py b/research/slim/nets/inception_v4_test.py
index 224933f8..eb717333 100644
--- a/research/slim/nets/inception_v4_test.py
+++ b/research/slim/nets/inception_v4_test.py
@@ -18,6 +18,7 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import inception
 
@@ -259,7 +260,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     inputs = tf.placeholder(tf.float32, (1, height, width, 3))
-    with tf.contrib.slim.arg_scope(inception.inception_v4_arg_scope()):
+    with contrib_slim.arg_scope(inception.inception_v4_arg_scope()):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
     self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
@@ -268,7 +269,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     inputs = tf.placeholder(tf.float32, (1, height, width, 3))
-    with tf.contrib.slim.arg_scope(
+    with contrib_slim.arg_scope(
         inception.inception_v4_arg_scope(batch_norm_scale=True)):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
diff --git a/research/slim/nets/lenet.py b/research/slim/nets/lenet.py
index c79dbfac..5d1d936a 100644
--- a/research/slim/nets/lenet.py
+++ b/research/slim/nets/lenet.py
@@ -19,8 +19,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def lenet(images, num_classes=10, is_training=False,
diff --git a/research/slim/nets/mobilenet/README.md b/research/slim/nets/mobilenet/README.md
index 8b093f98..bfdc1d94 100644
--- a/research/slim/nets/mobilenet/README.md
+++ b/research/slim/nets/mobilenet/README.md
@@ -1,4 +1,4 @@
-# MobilenNet
+# MobileNet
 
 This folder contains building code for
 [MobileNetV2](https://arxiv.org/abs/1801.04381) and
@@ -8,6 +8,12 @@ definition for each model is located in [mobilenet_v2.py](mobilenet_v2.py) and
 
 For MobilenetV1 please refer to this [page](../mobilenet_v1.md)
 
+We have also introduced a family of MobileNets customized for the Edge TPU
+accelerator found in
+[Google Pixel4](https://blog.google/products/pixel/pixel-4/) devices. The
+architectural definition for MobileNetEdgeTPU is located in
+[mobilenet_v3.py](mobilenet_v3.py)
+
 ## Performance
 
 ### Mobilenet V3 latency
@@ -33,6 +39,14 @@ numbers. We estimate it to be comparable to MobileNetV2 numbers.
 
 ![madds_top1_accuracy](g3doc/madds_top1_accuracy.png)
 
+### Mobilenet EdgeTPU latency
+
+The figure below shows the Pixel 4 Edge TPU latency of int8-quantized Mobilenet
+EdgeTPU compared with MobilenetV2 and the minimalistic variants of MobilenetV3
+(see below).
+
+![Mobilenet Edge TPU latency for Pixel 4 Edge TPU.png](g3doc/edgetpu_latency.png)
+
 ## Pretrained models
 
 ### Mobilenet V3 Imagenet Checkpoints
@@ -43,7 +57,7 @@ large and small models this page also contains so-called minimalistic models,
 these models have the same per-layer dimensions characteristic as MobilenetV3
 however, they don't utilize any of the advanced blocks (squeeze-and-excite
 units, hard-swish, and 5x5 convolutions). While these models are less efficient
-on CPU, we find that they are much more performant on GPU/DSP/EdgeTpu.
+on CPU, we find that they are much more performant on GPU/DSP.
 
 | Imagenet Checkpoint | MACs (M) | Params (M) | Top1 | Pixel 1 | Pixel 2 | Pixel 3 |
 | ------------------ | -------- | ---------- | ---- | ------- | ------- | ------- |
@@ -62,6 +76,20 @@ on CPU, we find that they are much more performant on GPU/DSP/EdgeTpu.
 | [Large minimalistic (8-bit)][lm8]   | 209      | 3.9        | 71.3 | 37      | 35      | 27      |
 | [Small minimalistic (float)]   | 65       | 2.0        | 61.9 | 12.2    | 15.1    | 11      |
 
+#### Edge TPU checkpoints:
+
+| Imagenet          | MACs (M) | Params (M) | Top1 | Pixel 4 | Pixel 4 CPU |
+: Checkpoint        :          :            :      : EdgeTPU :             :
+| ----------------- | -------- | ---------- | ---- | ------- | ----------- |
+| [MobilenetEdgeTPU | 624      | 2.9        | 73.5 | 3.1     | 13.8        |
+: dm=0.75 (8-bit)]  :          :            :      :         :             :
+| [MobilenetEdgeTPU | 990      | 4.0        | 75.6 | 3.6     | 20.6        |
+: dm=1 (8-bit)]     :          :            :      :         :             :
+
+Note: 8-bit quantized versions of the MobilenetEdgeTPU models were obtained
+using Tensorflow Lite's
+[post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
+tool.
 
 [Small minimalistic (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small-minimalistic_224_1.0_float.tgz
 [Large minimalistic (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large-minimalistic_224_1.0_float.tgz
@@ -72,6 +100,8 @@ on CPU, we find that they are much more performant on GPU/DSP/EdgeTpu.
 [Small dm=1 (8-bit)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_1.0_uint8.tgz
 [Large dm=0.75 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-large_224_0.75_float.tgz
 [Small dm=0.75 (float)]: https://storage.googleapis.com/mobilenet_v3/checkpoints/v3-small_224_0.75_float.tgz
+[MobilenetEdgeTPU dm=0.75 (8-bit)]: https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_0.75.tgz
+[MobilenetEdgeTPU dm=1 (8-bit)]: https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_1.0.tgz
 
 ### Mobilenet V2 Imagenet Checkpoints
 
diff --git a/research/slim/nets/mobilenet/conv_blocks.py b/research/slim/nets/mobilenet/conv_blocks.py
index d4d431e7..61a7016a 100644
--- a/research/slim/nets/mobilenet/conv_blocks.py
+++ b/research/slim/nets/mobilenet/conv_blocks.py
@@ -17,8 +17,9 @@ import contextlib
 import functools
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def _fixed_padding(inputs, kernel_size, rate=1):
@@ -463,4 +464,3 @@ def squeeze_excite(input_tensor,
           align_corners=True)
     result = input_tensor * excite
   return result
-
diff --git a/research/slim/nets/mobilenet/g3doc/edgetpu_latency.png b/research/slim/nets/mobilenet/g3doc/edgetpu_latency.png
new file mode 100644
index 00000000..05ebc50e
Binary files /dev/null and b/research/slim/nets/mobilenet/g3doc/edgetpu_latency.png differ
diff --git a/research/slim/nets/mobilenet/mobilenet.py b/research/slim/nets/mobilenet/mobilenet.py
index 31e71846..67569043 100644
--- a/research/slim/nets/mobilenet/mobilenet.py
+++ b/research/slim/nets/mobilenet/mobilenet.py
@@ -23,9 +23,9 @@ import copy
 import os
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 @slim.add_arg_scope
diff --git a/research/slim/nets/mobilenet/mobilenet_v2.py b/research/slim/nets/mobilenet/mobilenet_v2.py
index d2d736a3..7a06fc24 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2.py
@@ -28,11 +28,13 @@ import copy
 import functools
 
 import tensorflow as tf
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import slim as contrib_slim
 
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
-slim = tf.contrib.slim
+slim = contrib_slim
 op = lib.op
 
 expand_input = ops.expand_input_by_factor
@@ -84,18 +86,18 @@ V2_DEF = dict(
 # Mobilenet v2 Definition with group normalization.
 V2_DEF_GROUP_NORM = copy.deepcopy(V2_DEF)
 V2_DEF_GROUP_NORM['defaults'] = {
-    (tf.contrib.slim.conv2d, tf.contrib.slim.fully_connected,
-     tf.contrib.slim.separable_conv2d): {
-        'normalizer_fn': tf.contrib.layers.group_norm,  # pylint: disable=C0330
+    (contrib_slim.conv2d, contrib_slim.fully_connected,
+     contrib_slim.separable_conv2d): {
+        'normalizer_fn': contrib_layers.group_norm,  # pylint: disable=C0330
         'activation_fn': tf.nn.relu6,  # pylint: disable=C0330
     },  # pylint: disable=C0330
     (ops.expanded_conv,): {
         'expansion_size': ops.expand_input_by_factor(6),
         'split_expansion': 1,
-        'normalizer_fn': tf.contrib.layers.group_norm,
+        'normalizer_fn': contrib_layers.group_norm,
         'residual': True
     },
-    (tf.contrib.slim.conv2d, tf.contrib.slim.separable_conv2d): {
+    (contrib_slim.conv2d, contrib_slim.separable_conv2d): {
         'padding': 'SAME'
     }
 }
@@ -213,7 +215,7 @@ def mobilenet_base_group_norm(input_tensor, depth_multiplier=1.0, **kwargs):
   """Creates base of the mobilenet (no pooling and no logits) ."""
   kwargs['conv_defs'] = V2_DEF_GROUP_NORM
   kwargs['conv_defs']['defaults'].update({
-      (tf.contrib.layers.group_norm,): {
+      (contrib_layers.group_norm,): {
           'groups': kwargs.pop('groups', 8)
       }
   })
diff --git a/research/slim/nets/mobilenet/mobilenet_v2_test.py b/research/slim/nets/mobilenet/mobilenet_v2_test.py
index c31d7737..820c0ab5 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2_test.py
@@ -19,12 +19,13 @@ from __future__ import division
 from __future__ import print_function
 import copy
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def find_ops(optype):
diff --git a/research/slim/nets/mobilenet/mobilenet_v3.py b/research/slim/nets/mobilenet/mobilenet_v3.py
index 3ea393c5..997ddaa4 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3.py
@@ -23,11 +23,12 @@ import functools
 import numpy as np
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
-slim = tf.contrib.slim
+slim = contrib_slim
 op = lib.op
 expand_input = ops.expand_input_by_factor
 
@@ -57,7 +58,7 @@ def reduce_to_1x1(input_tensor, default_size=7, **kwargs):
   return slim.avg_pool2d(input_tensor, kernel_size=k, **kwargs)
 
 
-def mbv3_op(ef, n, k, s=1, act=tf.nn.relu, se=None):
+def mbv3_op(ef, n, k, s=1, act=tf.nn.relu, se=None, **kwargs):
   """Defines a single Mobilenet V3 convolution block.
 
   Args:
@@ -67,14 +68,44 @@ def mbv3_op(ef, n, k, s=1, act=tf.nn.relu, se=None):
     s: stride
     act: activation function in inner layers
     se: squeeze excite function.
+    **kwargs: passed to expanded_conv
 
   Returns:
     An object (lib._Op) for inserting in conv_def, representing this operation.
   """
-  return op(ops.expanded_conv, expansion_size=expand_input(ef),
-            kernel_size=(k, k), stride=s, num_outputs=n,
-            inner_activation_fn=act,
-            expansion_transform=se)
+  return op(
+      ops.expanded_conv,
+      expansion_size=expand_input(ef),
+      kernel_size=(k, k),
+      stride=s,
+      num_outputs=n,
+      inner_activation_fn=act,
+      expansion_transform=se,
+      **kwargs)
+
+
+def mbv3_fused(ef, n, k, s=1, **kwargs):
+  """Defines a single Mobilenet V3 convolution block.
+
+  Args:
+    ef: expansion factor
+    n: number of output channels
+    k: stride of depthwise
+    s: stride
+    **kwargs: will be passed to mbv3_op
+
+  Returns:
+    An object (lib._Op) for inserting in conv_def, representing this operation.
+  """
+  expansion_fn = functools.partial(slim.conv2d, kernel_size=k, stride=s)
+  return mbv3_op(
+      ef,
+      n,
+      k=1,
+      s=s,
+      depthwise_location=None,
+      expansion_fn=expansion_fn,
+      **kwargs)
 
 
 mbv3_op_se = functools.partial(mbv3_op, se=_se4)
@@ -206,6 +237,38 @@ V3_SMALL_MINIMALISTIC = dict(
     ]))
 
 
+# EdgeTPU friendly variant of MobilenetV3 that uses fused convolutions
+# instead of depthwise in the early layers.
+V3_EDGETPU = dict(
+    defaults=dict(DEFAULTS),
+    spec=[
+        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=(3, 3)),
+        mbv3_fused(k=3, s=1, ef=1, n=16),
+        mbv3_fused(k=3, s=2, ef=8, n=32),
+        mbv3_fused(k=3, s=1, ef=4, n=32),
+        mbv3_fused(k=3, s=1, ef=4, n=32),
+        mbv3_fused(k=3, s=1, ef=4, n=32),
+        mbv3_fused(k=3, s=2, ef=8, n=48),
+        mbv3_fused(k=3, s=1, ef=4, n=48),
+        mbv3_fused(k=3, s=1, ef=4, n=48),
+        mbv3_fused(k=3, s=1, ef=4, n=48),
+        mbv3_op(k=3, s=2, ef=8, n=96),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=3, s=1, ef=8, n=96, residual=False),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=3, s=1, ef=4, n=96),
+        mbv3_op(k=5, s=2, ef=8, n=160),
+        mbv3_op(k=5, s=1, ef=4, n=160),
+        mbv3_op(k=5, s=1, ef=4, n=160),
+        mbv3_op(k=5, s=1, ef=4, n=160),
+        mbv3_op(k=3, s=1, ef=8, n=192),
+        op(slim.conv2d, stride=1, num_outputs=1280, kernel_size=(1, 1)),
+    ])
+
+
 @slim.add_arg_scope
 def mobilenet(input_tensor,
               num_classes=1001,
@@ -275,15 +338,26 @@ def mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):
       input_tensor, depth_multiplier=depth_multiplier, base_only=True, **kwargs)
 
 
-def wrapped_partial(func, *args, **kwargs):
-  partial_func = functools.partial(func, *args, **kwargs)
+def wrapped_partial(func, new_defaults=None,
+                    **kwargs):
+  """Partial function with new default parameters and updated docstring."""
+  if not new_defaults:
+    new_defaults = {}
+  def func_wrapper(*f_args, **f_kwargs):
+    new_kwargs = dict(new_defaults)
+    new_kwargs.update(f_kwargs)
+    return func(*f_args, **new_kwargs)
+  functools.update_wrapper(func_wrapper, func)
+  partial_func = functools.partial(func_wrapper, **kwargs)
   functools.update_wrapper(partial_func, func)
   return partial_func
 
 
 large = wrapped_partial(mobilenet, conv_defs=V3_LARGE)
 small = wrapped_partial(mobilenet, conv_defs=V3_SMALL)
-
+edge_tpu = wrapped_partial(mobilenet,
+                           new_defaults={'scope': 'MobilenetEdgeTPU'},
+                           conv_defs=V3_EDGETPU)
 
 # Minimalistic model that does not have Squeeze Excite blocks,
 # Hardswish, or 5x5 depthwise convolution.
diff --git a/research/slim/nets/mobilenet/mobilenet_v3_test.py b/research/slim/nets/mobilenet/mobilenet_v3_test.py
index 86c3cf57..a9656fc5 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3_test.py
@@ -42,6 +42,21 @@ class MobilenetV3Test(absltest.TestCase):
         conv_defs=mobilenet_v3.V3_SMALL)
     self.assertEqual(endpoints['layer_15'].shape, [1, 1, 1, 1024])
 
+  def testMobilenetEdgeTpu(self):
+    _, endpoints = mobilenet_v3.edge_tpu(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)))
+    self.assertIn('Inference mode is created by default',
+                  mobilenet_v3.edge_tpu.__doc__)
+    self.assertEqual(endpoints['layer_24'].shape, [1, 7, 7, 1280])
+    self.assertStartsWith(
+        endpoints['layer_24'].name, 'MobilenetEdgeTPU')
+
+  def testMobilenetEdgeTpuChangeScope(self):
+    _, endpoints = mobilenet_v3.edge_tpu(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)), scope='Scope')
+    self.assertStartsWith(
+        endpoints['layer_24'].name, 'Scope')
+
   def testMobilenetV3BaseOnly(self):
     result, endpoints = mobilenet_v3.mobilenet(
         tf.placeholder(tf.float32, (1, 224, 224, 3)),
diff --git a/research/slim/nets/mobilenet_v1.py b/research/slim/nets/mobilenet_v1.py
index 413ba200..f8cffd43 100644
--- a/research/slim/nets/mobilenet_v1.py
+++ b/research/slim/nets/mobilenet_v1.py
@@ -109,8 +109,10 @@ from collections import namedtuple
 import functools
 
 import tensorflow as tf
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 # Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
 # Conv defines 3x3 convolution layers
@@ -307,7 +309,7 @@ def mobilenet_v1(inputs,
                  min_depth=8,
                  depth_multiplier=1.0,
                  conv_defs=None,
-                 prediction_fn=tf.contrib.layers.softmax,
+                 prediction_fn=contrib_layers.softmax,
                  spatial_squeeze=True,
                  reuse=None,
                  scope='MobilenetV1',
@@ -461,7 +463,7 @@ def mobilenet_v1_arg_scope(
 
   # Set weight_decay for weights in Conv and DepthSepConv layers.
   weights_init = tf.truncated_normal_initializer(stddev=stddev)
-  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  regularizer = contrib_layers.l2_regularizer(weight_decay)
   if regularize_depthwise:
     depthwise_regularizer = regularizer
   else:
diff --git a/research/slim/nets/mobilenet_v1_eval.py b/research/slim/nets/mobilenet_v1_eval.py
index 5b42a382..97eb4110 100644
--- a/research/slim/nets/mobilenet_v1_eval.py
+++ b/research/slim/nets/mobilenet_v1_eval.py
@@ -20,12 +20,14 @@ from __future__ import print_function
 
 import math
 import tensorflow as tf
+from tensorflow.contrib import quantize as contrib_quantize
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import mobilenet_v1
 from preprocessing import preprocessing_factory
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 flags = tf.app.flags
 
@@ -124,7 +126,7 @@ def build_model():
           num_classes=FLAGS.num_classes)
 
     if FLAGS.quantize:
-      tf.contrib.quantize.create_eval_graph()
+      contrib_quantize.create_eval_graph()
 
     eval_ops = metrics(logits, labels)
 
diff --git a/research/slim/nets/mobilenet_v1_test.py b/research/slim/nets/mobilenet_v1_test.py
index 669a3ae3..fb43a440 100644
--- a/research/slim/nets/mobilenet_v1_test.py
+++ b/research/slim/nets/mobilenet_v1_test.py
@@ -20,10 +20,11 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import mobilenet_v1
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class MobilenetV1Test(tf.test.TestCase):
diff --git a/research/slim/nets/mobilenet_v1_train.py b/research/slim/nets/mobilenet_v1_train.py
index f8328aa3..a0ff7354 100644
--- a/research/slim/nets/mobilenet_v1_train.py
+++ b/research/slim/nets/mobilenet_v1_train.py
@@ -19,12 +19,14 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import quantize as contrib_quantize
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import mobilenet_v1
 from preprocessing import preprocessing_factory
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 flags = tf.app.flags
 
@@ -136,7 +138,7 @@ def build_model():
     # quant_delay delays start of quantization till quant_delay steps, allowing
     # for better model accuracy.
     if FLAGS.quantize:
-      tf.contrib.quantize.create_training_graph(quant_delay=get_quant_delay())
+      contrib_quantize.create_training_graph(quant_delay=get_quant_delay())
 
     total_loss = tf.losses.get_total_loss(name='total_loss')
     # Configure the learning rate using an exponential decay.
diff --git a/research/slim/nets/nasnet/nasnet.py b/research/slim/nets/nasnet/nasnet.py
index a5dc0dc1..b7807d39 100644
--- a/research/slim/nets/nasnet/nasnet.py
+++ b/research/slim/nets/nasnet/nasnet.py
@@ -22,11 +22,15 @@ from __future__ import print_function
 
 import copy
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import slim as contrib_slim
+from tensorflow.contrib import training as contrib_training
 
 from nets.nasnet import nasnet_utils
 
-arg_scope = tf.contrib.framework.arg_scope
-slim = tf.contrib.slim
+arg_scope = contrib_framework.arg_scope
+slim = contrib_slim
 
 
 # Notes for training NASNet Cifar Model
@@ -37,7 +41,7 @@ slim = tf.contrib.slim
 # auxiliary head loss weighting: 0.4
 # clip global norm of all gradients by 5
 def cifar_config():
-  return tf.contrib.training.HParams(
+  return contrib_training.HParams(
       stem_multiplier=3.0,
       drop_path_keep_prob=0.6,
       num_cells=18,
@@ -67,7 +71,7 @@ def cifar_config():
 # label smoothing: 0.1
 # clip global norm of all gradients by 10
 def large_imagenet_config():
-  return tf.contrib.training.HParams(
+  return contrib_training.HParams(
       stem_multiplier=3.0,
       dense_dropout_keep_prob=0.5,
       num_cells=18,
@@ -94,7 +98,7 @@ def large_imagenet_config():
 # label smoothing: 0.1
 # clip global norm of all gradients by 10
 def mobile_imagenet_config():
-  return tf.contrib.training.HParams(
+  return contrib_training.HParams(
       stem_multiplier=1.0,
       dense_dropout_keep_prob=0.5,
       num_cells=12,
@@ -138,8 +142,8 @@ def nasnet_cifar_arg_scope(weight_decay=5e-4,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
-  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
+  weights_initializer = contrib_layers.variance_scaling_initializer(
       mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
@@ -174,8 +178,8 @@ def nasnet_mobile_arg_scope(weight_decay=4e-5,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
-  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
+  weights_initializer = contrib_layers.variance_scaling_initializer(
       mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
@@ -210,8 +214,8 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
-  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
+  weights_initializer = contrib_layers.variance_scaling_initializer(
       mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
@@ -244,7 +248,7 @@ def _build_aux_head(net, end_points, num_classes, hparams, scope):
       aux_logits = slim.conv2d(aux_logits, 768, shape, padding='VALID')
       aux_logits = slim.batch_norm(aux_logits, scope='aux_bn1')
       aux_logits = activation_fn(aux_logits)
-      aux_logits = tf.contrib.layers.flatten(aux_logits)
+      aux_logits = contrib_layers.flatten(aux_logits)
       aux_logits = slim.fully_connected(aux_logits, num_classes)
       end_points['AuxLogits'] = aux_logits
 
diff --git a/research/slim/nets/nasnet/nasnet_test.py b/research/slim/nets/nasnet/nasnet_test.py
index d4383545..b9a63919 100644
--- a/research/slim/nets/nasnet/nasnet_test.py
+++ b/research/slim/nets/nasnet/nasnet_test.py
@@ -18,10 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets.nasnet import nasnet
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class NASNetTest(tf.test.TestCase):
diff --git a/research/slim/nets/nasnet/nasnet_utils.py b/research/slim/nets/nasnet/nasnet_utils.py
index caa39b91..163e14a8 100644
--- a/research/slim/nets/nasnet/nasnet_utils.py
+++ b/research/slim/nets/nasnet/nasnet_utils.py
@@ -32,10 +32,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import slim as contrib_slim
 
-
-arg_scope = tf.contrib.framework.arg_scope
-slim = tf.contrib.slim
+arg_scope = contrib_framework.arg_scope
+slim = contrib_slim
 
 DATA_FORMAT_NCHW = 'NCHW'
 DATA_FORMAT_NHWC = 'NHWC'
@@ -55,14 +56,14 @@ def calc_reduction_layers(num_cells, num_reduction_layers):
   return reduction_layers
 
 
-@tf.contrib.framework.add_arg_scope
+@contrib_framework.add_arg_scope
 def get_channel_index(data_format=INVALID):
   assert data_format != INVALID
   axis = 3 if data_format == 'NHWC' else 1
   return axis
 
 
-@tf.contrib.framework.add_arg_scope
+@contrib_framework.add_arg_scope
 def get_channel_dim(shape, data_format=INVALID):
   assert data_format != INVALID
   assert len(shape) == 4
@@ -74,7 +75,7 @@ def get_channel_dim(shape, data_format=INVALID):
     raise ValueError('Not a valid data_format', data_format)
 
 
-@tf.contrib.framework.add_arg_scope
+@contrib_framework.add_arg_scope
 def global_avg_pool(x, data_format=INVALID):
   """Average pool away the height and width spatial dimensions of x."""
   assert data_format != INVALID
@@ -86,7 +87,7 @@ def global_avg_pool(x, data_format=INVALID):
     return tf.reduce_mean(x, [2, 3])
 
 
-@tf.contrib.framework.add_arg_scope
+@contrib_framework.add_arg_scope
 def factorized_reduction(net, output_filters, stride, data_format=INVALID):
   """Reduces the shape of net without information loss due to striding."""
   assert data_format != INVALID
@@ -129,7 +130,7 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
   return final_path
 
 
-@tf.contrib.framework.add_arg_scope
+@contrib_framework.add_arg_scope
 def drop_path(net, keep_prob, is_training=True):
   """Drops out a whole example hiddenstate with the specified probability."""
   if is_training:
@@ -422,7 +423,7 @@ class NasNetABaseCell(object):
     net = tf.concat(values=states_to_combine, axis=concat_axis)
     return net
 
-  @tf.contrib.framework.add_arg_scope  # No public API. For internal use only.
+  @contrib_framework.add_arg_scope  # No public API. For internal use only.
   def _apply_drop_path(self, net, current_step=None,
                        use_summaries=False, drop_connect_version='v3'):
     """Apply drop_path regularization.
diff --git a/research/slim/nets/nasnet/pnasnet.py b/research/slim/nets/nasnet/pnasnet.py
index 8e8427fe..42d8b8f6 100644
--- a/research/slim/nets/nasnet/pnasnet.py
+++ b/research/slim/nets/nasnet/pnasnet.py
@@ -23,17 +23,20 @@ from __future__ import print_function
 
 import copy
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import slim as contrib_slim
+from tensorflow.contrib import training as contrib_training
 
 from nets.nasnet import nasnet
 from nets.nasnet import nasnet_utils
 
-arg_scope = tf.contrib.framework.arg_scope
-slim = tf.contrib.slim
+arg_scope = contrib_framework.arg_scope
+slim = contrib_slim
 
 
 def large_imagenet_config():
   """Large ImageNet configuration based on PNASNet-5."""
-  return tf.contrib.training.HParams(
+  return contrib_training.HParams(
       stem_multiplier=3.0,
       dense_dropout_keep_prob=0.5,
       num_cells=12,
@@ -51,7 +54,7 @@ def large_imagenet_config():
 
 def mobile_imagenet_config():
   """Mobile ImageNet configuration based on PNASNet-5."""
-  return tf.contrib.training.HParams(
+  return contrib_training.HParams(
       stem_multiplier=1.0,
       dense_dropout_keep_prob=0.5,
       num_cells=9,
diff --git a/research/slim/nets/nasnet/pnasnet_test.py b/research/slim/nets/nasnet/pnasnet_test.py
index 2cbd8162..34f7946d 100644
--- a/research/slim/nets/nasnet/pnasnet_test.py
+++ b/research/slim/nets/nasnet/pnasnet_test.py
@@ -18,10 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets.nasnet import pnasnet
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class PNASNetTest(tf.test.TestCase):
diff --git a/research/slim/nets/nets_factory.py b/research/slim/nets/nets_factory.py
index ac47376e..c425c364 100644
--- a/research/slim/nets/nets_factory.py
+++ b/research/slim/nets/nets_factory.py
@@ -18,8 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 import functools
-
-import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import alexnet
 from nets import cifarnet
@@ -37,7 +36,7 @@ from nets.nasnet import nasnet
 from nets.nasnet import pnasnet
 
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 networks_map = {'alexnet_v2': alexnet.alexnet_v2,
                 'cifarnet': cifarnet.cifarnet,
diff --git a/research/slim/nets/overfeat.py b/research/slim/nets/overfeat.py
index 069f550e..cdf14ded 100644
--- a/research/slim/nets/overfeat.py
+++ b/research/slim/nets/overfeat.py
@@ -32,8 +32,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
 
 
diff --git a/research/slim/nets/overfeat_test.py b/research/slim/nets/overfeat_test.py
index dab0039e..fb3cb770 100644
--- a/research/slim/nets/overfeat_test.py
+++ b/research/slim/nets/overfeat_test.py
@@ -18,10 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import overfeat
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class OverFeatTest(tf.test.TestCase):
diff --git a/research/slim/nets/pix2pix.py b/research/slim/nets/pix2pix.py
index 8c3a2e99..0b70f1c4 100644
--- a/research/slim/nets/pix2pix.py
+++ b/research/slim/nets/pix2pix.py
@@ -33,8 +33,10 @@ import collections
 import functools
 
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
 
-layers = tf.contrib.layers
+layers = contrib_layers
 
 
 def pix2pix_arg_scope():
@@ -52,7 +54,7 @@ def pix2pix_arg_scope():
       'epsilon': 0.00001,
   }
 
-  with tf.contrib.framework.arg_scope(
+  with contrib_framework.arg_scope(
       [layers.conv2d, layers.conv2d_transpose],
       normalizer_fn=layers.instance_norm,
       normalizer_params=instance_norm_params,
@@ -165,11 +167,10 @@ def pix2pix_generator(net,
   # Encoder #
   ###########
   with tf.variable_scope('encoder'):
-    with tf.contrib.framework.arg_scope(
-        [layers.conv2d],
-        kernel_size=[4, 4],
-        stride=2,
-        activation_fn=tf.nn.leaky_relu):
+    with contrib_framework.arg_scope([layers.conv2d],
+                                     kernel_size=[4, 4],
+                                     stride=2,
+                                     activation_fn=tf.nn.leaky_relu):
 
       for block_id, block in enumerate(blocks):
         # No normalizer for the first encoder layers as per 'Image-to-Image',
@@ -196,7 +197,7 @@ def pix2pix_generator(net,
   with tf.variable_scope('decoder'):
     # Dropout is used at both train and test time as per 'Image-to-Image',
     # Section 2.1 (last paragraph).
-    with tf.contrib.framework.arg_scope([layers.dropout], is_training=True):
+    with contrib_framework.arg_scope([layers.dropout], is_training=True):
 
       for block_id, block in enumerate(reversed_blocks):
         if block_id > 0:
@@ -256,12 +257,11 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
     else:
       return net
 
-  with tf.contrib.framework.arg_scope(
-      [layers.conv2d],
-      kernel_size=[4, 4],
-      stride=2,
-      padding='valid',
-      activation_fn=activation_fn):
+  with contrib_framework.arg_scope([layers.conv2d],
+                                   kernel_size=[4, 4],
+                                   stride=2,
+                                   padding='valid',
+                                   activation_fn=activation_fn):
 
     # No normalization on the input layer.
     net = layers.conv2d(
diff --git a/research/slim/nets/pix2pix_test.py b/research/slim/nets/pix2pix_test.py
index ab5acb5c..7e782cc0 100644
--- a/research/slim/nets/pix2pix_test.py
+++ b/research/slim/nets/pix2pix_test.py
@@ -19,6 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
 from nets import pix2pix
 
 
@@ -35,7 +36,7 @@ class GeneratorTest(tf.test.TestCase):
     num_outputs = 4
 
     images = tf.ones((batch_size, height, width, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, _ = pix2pix.pix2pix_generator(
           images, num_outputs, blocks=self._reduced_default_blocks(),
           upsample_method='nn_upsample_conv')
@@ -52,7 +53,7 @@ class GeneratorTest(tf.test.TestCase):
     num_outputs = 4
 
     images = tf.ones((batch_size, height, width, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, _ = pix2pix.pix2pix_generator(
           images, num_outputs, blocks=self._reduced_default_blocks(),
           upsample_method='conv2d_transpose')
@@ -73,7 +74,7 @@ class GeneratorTest(tf.test.TestCase):
         pix2pix.Block(64, 0.5),
         pix2pix.Block(128, 0),
     ]
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       _, end_points = pix2pix.pix2pix_generator(
           images, num_outputs, blocks)
 
@@ -105,7 +106,7 @@ class DiscriminatorTest(tf.test.TestCase):
     output_size = self._layer_output_size(output_size, stride=1)
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, end_points = pix2pix.pix2pix_discriminator(
           images, num_filters=[64, 128, 256, 512])
     self.assertListEqual([batch_size, output_size, output_size, 1],
@@ -124,7 +125,7 @@ class DiscriminatorTest(tf.test.TestCase):
     output_size = self._layer_output_size(output_size, stride=1, pad=0)
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, end_points = pix2pix.pix2pix_discriminator(
           images, num_filters=[64, 128, 256, 512], padding=0)
     self.assertListEqual([batch_size, output_size, output_size, 1],
@@ -137,7 +138,7 @@ class DiscriminatorTest(tf.test.TestCase):
     input_size = 256
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       with self.assertRaises(TypeError):
         pix2pix.pix2pix_discriminator(
             images, num_filters=[64, 128, 256, 512], padding=1.5)
@@ -147,7 +148,7 @@ class DiscriminatorTest(tf.test.TestCase):
     input_size = 256
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
       with self.assertRaises(ValueError):
         pix2pix.pix2pix_discriminator(
             images, num_filters=[64, 128, 256, 512], padding=-1)
diff --git a/research/slim/nets/resnet_utils.py b/research/slim/nets/resnet_utils.py
index b4fc0e0f..599d1d03 100644
--- a/research/slim/nets/resnet_utils.py
+++ b/research/slim/nets/resnet_utils.py
@@ -39,8 +39,9 @@ from __future__ import print_function
 
 import collections
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):
diff --git a/research/slim/nets/resnet_v1.py b/research/slim/nets/resnet_v1.py
index de002921..148a7217 100644
--- a/research/slim/nets/resnet_v1.py
+++ b/research/slim/nets/resnet_v1.py
@@ -57,12 +57,13 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import resnet_utils
 
 
 resnet_arg_scope = resnet_utils.resnet_arg_scope
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class NoOpScope(object):
diff --git a/research/slim/nets/resnet_v1_test.py b/research/slim/nets/resnet_v1_test.py
index 2d4e7124..91df7f4c 100644
--- a/research/slim/nets/resnet_v1_test.py
+++ b/research/slim/nets/resnet_v1_test.py
@@ -20,11 +20,12 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import resnet_utils
 from nets import resnet_v1
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def create_test_input(batch_size, height, width, channels):
diff --git a/research/slim/nets/resnet_v2.py b/research/slim/nets/resnet_v2.py
index c719c1bd..07a9e5a6 100644
--- a/research/slim/nets/resnet_v2.py
+++ b/research/slim/nets/resnet_v2.py
@@ -51,10 +51,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import resnet_utils
 
-slim = tf.contrib.slim
+slim = contrib_slim
 resnet_arg_scope = resnet_utils.resnet_arg_scope
 
 
diff --git a/research/slim/nets/resnet_v2_test.py b/research/slim/nets/resnet_v2_test.py
index 891816d1..7e39b184 100644
--- a/research/slim/nets/resnet_v2_test.py
+++ b/research/slim/nets/resnet_v2_test.py
@@ -20,11 +20,12 @@ from __future__ import print_function
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import resnet_utils
 from nets import resnet_v2
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def create_test_input(batch_size, height, width, channels):
diff --git a/research/slim/nets/s3dg.py b/research/slim/nets/s3dg.py
index a13375e2..9fa8b423 100644
--- a/research/slim/nets/s3dg.py
+++ b/research/slim/nets/s3dg.py
@@ -25,6 +25,8 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
 
 from nets import i3d_utils
 
@@ -34,8 +36,8 @@ inception_block_v1_3d = i3d_utils.inception_block_v1_3d
 
 # Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more
 # update-to-date tf.contrib.* API.
-arg_scope = tf.contrib.framework.arg_scope
-layers = tf.contrib.layers
+arg_scope = contrib_framework.arg_scope
+layers = contrib_layers
 
 
 def s3dg_arg_scope(weight_decay=1e-7,
diff --git a/research/slim/nets/vgg.py b/research/slim/nets/vgg.py
index f80a43bc..960931db 100644
--- a/research/slim/nets/vgg.py
+++ b/research/slim/nets/vgg.py
@@ -42,8 +42,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def vgg_arg_scope(weight_decay=0.0005):
diff --git a/research/slim/nets/vgg_test.py b/research/slim/nets/vgg_test.py
index 6760c368..3a2e0c0c 100644
--- a/research/slim/nets/vgg_test.py
+++ b/research/slim/nets/vgg_test.py
@@ -18,10 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import vgg
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class VGGATest(tf.test.TestCase):
diff --git a/research/slim/preprocessing/cifarnet_preprocessing.py b/research/slim/preprocessing/cifarnet_preprocessing.py
index 40953b8a..2f66e77b 100644
--- a/research/slim/preprocessing/cifarnet_preprocessing.py
+++ b/research/slim/preprocessing/cifarnet_preprocessing.py
@@ -21,10 +21,11 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 _PADDING = 4
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def preprocess_for_train(image,
diff --git a/research/slim/preprocessing/lenet_preprocessing.py b/research/slim/preprocessing/lenet_preprocessing.py
index abc71748..d5cdec9b 100644
--- a/research/slim/preprocessing/lenet_preprocessing.py
+++ b/research/slim/preprocessing/lenet_preprocessing.py
@@ -19,8 +19,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def preprocess_image(image,
diff --git a/research/slim/preprocessing/preprocessing_factory.py b/research/slim/preprocessing/preprocessing_factory.py
index 788ca9ce..bbb63f83 100644
--- a/research/slim/preprocessing/preprocessing_factory.py
+++ b/research/slim/preprocessing/preprocessing_factory.py
@@ -17,15 +17,14 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-
-import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from preprocessing import cifarnet_preprocessing
 from preprocessing import inception_preprocessing
 from preprocessing import lenet_preprocessing
 from preprocessing import vgg_preprocessing
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def get_preprocessing(name, is_training=False, use_grayscale=False):
diff --git a/research/slim/preprocessing/vgg_preprocessing.py b/research/slim/preprocessing/vgg_preprocessing.py
index 1fbb1764..ae1db145 100644
--- a/research/slim/preprocessing/vgg_preprocessing.py
+++ b/research/slim/preprocessing/vgg_preprocessing.py
@@ -33,8 +33,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 _R_MEAN = 123.68
 _G_MEAN = 116.78
diff --git a/research/slim/train_image_classifier.py b/research/slim/train_image_classifier.py
index 79cf2343..388948a1 100644
--- a/research/slim/train_image_classifier.py
+++ b/research/slim/train_image_classifier.py
@@ -19,13 +19,15 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
+from tensorflow.contrib import quantize as contrib_quantize
+from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from deployment import model_deploy
 from nets import nets_factory
 from preprocessing import preprocessing_factory
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 tf.app.flags.DEFINE_string(
     'master', '', 'The address of the TensorFlow master to use.')
@@ -521,8 +523,7 @@ def main(_):
       moving_average_variables, variable_averages = None, None
 
     if FLAGS.quantize_delay >= 0:
-      tf.contrib.quantize.create_training_graph(
-          quant_delay=FLAGS.quantize_delay)
+      contrib_quantize.create_training_graph(quant_delay=FLAGS.quantize_delay)
 
     #########################################
     # Configure the optimization procedure. #
