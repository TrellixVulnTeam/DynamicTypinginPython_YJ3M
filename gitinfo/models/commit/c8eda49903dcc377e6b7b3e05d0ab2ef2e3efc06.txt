commit c8eda49903dcc377e6b7b3e05d0ab2ef2e3efc06
Author: Mark Daoust <markdaoust@google.com>
Date:   Wed Jul 11 15:19:45 2018 -0700

    Converted `wide.md` to notebook using `notedown`
    
    notedown wide.md --to notebook --output wide.ipynb

diff --git a/samples/core/tutorials/estimators/wide.ipynb b/samples/core/tutorials/estimators/wide.ipynb
new file mode 100644
index 00000000..c4ec1bf2
--- /dev/null
+++ b/samples/core/tutorials/estimators/wide.ipynb
@@ -0,0 +1,629 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# TensorFlow Linear Model Tutorial\n",
+    "\n",
+    "In this tutorial, we will use the tf.estimator API in TensorFlow to solve a\n",
+    "binary classification problem: Given census data about a person such as age,\n",
+    "education, marital status, and occupation (the features), we will try to predict\n",
+    "whether or not the person earns more than 50,000 dollars a year (the target\n",
+    "label). We will train a **logistic regression** model, and given an individual's\n",
+    "information our model will output a number between 0 and 1, which can be\n",
+    "interpreted as the probability that the individual has an annual income of over\n",
+    "50,000 dollars.\n",
+    "\n",
+    "## Setup\n",
+    "\n",
+    "To try the code for this tutorial:\n",
+    "\n",
+    "1.  @{$install$Install TensorFlow} if you haven't already.\n",
+    "\n",
+    "2.  Download [the tutorial code](https://github.com/tensorflow/models/tree/master/official/wide_deep/).\n",
+    "\n",
+    "3. Execute the data download script we provide to you:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "$ python data_download.py"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "4. Execute the tutorial code with the following command to train the linear\n",
+    "model described in this tutorial:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "$ python wide_deep.py --model_type=wide"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Read on to find out how this code builds its linear model.\n",
+    "\n",
+    "## Reading The Census Data\n",
+    "\n",
+    "The dataset we'll be using is the\n",
+    "[Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income).\n",
+    "We have provided\n",
+    "[data_download.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/data_download.py)\n",
+    "which downloads the code and performs some additional cleanup.\n",
+    "\n",
+    "Since the task is a binary classification problem, we'll construct a label\n",
+    "column named \"label\" whose value is 1 if the income is over 50K, and 0\n",
+    "otherwise. For reference, see `input_fn` in\n",
+    "[wide_deep.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/wide_deep.py).\n",
+    "\n",
+    "Next, let's take a look at the dataframe and see which columns we can use to\n",
+    "predict the target label. The columns can be grouped into two typesâ€”categorical\n",
+    "and continuous columns:\n",
+    "\n",
+    "*   A column is called **categorical** if its value can only be one of the\n",
+    "    categories in a finite set. For example, the relationship status of a person\n",
+    "    (wife, husband, unmarried, etc.) or the education level (high school,\n",
+    "    college, etc.) are categorical columns.\n",
+    "*   A column is called **continuous** if its value can be any numerical value in\n",
+    "    a continuous range. For example, the capital gain of a person (e.g. $14,084)\n",
+    "    is a continuous column.\n",
+    "\n",
+    "Here's a list of columns available in the Census Income dataset:\n",
+    "\n",
+    "| Column Name    | Type        | Description                       |\n",
+    "| -------------- | ----------- | --------------------------------- |\n",
+    "| age            | Continuous  | The age of the individual         |\n",
+    "| workclass      | Categorical | The type of employer the          |\n",
+    ":                :             : individual has (government,       :\n",
+    ":                :             : military, private, etc.).         :\n",
+    "| fnlwgt         | Continuous  | The number of people the census   |\n",
+    ":                :             : takers believe that observation   :\n",
+    ":                :             : represents (sample weight). Final :\n",
+    ":                :             : weight will not be used.          :\n",
+    "| education      | Categorical | The highest level of education    |\n",
+    ":                :             : achieved for that individual.     :\n",
+    "| education_num  | Continuous  | The highest level of education in |\n",
+    ":                :             : numerical form.                   :\n",
+    "| marital_status | Categorical | Marital status of the individual. |\n",
+    "| occupation     | Categorical | The occupation of the individual. |\n",
+    "| relationship   | Categorical | Wife, Own-child, Husband,         |\n",
+    ":                :             : Not-in-family, Other-relative,    :\n",
+    ":                :             : Unmarried.                        :\n",
+    "| race           | Categorical | Amer-Indian-Eskimo, Asian-Pac-    |\n",
+    ":                :             : Islander, Black, White, Other.    :\n",
+    "| gender         | Categorical | Female, Male.                     |\n",
+    "| capital_gain   | Continuous  | Capital gains recorded.           |\n",
+    "| capital_loss   | Continuous  | Capital Losses recorded.          |\n",
+    "| hours_per_week | Continuous  | Hours worked per week.            |\n",
+    "| native_country | Categorical | Country of origin of the          |\n",
+    ":                :             : individual.                       :\n",
+    "| income_bracket | Categorical | \">50K\" or \"<=50K\", meaning        |\n",
+    ":                :             : whether the person makes more     :\n",
+    ":                :             : than $50,000 annually.            :\n",
+    "\n",
+    "## Converting Data into Tensors\n",
+    "\n",
+    "When building a tf.estimator model, the input data is specified by means of an\n",
+    "Input Builder function. This builder function will not be called until it is\n",
+    "later passed to tf.estimator.Estimator methods such as `train` and `evaluate`.\n",
+    "The purpose of this function is to construct the input data, which is\n",
+    "represented in the form of @{tf.Tensor}s or @{tf.SparseTensor}s.\n",
+    "In more detail, the input builder function returns the following as a pair:\n",
+    "\n",
+    "1.  `features`: A dict from feature column names to `Tensors` or\n",
+    "    `SparseTensors`.\n",
+    "2.  `labels`: A `Tensor` containing the label column.\n",
+    "\n",
+    "The keys of the `features` will be used to construct columns in the next\n",
+    "section. Because we want to call the `train` and `evaluate` methods with\n",
+    "different data, we define a method that returns an input function based on the\n",
+    "given data. Note that the returned input function will be called while\n",
+    "constructing the TensorFlow graph, not while running the graph. What it is\n",
+    "returning is a representation of the input data as the fundamental unit of\n",
+    "TensorFlow computations, a `Tensor` (or `SparseTensor`).\n",
+    "\n",
+    "Each continuous column in the train or test data will be converted into a\n",
+    "`Tensor`, which in general is a good format to represent dense data. For\n",
+    "categorical data, we must represent the data as a `SparseTensor`. This data\n",
+    "format is good for representing sparse data. Our `input_fn` uses the `tf.data`\n",
+    "API, which makes it easy to apply transformations to our dataset:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
+    "  \"\"\"Generate an input function for the Estimator.\"\"\"\n",
+    "  assert tf.gfile.Exists(data_file), (\n",
+    "      '%s not found. Please make sure you have either run data_download.py or '\n",
+    "      'set both arguments --train_data and --test_data.' % data_file)\n",
+    "\n",
+    "  def parse_csv(value):\n",
+    "    print('Parsing', data_file)\n",
+    "    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
+    "    features = dict(zip(_CSV_COLUMNS, columns))\n",
+    "    labels = features.pop('income_bracket')\n",
+    "    return features, tf.equal(labels, '>50K')\n",
+    "\n",
+    "  # Extract lines from input files using the Dataset API.\n",
+    "  dataset = tf.data.TextLineDataset(data_file)\n",
+    "\n",
+    "  if shuffle:\n",
+    "    dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n",
+    "\n",
+    "  dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
+    "\n",
+    "  # We call repeat after shuffling, rather than before, to prevent separate\n",
+    "  # epochs from blending together.\n",
+    "  dataset = dataset.repeat(num_epochs)\n",
+    "  dataset = dataset.batch(batch_size)\n",
+    "\n",
+    "  iterator = dataset.make_one_shot_iterator()\n",
+    "  features, labels = iterator.get_next()\n",
+    "  return features, labels"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Selecting and Engineering Features for the Model\n",
+    "\n",
+    "Selecting and crafting the right set of feature columns is key to learning an\n",
+    "effective model. A **feature column** can be either one of the raw columns in\n",
+    "the original dataframe (let's call them **base feature columns**), or any new\n",
+    "columns created based on some transformations defined over one or multiple base\n",
+    "columns (let's call them **derived feature columns**). Basically, \"feature\n",
+    "column\" is an abstract concept of any raw or derived variable that can be used\n",
+    "to predict the target label.\n",
+    "\n",
+    "### Base Categorical Feature Columns\n",
+    "\n",
+    "To define a feature column for a categorical feature, we can create a\n",
+    "`CategoricalColumn` using the tf.feature_column API. If you know the set of all\n",
+    "possible feature values of a column and there are only a few of them, you can\n",
+    "use `categorical_column_with_vocabulary_list`. Each key in the list will get\n",
+    "assigned an auto-incremental ID starting from 0. For example, for the\n",
+    "`relationship` column we can assign the feature string \"Husband\" to an integer\n",
+    "ID of 0 and \"Not-in-family\" to 1, etc., by doing:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
+    "    'relationship', [\n",
+    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
+    "        'Other-relative'])"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "What if we don't know the set of possible values in advance? Not a problem. We\n",
+    "can use `categorical_column_with_hash_bucket` instead:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
+    "    'occupation', hash_bucket_size=1000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "What will happen is that each possible value in the feature column `occupation`\n",
+    "will be hashed to an integer ID as we encounter them in training. See an example\n",
+    "illustration below:\n",
+    "\n",
+    "ID  | Feature\n",
+    "--- | -------------\n",
+    "... |\n",
+    "9   | `\"Machine-op-inspct\"`\n",
+    "... |\n",
+    "103 | `\"Farming-fishing\"`\n",
+    "... |\n",
+    "375 | `\"Protective-serv\"`\n",
+    "... |\n",
+    "\n",
+    "No matter which way we choose to define a `SparseColumn`, each feature string\n",
+    "will be mapped into an integer ID by looking up a fixed mapping or by hashing.\n",
+    "Note that hashing collisions are possible, but may not significantly impact the\n",
+    "model quality. Under the hood, the `LinearModel` class is responsible for\n",
+    "managing the mapping and creating `tf.Variable` to store the model parameters\n",
+    "(also known as model weights) for each feature ID. The model parameters will be\n",
+    "learned through the model training process we'll go through later.\n",
+    "\n",
+    "We'll do the similar trick to define the other categorical features:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
+    "    'education', [\n",
+    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
+    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
+    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
+    "\n",
+    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
+    "    'marital_status', [\n",
+    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
+    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
+    "\n",
+    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
+    "    'relationship', [\n",
+    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
+    "        'Other-relative'])\n",
+    "\n",
+    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
+    "    'workclass', [\n",
+    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
+    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
+    "\n",
+    "# To show an example of hashing:\n",
+    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
+    "    'occupation', hash_bucket_size=1000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Base Continuous Feature Columns\n",
+    "\n",
+    "Similarly, we can define a `NumericColumn` for each continuous feature column\n",
+    "that we want to use in the model:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "age = tf.feature_column.numeric_column('age')\n",
+    "education_num = tf.feature_column.numeric_column('education_num')\n",
+    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
+    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
+    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Making Continuous Features Categorical through Bucketization\n",
+    "\n",
+    "Sometimes the relationship between a continuous feature and the label is not\n",
+    "linear. As a hypothetical example, a person's income may grow with age in the\n",
+    "early stage of one's career, then the growth may slow at some point, and finally\n",
+    "the income decreases after retirement. In this scenario, using the raw `age` as\n",
+    "a real-valued feature column might not be a good choice because the model can\n",
+    "only learn one of the three cases:\n",
+    "\n",
+    "1.  Income always increases at some rate as age grows (positive correlation),\n",
+    "1.  Income always decreases at some rate as age grows (negative correlation), or\n",
+    "1.  Income stays the same no matter at what age (no correlation)\n",
+    "\n",
+    "If we want to learn the fine-grained correlation between income and each age\n",
+    "group separately, we can leverage **bucketization**. Bucketization is a process\n",
+    "of dividing the entire range of a continuous feature into a set of consecutive\n",
+    "bins/buckets, and then converting the original numerical feature into a bucket\n",
+    "ID (as a categorical feature) depending on which bucket that value falls into.\n",
+    "So, we can define a `bucketized_column` over `age` as:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "age_buckets = tf.feature_column.bucketized_column(\n",
+    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "where the `boundaries` is a list of bucket boundaries. In this case, there are\n",
+    "10 boundaries, resulting in 11 age group buckets (from age 17 and below, 18-24,\n",
+    "25-29, ..., to 65 and over).\n",
+    "\n",
+    "### Intersecting Multiple Columns with CrossedColumn\n",
+    "\n",
+    "Using each base feature column separately may not be enough to explain the data.\n",
+    "For example, the correlation between education and the label (earning > 50,000\n",
+    "dollars) may be different for different occupations. Therefore, if we only learn\n",
+    "a single model weight for `education=\"Bachelors\"` and `education=\"Masters\"`, we\n",
+    "won't be able to capture every single education-occupation combination (e.g.\n",
+    "distinguishing between `education=\"Bachelors\" AND occupation=\"Exec-managerial\"`\n",
+    "and `education=\"Bachelors\" AND occupation=\"Craft-repair\"`). To learn the\n",
+    "differences between different feature combinations, we can add **crossed feature\n",
+    "columns** to the model."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "education_x_occupation = tf.feature_column.crossed_column(\n",
+    "    ['education', 'occupation'], hash_bucket_size=1000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "We can also create a `CrossedColumn` over more than two columns. Each\n",
+    "constituent column can be either a base feature column that is categorical\n",
+    "(`SparseColumn`), a bucketized real-valued feature column (`BucketizedColumn`),\n",
+    "or even another `CrossColumn`. Here's an example:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(\n",
+    "    [age_buckets, 'education', 'occupation'], hash_bucket_size=1000)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Defining The Logistic Regression Model\n",
+    "\n",
+    "After processing the input data and defining all the feature columns, we're now\n",
+    "ready to put them all together and build a Logistic Regression model. In the\n",
+    "previous section we've seen several types of base and derived feature columns,\n",
+    "including:\n",
+    "\n",
+    "*   `CategoricalColumn`\n",
+    "*   `NumericColumn`\n",
+    "*   `BucketizedColumn`\n",
+    "*   `CrossedColumn`\n",
+    "\n",
+    "All of these are subclasses of the abstract `FeatureColumn` class, and can be\n",
+    "added to the `feature_columns` field of a model:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "base_columns = [\n",
+    "    education, marital_status, relationship, workclass, occupation,\n",
+    "    age_buckets,\n",
+    "]\n",
+    "crossed_columns = [\n",
+    "    tf.feature_column.crossed_column(\n",
+    "        ['education', 'occupation'], hash_bucket_size=1000),\n",
+    "    tf.feature_column.crossed_column(\n",
+    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
+    "]\n",
+    "\n",
+    "model_dir = tempfile.mkdtemp()\n",
+    "model = tf.estimator.LinearClassifier(\n",
+    "    model_dir=model_dir, feature_columns=base_columns + crossed_columns)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The model also automatically learns a bias term, which controls the prediction\n",
+    "one would make without observing any features (see the section \"How Logistic\n",
+    "Regression Works\" for more explanations). The learned model files will be stored\n",
+    "in `model_dir`.\n",
+    "\n",
+    "## Training and Evaluating Our Model\n",
+    "\n",
+    "After adding all the features to the model, now let's look at how to actually\n",
+    "train the model. Training a model is just a single command using the\n",
+    "tf.estimator API:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model.train(input_fn=lambda: input_fn(train_data, num_epochs, True, batch_size))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "After the model is trained, we can evaluate how good our model is at predicting\n",
+    "the labels of the holdout data:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "results = model.evaluate(input_fn=lambda: input_fn(\n",
+    "    test_data, 1, False, batch_size))\n",
+    "for key in sorted(results):\n",
+    "  print('%s: %s' % (key, results[key]))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The first line of the final output should be something like\n",
+    "`accuracy: 0.83557522`, which means the accuracy is 83.6%. Feel free to try more\n",
+    "features and transformations and see if you can do even better!\n",
+    "\n",
+    "After the model is evaluated, we can use the model to predict whether an individual has an annual income of over\n",
+    "50,000 dollars given an individual's information input."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "  pred_iter = model.predict(input_fn=lambda: input_fn(FLAGS.test_data, 1, False, 1))\n",
+    "  for pred in pred_iter:\n",
+    "    print(pred['classes'])"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "The model prediction output would be like `[b'1']` or `[b'0']` which means whether corresponding individual has an annual income of over 50,000 dollars or not.\n",
+    "\n",
+    "If you'd like to see a working end-to-end example, you can download our\n",
+    "[example code](https://github.com/tensorflow/models/tree/master/official/wide_deep/wide_deep.py)\n",
+    "and set the `model_type` flag to `wide`.\n",
+    "\n",
+    "## Adding Regularization to Prevent Overfitting\n",
+    "\n",
+    "Regularization is a technique used to avoid **overfitting**. Overfitting happens\n",
+    "when your model does well on the data it is trained on, but worse on test data\n",
+    "that the model has not seen before, such as live traffic. Overfitting generally\n",
+    "occurs when a model is excessively complex, such as having too many parameters\n",
+    "relative to the number of observed training data. Regularization allows for you\n",
+    "to control your model's complexity and makes the model more generalizable to\n",
+    "unseen data.\n",
+    "\n",
+    "In the Linear Model library, you can add L1 and L2 regularizations to the model\n",
+    "as:"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model = tf.estimator.LinearClassifier(\n",
+    "    model_dir=model_dir, feature_columns=base_columns + crossed_columns,\n",
+    "    optimizer=tf.train.FtrlOptimizer(\n",
+    "        learning_rate=0.1,\n",
+    "        l1_regularization_strength=1.0,\n",
+    "        l2_regularization_strength=1.0))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "One important difference between L1 and L2 regularization is that L1\n",
+    "regularization tends to make model weights stay at zero, creating sparser\n",
+    "models, whereas L2 regularization also tries to make the model weights closer to\n",
+    "zero but not necessarily zero. Therefore, if you increase the strength of L1\n",
+    "regularization, you will have a smaller model size because many of the model\n",
+    "weights will be zero. This is often desirable when the feature space is very\n",
+    "large but sparse, and when there are resource constraints that prevent you from\n",
+    "serving a model that is too large.\n",
+    "\n",
+    "In practice, you should try various combinations of L1, L2 regularization\n",
+    "strengths and find the best parameters that best control overfitting and give\n",
+    "you a desirable model size.\n",
+    "\n",
+    "## How Logistic Regression Works\n",
+    "\n",
+    "Finally, let's take a minute to talk about what the Logistic Regression model\n",
+    "actually looks like in case you're not already familiar with it. We'll denote\n",
+    "the label as \\\\(Y\\\\), and the set of observed features as a feature vector\n",
+    "\\\\(\\mathbf{x}=[x_1, x_2, ..., x_d]\\\\). We define \\\\(Y=1\\\\) if an individual\n",
+    "earned > 50,000 dollars and \\\\(Y=0\\\\) otherwise. In Logistic Regression, the\n",
+    "probability of the label being positive (\\\\(Y=1\\\\)) given the features\n",
+    "\\\\(\\mathbf{x}\\\\) is given as:\n",
+    "\n",
+    "$$ P(Y=1|\\mathbf{x}) = \\frac{1}{1+\\exp(-(\\mathbf{w}^T\\mathbf{x}+b))}$$\n",
+    "\n",
+    "where \\\\(\\mathbf{w}=[w_1, w_2, ..., w_d]\\\\) are the model weights for the\n",
+    "features \\\\(\\mathbf{x}=[x_1, x_2, ..., x_d]\\\\). \\\\(b\\\\) is a constant that is\n",
+    "often called the **bias** of the model. The equation consists of two partsâ€”A\n",
+    "linear model and a logistic function:\n",
+    "\n",
+    "*   **Linear Model**: First, we can see that \\\\(\\mathbf{w}^T\\mathbf{x}+b = b +\n",
+    "    w_1x_1 + ... +w_dx_d\\\\) is a linear model where the output is a linear\n",
+    "    function of the input features \\\\(\\mathbf{x}\\\\). The bias \\\\(b\\\\) is the\n",
+    "    prediction one would make without observing any features. The model weight\n",
+    "    \\\\(w_i\\\\) reflects how the feature \\\\(x_i\\\\) is correlated with the positive\n",
+    "    label. If \\\\(x_i\\\\) is positively correlated with the positive label, the\n",
+    "    weight \\\\(w_i\\\\) increases, and the probability \\\\(P(Y=1|\\mathbf{x})\\\\) will\n",
+    "    be closer to 1. On the other hand, if \\\\(x_i\\\\) is negatively correlated\n",
+    "    with the positive label, then the weight \\\\(w_i\\\\) decreases and the\n",
+    "    probability \\\\(P(Y=1|\\mathbf{x})\\\\) will be closer to 0.\n",
+    "\n",
+    "*   **Logistic Function**: Second, we can see that there's a logistic function\n",
+    "    (also known as the sigmoid function) \\\\(S(t) = 1/(1+\\exp(-t))\\\\) being\n",
+    "    applied to the linear model. The logistic function is used to convert the\n",
+    "    output of the linear model \\\\(\\mathbf{w}^T\\mathbf{x}+b\\\\) from any real\n",
+    "    number into the range of \\\\([0, 1]\\\\), which can be interpreted as a\n",
+    "    probability.\n",
+    "\n",
+    "Model training is an optimization problem: The goal is to find a set of model\n",
+    "weights (i.e. model parameters) to minimize a **loss function** defined over the\n",
+    "training data, such as logistic loss for Logistic Regression models. The loss\n",
+    "function measures the discrepancy between the ground-truth label and the model's\n",
+    "prediction. If the prediction is very close to the ground-truth label, the loss\n",
+    "value will be low; if the prediction is very far from the label, then the loss\n",
+    "value would be high.\n",
+    "\n",
+    "## Learn Deeper\n",
+    "\n",
+    "If you're interested in learning more, check out our\n",
+    "@{$wide_and_deep$Wide & Deep Learning Tutorial} where we'll show you how to\n",
+    "combine the strengths of linear models and deep neural networks by jointly\n",
+    "training them using the tf.estimator API."
+   ]
+  }
+ ],
+ "metadata": {},
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/samples/core/tutorials/estimators/wide.md b/samples/core/tutorials/estimators/wide.md
deleted file mode 100644
index 27ce75a3..00000000
--- a/samples/core/tutorials/estimators/wide.md
+++ /dev/null
@@ -1,461 +0,0 @@
-# TensorFlow Linear Model Tutorial
-
-In this tutorial, we will use the tf.estimator API in TensorFlow to solve a
-binary classification problem: Given census data about a person such as age,
-education, marital status, and occupation (the features), we will try to predict
-whether or not the person earns more than 50,000 dollars a year (the target
-label). We will train a **logistic regression** model, and given an individual's
-information our model will output a number between 0 and 1, which can be
-interpreted as the probability that the individual has an annual income of over
-50,000 dollars.
-
-## Setup
-
-To try the code for this tutorial:
-
-1.  @{$install$Install TensorFlow} if you haven't already.
-
-2.  Download [the tutorial code](https://github.com/tensorflow/models/tree/master/official/wide_deep/).
-
-3. Execute the data download script we provide to you:
-
-        $ python data_download.py
-
-4. Execute the tutorial code with the following command to train the linear
-model described in this tutorial:
-
-        $ python wide_deep.py --model_type=wide
-
-Read on to find out how this code builds its linear model.
-
-## Reading The Census Data
-
-The dataset we'll be using is the
-[Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income).
-We have provided
-[data_download.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/data_download.py)
-which downloads the code and performs some additional cleanup.
-
-Since the task is a binary classification problem, we'll construct a label
-column named "label" whose value is 1 if the income is over 50K, and 0
-otherwise. For reference, see `input_fn` in
-[wide_deep.py](https://github.com/tensorflow/models/tree/master/official/wide_deep/wide_deep.py).
-
-Next, let's take a look at the dataframe and see which columns we can use to
-predict the target label. The columns can be grouped into two typesâ€”categorical
-and continuous columns:
-
-*   A column is called **categorical** if its value can only be one of the
-    categories in a finite set. For example, the relationship status of a person
-    (wife, husband, unmarried, etc.) or the education level (high school,
-    college, etc.) are categorical columns.
-*   A column is called **continuous** if its value can be any numerical value in
-    a continuous range. For example, the capital gain of a person (e.g. $14,084)
-    is a continuous column.
-
-Here's a list of columns available in the Census Income dataset:
-
-| Column Name    | Type        | Description                       |
-| -------------- | ----------- | --------------------------------- |
-| age            | Continuous  | The age of the individual         |
-| workclass      | Categorical | The type of employer the          |
-:                :             : individual has (government,       :
-:                :             : military, private, etc.).         :
-| fnlwgt         | Continuous  | The number of people the census   |
-:                :             : takers believe that observation   :
-:                :             : represents (sample weight). Final :
-:                :             : weight will not be used.          :
-| education      | Categorical | The highest level of education    |
-:                :             : achieved for that individual.     :
-| education_num  | Continuous  | The highest level of education in |
-:                :             : numerical form.                   :
-| marital_status | Categorical | Marital status of the individual. |
-| occupation     | Categorical | The occupation of the individual. |
-| relationship   | Categorical | Wife, Own-child, Husband,         |
-:                :             : Not-in-family, Other-relative,    :
-:                :             : Unmarried.                        :
-| race           | Categorical | Amer-Indian-Eskimo, Asian-Pac-    |
-:                :             : Islander, Black, White, Other.    :
-| gender         | Categorical | Female, Male.                     |
-| capital_gain   | Continuous  | Capital gains recorded.           |
-| capital_loss   | Continuous  | Capital Losses recorded.          |
-| hours_per_week | Continuous  | Hours worked per week.            |
-| native_country | Categorical | Country of origin of the          |
-:                :             : individual.                       :
-| income_bracket | Categorical | ">50K" or "<=50K", meaning        |
-:                :             : whether the person makes more     :
-:                :             : than $50,000 annually.            :
-
-## Converting Data into Tensors
-
-When building a tf.estimator model, the input data is specified by means of an
-Input Builder function. This builder function will not be called until it is
-later passed to tf.estimator.Estimator methods such as `train` and `evaluate`.
-The purpose of this function is to construct the input data, which is
-represented in the form of @{tf.Tensor}s or @{tf.SparseTensor}s.
-In more detail, the input builder function returns the following as a pair:
-
-1.  `features`: A dict from feature column names to `Tensors` or
-    `SparseTensors`.
-2.  `labels`: A `Tensor` containing the label column.
-
-The keys of the `features` will be used to construct columns in the next
-section. Because we want to call the `train` and `evaluate` methods with
-different data, we define a method that returns an input function based on the
-given data. Note that the returned input function will be called while
-constructing the TensorFlow graph, not while running the graph. What it is
-returning is a representation of the input data as the fundamental unit of
-TensorFlow computations, a `Tensor` (or `SparseTensor`).
-
-Each continuous column in the train or test data will be converted into a
-`Tensor`, which in general is a good format to represent dense data. For
-categorical data, we must represent the data as a `SparseTensor`. This data
-format is good for representing sparse data. Our `input_fn` uses the `tf.data`
-API, which makes it easy to apply transformations to our dataset:
-
-```python
-def input_fn(data_file, num_epochs, shuffle, batch_size):
-  """Generate an input function for the Estimator."""
-  assert tf.gfile.Exists(data_file), (
-      '%s not found. Please make sure you have either run data_download.py or '
-      'set both arguments --train_data and --test_data.' % data_file)
-
-  def parse_csv(value):
-    print('Parsing', data_file)
-    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)
-    features = dict(zip(_CSV_COLUMNS, columns))
-    labels = features.pop('income_bracket')
-    return features, tf.equal(labels, '>50K')
-
-  # Extract lines from input files using the Dataset API.
-  dataset = tf.data.TextLineDataset(data_file)
-
-  if shuffle:
-    dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
-
-  dataset = dataset.map(parse_csv, num_parallel_calls=5)
-
-  # We call repeat after shuffling, rather than before, to prevent separate
-  # epochs from blending together.
-  dataset = dataset.repeat(num_epochs)
-  dataset = dataset.batch(batch_size)
-
-  iterator = dataset.make_one_shot_iterator()
-  features, labels = iterator.get_next()
-  return features, labels
-```
-
-## Selecting and Engineering Features for the Model
-
-Selecting and crafting the right set of feature columns is key to learning an
-effective model. A **feature column** can be either one of the raw columns in
-the original dataframe (let's call them **base feature columns**), or any new
-columns created based on some transformations defined over one or multiple base
-columns (let's call them **derived feature columns**). Basically, "feature
-column" is an abstract concept of any raw or derived variable that can be used
-to predict the target label.
-
-### Base Categorical Feature Columns
-
-To define a feature column for a categorical feature, we can create a
-`CategoricalColumn` using the tf.feature_column API. If you know the set of all
-possible feature values of a column and there are only a few of them, you can
-use `categorical_column_with_vocabulary_list`. Each key in the list will get
-assigned an auto-incremental ID starting from 0. For example, for the
-`relationship` column we can assign the feature string "Husband" to an integer
-ID of 0 and "Not-in-family" to 1, etc., by doing:
-
-```python
-relationship = tf.feature_column.categorical_column_with_vocabulary_list(
-    'relationship', [
-        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',
-        'Other-relative'])
-```
-
-What if we don't know the set of possible values in advance? Not a problem. We
-can use `categorical_column_with_hash_bucket` instead:
-
-```python
-occupation = tf.feature_column.categorical_column_with_hash_bucket(
-    'occupation', hash_bucket_size=1000)
-```
-
-What will happen is that each possible value in the feature column `occupation`
-will be hashed to an integer ID as we encounter them in training. See an example
-illustration below:
-
-ID  | Feature
---- | -------------
-... |
-9   | `"Machine-op-inspct"`
-... |
-103 | `"Farming-fishing"`
-... |
-375 | `"Protective-serv"`
-... |
-
-No matter which way we choose to define a `SparseColumn`, each feature string
-will be mapped into an integer ID by looking up a fixed mapping or by hashing.
-Note that hashing collisions are possible, but may not significantly impact the
-model quality. Under the hood, the `LinearModel` class is responsible for
-managing the mapping and creating `tf.Variable` to store the model parameters
-(also known as model weights) for each feature ID. The model parameters will be
-learned through the model training process we'll go through later.
-
-We'll do the similar trick to define the other categorical features:
-
-```python
-education = tf.feature_column.categorical_column_with_vocabulary_list(
-    'education', [
-        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',
-        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',
-        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])
-
-marital_status = tf.feature_column.categorical_column_with_vocabulary_list(
-    'marital_status', [
-        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',
-        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])
-
-relationship = tf.feature_column.categorical_column_with_vocabulary_list(
-    'relationship', [
-        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',
-        'Other-relative'])
-
-workclass = tf.feature_column.categorical_column_with_vocabulary_list(
-    'workclass', [
-        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',
-        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])
-
-# To show an example of hashing:
-occupation = tf.feature_column.categorical_column_with_hash_bucket(
-    'occupation', hash_bucket_size=1000)
-```
-
-### Base Continuous Feature Columns
-
-Similarly, we can define a `NumericColumn` for each continuous feature column
-that we want to use in the model:
-
-```python
-age = tf.feature_column.numeric_column('age')
-education_num = tf.feature_column.numeric_column('education_num')
-capital_gain = tf.feature_column.numeric_column('capital_gain')
-capital_loss = tf.feature_column.numeric_column('capital_loss')
-hours_per_week = tf.feature_column.numeric_column('hours_per_week')
-```
-
-### Making Continuous Features Categorical through Bucketization
-
-Sometimes the relationship between a continuous feature and the label is not
-linear. As a hypothetical example, a person's income may grow with age in the
-early stage of one's career, then the growth may slow at some point, and finally
-the income decreases after retirement. In this scenario, using the raw `age` as
-a real-valued feature column might not be a good choice because the model can
-only learn one of the three cases:
-
-1.  Income always increases at some rate as age grows (positive correlation),
-1.  Income always decreases at some rate as age grows (negative correlation), or
-1.  Income stays the same no matter at what age (no correlation)
-
-If we want to learn the fine-grained correlation between income and each age
-group separately, we can leverage **bucketization**. Bucketization is a process
-of dividing the entire range of a continuous feature into a set of consecutive
-bins/buckets, and then converting the original numerical feature into a bucket
-ID (as a categorical feature) depending on which bucket that value falls into.
-So, we can define a `bucketized_column` over `age` as:
-
-```python
-age_buckets = tf.feature_column.bucketized_column(
-    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
-```
-
-where the `boundaries` is a list of bucket boundaries. In this case, there are
-10 boundaries, resulting in 11 age group buckets (from age 17 and below, 18-24,
-25-29, ..., to 65 and over).
-
-### Intersecting Multiple Columns with CrossedColumn
-
-Using each base feature column separately may not be enough to explain the data.
-For example, the correlation between education and the label (earning > 50,000
-dollars) may be different for different occupations. Therefore, if we only learn
-a single model weight for `education="Bachelors"` and `education="Masters"`, we
-won't be able to capture every single education-occupation combination (e.g.
-distinguishing between `education="Bachelors" AND occupation="Exec-managerial"`
-and `education="Bachelors" AND occupation="Craft-repair"`). To learn the
-differences between different feature combinations, we can add **crossed feature
-columns** to the model.
-
-```python
-education_x_occupation = tf.feature_column.crossed_column(
-    ['education', 'occupation'], hash_bucket_size=1000)
-```
-
-We can also create a `CrossedColumn` over more than two columns. Each
-constituent column can be either a base feature column that is categorical
-(`SparseColumn`), a bucketized real-valued feature column (`BucketizedColumn`),
-or even another `CrossColumn`. Here's an example:
-
-```python
-age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(
-    [age_buckets, 'education', 'occupation'], hash_bucket_size=1000)
-```
-
-## Defining The Logistic Regression Model
-
-After processing the input data and defining all the feature columns, we're now
-ready to put them all together and build a Logistic Regression model. In the
-previous section we've seen several types of base and derived feature columns,
-including:
-
-*   `CategoricalColumn`
-*   `NumericColumn`
-*   `BucketizedColumn`
-*   `CrossedColumn`
-
-All of these are subclasses of the abstract `FeatureColumn` class, and can be
-added to the `feature_columns` field of a model:
-
-```python
-base_columns = [
-    education, marital_status, relationship, workclass, occupation,
-    age_buckets,
-]
-crossed_columns = [
-    tf.feature_column.crossed_column(
-        ['education', 'occupation'], hash_bucket_size=1000),
-    tf.feature_column.crossed_column(
-        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),
-]
-
-model_dir = tempfile.mkdtemp()
-model = tf.estimator.LinearClassifier(
-    model_dir=model_dir, feature_columns=base_columns + crossed_columns)
-```
-
-The model also automatically learns a bias term, which controls the prediction
-one would make without observing any features (see the section "How Logistic
-Regression Works" for more explanations). The learned model files will be stored
-in `model_dir`.
-
-## Training and Evaluating Our Model
-
-After adding all the features to the model, now let's look at how to actually
-train the model. Training a model is just a single command using the
-tf.estimator API:
-
-```python
-model.train(input_fn=lambda: input_fn(train_data, num_epochs, True, batch_size))
-```
-
-After the model is trained, we can evaluate how good our model is at predicting
-the labels of the holdout data:
-
-```python
-results = model.evaluate(input_fn=lambda: input_fn(
-    test_data, 1, False, batch_size))
-for key in sorted(results):
-  print('%s: %s' % (key, results[key]))
-```
-
-The first line of the final output should be something like
-`accuracy: 0.83557522`, which means the accuracy is 83.6%. Feel free to try more
-features and transformations and see if you can do even better!
-
-After the model is evaluated, we can use the model to predict whether an individual has an annual income of over
-50,000 dollars given an individual's information input.
-```python
-  pred_iter = model.predict(input_fn=lambda: input_fn(FLAGS.test_data, 1, False, 1))
-  for pred in pred_iter:
-    print(pred['classes'])
-```
-
-The model prediction output would be like `[b'1']` or `[b'0']` which means whether corresponding individual has an annual income of over 50,000 dollars or not.
-
-If you'd like to see a working end-to-end example, you can download our
-[example code](https://github.com/tensorflow/models/tree/master/official/wide_deep/wide_deep.py)
-and set the `model_type` flag to `wide`.
-
-## Adding Regularization to Prevent Overfitting
-
-Regularization is a technique used to avoid **overfitting**. Overfitting happens
-when your model does well on the data it is trained on, but worse on test data
-that the model has not seen before, such as live traffic. Overfitting generally
-occurs when a model is excessively complex, such as having too many parameters
-relative to the number of observed training data. Regularization allows for you
-to control your model's complexity and makes the model more generalizable to
-unseen data.
-
-In the Linear Model library, you can add L1 and L2 regularizations to the model
-as:
-
-```
-model = tf.estimator.LinearClassifier(
-    model_dir=model_dir, feature_columns=base_columns + crossed_columns,
-    optimizer=tf.train.FtrlOptimizer(
-        learning_rate=0.1,
-        l1_regularization_strength=1.0,
-        l2_regularization_strength=1.0))
-```
-
-One important difference between L1 and L2 regularization is that L1
-regularization tends to make model weights stay at zero, creating sparser
-models, whereas L2 regularization also tries to make the model weights closer to
-zero but not necessarily zero. Therefore, if you increase the strength of L1
-regularization, you will have a smaller model size because many of the model
-weights will be zero. This is often desirable when the feature space is very
-large but sparse, and when there are resource constraints that prevent you from
-serving a model that is too large.
-
-In practice, you should try various combinations of L1, L2 regularization
-strengths and find the best parameters that best control overfitting and give
-you a desirable model size.
-
-## How Logistic Regression Works
-
-Finally, let's take a minute to talk about what the Logistic Regression model
-actually looks like in case you're not already familiar with it. We'll denote
-the label as \\(Y\\), and the set of observed features as a feature vector
-\\(\mathbf{x}=[x_1, x_2, ..., x_d]\\). We define \\(Y=1\\) if an individual
-earned > 50,000 dollars and \\(Y=0\\) otherwise. In Logistic Regression, the
-probability of the label being positive (\\(Y=1\\)) given the features
-\\(\mathbf{x}\\) is given as:
-
-$$ P(Y=1|\mathbf{x}) = \frac{1}{1+\exp(-(\mathbf{w}^T\mathbf{x}+b))}$$
-
-where \\(\mathbf{w}=[w_1, w_2, ..., w_d]\\) are the model weights for the
-features \\(\mathbf{x}=[x_1, x_2, ..., x_d]\\). \\(b\\) is a constant that is
-often called the **bias** of the model. The equation consists of two partsâ€”A
-linear model and a logistic function:
-
-*   **Linear Model**: First, we can see that \\(\mathbf{w}^T\mathbf{x}+b = b +
-    w_1x_1 + ... +w_dx_d\\) is a linear model where the output is a linear
-    function of the input features \\(\mathbf{x}\\). The bias \\(b\\) is the
-    prediction one would make without observing any features. The model weight
-    \\(w_i\\) reflects how the feature \\(x_i\\) is correlated with the positive
-    label. If \\(x_i\\) is positively correlated with the positive label, the
-    weight \\(w_i\\) increases, and the probability \\(P(Y=1|\mathbf{x})\\) will
-    be closer to 1. On the other hand, if \\(x_i\\) is negatively correlated
-    with the positive label, then the weight \\(w_i\\) decreases and the
-    probability \\(P(Y=1|\mathbf{x})\\) will be closer to 0.
-
-*   **Logistic Function**: Second, we can see that there's a logistic function
-    (also known as the sigmoid function) \\(S(t) = 1/(1+\exp(-t))\\) being
-    applied to the linear model. The logistic function is used to convert the
-    output of the linear model \\(\mathbf{w}^T\mathbf{x}+b\\) from any real
-    number into the range of \\([0, 1]\\), which can be interpreted as a
-    probability.
-
-Model training is an optimization problem: The goal is to find a set of model
-weights (i.e. model parameters) to minimize a **loss function** defined over the
-training data, such as logistic loss for Logistic Regression models. The loss
-function measures the discrepancy between the ground-truth label and the model's
-prediction. If the prediction is very close to the ground-truth label, the loss
-value will be low; if the prediction is very far from the label, then the loss
-value would be high.
-
-## Learn Deeper
-
-If you're interested in learning more, check out our
-@{$wide_and_deep$Wide & Deep Learning Tutorial} where we'll show you how to
-combine the strengths of linear models and deep neural networks by jointly
-training them using the tf.estimator API.
