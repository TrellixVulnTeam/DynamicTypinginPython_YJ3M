commit ac5fff19dcb54bb341b7bfb3cf9f554f2d5f92a5
Author: Hongkun Yu <hongkuny@google.com>
Date:   Tue May 12 11:22:03 2020 -0700

    Make Transformer Layer back compatible.
    
    PiperOrigin-RevId: 311165658

diff --git a/official/nlp/modeling/layers/transformer.py b/official/nlp/modeling/layers/transformer.py
index c32610d4..b64fc0a7 100644
--- a/official/nlp/modeling/layers/transformer.py
+++ b/official/nlp/modeling/layers/transformer.py
@@ -117,7 +117,10 @@ class Transformer(tf.keras.layers.Layer):
         kernel_constraint=self._kernel_constraint,
         bias_constraint=self._bias_constraint,
         name="self_attention")
-
+    # pylint: disable=protected-access
+    self._attention_layer.build([input_tensor_shape] * 3)
+    self._attention_output_dense = self._attention_layer._output_dense
+    # pylint: enable=protected-access
     self._attention_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
     # Use float32 in layernorm for numeric stability.
     # It is probably safe in mixed_float16, but we haven't validated this yet.
