commit 67f81649f486cd9bcb5455e5037c2fe0d278a5fe
Author: guptapriya <14104855+guptapriya@users.noreply.github.com>
Date:   Thu Jul 18 22:27:29 2019 -0700

    Remove loss layer

diff --git a/official/transformer/v2/metrics.py b/official/transformer/v2/metrics.py
index 8b49799b..4bd6bba6 100644
--- a/official/transformer/v2/metrics.py
+++ b/official/transformer/v2/metrics.py
@@ -181,25 +181,3 @@ def transformer_loss(logits, labels, smoothing, vocab_size):
   xentropy, weights = padded_cross_entropy_loss(logits, labels, smoothing,
                                                 vocab_size)
   return tf.reduce_sum(xentropy) / tf.reduce_sum(weights)
-
-
-class LossLayer(tf.keras.layers.Layer):
-  """Custom a layer of transformer loss for Transformer model."""
-
-  def __init__(self, vocab_size, label_smoothing):
-    super(LossLayer, self).__init__()
-    self.vocab_size = vocab_size
-    self.label_smoothing = label_smoothing
-
-  def get_config(self):
-    return {
-        "vocab_size": self.vocab_size,
-        "label_smoothing": self.label_smoothing,
-    }
-
-  def call(self, inputs):
-    logits, targets = inputs[0], inputs[1]
-    loss = transformer_loss(logits, targets, self.label_smoothing,
-                            self.vocab_size)
-    self.add_loss(loss)
-    return logits
