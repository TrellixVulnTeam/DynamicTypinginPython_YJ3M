commit 16da0022314521e604325a49fe2bb3ebd527ccf5
Author: David E. Weekly <david@weekly.org>
Date:   Tue Dec 20 20:56:34 2016 -0800

    Update deprecated sum_of_squares (TF 0.10) to mean_squared_error (TF 0.12)

diff --git a/slim/slim_walkthough.ipynb b/slim/slim_walkthough.ipynb
index 5804c7b2..8f26bb56 100644
--- a/slim/slim_walkthough.ipynb
+++ b/slim/slim_walkthough.ipynb
@@ -232,7 +232,7 @@
    },
    "outputs": [],
    "source": [
-    "# The following snippet trains the regression model using a sum_of_squares loss.\n",
+    "# The following snippet trains the regression model using a mean_squared_error loss.\n",
     "ckpt_dir = '/tmp/regression_model/'\n",
     "\n",
     "with tf.Graph().as_default():\n",
@@ -244,7 +244,7 @@
     "    predictions, nodes = regression_model(inputs, is_training=True)\n",
     "\n",
     "    # Add the loss function to the graph.\n",
-    "    loss = slim.losses.sum_of_squares(predictions, targets)\n",
+    "    loss = slim.losses.mean_squared_error(predictions, targets)\n",
     "    \n",
     "    # The total loss is the uers's loss plus any regularization losses.\n",
     "    total_loss = slim.losses.get_total_loss()\n",
@@ -289,12 +289,12 @@
     "    predictions, end_points = regression_model(inputs, is_training=True)\n",
     "\n",
     "    # Add multiple loss nodes.\n",
-    "    sum_of_squares_loss = slim.losses.sum_of_squares(predictions, targets)\n",
+    "    mean_squared_error_loss = slim.losses.mean_squared_error(predictions, targets)\n",
     "    absolute_difference_loss = slim.losses.absolute_difference(predictions, targets)\n",
     "\n",
     "    # The following two ways to compute the total loss are equivalent\n",
     "    regularization_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
-    "    total_loss1 = sum_of_squares_loss + absolute_difference_loss + regularization_loss\n",
+    "    total_loss1 = mean_squared_error_loss + absolute_difference_loss + regularization_loss\n",
     "\n",
     "    # Regularization Loss is included in the total loss by default.\n",
     "    # This is good for training, but not for testing.\n",
