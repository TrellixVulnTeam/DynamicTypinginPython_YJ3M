commit 530dc5395da7d81f1edb4a416a674ba842990638
Author: Akshay Agrawal <akshayka@google.com>
Date:   Wed Apr 4 16:55:15 2018 -0700

    Delete equivalence between step and learning rate; the two are different concepts.

diff --git a/samples/core/get_started/eager.ipynb b/samples/core/get_started/eager.ipynb
index c1add81c..60f8b56c 100644
--- a/samples/core/get_started/eager.ipynb
+++ b/samples/core/get_started/eager.ipynb
@@ -534,7 +534,7 @@
       "source": [
         "### Create an optimizer\n",
         "\n",
-        "An *[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* applies the computed gradients to the model's variables to minimize the `loss` function. You can think of a curved surface (see Figure 3) and we want to find its lowest point by walking around. The gradients point in the direction of steepest ascent—so we'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradients for each *step* (or *[learning rate](https://developers.google.com/machine-learning/crash-course/glossary#learning_rate)*), we'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize loss. And the lower the loss, the better the model's predictions.\n",
+        "An *[optimizer](https://developers.google.com/machine-learning/crash-course/glossary#optimizer)* applies the computed gradients to the model's variables to minimize the `loss` function. You can think of a curved surface (see Figure 3) and we want to find its lowest point by walking around. The gradients point in the direction of steepest ascent—so we'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each example, we'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize loss. And the lower the loss, the better the model's predictions.\n",
         "\n",
         "<table>\n",
         "  <tr><td>\n",
