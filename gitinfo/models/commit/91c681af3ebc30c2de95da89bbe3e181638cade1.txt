commit 91c681af3ebc30c2de95da89bbe3e181638cade1
Author: Haitang Hu <hthu@google.com>
Date:   Thu Feb 13 08:38:53 2020 -0800

    Revert log passing change since it might hurt performance.
    
    PiperOrigin-RevId: 294922828

diff --git a/official/modeling/model_training_utils.py b/official/modeling/model_training_utils.py
index ffad1b84..69c90c0f 100644
--- a/official/modeling/model_training_utils.py
+++ b/official/modeling/model_training_utils.py
@@ -329,12 +329,12 @@ def run_customized_training_loop(
       for callback in custom_callbacks:
         callback.on_batch_begin(batch)
 
-    def _run_callbacks_on_batch_end(batch, logs):
+    def _run_callbacks_on_batch_end(batch):
       """Runs custom callbacks at the end of every step."""
       if not custom_callbacks:
         return
       for callback in custom_callbacks:
-        callback.on_batch_end(batch, logs)
+        callback.on_batch_end(batch)
 
     # Training loop starts here.
     checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
@@ -371,10 +371,10 @@ def run_customized_training_loop(
         # Converts steps to a Tensor to avoid tf.function retracing.
         train_steps(train_iterator,
                     tf.convert_to_tensor(steps, dtype=tf.int32))
-      train_loss = _float_metric_value(train_loss_metric)
-      _run_callbacks_on_batch_end(current_step, {'loss': train_loss})
+      _run_callbacks_on_batch_end(current_step)
       current_step += steps
 
+      train_loss = _float_metric_value(train_loss_metric)
       # Updates training logging.
       training_status = 'Train Step: %d/%d  / loss = %s' % (
           current_step, total_training_steps, train_loss)
