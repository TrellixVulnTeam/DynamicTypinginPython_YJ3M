commit 4b5a0801e067659092fd976fea8352735c48965b
Author: Andrew M. Dai <adai@google.com>
Date:   Mon Oct 16 14:05:49 2017 -0700

    Switch from static_rnn to dynamic_rnn. This significantly reduces CPU/GPU
    memory and fixes the OOM errors.
    
    PiperOrigin-RevId: 172374935

diff --git a/research/adversarial_text/layers.py b/research/adversarial_text/layers.py
index 138864bc..a7fe7612 100644
--- a/research/adversarial_text/layers.py
+++ b/research/adversarial_text/layers.py
@@ -95,7 +95,7 @@ class Embedding(K.layers.Layer):
 
 
 class LSTM(object):
-  """LSTM layer using static_rnn.
+  """LSTM layer using dynamic_rnn.
 
   Exposes variables in `trainable_weights` property.
   """
@@ -119,15 +119,11 @@ class LSTM(object):
       ])
 
       # shape(x) = (batch_size, num_timesteps, embedding_dim)
-      # Convert into a time-major list for static_rnn
-      x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))
 
-      lstm_out, next_state = tf.contrib.rnn.static_rnn(
+      lstm_out, next_state = tf.nn.dynamic_rnn(
           cell, x, initial_state=initial_state, sequence_length=seq_length)
 
-      lstm_out = tf.stack(lstm_out)
-      # shape(lstm_out) = (timesteps, batch_size, cell_size)
-      lstm_out = tf.transpose(lstm_out, [1, 0, 2])
+      # shape(lstm_out) = (batch_size, timesteps, cell_size)
 
       if self.keep_prob < 1.:
         lstm_out = tf.nn.dropout(lstm_out, self.keep_prob)
