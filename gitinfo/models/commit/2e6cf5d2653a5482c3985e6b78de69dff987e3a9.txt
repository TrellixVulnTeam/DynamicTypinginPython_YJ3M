commit 2e6cf5d2653a5482c3985e6b78de69dff987e3a9
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Jun 9 10:35:47 2020 -0700

    Internal change
    
    PiperOrigin-RevId: 315511975

diff --git a/official/nlp/nhnet/README.md b/official/nlp/nhnet/README.md
index 1664bd30..14c55636 100644
--- a/official/nlp/nhnet/README.md
+++ b/official/nlp/nhnet/README.md
@@ -39,13 +39,14 @@ First, install the `news-please` CLI (requires python 3.x)
 $ pip3 install news-please
 ```
 
-Next, run the crawler with our provided config and URL list
+Next, run the crawler with our provided [config and URL list](https://github.com/google-research-datasets/NewSHead/releases)
 
 ```shell
-# Sets to path of the downloaded data folder
+# Sets to path of the downloaded data folder.
 $ DATA_FOLDER=/path/to/downloaded_dataset
 
-# Uses CLI interface to crawl
+# Uses CLI interface to crawl. We assume news_please subfolder contains the
+# decompressed config.cfg and sitelist.hjson.
 $ news-please -c $DATA_FOLDER/news_please
 ```
 By default, it will store crawled
@@ -80,7 +81,7 @@ Next, we can run the following data preprocess script which may take a few hours
 
 
 ```shell
-# Recall that we use DATA_FOLDER=/path/to/downloaded_dataset
+# Recall that we use DATA_FOLDER=/path/to/downloaded_dataset.
 $ python3 raw_data_preprocess.py \
     -crawled_articles=/tmp/nhnet \
     -vocab=/path/to/bert_checkpoint/vocab.txt \
