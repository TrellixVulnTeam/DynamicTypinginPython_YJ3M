commit 8e25697bd57dc3e79702c75f28c6dd4ac6a97cbc
Author: lcchen <lcchen@google.com>
Date:   Wed Mar 14 17:30:29 2018 -0700

    update dataset examples

diff --git a/research/deeplab/g3doc/cityscapes.md b/research/deeplab/g3doc/cityscapes.md
index 5f302689..cec37dea 100644
--- a/research/deeplab/g3doc/cityscapes.md
+++ b/research/deeplab/g3doc/cityscapes.md
@@ -42,14 +42,7 @@ A local training job using `xception_65` can be run with the following command:
 # From tensorflow/models/research/
 python deeplab/train.py \
     --logtostderr \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
     --training_number_of_steps=90000 \
->>>>>>> origin/master
-=======
-    --training_number_of_steps=90000 \
->>>>>>> origin/master
     --train_split="train" \
     --model_variant="xception_65" \
     --atrous_rates=6 \
@@ -60,16 +53,8 @@ python deeplab/train.py \
     --train_crop_size=769 \
     --train_crop_size=769 \
     --train_batch_size=1 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
-    --dataset="cityscapes" \
-    --train_split="train" \
->>>>>>> origin/master
-=======
     --dataset="cityscapes" \
     --train_split="train" \
->>>>>>> origin/master
     --tf_initial_checkpoints=${PATH_TO_INITIAL_CHECKPOINT} \
     --train_logdir=${PATH_TO_TRAIN_DIR} \
     --dataset_dir=${PATH_TO_DATASET}
@@ -80,16 +65,6 @@ where ${PATH_TO_INITIAL_CHECKPOINT} is the path to the initial checkpoint
 directory in which training checkpoints and events will be written to, and
 ${PATH_TO_DATASET} is the directory in which the Cityscapes dataset resides.
 
-<<<<<<< HEAD
-<<<<<<< HEAD
-Note that for {train,eval,vis}.py:
-
-1.  We use small batch size during training. The users could change it based on
-    the available GPU memory and also set `fine_tune_batch_norm` to be False or
-    True depending on the use case.
-=======
-=======
->>>>>>> origin/master
 **Note that for {train,eval,vis}.py**:
 
 1.  In order to reproduce our results, one needs to use large batch size (> 8),
@@ -98,10 +73,6 @@ Note that for {train,eval,vis}.py:
     GPU memory at hand, please fine-tune from our provided checkpoints whose
     batch norm parameters have been trained, and use smaller learning rate with
     fine_tune_batch_norm = False.
-<<<<<<< HEAD
->>>>>>> origin/master
-=======
->>>>>>> origin/master
 
 2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
     setting output_stride=8.
@@ -125,16 +96,8 @@ python deeplab/eval.py \
     --decoder_output_stride=4 \
     --eval_crop_size=1025 \
     --eval_crop_size=2049 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
     --dataset="cityscapes" \
     --eval_split="val" \
->>>>>>> origin/master
-=======
-    --dataset="cityscapes" \
-    --eval_split="val" \
->>>>>>> origin/master
     --checkpoint_dir=${PATH_TO_CHECKPOINT} \
     --eval_logdir=${PATH_TO_EVAL_DIR} \
     --dataset_dir=${PATH_TO_DATASET}
@@ -161,16 +124,8 @@ python deeplab/vis.py \
     --decoder_output_stride=4 \
     --vis_crop_size=1025 \
     --vis_crop_size=2049 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
-    --dataset="cityscapes" \
-    --vis_split="val" \
->>>>>>> origin/master
-=======
     --dataset="cityscapes" \
     --vis_split="val" \
->>>>>>> origin/master
     --colormap_type="cityscapes" \
     --checkpoint_dir=${PATH_TO_CHECKPOINT} \
     --vis_logdir=${PATH_TO_VIS_DIR} \
diff --git a/research/deeplab/g3doc/pascal.md b/research/deeplab/g3doc/pascal.md
index 7314c693..81bf6a80 100644
--- a/research/deeplab/g3doc/pascal.md
+++ b/research/deeplab/g3doc/pascal.md
@@ -44,14 +44,7 @@ A local training job using `xception_65` can be run with the following command:
 # From tensorflow/models/research/
 python deeplab/train.py \
     --logtostderr \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
     --training_number_of_steps=30000 \
->>>>>>> origin/master
-=======
-    --training_number_of_steps=30000 \
->>>>>>> origin/master
     --train_split="train" \
     --model_variant="xception_65" \
     --atrous_rates=6 \
@@ -62,16 +55,8 @@ python deeplab/train.py \
     --train_crop_size=513 \
     --train_crop_size=513 \
     --train_batch_size=1 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
-    --dataset="pascal_voc_seg" \
-    --train_split="train" \
->>>>>>> origin/master
-=======
     --dataset="pascal_voc_seg" \
     --train_split="train" \
->>>>>>> origin/master
     --tf_initial_checkpoints=${PATH_TO_INITIAL_CHECKPOINT} \
     --train_logdir=${PATH_TO_TRAIN_DIR} \
     --dataset_dir=${PATH_TO_DATASET}
@@ -83,16 +68,6 @@ directory in which training checkpoints and events will be written to, and
 ${PATH_TO_DATASET} is the directory in which the PASCAL VOC 2012 dataset
 resides.
 
-<<<<<<< HEAD
-<<<<<<< HEAD
-Note that for {train,eval,vis}.py:
-
-1.  We use small batch size during training. The users could change it based on
-    the available GPU memory and also set `fine_tune_batch_norm` to be False or
-    True depending on the use case.
-=======
-=======
->>>>>>> origin/master
 **Note that for {train,eval,vis}.py:**
 
 1.  In order to reproduce our results, one needs to use large batch size (> 12),
@@ -101,10 +76,6 @@ Note that for {train,eval,vis}.py:
     GPU memory at hand, please fine-tune from our provided checkpoints whose
     batch norm parameters have been trained, and use smaller learning rate with
     fine_tune_batch_norm = False.
-<<<<<<< HEAD
->>>>>>> origin/master
-=======
->>>>>>> origin/master
 
 2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
     setting output_stride=8.
@@ -128,16 +99,8 @@ python deeplab/eval.py \
     --decoder_output_stride=4 \
     --eval_crop_size=513 \
     --eval_crop_size=513 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
     --dataset="pascal_voc_seg" \
     --eval_split="val" \
->>>>>>> origin/master
-=======
-    --dataset="pascal_voc_seg" \
-    --eval_split="val" \
->>>>>>> origin/master
     --checkpoint_dir=${PATH_TO_CHECKPOINT} \
     --eval_logdir=${PATH_TO_EVAL_DIR} \
     --dataset_dir=${PATH_TO_DATASET}
@@ -164,16 +127,8 @@ python deeplab/vis.py \
     --decoder_output_stride=4 \
     --vis_crop_size=513 \
     --vis_crop_size=513 \
-<<<<<<< HEAD
-<<<<<<< HEAD
-=======
-    --dataset="pascal_voc_seg" \
-    --vis_split="val" \
->>>>>>> origin/master
-=======
     --dataset="pascal_voc_seg" \
     --vis_split="val" \
->>>>>>> origin/master
     --checkpoint_dir=${PATH_TO_CHECKPOINT} \
     --vis_logdir=${PATH_TO_VIS_DIR} \
     --dataset_dir=${PATH_TO_DATASET}
