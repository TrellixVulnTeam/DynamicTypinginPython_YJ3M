commit ee832b6664dcff991ccb0554d3ce4b4e16b0bd68
Author: Marianne Linhares Monteiro <monteirom@google.com>
Date:   Wed Aug 2 09:48:37 2017 -0400

    Adding adjust_learning_rate option

diff --git a/tutorials/image/cifar10_estimator/cifar10_main.py b/tutorials/image/cifar10_estimator/cifar10_main.py
index 02daee3d..335da1ac 100644
--- a/tutorials/image/cifar10_estimator/cifar10_main.py
+++ b/tutorials/image/cifar10_estimator/cifar10_main.py
@@ -73,23 +73,31 @@ tf.flags.DEFINE_float('momentum', 0.9, 'Momentum for MomentumOptimizer.')
 
 tf.flags.DEFINE_float('weight_decay', 2e-4, 'Weight decay for convolutions.')
 
+tf.flags.DEFINE_float('adjust_learning_rate', 1,
+                      """This value will be multiplied by the learning rate.
+                      By default the learning rate is
+                      [0.1, 0.001, 0.0001, 0.00002]
+                      """.)
+
 tf.flags.DEFINE_boolean('use_distortion_for_training', True,
                         'If doing image distortion for training.')
 
 tf.flags.DEFINE_boolean('run_experiment', False,
-                        'If True will run an experiment,'
-                        'otherwise will run training and evaluation'
-                        'using the estimator interface.'
-                        'Experiments perform training on several workers in'
-                        'parallel, in other words experiments know how to'
-                        ' invoke train and eval in a sensible fashion for'
-                        ' distributed training.')
+                        """If True will run an experiment,
+                        otherwise will run training and evaluation
+                        using the estimator interface.
+                        Experiments perform training on several workers in
+                        parallel, in other words experiments know how to
+                        invoke train and eval in a sensible fashion for
+                        distributed training.
+                        """)
 
 tf.flags.DEFINE_boolean('sync', False,
-                        'If true when running in a distributed environment'
-                        'will run on sync mode')
+                        """If true when running in a distributed environment
+                        will run on sync mode.
+                        """)
 
-tf.flags.DEFINE_integer('num_workers', 1, 'Number of workers')
+tf.flags.DEFINE_integer('num_workers', 1, 'Number of workers.')
 
 # Perf flags
 tf.flags.DEFINE_integer('num_intra_threads', 1,
@@ -308,7 +316,10 @@ def _resnet_model_fn(features, labels, mode):
         num_batches_per_epoch * x
         for x in np.array([82, 123, 300], dtype=np.int64)
     ]
-    staged_lr = [0.1, 0.01, 0.001, 0.0002]
+    staged_lr = [
+        FLAGS.adjust_learning_rate * x 
+        for x in [0.1, 0.01, 0.001, 0.0002]]
+    
     learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),
                                                 boundaries, staged_lr)
     # Create a nicely-named tensor for logging
