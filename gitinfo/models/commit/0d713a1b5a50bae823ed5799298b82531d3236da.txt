commit 0d713a1b5a50bae823ed5799298b82531d3236da
Author: Alan Yee <alyee@ucsd.edu>
Date:   Fri Aug 18 14:28:11 2017 -0700

    Update VariationalAutoencoderRunner
    
    -Fixed print styling
    -Spaced import statements according to PEP 8

diff --git a/autoencoder/VariationalAutoencoderRunner.py b/autoencoder/VariationalAutoencoderRunner.py
index 11426817..5debf1c4 100644
--- a/autoencoder/VariationalAutoencoderRunner.py
+++ b/autoencoder/VariationalAutoencoderRunner.py
@@ -1,5 +1,8 @@
-import numpy as np
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
+import numpy as np
 import sklearn.preprocessing as prep
 import tensorflow as tf
 from tensorflow.examples.tutorials.mnist import input_data
@@ -9,7 +12,6 @@ from autoencoder_models.VariationalAutoencoder import VariationalAutoencoder
 mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
 
 
-
 def min_max_scale(X_train, X_test):
     preprocessor = prep.MinMaxScaler().fit(X_train)
     X_train = preprocessor.transform(X_train)
@@ -29,9 +31,10 @@ training_epochs = 20
 batch_size = 128
 display_step = 1
 
-autoencoder = VariationalAutoencoder(n_input = 784,
-                                     n_hidden = 200,
-                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+autoencoder = VariationalAutoencoder(
+    n_input = 784,
+    n_hidden = 200,
+    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
 
 for epoch in range(training_epochs):
     avg_cost = 0.
@@ -47,6 +50,6 @@ for epoch in range(training_epochs):
 
     # Display logs per epoch step
     if epoch % display_step == 0:
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1), "Cost: ", "{:.9f}".format(avg_cost))
 
 print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
