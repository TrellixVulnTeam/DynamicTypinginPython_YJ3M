commit 91495d139f81bb91afe318d611b5a5e8674ef795
Author: Chen Chen <chendouble@google.com>
Date:   Wed Apr 8 12:26:48 2020 -0700

    Internal change
    
    PiperOrigin-RevId: 305530702

diff --git a/official/nlp/bert/bert_models.py b/official/nlp/bert/bert_models.py
index 462cb7f9..fb497896 100644
--- a/official/nlp/bert/bert_models.py
+++ b/official/nlp/bert/bert_models.py
@@ -121,7 +121,8 @@ def get_transformer_encoder(bert_config,
         attention_dropout_rate=bert_config.attention_probs_dropout_prob,
     )
     kwargs = dict(embedding_cfg=embedding_cfg, hidden_cfg=hidden_cfg,
-                  num_hidden_instances=bert_config.num_hidden_layers,)
+                  num_hidden_instances=bert_config.num_hidden_layers,
+                  num_output_classes=bert_config.hidden_size)
 
     # Relies on gin configuration to define the Transformer encoder arguments.
     return transformer_encoder_cls(**kwargs)
