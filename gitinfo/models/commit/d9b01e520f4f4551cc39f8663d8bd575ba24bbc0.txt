commit d9b01e520f4f4551cc39f8663d8bd575ba24bbc0
Author: Alan Yee <alyee@ucsd.edu>
Date:   Fri Aug 18 14:35:39 2017 -0700

    Update AutoencoderRunner.py
    
    -Fixed print output styling
    -Fixed code according to PEP 8

diff --git a/autoencoder/AutoencoderRunner.py b/autoencoder/AutoencoderRunner.py
index 4d708742..6d3808fa 100644
--- a/autoencoder/AutoencoderRunner.py
+++ b/autoencoder/AutoencoderRunner.py
@@ -1,5 +1,8 @@
-import numpy as np
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
+import numpy as np
 import sklearn.preprocessing as prep
 import tensorflow as tf
 from tensorflow.examples.tutorials.mnist import input_data
@@ -8,16 +11,19 @@ from autoencoder_models.Autoencoder import Autoencoder
 
 mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
 
+
 def standard_scale(X_train, X_test):
     preprocessor = prep.StandardScaler().fit(X_train)
     X_train = preprocessor.transform(X_train)
     X_test = preprocessor.transform(X_test)
     return X_train, X_test
 
+
 def get_random_block_from_data(data, batch_size):
     start_index = np.random.randint(0, len(data) - batch_size)
     return data[start_index:(start_index + batch_size)]
 
+
 X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
 
 n_samples = int(mnist.train.num_examples)
@@ -25,10 +31,11 @@ training_epochs = 20
 batch_size = 128
 display_step = 1
 
-autoencoder = Autoencoder(n_input = 784,
-                          n_hidden = 200,
-                          transfer_function = tf.nn.softplus,
-                          optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+autoencoder = Autoencoder(
+    n_input = 784,
+    n_hidden = 200,
+    transfer_function = tf.nn.softplus,
+    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
 
 for epoch in range(training_epochs):
     avg_cost = 0.
@@ -44,6 +51,7 @@ for epoch in range(training_epochs):
 
     # Display logs per epoch step
     if epoch % display_step == 0:
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
-
+        print("Epoch: ", '%d,' % (epoch + 1),
+              "Cost: ", "{:.9f}".format(avg_cost))
+        
 print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
