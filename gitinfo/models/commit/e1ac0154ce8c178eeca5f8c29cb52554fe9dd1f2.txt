commit e1ac0154ce8c178eeca5f8c29cb52554fe9dd1f2
Author: Alexander Gorban <gorban@google.com>
Date:   Fri Nov 10 21:28:31 2017 -0800

    Add the missing is_training to batch_norm and dropout.

diff --git a/research/attention_ocr/python/model.py b/research/attention_ocr/python/model.py
index 7c49d864..2a85302c 100644
--- a/research/attention_ocr/python/model.py
+++ b/research/attention_ocr/python/model.py
@@ -201,9 +201,11 @@ class Model(object):
     with tf.variable_scope('conv_tower_fn/INCE'):
       if reuse:
         tf.get_variable_scope().reuse_variables()
-      with slim.arg_scope(inception.inception_v3_arg_scope()):
-        net, _ = inception.inception_v3_base(
-            images, final_endpoint=mparams.final_endpoint)
+      with slim.arg_scope(
+        [slim.batch_norm, slim.dropout], is_training=is_training):  
+          with slim.arg_scope(inception.inception_v3_arg_scope()):
+            net, _ = inception.inception_v3_base(
+                images, final_endpoint=mparams.final_endpoint)
       return net
 
   def _create_lstm_inputs(self, net):
