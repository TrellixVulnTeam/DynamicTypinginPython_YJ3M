commit 4140da21e24e55404c502b1d07be8fb775f8a719
Author: Hongkun Yu <hongkuny@google.com>
Date:   Wed Jun 24 22:27:41 2020 -0700

    Multichannel attention: Override _build_attention method instead of build()
    
    PiperOrigin-RevId: 318208409

diff --git a/official/nlp/modeling/layers/multi_channel_attention.py b/official/nlp/modeling/layers/multi_channel_attention.py
index 499d977c..160c8aae 100644
--- a/official/nlp/modeling/layers/multi_channel_attention.py
+++ b/official/nlp/modeling/layers/multi_channel_attention.py
@@ -117,8 +117,8 @@ class MultiChannelAttention(attention.MultiHeadAttention):
   cross-attention target sequences.
   """
 
-  def build(self, input_shape):
-    super(MultiChannelAttention, self).build(input_shape)
+  def _build_attention(self, qkv_rank):
+    super(MultiChannelAttention, self)._build_attention(qkv_rank)
     self._masked_softmax = masked_softmax.MaskedSoftmax(mask_expansion_axes=[2])
 
   def call(self, inputs, attention_mask=None):
