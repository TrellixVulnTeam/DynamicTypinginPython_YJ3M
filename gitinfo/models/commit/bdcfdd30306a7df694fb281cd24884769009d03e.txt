commit bdcfdd30306a7df694fb281cd24884769009d03e
Author: yukun <yukun@google.com>
Date:   Mon Mar 5 15:29:23 2018 -0800

    Added deeplab model to research folder

diff --git a/CODEOWNERS b/CODEOWNERS
index eb2e12c4..c2977b4e 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -8,6 +8,7 @@
 /research/brain_coder/ @danabo
 /research/cognitive_mapping_and_planning/ @s-gupta
 /research/compression/ @nmjohn
+/research/deeplab/ @aquariusjay @yknzhu @gpapan
 /research/delf/ @andrefaraujo
 /research/differential_privacy/ @panyx0718
 /research/domain_adaptation/ @bousmalis @dmrd
diff --git a/research/deeplab/README.md b/research/deeplab/README.md
new file mode 100644
index 00000000..8ad49221
--- /dev/null
+++ b/research/deeplab/README.md
@@ -0,0 +1,159 @@
+# DeepLab: Deep Labelling for Semantic Image Segmentation
+
+DeepLab is a state-of-art deep learning model for semantic image segmentation,
+where the goal is to assign semantic labels (e.g., person, dog, cat and so on)
+to every pixel in the input image. Current implementation includes the following
+features:
+
+1.  DeepLabv1 [1]: We use *atrous convolution* to explicitly control the
+    resolution at which feature responses are computed within Deep Convolutional
+    Neural Networks.
+
+2.  DeepLabv2 [2]: We use *atrous spatial pyramid pooling* (ASPP) to robustly
+    segment objects at multiple scales with filters at multiple sampling rates
+    and effective fields-of-views.
+
+3.  DeepLabv3 [3]: We augment the ASPP module with *image-level feature* [5, 6]
+    to capture longer range information. We also include *batch normalization*
+    [7] parameters to facilitate the training. In particular, we applying atrous
+    convolution to extract output features at different output strides during
+    training and evaluation, which efficiently enables training BN at output
+    stride = 16 and attains a high performance at output stride = 8 during
+    evaluation.
+
+4.  DeepLabv3+ [4]: We extend DeepLabv3 to include a simple yet effective
+    decoder module to refine the segmentation results especially along object
+    boundaries. Furthermore, in this encoder-decoder structure one can
+    arbitrarily control the resolution of extracted encoder features by atrous
+    convolution to trade-off precision and runtime.
+
+If you find the code useful for your research, please consider citing our latest
+work:
+
+```
+@article{deeplabv3plus2018,
+  title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
+  author={Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},
+  journal={arXiv:1802.02611},
+  year={2018}
+}
+```
+
+
+In the current implementation, we support adopting the following network
+backbones:
+
+1.  MobileNetv2 [8]: A fast network structure designed for mobile devices. **We
+will provide MobileNetv2 support in the next update. Please stay tuned.**
+
+2.  Xception [9, 10]: A powerful network structure intended for server-side
+    deployment.
+
+This directory contains our TensorFlow [11] implementation. We provide codes
+allowing users to train the model, evaluate results in terms of mIOU (mean
+intersection-over-union), and visualize segmentation results. We use PASCAL VOC
+2012 [12] and Cityscapes [13] semantic segmentation benchmarks as an example in
+the code.
+
+Some segmentation results on Flickr images:
+<p align="center">
+    <img src="g3doc/img/vis1.png" width=600></br>
+    <img src="g3doc/img/vis2.png" width=600></br>
+    <img src="g3doc/img/vis3.png" width=600></br>
+</p>
+
+## Contacts (Maintainers)
+
+*   Liang-Chieh Chen, github: [aquariusjay](https://github.com/aquariusjay)
+*   YuKun Zhu, github: [yknzhu](https://github.com/YknZhu)
+*   George Papandreou, github: [gpapan](https://github.com/gpapan)
+
+## Tables of Contents
+
+Demo:
+
+*   <a href='deeplab_demo.ipynb'>Jupyter notebook for off-the-shelf inference.</a><br>
+
+Running:
+
+*   <a href='g3doc/installation.md'>Installation.</a><br>
+*   <a href='g3doc/pascal.md'>Running DeepLab on PASCAL VOC 2012 semantic segmentation dataset.</a><br>
+*   <a href='g3doc/cityscapes.md'>Running DeepLab on Cityscapes semantic segmentation dataset.</a><br>
+
+Models:
+
+*   <a href='g3doc/model_zoo.md'>Checkpoints and frozen inference graphs.</a><br>
+
+Misc:
+
+*   Please check <a href='g3doc/faq.md'>FAQ<a/a> if you have some questions before reporting the issues.<br>
+
+## Getting Help
+
+To get help with issues you may encounter while using the DeepLab Tensorflow
+implementation, create a new question on [StackOverflow](https://stackoverflow.com/)
+with the tags "tensorflow" and "deeplab".
+
+Please report bugs (i.e., broken code, not usage questions) to the
+tensorflow/models GitHub [issue
+tracker](https://github.com/tensorflow/models/issues), prefixing the issue name
+with "deeplab".
+
+## References
+
+1.  **Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs**<br />
+    Liang-Chieh Chen+, George Papandreou+, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille (+ equal
+    contribution). <br />
+    [[link]](https://arxiv.org/abs/1412.7062). In ICLR, 2015.
+
+2.  **DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,**
+    **Atrous Convolution, and Fully Connected CRFs** <br />
+    Liang-Chieh Chen+, George Papandreou+, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille (+ equal
+    contribution). <br />
+    [[link]](http://arxiv.org/abs/1606.00915). TPAMI 2017.
+
+3.  **Rethinking Atrous Convolution for Semantic Image Segmentation**<br />
+    Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam.<br />
+    [[link]](http://arxiv.org/abs/1706.05587). arXiv: 1706.05587, 2017.
+
+4.  **Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation**<br />
+    Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam. arXiv: 1802.02611.<br />
+    [[link]](https://arxiv.org/abs/1802.02611). arXiv: 1802.02611, 2018.
+
+5.  **ParseNet: Looking Wider to See Better**<br />
+    Wei Liu, Andrew Rabinovich, Alexander C Berg<br />
+    [[link]](https://arxiv.org/abs/1506.04579). arXiv:1506.04579, 2015.
+
+6.  **Pyramid Scene Parsing Network**<br />
+    Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia<br />
+    [[link]](https://arxiv.org/abs/1612.01105). In CVPR, 2017.
+
+7.  **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate shift**<br />
+    Sergey Ioffe, Christian Szegedy <br />
+    [[link]](https://arxiv.org/abs/1502.03167). In ICML, 2015.
+
+8.  **Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation**<br />
+    Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen<br />
+    [[link]](https://arxiv.org/abs/1801.04381). arXiv:1801.04381, 2018.
+
+9.  **Xception: Deep Learning with Depthwise Separable Convolutions**<br />
+    François Chollet<br />
+    [[link]](https://arxiv.org/abs/1610.02357). In CVPR, 2017.
+
+10. **Deformable Convolutional Networks -- COCO Detection and Segmentation Challenge 2017 Entry**<br />
+    Haozhi Qi, Zheng Zhang, Bin Xiao, Han Hu, Bowen Cheng, Yichen Wei, Jifeng Dai<br />
+    [[link]](http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf). ICCV COCO Challenge
+    Workshop, 2017.
+
+11. **Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems**<br />
+    M. Abadi, A. Agarwal, et al. <br />
+    [[link]](https://arxiv.org/abs/1603.04467). arXiv:1603.04467, 2016.
+
+12. **The Pascal Visual Object Classes Challenge – A Retrospective,** <br />
+    Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John
+    Winn, and Andrew Zisserma. <br />
+    [[link]](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/). IJCV, 2014.
+
+13. **The Cityscapes Dataset for Semantic Urban Scene Understanding**<br />
+    Cordts, Marius, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele. <br />
+    [[link]](https://www.cityscapes-dataset.com/). In CVPR, 2016.
diff --git a/research/deeplab/__init__.py b/research/deeplab/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/deeplab/common.py b/research/deeplab/common.py
new file mode 100644
index 00000000..28741e53
--- /dev/null
+++ b/research/deeplab/common.py
@@ -0,0 +1,138 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Provides flags that are common to scripts.
+
+Common flags from train/eval/vis/export_model.py are collected in this script.
+"""
+import collections
+
+import tensorflow as tf
+
+flags = tf.app.flags
+
+# Flags for input preprocessing.
+
+flags.DEFINE_integer('min_resize_value', None,
+                     'Desired size of the smaller image side.')
+
+flags.DEFINE_integer('max_resize_value', None,
+                     'Maximum allowed size of the larger image side.')
+
+flags.DEFINE_integer('resize_factor', None,
+                     'Resized dimensions are multiple of factor plus one.')
+
+# Model dependent flags.
+
+flags.DEFINE_integer('logits_kernel_size', 1,
+                     'The kernel size for the convolutional kernel that '
+                     'generates logits.')
+
+# We will support `mobilenet_v2' in the coming update. When using
+# 'xception_65', we set atrous_rates = [6, 12, 18] (output stride 16) and
+# decoder_output_stride = 4.
+flags.DEFINE_enum('model_variant', 'xception_65', ['xception_65'],
+                  'DeepLab model variants.')
+
+flags.DEFINE_multi_float('image_pyramid', None,
+                         'Input scales for multi-scale feature extraction.')
+
+flags.DEFINE_boolean('add_image_level_feature', True,
+                     'Add image level feature.')
+
+flags.DEFINE_boolean('aspp_with_batch_norm', True,
+                     'Use batch norm parameters for ASPP or not.')
+
+flags.DEFINE_boolean('aspp_with_separable_conv', True,
+                     'Use separable convolution for ASPP or not.')
+
+flags.DEFINE_multi_integer('multi_grid', None,
+                           'Employ a hierarchy of atrous rates for ResNet.')
+
+# For `xception_65`, use decoder_output_stride = 4.
+flags.DEFINE_integer('decoder_output_stride', None,
+                     'The ratio of input to output spatial resolution when '
+                     'employing decoder to refine segmentation results.')
+
+flags.DEFINE_boolean('decoder_use_separable_conv', True,
+                     'Employ separable convolution for decoder or not.')
+
+flags.DEFINE_enum('merge_method', 'max', ['max', 'avg'],
+                  'Scheme to merge multi scale features.')
+
+FLAGS = flags.FLAGS
+
+# Constants
+
+# Perform semantic segmentation predictions.
+OUTPUT_TYPE = 'semantic'
+
+# Semantic segmentation item names.
+LABELS_CLASS = 'labels_class'
+IMAGE = 'image'
+HEIGHT = 'height'
+WIDTH = 'width'
+IMAGE_NAME = 'image_name'
+LABEL = 'label'
+ORIGINAL_IMAGE = 'original_image'
+
+# Test set name.
+TEST_SET = 'test'
+
+
+class ModelOptions(
+    collections.namedtuple('ModelOptions', [
+        'outputs_to_num_classes',
+        'crop_size',
+        'atrous_rates',
+        'output_stride',
+        'merge_method',
+        'add_image_level_feature',
+        'aspp_with_batch_norm',
+        'aspp_with_separable_conv',
+        'multi_grid',
+        'decoder_output_stride',
+        'decoder_use_separable_conv',
+        'logits_kernel_size',
+        'model_variant'
+    ])):
+  """Immutable class to hold model options."""
+
+  __slots__ = ()
+
+  def __new__(cls,
+              outputs_to_num_classes,
+              crop_size=None,
+              atrous_rates=None,
+              output_stride=8):
+    """Constructor to set default values.
+
+    Args:
+      outputs_to_num_classes: A dictionary from output type to the number of
+        classes. For example, for the task of semantic segmentation with 21
+        semantic classes, we would have outputs_to_num_classes['semantic'] = 21.
+      crop_size: A tuple [crop_height, crop_width].
+      atrous_rates: A list of atrous convolution rates for ASPP.
+      output_stride: The ratio of input to output spatial resolution.
+
+    Returns:
+      A new ModelOptions instance.
+    """
+    return super(ModelOptions, cls).__new__(
+        cls, outputs_to_num_classes, crop_size, atrous_rates, output_stride,
+        FLAGS.merge_method, FLAGS.add_image_level_feature,
+        FLAGS.aspp_with_batch_norm, FLAGS.aspp_with_separable_conv,
+        FLAGS.multi_grid, FLAGS.decoder_output_stride,
+        FLAGS.decoder_use_separable_conv, FLAGS.logits_kernel_size,
+        FLAGS.model_variant)
diff --git a/research/deeplab/core/__init__.py b/research/deeplab/core/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/deeplab/core/feature_extractor.py b/research/deeplab/core/feature_extractor.py
new file mode 100644
index 00000000..1ef93568
--- /dev/null
+++ b/research/deeplab/core/feature_extractor.py
@@ -0,0 +1,198 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Extracts features for different models."""
+import functools
+import tensorflow as tf
+
+from deeplab.core import xception
+
+
+slim = tf.contrib.slim
+
+
+# A map from network name to network function.
+networks_map = {
+    'xception_65': xception.xception_65,
+}
+
+# A map from network name to network arg scope.
+arg_scopes_map = {
+    'xception_65': xception.xception_arg_scope,
+}
+
+# Names for end point features.
+DECODER_END_POINTS = 'decoder_end_points'
+
+# A dictionary from network name to a map of end point features.
+networks_to_feature_maps = {
+    'xception_65': {
+        DECODER_END_POINTS: [
+            'entry_flow/block2/unit_1/xception_module/'
+            'separable_conv2_pointwise',
+        ],
+    }
+}
+
+# A map from feature extractor name to the network name scope used in the
+# ImageNet pretrained versions of these models.
+name_scope = {
+    'xception_65': 'xception_65',
+}
+
+# Mean pixel value.
+_MEAN_RGB = [123.15, 115.90, 103.06]
+
+
+def _preprocess_subtract_imagenet_mean(inputs):
+  """Subtract Imagenet mean RGB value."""
+  mean_rgb = tf.reshape(_MEAN_RGB, [1, 1, 1, 3])
+  return inputs - mean_rgb
+
+
+def _preprocess_zero_mean_unit_range(inputs):
+  """Map image values from [0, 255] to [-1, 1]."""
+  return (2.0 / 255.0) * tf.to_float(inputs) - 1.0
+
+
+_PREPROCESS_FN = {
+    'xception_65': _preprocess_zero_mean_unit_range,
+}
+
+
+def mean_pixel(model_variant=None):
+  """Gets mean pixel value.
+
+  This function returns different mean pixel value, depending on the input
+  model_variant which adopts different preprocessing functions. We currently
+  handle the following preprocessing functions:
+  (1) _preprocess_subtract_imagenet_mean. We simply return mean pixel value.
+  (2) _preprocess_zero_mean_unit_range. We return [127.5, 127.5, 127.5].
+  The return values are used in a way that the padded regions after
+  pre-processing will contain value 0.
+
+  Args:
+    model_variant: Model variant (string) for feature extraction. For
+      backwards compatibility, model_variant=None returns _MEAN_RGB.
+
+  Returns:
+    Mean pixel value.
+  """
+  if model_variant is None:
+    return _MEAN_RGB
+  else:
+    return [127.5, 127.5, 127.5]
+
+
+def extract_features(images,
+                     output_stride=8,
+                     multi_grid=None,
+                     model_variant=None,
+                     weight_decay=0.0001,
+                     reuse=None,
+                     is_training=False,
+                     fine_tune_batch_norm=False,
+                     regularize_depthwise=False,
+                     preprocess_images=True,
+                     num_classes=None,
+                     global_pool=False):
+  """Extracts features by the parituclar model_variant.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    output_stride: The ratio of input to output spatial resolution.
+    multi_grid: Employ a hierarchy of different atrous rates within network.
+    model_variant: Model variant for feature extraction.
+    weight_decay: The weight decay for model variables.
+    reuse: Reuse the model variables or not.
+    is_training: Is training or not.
+    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    regularize_depthwise: Whether or not apply L2-norm regularization on the
+      depthwise convolution weights.
+    preprocess_images: Performs preprocessing on images or not. Defaults to
+      True. Set to False if preprocessing will be done by other functions. We
+      supprot two types of preprocessing: (1) Mean pixel substraction and (2)
+      Pixel values normalization to be [-1, 1].
+    num_classes: Number of classes for image classification task. Defaults
+      to None for dense prediction tasks.
+    global_pool: Global pooling for image classification task. Defaults to
+      False, since dense prediction tasks do not use this.
+
+  Returns:
+    features: A tensor of size [batch, feature_height, feature_width,
+      feature_channels], where feature_height/feature_width are determined
+      by the images height/width and output_stride.
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+
+  Raises:
+    ValueError: Unrecognized model variant.
+  """
+  if 'xception' in model_variant:
+    arg_scope = arg_scopes_map[model_variant](
+        weight_decay=weight_decay,
+        batch_norm_decay=0.9997,
+        batch_norm_epsilon=1e-3,
+        batch_norm_scale=True,
+        regularize_depthwise=regularize_depthwise)
+    features, end_points = get_network(
+        model_variant, preprocess_images, arg_scope)(
+            inputs=images,
+            num_classes=num_classes,
+            is_training=(is_training and fine_tune_batch_norm),
+            global_pool=global_pool,
+            output_stride=output_stride,
+            regularize_depthwise=regularize_depthwise,
+            multi_grid=multi_grid,
+            reuse=reuse,
+            scope=name_scope[model_variant])
+  elif 'mobilenet' in model_variant:
+    raise ValueError('MobileNetv2 support is coming soon.')
+  else:
+    raise ValueError('Unknown model variant %s.' % model_variant)
+
+  return features, end_points
+
+
+def get_network(network_name, preprocess_images, arg_scope=None):
+  """Gets the network.
+
+  Args:
+    network_name: Network name.
+    preprocess_images: Preprocesses the images or not.
+    arg_scope: Optional, arg_scope to build the network. If not provided the
+      default arg_scope of the network would be used.
+
+  Returns:
+    A network function that is used to extract features.
+
+  Raises:
+    ValueError: network is not supported.
+  """
+  if network_name not in networks_map:
+    raise ValueError('Unsupported network %s.' % network_name)
+  arg_scope = arg_scope or arg_scopes_map[network_name]()
+  def _identity_function(inputs):
+    return inputs
+  if preprocess_images:
+    preprocess_function = _PREPROCESS_FN[network_name]
+  else:
+    preprocess_function = _identity_function
+  func = networks_map[network_name]
+  @functools.wraps(func)
+  def network_fn(inputs, *args, **kwargs):
+    with slim.arg_scope(arg_scope):
+      return func(preprocess_function(inputs), *args, **kwargs)
+  return network_fn
diff --git a/research/deeplab/core/preprocess_utils.py b/research/deeplab/core/preprocess_utils.py
new file mode 100644
index 00000000..f6026505
--- /dev/null
+++ b/research/deeplab/core/preprocess_utils.py
@@ -0,0 +1,445 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Utility functions related to preprocessing inputs."""
+import tensorflow as tf
+
+
+def flip_dim(tensor_list, prob=0.5, dim=1):
+  """Randomly flips a dimension of the given tensor.
+
+  The decision to randomly flip the `Tensors` is made together. In other words,
+  all or none of the images pass in are flipped.
+
+  Note that tf.random_flip_left_right and tf.random_flip_up_down isn't used so
+  that we can control for the probability as well as ensure the same decision
+  is applied across the images.
+
+  Args:
+    tensor_list: A list of `Tensors` with the same number of dimensions.
+    prob: The probability of a left-right flip.
+    dim: The dimension to flip, 0, 1, ..
+
+  Returns:
+    outputs: A list of the possibly flipped `Tensors` as well as an indicator
+    `Tensor` at the end whose value is `True` if the inputs were flipped and
+    `False` otherwise.
+
+  Raises:
+    ValueError: If dim is negative or greater than the dimension of a `Tensor`.
+  """
+  random_value = tf.random_uniform([])
+
+  def flip():
+    flipped = []
+    for tensor in tensor_list:
+      if dim < 0 or dim >= len(tensor.get_shape().as_list()):
+        raise ValueError('dim must represent a valid dimension.')
+      flipped.append(tf.reverse_v2(tensor, [dim]))
+    return flipped
+
+  is_flipped = tf.less_equal(random_value, prob)
+  outputs = tf.cond(is_flipped, flip, lambda: tensor_list)
+  if not isinstance(outputs, (list, tuple)):
+    outputs = [outputs]
+  outputs.append(is_flipped)
+
+  return outputs
+
+
+def pad_to_bounding_box(image, offset_height, offset_width, target_height,
+                        target_width, pad_value):
+  """Pads the given image with the given pad_value.
+
+  Works like tf.image.pad_to_bounding_box, except it can pad the image
+  with any given arbitrary pad value and also handle images whose sizes are not
+  known during graph construction.
+
+  Args:
+    image: 3-D tensor with shape [height, width, channels]
+    offset_height: Number of rows of zeros to add on top.
+    offset_width: Number of columns of zeros to add on the left.
+    target_height: Height of output image.
+    target_width: Width of output image.
+    pad_value: Value to pad the image tensor with.
+
+  Returns:
+    3-D tensor of shape [target_height, target_width, channels].
+
+  Raises:
+    ValueError: If the shape of image is incompatible with the offset_* or
+    target_* arguments.
+  """
+  image_rank = tf.rank(image)
+  image_rank_assert = tf.Assert(
+      tf.equal(image_rank, 3),
+      ['Wrong image tensor rank [Expected] [Actual]',
+       3, image_rank])
+  with tf.control_dependencies([image_rank_assert]):
+    image -= pad_value
+  image_shape = tf.shape(image)
+  height, width = image_shape[0], image_shape[1]
+  target_width_assert = tf.Assert(
+      tf.greater_equal(
+          target_width, width),
+      ['target_width must be >= width'])
+  target_height_assert = tf.Assert(
+      tf.greater_equal(target_height, height),
+      ['target_height must be >= height'])
+  with tf.control_dependencies([target_width_assert]):
+    after_padding_width = target_width - offset_width - width
+  with tf.control_dependencies([target_height_assert]):
+    after_padding_height = target_height - offset_height - height
+  offset_assert = tf.Assert(
+      tf.logical_and(
+          tf.greater_equal(after_padding_width, 0),
+          tf.greater_equal(after_padding_height, 0)),
+      ['target size not possible with the given target offsets'])
+
+  height_params = tf.stack([offset_height, after_padding_height])
+  width_params = tf.stack([offset_width, after_padding_width])
+  channel_params = tf.stack([0, 0])
+  with tf.control_dependencies([offset_assert]):
+    paddings = tf.stack([height_params, width_params, channel_params])
+  padded = tf.pad(image, paddings)
+  return padded + pad_value
+
+
+def _crop(image, offset_height, offset_width, crop_height, crop_width):
+  """Crops the given image using the provided offsets and sizes.
+
+  Note that the method doesn't assume we know the input image size but it does
+  assume we know the input image rank.
+
+  Args:
+    image: an image of shape [height, width, channels].
+    offset_height: a scalar tensor indicating the height offset.
+    offset_width: a scalar tensor indicating the width offset.
+    crop_height: the height of the cropped image.
+    crop_width: the width of the cropped image.
+
+  Returns:
+    The cropped (and resized) image.
+
+  Raises:
+    ValueError: if `image` doesn't have rank of 3.
+    InvalidArgumentError: if the rank is not 3 or if the image dimensions are
+      less than the crop size.
+  """
+  original_shape = tf.shape(image)
+
+  if len(image.get_shape().as_list()) != 3:
+    raise ValueError('input must have rank of 3')
+  original_channels = image.get_shape().as_list()[2]
+
+  rank_assertion = tf.Assert(
+      tf.equal(tf.rank(image), 3),
+      ['Rank of image must be equal to 3.'])
+  with tf.control_dependencies([rank_assertion]):
+    cropped_shape = tf.stack([crop_height, crop_width, original_shape[2]])
+
+  size_assertion = tf.Assert(
+      tf.logical_and(
+          tf.greater_equal(original_shape[0], crop_height),
+          tf.greater_equal(original_shape[1], crop_width)),
+      ['Crop size greater than the image size.'])
+
+  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))
+
+  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to
+  # define the crop size.
+  with tf.control_dependencies([size_assertion]):
+    image = tf.slice(image, offsets, cropped_shape)
+  image = tf.reshape(image, cropped_shape)
+  image.set_shape([crop_height, crop_width, original_channels])
+  return image
+
+
+def random_crop(image_list, crop_height, crop_width):
+  """Crops the given list of images.
+
+  The function applies the same crop to each image in the list. This can be
+  effectively applied when there are multiple image inputs of the same
+  dimension such as:
+
+    image, depths, normals = random_crop([image, depths, normals], 120, 150)
+
+  Args:
+    image_list: a list of image tensors of the same dimension but possibly
+      varying channel.
+    crop_height: the new height.
+    crop_width: the new width.
+
+  Returns:
+    the image_list with cropped images.
+
+  Raises:
+    ValueError: if there are multiple image inputs provided with different size
+      or the images are smaller than the crop dimensions.
+  """
+  if not image_list:
+    raise ValueError('Empty image_list.')
+
+  # Compute the rank assertions.
+  rank_assertions = []
+  for i in range(len(image_list)):
+    image_rank = tf.rank(image_list[i])
+    rank_assert = tf.Assert(
+        tf.equal(image_rank, 3),
+        ['Wrong rank for tensor  %s [expected] [actual]',
+         image_list[i].name, 3, image_rank])
+    rank_assertions.append(rank_assert)
+
+  with tf.control_dependencies([rank_assertions[0]]):
+    image_shape = tf.shape(image_list[0])
+  image_height = image_shape[0]
+  image_width = image_shape[1]
+  crop_size_assert = tf.Assert(
+      tf.logical_and(
+          tf.greater_equal(image_height, crop_height),
+          tf.greater_equal(image_width, crop_width)),
+      ['Crop size greater than the image size.'])
+
+  asserts = [rank_assertions[0], crop_size_assert]
+
+  for i in range(1, len(image_list)):
+    image = image_list[i]
+    asserts.append(rank_assertions[i])
+    with tf.control_dependencies([rank_assertions[i]]):
+      shape = tf.shape(image)
+    height = shape[0]
+    width = shape[1]
+
+    height_assert = tf.Assert(
+        tf.equal(height, image_height),
+        ['Wrong height for tensor %s [expected][actual]',
+         image.name, height, image_height])
+    width_assert = tf.Assert(
+        tf.equal(width, image_width),
+        ['Wrong width for tensor %s [expected][actual]',
+         image.name, width, image_width])
+    asserts.extend([height_assert, width_assert])
+
+  # Create a random bounding box.
+  #
+  # Use tf.random_uniform and not numpy.random.rand as doing the former would
+  # generate random numbers at graph eval time, unlike the latter which
+  # generates random numbers at graph definition time.
+  with tf.control_dependencies(asserts):
+    max_offset_height = tf.reshape(image_height - crop_height + 1, [])
+    max_offset_width = tf.reshape(image_width - crop_width + 1, [])
+  offset_height = tf.random_uniform(
+      [], maxval=max_offset_height, dtype=tf.int32)
+  offset_width = tf.random_uniform(
+      [], maxval=max_offset_width, dtype=tf.int32)
+
+  return [_crop(image, offset_height, offset_width,
+                crop_height, crop_width) for image in image_list]
+
+
+def get_random_scale(min_scale_factor, max_scale_factor, step_size):
+  """Gets a random scale value.
+
+  Args:
+    min_scale_factor: Minimum scale value.
+    max_scale_factor: Maximum scale value.
+    step_size: The step size from minimum to maximum value.
+
+  Returns:
+    A random scale value selected between minimum and maximum value.
+
+  Raises:
+    ValueError: min_scale_factor has unexpected value.
+  """
+  if min_scale_factor < 0 or min_scale_factor > max_scale_factor:
+    raise ValueError('Unexpected value of min_scale_factor.')
+
+  if min_scale_factor == max_scale_factor:
+    return tf.to_float(min_scale_factor)
+
+  # When step_size = 0, we sample the value uniformly from [min, max).
+  if step_size == 0:
+    return tf.random_uniform([1],
+                             minval=min_scale_factor,
+                             maxval=max_scale_factor)
+
+  # When step_size != 0, we randomly select one discrete value from [min, max].
+  num_steps = int((max_scale_factor - min_scale_factor) / step_size + 1)
+  scale_factors = tf.lin_space(min_scale_factor, max_scale_factor, num_steps)
+  shuffled_scale_factors = tf.random_shuffle(scale_factors)
+  return shuffled_scale_factors[0]
+
+
+def randomly_scale_image_and_label(image, label=None, scale=1.0):
+  """Randomly scales image and label.
+
+  Args:
+    image: Image with shape [height, width, 3].
+    label: Label with shape [height, width, 1].
+    scale: The value to scale image and label.
+
+  Returns:
+    Scaled image and label.
+  """
+  # No random scaling if scale == 1.
+  if scale == 1.0:
+    return image, label
+  image_shape = tf.shape(image)
+  new_dim = tf.to_int32(tf.to_float([image_shape[0], image_shape[1]]) * scale)
+
+  # Need squeeze and expand_dims because image interpolation takes
+  # 4D tensors as input.
+  image = tf.squeeze(tf.image.resize_bilinear(
+      tf.expand_dims(image, 0),
+      new_dim,
+      align_corners=True), [0])
+  if label is not None:
+    label = tf.squeeze(tf.image.resize_nearest_neighbor(
+        tf.expand_dims(label, 0),
+        new_dim,
+        align_corners=True), [0])
+
+  return image, label
+
+
+def resolve_shape(tensor, rank=None, scope=None):
+  """Fully resolves the shape of a Tensor.
+
+  Use as much as possible the shape components already known during graph
+  creation and resolve the remaining ones during runtime.
+
+  Args:
+    tensor: Input tensor whose shape we query.
+    rank: The rank of the tensor, provided that we know it.
+    scope: Optional name scope.
+
+  Returns:
+    shape: The full shape of the tensor.
+  """
+  with tf.name_scope(scope, 'resolve_shape', [tensor]):
+    if rank is not None:
+      shape = tensor.get_shape().with_rank(rank).as_list()
+    else:
+      shape = tensor.get_shape().as_list()
+
+    if None in shape:
+      shape_dynamic = tf.shape(tensor)
+      for i in range(len(shape)):
+        if shape[i] is None:
+          shape[i] = shape_dynamic[i]
+
+    return shape
+
+
+def resize_to_range(image,
+                    label=None,
+                    min_size=None,
+                    max_size=None,
+                    factor=None,
+                    align_corners=True,
+                    label_layout_is_chw=False,
+                    scope=None,
+                    method=tf.image.ResizeMethod.BILINEAR):
+  """Resizes image or label so their sides are within the provided range.
+
+  The output size can be described by two cases:
+  1. If the image can be rescaled so its minimum size is equal to min_size
+     without the other side exceeding max_size, then do so.
+  2. Otherwise, resize so the largest side is equal to max_size.
+
+  An integer in `range(factor)` is added to the computed sides so that the
+  final dimensions are multiples of `factor` plus one.
+
+  Args:
+    image: A 3D tensor of shape [height, width, channels].
+    label: (optional) A 3D tensor of shape [height, width, channels] (default)
+      or [channels, height, width] when label_layout_is_chw = True.
+    min_size: (scalar) desired size of the smaller image side.
+    max_size: (scalar) maximum allowed size of the larger image side. Note
+      that the output dimension is no larger than max_size and may be slightly
+      smaller than min_size when factor is not None.
+    factor: Make output size multiple of factor plus one.
+    align_corners: If True, exactly align all 4 corners of input and output.
+    label_layout_is_chw: If true, the label has shape [channel, height, width].
+      We support this case because for some instance segmentation dataset, the
+      instance segmentation is saved as [num_instances, height, width].
+    scope: Optional name scope.
+    method: Image resize method. Defaults to tf.image.ResizeMethod.BILINEAR.
+
+  Returns:
+    A 3-D tensor of shape [new_height, new_width, channels], where the image
+    has been resized (with the specified method) so that
+    min(new_height, new_width) == ceil(min_size) or
+    max(new_height, new_width) == ceil(max_size).
+
+  Raises:
+    ValueError: If the image is not a 3D tensor.
+  """
+  with tf.name_scope(scope, 'resize_to_range', [image]):
+    new_tensor_list = []
+    min_size = tf.to_float(min_size)
+    if max_size is not None:
+      max_size = tf.to_float(max_size)
+      # Modify the max_size to be a multiple of factor plus 1 and make sure the
+      # max dimension after resizing is no larger than max_size.
+      if factor is not None:
+        max_size = (max_size + (factor - (max_size - 1) % factor) % factor
+                    - factor)
+
+    [orig_height, orig_width, _] = resolve_shape(image, rank=3)
+    orig_height = tf.to_float(orig_height)
+    orig_width = tf.to_float(orig_width)
+    orig_min_size = tf.minimum(orig_height, orig_width)
+
+    # Calculate the larger of the possible sizes
+    large_scale_factor = min_size / orig_min_size
+    large_height = tf.to_int32(tf.ceil(orig_height * large_scale_factor))
+    large_width = tf.to_int32(tf.ceil(orig_width * large_scale_factor))
+    large_size = tf.stack([large_height, large_width])
+
+    new_size = large_size
+    if max_size is not None:
+      # Calculate the smaller of the possible sizes, use that if the larger
+      # is too big.
+      orig_max_size = tf.maximum(orig_height, orig_width)
+      small_scale_factor = max_size / orig_max_size
+      small_height = tf.to_int32(tf.ceil(orig_height * small_scale_factor))
+      small_width = tf.to_int32(tf.ceil(orig_width * small_scale_factor))
+      small_size = tf.stack([small_height, small_width])
+      new_size = tf.cond(
+          tf.to_float(tf.reduce_max(large_size)) > max_size,
+          lambda: small_size,
+          lambda: large_size)
+    # Ensure that both output sides are multiples of factor plus one.
+    if factor is not None:
+      new_size += (factor - (new_size - 1) % factor) % factor
+    new_tensor_list.append(tf.image.resize_images(
+        image, new_size, method=method, align_corners=align_corners))
+    if label is not None:
+      if label_layout_is_chw:
+        # Input label has shape [channel, height, width].
+        resized_label = tf.expand_dims(label, 3)
+        resized_label = tf.image.resize_nearest_neighbor(
+            resized_label, new_size, align_corners=align_corners)
+        resized_label = tf.squeeze(resized_label, 3)
+      else:
+        # Input label has shape [height, width, channel].
+        resized_label = tf.image.resize_images(
+            label, new_size, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+            align_corners=align_corners)
+      new_tensor_list.append(resized_label)
+    else:
+      new_tensor_list.append(None)
+    return new_tensor_list
diff --git a/research/deeplab/core/preprocess_utils_test.py b/research/deeplab/core/preprocess_utils_test.py
new file mode 100644
index 00000000..bca14d4b
--- /dev/null
+++ b/research/deeplab/core/preprocess_utils_test.py
@@ -0,0 +1,432 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for preprocess_utils."""
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.framework import errors
+from deeplab.core import preprocess_utils
+
+
+class PreprocessUtilsTest(tf.test.TestCase):
+
+  def testNoFlipWhenProbIsZero(self):
+    numpy_image = np.dstack([[[5., 6.],
+                              [9., 0.]],
+                             [[4., 3.],
+                              [3., 5.]]])
+    image = tf.convert_to_tensor(numpy_image)
+
+    with self.test_session():
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=0, dim=0)
+      self.assertAllEqual(numpy_image, actual.eval())
+      self.assertAllEqual(False, is_flipped.eval())
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=0, dim=1)
+      self.assertAllEqual(numpy_image, actual.eval())
+      self.assertAllEqual(False, is_flipped.eval())
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=0, dim=2)
+      self.assertAllEqual(numpy_image, actual.eval())
+      self.assertAllEqual(False, is_flipped.eval())
+
+  def testFlipWhenProbIsOne(self):
+    numpy_image = np.dstack([[[5., 6.],
+                              [9., 0.]],
+                             [[4., 3.],
+                              [3., 5.]]])
+    dim0_flipped = np.dstack([[[9., 0.],
+                               [5., 6.]],
+                              [[3., 5.],
+                               [4., 3.]]])
+    dim1_flipped = np.dstack([[[6., 5.],
+                               [0., 9.]],
+                              [[3., 4.],
+                               [5., 3.]]])
+    dim2_flipped = np.dstack([[[4., 3.],
+                               [3., 5.]],
+                              [[5., 6.],
+                               [9., 0.]]])
+    image = tf.convert_to_tensor(numpy_image)
+
+    with self.test_session():
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=1, dim=0)
+      self.assertAllEqual(dim0_flipped, actual.eval())
+      self.assertAllEqual(True, is_flipped.eval())
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=1, dim=1)
+      self.assertAllEqual(dim1_flipped, actual.eval())
+      self.assertAllEqual(True, is_flipped.eval())
+      actual, is_flipped = preprocess_utils.flip_dim([image], prob=1, dim=2)
+      self.assertAllEqual(dim2_flipped, actual.eval())
+      self.assertAllEqual(True, is_flipped.eval())
+
+  def testFlipMultipleImagesConsistentlyWhenProbIsOne(self):
+    numpy_image = np.dstack([[[5., 6.],
+                              [9., 0.]],
+                             [[4., 3.],
+                              [3., 5.]]])
+    numpy_label = np.dstack([[[0., 1.],
+                              [2., 3.]]])
+    image_dim1_flipped = np.dstack([[[6., 5.],
+                                     [0., 9.]],
+                                    [[3., 4.],
+                                     [5., 3.]]])
+    label_dim1_flipped = np.dstack([[[1., 0.],
+                                     [3., 2.]]])
+    image = tf.convert_to_tensor(numpy_image)
+    label = tf.convert_to_tensor(numpy_label)
+
+    with self.test_session() as sess:
+      image, label, is_flipped = preprocess_utils.flip_dim(
+          [image, label], prob=1, dim=1)
+      actual_image, actual_label = sess.run([image, label])
+      self.assertAllEqual(image_dim1_flipped, actual_image)
+      self.assertAllEqual(label_dim1_flipped, actual_label)
+      self.assertEqual(True, is_flipped.eval())
+
+  def testReturnRandomFlipsOnMultipleEvals(self):
+    numpy_image = np.dstack([[[5., 6.],
+                              [9., 0.]],
+                             [[4., 3.],
+                              [3., 5.]]])
+    dim1_flipped = np.dstack([[[6., 5.],
+                               [0., 9.]],
+                              [[3., 4.],
+                               [5., 3.]]])
+    image = tf.convert_to_tensor(numpy_image)
+    tf.set_random_seed(53)
+
+    with self.test_session() as sess:
+      actual, is_flipped = preprocess_utils.flip_dim(
+          [image], prob=0.5, dim=1)
+      actual_image, actual_is_flipped = sess.run([actual, is_flipped])
+      self.assertAllEqual(numpy_image, actual_image)
+      self.assertEqual(False, actual_is_flipped)
+      actual_image, actual_is_flipped = sess.run([actual, is_flipped])
+      self.assertAllEqual(dim1_flipped, actual_image)
+      self.assertEqual(True, actual_is_flipped)
+
+  def testReturnCorrectCropOfSingleImage(self):
+    np.random.seed(0)
+
+    height, width = 10, 20
+    image = np.random.randint(0, 256, size=(height, width, 3))
+
+    crop_height, crop_width = 2, 4
+
+    image_placeholder = tf.placeholder(tf.int32, shape=(None, None, 3))
+    [cropped] = preprocess_utils.random_crop([image_placeholder],
+                                             crop_height,
+                                             crop_width)
+
+    with self.test_session():
+      cropped_image = cropped.eval(feed_dict={image_placeholder: image})
+
+    # Ensure we can find the cropped image in the original:
+    is_found = False
+    for x in range(0, width - crop_width + 1):
+      for y in range(0, height - crop_height + 1):
+        if np.isclose(image[y:y+crop_height, x:x+crop_width, :],
+                      cropped_image).all():
+          is_found = True
+          break
+
+    self.assertTrue(is_found)
+
+  def testRandomCropMaintainsNumberOfChannels(self):
+    np.random.seed(0)
+
+    crop_height, crop_width = 10, 20
+    image = np.random.randint(0, 256, size=(100, 200, 3))
+
+    tf.set_random_seed(37)
+    image_placeholder = tf.placeholder(tf.int32, shape=(None, None, 3))
+    [cropped] = preprocess_utils.random_crop(
+        [image_placeholder], crop_height, crop_width)
+
+    with self.test_session():
+      cropped_image = cropped.eval(feed_dict={image_placeholder: image})
+      self.assertTupleEqual(cropped_image.shape, (crop_height, crop_width, 3))
+
+  def testReturnDifferentCropAreasOnTwoEvals(self):
+    tf.set_random_seed(0)
+
+    crop_height, crop_width = 2, 3
+    image = np.random.randint(0, 256, size=(100, 200, 3))
+    image_placeholder = tf.placeholder(tf.int32, shape=(None, None, 3))
+    [cropped] = preprocess_utils.random_crop(
+        [image_placeholder], crop_height, crop_width)
+
+    with self.test_session():
+      crop0 = cropped.eval(feed_dict={image_placeholder: image})
+      crop1 = cropped.eval(feed_dict={image_placeholder: image})
+      self.assertFalse(np.isclose(crop0, crop1).all())
+
+  def testReturnConsistenCropsOfImagesInTheList(self):
+    tf.set_random_seed(0)
+
+    height, width = 10, 20
+    crop_height, crop_width = 2, 3
+    labels = np.linspace(0, height * width-1, height * width)
+    labels = labels.reshape((height, width, 1))
+    image = np.tile(labels, (1, 1, 3))
+
+    image_placeholder = tf.placeholder(tf.int32, shape=(None, None, 3))
+    label_placeholder = tf.placeholder(tf.int32, shape=(None, None, 1))
+    [cropped_image, cropped_label] = preprocess_utils.random_crop(
+        [image_placeholder, label_placeholder], crop_height, crop_width)
+
+    with self.test_session() as sess:
+      cropped_image, cropped_labels = sess.run([cropped_image, cropped_label],
+                                               feed_dict={
+                                                   image_placeholder: image,
+                                                   label_placeholder: labels})
+      for i in range(3):
+        self.assertAllEqual(cropped_image[:, :, i], cropped_labels.squeeze())
+
+  def testDieOnRandomCropWhenImagesWithDifferentWidth(self):
+    crop_height, crop_width = 2, 3
+    image1 = tf.placeholder(tf.float32, name='image1', shape=(None, None, 3))
+    image2 = tf.placeholder(tf.float32, name='image2', shape=(None, None, 1))
+    cropped = preprocess_utils.random_crop(
+        [image1, image2], crop_height, crop_width)
+
+    with self.test_session() as sess:
+      with self.assertRaises(errors.InvalidArgumentError):
+        sess.run(cropped, feed_dict={image1: np.random.rand(4, 5, 3),
+                                     image2: np.random.rand(4, 6, 1)})
+
+  def testDieOnRandomCropWhenImagesWithDifferentHeight(self):
+    crop_height, crop_width = 2, 3
+    image1 = tf.placeholder(tf.float32, name='image1', shape=(None, None, 3))
+    image2 = tf.placeholder(tf.float32, name='image2', shape=(None, None, 1))
+    cropped = preprocess_utils.random_crop(
+        [image1, image2], crop_height, crop_width)
+
+    with self.test_session() as sess:
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'Wrong height for tensor'):
+        sess.run(cropped, feed_dict={image1: np.random.rand(4, 5, 3),
+                                     image2: np.random.rand(3, 5, 1)})
+
+  def testDieOnRandomCropWhenCropSizeIsGreaterThanImage(self):
+    crop_height, crop_width = 5, 9
+    image1 = tf.placeholder(tf.float32, name='image1', shape=(None, None, 3))
+    image2 = tf.placeholder(tf.float32, name='image2', shape=(None, None, 1))
+    cropped = preprocess_utils.random_crop(
+        [image1, image2], crop_height, crop_width)
+
+    with self.test_session() as sess:
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'Crop size greater than the image size.'):
+        sess.run(cropped, feed_dict={image1: np.random.rand(4, 5, 3),
+                                     image2: np.random.rand(4, 5, 1)})
+
+  def testReturnPaddedImageWithNonZeroPadValue(self):
+    for dtype in [np.int32, np.int64, np.float32, np.float64]:
+      image = np.dstack([[[5, 6],
+                          [9, 0]],
+                         [[4, 3],
+                          [3, 5]]]).astype(dtype)
+      expected_image = np.dstack([[[255, 255, 255, 255, 255],
+                                   [255, 255, 255, 255, 255],
+                                   [255, 5, 6, 255, 255],
+                                   [255, 9, 0, 255, 255],
+                                   [255, 255, 255, 255, 255]],
+                                  [[255, 255, 255, 255, 255],
+                                   [255, 255, 255, 255, 255],
+                                   [255, 4, 3, 255, 255],
+                                   [255, 3, 5, 255, 255],
+                                   [255, 255, 255, 255, 255]]]).astype(dtype)
+
+      with self.test_session():
+        image_placeholder = tf.placeholder(tf.float32)
+        padded_image = preprocess_utils.pad_to_bounding_box(
+            image_placeholder, 2, 1, 5, 5, 255)
+        self.assertAllClose(padded_image.eval(
+            feed_dict={image_placeholder: image}), expected_image)
+
+  def testReturnOriginalImageWhenTargetSizeIsEqualToImageSize(self):
+    image = np.dstack([[[5, 6],
+                        [9, 0]],
+                       [[4, 3],
+                        [3, 5]]])
+
+    with self.test_session():
+      image_placeholder = tf.placeholder(tf.float32)
+      padded_image = preprocess_utils.pad_to_bounding_box(
+          image_placeholder, 0, 0, 2, 2, 255)
+      self.assertAllClose(padded_image.eval(
+          feed_dict={image_placeholder: image}), image)
+
+  def testDieOnTargetSizeGreaterThanImageSize(self):
+    image = np.dstack([[[5, 6],
+                        [9, 0]],
+                       [[4, 3],
+                        [3, 5]]])
+    with self.test_session():
+      image_placeholder = tf.placeholder(tf.float32)
+      padded_image = preprocess_utils.pad_to_bounding_box(
+          image_placeholder, 0, 0, 2, 1, 255)
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'target_width must be >= width'):
+        padded_image.eval(feed_dict={image_placeholder: image})
+      padded_image = preprocess_utils.pad_to_bounding_box(
+          image_placeholder, 0, 0, 1, 2, 255)
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'target_height must be >= height'):
+        padded_image.eval(feed_dict={image_placeholder: image})
+
+  def testDieIfTargetSizeNotPossibleWithGivenOffset(self):
+    image = np.dstack([[[5, 6],
+                        [9, 0]],
+                       [[4, 3],
+                        [3, 5]]])
+    with self.test_session():
+      image_placeholder = tf.placeholder(tf.float32)
+      padded_image = preprocess_utils.pad_to_bounding_box(
+          image_placeholder, 3, 0, 4, 4, 255)
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'target size not possible with the given target offsets'):
+        padded_image.eval(feed_dict={image_placeholder: image})
+
+  def testDieIfImageTensorRankIsNotThree(self):
+    image = np.vstack([[5, 6],
+                       [9, 0]])
+    with self.test_session():
+      image_placeholder = tf.placeholder(tf.float32)
+      padded_image = preprocess_utils.pad_to_bounding_box(
+          image_placeholder, 0, 0, 2, 2, 255)
+      with self.assertRaisesWithPredicateMatch(
+          errors.InvalidArgumentError,
+          'Wrong image tensor rank'):
+        padded_image.eval(feed_dict={image_placeholder: image})
+
+  def testResizeTensorsToRange(self):
+    test_shapes = [[60, 40],
+                   [15, 30],
+                   [15, 50]]
+    min_size = 50
+    max_size = 100
+    factor = None
+    expected_shape_list = [(75, 50, 3),
+                           (50, 100, 3),
+                           (30, 100, 3)]
+    for i, test_shape in enumerate(test_shapes):
+      image = tf.random_normal([test_shape[0], test_shape[1], 3])
+      new_tensor_list = preprocess_utils.resize_to_range(
+          image=image,
+          label=None,
+          min_size=min_size,
+          max_size=max_size,
+          factor=factor,
+          align_corners=True)
+      with self.test_session() as session:
+        resized_image = session.run(new_tensor_list[0])
+        self.assertEqual(resized_image.shape, expected_shape_list[i])
+
+  def testResizeTensorsToRangeWithFactor(self):
+    test_shapes = [[60, 40],
+                   [15, 30],
+                   [15, 50]]
+    min_size = 50
+    max_size = 98
+    factor = 8
+    expected_image_shape_list = [(81, 57, 3),
+                                 (49, 97, 3),
+                                 (33, 97, 3)]
+    expected_label_shape_list = [(81, 57, 1),
+                                 (49, 97, 1),
+                                 (33, 97, 1)]
+    for i, test_shape in enumerate(test_shapes):
+      image = tf.random_normal([test_shape[0], test_shape[1], 3])
+      label = tf.random_normal([test_shape[0], test_shape[1], 1])
+      new_tensor_list = preprocess_utils.resize_to_range(
+          image=image,
+          label=label,
+          min_size=min_size,
+          max_size=max_size,
+          factor=factor,
+          align_corners=True)
+      with self.test_session() as session:
+        new_tensor_list = session.run(new_tensor_list)
+        self.assertEqual(new_tensor_list[0].shape, expected_image_shape_list[i])
+        self.assertEqual(new_tensor_list[1].shape, expected_label_shape_list[i])
+
+  def testResizeTensorsToRangeWithFactorAndLabelShapeCHW(self):
+    test_shapes = [[60, 40],
+                   [15, 30],
+                   [15, 50]]
+    min_size = 50
+    max_size = 98
+    factor = 8
+    expected_image_shape_list = [(81, 57, 3),
+                                 (49, 97, 3),
+                                 (33, 97, 3)]
+    expected_label_shape_list = [(5, 81, 57),
+                                 (5, 49, 97),
+                                 (5, 33, 97)]
+    for i, test_shape in enumerate(test_shapes):
+      image = tf.random_normal([test_shape[0], test_shape[1], 3])
+      label = tf.random_normal([5, test_shape[0], test_shape[1]])
+      new_tensor_list = preprocess_utils.resize_to_range(
+          image=image,
+          label=label,
+          min_size=min_size,
+          max_size=max_size,
+          factor=factor,
+          align_corners=True,
+          label_layout_is_chw=True)
+      with self.test_session() as session:
+        new_tensor_list = session.run(new_tensor_list)
+        self.assertEqual(new_tensor_list[0].shape, expected_image_shape_list[i])
+        self.assertEqual(new_tensor_list[1].shape, expected_label_shape_list[i])
+
+  def testResizeTensorsToRangeWithSimilarMinMaxSizes(self):
+    test_shapes = [[60, 40],
+                   [15, 30],
+                   [15, 50]]
+    # Values set so that one of the side = 97.
+    min_size = 96
+    max_size = 98
+    factor = 8
+    expected_image_shape_list = [(97, 65, 3),
+                                 (49, 97, 3),
+                                 (33, 97, 3)]
+    expected_label_shape_list = [(97, 65, 1),
+                                 (49, 97, 1),
+                                 (33, 97, 1)]
+    for i, test_shape in enumerate(test_shapes):
+      image = tf.random_normal([test_shape[0], test_shape[1], 3])
+      label = tf.random_normal([test_shape[0], test_shape[1], 1])
+      new_tensor_list = preprocess_utils.resize_to_range(
+          image=image,
+          label=label,
+          min_size=min_size,
+          max_size=max_size,
+          factor=factor,
+          align_corners=True)
+      with self.test_session() as session:
+        new_tensor_list = session.run(new_tensor_list)
+        self.assertEqual(new_tensor_list[0].shape, expected_image_shape_list[i])
+        self.assertEqual(new_tensor_list[1].shape, expected_label_shape_list[i])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/core/xception.py b/research/deeplab/core/xception.py
new file mode 100644
index 00000000..8b8f55e0
--- /dev/null
+++ b/research/deeplab/core/xception.py
@@ -0,0 +1,613 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Xception model.
+
+"Xception: Deep Learning with Depthwise Separable Convolutions"
+Fran{\c{c}}ois Chollet
+https://arxiv.org/abs/1610.02357
+
+We implement the modified version by Jifeng Dai et al. for their COCO 2017
+detection challenge submission, where the model is made deeper and has aligned
+features for dense prediction tasks. See their slides for details:
+
+"Deformable Convolutional Networks -- COCO Detection and Segmentation Challenge
+2017 Entry"
+Haozhi Qi, Zheng Zhang, Bin Xiao, Han Hu, Bowen Cheng, Yichen Wei and Jifeng Dai
+ICCV 2017 COCO Challenge workshop
+http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf
+
+We made a few more changes on top of MSRA's modifications:
+1. Fully convolutional: All the max-pooling layers are replaced with separable
+  conv2d with stride = 2. This allows us to use atrous convolution to extract
+  feature maps at any resolution.
+
+2. We support adding ReLU and BatchNorm after depthwise convolution, motivated
+  by the design of MobileNetv1.
+
+"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
+Applications"
+Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
+Tobias Weyand, Marco Andreetto, Hartwig Adam
+https://arxiv.org/abs/1704.04861
+"""
+import collections
+import tensorflow as tf
+
+from tensorflow.contrib.slim.nets import resnet_utils
+
+slim = tf.contrib.slim
+
+
+_DEFAULT_MULTI_GRID = [1, 1, 1]
+
+
+class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):
+  """A named tuple describing an Xception block.
+
+  Its parts are:
+    scope: The scope of the block.
+    unit_fn: The Xception unit function which takes as input a tensor and
+      returns another tensor with the output of the Xception unit.
+    args: A list of length equal to the number of units in the block. The list
+      contains one dictionary for each unit in the block to serve as argument to
+      unit_fn.
+  """
+
+
+def fixed_padding(inputs, kernel_size, rate=1):
+  """Pads the input along the spatial dimensions independently of input size.
+
+  Args:
+    inputs: A tensor of size [batch, height_in, width_in, channels].
+    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.
+                 Should be a positive integer.
+    rate: An integer, rate for atrous convolution.
+
+  Returns:
+    output: A tensor of size [batch, height_out, width_out, channels] with the
+      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).
+  """
+  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)
+  pad_total = kernel_size_effective - 1
+  pad_beg = pad_total // 2
+  pad_end = pad_total - pad_beg
+  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
+                                  [pad_beg, pad_end], [0, 0]])
+  return padded_inputs
+
+
+@slim.add_arg_scope
+def separable_conv2d_same(inputs,
+                          num_outputs,
+                          kernel_size,
+                          depth_multiplier,
+                          stride,
+                          rate=1,
+                          use_explicit_padding=True,
+                          regularize_depthwise=False,
+                          scope=None,
+                          **kwargs):
+  """Strided 2-D separable convolution with 'SAME' padding.
+
+  If stride > 1 and use_explicit_padding is True, then we do explicit zero-
+  padding, followed by conv2d with 'VALID' padding.
+
+  Note that
+
+     net = separable_conv2d_same(inputs, num_outputs, 3,
+       depth_multiplier=1, stride=stride)
+
+  is equivalent to
+
+     net = slim.separable_conv2d(inputs, num_outputs, 3,
+       depth_multiplier=1, stride=1, padding='SAME')
+     net = resnet_utils.subsample(net, factor=stride)
+
+  whereas
+
+     net = slim.separable_conv2d(inputs, num_outputs, 3, stride=stride,
+       depth_multiplier=1, padding='SAME')
+
+  is different when the input's height or width is even, which is why we add the
+  current function.
+
+  Consequently, if the input feature map has even height or width, setting
+  `use_explicit_padding=False` will result in feature misalignment by one pixel
+  along the corresponding dimension.
+
+  Args:
+    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].
+    num_outputs: An integer, the number of output filters.
+    kernel_size: An int with the kernel_size of the filters.
+    depth_multiplier: The number of depthwise convolution output channels for
+      each input channel. The total number of depthwise convolution output
+      channels will be equal to `num_filters_in * depth_multiplier`.
+    stride: An integer, the output stride.
+    rate: An integer, rate for atrous convolution.
+    use_explicit_padding: If True, use explicit padding to make the model fully
+      compatible with the open source version, otherwise use the native
+      Tensorflow 'SAME' padding.
+    regularize_depthwise: Whether or not apply L2-norm regularization on the
+      depthwise convolution weights.
+    scope: Scope.
+    **kwargs: additional keyword arguments to pass to slim.conv2d
+
+  Returns:
+    output: A 4-D tensor of size [batch, height_out, width_out, channels] with
+      the convolution output.
+  """
+  def _separable_conv2d(padding):
+    """Wrapper for separable conv2d."""
+    return slim.separable_conv2d(inputs,
+                                 num_outputs,
+                                 kernel_size,
+                                 depth_multiplier=depth_multiplier,
+                                 stride=stride,
+                                 rate=rate,
+                                 padding=padding,
+                                 scope=scope,
+                                 **kwargs)
+  def _split_separable_conv2d(padding):
+    """Splits separable conv2d into depthwise and pointwise conv2d."""
+    outputs = slim.separable_conv2d(inputs,
+                                    None,
+                                    kernel_size,
+                                    depth_multiplier=depth_multiplier,
+                                    stride=stride,
+                                    rate=rate,
+                                    padding=padding,
+                                    scope=scope + '_depthwise',
+                                    **kwargs)
+    return slim.conv2d(outputs,
+                       num_outputs,
+                       1,
+                       scope=scope + '_pointwise',
+                       **kwargs)
+  if stride == 1 or not use_explicit_padding:
+    if regularize_depthwise:
+      outputs = _separable_conv2d(padding='SAME')
+    else:
+      outputs = _split_separable_conv2d(padding='SAME')
+  else:
+    inputs = fixed_padding(inputs, kernel_size, rate)
+    if regularize_depthwise:
+      outputs = _separable_conv2d(padding='VALID')
+    else:
+      outputs = _split_separable_conv2d(padding='VALID')
+  return outputs
+
+
+@slim.add_arg_scope
+def xception_module(inputs,
+                    depth_list,
+                    skip_connection_type,
+                    stride,
+                    unit_rate_list=None,
+                    rate=1,
+                    activation_fn_in_separable_conv=False,
+                    regularize_depthwise=False,
+                    outputs_collections=None,
+                    scope=None):
+  """An Xception module.
+
+  The output of one Xception module is equal to the sum of `residual` and
+  `shortcut`, where `residual` is the feature computed by three separable
+  convolution. The `shortcut` is the feature computed by 1x1 convolution with
+  or without striding. In some cases, the `shortcut` path could be a simple
+  identity function or none (i.e, no shortcut).
+
+  Note that we replace the max pooling operations in the Xception module with
+  another separable convolution with striding, since atrous rate is not properly
+  supported in current TensorFlow max pooling implementation.
+
+  Args:
+    inputs: A tensor of size [batch, height, width, channels].
+    depth_list: A list of three integers specifying the depth values of one
+      Xception module.
+    skip_connection_type: Skip connection type for the residual path. Only
+      supports 'conv', 'sum', or 'none'.
+    stride: The block unit's stride. Determines the amount of downsampling of
+      the units output compared to its input.
+    unit_rate_list: A list of three integers, determining the unit rate for
+      each separable convolution in the xception module.
+    rate: An integer, rate for atrous convolution.
+    activation_fn_in_separable_conv: Includes activation function in the
+      separable convolution or not.
+    regularize_depthwise: Whether or not apply L2-norm regularization on the
+      depthwise convolution weights.
+    outputs_collections: Collection to add the Xception unit output.
+    scope: Optional variable_scope.
+
+  Returns:
+    The Xception module's output.
+
+  Raises:
+    ValueError: If depth_list and unit_rate_list do not contain three elements,
+      or if stride != 1 for the third separable convolution operation in the
+      residual path, or unsupported skip connection type.
+  """
+  if len(depth_list) != 3:
+    raise ValueError('Expect three elements in depth_list.')
+  if unit_rate_list:
+    if len(unit_rate_list) != 3:
+      raise ValueError('Expect three elements in unit_rate_list.')
+
+  with tf.variable_scope(scope, 'xception_module', [inputs]) as sc:
+    residual = inputs
+
+    def _separable_conv(features, depth, kernel_size, depth_multiplier,
+                        regularize_depthwise, rate, stride, scope):
+      if activation_fn_in_separable_conv:
+        activation_fn = tf.nn.relu
+      else:
+        activation_fn = None
+        features = tf.nn.relu(features)
+      return separable_conv2d_same(features,
+                                   depth,
+                                   kernel_size,
+                                   depth_multiplier=depth_multiplier,
+                                   stride=stride,
+                                   rate=rate,
+                                   activation_fn=activation_fn,
+                                   regularize_depthwise=regularize_depthwise,
+                                   scope=scope)
+    for i in range(3):
+      residual = _separable_conv(residual,
+                                 depth_list[i],
+                                 kernel_size=3,
+                                 depth_multiplier=1,
+                                 regularize_depthwise=regularize_depthwise,
+                                 rate=rate*unit_rate_list[i],
+                                 stride=stride if i == 2 else 1,
+                                 scope='separable_conv' + str(i+1))
+    if skip_connection_type == 'conv':
+      shortcut = slim.conv2d(inputs,
+                             depth_list[-1],
+                             [1, 1],
+                             stride=stride,
+                             activation_fn=None,
+                             scope='shortcut')
+      outputs = residual + shortcut
+    elif skip_connection_type == 'sum':
+      outputs = residual + inputs
+    elif skip_connection_type == 'none':
+      outputs = residual
+    else:
+      raise ValueError('Unsupported skip connection type.')
+
+    return slim.utils.collect_named_outputs(outputs_collections,
+                                            sc.name,
+                                            outputs)
+
+
+@slim.add_arg_scope
+def stack_blocks_dense(net,
+                       blocks,
+                       output_stride=None,
+                       outputs_collections=None):
+  """Stacks Xception blocks and controls output feature density.
+
+  First, this function creates scopes for the Xception in the form of
+  'block_name/unit_1', 'block_name/unit_2', etc.
+
+  Second, this function allows the user to explicitly control the output
+  stride, which is the ratio of the input to output spatial resolution. This
+  is useful for dense prediction tasks such as semantic segmentation or
+  object detection.
+
+  Control of the output feature density is implemented by atrous convolution.
+
+  Args:
+    net: A tensor of size [batch, height, width, channels].
+    blocks: A list of length equal to the number of Xception blocks. Each
+      element is an Xception Block object describing the units in the block.
+    output_stride: If None, then the output will be computed at the nominal
+      network stride. If output_stride is not None, it specifies the requested
+      ratio of input to output spatial resolution, which needs to be equal to
+      the product of unit strides from the start up to some level of Xception.
+      For example, if the Xception employs units with strides 1, 2, 1, 3, 4, 1,
+      then valid values for the output_stride are 1, 2, 6, 24 or None (which
+      is equivalent to output_stride=24).
+    outputs_collections: Collection to add the Xception block outputs.
+
+  Returns:
+    net: Output tensor with stride equal to the specified output_stride.
+
+  Raises:
+    ValueError: If the target output_stride is not valid.
+  """
+  # The current_stride variable keeps track of the effective stride of the
+  # activations. This allows us to invoke atrous convolution whenever applying
+  # the next residual unit would result in the activations having stride larger
+  # than the target output_stride.
+  current_stride = 1
+
+  # The atrous convolution rate parameter.
+  rate = 1
+
+  for block in blocks:
+    with tf.variable_scope(block.scope, 'block', [net]) as sc:
+      for i, unit in enumerate(block.args):
+        if output_stride is not None and current_stride > output_stride:
+          raise ValueError('The target output_stride cannot be reached.')
+        with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
+          # If we have reached the target output_stride, then we need to employ
+          # atrous convolution with stride=1 and multiply the atrous rate by the
+          # current unit's stride for use in subsequent layers.
+          if output_stride is not None and current_stride == output_stride:
+            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))
+            rate *= unit.get('stride', 1)
+          else:
+            net = block.unit_fn(net, rate=1, **unit)
+            current_stride *= unit.get('stride', 1)
+
+      # Collect activations at the block's end before performing subsampling.
+      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)
+
+  if output_stride is not None and current_stride != output_stride:
+    raise ValueError('The target output_stride cannot be reached.')
+
+  return net
+
+
+def xception(inputs,
+             blocks,
+             num_classes=None,
+             is_training=True,
+             global_pool=True,
+             keep_prob=0.5,
+             output_stride=None,
+             reuse=None,
+             scope=None):
+  """Generator for Xception models.
+
+  This function generates a family of Xception models. See the xception_*()
+  methods for specific model instantiations, obtained by selecting different
+  block instantiations that produce Xception of various depths.
+
+  Args:
+    inputs: A tensor of size [batch, height_in, width_in, channels]. Must be
+      floating point. If a pretrained checkpoint is used, pixel values should be
+      the same as during training (see go/slim-classification-models for
+      specifics).
+    blocks: A list of length equal to the number of Xception blocks. Each
+      element is an Xception Block object describing the units in the block.
+    num_classes: Number of predicted classes for classification tasks.
+      If 0 or None, we return the features before the logit layer.
+    is_training: whether batch_norm layers are in training mode.
+    global_pool: If True, we perform global average pooling before computing the
+      logits. Set to True for image classification, False for dense prediction.
+    keep_prob: Keep probability used in the pre-logits dropout layer.
+    output_stride: If None, then the output will be computed at the nominal
+      network stride. If output_stride is not None, it specifies the requested
+      ratio of input to output spatial resolution.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
+    scope: Optional variable_scope.
+
+  Returns:
+    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].
+      If global_pool is False, then height_out and width_out are reduced by a
+      factor of output_stride compared to the respective height_in and width_in,
+      else both height_out and width_out equal one. If num_classes is 0 or None,
+      then net is the output of the last Xception block, potentially after
+      global average pooling. If num_classes is a non-zero integer, net contains
+      the pre-softmax activations.
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+
+  Raises:
+    ValueError: If the target output_stride is not valid.
+  """
+  with tf.variable_scope(
+      scope, 'xception', [inputs], reuse=reuse) as sc:
+    end_points_collection = sc.original_name_scope + 'end_points'
+    with slim.arg_scope([slim.conv2d,
+                         slim.separable_conv2d,
+                         xception_module,
+                         stack_blocks_dense],
+                        outputs_collections=end_points_collection):
+      with slim.arg_scope([slim.batch_norm], is_training=is_training):
+        net = inputs
+        if output_stride is not None:
+          if output_stride % 2 != 0:
+            raise ValueError('The output_stride needs to be a multiple of 2.')
+          output_stride /= 2
+        # Root block function operated on inputs.
+        net = resnet_utils.conv2d_same(net, 32, 3, stride=2,
+                                       scope='entry_flow/conv1_1')
+        net = resnet_utils.conv2d_same(net, 64, 3, stride=1,
+                                       scope='entry_flow/conv1_2')
+
+        # Extract features for entry_flow, middle_flow, and exit_flow.
+        net = stack_blocks_dense(net, blocks, output_stride)
+
+        # Convert end_points_collection into a dictionary of end_points.
+        end_points = slim.utils.convert_collection_to_dict(
+            end_points_collection, clear_collection=True)
+
+        if global_pool:
+          # Global average pooling.
+          net = tf.reduce_mean(net, [1, 2], name='global_pool', keepdims=True)
+          end_points['global_pool'] = net
+        if num_classes:
+          net = slim.dropout(net, keep_prob=keep_prob, is_training=is_training,
+                             scope='prelogits_dropout')
+          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
+                            normalizer_fn=None, scope='logits')
+          end_points[sc.name + '/logits'] = net
+          end_points['predictions'] = slim.softmax(net, scope='predictions')
+        return net, end_points
+
+
+def xception_block(scope,
+                   depth_list,
+                   skip_connection_type,
+                   activation_fn_in_separable_conv,
+                   regularize_depthwise,
+                   num_units,
+                   stride,
+                   unit_rate_list=None):
+  """Helper function for creating a Xception block.
+
+  Args:
+    scope: The scope of the block.
+    depth_list: The depth of the bottleneck layer for each unit.
+    skip_connection_type: Skip connection type for the residual path. Only
+      supports 'conv', 'sum', or 'none'.
+    activation_fn_in_separable_conv: Includes activation function in the
+      separable convolution or not.
+    regularize_depthwise: Whether or not apply L2-norm regularization on the
+      depthwise convolution weights.
+    num_units: The number of units in the block.
+    stride: The stride of the block, implemented as a stride in the last unit.
+      All other units have stride=1.
+    unit_rate_list: A list of three integers, determining the unit rate in the
+      corresponding xception block.
+
+  Returns:
+    An Xception block.
+  """
+  if unit_rate_list is None:
+    unit_rate_list = _DEFAULT_MULTI_GRID
+  return Block(scope, xception_module, [{
+      'depth_list': depth_list,
+      'skip_connection_type': skip_connection_type,
+      'activation_fn_in_separable_conv': activation_fn_in_separable_conv,
+      'regularize_depthwise': regularize_depthwise,
+      'stride': stride,
+      'unit_rate_list': unit_rate_list,
+  }] * num_units)
+
+
+def xception_65(inputs,
+                num_classes=None,
+                is_training=True,
+                global_pool=True,
+                keep_prob=0.5,
+                output_stride=None,
+                regularize_depthwise=False,
+                multi_grid=None,
+                reuse=None,
+                scope='xception_65'):
+  """Xception-65 model."""
+  blocks = [
+      xception_block('entry_flow/block1',
+                     depth_list=[128, 128, 128],
+                     skip_connection_type='conv',
+                     activation_fn_in_separable_conv=False,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=1,
+                     stride=2),
+      xception_block('entry_flow/block2',
+                     depth_list=[256, 256, 256],
+                     skip_connection_type='conv',
+                     activation_fn_in_separable_conv=False,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=1,
+                     stride=2),
+      xception_block('entry_flow/block3',
+                     depth_list=[728, 728, 728],
+                     skip_connection_type='conv',
+                     activation_fn_in_separable_conv=False,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=1,
+                     stride=2),
+      xception_block('middle_flow/block1',
+                     depth_list=[728, 728, 728],
+                     skip_connection_type='sum',
+                     activation_fn_in_separable_conv=False,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=16,
+                     stride=1),
+      xception_block('exit_flow/block1',
+                     depth_list=[728, 1024, 1024],
+                     skip_connection_type='conv',
+                     activation_fn_in_separable_conv=False,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=1,
+                     stride=2),
+      xception_block('exit_flow/block2',
+                     depth_list=[1536, 1536, 2048],
+                     skip_connection_type='none',
+                     activation_fn_in_separable_conv=True,
+                     regularize_depthwise=regularize_depthwise,
+                     num_units=1,
+                     stride=1,
+                     unit_rate_list=multi_grid),
+  ]
+  return xception(inputs,
+                  blocks=blocks,
+                  num_classes=num_classes,
+                  is_training=is_training,
+                  global_pool=global_pool,
+                  keep_prob=keep_prob,
+                  output_stride=output_stride,
+                  reuse=reuse,
+                  scope=scope)
+
+
+def xception_arg_scope(weight_decay=0.00004,
+                       batch_norm_decay=0.9997,
+                       batch_norm_epsilon=0.001,
+                       batch_norm_scale=True,
+                       weights_initializer_stddev=0.09,
+                       activation_fn=tf.nn.relu,
+                       regularize_depthwise=False,
+                       use_batch_norm=True):
+  """Defines the default Xception arg scope.
+
+  Args:
+    weight_decay: The weight decay to use for regularizing the model.
+    batch_norm_decay: The moving average decay when estimating layer activation
+      statistics in batch normalization.
+    batch_norm_epsilon: Small constant to prevent division by zero when
+      normalizing activations by their variance in batch normalization.
+    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the
+      activations in the batch normalization layer.
+    weights_initializer_stddev: The standard deviation of the trunctated normal
+      weight initializer.
+    activation_fn: The activation function in Xception.
+    regularize_depthwise: Whether or not apply L2-norm regularization on the
+      depthwise convolution weights.
+    use_batch_norm: Whether or not to use batch normalization.
+
+  Returns:
+    An `arg_scope` to use for the Xception models.
+  """
+  batch_norm_params = {
+      'decay': batch_norm_decay,
+      'epsilon': batch_norm_epsilon,
+      'scale': batch_norm_scale,
+  }
+  if regularize_depthwise:
+    depthwise_regularizer = slim.l2_regularizer(weight_decay)
+  else:
+    depthwise_regularizer = None
+  with slim.arg_scope(
+      [slim.conv2d, slim.separable_conv2d],
+      weights_initializer=tf.truncated_normal_initializer(
+          stddev=weights_initializer_stddev),
+      activation_fn=activation_fn,
+      normalizer_fn=slim.batch_norm if use_batch_norm else None):
+    with slim.arg_scope([slim.batch_norm], **batch_norm_params):
+      with slim.arg_scope(
+          [slim.conv2d],
+          weights_regularizer=slim.l2_regularizer(weight_decay)):
+        with slim.arg_scope(
+            [slim.separable_conv2d],
+            weights_regularizer=depthwise_regularizer) as arg_sc:
+          return arg_sc
diff --git a/research/deeplab/core/xception_test.py b/research/deeplab/core/xception_test.py
new file mode 100644
index 00000000..4bed6c29
--- /dev/null
+++ b/research/deeplab/core/xception_test.py
@@ -0,0 +1,466 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for xception.py."""
+import numpy as np
+import tensorflow as tf
+
+from deeplab.core import xception
+from tensorflow.contrib.slim.nets import resnet_utils
+
+slim = tf.contrib.slim
+
+
+def create_test_input(batch, height, width, channels):
+  """Create test input tensor."""
+  if None in [batch, height, width, channels]:
+    return tf.placeholder(tf.float32, (batch, height, width, channels))
+  else:
+    return tf.to_float(
+        np.tile(
+            np.reshape(
+                np.reshape(np.arange(height), [height, 1]) +
+                np.reshape(np.arange(width), [1, width]),
+                [1, height, width, 1]),
+            [batch, 1, 1, channels]))
+
+
+class UtilityFunctionTest(tf.test.TestCase):
+
+  def testSeparableConv2DSameWithInputEvenSize(self):
+    n, n2 = 4, 2
+
+    # Input image.
+    x = create_test_input(1, n, n, 1)
+
+    # Convolution kernel.
+    dw = create_test_input(1, 3, 3, 1)
+    dw = tf.reshape(dw, [3, 3, 1, 1])
+
+    tf.get_variable('Conv/depthwise_weights', initializer=dw)
+    tf.get_variable('Conv/pointwise_weights',
+                    initializer=tf.ones([1, 1, 1, 1]))
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
+
+    y1 = slim.separable_conv2d(x, 1, [3, 3], depth_multiplier=1,
+                               stride=1, scope='Conv')
+    y1_expected = tf.to_float([[14, 28, 43, 26],
+                               [28, 48, 66, 37],
+                               [43, 66, 84, 46],
+                               [26, 37, 46, 22]])
+    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
+
+    y2 = resnet_utils.subsample(y1, 2)
+    y2_expected = tf.to_float([[14, 43],
+                               [43, 84]])
+    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
+
+    y3 = xception.separable_conv2d_same(x, 1, 3, depth_multiplier=1,
+                                        regularize_depthwise=True,
+                                        stride=2, scope='Conv')
+    y3_expected = y2_expected
+
+    y4 = slim.separable_conv2d(x, 1, [3, 3], depth_multiplier=1,
+                               stride=2, scope='Conv')
+    y4_expected = tf.to_float([[48, 37],
+                               [37, 22]])
+    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])
+
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      self.assertAllClose(y1.eval(), y1_expected.eval())
+      self.assertAllClose(y2.eval(), y2_expected.eval())
+      self.assertAllClose(y3.eval(), y3_expected.eval())
+      self.assertAllClose(y4.eval(), y4_expected.eval())
+
+  def testSeparableConv2DSameWithInputOddSize(self):
+    n, n2 = 5, 3
+
+    # Input image.
+    x = create_test_input(1, n, n, 1)
+
+    # Convolution kernel.
+    dw = create_test_input(1, 3, 3, 1)
+    dw = tf.reshape(dw, [3, 3, 1, 1])
+
+    tf.get_variable('Conv/depthwise_weights', initializer=dw)
+    tf.get_variable('Conv/pointwise_weights',
+                    initializer=tf.ones([1, 1, 1, 1]))
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
+
+    y1 = slim.separable_conv2d(x, 1, [3, 3], depth_multiplier=1,
+                               stride=1, scope='Conv')
+    y1_expected = tf.to_float([[14, 28, 43, 58, 34],
+                               [28, 48, 66, 84, 46],
+                               [43, 66, 84, 102, 55],
+                               [58, 84, 102, 120, 64],
+                               [34, 46, 55, 64, 30]])
+    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
+
+    y2 = resnet_utils.subsample(y1, 2)
+    y2_expected = tf.to_float([[14, 43, 34],
+                               [43, 84, 55],
+                               [34, 55, 30]])
+    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
+
+    y3 = xception.separable_conv2d_same(x, 1, 3, depth_multiplier=1,
+                                        regularize_depthwise=True,
+                                        stride=2, scope='Conv')
+    y3_expected = y2_expected
+
+    y4 = slim.separable_conv2d(x, 1, [3, 3], depth_multiplier=1,
+                               stride=2, scope='Conv')
+    y4_expected = y2_expected
+
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      self.assertAllClose(y1.eval(), y1_expected.eval())
+      self.assertAllClose(y2.eval(), y2_expected.eval())
+      self.assertAllClose(y3.eval(), y3_expected.eval())
+      self.assertAllClose(y4.eval(), y4_expected.eval())
+
+
+class XceptionNetworkTest(tf.test.TestCase):
+  """Tests with small Xception network."""
+
+  def _xception_small(self,
+                      inputs,
+                      num_classes=None,
+                      is_training=True,
+                      global_pool=True,
+                      output_stride=None,
+                      regularize_depthwise=True,
+                      reuse=None,
+                      scope='xception_small'):
+    """A shallow and thin Xception for faster tests."""
+    block = xception.xception_block
+    blocks = [
+        block('entry_flow/block1',
+              depth_list=[1, 1, 1],
+              skip_connection_type='conv',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=2),
+        block('entry_flow/block2',
+              depth_list=[2, 2, 2],
+              skip_connection_type='conv',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=2),
+        block('entry_flow/block3',
+              depth_list=[4, 4, 4],
+              skip_connection_type='conv',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=1),
+        block('entry_flow/block4',
+              depth_list=[4, 4, 4],
+              skip_connection_type='conv',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=2),
+        block('middle_flow/block1',
+              depth_list=[4, 4, 4],
+              skip_connection_type='sum',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=2,
+              stride=1),
+        block('exit_flow/block1',
+              depth_list=[8, 8, 8],
+              skip_connection_type='conv',
+              activation_fn_in_separable_conv=False,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=2),
+        block('exit_flow/block2',
+              depth_list=[16, 16, 16],
+              skip_connection_type='none',
+              activation_fn_in_separable_conv=True,
+              regularize_depthwise=regularize_depthwise,
+              num_units=1,
+              stride=1),
+    ]
+    return xception.xception(inputs,
+                             blocks=blocks,
+                             num_classes=num_classes,
+                             is_training=is_training,
+                             global_pool=global_pool,
+                             output_stride=output_stride,
+                             reuse=reuse,
+                             scope=scope)
+
+  def testClassificationEndPoints(self):
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(2, 224, 224, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      logits, end_points = self._xception_small(
+          inputs,
+          num_classes=num_classes,
+          global_pool=global_pool,
+          scope='xception')
+    self.assertTrue(
+        logits.op.name.startswith('xception/logits'))
+    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])
+    self.assertTrue('predictions' in end_points)
+    self.assertListEqual(end_points['predictions'].get_shape().as_list(),
+                         [2, 1, 1, num_classes])
+    self.assertTrue('global_pool' in end_points)
+    self.assertListEqual(end_points['global_pool'].get_shape().as_list(),
+                         [2, 1, 1, 16])
+
+  def testEndpointNames(self):
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(2, 224, 224, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points = self._xception_small(
+          inputs,
+          num_classes=num_classes,
+          global_pool=global_pool,
+          scope='xception')
+    expected = [
+        'xception/entry_flow/conv1_1',
+        'xception/entry_flow/conv1_2',
+        'xception/entry_flow/block1/unit_1/xception_module/separable_conv1',
+        'xception/entry_flow/block1/unit_1/xception_module/separable_conv2',
+        'xception/entry_flow/block1/unit_1/xception_module/separable_conv3',
+        'xception/entry_flow/block1/unit_1/xception_module/shortcut',
+        'xception/entry_flow/block1/unit_1/xception_module',
+        'xception/entry_flow/block1',
+        'xception/entry_flow/block2/unit_1/xception_module/separable_conv1',
+        'xception/entry_flow/block2/unit_1/xception_module/separable_conv2',
+        'xception/entry_flow/block2/unit_1/xception_module/separable_conv3',
+        'xception/entry_flow/block2/unit_1/xception_module/shortcut',
+        'xception/entry_flow/block2/unit_1/xception_module',
+        'xception/entry_flow/block2',
+        'xception/entry_flow/block3/unit_1/xception_module/separable_conv1',
+        'xception/entry_flow/block3/unit_1/xception_module/separable_conv2',
+        'xception/entry_flow/block3/unit_1/xception_module/separable_conv3',
+        'xception/entry_flow/block3/unit_1/xception_module/shortcut',
+        'xception/entry_flow/block3/unit_1/xception_module',
+        'xception/entry_flow/block3',
+        'xception/entry_flow/block4/unit_1/xception_module/separable_conv1',
+        'xception/entry_flow/block4/unit_1/xception_module/separable_conv2',
+        'xception/entry_flow/block4/unit_1/xception_module/separable_conv3',
+        'xception/entry_flow/block4/unit_1/xception_module/shortcut',
+        'xception/entry_flow/block4/unit_1/xception_module',
+        'xception/entry_flow/block4',
+        'xception/middle_flow/block1/unit_1/xception_module/separable_conv1',
+        'xception/middle_flow/block1/unit_1/xception_module/separable_conv2',
+        'xception/middle_flow/block1/unit_1/xception_module/separable_conv3',
+        'xception/middle_flow/block1/unit_1/xception_module',
+        'xception/middle_flow/block1/unit_2/xception_module/separable_conv1',
+        'xception/middle_flow/block1/unit_2/xception_module/separable_conv2',
+        'xception/middle_flow/block1/unit_2/xception_module/separable_conv3',
+        'xception/middle_flow/block1/unit_2/xception_module',
+        'xception/middle_flow/block1',
+        'xception/exit_flow/block1/unit_1/xception_module/separable_conv1',
+        'xception/exit_flow/block1/unit_1/xception_module/separable_conv2',
+        'xception/exit_flow/block1/unit_1/xception_module/separable_conv3',
+        'xception/exit_flow/block1/unit_1/xception_module/shortcut',
+        'xception/exit_flow/block1/unit_1/xception_module',
+        'xception/exit_flow/block1',
+        'xception/exit_flow/block2/unit_1/xception_module/separable_conv1',
+        'xception/exit_flow/block2/unit_1/xception_module/separable_conv2',
+        'xception/exit_flow/block2/unit_1/xception_module/separable_conv3',
+        'xception/exit_flow/block2/unit_1/xception_module',
+        'xception/exit_flow/block2',
+        'global_pool',
+        'xception/logits',
+        'predictions',
+    ]
+    self.assertItemsEqual(end_points.keys(), expected)
+
+  def testClassificationShapes(self):
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(2, 224, 224, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points = self._xception_small(
+          inputs,
+          num_classes,
+          global_pool=global_pool,
+          scope='xception')
+      endpoint_to_shape = {
+          'xception/entry_flow/conv1_1': [2, 112, 112, 32],
+          'xception/entry_flow/block1': [2, 56, 56, 1],
+          'xception/entry_flow/block2': [2, 28, 28, 2],
+          'xception/entry_flow/block4': [2, 14, 14, 4],
+          'xception/middle_flow/block1': [2, 14, 14, 4],
+          'xception/exit_flow/block1': [2, 7, 7, 8],
+          'xception/exit_flow/block2': [2, 7, 7, 16]}
+      for endpoint, shape in endpoint_to_shape.iteritems():
+        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)
+
+  def testFullyConvolutionalEndpointShapes(self):
+    global_pool = False
+    num_classes = 10
+    inputs = create_test_input(2, 321, 321, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points = self._xception_small(
+          inputs,
+          num_classes,
+          global_pool=global_pool,
+          scope='xception')
+      endpoint_to_shape = {
+          'xception/entry_flow/conv1_1': [2, 161, 161, 32],
+          'xception/entry_flow/block1': [2, 81, 81, 1],
+          'xception/entry_flow/block2': [2, 41, 41, 2],
+          'xception/entry_flow/block4': [2, 21, 21, 4],
+          'xception/middle_flow/block1': [2, 21, 21, 4],
+          'xception/exit_flow/block1': [2, 11, 11, 8],
+          'xception/exit_flow/block2': [2, 11, 11, 16]}
+      for endpoint, shape in endpoint_to_shape.iteritems():
+        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)
+
+  def testAtrousFullyConvolutionalEndpointShapes(self):
+    global_pool = False
+    num_classes = 10
+    output_stride = 8
+    inputs = create_test_input(2, 321, 321, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points = self._xception_small(
+          inputs,
+          num_classes,
+          global_pool=global_pool,
+          output_stride=output_stride,
+          scope='xception')
+      endpoint_to_shape = {
+          'xception/entry_flow/block1': [2, 81, 81, 1],
+          'xception/entry_flow/block2': [2, 41, 41, 2],
+          'xception/entry_flow/block4': [2, 41, 41, 4],
+          'xception/middle_flow/block1': [2, 41, 41, 4],
+          'xception/exit_flow/block1': [2, 41, 41, 8],
+          'xception/exit_flow/block2': [2, 41, 41, 16]}
+      for endpoint, shape in endpoint_to_shape.iteritems():
+        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)
+
+  def testAtrousFullyConvolutionalValues(self):
+    """Verify dense feature extraction with atrous convolution."""
+    nominal_stride = 32
+    for output_stride in [4, 8, 16, 32, None]:
+      with slim.arg_scope(xception.xception_arg_scope()):
+        with tf.Graph().as_default():
+          with self.test_session() as sess:
+            tf.set_random_seed(0)
+            inputs = create_test_input(2, 96, 97, 3)
+            # Dense feature extraction followed by subsampling.
+            output, _ = self._xception_small(
+                inputs,
+                None,
+                is_training=False,
+                global_pool=False,
+                output_stride=output_stride)
+            if output_stride is None:
+              factor = 1
+            else:
+              factor = nominal_stride // output_stride
+            output = resnet_utils.subsample(output, factor)
+            # Make the two networks use the same weights.
+            tf.get_variable_scope().reuse_variables()
+            # Feature extraction at the nominal network rate.
+            expected, _ = self._xception_small(
+                inputs,
+                None,
+                is_training=False,
+                global_pool=False)
+            sess.run(tf.global_variables_initializer())
+            self.assertAllClose(output.eval(), expected.eval(),
+                                atol=1e-5, rtol=1e-5)
+
+  def testUnknownBatchSize(self):
+    batch = 2
+    height, width = 65, 65
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(None, height, width, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      logits, _ = self._xception_small(
+          inputs,
+          num_classes,
+          global_pool=global_pool,
+          scope='xception')
+    self.assertTrue(logits.op.name.startswith('xception/logits'))
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [None, 1, 1, num_classes])
+    images = create_test_input(batch, height, width, 3)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(logits, {inputs: images.eval()})
+      self.assertEquals(output.shape, (batch, 1, 1, num_classes))
+
+  def testFullyConvolutionalUnknownHeightWidth(self):
+    batch = 2
+    height, width = 65, 65
+    global_pool = False
+    inputs = create_test_input(batch, None, None, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      output, _ = self._xception_small(
+          inputs,
+          None,
+          global_pool=global_pool)
+    self.assertListEqual(output.get_shape().as_list(),
+                         [batch, None, None, 16])
+    images = create_test_input(batch, height, width, 3)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(output, {inputs: images.eval()})
+      self.assertEquals(output.shape, (batch, 3, 3, 16))
+
+  def testAtrousFullyConvolutionalUnknownHeightWidth(self):
+    batch = 2
+    height, width = 65, 65
+    global_pool = False
+    output_stride = 8
+    inputs = create_test_input(batch, None, None, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      output, _ = self._xception_small(
+          inputs,
+          None,
+          global_pool=global_pool,
+          output_stride=output_stride)
+    self.assertListEqual(output.get_shape().as_list(),
+                         [batch, None, None, 16])
+    images = create_test_input(batch, height, width, 3)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(output, {inputs: images.eval()})
+      self.assertEquals(output.shape, (batch, 9, 9, 16))
+
+  def testEndpointsReuse(self):
+    inputs = create_test_input(2, 32, 32, 3)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points0 = xception.xception_65(
+          inputs,
+          num_classes=10,
+          reuse=False)
+    with slim.arg_scope(xception.xception_arg_scope()):
+      _, end_points1 = xception.xception_65(
+          inputs,
+          num_classes=10,
+          reuse=True)
+    self.assertItemsEqual(end_points0.keys(), end_points1.keys())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/datasets/__init__.py b/research/deeplab/datasets/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/deeplab/datasets/build_cityscapes_data.py b/research/deeplab/datasets/build_cityscapes_data.py
new file mode 100644
index 00000000..8219820e
--- /dev/null
+++ b/research/deeplab/datasets/build_cityscapes_data.py
@@ -0,0 +1,183 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Converts Cityscapes data to TFRecord file format with Example protos.
+
+The Cityscapes dataset is expected to have the following directory structure:
+
+  + cityscapes
+     - build_cityscapes_data.py (current working directiory).
+     - build_data.py
+     + cityscapesscripts
+       + annotation
+       + evaluation
+       + helpers
+       + preparation
+       + viewer
+     + gtFine
+       + train
+       + val
+       + test
+     + leftImg8bit
+       + train
+       + val
+       + test
+     + tfrecord
+
+This script converts data into sharded data files and save at tfrecord folder.
+
+Note that before running this script, the users should (1) register the
+Cityscapes dataset website at https://www.cityscapes-dataset.com to
+download the dataset, and (2) run the script provided by Cityscapes
+`preparation/createTrainIdLabelImgs.py` to generate the training groundtruth.
+
+Also note that the tensorflow model will be trained with `TrainId' instead
+of `EvalId' used on the evaluation server. Thus, the users need to convert
+the predicted labels to `EvalId` for evaluation on the server. See the
+vis.py for more details.
+
+The Example proto contains the following fields:
+
+  image/encoded: encoded image content.
+  image/filename: image filename.
+  image/format: image file format.
+  image/height: image height.
+  image/width: image width.
+  image/channels: image channels.
+  image/segmentation/class/encoded: encoded semantic segmentation content.
+  image/segmentation/class/format: semantic segmentation file format.
+"""
+import glob
+import math
+import os.path
+import re
+import sys
+import build_data
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_string('cityscapes_root',
+                           './cityscapes',
+                           'Cityscapes dataset root folder.')
+
+tf.app.flags.DEFINE_string(
+    'output_dir',
+    './tfrecord',
+    'Path to save converted SSTable of TensorFlow examples.')
+
+
+_NUM_SHARDS = 10
+
+# A map from data type to folder name that saves the data.
+_FOLDERS_MAP = {
+    'image': 'leftImg8bit',
+    'label': 'gtFine',
+}
+
+# A map from data type to filename postfix.
+_POSTFIX_MAP = {
+    'image': '_leftImg8bit',
+    'label': '_gtFine_labelTrainIds',
+}
+
+# A map from data type to data format.
+_DATA_FORMAT_MAP = {
+    'image': 'png',
+    'label': 'png',
+}
+
+# Image file pattern.
+_IMAGE_FILENAME_RE = re.compile('(.+)' + _POSTFIX_MAP['image'])
+
+
+def _get_files(data, dataset_split):
+  """Gets files for the specified data type and dataset split.
+
+  Args:
+    data: String, desired data ('image' or 'label').
+    dataset_split: String, dataset split ('train', 'val', 'test')
+
+  Returns:
+    A list of sorted file names or None when getting label for
+      test set.
+  """
+  if data == 'label' and dataset_split == 'test':
+    return None
+  pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])
+  search_files = os.path.join(
+      FLAGS.cityscapes_root, _FOLDERS_MAP[data], dataset_split, '*', pattern)
+  filenames = glob.glob(search_files)
+  return sorted(filenames)
+
+
+def _convert_dataset(dataset_split):
+  """Converts the specified dataset split to TFRecord format.
+
+  Args:
+    dataset_split: The dataset split (e.g., train, val).
+
+  Raises:
+    RuntimeError: If loaded image and label have different shape, or if the
+      image file with specified postfix could not be found.
+  """
+  image_files = _get_files('image', dataset_split)
+  label_files = _get_files('label', dataset_split)
+
+  num_images = len(image_files)
+  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
+
+  image_reader = build_data.ImageReader('png', channels=3)
+  label_reader = build_data.ImageReader('png', channels=1)
+
+  for shard_id in range(_NUM_SHARDS):
+    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
+        dataset_split, shard_id, _NUM_SHARDS)
+    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
+    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
+      start_idx = shard_id * num_per_shard
+      end_idx = min((shard_id + 1) * num_per_shard, num_images)
+      for i in range(start_idx, end_idx):
+        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
+            i + 1, num_images, shard_id))
+        sys.stdout.flush()
+        # Read the image.
+        image_data = tf.gfile.FastGFile(image_files[i], 'r').read()
+        height, width = image_reader.read_image_dims(image_data)
+        # Read the semantic segmentation annotation.
+        seg_data = tf.gfile.FastGFile(label_files[i], 'r').read()
+        seg_height, seg_width = label_reader.read_image_dims(seg_data)
+        if height != seg_height or width != seg_width:
+          raise RuntimeError('Shape mismatched between image and label.')
+        # Convert to tf example.
+        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
+        if re_match is None:
+          raise RuntimeError('Invalid image filename: ' + image_files[i])
+        filename = os.path.basename(re_match.group(1))
+        example = build_data.image_seg_to_tfexample(
+            image_data, filename, height, width, seg_data)
+        tfrecord_writer.write(example.SerializeToString())
+    sys.stdout.write('\n')
+    sys.stdout.flush()
+
+
+def main(unused_argv):
+  # Only support converting 'train' and 'val' sets for now.
+  for dataset_split in ['train', 'val']:
+    _convert_dataset(dataset_split)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/deeplab/datasets/build_data.py b/research/deeplab/datasets/build_data.py
new file mode 100644
index 00000000..6b47dc10
--- /dev/null
+++ b/research/deeplab/datasets/build_data.py
@@ -0,0 +1,156 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Contains common utility functions and classes for building dataset.
+
+This script contains utility functions and classes to converts dataset to
+TFRecord file format with Example protos.
+
+The Example proto contains the following fields:
+
+  image/encoded: encoded image content.
+  image/filename: image filename.
+  image/format: image file format.
+  image/height: image height.
+  image/width: image width.
+  image/channels: image channels.
+  image/segmentation/class/encoded: encoded semantic segmentation content.
+  image/segmentation/class/format: semantic segmentation file format.
+"""
+import collections
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_enum('image_format', 'png', ['jpg', 'jpeg', 'png'],
+                         'Image format.')
+
+tf.app.flags.DEFINE_enum('label_format', 'png', ['png'],
+                         'Segmentation label format.')
+
+# A map from image format to expected data format.
+_IMAGE_FORMAT_MAP = {
+    'jpg': 'jpeg',
+    'jpeg': 'jpeg',
+    'png': 'png',
+}
+
+
+class ImageReader(object):
+  """Helper class that provides TensorFlow image coding utilities."""
+
+  def __init__(self, image_format='jpeg', channels=3):
+    """Class constructor.
+
+    Args:
+      image_format: Image format. Only 'jpeg', 'jpg', or 'png' are supported.
+      channels: Image channels.
+    """
+    with tf.Graph().as_default():
+      self._decode_data = tf.placeholder(dtype=tf.string)
+      self._image_format = image_format
+      self._session = tf.Session()
+      if self._image_format in ('jpeg', 'jpg'):
+        self._decode = tf.image.decode_jpeg(self._decode_data,
+                                            channels=channels)
+      elif self._image_format == 'png':
+        self._decode = tf.image.decode_png(self._decode_data,
+                                           channels=channels)
+
+  def read_image_dims(self, image_data):
+    """Reads the image dimensions.
+
+    Args:
+      image_data: string of image data.
+
+    Returns:
+      image_height and image_width.
+    """
+    image = self.decode_image(image_data)
+    return image.shape[:2]
+
+  def decode_image(self, image_data):
+    """Decodes the image data string.
+
+    Args:
+      image_data: string of image data.
+
+    Returns:
+      Decoded image data.
+
+    Raises:
+      ValueError: Value of image channels not supported.
+    """
+    image = self._session.run(self._decode,
+                              feed_dict={self._decode_data: image_data})
+    if len(image.shape) != 3 or image.shape[2] not in (1, 3):
+      raise ValueError('The image channels not supported.')
+
+    return image
+
+
+def _int64_list_feature(values):
+  """Returns a TF-Feature of int64_list.
+
+  Args:
+    values: A scalar or list of values.
+
+  Returns:
+    A TF-Feature.
+  """
+  if not isinstance(values, collections.Iterable):
+    values = [values]
+
+  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))
+
+
+def _bytes_list_feature(values):
+  """Returns a TF-Feature of bytes.
+
+  Args:
+    values: A string.
+
+  Returns:
+    A TF-Feature.
+  """
+  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))
+
+
+def image_seg_to_tfexample(image_data, filename, height, width, seg_data):
+  """Converts one image/segmentation pair to tf example.
+
+  Args:
+    image_data: string of image data.
+    filename: image filename.
+    height: image height.
+    width: image width.
+    seg_data: string of semantic segmentation data.
+
+  Returns:
+    tf example of one image/segmentation pair.
+  """
+  return tf.train.Example(features=tf.train.Features(feature={
+      'image/encoded': _bytes_list_feature(image_data),
+      'image/filename': _bytes_list_feature(filename),
+      'image/format': _bytes_list_feature(
+          _IMAGE_FORMAT_MAP[FLAGS.image_format]),
+      'image/height': _int64_list_feature(height),
+      'image/width': _int64_list_feature(width),
+      'image/channels': _int64_list_feature(3),
+      'image/segmentation/class/encoded': (
+          _bytes_list_feature(seg_data)),
+      'image/segmentation/class/format': _bytes_list_feature(
+          FLAGS.label_format),
+  }))
diff --git a/research/deeplab/datasets/build_voc2012_data.py b/research/deeplab/datasets/build_voc2012_data.py
new file mode 100644
index 00000000..2a59e0dc
--- /dev/null
+++ b/research/deeplab/datasets/build_voc2012_data.py
@@ -0,0 +1,142 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Converts PASCAL VOC 2012 data to TFRecord file format with Example protos.
+
+PASCAL VOC 2012 dataset is expected to have the following directory structure:
+
+  + pascal_voc_seg
+    - build_data.py
+    - build_voc2012_data.py (current working directory).
+    + VOCdevkit
+      + VOC2012
+        + JPEGImages
+        + SegmentationClass
+        + ImageSets
+          + Segmentation
+    + tfrecord
+
+Image folder:
+  ./VOCdevkit/VOC2012/JPEGImages
+
+Semantic segmentation annotations:
+  ./VOCdevkit/VOC2012/SegmentationClass
+
+list folder:
+  ./VOCdevkit/VOC2012/ImageSets/Segmentation
+
+This script converts data into sharded data files and save at tfrecord folder.
+
+The Example proto contains the following fields:
+
+  image/encoded: encoded image content.
+  image/filename: image filename.
+  image/format: image file format.
+  image/height: image height.
+  image/width: image width.
+  image/channels: image channels.
+  image/segmentation/class/encoded: encoded semantic segmentation content.
+  image/segmentation/class/format: semantic segmentation file format.
+"""
+import glob
+import math
+import os.path
+import sys
+import build_data
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_string('image_folder',
+                           './VOCdevkit/VOC2012/JPEGImages',
+                           'Folder containing images.')
+
+tf.app.flags.DEFINE_string(
+    'semantic_segmentation_folder',
+    './VOCdevkit/VOC2012/SegmentationClassRaw',
+    'Folder containing semantic segmentation annotations.')
+
+tf.app.flags.DEFINE_string(
+    'list_folder',
+    './VOCdevkit/VOC2012/ImageSets/Segmentation',
+    'Folder containing lists for training and validation')
+
+tf.app.flags.DEFINE_string(
+    'output_dir',
+    './tfrecord',
+    'Path to save converted SSTable of TensorFlow examples.')
+
+
+_NUM_SHARDS = 4
+
+
+def _convert_dataset(dataset_split):
+  """Converts the specified dataset split to TFRecord format.
+
+  Args:
+    dataset_split: The dataset split (e.g., train, test).
+
+  Raises:
+    RuntimeError: If loaded image and label have different shape.
+  """
+  dataset = os.path.basename(dataset_split)[:-4]
+  sys.stdout.write('Processing ' + dataset)
+  filenames = [x.strip('\n') for x in open(dataset_split, 'r')]
+  num_images = len(filenames)
+  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
+
+  image_reader = build_data.ImageReader('jpeg', channels=3)
+  label_reader = build_data.ImageReader('png', channels=1)
+
+  for shard_id in range(_NUM_SHARDS):
+    output_filename = os.path.join(
+        FLAGS.output_dir,
+        '%s-%05d-of-%05d.tfrecord' % (dataset, shard_id, _NUM_SHARDS))
+    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
+      start_idx = shard_id * num_per_shard
+      end_idx = min((shard_id + 1) * num_per_shard, num_images)
+      for i in range(start_idx, end_idx):
+        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
+            i + 1, len(filenames), shard_id))
+        sys.stdout.flush()
+        # Read the image.
+        image_filename = os.path.join(
+            FLAGS.image_folder, filenames[i] + '.' + FLAGS.image_format)
+        image_data = tf.gfile.FastGFile(image_filename, 'r').read()
+        height, width = image_reader.read_image_dims(image_data)
+        # Read the semantic segmentation annotation.
+        seg_filename = os.path.join(
+            FLAGS.semantic_segmentation_folder,
+            filenames[i] + '.' + FLAGS.label_format)
+        seg_data = tf.gfile.FastGFile(seg_filename, 'r').read()
+        seg_height, seg_width = label_reader.read_image_dims(seg_data)
+        if height != seg_height or width != seg_width:
+          raise RuntimeError('Shape mismatched between image and label.')
+        # Convert to tf example.
+        example = build_data.image_seg_to_tfexample(
+            image_data, filenames[i], height, width, seg_data)
+        tfrecord_writer.write(example.SerializeToString())
+    sys.stdout.write('\n')
+    sys.stdout.flush()
+
+
+def main(unused_argv):
+  dataset_splits = glob.glob(os.path.join(FLAGS.list_folder, '*.txt'))
+  for dataset_split in dataset_splits:
+    _convert_dataset(dataset_split)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/deeplab/datasets/convert_cityscapes.sh b/research/deeplab/datasets/convert_cityscapes.sh
new file mode 100644
index 00000000..e1b5adfb
--- /dev/null
+++ b/research/deeplab/datasets/convert_cityscapes.sh
@@ -0,0 +1,55 @@
+#!/bin/bash
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+#
+# Script to preprocess the Cityscapes dataset. Note (1) the users should register
+# the Cityscapes dataset website: https://www.cityscapes-dataset.com/downloads/ to
+# download the dataset, and (2) the users should run the script provided by Cityscapes
+# `preparation/createTrainIdLabelImgs.py` to generate the training groundtruth.
+#
+# Usage:
+#   bash ./preprocess_cityscapes.sh
+#
+# The folder structure is assumed to be:
+#  + data
+#    - build_cityscapes_data.py
+#    + cityscapes
+#      + cityscapesscripts
+#      + gtFine
+#      + leftImg8bit
+#
+
+# Exit immediately if a command exits with a non-zero status.
+set -e
+
+CURRENT_DIR=$(pwd)
+WORK_DIR="."
+
+cd "${CURRENT_DIR}"
+
+# Root path for PASCAL VOC 2012 dataset.
+CITYSCAPES_ROOT="${WORK_DIR}/cityscapes"
+
+# Build TFRecords of the dataset.
+# First, create output directory for storing TFRecords.
+OUTPUT_DIR="${CITYSCAPES_ROOT}/tfrecord"
+mkdir -p "${OUTPUT_DIR}"
+
+BUILD_SCRIPT="${WORK_DIR}/build_cityscapes_data.py"
+
+echo "Converting Cityscapes dataset..."
+python "${BUILD_SCRIPT}" \
+  --cityscapes_root="${CITYSCAPES_ROOT}" \
+  --output_dir="${OUTPUT_DIR}" \
diff --git a/research/deeplab/datasets/download_and_convert_voc2012.sh b/research/deeplab/datasets/download_and_convert_voc2012.sh
new file mode 100644
index 00000000..9d4e1932
--- /dev/null
+++ b/research/deeplab/datasets/download_and_convert_voc2012.sh
@@ -0,0 +1,89 @@
+#!/bin/bash
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+#
+# Script to download and preprocess the PASCAL VOC 2012 dataset.
+#
+# Usage:
+#   bash ./download_and_preprocess_voc2012.sh
+#
+# The folder structure is assumed to be:
+#  + data
+#     - build_data.py
+#     - build_voc2012_data.py
+#     - download_and_preprocess_voc2012.sh
+#     - remove_gt_colormap.py
+#     + VOCdevkit
+#       + VOC2012
+#         + JPEGImages
+#         + SegmentationClass
+#
+
+# Exit immediately if a command exits with a non-zero status.
+set -e
+
+CURRENT_DIR=$(pwd)
+WORK_DIR="./pascal_voc_seg"
+mkdir -p ${WORK_DIR}
+cd ${WORK_DIR}
+
+# Helper function to download and unpack VOC 2012 dataset.
+function download_and_uncompress() {
+  local BASE_URL=${1}
+  local FILENAME=${2}
+
+  if [ ! -f ${FILENAME} ]; then
+    echo "Downloading ${FILENAME} to ${WORK_DIR}"
+    wget -nd -c "${BASE_URL}/${FILENAME}"
+  fi
+  echo "Uncompressing ${FILENAME}"
+  tar -xf ${FILENAME}
+}
+
+# Download the images.
+BASE_URL="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/"
+FILENAME="VOCtrainval_11-May-2012.tar"
+
+download_and_uncompress ${BASE_URL} ${FILENAME}
+
+cd "${CURRENT_DIR}"
+
+# Root path for PASCAL VOC 2012 dataset.
+PASCAL_ROOT="${WORK_DIR}/VOCdevkit/VOC2012"
+
+# Remove the colormap in the ground truth annotations.
+SEG_FOLDER="${PASCAL_ROOT}/SegmentationClass"
+SEMANTIC_SEG_FOLDER="${PASCAL_ROOT}/SegmentationClassRaw"
+
+echo "Removing the color map in ground truth annotations..."
+python ./remove_gt_colormap.py \
+  --original_gt_folder="${SEG_FOLDER}" \
+  --output_dir="${SEMANTIC_SEG_FOLDER}"
+
+# Build TFRecords of the dataset.
+# First, create output directory for storing TFRecords.
+OUTPUT_DIR="${WORK_DIR}/tfrecord"
+mkdir -p "${OUTPUT_DIR}"
+
+IMAGE_FOLDER="${PASCAL_ROOT}/JPEGImages"
+LIST_FOLDER="${PASCAL_ROOT}/ImageSets/Segmentation"
+
+echo "Converting PASCAL VOC 2012 dataset..."
+python ./build_voc2012_data.py \
+  --image_folder="${IMAGE_FOLDER}" \
+  --semantic_segmentation_folder="${SEMANTIC_SEG_FOLDER}" \
+  --list_folder="${LIST_FOLDER}" \
+  --image_format="jpg" \
+  --output_dir="${OUTPUT_DIR}"
diff --git a/research/deeplab/datasets/remove_gt_colormap.py b/research/deeplab/datasets/remove_gt_colormap.py
new file mode 100644
index 00000000..6320ab20
--- /dev/null
+++ b/research/deeplab/datasets/remove_gt_colormap.py
@@ -0,0 +1,83 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Removes the color map from segmentation annotations.
+
+Removes the color map from the ground truth segmentation annotations and save
+the results to output_dir.
+"""
+import glob
+import os.path
+import numpy as np
+
+from PIL import Image
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_string('original_gt_folder',
+                           './VOCdevkit/VOC2012/SegmentationClass',
+                           'Original ground truth annotations.')
+
+tf.app.flags.DEFINE_string('segmentation_format', 'png', 'Segmentation format.')
+
+tf.app.flags.DEFINE_string('output_dir',
+                           './VOCdevkit/VOC2012/SegmentationClassRaw',
+                           'folder to save modified ground truth annotations.')
+
+
+def _remove_colormap(filename):
+  """Removes the color map from the annotation.
+
+  Args:
+    filename: Ground truth annotation filename.
+
+  Returns:
+    Annotation without color map.
+  """
+  return np.array(Image.open(filename))
+
+
+def _save_annotation(annotation, filename):
+  """Saves the annotation as png file.
+
+  Args:
+    annotation: Segmentation annotation.
+    filename: Output filename.
+  """
+  pil_image = Image.fromarray(annotation.astype(dtype=np.uint8))
+  with tf.gfile.Open(filename, mode='w') as f:
+    pil_image.save(f, 'PNG')
+
+
+def main(unused_argv):
+  # Create the output directory if not exists.
+  if not tf.gfile.IsDirectory(FLAGS.output_dir):
+    tf.gfile.MakeDirs(FLAGS.output_dir)
+
+  annotations = glob.glob(os.path.join(FLAGS.original_gt_folder,
+                                       '*.' + FLAGS.segmentation_format))
+  for annotation in annotations:
+    raw_annotation = _remove_colormap(annotation)
+    filename = os.path.basename(annotation)[:-4]
+    _save_annotation(raw_annotation,
+                     os.path.join(
+                         FLAGS.output_dir,
+                         filename + '.' + FLAGS.segmentation_format))
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/deeplab/datasets/segmentation_dataset.py b/research/deeplab/datasets/segmentation_dataset.py
new file mode 100644
index 00000000..a7772524
--- /dev/null
+++ b/research/deeplab/datasets/segmentation_dataset.py
@@ -0,0 +1,174 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Provides data from semantic segmentation datasets.
+
+The SegmentationDataset class provides both images and annotations (semantic
+segmentation and/or instance segmentation) for TensorFlow. Currently, we
+support the following datasets:
+
+1. PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).
+
+PASCAL VOC 2012 semantic segmentation dataset annotates 20 foreground objects
+(e.g., bike, person, and so on) and leaves all the other semantic classes as
+one background class. The dataset contains 1464, 1449, and 1456 annotated
+images for the training, validation and test respectively.
+
+2. Cityscapes dataset (https://www.cityscapes-dataset.com)
+
+The Cityscapes dataset contains 19 semantic labels (such as road, person, car,
+and so on) for urban street scenes.
+
+References:
+  M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn,
+  and A. Zisserman, The pascal visual object classes challenge a retrospective.
+  IJCV, 2014.
+
+  M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
+  U. Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban
+  scene understanding," In Proc. of CVPR, 2016.
+"""
+import collections
+import os.path
+import tensorflow as tf
+
+slim = tf.contrib.slim
+
+dataset = slim.dataset
+
+tfexample_decoder = slim.tfexample_decoder
+
+
+_ITEMS_TO_DESCRIPTIONS = {
+    'image': 'A color image of varying height and width.',
+    'labels_class': ('A semantic segmentation label whose size matches image.'
+                     'Its values range from 0 (background) to num_classes.'),
+}
+
+# Named tuple to describe the dataset properties.
+DatasetDescriptor = collections.namedtuple(
+    'DatasetDescriptor',
+    ['splits_to_sizes',   # Splits of the dataset into training, val, and test.
+     'num_classes',   # Number of semantic classes.
+     'ignore_label',  # Ignore label value.
+    ]
+)
+
+_CITYSCAPES_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 2975,
+        'val': 500,
+    },
+    num_classes=19,
+    ignore_label=255,
+)
+
+_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 1464,
+        'trainval': 2913,
+        'val': 1449,
+    },
+    num_classes=21,
+    ignore_label=255,
+)
+
+
+_DATASETS_INFORMATION = {
+    'cityscapes': _CITYSCAPES_INFORMATION,
+    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
+}
+
+# Default file pattern of TFRecord of TensorFlow Example.
+_FILE_PATTERN = '%s-*'
+
+
+def get_cityscapes_dataset_name():
+  return 'cityscapes'
+
+
+def get_dataset(dataset_name, split_name, dataset_dir):
+  """Gets an instance of slim Dataset.
+
+  Args:
+    dataset_name: Dataset name.
+    split_name: A train/val Split name.
+    dataset_dir: The directory of the dataset sources.
+
+  Returns:
+    An instance of slim Dataset.
+
+  Raises:
+    ValueError: if the dataset_name or split_name is not recognized.
+  """
+  if dataset_name not in _DATASETS_INFORMATION:
+    raise ValueError('The specified dataset is not supported yet.')
+
+  splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes
+
+  if split_name not in splits_to_sizes:
+    raise ValueError('data split name %s not recognized' % split_name)
+
+  # Prepare the variables for different datasets.
+  num_classes = _DATASETS_INFORMATION[dataset_name].num_classes
+  ignore_label = _DATASETS_INFORMATION[dataset_name].ignore_label
+
+  file_pattern = _FILE_PATTERN
+  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)
+
+  # Specify how the TF-Examples are decoded.
+  keys_to_features = {
+      'image/encoded': tf.FixedLenFeature(
+          (), tf.string, default_value=''),
+      'image/filename': tf.FixedLenFeature(
+          (), tf.string, default_value=''),
+      'image/format': tf.FixedLenFeature(
+          (), tf.string, default_value='jpeg'),
+      'image/height': tf.FixedLenFeature(
+          (), tf.int64, default_value=0),
+      'image/width': tf.FixedLenFeature(
+          (), tf.int64, default_value=0),
+      'image/segmentation/class/encoded': tf.FixedLenFeature(
+          (), tf.string, default_value=''),
+      'image/segmentation/class/format': tf.FixedLenFeature(
+          (), tf.string, default_value='png'),
+  }
+  items_to_handlers = {
+      'image': tfexample_decoder.Image(
+          image_key='image/encoded',
+          format_key='image/format',
+          channels=3),
+      'image_name': tfexample_decoder.Tensor('image/filename'),
+      'height': tfexample_decoder.Tensor('image/height'),
+      'width': tfexample_decoder.Tensor('image/width'),
+      'labels_class': tfexample_decoder.Image(
+          image_key='image/segmentation/class/encoded',
+          format_key='image/segmentation/class/format',
+          channels=1),
+  }
+
+  decoder = tfexample_decoder.TFExampleDecoder(
+      keys_to_features, items_to_handlers)
+
+  return dataset.Dataset(
+      data_sources=file_pattern,
+      reader=tf.TFRecordReader,
+      decoder=decoder,
+      num_samples=splits_to_sizes[split_name],
+      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,
+      ignore_label=ignore_label,
+      num_classes=num_classes,
+      name=dataset_name,
+      multi_label=True)
diff --git a/research/deeplab/deeplab_demo.ipynb b/research/deeplab/deeplab_demo.ipynb
new file mode 100644
index 00000000..8578a1a8
--- /dev/null
+++ b/research/deeplab/deeplab_demo.ipynb
@@ -0,0 +1,342 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# DeepLab Demo\n",
+    "\n",
+    "This demo will demostrate the steps to run deeplab semantic segmentation model on sample input images.\n",
+    "\n",
+    "## Prerequisites\n",
+    "\n",
+    "Running this demo requires the following libraries:\n",
+    "\n",
+    "* Jupyter notebook (Python 2)\n",
+    "* Tensorflow (>= v1.5.0)\n",
+    "* Matplotlib\n",
+    "* Pillow\n",
+    "* numpy\n",
+    "* ipywidgets (follow the setup [here](https://ipywidgets.readthedocs.io/en/stable/user_install.html))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Imports"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import collections\n",
+    "import os\n",
+    "import StringIO\n",
+    "import sys\n",
+    "import tarfile\n",
+    "import tempfile\n",
+    "import urllib\n",
+    "\n",
+    "from IPython import display\n",
+    "from ipywidgets import interact\n",
+    "from ipywidgets import interactive\n",
+    "from matplotlib import gridspec\n",
+    "from matplotlib import pyplot as plt\n",
+    "import numpy as np\n",
+    "from PIL import Image\n",
+    "\n",
+    "import tensorflow as tf\n",
+    "\n",
+    "if tf.__version__ < '1.5.0':\n",
+    "    raise ImportError('Please upgrade your tensorflow installation to v1.5.0 or newer!')\n",
+    "\n",
+    "# Needed to show segmentation colormap labels\n",
+    "sys.path.append('utils')\n",
+    "import get_dataset_colormap"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Select and download models"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "_MODEL_URLS = {\n",
+    "    'xception_coco_voctrainaug': 'http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
+    "    'xception_coco_voctrainval': 'http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
+    "}\n",
+    "\n",
+    "Config = collections.namedtuple('Config', 'model_url, model_dir')\n",
+    "\n",
+    "def get_config(model_name, model_dir):\n",
+    "    return Config(_MODEL_URLS[model_name], model_dir)\n",
+    "\n",
+    "config_widget = interactive(get_config, model_name=_MODEL_URLS.keys(), model_dir='')\n",
+    "display.display(config_widget)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Check configuration and download the model\n",
+    "\n",
+    "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
+    "\n",
+    "config = config_widget.result\n",
+    "\n",
+    "model_dir = config.model_dir or tempfile.mkdtemp()\n",
+    "tf.gfile.MakeDirs(model_dir)\n",
+    "\n",
+    "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
+    "print 'downloading model to %s, this might take a while...' % download_path\n",
+    "urllib.urlretrieve(config.model_url, download_path)\n",
+    "print 'download completed!'"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Load model in TensorFlow"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "_FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
+    "\n",
+    "\n",
+    "class DeepLabModel(object):\n",
+    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
+    "    \n",
+    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
+    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
+    "    INPUT_SIZE = 513\n",
+    "\n",
+    "    def __init__(self, tarball_path):\n",
+    "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
+    "        self.graph = tf.Graph()\n",
+    "        \n",
+    "        graph_def = None\n",
+    "        # Extract frozen graph from tar archive.\n",
+    "        tar_file = tarfile.open(tarball_path)\n",
+    "        for tar_info in tar_file.getmembers():\n",
+    "            if _FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
+    "                file_handle = tar_file.extractfile(tar_info)\n",
+    "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
+    "                break\n",
+    "\n",
+    "        tar_file.close()\n",
+    "        \n",
+    "        if graph_def is None:\n",
+    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
+    "\n",
+    "        with self.graph.as_default():      \n",
+    "            tf.import_graph_def(graph_def, name='')\n",
+    "        \n",
+    "        self.sess = tf.Session(graph=self.graph)\n",
+    "            \n",
+    "    def run(self, image):\n",
+    "        \"\"\"Runs inference on a single image.\n",
+    "        \n",
+    "        Args:\n",
+    "            image: A PIL.Image object, raw input image.\n",
+    "            \n",
+    "        Returns:\n",
+    "            resized_image: RGB image resized from original input image.\n",
+    "            seg_map: Segmentation map of `resized_image`.\n",
+    "        \"\"\"\n",
+    "        width, height = image.size\n",
+    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
+    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
+    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
+    "        batch_seg_map = self.sess.run(\n",
+    "            self.OUTPUT_TENSOR_NAME,\n",
+    "            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
+    "        seg_map = batch_seg_map[0]\n",
+    "        return resized_image, seg_map\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model = DeepLabModel(download_path)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Helper methods"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "LABEL_NAMES = np.asarray([\n",
+    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
+    "    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
+    "    'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
+    "    'train', 'tv'\n",
+    "])\n",
+    "\n",
+    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
+    "FULL_COLOR_MAP = get_dataset_colormap.label_to_color_image(FULL_LABEL_MAP)\n",
+    "\n",
+    "\n",
+    "def vis_segmentation(image, seg_map):\n",
+    "    plt.figure(figsize=(15, 5))\n",
+    "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
+    "\n",
+    "    plt.subplot(grid_spec[0])\n",
+    "    plt.imshow(image)\n",
+    "    plt.axis('off')\n",
+    "    plt.title('input image')\n",
+    "    \n",
+    "    plt.subplot(grid_spec[1])\n",
+    "    seg_image = get_dataset_colormap.label_to_color_image(\n",
+    "        seg_map, get_dataset_colormap.get_pascal_name()).astype(np.uint8)\n",
+    "    plt.imshow(seg_image)\n",
+    "    plt.axis('off')\n",
+    "    plt.title('segmentation map')\n",
+    "\n",
+    "    plt.subplot(grid_spec[2])\n",
+    "    plt.imshow(image)\n",
+    "    plt.imshow(seg_image, alpha=0.7)\n",
+    "    plt.axis('off')\n",
+    "    plt.title('segmentation overlay')\n",
+    "    \n",
+    "    unique_labels = np.unique(seg_map)\n",
+    "    ax = plt.subplot(grid_spec[3])\n",
+    "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
+    "    ax.yaxis.tick_right()\n",
+    "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
+    "    plt.xticks([], [])\n",
+    "    ax.tick_params(width=0)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Run on sample images"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Note that we are using single scale inference in the demo for fast\n",
+    "# computation, so the results may slightly differ from the visualizations\n",
+    "# in README, which uses multi-scale and left-right flipped inputs.\n",
+    "\n",
+    "IMAGE_DIR = 'g3doc/img'\n",
+    "\n",
+    "def run_demo_image(image_name):\n",
+    "    try:\n",
+    "        image_path = os.path.join(IMAGE_DIR, image_name)\n",
+    "        orignal_im = Image.open(image_path)\n",
+    "    except IOError:\n",
+    "        print 'Failed to read image from %s.' % image_path \n",
+    "        return \n",
+    "    print 'running deeplab on image %s...' % image_name\n",
+    "    resized_im, seg_map = model.run(orignal_im)\n",
+    "    \n",
+    "    vis_segmentation(resized_im, seg_map)\n",
+    "\n",
+    "_ = interact(run_demo_image, image_name=['image1.jpg', 'image2.jpg', 'image3.jpg'])"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Run on internet images"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": [
+    "def get_an_internet_image(url):\n",
+    "    if not url:\n",
+    "        return\n",
+    "\n",
+    "    try:\n",
+    "        # Prefix with 'file://' for local file.\n",
+    "        if os.path.exists(url):\n",
+    "            url = 'file://' + url\n",
+    "        f = urllib.urlopen(url)\n",
+    "        jpeg_str = f.read()\n",
+    "    except IOError:\n",
+    "        print 'invalid url: ' + url\n",
+    "        return\n",
+    "\n",
+    "    orignal_im = Image.open(StringIO.StringIO(jpeg_str))\n",
+    "    print 'running deeplab on image %s...' % url\n",
+    "    resized_im, seg_map = model.run(orignal_im)\n",
+    "    \n",
+    "    vis_segmentation(resized_im, seg_map)\n",
+    "\n",
+    "_ = interact(get_an_internet_image, url='')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 2",
+   "language": "python",
+   "name": "python2"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 2
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython2",
+   "version": "2.7.13"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
diff --git a/research/deeplab/eval.py b/research/deeplab/eval.py
new file mode 100644
index 00000000..63669be7
--- /dev/null
+++ b/research/deeplab/eval.py
@@ -0,0 +1,175 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Evaluation script for the DeepLab model.
+
+See model.py for more details and usage.
+"""
+
+import math
+import tensorflow as tf
+from deeplab import common
+from deeplab import model
+from deeplab.datasets import segmentation_dataset
+from deeplab.utils import input_generator
+
+slim = tf.contrib.slim
+
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')
+
+# Settings for log directories.
+
+flags.DEFINE_string('eval_logdir', None, 'Where to write the event logs.')
+
+flags.DEFINE_string('checkpoint_dir', None, 'Directory of model checkpoints.')
+
+# Settings for evaluating the model.
+
+flags.DEFINE_integer('eval_batch_size', 1,
+                     'The number of images in each batch during evaluation.')
+
+flags.DEFINE_multi_integer('eval_crop_size', [513, 513],
+                           'Image crop size [height, width] for evaluation.')
+
+flags.DEFINE_integer('eval_interval_secs', 60 * 5,
+                     'How often (in seconds) to run evaluation.')
+
+# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or
+# rates = [6, 12, 18] if output_stride = 16. Note one could use different
+# atrous_rates/output_stride during training/evaluation.
+flags.DEFINE_multi_integer('atrous_rates', None,
+                           'Atrous rates for atrous spatial pyramid pooling.')
+
+flags.DEFINE_integer('output_stride', 16,
+                     'The ratio of input to output spatial resolution.')
+
+# Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale test.
+flags.DEFINE_multi_float('eval_scales', [1.0],
+                         'The scales to resize images for evaluation.')
+
+# Change to True for adding flipped images during test.
+flags.DEFINE_bool('add_flipped_images', False,
+                  'Add flipped images for evaluation or not.')
+
+# Dataset settings.
+
+flags.DEFINE_string('dataset', 'pascal_voc_seg',
+                    'Name of the segmentation dataset.')
+
+flags.DEFINE_string('eval_split', 'val',
+                    'Which split of the dataset used for evaluation')
+
+flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')
+
+flags.DEFINE_integer('max_number_of_evaluations', 0,
+                     'Maximum number of eval iterations. Will loop '
+                     'indefinitely upon nonpositive values.')
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  # Get dataset-dependent information.
+  dataset = segmentation_dataset.get_dataset(
+      FLAGS.dataset, FLAGS.eval_split, dataset_dir=FLAGS.dataset_dir)
+
+  tf.gfile.MakeDirs(FLAGS.eval_logdir)
+  tf.logging.info('Evaluating on %s set', FLAGS.eval_split)
+
+  with tf.Graph().as_default():
+    samples = input_generator.get(
+        dataset,
+        FLAGS.eval_crop_size,
+        FLAGS.eval_batch_size,
+        min_resize_value=FLAGS.min_resize_value,
+        max_resize_value=FLAGS.max_resize_value,
+        resize_factor=FLAGS.resize_factor,
+        dataset_split=FLAGS.eval_split,
+        is_training=False,
+        model_variant=FLAGS.model_variant)
+
+    model_options = common.ModelOptions(
+        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},
+        crop_size=FLAGS.eval_crop_size,
+        atrous_rates=FLAGS.atrous_rates,
+        output_stride=FLAGS.output_stride)
+
+    if tuple(FLAGS.eval_scales) == (1.0,):
+      tf.logging.info('Performing single-scale test.')
+      predictions = model.predict_labels(samples[common.IMAGE], model_options,
+                                         image_pyramid=FLAGS.image_pyramid)
+    else:
+      tf.logging.info('Performing multi-scale test.')
+      predictions = model.predict_labels_multi_scale(
+          samples[common.IMAGE],
+          model_options=model_options,
+          eval_scales=FLAGS.eval_scales,
+          add_flipped_images=FLAGS.add_flipped_images)
+    predictions = predictions[common.OUTPUT_TYPE]
+    predictions = tf.reshape(predictions, shape=[-1])
+    labels = tf.reshape(samples[common.LABEL], shape=[-1])
+    weights = tf.to_float(tf.not_equal(labels, dataset.ignore_label))
+
+    # Set ignore_label regions to label 0, because metrics.mean_iou requires
+    # range of labels = [0, dataset.num_classes). Note the ignore_lable regions
+    # are not evaluated since the corresponding regions contain weights = 0.
+    labels = tf.where(
+        tf.equal(labels, dataset.ignore_label), tf.zeros_like(labels), labels)
+
+    predictions_tag = 'miou'
+    for eval_scale in FLAGS.eval_scales:
+      predictions_tag += '_' + str(eval_scale)
+    if FLAGS.add_flipped_images:
+      predictions_tag += '_flipped'
+
+    # Define the evaluation metric.
+    metric_map = {}
+    metric_map[predictions_tag] = tf.metrics.mean_iou(
+        predictions, labels, dataset.num_classes, weights=weights)
+
+    metrics_to_values, metrics_to_updates = (
+        tf.contrib.metrics.aggregate_metric_map(metric_map))
+
+    for metric_name, metric_value in metrics_to_values.iteritems():
+      slim.summaries.add_scalar_summary(
+          metric_value, metric_name, print_summary=True)
+
+    num_batches = int(
+        math.ceil(dataset.num_samples / float(FLAGS.eval_batch_size)))
+
+    tf.logging.info('Eval num images %d', dataset.num_samples)
+    tf.logging.info('Eval batch size %d and num batch %d',
+                    FLAGS.eval_batch_size, num_batches)
+
+    num_eval_iters = None
+    if FLAGS.max_number_of_evaluations > 0:
+      num_eval_iters = FLAGS.max_number_of_evaluations
+    slim.evaluation.evaluation_loop(
+        master=FLAGS.master,
+        checkpoint_dir=FLAGS.checkpoint_dir,
+        logdir=FLAGS.eval_logdir,
+        num_evals=num_batches,
+        eval_op=metrics_to_updates.values(),
+        max_number_of_evaluations=num_eval_iters,
+        eval_interval_secs=FLAGS.eval_interval_secs)
+
+
+if __name__ == '__main__':
+  flags.mark_flag_as_required('checkpoint_dir')
+  flags.mark_flag_as_required('eval_logdir')
+  flags.mark_flag_as_required('dataset_dir')
+  tf.app.run()
diff --git a/research/deeplab/export_model.py b/research/deeplab/export_model.py
new file mode 100644
index 00000000..64eaa33f
--- /dev/null
+++ b/research/deeplab/export_model.py
@@ -0,0 +1,165 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Exports trained model to TensorFlow frozen graph."""
+
+import os
+import tensorflow as tf
+
+from tensorflow.python.tools import freeze_graph
+from deeplab import common
+from deeplab import input_preprocess
+from deeplab import model
+
+slim = tf.contrib.slim
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('checkpoint_path', None, 'Checkpoint path')
+
+flags.DEFINE_string('export_path', None,
+                    'Path to output Tensorflow frozen graph.')
+
+flags.DEFINE_integer('num_classes', 21, 'Number of classes.')
+
+flags.DEFINE_multi_integer('crop_size', [513, 513],
+                           'Crop size [height, width].')
+
+# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or
+# rates = [6, 12, 18] if output_stride = 16. For `mobilenet_v2`, use None. Note
+# one could use different atrous_rates/output_stride during training/evaluation.
+flags.DEFINE_multi_integer('atrous_rates', None,
+                           'Atrous rates for atrous spatial pyramid pooling.')
+
+flags.DEFINE_integer('output_stride', 8,
+                     'The ratio of input to output spatial resolution.')
+
+# Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale inference.
+flags.DEFINE_multi_float('inference_scales', [1.0],
+                         'The scales to resize images for inference.')
+
+flags.DEFINE_bool('add_flipped_images', False,
+                  'Add flipped images during inference or not.')
+
+# Input name of the exported model.
+_INPUT_NAME = 'ImageTensor'
+
+# Output name of the exported model.
+_OUTPUT_NAME = 'SemanticPredictions'
+
+
+def _create_input_tensors():
+  """Creates and prepares input tensors for DeepLab model.
+
+  This method creates a 4-D uint8 image tensor 'ImageTensor' with shape
+  [1, None, None, 3]. The actual input tensor name to use during inference is
+  'ImageTensor:0'.
+
+  Returns:
+    image: Preprocessed 4-D float32 tensor with shape [1, crop_height,
+      crop_width, 3].
+    original_image_size: Original image shape tensor [height, width].
+    resized_image_size: Resized image shape tensor [height, width].
+  """
+  # input_preprocess takes 4-D image tensor as input.
+  input_image = tf.placeholder(tf.uint8, [1, None, None, 3], name=_INPUT_NAME)
+  original_image_size = tf.shape(input_image)[1:3]
+
+  # Squeeze the dimension in axis=0 since `preprocess_image_and_label` assumes
+  # image to be 3-D.
+  image = tf.squeeze(input_image, axis=0)
+  resized_image, image, _ = input_preprocess.preprocess_image_and_label(
+      image,
+      label=None,
+      crop_height=FLAGS.crop_size[0],
+      crop_width=FLAGS.crop_size[1],
+      min_resize_value=FLAGS.min_resize_value,
+      max_resize_value=FLAGS.max_resize_value,
+      resize_factor=FLAGS.resize_factor,
+      is_training=False,
+      model_variant=FLAGS.model_variant)
+  resized_image_size = tf.shape(resized_image)[:2]
+
+  # Expand the dimension in axis=0, since the following operations assume the
+  # image to be 4-D.
+  image = tf.expand_dims(image, 0)
+
+  return image, original_image_size, resized_image_size
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.logging.info('Prepare to export model to: %s', FLAGS.export_path)
+
+  with tf.Graph().as_default():
+    image, image_size, resized_image_size = _create_input_tensors()
+
+    model_options = common.ModelOptions(
+        outputs_to_num_classes={common.OUTPUT_TYPE: FLAGS.num_classes},
+        crop_size=FLAGS.crop_size,
+        atrous_rates=FLAGS.atrous_rates,
+        output_stride=FLAGS.output_stride)
+
+    if tuple(FLAGS.inference_scales) == (1.0,):
+      tf.logging.info('Exported model performs single-scale inference.')
+      predictions = model.predict_labels(
+          image,
+          model_options=model_options,
+          image_pyramid=FLAGS.image_pyramid)
+    else:
+      tf.logging.info('Exported model performs multi-scale inference.')
+      predictions = model.predict_labels_multi_scale(
+          image,
+          model_options=model_options,
+          eval_scales=FLAGS.inference_scales,
+          add_flipped_images=FLAGS.add_flipped_images)
+
+    # Crop the valid regions from the predictions.
+    semantic_predictions = tf.slice(
+        predictions[common.OUTPUT_TYPE],
+        [0, 0, 0],
+        [1, resized_image_size[0], resized_image_size[1]])
+    # Resize back the prediction to the original image size.
+    def _resize_label(label, label_size):
+      # Expand dimension of label to [1, height, width, 1] for resize operation.
+      label = tf.expand_dims(label, 3)
+      resized_label = tf.image.resize_images(
+          label,
+          label_size,
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+          align_corners=True)
+      return tf.squeeze(resized_label, 3)
+    semantic_predictions = _resize_label(semantic_predictions, image_size)
+    semantic_predictions = tf.identity(semantic_predictions, name=_OUTPUT_NAME)
+
+    saver = tf.train.Saver(tf.model_variables())
+
+    tf.gfile.MakeDirs(os.path.dirname(FLAGS.export_path))
+    freeze_graph.freeze_graph_with_def_protos(
+        tf.get_default_graph().as_graph_def(add_shapes=True),
+        saver.as_saver_def(),
+        FLAGS.checkpoint_path,
+        _OUTPUT_NAME,
+        restore_op_name=None,
+        filename_tensor_name=None,
+        output_graph=FLAGS.export_path,
+        clear_devices=True,
+        initializer_nodes=None)
+
+
+if __name__ == '__main__':
+  flags.mark_flag_as_required('checkpoint_path')
+  flags.mark_flag_as_required('export_path')
+  tf.app.run()
diff --git a/research/deeplab/g3doc/cityscapes.md b/research/deeplab/g3doc/cityscapes.md
new file mode 100644
index 00000000..d729788a
--- /dev/null
+++ b/research/deeplab/g3doc/cityscapes.md
@@ -0,0 +1,145 @@
+# Running DeepLab on Cityscapes Semantic Segmentation Dataset
+
+This page walks through the steps required to run DeepLab on Cityscapes on a
+local machine.
+
+## Download dataset and convert to TFRecord
+
+We have prepared the script (under the folder `datasets`) to convert Cityscapes
+dataset to TFRecord. The users are required to download the dataset beforehand
+by registering the [website](https://www.cityscapes-dataset.com/).
+
+```bash
+# From the tensorflow/models/research/deeplab/datasets directory.
+sh convert_cityscapes.sh
+```
+
+The converted dataset will be saved at ./deeplab/datasets/cityscapes/tfrecord.
+
+## Recommended Directory Structure for Training and Evaluation
+
+```
++ datasets
+  + cityscapes
+    + leftImg8bit
+    + gtFine
+    + tfrecord
+    + exp
+      + train_on_train_set
+        + train
+        + eval
+        + vis
+```
+
+where the folder `train_on_train_set` stores the train/eval/vis events and
+results (when training DeepLab on the Cityscapes train set).
+
+## Running the train/eval/vis jobs
+
+A local training job using `xception_65` can be run with the following command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/train.py \
+    --logtostderr \
+    --train_split="train" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --train_crop_size=769 \
+    --train_crop_size=769 \
+    --train_batch_size=1 \
+    --tf_initial_checkpoints=${PATH_TO_INITIAL_CHECKPOINT} \
+    --train_logdir=${PATH_TO_TRAIN_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_INITIAL_CHECKPOINT} is the path to the initial checkpoint
+(usually an ImageNet pretrained checkpoint), ${PATH_TO_TRAIN_DIR} is the
+directory in which training checkpoints and events will be written to, and
+${PATH_TO_DATASET} is the directory in which the Cityscapes dataset resides.
+
+Note that for {train,eval,vis}.py:
+
+1.  We use small batch size during training. The users could change it based on
+    the available GPU memory and also set `fine_tune_batch_norm` to be False or
+    True depending on the use case.
+
+2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
+    setting output_stride=8.
+
+3.  The users could skip the flag, `decoder_output_stride`, if you do not want
+    to use the decoder structure.
+
+A local evaluation job using `xception_65` can be run with the following
+command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/eval.py \
+    --logtostderr \
+    --eval_split="val" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --eval_crop_size=1025 \
+    --eval_crop_size=2049 \
+    --checkpoint_dir=${PATH_TO_CHECKPOINT} \
+    --eval_logdir=${PATH_TO_EVAL_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_CHECKPOINT} is the path to the trained checkpoint (i.e., the
+path to train_logdir), ${PATH_TO_EVAL_DIR} is the directory in which evaluation
+events will be written to, and ${PATH_TO_DATASET} is the directory in which the
+Cityscapes dataset resides.
+
+A local visualization job using `xception_65` can be run with the following
+command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/vis.py \
+    --logtostderr \
+    --vis_split="val" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --vis_crop_size=1025 \
+    --vis_crop_size=2049 \
+    --colormap_type="cityscapes" \
+    --checkpoint_dir=${PATH_TO_CHECKPOINT} \
+    --vis_logdir=${PATH_TO_VIS_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_CHECKPOINT} is the path to the trained checkpoint (i.e., the
+path to train_logdir), ${PATH_TO_VIS_DIR} is the directory in which evaluation
+events will be written to, and ${PATH_TO_DATASET} is the directory in which the
+Cityscapes dataset resides. Note that if the users would like to save the
+segmentation results for evaluation server, set also_save_raw_predictions =
+True.
+
+## Running Tensorboard
+
+Progress for training and evaluation jobs can be inspected using Tensorboard. If
+using the recommended directory structure, Tensorboard can be run using the
+following command:
+
+```bash
+tensorboard --logdir=${PATH_TO_LOG_DIRECTORY}
+```
+
+where `${PATH_TO_LOG_DIRECTORY}` points to the directory that contains the
+train, eval, and vis directories (e.g., the folder `train_on_train_set` in the
+above example). Please note it may take Tensorboard a couple minutes to populate
+with data.
diff --git a/research/deeplab/g3doc/export_model.md b/research/deeplab/g3doc/export_model.md
new file mode 100644
index 00000000..c41649e6
--- /dev/null
+++ b/research/deeplab/g3doc/export_model.md
@@ -0,0 +1,23 @@
+# Export trained deeplab model to frozen inference graph
+
+After model training finishes, you could export it to a frozen TensorFlow
+inference graph proto. Your trained model checkpoint usually includes the
+following files:
+
+*   model.ckpt-${CHECKPOINT_NUMBER}.data-00000-of-00001,
+*   model.ckpt-${CHECKPOINT_NUMBER}.index
+*   model.ckpt-${CHECKPOINT_NUMBER}.meta
+
+After you have identified a candidate checkpoint to export, you can run the
+following commandline to export to a frozen graph:
+
+```bash
+# From tensorflow/models/research/
+# Assume all checkpoint files share the same path prefix `${CHECKPOINT_PATH}`.
+python deeplab/export_model.py \
+    --checkpoint_path=${CHECKPOINT_PATH} \
+    --export_path=${OUTPUT_DIR}/frozen_inference_graph.pb
+```
+
+Please also add other model specific flags as you use for training, such as
+`model_variant`, `add_image_level_feature`, etc.
diff --git a/research/deeplab/g3doc/faq.md b/research/deeplab/g3doc/faq.md
new file mode 100644
index 00000000..cafa4ab6
--- /dev/null
+++ b/research/deeplab/g3doc/faq.md
@@ -0,0 +1,39 @@
+#FAQ
+___
+Q1: What if I want to use other network backbones, such as ResNet [1], instead of only those provided ones (e.g., Xception)?
+
+A: The users could modify the provided core/feature_extractor.py to support more network backbones.
+___
+Q2: What if I want to train the model on other datasets?
+
+A: The users could modify the provided dataset/build_{cityscapes,voc2012}_data.py and dataset/segmentation_dataset.py to build their own dataset.
+___
+Q3: Where can I download the PASCAL VOC augmented training set?
+
+A: The PASCAL VOC augmented training set is provided by Bharath Hariharan et al. [2] Please refer to their [website](http://home.bharathh.info/pubs/codes/SBD/download.html) for details and consider citing their paper if using the dataset.
+___
+Q4: Why the implementation does not include DenseCRF [3]?
+
+A: We have not tried this. The interested users could take a look at Philipp Krähenbühl's [website](http://graphics.stanford.edu/projects/densecrf/) and [paper](https://arxiv.org/abs/1210.5644) for details.
+___
+Q5: What if I want to train the model and fine-tune the batch normalization parameters?
+
+A: Fine-tuning batch normalization requires large batch size, and thus in the train.py we suggest setting `num_clones` (number of GPUs on one machine) and `train_batch_size` to be as large as possible.
+___
+Q6: How can I train the model asynchronously?
+
+A: In the train.py, the users could set `num_replicas` (number of machines for training) and `num_ps_tasks` (we usually set `num_ps_tasks` = `num_replicas` / 2). See slim.deployment.model_deploy for more details.
+___
+## References
+
+1. **Deep Residual Learning for Image Recognition**<br />
+   Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun<br />
+   [[link]](https://arxiv.org/abs/1512.03385), In CVPR, 2016.
+
+2. **Semantic Contours from Inverse Detectors**<br />
+   Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, Jitendra Malik<br />
+   [[link]](http://home.bharathh.info/pubs/codes/SBD/download.html), In ICCV, 2011.
+
+3. **Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials**<br />
+   Philipp Krähenbühl, Vladlen Koltun<br />
+   [[link]](http://graphics.stanford.edu/projects/densecrf/), In NIPS, 2011.
diff --git a/research/deeplab/g3doc/img/image1.jpg b/research/deeplab/g3doc/img/image1.jpg
new file mode 100644
index 00000000..939b6f9c
Binary files /dev/null and b/research/deeplab/g3doc/img/image1.jpg differ
diff --git a/research/deeplab/g3doc/img/image2.jpg b/research/deeplab/g3doc/img/image2.jpg
new file mode 100644
index 00000000..5ec1b8ac
Binary files /dev/null and b/research/deeplab/g3doc/img/image2.jpg differ
diff --git a/research/deeplab/g3doc/img/image3.jpg b/research/deeplab/g3doc/img/image3.jpg
new file mode 100644
index 00000000..d788e3dc
Binary files /dev/null and b/research/deeplab/g3doc/img/image3.jpg differ
diff --git a/research/deeplab/g3doc/img/image_info.txt b/research/deeplab/g3doc/img/image_info.txt
new file mode 100644
index 00000000..583d113e
--- /dev/null
+++ b/research/deeplab/g3doc/img/image_info.txt
@@ -0,0 +1,13 @@
+Image provenance:
+
+image1.jpg: Philippe Put,
+  https://www.flickr.com/photos/34547181@N00/14499172124
+
+image2.jpg: Peretz Partensky
+  https://www.flickr.com/photos/ifl/3926001309
+
+image3.jpg: Peter Harrison
+  https://www.flickr.com/photos/devcentre/392585679
+
+
+vis[1-3].png: Showing original image together with DeepLab segmentation map.
diff --git a/research/deeplab/g3doc/img/vis1.png b/research/deeplab/g3doc/img/vis1.png
new file mode 100644
index 00000000..41b8ecd8
Binary files /dev/null and b/research/deeplab/g3doc/img/vis1.png differ
diff --git a/research/deeplab/g3doc/img/vis2.png b/research/deeplab/g3doc/img/vis2.png
new file mode 100644
index 00000000..7fa7a4ca
Binary files /dev/null and b/research/deeplab/g3doc/img/vis2.png differ
diff --git a/research/deeplab/g3doc/img/vis3.png b/research/deeplab/g3doc/img/vis3.png
new file mode 100644
index 00000000..813b6340
Binary files /dev/null and b/research/deeplab/g3doc/img/vis3.png differ
diff --git a/research/deeplab/g3doc/installation.md b/research/deeplab/g3doc/installation.md
new file mode 100644
index 00000000..2716efff
--- /dev/null
+++ b/research/deeplab/g3doc/installation.md
@@ -0,0 +1,56 @@
+# Installation
+
+## Dependencies
+
+DeepLab depends on the following libraries:
+
+*   Numpy
+*   Pillow 1.0
+*   tf Slim (which is included in the "tensorflow/models/research/" checkout)
+*   Jupyter notebook
+*   Matplotlib
+*   Tensorflow
+
+For detailed steps to install Tensorflow, follow the [Tensorflow installation
+instructions](https://www.tensorflow.org/install/). A typical user can install
+Tensorflow using one of the following commands:
+
+```bash
+# For CPU
+pip install tensorflow
+# For GPU
+pip install tensorflow-gpu
+```
+
+The remaining libraries can be installed on Ubuntu 14.04 using via apt-get:
+
+```bash
+sudo apt-get install python-pil python-numpy
+sudo pip install jupyter
+sudo pip install matplotlib
+```
+
+## Add Libraries to PYTHONPATH
+
+When running locally, the tensorflow/models/research/ and slim directories
+should be appended to PYTHONPATH. This can be done by running the following from
+tensorflow/models/research/:
+
+```bash
+# From tensorflow/models/research/
+export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
+```
+
+Note: This command needs to run from every new terminal you start. If you wish
+to avoid running this manually, you can add it as a new line to the end of your
+~/.bashrc file.
+
+# Testing the Installation
+
+You can test if you have successfully installed the Tensorflow DeepLab by
+running the following command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/model_test.py
+```
diff --git a/research/deeplab/g3doc/model_zoo.md b/research/deeplab/g3doc/model_zoo.md
new file mode 100644
index 00000000..a279db85
--- /dev/null
+++ b/research/deeplab/g3doc/model_zoo.md
@@ -0,0 +1,142 @@
+# TensorFlow DeepLab Model Zoo
+
+We provide deeplab models pretrained on PASCAL VOC 2012 and Cityscapes datasets
+for reproducing our results, as well as some checkpoints that are only
+pretrained on ImageNet for training your own models.
+
+## DeepLab models trained on PASCAL VOC 2012
+
+Un-tar'ed directory includes:
+
+*   a frozen inference graph (`frozen_inference_graph.pb`). All frozen inference
+    graphs use output stride of 8 and a single eval scale of 1.0. No left-right
+    flips are used.
+
+*   a checkpoint (`model.ckpt.data-00000-of-00001`, `model.ckpt.index`)
+
+### Model details
+
+We provide several checkpoints that have been pretrained on VOC 2012 train_aug
+set or train_aug + trainval set. In the former case, one could train their model
+with smaller batch size and freeze batch normalization when limited GPU memory
+is available, since we have already fine-tuned the batch normalization for you.
+In the latter case, one could directly evaluate the checkpoints on VOC 2012 test
+set or use this checkpoint for demo.
+
+| Checkpoint name               | Network      | Pretrained | ASPP  | Decoder |
+:                               : backbone     : dataset    :       :         :
+| ----------------------------- | :----------: | :--------: | :---: | :-----: |
+| xception_coco_voc_trainaug    | Xception_65  | MS-COCO    | [6,   | OS = 4  |
+:                               :              : <br> VOC   : 12,   :         :
+:                               :              : 2012       : 18]   :         :
+:                               :              : train_aug  : for   :         :
+:                               :              : set        : OS=16 :         :
+:                               :              :            : <br>  :         :
+:                               :              :            : [12,  :         :
+:                               :              :            : 24,   :         :
+:                               :              :            : 36]   :         :
+:                               :              :            : for   :         :
+:                               :              :            : OS=8  :         :
+| xception_coco_voc_trainval    | Xception_65  | MS-COCO    | [6,   | OS = 4  |
+:                               :              : <br> VOC   : 12,   :         :
+:                               :              : 2012       : 18]   :         :
+:                               :              : train_aug  : for   :         :
+:                               :              : + trainval : OS=16 :         :
+:                               :              : sets       : <br>  :         :
+:                               :              :            : [12,  :         :
+:                               :              :            : 24,   :         :
+:                               :              :            : 36]   :         :
+:                               :              :            : for   :         :
+:                               :              :            : OS=8  :         :
+
+In the table, **OS** denotes output stride.
+
+Checkpoint name                                                                                                          | Eval OS   | Eval scales                | Left-right Flip | Multiply-Adds        | Runtime (sec)  | PASCAL mIOU                    | File Size
+------------------------------------------------------------------------------------------------------------------------ | :-------: | :------------------------: | :-------------: | :------------------: | :------------: | :----------------------------: | :-------:
+[xception_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_pascal_train_aug_2018_01_04.tar.gz)         | 16 <br> 8 | [1.0] <br> [0.5:0.25:1.75] | No <br> Yes     | 54.17B <br> 3055.35B | 0.7 <br> 223.2 | 82.20% (val) <br> 83.58% (val) | 439MB
+[xception_coco_voc_trainval](http://download.tensorflow.org/models/deeplabv3_pascal_trainval_2018_01_04.tar.gz)          | 8         | [0.5:0.25:1.75]            | Yes             | 3055.35B             | 223.2          | 87.80% (**test**)              | 439MB
+
+In the table, we report both computation complexity (in terms of Multiply-Adds
+and CPU Runtime) and segmentation performance (in terms of mIOU) on the PASCAL
+VOC val or test set. The reported runtime is calculated by tfprof on a
+workstation with CPU E5-1650 v3 @ 3.50GHz and 32GB memory. Note that applying
+multi-scale inputs and left-right flips increases the segmentation performance
+but also significantly increases the computation and thus may not be suitable
+for real-time applications.
+
+## DeepLab models trained on Cityscapes
+
+### Model details
+
+We provide several checkpoints that have been pretrained on Cityscapes
+train_fine set.
+
+Checkpoint name                       | Network backbone | Pretrained dataset                      | ASPP                                             | Decoder
+------------------------------------- | :--------------: | :-------------------------------------: | :----------------------------------------------: | :-----:
+xception_cityscapes_trainfine         | Xception_65      | ImageNet <br> Cityscapes train_fine set | [6, 12, 18] for OS=16 <br> [12, 24, 36] for OS=8 | OS = 4
+
+In the table, **OS** denotes output stride.
+
+Checkpoint name                                                                                                                  | Eval OS   | Eval scales                 | Left-right Flip | Multiply-Adds         | Runtime (sec)  | Cityscapes mIOU                | File Size
+-------------------------------------------------------------------------------------------------------------------------------- | :-------: | :-------------------------: | :-------------: | :-------------------: | :------------: | :----------------------------: | :-------:
+[xception_cityscapes_trainfine](http://download.tensorflow.org/models/deeplabv3_cityscapes_train_2018_02_06.tar.gz)              | 16 <br> 8 | [1.0] <br> [0.75:0.25:1.25] | No <br> Yes     | 418.64B <br> 8677.92B | 5.0 <br> 422.8 | 78.79% (val) <br> 80.42% (val) | 439MB
+
+## Checkpoints pretrained on ImageNet
+
+Un-tar'ed directory includes:
+
+*   model checkpoint (`model.ckpt.data-00000-of-00001`, `model.ckpt.index`).
+
+### Model details
+
+We also provide some checkpoints that are only pretrained on ImageNet so that
+one could use this for training your own models.
+
+*   xception: We adapt the original Xception model to the task of semantic
+    segmentation with the following changes: (1) more layers, (2) all max
+    pooling operations are replaced by strided (atrous) separable convolutions,
+    and (3) extra batch-norm and ReLU after each 3x3 depthwise convolution are
+    added.
+
+Model name                                                                             | File Size
+-------------------------------------------------------------------------------------- | :-------:
+[xception](http://download.tensorflow.org/models/deeplabv3_xception_2018_01_04.tar.gz) | 447MB
+
+## References
+
+1.  **Mobilenets: Efficient convolutional neural networks for mobile vision applications**<br />
+    Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam<br />
+    [[link]](https://arxiv.org/abs/1704.04861). arXiv:1704.04861, 2017.
+
+2.  **Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation**<br />
+    Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen<br />
+    [[link]](https://arxiv.org/abs/1801.04381). arXiv:1801.04381, 2018.
+
+3.  **Xception: Deep Learning with Depthwise Separable Convolutions**<br />
+    François Chollet<br />
+    [[link]](https://arxiv.org/abs/1610.02357). In the Proc. of CVPR, 2017.
+
+4.  **Deformable Convolutional Networks -- COCO Detection and Segmentation Challenge 2017 Entry**<br />
+    Haozhi Qi, Zheng Zhang, Bin Xiao, Han Hu, Bowen Cheng, Yichen Wei, Jifeng Dai<br />
+    [[link]](http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf). ICCV COCO Challenge
+    Workshop, 2017.
+
+5.  **The Pascal Visual Object Classes Challenge: A Retrospective**<br />
+    Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, Andrew Zisserman<br />
+    [[link]](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/). IJCV, 2014.
+
+6.  **Semantic Contours from Inverse Detectors**<br />
+    Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, Jitendra Malik<br />
+    [[link]](http://home.bharathh.info/pubs/codes/SBD/download.html). In the Proc. of ICCV, 2011.
+
+7.  **The Cityscapes Dataset for Semantic Urban Scene Understanding**<br />
+    Cordts, Marius, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele. <br />
+    [[link]](https://www.cityscapes-dataset.com/). In the Proc. of CVPR, 2016.
+
+8.  **Microsoft COCO: Common Objects in Context**<br />
+    Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollar<br />
+    [[link]](http://cocodataset.org/). In the Proc. of ECCV, 2014.
+
+9.  **ImageNet Large Scale Visual Recognition Challenge**<br />
+    Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei<br />
+    [[link]](http://www.image-net.org/). IJCV, 2015.
diff --git a/research/deeplab/g3doc/pascal.md b/research/deeplab/g3doc/pascal.md
new file mode 100644
index 00000000..d0827d35
--- /dev/null
+++ b/research/deeplab/g3doc/pascal.md
@@ -0,0 +1,157 @@
+# Running DeepLab on PASCAL VOC 2012 Semantic Segmentation Dataset
+
+This page walks through the steps required to run DeepLab on PASCAL VOC 2012 on
+a local machine.
+
+## Download dataset and convert to TFRecord
+
+We have prepared the script (under the folder `datasets`) to download and
+convert PASCAL VOC 2012 semantic segmentation dataset to TFRecord.
+
+```bash
+# From the tensorflow/models/research/deeplab/datasets directory.
+sh download_and_convert_voc2012.sh
+```
+
+The converted dataset will be saved at
+./deeplab/datasets/pascal_voc_seg/tfrecord
+
+## Recommended Directory Structure for Training and Evaluation
+
+```
++ datasets
+  + pascal_voc_seg
+    + VOCdevkit
+      + VOC2012
+        + JPEGImages
+        + SegmentationClass
+    + tfrecord
+    + exp
+      + train_on_train_set
+        + train
+        + eval
+        + vis
+```
+
+where the folder `train_on_train_set` stores the train/eval/vis events and
+results (when training DeepLab on the PASCAL VOC 2012 train set).
+
+## Running the train/eval/vis jobs
+
+A local training job using `xception_65` can be run with the following command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/train.py \
+    --logtostderr \
+    --train_split="train" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --train_crop_size=513 \
+    --train_crop_size=513 \
+    --train_batch_size=1 \
+    --tf_initial_checkpoints=${PATH_TO_INITIAL_CHECKPOINT} \
+    --train_logdir=${PATH_TO_TRAIN_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_INITIAL_CHECKPOINT} is the path to the initial checkpoint
+(usually an ImageNet pretrained checkpoint), ${PATH_TO_TRAIN_DIR} is the
+directory in which training checkpoints and events will be written to, and
+${PATH_TO_DATASET} is the directory in which the PASCAL VOC 2012 dataset
+resides.
+
+Note that for {train,eval,vis}.py:
+
+1.  We use small batch size during training. The users could change it based on
+    the available GPU memory and also set `fine_tune_batch_norm` to be False or
+    True depending on the use case.
+
+2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
+    setting output_stride=8.
+
+3.  The users could skip the flag, `decoder_output_stride`, if you do not want
+    to use the decoder structure.
+
+A local evaluation job using `xception_65` can be run with the following
+command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/eval.py \
+    --logtostderr \
+    --eval_split="val" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --eval_crop_size=513 \
+    --eval_crop_size=513 \
+    --checkpoint_dir=${PATH_TO_CHECKPOINT} \
+    --eval_logdir=${PATH_TO_EVAL_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_CHECKPOINT} is the path to the trained checkpoint (i.e., the
+path to train_logdir), ${PATH_TO_EVAL_DIR} is the directory in which evaluation
+events will be written to, and ${PATH_TO_DATASET} is the directory in which the
+PASCAL VOC 2012 dataset resides.
+
+A local visualization job using `xception_65` can be run with the following
+command:
+
+```bash
+# From tensorflow/models/research/
+python deeplab/vis.py \
+    --logtostderr \
+    --vis_split="val" \
+    --model_variant="xception_65" \
+    --atrous_rates=6 \
+    --atrous_rates=12 \
+    --atrous_rates=18 \
+    --output_stride=16 \
+    --decoder_output_stride=4 \
+    --vis_crop_size=513 \
+    --vis_crop_size=513 \
+    --checkpoint_dir=${PATH_TO_CHECKPOINT} \
+    --vis_logdir=${PATH_TO_VIS_DIR} \
+    --dataset_dir=${PATH_TO_DATASET}
+```
+
+where ${PATH_TO_CHECKPOINT} is the path to the trained checkpoint (i.e., the
+path to train_logdir), ${PATH_TO_VIS_DIR} is the directory in which evaluation
+events will be written to, and ${PATH_TO_DATASET} is the directory in which the
+PASCAL VOC 2012 dataset resides. Note that if the users would like to save the
+segmentation results for evaluation server, set also_save_raw_predictions =
+True.
+
+## Running Tensorboard
+
+Progress for training and evaluation jobs can be inspected using Tensorboard. If
+using the recommended directory structure, Tensorboard can be run using the
+following command:
+
+```bash
+tensorboard --logdir=${PATH_TO_LOG_DIRECTORY}
+```
+
+where `${PATH_TO_LOG_DIRECTORY}` points to the directory that contains the
+train, eval, and vis directories (e.g., the folder `train_on_train_set` in the
+above example). Please note it may take Tensorboard a couple minutes to populate
+with data.
+
+## Example
+
+We provide a script to run the {train,eval,vis,export_model}.py on the PASCAL VOC
+2012 dataset as an example. See the code in local_test.sh for details.
+
+```bash
+# From tensorflow/models/research/deeplab
+sh local_test.sh
+```
diff --git a/research/deeplab/input_preprocess.py b/research/deeplab/input_preprocess.py
new file mode 100644
index 00000000..c8d5768a
--- /dev/null
+++ b/research/deeplab/input_preprocess.py
@@ -0,0 +1,137 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Prepares the data used for DeepLab training/evaluation."""
+import tensorflow as tf
+from deeplab.core import feature_extractor
+from deeplab.core import preprocess_utils
+
+
+# The probability of flipping the images and labels
+# left-right during training
+_PROB_OF_FLIP = 0.5
+
+
+def preprocess_image_and_label(image,
+                               label,
+                               crop_height,
+                               crop_width,
+                               min_resize_value=None,
+                               max_resize_value=None,
+                               resize_factor=None,
+                               min_scale_factor=1.,
+                               max_scale_factor=1.,
+                               scale_factor_step_size=0,
+                               ignore_label=255,
+                               is_training=True,
+                               model_variant=None):
+  """Preprocesses the image and label.
+
+  Args:
+    image: Input image.
+    label: Ground truth annotation label.
+    crop_height: The height value used to crop the image and label.
+    crop_width: The width value used to crop the image and label.
+    min_resize_value: Desired size of the smaller image side.
+    max_resize_value: Maximum allowed size of the larger image side.
+    resize_factor: Resized dimensions are multiple of factor plus one.
+    min_scale_factor: Minimum scale factor value.
+    max_scale_factor: Maximum scale factor value.
+    scale_factor_step_size: The step size from min scale factor to max scale
+      factor. The input is randomly scaled based on the value of
+      (min_scale_factor, max_scale_factor, scale_factor_step_size).
+    ignore_label: The label value which will be ignored for training and
+      evaluation.
+    is_training: If the preprocessing is used for training or not.
+    model_variant: Model variant (string) for choosing how to mean-subtract the
+      images. See feature_extractor.network_map for supported model variants.
+
+  Returns:
+    original_image: Original image (could be resized).
+    processed_image: Preprocessed image.
+    label: Preprocessed ground truth segmentation label.
+
+  Raises:
+    ValueError: Ground truth label not provided during training.
+  """
+  if is_training and label is None:
+    raise ValueError('During training, label must be provided.')
+  if model_variant is None:
+    tf.logging.warning('Default mean-subtraction is performed. Please specify '
+                       'a model_variant. See feature_extractor.network_map for '
+                       'supported model variants.')
+
+  # Keep reference to original image.
+  original_image = image
+
+  processed_image = tf.cast(image, tf.float32)
+
+  if label is not None:
+    label = tf.cast(label, tf.int32)
+
+  # Resize image and label to the desired range.
+  if min_resize_value is not None or max_resize_value is not None:
+    [processed_image, label] = (
+        preprocess_utils.resize_to_range(
+            image=processed_image,
+            label=label,
+            min_size=min_resize_value,
+            max_size=max_resize_value,
+            factor=resize_factor,
+            align_corners=True))
+    # The `original_image` becomes the resized image.
+    original_image = tf.identity(processed_image)
+
+  # Data augmentation by randomly scaling the inputs.
+  scale = preprocess_utils.get_random_scale(
+      min_scale_factor, max_scale_factor, scale_factor_step_size)
+  processed_image, label = preprocess_utils.randomly_scale_image_and_label(
+      processed_image, label, scale)
+  processed_image.set_shape([None, None, 3])
+
+  # Pad image and label to have dimensions >= [crop_height, crop_width]
+  image_shape = tf.shape(processed_image)
+  image_height = image_shape[0]
+  image_width = image_shape[1]
+
+  target_height = image_height + tf.maximum(crop_height - image_height, 0)
+  target_width = image_width + tf.maximum(crop_width - image_width, 0)
+
+  # Pad image with mean pixel value.
+  mean_pixel = tf.reshape(
+      feature_extractor.mean_pixel(model_variant), [1, 1, 3])
+  processed_image = preprocess_utils.pad_to_bounding_box(
+      processed_image, 0, 0, target_height, target_width, mean_pixel)
+
+  if label is not None:
+    label = preprocess_utils.pad_to_bounding_box(
+        label, 0, 0, target_height, target_width, ignore_label)
+
+  # Randomly crop the image and label.
+  if is_training and label is not None:
+    processed_image, label = preprocess_utils.random_crop(
+        [processed_image, label], crop_height, crop_width)
+
+  processed_image.set_shape([crop_height, crop_width, 3])
+
+  if label is not None:
+    label.set_shape([crop_height, crop_width, 1])
+
+  if is_training:
+    # Randomly left-right flip the image and label.
+    processed_image, label, _ = preprocess_utils.flip_dim(
+        [processed_image, label], _PROB_OF_FLIP, dim=1)
+
+  return original_image, processed_image, label
diff --git a/research/deeplab/local_test.sh b/research/deeplab/local_test.sh
new file mode 100644
index 00000000..65c827af
--- /dev/null
+++ b/research/deeplab/local_test.sh
@@ -0,0 +1,150 @@
+#!/bin/bash
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+#
+# This script is used to run local test on PASCAL VOC 2012. Users could also
+# modify from this script for their use case.
+#
+# Usage:
+#   # From the tensorflow/models/research/deeplab directory.
+#   sh ./local_test.sh
+#
+#
+
+# Exit immediately if a command exits with a non-zero status.
+set -e
+
+# Move one-level up to tensorflow/models/research directory.
+cd ..
+
+# Update PYTHONPATH.
+export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
+
+# Set up the working environment.
+CURRENT_DIR=$(pwd)
+WORK_DIR="${CURRENT_DIR}/deeplab"
+
+# Run model_test first to make sure the PYTHONPATH is correctly set.
+python "${WORK_DIR}"/model_test.py -v
+
+# Go to datasets folder and download PASCAL VOC 2012 segmentation dataset.
+DATASET_DIR="datasets"
+cd "${WORK_DIR}/${DATASET_DIR}"
+sh download_and_convert_voc2012.sh
+
+# Go back to original directory.
+cd "${CURRENT_DIR}"
+
+# Set up the working directories.
+PASCAL_FOLDER="pascal_voc_seg"
+EXP_FOLDER="exp/train_on_trainval_set"
+INIT_FOLDER="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/init_models"
+TRAIN_LOGDIR="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/train"
+EVAL_LOGDIR="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/eval"
+VIS_LOGDIR="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/vis"
+EXPORT_DIR="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/${EXP_FOLDER}/export"
+mkdir -p "${INIT_FOLDER}"
+mkdir -p "${TRAIN_LOGDIR}"
+mkdir -p "${EVAL_LOGDIR}"
+mkdir -p "${VIS_LOGDIR}"
+mkdir -p "${EXPORT_DIR}"
+
+# Copy locally the trained checkpoint as the initial checkpoint.
+TF_INIT_ROOT="http://download.tensorflow.org/models"
+TF_INIT_CKPT="deeplabv3_pascal_train_aug_2018_01_04.tar.gz"
+cd "${INIT_FOLDER}"
+wget -nd -c "${TF_INIT_ROOT}/${TF_INIT_CKPT}"
+tar -xf "${TF_INIT_CKPT}"
+cd "${CURRENT_DIR}"
+
+PASCAL_DATASET="${WORK_DIR}/${DATASET_DIR}/${PASCAL_FOLDER}/tfrecord"
+
+# Train 10 iterations.
+NUM_ITERATIONS=10
+python "${WORK_DIR}"/train.py \
+  --logtostderr \
+  --train_split="trainval" \
+  --model_variant="xception_65" \
+  --atrous_rates=6 \
+  --atrous_rates=12 \
+  --atrous_rates=18 \
+  --output_stride=16 \
+  --decoder_output_stride=4 \
+  --train_crop_size=513 \
+  --train_crop_size=513 \
+  --train_batch_size=4 \
+  --training_number_of_steps="${NUM_ITERATIONS}" \
+  --fine_tune_batch_norm=true \
+  --tf_initial_checkpoint="${INIT_FOLDER}/deeplabv3_pascal_train_aug/model.ckpt" \
+  --train_logdir="${TRAIN_LOGDIR}" \
+  --dataset_dir="${PASCAL_DATASET}"
+
+# Run evaluation. This performs eval over the full val split (1449 images) and
+# will take a while.
+# Using the provided checkpoint, one should expect mIOU=82.20%.
+python "${WORK_DIR}"/eval.py \
+  --logtostderr \
+  --eval_split="val" \
+  --model_variant="xception_65" \
+  --atrous_rates=6 \
+  --atrous_rates=12 \
+  --atrous_rates=18 \
+  --output_stride=16 \
+  --decoder_output_stride=4 \
+  --eval_crop_size=513 \
+  --eval_crop_size=513 \
+  --checkpoint_dir="${TRAIN_LOGDIR}" \
+  --eval_logdir="${EVAL_LOGDIR}" \
+  --dataset_dir="${PASCAL_DATASET}" \
+  --max_number_of_evaluations=1
+
+# Visualize the results.
+python "${WORK_DIR}"/vis.py \
+  --logtostderr \
+  --vis_split="val" \
+  --model_variant="xception_65" \
+  --atrous_rates=6 \
+  --atrous_rates=12 \
+  --atrous_rates=18 \
+  --output_stride=16 \
+  --decoder_output_stride=4 \
+  --vis_crop_size=513 \
+  --vis_crop_size=513 \
+  --checkpoint_dir="${TRAIN_LOGDIR}" \
+  --vis_logdir="${VIS_LOGDIR}" \
+  --dataset_dir="${PASCAL_DATASET}" \
+  --max_number_of_iterations=1
+
+# Export the trained checkpoint.
+CKPT_PATH="${TRAIN_LOGDIR}/model.ckpt-${NUM_ITERATIONS}"
+EXPORT_PATH="${EXPORT_DIR}/frozen_inference_graph.pb"
+
+python "${WORK_DIR}"/export_model.py \
+  --logtostderr \
+  --checkpoint_path="${CKPT_PATH}" \
+  --export_path="${EXPORT_PATH}" \
+  --model_variant="xception_65" \
+  --atrous_rates=6 \
+  --atrous_rates=12 \
+  --atrous_rates=18 \
+  --output_stride=16 \
+  --decoder_output_stride=4 \
+  --num_classes=21 \
+  --crop_size=513 \
+  --crop_size=513 \
+  --inference_scales=1.0
+
+# Run inference with the exported checkpoint.
+# Please refer to the provided deeplab_demo.ipynb for an example.
diff --git a/research/deeplab/model.py b/research/deeplab/model.py
new file mode 100644
index 00000000..d25832bb
--- /dev/null
+++ b/research/deeplab/model.py
@@ -0,0 +1,685 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Provides DeepLab model definition and helper functions.
+
+DeepLab is a deep learning system for semantic image segmentation with
+the following features:
+
+(1) Atrous convolution to explicitly control the resolution at which
+feature responses are computed within Deep Convolutional Neural Networks.
+
+(2) Atrous spatial pyramid pooling (ASPP) to robustly segment objects at
+multiple scales with filters at multiple sampling rates and effective
+fields-of-views.
+
+(3) ASPP module augmented with image-level feature and batch normalization.
+
+(4) A simple yet effective decoder module to recover the object boundaries.
+
+See the following papers for more details:
+
+"Encoder-Decoder with Atrous Separable Convolution for Semantic Image
+Segmentation"
+Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam.
+(https://arxiv.org/abs1802.02611)
+
+"Rethinking Atrous Convolution for Semantic Image Segmentation,"
+Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam
+(https://arxiv.org/abs/1706.05587)
+
+"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
+Atrous Convolution, and Fully Connected CRFs",
+Liang-Chieh Chen*, George Papandreou*, Iasonas Kokkinos, Kevin Murphy,
+Alan L Yuille (* equal contribution)
+(https://arxiv.org/abs/1606.00915)
+
+"Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected
+CRFs"
+Liang-Chieh Chen*, George Papandreou*, Iasonas Kokkinos, Kevin Murphy,
+Alan L. Yuille (* equal contribution)
+(https://arxiv.org/abs/1412.7062)
+"""
+import tensorflow as tf
+from deeplab.core import feature_extractor
+
+slim = tf.contrib.slim
+
+_LOGITS_SCOPE_NAME = 'logits'
+_MERGED_LOGITS_SCOPE = 'merged_logits'
+_IMAGE_POOLING_SCOPE = 'image_pooling'
+_ASPP_SCOPE = 'aspp'
+_CONCAT_PROJECTION_SCOPE = 'concat_projection'
+_DECODER_SCOPE = 'decoder'
+
+
+def get_extra_layer_scopes():
+  """Gets the scopes for extra layers.
+
+  Returns:
+    A list of scopes for extra layers.
+  """
+  return [
+      _LOGITS_SCOPE_NAME,
+      _IMAGE_POOLING_SCOPE,
+      _ASPP_SCOPE,
+      _CONCAT_PROJECTION_SCOPE,
+      _DECODER_SCOPE,
+  ]
+
+
+def predict_labels_multi_scale(images,
+                               model_options,
+                               eval_scales=(1.0,),
+                               add_flipped_images=False):
+  """Predicts segmentation labels.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    model_options: A ModelOptions instance to configure models.
+    eval_scales: The scales to resize images for evaluation.
+    add_flipped_images: Add flipped images for evaluation or not.
+
+  Returns:
+    A dictionary with keys specifying the output_type (e.g., semantic
+      prediction) and values storing Tensors representing predictions (argmax
+      over channels). Each prediction has size [batch, height, width].
+  """
+  outputs_to_predictions = {
+      output: []
+      for output in model_options.outputs_to_num_classes
+  }
+
+  for i, image_scale in enumerate(eval_scales):
+    with tf.variable_scope(tf.get_variable_scope(), reuse=True if i else None):
+      outputs_to_scales_to_logits = multi_scale_logits(
+          images,
+          model_options=model_options,
+          image_pyramid=[image_scale],
+          is_training=False,
+          fine_tune_batch_norm=False)
+
+    if add_flipped_images:
+      with tf.variable_scope(tf.get_variable_scope(), reuse=True):
+        outputs_to_scales_to_logits_reversed = multi_scale_logits(
+            tf.reverse_v2(images, [2]),
+            model_options=model_options,
+            image_pyramid=[image_scale],
+            is_training=False,
+            fine_tune_batch_norm=False)
+
+    for output in sorted(outputs_to_scales_to_logits):
+      scales_to_logits = outputs_to_scales_to_logits[output]
+      logits = tf.image.resize_bilinear(
+          scales_to_logits[_MERGED_LOGITS_SCOPE],
+          tf.shape(images)[1:3],
+          align_corners=True)
+      outputs_to_predictions[output].append(
+          tf.expand_dims(tf.nn.softmax(logits), 4))
+
+      if add_flipped_images:
+        scales_to_logits_reversed = (
+            outputs_to_scales_to_logits_reversed[output])
+        logits_reversed = tf.image.resize_bilinear(
+            tf.reverse_v2(scales_to_logits_reversed[_MERGED_LOGITS_SCOPE], [2]),
+            tf.shape(images)[1:3],
+            align_corners=True)
+        outputs_to_predictions[output].append(
+            tf.expand_dims(tf.nn.softmax(logits_reversed), 4))
+
+  for output in sorted(outputs_to_predictions):
+    predictions = outputs_to_predictions[output]
+    # Compute average prediction across different scales and flipped images.
+    predictions = tf.reduce_mean(tf.concat(predictions, 4), axis=4)
+    outputs_to_predictions[output] = tf.argmax(predictions, 3)
+
+  return outputs_to_predictions
+
+
+def predict_labels(images, model_options, image_pyramid=None):
+  """Predicts segmentation labels.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    model_options: A ModelOptions instance to configure models.
+    image_pyramid: Input image scales for multi-scale feature extraction.
+
+  Returns:
+    A dictionary with keys specifying the output_type (e.g., semantic
+      prediction) and values storing Tensors representing predictions (argmax
+      over channels). Each prediction has size [batch, height, width].
+  """
+  outputs_to_scales_to_logits = multi_scale_logits(
+      images,
+      model_options=model_options,
+      image_pyramid=image_pyramid,
+      is_training=False,
+      fine_tune_batch_norm=False)
+
+  predictions = {}
+  for output in sorted(outputs_to_scales_to_logits):
+    scales_to_logits = outputs_to_scales_to_logits[output]
+    logits = tf.image.resize_bilinear(
+        scales_to_logits[_MERGED_LOGITS_SCOPE],
+        tf.shape(images)[1:3],
+        align_corners=True)
+    predictions[output] = tf.argmax(logits, 3)
+
+  return predictions
+
+
+def scale_dimension(dim, scale):
+  """Scales the input dimension.
+
+  Args:
+    dim: Input dimension (a scalar or a scalar Tensor).
+    scale: The amount of scaling applied to the input.
+
+  Returns:
+    Scaled dimension.
+  """
+  if isinstance(dim, tf.Tensor):
+    return tf.cast((tf.to_float(dim) - 1.0) * scale + 1.0, dtype=tf.int32)
+  else:
+    return int((float(dim) - 1.0) * scale + 1.0)
+
+
+def multi_scale_logits(images,
+                       model_options,
+                       image_pyramid,
+                       weight_decay=0.0001,
+                       is_training=False,
+                       fine_tune_batch_norm=False):
+  """Gets the logits for multi-scale inputs.
+
+  The returned logits are all downsampled (due to max-pooling layers)
+  for both training and evaluation.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    model_options: A ModelOptions instance to configure models.
+    image_pyramid: Input image scales for multi-scale feature extraction.
+
+    weight_decay: The weight decay for model variables.
+    is_training: Is training or not.
+    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+
+  Returns:
+    outputs_to_scales_to_logits: A map of maps from output_type (e.g.,
+      semantic prediction) to a dictionary of multi-scale logits names to
+      logits. For each output_type, the dictionary has keys which
+      correspond to the scales and values which correspond to the logits.
+      For example, if `scales` equals [1.0, 1.5], then the keys would
+      include 'merged_logits', 'logits_1.00' and 'logits_1.50'.
+
+  Raises:
+    ValueError: If model_options doesn't specify crop_size and its
+      add_image_level_feature = True, since add_image_level_feature requires
+      crop_size information. Or, if model_options has model_variant =
+      'mobilenet_v2' but atrous_rates or decoder_output_stride are not None.
+  """
+  # Setup default values.
+  if not image_pyramid:
+    image_pyramid = [1.0]
+
+  if model_options.crop_size is None and model_options.add_image_level_feature:
+    raise ValueError(
+        'Crop size must be specified for using image-level feature.')
+
+  crop_height = (
+      model_options.crop_size[0]
+      if model_options.crop_size else tf.shape(images)[1])
+  crop_width = (
+      model_options.crop_size[1]
+      if model_options.crop_size else tf.shape(images)[2])
+
+  # Compute the height, width for the output logits.
+  logits_output_stride = (
+      model_options.decoder_output_stride or model_options.output_stride)
+
+  logits_height = scale_dimension(
+      crop_height,
+      max(1.0, max(image_pyramid)) / logits_output_stride)
+  logits_width = scale_dimension(
+      crop_width,
+      max(1.0, max(image_pyramid)) / logits_output_stride)
+
+  # Compute the logits for each scale in the image pyramid.
+  outputs_to_scales_to_logits = {
+      k: {}
+      for k in model_options.outputs_to_num_classes
+  }
+
+  for count, image_scale in enumerate(image_pyramid):
+    if image_scale != 1.0:
+      scaled_height = scale_dimension(crop_height, image_scale)
+      scaled_width = scale_dimension(crop_width, image_scale)
+      scaled_crop_size = [scaled_height, scaled_width]
+      scaled_images = tf.image.resize_bilinear(
+          images, scaled_crop_size, align_corners=True)
+      if model_options.crop_size:
+        scaled_images.set_shape([None, scaled_height, scaled_width, 3])
+    else:
+      scaled_crop_size = model_options.crop_size
+      scaled_images = images
+
+    updated_options = model_options._replace(crop_size=scaled_crop_size)
+    outputs_to_logits = _get_logits(
+        scaled_images,
+        updated_options,
+        weight_decay=weight_decay,
+        reuse=True if count else None,
+        is_training=is_training,
+        fine_tune_batch_norm=fine_tune_batch_norm)
+
+    # Resize the logits to have the same dimension before merging.
+    for output in sorted(outputs_to_logits):
+      outputs_to_logits[output] = tf.image.resize_bilinear(
+          outputs_to_logits[output], [logits_height, logits_width],
+          align_corners=True)
+
+    # Return when only one input scale.
+    if len(image_pyramid) == 1:
+      for output in sorted(model_options.outputs_to_num_classes):
+        outputs_to_scales_to_logits[output][
+            _MERGED_LOGITS_SCOPE] = outputs_to_logits[output]
+      return outputs_to_scales_to_logits
+
+    # Save logits to the output map.
+    for output in sorted(model_options.outputs_to_num_classes):
+      outputs_to_scales_to_logits[output][
+          'logits_%.2f' % image_scale] = outputs_to_logits[output]
+
+  # Merge the logits from all the multi-scale inputs.
+  for output in sorted(model_options.outputs_to_num_classes):
+    # Concatenate the multi-scale logits for each output type.
+    all_logits = [
+        tf.expand_dims(logits, axis=4)
+        for logits in outputs_to_scales_to_logits[output].values()
+    ]
+    all_logits = tf.concat(all_logits, 4)
+    merge_fn = (
+        tf.reduce_max
+        if model_options.merge_method == 'max' else tf.reduce_mean)
+    outputs_to_scales_to_logits[output][_MERGED_LOGITS_SCOPE] = merge_fn(
+        all_logits, axis=4)
+
+  return outputs_to_scales_to_logits
+
+
+def _extract_features(images,
+                      model_options,
+                      weight_decay=0.0001,
+                      reuse=None,
+                      is_training=False,
+                      fine_tune_batch_norm=False):
+  """Extracts features by the particular model_variant.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    model_options: A ModelOptions instance to configure models.
+    weight_decay: The weight decay for model variables.
+    reuse: Reuse the model variables or not.
+    is_training: Is training or not.
+    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+
+  Returns:
+    concat_logits: A tensor of size [batch, feature_height, feature_width,
+      feature_channels], where feature_height/feature_width are determined by
+      the images height/width and output_stride.
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+  """
+  features, end_points = feature_extractor.extract_features(
+      images,
+      output_stride=model_options.output_stride,
+      multi_grid=model_options.multi_grid,
+      model_variant=model_options.model_variant,
+      weight_decay=weight_decay,
+      reuse=reuse,
+      is_training=is_training,
+      fine_tune_batch_norm=fine_tune_batch_norm)
+
+  if not model_options.aspp_with_batch_norm:
+    return features, end_points
+  else:
+    batch_norm_params = {
+        'is_training': is_training and fine_tune_batch_norm,
+        'decay': 0.9997,
+        'epsilon': 1e-5,
+        'scale': True,
+    }
+
+    with slim.arg_scope(
+        [slim.conv2d, slim.separable_conv2d],
+        weights_regularizer=slim.l2_regularizer(weight_decay),
+        activation_fn=tf.nn.relu,
+        normalizer_fn=slim.batch_norm,
+        padding='SAME',
+        stride=1,
+        reuse=reuse):
+      with slim.arg_scope([slim.batch_norm], **batch_norm_params):
+        depth = 256
+        branch_logits = []
+
+        if model_options.add_image_level_feature:
+          pool_height = scale_dimension(model_options.crop_size[0],
+                                        1. / model_options.output_stride)
+          pool_width = scale_dimension(model_options.crop_size[1],
+                                       1. / model_options.output_stride)
+          image_feature = slim.avg_pool2d(
+              features, [pool_height, pool_width], [pool_height, pool_width],
+              padding='VALID')
+          image_feature = slim.conv2d(
+              image_feature, depth, 1, scope=_IMAGE_POOLING_SCOPE)
+          image_feature = tf.image.resize_bilinear(
+              image_feature, [pool_height, pool_width], align_corners=True)
+          image_feature.set_shape([None, pool_height, pool_width, depth])
+          branch_logits.append(image_feature)
+
+        # Employ a 1x1 convolution.
+        branch_logits.append(slim.conv2d(features, depth, 1,
+                                         scope=_ASPP_SCOPE + str(0)))
+
+        if model_options.atrous_rates:
+          # Employ 3x3 convolutions with different atrous rates.
+          for i, rate in enumerate(model_options.atrous_rates, 1):
+            scope = _ASPP_SCOPE + str(i)
+            if model_options.aspp_with_separable_conv:
+              aspp_features = _split_separable_conv2d(
+                  features,
+                  filters=depth,
+                  rate=rate,
+                  weight_decay=weight_decay,
+                  scope=scope)
+            else:
+              aspp_features = slim.conv2d(
+                  features, depth, 3, rate=rate, scope=scope)
+            branch_logits.append(aspp_features)
+
+        # Merge branch logits.
+        concat_logits = tf.concat(branch_logits, 3)
+        concat_logits = slim.conv2d(
+            concat_logits, depth, 1, scope=_CONCAT_PROJECTION_SCOPE)
+        concat_logits = slim.dropout(
+            concat_logits,
+            keep_prob=0.9,
+            is_training=is_training,
+            scope=_CONCAT_PROJECTION_SCOPE + '_dropout')
+
+        return concat_logits, end_points
+
+
+def _get_logits(images,
+                model_options,
+                weight_decay=0.0001,
+                reuse=None,
+                is_training=False,
+                fine_tune_batch_norm=False):
+  """Gets the logits by atrous/image spatial pyramid pooling.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    model_options: A ModelOptions instance to configure models.
+    weight_decay: The weight decay for model variables.
+    reuse: Reuse the model variables or not.
+    is_training: Is training or not.
+    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+
+  Returns:
+    outputs_to_logits: A map from output_type to logits.
+  """
+  features, end_points = _extract_features(
+      images,
+      model_options,
+      weight_decay=weight_decay,
+      reuse=reuse,
+      is_training=is_training,
+      fine_tune_batch_norm=fine_tune_batch_norm)
+
+  if model_options.decoder_output_stride is not None:
+    decoder_height = scale_dimension(model_options.crop_size[0],
+                                     1.0 / model_options.decoder_output_stride)
+    decoder_width = scale_dimension(model_options.crop_size[1],
+                                    1.0 / model_options.decoder_output_stride)
+    features = refine_by_decoder(
+        features,
+        end_points,
+        decoder_height=decoder_height,
+        decoder_width=decoder_width,
+        decoder_use_separable_conv=model_options.decoder_use_separable_conv,
+        model_variant=model_options.model_variant,
+        weight_decay=weight_decay,
+        reuse=reuse,
+        is_training=is_training,
+        fine_tune_batch_norm=fine_tune_batch_norm)
+
+  outputs_to_logits = {}
+  for output in sorted(model_options.outputs_to_num_classes):
+    outputs_to_logits[output] = _get_branch_logits(
+        features,
+        model_options.outputs_to_num_classes[output],
+        model_options.atrous_rates,
+        aspp_with_batch_norm=model_options.aspp_with_batch_norm,
+        kernel_size=model_options.logits_kernel_size,
+        weight_decay=weight_decay,
+        reuse=reuse,
+        scope_suffix=output)
+
+  return outputs_to_logits
+
+
+def refine_by_decoder(features,
+                      end_points,
+                      decoder_height,
+                      decoder_width,
+                      decoder_use_separable_conv=False,
+                      model_variant=None,
+                      weight_decay=0.0001,
+                      reuse=None,
+                      is_training=False,
+                      fine_tune_batch_norm=False):
+  """Adds the decoder to obtain sharper segmentation results.
+
+  Args:
+    features: A tensor of size [batch, features_height, features_width,
+      features_channels].
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+    decoder_height: The height of decoder feature maps.
+    decoder_width: The width of decoder feature maps.
+    decoder_use_separable_conv: Employ separable convolution for decoder or not.
+    model_variant: Model variant for feature extraction.
+    weight_decay: The weight decay for model variables.
+    reuse: Reuse the model variables or not.
+    is_training: Is training or not.
+    fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+
+  Returns:
+    Decoder output with size [batch, decoder_height, decoder_width,
+      decoder_channels].
+  """
+  batch_norm_params = {
+      'is_training': is_training and fine_tune_batch_norm,
+      'decay': 0.9997,
+      'epsilon': 1e-5,
+      'scale': True,
+  }
+
+  with slim.arg_scope(
+      [slim.conv2d, slim.separable_conv2d],
+      weights_regularizer=slim.l2_regularizer(weight_decay),
+      activation_fn=tf.nn.relu,
+      normalizer_fn=slim.batch_norm,
+      padding='SAME',
+      stride=1,
+      reuse=reuse):
+    with slim.arg_scope([slim.batch_norm], **batch_norm_params):
+      with tf.variable_scope(_DECODER_SCOPE, _DECODER_SCOPE, [features]):
+        feature_list = feature_extractor.networks_to_feature_maps[
+            model_variant][feature_extractor.DECODER_END_POINTS]
+        if feature_list is None:
+          tf.logging.info('Not found any decoder end points.')
+          return features
+        else:
+          decoder_features = features
+          for i, name in enumerate(feature_list):
+            decoder_features_list = [decoder_features]
+            feature_name = '{}/{}'.format(
+                feature_extractor.name_scope[model_variant], name)
+            decoder_features_list.append(
+                slim.conv2d(
+                    end_points[feature_name],
+                    48,
+                    1,
+                    scope='feature_projection' + str(i)))
+            # Resize to decoder_height/decoder_width.
+            for j, feature in enumerate(decoder_features_list):
+              decoder_features_list[j] = tf.image.resize_bilinear(
+                  feature, [decoder_height, decoder_width], align_corners=True)
+              decoder_features_list[j].set_shape(
+                  [None, decoder_height, decoder_width, None])
+            decoder_depth = 256
+            if decoder_use_separable_conv:
+              decoder_features = _split_separable_conv2d(
+                  tf.concat(decoder_features_list, 3),
+                  filters=decoder_depth,
+                  rate=1,
+                  weight_decay=weight_decay,
+                  scope='decoder_conv0')
+              decoder_features = _split_separable_conv2d(
+                  decoder_features,
+                  filters=decoder_depth,
+                  rate=1,
+                  weight_decay=weight_decay,
+                  scope='decoder_conv1')
+            else:
+              num_convs = 2
+              decoder_features = slim.repeat(
+                  tf.concat(decoder_features_list, 3),
+                  num_convs,
+                  slim.conv2d,
+                  decoder_depth,
+                  3,
+                  scope='decoder_conv' + str(i))
+          return decoder_features
+
+
+def _get_branch_logits(features,
+                       num_classes,
+                       atrous_rates=None,
+                       aspp_with_batch_norm=False,
+                       kernel_size=1,
+                       weight_decay=0.0001,
+                       reuse=None,
+                       scope_suffix=''):
+  """Gets the logits from each model's branch.
+
+  The underlying model is branched out in the last layer when atrous
+  spatial pyramid pooling is employed, and all branches are sum-merged
+  to form the final logits.
+
+  Args:
+    features: A float tensor of shape [batch, height, width, channels].
+    num_classes: Number of classes to predict.
+    atrous_rates: A list of atrous convolution rates for last layer.
+    aspp_with_batch_norm: Use batch normalization layers for ASPP.
+    kernel_size: Kernel size for convolution.
+    weight_decay: Weight decay for the model variables.
+    reuse: Reuse model variables or not.
+    scope_suffix: Scope suffix for the model variables.
+
+  Returns:
+    Merged logits with shape [batch, height, width, num_classes].
+
+  Raises:
+    ValueError: Upon invalid input kernel_size value.
+  """
+  # When using batch normalization with ASPP, ASPP has been applied before
+  # in _extract_features, and thus we simply apply 1x1 convolution here.
+  if aspp_with_batch_norm or atrous_rates is None:
+    if kernel_size != 1:
+      raise ValueError('Kernel size must be 1 when atrous_rates is None or '
+                       'using aspp_with_batch_norm. Gets %d.' % kernel_size)
+    atrous_rates = [1]
+
+  with slim.arg_scope(
+      [slim.conv2d],
+      weights_regularizer=slim.l2_regularizer(weight_decay),
+      weights_initializer=tf.truncated_normal_initializer(stddev=0.01),
+      reuse=reuse):
+    with tf.variable_scope(_LOGITS_SCOPE_NAME, _LOGITS_SCOPE_NAME, [features]):
+      branch_logits = []
+      for i, rate in enumerate(atrous_rates):
+        scope = scope_suffix
+        if i:
+          scope += '_%d' % i
+
+        branch_logits.append(
+            slim.conv2d(
+                features,
+                num_classes,
+                kernel_size=kernel_size,
+                rate=rate,
+                activation_fn=None,
+                normalizer_fn=None,
+                scope=scope))
+
+      return tf.add_n(branch_logits)
+
+
+def _split_separable_conv2d(inputs,
+                            filters,
+                            rate=1,
+                            weight_decay=0.00004,
+                            depthwise_weights_initializer_stddev=0.33,
+                            pointwise_weights_initializer_stddev=0.06,
+                            scope=None):
+  """Splits a separable conv2d into depthwise and pointwise conv2d.
+
+  This operation differs from `tf.layers.separable_conv2d` as this operation
+  applies activation function between depthwise and pointwise conv2d.
+
+  Args:
+    inputs: Input tensor with shape [batch, height, width, channels].
+    filters: Number of filters in the 1x1 pointwise convolution.
+    rate: Atrous convolution rate for the depthwise convolution.
+    weight_decay: The weight decay to use for regularizing the model.
+    depthwise_weights_initializer_stddev: The standard deviation of the
+      truncated normal weight initializer for depthwise convolution.
+    pointwise_weights_initializer_stddev: The standard deviation of the
+      truncated normal weight initializer for pointwise convolution.
+    scope: Optional scope for the operation.
+
+  Returns:
+    Computed features after split separable conv2d.
+  """
+  outputs = slim.separable_conv2d(
+      inputs,
+      None,
+      3,
+      depth_multiplier=1,
+      rate=rate,
+      weights_initializer=tf.truncated_normal_initializer(
+          stddev=depthwise_weights_initializer_stddev),
+      weights_regularizer=None,
+      scope=scope + '_depthwise')
+  return slim.conv2d(
+      outputs,
+      filters,
+      1,
+      weights_initializer=tf.truncated_normal_initializer(
+          stddev=pointwise_weights_initializer_stddev),
+      weights_regularizer=slim.l2_regularizer(weight_decay),
+      scope=scope + '_pointwise')
diff --git a/research/deeplab/model_test.py b/research/deeplab/model_test.py
new file mode 100644
index 00000000..aacd1d3b
--- /dev/null
+++ b/research/deeplab/model_test.py
@@ -0,0 +1,124 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for DeepLab model and some helper functions."""
+
+import tensorflow as tf
+
+from deeplab import common
+from deeplab import model
+
+
+class DeeplabModelTest(tf.test.TestCase):
+
+  def testScaleDimensionOutput(self):
+    self.assertEqual(161, model.scale_dimension(321, 0.5))
+    self.assertEqual(193, model.scale_dimension(321, 0.6))
+    self.assertEqual(241, model.scale_dimension(321, 0.75))
+
+  def testWrongDeepLabVariant(self):
+    model_options = common.ModelOptions([])._replace(
+        model_variant='no_such_variant')
+    with self.assertRaises(ValueError):
+      model._get_logits(images=[], model_options=model_options)
+
+  def testBuildDeepLabv2(self):
+    batch_size = 2
+    crop_size = [41, 41]
+
+    # Test with two image_pyramids.
+    image_pyramids = [[1], [0.5, 1]]
+
+    # Test two model variants.
+    model_variants = ['xception_65']
+
+    # Test with two output_types.
+    outputs_to_num_classes = {'semantic': 3,
+                              'direction': 2}
+
+    expected_endpoints = [['merged_logits'],
+                          ['merged_logits',
+                           'logits_0.50',
+                           'logits_1.00']]
+    expected_num_logits = [1, 3]
+
+    for model_variant in model_variants:
+      model_options = common.ModelOptions(outputs_to_num_classes)._replace(
+          add_image_level_feature=False,
+          aspp_with_batch_norm=False,
+          aspp_with_separable_conv=False,
+          model_variant=model_variant)
+
+      for i, image_pyramid in enumerate(image_pyramids):
+        g = tf.Graph()
+        with g.as_default():
+          with self.test_session(graph=g):
+            inputs = tf.random_uniform(
+                (batch_size, crop_size[0], crop_size[1], 3))
+            outputs_to_scales_to_logits = model.multi_scale_logits(
+                inputs, model_options, image_pyramid=image_pyramid)
+
+            # Check computed results for each output type.
+            for output in outputs_to_num_classes:
+              scales_to_logits = outputs_to_scales_to_logits[output]
+              self.assertListEqual(sorted(scales_to_logits.keys()),
+                                   sorted(expected_endpoints[i]))
+
+              # Expected number of logits = len(image_pyramid) + 1, since the
+              # last logits is merged from all the scales.
+              self.assertEquals(len(scales_to_logits), expected_num_logits[i])
+
+  def testForwardpassDeepLabv3plus(self):
+    crop_size = [33, 33]
+    outputs_to_num_classes = {'semantic': 3}
+
+    model_options = common.ModelOptions(
+        outputs_to_num_classes,
+        crop_size,
+        atrous_rates=[6],
+        output_stride=16
+    )._replace(
+        add_image_level_feature=True,
+        aspp_with_batch_norm=True,
+        aspp_with_separable_conv=True,
+        decoder_output_stride=4,
+        decoder_use_separable_conv=True,
+        logits_kernel_size=1,
+        model_variant='xception_65')
+
+    g = tf.Graph()
+    with g.as_default():
+      with self.test_session(graph=g) as sess:
+        inputs = tf.random_uniform(
+            (1, crop_size[0], crop_size[1], 3))
+        outputs_to_scales_to_logits = model.multi_scale_logits(
+            inputs,
+            model_options,
+            image_pyramid=[1.0])
+
+        sess.run(tf.global_variables_initializer())
+        outputs_to_scales_to_logits = sess.run(outputs_to_scales_to_logits)
+
+        # Check computed results for each output type.
+        for output in outputs_to_num_classes:
+          scales_to_logits = outputs_to_scales_to_logits[output]
+          # Expect only one output.
+          self.assertEquals(len(scales_to_logits), 1)
+          for logits in scales_to_logits.values():
+            self.assertTrue(logits.any())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/train.py b/research/deeplab/train.py
new file mode 100644
index 00000000..7461abf2
--- /dev/null
+++ b/research/deeplab/train.py
@@ -0,0 +1,347 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Training script for the DeepLab model.
+
+See model.py for more details and usage.
+"""
+
+import tensorflow as tf
+from deeplab import common
+from deeplab import model
+from deeplab.datasets import segmentation_dataset
+from deeplab.utils import input_generator
+from deeplab.utils import train_utils
+from deployment import model_deploy
+
+slim = tf.contrib.slim
+
+prefetch_queue = slim.prefetch_queue
+
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+
+# Settings for multi-GPUs/multi-replicas training.
+
+flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy.')
+
+flags.DEFINE_boolean('clone_on_cpu', False, 'Use CPUs to deploy clones.')
+
+flags.DEFINE_integer('num_replicas', 1, 'Number of worker replicas.')
+
+flags.DEFINE_integer('startup_delay_steps', 15,
+                     'Number of training steps between replicas startup.')
+
+flags.DEFINE_integer('num_ps_tasks', 0,
+                     'The number of parameter servers. If the value is 0, then '
+                     'the parameters are handled locally by the worker.')
+
+flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')
+
+flags.DEFINE_integer('task', 0, 'The task ID.')
+
+# Settings for logging.
+
+flags.DEFINE_string('train_logdir', None,
+                    'Where the checkpoint and logs are stored.')
+
+flags.DEFINE_integer('log_steps', 10,
+                     'Display logging information at every log_steps.')
+
+flags.DEFINE_integer('save_interval_secs', 1200,
+                     'How often, in seconds, we save the model to disk.')
+
+flags.DEFINE_integer('save_summaries_secs', 600,
+                     'How often, in seconds, we compute the summaries.')
+
+# Settings for training strategry.
+
+flags.DEFINE_enum('learning_policy', 'poly', ['poly', 'step'],
+                  'Learning rate policy for training.')
+
+# Use 0.007 when training on PASCAL augmented training set, train_aug. When
+# fine-tuning on PASCAL trainval set, use learning rate=0.0001.
+flags.DEFINE_float('base_learning_rate', .0001,
+                   'The base learning rate for model training.')
+
+flags.DEFINE_float('learning_rate_decay_factor', 0.1,
+                   'The rate to decay the base learning rate.')
+
+flags.DEFINE_integer('learning_rate_decay_step', 2000,
+                     'Decay the base learning rate at a fixed step.')
+
+flags.DEFINE_float('learning_power', 0.9,
+                   'The power value used in the poly learning policy.')
+
+flags.DEFINE_integer('training_number_of_steps', 30000,
+                     'The number of steps used for training')
+
+flags.DEFINE_float('momentum', 0.9, 'The momentum value to use')
+
+# When fine_tune_batch_norm=True, use at least batch size larger than 12
+# (batch size more than 16 is better). Otherwise, one could use smaller batch
+# size and set fine_tune_batch_norm=False.
+flags.DEFINE_integer('train_batch_size', 8,
+                     'The number of images in each batch during training.')
+
+flags.DEFINE_float('weight_decay', 0.00004,
+                   'The value of the weight decay for training.')
+
+flags.DEFINE_multi_integer('train_crop_size', [513, 513],
+                           'Image crop size [height, width] during training.')
+
+flags.DEFINE_float('last_layer_gradient_multiplier', 1.0,
+                   'The gradient multiplier for last layers, which is used to '
+                   'boost the gradient of last layers if the value > 1.')
+
+flags.DEFINE_boolean('upsample_logits', True,
+                     'Upsample logits during training.')
+
+# Settings for fine-tuning the network.
+
+flags.DEFINE_string('tf_initial_checkpoint', None,
+                    'The initial checkpoint in tensorflow format.')
+
+# Set to False if one does not want to re-use the trained classifier weights.
+flags.DEFINE_boolean('initialize_last_layer', True,
+                     'Initialize the last layer.')
+
+flags.DEFINE_integer('slow_start_step', 0,
+                     'Training model with small learning rate for few steps.')
+
+flags.DEFINE_float('slow_start_learning_rate', 1e-4,
+                   'Learning rate employed during slow start.')
+
+# Set to True if one wants to fine-tune the batch norm parameters in DeepLabv3.
+# Set to False and use small batch size to save GPU memory.
+flags.DEFINE_boolean('fine_tune_batch_norm', True,
+                     'Fine tune the batch norm parameters or not.')
+
+flags.DEFINE_float('min_scale_factor', 0.5,
+                   'Mininum scale factor for data augmentation.')
+
+flags.DEFINE_float('max_scale_factor', 2.,
+                   'Maximum scale factor for data augmentation.')
+
+flags.DEFINE_float('scale_factor_step_size', 0.25,
+                   'Scale factor step size for data augmentation.')
+
+# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or
+# rates = [6, 12, 18] if output_stride = 16. Note one could use different
+# atrous_rates/output_stride during training/evaluation.
+flags.DEFINE_multi_integer('atrous_rates', None,
+                           'Atrous rates for atrous spatial pyramid pooling.')
+
+flags.DEFINE_integer('output_stride', 16,
+                     'The ratio of input to output spatial resolution.')
+
+# Dataset settings.
+flags.DEFINE_string('dataset', 'pascal_voc_seg',
+                    'Name of the segmentation dataset.')
+
+flags.DEFINE_string('train_split', 'train',
+                    'Which split of the dataset to be used for training')
+
+flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')
+
+
+def _build_deeplab(inputs_queue, outputs_to_num_classes, ignore_label):
+  """Builds a clone of DeepLab.
+
+  Args:
+    inputs_queue: A prefetch queue for images and labels.
+    outputs_to_num_classes: A map from output type to the number of classes.
+      For example, for the task of semantic segmentation with 21 semantic
+      classes, we would have outputs_to_num_classes['semantic'] = 21.
+    ignore_label: Ignore label.
+
+  Returns:
+    A map of maps from output_type (e.g., semantic prediction) to a
+      dictionary of multi-scale logits names to logits. For each output_type,
+      the dictionary has keys which correspond to the scales and values which
+      correspond to the logits. For example, if `scales` equals [1.0, 1.5],
+      then the keys would include 'merged_logits', 'logits_1.00' and
+      'logits_1.50'.
+  """
+  samples = inputs_queue.dequeue()
+
+  model_options = common.ModelOptions(
+      outputs_to_num_classes=outputs_to_num_classes,
+      crop_size=FLAGS.train_crop_size,
+      atrous_rates=FLAGS.atrous_rates,
+      output_stride=FLAGS.output_stride)
+  outputs_to_scales_to_logits = model.multi_scale_logits(
+      samples[common.IMAGE],
+      model_options=model_options,
+      image_pyramid=FLAGS.image_pyramid,
+      weight_decay=FLAGS.weight_decay,
+      is_training=True,
+      fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)
+
+  for output, num_classes in outputs_to_num_classes.iteritems():
+    train_utils.add_softmax_cross_entropy_loss_for_each_scale(
+        outputs_to_scales_to_logits[output],
+        samples[common.LABEL],
+        num_classes,
+        ignore_label,
+        loss_weight=1.0,
+        upsample_logits=FLAGS.upsample_logits,
+        scope=output)
+
+  return outputs_to_scales_to_logits
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  # Set up deployment (i.e., multi-GPUs and/or multi-replicas).
+  config = model_deploy.DeploymentConfig(
+      num_clones=FLAGS.num_clones,
+      clone_on_cpu=FLAGS.clone_on_cpu,
+      replica_id=FLAGS.task,
+      num_replicas=FLAGS.num_replicas,
+      num_ps_tasks=FLAGS.num_ps_tasks)
+
+  # Split the batch across GPUs.
+  assert FLAGS.train_batch_size % config.num_clones == 0, (
+      'Training batch size not divisble by number of clones (GPUs).')
+
+  clone_batch_size = FLAGS.train_batch_size / config.num_clones
+
+  # Get dataset-dependent information.
+  dataset = segmentation_dataset.get_dataset(
+      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)
+
+  tf.gfile.MakeDirs(FLAGS.train_logdir)
+  tf.logging.info('Training on %s set', FLAGS.train_split)
+
+  with tf.Graph().as_default():
+    with tf.device(config.inputs_device()):
+      samples = input_generator.get(
+          dataset,
+          FLAGS.train_crop_size,
+          clone_batch_size,
+          min_resize_value=FLAGS.min_resize_value,
+          max_resize_value=FLAGS.max_resize_value,
+          resize_factor=FLAGS.resize_factor,
+          min_scale_factor=FLAGS.min_scale_factor,
+          max_scale_factor=FLAGS.max_scale_factor,
+          scale_factor_step_size=FLAGS.scale_factor_step_size,
+          dataset_split=FLAGS.train_split,
+          is_training=True,
+          model_variant=FLAGS.model_variant)
+      inputs_queue = prefetch_queue.prefetch_queue(
+          samples, capacity=128 * config.num_clones)
+
+    # Create the global step on the device storing the variables.
+    with tf.device(config.variables_device()):
+      global_step = tf.train.get_or_create_global_step()
+
+      # Define the model and create clones.
+      model_fn = _build_deeplab
+      model_args = (inputs_queue, {
+          common.OUTPUT_TYPE: dataset.num_classes
+      }, dataset.ignore_label)
+      clones = model_deploy.create_clones(config, model_fn, args=model_args)
+
+      # Gather update_ops from the first clone. These contain, for example,
+      # the updates for the batch_norm variables created by model_fn.
+      first_clone_scope = config.clone_scope(0)
+      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
+
+    # Gather initial summaries.
+    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
+
+    # Add summaries for model variables.
+    for model_var in slim.get_model_variables():
+      summaries.add(tf.summary.histogram(model_var.op.name, model_var))
+
+    # Add summaries for losses.
+    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):
+      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))
+
+    # Build the optimizer based on the device specification.
+    with tf.device(config.optimizer_device()):
+      learning_rate = train_utils.get_model_learning_rate(
+          FLAGS.learning_policy, FLAGS.base_learning_rate,
+          FLAGS.learning_rate_decay_step, FLAGS.learning_rate_decay_factor,
+          FLAGS.training_number_of_steps, FLAGS.learning_power,
+          FLAGS.slow_start_step, FLAGS.slow_start_learning_rate)
+      optimizer = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
+
+    startup_delay_steps = FLAGS.task * FLAGS.startup_delay_steps
+    for variable in slim.get_model_variables():
+      summaries.add(tf.summary.histogram(variable.op.name, variable))
+
+    with tf.device(config.variables_device()):
+      total_loss, grads_and_vars = model_deploy.optimize_clones(
+          clones, optimizer)
+      total_loss = tf.check_numerics(total_loss, 'Loss is inf or nan.')
+      summaries.add(tf.summary.scalar('total_loss', total_loss))
+
+      # Modify the gradients for biases and last layer variables.
+      last_layers = model.get_extra_layer_scopes()
+      grad_mult = train_utils.get_model_gradient_multipliers(
+          last_layers, FLAGS.last_layer_gradient_multiplier)
+      if grad_mult:
+        grads_and_vars = slim.learning.multiply_gradients(
+            grads_and_vars, grad_mult)
+
+      # Create gradient update op.
+      grad_updates = optimizer.apply_gradients(
+          grads_and_vars, global_step=global_step)
+      update_ops.append(grad_updates)
+      update_op = tf.group(*update_ops)
+      with tf.control_dependencies([update_op]):
+        train_tensor = tf.identity(total_loss, name='train_op')
+
+    # Add the summaries from the first clone. These contain the summaries
+    # created by model_fn and either optimize_clones() or _gather_clone_loss().
+    summaries |= set(
+        tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone_scope))
+
+    # Merge all summaries together.
+    summary_op = tf.summary.merge(list(summaries))
+
+    # Soft placement allows placing on CPU ops without GPU implementation.
+    session_config = tf.ConfigProto(
+        allow_soft_placement=True, log_device_placement=False)
+
+    # Start the training.
+    slim.learning.train(
+        train_tensor,
+        logdir=FLAGS.train_logdir,
+        log_every_n_steps=FLAGS.log_steps,
+        master=FLAGS.master,
+        number_of_steps=FLAGS.training_number_of_steps,
+        is_chief=(FLAGS.task == 0),
+        session_config=session_config,
+        startup_delay_steps=startup_delay_steps,
+        init_fn=train_utils.get_model_init_fn(
+            FLAGS.train_logdir,
+            FLAGS.tf_initial_checkpoint,
+            FLAGS.initialize_last_layer,
+            last_layers,
+            ignore_missing_vars=True),
+        summary_op=summary_op,
+        save_summaries_secs=FLAGS.save_summaries_secs,
+        save_interval_secs=FLAGS.save_interval_secs)
+
+
+if __name__ == '__main__':
+  flags.mark_flag_as_required('train_logdir')
+  flags.mark_flag_as_required('tf_initial_checkpoint')
+  flags.mark_flag_as_required('dataset_dir')
+  tf.app.run()
diff --git a/research/deeplab/utils/__init__.py b/research/deeplab/utils/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/deeplab/utils/get_dataset_colormap.py b/research/deeplab/utils/get_dataset_colormap.py
new file mode 100644
index 00000000..297873fc
--- /dev/null
+++ b/research/deeplab/utils/get_dataset_colormap.py
@@ -0,0 +1,148 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Visualizes the segmentation results via specified color map.
+
+Visualizes the semantic segmentation results by the color map
+defined by the different datasets. Supported colormaps are:
+
+1. PASCAL VOC semantic segmentation benchmark.
+Website: http://host.robots.ox.ac.uk/pascal/VOC/
+"""
+
+import numpy as np
+
+# Dataset names.
+_CITYSCAPES = 'cityscapes'
+_PASCAL = 'pascal'
+
+# Max number of entries in the colormap for each dataset.
+_DATASET_MAX_ENTRIES = {
+    _CITYSCAPES: 19,
+    _PASCAL: 256,
+}
+
+
+def create_cityscapes_label_colormap():
+  """Creates a label colormap used in CITYSCAPES segmentation benchmark.
+
+  Returns:
+    A Colormap for visualizing segmentation results.
+  """
+  colormap = np.asarray([
+      [128, 64, 128],
+      [244, 35, 232],
+      [70, 70, 70],
+      [102, 102, 156],
+      [190, 153, 153],
+      [153, 153, 153],
+      [250, 170, 30],
+      [220, 220, 0],
+      [107, 142, 35],
+      [152, 251, 152],
+      [70, 130, 180],
+      [220, 20, 60],
+      [255, 0, 0],
+      [0, 0, 142],
+      [0, 0, 70],
+      [0, 60, 100],
+      [0, 80, 100],
+      [0, 0, 230],
+      [119, 11, 32],
+  ])
+  return colormap
+
+
+def get_pascal_name():
+  return _PASCAL
+
+
+def get_cityscapes_name():
+  return _CITYSCAPES
+
+
+def bit_get(val, idx):
+  """Gets the bit value.
+
+  Args:
+    val: Input value, int or numpy int array.
+    idx: Which bit of the input val.
+
+  Returns:
+    The "idx"-th bit of input val.
+  """
+  return (val >> idx) & 1
+
+
+def create_pascal_label_colormap():
+  """Creates a label colormap used in PASCAL VOC segmentation benchmark.
+
+  Returns:
+    A Colormap for visualizing segmentation results.
+  """
+  colormap = np.zeros((_DATASET_MAX_ENTRIES[_PASCAL], 3), dtype=int)
+  ind = np.arange(_DATASET_MAX_ENTRIES[_PASCAL], dtype=int)
+
+  for shift in reversed(range(8)):
+    for channel in range(3):
+      colormap[:, channel] |= bit_get(ind, channel) << shift
+    ind >>= 3
+
+  return colormap
+
+
+def create_label_colormap(dataset=_PASCAL):
+  """Creates a label colormap for the specified dataset.
+
+  Args:
+    dataset: The colormap used in the dataset.
+
+  Returns:
+    A numpy array of the dataset colormap.
+
+  Raises:
+    ValueError: If the dataset is not supported.
+  """
+  if dataset == _PASCAL:
+    return create_pascal_label_colormap()
+  elif dataset == _CITYSCAPES:
+    return create_cityscapes_label_colormap()
+  else:
+    raise ValueError('Unsupported dataset.')
+
+
+def label_to_color_image(label, dataset=_PASCAL):
+  """Adds color defined by the dataset colormap to the label.
+
+  Args:
+    label: A 2D array with integer type, storing the segmentation label.
+    dataset: The colormap used in the dataset.
+
+  Returns:
+    result: A 2D array with floating type. The element of the array
+      is the color indexed by the corresponding element in the input label
+      to the PASCAL color map.
+
+  Raises:
+    ValueError: If label is not of rank 2 or its value is larger than color
+      map maximum entry.
+  """
+  if label.ndim != 2:
+    raise ValueError('Expect 2-D input label')
+
+  if np.max(label) >= _DATASET_MAX_ENTRIES[dataset]:
+    raise ValueError('label value too large.')
+
+  colormap = create_label_colormap(dataset)
+  return colormap[label]
diff --git a/research/deeplab/utils/get_dataset_colormap_test.py b/research/deeplab/utils/get_dataset_colormap_test.py
new file mode 100644
index 00000000..ec02d8e9
--- /dev/null
+++ b/research/deeplab/utils/get_dataset_colormap_test.py
@@ -0,0 +1,75 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for get_dataset_colormap.py."""
+
+import numpy as np
+import tensorflow as tf
+
+from deeplab.utils import get_dataset_colormap
+
+
+class VisualizationUtilTest(tf.test.TestCase):
+
+  def testBitGet(self):
+    """Test that if the returned bit value is correct."""
+    self.assertEqual(1, get_dataset_colormap.bit_get(9, 0))
+    self.assertEqual(0, get_dataset_colormap.bit_get(9, 1))
+    self.assertEqual(0, get_dataset_colormap.bit_get(9, 2))
+    self.assertEqual(1, get_dataset_colormap.bit_get(9, 3))
+
+  def testPASCALLabelColorMapValue(self):
+    """Test the getd color map value."""
+    colormap = get_dataset_colormap.create_pascal_label_colormap()
+
+    # Only test a few sampled entries in the color map.
+    self.assertTrue(np.array_equal([128., 0., 128.], colormap[5, :]))
+    self.assertTrue(np.array_equal([128., 192., 128.], colormap[23, :]))
+    self.assertTrue(np.array_equal([128., 0., 192.], colormap[37, :]))
+    self.assertTrue(np.array_equal([224., 192., 192.], colormap[127, :]))
+    self.assertTrue(np.array_equal([192., 160., 192.], colormap[175, :]))
+
+  def testLabelToPASCALColorImage(self):
+    """Test the value of the converted label value."""
+    label = np.array([[0, 16, 16], [52, 7, 52]])
+    expected_result = np.array([
+        [[0, 0, 0], [0, 64, 0], [0, 64, 0]],
+        [[0, 64, 192], [128, 128, 128], [0, 64, 192]]
+    ])
+    colored_label = get_dataset_colormap.label_to_color_image(
+        label, get_dataset_colormap.get_pascal_name())
+    self.assertTrue(np.array_equal(expected_result, colored_label))
+
+  def testUnExpectedLabelValueForLabelToPASCALColorImage(self):
+    """Raise ValueError when input value exceeds range."""
+    label = np.array([[120], [300]])
+    with self.assertRaises(ValueError):
+      get_dataset_colormap.label_to_color_image(
+          label, get_dataset_colormap.get_pascal_name())
+
+  def testUnExpectedLabelDimensionForLabelToPASCALColorImage(self):
+    """Raise ValueError if input dimension is not correct."""
+    label = np.array([120])
+    with self.assertRaises(ValueError):
+      get_dataset_colormap.label_to_color_image(
+          label, get_dataset_colormap.get_pascal_name())
+
+  def testGetColormapForUnsupportedDataset(self):
+    with self.assertRaises(ValueError):
+      get_dataset_colormap.create_label_colormap('unsupported_dataset')
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/utils/input_generator.py b/research/deeplab/utils/input_generator.py
new file mode 100644
index 00000000..4d4d09a5
--- /dev/null
+++ b/research/deeplab/utils/input_generator.py
@@ -0,0 +1,168 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Wrapper for providing semantic segmentation data."""
+
+import tensorflow as tf
+from deeplab import common
+from deeplab import input_preprocess
+
+slim = tf.contrib.slim
+
+dataset_data_provider = slim.dataset_data_provider
+
+
+def _get_data(data_provider, dataset_split):
+  """Gets data from data provider.
+
+  Args:
+    data_provider: An object of slim.data_provider.
+    dataset_split: Dataset split.
+
+  Returns:
+    image: Image Tensor.
+    label: Label Tensor storing segmentation annotations.
+    image_name: Image name.
+    height: Image height.
+    width: Image width.
+
+  Raises:
+    ValueError: Failed to find label.
+  """
+  if common.LABELS_CLASS not in data_provider.list_items():
+    raise ValueError('Failed to find labels.')
+
+  image, height, width = data_provider.get(
+      [common.IMAGE, common.HEIGHT, common.WIDTH])
+
+  # Some datasets do not contain image_name.
+  if common.IMAGE_NAME in data_provider.list_items():
+    image_name, = data_provider.get([common.IMAGE_NAME])
+  else:
+    image_name = tf.constant('')
+
+  label = None
+  if dataset_split != common.TEST_SET:
+    label, = data_provider.get([common.LABELS_CLASS])
+
+  return image, label, image_name, height, width
+
+
+def get(dataset,
+        crop_size,
+        batch_size,
+        min_resize_value=None,
+        max_resize_value=None,
+        resize_factor=None,
+        min_scale_factor=1.,
+        max_scale_factor=1.,
+        scale_factor_step_size=0,
+        num_readers=1,
+        num_threads=1,
+        dataset_split=None,
+        is_training=True,
+        model_variant=None):
+  """Gets the dataset split for semantic segmentation.
+
+  This functions gets the dataset split for semantic segmentation. In
+  particular, it is a wrapper of (1) dataset_data_provider which returns the raw
+  dataset split, (2) input_preprcess which preprocess the raw data, and (3) the
+  Tensorflow operation of batching the preprocessed data. Then, the output could
+  be directly used by training, evaluation or visualization.
+
+  Args:
+    dataset: An instance of slim Dataset.
+    crop_size: Image crop size [height, width].
+    batch_size: Batch size.
+    min_resize_value: Desired size of the smaller image side.
+    max_resize_value: Maximum allowed size of the larger image side.
+    resize_factor: Resized dimensions are multiple of factor plus one.
+    min_scale_factor: Minimum scale factor value.
+    max_scale_factor: Maximum scale factor value.
+    scale_factor_step_size: The step size from min scale factor to max scale
+      factor. The input is randomly scaled based on the value of
+      (min_scale_factor, max_scale_factor, scale_factor_step_size).
+    num_readers: Number of readers for data provider.
+    num_threads: Number of threads for batching data.
+    dataset_split: Dataset split.
+    is_training: Is training or not.
+    model_variant: Model variant (string) for choosing how to mean-subtract the
+      images. See feature_extractor.network_map for supported model variants.
+
+  Returns:
+    A dictionary of batched Tensors for semantic segmentation.
+
+  Raises:
+    ValueError: dataset_split is None, failed to find labels, or label shape
+      is not valid.
+  """
+  if dataset_split is None:
+    raise ValueError('Unknown dataset split.')
+  if model_variant is None:
+    tf.logging.warning('Please specify a model_variant. See '
+                       'feature_extractor.network_map for supported model '
+                       'variants.')
+
+  data_provider = dataset_data_provider.DatasetDataProvider(
+      dataset,
+      num_readers=num_readers,
+      num_epochs=None if is_training else 1,
+      shuffle=is_training)
+  image, label, image_name, height, width = _get_data(data_provider,
+                                                      dataset_split)
+  if label is not None:
+    if label.shape.ndims == 2:
+      label = tf.expand_dims(label, 2)
+    elif label.shape.ndims == 3 and label.shape.dims[2] == 1:
+      pass
+    else:
+      raise ValueError('Input label shape must be [height, width], or '
+                       '[height, width, 1].')
+
+    label.set_shape([None, None, 1])
+  original_image, image, label = input_preprocess.preprocess_image_and_label(
+      image,
+      label,
+      crop_height=crop_size[0],
+      crop_width=crop_size[1],
+      min_resize_value=min_resize_value,
+      max_resize_value=max_resize_value,
+      resize_factor=resize_factor,
+      min_scale_factor=min_scale_factor,
+      max_scale_factor=max_scale_factor,
+      scale_factor_step_size=scale_factor_step_size,
+      ignore_label=dataset.ignore_label,
+      is_training=is_training,
+      model_variant=model_variant)
+  sample = {
+      common.IMAGE: image,
+      common.IMAGE_NAME: image_name,
+      common.HEIGHT: height,
+      common.WIDTH: width
+  }
+  if label is not None:
+    sample[common.LABEL] = label
+
+  if not is_training:
+    # Original image is only used during visualization.
+    sample[common.ORIGINAL_IMAGE] = original_image,
+    num_threads = 1
+
+  return tf.train.batch(
+      sample,
+      batch_size=batch_size,
+      num_threads=num_threads,
+      capacity=32 * batch_size,
+      allow_smaller_final_batch=not is_training,
+      dynamic_pad=True)
diff --git a/research/deeplab/utils/save_annotation.py b/research/deeplab/utils/save_annotation.py
new file mode 100644
index 00000000..9f3c7e78
--- /dev/null
+++ b/research/deeplab/utils/save_annotation.py
@@ -0,0 +1,52 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Saves an annotation as one png image.
+
+This script saves an annotation as one png image, and has the option to add
+colormap to the png image for better visualization.
+"""
+
+import numpy as np
+import PIL.Image as img
+import tensorflow as tf
+
+from deeplab.utils import get_dataset_colormap
+
+
+def save_annotation(label,
+                    save_dir,
+                    filename,
+                    add_colormap=True,
+                    colormap_type=get_dataset_colormap.get_pascal_name()):
+  """Saves the given label to image on disk.
+
+  Args:
+    label: The numpy array to be saved. The data will be converted
+      to uint8 and saved as png image.
+    save_dir: The directory to which the results will be saved.
+    filename: The image filename.
+    add_colormap: Add color map to the label or not.
+    colormap_type: Colormap type for visualization.
+  """
+  # Add colormap for visualizing the prediction.
+  if add_colormap:
+    colored_label = get_dataset_colormap.label_to_color_image(
+        label, colormap_type)
+  else:
+    colored_label = label
+
+  pil_image = img.fromarray(colored_label.astype(dtype=np.uint8))
+  with tf.gfile.Open('%s/%s.png' % (save_dir, filename), mode='w') as f:
+    pil_image.save(f, 'PNG')
diff --git a/research/deeplab/utils/train_utils.py b/research/deeplab/utils/train_utils.py
new file mode 100644
index 00000000..e2562bfb
--- /dev/null
+++ b/research/deeplab/utils/train_utils.py
@@ -0,0 +1,202 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Utility functions for training."""
+
+import tensorflow as tf
+
+slim = tf.contrib.slim
+
+
+def add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,
+                                                  labels,
+                                                  num_classes,
+                                                  ignore_label,
+                                                  loss_weight=1.0,
+                                                  upsample_logits=True,
+                                                  scope=None):
+  """Adds softmax cross entropy loss for logits of each scale.
+
+  Args:
+    scales_to_logits: A map from logits names for different scales to logits.
+      The logits have shape [batch, logits_height, logits_width, num_classes].
+    labels: Groundtruth labels with shape [batch, image_height, image_width, 1].
+    num_classes: Integer, number of target classes.
+    ignore_label: Integer, label to ignore.
+    loss_weight: Float, loss weight.
+    upsample_logits: Boolean, upsample logits or not.
+    scope: String, the scope for the loss.
+
+  Raises:
+    ValueError: Label or logits is None.
+  """
+  if labels is None:
+    raise ValueError('No label for softmax cross entropy loss.')
+
+  for scale, logits in scales_to_logits.iteritems():
+    loss_scope = None
+    if scope:
+      loss_scope = '%s_%s' % (scope, scale)
+
+    if upsample_logits:
+      # Label is not downsampled, and instead we upsample logits.
+      logits = tf.image.resize_bilinear(
+          logits, tf.shape(labels)[1:3], align_corners=True)
+      scaled_labels = labels
+    else:
+      # Label is downsampled to the same size as logits.
+      scaled_labels = tf.image.resize_nearest_neighbor(
+          labels, tf.shape(logits)[1:3], align_corners=True)
+
+    scaled_labels = tf.reshape(scaled_labels, shape=[-1])
+    not_ignore_mask = tf.to_float(tf.not_equal(scaled_labels,
+                                               ignore_label)) * loss_weight
+    one_hot_labels = slim.one_hot_encoding(
+        scaled_labels, num_classes, on_value=1.0, off_value=0.0)
+    tf.losses.softmax_cross_entropy(
+        one_hot_labels,
+        tf.reshape(logits, shape=[-1, num_classes]),
+        weights=not_ignore_mask,
+        scope=loss_scope)
+
+
+def get_model_init_fn(train_logdir,
+                      tf_initial_checkpoint,
+                      initialize_last_layer,
+                      last_layers,
+                      ignore_missing_vars=False):
+  """Gets the function initializing model variables from a checkpoint.
+
+  Args:
+    train_logdir: Log directory for training.
+    tf_initial_checkpoint: TensorFlow checkpoint for initialization.
+    initialize_last_layer: Initialize last layer or not.
+    last_layers: Last layers of the model.
+    ignore_missing_vars: Ignore missing variables in the checkpoint.
+
+  Returns:
+    Initialization function.
+  """
+  if tf_initial_checkpoint is None:
+    tf.logging.info('Not initializing the model from a checkpoint.')
+    return None
+
+  if tf.train.latest_checkpoint(train_logdir):
+    tf.logging.info('Ignoring initialization; other checkpoint exists')
+    return None
+
+  tf.logging.info('Initializing model from path: %s', tf_initial_checkpoint)
+
+  # Variables that will not be restored.
+  exclude_list = ['global_step']
+  if not initialize_last_layer:
+    exclude_list.extend(last_layers)
+
+  variables_to_restore = slim.get_variables_to_restore(exclude=exclude_list)
+
+  return slim.assign_from_checkpoint_fn(
+      tf_initial_checkpoint,
+      variables_to_restore,
+      ignore_missing_vars=ignore_missing_vars)
+
+
+def get_model_gradient_multipliers(last_layers, last_layer_gradient_multiplier):
+  """Gets the gradient multipliers.
+
+  The gradient multipliers will adjust the learning rates for model
+  variables. For the task of semantic segmentation, the models are
+  usually fine-tuned from the models trained on the task of image
+  classification. To fine-tune the models, we usually set larger (e.g.,
+  10 times larger) learning rate for the parameters of last layer.
+
+  Args:
+    last_layers: Scopes of last layers.
+    last_layer_gradient_multiplier: The gradient multiplier for last layers.
+
+  Returns:
+    The gradient multiplier map with variables as key, and multipliers as value.
+  """
+  gradient_multipliers = {}
+
+  for var in slim.get_model_variables():
+    # Double the learning rate for biases.
+    if 'biases' in var.op.name:
+      gradient_multipliers[var.op.name] = 2.
+
+    # Use larger learning rate for last layer variables.
+    for layer in last_layers:
+      if layer in var.op.name and 'biases' in var.op.name:
+        gradient_multipliers[var.op.name] = 2 * last_layer_gradient_multiplier
+        break
+      elif layer in var.op.name:
+        gradient_multipliers[var.op.name] = last_layer_gradient_multiplier
+        break
+
+  return gradient_multipliers
+
+
+def get_model_learning_rate(
+    learning_policy, base_learning_rate, learning_rate_decay_step,
+    learning_rate_decay_factor, training_number_of_steps, learning_power,
+    slow_start_step, slow_start_learning_rate):
+  """Gets model's learning rate.
+
+  Computes the model's learning rate for different learning policy.
+  Right now, only "step" and "poly" are supported.
+  (1) The learning policy for "step" is computed as follows:
+    current_learning_rate = base_learning_rate *
+      learning_rate_decay_factor ^ (global_step / learning_rate_decay_step)
+  See tf.train.exponential_decay for details.
+  (2) The learning policy for "poly" is computed as follows:
+    current_learning_rate = base_learning_rate *
+      (1 - global_step / training_number_of_steps) ^ learning_power
+
+  Args:
+    learning_policy: Learning rate policy for training.
+    base_learning_rate: The base learning rate for model training.
+    learning_rate_decay_step: Decay the base learning rate at a fixed step.
+    learning_rate_decay_factor: The rate to decay the base learning rate.
+    training_number_of_steps: Number of steps for training.
+    learning_power: Power used for 'poly' learning policy.
+    slow_start_step: Training model with small learning rate for the first
+      few steps.
+    slow_start_learning_rate: The learning rate employed during slow start.
+
+  Returns:
+    Learning rate for the specified learning policy.
+
+  Raises:
+    ValueError: If learning policy is not recognized.
+  """
+  global_step = tf.train.get_or_create_global_step()
+  if learning_policy == 'step':
+    learning_rate = tf.train.exponential_decay(
+        base_learning_rate,
+        global_step,
+        learning_rate_decay_step,
+        learning_rate_decay_factor,
+        staircase=True)
+  elif learning_policy == 'poly':
+    learning_rate = tf.train.polynomial_decay(
+        base_learning_rate,
+        global_step,
+        training_number_of_steps,
+        end_learning_rate=0,
+        power=learning_power)
+  else:
+    raise ValueError('Unknown learning policy.')
+
+  # Employ small learning rate at the first few steps for warm start.
+  return tf.where(global_step < slow_start_step, slow_start_learning_rate,
+                  learning_rate)
diff --git a/research/deeplab/vis.py b/research/deeplab/vis.py
new file mode 100644
index 00000000..2dc89c56
--- /dev/null
+++ b/research/deeplab/vis.py
@@ -0,0 +1,320 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Segmentation results visualization on a given set of images.
+
+See model.py for more details and usage.
+"""
+import math
+import os.path
+import time
+import numpy as np
+import tensorflow as tf
+from deeplab import common
+from deeplab import model
+from deeplab.datasets import segmentation_dataset
+from deeplab.utils import input_generator
+from deeplab.utils import save_annotation
+
+slim = tf.contrib.slim
+
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')
+
+# Settings for log directories.
+
+flags.DEFINE_string('vis_logdir', None, 'Where to write the event logs.')
+
+flags.DEFINE_string('checkpoint_dir', None, 'Directory of model checkpoints.')
+
+# Settings for visualizing the model.
+
+flags.DEFINE_integer('vis_batch_size', 1,
+                     'The number of images in each batch during evaluation.')
+
+flags.DEFINE_multi_integer('vis_crop_size', [513, 513],
+                           'Crop size [height, width] for visualization.')
+
+flags.DEFINE_integer('eval_interval_secs', 60 * 5,
+                     'How often (in seconds) to run evaluation.')
+
+# For `xception_65`, use atrous_rates = [12, 24, 36] if output_stride = 8, or
+# rates = [6, 12, 18] if output_stride = 16. Note one could use different
+# atrous_rates/output_stride during training/evaluation.
+flags.DEFINE_multi_integer('atrous_rates', None,
+                           'Atrous rates for atrous spatial pyramid pooling.')
+
+flags.DEFINE_integer('output_stride', 16,
+                     'The ratio of input to output spatial resolution.')
+
+# Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale test.
+flags.DEFINE_multi_float('eval_scales', [1.0],
+                         'The scales to resize images for evaluation.')
+
+# Change to True for adding flipped images during test.
+flags.DEFINE_bool('add_flipped_images', False,
+                  'Add flipped images for evaluation or not.')
+
+# Dataset settings.
+
+flags.DEFINE_string('dataset', 'pascal_voc_seg',
+                    'Name of the segmentation dataset.')
+
+flags.DEFINE_string('vis_split', 'val',
+                    'Which split of the dataset used for visualizing results')
+
+flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')
+
+flags.DEFINE_enum('colormap_type', 'pascal', ['pascal', 'cityscapes'],
+                  'Visualization colormap type.')
+
+flags.DEFINE_boolean('also_save_raw_predictions', False,
+                     'Also save raw predictions.')
+
+flags.DEFINE_integer('max_number_of_iterations', 0,
+                     'Maximum number of visualization iterations. Will loop '
+                     'indefinitely upon nonpositive values.')
+
+# The folder where semantic segmentation predictions are saved.
+_SEMANTIC_PREDICTION_SAVE_FOLDER = 'segmentation_results'
+
+# The folder where raw semantic segmentation predictions are saved.
+_RAW_SEMANTIC_PREDICTION_SAVE_FOLDER = 'raw_segmentation_results'
+
+# The format to save image.
+_IMAGE_FORMAT = '%06d_image'
+
+# The format to save prediction
+_PREDICTION_FORMAT = '%06d_prediction'
+
+# To evaluate Cityscapes results on the evaluation server, the labels used
+# during training should be mapped to the labels for evaluation.
+_CITYSCAPES_TRAIN_ID_TO_EVAL_ID = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,
+                                   23, 24, 25, 26, 27, 28, 31, 32, 33]
+
+
+def _convert_train_id_to_eval_id(prediction, train_id_to_eval_id):
+  """Converts the predicted label for evaluation.
+
+  There are cases where the training labels are not equal to the evaluation
+  labels. This function is used to perform the conversion so that we could
+  evaluate the results on the evaluation server.
+
+  Args:
+    prediction: Semantic segmentation prediction.
+    train_id_to_eval_id: A list mapping from train id to evaluation id.
+
+  Returns:
+    Semantic segmentation prediction whose labels have been changed.
+  """
+  converted_prediction = prediction.copy()
+  for train_id, eval_id in enumerate(train_id_to_eval_id):
+    converted_prediction[prediction == train_id] = eval_id
+
+  return converted_prediction
+
+
+def _process_batch(sess, original_images, semantic_predictions, image_names,
+                   image_heights, image_widths, image_id_offset, save_dir,
+                   raw_save_dir, train_id_to_eval_id=None):
+  """Evaluates one single batch qualitatively.
+
+  Args:
+    sess: TensorFlow session.
+    original_images: One batch of original images.
+    semantic_predictions: One batch of semantic segmentation predictions.
+    image_names: Image names.
+    image_heights: Image heights.
+    image_widths: Image widths.
+    image_id_offset: Image id offset for indexing images.
+    save_dir: The directory where the predictions will be saved.
+    raw_save_dir: The directory where the raw predictions will be saved.
+    train_id_to_eval_id: A list mapping from train id to eval id.
+  """
+  (original_images,
+   semantic_predictions,
+   image_names,
+   image_heights,
+   image_widths) = sess.run([original_images, semantic_predictions,
+                             image_names, image_heights, image_widths])
+
+  num_image = semantic_predictions.shape[0]
+  for i in range(num_image):
+    image_height = np.squeeze(image_heights[i])
+    image_width = np.squeeze(image_widths[i])
+    original_image = np.squeeze(original_images[i])
+    semantic_prediction = np.squeeze(semantic_predictions[i])
+    crop_semantic_prediction = semantic_prediction[:image_height, :image_width]
+
+    # Save image.
+    save_annotation.save_annotation(
+        original_image, save_dir, _IMAGE_FORMAT % (image_id_offset + i),
+        add_colormap=False)
+
+    # Save prediction.
+    save_annotation.save_annotation(
+        crop_semantic_prediction, save_dir,
+        _PREDICTION_FORMAT % (image_id_offset + i), add_colormap=True,
+        colormap_type=FLAGS.colormap_type)
+
+    if FLAGS.also_save_raw_predictions:
+      image_filename = image_names[i]
+
+      if train_id_to_eval_id is not None:
+        crop_semantic_prediction = _convert_train_id_to_eval_id(
+            crop_semantic_prediction,
+            train_id_to_eval_id)
+      save_annotation.save_annotation(
+          crop_semantic_prediction, raw_save_dir, image_filename,
+          add_colormap=False)
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  # Get dataset-dependent information.
+  dataset = segmentation_dataset.get_dataset(
+      FLAGS.dataset, FLAGS.vis_split, dataset_dir=FLAGS.dataset_dir)
+  train_id_to_eval_id = None
+  if dataset.name == segmentation_dataset.get_cityscapes_dataset_name():
+    tf.logging.info('Cityscapes requires converting train_id to eval_id.')
+    train_id_to_eval_id = _CITYSCAPES_TRAIN_ID_TO_EVAL_ID
+
+  # Prepare for visualization.
+  tf.gfile.MakeDirs(FLAGS.vis_logdir)
+  save_dir = os.path.join(FLAGS.vis_logdir, _SEMANTIC_PREDICTION_SAVE_FOLDER)
+  tf.gfile.MakeDirs(save_dir)
+  raw_save_dir = os.path.join(
+      FLAGS.vis_logdir, _RAW_SEMANTIC_PREDICTION_SAVE_FOLDER)
+  tf.gfile.MakeDirs(raw_save_dir)
+
+  tf.logging.info('Visualizing on %s set', FLAGS.vis_split)
+
+  g = tf.Graph()
+  with g.as_default():
+    samples = input_generator.get(dataset,
+                                  FLAGS.vis_crop_size,
+                                  FLAGS.vis_batch_size,
+                                  min_resize_value=FLAGS.min_resize_value,
+                                  max_resize_value=FLAGS.max_resize_value,
+                                  resize_factor=FLAGS.resize_factor,
+                                  dataset_split=FLAGS.vis_split,
+                                  is_training=False,
+                                  model_variant=FLAGS.model_variant)
+
+    model_options = common.ModelOptions(
+        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},
+        crop_size=FLAGS.vis_crop_size,
+        atrous_rates=FLAGS.atrous_rates,
+        output_stride=FLAGS.output_stride)
+
+    if tuple(FLAGS.eval_scales) == (1.0,):
+      tf.logging.info('Performing single-scale test.')
+      predictions = model.predict_labels(
+          samples[common.IMAGE],
+          model_options=model_options,
+          image_pyramid=FLAGS.image_pyramid)
+    else:
+      tf.logging.info('Performing multi-scale test.')
+      predictions = model.predict_labels_multi_scale(
+          samples[common.IMAGE],
+          model_options=model_options,
+          eval_scales=FLAGS.eval_scales,
+          add_flipped_images=FLAGS.add_flipped_images)
+    predictions = predictions[common.OUTPUT_TYPE]
+
+    if FLAGS.min_resize_value and FLAGS.max_resize_value:
+      # Only support batch_size = 1, since we assume the dimensions of original
+      # image after tf.squeeze is [height, width, 3].
+      assert FLAGS.vis_batch_size == 1
+
+      # Reverse the resizing and padding operations performed in preprocessing.
+      # First, we slice the valid regions (i.e., remove padded region) and then
+      # we reisze the predictions back.
+      original_image = tf.squeeze(samples[common.ORIGINAL_IMAGE])
+      original_image_shape = tf.shape(original_image)
+      predictions = tf.slice(
+          predictions,
+          [0, 0, 0],
+          [1, original_image_shape[0], original_image_shape[1]])
+      resized_shape = tf.to_int32([tf.squeeze(samples[common.HEIGHT]),
+                                   tf.squeeze(samples[common.WIDTH])])
+      predictions = tf.squeeze(
+          tf.image.resize_images(tf.expand_dims(predictions, 3),
+                                 resized_shape,
+                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+                                 align_corners=True), 3)
+
+    tf.train.get_or_create_global_step()
+    saver = tf.train.Saver(slim.get_variables_to_restore())
+    sv = tf.train.Supervisor(graph=g,
+                             logdir=FLAGS.vis_logdir,
+                             init_op=tf.global_variables_initializer(),
+                             summary_op=None,
+                             summary_writer=None,
+                             global_step=None,
+                             saver=saver)
+    num_batches = int(math.ceil(
+        dataset.num_samples / float(FLAGS.vis_batch_size)))
+    last_checkpoint = None
+
+    # Loop to visualize the results when new checkpoint is created.
+    num_iters = 0
+    while (FLAGS.max_number_of_iterations <= 0 or
+           num_iters < FLAGS.max_number_of_iterations):
+      num_iters += 1
+      last_checkpoint = slim.evaluation.wait_for_new_checkpoint(
+          FLAGS.checkpoint_dir, last_checkpoint)
+      start = time.time()
+      tf.logging.info(
+          'Starting visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',
+                                                       time.gmtime()))
+      tf.logging.info('Visualizing with model %s', last_checkpoint)
+
+      with sv.managed_session(FLAGS.master,
+                              start_standard_services=False) as sess:
+        sv.start_queue_runners(sess)
+        sv.saver.restore(sess, last_checkpoint)
+
+        image_id_offset = 0
+        for batch in range(num_batches):
+          tf.logging.info('Visualizing batch %d / %d', batch + 1, num_batches)
+          _process_batch(sess=sess,
+                         original_images=samples[common.ORIGINAL_IMAGE],
+                         semantic_predictions=predictions,
+                         image_names=samples[common.IMAGE_NAME],
+                         image_heights=samples[common.HEIGHT],
+                         image_widths=samples[common.WIDTH],
+                         image_id_offset=image_id_offset,
+                         save_dir=save_dir,
+                         raw_save_dir=raw_save_dir,
+                         train_id_to_eval_id=train_id_to_eval_id)
+          image_id_offset += FLAGS.vis_batch_size
+
+      tf.logging.info(
+          'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',
+                                                       time.gmtime()))
+      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()
+      if time_to_next_eval > 0:
+        time.sleep(time_to_next_eval)
+
+
+if __name__ == '__main__':
+  flags.mark_flag_as_required('checkpoint_dir')
+  flags.mark_flag_as_required('vis_logdir')
+  flags.mark_flag_as_required('dataset_dir')
+  tf.app.run()
