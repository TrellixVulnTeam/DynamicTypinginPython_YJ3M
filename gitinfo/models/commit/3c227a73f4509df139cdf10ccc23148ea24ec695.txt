commit 3c227a73f4509df139cdf10ccc23148ea24ec695
Author: Hongkun Yu <hongkuny@google.com>
Date:   Wed Apr 15 16:58:48 2020 -0700

    Remove __init__ in CachedAttention, which is the same as its parent class.
    
    PiperOrigin-RevId: 306748161

diff --git a/official/nlp/modeling/layers/attention.py b/official/nlp/modeling/layers/attention.py
index aa01daae..48d243b0 100644
--- a/official/nlp/modeling/layers/attention.py
+++ b/official/nlp/modeling/layers/attention.py
@@ -186,15 +186,9 @@ class MultiHeadAttention(tf.keras.layers.Layer):
 class CachedAttention(MultiHeadAttention):
   """Attention layer with cache used for auto-agressive decoding.
 
-  Arguments:
-    num_heads: Number of attention heads.
-    head_size: Size of each attention head.
-    **kwargs: Other keyword arguments inherit from `Attention` class.
+  Arguments are the same as `MultiHeadAttention` layer.
   """
 
-  def __init__(self, num_heads, head_size, **kwargs):
-    super(CachedAttention, self).__init__(num_heads, head_size, **kwargs)
-
   def _update_cache(self, key_tensor, value_tensor, cache, decode_loop_step):
     """Updates cache states and gets full-length key/value tensors."""
     # Combines cached keys and values with new keys and values.
