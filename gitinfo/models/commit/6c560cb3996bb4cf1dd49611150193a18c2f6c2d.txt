commit 6c560cb3996bb4cf1dd49611150193a18c2f6c2d
Author: josh11b <josh11b@users.noreply.github.com>
Date:   Wed Oct 24 11:58:31 2018 -0700

    AllReduceCrossTowerOps -> AllReduceCrossDeviceOps

diff --git a/official/utils/misc/distribution_utils.py b/official/utils/misc/distribution_utils.py
index e4726558..f6228efe 100644
--- a/official/utils/misc/distribution_utils.py
+++ b/official/utils/misc/distribution_utils.py
@@ -27,7 +27,7 @@ def get_distribution_strategy(num_gpus, all_reduce_alg=None):
   Args:
     num_gpus: Number of GPUs to run this model.
     all_reduce_alg: Specify which algorithm to use when performing all-reduce.
-      See tf.contrib.distribute.AllReduceCrossTowerOps for available algorithms.
+      See tf.contrib.distribute.AllReduceCrossDeviceOps for available algorithms.
       If None, DistributionStrategy will choose based on device topology.
 
   Returns:
@@ -41,7 +41,7 @@ def get_distribution_strategy(num_gpus, all_reduce_alg=None):
     if all_reduce_alg:
       return tf.contrib.distribute.MirroredStrategy(
           num_gpus=num_gpus,
-          cross_tower_ops=tf.contrib.distribute.AllReduceCrossTowerOps(
+          cross_tower_ops=tf.contrib.distribute.AllReduceCrossDeviceOps(
               all_reduce_alg, num_packs=2))
     else:
       return tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)
