commit 47d10833c7c380f0e16bb1336f3d03e6101a3400
Author: Pengchong Jin <pengchong@google.com>
Date:   Wed Apr 15 01:05:12 2020 -0700

    Base model refactor.
    
    PiperOrigin-RevId: 306597558

diff --git a/official/vision/detection/modeling/base_model.py b/official/vision/detection/modeling/base_model.py
index 65778a40..a6bf08b8 100644
--- a/official/vision/detection/modeling/base_model.py
+++ b/official/vision/detection/modeling/base_model.py
@@ -24,37 +24,7 @@ import re
 import tensorflow.compat.v2 as tf
 from official.vision.detection.modeling import checkpoint_utils
 from official.vision.detection.modeling import learning_rates
-
-
-class OptimizerFactory(object):
-  """Class to generate optimizer function."""
-
-  def __init__(self, params):
-    """Creates optimized based on the specified flags."""
-    if params.type == 'momentum':
-      nesterov = False
-      try:
-        nesterov = params.nesterov
-      except AttributeError:
-        pass
-      self._optimizer = functools.partial(
-          tf.keras.optimizers.SGD,
-          momentum=params.momentum,
-          nesterov=nesterov)
-    elif params.type == 'adam':
-      self._optimizer = tf.keras.optimizers.Adam
-    elif params.type == 'adadelta':
-      self._optimizer = tf.keras.optimizers.Adadelta
-    elif params.type == 'adagrad':
-      self._optimizer = tf.keras.optimizers.Adagrad
-    elif params.type == 'rmsprop':
-      self._optimizer = functools.partial(
-          tf.keras.optimizers.RMSprop, momentum=params.momentum)
-    else:
-      raise ValueError('Unsupported optimizer type %s.' % self._optimizer)
-
-  def __call__(self, learning_rate):
-    return self._optimizer(learning_rate=learning_rate)
+from official.vision.detection.modeling import optimizers
 
 
 def _make_filter_trainable_variables_fn(frozen_variable_prefix):
@@ -94,7 +64,7 @@ class Model(object):
       tf.compat.v2.keras.mixed_precision.experimental.set_policy(policy)
 
     # Optimization.
-    self._optimizer_fn = OptimizerFactory(params.train.optimizer)
+    self._optimizer_fn = optimizers.OptimizerFactory(params.train.optimizer)
     self._learning_rate = learning_rates.learning_rate_generator(
         params.train.learning_rate)
 
diff --git a/official/vision/detection/modeling/optimizers.py b/official/vision/detection/modeling/optimizers.py
new file mode 100644
index 00000000..b7a127f3
--- /dev/null
+++ b/official/vision/detection/modeling/optimizers.py
@@ -0,0 +1,55 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Optimizers."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+import numpy as np
+import tensorflow.compat.v2 as tf
+
+
+class OptimizerFactory(object):
+  """Class to generate optimizer function."""
+
+  def __init__(self, params):
+    """Creates optimized based on the specified flags."""
+    if params.type == 'momentum':
+      nesterov = False
+      try:
+        nesterov = params.nesterov
+      except AttributeError:
+        pass
+      self._optimizer = functools.partial(
+          tf.keras.optimizers.SGD,
+          momentum=params.momentum,
+          nesterov=nesterov)
+    elif params.type == 'adam':
+      self._optimizer = tf.keras.optimizers.Adam
+    elif params.type == 'adadelta':
+      self._optimizer = tf.keras.optimizers.Adadelta
+    elif params.type == 'adagrad':
+      self._optimizer = tf.keras.optimizers.Adagrad
+    elif params.type == 'rmsprop':
+      self._optimizer = functools.partial(
+          tf.keras.optimizers.RMSprop, momentum=params.momentum)
+    else:
+      raise ValueError('Unsupported optimizer type `{}`.'.format(params.type))
+
+  def __call__(self, learning_rate):
+    return self._optimizer(learning_rate=learning_rate)
