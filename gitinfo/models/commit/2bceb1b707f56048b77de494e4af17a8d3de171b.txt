commit 2bceb1b707f56048b77de494e4af17a8d3de171b
Author: Yeqing Li <yeqing@google.com>
Date:   Mon Oct 21 15:42:30 2019 -0700

    Move sptial_transforms to ops folder.
    
    PiperOrigin-RevId: 275940140

diff --git a/official/vision/detection/modeling/architecture/fpn.py b/official/vision/detection/modeling/architecture/fpn.py
index e629d499..6af6af17 100644
--- a/official/vision/detection/modeling/architecture/fpn.py
+++ b/official/vision/detection/modeling/architecture/fpn.py
@@ -28,7 +28,7 @@ import tensorflow.compat.v2 as tf
 
 from tensorflow.python.keras import backend
 from official.vision.detection.modeling.architecture import nn_ops
-from official.vision.detection.utils import spatial_transform
+from official.vision.detection.ops import spatial_transform_ops
 
 
 class Fpn(object):
@@ -91,7 +91,7 @@ class Fpn(object):
       # Adds top-down path.
       feats = {backbone_max_level: feats_lateral[backbone_max_level]}
       for level in range(backbone_max_level - 1, self._min_level - 1, -1):
-        feats[level] = spatial_transform.nearest_upsampling(
+        feats[level] = spatial_transform_ops.nearest_upsampling(
             feats[level + 1], 2) + feats_lateral[level]
 
       # Adds post-hoc 3x3 convolution kernel.
diff --git a/official/vision/detection/modeling/architecture/heads.py b/official/vision/detection/modeling/architecture/heads.py
index b0703378..65d3def3 100644
--- a/official/vision/detection/modeling/architecture/heads.py
+++ b/official/vision/detection/modeling/architecture/heads.py
@@ -25,7 +25,7 @@ import numpy as np
 import tensorflow.compat.v2 as tf
 from tensorflow.python.keras import backend
 from official.vision.detection.modeling.architecture import nn_ops
-from official.vision.detection.utils import spatial_transform
+from official.vision.detection.ops import spatial_transform_ops
 
 
 class RpnHead(object):
@@ -542,7 +542,7 @@ class ShapemaskPriorHead(object):
       level_outer_boxes = outer_boxes / tf.pow(
           2., tf.expand_dims(detection_prior_levels, -1))
       detection_prior_levels = tf.cast(detection_prior_levels, tf.int32)
-      uniform_priors = spatial_transform.crop_mask_in_target_box(
+      uniform_priors = spatial_transform_ops.crop_mask_in_target_box(
           tf.ones([
               batch_size, self._num_of_instances, self._mask_crop_size,
               self._mask_crop_size
@@ -550,7 +550,7 @@ class ShapemaskPriorHead(object):
 
       # Prepare crop features.
       multi_level_features = self._get_multilevel_features(fpn_features)
-      crop_features = spatial_transform.single_level_feature_crop(
+      crop_features = spatial_transform_ops.single_level_feature_crop(
           multi_level_features, level_outer_boxes, detection_prior_levels,
           self._min_mask_level, self._mask_crop_size)
 
@@ -562,7 +562,7 @@ class ShapemaskPriorHead(object):
           batch_size, self._num_of_instances, self._mask_crop_size,
           self._mask_crop_size
       ])
-      predicted_detection_priors = spatial_transform.crop_mask_in_target_box(
+      predicted_detection_priors = spatial_transform_ops.crop_mask_in_target_box(
           fused_shape_priors, boxes, outer_boxes, self._mask_crop_size)
       predicted_detection_priors = tf.reshape(
           predicted_detection_priors,
diff --git a/official/vision/detection/utils/spatial_transform.py b/official/vision/detection/ops/spatial_transform_ops.py
similarity index 100%
rename from official/vision/detection/utils/spatial_transform.py
rename to official/vision/detection/ops/spatial_transform_ops.py
