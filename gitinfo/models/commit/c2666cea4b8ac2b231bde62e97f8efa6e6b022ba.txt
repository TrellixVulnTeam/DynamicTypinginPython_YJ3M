commit c2666cea4b8ac2b231bde62e97f8efa6e6b022ba
Author: Hongkun Yu <hongkuny@google.com>
Date:   Tue May 19 13:03:46 2020 -0700

    [Clean up] Remove enable_eager in the session config: Model garden is TF2 only now.
    Remove is_v2_0
    
    PiperOrigin-RevId: 312336907

diff --git a/official/benchmark/models/resnet_cifar_main.py b/official/benchmark/models/resnet_cifar_main.py
index b5a05f90..4a02fec8 100644
--- a/official/benchmark/models/resnet_cifar_main.py
+++ b/official/benchmark/models/resnet_cifar_main.py
@@ -119,7 +119,6 @@ def run(flags_obj):
     Dictionary of training and eval stats.
   """
   keras_utils.set_session_config(
-      enable_eager=flags_obj.enable_eager,
       enable_xla=flags_obj.enable_xla)
 
   # Execute flag override logic for better model performance
diff --git a/official/benchmark/models/resnet_cifar_test.py b/official/benchmark/models/resnet_cifar_test.py
index 95cf438f..c160f44e 100644
--- a/official/benchmark/models/resnet_cifar_test.py
+++ b/official/benchmark/models/resnet_cifar_test.py
@@ -26,7 +26,6 @@ from tensorflow.python.eager import context
 from tensorflow.python.platform import googletest
 from official.benchmark.models import cifar_preprocessing
 from official.benchmark.models import resnet_cifar_main
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 
 
@@ -60,8 +59,6 @@ class KerasCifarTest(googletest.TestCase):
 
   def test_end_to_end_no_dist_strat(self):
     """Test Keras model with 1 GPU, no distribution strategy."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     extra_flags = [
         "-distribution_strategy", "off",
@@ -94,8 +91,6 @@ class KerasCifarTest(googletest.TestCase):
 
   def test_end_to_end_1_gpu(self):
     """Test Keras model with 1 GPU."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 1:
       self.skipTest(
@@ -140,8 +135,6 @@ class KerasCifarTest(googletest.TestCase):
 
   def test_end_to_end_2_gpu(self):
     """Test Keras model with 2 GPUs."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 2:
       self.skipTest(
diff --git a/official/benchmark/models/resnet_imagenet_main.py b/official/benchmark/models/resnet_imagenet_main.py
index 515c4dc1..5a3cd503 100644
--- a/official/benchmark/models/resnet_imagenet_main.py
+++ b/official/benchmark/models/resnet_imagenet_main.py
@@ -51,7 +51,6 @@ def run(flags_obj):
     Dictionary of training and eval stats.
   """
   keras_utils.set_session_config(
-      enable_eager=flags_obj.enable_eager,
       enable_xla=flags_obj.enable_xla)
 
   # Execute flag override logic for better model performance
diff --git a/official/benchmark/models/resnet_imagenet_test.py b/official/benchmark/models/resnet_imagenet_test.py
index 68009648..44b8ce27 100644
--- a/official/benchmark/models/resnet_imagenet_test.py
+++ b/official/benchmark/models/resnet_imagenet_test.py
@@ -23,7 +23,6 @@ import tensorflow as tf
 
 from tensorflow.python.eager import context
 from official.benchmark.models import resnet_imagenet_main
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 from official.vision.image_classification.resnet import imagenet_preprocessing
 
@@ -85,8 +84,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_no_dist_strat(self, flags_key):
     """Test Keras model with 1 GPU, no distribution strategy."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     extra_flags = [
         "-distribution_strategy", "off",
@@ -115,8 +112,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_1_gpu(self, flags_key):
     """Test Keras model with 1 GPU."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 1:
       self.skipTest(
@@ -138,8 +133,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_1_gpu_fp16(self, flags_key):
     """Test Keras model with 1 GPU and fp16."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 1:
       self.skipTest(
@@ -164,8 +157,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_2_gpu(self, flags_key):
     """Test Keras model with 2 GPUs."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 2:
       self.skipTest(
@@ -186,8 +177,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_xla_2_gpu(self, flags_key):
     """Test Keras model with XLA and 2 GPUs."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 2:
       self.skipTest(
@@ -209,8 +198,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_2_gpu_fp16(self, flags_key):
     """Test Keras model with 2 GPUs and fp16."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     if context.num_gpus() < 2:
       self.skipTest(
@@ -235,9 +222,6 @@ class KerasImagenetTest(tf.test.TestCase):
 
   def test_end_to_end_xla_2_gpu_fp16(self, flags_key):
     """Test Keras model with XLA, 2 GPUs and fp16."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
-
     if context.num_gpus() < 2:
       self.skipTest(
           "{} GPUs are not available for this test. {} GPUs are available".
diff --git a/official/benchmark/models/resnet_imagenet_test_tpu.py b/official/benchmark/models/resnet_imagenet_test_tpu.py
index 6022023b..ead92312 100644
--- a/official/benchmark/models/resnet_imagenet_test_tpu.py
+++ b/official/benchmark/models/resnet_imagenet_test_tpu.py
@@ -21,7 +21,6 @@ from __future__ import print_function
 from absl.testing import parameterized
 import tensorflow as tf
 from official.benchmark.models import resnet_imagenet_main
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 from official.vision.image_classification.resnet import imagenet_preprocessing
 
@@ -70,8 +69,6 @@ class KerasImagenetTest(tf.test.TestCase, parameterized.TestCase):
   ])
   def test_end_to_end_tpu(self, flags_key):
     """Test Keras model with TPU distribution strategy."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     extra_flags = [
         "-distribution_strategy", "tpu",
@@ -89,8 +86,6 @@ class KerasImagenetTest(tf.test.TestCase, parameterized.TestCase):
   @parameterized.parameters(["resnet"])
   def test_end_to_end_tpu_bf16(self, flags_key):
     """Test Keras model with TPU and bfloat16 activation."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     extra_flags = [
         "-distribution_strategy", "tpu",
diff --git a/official/benchmark/models/shakespeare/shakespeare_main.py b/official/benchmark/models/shakespeare/shakespeare_main.py
index d3a61627..6928dd1d 100644
--- a/official/benchmark/models/shakespeare/shakespeare_main.py
+++ b/official/benchmark/models/shakespeare/shakespeare_main.py
@@ -139,7 +139,6 @@ def build_model(vocab_size,
   Returns:
     A Keras Model.
   """
-  assert keras_utils.is_v2_0()
   LSTM = functools.partial(tf.keras.layers.LSTM, implementation=2)
 
   # By indirecting the activation through a lambda layer, the logic to dispatch
@@ -275,7 +274,6 @@ def run(flags_obj):
     tf.keras.mixed_precision.experimental.set_policy(policy)
 
   keras_utils.set_session_config(
-      enable_eager=flags_obj.enable_eager,
       enable_xla=flags_obj.enable_xla)
 
   strategy = distribution_utils.get_distribution_strategy(
diff --git a/official/benchmark/shakespeare_benchmark.py b/official/benchmark/shakespeare_benchmark.py
index ee42ffef..430ab75d 100644
--- a/official/benchmark/shakespeare_benchmark.py
+++ b/official/benchmark/shakespeare_benchmark.py
@@ -273,7 +273,6 @@ class ShakespeareKerasBenchmarkReal(ShakespeareBenchmarkBase):
     FLAGS.num_gpus = 1
     FLAGS.batch_size = 64
     FLAGS.cudnn = False
-    FLAGS.enable_eager = keras_utils.is_v2_0()
     self._run_and_report_benchmark()
 
   def benchmark_1_gpu_no_ds(self):
@@ -307,7 +306,6 @@ class ShakespeareKerasBenchmarkReal(ShakespeareBenchmarkBase):
     FLAGS.num_gpus = 1
     FLAGS.batch_size = 64
     FLAGS.cudnn = False
-    FLAGS.enable_eager = keras_utils.is_v2_0()
     FLAGS.enable_xla = True
     self._run_and_report_benchmark()
 
@@ -326,7 +324,6 @@ class ShakespeareKerasBenchmarkReal(ShakespeareBenchmarkBase):
     FLAGS.batch_size = 64 * 8
     FLAGS.log_steps = 10
     FLAGS.cudnn = False
-    FLAGS.enable_eager = keras_utils.is_v2_0()
     self._run_and_report_benchmark()
 
   def benchmark_xla_8_gpu(self):
@@ -345,7 +342,6 @@ class ShakespeareKerasBenchmarkReal(ShakespeareBenchmarkBase):
     FLAGS.batch_size = 64 * 8
     FLAGS.log_steps = 10
     FLAGS.cudnn = False
-    FLAGS.enable_eager = keras_utils.is_v2_0()
     FLAGS.enable_xla = True
     self._run_and_report_benchmark()
 
diff --git a/official/nlp/bert/run_classifier.py b/official/nlp/bert/run_classifier.py
index e4cb3e53..626ebd50 100644
--- a/official/nlp/bert/run_classifier.py
+++ b/official/nlp/bert/run_classifier.py
@@ -347,7 +347,7 @@ def run_bert(strategy,
   if FLAGS.mode != 'train_and_eval':
     raise ValueError('Unsupported mode is specified: %s' % FLAGS.mode)
   # Enables XLA in Session Config. Should not be set for TPU.
-  keras_utils.set_config_v2(FLAGS.enable_xla)
+  keras_utils.set_session_config(FLAGS.enable_xla)
   performance.set_mixed_precision_policy(common_flags.dtype())
 
   epochs = FLAGS.num_train_epochs
diff --git a/official/nlp/bert/run_squad_helper.py b/official/nlp/bert/run_squad_helper.py
index 07c22ed3..8c3c7819 100644
--- a/official/nlp/bert/run_squad_helper.py
+++ b/official/nlp/bert/run_squad_helper.py
@@ -227,7 +227,7 @@ def train_squad(strategy,
     logging.info('Training using customized training loop with distribution'
                  ' strategy.')
   # Enables XLA in Session Config. Should not be set for TPU.
-  keras_utils.set_config_v2(FLAGS.enable_xla)
+  keras_utils.set_session_config(FLAGS.enable_xla)
   performance.set_mixed_precision_policy(common_flags.dtype())
 
   epochs = FLAGS.num_train_epochs
diff --git a/official/r1/boosted_trees/train_higgs.py b/official/r1/boosted_trees/train_higgs.py
index a89e4c22..5f3f2547 100644
--- a/official/r1/boosted_trees/train_higgs.py
+++ b/official/r1/boosted_trees/train_higgs.py
@@ -48,7 +48,7 @@ import os
 from absl import app as absl_app
 from absl import flags
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from official.r1.utils.logs import logger
 from official.utils.flags import core as flags_core
diff --git a/official/r1/mnist/mnist.py b/official/r1/mnist/mnist.py
index 062d5bcf..a93358d8 100644
--- a/official/r1/mnist/mnist.py
+++ b/official/r1/mnist/mnist.py
@@ -21,7 +21,7 @@ from absl import app as absl_app
 from absl import flags
 from absl import logging
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from official.r1.mnist import dataset
 from official.r1.utils.logs import hooks_helper
diff --git a/official/r1/mnist/mnist_eager_test.py b/official/r1/mnist/mnist_eager_test.py
deleted file mode 100644
index 2fe7e66f..00000000
--- a/official/r1/mnist/mnist_eager_test.py
+++ /dev/null
@@ -1,95 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import unittest
-
-import tensorflow as tf  # pylint: disable=g-bad-import-order
-from tensorflow.python import eager as tfe  # pylint: disable=g-bad-import-order
-
-from official.r1.mnist import mnist
-from official.r1.mnist import mnist_eager
-from official.utils.misc import keras_utils
-
-
-def device():
-  return '/device:GPU:0' if tfe.context.num_gpus() else '/device:CPU:0'
-
-
-def data_format():
-  return 'channels_first' if tfe.context.num_gpus() else 'channels_last'
-
-
-def random_dataset():
-  batch_size = 64
-  images = tf.random_normal([batch_size, 784])
-  labels = tf.random_uniform([batch_size], minval=0, maxval=10, dtype=tf.int32)
-  return tf.data.Dataset.from_tensors((images, labels))
-
-
-def train(defun=False):
-  model = mnist.create_model(data_format())
-  if defun:
-    model.call = tf.function(model.call)
-  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
-  dataset = random_dataset()
-  with tf.device(device()):
-    mnist_eager.train(model, optimizer, dataset,
-                      step_counter=tf.train.get_or_create_global_step())
-
-
-def evaluate(defun=False):
-  model = mnist.create_model(data_format())
-  dataset = random_dataset()
-  if defun:
-    model.call = tf.function(model.call)
-  with tf.device(device()):
-    mnist_eager.test(model, dataset)
-
-
-class MNISTTest(tf.test.TestCase):
-  """Run tests for MNIST eager loop.
-
-  MNIST eager uses contrib and will not work with TF 2.0.  All tests are
-  disabled if using TF 2.0.
-  """
-
-  def setUp(self):
-    if not keras_utils.is_v2_0():
-      tf.compat.v1.enable_v2_behavior()
-    super(MNISTTest, self).setUp()
-
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
-  def test_train(self):
-    train(defun=False)
-
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
-  def test_evaluate(self):
-    evaluate(defun=False)
-
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
-  def test_train_with_defun(self):
-    train(defun=True)
-
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
-  def test_evaluate_with_defun(self):
-    evaluate(defun=True)
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/official/r1/mnist/mnist_test.py b/official/r1/mnist/mnist_test.py
index 207c0d0b..87e05712 100644
--- a/official/r1/mnist/mnist_test.py
+++ b/official/r1/mnist/mnist_test.py
@@ -18,12 +18,10 @@ from __future__ import division
 from __future__ import print_function
 
 import time
-import unittest
 
-import tensorflow as tf  # pylint: disable=g-bad-import-order
+import tensorflow.compat.v1 as tf  # pylint: disable=g-bad-import-order
 from absl import logging
 from official.r1.mnist import mnist
-from official.utils.misc import keras_utils
 
 BATCH_SIZE = 100
 
@@ -51,7 +49,6 @@ class Tests(tf.test.TestCase):
   using TF 2.0.
   """
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_mnist(self):
     classifier = make_estimator()
     classifier.train(input_fn=dummy_input_fn, steps=2)
@@ -71,7 +68,6 @@ class Tests(tf.test.TestCase):
       self.assertEqual(predictions['probabilities'].shape, (10,))
       self.assertEqual(predictions['classes'].shape, ())
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def mnist_model_fn_helper(self, mode, multi_gpu=False):
     features, labels = dummy_input_fn()
     image_count = features.shape[0]
@@ -99,19 +95,15 @@ class Tests(tf.test.TestCase):
       self.assertEqual(eval_metric_ops['accuracy'][0].dtype, tf.float32)
       self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_mnist_model_fn_train_mode(self):
     self.mnist_model_fn_helper(tf.estimator.ModeKeys.TRAIN)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_mnist_model_fn_train_mode_multi_gpu(self):
     self.mnist_model_fn_helper(tf.estimator.ModeKeys.TRAIN, multi_gpu=True)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_mnist_model_fn_eval_mode(self):
     self.mnist_model_fn_helper(tf.estimator.ModeKeys.EVAL)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_mnist_model_fn_predict_mode(self):
     self.mnist_model_fn_helper(tf.estimator.ModeKeys.PREDICT)
 
@@ -144,4 +136,5 @@ class Benchmarks(tf.test.Benchmark):
 
 if __name__ == '__main__':
   logging.set_verbosity(logging.ERROR)
+  tf.disable_v2_behavior()
   tf.test.main()
diff --git a/official/r1/resnet/cifar10_test.py b/official/r1/resnet/cifar10_test.py
index 827cb0c4..ba40eb2c 100644
--- a/official/r1/resnet/cifar10_test.py
+++ b/official/r1/resnet/cifar10_test.py
@@ -24,7 +24,6 @@ import numpy as np
 import tensorflow as tf
 
 from official.r1.resnet import cifar10_main
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 
 logging.set_verbosity(logging.ERROR)
@@ -44,8 +43,7 @@ class BaseTest(tf.test.TestCase):
   @classmethod
   def setUpClass(cls):  # pylint: disable=invalid-name
     super(BaseTest, cls).setUpClass()
-    if keras_utils.is_v2_0:
-      tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_eager_execution()
     cifar10_main.define_cifar_flags()
 
   def setUp(self):
diff --git a/official/r1/resnet/imagenet_test.py b/official/r1/resnet/imagenet_test.py
index 7d33ef09..c25cafb8 100644
--- a/official/r1/resnet/imagenet_test.py
+++ b/official/r1/resnet/imagenet_test.py
@@ -23,7 +23,6 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 from absl import logging
 
 from official.r1.resnet import imagenet_main
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 
 logging.set_verbosity(logging.ERROR)
@@ -43,8 +42,7 @@ class BaseTest(tf.test.TestCase):
 
   def setUp(self):
     super(BaseTest, self).setUp()
-    if keras_utils.is_v2_0:
-      tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_eager_execution()
     self._num_validation_images = imagenet_main.NUM_IMAGES['validation']
     imagenet_main.NUM_IMAGES['validation'] = 4
 
diff --git a/official/r1/utils/data/file_io_test.py b/official/r1/utils/data/file_io_test.py
index ba90d94e..529cb459 100644
--- a/official/r1/utils/data/file_io_test.py
+++ b/official/r1/utils/data/file_io_test.py
@@ -28,7 +28,6 @@ import tensorflow as tf
 # pylint: enable=wrong-import-order
 
 from official.r1.utils.data import file_io
-from official.utils.misc import keras_utils
 
 
 _RAW_ROW = "raw_row"
@@ -108,8 +107,7 @@ class BaseTest(tf.test.TestCase):
 
   def setUp(self):
     super(BaseTest, self).setUp()
-    if keras_utils.is_v2_0:
-      tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_eager_execution()
 
   def _test_sharding(self, row_count, cpu_count, expected):
     df = pd.DataFrame({_DUMMY_COL: list(range(row_count))})
diff --git a/official/r1/wide_deep/census_dataset.py b/official/r1/wide_deep/census_dataset.py
index 7aac80eb..f3a07ac6 100644
--- a/official/r1/wide_deep/census_dataset.py
+++ b/official/r1/wide_deep/census_dataset.py
@@ -26,7 +26,7 @@ from absl import app as absl_app
 from absl import flags
 from six.moves import urllib
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 # pylint: enable=wrong-import-order
 
 from official.utils.flags import core as flags_core
diff --git a/official/r1/wide_deep/census_main.py b/official/r1/wide_deep/census_main.py
index b908dfa6..39a1610e 100644
--- a/official/r1/wide_deep/census_main.py
+++ b/official/r1/wide_deep/census_main.py
@@ -18,7 +18,7 @@ import os
 
 from absl import app as absl_app
 from absl import flags
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from official.r1.utils.logs import logger
 from official.r1.wide_deep import census_dataset
 from official.r1.wide_deep import wide_deep_run_loop
diff --git a/official/r1/wide_deep/census_test.py b/official/r1/wide_deep/census_test.py
index 8e1c8657..81165156 100644
--- a/official/r1/wide_deep/census_test.py
+++ b/official/r1/wide_deep/census_test.py
@@ -18,15 +18,13 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import unittest
 
-import tensorflow as tf  # pylint: disable=g-bad-import-order
 from absl import logging
+import tensorflow.compat.v1 as tf
 
-from official.utils.misc import keras_utils
-from official.utils.testing import integration
 from official.r1.wide_deep import census_dataset
 from official.r1.wide_deep import census_main
+from official.utils.testing import integration
 
 logging.set_verbosity(logging.ERROR)
 
@@ -73,7 +71,6 @@ class BaseTest(tf.test.TestCase):
           os.path.join(self.temp_dir, fname), 'w') as test_csv:
         test_csv.write(test_csv_contents)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_input_fn(self):
     dataset = census_dataset.input_fn(self.input_csv, 1, False, 1)
     features, labels = dataset.make_one_shot_iterator().get_next()
@@ -127,11 +124,9 @@ class BaseTest(tf.test.TestCase):
                        initial_results['auc_precision_recall'])
     self.assertGreater(final_results['accuracy'], initial_results['accuracy'])
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_wide_deep_estimator_training(self):
     self.build_and_test_estimator('wide_deep')
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_end_to_end_wide(self):
     integration.run_synthetic(
         main=census_main.main, tmp_root=self.get_temp_dir(),
@@ -142,7 +137,6 @@ class BaseTest(tf.test.TestCase):
         ],
         synth=False)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_end_to_end_deep(self):
     integration.run_synthetic(
         main=census_main.main, tmp_root=self.get_temp_dir(),
@@ -153,7 +147,6 @@ class BaseTest(tf.test.TestCase):
         ],
         synth=False)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), 'TF 1.0 only test.')
   def test_end_to_end_wide_deep(self):
     integration.run_synthetic(
         main=census_main.main, tmp_root=self.get_temp_dir(),
@@ -166,4 +159,5 @@ class BaseTest(tf.test.TestCase):
 
 
 if __name__ == '__main__':
+  tf.disable_eager_execution()
   tf.test.main()
diff --git a/official/r1/wide_deep/movielens_dataset.py b/official/r1/wide_deep/movielens_dataset.py
index 311eb7f3..676062cb 100644
--- a/official/r1/wide_deep/movielens_dataset.py
+++ b/official/r1/wide_deep/movielens_dataset.py
@@ -25,7 +25,7 @@ import os
 from absl import app as absl_app
 from absl import flags
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 # pylint: enable=wrong-import-order
 
 from official.recommendation import movielens
diff --git a/official/r1/wide_deep/movielens_main.py b/official/r1/wide_deep/movielens_main.py
index 10cb53be..45f7453c 100644
--- a/official/r1/wide_deep/movielens_main.py
+++ b/official/r1/wide_deep/movielens_main.py
@@ -22,7 +22,7 @@ import os
 
 from absl import app as absl_app
 from absl import flags
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from official.r1.utils.logs import logger
 from official.r1.wide_deep import movielens_dataset
 from official.r1.wide_deep import wide_deep_run_loop
diff --git a/official/r1/wide_deep/movielens_test.py b/official/r1/wide_deep/movielens_test.py
index 5117f628..01e6b116 100644
--- a/official/r1/wide_deep/movielens_test.py
+++ b/official/r1/wide_deep/movielens_test.py
@@ -18,13 +18,11 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import unittest
 
 import numpy as np
-import tensorflow as tf  # pylint: disable=g-bad-import-order
+import tensorflow.compat.v1 as tf
 
 from official.recommendation import movielens
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 from official.r1.wide_deep import movielens_dataset
 from official.r1.wide_deep import movielens_main
@@ -85,7 +83,6 @@ class BaseTest(tf.test.TestCase):
     with tf.io.gfile.GFile(self.item_csv, "w") as f:
       f.write(TEST_ITEM_DATA)
 
-  @unittest.skipIf(keras_utils.is_v2_0(), "TF 1.0 only test.")
   def test_input_fn(self):
     train_input_fn, _, _ = movielens_dataset.construct_input_fns(
         dataset=movielens.ML_1M, data_dir=self.temp_dir, batch_size=8, repeat=1)
@@ -103,7 +100,6 @@ class BaseTest(tf.test.TestCase):
 
       self.assertAllClose(labels[0], [1.0])
 
-  @unittest.skipIf(keras_utils.is_v2_0(), "TF 1.0 only test.")
   def test_end_to_end_deep(self):
     integration.run_synthetic(
         main=movielens_main.main, tmp_root=self.temp_dir,
@@ -117,4 +113,5 @@ class BaseTest(tf.test.TestCase):
 
 
 if __name__ == "__main__":
+  tf.disable_eager_execution()
   tf.test.main()
diff --git a/official/r1/wide_deep/wide_deep_run_loop.py b/official/r1/wide_deep/wide_deep_run_loop.py
index 843916d6..d81bfc85 100644
--- a/official/r1/wide_deep/wide_deep_run_loop.py
+++ b/official/r1/wide_deep/wide_deep_run_loop.py
@@ -23,7 +23,7 @@ import shutil
 
 from absl import app as absl_app
 from absl import flags
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from official.r1.utils.logs import hooks_helper
 from official.r1.utils.logs import logger
diff --git a/official/recommendation/data_test.py b/official/recommendation/data_test.py
index 5641e9b6..9541ee3f 100644
--- a/official/recommendation/data_test.py
+++ b/official/recommendation/data_test.py
@@ -31,7 +31,6 @@ from official.recommendation import constants as rconst
 from official.recommendation import data_preprocessing
 from official.recommendation import movielens
 from official.recommendation import popen_helper
-from official.utils.misc import keras_utils
 
 
 DATASET = "ml-test"
@@ -59,8 +58,7 @@ def mock_download(*args, **kwargs):
 class BaseTest(tf.test.TestCase):
 
   def setUp(self):
-    if keras_utils.is_v2_0:
-      tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_eager_execution()
     self.temp_data_dir = self.get_temp_dir()
     ratings_folder = os.path.join(self.temp_data_dir, DATASET)
     tf.io.gfile.makedirs(ratings_folder)
diff --git a/official/recommendation/ncf_keras_main.py b/official/recommendation/ncf_keras_main.py
index 71e738b5..877e116e 100644
--- a/official/recommendation/ncf_keras_main.py
+++ b/official/recommendation/ncf_keras_main.py
@@ -220,14 +220,6 @@ def run_ncf(_):
   params = ncf_common.parse_flags(FLAGS)
   params["distribute_strategy"] = strategy
 
-  if not keras_utils.is_v2_0() and strategy is not None:
-    logging.error("NCF Keras only works with distribution strategy in TF 2.0")
-    return
-  if (params["keras_use_ctl"] and (
-      not keras_utils.is_v2_0() or strategy is None)):
-    logging.error(
-        "Custom training loop only works with tensorflow 2.0 and dist strat.")
-    return
   if params["use_tpu"] and not params["keras_use_ctl"]:
     logging.error("Custom training loop must be used when using TPUStrategy.")
     return
diff --git a/official/recommendation/ncf_test.py b/official/recommendation/ncf_test.py
index 03e0ee14..5103283e 100644
--- a/official/recommendation/ncf_test.py
+++ b/official/recommendation/ncf_test.py
@@ -18,21 +18,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import math
 import unittest
 
-import numpy as np
 import tensorflow as tf
 from tensorflow.python.eager import context  # pylint: disable=ungrouped-imports
 from official.recommendation import constants as rconst
-from official.recommendation import data_pipeline
 from official.recommendation import ncf_common
 from official.recommendation import ncf_keras_main
-from official.recommendation import neumf_model
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 
-
 NUM_TRAIN_NEG = 4
 
 
@@ -52,139 +46,6 @@ class NcfTest(tf.test.TestCase):
     rconst.NUM_EVAL_NEGATIVES = self.num_eval_negatives_old
     rconst.TOP_K = self.top_k_old
 
-  @unittest.skipIf(keras_utils.is_v2_0(), "TODO(b/136018594)")
-  def get_hit_rate_and_ndcg(self, predicted_scores_by_user, items_by_user,
-                            top_k=rconst.TOP_K, match_mlperf=False):
-    rconst.TOP_K = top_k
-    rconst.NUM_EVAL_NEGATIVES = predicted_scores_by_user.shape[1] - 1
-    batch_size = items_by_user.shape[0]
-
-    users = np.repeat(np.arange(batch_size)[:, np.newaxis],
-                      rconst.NUM_EVAL_NEGATIVES + 1, axis=1)
-    users, items, duplicate_mask = \
-      data_pipeline.BaseDataConstructor._assemble_eval_batch(
-          users, items_by_user[:, -1:], items_by_user[:, :-1], batch_size)
-
-    g = tf.Graph()
-    with g.as_default():
-      logits = tf.convert_to_tensor(
-          predicted_scores_by_user.reshape((-1, 1)), tf.float32)
-      softmax_logits = tf.concat([tf.zeros(logits.shape, dtype=logits.dtype),
-                                  logits], axis=1)
-      duplicate_mask = tf.convert_to_tensor(duplicate_mask, tf.float32)
-
-      metric_ops = neumf_model._get_estimator_spec_with_metrics(
-          logits=logits, softmax_logits=softmax_logits,
-          duplicate_mask=duplicate_mask, num_training_neg=NUM_TRAIN_NEG,
-          match_mlperf=match_mlperf).eval_metric_ops
-
-      hr = metric_ops[rconst.HR_KEY]
-      ndcg = metric_ops[rconst.NDCG_KEY]
-
-      init = [tf.compat.v1.global_variables_initializer(),
-              tf.compat.v1.local_variables_initializer()]
-
-    with self.session(graph=g) as sess:
-      sess.run(init)
-      return sess.run([hr[1], ndcg[1]])
-
-  def test_hit_rate_and_ndcg(self):
-    # Test with no duplicate items
-    predictions = np.array([
-        [2., 0., 1.],  # In top 2
-        [1., 0., 2.],  # In top 1
-        [2., 1., 0.],  # In top 3
-        [3., 4., 2.]   # In top 3
-    ])
-    items = np.array([
-        [2, 3, 1],
-        [3, 1, 2],
-        [2, 1, 3],
-        [1, 3, 2],
-    ])
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 1)
-    self.assertAlmostEqual(hr, 1 / 4)
-    self.assertAlmostEqual(ndcg, 1 / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 2)
-    self.assertAlmostEqual(hr, 2 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 3)
-    self.assertAlmostEqual(hr, 4 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3) +
-                                  2 * math.log(2) / math.log(4)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 1,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 1 / 4)
-    self.assertAlmostEqual(ndcg, 1 / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 2,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 2 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 3,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 4 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3) +
-                                  2 * math.log(2) / math.log(4)) / 4)
-
-    # Test with duplicate items. In the MLPerf case, we treat the duplicates as
-    # a single item. Otherwise, we treat the duplicates as separate items.
-    predictions = np.array([
-        [2., 2., 3., 1.],  # In top 4. MLPerf: In top 3
-        [1., 0., 2., 3.],  # In top 1. MLPerf: In top 1
-        [2., 3., 2., 0.],  # In top 4. MLPerf: In top 3
-        [2., 4., 2., 3.]   # In top 2. MLPerf: In top 2
-    ])
-    items = np.array([
-        [2, 2, 3, 1],
-        [2, 3, 4, 1],
-        [2, 3, 2, 1],
-        [3, 2, 1, 4],
-    ])
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 1)
-    self.assertAlmostEqual(hr, 1 / 4)
-    self.assertAlmostEqual(ndcg, 1 / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 2)
-    self.assertAlmostEqual(hr, 2 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 3)
-    self.assertAlmostEqual(hr, 2 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 4)
-    self.assertAlmostEqual(hr, 4 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3) +
-                                  2 * math.log(2) / math.log(5)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 1,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 1 / 4)
-    self.assertAlmostEqual(ndcg, 1 / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 2,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 2 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 3,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 4 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3) +
-                                  2 * math.log(2) / math.log(4)) / 4)
-
-    hr, ndcg = self.get_hit_rate_and_ndcg(predictions, items, 4,
-                                          match_mlperf=True)
-    self.assertAlmostEqual(hr, 4 / 4)
-    self.assertAlmostEqual(ndcg, (1 + math.log(2) / math.log(3) +
-                                  2 * math.log(2) / math.log(4)) / 4)
-
   _BASE_END_TO_END_FLAGS = ['-batch_size', '1044', '-train_epochs', '1']
 
   @unittest.mock.patch.object(rconst, "SYNTHETIC_BATCHES_PER_EPOCH", 100)
@@ -195,14 +56,12 @@ class NcfTest(tf.test.TestCase):
         ['-distribution_strategy', 'off'])
 
   @unittest.mock.patch.object(rconst, "SYNTHETIC_BATCHES_PER_EPOCH", 100)
-  @unittest.skipUnless(keras_utils.is_v2_0(), 'TF 2.0 only test.')
   def test_end_to_end_keras_dist_strat(self):
     integration.run_synthetic(
         ncf_keras_main.main, tmp_root=self.get_temp_dir(),
         extra_flags=self._BASE_END_TO_END_FLAGS + ['-num_gpus', '0'])
 
   @unittest.mock.patch.object(rconst, "SYNTHETIC_BATCHES_PER_EPOCH", 100)
-  @unittest.skipUnless(keras_utils.is_v2_0(), 'TF 2.0 only test.')
   def test_end_to_end_keras_dist_strat_ctl(self):
     flags = (self._BASE_END_TO_END_FLAGS +
              ['-num_gpus', '0'] +
@@ -212,7 +71,6 @@ class NcfTest(tf.test.TestCase):
         extra_flags=flags)
 
   @unittest.mock.patch.object(rconst, "SYNTHETIC_BATCHES_PER_EPOCH", 100)
-  @unittest.skipUnless(keras_utils.is_v2_0(), 'TF 2.0 only test.')
   def test_end_to_end_keras_1_gpu_dist_strat_fp16(self):
     if context.num_gpus() < 1:
       self.skipTest(
@@ -225,7 +83,6 @@ class NcfTest(tf.test.TestCase):
                                                    '--dtype', 'fp16'])
 
   @unittest.mock.patch.object(rconst, "SYNTHETIC_BATCHES_PER_EPOCH", 100)
-  @unittest.skipUnless(keras_utils.is_v2_0(), 'TF 2.0 only test.')
   def test_end_to_end_keras_1_gpu_dist_strat_ctl_fp16(self):
     if context.num_gpus() < 1:
       self.skipTest(
@@ -239,7 +96,6 @@ class NcfTest(tf.test.TestCase):
                                                    '--keras_use_ctl'])
 
   @unittest.mock.patch.object(rconst, 'SYNTHETIC_BATCHES_PER_EPOCH', 100)
-  @unittest.skipUnless(keras_utils.is_v2_0(), 'TF 2.0 only test.')
   def test_end_to_end_keras_2_gpu_fp16(self):
     if context.num_gpus() < 2:
       self.skipTest(
diff --git a/official/utils/misc/keras_utils.py b/official/utils/misc/keras_utils.py
index 2866dac7..ebfa0681 100644
--- a/official/utils/misc/keras_utils.py
+++ b/official/utils/misc/keras_utils.py
@@ -24,7 +24,6 @@ import time
 
 from absl import logging
 import tensorflow as tf
-from tensorflow.python import tf2
 
 
 class BatchTimestamp(object):
@@ -150,39 +149,13 @@ class SimpleCheckpoint(tf.keras.callbacks.Callback):
     self.checkpoint_manager.save(checkpoint_number=step_counter)
 
 
-def set_session_config(enable_eager=False,
-                       enable_xla=False):
+def set_session_config(enable_xla=False):
   """Sets the session config."""
-  if is_v2_0():
-    set_config_v2(enable_xla=enable_xla)
-  else:
-    config = get_config_proto_v1(enable_xla=enable_xla)
-    if enable_eager:
-      tf.compat.v1.enable_eager_execution(config=config)
-    else:
-      sess = tf.compat.v1.Session(config=config)
-      tf.compat.v1.keras.backend.set_session(sess)
-
-
-def get_config_proto_v1(enable_xla=False):
-  """Return config proto according to flag settings, or None to use default."""
-  config = None
-  if enable_xla:
-    config = tf.compat.v1.ConfigProto()
-    config.graph_options.optimizer_options.global_jit_level = (
-        tf.OptimizerOptions.ON_2)
-  return config
-
-
-def set_config_v2(enable_xla=False):
-  """Config eager context according to flag values using TF 2.0 API."""
   if enable_xla:
     tf.config.optimizer.set_jit(True)
 
-
-def is_v2_0():
-  """Returns true if using tf 2.0."""
-  return tf2.enabled()
+# TODO(hongkuny): remove set_config_v2 globally.
+set_config_v2 = set_session_config
 
 
 def set_gpu_thread_mode_and_count(gpu_thread_mode,
diff --git a/official/utils/misc/model_helpers_test.py b/official/utils/misc/model_helpers_test.py
index f34a594d..9f2487e4 100644
--- a/official/utils/misc/model_helpers_test.py
+++ b/official/utils/misc/model_helpers_test.py
@@ -20,7 +20,6 @@ from __future__ import print_function
 
 import tensorflow as tf  # pylint: disable=g-bad-import-order
 
-from official.utils.misc import keras_utils
 from official.utils.misc import model_helpers
 
 
@@ -29,8 +28,7 @@ class PastStopThresholdTest(tf.test.TestCase):
 
   def setUp(self):
     super(PastStopThresholdTest, self).setUp()
-    if keras_utils.is_v2_0:
-      tf.compat.v1.disable_eager_execution()
+    tf.compat.v1.disable_eager_execution()
 
   def test_past_stop_threshold(self):
     """Tests for normal operating conditions."""
diff --git a/official/vision/image_classification/mnist_test.py b/official/vision/image_classification/mnist_test.py
index a9c233d8..c05efcfe 100644
--- a/official/vision/image_classification/mnist_test.py
+++ b/official/vision/image_classification/mnist_test.py
@@ -25,7 +25,6 @@ import tensorflow as tf
 
 from tensorflow.python.distribute import combinations
 from tensorflow.python.distribute import strategy_combinations
-from official.utils.misc import keras_utils
 from official.utils.testing import integration
 from official.vision.image_classification import mnist_main
 
@@ -57,8 +56,6 @@ class KerasMnistTest(tf.test.TestCase, parameterized.TestCase):
   @combinations.generate(eager_strategy_combinations())
   def test_end_to_end(self, distribution):
     """Test Keras MNIST model with `strategy`."""
-    config = keras_utils.get_config_proto_v1()
-    tf.compat.v1.enable_eager_execution(config=config)
 
     extra_flags = [
         "-train_epochs", "1",
diff --git a/official/vision/image_classification/resnet/resnet_ctl_imagenet_main.py b/official/vision/image_classification/resnet/resnet_ctl_imagenet_main.py
index dfd0684b..c128dc0b 100644
--- a/official/vision/image_classification/resnet/resnet_ctl_imagenet_main.py
+++ b/official/vision/image_classification/resnet/resnet_ctl_imagenet_main.py
@@ -109,7 +109,6 @@ def run(flags_obj):
     Dictionary of training and eval stats.
   """
   keras_utils.set_session_config(
-      enable_eager=flags_obj.enable_eager,
       enable_xla=flags_obj.enable_xla)
   performance.set_mixed_precision_policy(flags_core.get_tf_dtype(flags_obj))
 
