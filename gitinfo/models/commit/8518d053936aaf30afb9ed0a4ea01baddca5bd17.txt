commit 8518d053936aaf30afb9ed0a4ea01baddca5bd17
Author: pkulzc <lzc@google.com>
Date:   Tue May 12 11:41:07 2020 -0700

    Open source MnasFPN and minor fixes to OD API (#8484)
    
    310447280  by lzc:
    
        Internal change
    
    310420845  by Zhichao Lu:
    
        Open source the internal Context RCNN code.
    
    --
    310362339  by Zhichao Lu:
    
        Internal change
    
    310259448  by lzc:
    
        Update required TF version for OD API.
    
    --
    310252159  by Zhichao Lu:
    
        Port patch_ops_test to TF1/TF2 as TPUs.
    
    --
    310247180  by Zhichao Lu:
    
        Ignore keypoint heatmap loss in the regions/bounding boxes with target keypoint
        class but no valid keypoint annotations.
    
    --
    310178294  by Zhichao Lu:
    
        Opensource MnasFPN
        https://arxiv.org/abs/1912.01106
    
    --
    310094222  by lzc:
    
        Internal changes.
    
    --
    310085250  by lzc:
    
        Internal Change.
    
    --
    310016447  by huizhongc:
    
        Remove unrecognized classes from labeled_classes.
    
    --
    310009470  by rathodv:
    
        Mark batcher.py as TF1 only.
    
    --
    310001984  by rathodv:
    
        Update core/preprocessor.py to be compatible with TF1/TF2..
    
    --
    309455035  by Zhichao Lu:
    
        Makes the freezable_batch_norm_test run w/ v2 behavior.
    
        The main change is in v2 updates will happen right away when running batchnorm in training mode. So, we need to restore the weights between batchnorm calls to make sure the numerical checks all start from the same place.
    
    --
    309425881  by Zhichao Lu:
    
        Make TF1/TF2 optimizer builder tests explicit.
    
    --
    309408646  by Zhichao Lu:
    
        Make dataset builder tests TF1 and TF2 compatible.
    
    --
    309246305  by Zhichao Lu:
    
        Added the functionality of combining the person keypoints and object detection
        annotations in the binary that converts the COCO raw data to TfRecord.
    
    --
    309125076  by Zhichao Lu:
    
        Convert target_assigner_utils to TF1/TF2.
    
    --
    308966359  by huizhongc:
    
        Support SSD training with partially labeled groundtruth.
    
    --
    308937159  by rathodv:
    
        Update core/target_assigner.py to be compatible with TF1/TF2.
    
    --
    308774302  by Zhichao Lu:
    
        Internal
    
    --
    308732860  by rathodv:
    
        Make core/prefetcher.py  compatible with TF1 only.
    
    --
    308726984  by rathodv:
    
        Update core/multiclass_nms_test.py to be TF1/TF2 compatible.
    
    --
    308714718  by rathodv:
    
        Update core/region_similarity_calculator_test.py to be TF1/TF2 compatible.
    
    --
    308707960  by rathodv:
    
        Update core/minibatch_sampler_test.py to be TF1/TF2 compatible.
    
    --
    308700595  by rathodv:
    
        Update core/losses_test.py to be TF1/TF2 compatible and remove losses_test_v2.py
    
    --
    308361472  by rathodv:
    
        Update core/matcher_test.py to be TF1/TF2 compatible.
    
    --
    308335846  by Zhichao Lu:
    
        Updated the COCO evaluation logics and populated the groundturth area
        information through. This change matches the groundtruth format expected by the
        COCO keypoint evaluation.
    
    --
    308256924  by rathodv:
    
        Update core/keypoints_ops_test.py to be TF1/TF2 compatible.
    
    --
    308256826  by rathodv:
    
        Update class_agnostic_nms_test.py to be TF1/TF2 compatible.
    
    --
    308256112  by rathodv:
    
        Update box_list_ops_test.py to be TF1/TF2 compatible.
    
    --
    308159360  by Zhichao Lu:
    
        Internal change
    
    308145008  by Zhichao Lu:
    
        Added 'image/class/confidence' field in the TFExample decoder.
    
    --
    307651875  by rathodv:
    
        Refactor core/box_list.py to support TF1/TF2.
    
    --
    307651798  by rathodv:
    
        Modify box_coder.py base class to work with with TF1/TF2
    
    --
    307651652  by rathodv:
    
        Refactor core/balanced_positive_negative_sampler.py to support TF1/TF2.
    
    --
    307651571  by rathodv:
    
        Modify BoxCoders tests to use test_case:execute method to allow testing with TF1.X and TF2.X
    
    --
    307651480  by rathodv:
    
        Modify Matcher tests to use test_case:execute method to allow testing with TF1.X and TF2.X
    
    --
    307651409  by rathodv:
    
        Modify AnchorGenerator tests to use test_case:execute method to allow testing with TF1.X and TF2.X
    
    --
    307651314  by rathodv:
    
        Refactor model_builder to support TF1 or TF2 models based on TensorFlow version.
    
    --
    307092053  by Zhichao Lu:
    
        Use manager to save checkpoint.
    
    --
    307071352  by ronnyvotel:
    
        Fixing keypoint visibilities. Now by default, the visibility is marked True if the keypoint is labeled (regardless of whether it is visible or not).
        Also, if visibilities are not present in the dataset, they will be created based on whether the keypoint coordinates are finite (vis = True) or NaN (vis = False).
    
    --
    307069557  by Zhichao Lu:
    
        Internal change to add few fields related to postprocessing parameters in
        center_net.proto and populate those parameters to the keypoint postprocessing
        functions.
    
    --
    307012091  by Zhichao Lu:
    
        Make Adam Optimizer's epsilon proto configurable.
    
        Potential issue: tf.compat.v1's AdamOptimizer has a default epsilon on 1e-08 ([doc-link](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer))  whereas tf.keras's AdamOptimizer has default epsilon 1e-07 ([doc-link](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam))
    
    --
    306858598  by Zhichao Lu:
    
        Internal changes to update the CenterNet model:
        1) Modified eval job loss computation to avoid averaging over batches with zero loss.
        2) Updated CenterNet keypoint heatmap target assigner to apply box size to heatmap Guassian standard deviation.
        3) Updated the CenterNet meta arch keypoint losses computation to apply weights outside of loss function.
    
    --
    306731223  by jonathanhuang:
    
        Internal change.
    
    --
    306549183  by rathodv:
    
        Internal Update.
    
    --
    306542930  by rathodv:
    
        Internal Update
    
    --
    306322697  by rathodv:
    
        Internal.
    
    --
    305345036  by Zhichao Lu:
    
        Adding COCO Camera Traps Json to tf.Example beam code
    
    --
    304104869  by lzc:
    
        Internal changes.
    
    --
    304068971  by jonathanhuang:
    
        Internal change.
    
    --
    304050469  by Zhichao Lu:
    
        Internal change.
    
    --
    303880642  by huizhongc:
    
        Support parsing partially labeled groundtruth.
    
    --
    303841743  by Zhichao Lu:
    
        Deprecate nms_on_host in SSDMetaArch.
    
    --
    303803204  by rathodv:
    
        Internal change.
    
    --
    303793895  by jonathanhuang:
    
        Internal change.
    
    --
    303467631  by rathodv:
    
        Py3 update for detection inference test.
    
    --
    303444542  by rathodv:
    
        Py3 update to metrics module
    
    --
    303421960  by rathodv:
    
        Update json_utils to python3.
    
    --
    302787583  by ronnyvotel:
    
        Coco results generator for submission to the coco test server.
    
    --
    302719091  by Zhichao Lu:
    
        Internal change to add the ResNet50 image feature extractor for CenterNet model.
    
    --
    302116230  by Zhichao Lu:
    
        Added the functions to overlay the heatmaps with images in visualization util
        library.
    
    --
    301888316  by Zhichao Lu:
    
        Fix checkpoint_filepath not defined error.
    
    --
    301840312  by ronnyvotel:
    
        Adding keypoint_scores to visualizations.
    
    --
    301683475  by ronnyvotel:
    
        Introducing the ability to preprocess `keypoint_visibilities`.
    
        Some data augmentation ops such as random crop can filter instances and keypoints. It's important to also filter keypoint visibilities, so that the groundtruth tensors are always in alignment.
    
    --
    301532344  by Zhichao Lu:
    
        Don't use tf.divide since "Quantization not yet supported for op: DIV"
    
    --
    301480348  by ronnyvotel:
    
        Introducing keypoint evaluation into model lib v2.
        Also, making some fixes to coco keypoint evaluation.
    
    --
    301454018  by Zhichao Lu:
    
        Added the image summary to visualize the train/eval input images and eval's
        prediction/groundtruth side-by-side image.
    
    --
    301317527  by Zhichao Lu:
    
        Updated the random_absolute_pad_image function in the preprocessor library to
        support the keypoints argument.
    
    --
    301300324  by Zhichao Lu:
    
        Apply name change(experimental_run_v2 -> run) for all callers in Tensorflow.
    
    --
    301297115  by ronnyvotel:
    
        Utility function for setting keypoint visibilities based on keypoint coordinates.
    
    --
    301248885  by Zhichao Lu:
    
        Allow MultiworkerMirroredStrategy(MWMS) use by adding checkpoint handling with temporary directories in model_lib_v2. Added missing WeakKeyDictionary cfer_fn_cache field in CollectiveAllReduceStrategyExtended.
    
    --
    301224559  by Zhichao Lu:
    
        ...1) Fixes model_lib to also use keypoints while preparing model groundtruth.
        ...2) Tests model_lib with newly added keypoint metrics config.
    
    --
    300836556  by Zhichao Lu:
    
        Internal changes to add keypoint estimation parameters in CenterNet proto.
    
    --
    300795208  by Zhichao Lu:
    
        Updated the eval_util library to populate the keypoint groundtruth to
        eval_dict.
    
    --
    299474766  by Zhichao Lu:
    
        ...Modifies eval_util to create Keypoint Evaluator objects when configured in eval config.
    
    --
    299453920  by Zhichao Lu:
    
        Add swish activation as a hyperperams option.
    
    --
    299240093  by ronnyvotel:
    
        Keypoint postprocessing for CenterNetMetaArch.
    
    --
    299176395  by Zhichao Lu:
    
        Internal change.
    
    --
    299135608  by Zhichao Lu:
    
        Internal changes to refactor the CenterNet model in preparation for keypoint estimation tasks.
    
    --
    298915482  by Zhichao Lu:
    
        Make dataset_builder aware of input_context for distributed training.
    
    --
    298713595  by Zhichao Lu:
    
        Handling data with negative size boxes.
    
    --
    298695964  by Zhichao Lu:
    
        Expose change_coordinate_frame as a config parameter; fix multiclass_scores optional field.
    
    --
    298492150  by Zhichao Lu:
    
        Rename optimizer_builder_test_v2.py -> optimizer_builder_v2_test.py
    
    --
    298476471  by Zhichao Lu:
    
        Internal changes to support CenterNet keypoint estimation.
    
    --
    298365851  by ronnyvotel:
    
        Fixing a bug where groundtruth_keypoint_weights were being padded with a dynamic dimension.
    
    --
    297843700  by Zhichao Lu:
    
        Internal change.
    
    --
    297706988  by lzc:
    
        Internal change.
    
    --
    297705287  by ronnyvotel:
    
        Creating the "snapping" behavior in CenterNet, where regressed keypoints are refined with updated candidate keypoints from a heatmap.
    
    --
    297700447  by Zhichao Lu:
    
        Improve checkpoint checking logic with TF2 loop.
    
    --
    297686094  by Zhichao Lu:
    
        Convert "import tensorflow as tf" to "import tensorflow.compat.v1".
    
    --
    297670468  by lzc:
    
        Internal change.
    
    --
    297241327  by Zhichao Lu:
    
        Convert "import tensorflow as tf" to "import tensorflow.compat.v1".
    
    --
    297205959  by Zhichao Lu:
    
        Internal changes to support refactored the centernet object detection target assigner into a separate library.
    
    --
    297143806  by Zhichao Lu:
    
        Convert "import tensorflow as tf" to "import tensorflow.compat.v1".
    
    --
    297129625  by Zhichao Lu:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297117070  by Zhichao Lu:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297030190  by Zhichao Lu:
    
        Add configuration options for visualizing keypoint edges
    
    --
    296359649  by Zhichao Lu:
    
        Support DepthwiseConv2dNative (of separable conv) in weight equalization loss.
    
    --
    296290582  by Zhichao Lu:
    
        Internal change.
    
    --
    296093857  by Zhichao Lu:
    
        Internal changes to add general target assigner utilities.
    
    --
    295975116  by Zhichao Lu:
    
        Fix visualize_boxes_and_labels_on_image_array to show max_boxes_to_draw correctly.
    
    --
    295819711  by Zhichao Lu:
    
        Adds a flag to visualize_boxes_and_labels_on_image_array to skip the drawing of axis aligned bounding boxes.
    
    --
    295811929  by Zhichao Lu:
    
        Keypoint support in random_square_crop_by_scale.
    
    --
    295788458  by rathodv:
    
        Remove unused checkpoint to reduce repo size on github
    
    --
    295787184  by Zhichao Lu:
    
        Enable visualization of edges between keypoints
    
    --
    295763508  by Zhichao Lu:
    
        [Context RCNN] Add an option to enable / disable cropping feature in the post
        process step in the meta archtecture.
    
    --
    295605344  by Zhichao Lu:
    
        internal change.
    
    --
    294926050  by ronnyvotel:
    
        Adding per-keypoint groundtruth weights. These weights are intended to be used as multipliers in a keypoint loss function.
    
        Groundtruth keypoint weights are constructed as follows:
        - Initialize the weight for each keypoint type based on user-specified weights in the input_reader proto
        - Mask out (i.e. make zero) all keypoint weights that are not visible.
    
    --
    294829061  by lzc:
    
        Internal change.
    
    --
    294566503  by Zhichao Lu:
    
        Changed internal CenterNet Model configuration.
    
    --
    294346662  by ronnyvotel:
    
        Using NaN values in keypoint coordinates that are not visible.
    
    --
    294333339  by Zhichao Lu:
    
        Change experimetna_distribute_dataset -> experimental_distribute_dataset_from_function
    
    --
    293928752  by Zhichao Lu:
    
        Internal change
    
    --
    293909384  by Zhichao Lu:
    
        Add capabilities to train 1024x1024 CenterNet models.
    
    --
    293637554  by ronnyvotel:
    
        Adding keypoint visibilities to TfExampleDecoder.
    
    --
    293501558  by lzc:
    
        Internal change.
    
    --
    293252851  by Zhichao Lu:
    
        Change tf.gfile.GFile to tf.io.gfile.GFile.
    
    --
    292730217  by Zhichao Lu:
    
        Internal change.
    
    --
    292456563  by lzc:
    
        Internal changes.
    
    --
    292355612  by Zhichao Lu:
    
        Use tf.gather and tf.scatter_nd instead of matrix ops.
    
    --
    292245265  by rathodv:
    
        Internal
    
    --
    291989323  by richardmunoz:
    
        Refactor out building a DataDecoder from building a tf.data.Dataset.
    
    --
    291950147  by Zhichao Lu:
    
        Flip bounding boxes in arbitrary shaped tensors.
    
    --
    291401052  by huizhongc:
    
        Fix multiscale grid anchor generator to allow fully convolutional inference. When exporting model with identity_resizer as image_resizer, there is an incorrect box offset on the detection results. We add the anchor offset to address this problem.
    
    --
    291298871  by Zhichao Lu:
    
        Py3 compatibility changes.
    
    --
    290957957  by Zhichao Lu:
    
        Hourglass feature extractor for CenterNet.
    
    --
    290564372  by Zhichao Lu:
    
        Internal change.
    
    --
    290155278  by rathodv:
    
        Remove Dataset Explorer.
    
    --
    290155153  by Zhichao Lu:
    
        Internal change
    
    --
    290122054  by Zhichao Lu:
    
        Unify the format in the faster_rcnn.proto
    
    --
    290116084  by Zhichao Lu:
    
        Deprecate tensorflow.contrib.
    
    --
    290100672  by Zhichao Lu:
    
        Update MobilenetV3 SSD candidates
    
    --
    289926392  by Zhichao Lu:
    
        Internal change
    
    --
    289553440  by Zhichao Lu:
    
        [Object Detection API] Fix the comments about the dimension of the rpn_box_encodings from 4-D to 3-D.
    
    --
    288994128  by lzc:
    
        Internal changes.
    
    --
    288942194  by lzc:
    
        Internal change.
    
    --
    288746124  by Zhichao Lu:
    
        Configurable channel mean/std. dev in CenterNet feature extractors.
    
    --
    288552509  by rathodv:
    
        Internal.
    
    --
    288541285  by rathodv:
    
        Internal update.
    
    --
    288396396  by Zhichao Lu:
    
        Make object detection import contrib explicitly
    
    --
    288255791  by rathodv:
    
        Internal
    
    --
    288078600  by Zhichao Lu:
    
        Fix model_lib_v2 test
    
    --
    287952244  by rathodv:
    
        Internal
    
    --
    287921774  by Zhichao Lu:
    
        internal change
    
    --
    287906173  by Zhichao Lu:
    
        internal change
    
    --
    287889407  by jonathanhuang:
    
        PY3 compatibility
    
    --
    287889042  by rathodv:
    
        Internal
    
    --
    287876178  by Zhichao Lu:
    
        Internal change.
    
    --
    287770490  by Zhichao Lu:
    
        Add CenterNet proto and builder
    
    --
    287694213  by Zhichao Lu:
    
        Support for running multiple steps per tf.function call.
    
    --
    287377183  by jonathanhuang:
    
        PY3 compatibility
    
    --
    287371344  by rathodv:
    
        Support loading keypoint labels and ids.
    
    --
    287368213  by rathodv:
    
        Add protos supporting keypoint evaluation.
    
    --
    286673200  by rathodv:
    
        dataset_tools PY3 migration
    
    --
    286635106  by Zhichao Lu:
    
        Update code for upcoming tf.contrib removal
    
    --
    286479439  by Zhichao Lu:
    
        Internal change
    
    --
    286311711  by Zhichao Lu:
    
        Skeleton of context model within TFODAPI
    
    --
    286005546  by Zhichao Lu:
    
        Fix Faster-RCNN training when using keep_aspect_ratio_resizer with pad_to_max_dimension
    
    --
    285906400  by derekjchow:
    
        Internal change
    
    --
    285822795  by Zhichao Lu:
    
        Add CenterNet meta arch target assigners.
    
    --
    285447238  by Zhichao Lu:
    
        Internal changes.
    
    --
    285016927  by Zhichao Lu:
    
        Make _dummy_computation a tf.function. This fixes breakage caused by
        cl/284256438
    
    --
    284827274  by Zhichao Lu:
    
        Convert to python 3.
    
    --
    284645593  by rathodv:
    
        Internal change
    
    --
    284639893  by rathodv:
    
        Add missing documentation for keypoints in eval_util.py.
    
    --
    284323712  by Zhichao Lu:
    
        Internal changes.
    
    --
    284295290  by Zhichao Lu:
    
        Updating input config proto and dataset builder to include context fields
    
        Updating standard_fields and tf_example_decoder to include context features
    
    --
    284226821  by derekjchow:
    
        Update exporter.
    
    --
    284211030  by Zhichao Lu:
    
        API changes in CenterNet informed by the experiments with hourlgass network.
    
    --
    284190451  by Zhichao Lu:
    
        Add support for CenterNet losses in protos and builders.
    
    --
    284093961  by lzc:
    
        Internal changes.
    
    --
    284028174  by Zhichao Lu:
    
        Internal change
    
    --
    284014719  by derekjchow:
    
        Do not pad top_down feature maps unnecessarily.
    
    --
    284005765  by Zhichao Lu:
    
        Add new pad_to_multiple_resizer
    
    --
    283858233  by Zhichao Lu:
    
        Make target assigner work when under tf.function.
    
    --
    283836611  by Zhichao Lu:
    
        Make config getters more general.
    
    --
    283808990  by Zhichao Lu:
    
        Internal change
    
    --
    283754588  by Zhichao Lu:
    
        Internal changes.
    
    --
    282460301  by Zhichao Lu:
    
        Add ability to restore v2 style checkpoints.
    
    --
    281605842  by lzc:
    
        Add option to disable loss computation in OD API eval job.
    
    --
    280298212  by Zhichao Lu:
    
        Add backwards compatible change
    
    --
    280237857  by Zhichao Lu:
    
        internal change
    
    --
    
    PiperOrigin-RevId: 310447280

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index 83823adc..7556f0a2 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -1,4 +1,4 @@
-![TensorFlow Requirement: 1.x](https://img.shields.io/badge/TensorFlow%20Requirement-1.x-brightgreen)
+![TensorFlow Requirement: 1.15](https://img.shields.io/badge/TensorFlow%20Requirement-1.15-brightgreen)
 ![TensorFlow 2 Not Supported](https://img.shields.io/badge/TensorFlow%202%20Not%20Supported-%E2%9C%95-red.svg)
 
 # Tensorflow Object Detection API
@@ -31,7 +31,7 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:l291WsrB-hQJ:scholar.go
 
 | Name | GitHub |
 | --- | --- |
-| Jonathan Huang | [jch1](https://github.com/jch1) | 
+| Jonathan Huang | [jch1](https://github.com/jch1) |
 | Vivek Rathod | [tombstone](https://github.com/tombstone) |
 | Ronny Votel | [ronnyvotel](https://github.com/ronnyvotel) |
 | Derek Chow | [derekjchow](https://github.com/derekjchow) |
@@ -40,7 +40,6 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:l291WsrB-hQJ:scholar.go
 | Alireza Fathi | [afathi3](https://github.com/afathi3) |
 | Zhichao Lu | [pkulzc](https://github.com/pkulzc) |
 
-
 ## Table of contents
 
 Setup:
@@ -105,6 +104,25 @@ reporting an issue.
 
 ## Release information
 
+### May 7th, 2020
+We have released a mobile model with the
+[MnasFPN head](https://arxiv.org/abs/1912.01106).
+
+
+* MnasFPN with MobileNet-V2 backbone is the most accurate (26.6 mAP at 183ms on
+Pixel 1) mobile detection model we have released to date. With depth-multiplier,
+MnasFPN with MobileNet-V2 backbone is 1.8 mAP higher than MobileNet-V3-Large
+with SSDLite (23.8 mAP vs 22.0 mAP) at similar latency (120ms) on Pixel 1.
+
+We have released model definition, model checkpoints trained on
+the COCO14 dataset and a converted TFLite model.
+
+<b>Thanks to contributors</b>: Bo Chen, Golnaz Ghiasi, Hanxiao Liu,
+Tsung-Yi Lin, Dmitry Kalenichenko, Hartwig Adam, Quoc Le, Zhichao Lu,
+Jonathan Huang.
+
+
+
 ### Nov 13th, 2019
 We have released MobileNetEdgeTPU SSDLite model.
 
diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
index 2f09771b..ff255fb3 100644
--- a/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
@@ -24,52 +24,51 @@ from object_detection.utils import test_case
 class FlexibleGridAnchorGeneratorTest(test_case.TestCase):
 
   def test_construct_single_anchor(self):
-    anchor_strides = [(32, 32),]
-    anchor_offsets = [(16, 16),]
-    base_sizes = [(128.0,)]
-    aspect_ratios = [(1.0,)]
-    im_height = 64
-    im_width = 64
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      anchor_strides = [(32, 32),]
+      anchor_offsets = [(16, 16),]
+      base_sizes = [(128.0,)]
+      aspect_ratios = [(1.0,)]
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
+    anchor_corners_out = self.execute(graph_fn, [])
     exp_anchor_corners = [[-48, -48, 80, 80],
                           [-48, -16, 80, 112],
                           [-16, -48, 112, 80],
                           [-16, -16, 112, 112]]
-    anchor_generator = fg.FlexibleGridAnchorGenerator(
-        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_single_anchor_unit_dimensions(self):
-    anchor_strides = [(32, 32),]
-    anchor_offsets = [(16, 16),]
-    base_sizes = [(32.0,)]
-    aspect_ratios = [(1.0,)]
-    im_height = 1
-    im_width = 1
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      anchor_strides = [(32, 32),]
+      anchor_offsets = [(16, 16),]
+      base_sizes = [(32.0,)]
+      aspect_ratios = [(1.0,)]
+      im_height = 1
+      im_width = 1
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
     # Positive offsets are produced.
     exp_anchor_corners = [[0, 0, 32, 32],
                           [0, 32, 32, 64],
                           [32, 0, 64, 32],
                           [32, 32, 64, 64]]
-
-    anchor_generator = fg.FlexibleGridAnchorGenerator(
-        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_normalized_anchors_fails_with_unit_dimensions(self):
     anchor_generator = fg.FlexibleGridAnchorGenerator(
@@ -80,27 +79,27 @@ class FlexibleGridAnchorGeneratorTest(test_case.TestCase):
           feature_map_shape_list=[(2, 2)], im_height=1, im_width=1)
 
   def test_construct_single_anchor_in_normalized_coordinates(self):
-    anchor_strides = [(32, 32),]
-    anchor_offsets = [(16, 16),]
-    base_sizes = [(128.0,)]
-    aspect_ratios = [(1.0,)]
-    im_height = 64
-    im_width = 128
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      anchor_strides = [(32, 32),]
+      anchor_offsets = [(16, 16),]
+      base_sizes = [(128.0,)]
+      aspect_ratios = [(1.0,)]
+      im_height = 64
+      im_width = 128
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=True)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
     exp_anchor_corners = [[-48./64, -48./128, 80./64, 80./128],
                           [-48./64, -16./128, 80./64, 112./128],
                           [-16./64, -48./128, 112./64, 80./128],
                           [-16./64, -16./128, 112./64, 112./128]]
-    anchor_generator = fg.FlexibleGridAnchorGenerator(
-        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
-        normalize_coordinates=True)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_num_anchors_per_location(self):
     anchor_strides = [(32, 32), (64, 64)]
@@ -115,29 +114,28 @@ class FlexibleGridAnchorGeneratorTest(test_case.TestCase):
     self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])
 
   def test_construct_single_anchor_dynamic_size(self):
-    anchor_strides = [(32, 32),]
-    anchor_offsets = [(0, 0),]
-    base_sizes = [(128.0,)]
-    aspect_ratios = [(1.0,)]
-    im_height = tf.constant(64)
-    im_width = tf.constant(64)
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      anchor_strides = [(32, 32),]
+      anchor_offsets = [(0, 0),]
+      base_sizes = [(128.0,)]
+      aspect_ratios = [(1.0,)]
+      im_height = tf.constant(64)
+      im_width = tf.constant(64)
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = fg.FlexibleGridAnchorGenerator(
+          base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
     # Zero offsets are used.
     exp_anchor_corners = [[-64, -64, 64, 64],
                           [-64, -32, 64, 96],
                           [-32, -64, 96, 64],
                           [-32, -32, 96, 96]]
-
-    anchor_generator = fg.FlexibleGridAnchorGenerator(
-        base_sizes, aspect_ratios, anchor_strides, anchor_offsets,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_single_anchor_with_odd_input_dimension(self):
 
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index 86007c99..171396f8 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -212,7 +212,7 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
     min_im_shape = tf.minimum(im_height, im_width)
     scale_height = min_im_shape / im_height
     scale_width = min_im_shape / im_width
-    if not tf.contrib.framework.is_tensor(self._base_anchor_size):
+    if not tf.is_tensor(self._base_anchor_size):
       base_anchor_size = [
           scale_height * tf.constant(self._base_anchor_size[0],
                                      dtype=tf.float32),
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
index cd2440a4..09f8f779 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -20,6 +20,8 @@ described in:
 T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
 """
 
+import tensorflow as tf
+
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.core import anchor_generator
 from object_detection.core import box_list_ops
@@ -85,8 +87,10 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
   def _generate(self, feature_map_shape_list, im_height=1, im_width=1):
     """Generates a collection of bounding boxes to be used as anchors.
 
-    Currently we require the input image shape to be statically defined.  That
-    is, im_height and im_width should be integers rather than tensors.
+    For training, we require the input image shape to be statically defined.
+    That is, im_height and im_width should be integers rather than tensors.
+    For inference, im_height and im_width can be either integers (for fixed
+    image size), or tensors (for arbitrary image size).
 
     Args:
       feature_map_shape_list: list of pairs of convnet layer resolutions in the
@@ -124,6 +128,9 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
           anchor_offset[0] = stride / 2.0
         if im_width % 2.0**level == 0 or im_width == 1:
           anchor_offset[1] = stride / 2.0
+      if tf.is_tensor(im_height) and tf.is_tensor(im_width):
+        anchor_offset[0] = stride / 2.0
+        anchor_offset[1] = stride / 2.0
       ag = grid_anchor_generator.GridAnchorGenerator(
           scales,
           aspect_ratios,
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
index 178705c1..7afc6e75 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -24,54 +24,55 @@ from object_detection.utils import test_case
 class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
 
   def test_construct_single_anchor(self):
-    min_level = 5
-    max_level = 5
-    anchor_scale = 4.0
-    aspect_ratios = [1.0]
-    scales_per_octave = 1
-    im_height = 64
-    im_width = 64
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      min_level = 5
+      max_level = 5
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
+
     exp_anchor_corners = [[-48, -48, 80, 80],
                           [-48, -16, 80, 112],
                           [-16, -48, 112, 80],
                           [-16, -16, 112, 112]]
-    anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_single_anchor_unit_dimensions(self):
-    min_level = 5
-    max_level = 5
-    anchor_scale = 1.0
-    aspect_ratios = [1.0]
-    scales_per_octave = 1
-    im_height = 1
-    im_width = 1
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      min_level = 5
+      max_level = 5
+      anchor_scale = 1.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 1
+      im_width = 1
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
+
     # Positive offsets are produced.
     exp_anchor_corners = [[0, 0, 32, 32],
                           [0, 32, 32, 64],
                           [32, 0, 64, 32],
                           [32, 32, 64, 64]]
-
-    anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_normalized_anchors_fails_with_unit_dimensions(self):
     anchor_generator = mg.MultiscaleGridAnchorGenerator(
@@ -82,28 +83,29 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
           feature_map_shape_list=[(2, 2)], im_height=1, im_width=1)
 
   def test_construct_single_anchor_in_normalized_coordinates(self):
-    min_level = 5
-    max_level = 5
-    anchor_scale = 4.0
-    aspect_ratios = [1.0]
-    scales_per_octave = 1
-    im_height = 64
-    im_width = 128
-    feature_map_shape_list = [(2, 2)]
+    def graph_fn():
+      min_level = 5
+      max_level = 5
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 64
+      im_width = 128
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=True)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
+
     exp_anchor_corners = [[-48./64, -48./128, 80./64, 80./128],
                           [-48./64, -16./128, 80./64, 112./128],
                           [-16./64, -48./128, 112./64, 80./128],
                           [-16./64, -16./128, 112./64, 112./128]]
-    anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        normalize_coordinates=True)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_num_anchors_per_location(self):
     min_level = 5
@@ -117,30 +119,34 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
     self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])
 
   def test_construct_single_anchor_dynamic_size(self):
-    min_level = 5
-    max_level = 5
-    anchor_scale = 4.0
-    aspect_ratios = [1.0]
-    scales_per_octave = 1
-    im_height = tf.constant(64)
-    im_width = tf.constant(64)
-    feature_map_shape_list = [(2, 2)]
-    # Zero offsets are used.
+    def graph_fn():
+      min_level = 5
+      max_level = 5
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = tf.constant(64)
+      im_width = tf.constant(64)
+      feature_map_shape_list = [(2, 2)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
+      return anchor_corners
+
     exp_anchor_corners = [[-64, -64, 64, 64],
                           [-64, -32, 64, 96],
                           [-32, -64, 96, 64],
                           [-32, -32, 96, 96]]
-
-    anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        normalize_coordinates=False)
-    anchors_list = anchor_generator.generate(
-        feature_map_shape_list, im_height=im_height, im_width=im_width)
-    anchor_corners = anchors_list[0].get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    # Add anchor offset.
+    anchor_offset = 2.0**5 / 2.0
+    exp_anchor_corners = [
+        [b + anchor_offset for b in a] for a in exp_anchor_corners
+    ]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_single_anchor_with_odd_input_dimension(self):
 
diff --git a/research/object_detection/box_coders/faster_rcnn_box_coder_test.py b/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
index b2135f06..0fed9fec 100644
--- a/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
+++ b/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
@@ -14,80 +14,99 @@
 # ==============================================================================
 
 """Tests for object_detection.box_coder.faster_rcnn_box_coder."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.box_coders import faster_rcnn_box_coder
 from object_detection.core import box_list
+from object_detection.utils import test_case
 
 
-class FasterRcnnBoxCoderTest(tf.test.TestCase):
+class FasterRcnnBoxCoderTest(test_case.TestCase):
 
   def test_get_correct_relative_codes_after_encoding(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
     expected_rel_codes = [[-0.5, -0.416666, -0.405465, -0.182321],
                           [-0.083333, -0.222222, -0.693147, -1.098612]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_relative_codes_after_encoding_with_scaling(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    scale_factors = [2, 3, 4, 5]
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
     expected_rel_codes = [[-1., -1.25, -1.62186, -0.911608],
                           [-0.166667, -0.666667, -2.772588, -5.493062]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
-        scale_factors=scale_factors)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      scale_factors = [2, 3, 4, 5]
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
+          scale_factors=scale_factors)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_boxes_after_decoding(self):
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [[-0.5, -0.416666, -0.405465, -0.182321],
-                 [-0.083333, -0.222222, -0.693147, -1.098612]]
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
+    rel_codes = np.array([[-0.5, -0.416666, -0.405465, -0.182321],
+                          [-0.083333, -0.222222, -0.693147, -1.098612]],
+                         np.float32)
     expected_boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      boxes_out, = sess.run([boxes.get()])
-      self.assertAllClose(boxes_out, expected_boxes)
+    def graph_fn(rel_codes, anchors):
+      anchors = box_list.BoxList(anchors)
+      coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
+      boxes = coder.decode(rel_codes, anchors)
+      return boxes.get()
+    boxes_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(boxes_out, expected_boxes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_boxes_after_decoding_with_scaling(self):
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [[-1., -1.25, -1.62186, -0.911608],
-                 [-0.166667, -0.666667, -2.772588, -5.493062]]
-    scale_factors = [2, 3, 4, 5]
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
+    rel_codes = np.array([[-1., -1.25, -1.62186, -0.911608],
+                          [-0.166667, -0.666667, -2.772588, -5.493062]],
+                         np.float32)
     expected_boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
-        scale_factors=scale_factors)
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      boxes_out, = sess.run([boxes.get()])
-      self.assertAllClose(boxes_out, expected_boxes)
+    def graph_fn(rel_codes, anchors):
+      scale_factors = [2, 3, 4, 5]
+      anchors = box_list.BoxList(anchors)
+      coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
+          scale_factors=scale_factors)
+      boxes = coder.decode(rel_codes, anchors).get()
+      return boxes
+    boxes_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(expected_boxes, boxes_out, rtol=1e-04,
+                        atol=1e-04)
 
   def test_very_small_Width_nan_after_encoding(self):
-    boxes = [[10.0, 10.0, 10.0000001, 20.0]]
-    anchors = [[15.0, 12.0, 30.0, 18.0]]
+    boxes = np.array([[10.0, 10.0, 10.0000001, 20.0]], np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0]], np.float32)
     expected_rel_codes = [[-0.833333, 0., -21.128731, 0.510826]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/box_coders/keypoint_box_coder_test.py b/research/object_detection/box_coders/keypoint_box_coder_test.py
index 330641e5..7c10cd2f 100644
--- a/research/object_detection/box_coders/keypoint_box_coder_test.py
+++ b/research/object_detection/box_coders/keypoint_box_coder_test.py
@@ -14,126 +14,137 @@
 # ==============================================================================
 
 """Tests for object_detection.box_coder.keypoint_box_coder."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.box_coders import keypoint_box_coder
 from object_detection.core import box_list
 from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
 
 
-class KeypointBoxCoderTest(tf.test.TestCase):
+class KeypointBoxCoderTest(test_case.TestCase):
 
   def test_get_correct_relative_codes_after_encoding(self):
-    boxes = [[10., 10., 20., 15.],
-             [0.2, 0.1, 0.5, 0.4]]
-    keypoints = [[[15., 12.], [10., 15.]],
-                 [[0.5, 0.3], [0.2, 0.4]]]
+    boxes = np.array([[10., 10., 20., 15.],
+                      [0.2, 0.1, 0.5, 0.4]], np.float32)
+    keypoints = np.array([[[15., 12.], [10., 15.]],
+                          [[0.5, 0.3], [0.2, 0.4]]], np.float32)
     num_keypoints = len(keypoints[0])
-    anchors = [[15., 12., 30., 18.],
-               [0.1, 0.0, 0.7, 0.9]]
+    anchors = np.array([[15., 12., 30., 18.],
+                        [0.1, 0.0, 0.7, 0.9]], np.float32)
     expected_rel_codes = [
         [-0.5, -0.416666, -0.405465, -0.182321,
          -0.5, -0.5, -0.833333, 0.],
         [-0.083333, -0.222222, -0.693147, -1.098612,
          0.166667, -0.166667, -0.333333, -0.055556]
     ]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, keypoints, anchors):
+      boxes = box_list.BoxList(boxes)
+      boxes.add_field(fields.BoxListFields.keypoints, keypoints)
+      anchors = box_list.BoxList(anchors)
+      coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, keypoints, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_relative_codes_after_encoding_with_scaling(self):
-    boxes = [[10., 10., 20., 15.],
-             [0.2, 0.1, 0.5, 0.4]]
-    keypoints = [[[15., 12.], [10., 15.]],
-                 [[0.5, 0.3], [0.2, 0.4]]]
+    boxes = np.array([[10., 10., 20., 15.],
+                      [0.2, 0.1, 0.5, 0.4]], np.float32)
+    keypoints = np.array([[[15., 12.], [10., 15.]],
+                          [[0.5, 0.3], [0.2, 0.4]]], np.float32)
     num_keypoints = len(keypoints[0])
-    anchors = [[15., 12., 30., 18.],
-               [0.1, 0.0, 0.7, 0.9]]
-    scale_factors = [2, 3, 4, 5]
+    anchors = np.array([[15., 12., 30., 18.],
+                        [0.1, 0.0, 0.7, 0.9]], np.float32)
     expected_rel_codes = [
         [-1., -1.25, -1.62186, -0.911608,
          -1.0, -1.5, -1.666667, 0.],
         [-0.166667, -0.666667, -2.772588, -5.493062,
          0.333333, -0.5, -0.666667, -0.166667]
     ]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = keypoint_box_coder.KeypointBoxCoder(
-        num_keypoints, scale_factors=scale_factors)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, keypoints, anchors):
+      scale_factors = [2, 3, 4, 5]
+      boxes = box_list.BoxList(boxes)
+      boxes.add_field(fields.BoxListFields.keypoints, keypoints)
+      anchors = box_list.BoxList(anchors)
+      coder = keypoint_box_coder.KeypointBoxCoder(
+          num_keypoints, scale_factors=scale_factors)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, keypoints, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_boxes_after_decoding(self):
-    anchors = [[15., 12., 30., 18.],
-               [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [
+    anchors = np.array([[15., 12., 30., 18.],
+                        [0.1, 0.0, 0.7, 0.9]], np.float32)
+    rel_codes = np.array([
         [-0.5, -0.416666, -0.405465, -0.182321,
          -0.5, -0.5, -0.833333, 0.],
         [-0.083333, -0.222222, -0.693147, -1.098612,
          0.166667, -0.166667, -0.333333, -0.055556]
-    ]
+    ], np.float32)
     expected_boxes = [[10., 10., 20., 15.],
                       [0.2, 0.1, 0.5, 0.4]]
     expected_keypoints = [[[15., 12.], [10., 15.]],
                           [[0.5, 0.3], [0.2, 0.4]]]
     num_keypoints = len(expected_keypoints[0])
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      boxes_out, keypoints_out = sess.run(
-          [boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)])
-      self.assertAllClose(boxes_out, expected_boxes)
-      self.assertAllClose(keypoints_out, expected_keypoints)
+    def graph_fn(rel_codes, anchors):
+      anchors = box_list.BoxList(anchors)
+      coder = keypoint_box_coder.KeypointBoxCoder(num_keypoints)
+      boxes = coder.decode(rel_codes, anchors)
+      return boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)
+    boxes_out, keypoints_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(keypoints_out, expected_keypoints, rtol=1e-04,
+                        atol=1e-04)
+    self.assertAllClose(boxes_out, expected_boxes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_get_correct_boxes_after_decoding_with_scaling(self):
-    anchors = [[15., 12., 30., 18.],
-               [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [
+    anchors = np.array([[15., 12., 30., 18.],
+                        [0.1, 0.0, 0.7, 0.9]], np.float32)
+    rel_codes = np.array([
         [-1., -1.25, -1.62186, -0.911608,
          -1.0, -1.5, -1.666667, 0.],
         [-0.166667, -0.666667, -2.772588, -5.493062,
          0.333333, -0.5, -0.666667, -0.166667]
-    ]
-    scale_factors = [2, 3, 4, 5]
+    ], np.float32)
     expected_boxes = [[10., 10., 20., 15.],
                       [0.2, 0.1, 0.5, 0.4]]
     expected_keypoints = [[[15., 12.], [10., 15.]],
                           [[0.5, 0.3], [0.2, 0.4]]]
     num_keypoints = len(expected_keypoints[0])
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = keypoint_box_coder.KeypointBoxCoder(
-        num_keypoints, scale_factors=scale_factors)
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      boxes_out, keypoints_out = sess.run(
-          [boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)])
-      self.assertAllClose(boxes_out, expected_boxes)
-      self.assertAllClose(keypoints_out, expected_keypoints)
+    def graph_fn(rel_codes, anchors):
+      scale_factors = [2, 3, 4, 5]
+      anchors = box_list.BoxList(anchors)
+      coder = keypoint_box_coder.KeypointBoxCoder(
+          num_keypoints, scale_factors=scale_factors)
+      boxes = coder.decode(rel_codes, anchors)
+      return boxes.get(), boxes.get_field(fields.BoxListFields.keypoints)
+    boxes_out, keypoints_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(keypoints_out, expected_keypoints, rtol=1e-04,
+                        atol=1e-04)
+    self.assertAllClose(boxes_out, expected_boxes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_very_small_width_nan_after_encoding(self):
-    boxes = [[10., 10., 10.0000001, 20.]]
-    keypoints = [[[10., 10.], [10.0000001, 20.]]]
-    anchors = [[15., 12., 30., 18.]]
+    boxes = np.array([[10., 10., 10.0000001, 20.]], np.float32)
+    keypoints = np.array([[[10., 10.], [10.0000001, 20.]]], np.float32)
+    anchors = np.array([[15., 12., 30., 18.]], np.float32)
     expected_rel_codes = [[-0.833333, 0., -21.128731, 0.510826,
                            -0.833333, -0.833333, -0.833333, 0.833333]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    boxes.add_field(fields.BoxListFields.keypoints, tf.constant(keypoints))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = keypoint_box_coder.KeypointBoxCoder(2)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      rel_codes_out, = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, keypoints, anchors):
+      boxes = box_list.BoxList(boxes)
+      boxes.add_field(fields.BoxListFields.keypoints, keypoints)
+      anchors = box_list.BoxList(anchors)
+      coder = keypoint_box_coder.KeypointBoxCoder(2)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, keypoints, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/box_coders/mean_stddev_box_coder_test.py b/research/object_detection/box_coders/mean_stddev_box_coder_test.py
index 3e0eba93..e4319bea 100644
--- a/research/object_detection/box_coders/mean_stddev_box_coder_test.py
+++ b/research/object_detection/box_coders/mean_stddev_box_coder_test.py
@@ -14,40 +14,47 @@
 # ==============================================================================
 
 """Tests for object_detection.box_coder.mean_stddev_boxcoder."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_list
+from object_detection.utils import test_case
 
 
-class MeanStddevBoxCoderTest(tf.test.TestCase):
+class MeanStddevBoxCoderTest(test_case.TestCase):
 
   def testGetCorrectRelativeCodesAfterEncoding(self):
-    box_corners = [[0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.5, 0.5]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
+    boxes = np.array([[0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.5, 0.5]], np.float32)
+    anchors = np.array([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]], np.float32)
     expected_rel_codes = [[0.0, 0.0, 0.0, 0.0], [-5.0, -5.0, -5.0, -3.0]]
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]])
-    priors = box_list.BoxList(prior_means)
 
-    coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-    rel_codes = coder.encode(boxes, priors)
-    with self.test_session() as sess:
-      rel_codes_out = sess.run(rel_codes)
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      anchors = box_list.BoxList(anchors)
+      boxes = box_list.BoxList(boxes)
+      coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def testGetCorrectBoxesAfterDecoding(self):
-    rel_codes = tf.constant([[0.0, 0.0, 0.0, 0.0], [-5.0, -5.0, -5.0, -3.0]])
+    rel_codes = np.array([[0.0, 0.0, 0.0, 0.0], [-5.0, -5.0, -5.0, -3.0]],
+                         np.float32)
     expected_box_corners = [[0.0, 0.0, 0.5, 0.5], [0.0, 0.0, 0.5, 0.5]]
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]])
-    priors = box_list.BoxList(prior_means)
-
-    coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-    decoded_boxes = coder.decode(rel_codes, priors)
-    decoded_box_corners = decoded_boxes.get()
-    with self.test_session() as sess:
-      decoded_out = sess.run(decoded_box_corners)
-      self.assertAllClose(decoded_out, expected_box_corners)
+    anchors = np.array([[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 0.8]], np.float32)
+
+    def graph_fn(rel_codes, anchors):
+      anchors = box_list.BoxList(anchors)
+      coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
+      decoded_boxes = coder.decode(rel_codes, anchors).get()
+      return decoded_boxes
+
+    decoded_boxes_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(decoded_boxes_out, expected_box_corners, rtol=1e-04,
+                        atol=1e-04)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/box_coders/square_box_coder_test.py b/research/object_detection/box_coders/square_box_coder_test.py
index 7f739c6b..d7ef6847 100644
--- a/research/object_detection/box_coders/square_box_coder_test.py
+++ b/research/object_detection/box_coders/square_box_coder_test.py
@@ -14,83 +14,100 @@
 # ==============================================================================
 
 """Tests for object_detection.box_coder.square_box_coder."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.box_coders import square_box_coder
 from object_detection.core import box_list
+from object_detection.utils import test_case
 
 
-class SquareBoxCoderTest(tf.test.TestCase):
+class SquareBoxCoderTest(test_case.TestCase):
 
   def test_correct_relative_codes_with_default_scale(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    scale_factors = None
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
     expected_rel_codes = [[-0.790569, -0.263523, -0.293893],
                           [-0.068041, -0.272166, -0.89588]]
-
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      (rel_codes_out,) = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      scale_factors = None
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_correct_relative_codes_with_non_default_scale(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    scale_factors = [2, 3, 4]
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
     expected_rel_codes = [[-1.581139, -0.790569, -1.175573],
                           [-0.136083, -0.816497, -3.583519]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      (rel_codes_out,) = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      scale_factors = [2, 3, 4]
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-03,
+                        atol=1e-03)
 
   def test_correct_relative_codes_with_small_width(self):
-    boxes = [[10.0, 10.0, 10.0000001, 20.0]]
-    anchors = [[15.0, 12.0, 30.0, 18.0]]
-    scale_factors = None
+    boxes = np.array([[10.0, 10.0, 10.0000001, 20.0]], np.float32)
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0]], np.float32)
     expected_rel_codes = [[-1.317616, 0., -20.670586]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
-    rel_codes = coder.encode(boxes, anchors)
-    with self.test_session() as sess:
-      (rel_codes_out,) = sess.run([rel_codes])
-      self.assertAllClose(rel_codes_out, expected_rel_codes)
+    def graph_fn(boxes, anchors):
+      scale_factors = None
+      boxes = box_list.BoxList(boxes)
+      anchors = box_list.BoxList(anchors)
+      coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
+      rel_codes = coder.encode(boxes, anchors)
+      return rel_codes
+    rel_codes_out = self.execute(graph_fn, [boxes, anchors])
+    self.assertAllClose(rel_codes_out, expected_rel_codes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_correct_boxes_with_default_scale(self):
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [[-0.5, -0.416666, -0.405465],
-                 [-0.083333, -0.222222, -0.693147]]
-    scale_factors = None
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
+    rel_codes = np.array([[-0.5, -0.416666, -0.405465],
+                          [-0.083333, -0.222222, -0.693147]], np.float32)
     expected_boxes = [[14.594306, 7.884875, 20.918861, 14.209432],
                       [0.155051, 0.102989, 0.522474, 0.470412]]
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      (boxes_out,) = sess.run([boxes.get()])
-      self.assertAllClose(boxes_out, expected_boxes)
+    def graph_fn(rel_codes, anchors):
+      scale_factors = None
+      anchors = box_list.BoxList(anchors)
+      coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
+      boxes = coder.decode(rel_codes, anchors).get()
+      return boxes
+    boxes_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(boxes_out, expected_boxes, rtol=1e-04,
+                        atol=1e-04)
 
   def test_correct_boxes_with_non_default_scale(self):
-    anchors = [[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]]
-    rel_codes = [[-1., -1.25, -1.62186], [-0.166667, -0.666667, -2.772588]]
-    scale_factors = [2, 3, 4]
+    anchors = np.array([[15.0, 12.0, 30.0, 18.0], [0.1, 0.0, 0.7, 0.9]],
+                       np.float32)
+    rel_codes = np.array(
+        [[-1., -1.25, -1.62186], [-0.166667, -0.666667, -2.772588]], np.float32)
     expected_boxes = [[14.594306, 7.884875, 20.918861, 14.209432],
                       [0.155051, 0.102989, 0.522474, 0.470412]]
-    anchors = box_list.BoxList(tf.constant(anchors))
-    coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
-    boxes = coder.decode(rel_codes, anchors)
-    with self.test_session() as sess:
-      (boxes_out,) = sess.run([boxes.get()])
-      self.assertAllClose(boxes_out, expected_boxes)
+    def graph_fn(rel_codes, anchors):
+      scale_factors = [2, 3, 4]
+      anchors = box_list.BoxList(anchors)
+      coder = square_box_coder.SquareBoxCoder(scale_factors=scale_factors)
+      boxes = coder.decode(rel_codes, anchors).get()
+      return boxes
+    boxes_out = self.execute(graph_fn, [rel_codes, anchors])
+    self.assertAllClose(boxes_out, expected_boxes, rtol=1e-04,
+                        atol=1e-04)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/anchor_generator_builder.py b/research/object_detection/builders/anchor_generator_builder.py
index 81219a1b..7880210a 100644
--- a/research/object_detection/builders/anchor_generator_builder.py
+++ b/research/object_detection/builders/anchor_generator_builder.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,10 @@
 
 """A function to build an object detection anchor generator from config."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+from six.moves import zip
 from object_detection.anchor_generators import flexible_grid_anchor_generator
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.anchor_generators import multiple_grid_anchor_generator
@@ -58,12 +63,14 @@ def build(anchor_generator_config):
     ssd_anchor_generator_config = anchor_generator_config.ssd_anchor_generator
     anchor_strides = None
     if ssd_anchor_generator_config.height_stride:
-      anchor_strides = zip(ssd_anchor_generator_config.height_stride,
-                           ssd_anchor_generator_config.width_stride)
+      anchor_strides = list(
+          zip(ssd_anchor_generator_config.height_stride,
+              ssd_anchor_generator_config.width_stride))
     anchor_offsets = None
     if ssd_anchor_generator_config.height_offset:
-      anchor_offsets = zip(ssd_anchor_generator_config.height_offset,
-                           ssd_anchor_generator_config.width_offset)
+      anchor_offsets = list(
+          zip(ssd_anchor_generator_config.height_offset,
+              ssd_anchor_generator_config.width_offset))
     return multiple_grid_anchor_generator.create_ssd_anchors(
         num_layers=ssd_anchor_generator_config.num_layers,
         min_scale=ssd_anchor_generator_config.min_scale,
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index 4b2cf5df..0049f9af 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,14 @@
 
 """Tests for anchor_generator_builder."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import math
 
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
 
 from google.protobuf import text_format
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 2494bc30..0835fbb9 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -487,8 +488,8 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
     self.assertEqual(box_head._box_code_size, 4)
-    self.assertTrue(
-        mask_rcnn_box_predictor.MASK_PREDICTIONS in third_stage_heads)
+    self.assertIn(
+        mask_rcnn_box_predictor.MASK_PREDICTIONS, third_stage_heads)
     self.assertEqual(
         third_stage_heads[mask_rcnn_box_predictor.MASK_PREDICTIONS]
         ._mask_prediction_conv_depth, 512)
@@ -527,8 +528,8 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
     self.assertEqual(box_head._box_code_size, 4)
-    self.assertTrue(
-        mask_rcnn_box_predictor.MASK_PREDICTIONS in third_stage_heads)
+    self.assertIn(
+        mask_rcnn_box_predictor.MASK_PREDICTIONS, third_stage_heads)
     self.assertEqual(
         third_stage_heads[mask_rcnn_box_predictor.MASK_PREDICTIONS]
         ._mask_prediction_conv_depth, 512)
diff --git a/research/object_detection/builders/calibration_builder_test.py b/research/object_detection/builders/calibration_builder_test.py
index 05971e69..e6752635 100644
--- a/research/object_detection/builders/calibration_builder_test.py
+++ b/research/object_detection/builders/calibration_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,12 @@
 
 """Tests for calibration_builder."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 import numpy as np
 from scipy import interpolate
+from six.moves import zip
 import tensorflow as tf
 from object_detection.builders import calibration_builder
 from object_detection.protos import calibration_pb2
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 158abb19..30171cce 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -21,10 +22,15 @@ Note: If users wishes to also use their own InputReaders with the Object
 Detection configuration framework, they should define their own builder function
 that wraps the build function.
 """
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import functools
 import tensorflow as tf
 
-from object_detection.data_decoders import tf_example_decoder
+from tensorflow.contrib import data as tf_data
+from object_detection.builders import decoder_builder
 from object_detection.protos import input_reader_pb2
 
 
@@ -45,14 +51,20 @@ def make_initializable_iterator(dataset):
   return iterator
 
 
-def read_dataset(file_read_func, input_files, config):
+def read_dataset(file_read_func, input_files, config,
+                 filename_shard_fn=None):
   """Reads a dataset, and handles repetition and shuffling.
 
   Args:
-    file_read_func: Function to use in tf.contrib.data.parallel_interleave, to
+    file_read_func: Function to use in tf_data.parallel_interleave, to
       read every individual file into a tf.data.Dataset.
     input_files: A list of file paths to read.
     config: A input_reader_builder.InputReader object.
+    filename_shard_fn: optional, A funciton used to shard filenames across
+      replicas. This function takes as input a TF dataset of filenames and
+      is expected to return its sharded version. It is useful when the
+      dataset is being loaded on one of possibly many replicas and we want
+      to evenly shard the files between the replicas.
 
   Returns:
     A tf.data.Dataset of (undecoded) tf-records based on config.
@@ -77,9 +89,12 @@ def read_dataset(file_read_func, input_files, config):
   elif num_readers > 1:
     tf.logging.warning('`shuffle` is false, but the input data stream is '
                        'still slightly shuffled since `num_readers` > 1.')
+  if filename_shard_fn:
+    filename_dataset = filename_shard_fn(filename_dataset)
+
   filename_dataset = filename_dataset.repeat(config.num_epochs or None)
   records_dataset = filename_dataset.apply(
-      tf.contrib.data.parallel_interleave(
+      tf_data.parallel_interleave(
           file_read_func,
           cycle_length=num_readers,
           block_length=config.read_block_length,
@@ -89,7 +104,21 @@ def read_dataset(file_read_func, input_files, config):
   return records_dataset
 
 
-def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
+def shard_function_for_context(input_context):
+  """Returns a function that shards filenames based on the input context."""
+
+  if input_context is None:
+    return None
+
+  def shard_fn(dataset):
+    return dataset.shard(
+        input_context.num_input_pipelines, input_context.input_pipeline_id)
+
+  return shard_fn
+
+
+def build(input_reader_config, batch_size=None, transform_input_data_fn=None,
+          input_context=None):
   """Builds a tf.data.Dataset.
 
   Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all
@@ -100,6 +129,9 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
     batch_size: Batch size. If batch size is None, no batching is performed.
     transform_input_data_fn: Function to apply transformation to all records,
       or None if no extra decoding is required.
+    input_context: optional, A tf.distribute.InputContext object used to
+      shard filenames and compute per-replica batch_size when this function
+      is being called per-replica.
 
   Returns:
     A tf.data.Dataset based on the input_reader_config.
@@ -112,23 +144,14 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
     raise ValueError('input_reader_config not of type '
                      'input_reader_pb2.InputReader.')
 
+  decoder = decoder_builder.build(input_reader_config)
+
   if input_reader_config.WhichOneof('input_reader') == 'tf_record_input_reader':
     config = input_reader_config.tf_record_input_reader
     if not config.input_path:
       raise ValueError('At least one input path must be specified in '
                        '`input_reader_config`.')
 
-    label_map_proto_file = None
-    if input_reader_config.HasField('label_map_path'):
-      label_map_proto_file = input_reader_config.label_map_path
-    decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=input_reader_config.load_instance_masks,
-        load_multiclass_scores=input_reader_config.load_multiclass_scores,
-        instance_mask_type=input_reader_config.mask_type,
-        label_map_proto_file=label_map_proto_file,
-        use_display_name=input_reader_config.use_display_name,
-        num_additional_channels=input_reader_config.num_additional_channels)
-
     def process_fn(value):
       """Sets up tf graph that decodes, transforms and pads input data."""
       processed_tensors = decoder.decode(value)
@@ -136,9 +159,13 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
         processed_tensors = transform_input_data_fn(processed_tensors)
       return processed_tensors
 
+    shard_fn = shard_function_for_context(input_context)
+    if input_context is not None:
+      batch_size = input_context.get_per_replica_batch_size(batch_size)
+
     dataset = read_dataset(
         functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000),
-        config.input_path[:], input_reader_config)
+        config.input_path[:], input_reader_config, filename_shard_fn=shard_fn)
     if input_reader_config.sample_1_of_n_examples > 1:
       dataset = dataset.shard(input_reader_config.sample_1_of_n_examples, 0)
     # TODO(rathodv): make batch size a required argument once the old binaries
@@ -155,7 +182,7 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
     dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
     if batch_size:
       dataset = dataset.apply(
-          tf.contrib.data.batch_and_drop_remainder(batch_size))
+          tf_data.batch_and_drop_remainder(batch_size))
     dataset = dataset.prefetch(input_reader_config.num_prefetch_batches)
     return dataset
 
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index 78677310..5febeee6 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,8 +15,13 @@
 # ==============================================================================
 """Tests for dataset_builder."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import os
 import numpy as np
+from six.moves import range
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -24,25 +30,63 @@ from object_detection.builders import dataset_builder
 from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
+from object_detection.utils import test_case
 
 
-class DatasetBuilderTest(tf.test.TestCase):
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import lookup as contrib_lookup
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
-  def create_tf_record(self, has_additional_channels=False, num_examples=1):
-    path = os.path.join(self.get_temp_dir(), 'tfrecord')
-    writer = tf.python_io.TFRecordWriter(path)
 
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
-    additional_channels_tensor = np.random.randint(
-        255, size=(4, 5, 1)).astype(np.uint8)
-    flat_mask = (4 * 5) * [1.0]
-    with self.test_session():
-      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
+def get_iterator_next_for_testing(dataset, is_tf2):
+
+  # In TF2, lookup tables are not supported in one shot iterators, but
+  # initialization is implicit.
+  if is_tf2:
+    return dataset.make_initializable_iterator().get_next()
+  # In TF1, we use one shot iterator because it does not require running
+  # a separate init op.
+  else:
+    return dataset.make_one_shot_iterator().get_next()
+
+
+class DatasetBuilderTest(test_case.TestCase):
+
+  def create_tf_record(self, has_additional_channels=False, num_shards=1,
+                       num_examples_per_shard=1):
+
+    def dummy_jpeg_fn():
+      image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+      additional_channels_tensor = np.random.randint(
+          255, size=(4, 5, 1)).astype(np.uint8)
+      encoded_jpeg = tf.image.encode_jpeg(image_tensor)
       encoded_additional_channels_jpeg = tf.image.encode_jpeg(
-          tf.constant(additional_channels_tensor)).eval()
-      for i in range(num_examples):
+          additional_channels_tensor)
+
+      return encoded_jpeg, encoded_additional_channels_jpeg
+
+    encoded_jpeg, encoded_additional_channels_jpeg = self.execute(
+        dummy_jpeg_fn, [])
+
+    tmp_dir = self.get_temp_dir()
+    flat_mask = (4 * 5) * [1.0]
+
+    for i in range(num_shards):
+      path = os.path.join(tmp_dir, '%05d.tfrecord' % i)
+      writer = tf.python_io.TFRecordWriter(path)
+
+      for j in range(num_examples_per_shard):
+        if num_shards > 1:
+          source_id = (str(i) + '_' + str(j)).encode()
+        else:
+          source_id = str(j).encode()
+
         features = {
-            'image/source_id': dataset_util.bytes_feature(str(i)),
+            'image/source_id': dataset_util.bytes_feature(source_id),
             'image/encoded': dataset_util.bytes_feature(encoded_jpeg),
             'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
             'image/height': dataset_util.int64_feature(4),
@@ -54,15 +98,18 @@ class DatasetBuilderTest(tf.test.TestCase):
             'image/object/class/label': dataset_util.int64_list_feature([2]),
             'image/object/mask': dataset_util.float_list_feature(flat_mask),
         }
+
         if has_additional_channels:
           additional_channels_key = 'image/additional_channels/encoded'
           features[additional_channels_key] = dataset_util.bytes_list_feature(
               [encoded_additional_channels_jpeg] * 2)
+
         example = tf.train.Example(features=tf.train.Features(feature=features))
         writer.write(example.SerializeToString())
+
       writer.close()
 
-    return path
+    return os.path.join(self.get_temp_dir(), '?????.tfrecord')
 
   def test_build_tf_record_input_reader(self):
     tf_record_path = self.create_tf_record()
@@ -76,19 +123,21 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
+    def graph_fn():
+      return get_iterator_next_for_testing(
+          dataset_builder.build(input_reader_proto, batch_size=1),
+          self.is_tf2())
 
-    self.assertTrue(
-        fields.InputDataFields.groundtruth_instance_masks not in output_dict)
-    self.assertEquals((1, 4, 5, 3),
-                      output_dict[fields.InputDataFields.image].shape)
+    output_dict = self.execute(graph_fn, [])
+
+    self.assertNotIn(
+        fields.InputDataFields.groundtruth_instance_masks, output_dict)
+    self.assertEqual((1, 4, 5, 3),
+                     output_dict[fields.InputDataFields.image].shape)
     self.assertAllEqual([[2]],
                         output_dict[fields.InputDataFields.groundtruth_classes])
-    self.assertEquals(
+    self.assertEqual(
         (1, 1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
     self.assertAllEqual(
         [0.0, 0.0, 1.0, 1.0],
@@ -107,11 +156,14 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
+    def graph_fn():
+      return get_iterator_next_for_testing(
+          dataset_builder.build(input_reader_proto, batch_size=1),
+          self.is_tf2()
+      )
+
+    output_dict = self.execute(graph_fn, [])
     self.assertAllEqual(
         (1, 1, 4, 5),
         output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
@@ -134,14 +186,14 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(
-            input_reader_proto,
-            transform_input_data_fn=one_hot_class_encoding_fn,
-            batch_size=2)).get_next()
+    def graph_fn():
+      return dataset_builder.make_initializable_iterator(
+          dataset_builder.build(
+              input_reader_proto,
+              transform_input_data_fn=one_hot_class_encoding_fn,
+              batch_size=2)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
+    output_dict = self.execute(graph_fn, [])
 
     self.assertAllEqual([2, 4, 5, 3],
                         output_dict[fields.InputDataFields.image].shape)
@@ -172,14 +224,14 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(
-            input_reader_proto,
-            transform_input_data_fn=one_hot_class_encoding_fn,
-            batch_size=2)).get_next()
+    def graph_fn():
+      return dataset_builder.make_initializable_iterator(
+          dataset_builder.build(
+              input_reader_proto,
+              transform_input_data_fn=one_hot_class_encoding_fn,
+              batch_size=2)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
+    output_dict = self.execute(graph_fn, [])
 
     self.assertAllEqual(
         [2, 1, 4, 5],
@@ -197,7 +249,7 @@ class DatasetBuilderTest(tf.test.TestCase):
       dataset_builder.build(input_reader_proto, batch_size=1)
 
   def test_sample_all_data(self):
-    tf_record_path = self.create_tf_record(num_examples=2)
+    tf_record_path = self.create_tf_record(num_examples_per_shard=2)
 
     input_reader_text_proto = """
       shuffle: false
@@ -209,17 +261,22 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
-      self.assertAllEqual(['0'], output_dict[fields.InputDataFields.source_id])
-      output_dict = sess.run(tensor_dict)
-      self.assertEquals(['1'], output_dict[fields.InputDataFields.source_id])
+    def graph_fn():
+      dataset = dataset_builder.build(input_reader_proto, batch_size=1)
+      sample1_ds = dataset.take(1)
+      sample2_ds = dataset.skip(1)
+      iter1 = dataset_builder.make_initializable_iterator(sample1_ds)
+      iter2 = dataset_builder.make_initializable_iterator(sample2_ds)
+
+      return iter1.get_next(), iter2.get_next()
+
+    output_dict1, output_dict2 = self.execute(graph_fn, [])
+    self.assertAllEqual(['0'], output_dict1[fields.InputDataFields.source_id])
+    self.assertEqual([b'1'], output_dict2[fields.InputDataFields.source_id])
 
   def test_sample_one_of_n_shards(self):
-    tf_record_path = self.create_tf_record(num_examples=4)
+    tf_record_path = self.create_tf_record(num_examples_per_shard=4)
 
     input_reader_text_proto = """
       shuffle: false
@@ -231,17 +288,99 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_builder.make_initializable_iterator(
-        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
-    with tf.train.MonitoredSession() as sess:
-      output_dict = sess.run(tensor_dict)
-      self.assertAllEqual(['0'], output_dict[fields.InputDataFields.source_id])
-      output_dict = sess.run(tensor_dict)
-      self.assertEquals(['2'], output_dict[fields.InputDataFields.source_id])
+    def graph_fn():
+      dataset = dataset_builder.build(input_reader_proto, batch_size=1)
+      sample1_ds = dataset.take(1)
+      sample2_ds = dataset.skip(1)
+      iter1 = dataset_builder.make_initializable_iterator(sample1_ds)
+      iter2 = dataset_builder.make_initializable_iterator(sample2_ds)
+
+      return iter1.get_next(), iter2.get_next()
+
+    output_dict1, output_dict2 = self.execute(graph_fn, [])
+    self.assertAllEqual([b'0'], output_dict1[fields.InputDataFields.source_id])
+    self.assertEqual([b'2'], output_dict2[fields.InputDataFields.source_id])
+
+  def test_no_input_context(self):
+    """Test that all samples are read with no input context given."""
+    tf_record_path = self.create_tf_record(num_examples_per_shard=16,
+                                           num_shards=2)
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      num_epochs: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+
+    for i in range(4):
+
+      # pylint:disable=cell-var-from-loop
+      def graph_fn():
+        dataset = dataset_builder.build(input_reader_proto, batch_size=8)
+        dataset = dataset.skip(i)
+        return get_iterator_next_for_testing(dataset, self.is_tf2())
 
+      batch = self.execute(graph_fn, [])
+      self.assertEqual(batch['image'].shape, (8, 4, 5, 3))
 
-class ReadDatasetTest(tf.test.TestCase):
+    def graph_fn_last_batch():
+      dataset = dataset_builder.build(input_reader_proto, batch_size=8)
+      dataset = dataset.skip(4)
+      return get_iterator_next_for_testing(dataset, self.is_tf2())
+
+    self.assertRaises(tf.errors.OutOfRangeError, self.execute,
+                      compute_fn=graph_fn_last_batch, inputs=[])
+
+  def test_with_input_context(self):
+    """Test that a subset is read with input context given."""
+    tf_record_path = self.create_tf_record(num_examples_per_shard=16,
+                                           num_shards=2)
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      num_epochs: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+
+    input_context = tf.distribute.InputContext(
+        num_input_pipelines=2, input_pipeline_id=0, num_replicas_in_sync=4
+    )
+
+    for i in range(8):
+
+      # pylint:disable=cell-var-from-loop
+      def graph_fn():
+
+        dataset = dataset_builder.build(input_reader_proto, batch_size=8,
+                                        input_context=input_context)
+        dataset = dataset.skip(i)
+        return get_iterator_next_for_testing(dataset, self.is_tf2())
+
+      batch = self.execute(graph_fn, [])
+      self.assertEqual(batch['image'].shape, (2, 4, 5, 3))
+
+    def graph_fn_last_batch():
+      dataset = dataset_builder.build(input_reader_proto, batch_size=8,
+                                      input_context=input_context)
+      dataset = dataset.skip(8)
+      return get_iterator_next_for_testing(dataset, self.is_tf2())
+
+    self.assertRaises(tf.errors.OutOfRangeError, self.execute,
+                      compute_fn=graph_fn_last_batch, inputs=[])
+
+
+class ReadDatasetTest(test_case.TestCase):
 
   def setUp(self):
     self._path_template = os.path.join(self.get_temp_dir(), 'examples_%s.txt')
@@ -258,7 +397,9 @@ class ReadDatasetTest(tf.test.TestCase):
       with tf.gfile.Open(path, 'wb') as f:
         f.write('\n'.join([str(i)] * 5))
 
-  def _get_dataset_next(self, files, config, batch_size):
+    super(ReadDatasetTest, self).setUp()
+
+  def _get_dataset_next(self, files, config, batch_size, num_batches_skip=0):
 
     def decode_func(value):
       return [tf.string_to_number(value, out_type=tf.int32)]
@@ -267,50 +408,62 @@ class ReadDatasetTest(tf.test.TestCase):
                                            config)
     dataset = dataset.map(decode_func)
     dataset = dataset.batch(batch_size)
-    return dataset.make_one_shot_iterator().get_next()
+
+    if num_batches_skip > 0:
+      dataset = dataset.skip(num_batches_skip)
+
+    return get_iterator_next_for_testing(dataset, self.is_tf2())
 
   def test_make_initializable_iterator_with_hashTable(self):
-    keys = [1, 0, -1]
-    dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])
-    table = tf.contrib.lookup.HashTable(
-        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
-            keys=keys, values=list(reversed(keys))),
-        default_value=100)
-    dataset = dataset.map(table.lookup)
-    data = dataset_builder.make_initializable_iterator(dataset).get_next()
-    init = tf.tables_initializer()
-
-    with self.test_session() as sess:
-      sess.run(init)
-      self.assertAllEqual(sess.run(data), [-1, 100, 1, 100])
+
+    def graph_fn():
+      keys = [1, 0, -1]
+      dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])
+      table = contrib_lookup.HashTable(
+          initializer=contrib_lookup.KeyValueTensorInitializer(
+              keys=keys, values=list(reversed(keys))),
+          default_value=100)
+      dataset = dataset.map(table.lookup)
+      return dataset_builder.make_initializable_iterator(dataset).get_next()
+
+    result = self.execute(graph_fn, [])
+    self.assertAllEqual(result, [-1, 100, 1, 100])
 
   def test_read_dataset(self):
     config = input_reader_pb2.InputReader()
     config.num_readers = 1
     config.shuffle = False
 
-    data = self._get_dataset_next(
-        [self._path_template % '*'], config, batch_size=20)
-    with self.test_session() as sess:
-      self.assertAllEqual(
-          sess.run(data), [[
-              1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3, 30, 4, 40, 5,
-              50
-          ]])
+    def graph_fn():
+      return self._get_dataset_next(
+          [self._path_template % '*'], config, batch_size=20)
+
+    data = self.execute(graph_fn, [])
+    # Note that the execute function extracts single outputs if the return
+    # value is of size 1.
+    self.assertAllEqual(
+        data, [
+            1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3, 30, 4, 40, 5,
+            50
+        ])
 
   def test_reduce_num_reader(self):
     config = input_reader_pb2.InputReader()
     config.num_readers = 10
     config.shuffle = False
 
-    data = self._get_dataset_next(
-        [self._path_template % '*'], config, batch_size=20)
-    with self.test_session() as sess:
-      self.assertAllEqual(
-          sess.run(data), [[
-              1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3, 30, 4, 40, 5,
-              50
-          ]])
+    def graph_fn():
+      return self._get_dataset_next(
+          [self._path_template % '*'], config, batch_size=20)
+
+    data = self.execute(graph_fn, [])
+    # Note that the execute function extracts single outputs if the return
+    # value is of size 1.
+    self.assertAllEqual(
+        data, [
+            1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3, 30, 4, 40, 5,
+            50
+        ])
 
   def test_enable_shuffle(self):
     config = input_reader_pb2.InputReader()
@@ -318,25 +471,30 @@ class ReadDatasetTest(tf.test.TestCase):
     config.shuffle = True
 
     tf.set_random_seed(1)  # Set graph level seed.
-    data = self._get_dataset_next(
-        [self._shuffle_path_template % '*'], config, batch_size=10)
+
+    def graph_fn():
+      return self._get_dataset_next(
+          [self._shuffle_path_template % '*'], config, batch_size=10)
     expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
+    data = self.execute(graph_fn, [])
 
-    with self.test_session() as sess:
-      self.assertTrue(
-          np.any(np.not_equal(sess.run(data), expected_non_shuffle_output)))
+    self.assertTrue(
+        np.any(np.not_equal(data, expected_non_shuffle_output)))
 
   def test_disable_shuffle_(self):
     config = input_reader_pb2.InputReader()
     config.num_readers = 1
     config.shuffle = False
 
-    data = self._get_dataset_next(
-        [self._shuffle_path_template % '*'], config, batch_size=10)
+    def graph_fn():
+      return self._get_dataset_next(
+          [self._shuffle_path_template % '*'], config, batch_size=10)
     expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
 
-    with self.test_session() as sess:
-      self.assertAllEqual(sess.run(data), [expected_non_shuffle_output])
+    # Note that the execute function extracts single outputs if the return
+    # value is of size 1.
+    data = self.execute(graph_fn, [])
+    self.assertAllEqual(data, expected_non_shuffle_output)
 
   def test_read_dataset_single_epoch(self):
     config = input_reader_pb2.InputReader()
@@ -344,12 +502,24 @@ class ReadDatasetTest(tf.test.TestCase):
     config.num_readers = 1
     config.shuffle = False
 
-    data = self._get_dataset_next(
-        [self._path_template % '0'], config, batch_size=30)
-    with self.test_session() as sess:
-      # First batch will retrieve as much as it can, second batch will fail.
-      self.assertAllEqual(sess.run(data), [[1, 10]])
-      self.assertRaises(tf.errors.OutOfRangeError, sess.run, data)
+    def graph_fn():
+      return self._get_dataset_next(
+          [self._path_template % '0'], config, batch_size=30)
+
+    data = self.execute(graph_fn, [])
+
+    # Note that the execute function extracts single outputs if the return
+    # value is of size 1.
+    self.assertAllEqual(data, [1, 10])
+
+    # First batch will retrieve as much as it can, second batch will fail.
+    def graph_fn_second_batch():
+      return self._get_dataset_next(
+          [self._path_template % '0'], config, batch_size=30,
+          num_batches_skip=1)
+
+    self.assertRaises(tf.errors.OutOfRangeError, self.execute,
+                      compute_fn=graph_fn_second_batch, inputs=[])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/decoder_builder.py b/research/object_detection/builders/decoder_builder.py
new file mode 100644
index 00000000..d5fde547
--- /dev/null
+++ b/research/object_detection/builders/decoder_builder.py
@@ -0,0 +1,61 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""DataDecoder builder.
+
+Creates DataDecoders from InputReader configs.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from object_detection.data_decoders import tf_example_decoder
+from object_detection.protos import input_reader_pb2
+
+
+def build(input_reader_config):
+  """Builds a DataDecoder based only on the open source config proto.
+
+  Args:
+    input_reader_config: An input_reader_pb2.InputReader object.
+
+  Returns:
+    A DataDecoder based on the input_reader_config.
+
+  Raises:
+    ValueError: On invalid input reader proto.
+  """
+  if not isinstance(input_reader_config, input_reader_pb2.InputReader):
+    raise ValueError('input_reader_config not of type '
+                     'input_reader_pb2.InputReader.')
+
+  if input_reader_config.WhichOneof('input_reader') == 'tf_record_input_reader':
+    label_map_proto_file = None
+    if input_reader_config.HasField('label_map_path'):
+      label_map_proto_file = input_reader_config.label_map_path
+    decoder = tf_example_decoder.TfExampleDecoder(
+        load_instance_masks=input_reader_config.load_instance_masks,
+        load_multiclass_scores=input_reader_config.load_multiclass_scores,
+        load_context_features=input_reader_config.load_context_features,
+        instance_mask_type=input_reader_config.mask_type,
+        label_map_proto_file=label_map_proto_file,
+        use_display_name=input_reader_config.use_display_name,
+        num_additional_channels=input_reader_config.num_additional_channels,
+        num_keypoints=input_reader_config.num_keypoints)
+
+    return decoder
+
+  raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/decoder_builder_test.py b/research/object_detection/builders/decoder_builder_test.py
new file mode 100644
index 00000000..49d17cfa
--- /dev/null
+++ b/research/object_detection/builders/decoder_builder_test.py
@@ -0,0 +1,105 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for decoder_builder."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import decoder_builder
+from object_detection.core import standard_fields as fields
+from object_detection.protos import input_reader_pb2
+from object_detection.utils import dataset_util
+
+
+class DecoderBuilderTest(tf.test.TestCase):
+
+  def _make_serialized_tf_example(self, has_additional_channels=False):
+    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    additional_channels_tensor = np.random.randint(
+        255, size=(4, 5, 1)).astype(np.uint8)
+    flat_mask = (4 * 5) * [1.0]
+    with self.test_session():
+      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
+      encoded_additional_channels_jpeg = tf.image.encode_jpeg(
+          tf.constant(additional_channels_tensor)).eval()
+    features = {
+        'image/source_id': dataset_util.bytes_feature('0'.encode()),
+        'image/encoded': dataset_util.bytes_feature(encoded_jpeg),
+        'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
+        'image/height': dataset_util.int64_feature(4),
+        'image/width': dataset_util.int64_feature(5),
+        'image/object/bbox/xmin': dataset_util.float_list_feature([0.0]),
+        'image/object/bbox/xmax': dataset_util.float_list_feature([1.0]),
+        'image/object/bbox/ymin': dataset_util.float_list_feature([0.0]),
+        'image/object/bbox/ymax': dataset_util.float_list_feature([1.0]),
+        'image/object/class/label': dataset_util.int64_list_feature([2]),
+        'image/object/mask': dataset_util.float_list_feature(flat_mask),
+    }
+    if has_additional_channels:
+      additional_channels_key = 'image/additional_channels/encoded'
+      features[additional_channels_key] = dataset_util.bytes_list_feature(
+          [encoded_additional_channels_jpeg] * 2)
+    example = tf.train.Example(features=tf.train.Features(feature=features))
+    return example.SerializeToString()
+
+  def test_build_tf_record_input_reader(self):
+    input_reader_text_proto = 'tf_record_input_reader {}'
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Parse(input_reader_text_proto, input_reader_proto)
+
+    decoder = decoder_builder.build(input_reader_proto)
+    tensor_dict = decoder.decode(self._make_serialized_tf_example())
+
+    with tf.train.MonitoredSession() as sess:
+      output_dict = sess.run(tensor_dict)
+
+    self.assertNotIn(
+        fields.InputDataFields.groundtruth_instance_masks, output_dict)
+    self.assertEqual((4, 5, 3), output_dict[fields.InputDataFields.image].shape)
+    self.assertAllEqual([2],
+                        output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
+        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllEqual(
+        [0.0, 0.0, 1.0, 1.0],
+        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+
+  def test_build_tf_record_input_reader_and_load_instance_masks(self):
+    input_reader_text_proto = """
+      load_instance_masks: true
+      tf_record_input_reader {}
+    """
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Parse(input_reader_text_proto, input_reader_proto)
+
+    decoder = decoder_builder.build(input_reader_proto)
+    tensor_dict = decoder.decode(self._make_serialized_tf_example())
+
+    with tf.train.MonitoredSession() as sess:
+      output_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(
+        (1, 4, 5),
+        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/builders/graph_rewriter_builder.py b/research/object_detection/builders/graph_rewriter_builder.py
index 53267f30..ddf8bb12 100644
--- a/research/object_detection/builders/graph_rewriter_builder.py
+++ b/research/object_detection/builders/graph_rewriter_builder.py
@@ -16,6 +16,15 @@
 
 import tensorflow as tf
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import layers as contrib_layers
+  from tensorflow.contrib import quantize as contrib_quantize
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 
 def build(graph_rewriter_config, is_training):
   """Returns a function that modifies default graph based on options.
@@ -32,14 +41,15 @@ def build(graph_rewriter_config, is_training):
 
     # Quantize the graph by inserting quantize ops for weights and activations
     if is_training:
-      tf.contrib.quantize.experimental_create_training_graph(
+      contrib_quantize.experimental_create_training_graph(
           input_graph=tf.get_default_graph(),
           quant_delay=graph_rewriter_config.quantization.delay
       )
     else:
-      tf.contrib.quantize.experimental_create_eval_graph(
+      contrib_quantize.experimental_create_eval_graph(
           input_graph=tf.get_default_graph()
       )
 
-    tf.contrib.layers.summarize_collection('quant_vars')
+    contrib_layers.summarize_collection('quant_vars')
+
   return graph_rewrite_fn
diff --git a/research/object_detection/builders/graph_rewriter_builder_test.py b/research/object_detection/builders/graph_rewriter_builder_test.py
index 72730e72..34a25d30 100644
--- a/research/object_detection/builders/graph_rewriter_builder_test.py
+++ b/research/object_detection/builders/graph_rewriter_builder_test.py
@@ -18,14 +18,23 @@ import tensorflow as tf
 from object_detection.builders import graph_rewriter_builder
 from object_detection.protos import graph_rewriter_pb2
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import layers as contrib_layers
+  from tensorflow.contrib import quantize as contrib_quantize
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 
 class QuantizationBuilderTest(tf.test.TestCase):
 
   def testQuantizationBuilderSetsUpCorrectTrainArguments(self):
     with mock.patch.object(
-        tf.contrib.quantize,
+        contrib_quantize,
         'experimental_create_training_graph') as mock_quant_fn:
-      with mock.patch.object(tf.contrib.layers,
+      with mock.patch.object(contrib_layers,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_proto.quantization.delay = 10
@@ -40,9 +49,9 @@ class QuantizationBuilderTest(tf.test.TestCase):
         mock_summarize_col.assert_called_with('quant_vars')
 
   def testQuantizationBuilderSetsUpCorrectEvalArguments(self):
-    with mock.patch.object(tf.contrib.quantize,
+    with mock.patch.object(contrib_quantize,
                            'experimental_create_eval_graph') as mock_quant_fn:
-      with mock.patch.object(tf.contrib.layers,
+      with mock.patch.object(contrib_layers,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_proto.quantization.delay = 10
diff --git a/research/object_detection/builders/hyperparams_builder.py b/research/object_detection/builders/hyperparams_builder.py
index cd503e22..d8a188bf 100644
--- a/research/object_detection/builders/hyperparams_builder.py
+++ b/research/object_detection/builders/hyperparams_builder.py
@@ -20,7 +20,14 @@ from object_detection.core import freezable_batch_norm
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import context_manager
 
-slim = tf.contrib.slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim
+  from tensorflow.contrib import layers as contrib_layers
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 class KerasLayerHyperparams(object):
@@ -216,7 +223,7 @@ def build(hyperparams_config, is_training):
     batch_norm_params = _build_batch_norm_params(
         hyperparams_config.batch_norm, is_training)
   if hyperparams_config.HasField('group_norm'):
-    normalizer_fn = tf.contrib.layers.group_norm
+    normalizer_fn = contrib_layers.group_norm
   affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]
   if hyperparams_config.HasField('op') and (
       hyperparams_config.op == hyperparams_pb2.Hyperparams.FC):
@@ -256,6 +263,8 @@ def _build_activation_fn(activation_fn):
     return tf.nn.relu
   if activation_fn == hyperparams_pb2.Hyperparams.RELU_6:
     return tf.nn.relu6
+  if activation_fn == hyperparams_pb2.Hyperparams.SWISH:
+    return tf.nn.swish
   raise ValueError('Unknown activation function: {}'.format(activation_fn))
 
 
@@ -301,6 +310,8 @@ def _build_keras_regularizer(regularizer):
     # weight by a factor of 2
     return tf.keras.regularizers.l2(
         float(regularizer.l2_regularizer.weight * 0.5))
+  if regularizer_oneof is None:
+    return None
   raise ValueError('Unknown regularizer function: {}'.format(regularizer_oneof))
 
 
@@ -369,6 +380,8 @@ def _build_initializer(initializer, build_for_keras=False):
           factor=initializer.variance_scaling_initializer.factor,
           mode=mode,
           uniform=initializer.variance_scaling_initializer.uniform)
+  if initializer_oneof is None:
+    return None
   raise ValueError('Unknown initializer function: {}'.format(
       initializer_oneof))
 
diff --git a/research/object_detection/builders/hyperparams_builder_test.py b/research/object_detection/builders/hyperparams_builder_test.py
index a83b9eea..be6d5749 100644
--- a/research/object_detection/builders/hyperparams_builder_test.py
+++ b/research/object_detection/builders/hyperparams_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -24,7 +25,13 @@ from object_detection.builders import hyperparams_builder
 from object_detection.core import freezable_batch_norm
 from object_detection.protos import hyperparams_pb2
 
-slim = tf.contrib.slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 def _get_scope_key(op):
@@ -49,7 +56,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
                                          is_training=True)
     scope = scope_fn()
-    self.assertTrue(_get_scope_key(slim.conv2d) in scope)
+    self.assertIn(_get_scope_key(slim.conv2d), scope)
 
   def test_default_arg_scope_has_separable_conv2d_op(self):
     conv_hyperparams_text_proto = """
@@ -67,7 +74,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
                                          is_training=True)
     scope = scope_fn()
-    self.assertTrue(_get_scope_key(slim.separable_conv2d) in scope)
+    self.assertIn(_get_scope_key(slim.separable_conv2d), scope)
 
   def test_default_arg_scope_has_conv2d_transpose_op(self):
     conv_hyperparams_text_proto = """
@@ -85,7 +92,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
                                          is_training=True)
     scope = scope_fn()
-    self.assertTrue(_get_scope_key(slim.conv2d_transpose) in scope)
+    self.assertIn(_get_scope_key(slim.conv2d_transpose), scope)
 
   def test_explicit_fc_op_arg_scope_has_fully_connected_op(self):
     conv_hyperparams_text_proto = """
@@ -104,7 +111,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
                                          is_training=True)
     scope = scope_fn()
-    self.assertTrue(_get_scope_key(slim.fully_connected) in scope)
+    self.assertIn(_get_scope_key(slim.fully_connected), scope)
 
   def test_separable_conv2d_and_conv2d_and_transpose_have_same_parameters(self):
     conv_hyperparams_text_proto = """
@@ -143,7 +150,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
                                          is_training=True)
     scope = scope_fn()
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = list(scope.values())[0]
     regularizer = conv_scope_arguments['weights_regularizer']
     weights = np.array([1., -1, 4., 2.])
     with self.test_session() as sess:
@@ -284,8 +291,8 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self.assertTrue(batch_norm_params['scale'])
 
     batch_norm_layer = keras_config.build_batch_norm()
-    self.assertTrue(isinstance(batch_norm_layer,
-                               freezable_batch_norm.FreezableBatchNorm))
+    self.assertIsInstance(batch_norm_layer,
+                          freezable_batch_norm.FreezableBatchNorm)
 
   def test_return_non_default_batch_norm_params_keras_override(
       self):
@@ -420,8 +427,8 @@ class HyperparamsBuilderTest(tf.test.TestCase):
 
     # The batch norm builder should build an identity Lambda layer
     identity_layer = keras_config.build_batch_norm()
-    self.assertTrue(isinstance(identity_layer,
-                               tf.keras.layers.Lambda))
+    self.assertIsInstance(identity_layer,
+                          tf.keras.layers.Lambda)
 
   def test_use_none_activation(self):
     conv_hyperparams_text_proto = """
@@ -463,7 +470,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self.assertEqual(
         keras_config.params(include_activation=True)['activation'], None)
     activation_layer = keras_config.build_activation_layer()
-    self.assertTrue(isinstance(activation_layer, tf.keras.layers.Lambda))
+    self.assertIsInstance(activation_layer, tf.keras.layers.Lambda)
     self.assertEqual(activation_layer.function, tf.identity)
 
   def test_use_relu_activation(self):
@@ -506,7 +513,7 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self.assertEqual(
         keras_config.params(include_activation=True)['activation'], tf.nn.relu)
     activation_layer = keras_config.build_activation_layer()
-    self.assertTrue(isinstance(activation_layer, tf.keras.layers.Lambda))
+    self.assertIsInstance(activation_layer, tf.keras.layers.Lambda)
     self.assertEqual(activation_layer.function, tf.nn.relu)
 
   def test_use_relu_6_activation(self):
@@ -549,9 +556,52 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self.assertEqual(
         keras_config.params(include_activation=True)['activation'], tf.nn.relu6)
     activation_layer = keras_config.build_activation_layer()
-    self.assertTrue(isinstance(activation_layer, tf.keras.layers.Lambda))
+    self.assertIsInstance(activation_layer, tf.keras.layers.Lambda)
     self.assertEqual(activation_layer.function, tf.nn.relu6)
 
+  def test_use_swish_activation(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: SWISH
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
+                                         is_training=True)
+    scope = scope_fn()
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
+    self.assertEqual(conv_scope_arguments['activation_fn'], tf.nn.swish)
+
+  def test_use_swish_activation_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: SWISH
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    self.assertEqual(keras_config.params()['activation'], None)
+    self.assertEqual(
+        keras_config.params(include_activation=True)['activation'], tf.nn.swish)
+    activation_layer = keras_config.build_activation_layer()
+    self.assertIsInstance(activation_layer, tf.keras.layers.Lambda)
+    self.assertEqual(activation_layer.function, tf.nn.swish)
+
   def test_override_activation_keras(self):
     conv_hyperparams_text_proto = """
       regularizer {
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index bb24ef8a..794af688 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -133,9 +133,22 @@ def build(image_resizer_config):
           'Invalid image resizer condition option for '
           'ConditionalShapeResizer: \'%s\'.'
           % conditional_shape_resize_config.condition)
-
     if not conditional_shape_resize_config.convert_to_grayscale:
       return image_resizer_fn
+  elif image_resizer_oneof == 'pad_to_multiple_resizer':
+    pad_to_multiple_resizer_config = (
+        image_resizer_config.pad_to_multiple_resizer)
+
+    if pad_to_multiple_resizer_config.multiple < 0:
+      raise ValueError('`multiple` for pad_to_multiple_resizer should be > 0.')
+
+    else:
+      image_resizer_fn = functools.partial(
+          preprocessor.resize_pad_to_multiple,
+          multiple=pad_to_multiple_resizer_config.multiple)
+
+    if not pad_to_multiple_resizer_config.convert_to_grayscale:
+      return image_resizer_fn
   else:
     raise ValueError(
         'Invalid image resizer option: \'%s\'.' % image_resizer_oneof)
@@ -149,16 +162,16 @@ def build(image_resizer_config):
         width] containing instance masks.
 
     Returns:
-    Note that the position of the resized_image_shape changes based on whether
-    masks are present.
-    resized_image: A 3D tensor of shape [new_height, new_width, 1],
-      where the image has been resized (with bilinear interpolation) so that
-      min(new_height, new_width) == min_dimension or
-      max(new_height, new_width) == max_dimension.
-    resized_masks: If masks is not None, also outputs masks. A 3D tensor of
-      shape [num_instances, new_height, new_width].
-    resized_image_shape: A 1D tensor of shape [3] containing shape of the
-      resized image.
+      Note that the position of the resized_image_shape changes based on whether
+      masks are present.
+      resized_image: A 3D tensor of shape [new_height, new_width, 1],
+        where the image has been resized (with bilinear interpolation) so that
+        min(new_height, new_width) == min_dimension or
+        max(new_height, new_width) == max_dimension.
+      resized_masks: If masks is not None, also outputs masks. A 3D tensor of
+        shape [num_instances, new_height, new_width].
+      resized_image_shape: A 1D tensor of shape [3] containing shape of the
+        resized image.
     """
     # image_resizer_fn returns [resized_image, resized_image_shape] if
     # mask==None, otherwise it returns
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index dcf7bf13..655cbece 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -211,6 +211,31 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     with self.assertRaises(ValueError):
       image_resizer_builder.build(invalid_image_resizer_text_proto)
 
+  def test_build_pad_to_multiple_resizer(self):
+    """Test building a pad_to_multiple_resizer from proto."""
+    image_resizer_text_proto = """
+      pad_to_multiple_resizer {
+        multiple: 32
+      }
+    """
+    input_shape = (60, 30, 3)
+    expected_output_shape = (64, 32, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
+  def test_build_pad_to_multiple_resizer_invalid_multiple(self):
+    """Test that building a pad_to_multiple_resizer errors with invalid multiple."""
+
+    image_resizer_text_proto = """
+      pad_to_multiple_resizer {
+        multiple: -10
+      }
+    """
+
+    with self.assertRaises(ValueError):
+      image_resizer_builder.build(image_resizer_text_proto)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/input_reader_builder.py b/research/object_detection/builders/input_reader_builder.py
index 8cb5e2f0..e4717a87 100644
--- a/research/object_detection/builders/input_reader_builder.py
+++ b/research/object_detection/builders/input_reader_builder.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -23,12 +24,24 @@ Detection configuration framework, they should define their own builder function
 that wraps the build function.
 """
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import tensorflow as tf
 
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
 
-parallel_reader = tf.contrib.slim.parallel_reader
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
+parallel_reader = contrib_slim.parallel_reader
 
 
 def build(input_reader_config):
@@ -70,7 +83,8 @@ def build(input_reader_config):
     decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=input_reader_config.load_instance_masks,
         instance_mask_type=input_reader_config.mask_type,
-        label_map_proto_file=label_map_proto_file)
+        label_map_proto_file=label_map_proto_file,
+        load_context_features=input_reader_config.load_context_features)
     return decoder.decode(string_tensor)
 
   raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/input_reader_builder_test.py b/research/object_detection/builders/input_reader_builder_test.py
index c2c8ef4f..69a09c4a 100644
--- a/research/object_detection/builders/input_reader_builder_test.py
+++ b/research/object_detection/builders/input_reader_builder_test.py
@@ -54,6 +54,48 @@ class InputReaderBuilderTest(tf.test.TestCase):
 
     return path
 
+  def create_tf_record_with_context(self):
+    path = os.path.join(self.get_temp_dir(), 'tfrecord')
+    writer = tf.python_io.TFRecordWriter(path)
+
+    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    flat_mask = (4 * 5) * [1.0]
+    context_features = (10 * 3) * [1.0]
+    with self.test_session():
+      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature('jpeg'.encode('utf8')),
+                'image/height':
+                    dataset_util.int64_feature(4),
+                'image/width':
+                    dataset_util.int64_feature(5),
+                'image/object/bbox/xmin':
+                    dataset_util.float_list_feature([0.0]),
+                'image/object/bbox/xmax':
+                    dataset_util.float_list_feature([1.0]),
+                'image/object/bbox/ymin':
+                    dataset_util.float_list_feature([0.0]),
+                'image/object/bbox/ymax':
+                    dataset_util.float_list_feature([1.0]),
+                'image/object/class/label':
+                    dataset_util.int64_list_feature([2]),
+                'image/object/mask':
+                    dataset_util.float_list_feature(flat_mask),
+                'image/context_features':
+                    dataset_util.float_list_feature(context_features),
+                'image/context_feature_length':
+                    dataset_util.int64_list_feature([10]),
+            }))
+    writer.write(example.SerializeToString())
+    writer.close()
+
+    return path
+
   def test_build_tf_record_input_reader(self):
     tf_record_path = self.create_tf_record()
 
@@ -71,18 +113,53 @@ class InputReaderBuilderTest(tf.test.TestCase):
     with tf.train.MonitoredSession() as sess:
       output_dict = sess.run(tensor_dict)
 
-    self.assertTrue(fields.InputDataFields.groundtruth_instance_masks
-                    not in output_dict)
-    self.assertEquals(
-        (4, 5, 3), output_dict[fields.InputDataFields.image].shape)
-    self.assertEquals(
-        [2], output_dict[fields.InputDataFields.groundtruth_classes])
-    self.assertEquals(
+    self.assertNotIn(fields.InputDataFields.groundtruth_instance_masks,
+                     output_dict)
+    self.assertEqual((4, 5, 3), output_dict[fields.InputDataFields.image].shape)
+    self.assertEqual([2],
+                     output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
         (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
     self.assertAllEqual(
         [0.0, 0.0, 1.0, 1.0],
         output_dict[fields.InputDataFields.groundtruth_boxes][0])
 
+  def test_build_tf_record_input_reader_with_context(self):
+    tf_record_path = self.create_tf_record_with_context()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    input_reader_proto.load_context_features = True
+    tensor_dict = input_reader_builder.build(input_reader_proto)
+
+    with tf.train.MonitoredSession() as sess:
+      output_dict = sess.run(tensor_dict)
+
+    self.assertNotIn(fields.InputDataFields.groundtruth_instance_masks,
+                     output_dict)
+    self.assertEqual((4, 5, 3), output_dict[fields.InputDataFields.image].shape)
+    self.assertEqual([2],
+                     output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
+        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllEqual(
+        [0.0, 0.0, 1.0, 1.0],
+        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+    self.assertAllEqual(
+        [0.0, 0.0, 1.0, 1.0],
+        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+    self.assertAllEqual(
+        (3, 10), output_dict[fields.InputDataFields.context_features].shape)
+    self.assertAllEqual(
+        (10), output_dict[fields.InputDataFields.context_feature_length])
+
   def test_build_tf_record_input_reader_and_load_instance_masks(self):
     tf_record_path = self.create_tf_record()
 
@@ -101,11 +178,10 @@ class InputReaderBuilderTest(tf.test.TestCase):
     with tf.train.MonitoredSession() as sess:
       output_dict = sess.run(tensor_dict)
 
-    self.assertEquals(
-        (4, 5, 3), output_dict[fields.InputDataFields.image].shape)
-    self.assertEquals(
-        [2], output_dict[fields.InputDataFields.groundtruth_classes])
-    self.assertEquals(
+    self.assertEqual((4, 5, 3), output_dict[fields.InputDataFields.image].shape)
+    self.assertEqual([2],
+                     output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
         (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
     self.assertAllEqual(
         [0.0, 0.0, 1.0, 1.0],
diff --git a/research/object_detection/builders/losses_builder.py b/research/object_detection/builders/losses_builder.py
index 2b98d0aa..5a69c9b6 100644
--- a/research/object_detection/builders/losses_builder.py
+++ b/research/object_detection/builders/losses_builder.py
@@ -201,6 +201,9 @@ def _build_localization_loss(loss_config):
   if loss_type == 'weighted_iou':
     return losses.WeightedIOULocalizationLoss()
 
+  if loss_type == 'l1_localization_loss':
+    return losses.L1LocalizationLoss()
+
   raise ValueError('Empty loss config.')
 
 
@@ -249,4 +252,9 @@ def _build_classification_loss(loss_config):
         alpha=config.alpha,
         bootstrap_type=('hard' if config.hard_bootstrap else 'soft'))
 
+  if loss_type == 'penalty_reduced_logistic_focal_loss':
+    config = loss_config.penalty_reduced_logistic_focal_loss
+    return losses.PenaltyReducedLogisticFocalLoss(
+        alpha=config.alpha, beta=config.beta)
+
   raise ValueError('Empty loss config.')
diff --git a/research/object_detection/builders/losses_builder_test.py b/research/object_detection/builders/losses_builder_test.py
index 24b96b40..18c78cd5 100644
--- a/research/object_detection/builders/losses_builder_test.py
+++ b/research/object_detection/builders/losses_builder_test.py
@@ -40,8 +40,8 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, localization_loss, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedL2LocalizationLoss))
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedL2LocalizationLoss)
 
   def test_build_weighted_smooth_l1_localization_loss_default_delta(self):
     losses_text_proto = """
@@ -57,8 +57,8 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, localization_loss, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedSmoothL1LocalizationLoss))
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedSmoothL1LocalizationLoss)
     self.assertAlmostEqual(localization_loss._delta, 1.0)
 
   def test_build_weighted_smooth_l1_localization_loss_non_default_delta(self):
@@ -76,8 +76,8 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, localization_loss, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedSmoothL1LocalizationLoss))
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedSmoothL1LocalizationLoss)
     self.assertAlmostEqual(localization_loss._delta, 0.1)
 
   def test_build_weighted_iou_localization_loss(self):
@@ -94,8 +94,8 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, localization_loss, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedIOULocalizationLoss))
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedIOULocalizationLoss)
 
   def test_anchorwise_output(self):
     losses_text_proto = """
@@ -111,8 +111,8 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, localization_loss, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedSmoothL1LocalizationLoss))
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedSmoothL1LocalizationLoss)
     predictions = tf.constant([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]]])
     targets = tf.constant([[[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]]])
     weights = tf.constant([[1.0, 1.0]])
@@ -132,6 +132,7 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
       losses_builder._build_localization_loss(losses_proto)
 
 
+
 class ClassificationLossBuilderTest(tf.test.TestCase):
 
   def test_build_weighted_sigmoid_classification_loss(self):
@@ -148,8 +149,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSigmoidClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSigmoidClassificationLoss)
 
   def test_build_weighted_sigmoid_focal_classification_loss(self):
     losses_text_proto = """
@@ -165,8 +166,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.SigmoidFocalClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.SigmoidFocalClassificationLoss)
     self.assertAlmostEqual(classification_loss._alpha, None)
     self.assertAlmostEqual(classification_loss._gamma, 2.0)
 
@@ -186,8 +187,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.SigmoidFocalClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.SigmoidFocalClassificationLoss)
     self.assertAlmostEqual(classification_loss._alpha, 0.25)
     self.assertAlmostEqual(classification_loss._gamma, 3.0)
 
@@ -205,8 +206,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSoftmaxClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
 
   def test_build_weighted_logits_softmax_classification_loss(self):
     losses_text_proto = """
@@ -222,9 +223,9 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(
-        isinstance(classification_loss,
-                   losses.WeightedSoftmaxClassificationAgainstLogitsLoss))
+    self.assertIsInstance(
+        classification_loss,
+        losses.WeightedSoftmaxClassificationAgainstLogitsLoss)
 
   def test_build_weighted_softmax_classification_loss_with_logit_scale(self):
     losses_text_proto = """
@@ -241,8 +242,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSoftmaxClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
 
   def test_build_bootstrapped_sigmoid_classification_loss(self):
     losses_text_proto = """
@@ -259,8 +260,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.BootstrappedSigmoidClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.BootstrappedSigmoidClassificationLoss)
 
   def test_anchorwise_output(self):
     losses_text_proto = """
@@ -277,8 +278,8 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss, _, _, _, _, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSigmoidClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSigmoidClassificationLoss)
     predictions = tf.constant([[[0.0, 1.0, 0.0], [0.0, 0.5, 0.5]]])
     targets = tf.constant([[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]])
     weights = tf.constant([[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]])
@@ -298,6 +299,7 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
       losses_builder.build(losses_proto)
 
 
+
 class HardExampleMinerBuilderTest(tf.test.TestCase):
 
   def test_do_not_build_hard_example_miner_by_default(self):
@@ -333,7 +335,7 @@ class HardExampleMinerBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, _, _, _, hard_example_miner, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
     self.assertEqual(hard_example_miner._loss_type, 'cls')
 
   def test_build_hard_example_miner_for_localization_loss(self):
@@ -353,7 +355,7 @@ class HardExampleMinerBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, _, _, _, hard_example_miner, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
     self.assertEqual(hard_example_miner._loss_type, 'loc')
 
   def test_build_hard_example_miner_with_non_default_values(self):
@@ -377,7 +379,7 @@ class HardExampleMinerBuilderTest(tf.test.TestCase):
     losses_proto = losses_pb2.Loss()
     text_format.Merge(losses_text_proto, losses_proto)
     _, _, _, _, hard_example_miner, _, _ = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
     self.assertEqual(hard_example_miner._num_hard_examples, 32)
     self.assertAlmostEqual(hard_example_miner._iou_threshold, 0.5)
     self.assertEqual(hard_example_miner._max_negatives_per_positive, 10)
@@ -406,11 +408,11 @@ class LossBuilderTest(tf.test.TestCase):
     (classification_loss, localization_loss, classification_weight,
      localization_weight, hard_example_miner, _,
      _) = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSoftmaxClassificationLoss))
-    self.assertTrue(isinstance(localization_loss,
-                               losses.WeightedL2LocalizationLoss))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
+    self.assertIsInstance(localization_loss,
+                          losses.WeightedL2LocalizationLoss)
     self.assertAlmostEqual(classification_weight, 0.8)
     self.assertAlmostEqual(localization_weight, 0.2)
 
@@ -434,12 +436,10 @@ class LossBuilderTest(tf.test.TestCase):
     (classification_loss, localization_loss, classification_weight,
      localization_weight, hard_example_miner, _,
      _) = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
-    self.assertTrue(
-        isinstance(classification_loss,
-                   losses.WeightedSoftmaxClassificationLoss))
-    self.assertTrue(
-        isinstance(localization_loss, losses.WeightedL2LocalizationLoss))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
+    self.assertIsInstance(localization_loss, losses.WeightedL2LocalizationLoss)
     self.assertAlmostEqual(classification_weight, 0.8)
     self.assertAlmostEqual(localization_weight, 0.2)
 
@@ -464,12 +464,10 @@ class LossBuilderTest(tf.test.TestCase):
     (classification_loss, localization_loss, classification_weight,
      localization_weight, hard_example_miner, _,
      _) = losses_builder.build(losses_proto)
-    self.assertTrue(isinstance(hard_example_miner, losses.HardExampleMiner))
-    self.assertTrue(
-        isinstance(classification_loss,
-                   losses.WeightedSoftmaxClassificationLoss))
-    self.assertTrue(
-        isinstance(localization_loss, losses.WeightedL2LocalizationLoss))
+    self.assertIsInstance(hard_example_miner, losses.HardExampleMiner)
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
+    self.assertIsInstance(localization_loss, losses.WeightedL2LocalizationLoss)
     self.assertAlmostEqual(classification_weight, 0.8)
     self.assertAlmostEqual(localization_weight, 0.2)
 
@@ -505,8 +503,8 @@ class FasterRcnnClassificationLossBuilderTest(tf.test.TestCase):
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss = losses_builder.build_faster_rcnn_classification_loss(
         losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSigmoidClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSigmoidClassificationLoss)
 
   def test_build_softmax_loss(self):
     losses_text_proto = """
@@ -517,8 +515,8 @@ class FasterRcnnClassificationLossBuilderTest(tf.test.TestCase):
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss = losses_builder.build_faster_rcnn_classification_loss(
         losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSoftmaxClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
 
   def test_build_logits_softmax_loss(self):
     losses_text_proto = """
@@ -542,9 +540,8 @@ class FasterRcnnClassificationLossBuilderTest(tf.test.TestCase):
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss = losses_builder.build_faster_rcnn_classification_loss(
         losses_proto)
-    self.assertTrue(
-        isinstance(classification_loss,
-                   losses.SigmoidFocalClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.SigmoidFocalClassificationLoss)
 
   def test_build_softmax_loss_by_default(self):
     losses_text_proto = """
@@ -553,8 +550,8 @@ class FasterRcnnClassificationLossBuilderTest(tf.test.TestCase):
     text_format.Merge(losses_text_proto, losses_proto)
     classification_loss = losses_builder.build_faster_rcnn_classification_loss(
         losses_proto)
-    self.assertTrue(isinstance(classification_loss,
-                               losses.WeightedSoftmaxClassificationLoss))
+    self.assertIsInstance(classification_loss,
+                          losses.WeightedSoftmaxClassificationLoss)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 4eb0368a..9f5555a8 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -16,7 +16,6 @@
 """A function to build a DetectionModel from configuration."""
 
 import functools
-
 from object_detection.builders import anchor_generator_builder
 from object_detection.builders import box_coder_builder
 from object_detection.builders import box_predictor_builder
@@ -32,96 +31,162 @@ from object_detection.core import target_assigner
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.meta_architectures import rfcn_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
-from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res_keras
-from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
-from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
-from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
-from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
-from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
-from object_detection.models import ssd_resnet_v1_fpn_keras_feature_extractor as ssd_resnet_v1_fpn_keras
-from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
-from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
-from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
-from object_detection.models.ssd_mobilenet_edgetpu_feature_extractor import SSDMobileNetEdgeTPUFeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_fpn_keras_feature_extractor import SSDMobileNetV1FpnKerasFeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_keras_feature_extractor import SSDMobileNetV1KerasFeatureExtractor
-from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor import SSDMobileNetV2FpnKerasFeatureExtractor
-from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor
-from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3LargeFeatureExtractor
-from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3SmallFeatureExtractor
-from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor
-from object_detection.predictors import rfcn_box_predictor
-from object_detection.predictors import rfcn_keras_box_predictor
 from object_detection.predictors.heads import mask_head
+from object_detection.protos import losses_pb2
 from object_detection.protos import model_pb2
+from object_detection.utils import label_map_util
 from object_detection.utils import ops
+from object_detection.utils import tf_version
+
+## Feature Extractors for TF
+## This section conditionally imports different feature extractors based on the
+## Tensorflow version.
+##
+# pylint: disable=g-import-not-at-top
+if tf_version.is_tf2():
+  from object_detection.models import center_net_hourglass_feature_extractor
+  from object_detection.models import center_net_resnet_feature_extractor
+  from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res_keras
+  from object_detection.models import faster_rcnn_resnet_keras_feature_extractor as frcnn_resnet_keras
+  from object_detection.models import ssd_resnet_v1_fpn_keras_feature_extractor as ssd_resnet_v1_fpn_keras
+  from object_detection.models.ssd_mobilenet_v1_fpn_keras_feature_extractor import SSDMobileNetV1FpnKerasFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v1_keras_feature_extractor import SSDMobileNetV1KerasFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v2_fpn_keras_feature_extractor import SSDMobileNetV2FpnKerasFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v2_keras_feature_extractor import SSDMobileNetV2KerasFeatureExtractor
+  from object_detection.predictors import rfcn_keras_box_predictor
+
+if tf_version.is_tf1():
+  from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
+  from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
+  from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
+  from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
+  from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
+  from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
+  from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
+  from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
+  from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
+  from object_detection.models.ssd_mobilenet_v2_fpn_feature_extractor import SSDMobileNetV2FpnFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v2_mnasfpn_feature_extractor import SSDMobileNetV2MnasFPNFeatureExtractor
+  from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
+  from object_detection.models.ssd_mobilenet_edgetpu_feature_extractor import SSDMobileNetEdgeTPUFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
+  from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
+  from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3LargeFeatureExtractor
+  from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3SmallFeatureExtractor
+  from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor
+  from object_detection.predictors import rfcn_box_predictor
+# pylint: enable=g-import-not-at-top
+
+if tf_version.is_tf2():
+  SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
+      'ssd_mobilenet_v1_keras': SSDMobileNetV1KerasFeatureExtractor,
+      'ssd_mobilenet_v1_fpn_keras': SSDMobileNetV1FpnKerasFeatureExtractor,
+      'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor,
+      'ssd_mobilenet_v2_fpn_keras': SSDMobileNetV2FpnKerasFeatureExtractor,
+      'ssd_resnet50_v1_fpn_keras':
+          ssd_resnet_v1_fpn_keras.SSDResNet50V1FpnKerasFeatureExtractor,
+      'ssd_resnet101_v1_fpn_keras':
+          ssd_resnet_v1_fpn_keras.SSDResNet101V1FpnKerasFeatureExtractor,
+      'ssd_resnet152_v1_fpn_keras':
+          ssd_resnet_v1_fpn_keras.SSDResNet152V1FpnKerasFeatureExtractor,
+  }
 
-# A map of names to SSD feature extractors.
-SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'ssd_inception_v2': SSDInceptionV2FeatureExtractor,
-    'ssd_inception_v3': SSDInceptionV3FeatureExtractor,
-    'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,
-    'ssd_mobilenet_v1_fpn': SSDMobileNetV1FpnFeatureExtractor,
-    'ssd_mobilenet_v1_ppn': SSDMobileNetV1PpnFeatureExtractor,
-    'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,
-    'ssd_mobilenet_v2_fpn': SSDMobileNetV2FpnFeatureExtractor,
-    'ssd_mobilenet_v3_large': SSDMobileNetV3LargeFeatureExtractor,
-    'ssd_mobilenet_v3_small': SSDMobileNetV3SmallFeatureExtractor,
-    'ssd_mobilenet_edgetpu': SSDMobileNetEdgeTPUFeatureExtractor,
-    'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
-    'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
-    'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
-    'ssd_resnet50_v1_ppn': ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
-    'ssd_resnet101_v1_ppn':
-        ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
-    'ssd_resnet152_v1_ppn':
-        ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor,
-    'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,
-    'ssd_pnasnet': SSDPNASNetFeatureExtractor,
-}
+  FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
+      'faster_rcnn_resnet50_keras':
+          frcnn_resnet_keras.FasterRCNNResnet50KerasFeatureExtractor,
+      'faster_rcnn_resnet101_keras':
+          frcnn_resnet_keras.FasterRCNNResnet101KerasFeatureExtractor,
+      'faster_rcnn_resnet152_keras':
+          frcnn_resnet_keras.FasterRCNNResnet152KerasFeatureExtractor,
+      'faster_rcnn_inception_resnet_v2_keras':
+      frcnn_inc_res_keras.FasterRCNNInceptionResnetV2KerasFeatureExtractor,
+  }
 
-SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'ssd_mobilenet_v1_keras': SSDMobileNetV1KerasFeatureExtractor,
-    'ssd_mobilenet_v1_fpn_keras': SSDMobileNetV1FpnKerasFeatureExtractor,
-    'ssd_mobilenet_v2_keras': SSDMobileNetV2KerasFeatureExtractor,
-    'ssd_mobilenet_v2_fpn_keras': SSDMobileNetV2FpnKerasFeatureExtractor,
-    'ssd_resnet50_v1_fpn_keras':
-        ssd_resnet_v1_fpn_keras.SSDResNet50V1FpnKerasFeatureExtractor,
-    'ssd_resnet101_v1_fpn_keras':
-        ssd_resnet_v1_fpn_keras.SSDResNet101V1FpnKerasFeatureExtractor,
-    'ssd_resnet152_v1_fpn_keras':
-        ssd_resnet_v1_fpn_keras.SSDResNet152V1FpnKerasFeatureExtractor,
-}
+  CENTER_NET_EXTRACTOR_FUNCTION_MAP = {
+      'resnet_v2_101': center_net_resnet_feature_extractor.resnet_v2_101,
+      'resnet_v2_50': center_net_resnet_feature_extractor.resnet_v2_50,
+      'hourglass_104': center_net_hourglass_feature_extractor.hourglass_104,
+  }
 
-# A map of names to Faster R-CNN feature extractors.
-FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'faster_rcnn_nas':
-    frcnn_nas.FasterRCNNNASFeatureExtractor,
-    'faster_rcnn_pnas':
-    frcnn_pnas.FasterRCNNPNASFeatureExtractor,
-    'faster_rcnn_inception_resnet_v2':
-    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,
-    'faster_rcnn_inception_v2':
-    frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,
-    'faster_rcnn_resnet50':
-    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
-    'faster_rcnn_resnet101':
-    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,
-    'faster_rcnn_resnet152':
-    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,
-}
+  FEATURE_EXTRACTOR_MAPS = [
+      CENTER_NET_EXTRACTOR_FUNCTION_MAP,
+      FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP,
+      SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP
+  ]
+
+if tf_version.is_tf1():
+  SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
+      'ssd_inception_v2':
+          SSDInceptionV2FeatureExtractor,
+      'ssd_inception_v3':
+          SSDInceptionV3FeatureExtractor,
+      'ssd_mobilenet_v1':
+          SSDMobileNetV1FeatureExtractor,
+      'ssd_mobilenet_v1_fpn':
+          SSDMobileNetV1FpnFeatureExtractor,
+      'ssd_mobilenet_v1_ppn':
+          SSDMobileNetV1PpnFeatureExtractor,
+      'ssd_mobilenet_v2':
+          SSDMobileNetV2FeatureExtractor,
+      'ssd_mobilenet_v2_fpn':
+          SSDMobileNetV2FpnFeatureExtractor,
+      'ssd_mobilenet_v2_mnasfpn':
+          SSDMobileNetV2MnasFPNFeatureExtractor,
+      'ssd_mobilenet_v3_large':
+          SSDMobileNetV3LargeFeatureExtractor,
+      'ssd_mobilenet_v3_small':
+          SSDMobileNetV3SmallFeatureExtractor,
+      'ssd_mobilenet_edgetpu':
+          SSDMobileNetEdgeTPUFeatureExtractor,
+      'ssd_resnet50_v1_fpn':
+          ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
+      'ssd_resnet101_v1_fpn':
+          ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
+      'ssd_resnet152_v1_fpn':
+          ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
+      'ssd_resnet50_v1_ppn':
+          ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
+      'ssd_resnet101_v1_ppn':
+          ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
+      'ssd_resnet152_v1_ppn':
+          ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor,
+      'embedded_ssd_mobilenet_v1':
+          EmbeddedSSDMobileNetV1FeatureExtractor,
+      'ssd_pnasnet':
+          SSDPNASNetFeatureExtractor,
+  }
+
+  FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
+      'faster_rcnn_nas':
+      frcnn_nas.FasterRCNNNASFeatureExtractor,
+      'faster_rcnn_pnas':
+      frcnn_pnas.FasterRCNNPNASFeatureExtractor,
+      'faster_rcnn_inception_resnet_v2':
+      frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,
+      'faster_rcnn_inception_v2':
+      frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,
+      'faster_rcnn_resnet50':
+      frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
+      'faster_rcnn_resnet101':
+      frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,
+      'faster_rcnn_resnet152':
+      frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,
+  }
+
+  FEATURE_EXTRACTOR_MAPS = [
+      SSD_FEATURE_EXTRACTOR_CLASS_MAP,
+      FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP
+  ]
 
-FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP = {
-    'faster_rcnn_inception_resnet_v2_keras':
-    frcnn_inc_res_keras.FasterRCNNInceptionResnetV2KerasFeatureExtractor,
-}
+
+def _check_feature_extractor_exists(feature_extractor_type):
+  feature_extractors = set().union(*FEATURE_EXTRACTOR_MAPS)
+  if feature_extractor_type not in feature_extractors:
+    raise ValueError('{} is not supported. See `model_builder.py` for features '
+                     'extractors compatible with different versions of '
+                     'Tensorflow'.format(feature_extractor_type))
 
 
 def _build_ssd_feature_extractor(feature_extractor_config,
@@ -146,14 +211,14 @@ def _build_ssd_feature_extractor(feature_extractor_config,
     ValueError: On invalid feature extractor type.
   """
   feature_type = feature_extractor_config.type
-  is_keras_extractor = feature_type in SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP
   depth_multiplier = feature_extractor_config.depth_multiplier
   min_depth = feature_extractor_config.min_depth
   pad_to_multiple = feature_extractor_config.pad_to_multiple
   use_explicit_padding = feature_extractor_config.use_explicit_padding
   use_depthwise = feature_extractor_config.use_depthwise
 
-  if is_keras_extractor:
+  is_keras = tf_version.is_tf2()
+  if is_keras:
     conv_hyperparams = hyperparams_builder.KerasLayerHyperparams(
         feature_extractor_config.conv_hyperparams)
   else:
@@ -162,11 +227,10 @@ def _build_ssd_feature_extractor(feature_extractor_config,
   override_base_feature_extractor_hyperparams = (
       feature_extractor_config.override_base_feature_extractor_hyperparams)
 
-  if (feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP) and (
-      not is_keras_extractor):
+  if not is_keras and feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP:
     raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))
 
-  if is_keras_extractor:
+  if is_keras:
     feature_extractor_class = SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP[
         feature_type]
   else:
@@ -197,7 +261,7 @@ def _build_ssd_feature_extractor(feature_extractor_config,
   if feature_extractor_config.HasField('num_layers'):
     kwargs.update({'num_layers': feature_extractor_config.num_layers})
 
-  if is_keras_extractor:
+  if is_keras:
     kwargs.update({
         'conv_hyperparams': conv_hyperparams,
         'inplace_batchnorm_update': False,
@@ -209,6 +273,7 @@ def _build_ssd_feature_extractor(feature_extractor_config,
         'reuse_weights': reuse_weights,
     })
 
+
   if feature_extractor_config.HasField('fpn'):
     kwargs.update({
         'fpn_min_level':
@@ -239,6 +304,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
       model_class_map).
   """
   num_classes = ssd_config.num_classes
+  _check_feature_extractor_exists(ssd_config.feature_extractor.type)
 
   # Feature extractor
   feature_extractor = _build_ssd_feature_extractor(
@@ -325,7 +391,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
 
 
 def _build_faster_rcnn_feature_extractor(
-    feature_extractor_config, is_training, reuse_weights=None,
+    feature_extractor_config, is_training, reuse_weights=True,
     inplace_batchnorm_update=False):
   """Builds a faster_rcnn_meta_arch.FasterRCNNFeatureExtractor based on config.
 
@@ -422,9 +488,8 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
   """
   num_classes = frcnn_config.num_classes
   image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)
-
-  is_keras = (frcnn_config.feature_extractor.type in
-              FASTER_RCNN_KERAS_FEATURE_EXTRACTOR_CLASS_MAP)
+  _check_feature_extractor_exists(frcnn_config.feature_extractor.type)
+  is_keras = tf_version.is_tf2()
 
   if is_keras:
     feature_extractor = _build_faster_rcnn_keras_feature_extractor(
@@ -536,54 +601,98 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       frcnn_config.clip_anchors_to_image)
 
   common_kwargs = {
-      'is_training': is_training,
-      'num_classes': num_classes,
-      'image_resizer_fn': image_resizer_fn,
-      'feature_extractor': feature_extractor,
-      'number_of_stages': number_of_stages,
-      'first_stage_anchor_generator': first_stage_anchor_generator,
-      'first_stage_target_assigner': first_stage_target_assigner,
-      'first_stage_atrous_rate': first_stage_atrous_rate,
+      'is_training':
+          is_training,
+      'num_classes':
+          num_classes,
+      'image_resizer_fn':
+          image_resizer_fn,
+      'feature_extractor':
+          feature_extractor,
+      'number_of_stages':
+          number_of_stages,
+      'first_stage_anchor_generator':
+          first_stage_anchor_generator,
+      'first_stage_target_assigner':
+          first_stage_target_assigner,
+      'first_stage_atrous_rate':
+          first_stage_atrous_rate,
       'first_stage_box_predictor_arg_scope_fn':
-      first_stage_box_predictor_arg_scope_fn,
+          first_stage_box_predictor_arg_scope_fn,
       'first_stage_box_predictor_kernel_size':
-      first_stage_box_predictor_kernel_size,
-      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,
-      'first_stage_minibatch_size': first_stage_minibatch_size,
-      'first_stage_sampler': first_stage_sampler,
-      'first_stage_non_max_suppression_fn': first_stage_non_max_suppression_fn,
-      'first_stage_max_proposals': first_stage_max_proposals,
-      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,
-      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,
-      'second_stage_target_assigner': second_stage_target_assigner,
-      'second_stage_batch_size': second_stage_batch_size,
-      'second_stage_sampler': second_stage_sampler,
+          first_stage_box_predictor_kernel_size,
+      'first_stage_box_predictor_depth':
+          first_stage_box_predictor_depth,
+      'first_stage_minibatch_size':
+          first_stage_minibatch_size,
+      'first_stage_sampler':
+          first_stage_sampler,
+      'first_stage_non_max_suppression_fn':
+          first_stage_non_max_suppression_fn,
+      'first_stage_max_proposals':
+          first_stage_max_proposals,
+      'first_stage_localization_loss_weight':
+          first_stage_loc_loss_weight,
+      'first_stage_objectness_loss_weight':
+          first_stage_obj_loss_weight,
+      'second_stage_target_assigner':
+          second_stage_target_assigner,
+      'second_stage_batch_size':
+          second_stage_batch_size,
+      'second_stage_sampler':
+          second_stage_sampler,
       'second_stage_non_max_suppression_fn':
-      second_stage_non_max_suppression_fn,
-      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,
+          second_stage_non_max_suppression_fn,
+      'second_stage_score_conversion_fn':
+          second_stage_score_conversion_fn,
       'second_stage_localization_loss_weight':
-      second_stage_localization_loss_weight,
+          second_stage_localization_loss_weight,
       'second_stage_classification_loss':
-      second_stage_classification_loss,
+          second_stage_classification_loss,
       'second_stage_classification_loss_weight':
-      second_stage_classification_loss_weight,
-      'hard_example_miner': hard_example_miner,
-      'add_summaries': add_summaries,
-      'crop_and_resize_fn': crop_and_resize_fn,
-      'clip_anchors_to_image': clip_anchors_to_image,
-      'use_static_shapes': use_static_shapes,
-      'resize_masks': frcnn_config.resize_masks,
-      'return_raw_detections_during_predict': (
-          frcnn_config.return_raw_detections_during_predict)
+          second_stage_classification_loss_weight,
+      'hard_example_miner':
+          hard_example_miner,
+      'add_summaries':
+          add_summaries,
+      'crop_and_resize_fn':
+          crop_and_resize_fn,
+      'clip_anchors_to_image':
+          clip_anchors_to_image,
+      'use_static_shapes':
+          use_static_shapes,
+      'resize_masks':
+          frcnn_config.resize_masks,
+      'return_raw_detections_during_predict':
+          frcnn_config.return_raw_detections_during_predict,
+      'output_final_box_features':
+          frcnn_config.output_final_box_features
   }
 
-  if (isinstance(second_stage_box_predictor,
-                 rfcn_box_predictor.RfcnBoxPredictor) or
-      isinstance(second_stage_box_predictor,
-                 rfcn_keras_box_predictor.RfcnKerasBoxPredictor)):
+  if ((not is_keras and isinstance(second_stage_box_predictor,
+                                   rfcn_box_predictor.RfcnBoxPredictor)) or
+      (is_keras and
+       isinstance(second_stage_box_predictor,
+                  rfcn_keras_box_predictor.RfcnKerasBoxPredictor))):
     return rfcn_meta_arch.RFCNMetaArch(
         second_stage_rfcn_box_predictor=second_stage_box_predictor,
         **common_kwargs)
+  elif frcnn_config.HasField('context_config'):
+    context_config = frcnn_config.context_config
+    common_kwargs.update({
+        'attention_bottleneck_dimension':
+            context_config.attention_bottleneck_dimension,
+        'attention_temperature':
+            context_config.attention_temperature
+    })
+    return context_rcnn_meta_arch.ContextRCNNMetaArch(
+        initial_crop_size=initial_crop_size,
+        maxpool_kernel_size=maxpool_kernel_size,
+        maxpool_stride=maxpool_stride,
+        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,
+        second_stage_mask_prediction_loss_weight=(
+            second_stage_mask_prediction_loss_weight),
+        **common_kwargs)
   else:
     return faster_rcnn_meta_arch.FasterRCNNMetaArch(
         initial_crop_size=initial_crop_size,
@@ -602,10 +711,170 @@ def _build_experimental_model(config, is_training, add_summaries=True):
   return EXPERIMENTAL_META_ARCH_BUILDER_MAP[config.name](
       is_training, add_summaries)
 
-META_ARCHITECURE_BUILDER_MAP = {
+
+# The class ID in the groundtruth/model architecture is usually 0-based while
+# the ID in the label map is 1-based. The offset is used to convert between the
+# the two.
+CLASS_ID_OFFSET = 1
+KEYPOINT_STD_DEV_DEFAULT = 1.0
+
+
+def keypoint_proto_to_params(kp_config, keypoint_map_dict):
+  """Converts CenterNet.KeypointEstimation proto to parameter namedtuple."""
+  label_map_item = keypoint_map_dict[kp_config.keypoint_class_name]
+
+  classification_loss, localization_loss, _, _, _, _, _ = (
+      losses_builder.build(kp_config.loss))
+
+  keypoint_indices = [
+      keypoint.id for keypoint in label_map_item.keypoints
+  ]
+  keypoint_labels = [
+      keypoint.label for keypoint in label_map_item.keypoints
+  ]
+  keypoint_std_dev_dict = {
+      label: KEYPOINT_STD_DEV_DEFAULT for label in keypoint_labels
+  }
+  if kp_config.keypoint_label_to_std:
+    for label, value in kp_config.keypoint_label_to_std.items():
+      keypoint_std_dev_dict[label] = value
+  keypoint_std_dev = [keypoint_std_dev_dict[label] for label in keypoint_labels]
+  return center_net_meta_arch.KeypointEstimationParams(
+      task_name=kp_config.task_name,
+      class_id=label_map_item.id - CLASS_ID_OFFSET,
+      keypoint_indices=keypoint_indices,
+      classification_loss=classification_loss,
+      localization_loss=localization_loss,
+      keypoint_labels=keypoint_labels,
+      keypoint_std_dev=keypoint_std_dev,
+      task_loss_weight=kp_config.task_loss_weight,
+      keypoint_regression_loss_weight=kp_config.keypoint_regression_loss_weight,
+      keypoint_heatmap_loss_weight=kp_config.keypoint_heatmap_loss_weight,
+      keypoint_offset_loss_weight=kp_config.keypoint_offset_loss_weight,
+      heatmap_bias_init=kp_config.heatmap_bias_init,
+      keypoint_candidate_score_threshold=(
+          kp_config.keypoint_candidate_score_threshold),
+      num_candidates_per_keypoint=kp_config.num_candidates_per_keypoint,
+      peak_max_pool_kernel_size=kp_config.peak_max_pool_kernel_size,
+      unmatched_keypoint_score=kp_config.unmatched_keypoint_score,
+      box_scale=kp_config.box_scale,
+      candidate_search_scale=kp_config.candidate_search_scale,
+      candidate_ranking_mode=kp_config.candidate_ranking_mode)
+
+
+def object_detection_proto_to_params(od_config):
+  """Converts CenterNet.ObjectDetection proto to parameter namedtuple."""
+  loss = losses_pb2.Loss()
+  # Add dummy classification loss to avoid the loss_builder throwing error.
+  # TODO(yuhuic): update the loss builder to take the classification loss
+  # directly.
+  loss.classification_loss.weighted_sigmoid.CopyFrom(
+      losses_pb2.WeightedSigmoidClassificationLoss())
+  loss.localization_loss.CopyFrom(od_config.localization_loss)
+  _, localization_loss, _, _, _, _, _ = (losses_builder.build(loss))
+  return center_net_meta_arch.ObjectDetectionParams(
+      localization_loss=localization_loss,
+      scale_loss_weight=od_config.scale_loss_weight,
+      offset_loss_weight=od_config.offset_loss_weight,
+      task_loss_weight=od_config.task_loss_weight)
+
+
+def object_center_proto_to_params(oc_config):
+  """Converts CenterNet.ObjectCenter proto to parameter namedtuple."""
+  loss = losses_pb2.Loss()
+  # Add dummy localization loss to avoid the loss_builder throwing error.
+  # TODO(yuhuic): update the loss builder to take the localization loss
+  # directly.
+  loss.localization_loss.weighted_l2.CopyFrom(
+      losses_pb2.WeightedL2LocalizationLoss())
+  loss.classification_loss.CopyFrom(oc_config.classification_loss)
+  classification_loss, _, _, _, _, _, _ = (losses_builder.build(loss))
+  return center_net_meta_arch.ObjectCenterParams(
+      classification_loss=classification_loss,
+      object_center_loss_weight=oc_config.object_center_loss_weight,
+      heatmap_bias_init=oc_config.heatmap_bias_init,
+      min_box_overlap_iou=oc_config.min_box_overlap_iou,
+      max_box_predictions=oc_config.max_box_predictions)
+
+
+def _build_center_net_model(center_net_config, is_training, add_summaries):
+  """Build a CenterNet detection model.
+
+  Args:
+    center_net_config: A CenterNet proto object with model configuration.
+    is_training: True if this model is being built for training purposes.
+    add_summaries: Whether to add tf summaries in the model.
+
+  Returns:
+    CenterNetMetaArch based on the config.
+
+  """
+
+  image_resizer_fn = image_resizer_builder.build(
+      center_net_config.image_resizer)
+  _check_feature_extractor_exists(center_net_config.feature_extractor.type)
+  feature_extractor = _build_center_net_feature_extractor(
+      center_net_config.feature_extractor)
+  object_center_params = object_center_proto_to_params(
+      center_net_config.object_center_params)
+
+  object_detection_params = None
+  if center_net_config.HasField('object_detection_task'):
+    object_detection_params = object_detection_proto_to_params(
+        center_net_config.object_detection_task)
+
+  keypoint_params_dict = None
+  if center_net_config.keypoint_estimation_task:
+    label_map_proto = label_map_util.load_labelmap(
+        center_net_config.keypoint_label_map_path)
+    keypoint_map_dict = {
+        item.name: item for item in label_map_proto.item if item.keypoints
+    }
+    keypoint_params_dict = {}
+    keypoint_class_id_set = set()
+    all_keypoint_indices = []
+    for task in center_net_config.keypoint_estimation_task:
+      kp_params = keypoint_proto_to_params(task, keypoint_map_dict)
+      keypoint_params_dict[task.task_name] = kp_params
+      all_keypoint_indices.extend(kp_params.keypoint_indices)
+      if kp_params.class_id in keypoint_class_id_set:
+        raise ValueError(('Multiple keypoint tasks map to the same class id is '
+                          'not allowed: %d' % kp_params.class_id))
+      else:
+        keypoint_class_id_set.add(kp_params.class_id)
+    if len(all_keypoint_indices) > len(set(all_keypoint_indices)):
+      raise ValueError('Some keypoint indices are used more than once.')
+  return center_net_meta_arch.CenterNetMetaArch(
+      is_training=is_training,
+      add_summaries=add_summaries,
+      num_classes=center_net_config.num_classes,
+      feature_extractor=feature_extractor,
+      image_resizer_fn=image_resizer_fn,
+      object_center_params=object_center_params,
+      object_detection_params=object_detection_params,
+      keypoint_params_dict=keypoint_params_dict)
+
+
+def _build_center_net_feature_extractor(
+    feature_extractor_config):
+  """Build a CenterNet feature extractor from the given config."""
+
+  if feature_extractor_config.type not in CENTER_NET_EXTRACTOR_FUNCTION_MAP:
+    raise ValueError('\'{}\' is not a known CenterNet feature extractor type'
+                     .format(feature_extractor_config.type))
+
+  return CENTER_NET_EXTRACTOR_FUNCTION_MAP[feature_extractor_config.type](
+      channel_means=list(feature_extractor_config.channel_means),
+      channel_stds=list(feature_extractor_config.channel_stds),
+      bgr_ordering=feature_extractor_config.bgr_ordering
+  )
+
+
+META_ARCH_BUILDER_MAP = {
     'ssd': _build_ssd_model,
     'faster_rcnn': _build_faster_rcnn_model,
-    'experimental_model': _build_experimental_model
+    'experimental_model': _build_experimental_model,
+    'center_net': _build_center_net_model
 }
 
 
@@ -628,9 +897,9 @@ def build(model_config, is_training, add_summaries=True):
 
   meta_architecture = model_config.WhichOneof('model')
 
-  if meta_architecture not in META_ARCHITECURE_BUILDER_MAP:
+  if meta_architecture not in META_ARCH_BUILDER_MAP:
     raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))
   else:
-    build_func = META_ARCHITECURE_BUILDER_MAP[meta_architecture]
+    build_func = META_ARCH_BUILDER_MAP[meta_architecture]
     return build_func(getattr(model_config, meta_architecture), is_training,
                       add_summaries)
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index ed3bf504..511c640f 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -12,25 +13,34 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Tests for object_detection.models.model_builder."""
 
 from absl.testing import parameterized
 
-import tensorflow as tf
-
 from google.protobuf import text_format
 from object_detection.builders import model_builder
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.meta_architectures import rfcn_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch
-from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
 from object_detection.protos import hyperparams_pb2
 from object_detection.protos import losses_pb2
 from object_detection.protos import model_pb2
+from object_detection.utils import test_case
+
+
+class ModelBuilderTest(test_case.TestCase, parameterized.TestCase):
 
+  def default_ssd_feature_extractor(self):
+    raise NotImplementedError
 
-class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
+  def default_faster_rcnn_feature_extractor(self):
+    raise NotImplementedError
+
+  def ssd_feature_extractors(self):
+    raise NotImplementedError
+
+  def faster_rcnn_feature_extractors(self):
+    raise NotImplementedError
 
   def create_model(self, model_config, is_training=True):
     """Builds a DetectionModel based on the model config.
@@ -50,7 +60,6 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
     model_text_proto = """
       ssd {
         feature_extractor {
-          type: 'ssd_inception_v2'
           conv_hyperparams {
             regularizer {
                 l2_regularizer {
@@ -113,6 +122,8 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
+    model_proto.ssd.feature_extractor.type = (self.
+                                              default_ssd_feature_extractor())
     return model_proto
 
   def create_default_faster_rcnn_model_proto(self):
@@ -127,9 +138,6 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
             max_dimension: 1024
           }
         }
-        feature_extractor {
-          type: 'faster_rcnn_resnet101'
-        }
         first_stage_anchor_generator {
           grid_anchor_generator {
             scales: [0.25, 0.5, 1.0, 2.0]
@@ -188,17 +196,14 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
+    (model_proto.faster_rcnn.feature_extractor.type
+    ) = self.default_faster_rcnn_feature_extractor()
     return model_proto
 
   def test_create_ssd_models_from_config(self):
     model_proto = self.create_default_ssd_model_proto()
-    ssd_feature_extractor_map = {}
-    ssd_feature_extractor_map.update(
-        model_builder.SSD_FEATURE_EXTRACTOR_CLASS_MAP)
-    ssd_feature_extractor_map.update(
-        model_builder.SSD_KERAS_FEATURE_EXTRACTOR_CLASS_MAP)
-
-    for extractor_type, extractor_class in ssd_feature_extractor_map.items():
+    for extractor_type, extractor_class in self.ssd_feature_extractors().items(
+    ):
       model_proto.ssd.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
@@ -206,12 +211,9 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
 
   def test_create_ssd_fpn_model_from_config(self):
     model_proto = self.create_default_ssd_model_proto()
-    model_proto.ssd.feature_extractor.type = 'ssd_resnet101_v1_fpn'
     model_proto.ssd.feature_extractor.fpn.min_level = 3
     model_proto.ssd.feature_extractor.fpn.max_level = 7
     model = model_builder.build(model_proto, is_training=True)
-    self.assertIsInstance(model._feature_extractor,
-                          ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor)
     self.assertEqual(model._feature_extractor._fpn_min_level, 3)
     self.assertEqual(model._feature_extractor._fpn_max_level, 7)
 
@@ -238,8 +240,9 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
           'enable_mask_prediction': False
       },
   )
-  def test_create_faster_rcnn_models_from_config(
-      self, use_matmul_crop_and_resize, enable_mask_prediction):
+  def test_create_faster_rcnn_models_from_config(self,
+                                                 use_matmul_crop_and_resize,
+                                                 enable_mask_prediction):
     model_proto = self.create_default_faster_rcnn_model_proto()
     faster_rcnn_config = model_proto.faster_rcnn
     faster_rcnn_config.use_matmul_crop_and_resize = use_matmul_crop_and_resize
@@ -250,7 +253,7 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
       mask_predictor_config.predict_instance_masks = True
 
     for extractor_type, extractor_class in (
-        model_builder.FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP.items()):
+        self.faster_rcnn_feature_extractors().items()):
       faster_rcnn_config.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
@@ -270,52 +273,59 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
         model_proto.faster_rcnn.second_stage_box_predictor.rfcn_box_predictor)
     rfcn_predictor_config.conv_hyperparams.op = hyperparams_pb2.Hyperparams.CONV
     for extractor_type, extractor_class in (
-        model_builder.FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP.items()):
+        self.faster_rcnn_feature_extractors().items()):
       model_proto.faster_rcnn.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)
       self.assertIsInstance(model._feature_extractor, extractor_class)
 
+  @parameterized.parameters(True, False)
+  def test_create_faster_rcnn_from_config_with_crop_feature(
+      self, output_final_box_features):
+    model_proto = self.create_default_faster_rcnn_model_proto()
+    model_proto.faster_rcnn.output_final_box_features = (
+        output_final_box_features)
+    _ = model_builder.build(model_proto, is_training=True)
+
   def test_invalid_model_config_proto(self):
     model_proto = ''
-    with self.assertRaisesRegexp(
+    with self.assertRaisesRegex(
         ValueError, 'model_config not of type model_pb2.DetectionModel.'):
       model_builder.build(model_proto, is_training=True)
 
   def test_unknown_meta_architecture(self):
     model_proto = model_pb2.DetectionModel()
-    with self.assertRaisesRegexp(ValueError, 'Unknown meta architecture'):
+    with self.assertRaisesRegex(ValueError, 'Unknown meta architecture'):
       model_builder.build(model_proto, is_training=True)
 
   def test_unknown_ssd_feature_extractor(self):
     model_proto = self.create_default_ssd_model_proto()
     model_proto.ssd.feature_extractor.type = 'unknown_feature_extractor'
-    with self.assertRaisesRegexp(ValueError, 'Unknown ssd feature_extractor'):
+    with self.assertRaises(ValueError):
       model_builder.build(model_proto, is_training=True)
 
   def test_unknown_faster_rcnn_feature_extractor(self):
     model_proto = self.create_default_faster_rcnn_model_proto()
     model_proto.faster_rcnn.feature_extractor.type = 'unknown_feature_extractor'
-    with self.assertRaisesRegexp(ValueError,
-                                 'Unknown Faster R-CNN feature_extractor'):
+    with self.assertRaises(ValueError):
       model_builder.build(model_proto, is_training=True)
 
   def test_invalid_first_stage_nms_iou_threshold(self):
     model_proto = self.create_default_faster_rcnn_model_proto()
     model_proto.faster_rcnn.first_stage_nms_iou_threshold = 1.1
-    with self.assertRaisesRegexp(ValueError,
-                                 r'iou_threshold not in \[0, 1\.0\]'):
+    with self.assertRaisesRegex(ValueError,
+                                r'iou_threshold not in \[0, 1\.0\]'):
       model_builder.build(model_proto, is_training=True)
     model_proto.faster_rcnn.first_stage_nms_iou_threshold = -0.1
-    with self.assertRaisesRegexp(ValueError,
-                                 r'iou_threshold not in \[0, 1\.0\]'):
+    with self.assertRaisesRegex(ValueError,
+                                r'iou_threshold not in \[0, 1\.0\]'):
       model_builder.build(model_proto, is_training=True)
 
   def test_invalid_second_stage_batch_size(self):
     model_proto = self.create_default_faster_rcnn_model_proto()
     model_proto.faster_rcnn.first_stage_max_proposals = 1
     model_proto.faster_rcnn.second_stage_batch_size = 2
-    with self.assertRaisesRegexp(
+    with self.assertRaisesRegex(
         ValueError, 'second_stage_batch_size should be no greater '
         'than first_stage_max_proposals.'):
       model_builder.build(model_proto, is_training=True)
@@ -323,8 +333,8 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
   def test_invalid_faster_rcnn_batchnorm_update(self):
     model_proto = self.create_default_faster_rcnn_model_proto()
     model_proto.faster_rcnn.inplace_batchnorm_update = True
-    with self.assertRaisesRegexp(ValueError,
-                                 'inplace batchnorm updates not supported'):
+    with self.assertRaisesRegex(ValueError,
+                                'inplace batchnorm updates not supported'):
       model_builder.build(model_proto, is_training=True)
 
   def test_create_experimental_model(self):
@@ -340,7 +350,3 @@ class ModelBuilderTest(tf.test.TestCase, parameterized.TestCase):
     text_format.Merge(model_text_proto, model_proto)
 
     self.assertEqual(model_builder.build(model_proto, is_training=True), 42)
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/builders/model_builder_tf1_test.py b/research/object_detection/builders/model_builder_tf1_test.py
new file mode 100644
index 00000000..eedef968
--- /dev/null
+++ b/research/object_detection/builders/model_builder_tf1_test.py
@@ -0,0 +1,44 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for model_builder under TensorFlow 1.X."""
+
+from absl.testing import parameterized
+import tensorflow as tf
+
+from object_detection.builders import model_builder
+from object_detection.builders import model_builder_test
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.protos import losses_pb2
+
+
+class ModelBuilderTF1Test(model_builder_test.ModelBuilderTest):
+
+  def default_ssd_feature_extractor(self):
+    return 'ssd_resnet50_v1_fpn'
+
+  def default_faster_rcnn_feature_extractor(self):
+    return 'faster_rcnn_resnet101'
+
+  def ssd_feature_extractors(self):
+    return model_builder.SSD_FEATURE_EXTRACTOR_CLASS_MAP
+
+  def faster_rcnn_feature_extractors(self):
+    return model_builder.FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP
+
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index 1cd2a61b..3576b18c 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -18,6 +18,7 @@
 import tensorflow as tf
 
 
+from tensorflow.contrib import opt as tf_opt
 from object_detection.utils import learning_schedules
 
 
@@ -64,14 +65,14 @@ def build_optimizers_tf_v1(optimizer_config, global_step=None):
     learning_rate = _create_learning_rate(config.learning_rate,
                                           global_step=global_step)
     summary_vars.append(learning_rate)
-    optimizer = tf.train.AdamOptimizer(learning_rate)
+    optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=config.epsilon)
 
 
   if optimizer is None:
     raise ValueError('Optimizer %s not supported.' % optimizer_type)
 
   if optimizer_config.use_moving_average:
-    optimizer = tf.contrib.opt.MovingAverageOptimizer(
+    optimizer = tf_opt.MovingAverageOptimizer(
         optimizer, average_decay=optimizer_config.moving_average_decay)
 
   return optimizer, summary_vars
@@ -120,7 +121,7 @@ def build_optimizers_tf_v2(optimizer_config, global_step=None):
     learning_rate = _create_learning_rate(config.learning_rate,
                                           global_step=global_step)
     summary_vars.append(learning_rate)
-    optimizer = tf.keras.optimizers.Adam(learning_rate)
+    optimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=config.epsilon)
 
   if optimizer is None:
     raise ValueError('Optimizer %s not supported.' % optimizer_type)
diff --git a/research/object_detection/builders/optimizer_builder_test.py b/research/object_detection/builders/optimizer_builder_tf1_test.py
similarity index 86%
rename from research/object_detection/builders/optimizer_builder_test.py
rename to research/object_detection/builders/optimizer_builder_tf1_test.py
index 343a858f..aafb53e6 100644
--- a/research/object_detection/builders/optimizer_builder_test.py
+++ b/research/object_detection/builders/optimizer_builder_tf1_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,11 @@
 
 """Tests for optimizer_builder."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import six
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -22,6 +28,14 @@ from google.protobuf import text_format
 from object_detection.builders import optimizer_builder
 from object_detection.protos import optimizer_pb2
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import opt as contrib_opt
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 
 class LearningRateBuilderTest(tf.test.TestCase):
 
@@ -35,7 +49,8 @@ class LearningRateBuilderTest(tf.test.TestCase):
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
         learning_rate_proto)
-    self.assertTrue(learning_rate.op.name.endswith('learning_rate'))
+    self.assertTrue(
+        six.ensure_str(learning_rate.op.name).endswith('learning_rate'))
     with self.test_session():
       learning_rate_out = learning_rate.eval()
     self.assertAlmostEqual(learning_rate_out, 0.004)
@@ -53,8 +68,9 @@ class LearningRateBuilderTest(tf.test.TestCase):
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
         learning_rate_proto)
-    self.assertTrue(learning_rate.op.name.endswith('learning_rate'))
-    self.assertTrue(isinstance(learning_rate, tf.Tensor))
+    self.assertTrue(
+        six.ensure_str(learning_rate.op.name).endswith('learning_rate'))
+    self.assertIsInstance(learning_rate, tf.Tensor)
 
   def testBuildManualStepLearningRate(self):
     learning_rate_text_proto = """
@@ -75,7 +91,7 @@ class LearningRateBuilderTest(tf.test.TestCase):
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
         learning_rate_proto)
-    self.assertTrue(isinstance(learning_rate, tf.Tensor))
+    self.assertIsInstance(learning_rate, tf.Tensor)
 
   def testBuildCosineDecayLearningRate(self):
     learning_rate_text_proto = """
@@ -91,7 +107,7 @@ class LearningRateBuilderTest(tf.test.TestCase):
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
         learning_rate_proto)
-    self.assertTrue(isinstance(learning_rate, tf.Tensor))
+    self.assertIsInstance(learning_rate, tf.Tensor)
 
   def testRaiseErrorOnEmptyLearningRate(self):
     learning_rate_text_proto = """
@@ -123,7 +139,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.RMSPropOptimizer))
+    self.assertIsInstance(optimizer, tf.train.RMSPropOptimizer)
 
   def testBuildMomentumOptimizer(self):
     optimizer_text_proto = """
@@ -140,11 +156,12 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.MomentumOptimizer))
+    self.assertIsInstance(optimizer, tf.train.MomentumOptimizer)
 
   def testBuildAdamOptimizer(self):
     optimizer_text_proto = """
       adam_optimizer: {
+        epsilon: 1e-6
         learning_rate: {
           constant_learning_rate {
             learning_rate: 0.002
@@ -156,7 +173,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(isinstance(optimizer, tf.train.AdamOptimizer))
+    self.assertIsInstance(optimizer, tf.train.AdamOptimizer)
 
   def testBuildMovingAverageOptimizer(self):
     optimizer_text_proto = """
@@ -172,8 +189,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(
-        isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
+    self.assertIsInstance(optimizer, contrib_opt.MovingAverageOptimizer)
 
   def testBuildMovingAverageOptimizerWithNonDefaultDecay(self):
     optimizer_text_proto = """
@@ -190,8 +206,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     optimizer, _ = optimizer_builder.build(optimizer_proto)
-    self.assertTrue(
-        isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
+    self.assertIsInstance(optimizer, contrib_opt.MovingAverageOptimizer)
     # TODO(rathodv): Find a way to not depend on the private members.
     self.assertAlmostEqual(optimizer._ema._decay, 0.2)
 
diff --git a/research/object_detection/builders/post_processing_builder.py b/research/object_detection/builders/post_processing_builder.py
index 35126aba..16b6a4c3 100644
--- a/research/object_detection/builders/post_processing_builder.py
+++ b/research/object_detection/builders/post_processing_builder.py
@@ -102,7 +102,7 @@ def _build_non_max_suppressor(nms_config):
       soft_nms_sigma=nms_config.soft_nms_sigma,
       use_partitioned_nms=nms_config.use_partitioned_nms,
       use_combined_nms=nms_config.use_combined_nms,
-      change_coordinate_frame=True)
+      change_coordinate_frame=nms_config.change_coordinate_frame)
 
   return non_max_suppressor_fn
 
@@ -110,7 +110,7 @@ def _build_non_max_suppressor(nms_config):
 def _score_converter_fn_with_logit_scale(tf_score_converter_fn, logit_scale):
   """Create a function to scale logits then apply a Tensorflow function."""
   def score_converter_fn(logits):
-    scaled_logits = tf.divide(logits, logit_scale, name='scale_logits')
+    scaled_logits = tf.multiply(logits, 1.0 / logit_scale, name='scale_logits')
     return tf_score_converter_fn(scaled_logits, name='convert_scores')
   score_converter_fn.__name__ = '%s_with_logit_scale' % (
       tf_score_converter_fn.__name__)
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index de71905c..a6723fe1 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -150,7 +150,7 @@ def build(preprocessor_step_config):
     return (preprocessor.random_horizontal_flip,
             {
                 'keypoint_flip_permutation': tuple(
-                    config.keypoint_flip_permutation),
+                    config.keypoint_flip_permutation) or None,
             })
 
   if step_type == 'random_vertical_flip':
@@ -158,7 +158,7 @@ def build(preprocessor_step_config):
     return (preprocessor.random_vertical_flip,
             {
                 'keypoint_flip_permutation': tuple(
-                    config.keypoint_flip_permutation),
+                    config.keypoint_flip_permutation) or None,
             })
 
   if step_type == 'random_rotation90':
@@ -400,4 +400,13 @@ def build(preprocessor_step_config):
       kwargs['random_coef'] = [op.random_coef for op in config.operations]
     return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio, kwargs)
 
+  if step_type == 'random_square_crop_by_scale':
+    config = preprocessor_step_config.random_square_crop_by_scale
+    return preprocessor.random_square_crop_by_scale, {
+        'scale_min': config.scale_min,
+        'scale_max': config.scale_max,
+        'max_border': config.max_border,
+        'num_scales': config.num_scales
+    }
+
   raise ValueError('Unknown preprocessing step.')
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index 8e3343ac..28ab8fb9 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -723,6 +723,25 @@ class PreprocessorBuilderTest(tf.test.TestCase):
     self.assertEqual(function, preprocessor.convert_class_logits_to_softmax)
     self.assertEqual(args, {'temperature': 2})
 
+  def test_random_crop_by_scale(self):
+    preprocessor_text_proto = """
+    random_square_crop_by_scale {
+      scale_min: 0.25
+      scale_max: 2.0
+      num_scales: 8
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Merge(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.random_square_crop_by_scale)
+    self.assertEqual(args, {
+        'scale_min': 0.25,
+        'scale_max': 2.0,
+        'num_scales': 8,
+        'max_border': 128
+    })
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/balanced_positive_negative_sampler.py b/research/object_detection/core/balanced_positive_negative_sampler.py
index 89c1fc7f..1d28090a 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler.py
@@ -33,7 +33,6 @@ when number of examples set to True in indicator is less than batch_size.
 import tensorflow as tf
 
 from object_detection.core import minibatch_sampler
-from object_detection.utils import ops
 
 
 class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
@@ -158,19 +157,17 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
     # Shuffle indicator and label. Need to store the permutation to restore the
     # order post sampling.
     permutation = tf.random_shuffle(tf.range(input_length))
-    indicator = ops.matmul_gather_on_zeroth_axis(
-        tf.cast(indicator, tf.float32), permutation)
-    labels = ops.matmul_gather_on_zeroth_axis(
-        tf.cast(labels, tf.float32), permutation)
+    indicator = tf.gather(indicator, permutation, axis=0)
+    labels = tf.gather(labels, permutation, axis=0)
 
     # index (starting from 1) when indicator is True, 0 when False
     indicator_idx = tf.where(
-        tf.cast(indicator, tf.bool), tf.range(1, input_length + 1),
+        indicator, tf.range(1, input_length + 1),
         tf.zeros(input_length, tf.int32))
 
     # Replace -1 for negative, +1 for positive labels
     signed_label = tf.where(
-        tf.cast(labels, tf.bool), tf.ones(input_length, tf.int32),
+        labels, tf.ones(input_length, tf.int32),
         tf.scalar_mul(-1, tf.ones(input_length, tf.int32)))
     # negative of index for negative label, positive index for positive label,
     # 0 when indicator is False.
@@ -198,11 +195,10 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
         axis=0), tf.bool)
 
     # project back the order based on stored permutations
-    reprojections = tf.one_hot(permutation, depth=input_length,
-                               dtype=tf.float32)
-    return tf.cast(tf.tensordot(
-        tf.cast(sampled_idx_indicator, tf.float32),
-        reprojections, axes=[0, 0]), tf.bool)
+    idx_indicator = tf.scatter_nd(
+        tf.expand_dims(permutation, -1), sampled_idx_indicator,
+        shape=(input_length,))
+    return idx_indicator
 
   def subsample(self, indicator, batch_size, labels, scope=None):
     """Returns subsampled minibatch.
diff --git a/research/object_detection/core/balanced_positive_negative_sampler_test.py b/research/object_detection/core/balanced_positive_negative_sampler_test.py
index 1df28e4c..b302b802 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler_test.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler_test.py
@@ -24,24 +24,27 @@ from object_detection.utils import test_case
 
 class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
 
-  def test_subsample_all_examples_dynamic(self):
+  def test_subsample_all_examples(self):
+    if self.has_tpu(): return
     numpy_labels = np.random.permutation(300)
-    indicator = tf.constant(np.ones(300) == 1)
+    indicator = np.array(np.ones(300) == 1, np.bool)
     numpy_labels = (numpy_labels - 200) > 0
 
-    labels = tf.constant(numpy_labels)
+    labels = np.array(numpy_labels, np.bool)
 
-    sampler = (
-        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
-    is_sampled = sampler.subsample(indicator, 64, labels)
-    with self.test_session() as sess:
-      is_sampled = sess.run(is_sampled)
-      self.assertTrue(sum(is_sampled) == 64)
-      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 32)
-      self.assertTrue(sum(np.logical_and(
-          np.logical_not(numpy_labels), is_sampled)) == 32)
+    def graph_fn(indicator, labels):
+      sampler = (
+          balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
+      return sampler.subsample(indicator, 64, labels)
+
+    is_sampled = self.execute_cpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 64)
+    self.assertEqual(sum(np.logical_and(numpy_labels, is_sampled)), 32)
+    self.assertEqual(sum(np.logical_and(
+        np.logical_not(numpy_labels), is_sampled)), 32)
 
   def test_subsample_all_examples_static(self):
+    if not self.has_tpu(): return
     numpy_labels = np.random.permutation(300)
     indicator = np.array(np.ones(300) == 1, np.bool)
     numpy_labels = (numpy_labels - 200) > 0
@@ -54,35 +57,37 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
               is_static=True))
       return sampler.subsample(indicator, 64, labels)
 
-    is_sampled = self.execute(graph_fn, [indicator, labels])
-    self.assertTrue(sum(is_sampled) == 64)
-    self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 32)
-    self.assertTrue(sum(np.logical_and(
-        np.logical_not(numpy_labels), is_sampled)) == 32)
+    is_sampled = self.execute_tpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 64)
+    self.assertEqual(sum(np.logical_and(numpy_labels, is_sampled)), 32)
+    self.assertEqual(sum(np.logical_and(
+        np.logical_not(numpy_labels), is_sampled)), 32)
 
-  def test_subsample_selection_dynamic(self):
+  def test_subsample_selection(self):
+    if self.has_tpu(): return
     # Test random sampling when only some examples can be sampled:
-    # 100 samples, 20 positives, 10 positives cannot be sampled
+    # 100 samples, 20 positives, 10 positives cannot be sampled.
     numpy_labels = np.arange(100)
     numpy_indicator = numpy_labels < 90
-    indicator = tf.constant(numpy_indicator)
+    indicator = np.array(numpy_indicator, np.bool)
     numpy_labels = (numpy_labels - 80) >= 0
 
-    labels = tf.constant(numpy_labels)
+    labels = np.array(numpy_labels, np.bool)
 
-    sampler = (
-        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
-    is_sampled = sampler.subsample(indicator, 64, labels)
-    with self.test_session() as sess:
-      is_sampled = sess.run(is_sampled)
-      self.assertTrue(sum(is_sampled) == 64)
-      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10)
-      self.assertTrue(sum(np.logical_and(
-          np.logical_not(numpy_labels), is_sampled)) == 54)
-      self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
-                                                     numpy_indicator))
+    def graph_fn(indicator, labels):
+      sampler = (
+          balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
+      return sampler.subsample(indicator, 64, labels)
+
+    is_sampled = self.execute_cpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 64)
+    self.assertEqual(sum(np.logical_and(numpy_labels, is_sampled)), 10)
+    self.assertEqual(sum(np.logical_and(
+        np.logical_not(numpy_labels), is_sampled)), 54)
+    self.assertAllEqual(is_sampled, np.logical_and(is_sampled, numpy_indicator))
 
   def test_subsample_selection_static(self):
+    if not self.has_tpu(): return
     # Test random sampling when only some examples can be sampled:
     # 100 samples, 20 positives, 10 positives cannot be sampled.
     numpy_labels = np.arange(100)
@@ -98,37 +103,41 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
               is_static=True))
       return sampler.subsample(indicator, 64, labels)
 
-    is_sampled = self.execute(graph_fn, [indicator, labels])
-    self.assertTrue(sum(is_sampled) == 64)
-    self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10)
-    self.assertTrue(sum(np.logical_and(
-        np.logical_not(numpy_labels), is_sampled)) == 54)
+    is_sampled = self.execute_tpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 64)
+    self.assertEqual(sum(np.logical_and(numpy_labels, is_sampled)), 10)
+    self.assertEqual(sum(np.logical_and(
+        np.logical_not(numpy_labels), is_sampled)), 54)
     self.assertAllEqual(is_sampled, np.logical_and(is_sampled, numpy_indicator))
 
-  def test_subsample_selection_larger_batch_size_dynamic(self):
+  def test_subsample_selection_larger_batch_size(self):
+    if self.has_tpu(): return
     # Test random sampling when total number of examples that can be sampled are
     # less than batch size:
     # 100 samples, 50 positives, 40 positives cannot be sampled, batch size 64.
+    # It should still return 64 samples, with 4 of them that couldn't have been
+    # sampled.
     numpy_labels = np.arange(100)
     numpy_indicator = numpy_labels < 60
-    indicator = tf.constant(numpy_indicator)
+    indicator = np.array(numpy_indicator, np.bool)
     numpy_labels = (numpy_labels - 50) >= 0
 
-    labels = tf.constant(numpy_labels)
+    labels = np.array(numpy_labels, np.bool)
 
-    sampler = (
-        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
-    is_sampled = sampler.subsample(indicator, 64, labels)
-    with self.test_session() as sess:
-      is_sampled = sess.run(is_sampled)
-      self.assertTrue(sum(is_sampled) == 60)
-      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10)
-      self.assertTrue(
-          sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)) == 50)
-      self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
-                                                     numpy_indicator))
+    def graph_fn(indicator, labels):
+      sampler = (
+          balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
+      return sampler.subsample(indicator, 64, labels)
+
+    is_sampled = self.execute_cpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 60)
+    self.assertGreaterEqual(sum(np.logical_and(numpy_labels, is_sampled)), 10)
+    self.assertGreaterEqual(
+        sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)), 50)
+    self.assertEqual(sum(np.logical_and(is_sampled, numpy_indicator)), 60)
 
   def test_subsample_selection_larger_batch_size_static(self):
+    if not self.has_tpu(): return
     # Test random sampling when total number of examples that can be sampled are
     # less than batch size:
     # 100 samples, 50 positives, 40 positives cannot be sampled, batch size 64.
@@ -147,34 +156,33 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
               is_static=True))
       return sampler.subsample(indicator, 64, labels)
 
-    is_sampled = self.execute(graph_fn, [indicator, labels])
-    self.assertTrue(sum(is_sampled) == 64)
-    self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) >= 10)
-    self.assertTrue(
-        sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)) >= 50)
-    self.assertTrue(sum(np.logical_and(is_sampled, numpy_indicator)) == 60)
+    is_sampled = self.execute_tpu(graph_fn, [indicator, labels])
+    self.assertEqual(sum(is_sampled), 64)
+    self.assertGreaterEqual(sum(np.logical_and(numpy_labels, is_sampled)), 10)
+    self.assertGreaterEqual(
+        sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)), 50)
+    self.assertEqual(sum(np.logical_and(is_sampled, numpy_indicator)), 60)
 
   def test_subsample_selection_no_batch_size(self):
+    if self.has_tpu(): return
     # Test random sampling when only some examples can be sampled:
     # 1000 samples, 6 positives (5 can be sampled).
     numpy_labels = np.arange(1000)
     numpy_indicator = numpy_labels < 999
-    indicator = tf.constant(numpy_indicator)
     numpy_labels = (numpy_labels - 994) >= 0
 
-    labels = tf.constant(numpy_labels)
-
-    sampler = (balanced_positive_negative_sampler.
-               BalancedPositiveNegativeSampler(0.01))
-    is_sampled = sampler.subsample(indicator, None, labels)
-    with self.test_session() as sess:
-      is_sampled = sess.run(is_sampled)
-      self.assertTrue(sum(is_sampled) == 500)
-      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 5)
-      self.assertTrue(sum(np.logical_and(
-          np.logical_not(numpy_labels), is_sampled)) == 495)
-      self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
-                                                     numpy_indicator))
+    def graph_fn(indicator, labels):
+      sampler = (balanced_positive_negative_sampler.
+                 BalancedPositiveNegativeSampler(0.01))
+      is_sampled = sampler.subsample(indicator, None, labels)
+      return is_sampled
+    is_sampled_out = self.execute_cpu(graph_fn, [numpy_indicator, numpy_labels])
+    self.assertEqual(sum(is_sampled_out), 500)
+    self.assertEqual(sum(np.logical_and(numpy_labels, is_sampled_out)), 5)
+    self.assertEqual(sum(np.logical_and(
+        np.logical_not(numpy_labels), is_sampled_out)), 495)
+    self.assertAllEqual(is_sampled_out, np.logical_and(is_sampled_out,
+                                                       numpy_indicator))
 
   def test_subsample_selection_no_batch_size_static(self):
     labels = tf.constant([[True, False, False]])
diff --git a/research/object_detection/core/batcher.py b/research/object_detection/core/batcher.py
index 825b90d8..95714faf 100644
--- a/research/object_detection/core/batcher.py
+++ b/research/object_detection/core/batcher.py
@@ -24,6 +24,10 @@ from six.moves import range
 import tensorflow as tf
 
 from object_detection.core import prefetcher
+from object_detection.utils import tf_version
+
+if not tf_version.is_tf1():
+  raise ValueError('`batcher.py` is only supported in Tensorflow 1.X')
 
 rt_shape_str = '_runtime_shapes'
 
diff --git a/research/object_detection/core/batcher_test.py b/research/object_detection/core/batcher_tf1_test.py
similarity index 98%
rename from research/object_detection/core/batcher_test.py
rename to research/object_detection/core/batcher_tf1_test.py
index a6c9faf2..ed648b89 100644
--- a/research/object_detection/core/batcher_test.py
+++ b/research/object_detection/core/batcher_tf1_test.py
@@ -22,10 +22,11 @@ from __future__ import print_function
 import numpy as np
 from six.moves import range
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.core import batcher
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class BatcherTest(tf.test.TestCase):
diff --git a/research/object_detection/core/box_coder_test.py b/research/object_detection/core/box_coder_test.py
index c087a325..5c494813 100644
--- a/research/object_detection/core/box_coder_test.py
+++ b/research/object_detection/core/box_coder_test.py
@@ -14,11 +14,11 @@
 # ==============================================================================
 
 """Tests for object_detection.core.box_coder."""
-
 import tensorflow as tf
 
 from object_detection.core import box_coder
 from object_detection.core import box_list
+from object_detection.utils import test_case
 
 
 class MockBoxCoder(box_coder.BoxCoder):
@@ -34,27 +34,28 @@ class MockBoxCoder(box_coder.BoxCoder):
     return box_list.BoxList(rel_codes / 2.0)
 
 
-class BoxCoderTest(tf.test.TestCase):
+class BoxCoderTest(test_case.TestCase):
 
   def test_batch_decode(self):
-    mock_anchor_corners = tf.constant(
-        [[0, 0.1, 0.2, 0.3], [0.2, 0.4, 0.4, 0.6]], tf.float32)
-    mock_anchors = box_list.BoxList(mock_anchor_corners)
-    mock_box_coder = MockBoxCoder()
 
     expected_boxes = [[[0.0, 0.1, 0.5, 0.6], [0.5, 0.6, 0.7, 0.8]],
                       [[0.1, 0.2, 0.3, 0.4], [0.7, 0.8, 0.9, 1.0]]]
 
-    encoded_boxes_list = [mock_box_coder.encode(
-        box_list.BoxList(tf.constant(boxes)), mock_anchors)
-                          for boxes in expected_boxes]
-    encoded_boxes = tf.stack(encoded_boxes_list)
-    decoded_boxes = box_coder.batch_decode(
-        encoded_boxes, mock_box_coder, mock_anchors)
-
-    with self.test_session() as sess:
-      decoded_boxes_result = sess.run(decoded_boxes)
-      self.assertAllClose(expected_boxes, decoded_boxes_result)
+    def graph_fn():
+      mock_anchor_corners = tf.constant(
+          [[0, 0.1, 0.2, 0.3], [0.2, 0.4, 0.4, 0.6]], tf.float32)
+      mock_anchors = box_list.BoxList(mock_anchor_corners)
+      mock_box_coder = MockBoxCoder()
+
+      encoded_boxes_list = [mock_box_coder.encode(
+          box_list.BoxList(tf.constant(boxes)), mock_anchors)
+                            for boxes in expected_boxes]
+      encoded_boxes = tf.stack(encoded_boxes_list)
+      decoded_boxes = box_coder.batch_decode(
+          encoded_boxes, mock_box_coder, mock_anchors)
+      return decoded_boxes
+    decoded_boxes_result = self.execute(graph_fn, [])
+    self.assertAllClose(expected_boxes, decoded_boxes_result)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index 0bd3788f..06091a34 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -105,6 +105,31 @@ def scale(boxlist, y_scale, x_scale, scope=None):
     return _copy_extra_fields(scaled_boxlist, boxlist)
 
 
+def scale_height_width(boxlist, y_scale, x_scale, scope=None):
+  """Scale the height and width of boxes, leaving centers unchanged.
+
+  Args:
+    boxlist: BoxList holding N boxes
+    y_scale: (float) scalar tensor
+    x_scale: (float) scalar tensor
+    scope: name scope.
+
+  Returns:
+    boxlist: BoxList holding N boxes
+  """
+  with tf.name_scope(scope, 'ScaleHeightWidth'):
+    y_scale = tf.cast(y_scale, tf.float32)
+    x_scale = tf.cast(x_scale, tf.float32)
+    yc, xc, height_orig, width_orig = boxlist.get_center_coordinates_and_sizes()
+    y_min = yc - 0.5 * y_scale * height_orig
+    y_max = yc + 0.5 * y_scale * height_orig
+    x_min = xc - 0.5 * x_scale * width_orig
+    x_max = xc + 0.5 * x_scale * width_orig
+    scaled_boxlist = box_list.BoxList(
+        tf.stack([y_min, x_min, y_max, x_max], 1))
+    return _copy_extra_fields(scaled_boxlist, boxlist)
+
+
 def clip_to_window(boxlist, window, filter_nonoverlapping=True, scope=None):
   """Clip bounding boxes to a window.
 
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index efe29913..810ed68c 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -26,342 +26,371 @@ class BoxListOpsTest(test_case.TestCase):
   """Tests for common bounding box operations."""
 
   def test_area(self):
-    corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])
+    def graph_fn():
+      corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])
+      boxes = box_list.BoxList(corners)
+      areas = box_list_ops.area(boxes)
+      return areas
+    areas_out = self.execute(graph_fn, [])
     exp_output = [200.0, 4.0]
-    boxes = box_list.BoxList(corners)
-    areas = box_list_ops.area(boxes)
-    with self.test_session() as sess:
-      areas_output = sess.run(areas)
-      self.assertAllClose(areas_output, exp_output)
+    self.assertAllClose(areas_out, exp_output)
 
   def test_height_width(self):
-    corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])
+    def graph_fn():
+      corners = tf.constant([[0.0, 0.0, 10.0, 20.0], [1.0, 2.0, 3.0, 4.0]])
+      boxes = box_list.BoxList(corners)
+      return box_list_ops.height_width(boxes)
+    heights_out, widths_out = self.execute(graph_fn, [])
     exp_output_heights = [10., 2.]
     exp_output_widths = [20., 2.]
-    boxes = box_list.BoxList(corners)
-    heights, widths = box_list_ops.height_width(boxes)
-    with self.test_session() as sess:
-      output_heights, output_widths = sess.run([heights, widths])
-      self.assertAllClose(output_heights, exp_output_heights)
-      self.assertAllClose(output_widths, exp_output_widths)
+    self.assertAllClose(heights_out, exp_output_heights)
+    self.assertAllClose(widths_out, exp_output_widths)
 
   def test_scale(self):
-    corners = tf.constant([[0, 0, 100, 200], [50, 120, 100, 140]],
-                          dtype=tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.constant([[1], [2]]))
+    def graph_fn():
+      corners = tf.constant([[0, 0, 100, 200], [50, 120, 100, 140]],
+                            dtype=tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2]]))
 
-    y_scale = tf.constant(1.0/100)
-    x_scale = tf.constant(1.0/200)
-    scaled_boxes = box_list_ops.scale(boxes, y_scale, x_scale)
+      y_scale = tf.constant(1.0/100)
+      x_scale = tf.constant(1.0/200)
+      scaled_boxes = box_list_ops.scale(boxes, y_scale, x_scale)
+      return scaled_boxes.get(), scaled_boxes.get_field('extra_data')
+    scaled_corners_out, extra_data_out = self.execute(graph_fn, [])
     exp_output = [[0, 0, 1, 1], [0.5, 0.6, 1.0, 0.7]]
-    with self.test_session() as sess:
-      scaled_corners_out = sess.run(scaled_boxes.get())
-      self.assertAllClose(scaled_corners_out, exp_output)
-      extra_data_out = sess.run(scaled_boxes.get_field('extra_data'))
-      self.assertAllEqual(extra_data_out, [[1], [2]])
+    self.assertAllClose(scaled_corners_out, exp_output)
+    self.assertAllEqual(extra_data_out, [[1], [2]])
+
+  def test_scale_height_width(self):
+    def graph_fn():
+      corners = tf.constant([[-10, -20, 10, 20], [0, 100, 100, 200]],
+                            dtype=tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2]]))
+
+      y_scale = tf.constant(2.)
+      x_scale = tf.constant(0.5)
+      scaled_boxes = box_list_ops.scale_height_width(boxes, y_scale, x_scale)
+      return scaled_boxes.get(), scaled_boxes.get_field('extra_data')
+    exp_output = [
+        [-20., -10, 20., 10],
+        [-50., 125, 150., 175.]]
+    scaled_corners_out, extra_data_out = self.execute(graph_fn, [])
+    self.assertAllClose(scaled_corners_out, exp_output)
+    self.assertAllEqual(extra_data_out, [[1], [2]])
 
   def test_clip_to_window_filter_boxes_which_fall_outside_the_window(
       self):
-    window = tf.constant([0, 0, 9, 14], tf.float32)
-    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
-                           [-1.0, -2.0, 4.0, 5.0],
-                           [2.0, 3.0, 5.0, 9.0],
-                           [0.0, 0.0, 9.0, 14.0],
-                           [-100.0, -100.0, 300.0, 600.0],
-                           [-10.0, -10.0, -9.0, -9.0]])
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+    def graph_fn():
+      window = tf.constant([0, 0, 9, 14], tf.float32)
+      corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
+                             [-1.0, -2.0, 4.0, 5.0],
+                             [2.0, 3.0, 5.0, 9.0],
+                             [0.0, 0.0, 9.0, 14.0],
+                             [-100.0, -100.0, 300.0, 600.0],
+                             [-10.0, -10.0, -9.0, -9.0]])
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+      pruned = box_list_ops.clip_to_window(
+          boxes, window, filter_nonoverlapping=True)
+      return pruned.get(), pruned.get_field('extra_data')
     exp_output = [[5.0, 5.0, 6.0, 6.0], [0.0, 0.0, 4.0, 5.0],
                   [2.0, 3.0, 5.0, 9.0], [0.0, 0.0, 9.0, 14.0],
                   [0.0, 0.0, 9.0, 14.0]]
-    pruned = box_list_ops.clip_to_window(
-        boxes, window, filter_nonoverlapping=True)
-    with self.test_session() as sess:
-      pruned_output = sess.run(pruned.get())
-      self.assertAllClose(pruned_output, exp_output)
-      extra_data_out = sess.run(pruned.get_field('extra_data'))
-      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5]])
+    pruned_output, extra_data_out = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(pruned_output, exp_output)
+    self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5]])
 
   def test_clip_to_window_without_filtering_boxes_which_fall_outside_the_window(
       self):
-    window = tf.constant([0, 0, 9, 14], tf.float32)
-    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
-                           [-1.0, -2.0, 4.0, 5.0],
-                           [2.0, 3.0, 5.0, 9.0],
-                           [0.0, 0.0, 9.0, 14.0],
-                           [-100.0, -100.0, 300.0, 600.0],
-                           [-10.0, -10.0, -9.0, -9.0]])
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+    def graph_fn():
+      window = tf.constant([0, 0, 9, 14], tf.float32)
+      corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
+                             [-1.0, -2.0, 4.0, 5.0],
+                             [2.0, 3.0, 5.0, 9.0],
+                             [0.0, 0.0, 9.0, 14.0],
+                             [-100.0, -100.0, 300.0, 600.0],
+                             [-10.0, -10.0, -9.0, -9.0]])
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+      pruned = box_list_ops.clip_to_window(
+          boxes, window, filter_nonoverlapping=False)
+      return pruned.get(), pruned.get_field('extra_data')
+    pruned_output, extra_data_out = self.execute(graph_fn, [])
     exp_output = [[5.0, 5.0, 6.0, 6.0], [0.0, 0.0, 4.0, 5.0],
                   [2.0, 3.0, 5.0, 9.0], [0.0, 0.0, 9.0, 14.0],
                   [0.0, 0.0, 9.0, 14.0], [0.0, 0.0, 0.0, 0.0]]
-    pruned = box_list_ops.clip_to_window(
-        boxes, window, filter_nonoverlapping=False)
-    with self.test_session() as sess:
-      pruned_output = sess.run(pruned.get())
-      self.assertAllClose(pruned_output, exp_output)
-      extra_data_out = sess.run(pruned.get_field('extra_data'))
-      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5], [6]])
+    self.assertAllClose(pruned_output, exp_output)
+    self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [5], [6]])
 
   def test_prune_outside_window_filters_boxes_which_fall_outside_the_window(
       self):
-    window = tf.constant([0, 0, 9, 14], tf.float32)
-    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
-                           [-1.0, -2.0, 4.0, 5.0],
-                           [2.0, 3.0, 5.0, 9.0],
-                           [0.0, 0.0, 9.0, 14.0],
-                           [-10.0, -10.0, -9.0, -9.0],
-                           [-100.0, -100.0, 300.0, 600.0]])
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+    def graph_fn():
+      window = tf.constant([0, 0, 9, 14], tf.float32)
+      corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
+                             [-1.0, -2.0, 4.0, 5.0],
+                             [2.0, 3.0, 5.0, 9.0],
+                             [0.0, 0.0, 9.0, 14.0],
+                             [-10.0, -10.0, -9.0, -9.0],
+                             [-100.0, -100.0, 300.0, 600.0]])
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+      pruned, keep_indices = box_list_ops.prune_outside_window(boxes, window)
+      return pruned.get(), pruned.get_field('extra_data'), keep_indices
+    pruned_output, extra_data_out, keep_indices_out = self.execute_cpu(graph_fn,
+                                                                       [])
     exp_output = [[5.0, 5.0, 6.0, 6.0],
                   [2.0, 3.0, 5.0, 9.0],
                   [0.0, 0.0, 9.0, 14.0]]
-    pruned, keep_indices = box_list_ops.prune_outside_window(boxes, window)
-    with self.test_session() as sess:
-      pruned_output = sess.run(pruned.get())
-      self.assertAllClose(pruned_output, exp_output)
-      keep_indices_out = sess.run(keep_indices)
-      self.assertAllEqual(keep_indices_out, [0, 2, 3])
-      extra_data_out = sess.run(pruned.get_field('extra_data'))
-      self.assertAllEqual(extra_data_out, [[1], [3], [4]])
+    self.assertAllClose(pruned_output, exp_output)
+    self.assertAllEqual(keep_indices_out, [0, 2, 3])
+    self.assertAllEqual(extra_data_out, [[1], [3], [4]])
 
   def test_prune_completely_outside_window(self):
-    window = tf.constant([0, 0, 9, 14], tf.float32)
-    corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
-                           [-1.0, -2.0, 4.0, 5.0],
-                           [2.0, 3.0, 5.0, 9.0],
-                           [0.0, 0.0, 9.0, 14.0],
-                           [-10.0, -10.0, -9.0, -9.0],
-                           [-100.0, -100.0, 300.0, 600.0]])
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+    def graph_fn():
+      window = tf.constant([0, 0, 9, 14], tf.float32)
+      corners = tf.constant([[5.0, 5.0, 6.0, 6.0],
+                             [-1.0, -2.0, 4.0, 5.0],
+                             [2.0, 3.0, 5.0, 9.0],
+                             [0.0, 0.0, 9.0, 14.0],
+                             [-10.0, -10.0, -9.0, -9.0],
+                             [-100.0, -100.0, 300.0, 600.0]])
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.constant([[1], [2], [3], [4], [5], [6]]))
+      pruned, keep_indices = box_list_ops.prune_completely_outside_window(
+          boxes, window)
+      return pruned.get(), pruned.get_field('extra_data'), keep_indices
+    pruned_output, extra_data_out, keep_indices_out = self.execute(graph_fn, [])
     exp_output = [[5.0, 5.0, 6.0, 6.0],
                   [-1.0, -2.0, 4.0, 5.0],
                   [2.0, 3.0, 5.0, 9.0],
                   [0.0, 0.0, 9.0, 14.0],
                   [-100.0, -100.0, 300.0, 600.0]]
-    pruned, keep_indices = box_list_ops.prune_completely_outside_window(boxes,
-                                                                        window)
-    with self.test_session() as sess:
-      pruned_output = sess.run(pruned.get())
-      self.assertAllClose(pruned_output, exp_output)
-      keep_indices_out = sess.run(keep_indices)
-      self.assertAllEqual(keep_indices_out, [0, 1, 2, 3, 5])
-      extra_data_out = sess.run(pruned.get_field('extra_data'))
-      self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [6]])
+    self.assertAllClose(pruned_output, exp_output)
+    self.assertAllEqual(keep_indices_out, [0, 1, 2, 3, 5])
+    self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [6]])
 
   def test_prune_completely_outside_window_with_empty_boxlist(self):
-    window = tf.constant([0, 0, 9, 14], tf.float32)
-    corners = tf.zeros(shape=[0, 4], dtype=tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('extra_data', tf.zeros(shape=[0], dtype=tf.int32))
-    pruned, keep_indices = box_list_ops.prune_completely_outside_window(boxes,
-                                                                        window)
-    pruned_boxes = pruned.get()
-    extra = pruned.get_field('extra_data')
-
+    def graph_fn():
+      window = tf.constant([0, 0, 9, 14], tf.float32)
+      corners = tf.zeros(shape=[0, 4], dtype=tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('extra_data', tf.zeros(shape=[0], dtype=tf.int32))
+      pruned, keep_indices = box_list_ops.prune_completely_outside_window(
+          boxes, window)
+      pruned_boxes = pruned.get()
+      extra = pruned.get_field('extra_data')
+      return pruned_boxes, extra, keep_indices
+
+    pruned_boxes_out, extra_out, keep_indices_out = self.execute(graph_fn, [])
     exp_pruned_boxes = np.zeros(shape=[0, 4], dtype=np.float32)
     exp_extra = np.zeros(shape=[0], dtype=np.int32)
-    with self.test_session() as sess:
-      pruned_boxes_out, keep_indices_out, extra_out = sess.run(
-          [pruned_boxes, keep_indices, extra])
-      self.assertAllClose(exp_pruned_boxes, pruned_boxes_out)
-      self.assertAllEqual([], keep_indices_out)
-      self.assertAllEqual(exp_extra, extra_out)
+    self.assertAllClose(exp_pruned_boxes, pruned_boxes_out)
+    self.assertAllEqual([], keep_indices_out)
+    self.assertAllEqual(exp_extra, extra_out)
 
   def test_intersection(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      intersect = box_list_ops.intersection(boxes1, boxes2)
+      return intersect
     exp_output = [[2.0, 0.0, 6.0], [1.0, 0.0, 5.0]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    intersect = box_list_ops.intersection(boxes1, boxes2)
-    with self.test_session() as sess:
-      intersect_output = sess.run(intersect)
-      self.assertAllClose(intersect_output, exp_output)
+    intersect_out = self.execute(graph_fn, [])
+    self.assertAllClose(intersect_out, exp_output)
 
   def test_matched_intersection(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      intersect = box_list_ops.matched_intersection(boxes1, boxes2)
+      return intersect
     exp_output = [2.0, 0.0]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    intersect = box_list_ops.matched_intersection(boxes1, boxes2)
-    with self.test_session() as sess:
-      intersect_output = sess.run(intersect)
-      self.assertAllClose(intersect_output, exp_output)
+    intersect_out = self.execute(graph_fn, [])
+    self.assertAllClose(intersect_out, exp_output)
 
   def test_iou(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      iou = box_list_ops.iou(boxes1, boxes2)
+      return iou
     exp_output = [[2.0 / 16.0, 0, 6.0 / 400.0], [1.0 / 16.0, 0.0, 5.0 / 400.0]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    iou = box_list_ops.iou(boxes1, boxes2)
-    with self.test_session() as sess:
-      iou_output = sess.run(iou)
-      self.assertAllClose(iou_output, exp_output)
+    iou_output = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output, exp_output)
 
   def test_matched_iou(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      iou = box_list_ops.matched_iou(boxes1, boxes2)
+      return iou
     exp_output = [2.0 / 16.0, 0]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    iou = box_list_ops.matched_iou(boxes1, boxes2)
-    with self.test_session() as sess:
-      iou_output = sess.run(iou)
-      self.assertAllClose(iou_output, exp_output)
+    iou_output = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output, exp_output)
 
   def test_iouworks_on_empty_inputs(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    boxes_empty = box_list.BoxList(tf.zeros((0, 4)))
-    iou_empty_1 = box_list_ops.iou(boxes1, boxes_empty)
-    iou_empty_2 = box_list_ops.iou(boxes_empty, boxes2)
-    iou_empty_3 = box_list_ops.iou(boxes_empty, boxes_empty)
-    with self.test_session() as sess:
-      iou_output_1, iou_output_2, iou_output_3 = sess.run(
-          [iou_empty_1, iou_empty_2, iou_empty_3])
-      self.assertAllEqual(iou_output_1.shape, (2, 0))
-      self.assertAllEqual(iou_output_2.shape, (0, 3))
-      self.assertAllEqual(iou_output_3.shape, (0, 0))
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      boxes_empty = box_list.BoxList(tf.zeros((0, 4)))
+      iou_empty_1 = box_list_ops.iou(boxes1, boxes_empty)
+      iou_empty_2 = box_list_ops.iou(boxes_empty, boxes2)
+      iou_empty_3 = box_list_ops.iou(boxes_empty, boxes_empty)
+      return iou_empty_1, iou_empty_2, iou_empty_3
+    iou_output_1, iou_output_2, iou_output_3 = self.execute(graph_fn, [])
+    self.assertAllEqual(iou_output_1.shape, (2, 0))
+    self.assertAllEqual(iou_output_2.shape, (0, 3))
+    self.assertAllEqual(iou_output_3.shape, (0, 0))
 
   def test_ioa(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      ioa_1 = box_list_ops.ioa(boxes1, boxes2)
+      ioa_2 = box_list_ops.ioa(boxes2, boxes1)
+      return ioa_1, ioa_2
     exp_output_1 = [[2.0 / 12.0, 0, 6.0 / 400.0],
                     [1.0 / 12.0, 0.0, 5.0 / 400.0]]
     exp_output_2 = [[2.0 / 6.0, 1.0 / 5.0],
                     [0, 0],
                     [6.0 / 6.0, 5.0 / 5.0]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    ioa_1 = box_list_ops.ioa(boxes1, boxes2)
-    ioa_2 = box_list_ops.ioa(boxes2, boxes1)
-    with self.test_session() as sess:
-      ioa_output_1, ioa_output_2 = sess.run([ioa_1, ioa_2])
-      self.assertAllClose(ioa_output_1, exp_output_1)
-      self.assertAllClose(ioa_output_2, exp_output_2)
+    ioa_output_1, ioa_output_2 = self.execute(graph_fn, [])
+    self.assertAllClose(ioa_output_1, exp_output_1)
+    self.assertAllClose(ioa_output_2, exp_output_2)
 
   def test_prune_non_overlapping_boxes(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    minoverlap = 0.5
-
-    exp_output_1 = boxes1
-    exp_output_2 = box_list.BoxList(tf.constant(0.0, shape=[0, 4]))
-    output_1, keep_indices_1 = box_list_ops.prune_non_overlapping_boxes(
-        boxes1, boxes2, min_overlap=minoverlap)
-    output_2, keep_indices_2 = box_list_ops.prune_non_overlapping_boxes(
-        boxes2, boxes1, min_overlap=minoverlap)
-    with self.test_session() as sess:
-      (output_1_, keep_indices_1_, output_2_, keep_indices_2_, exp_output_1_,
-       exp_output_2_) = sess.run(
-           [output_1.get(), keep_indices_1,
-            output_2.get(), keep_indices_2,
-            exp_output_1.get(), exp_output_2.get()])
-      self.assertAllClose(output_1_, exp_output_1_)
-      self.assertAllClose(output_2_, exp_output_2_)
-      self.assertAllEqual(keep_indices_1_, [0, 1])
-      self.assertAllEqual(keep_indices_2_, [])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      minoverlap = 0.5
+
+      exp_output_1 = boxes1
+      exp_output_2 = box_list.BoxList(tf.constant(0.0, shape=[0, 4]))
+      output_1, keep_indices_1 = box_list_ops.prune_non_overlapping_boxes(
+          boxes1, boxes2, min_overlap=minoverlap)
+      output_2, keep_indices_2 = box_list_ops.prune_non_overlapping_boxes(
+          boxes2, boxes1, min_overlap=minoverlap)
+      return (output_1.get(), keep_indices_1, output_2.get(), keep_indices_2,
+              exp_output_1.get(), exp_output_2.get())
+
+    (output_1_, keep_indices_1_, output_2_, keep_indices_2_, exp_output_1_,
+     exp_output_2_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(output_1_, exp_output_1_)
+    self.assertAllClose(output_2_, exp_output_2_)
+    self.assertAllEqual(keep_indices_1_, [0, 1])
+    self.assertAllEqual(keep_indices_2_, [])
 
   def test_prune_small_boxes(self):
-    boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],
-                         [5.0, 6.0, 10.0, 7.0],
-                         [3.0, 4.0, 6.0, 8.0],
-                         [14.0, 14.0, 15.0, 15.0],
-                         [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],
+                           [5.0, 6.0, 10.0, 7.0],
+                           [3.0, 4.0, 6.0, 8.0],
+                           [14.0, 14.0, 15.0, 15.0],
+                           [0.0, 0.0, 20.0, 20.0]])
+      boxes = box_list.BoxList(boxes)
+      pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)
+      return pruned_boxes.get()
     exp_boxes = [[3.0, 4.0, 6.0, 8.0],
                  [0.0, 0.0, 20.0, 20.0]]
-    boxes = box_list.BoxList(boxes)
-    pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)
-    with self.test_session() as sess:
-      pruned_boxes = sess.run(pruned_boxes.get())
-      self.assertAllEqual(pruned_boxes, exp_boxes)
+    pruned_boxes = self.execute(graph_fn, [])
+    self.assertAllEqual(pruned_boxes, exp_boxes)
 
   def test_prune_small_boxes_prunes_boxes_with_negative_side(self):
-    boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],
-                         [5.0, 6.0, 10.0, 7.0],
-                         [3.0, 4.0, 6.0, 8.0],
-                         [14.0, 14.0, 15.0, 15.0],
-                         [0.0, 0.0, 20.0, 20.0],
-                         [2.0, 3.0, 1.5, 7.0],  # negative height
-                         [2.0, 3.0, 5.0, 1.7]])  # negative width
+    def graph_fn():
+      boxes = tf.constant([[4.0, 3.0, 7.0, 5.0],
+                           [5.0, 6.0, 10.0, 7.0],
+                           [3.0, 4.0, 6.0, 8.0],
+                           [14.0, 14.0, 15.0, 15.0],
+                           [0.0, 0.0, 20.0, 20.0],
+                           [2.0, 3.0, 1.5, 7.0],  # negative height
+                           [2.0, 3.0, 5.0, 1.7]])  # negative width
+      boxes = box_list.BoxList(boxes)
+      pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)
+      return pruned_boxes.get()
     exp_boxes = [[3.0, 4.0, 6.0, 8.0],
                  [0.0, 0.0, 20.0, 20.0]]
-    boxes = box_list.BoxList(boxes)
-    pruned_boxes = box_list_ops.prune_small_boxes(boxes, 3)
-    with self.test_session() as sess:
-      pruned_boxes = sess.run(pruned_boxes.get())
-      self.assertAllEqual(pruned_boxes, exp_boxes)
+    pruned_boxes = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(pruned_boxes, exp_boxes)
 
   def test_change_coordinate_frame(self):
-    corners = tf.constant([[0.25, 0.5, 0.75, 0.75], [0.5, 0.0, 1.0, 1.0]])
-    window = tf.constant([0.25, 0.25, 0.75, 0.75])
-    boxes = box_list.BoxList(corners)
-
-    expected_corners = tf.constant([[0, 0.5, 1.0, 1.0], [0.5, -0.5, 1.5, 1.5]])
-    expected_boxes = box_list.BoxList(expected_corners)
-    output = box_list_ops.change_coordinate_frame(boxes, window)
+    def graph_fn():
+      corners = tf.constant([[0.25, 0.5, 0.75, 0.75], [0.5, 0.0, 1.0, 1.0]])
+      window = tf.constant([0.25, 0.25, 0.75, 0.75])
+      boxes = box_list.BoxList(corners)
 
-    with self.test_session() as sess:
-      output_, expected_boxes_ = sess.run([output.get(), expected_boxes.get()])
-      self.assertAllClose(output_, expected_boxes_)
+      expected_corners = tf.constant([[0, 0.5, 1.0, 1.0],
+                                      [0.5, -0.5, 1.5, 1.5]])
+      expected_boxes = box_list.BoxList(expected_corners)
+      output = box_list_ops.change_coordinate_frame(boxes, window)
+      return output.get(), expected_boxes.get()
+    output_, expected_boxes_ = self.execute(graph_fn, [])
+    self.assertAllClose(output_, expected_boxes_)
 
   def test_ioaworks_on_empty_inputs(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    boxes_empty = box_list.BoxList(tf.zeros((0, 4)))
-    ioa_empty_1 = box_list_ops.ioa(boxes1, boxes_empty)
-    ioa_empty_2 = box_list_ops.ioa(boxes_empty, boxes2)
-    ioa_empty_3 = box_list_ops.ioa(boxes_empty, boxes_empty)
-    with self.test_session() as sess:
-      ioa_output_1, ioa_output_2, ioa_output_3 = sess.run(
-          [ioa_empty_1, ioa_empty_2, ioa_empty_3])
-      self.assertAllEqual(ioa_output_1.shape, (2, 0))
-      self.assertAllEqual(ioa_output_2.shape, (0, 3))
-      self.assertAllEqual(ioa_output_3.shape, (0, 0))
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      boxes_empty = box_list.BoxList(tf.zeros((0, 4)))
+      ioa_empty_1 = box_list_ops.ioa(boxes1, boxes_empty)
+      ioa_empty_2 = box_list_ops.ioa(boxes_empty, boxes2)
+      ioa_empty_3 = box_list_ops.ioa(boxes_empty, boxes_empty)
+      return ioa_empty_1, ioa_empty_2, ioa_empty_3
+    ioa_output_1, ioa_output_2, ioa_output_3 = self.execute(graph_fn, [])
+    self.assertAllEqual(ioa_output_1.shape, (2, 0))
+    self.assertAllEqual(ioa_output_2.shape, (0, 3))
+    self.assertAllEqual(ioa_output_3.shape, (0, 0))
 
   def test_pairwise_distances(self):
-    corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],
-                            [1.0, 1.0, 0.0, 2.0]])
-    corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],
-                            [-4.0, 0.0, 0.0, 3.0],
-                            [0.0, 0.0, 0.0, 0.0]])
+    def graph_fn():
+      corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],
+                              [1.0, 1.0, 0.0, 2.0]])
+      corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],
+                              [-4.0, 0.0, 0.0, 3.0],
+                              [0.0, 0.0, 0.0, 0.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      dist_matrix = box_list_ops.sq_dist(boxes1, boxes2)
+      return dist_matrix
     exp_output = [[26, 25, 0], [18, 27, 6]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    dist_matrix = box_list_ops.sq_dist(boxes1, boxes2)
-    with self.test_session() as sess:
-      dist_output = sess.run(dist_matrix)
-      self.assertAllClose(dist_output, exp_output)
+    dist_output = self.execute(graph_fn, [])
+    self.assertAllClose(dist_output, exp_output)
 
   def test_boolean_mask(self):
-    corners = tf.constant(
-        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])
-    indicator = tf.constant([True, False, True, False, True], tf.bool)
+    def graph_fn():
+      corners = tf.constant(
+          [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])
+      indicator = tf.constant([True, False, True, False, True], tf.bool)
+      boxes = box_list.BoxList(corners)
+      subset = box_list_ops.boolean_mask(boxes, indicator)
+      return subset.get()
     expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
-    boxes = box_list.BoxList(corners)
-    subset = box_list_ops.boolean_mask(boxes, indicator)
-    with self.test_session() as sess:
-      subset_output = sess.run(subset.get())
-      self.assertAllClose(subset_output, expected_subset)
+    subset_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(subset_output, expected_subset)
 
   def test_static_boolean_mask_with_field(self):
 
@@ -380,49 +409,25 @@ class BoxListOpsTest(test_case.TestCase):
         dtype=np.float32)
     indicator = np.array([True, False, True, False, True], dtype=np.bool)
     weights = np.array([[.1], [.3], [.5], [.7], [.9]], dtype=np.float32)
-    result_boxes, result_weights = self.execute(graph_fn,
-                                                [corners, weights, indicator])
+    result_boxes, result_weights = self.execute_cpu(
+        graph_fn, [corners, weights, indicator])
     expected_boxes = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
     expected_weights = [[.1], [.5], [.9]]
 
     self.assertAllClose(result_boxes, expected_boxes)
     self.assertAllClose(result_weights, expected_weights)
 
-  def test_dynamic_boolean_mask_with_field(self):
-    corners = tf.placeholder(tf.float32, [None, 4])
-    indicator = tf.placeholder(tf.bool, [None])
-    weights = tf.placeholder(tf.float32, [None, 1])
-    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
-    expected_weights = [[.1], [.5], [.9]]
-
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('weights', weights)
-    subset = box_list_ops.boolean_mask(boxes, indicator, ['weights'])
-    with self.test_session() as sess:
-      subset_output, weights_output = sess.run(
-          [subset.get(), subset.get_field('weights')],
-          feed_dict={
-              corners:
-                  np.array(
-                      [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]]),
-              indicator:
-                  np.array([True, False, True, False, True]).astype(np.bool),
-              weights:
-                  np.array([[.1], [.3], [.5], [.7], [.9]])
-          })
-      self.assertAllClose(subset_output, expected_subset)
-      self.assertAllClose(weights_output, expected_weights)
-
   def test_gather(self):
-    corners = tf.constant(
-        [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])
-    indices = tf.constant([0, 2, 4], tf.int32)
+    def graph_fn():
+      corners = tf.constant(
+          [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])
+      indices = tf.constant([0, 2, 4], tf.int32)
+      boxes = box_list.BoxList(corners)
+      subset = box_list_ops.gather(boxes, indices)
+      return subset.get()
     expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
-    boxes = box_list.BoxList(corners)
-    subset = box_list_ops.gather(boxes, indices)
-    with self.test_session() as sess:
-      subset_output = sess.run(subset.get())
-      self.assertAllClose(subset_output, expected_subset)
+    subset_output = self.execute(graph_fn, [])
+    self.assertAllClose(subset_output, expected_subset)
 
   def test_static_gather_with_field(self):
 
@@ -445,32 +450,6 @@ class BoxListOpsTest(test_case.TestCase):
     self.assertAllClose(result_boxes, expected_boxes)
     self.assertAllClose(result_weights, expected_weights)
 
-  def test_dynamic_gather_with_field(self):
-    corners = tf.placeholder(tf.float32, [None, 4])
-    indices = tf.placeholder(tf.int32, [None])
-    weights = tf.placeholder(tf.float32, [None, 1])
-    expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
-    expected_weights = [[.1], [.5], [.9]]
-
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('weights', weights)
-    subset = box_list_ops.gather(boxes, indices, ['weights'],
-                                 use_static_shapes=True)
-    with self.test_session() as sess:
-      subset_output, weights_output = sess.run(
-          [subset.get(), subset.get_field('weights')],
-          feed_dict={
-              corners:
-                  np.array(
-                      [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]]),
-              indices:
-                  np.array([0, 2, 4]).astype(np.int32),
-              weights:
-                  np.array([[.1], [.3], [.5], [.7], [.9]])
-          })
-      self.assertAllClose(subset_output, expected_subset)
-      self.assertAllClose(weights_output, expected_weights)
-
   def test_gather_with_invalid_field(self):
     corners = tf.constant([4 * [0.0], 4 * [1.0]])
     indices = tf.constant([0, 1], tf.int32)
@@ -494,74 +473,74 @@ class BoxListOpsTest(test_case.TestCase):
       _ = box_list_ops.gather(boxes, indices_2d)
 
   def test_gather_with_dynamic_indexing(self):
-    corners = tf.constant([4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]
-                          ])
-    weights = tf.constant([.5, .3, .7, .1, .9], tf.float32)
-    indices = tf.reshape(tf.where(tf.greater(weights, 0.4)), [-1])
+    def graph_fn():
+      corners = tf.constant(
+          [4 * [0.0], 4 * [1.0], 4 * [2.0], 4 * [3.0], 4 * [4.0]])
+      weights = tf.constant([.5, .3, .7, .1, .9], tf.float32)
+      indices = tf.reshape(tf.where(tf.greater(weights, 0.4)), [-1])
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('weights', weights)
+      subset = box_list_ops.gather(boxes, indices, ['weights'])
+      return subset.get(), subset.get_field('weights')
     expected_subset = [4 * [0.0], 4 * [2.0], 4 * [4.0]]
     expected_weights = [.5, .7, .9]
-
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('weights', weights)
-    subset = box_list_ops.gather(boxes, indices, ['weights'])
-    with self.test_session() as sess:
-      subset_output, weights_output = sess.run([subset.get(), subset.get_field(
-          'weights')])
-      self.assertAllClose(subset_output, expected_subset)
-      self.assertAllClose(weights_output, expected_weights)
+    subset_output, weights_output = self.execute(graph_fn, [])
+    self.assertAllClose(subset_output, expected_subset)
+    self.assertAllClose(weights_output, expected_weights)
 
   def test_sort_by_field_ascending_order(self):
     exp_corners = [[0, 0, 1, 1], [0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9],
                    [0, 10, 1, 11], [0, 10.1, 1, 11.1], [0, 100, 1, 101]]
     exp_scores = [.95, .9, .75, .6, .5, .3]
     exp_weights = [.2, .45, .6, .75, .8, .92]
-    shuffle = [2, 4, 0, 5, 1, 3]
-    corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant(
-        [exp_scores[i] for i in shuffle], tf.float32))
-    boxes.add_field('weights', tf.constant(
-        [exp_weights[i] for i in shuffle], tf.float32))
-    sort_by_weight = box_list_ops.sort_by_field(
-        boxes,
-        'weights',
-        order=box_list_ops.SortOrder.ascend)
-    with self.test_session() as sess:
-      corners_out, scores_out, weights_out = sess.run([
-          sort_by_weight.get(),
-          sort_by_weight.get_field('scores'),
-          sort_by_weight.get_field('weights')])
-      self.assertAllClose(corners_out, exp_corners)
-      self.assertAllClose(scores_out, exp_scores)
-      self.assertAllClose(weights_out, exp_weights)
+
+    def graph_fn():
+      shuffle = [2, 4, 0, 5, 1, 3]
+      corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant(
+          [exp_scores[i] for i in shuffle], tf.float32))
+      boxes.add_field('weights', tf.constant(
+          [exp_weights[i] for i in shuffle], tf.float32))
+      sort_by_weight = box_list_ops.sort_by_field(
+          boxes,
+          'weights',
+          order=box_list_ops.SortOrder.ascend)
+      return [sort_by_weight.get(), sort_by_weight.get_field('scores'),
+              sort_by_weight.get_field('weights')]
+    corners_out, scores_out, weights_out = self.execute(graph_fn, [])
+    self.assertAllClose(corners_out, exp_corners)
+    self.assertAllClose(scores_out, exp_scores)
+    self.assertAllClose(weights_out, exp_weights)
 
   def test_sort_by_field_descending_order(self):
     exp_corners = [[0, 0, 1, 1], [0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9],
                    [0, 10, 1, 11], [0, 10.1, 1, 11.1], [0, 100, 1, 101]]
     exp_scores = [.95, .9, .75, .6, .5, .3]
     exp_weights = [.2, .45, .6, .75, .8, .92]
-    shuffle = [2, 4, 0, 5, 1, 3]
 
-    corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant(
-        [exp_scores[i] for i in shuffle], tf.float32))
-    boxes.add_field('weights', tf.constant(
-        [exp_weights[i] for i in shuffle], tf.float32))
-
-    sort_by_score = box_list_ops.sort_by_field(boxes, 'scores')
-    with self.test_session() as sess:
-      corners_out, scores_out, weights_out = sess.run([sort_by_score.get(
-      ), sort_by_score.get_field('scores'), sort_by_score.get_field('weights')])
-      self.assertAllClose(corners_out, exp_corners)
-      self.assertAllClose(scores_out, exp_scores)
-      self.assertAllClose(weights_out, exp_weights)
+    def graph_fn():
+      shuffle = [2, 4, 0, 5, 1, 3]
+      corners = tf.constant([exp_corners[i] for i in shuffle], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant(
+          [exp_scores[i] for i in shuffle], tf.float32))
+      boxes.add_field('weights', tf.constant(
+          [exp_weights[i] for i in shuffle], tf.float32))
+      sort_by_score = box_list_ops.sort_by_field(boxes, 'scores')
+      return (sort_by_score.get(), sort_by_score.get_field('scores'),
+              sort_by_score.get_field('weights'))
+
+    corners_out, scores_out, weights_out = self.execute(graph_fn, [])
+    self.assertAllClose(corners_out, exp_corners)
+    self.assertAllClose(scores_out, exp_scores)
+    self.assertAllClose(weights_out, exp_weights)
 
   def test_sort_by_field_invalid_inputs(self):
     corners = tf.constant([4 * [0.0], 4 * [0.5], 4 * [1.0], 4 * [2.0], 4 *
                            [3.0], 4 * [4.0]])
     misc = tf.constant([[.95, .9], [.5, .3]], tf.float32)
-    weights = tf.constant([.1, .2], tf.float32)
+    weights = tf.constant([[.1, .2]], tf.float32)
     boxes = box_list.BoxList(corners)
     boxes.add_field('misc', misc)
     boxes.add_field('weights', weights)
@@ -576,158 +555,152 @@ class BoxListOpsTest(test_case.TestCase):
       box_list_ops.sort_by_field(boxes, 'weights')
 
   def test_visualize_boxes_in_image(self):
-    image = tf.zeros((6, 4, 3))
-    corners = tf.constant([[0, 0, 5, 3],
-                           [0, 0, 3, 2]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    image_and_boxes = box_list_ops.visualize_boxes_in_image(image, boxes)
-    image_and_boxes_bw = tf.cast(
-        tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0), dtype=tf.float32)
+    def graph_fn():
+      image = tf.zeros((6, 4, 3))
+      corners = tf.constant([[0, 0, 5, 3],
+                             [0, 0, 3, 2]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      image_and_boxes = box_list_ops.visualize_boxes_in_image(image, boxes)
+      image_and_boxes_bw = tf.cast(
+          tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0), dtype=tf.float32)
+      return image_and_boxes_bw
     exp_result = [[1, 1, 1, 0],
                   [1, 1, 1, 0],
                   [1, 1, 1, 0],
                   [1, 0, 1, 0],
                   [1, 1, 1, 0],
                   [0, 0, 0, 0]]
-    with self.test_session() as sess:
-      output = sess.run(image_and_boxes_bw)
-      self.assertAllEqual(output.astype(int), exp_result)
+    output = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(output.astype(int), exp_result)
 
   def test_filter_field_value_equals(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1],
-                           [0, -0.1, 1, 0.9],
-                           [0, 10, 1, 11],
-                           [0, 10.1, 1, 11.1],
-                           [0, 100, 1, 101]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('classes', tf.constant([1, 2, 1, 2, 2, 1]))
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1],
+                             [0, -0.1, 1, 0.9],
+                             [0, 10, 1, 11],
+                             [0, 10.1, 1, 11.1],
+                             [0, 100, 1, 101]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('classes', tf.constant([1, 2, 1, 2, 2, 1]))
+      filtered_boxes1 = box_list_ops.filter_field_value_equals(
+          boxes, 'classes', 1)
+      filtered_boxes2 = box_list_ops.filter_field_value_equals(
+          boxes, 'classes', 2)
+      return filtered_boxes1.get(), filtered_boxes2.get()
     exp_output1 = [[0, 0, 1, 1], [0, -0.1, 1, 0.9], [0, 100, 1, 101]]
     exp_output2 = [[0, 0.1, 1, 1.1], [0, 10, 1, 11], [0, 10.1, 1, 11.1]]
-
-    filtered_boxes1 = box_list_ops.filter_field_value_equals(
-        boxes, 'classes', 1)
-    filtered_boxes2 = box_list_ops.filter_field_value_equals(
-        boxes, 'classes', 2)
-    with self.test_session() as sess:
-      filtered_output1, filtered_output2 = sess.run([filtered_boxes1.get(),
-                                                     filtered_boxes2.get()])
-      self.assertAllClose(filtered_output1, exp_output1)
-      self.assertAllClose(filtered_output2, exp_output2)
+    filtered_output1, filtered_output2 = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(filtered_output1, exp_output1)
+    self.assertAllClose(filtered_output2, exp_output2)
 
   def test_filter_greater_than(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1],
-                           [0, -0.1, 1, 0.9],
-                           [0, 10, 1, 11],
-                           [0, 10.1, 1, 11.1],
-                           [0, 100, 1, 101]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant([.1, .75, .9, .5, .5, .8]))
-    thresh = .6
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1],
+                             [0, -0.1, 1, 0.9],
+                             [0, 10, 1, 11],
+                             [0, 10.1, 1, 11.1],
+                             [0, 100, 1, 101]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant([.1, .75, .9, .5, .5, .8]))
+      thresh = .6
+      filtered_boxes = box_list_ops.filter_greater_than(boxes, thresh)
+      return filtered_boxes.get()
     exp_output = [[0, 0.1, 1, 1.1], [0, -0.1, 1, 0.9], [0, 100, 1, 101]]
-
-    filtered_boxes = box_list_ops.filter_greater_than(boxes, thresh)
-    with self.test_session() as sess:
-      filtered_output = sess.run(filtered_boxes.get())
-      self.assertAllClose(filtered_output, exp_output)
+    filtered_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(filtered_output, exp_output)
 
   def test_clip_box_list(self):
-    boxlist = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
-                     [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))
-    boxlist.add_field('classes', tf.constant([0, 0, 1, 1]))
-    boxlist.add_field('scores', tf.constant([0.75, 0.65, 0.3, 0.2]))
-    num_boxes = 2
-    clipped_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)
+    def graph_fn():
+      boxlist = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
+                       [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))
+      boxlist.add_field('classes', tf.constant([0, 0, 1, 1]))
+      boxlist.add_field('scores', tf.constant([0.75, 0.65, 0.3, 0.2]))
+      num_boxes = 2
+      clipped_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)
+      return (clipped_boxlist.get(), clipped_boxlist.get_field('classes'),
+              clipped_boxlist.get_field('scores'))
 
     expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]
     expected_classes = [0, 0]
     expected_scores = [0.75, 0.65]
-    with self.test_session() as sess:
-      boxes_out, classes_out, scores_out = sess.run(
-          [clipped_boxlist.get(), clipped_boxlist.get_field('classes'),
-           clipped_boxlist.get_field('scores')])
+    boxes_out, classes_out, scores_out = self.execute(graph_fn, [])
 
-      self.assertAllClose(expected_boxes, boxes_out)
-      self.assertAllEqual(expected_classes, classes_out)
-      self.assertAllClose(expected_scores, scores_out)
+    self.assertAllClose(expected_boxes, boxes_out)
+    self.assertAllEqual(expected_classes, classes_out)
+    self.assertAllClose(expected_scores, scores_out)
 
   def test_pad_box_list(self):
-    boxlist = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))
-    boxlist.add_field('classes', tf.constant([0, 1]))
-    boxlist.add_field('scores', tf.constant([0.75, 0.2]))
-    num_boxes = 4
-    padded_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)
-
+    def graph_fn():
+      boxlist = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))
+      boxlist.add_field('classes', tf.constant([0, 1]))
+      boxlist.add_field('scores', tf.constant([0.75, 0.2]))
+      num_boxes = 4
+      padded_boxlist = box_list_ops.pad_or_clip_box_list(boxlist, num_boxes)
+      return (padded_boxlist.get(), padded_boxlist.get_field('classes'),
+              padded_boxlist.get_field('scores'))
     expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
                       [0, 0, 0, 0], [0, 0, 0, 0]]
     expected_classes = [0, 1, 0, 0]
     expected_scores = [0.75, 0.2, 0, 0]
-    with self.test_session() as sess:
-      boxes_out, classes_out, scores_out = sess.run(
-          [padded_boxlist.get(), padded_boxlist.get_field('classes'),
-           padded_boxlist.get_field('scores')])
-
-      self.assertAllClose(expected_boxes, boxes_out)
-      self.assertAllEqual(expected_classes, classes_out)
-      self.assertAllClose(expected_scores, scores_out)
+    boxes_out, classes_out, scores_out = self.execute(graph_fn, [])
+    self.assertAllClose(expected_boxes, boxes_out)
+    self.assertAllEqual(expected_classes, classes_out)
+    self.assertAllClose(expected_scores, scores_out)
 
   def test_select_random_box(self):
     boxes = [[0., 0., 1., 1.],
              [0., 1., 2., 3.],
              [0., 2., 3., 4.]]
-
-    corners = tf.constant(boxes, dtype=tf.float32)
-    boxlist = box_list.BoxList(corners)
-    random_bbox, valid = box_list_ops.select_random_box(boxlist)
-    with self.test_session() as sess:
-      random_bbox_out, valid_out = sess.run([random_bbox, valid])
-
+    def graph_fn():
+      corners = tf.constant(boxes, dtype=tf.float32)
+      boxlist = box_list.BoxList(corners)
+      random_bbox, valid = box_list_ops.select_random_box(boxlist)
+      return random_bbox, valid
+    random_bbox_out, valid_out = self.execute(graph_fn, [])
     norm_small = any(
         [np.linalg.norm(random_bbox_out - box) < 1e-6 for box in boxes])
-
     self.assertTrue(norm_small)
     self.assertTrue(valid_out)
 
   def test_select_random_box_with_empty_boxlist(self):
-    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
-    boxlist = box_list.BoxList(corners)
-    random_bbox, valid = box_list_ops.select_random_box(boxlist)
-    with self.test_session() as sess:
-      random_bbox_out, valid_out = sess.run([random_bbox, valid])
-
+    def graph_fn():
+      corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
+      boxlist = box_list.BoxList(corners)
+      random_bbox, valid = box_list_ops.select_random_box(boxlist)
+      return random_bbox, valid
+    random_bbox_out, valid_out = self.execute_cpu(graph_fn, [])
     expected_bbox_out = np.array([[-1., -1., -1., -1.]], dtype=np.float32)
     self.assertAllEqual(expected_bbox_out, random_bbox_out)
     self.assertFalse(valid_out)
 
   def test_get_minimal_coverage_box(self):
-    boxes = [[0., 0., 1., 1.],
-             [-1., 1., 2., 3.],
-             [0., 2., 3., 4.]]
-
+    def graph_fn():
+      boxes = [[0., 0., 1., 1.],
+               [-1., 1., 2., 3.],
+               [0., 2., 3., 4.]]
+      corners = tf.constant(boxes, dtype=tf.float32)
+      boxlist = box_list.BoxList(corners)
+      coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
+      return coverage_box
+    coverage_box_out = self.execute(graph_fn, [])
     expected_coverage_box = [[-1., 0., 3., 4.]]
-
-    corners = tf.constant(boxes, dtype=tf.float32)
-    boxlist = box_list.BoxList(corners)
-    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
-    with self.test_session() as sess:
-      coverage_box_out = sess.run(coverage_box)
-
     self.assertAllClose(expected_coverage_box, coverage_box_out)
 
   def test_get_minimal_coverage_box_with_empty_boxlist(self):
-    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
-    boxlist = box_list.BoxList(corners)
-    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
-    with self.test_session() as sess:
-      coverage_box_out = sess.run(coverage_box)
-
+    def graph_fn():
+      corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
+      boxlist = box_list.BoxList(corners)
+      coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
+      return coverage_box
+    coverage_box_out = self.execute(graph_fn, [])
     self.assertAllClose([[0.0, 0.0, 1.0, 1.0]], coverage_box_out)
 
 
-class ConcatenateTest(tf.test.TestCase):
+class ConcatenateTest(test_case.TestCase):
 
   def test_invalid_input_box_list_list(self):
     with self.assertRaises(ValueError):
@@ -762,346 +735,344 @@ class ConcatenateTest(tf.test.TestCase):
       box_list_ops.concatenate([boxlist1, boxlist2])
 
   def test_concatenate_is_correct(self):
-    corners1 = tf.constant([[0, 0, 0, 0], [1, 2, 3, 4]], tf.float32)
-    scores1 = tf.constant([1.0, 2.1])
-    corners2 = tf.constant([[0, 3, 1, 6], [2, 4, 3, 8], [1, 0, 5, 10]],
-                           tf.float32)
-    scores2 = tf.constant([1.0, 2.1, 5.6])
-
+    def graph_fn():
+      corners1 = tf.constant([[0, 0, 0, 0], [1, 2, 3, 4]], tf.float32)
+      scores1 = tf.constant([1.0, 2.1])
+      corners2 = tf.constant([[0, 3, 1, 6], [2, 4, 3, 8], [1, 0, 5, 10]],
+                             tf.float32)
+      scores2 = tf.constant([1.0, 2.1, 5.6])
+      boxlist1 = box_list.BoxList(corners1)
+      boxlist1.add_field('scores', scores1)
+      boxlist2 = box_list.BoxList(corners2)
+      boxlist2.add_field('scores', scores2)
+      result = box_list_ops.concatenate([boxlist1, boxlist2])
+      return result.get(), result.get_field('scores')
     exp_corners = [[0, 0, 0, 0],
                    [1, 2, 3, 4],
                    [0, 3, 1, 6],
                    [2, 4, 3, 8],
                    [1, 0, 5, 10]]
     exp_scores = [1.0, 2.1, 1.0, 2.1, 5.6]
-
-    boxlist1 = box_list.BoxList(corners1)
-    boxlist1.add_field('scores', scores1)
-    boxlist2 = box_list.BoxList(corners2)
-    boxlist2.add_field('scores', scores2)
-    result = box_list_ops.concatenate([boxlist1, boxlist2])
-    with self.test_session() as sess:
-      corners_output, scores_output = sess.run(
-          [result.get(), result.get_field('scores')])
-      self.assertAllClose(corners_output, exp_corners)
-      self.assertAllClose(scores_output, exp_scores)
+    corners_output, scores_output = self.execute(graph_fn, [])
+    self.assertAllClose(corners_output, exp_corners)
+    self.assertAllClose(scores_output, exp_scores)
 
 
-class NonMaxSuppressionTest(tf.test.TestCase):
+class NonMaxSuppressionTest(test_case.TestCase):
 
   def test_select_from_three_clusters(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1],
-                           [0, -0.1, 1, 0.9],
-                           [0, 10, 1, 11],
-                           [0, 10.1, 1, 11.1],
-                           [0, 100, 1, 101]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
-    iou_thresh = .5
-    max_output_size = 3
-
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1],
+                             [0, -0.1, 1, 0.9],
+                             [0, 10, 1, 11],
+                             [0, 10.1, 1, 11.1],
+                             [0, 100, 1, 101]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
+      iou_thresh = .5
+      max_output_size = 3
+      nms = box_list_ops.non_max_suppression(
+          boxes, iou_thresh, max_output_size)
+      return nms.get()
     exp_nms = [[0, 10, 1, 11],
                [0, 0, 1, 1],
                [0, 100, 1, 101]]
-    nms = box_list_ops.non_max_suppression(
-        boxes, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+    nms_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_output, exp_nms)
 
   def test_select_at_most_two_boxes_from_three_clusters(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1],
-                           [0, -0.1, 1, 0.9],
-                           [0, 10, 1, 11],
-                           [0, 10.1, 1, 11.1],
-                           [0, 100, 1, 101]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
-    iou_thresh = .5
-    max_output_size = 2
-
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1],
+                             [0, -0.1, 1, 0.9],
+                             [0, 10, 1, 11],
+                             [0, 10.1, 1, 11.1],
+                             [0, 100, 1, 101]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
+      iou_thresh = .5
+      max_output_size = 2
+      nms = box_list_ops.non_max_suppression(
+          boxes, iou_thresh, max_output_size)
+      return nms.get()
     exp_nms = [[0, 10, 1, 11],
                [0, 0, 1, 1]]
-    nms = box_list_ops.non_max_suppression(
-        boxes, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+    nms_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_output, exp_nms)
 
   def test_select_at_most_thirty_boxes_from_three_clusters(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1],
-                           [0, -0.1, 1, 0.9],
-                           [0, 10, 1, 11],
-                           [0, 10.1, 1, 11.1],
-                           [0, 100, 1, 101]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
-    iou_thresh = .5
-    max_output_size = 30
-
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1],
+                             [0, -0.1, 1, 0.9],
+                             [0, 10, 1, 11],
+                             [0, 10.1, 1, 11.1],
+                             [0, 100, 1, 101]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant([.9, .75, .6, .95, .5, .3]))
+      iou_thresh = .5
+      max_output_size = 30
+      nms = box_list_ops.non_max_suppression(
+          boxes, iou_thresh, max_output_size)
+      return nms.get()
     exp_nms = [[0, 10, 1, 11],
                [0, 0, 1, 1],
                [0, 100, 1, 101]]
-    nms = box_list_ops.non_max_suppression(
-        boxes, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+    nms_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_output, exp_nms)
 
   def test_select_single_box(self):
-    corners = tf.constant([[0, 0, 1, 1]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant([.9]))
-    iou_thresh = .5
-    max_output_size = 3
-
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant([.9]))
+      iou_thresh = .5
+      max_output_size = 3
+      nms = box_list_ops.non_max_suppression(
+          boxes, iou_thresh, max_output_size)
+      return nms.get()
     exp_nms = [[0, 0, 1, 1]]
-    nms = box_list_ops.non_max_suppression(
-        boxes, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+    nms_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_output, exp_nms)
 
   def test_select_from_ten_identical_boxes(self):
-    corners = tf.constant(10 * [[0, 0, 1, 1]], tf.float32)
-    boxes = box_list.BoxList(corners)
-    boxes.add_field('scores', tf.constant(10 * [.9]))
-    iou_thresh = .5
-    max_output_size = 3
-
+    def graph_fn():
+      corners = tf.constant(10 * [[0, 0, 1, 1]], tf.float32)
+      boxes = box_list.BoxList(corners)
+      boxes.add_field('scores', tf.constant(10 * [.9]))
+      iou_thresh = .5
+      max_output_size = 3
+      nms = box_list_ops.non_max_suppression(
+          boxes, iou_thresh, max_output_size)
+      return nms.get()
     exp_nms = [[0, 0, 1, 1]]
-    nms = box_list_ops.non_max_suppression(
-        boxes, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+    nms_output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_output, exp_nms)
 
   def test_copy_extra_fields(self):
-    corners = tf.constant([[0, 0, 1, 1],
-                           [0, 0.1, 1, 1.1]], tf.float32)
-    boxes = box_list.BoxList(corners)
     tensor1 = np.array([[1], [4]])
     tensor2 = np.array([[1, 1], [2, 2]])
-    boxes.add_field('tensor1', tf.constant(tensor1))
-    boxes.add_field('tensor2', tf.constant(tensor2))
-    new_boxes = box_list.BoxList(tf.constant([[0, 0, 10, 10],
-                                              [1, 3, 5, 5]], tf.float32))
-    new_boxes = box_list_ops._copy_extra_fields(new_boxes, boxes)
-    with self.test_session() as sess:
-      self.assertAllClose(tensor1, sess.run(new_boxes.get_field('tensor1')))
-      self.assertAllClose(tensor2, sess.run(new_boxes.get_field('tensor2')))
+    def graph_fn():
+      corners = tf.constant([[0, 0, 1, 1],
+                             [0, 0.1, 1, 1.1]], tf.float32)
+      boxes = box_list.BoxList(corners)
 
+      boxes.add_field('tensor1', tf.constant(tensor1))
+      boxes.add_field('tensor2', tf.constant(tensor2))
+      new_boxes = box_list.BoxList(tf.constant([[0, 0, 10, 10],
+                                                [1, 3, 5, 5]], tf.float32))
+      new_boxes = box_list_ops._copy_extra_fields(new_boxes, boxes)
+      return new_boxes.get_field('tensor1'), new_boxes.get_field('tensor2')
+    tensor1_out, tensor2_out = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(tensor1, tensor1_out)
+    self.assertAllClose(tensor2, tensor2_out)
 
-class CoordinatesConversionTest(tf.test.TestCase):
+
+class CoordinatesConversionTest(test_case.TestCase):
 
   def test_to_normalized_coordinates(self):
-    coordinates = tf.constant([[0, 0, 100, 100],
-                               [25, 25, 75, 75]], tf.float32)
-    img = tf.ones((128, 100, 100, 3))
-    boxlist = box_list.BoxList(coordinates)
-    normalized_boxlist = box_list_ops.to_normalized_coordinates(
-        boxlist, tf.shape(img)[1], tf.shape(img)[2])
+    def graph_fn():
+      coordinates = tf.constant([[0, 0, 100, 100],
+                                 [25, 25, 75, 75]], tf.float32)
+      img = tf.ones((128, 100, 100, 3))
+      boxlist = box_list.BoxList(coordinates)
+      normalized_boxlist = box_list_ops.to_normalized_coordinates(
+          boxlist, tf.shape(img)[1], tf.shape(img)[2])
+      return normalized_boxlist.get()
     expected_boxes = [[0, 0, 1, 1],
                       [0.25, 0.25, 0.75, 0.75]]
-
-    with self.test_session() as sess:
-      normalized_boxes = sess.run(normalized_boxlist.get())
-      self.assertAllClose(normalized_boxes, expected_boxes)
+    normalized_boxes = self.execute(graph_fn, [])
+    self.assertAllClose(normalized_boxes, expected_boxes)
 
   def test_to_normalized_coordinates_already_normalized(self):
-    coordinates = tf.constant([[0, 0, 1, 1],
-                               [0.25, 0.25, 0.75, 0.75]], tf.float32)
-    img = tf.ones((128, 100, 100, 3))
-    boxlist = box_list.BoxList(coordinates)
-    normalized_boxlist = box_list_ops.to_normalized_coordinates(
-        boxlist, tf.shape(img)[1], tf.shape(img)[2])
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('assertion failed'):
-        sess.run(normalized_boxlist.get())
+    def graph_fn():
+      coordinates = tf.constant([[0, 0, 1, 1],
+                                 [0.25, 0.25, 0.75, 0.75]], tf.float32)
+      img = tf.ones((128, 100, 100, 3))
+      boxlist = box_list.BoxList(coordinates)
+      normalized_boxlist = box_list_ops.to_normalized_coordinates(
+          boxlist, tf.shape(img)[1], tf.shape(img)[2])
+      return normalized_boxlist.get()
+    with self.assertRaisesOpError('assertion failed'):
+      self.execute_cpu(graph_fn, [])
 
   def test_to_absolute_coordinates(self):
-    coordinates = tf.constant([[0, 0, 1, 1],
-                               [0.25, 0.25, 0.75, 0.75]], tf.float32)
-    img = tf.ones((128, 100, 100, 3))
-    boxlist = box_list.BoxList(coordinates)
-    absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                            tf.shape(img)[1],
-                                                            tf.shape(img)[2])
+    def graph_fn():
+      coordinates = tf.constant([[0, 0, 1, 1],
+                                 [0.25, 0.25, 0.75, 0.75]], tf.float32)
+      img = tf.ones((128, 100, 100, 3))
+      boxlist = box_list.BoxList(coordinates)
+      absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
+                                                              tf.shape(img)[1],
+                                                              tf.shape(img)[2])
+      return absolute_boxlist.get()
     expected_boxes = [[0, 0, 100, 100],
                       [25, 25, 75, 75]]
-
-    with self.test_session() as sess:
-      absolute_boxes = sess.run(absolute_boxlist.get())
-      self.assertAllClose(absolute_boxes, expected_boxes)
+    absolute_boxes = self.execute(graph_fn, [])
+    self.assertAllClose(absolute_boxes, expected_boxes)
 
   def test_to_absolute_coordinates_already_abolute(self):
-    coordinates = tf.constant([[0, 0, 100, 100],
-                               [25, 25, 75, 75]], tf.float32)
-    img = tf.ones((128, 100, 100, 3))
-    boxlist = box_list.BoxList(coordinates)
-    absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                            tf.shape(img)[1],
-                                                            tf.shape(img)[2])
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('assertion failed'):
-        sess.run(absolute_boxlist.get())
+    def graph_fn():
+      coordinates = tf.constant([[0, 0, 100, 100],
+                                 [25, 25, 75, 75]], tf.float32)
+      img = tf.ones((128, 100, 100, 3))
+      boxlist = box_list.BoxList(coordinates)
+      absolute_boxlist = box_list_ops.to_absolute_coordinates(boxlist,
+                                                              tf.shape(img)[1],
+                                                              tf.shape(img)[2])
+      return absolute_boxlist.get()
+    with self.assertRaisesOpError('assertion failed'):
+      self.execute_cpu(graph_fn, [])
 
   def test_convert_to_normalized_and_back(self):
     coordinates = np.random.uniform(size=(100, 4))
     coordinates = np.round(np.sort(coordinates) * 200)
     coordinates[:, 2:4] += 1
     coordinates[99, :] = [0, 0, 201, 201]
-    img = tf.ones((128, 202, 202, 3))
-
-    boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
-    boxlist = box_list_ops.to_normalized_coordinates(boxlist,
+    def graph_fn():
+      img = tf.ones((128, 202, 202, 3))
+
+      boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
+      boxlist = box_list_ops.to_normalized_coordinates(boxlist,
+                                                       tf.shape(img)[1],
+                                                       tf.shape(img)[2])
+      boxlist = box_list_ops.to_absolute_coordinates(boxlist,
                                                      tf.shape(img)[1],
                                                      tf.shape(img)[2])
-    boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                   tf.shape(img)[1],
-                                                   tf.shape(img)[2])
-
-    with self.test_session() as sess:
-      out = sess.run(boxlist.get())
-      self.assertAllClose(out, coordinates)
+      return boxlist.get()
+    out = self.execute(graph_fn, [])
+    self.assertAllClose(out, coordinates)
 
   def test_convert_to_absolute_and_back(self):
     coordinates = np.random.uniform(size=(100, 4))
     coordinates = np.sort(coordinates)
     coordinates[99, :] = [0, 0, 1, 1]
-    img = tf.ones((128, 202, 202, 3))
-
-    boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
-    boxlist = box_list_ops.to_absolute_coordinates(boxlist,
-                                                   tf.shape(img)[1],
-                                                   tf.shape(img)[2])
-    boxlist = box_list_ops.to_normalized_coordinates(boxlist,
+    def graph_fn():
+      img = tf.ones((128, 202, 202, 3))
+      boxlist = box_list.BoxList(tf.constant(coordinates, tf.float32))
+      boxlist = box_list_ops.to_absolute_coordinates(boxlist,
                                                      tf.shape(img)[1],
                                                      tf.shape(img)[2])
-
-    with self.test_session() as sess:
-      out = sess.run(boxlist.get())
-      self.assertAllClose(out, coordinates)
+      boxlist = box_list_ops.to_normalized_coordinates(boxlist,
+                                                       tf.shape(img)[1],
+                                                       tf.shape(img)[2])
+      return boxlist.get()
+    out = self.execute(graph_fn, [])
+    self.assertAllClose(out, coordinates)
 
   def test_to_absolute_coordinates_maximum_coordinate_check(self):
-    coordinates = tf.constant([[0, 0, 1.2, 1.2],
-                               [0.25, 0.25, 0.75, 0.75]], tf.float32)
-    img = tf.ones((128, 100, 100, 3))
-    boxlist = box_list.BoxList(coordinates)
-    absolute_boxlist = box_list_ops.to_absolute_coordinates(
-        boxlist,
-        tf.shape(img)[1],
-        tf.shape(img)[2],
-        maximum_normalized_coordinate=1.1)
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('assertion failed'):
-        sess.run(absolute_boxlist.get())
-
-
-class BoxRefinementTest(tf.test.TestCase):
+    def graph_fn():
+      coordinates = tf.constant([[0, 0, 1.2, 1.2],
+                                 [0.25, 0.25, 0.75, 0.75]], tf.float32)
+      img = tf.ones((128, 100, 100, 3))
+      boxlist = box_list.BoxList(coordinates)
+      absolute_boxlist = box_list_ops.to_absolute_coordinates(
+          boxlist,
+          tf.shape(img)[1],
+          tf.shape(img)[2],
+          maximum_normalized_coordinate=1.1)
+      return absolute_boxlist.get()
+    with self.assertRaisesOpError('assertion failed'):
+      self.execute_cpu(graph_fn, [])
+
+
+class BoxRefinementTest(test_case.TestCase):
 
   def test_box_voting(self):
-    candidates = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.6, 0.6, 0.8, 0.8]], tf.float32))
-    candidates.add_field('ExtraField', tf.constant([1, 2]))
-    pool = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
-                     [0.6, 0.6, 0.8, 0.8]], tf.float32))
-    pool.add_field('scores', tf.constant([0.75, 0.25, 0.3]))
-    averaged_boxes = box_list_ops.box_voting(candidates, pool)
+    def graph_fn():
+      candidates = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.6, 0.6, 0.8, 0.8]], tf.float32))
+      candidates.add_field('ExtraField', tf.constant([1, 2]))
+      pool = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
+                       [0.6, 0.6, 0.8, 0.8]], tf.float32))
+      pool.add_field('scores', tf.constant([0.75, 0.25, 0.3]))
+      averaged_boxes = box_list_ops.box_voting(candidates, pool)
+      return (averaged_boxes.get(), averaged_boxes.get_field('scores'),
+              averaged_boxes.get_field('ExtraField'))
+
     expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8]]
     expected_scores = [0.5, 0.3]
-    with self.test_session() as sess:
-      boxes_out, scores_out, extra_field_out = sess.run(
-          [averaged_boxes.get(), averaged_boxes.get_field('scores'),
-           averaged_boxes.get_field('ExtraField')])
-
-      self.assertAllClose(expected_boxes, boxes_out)
-      self.assertAllClose(expected_scores, scores_out)
-      self.assertAllEqual(extra_field_out, [1, 2])
+    boxes_out, scores_out, extra_field_out = self.execute(graph_fn, [])
+    self.assertAllClose(expected_boxes, boxes_out)
+    self.assertAllClose(expected_scores, scores_out)
+    self.assertAllEqual(extra_field_out, [1, 2])
 
   def test_box_voting_fails_with_negative_scores(self):
-    candidates = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
-    pool = box_list.BoxList(tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
-    pool.add_field('scores', tf.constant([-0.2]))
-    averaged_boxes = box_list_ops.box_voting(candidates, pool)
+    def graph_fn():
+      candidates = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
+      pool = box_list.BoxList(tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
+      pool.add_field('scores', tf.constant([-0.2]))
+      averaged_boxes = box_list_ops.box_voting(candidates, pool)
+      return averaged_boxes.get()
 
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('Scores must be non negative'):
-        sess.run([averaged_boxes.get()])
+    with self.assertRaisesOpError('Scores must be non negative'):
+      self.execute_cpu(graph_fn, [])
 
   def test_box_voting_fails_when_unmatched(self):
-    candidates = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
-    pool = box_list.BoxList(tf.constant([[0.6, 0.6, 0.8, 0.8]], tf.float32))
-    pool.add_field('scores', tf.constant([0.2]))
-    averaged_boxes = box_list_ops.box_voting(candidates, pool)
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('Each box in selected_boxes must match '
-                                    'with at least one box in pool_boxes.'):
-        sess.run([averaged_boxes.get()])
+    def graph_fn():
+      candidates = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4]], tf.float32))
+      pool = box_list.BoxList(tf.constant([[0.6, 0.6, 0.8, 0.8]], tf.float32))
+      pool.add_field('scores', tf.constant([0.2]))
+      averaged_boxes = box_list_ops.box_voting(candidates, pool)
+      return averaged_boxes.get()
+    with self.assertRaisesOpError('Each box in selected_boxes must match '
+                                  'with at least one box in pool_boxes.'):
+      self.execute_cpu(graph_fn, [])
 
   def test_refine_boxes(self):
-    pool = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
-                     [0.6, 0.6, 0.8, 0.8]], tf.float32))
-    pool.add_field('ExtraField', tf.constant([1, 2, 3]))
-    pool.add_field('scores', tf.constant([0.75, 0.25, 0.3]))
-    refined_boxes = box_list_ops.refine_boxes(pool, 0.5, 10)
-
+    def graph_fn():
+      pool = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
+                       [0.6, 0.6, 0.8, 0.8]], tf.float32))
+      pool.add_field('ExtraField', tf.constant([1, 2, 3]))
+      pool.add_field('scores', tf.constant([0.75, 0.25, 0.3]))
+      averaged_boxes = box_list_ops.refine_boxes(pool, 0.5, 10)
+      return (averaged_boxes.get(), averaged_boxes.get_field('scores'),
+              averaged_boxes.get_field('ExtraField'))
+    boxes_out, scores_out, extra_field_out = self.execute_cpu(graph_fn, [])
     expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8]]
     expected_scores = [0.5, 0.3]
-    with self.test_session() as sess:
-      boxes_out, scores_out, extra_field_out = sess.run(
-          [refined_boxes.get(), refined_boxes.get_field('scores'),
-           refined_boxes.get_field('ExtraField')])
-
-      self.assertAllClose(expected_boxes, boxes_out)
-      self.assertAllClose(expected_scores, scores_out)
-      self.assertAllEqual(extra_field_out, [1, 3])
+    self.assertAllClose(expected_boxes, boxes_out)
+    self.assertAllClose(expected_scores, scores_out)
+    self.assertAllEqual(extra_field_out, [1, 3])
 
   def test_refine_boxes_multi_class(self):
-    pool = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
-                     [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))
-    pool.add_field('classes', tf.constant([0, 0, 1, 1]))
-    pool.add_field('scores', tf.constant([0.75, 0.25, 0.3, 0.2]))
-    refined_boxes = box_list_ops.refine_boxes_multi_class(pool, 3, 0.5, 10)
-
+    def graph_fn():
+      pool = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5],
+                       [0.6, 0.6, 0.8, 0.8], [0.2, 0.2, 0.3, 0.3]], tf.float32))
+      pool.add_field('classes', tf.constant([0, 0, 1, 1]))
+      pool.add_field('scores', tf.constant([0.75, 0.25, 0.3, 0.2]))
+      averaged_boxes = box_list_ops.refine_boxes_multi_class(pool, 3, 0.5, 10)
+      return (averaged_boxes.get(), averaged_boxes.get_field('scores'),
+              averaged_boxes.get_field('classes'))
+    boxes_out, scores_out, extra_field_out = self.execute_cpu(graph_fn, [])
     expected_boxes = [[0.1, 0.1, 0.425, 0.425], [0.6, 0.6, 0.8, 0.8],
                       [0.2, 0.2, 0.3, 0.3]]
     expected_scores = [0.5, 0.3, 0.2]
-    with self.test_session() as sess:
-      boxes_out, scores_out, extra_field_out = sess.run(
-          [refined_boxes.get(), refined_boxes.get_field('scores'),
-           refined_boxes.get_field('classes')])
-
-      self.assertAllClose(expected_boxes, boxes_out)
-      self.assertAllClose(expected_scores, scores_out)
-      self.assertAllEqual(extra_field_out, [0, 1, 1])
+    self.assertAllClose(expected_boxes, boxes_out)
+    self.assertAllClose(expected_scores, scores_out)
+    self.assertAllEqual(extra_field_out, [0, 1, 1])
 
   def test_sample_boxes_by_jittering(self):
-    boxes = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4],
-                     [0.1, 0.1, 0.5, 0.5],
-                     [0.6, 0.6, 0.8, 0.8],
-                     [0.2, 0.2, 0.3, 0.3]], tf.float32))
-    sampled_boxes = box_list_ops.sample_boxes_by_jittering(
-        boxlist=boxes, num_boxes_to_sample=10)
-    iou = box_list_ops.iou(boxes, sampled_boxes)
-    iou_max = tf.reduce_max(iou, axis=0)
-    with self.test_session() as sess:
-      (np_sampled_boxes, np_iou_max) = sess.run([sampled_boxes.get(), iou_max])
-      self.assertAllEqual(np_sampled_boxes.shape, [10, 4])
-      self.assertAllGreater(np_iou_max, 0.5)
+    def graph_fn():
+      boxes = box_list.BoxList(
+          tf.constant([[0.1, 0.1, 0.4, 0.4],
+                       [0.1, 0.1, 0.5, 0.5],
+                       [0.6, 0.6, 0.8, 0.8],
+                       [0.2, 0.2, 0.3, 0.3]], tf.float32))
+      sampled_boxes = box_list_ops.sample_boxes_by_jittering(
+          boxlist=boxes, num_boxes_to_sample=10)
+      iou = box_list_ops.iou(boxes, sampled_boxes)
+      iou_max = tf.reduce_max(iou, axis=0)
+      return sampled_boxes.get(), iou_max
+    np_sampled_boxes, np_iou_max = self.execute(graph_fn, [])
+    self.assertAllEqual(np_sampled_boxes.shape, [10, 4])
+    self.assertAllGreater(np_iou_max, 0.3)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/box_list_test.py b/research/object_detection/core/box_list_test.py
index edc00ebb..d6ac68dd 100644
--- a/research/object_detection/core/box_list_test.py
+++ b/research/object_detection/core/box_list_test.py
@@ -14,53 +14,56 @@
 # ==============================================================================
 
 """Tests for object_detection.core.box_list."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.core import box_list
+from object_detection.utils import test_case
 
 
-class BoxListTest(tf.test.TestCase):
+class BoxListTest(test_case.TestCase):
   """Tests for BoxList class."""
 
   def test_num_boxes(self):
-    data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)
-    expected_num_boxes = 3
-
-    boxes = box_list.BoxList(data)
-    with self.test_session() as sess:
-      num_boxes_output = sess.run(boxes.num_boxes())
-      self.assertEquals(num_boxes_output, expected_num_boxes)
+    def graph_fn():
+      data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)
+      boxes = box_list.BoxList(data)
+      return boxes.num_boxes()
+    num_boxes_out = self.execute(graph_fn, [])
+    self.assertEqual(num_boxes_out, 3)
 
   def test_get_correct_center_coordinates_and_sizes(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    centers_sizes = boxes.get_center_coordinates_and_sizes()
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    def graph_fn(boxes):
+      boxes = box_list.BoxList(boxes)
+      centers_sizes = boxes.get_center_coordinates_and_sizes()
+      return centers_sizes
+    centers_sizes_out = self.execute(graph_fn, [boxes])
     expected_centers_sizes = [[15, 0.35], [12.5, 0.25], [10, 0.3], [5, 0.3]]
-    with self.test_session() as sess:
-      centers_sizes_out = sess.run(centers_sizes)
-      self.assertAllClose(centers_sizes_out, expected_centers_sizes)
+    self.assertAllClose(centers_sizes_out, expected_centers_sizes)
 
   def test_create_box_list_with_dynamic_shape(self):
-    data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)
-    indices = tf.reshape(tf.where(tf.greater([1, 0, 1], 0)), [-1])
-    data = tf.gather(data, indices)
-    assert data.get_shape().as_list() == [None, 4]
-    expected_num_boxes = 2
-
-    boxes = box_list.BoxList(data)
-    with self.test_session() as sess:
-      num_boxes_output = sess.run(boxes.num_boxes())
-      self.assertEquals(num_boxes_output, expected_num_boxes)
+    def graph_fn():
+      data = tf.constant([[0, 0, 1, 1], [1, 1, 2, 3], [3, 4, 5, 5]], tf.float32)
+      indices = tf.reshape(tf.where(tf.greater([1, 0, 1], 0)), [-1])
+      data = tf.gather(data, indices)
+      assert data.get_shape().as_list() == [None, 4]
+      boxes = box_list.BoxList(data)
+      return boxes.num_boxes()
+    num_boxes = self.execute(graph_fn, [])
+    self.assertEqual(num_boxes, 2)
 
   def test_transpose_coordinates(self):
-    boxes = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
-    boxes = box_list.BoxList(tf.constant(boxes))
-    boxes.transpose_coordinates()
+    boxes = np.array([[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]],
+                     np.float32)
+    def graph_fn(boxes):
+      boxes = box_list.BoxList(boxes)
+      boxes.transpose_coordinates()
+      return boxes.get()
+    transpoded_boxes = self.execute(graph_fn, [boxes])
     expected_corners = [[10.0, 10.0, 15.0, 20.0], [0.1, 0.2, 0.4, 0.5]]
-    with self.test_session() as sess:
-      corners_out = sess.run(boxes.get())
-      self.assertAllClose(corners_out, expected_corners)
+    self.assertAllClose(transpoded_boxes, expected_corners)
 
   def test_box_list_invalid_inputs(self):
     data0 = tf.constant([[[0, 0, 1, 1], [3, 4, 5, 5]]], tf.float32)
@@ -77,49 +80,33 @@ class BoxListTest(tf.test.TestCase):
   def test_num_boxes_static(self):
     box_corners = [[10.0, 10.0, 20.0, 15.0], [0.2, 0.1, 0.5, 0.4]]
     boxes = box_list.BoxList(tf.constant(box_corners))
-    self.assertEquals(boxes.num_boxes_static(), 2)
-    self.assertEquals(type(boxes.num_boxes_static()), int)
-
-  def test_num_boxes_static_for_uninferrable_shape(self):
-    placeholder = tf.placeholder(tf.float32, shape=[None, 4])
-    boxes = box_list.BoxList(placeholder)
-    self.assertEquals(boxes.num_boxes_static(), None)
+    self.assertEqual(boxes.num_boxes_static(), 2)
+    self.assertEqual(type(boxes.num_boxes_static()), int)
 
   def test_as_tensor_dict(self):
-    boxlist = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))
-    boxlist.add_field('classes', tf.constant([0, 1]))
-    boxlist.add_field('scores', tf.constant([0.75, 0.2]))
+    boxes = tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]],
+                        tf.float32)
+    boxlist = box_list.BoxList(boxes)
+    classes = tf.constant([0, 1])
+    boxlist.add_field('classes', classes)
+    scores = tf.constant([0.75, 0.2])
+    boxlist.add_field('scores', scores)
     tensor_dict = boxlist.as_tensor_dict()
 
-    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]
-    expected_classes = [0, 1]
-    expected_scores = [0.75, 0.2]
-
-    with self.test_session() as sess:
-      tensor_dict_out = sess.run(tensor_dict)
-      self.assertAllEqual(3, len(tensor_dict_out))
-      self.assertAllClose(expected_boxes, tensor_dict_out['boxes'])
-      self.assertAllEqual(expected_classes, tensor_dict_out['classes'])
-      self.assertAllClose(expected_scores, tensor_dict_out['scores'])
+    self.assertDictEqual(tensor_dict, {'scores': scores, 'classes': classes,
+                                       'boxes': boxes})
 
   def test_as_tensor_dict_with_features(self):
-    boxlist = box_list.BoxList(
-        tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]], tf.float32))
-    boxlist.add_field('classes', tf.constant([0, 1]))
-    boxlist.add_field('scores', tf.constant([0.75, 0.2]))
-    tensor_dict = boxlist.as_tensor_dict(['boxes', 'classes', 'scores'])
-
-    expected_boxes = [[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]]
-    expected_classes = [0, 1]
-    expected_scores = [0.75, 0.2]
-
-    with self.test_session() as sess:
-      tensor_dict_out = sess.run(tensor_dict)
-      self.assertAllEqual(3, len(tensor_dict_out))
-      self.assertAllClose(expected_boxes, tensor_dict_out['boxes'])
-      self.assertAllEqual(expected_classes, tensor_dict_out['classes'])
-      self.assertAllClose(expected_scores, tensor_dict_out['scores'])
+    boxes = tf.constant([[0.1, 0.1, 0.4, 0.4], [0.1, 0.1, 0.5, 0.5]],
+                        tf.float32)
+    boxlist = box_list.BoxList(boxes)
+    classes = tf.constant([0, 1])
+    boxlist.add_field('classes', classes)
+    scores = tf.constant([0.75, 0.2])
+    boxlist.add_field('scores', scores)
+    tensor_dict = boxlist.as_tensor_dict(['scores', 'classes'])
+
+    self.assertDictEqual(tensor_dict, {'scores': scores, 'classes': classes})
 
   def test_as_tensor_dict_missing_field(self):
     boxlist = box_list.BoxList(
diff --git a/research/object_detection/core/class_agnostic_nms_test.py b/research/object_detection/core/class_agnostic_nms_test.py
index 8a418b71..6dc9814d 100644
--- a/research/object_detection/core/class_agnostic_nms_test.py
+++ b/research/object_detection/core/class_agnostic_nms_test.py
@@ -24,82 +24,74 @@ class ClassAgnosticNonMaxSuppressionTest(test_case.TestCase,
                                          parameterized.TestCase):
 
   def test_class_agnostic_nms_select_with_shared_boxes(self):
-    boxes = tf.constant(
-        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
-         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
-         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_classes_per_detection = 1
-    max_output_size = 4
+    def graph_fn():
+      boxes = tf.constant(
+          [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+           [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+           [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], tf.float32)
+      scores = tf.constant([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                            [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]])
+      score_thresh = 0.1
+      iou_thresh = .5
+      max_classes_per_detection = 1
+      max_output_size = 4
+      nms, _ = post_processing.class_agnostic_non_max_suppression(
+          boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
+          max_output_size)
+      return (nms.get(), nms.get_field(fields.BoxListFields.scores),
+              nms.get_field(fields.BoxListFields.classes))
 
     exp_nms_corners = [[0, 10, 1, 11], [0, 0, 1, 1], [0, 1000, 1, 1002],
                        [0, 100, 1, 101]]
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
 
-    nms, _ = post_processing.class_agnostic_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
-        max_output_size)
-
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run([
-          nms.get(),
-          nms.get_field(fields.BoxListFields.scores),
-          nms.get_field(fields.BoxListFields.classes)
-      ])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nms_corners_output, exp_nms_corners)
+    self.assertAllClose(nms_scores_output, exp_nms_scores)
+    self.assertAllClose(nms_classes_output, exp_nms_classes)
 
 
   def test_class_agnostic_nms_select_with_per_class_boxes(self):
-    boxes = tf.constant(
-        [[[4, 5, 9, 10], [0, 0, 1, 1]],
-         [[0, 0.1, 1, 1.1], [4, 5, 9, 10]],
-         [[0, -0.1, 1, 0.9], [4, 5, 9, 10]],
-         [[0, 10, 1, 11], [4, 5, 9, 10]],
-         [[0, 10.1, 1, 11.1], [4, 5, 9, 10]],
-         [[0, 100, 1, 101], [4, 5, 9, 10]],
-         [[4, 5, 9, 10], [0, 1000, 1, 1002]],
-         [[4, 5, 9, 10], [0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.01, 0.9],
-                          [.75, 0.05],
-                          [.6, 0.01],
-                          [.95, 0],
-                          [.5, 0.01],
-                          [.3, 0.01],
-                          [.01, .85],
-                          [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_classes_per_detection = 1
-    max_output_size = 4
-
+    def graph_fn():
+      boxes = tf.constant(
+          [[[4, 5, 9, 10], [0, 0, 1, 1]],
+           [[0, 0.1, 1, 1.1], [4, 5, 9, 10]],
+           [[0, -0.1, 1, 0.9], [4, 5, 9, 10]],
+           [[0, 10, 1, 11], [4, 5, 9, 10]],
+           [[0, 10.1, 1, 11.1], [4, 5, 9, 10]],
+           [[0, 100, 1, 101], [4, 5, 9, 10]],
+           [[4, 5, 9, 10], [0, 1000, 1, 1002]],
+           [[4, 5, 9, 10], [0, 1000, 1, 1002.1]]], tf.float32)
+      scores = tf.constant([[.01, 0.9],
+                            [.75, 0.05],
+                            [.6, 0.01],
+                            [.95, 0],
+                            [.5, 0.01],
+                            [.3, 0.01],
+                            [.01, .85],
+                            [.01, .5]])
+      score_thresh = 0.1
+      iou_thresh = .5
+      max_classes_per_detection = 1
+      max_output_size = 4
+      nms, _ = post_processing.class_agnostic_non_max_suppression(
+          boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
+          max_output_size)
+      return (nms.get(), nms.get_field(fields.BoxListFields.scores),
+              nms.get_field(fields.BoxListFields.classes))
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output) = self.execute_cpu(graph_fn, [])
     exp_nms_corners = [[0, 10, 1, 11],
                        [0, 0, 1, 1],
                        [0, 1000, 1, 1002],
                        [0, 100, 1, 101]]
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 1, 1, 0]
-
-    nms, _ = post_processing.class_agnostic_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
-        max_output_size)
-
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run([
-          nms.get(),
-          nms.get_field(fields.BoxListFields.scores),
-          nms.get_field(fields.BoxListFields.classes)
-      ])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    self.assertAllClose(nms_corners_output, exp_nms_corners)
+    self.assertAllClose(nms_scores_output, exp_nms_scores)
+    self.assertAllClose(nms_classes_output, exp_nms_classes)
 
   # Two cases will be tested here: using / not using static shapes.
   # Named the two test cases for easier control during testing, with a flag of
@@ -109,46 +101,43 @@ class ClassAgnosticNonMaxSuppressionTest(test_case.TestCase,
   @parameterized.named_parameters(('', False), ('_use_static_shapes', True))
   def test_batch_classagnostic_nms_with_batch_size_1(self,
                                                      use_static_shapes=False):
-    boxes = tf.constant(
-        [[[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
-          [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
-          [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]]], tf.float32)
-    scores = tf.constant([[[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
-                           [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-    max_classes_per_detection = 1
-    use_class_agnostic_nms = True
-
+    def graph_fn():
+      boxes = tf.constant(
+          [[[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+            [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+            [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]]], tf.float32)
+      scores = tf.constant([[[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                             [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]]])
+      score_thresh = 0.1
+      iou_thresh = .5
+      max_output_size = 4
+      max_classes_per_detection = 1
+      use_class_agnostic_nms = True
+      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+       nmsed_additional_fields,
+       num_detections) = post_processing.batch_multiclass_non_max_suppression(
+           boxes,
+           scores,
+           score_thresh,
+           iou_thresh,
+           max_size_per_class=max_output_size,
+           max_total_size=max_output_size,
+           use_class_agnostic_nms=use_class_agnostic_nms,
+           use_static_shapes=use_static_shapes,
+           max_classes_per_detection=max_classes_per_detection)
+      self.assertIsNone(nmsed_masks)
+      self.assertIsNone(nmsed_additional_fields)
+      return (nmsed_boxes, nmsed_scores, nmsed_classes, num_detections)
     exp_nms_corners = [[[0, 10, 1, 11], [0, 0, 1, 1], [0, 1000, 1, 1002],
                         [0, 100, 1, 101]]]
     exp_nms_scores = [[.95, .9, .85, .3]]
     exp_nms_classes = [[0, 0, 1, 0]]
-
-    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-     nmsed_additional_fields,
-     num_detections) = post_processing.batch_multiclass_non_max_suppression(
-         boxes,
-         scores,
-         score_thresh,
-         iou_thresh,
-         max_size_per_class=max_output_size,
-         max_total_size=max_output_size,
-         use_class_agnostic_nms=use_class_agnostic_nms,
-         use_static_shapes=use_static_shapes,
-         max_classes_per_detection=max_classes_per_detection)
-
-    self.assertIsNone(nmsed_masks)
-    self.assertIsNone(nmsed_additional_fields)
-
-    with self.test_session() as sess:
-      (nmsed_boxes, nmsed_scores, nmsed_classes, num_detections) = sess.run(
-          [nmsed_boxes, nmsed_scores, nmsed_classes, num_detections])
-      self.assertAllClose(nmsed_boxes, exp_nms_corners)
-      self.assertAllClose(nmsed_scores, exp_nms_scores)
-      self.assertAllClose(nmsed_classes, exp_nms_classes)
-      self.assertEqual(num_detections, [4])
+    (nmsed_boxes, nmsed_scores, nmsed_classes,
+     num_detections) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(nmsed_boxes, exp_nms_corners)
+    self.assertAllClose(nmsed_scores, exp_nms_scores)
+    self.assertAllClose(nmsed_classes, exp_nms_classes)
+    self.assertEqual(num_detections, [4])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/freezable_batch_norm_test.py b/research/object_detection/core/freezable_batch_norm_test.py
index 3e061526..32a7f8c2 100644
--- a/research/object_detection/core/freezable_batch_norm_test.py
+++ b/research/object_detection/core/freezable_batch_norm_test.py
@@ -22,6 +22,7 @@ import numpy as np
 from six.moves import zip
 import tensorflow as tf
 
+
 from object_detection.core import freezable_batch_norm
 
 
@@ -36,6 +37,10 @@ class FreezableBatchNormTest(tf.test.TestCase):
     model.add(norm)
     return model, norm
 
+  def _copy_weights(self, source_weights, target_weights):
+    for source, target in zip(source_weights, target_weights):
+      target.assign(source)
+
   def _train_freezable_batch_norm(self, training_mean, training_var):
     model, _ = self._build_model()
     model.compile(loss='mse', optimizer='sgd')
@@ -53,136 +58,138 @@ class FreezableBatchNormTest(tf.test.TestCase):
       testing_mean, testing_var, training_arg, training_mean, training_var):
     out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32),
                       training=training_arg)
-    out = tf.keras.backend.eval(out_tensor)
-    out -= tf.keras.backend.eval(norm.beta)
-    out /= tf.keras.backend.eval(norm.gamma)
+    out = out_tensor
+    out -= norm.beta
+    out /= norm.gamma
 
     if not should_be_training:
       out *= training_var
       out += (training_mean - testing_mean)
       out /= testing_var
 
-    np.testing.assert_allclose(out.mean(), 0.0, atol=1.5e-1)
-    np.testing.assert_allclose(out.std(), 1.0, atol=1.5e-1)
+    np.testing.assert_allclose(out.numpy().mean(), 0.0, atol=1.5e-1)
+    np.testing.assert_allclose(out.numpy().std(), 1.0, atol=1.5e-1)
 
   def test_batchnorm_freezing_training_none(self):
-    with self.test_session():
-      training_mean = 5.0
-      training_var = 10.0
-
-      testing_mean = -10.0
-      testing_var = 5.0
-
-      # Initially train the batch norm, and save the weights
-      trained_weights = self._train_freezable_batch_norm(training_mean,
-                                                         training_var)
-
-      # Load the batch norm weights, freezing training to True.
-      # Apply the batch norm layer to testing data and ensure it is normalized
-      # according to the batch statistics.
-      model, norm = self._build_model(training=True)
-      for trained_weight, blank_weight in zip(trained_weights, model.weights):
-        weight_copy = blank_weight.assign(tf.keras.backend.eval(trained_weight))
-        tf.keras.backend.eval(weight_copy)
-
-      # centered on testing_mean, variance testing_var
-      test_data = np.random.normal(
-          loc=testing_mean,
-          scale=testing_var,
-          size=(1000, 10))
-
-      # Test with training=True passed to the call method:
-      training_arg = True
-      should_be_training = True
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      # Test with training=False passed to the call method:
-      training_arg = False
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      # Test the layer in various Keras learning phase scopes:
-      training_arg = None
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      tf.keras.backend.set_learning_phase(True)
-      should_be_training = True
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      tf.keras.backend.set_learning_phase(False)
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
+    training_mean = 5.0
+    training_var = 10.0
+
+    testing_mean = -10.0
+    testing_var = 5.0
+
+    # Initially train the batch norm, and save the weights
+    trained_weights = self._train_freezable_batch_norm(training_mean,
+                                                       training_var)
+
+    # Load the batch norm weights, freezing training to True.
+    # Apply the batch norm layer to testing data and ensure it is normalized
+    # according to the batch statistics.
+    model, norm = self._build_model(training=True)
+    self._copy_weights(trained_weights, model.weights)
+
+    # centered on testing_mean, variance testing_var
+    test_data = np.random.normal(
+        loc=testing_mean,
+        scale=testing_var,
+        size=(1000, 10))
+
+    # Test with training=True passed to the call method:
+    training_arg = True
+    should_be_training = True
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    # Reset the weights, because they may have been updating by
+    # running with training=True
+    self._copy_weights(trained_weights, model.weights)
+
+    # Test with training=False passed to the call method:
+    training_arg = False
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    # Test the layer in various Keras learning phase scopes:
+    training_arg = None
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    tf.keras.backend.set_learning_phase(True)
+    should_be_training = True
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    # Reset the weights, because they may have been updating by
+    # running with training=True
+    self._copy_weights(trained_weights, model.weights)
+
+    tf.keras.backend.set_learning_phase(False)
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
 
   def test_batchnorm_freezing_training_false(self):
-    with self.test_session():
-      training_mean = 5.0
-      training_var = 10.0
-
-      testing_mean = -10.0
-      testing_var = 5.0
-
-      # Initially train the batch norm, and save the weights
-      trained_weights = self._train_freezable_batch_norm(training_mean,
-                                                         training_var)
-
-      # Load the batch norm back up, freezing training to False.
-      # Apply the batch norm layer to testing data and ensure it is normalized
-      # according to the training data's statistics.
-      model, norm = self._build_model(training=False)
-      for trained_weight, blank_weight in zip(trained_weights, model.weights):
-        weight_copy = blank_weight.assign(tf.keras.backend.eval(trained_weight))
-        tf.keras.backend.eval(weight_copy)
-
-      # centered on testing_mean, variance testing_var
-      test_data = np.random.normal(
-          loc=testing_mean,
-          scale=testing_var,
-          size=(1000, 10))
-
-      # Make sure that the layer is never training
-      # Test with training=True passed to the call method:
-      training_arg = True
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      # Test with training=False passed to the call method:
-      training_arg = False
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      # Test the layer in various Keras learning phase scopes:
-      training_arg = None
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      tf.keras.backend.set_learning_phase(True)
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
-
-      tf.keras.backend.set_learning_phase(False)
-      should_be_training = False
-      self._test_batchnorm_layer(norm, should_be_training, test_data,
-                                 testing_mean, testing_var, training_arg,
-                                 training_mean, training_var)
+    training_mean = 5.0
+    training_var = 10.0
+
+    testing_mean = -10.0
+    testing_var = 5.0
+
+    # Initially train the batch norm, and save the weights
+    trained_weights = self._train_freezable_batch_norm(training_mean,
+                                                       training_var)
+
+    # Load the batch norm back up, freezing training to False.
+    # Apply the batch norm layer to testing data and ensure it is normalized
+    # according to the training data's statistics.
+    model, norm = self._build_model(training=False)
+    self._copy_weights(trained_weights, model.weights)
+
+    # centered on testing_mean, variance testing_var
+    test_data = np.random.normal(
+        loc=testing_mean,
+        scale=testing_var,
+        size=(1000, 10))
+
+    # Make sure that the layer is never training
+    # Test with training=True passed to the call method:
+    training_arg = True
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    # Test with training=False passed to the call method:
+    training_arg = False
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    # Test the layer in various Keras learning phase scopes:
+    training_arg = None
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    tf.keras.backend.set_learning_phase(True)
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
+
+    tf.keras.backend.set_learning_phase(False)
+    should_be_training = False
+    self._test_batchnorm_layer(norm, should_be_training, test_data,
+                               testing_mean, testing_var, training_arg,
+                               training_mean, training_var)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/keypoint_ops.py b/research/object_detection/core/keypoint_ops.py
index e520845f..cfcaa529 100644
--- a/research/object_detection/core/keypoint_ops.py
+++ b/research/object_detection/core/keypoint_ops.py
@@ -125,6 +125,24 @@ def change_coordinate_frame(keypoints, window, scope=None):
     return new_keypoints
 
 
+def keypoints_to_enclosing_bounding_boxes(keypoints):
+  """Creates enclosing bounding boxes from keypoints.
+
+  Args:
+    keypoints: a [num_instances, num_keypoints, 2] float32 tensor with keypoints
+      in [y, x] format.
+
+  Returns:
+    A [num_instances, 4] float32 tensor that tightly covers all the keypoints
+    for each instance.
+  """
+  ymin = tf.math.reduce_min(keypoints[:, :, 0], axis=1)
+  xmin = tf.math.reduce_min(keypoints[:, :, 1], axis=1)
+  ymax = tf.math.reduce_max(keypoints[:, :, 0], axis=1)
+  xmax = tf.math.reduce_max(keypoints[:, :, 1], axis=1)
+  return tf.stack([ymin, xmin, ymax, xmax], axis=1)
+
+
 def to_normalized_coordinates(keypoints, height, width,
                               check_range=True, scope=None):
   """Converts absolute keypoint coordinates to normalized coordinates in [0, 1].
@@ -280,3 +298,69 @@ def rot90(keypoints, scope=None):
     new_keypoints = tf.concat([v, u], 2)
     new_keypoints = tf.transpose(new_keypoints, [1, 0, 2])
     return new_keypoints
+
+
+def keypoint_weights_from_visibilities(keypoint_visibilities,
+                                       per_keypoint_weights=None):
+  """Returns a keypoint weights tensor.
+
+  During training, it is often beneficial to consider only those keypoints that
+  are labeled. This function returns a weights tensor that combines default
+  per-keypoint weights, as well as the visibilities of individual keypoints.
+
+  The returned tensor satisfies:
+  keypoint_weights[i, k] = per_keypoint_weights[k] * keypoint_visibilities[i, k]
+  where per_keypoint_weights[k] is set to 1 if not provided.
+
+  Args:
+    keypoint_visibilities: A [num_instances, num_keypoints] boolean tensor
+      indicating whether a keypoint is labeled (and perhaps even visible).
+    per_keypoint_weights: A list or 1-d tensor of length `num_keypoints` with
+      per-keypoint weights. If None, will use 1 for each visible keypoint
+      weight.
+
+  Returns:
+    A [num_instances, num_keypoints] float32 tensor with keypoint weights. Those
+    keypoints deemed visible will have the provided per-keypoint weight, and
+    all others will be set to zero.
+  """
+  if per_keypoint_weights is None:
+    num_keypoints = keypoint_visibilities.shape.as_list()[1]
+    per_keypoint_weight_mult = tf.ones((1, num_keypoints,), dtype=tf.float32)
+  else:
+    per_keypoint_weight_mult = tf.expand_dims(per_keypoint_weights, axis=0)
+  return per_keypoint_weight_mult * tf.cast(keypoint_visibilities, tf.float32)
+
+
+def set_keypoint_visibilities(keypoints, initial_keypoint_visibilities=None):
+  """Sets keypoint visibilities based on valid/invalid keypoints.
+
+  Some keypoint operations set invisible keypoints (e.g. cropped keypoints) to
+  NaN, without affecting any keypoint "visibility" variables. This function is
+  used to update (or create) keypoint visibilities to agree with visible /
+  invisible keypoint coordinates.
+
+  Args:
+    keypoints: a float32 tensor of shape [num_instances, num_keypoints, 2].
+    initial_keypoint_visibilities: a boolean tensor of shape
+      [num_instances, num_keypoints]. If provided, will maintain the visibility
+      designation of a keypoint, so long as the corresponding coordinates are
+      not NaN. If not provided, will create keypoint visibilities directly from
+      the values in `keypoints` (i.e. NaN coordinates map to False, otherwise
+      they map to True).
+
+  Returns:
+    keypoint_visibilities: a bool tensor of shape [num_instances, num_keypoints]
+    indicating whether a keypoint is visible or not.
+  """
+  if initial_keypoint_visibilities is not None:
+    keypoint_visibilities = tf.cast(initial_keypoint_visibilities, tf.bool)
+  else:
+    keypoint_visibilities = tf.ones_like(keypoints[:, :, 0], dtype=tf.bool)
+
+  keypoints_with_nan = tf.math.reduce_any(tf.math.is_nan(keypoints), axis=2)
+  keypoint_visibilities = tf.where(
+      keypoints_with_nan,
+      tf.zeros_like(keypoint_visibilities, dtype=tf.bool),
+      keypoint_visibilities)
+  return keypoint_visibilities
diff --git a/research/object_detection/core/keypoint_ops_test.py b/research/object_detection/core/keypoint_ops_test.py
index 1c09c55a..1af6ef81 100644
--- a/research/object_detection/core/keypoint_ops_test.py
+++ b/research/object_detection/core/keypoint_ops_test.py
@@ -18,183 +18,300 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.core import keypoint_ops
+from object_detection.utils import test_case
 
 
-class KeypointOpsTest(tf.test.TestCase):
+class KeypointOpsTest(test_case.TestCase):
   """Tests for common keypoint operations."""
 
   def test_scale(self):
-    keypoints = tf.constant([
-        [[0.0, 0.0], [100.0, 200.0]],
-        [[50.0, 120.0], [100.0, 140.0]]
-    ])
-    y_scale = tf.constant(1.0 / 100)
-    x_scale = tf.constant(1.0 / 200)
-
-    expected_keypoints = tf.constant([
-        [[0., 0.], [1.0, 1.0]],
-        [[0.5, 0.6], [1.0, 0.7]]
-    ])
-    output = keypoint_ops.scale(keypoints, y_scale, x_scale)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.0, 0.0], [100.0, 200.0]],
+          [[50.0, 120.0], [100.0, 140.0]]
+      ])
+      y_scale = tf.constant(1.0 / 100)
+      x_scale = tf.constant(1.0 / 200)
+
+      expected_keypoints = tf.constant([
+          [[0., 0.], [1.0, 1.0]],
+          [[0.5, 0.6], [1.0, 0.7]]
+      ])
+      output = keypoint_ops.scale(keypoints, y_scale, x_scale)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_clip_to_window(self):
-    keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-    window = tf.constant([0.25, 0.25, 0.75, 0.75])
-
-    expected_keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.25], [0.75, 0.75]]
-    ])
-    output = keypoint_ops.clip_to_window(keypoints, window)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      window = tf.constant([0.25, 0.25, 0.75, 0.75])
+
+      expected_keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.25], [0.75, 0.75]]
+      ])
+      output = keypoint_ops.clip_to_window(keypoints, window)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_prune_outside_window(self):
-    keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-    window = tf.constant([0.25, 0.25, 0.75, 0.75])
-
-    expected_keypoints = tf.constant([[[0.25, 0.5], [0.75, 0.75]],
-                                      [[np.nan, np.nan], [np.nan, np.nan]]])
-    output = keypoint_ops.prune_outside_window(keypoints, window)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      window = tf.constant([0.25, 0.25, 0.75, 0.75])
+
+      expected_keypoints = tf.constant([[[0.25, 0.5], [0.75, 0.75]],
+                                        [[np.nan, np.nan], [np.nan, np.nan]]])
+      output = keypoint_ops.prune_outside_window(keypoints, window)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_change_coordinate_frame(self):
-    keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-    window = tf.constant([0.25, 0.25, 0.75, 0.75])
-
-    expected_keypoints = tf.constant([
-        [[0, 0.5], [1.0, 1.0]],
-        [[0.5, -0.5], [1.5, 1.5]]
-    ])
-    output = keypoint_ops.change_coordinate_frame(keypoints, window)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      window = tf.constant([0.25, 0.25, 0.75, 0.75])
+
+      expected_keypoints = tf.constant([
+          [[0, 0.5], [1.0, 1.0]],
+          [[0.5, -0.5], [1.5, 1.5]]
+      ])
+      output = keypoint_ops.change_coordinate_frame(keypoints, window)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
+
+  def test_keypoints_to_enclosing_bounding_boxes(self):
+    def graph_fn():
+      keypoints = tf.constant(
+          [
+              [  # Instance 0.
+                  [5., 10.],
+                  [3., 20.],
+                  [8., 4.],
+              ],
+              [  # Instance 1.
+                  [2., 12.],
+                  [0., 3.],
+                  [5., 19.],
+              ],
+          ], dtype=tf.float32)
+      bboxes = keypoint_ops.keypoints_to_enclosing_bounding_boxes(keypoints)
+      return bboxes
+    output = self.execute(graph_fn, [])
+    expected_bboxes = np.array(
+        [
+            [3., 4., 8., 20.],
+            [0., 3., 5., 19.]
+        ])
+    self.assertAllClose(expected_bboxes, output)
 
   def test_to_normalized_coordinates(self):
-    keypoints = tf.constant([
-        [[10., 30.], [30., 45.]],
-        [[20., 0.], [40., 60.]]
-    ])
-    output = keypoint_ops.to_normalized_coordinates(
-        keypoints, 40, 60)
-    expected_keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[10., 30.], [30., 45.]],
+          [[20., 0.], [40., 60.]]
+      ])
+      output = keypoint_ops.to_normalized_coordinates(
+          keypoints, 40, 60)
+      expected_keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_to_normalized_coordinates_already_normalized(self):
-    keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-    output = keypoint_ops.to_normalized_coordinates(
-        keypoints, 40, 60)
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('assertion failed'):
-        sess.run(output)
+    if self.has_tpu(): return
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      output = keypoint_ops.to_normalized_coordinates(
+          keypoints, 40, 60)
+      return output
+    with self.assertRaisesOpError('assertion failed'):
+      self.execute_cpu(graph_fn, [])
 
   def test_to_absolute_coordinates(self):
-    keypoints = tf.constant([
-        [[0.25, 0.5], [0.75, 0.75]],
-        [[0.5, 0.0], [1.0, 1.0]]
-    ])
-    output = keypoint_ops.to_absolute_coordinates(
-        keypoints, 40, 60)
-    expected_keypoints = tf.constant([
-        [[10., 30.], [30., 45.]],
-        [[20., 0.], [40., 60.]]
-    ])
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.25, 0.5], [0.75, 0.75]],
+          [[0.5, 0.0], [1.0, 1.0]]
+      ])
+      output = keypoint_ops.to_absolute_coordinates(
+          keypoints, 40, 60)
+      expected_keypoints = tf.constant([
+          [[10., 30.], [30., 45.]],
+          [[20., 0.], [40., 60.]]
+      ])
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_to_absolute_coordinates_already_absolute(self):
-    keypoints = tf.constant([
-        [[10., 30.], [30., 45.]],
-        [[20., 0.], [40., 60.]]
-    ])
-    output = keypoint_ops.to_absolute_coordinates(
-        keypoints, 40, 60)
-
-    with self.test_session() as sess:
-      with self.assertRaisesOpError('assertion failed'):
-        sess.run(output)
+    if self.has_tpu(): return
+    def graph_fn():
+      keypoints = tf.constant([
+          [[10., 30.], [30., 45.]],
+          [[20., 0.], [40., 60.]]
+      ])
+      output = keypoint_ops.to_absolute_coordinates(
+          keypoints, 40, 60)
+      return output
+    with self.assertRaisesOpError('assertion failed'):
+      self.execute_cpu(graph_fn, [])
 
   def test_flip_horizontal(self):
-    keypoints = tf.constant([
-        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
-        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]
-    ])
-    flip_permutation = [0, 2, 1]
-
-    expected_keypoints = tf.constant([
-        [[0.1, 0.9], [0.3, 0.7], [0.2, 0.8]],
-        [[0.4, 0.6], [0.6, 0.4], [0.5, 0.5]],
-    ])
-    output = keypoint_ops.flip_horizontal(keypoints, 0.5, flip_permutation)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
+          [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]
+      ])
+      flip_permutation = [0, 2, 1]
+
+      expected_keypoints = tf.constant([
+          [[0.1, 0.9], [0.3, 0.7], [0.2, 0.8]],
+          [[0.4, 0.6], [0.6, 0.4], [0.5, 0.5]],
+      ])
+      output = keypoint_ops.flip_horizontal(keypoints, 0.5, flip_permutation)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_flip_vertical(self):
-    keypoints = tf.constant([
-        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
-        [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]
-    ])
-    flip_permutation = [0, 2, 1]
-
-    expected_keypoints = tf.constant([
-        [[0.9, 0.1], [0.7, 0.3], [0.8, 0.2]],
-        [[0.6, 0.4], [0.4, 0.6], [0.5, 0.5]],
-    ])
-    output = keypoint_ops.flip_vertical(keypoints, 0.5, flip_permutation)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
+          [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]]
+      ])
+      flip_permutation = [0, 2, 1]
+
+      expected_keypoints = tf.constant([
+          [[0.9, 0.1], [0.7, 0.3], [0.8, 0.2]],
+          [[0.6, 0.4], [0.4, 0.6], [0.5, 0.5]],
+      ])
+      output = keypoint_ops.flip_vertical(keypoints, 0.5, flip_permutation)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
 
   def test_rot90(self):
-    keypoints = tf.constant([
-        [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
-        [[0.4, 0.6], [0.5, 0.6], [0.6, 0.7]]
-    ])
-    expected_keypoints = tf.constant([
-        [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]],
-        [[0.4, 0.4], [0.4, 0.5], [0.3, 0.6]],
-    ])
-    output = keypoint_ops.rot90(keypoints)
-
-    with self.test_session() as sess:
-      output_, expected_keypoints_ = sess.run([output, expected_keypoints])
-      self.assertAllClose(output_, expected_keypoints_)
-
+    def graph_fn():
+      keypoints = tf.constant([
+          [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
+          [[0.4, 0.6], [0.5, 0.6], [0.6, 0.7]]
+      ])
+      expected_keypoints = tf.constant([
+          [[0.9, 0.1], [0.8, 0.2], [0.7, 0.3]],
+          [[0.4, 0.4], [0.4, 0.5], [0.3, 0.6]],
+      ])
+      output = keypoint_ops.rot90(keypoints)
+      return output, expected_keypoints
+    output, expected_keypoints = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoints)
+
+  def test_keypoint_weights_from_visibilities(self):
+    def graph_fn():
+      keypoint_visibilities = tf.constant([
+          [True, True, False],
+          [False, True, False]
+      ])
+      per_keypoint_weights = [1.0, 2.0, 3.0]
+      keypoint_weights = keypoint_ops.keypoint_weights_from_visibilities(
+          keypoint_visibilities, per_keypoint_weights)
+      return keypoint_weights
+    expected_keypoint_weights = [
+        [1.0, 2.0, 0.0],
+        [0.0, 2.0, 0.0]
+    ]
+    output = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_keypoint_weights)
+
+  def test_keypoint_weights_from_visibilities_no_per_kpt_weights(self):
+    def graph_fn():
+      keypoint_visibilities = tf.constant([
+          [True, True, False],
+          [False, True, False]
+      ])
+      keypoint_weights = keypoint_ops.keypoint_weights_from_visibilities(
+          keypoint_visibilities)
+      return keypoint_weights
+    expected_keypoint_weights = [
+        [1.0, 1.0, 0.0],
+        [0.0, 1.0, 0.0]
+    ]
+    output = self.execute(graph_fn, [])
+    self.assertAllClose(expected_keypoint_weights, output)
+
+  def test_set_keypoint_visibilities_no_initial_kpt_vis(self):
+    keypoints_np = np.array(
+        [
+            [[np.nan, 0.2],
+             [np.nan, np.nan],
+             [-3., 7.]],
+            [[0.5, 0.2],
+             [4., 1.0],
+             [-3., np.nan]],
+        ], dtype=np.float32)
+    def graph_fn():
+      keypoints = tf.constant(keypoints_np, dtype=tf.float32)
+      keypoint_visibilities = keypoint_ops.set_keypoint_visibilities(
+          keypoints)
+      return keypoint_visibilities
+
+    expected_kpt_vis = [
+        [False, False, True],
+        [True, True, False]
+    ]
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(expected_kpt_vis, output)
+
+  def test_set_keypoint_visibilities(self):
+    keypoints_np = np.array(
+        [
+            [[np.nan, 0.2],
+             [np.nan, np.nan],
+             [-3., 7.]],
+            [[0.5, 0.2],
+             [4., 1.0],
+             [-3., np.nan]],
+        ], dtype=np.float32)
+    initial_keypoint_visibilities_np = np.array(
+        [
+            [False,
+             True,  # Will be overriden by NaN coords.
+             False],  # Will be maintained, even though non-NaN coords.
+            [True,
+             False,  # Will be maintained, even though non-NaN coords.
+             False]
+        ])
+    def graph_fn():
+      keypoints = tf.constant(keypoints_np, dtype=tf.float32)
+      initial_keypoint_visibilities = tf.constant(
+          initial_keypoint_visibilities_np, dtype=tf.bool)
+      keypoint_visibilities = keypoint_ops.set_keypoint_visibilities(
+          keypoints, initial_keypoint_visibilities)
+      return keypoint_visibilities
+
+    expected_kpt_vis = [
+        [False, False, False],
+        [True, False, False]
+    ]
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(expected_kpt_vis, output)
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index b4fa4286..06151bb2 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -33,12 +33,13 @@ from __future__ import print_function
 import abc
 import six
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.utils import ops
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class Loss(six.with_metaclass(abc.ABCMeta, object)):
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index 56340ff3..3fee6d9f 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -27,938 +27,960 @@ import tensorflow as tf
 from object_detection.core import box_list
 from object_detection.core import losses
 from object_detection.core import matcher
+from object_detection.utils import test_case
 
 
-class WeightedL2LocalizationLossTest(tf.test.TestCase):
+class WeightedL2LocalizationLossTest(test_case.TestCase):
 
   def testReturnsCorrectWeightedLoss(self):
     batch_size = 3
     num_anchors = 10
     code_size = 4
-    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
-                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
-                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
-    loss_op = losses.WeightedL2LocalizationLoss()
-    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
-                                 weights=weights))
-
+    def graph_fn():
+      prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
+      target_tensor = tf.zeros([batch_size, num_anchors, code_size])
+      weights = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
+                             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
+                             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
+      loss_op = losses.WeightedL2LocalizationLoss()
+      loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
+                                   weights=weights))
+      return loss
     expected_loss = (3 * 5 * 4) / 2.0
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, expected_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, expected_loss)
 
   def testReturnsCorrectAnchorwiseLoss(self):
     batch_size = 3
     num_anchors = 16
     code_size = 4
-    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.ones([batch_size, num_anchors])
-    loss_op = losses.WeightedL2LocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
+    def graph_fn():
+      prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
+      target_tensor = tf.zeros([batch_size, num_anchors, code_size])
+      weights = tf.ones([batch_size, num_anchors])
+      loss_op = losses.WeightedL2LocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      return loss
     expected_loss = np.ones((batch_size, num_anchors)) * 2
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, expected_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, expected_loss)
 
   def testReturnsCorrectNanLoss(self):
     batch_size = 3
     num_anchors = 10
     code_size = 4
-    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
-    target_tensor = tf.concat([
-        tf.zeros([batch_size, num_anchors, code_size / 2]),
-        tf.ones([batch_size, num_anchors, code_size / 2]) * np.nan
-    ], axis=2)
-    weights = tf.ones([batch_size, num_anchors])
-    loss_op = losses.WeightedL2LocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights,
-                   ignore_nan_targets=True)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
+      target_tensor = tf.concat([
+          tf.zeros([batch_size, num_anchors, code_size / 2]),
+          tf.ones([batch_size, num_anchors, code_size / 2]) * np.nan
+      ], axis=2)
+      weights = tf.ones([batch_size, num_anchors])
+      loss_op = losses.WeightedL2LocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights,
+                     ignore_nan_targets=True)
+      loss = tf.reduce_sum(loss)
+      return loss
     expected_loss = (3 * 5 * 4) / 2.0
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, expected_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, expected_loss)
 
   def testReturnsCorrectWeightedLossWithLossesMask(self):
     batch_size = 4
     num_anchors = 10
     code_size = 4
-    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
-                           [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],
-                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
-                           [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
-    losses_mask = tf.constant([True, False, True, True], tf.bool)
-    loss_op = losses.WeightedL2LocalizationLoss()
-    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
-                                 weights=weights, losses_mask=losses_mask))
-
+    def graph_fn():
+      prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
+      target_tensor = tf.zeros([batch_size, num_anchors, code_size])
+      weights = tf.constant([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
+                             [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],
+                             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
+                             [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
+      losses_mask = tf.constant([True, False, True, True], tf.bool)
+      loss_op = losses.WeightedL2LocalizationLoss()
+      loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
+                                   weights=weights, losses_mask=losses_mask))
+      return loss
     expected_loss = (3 * 5 * 4) / 2.0
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, expected_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, expected_loss)
 
 
-class WeightedSmoothL1LocalizationLossTest(tf.test.TestCase):
+class WeightedSmoothL1LocalizationLossTest(test_case.TestCase):
 
   def testReturnsCorrectLoss(self):
     batch_size = 2
     num_anchors = 3
     code_size = 4
-    prediction_tensor = tf.constant([[[2.5, 0, .4, 0],
-                                      [0, 0, 0, 0],
-                                      [0, 2.5, 0, .4]],
-                                     [[3.5, 0, 0, 0],
-                                      [0, .4, 0, .9],
-                                      [0, 0, 1.5, 0]]], tf.float32)
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.constant([[2, 1, 1],
-                           [0, 3, 0]], tf.float32)
-    loss_op = losses.WeightedSmoothL1LocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[2.5, 0, .4, 0],
+                                        [0, 0, 0, 0],
+                                        [0, 2.5, 0, .4]],
+                                       [[3.5, 0, 0, 0],
+                                        [0, .4, 0, .9],
+                                        [0, 0, 1.5, 0]]], tf.float32)
+      target_tensor = tf.zeros([batch_size, num_anchors, code_size])
+      weights = tf.constant([[2, 1, 1],
+                             [0, 3, 0]], tf.float32)
+      loss_op = losses.WeightedSmoothL1LocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
     exp_loss = 7.695
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossWithLossesMask(self):
     batch_size = 3
     num_anchors = 3
     code_size = 4
-    prediction_tensor = tf.constant([[[2.5, 0, .4, 0],
-                                      [0, 0, 0, 0],
-                                      [0, 2.5, 0, .4]],
-                                     [[3.5, 0, 0, 0],
-                                      [0, .4, 0, .9],
-                                      [0, 0, 1.5, 0]],
-                                     [[3.5, 7., 0, 0],
-                                      [0, .4, 0, .9],
-                                      [2.2, 2.2, 1.5, 0]]], tf.float32)
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.constant([[2, 1, 1],
-                           [0, 3, 0],
-                           [4, 3, 0]], tf.float32)
-    losses_mask = tf.constant([True, True, False], tf.bool)
-    loss_op = losses.WeightedSmoothL1LocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights,
-                   losses_mask=losses_mask)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[2.5, 0, .4, 0],
+                                        [0, 0, 0, 0],
+                                        [0, 2.5, 0, .4]],
+                                       [[3.5, 0, 0, 0],
+                                        [0, .4, 0, .9],
+                                        [0, 0, 1.5, 0]],
+                                       [[3.5, 7., 0, 0],
+                                        [0, .4, 0, .9],
+                                        [2.2, 2.2, 1.5, 0]]], tf.float32)
+      target_tensor = tf.zeros([batch_size, num_anchors, code_size])
+      weights = tf.constant([[2, 1, 1],
+                             [0, 3, 0],
+                             [4, 3, 0]], tf.float32)
+      losses_mask = tf.constant([True, True, False], tf.bool)
+      loss_op = losses.WeightedSmoothL1LocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights,
+                     losses_mask=losses_mask)
+      loss = tf.reduce_sum(loss)
+      return loss
     exp_loss = 7.695
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
 
-class WeightedIOULocalizationLossTest(tf.test.TestCase):
+class WeightedIOULocalizationLossTest(test_case.TestCase):
 
   def testReturnsCorrectLoss(self):
-    prediction_tensor = tf.constant([[[1.5, 0, 2.4, 1],
-                                      [0, 0, 1, 1],
-                                      [0, 0, .5, .25]]])
-    target_tensor = tf.constant([[[1.5, 0, 2.4, 1],
-                                  [0, 0, 1, 1],
-                                  [50, 50, 500.5, 100.25]]])
-    weights = [[1.0, .5, 2.0]]
-    loss_op = losses.WeightedIOULocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[1.5, 0, 2.4, 1],
+                                        [0, 0, 1, 1],
+                                        [0, 0, .5, .25]]])
+      target_tensor = tf.constant([[[1.5, 0, 2.4, 1],
+                                    [0, 0, 1, 1],
+                                    [50, 50, 500.5, 100.25]]])
+      weights = [[1.0, .5, 2.0]]
+      loss_op = losses.WeightedIOULocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
     exp_loss = 2.0
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossWithNoLabels(self):
-    prediction_tensor = tf.constant([[[1.5, 0, 2.4, 1],
-                                      [0, 0, 1, 1],
-                                      [0, 0, .5, .25]]])
-    target_tensor = tf.constant([[[1.5, 0, 2.4, 1],
-                                  [0, 0, 1, 1],
-                                  [50, 50, 500.5, 100.25]]])
-    weights = [[1.0, .5, 2.0]]
-    losses_mask = tf.constant([False], tf.bool)
-    loss_op = losses.WeightedIOULocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights,
-                   losses_mask=losses_mask)
-    loss = tf.reduce_sum(loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[1.5, 0, 2.4, 1],
+                                        [0, 0, 1, 1],
+                                        [0, 0, .5, .25]]])
+      target_tensor = tf.constant([[[1.5, 0, 2.4, 1],
+                                    [0, 0, 1, 1],
+                                    [50, 50, 500.5, 100.25]]])
+      weights = [[1.0, .5, 2.0]]
+      losses_mask = tf.constant([False], tf.bool)
+      loss_op = losses.WeightedIOULocalizationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights,
+                     losses_mask=losses_mask)
+      loss = tf.reduce_sum(loss)
+      return loss
     exp_loss = 0.0
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
 
-class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
+class WeightedSigmoidClassificationLossTest(test_case.TestCase):
 
   def testReturnsCorrectLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    loss_op = losses.WeightedSigmoidClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      loss_op = losses.WeightedSigmoidClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
 
     exp_loss = -2 * math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    loss_op = losses.WeightedSigmoidClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss, axis=2)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      loss_op = losses.WeightedSigmoidClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss, axis=2)
+      return loss
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossWithClassIndices(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100, 100],
-                                      [100, -100, -100, -100],
-                                      [100, 0, -100, 100],
-                                      [-100, -100, 100, -100]],
-                                     [[-100, 0, 100, 100],
-                                      [-100, 100, -100, 100],
-                                      [100, 100, 100, 100],
-                                      [0, 0, -1, 100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0, 0],
-                                  [1, 0, 0, 1],
-                                  [1, 0, 0, 0],
-                                  [0, 0, 1, 1]],
-                                 [[0, 0, 1, 0],
-                                  [0, 1, 0, 0],
-                                  [1, 1, 1, 0],
-                                  [1, 0, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1, 1],
-                            [1, 1, 1, 1],
-                            [1, 1, 1, 1],
-                            [1, 1, 1, 1]],
-                           [[1, 1, 1, 1],
-                            [1, 1, 1, 1],
-                            [1, 1, 1, 1],
-                            [0, 0, 0, 0]]], tf.float32)
-    # Ignores the last class.
-    class_indices = tf.constant([0, 1, 2], tf.int32)
-    loss_op = losses.WeightedSigmoidClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights,
-                   class_indices=class_indices)
-    loss = tf.reduce_sum(loss, axis=2)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100, 100],
+                                        [100, -100, -100, -100],
+                                        [100, 0, -100, 100],
+                                        [-100, -100, 100, -100]],
+                                       [[-100, 0, 100, 100],
+                                        [-100, 100, -100, 100],
+                                        [100, 100, 100, 100],
+                                        [0, 0, -1, 100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0, 0],
+                                    [1, 0, 0, 1],
+                                    [1, 0, 0, 0],
+                                    [0, 0, 1, 1]],
+                                   [[0, 0, 1, 0],
+                                    [0, 1, 0, 0],
+                                    [1, 1, 1, 0],
+                                    [1, 0, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1, 1],
+                              [1, 1, 1, 1],
+                              [1, 1, 1, 1],
+                              [1, 1, 1, 1]],
+                             [[1, 1, 1, 1],
+                              [1, 1, 1, 1],
+                              [1, 1, 1, 1],
+                              [0, 0, 0, 0]]], tf.float32)
+      # Ignores the last class.
+      class_indices = tf.constant([0, 1, 2], tf.int32)
+      loss_op = losses.WeightedSigmoidClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights,
+                     class_indices=class_indices)
+      loss = tf.reduce_sum(loss, axis=2)
+      return loss
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossWithLossesMask(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]],
-                                 [[0, 0, 0],
-                                  [0, 0, 0],
-                                  [0, 0, 0],
-                                  [0, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    losses_mask = tf.constant([True, True, False], tf.bool)
-
-    loss_op = losses.WeightedSigmoidClassificationLoss()
-    loss_per_anchor = loss_op(prediction_tensor, target_tensor, weights=weights,
-                              losses_mask=losses_mask)
-    loss = tf.reduce_sum(loss_per_anchor)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]],
+                                   [[0, 0, 0],
+                                    [0, 0, 0],
+                                    [0, 0, 0],
+                                    [0, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      losses_mask = tf.constant([True, True, False], tf.bool)
+
+      loss_op = losses.WeightedSigmoidClassificationLoss()
+      loss_per_anchor = loss_op(prediction_tensor, target_tensor,
+                                weights=weights,
+                                losses_mask=losses_mask)
+      loss = tf.reduce_sum(loss_per_anchor)
+      return loss
 
     exp_loss = -2 * math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllEqual(prediction_tensor.shape.as_list(),
-                          loss_per_anchor.shape.as_list())
-      self.assertAllEqual(target_tensor.shape.as_list(),
-                          loss_per_anchor.shape.as_list())
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
 
 def _logit(probability):
   return math.log(probability / (1. - probability))
 
 
-class SigmoidFocalClassificationLossTest(tf.test.TestCase):
+class SigmoidFocalClassificationLossTest(test_case.TestCase):
 
   def testEasyExamplesProduceSmallLossComparedToSigmoidXEntropy(self):
-    prediction_tensor = tf.constant([[[_logit(0.97)],
-                                      [_logit(0.91)],
-                                      [_logit(0.73)],
-                                      [_logit(0.27)],
-                                      [_logit(0.09)],
-                                      [_logit(0.03)]]], tf.float32)
-    target_tensor = tf.constant([[[1],
-                                  [1],
-                                  [1],
-                                  [0],
-                                  [0],
-                                  [0]]], tf.float32)
-    weights = tf.constant([[[1], [1], [1], [1], [1], [1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
-                                                 target_tensor,
-                                                 weights=weights), axis=2)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      order_of_ratio = np.power(10,
-                                np.floor(np.log10(sigmoid_loss / focal_loss)))
-      self.assertAllClose(order_of_ratio, [[1000, 100, 10, 10, 100, 1000]])
+    def graph_fn():
+      prediction_tensor = tf.constant([[[_logit(0.97)],
+                                        [_logit(0.91)],
+                                        [_logit(0.73)],
+                                        [_logit(0.27)],
+                                        [_logit(0.09)],
+                                        [_logit(0.03)]]], tf.float32)
+      target_tensor = tf.constant([[[1],
+                                    [1],
+                                    [1],
+                                    [0],
+                                    [0],
+                                    [0]]], tf.float32)
+      weights = tf.constant([[[1], [1], [1], [1], [1], [1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0,
+                                                            alpha=None)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights), axis=2)
+      sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                   target_tensor,
+                                                   weights=weights), axis=2)
+      return sigmoid_loss, focal_loss
+
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    order_of_ratio = np.power(10,
+                              np.floor(np.log10(sigmoid_loss / focal_loss)))
+    self.assertAllClose(order_of_ratio, [[1000, 100, 10, 10, 100, 1000]])
 
   def testHardExamplesProduceLossComparableToSigmoidXEntropy(self):
-    prediction_tensor = tf.constant([[[_logit(0.55)],
-                                      [_logit(0.52)],
-                                      [_logit(0.50)],
-                                      [_logit(0.48)],
-                                      [_logit(0.45)]]], tf.float32)
-    target_tensor = tf.constant([[[1],
-                                  [1],
-                                  [1],
-                                  [0],
-                                  [0]]], tf.float32)
-    weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
-                                                 target_tensor,
-                                                 weights=weights), axis=2)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      order_of_ratio = np.power(10,
-                                np.floor(np.log10(sigmoid_loss / focal_loss)))
-      self.assertAllClose(order_of_ratio, [[1., 1., 1., 1., 1.]])
+    def graph_fn():
+      prediction_tensor = tf.constant([[[_logit(0.55)],
+                                        [_logit(0.52)],
+                                        [_logit(0.50)],
+                                        [_logit(0.48)],
+                                        [_logit(0.45)]]], tf.float32)
+      target_tensor = tf.constant([[[1],
+                                    [1],
+                                    [1],
+                                    [0],
+                                    [0]]], tf.float32)
+      weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0,
+                                                            alpha=None)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights), axis=2)
+      sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                   target_tensor,
+                                                   weights=weights), axis=2)
+      return sigmoid_loss, focal_loss
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    order_of_ratio = np.power(10,
+                              np.floor(np.log10(sigmoid_loss / focal_loss)))
+    self.assertAllClose(order_of_ratio, [[1., 1., 1., 1., 1.]])
 
   def testNonAnchorWiseOutputComparableToSigmoidXEntropy(self):
-    prediction_tensor = tf.constant([[[_logit(0.55)],
-                                      [_logit(0.52)],
-                                      [_logit(0.50)],
-                                      [_logit(0.48)],
-                                      [_logit(0.45)]]], tf.float32)
-    target_tensor = tf.constant([[[1],
-                                  [1],
-                                  [1],
-                                  [0],
-                                  [0]]], tf.float32)
-    weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights))
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
-                                                 target_tensor,
-                                                 weights=weights))
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      order_of_ratio = np.power(10,
-                                np.floor(np.log10(sigmoid_loss / focal_loss)))
-      self.assertAlmostEqual(order_of_ratio, 1.)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[_logit(0.55)],
+                                        [_logit(0.52)],
+                                        [_logit(0.50)],
+                                        [_logit(0.48)],
+                                        [_logit(0.45)]]], tf.float32)
+      target_tensor = tf.constant([[[1],
+                                    [1],
+                                    [1],
+                                    [0],
+                                    [0]]], tf.float32)
+      weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0,
+                                                            alpha=None)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights))
+      sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                   target_tensor,
+                                                   weights=weights))
+      return sigmoid_loss, focal_loss
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    order_of_ratio = np.power(10,
+                              np.floor(np.log10(sigmoid_loss / focal_loss)))
+    self.assertAlmostEqual(order_of_ratio, 1.)
 
   def testIgnoreNegativeExampleLossViaAlphaMultiplier(self):
-    prediction_tensor = tf.constant([[[_logit(0.55)],
-                                      [_logit(0.52)],
-                                      [_logit(0.50)],
-                                      [_logit(0.48)],
-                                      [_logit(0.45)]]], tf.float32)
-    target_tensor = tf.constant([[[1],
-                                  [1],
-                                  [1],
-                                  [0],
-                                  [0]]], tf.float32)
-    weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=1.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
-                                                 target_tensor,
-                                                 weights=weights), axis=2)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      self.assertAllClose(focal_loss[0][3:], [0., 0.])
-      order_of_ratio = np.power(10,
-                                np.floor(np.log10(sigmoid_loss[0][:3] /
-                                                  focal_loss[0][:3])))
-      self.assertAllClose(order_of_ratio, [1., 1., 1.])
+    def graph_fn():
+      prediction_tensor = tf.constant([[[_logit(0.55)],
+                                        [_logit(0.52)],
+                                        [_logit(0.50)],
+                                        [_logit(0.48)],
+                                        [_logit(0.45)]]], tf.float32)
+      target_tensor = tf.constant([[[1],
+                                    [1],
+                                    [1],
+                                    [0],
+                                    [0]]], tf.float32)
+      weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0,
+                                                            alpha=1.0)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights), axis=2)
+      sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                   target_tensor,
+                                                   weights=weights), axis=2)
+      return sigmoid_loss, focal_loss
+
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(focal_loss[0][3:], [0., 0.])
+    order_of_ratio = np.power(10,
+                              np.floor(np.log10(sigmoid_loss[0][:3] /
+                                                focal_loss[0][:3])))
+    self.assertAllClose(order_of_ratio, [1., 1., 1.])
 
   def testIgnorePositiveExampleLossViaAlphaMultiplier(self):
-    prediction_tensor = tf.constant([[[_logit(0.55)],
-                                      [_logit(0.52)],
-                                      [_logit(0.50)],
-                                      [_logit(0.48)],
-                                      [_logit(0.45)]]], tf.float32)
-    target_tensor = tf.constant([[[1],
-                                  [1],
-                                  [1],
-                                  [0],
-                                  [0]]], tf.float32)
-    weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights), axis=2)
-    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
-                                                 target_tensor,
-                                                 weights=weights), axis=2)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      self.assertAllClose(focal_loss[0][:3], [0., 0., 0.])
-      order_of_ratio = np.power(10,
-                                np.floor(np.log10(sigmoid_loss[0][3:] /
-                                                  focal_loss[0][3:])))
-      self.assertAllClose(order_of_ratio, [1., 1.])
+    def graph_fn():
+      prediction_tensor = tf.constant([[[_logit(0.55)],
+                                        [_logit(0.52)],
+                                        [_logit(0.50)],
+                                        [_logit(0.48)],
+                                        [_logit(0.45)]]], tf.float32)
+      target_tensor = tf.constant([[[1],
+                                    [1],
+                                    [1],
+                                    [0],
+                                    [0]]], tf.float32)
+      weights = tf.constant([[[1], [1], [1], [1], [1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0,
+                                                            alpha=0.0)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights), axis=2)
+      sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                   target_tensor,
+                                                   weights=weights), axis=2)
+      return sigmoid_loss, focal_loss
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(focal_loss[0][:3], [0., 0., 0.])
+    order_of_ratio = np.power(10,
+                              np.floor(np.log10(sigmoid_loss[0][3:] /
+                                                focal_loss[0][3:])))
+    self.assertAllClose(order_of_ratio, [1., 1.])
 
   def testSimilarToSigmoidXEntropyWithHalfAlphaAndZeroGammaUpToAScale(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.5, gamma=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      self.assertAllClose(sigmoid_loss, focal_loss * 2)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.5,
+                                                            gamma=0.0)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = focal_loss_op(prediction_tensor, target_tensor,
+                                 weights=weights)
+      sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
+                                     weights=weights)
+      return sigmoid_loss, focal_loss
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(sigmoid_loss, focal_loss * 2)
 
   def testSameAsSigmoidXEntropyWithNoAlphaAndZeroGamma(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=None, gamma=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
-
-    with self.test_session() as sess:
-      sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
-      self.assertAllClose(sigmoid_loss, focal_loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=None,
+                                                            gamma=0.0)
+      sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+      focal_loss = focal_loss_op(prediction_tensor, target_tensor,
+                                 weights=weights)
+      sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
+                                     weights=weights)
+      return sigmoid_loss, focal_loss
+    sigmoid_loss, focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(sigmoid_loss, focal_loss)
 
   def testExpectedLossWithAlphaOneAndZeroGamma(self):
-    # All zeros correspond to 0.5 probability.
-    prediction_tensor = tf.constant([[[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]],
-                                     [[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0, gamma=0.0)
-
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights))
-    with self.test_session() as sess:
-      focal_loss = sess.run(focal_loss)
-      self.assertAllClose(
-          (-math.log(.5) *  # x-entropy per class per anchor
-           1.0 *            # alpha
-           8),              # positives from 8 anchors
-          focal_loss)
+    def graph_fn():
+      # All zeros correspond to 0.5 probability.
+      prediction_tensor = tf.constant([[[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]],
+                                       [[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0,
+                                                            gamma=0.0)
+
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights))
+      return focal_loss
+    focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(
+        (-math.log(.5) *  # x-entropy per class per anchor
+         1.0 *            # alpha
+         8),              # positives from 8 anchors
+        focal_loss)
 
   def testExpectedLossWithAlpha75AndZeroGamma(self):
-    # All zeros correspond to 0.5 probability.
-    prediction_tensor = tf.constant([[[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]],
-                                     [[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)
-
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights))
-    with self.test_session() as sess:
-      focal_loss = sess.run(focal_loss)
-      self.assertAllClose(
-          (-math.log(.5) *  # x-entropy per class per anchor.
-           ((0.75 *         # alpha for positives.
-             8) +           # positives from 8 anchors.
-            (0.25 *         # alpha for negatives.
-             8 * 2))),      # negatives from 8 anchors for two classes.
-          focal_loss)
+    def graph_fn():
+      # All zeros correspond to 0.5 probability.
+      prediction_tensor = tf.constant([[[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]],
+                                       [[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75,
+                                                            gamma=0.0)
+
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights))
+      return focal_loss
+    focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(
+        (-math.log(.5) *  # x-entropy per class per anchor.
+         ((0.75 *         # alpha for positives.
+           8) +           # positives from 8 anchors.
+          (0.25 *         # alpha for negatives.
+           8 * 2))),      # negatives from 8 anchors for two classes.
+        focal_loss)
 
   def testExpectedLossWithLossesMask(self):
-    # All zeros correspond to 0.5 probability.
-    prediction_tensor = tf.constant([[[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]],
-                                     [[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]],
-                                     [[0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0],
-                                      [0, 0, 0]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0]],
-                                 [[1, 0, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    losses_mask = tf.constant([True, True, False], tf.bool)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)
-
-    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-                                             weights=weights,
-                                             losses_mask=losses_mask))
-    with self.test_session() as sess:
-      focal_loss = sess.run(focal_loss)
-      self.assertAllClose(
-          (-math.log(.5) *  # x-entropy per class per anchor.
-           ((0.75 *         # alpha for positives.
-             8) +           # positives from 8 anchors.
-            (0.25 *         # alpha for negatives.
-             8 * 2))),      # negatives from 8 anchors for two classes.
-          focal_loss)
-
-
-class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
+    def graph_fn():
+      # All zeros correspond to 0.5 probability.
+      prediction_tensor = tf.constant([[[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]],
+                                       [[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]],
+                                       [[0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0],
+                                        [0, 0, 0]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0]],
+                                   [[1, 0, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      losses_mask = tf.constant([True, True, False], tf.bool)
+      focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75,
+                                                            gamma=0.0)
+
+      focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                               weights=weights,
+                                               losses_mask=losses_mask))
+      return focal_loss
+    focal_loss = self.execute(graph_fn, [])
+    self.assertAllClose(
+        (-math.log(.5) *  # x-entropy per class per anchor.
+         ((0.75 *         # alpha for positives.
+           8) +           # positives from 8 anchors.
+          (0.25 *         # alpha for negatives.
+           8 * 2))),      # negatives from 8 anchors for two classes.
+        focal_loss)
+
+
+class WeightedSoftmaxClassificationLossTest(test_case.TestCase):
 
   def testReturnsCorrectLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [0, 1, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [0.5, 0.5, 0.5],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    loss_op = losses.WeightedSoftmaxClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [0, 1, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [0.5, 0.5, 0.5],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      loss_op = losses.WeightedSoftmaxClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = - 1.5 * math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [0, 1, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [0.5, 0.5, 0.5],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    loss_op = losses.WeightedSoftmaxClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [0, 1, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [0.5, 0.5, 0.5],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      loss_op = losses.WeightedSoftmaxClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = np.matrix([[0, 0, - 0.5 * math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLossWithHighLogitScaleSetting(self):
     """At very high logit_scale, all predictions will be ~0.33."""
-    # TODO(yonib): Also test logit_scale with anchorwise=False.
-    logit_scale = 10e16
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [0, 1, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    loss_op = losses.WeightedSoftmaxClassificationLoss(logit_scale=logit_scale)
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
+    def graph_fn():
+      # TODO(yonib): Also test logit_scale with anchorwise=False.
+      logit_scale = 10e16
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [0, 1, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      loss_op = losses.WeightedSoftmaxClassificationLoss(
+          logit_scale=logit_scale)
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      return loss
     uniform_distribution_loss = - math.log(.33333333333)
     exp_loss = np.matrix([[uniform_distribution_loss] * 4,
                           [uniform_distribution_loss] * 4])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossWithLossesMask(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [0, 1, 0],
-                                  [1, 0, 0]],
-                                 [[1, 0, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [0.5, 0.5, 0.5],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]]], tf.float32)
-    losses_mask = tf.constant([True, True, False], tf.bool)
-    loss_op = losses.WeightedSoftmaxClassificationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights,
-                   losses_mask=losses_mask)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [0, 1, 0],
+                                    [1, 0, 0]],
+                                   [[1, 0, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [0.5, 0.5, 0.5],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]]], tf.float32)
+      losses_mask = tf.constant([True, True, False], tf.bool)
+      loss_op = losses.WeightedSoftmaxClassificationLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights,
+                     losses_mask=losses_mask)
+      loss = tf.reduce_sum(loss)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = - 1.5 * math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
 
-class WeightedSoftmaxClassificationAgainstLogitsLossTest(tf.test.TestCase):
+class WeightedSoftmaxClassificationAgainstLogitsLossTest(test_case.TestCase):
 
   def testReturnsCorrectLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-
-    target_tensor = tf.constant([[[-100, 100, -100],
-                                  [100, -100, -100],
-                                  [100, -100, -100],
-                                  [-100, -100, 100]],
-                                 [[-100, -100, 100],
-                                  [-100, 100, -100],
-                                  [-100, 100, -100],
-                                  [100, -100, -100]]], tf.float32)
-    weights = tf.constant([[1, 1, .5, 1],
-                           [1, 1, 1, 1]], tf.float32)
-    weights_shape = tf.shape(weights)
-    weights_multiple = tf.concat(
-        [tf.ones_like(weights_shape), tf.constant([3])],
-        axis=0)
-    weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
-    loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+
+      target_tensor = tf.constant([[[-100, 100, -100],
+                                    [100, -100, -100],
+                                    [100, -100, -100],
+                                    [-100, -100, 100]],
+                                   [[-100, -100, 100],
+                                    [-100, 100, -100],
+                                    [-100, 100, -100],
+                                    [100, -100, -100]]], tf.float32)
+      weights = tf.constant([[1, 1, .5, 1],
+                             [1, 1, 1, 1]], tf.float32)
+      weights_shape = tf.shape(weights)
+      weights_multiple = tf.concat(
+          [tf.ones_like(weights_shape), tf.constant([3])],
+          axis=0)
+      weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
+      loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = - 1.5 * math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[-100, 100, -100],
-                                  [100, -100, -100],
-                                  [100, -100, -100],
-                                  [-100, -100, 100]],
-                                 [[-100, -100, 100],
-                                  [-100, 100, -100],
-                                  [-100, 100, -100],
-                                  [100, -100, -100]]], tf.float32)
-    weights = tf.constant([[1, 1, .5, 1],
-                           [1, 1, 1, 0]], tf.float32)
-    weights_shape = tf.shape(weights)
-    weights_multiple = tf.concat(
-        [tf.ones_like(weights_shape), tf.constant([3])],
-        axis=0)
-    weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
-    loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[-100, 100, -100],
+                                    [100, -100, -100],
+                                    [100, -100, -100],
+                                    [-100, -100, 100]],
+                                   [[-100, -100, 100],
+                                    [-100, 100, -100],
+                                    [-100, 100, -100],
+                                    [100, -100, -100]]], tf.float32)
+      weights = tf.constant([[1, 1, .5, 1],
+                             [1, 1, 1, 0]], tf.float32)
+      weights_shape = tf.shape(weights)
+      weights_multiple = tf.concat(
+          [tf.ones_like(weights_shape), tf.constant([3])],
+          axis=0)
+      weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
+      loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss()
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = np.matrix([[0, 0, - 0.5 * math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLossWithLogitScaleSetting(self):
-    logit_scale = 100.
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [0, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 0],
-                                      [-100, 100, -100],
-                                      [-100, 100, -100],
-                                      [100, -100, -100]]], tf.float32)
-    target_tensor = tf.constant([[[-100, 100, -100],
-                                  [100, -100, -100],
-                                  [0, 0, -100],
-                                  [-100, -100, 100]],
-                                 [[-100, 0, 0],
-                                  [-100, 100, -100],
-                                  [-100, 100, -100],
-                                  [100, -100, -100]]], tf.float32)
-    weights = tf.constant([[1, 1, .5, 1],
-                           [1, 1, 1, 0]], tf.float32)
-    weights_shape = tf.shape(weights)
-    weights_multiple = tf.concat(
-        [tf.ones_like(weights_shape), tf.constant([3])],
-        axis=0)
-    weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
-    loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss(
-        logit_scale=logit_scale)
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    def graph_fn():
+      logit_scale = 100.
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [0, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 0],
+                                        [-100, 100, -100],
+                                        [-100, 100, -100],
+                                        [100, -100, -100]]], tf.float32)
+      target_tensor = tf.constant([[[-100, 100, -100],
+                                    [100, -100, -100],
+                                    [0, 0, -100],
+                                    [-100, -100, 100]],
+                                   [[-100, 0, 0],
+                                    [-100, 100, -100],
+                                    [-100, 100, -100],
+                                    [100, -100, -100]]], tf.float32)
+      weights = tf.constant([[1, 1, .5, 1],
+                             [1, 1, 1, 0]], tf.float32)
+      weights_shape = tf.shape(weights)
+      weights_multiple = tf.concat(
+          [tf.ones_like(weights_shape), tf.constant([3])],
+          axis=0)
+      weights = tf.tile(tf.expand_dims(weights, 2), weights_multiple)
+      loss_op = losses.WeightedSoftmaxClassificationAgainstLogitsLoss(
+          logit_scale=logit_scale)
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      return loss
 
     # find softmax of the two prediction types above
     softmax_pred1 = [np.exp(-1), np.exp(-1), np.exp(1)]
@@ -976,226 +998,226 @@ class WeightedSoftmaxClassificationAgainstLogitsLossTest(tf.test.TestCase):
     exp_loss = np.matrix(
         [[exp_entropy1, exp_entropy1, exp_entropy2*.5, exp_entropy1],
          [exp_entropy2, exp_entropy1, exp_entropy1, 0.]])
+    loss_output = self.execute(graph_fn, [])
+    self.assertAllClose(loss_output, exp_loss)
 
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
 
-
-class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
+class BootstrappedSigmoidClassificationLossTest(test_case.TestCase):
 
   def testReturnsCorrectLossSoftBootstrapping(self):
-    prediction_tensor = tf.constant([[[-100, 100, 0],
-                                      [100, -100, -100],
-                                      [100, -100, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, -100, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    alpha = tf.constant(.5, tf.float32)
-    loss_op = losses.BootstrappedSigmoidClassificationLoss(
-        alpha, bootstrap_type='soft')
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, 0],
+                                        [100, -100, -100],
+                                        [100, -100, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, -100, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      alpha = tf.constant(.5, tf.float32)
+      loss_op = losses.BootstrappedSigmoidClassificationLoss(
+          alpha, bootstrap_type='soft')
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = -math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectLossHardBootstrapping(self):
-    prediction_tensor = tf.constant([[[-100, 100, 0],
-                                      [100, -100, -100],
-                                      [100, -100, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, -100, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    alpha = tf.constant(.5, tf.float32)
-    loss_op = losses.BootstrappedSigmoidClassificationLoss(
-        alpha, bootstrap_type='hard')
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, 0],
+                                        [100, -100, -100],
+                                        [100, -100, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, -100, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      alpha = tf.constant(.5, tf.float32)
+      loss_op = losses.BootstrappedSigmoidClassificationLoss(
+          alpha, bootstrap_type='hard')
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = -math.log(.5)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
   def testReturnsCorrectAnchorWiseLoss(self):
-    prediction_tensor = tf.constant([[[-100, 100, -100],
-                                      [100, -100, -100],
-                                      [100, 0, -100],
-                                      [-100, -100, 100]],
-                                     [[-100, 0, 100],
-                                      [-100, 100, -100],
-                                      [100, 100, 100],
-                                      [0, 0, -1]]], tf.float32)
-    target_tensor = tf.constant([[[0, 1, 0],
-                                  [1, 0, 0],
-                                  [1, 0, 0],
-                                  [0, 0, 1]],
-                                 [[0, 0, 1],
-                                  [0, 1, 0],
-                                  [1, 1, 1],
-                                  [1, 0, 0]]], tf.float32)
-    weights = tf.constant([[[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1]],
-                           [[1, 1, 1],
-                            [1, 1, 1],
-                            [1, 1, 1],
-                            [0, 0, 0]]], tf.float32)
-    alpha = tf.constant(.5, tf.float32)
-    loss_op = losses.BootstrappedSigmoidClassificationLoss(
-        alpha, bootstrap_type='hard')
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-    loss = tf.reduce_sum(loss, axis=2)
+    def graph_fn():
+      prediction_tensor = tf.constant([[[-100, 100, -100],
+                                        [100, -100, -100],
+                                        [100, 0, -100],
+                                        [-100, -100, 100]],
+                                       [[-100, 0, 100],
+                                        [-100, 100, -100],
+                                        [100, 100, 100],
+                                        [0, 0, -1]]], tf.float32)
+      target_tensor = tf.constant([[[0, 1, 0],
+                                    [1, 0, 0],
+                                    [1, 0, 0],
+                                    [0, 0, 1]],
+                                   [[0, 0, 1],
+                                    [0, 1, 0],
+                                    [1, 1, 1],
+                                    [1, 0, 0]]], tf.float32)
+      weights = tf.constant([[[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1]],
+                             [[1, 1, 1],
+                              [1, 1, 1],
+                              [1, 1, 1],
+                              [0, 0, 0]]], tf.float32)
+      alpha = tf.constant(.5, tf.float32)
+      loss_op = losses.BootstrappedSigmoidClassificationLoss(
+          alpha, bootstrap_type='hard')
+      loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+      loss = tf.reduce_sum(loss, axis=2)
+      return loss
+    loss_output = self.execute(graph_fn, [])
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      self.assertAllClose(loss_output, exp_loss)
+    self.assertAllClose(loss_output, exp_loss)
 
 
-class HardExampleMinerTest(tf.test.TestCase):
+class HardExampleMinerTest(test_case.TestCase):
 
   def testHardMiningWithSingleLossType(self):
-    location_losses = tf.constant([[100, 90, 80, 0],
-                                   [0, 1, 2, 3]], tf.float32)
-    cls_losses = tf.constant([[0, 10, 50, 110],
-                              [9, 6, 3, 0]], tf.float32)
-    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9]], tf.float32)
-    decoded_boxlist_list = []
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    # Uses only location loss to select hard examples
-    loss_op = losses.HardExampleMiner(num_hard_examples=1,
-                                      iou_threshold=0.0,
-                                      loss_type='loc',
-                                      cls_loss_weight=1,
-                                      loc_loss_weight=1)
-    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
-                                   decoded_boxlist_list)
+    def graph_fn():
+      location_losses = tf.constant([[100, 90, 80, 0],
+                                     [0, 1, 2, 3]], tf.float32)
+      cls_losses = tf.constant([[0, 10, 50, 110],
+                                [9, 6, 3, 0]], tf.float32)
+      box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9]], tf.float32)
+      decoded_boxlist_list = []
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      # Uses only location loss to select hard examples
+      loss_op = losses.HardExampleMiner(num_hard_examples=1,
+                                        iou_threshold=0.0,
+                                        loss_type='loc',
+                                        cls_loss_weight=1,
+                                        loc_loss_weight=1)
+      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
+                                     decoded_boxlist_list)
+      return loc_loss, cls_loss
+    loc_loss_output, cls_loss_output = self.execute(graph_fn, [])
     exp_loc_loss = 100 + 3
     exp_cls_loss = 0 + 0
-    with self.test_session() as sess:
-      loc_loss_output = sess.run(loc_loss)
-      self.assertAllClose(loc_loss_output, exp_loc_loss)
-      cls_loss_output = sess.run(cls_loss)
-      self.assertAllClose(cls_loss_output, exp_cls_loss)
+    self.assertAllClose(loc_loss_output, exp_loc_loss)
+    self.assertAllClose(cls_loss_output, exp_cls_loss)
 
   def testHardMiningWithBothLossType(self):
-    location_losses = tf.constant([[100, 90, 80, 0],
-                                   [0, 1, 2, 3]], tf.float32)
-    cls_losses = tf.constant([[0, 10, 50, 110],
-                              [9, 6, 3, 0]], tf.float32)
-    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9]], tf.float32)
-    decoded_boxlist_list = []
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    loss_op = losses.HardExampleMiner(num_hard_examples=1,
-                                      iou_threshold=0.0,
-                                      loss_type='both',
-                                      cls_loss_weight=1,
-                                      loc_loss_weight=1)
-    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
-                                   decoded_boxlist_list)
+    def graph_fn():
+      location_losses = tf.constant([[100, 90, 80, 0],
+                                     [0, 1, 2, 3]], tf.float32)
+      cls_losses = tf.constant([[0, 10, 50, 110],
+                                [9, 6, 3, 0]], tf.float32)
+      box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9]], tf.float32)
+      decoded_boxlist_list = []
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      loss_op = losses.HardExampleMiner(num_hard_examples=1,
+                                        iou_threshold=0.0,
+                                        loss_type='both',
+                                        cls_loss_weight=1,
+                                        loc_loss_weight=1)
+      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
+                                     decoded_boxlist_list)
+      return loc_loss, cls_loss
+    loc_loss_output, cls_loss_output = self.execute(graph_fn, [])
     exp_loc_loss = 80 + 0
     exp_cls_loss = 50 + 9
-    with self.test_session() as sess:
-      loc_loss_output = sess.run(loc_loss)
-      self.assertAllClose(loc_loss_output, exp_loc_loss)
-      cls_loss_output = sess.run(cls_loss)
-      self.assertAllClose(cls_loss_output, exp_cls_loss)
+    self.assertAllClose(loc_loss_output, exp_loc_loss)
+    self.assertAllClose(cls_loss_output, exp_cls_loss)
 
   def testHardMiningNMS(self):
-    location_losses = tf.constant([[100, 90, 80, 0],
-                                   [0, 1, 2, 3]], tf.float32)
-    cls_losses = tf.constant([[0, 10, 50, 110],
-                              [9, 6, 3, 0]], tf.float32)
-    box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
-                               [0.9, 0.9, 0.99, 0.99],
-                               [0.1, 0.1, 0.9, 0.9],
-                               [0.1, 0.1, 0.9, 0.9]], tf.float32)
-    decoded_boxlist_list = []
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
-    loss_op = losses.HardExampleMiner(num_hard_examples=2,
-                                      iou_threshold=0.5,
-                                      loss_type='cls',
-                                      cls_loss_weight=1,
-                                      loc_loss_weight=1)
-    (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
-                                   decoded_boxlist_list)
+    def graph_fn():
+      location_losses = tf.constant([[100, 90, 80, 0],
+                                     [0, 1, 2, 3]], tf.float32)
+      cls_losses = tf.constant([[0, 10, 50, 110],
+                                [9, 6, 3, 0]], tf.float32)
+      box_corners = tf.constant([[0.1, 0.1, 0.9, 0.9],
+                                 [0.9, 0.9, 0.99, 0.99],
+                                 [0.1, 0.1, 0.9, 0.9],
+                                 [0.1, 0.1, 0.9, 0.9]], tf.float32)
+      decoded_boxlist_list = []
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      decoded_boxlist_list.append(box_list.BoxList(box_corners))
+      loss_op = losses.HardExampleMiner(num_hard_examples=2,
+                                        iou_threshold=0.5,
+                                        loss_type='cls',
+                                        cls_loss_weight=1,
+                                        loc_loss_weight=1)
+      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
+                                     decoded_boxlist_list)
+      return loc_loss, cls_loss
+    loc_loss_output, cls_loss_output = self.execute(graph_fn, [])
     exp_loc_loss = 0 + 90 + 0 + 1
     exp_cls_loss = 110 + 10 + 9 + 6
-    with self.test_session() as sess:
-      loc_loss_output = sess.run(loc_loss)
-      self.assertAllClose(loc_loss_output, exp_loc_loss)
-      cls_loss_output = sess.run(cls_loss)
-      self.assertAllClose(cls_loss_output, exp_cls_loss)
+
+    self.assertAllClose(loc_loss_output, exp_loc_loss)
+    self.assertAllClose(cls_loss_output, exp_cls_loss)
 
   def testEnforceNegativesPerPositiveRatio(self):
-    location_losses = tf.constant([[100, 90, 80, 0, 1, 2,
-                                    3, 10, 20, 100, 20, 3]], tf.float32)
-    cls_losses = tf.constant([[0, 0, 100, 0, 90, 70,
-                               0, 60, 0, 17, 13, 0]], tf.float32)
-    box_corners = tf.constant([[0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.5, 0.1],
-                               [0.0, 0.0, 0.6, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.8, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 1.0, 0.1],
-                               [0.0, 0.0, 1.1, 0.1],
-                               [0.0, 0.0, 0.2, 0.1]], tf.float32)
-    match_results = tf.constant([2, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, 3])
-    match_list = [matcher.Match(match_results)]
-    decoded_boxlist_list = []
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
+    location_losses = np.array([[100, 90, 80, 0, 1, 2,
+                                 3, 10, 20, 100, 20, 3]], np.float32)
+    cls_losses = np.array([[0, 0, 100, 0, 90, 70,
+                            0, 60, 0, 17, 13, 0]], np.float32)
+    box_corners = np.array([[0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.5, 0.1],
+                            [0.0, 0.0, 0.6, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.8, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 1.0, 0.1],
+                            [0.0, 0.0, 1.1, 0.1],
+                            [0.0, 0.0, 0.2, 0.1]], np.float32)
+    match_results = np.array([2, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, 3],
+                             np.int32)
 
     max_negatives_per_positive_list = [0.0, 0.5, 1.0, 1.5, 10]
     exp_loc_loss_list = [80 + 2,
@@ -1209,43 +1231,43 @@ class HardExampleMinerTest(tf.test.TestCase):
                          100 + 90 + 70 + 60 + 17,
                          100 + 90 + 70 + 60 + 17 + 13]
 
+    # pylint: disable=cell-var-from-loop
     for max_negatives_per_positive, exp_loc_loss, exp_cls_loss in zip(
         max_negatives_per_positive_list, exp_loc_loss_list, exp_cls_loss_list):
-      loss_op = losses.HardExampleMiner(
-          num_hard_examples=None, iou_threshold=0.9999, loss_type='cls',
-          cls_loss_weight=1, loc_loss_weight=1,
-          max_negatives_per_positive=max_negatives_per_positive)
-      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
-                                     decoded_boxlist_list, match_list)
-      loss_op.summarize()
-
-      with self.test_session() as sess:
-        loc_loss_output = sess.run(loc_loss)
-        self.assertAllClose(loc_loss_output, exp_loc_loss)
-        cls_loss_output = sess.run(cls_loss)
-        self.assertAllClose(cls_loss_output, exp_cls_loss)
+      def graph_fn():
+        loss_op = losses.HardExampleMiner(
+            num_hard_examples=None, iou_threshold=0.9999, loss_type='cls',
+            cls_loss_weight=1, loc_loss_weight=1,
+            max_negatives_per_positive=max_negatives_per_positive)
+        match_list = [matcher.Match(tf.constant(match_results))]
+        decoded_boxlist_list = [box_list.BoxList(tf.constant(box_corners))]
+        (loc_loss, cls_loss) = loss_op(tf.constant(location_losses),
+                                       tf.constant(cls_losses),
+                                       decoded_boxlist_list, match_list)
+        return loc_loss, cls_loss
+      loc_loss_output, cls_loss_output = self.execute_cpu(graph_fn, [])
+      self.assertAllClose(loc_loss_output, exp_loc_loss)
+      self.assertAllClose(cls_loss_output, exp_cls_loss)
+    # pylint: enable=cell-var-from-loop
 
   def testEnforceNegativesPerPositiveRatioWithMinNegativesPerImage(self):
-    location_losses = tf.constant([[100, 90, 80, 0, 1, 2,
-                                    3, 10, 20, 100, 20, 3]], tf.float32)
-    cls_losses = tf.constant([[0, 0, 100, 0, 90, 70,
-                               0, 60, 0, 17, 13, 0]], tf.float32)
-    box_corners = tf.constant([[0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.5, 0.1],
-                               [0.0, 0.0, 0.6, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 0.8, 0.1],
-                               [0.0, 0.0, 0.2, 0.1],
-                               [0.0, 0.0, 1.0, 0.1],
-                               [0.0, 0.0, 1.1, 0.1],
-                               [0.0, 0.0, 0.2, 0.1]], tf.float32)
-    match_results = tf.constant([-1] * 12)
-    match_list = [matcher.Match(match_results)]
-    decoded_boxlist_list = []
-    decoded_boxlist_list.append(box_list.BoxList(box_corners))
+    location_losses = np.array([[100, 90, 80, 0, 1, 2,
+                                 3, 10, 20, 100, 20, 3]], np.float32)
+    cls_losses = np.array([[0, 0, 100, 0, 90, 70,
+                            0, 60, 0, 17, 13, 0]], np.float32)
+    box_corners = np.array([[0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.5, 0.1],
+                            [0.0, 0.0, 0.6, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 0.8, 0.1],
+                            [0.0, 0.0, 0.2, 0.1],
+                            [0.0, 0.0, 1.0, 0.1],
+                            [0.0, 0.0, 1.1, 0.1],
+                            [0.0, 0.0, 0.2, 0.1]], np.float32)
+    match_results = np.array([-1] * 12, np.int32)
 
     min_negatives_per_image_list = [0, 1, 2, 4, 5, 6]
     exp_loc_loss_list = [0,
@@ -1261,20 +1283,127 @@ class HardExampleMinerTest(tf.test.TestCase):
                          100 + 90 + 70 + 60 + 17,
                          100 + 90 + 70 + 60 + 17 + 13]
 
+    # pylint: disable=cell-var-from-loop
     for min_negatives_per_image, exp_loc_loss, exp_cls_loss in zip(
         min_negatives_per_image_list, exp_loc_loss_list, exp_cls_loss_list):
-      loss_op = losses.HardExampleMiner(
-          num_hard_examples=None, iou_threshold=0.9999, loss_type='cls',
-          cls_loss_weight=1, loc_loss_weight=1,
-          max_negatives_per_positive=3,
-          min_negatives_per_image=min_negatives_per_image)
-      (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
-                                     decoded_boxlist_list, match_list)
-      with self.test_session() as sess:
-        loc_loss_output = sess.run(loc_loss)
-        self.assertAllClose(loc_loss_output, exp_loc_loss)
-        cls_loss_output = sess.run(cls_loss)
-        self.assertAllClose(cls_loss_output, exp_cls_loss)
+      def graph_fn():
+        loss_op = losses.HardExampleMiner(
+            num_hard_examples=None, iou_threshold=0.9999, loss_type='cls',
+            cls_loss_weight=1, loc_loss_weight=1,
+            max_negatives_per_positive=3,
+            min_negatives_per_image=min_negatives_per_image)
+        match_list = [matcher.Match(tf.constant(match_results))]
+        decoded_boxlist_list = [box_list.BoxList(tf.constant(box_corners))]
+        (loc_loss, cls_loss) = loss_op(location_losses, cls_losses,
+                                       decoded_boxlist_list, match_list)
+        return loc_loss, cls_loss
+      loc_loss_output, cls_loss_output = self.execute_cpu(graph_fn, [])
+      self.assertAllClose(loc_loss_output, exp_loc_loss)
+      self.assertAllClose(cls_loss_output, exp_cls_loss)
+    # pylint: enable=cell-var-from-loop
+
+
+LOG_2 = np.log(2)
+LOG_3 = np.log(3)
+
+
+class PenaltyReducedLogisticFocalLossTest(test_case.TestCase):
+  """Testing loss function from Equation (1) in [1].
+
+  [1]: https://arxiv.org/abs/1904.07850
+  """
+
+  def setUp(self):
+    super(PenaltyReducedLogisticFocalLossTest, self).setUp()
+    self._prediction = np.array([
+        # First batch
+        [[1 / 2, 1 / 4, 3 / 4],
+         [3 / 4, 1 / 3, 1 / 3]],
+        # Second Batch
+        [[0.0, 1.0, 1 / 2],
+         [3 / 4, 2 / 3, 1 / 3]]], np.float32)
+    self._prediction = np.log(self._prediction/(1 - self._prediction))
+
+    self._target = np.array([
+        # First batch
+        [[1.0, 0.91, 1.0],
+         [0.36, 0.84, 1.0]],
+        # Second Batch
+        [[0.01, 1.0, 0.75],
+         [0.96, 1.0, 1.0]]], np.float32)
+
+  def test_returns_correct_loss(self):
+    def graph_fn(prediction, target):
+      weights = tf.constant([
+          [[1.0], [1.0]],
+          [[1.0], [1.0]],
+      ])
+      loss = losses.PenaltyReducedLogisticFocalLoss(alpha=2.0, beta=0.5)
+      computed_value = loss._compute_loss(prediction, target,
+                                          weights)
+      return computed_value
+    computed_value = self.execute(graph_fn, [self._prediction, self._target])
+    expected_value = np.array([
+        # First batch
+        [[1 / 4 * LOG_2,
+          0.3 * 0.0625 * (2 * LOG_2 - LOG_3),
+          1 / 16 * (2 * LOG_2 - LOG_3)],
+         [0.8 * 9 / 16 * 2 * LOG_2,
+          0.4 * 1 / 9 * (LOG_3 - LOG_2),
+          4 / 9 * LOG_3]],
+        # Second Batch
+        [[0.0,
+          0.0,
+          1 / 2 * 1 / 4 * LOG_2],
+         [0.2 * 9 / 16 * 2 * LOG_2,
+          1 / 9 * (LOG_3 - LOG_2),
+          4 / 9 * LOG_3]]])
+    self.assertAllClose(computed_value, expected_value, rtol=1e-3, atol=1e-3)
+
+  def test_returns_correct_loss_weighted(self):
+    def graph_fn(prediction, target):
+      weights = tf.constant([
+          [[1.0, 0.0, 1.0], [0.0, 0.0, 1.0]],
+          [[1.0, 1.0, 1.0], [0.0, 0.0, 0.0]],
+      ])
+
+      loss = losses.PenaltyReducedLogisticFocalLoss(alpha=2.0, beta=0.5)
+
+      computed_value = loss._compute_loss(prediction, target,
+                                          weights)
+      return computed_value
+    computed_value = self.execute(graph_fn, [self._prediction, self._target])
+    expected_value = np.array([
+        # First batch
+        [[1 / 4 * LOG_2,
+          0.0,
+          1 / 16 * (2 * LOG_2 - LOG_3)],
+         [0.0,
+          0.0,
+          4 / 9 * LOG_3]],
+        # Second Batch
+        [[0.0,
+          0.0,
+          1 / 2 * 1 / 4 * LOG_2],
+         [0.0,
+          0.0,
+          0.0]]])
+
+    self.assertAllClose(computed_value, expected_value, rtol=1e-3, atol=1e-3)
+
+
+class L1LocalizationLossTest(test_case.TestCase):
+
+  def test_returns_correct_loss(self):
+    def graph_fn():
+      loss = losses.L1LocalizationLoss()
+      pred = [[0.1, 0.2], [0.7, 0.5]]
+      target = [[0.9, 1.0], [0.1, 0.4]]
+
+      weights = [[1.0, 0.0], [1.0, 1.0]]
+      return loss._compute_loss(pred, target, weights)
+    computed_value = self.execute(graph_fn, [])
+    self.assertAllClose(computed_value, [[0.8, 0.0], [0.6, 0.1]], rtol=1e-6)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index 1ed32167..0b1d53cd 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -18,179 +18,173 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.core import matcher
+from object_detection.utils import test_case
 
 
-class MatchTest(tf.test.TestCase):
+class MatchTest(test_case.TestCase):
 
   def test_get_correct_matched_columnIndices(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      matched_column_indices = match.matched_column_indices()
+      return matched_column_indices
     expected_column_indices = [0, 1, 3, 5]
-    matched_column_indices = match.matched_column_indices()
-    self.assertEqual(matched_column_indices.dtype, tf.int32)
-    with self.test_session() as sess:
-      matched_column_indices = sess.run(matched_column_indices)
-      self.assertAllEqual(matched_column_indices, expected_column_indices)
+    matched_column_indices = self.execute(graph_fn, [])
+    self.assertAllEqual(matched_column_indices, expected_column_indices)
 
   def test_get_correct_counts(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 1, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 1, -2])
+      match = matcher.Match(match_results)
+      num_matched_columns = match.num_matched_columns()
+      num_unmatched_columns = match.num_unmatched_columns()
+      num_ignored_columns = match.num_ignored_columns()
+      num_matched_rows = match.num_matched_rows()
+      return [num_matched_columns, num_unmatched_columns, num_ignored_columns,
+              num_matched_rows]
+    (num_matched_columns_out, num_unmatched_columns_out,
+     num_ignored_columns_out,
+     num_matched_rows_out) = self.execute_cpu(graph_fn, [])
     exp_num_matched_columns = 4
     exp_num_unmatched_columns = 2
     exp_num_ignored_columns = 1
     exp_num_matched_rows = 3
-    num_matched_columns = match.num_matched_columns()
-    num_unmatched_columns = match.num_unmatched_columns()
-    num_ignored_columns = match.num_ignored_columns()
-    num_matched_rows = match.num_matched_rows()
-    self.assertEqual(num_matched_columns.dtype, tf.int32)
-    self.assertEqual(num_unmatched_columns.dtype, tf.int32)
-    self.assertEqual(num_ignored_columns.dtype, tf.int32)
-    self.assertEqual(num_matched_rows.dtype, tf.int32)
-    with self.test_session() as sess:
-      (num_matched_columns_out, num_unmatched_columns_out,
-       num_ignored_columns_out, num_matched_rows_out) = sess.run(
-           [num_matched_columns, num_unmatched_columns, num_ignored_columns,
-            num_matched_rows])
-      self.assertAllEqual(num_matched_columns_out, exp_num_matched_columns)
-      self.assertAllEqual(num_unmatched_columns_out, exp_num_unmatched_columns)
-      self.assertAllEqual(num_ignored_columns_out, exp_num_ignored_columns)
-      self.assertAllEqual(num_matched_rows_out, exp_num_matched_rows)
+    self.assertAllEqual(num_matched_columns_out, exp_num_matched_columns)
+    self.assertAllEqual(num_unmatched_columns_out, exp_num_unmatched_columns)
+    self.assertAllEqual(num_ignored_columns_out, exp_num_ignored_columns)
+    self.assertAllEqual(num_matched_rows_out, exp_num_matched_rows)
 
   def testGetCorrectUnmatchedColumnIndices(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      unmatched_column_indices = match.unmatched_column_indices()
+      return unmatched_column_indices
+    unmatched_column_indices = self.execute(graph_fn, [])
     expected_column_indices = [2, 4]
-    unmatched_column_indices = match.unmatched_column_indices()
-    self.assertEqual(unmatched_column_indices.dtype, tf.int32)
-    with self.test_session() as sess:
-      unmatched_column_indices = sess.run(unmatched_column_indices)
-      self.assertAllEqual(unmatched_column_indices, expected_column_indices)
+    self.assertAllEqual(unmatched_column_indices, expected_column_indices)
 
   def testGetCorrectMatchedRowIndices(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      matched_row_indices = match.matched_row_indices()
+      return matched_row_indices
+    matched_row_indices = self.execute(graph_fn, [])
     expected_row_indices = [3, 1, 0, 5]
-    matched_row_indices = match.matched_row_indices()
-    self.assertEqual(matched_row_indices.dtype, tf.int32)
-    with self.test_session() as sess:
-      matched_row_inds = sess.run(matched_row_indices)
-      self.assertAllEqual(matched_row_inds, expected_row_indices)
+    self.assertAllEqual(matched_row_indices, expected_row_indices)
 
   def test_get_correct_ignored_column_indices(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      ignored_column_indices = match.ignored_column_indices()
+      return ignored_column_indices
+    ignored_column_indices = self.execute(graph_fn, [])
     expected_column_indices = [6]
-    ignored_column_indices = match.ignored_column_indices()
-    self.assertEqual(ignored_column_indices.dtype, tf.int32)
-    with self.test_session() as sess:
-      ignored_column_indices = sess.run(ignored_column_indices)
-      self.assertAllEqual(ignored_column_indices, expected_column_indices)
+    self.assertAllEqual(ignored_column_indices, expected_column_indices)
 
   def test_get_correct_matched_column_indicator(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      matched_column_indicator = match.matched_column_indicator()
+      return matched_column_indicator
     expected_column_indicator = [True, True, False, True, False, True, False]
-    matched_column_indicator = match.matched_column_indicator()
-    self.assertEqual(matched_column_indicator.dtype, tf.bool)
-    with self.test_session() as sess:
-      matched_column_indicator = sess.run(matched_column_indicator)
-      self.assertAllEqual(matched_column_indicator, expected_column_indicator)
+    matched_column_indicator = self.execute(graph_fn, [])
+    self.assertAllEqual(matched_column_indicator, expected_column_indicator)
 
   def test_get_correct_unmatched_column_indicator(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      unmatched_column_indicator = match.unmatched_column_indicator()
+      return unmatched_column_indicator
     expected_column_indicator = [False, False, True, False, True, False, False]
-    unmatched_column_indicator = match.unmatched_column_indicator()
-    self.assertEqual(unmatched_column_indicator.dtype, tf.bool)
-    with self.test_session() as sess:
-      unmatched_column_indicator = sess.run(unmatched_column_indicator)
-      self.assertAllEqual(unmatched_column_indicator, expected_column_indicator)
+    unmatched_column_indicator = self.execute(graph_fn, [])
+    self.assertAllEqual(unmatched_column_indicator, expected_column_indicator)
 
   def test_get_correct_ignored_column_indicator(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      ignored_column_indicator = match.ignored_column_indicator()
+      return ignored_column_indicator
     expected_column_indicator = [False, False, False, False, False, False, True]
-    ignored_column_indicator = match.ignored_column_indicator()
-    self.assertEqual(ignored_column_indicator.dtype, tf.bool)
-    with self.test_session() as sess:
-      ignored_column_indicator = sess.run(ignored_column_indicator)
-      self.assertAllEqual(ignored_column_indicator, expected_column_indicator)
+    ignored_column_indicator = self.execute(graph_fn, [])
+    self.assertAllEqual(ignored_column_indicator, expected_column_indicator)
 
   def test_get_correct_unmatched_ignored_column_indices(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    match = matcher.Match(match_results)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      match = matcher.Match(match_results)
+      unmatched_ignored_column_indices = (match.
+                                          unmatched_or_ignored_column_indices())
+      return unmatched_ignored_column_indices
     expected_column_indices = [2, 4, 6]
-    unmatched_ignored_column_indices = (match.
-                                        unmatched_or_ignored_column_indices())
-    self.assertEqual(unmatched_ignored_column_indices.dtype, tf.int32)
-    with self.test_session() as sess:
-      unmatched_ignored_column_indices = sess.run(
-          unmatched_ignored_column_indices)
-      self.assertAllEqual(unmatched_ignored_column_indices,
-                          expected_column_indices)
+    unmatched_ignored_column_indices = self.execute(graph_fn, [])
+    self.assertAllEqual(unmatched_ignored_column_indices,
+                        expected_column_indices)
 
   def test_all_columns_accounted_for(self):
     # Note: deliberately setting to small number so not always
     # all possibilities appear (matched, unmatched, ignored)
+    def graph_fn():
+      match_results = tf.random_uniform(
+          [num_matches], minval=-2, maxval=5, dtype=tf.int32)
+      match = matcher.Match(match_results)
+      matched_column_indices = match.matched_column_indices()
+      unmatched_column_indices = match.unmatched_column_indices()
+      ignored_column_indices = match.ignored_column_indices()
+      return (matched_column_indices, unmatched_column_indices,
+              ignored_column_indices)
     num_matches = 10
-    match_results = tf.random_uniform(
-        [num_matches], minval=-2, maxval=5, dtype=tf.int32)
-    match = matcher.Match(match_results)
-    matched_column_indices = match.matched_column_indices()
-    unmatched_column_indices = match.unmatched_column_indices()
-    ignored_column_indices = match.ignored_column_indices()
-    with self.test_session() as sess:
-      matched, unmatched, ignored = sess.run([
-          matched_column_indices, unmatched_column_indices,
-          ignored_column_indices
-      ])
-      all_indices = np.hstack((matched, unmatched, ignored))
-      all_indices_sorted = np.sort(all_indices)
-      self.assertAllEqual(all_indices_sorted,
-                          np.arange(num_matches, dtype=np.int32))
+    matched, unmatched, ignored = self.execute(graph_fn, [])
+    all_indices = np.hstack((matched, unmatched, ignored))
+    all_indices_sorted = np.sort(all_indices)
+    self.assertAllEqual(all_indices_sorted,
+                        np.arange(num_matches, dtype=np.int32))
 
   def test_scalar_gather_based_on_match(self):
-    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
-    input_tensor = tf.constant([0, 1, 2, 3, 4, 5, 6, 7], dtype=tf.float32)
+    def graph_fn():
+      match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+      input_tensor = tf.constant([0, 1, 2, 3, 4, 5, 6, 7], dtype=tf.float32)
+      match = matcher.Match(match_results)
+      gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                    unmatched_value=100.,
+                                                    ignored_value=200.)
+      return gathered_tensor
     expected_gathered_tensor = [3, 1, 100, 0, 100, 5, 200]
-    match = matcher.Match(match_results)
-    gathered_tensor = match.gather_based_on_match(input_tensor,
-                                                  unmatched_value=100.,
-                                                  ignored_value=200.)
-    self.assertEqual(gathered_tensor.dtype, tf.float32)
-    with self.test_session():
-      gathered_tensor_out = gathered_tensor.eval()
+    gathered_tensor_out = self.execute(graph_fn, [])
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
 
   def test_multidimensional_gather_based_on_match(self):
-    match_results = tf.constant([1, -1, -2])
-    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
-                               dtype=tf.float32)
+    def graph_fn():
+      match_results = tf.constant([1, -1, -2])
+      input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
+                                 dtype=tf.float32)
+      match = matcher.Match(match_results)
+      gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                    unmatched_value=tf.zeros(4),
+                                                    ignored_value=tf.zeros(4))
+      return gathered_tensor
     expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]
-    match = matcher.Match(match_results)
-    gathered_tensor = match.gather_based_on_match(input_tensor,
-                                                  unmatched_value=tf.zeros(4),
-                                                  ignored_value=tf.zeros(4))
-    self.assertEqual(gathered_tensor.dtype, tf.float32)
-    with self.test_session():
-      gathered_tensor_out = gathered_tensor.eval()
+    gathered_tensor_out = self.execute(graph_fn, [])
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
 
   def test_multidimensional_gather_based_on_match_with_matmul_gather_op(self):
-    match_results = tf.constant([1, -1, -2])
-    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
-                               dtype=tf.float32)
+    def graph_fn():
+      match_results = tf.constant([1, -1, -2])
+      input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
+                                 dtype=tf.float32)
+      match = matcher.Match(match_results, use_matmul_gather=True)
+      gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                    unmatched_value=tf.zeros(4),
+                                                    ignored_value=tf.zeros(4))
+      return gathered_tensor
     expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]
-    match = matcher.Match(match_results, use_matmul_gather=True)
-    gathered_tensor = match.gather_based_on_match(input_tensor,
-                                                  unmatched_value=tf.zeros(4),
-                                                  ignored_value=tf.zeros(4))
-    self.assertEqual(gathered_tensor.dtype, tf.float32)
-    with self.test_session() as sess:
-      self.assertTrue(
-          all([op.name is not 'Gather' for op in sess.graph.get_operations()]))
-      gathered_tensor_out = gathered_tensor.eval()
+    gathered_tensor_out = self.execute(graph_fn, [])
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/minibatch_sampler_test.py b/research/object_detection/core/minibatch_sampler_test.py
index 7420ae5d..4f148c9a 100644
--- a/research/object_detection/core/minibatch_sampler_test.py
+++ b/research/object_detection/core/minibatch_sampler_test.py
@@ -19,63 +19,52 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.core import minibatch_sampler
+from object_detection.utils import test_case
 
 
-class MinibatchSamplerTest(tf.test.TestCase):
+class MinibatchSamplerTest(test_case.TestCase):
 
   def test_subsample_indicator_when_more_true_elements_than_num_samples(self):
-    np_indicator = [True, False, True, False, True, True, False]
-    indicator = tf.constant(np_indicator)
-    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
-        indicator, 3)
-    with self.test_session() as sess:
-      samples_out = sess.run(samples)
-      self.assertTrue(np.sum(samples_out), 3)
-      self.assertAllEqual(samples_out,
-                          np.logical_and(samples_out, np_indicator))
-
-  def test_subsample_when_more_true_elements_than_num_samples_no_shape(self):
-    np_indicator = [True, False, True, False, True, True, False]
-    indicator = tf.placeholder(tf.bool)
-    feed_dict = {indicator: np_indicator}
-
-    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
-        indicator, 3)
-    with self.test_session() as sess:
-      samples_out = sess.run(samples, feed_dict=feed_dict)
-      self.assertTrue(np.sum(samples_out), 3)
-      self.assertAllEqual(samples_out,
-                          np.logical_and(samples_out, np_indicator))
+    np_indicator = np.array([True, False, True, False, True, True, False])
+    def graph_fn(indicator):
+      samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
+          indicator, 3)
+      return samples
+    samples_out = self.execute(graph_fn, [np_indicator])
+    self.assertTrue(np.sum(samples_out), 3)
+    self.assertAllEqual(samples_out,
+                        np.logical_and(samples_out, np_indicator))
 
   def test_subsample_indicator_when_less_true_elements_than_num_samples(self):
-    np_indicator = [True, False, True, False, True, True, False]
-    indicator = tf.constant(np_indicator)
-    samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
-        indicator, 5)
-    with self.test_session() as sess:
-      samples_out = sess.run(samples)
-      self.assertTrue(np.sum(samples_out), 4)
-      self.assertAllEqual(samples_out,
-                          np.logical_and(samples_out, np_indicator))
+    np_indicator = np.array([True, False, True, False, True, True, False])
+    def graph_fn(indicator):
+      samples = minibatch_sampler.MinibatchSampler.subsample_indicator(
+          indicator, 5)
+      return samples
+    samples_out = self.execute(graph_fn, [np_indicator])
+    self.assertTrue(np.sum(samples_out), 4)
+    self.assertAllEqual(samples_out,
+                        np.logical_and(samples_out, np_indicator))
 
   def test_subsample_indicator_when_num_samples_is_zero(self):
-    np_indicator = [True, False, True, False, True, True, False]
-    indicator = tf.constant(np_indicator)
-    samples_none = minibatch_sampler.MinibatchSampler.subsample_indicator(
-        indicator, 0)
-    with self.test_session() as sess:
-      samples_none_out = sess.run(samples_none)
-      self.assertAllEqual(
-          np.zeros_like(samples_none_out, dtype=bool),
-          samples_none_out)
+    np_indicator = np.array([True, False, True, False, True, True, False])
+    def graph_fn(indicator):
+      samples_none = minibatch_sampler.MinibatchSampler.subsample_indicator(
+          indicator, 0)
+      return samples_none
+    samples_out = self.execute(graph_fn, [np_indicator])
+    self.assertAllEqual(
+        np.zeros_like(samples_out, dtype=bool),
+        samples_out)
 
   def test_subsample_indicator_when_indicator_all_false(self):
-    indicator_empty = tf.zeros([0], dtype=tf.bool)
-    samples_empty = minibatch_sampler.MinibatchSampler.subsample_indicator(
-        indicator_empty, 4)
-    with self.test_session() as sess:
-      samples_empty_out = sess.run(samples_empty)
-      self.assertEqual(0, samples_empty_out.size)
+    indicator_empty = np.zeros([0], dtype=np.bool)
+    def graph_fn(indicator):
+      samples_empty = minibatch_sampler.MinibatchSampler.subsample_indicator(
+          indicator, 4)
+      return samples_empty
+    samples_out = self.execute(graph_fn, [indicator_empty])
+    self.assertEqual(0, samples_out.size)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index b04d6250..1f768f1d 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -99,7 +99,8 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
 
     Args:
       field: a string key, options are
-        fields.BoxListFields.{boxes,classes,masks,keypoints} or
+        fields.BoxListFields.{boxes,classes,masks,keypoints,
+        keypoint_visibilities} or
         fields.InputDataFields.is_annotated.
 
     Returns:
@@ -119,7 +120,8 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
 
     Args:
       field: a string key, options are
-        fields.BoxListFields.{boxes,classes,masks,keypoints} or
+        fields.BoxListFields.{boxes,classes,masks,keypoints,
+        keypoint_visibilities} or
         fields.InputDataFields.is_annotated.
 
     Returns:
@@ -127,6 +129,25 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
     """
     return field in self._groundtruth_lists
 
+  @staticmethod
+  def get_side_inputs(features):
+    """Get side inputs from input features.
+
+    This placeholder method provides a way for a meta-architecture to specify
+    how to grab additional side inputs from input features (in addition to the
+    image itself) and allows models to depend on contextual information.  By
+    default, detection models do not use side information (and thus this method
+    returns an empty dictionary by default.  However it can be overridden if
+    side inputs are necessary."
+
+    Args:
+      features: A dictionary of tensors.
+
+    Returns:
+      An empty dictionary by default.
+    """
+    return {}
+
   @abc.abstractmethod
   def preprocess(self, inputs):
     """Input preprocessing.
@@ -171,7 +192,7 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
     pass
 
   @abc.abstractmethod
-  def predict(self, preprocessed_inputs, true_image_shapes):
+  def predict(self, preprocessed_inputs, true_image_shapes, **side_inputs):
     """Predict prediction tensors from inputs tensor.
 
     Outputs of this function can be passed to loss or postprocess functions.
@@ -183,6 +204,7 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
         of the form [height, width, channels] indicating the shapes
         of true images in the resized images, as resized images can be padded
         with zeros.
+      **side_inputs: additional tensors that are required by the network.
 
     Returns:
       prediction_dict: a dictionary holding prediction tensors to be
@@ -269,10 +291,13 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
                           groundtruth_classes_list,
                           groundtruth_masks_list=None,
                           groundtruth_keypoints_list=None,
+                          groundtruth_keypoint_visibilities_list=None,
                           groundtruth_weights_list=None,
                           groundtruth_confidences_list=None,
                           groundtruth_is_crowd_list=None,
-                          is_annotated_list=None):
+                          groundtruth_area_list=None,
+                          is_annotated_list=None,
+                          groundtruth_labeled_classes=None):
     """Provide groundtruth tensors.
 
     Args:
@@ -292,16 +317,25 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
       groundtruth_keypoints_list: a list of 3-D tf.float32 tensors of
         shape [num_boxes, num_keypoints, 2] containing keypoints.
         Keypoints are assumed to be provided in normalized coordinates and
-        missing keypoints should be encoded as NaN.
+        missing keypoints should be encoded as NaN (but it is recommended to use
+        `groundtruth_keypoint_visibilities_list`).
+      groundtruth_keypoint_visibilities_list: a list of 3-D tf.bool tensors
+        of shape [num_boxes, num_keypoints] containing keypoint visibilities.
       groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape
         [num_boxes] containing weights for groundtruth boxes.
       groundtruth_confidences_list: A list of 2-D tf.float32 tensors of shape
         [num_boxes, num_classes] containing class confidences for groundtruth
         boxes.
       groundtruth_is_crowd_list: A list of 1-D tf.bool tensors of shape
-        [num_boxes] containing is_crowd annotations
+        [num_boxes] containing is_crowd annotations.
+      groundtruth_area_list: A list of 1-D tf.float32 tensors of shape
+        [num_boxes] containing the area (in the original absolute coordinates)
+        of the annotations.
       is_annotated_list: A list of scalar tf.bool tensors indicating whether
         images have been labeled or not.
+      groundtruth_labeled_classes: A list of 1-D tf.float32 tensors of shape
+        [num_classes], containing label indices (1-indexed) of the classes that
+        are exhaustively annotated.
     """
     self._groundtruth_lists[fields.BoxListFields.boxes] = groundtruth_boxes_list
     self._groundtruth_lists[
@@ -318,12 +352,23 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
     if groundtruth_keypoints_list:
       self._groundtruth_lists[
           fields.BoxListFields.keypoints] = groundtruth_keypoints_list
+    if groundtruth_keypoint_visibilities_list:
+      self._groundtruth_lists[
+          fields.BoxListFields.keypoint_visibilities] = (
+              groundtruth_keypoint_visibilities_list)
     if groundtruth_is_crowd_list:
       self._groundtruth_lists[
           fields.BoxListFields.is_crowd] = groundtruth_is_crowd_list
+    if groundtruth_area_list:
+      self._groundtruth_lists[
+          fields.InputDataFields.groundtruth_area] = groundtruth_area_list
     if is_annotated_list:
       self._groundtruth_lists[
           fields.InputDataFields.is_annotated] = is_annotated_list
+    if groundtruth_labeled_classes:
+      self._groundtruth_lists[
+          fields.InputDataFields
+          .groundtruth_labeled_classes] = groundtruth_labeled_classes
 
   @abc.abstractmethod
   def regularization_losses(self):
diff --git a/research/object_detection/core/multiclass_nms_test.py b/research/object_detection/core/multiclass_nms_test.py
index 7cc01c65..d191e040 100644
--- a/research/object_detection/core/multiclass_nms_test.py
+++ b/research/object_detection/core/multiclass_nms_test.py
@@ -23,22 +23,23 @@ from object_detection.utils import test_case
 
 class MulticlassNonMaxSuppressionTest(test_case.TestCase):
 
-  def test_multiclass_nms_select_with_shared_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
+  def test_multiclass_nms_select_with_shared_boxes_cpu_only(self):
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
+
+    def graph_fn(boxes, scores):
+      score_thresh = 0.1
+      iou_thresh = .5
+      max_output_size = 4
+      nms, _ = post_processing.multiclass_non_max_suppression(
+          boxes, scores, score_thresh, iou_thresh, max_output_size)
+      return (nms.get(), nms.get_field(fields.BoxListFields.scores),
+              nms.get_field(fields.BoxListFields.classes))
 
     exp_nms_corners = [[0, 10, 1, 11],
                        [0, 0, 1, 1],
@@ -46,16 +47,11 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
                        [0, 100, 1, 101]]
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output) = self.execute_cpu(graph_fn, [boxes, scores])
+    self.assertAllClose(nms_corners_output, exp_nms_corners)
+    self.assertAllClose(nms_scores_output, exp_nms_scores)
+    self.assertAllClose(nms_classes_output, exp_nms_classes)
 
   def test_multiclass_nms_select_with_shared_boxes_pad_to_max_output_size(self):
     boxes = np.array([[[0, 0, 1, 1]],
@@ -106,83 +102,70 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
                         exp_nms_classes)
 
   def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
     num_keypoints = 6
-    keypoints = tf.tile(
-        tf.reshape(tf.range(8), [8, 1, 1]),
-        [1, num_keypoints, 2])
+    keypoints = np.tile(np.reshape(range(8), [8, 1, 1]),
+                        [1, num_keypoints, 2]).astype(np.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_output_size = 4
 
+    def graph_fn(boxes, scores, keypoints):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True,
+          additional_fields={fields.BoxListFields.keypoints: keypoints})
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms.get_field(fields.BoxListFields.keypoints), nms_valid
+      ]
+
     exp_nms_corners = [[0, 10, 1, 11],
                        [0, 0, 1, 1],
                        [0, 1000, 1, 1002],
                        [0, 100, 1, 101]]
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
-    exp_nms_keypoints_tensor = tf.tile(
-        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
+    exp_nms_keypoints = np.tile(
+        np.reshape(np.array([3, 0, 6, 5], np.float32), [4, 1, 1]),
         [1, num_keypoints, 2])
+    (nms_corners_output, nms_scores_output, nms_classes_output, nms_keypoints,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, keypoints])
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={fields.BoxListFields.keypoints: keypoints})
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_keypoints,
-       exp_nms_keypoints) = sess.run([
-           nms.get(),
-           nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes),
-           nms.get_field(fields.BoxListFields.keypoints),
-           exp_nms_keypoints_tensor
-       ])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_keypoints, exp_nms_keypoints)
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
+    self.assertAllEqual(nms_keypoints[:nms_valid], exp_nms_keypoints)
 
   def test_multiclass_nms_with_shared_boxes_given_keypoint_heatmaps(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-
-    num_boxes = tf.shape(boxes)[0]
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
+
+    num_boxes = boxes.shape[0]
     heatmap_height = 5
     heatmap_width = 5
     num_keypoints = 17
-    keypoint_heatmaps = tf.ones(
+    keypoint_heatmaps = np.ones(
         [num_boxes, heatmap_height, heatmap_width, num_keypoints],
-        dtype=tf.float32)
+        dtype=np.float32)
 
     score_thresh = 0.1
     iou_thresh = .5
@@ -197,55 +180,49 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_keypoint_heatmaps = np.ones(
         (4, heatmap_height, heatmap_width, num_keypoints), dtype=np.float32)
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={
-            fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps
-        })
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_keypoint_heatmaps) = sess.run(
-           [nms.get(),
-            nms.get_field(fields.BoxListFields.scores),
-            nms.get_field(fields.BoxListFields.classes),
-            nms.get_field(fields.BoxListFields.keypoint_heatmaps)])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_keypoint_heatmaps, exp_nms_keypoint_heatmaps)
+    def graph_fn(boxes, scores, keypoint_heatmaps):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True,
+          additional_fields={
+              fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps
+          })
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms.get_field(fields.BoxListFields.keypoint_heatmaps), nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output, nms_classes_output,
+     nms_keypoint_heatmaps,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, keypoint_heatmaps])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
+    self.assertAllEqual(nms_keypoint_heatmaps[:nms_valid],
+                        exp_nms_keypoint_heatmaps)
 
   def test_multiclass_nms_with_additional_fields(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
 
     coarse_boxes_key = 'coarse_boxes'
-    coarse_boxes = tf.constant([[0.1, 0.1, 1.1, 1.1],
-                                [0.1, 0.2, 1.1, 1.2],
-                                [0.1, -0.2, 1.1, 1.0],
-                                [0.1, 10.1, 1.1, 11.1],
-                                [0.1, 10.2, 1.1, 11.2],
-                                [0.1, 100.1, 1.1, 101.1],
-                                [0.1, 1000.1, 1.1, 1002.1],
-                                [0.1, 1000.1, 1.1, 1002.2]], tf.float32)
+    coarse_boxes = np.array(
+        [[0.1, 0.1, 1.1, 1.1], [0.1, 0.2, 1.1, 1.2], [0.1, -0.2, 1.1, 1.0],
+         [0.1, 10.1, 1.1, 11.1], [0.1, 10.2, 1.1, 11.2], [
+             0.1, 100.1, 1.1, 101.1
+         ], [0.1, 1000.1, 1.1, 1002.1], [0.1, 1000.1, 1.1, 1002.2]], np.float32)
 
     score_thresh = 0.1
     iou_thresh = .5
@@ -265,47 +242,44 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={coarse_boxes_key: coarse_boxes})
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_coarse_corners) = sess.run(
-           [nms.get(),
-            nms.get_field(fields.BoxListFields.scores),
-            nms.get_field(fields.BoxListFields.classes),
-            nms.get_field(coarse_boxes_key)])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_coarse_corners, exp_nms_coarse_corners)
+    def graph_fn(boxes, scores, coarse_boxes):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True,
+          additional_fields={coarse_boxes_key: coarse_boxes})
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms.get_field(coarse_boxes_key),
+          nms_valid,
+      ]
+
+    (nms_corners_output, nms_scores_output, nms_classes_output,
+     nms_coarse_corners,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, coarse_boxes])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
+    self.assertAllEqual(nms_coarse_corners[:nms_valid], exp_nms_coarse_corners)
 
   def test_multiclass_nms_select_with_shared_boxes_given_masks(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
     num_classes = 2
     mask_height = 3
     mask_width = 3
-    masks = tf.tile(
-        tf.reshape(tf.range(8), [8, 1, 1, 1]),
+    masks = np.tile(
+        np.reshape(range(8), [8, 1, 1, 1]),
         [1, num_classes, mask_height, mask_width])
     score_thresh = 0.1
     iou_thresh = .5
@@ -317,32 +291,37 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
                        [0, 100, 1, 101]]
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
-    exp_nms_masks_tensor = tf.tile(
-        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
+    exp_nms_masks_tensor = np.tile(
+        np.reshape(np.array([3, 0, 6, 5], np.float32), [4, 1, 1]),
         [1, mask_height, mask_width])
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size, masks=masks)
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_masks,
-       exp_nms_masks) = sess.run([nms.get(),
-                                  nms.get_field(fields.BoxListFields.scores),
-                                  nms.get_field(fields.BoxListFields.classes),
-                                  nms.get_field(fields.BoxListFields.masks),
-                                  exp_nms_masks_tensor])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_masks, exp_nms_masks)
+    def graph_fn(boxes, scores, masks):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          masks=masks,
+          pad_to_max_output_size=True)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms.get_field(fields.BoxListFields.masks), nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output, nms_classes_output, nms_masks,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, masks])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
+    self.assertAllEqual(nms_masks[:nms_valid], exp_nms_masks_tensor)
 
   def test_multiclass_nms_select_with_clip_window(self):
-    boxes = tf.constant([[[0, 0, 10, 10]],
-                         [[1, 1, 11, 11]]], tf.float32)
-    scores = tf.constant([[.9], [.75]])
-    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
+    boxes = np.array([[[0, 0, 10, 10]], [[1, 1, 11, 11]]], np.float32)
+    scores = np.array([[.9], [.75]], np.float32)
+    clip_window = np.array([5, 4, 8, 7], np.float32)
     score_thresh = 0.0
     iou_thresh = 0.5
     max_output_size = 100
@@ -351,26 +330,31 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.9]
     exp_nms_classes = [0]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        clip_window=clip_window)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    def graph_fn(boxes, scores, clip_window):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True,
+          clip_window=clip_window)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes), nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output, nms_classes_output,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, clip_window])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
 
   def test_multiclass_nms_select_with_clip_window_change_coordinate_frame(self):
-    boxes = tf.constant([[[0, 0, 10, 10]],
-                         [[1, 1, 11, 11]]], tf.float32)
-    scores = tf.constant([[.9], [.75]])
-    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
+    boxes = np.array([[[0, 0, 10, 10]], [[1, 1, 11, 11]]], np.float32)
+    scores = np.array([[.9], [.75]], np.float32)
+    clip_window = np.array([5, 4, 8, 7], np.float32)
     score_thresh = 0.0
     iou_thresh = 0.5
     max_output_size = 100
@@ -379,35 +363,36 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.9]
     exp_nms_classes = [0]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        clip_window=clip_window,
-        change_coordinate_frame=True)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    def graph_fn(boxes, scores, clip_window):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          clip_window=clip_window,
+          pad_to_max_output_size=True,
+          change_coordinate_frame=True)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes), nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output, nms_classes_output,
+     nms_valid) = self.execute(graph_fn, [boxes, scores, clip_window])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
 
   def test_multiclass_nms_select_with_per_class_cap(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_size_per_class = 2
@@ -418,29 +403,35 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.95, .9, .85]
     exp_nms_classes = [0, 0, 1]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_size_per_class)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    def graph_fn(boxes, scores):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_size_per_class,
+          pad_to_max_output_size=True)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output, nms_valid) = self.execute(graph_fn, [boxes, scores])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
 
   def test_multiclass_nms_select_with_total_cap(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_size_per_class = 4
@@ -451,27 +442,35 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.95, .9]
     exp_nms_classes = [0, 0]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_size_per_class,
-        max_total_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    def graph_fn(boxes, scores):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_size_per_class,
+          max_total_size,
+          pad_to_max_output_size=True)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output, nms_valid) = self.execute(graph_fn, [boxes, scores])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
 
   def test_multiclass_nms_threshold_then_select_with_shared_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]])
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]],
+                      np.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_output_size = 3
@@ -479,26 +478,32 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms = [[0, 10, 1, 11],
                [0, 0, 1, 1],
                [0, 100, 1, 101]]
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
+
+    def graph_fn(boxes, scores):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True)
+      return nms.get(), nms_valid
+
+    nms_output, nms_valid = self.execute(graph_fn, [boxes, scores])
+    self.assertAllClose(nms_output[:nms_valid], exp_nms)
 
   def test_multiclass_nms_select_with_separate_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1], [0, 0, 4, 5]],
-                         [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
-                         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11], [0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101], [0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
-                         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]],
-                        tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
+    boxes = np.array(
+        [[[0, 0, 1, 1], [0, 0, 4, 5]], [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
+         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]], [[0, 10, 1, 11], [
+             0, 10, 1, 11
+         ]], [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
+         [[0, 100, 1, 101], [0, 100, 1, 101]],
+         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
+         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
     score_thresh = 0.1
     iou_thresh = .5
     max_output_size = 4
@@ -510,16 +515,68 @@ class MulticlassNonMaxSuppressionTest(test_case.TestCase):
     exp_nms_scores = [.95, .9, .85, .3]
     exp_nms_classes = [0, 0, 1, 0]
 
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+    def graph_fn(boxes, scores):
+      nms, nms_valid = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_output_size,
+          pad_to_max_output_size=True)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes),
+          nms_valid
+      ]
+
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output, nms_valid) = self.execute(graph_fn, [boxes, scores])
+    self.assertAllClose(nms_corners_output[:nms_valid], exp_nms_corners)
+    self.assertAllClose(nms_scores_output[:nms_valid], exp_nms_scores)
+    self.assertAllClose(nms_classes_output[:nms_valid], exp_nms_classes)
+
+  def test_multiclass_soft_nms_select_with_shared_boxes_cpu_only(self):
+    boxes = np.array(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]],
+                      np.float32)
+    score_thresh = 0.1
+    iou_thresh = 1.0
+    max_output_size = 4
 
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 0.1, 1, 1.1]]
+    exp_nms_scores = [.95, .9, .85, .384]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    def graph_fn(boxes, scores):
+      nms, _ = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_size_per_class=max_output_size,
+          max_total_size=max_output_size,
+          soft_nms_sigma=0.5)
+      return [
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes)
+      ]
+
+    (nms_corners_output, nms_scores_output,
+     nms_classes_output) = self.execute_cpu(graph_fn, [boxes, scores])
+    self.assertAllClose(
+        nms_corners_output, exp_nms_corners, rtol=1e-2, atol=1e-2)
+    self.assertAllClose(nms_scores_output, exp_nms_scores, rtol=1e-2, atol=1e-2)
+    self.assertAllClose(
+        nms_classes_output, exp_nms_classes, rtol=1e-2, atol=1e-2)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 90f1e06d..5e594055 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -962,18 +962,18 @@ def batch_multiclass_non_max_suppression(boxes,
     if use_class_agnostic_nms:
       raise ValueError('class-agnostic NMS is not supported by combined_nms.')
     if clip_window is not None:
-      tf.compat.v1.logging.warning(
+      tf.logging.warning(
           'clip_window is not supported by combined_nms unless it is'
           ' [0. 0. 1. 1.] for each image.')
     if additional_fields is not None:
-      tf.compat.v1.logging.warning(
+      tf.logging.warning(
           'additional_fields is not supported by combined_nms.')
     if parallel_iterations != 32:
-      tf.compat.v1.logging.warning(
+      tf.logging.warning(
           'Number of batch items to be processed in parallel is'
           ' not configurable by combined_nms.')
     if max_classes_per_detection > 1:
-      tf.compat.v1.logging.warning(
+      tf.logging.warning(
           'max_classes_per_detection is not configurable by combined_nms.')
 
     with tf.name_scope(scope, 'CombinedNonMaxSuppression'):
@@ -1013,7 +1013,7 @@ def batch_multiclass_non_max_suppression(boxes,
   else:
     ordered_additional_fields = collections.OrderedDict(
         sorted(additional_fields.items(), key=lambda item: item[0]))
-  del additional_fields
+
   with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
     boxes_shape = boxes.shape
     batch_size = shape_utils.get_dim_as_int(boxes_shape[0])
diff --git a/research/object_detection/core/prefetcher.py b/research/object_detection/core/prefetcher.py
index 9bb7d658..52a9e3ea 100644
--- a/research/object_detection/core/prefetcher.py
+++ b/research/object_detection/core/prefetcher.py
@@ -16,6 +16,10 @@
 """Provides functions to prefetch tensors to feed into models."""
 import tensorflow as tf
 
+from object_detection.utils import tf_version
+if not tf_version.is_tf1():
+  raise ValueError('`prefetcher.py` is only supported in Tensorflow 1.X')
+
 
 def prefetch(tensor_dict, capacity):
   """Creates a prefetch queue for tensors.
diff --git a/research/object_detection/core/prefetcher_test.py b/research/object_detection/core/prefetcher_tf1_test.py
similarity index 96%
rename from research/object_detection/core/prefetcher_test.py
rename to research/object_detection/core/prefetcher_tf1_test.py
index 83782d35..8d16a74a 100644
--- a/research/object_detection/core/prefetcher_test.py
+++ b/research/object_detection/core/prefetcher_tf1_test.py
@@ -21,12 +21,15 @@ from __future__ import print_function
 from six.moves import range
 import tensorflow as tf
 
+# pylint: disable=g-bad-import-order,
 from object_detection.core import prefetcher
-
-slim = tf.contrib.slim
+from tensorflow.contrib import slim as contrib_slim
+slim = contrib_slim
+# pylint: disable=g-bad-import-order
 
 
 class PrefetcherTest(tf.test.TestCase):
+  """Test class for prefetcher."""
 
   def test_prefetch_tensors_with_fully_defined_shapes(self):
     with self.test_session() as sess:
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 1c74a585..703bb025 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -83,6 +83,7 @@ from object_detection.core import keypoint_ops
 from object_detection.core import preprocessor_cache
 from object_detection.core import standard_fields as fields
 from object_detection.utils import autoaugment_utils
+from object_detection.utils import ops
 from object_detection.utils import patch_ops
 from object_detection.utils import shape_utils
 
@@ -463,18 +464,18 @@ def _flip_boxes_left_right(boxes):
   """Left-right flip the boxes.
 
   Args:
-    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].
+    boxes: Float32 tensor containing the bounding boxes -> [..., 4].
            Boxes are in normalized form meaning their coordinates vary
            between [0, 1].
-           Each row is in the form of [ymin, xmin, ymax, xmax].
+           Each last dimension is in the form of [ymin, xmin, ymax, xmax].
 
   Returns:
     Flipped boxes.
   """
-  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=1)
+  ymin, xmin, ymax, xmax = tf.split(value=boxes, num_or_size_splits=4, axis=-1)
   flipped_xmin = tf.subtract(1.0, xmax)
   flipped_xmax = tf.subtract(1.0, xmin)
-  flipped_boxes = tf.concat([ymin, flipped_xmin, ymax, flipped_xmax], 1)
+  flipped_boxes = tf.concat([ymin, flipped_xmin, ymax, flipped_xmax], axis=-1)
   return flipped_boxes
 
 
@@ -566,6 +567,7 @@ def random_horizontal_flip(image,
                            boxes=None,
                            masks=None,
                            keypoints=None,
+                           keypoint_visibilities=None,
                            keypoint_flip_permutation=None,
                            seed=None,
                            preprocess_vars_cache=None):
@@ -586,6 +588,8 @@ def random_horizontal_flip(image,
     keypoints: (optional) rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]. The keypoints are in y-x
                normalized coordinates.
+    keypoint_visibilities: (optional) rank 2 bool tensor with shape
+                           [num_instances, num_keypoints].
     keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip
                                permutation.
     seed: random seed
@@ -597,8 +601,9 @@ def random_horizontal_flip(image,
   Returns:
     image: image which is the same shape as input image.
 
-    If boxes, masks, keypoints, and keypoint_flip_permutation are not None,
-    the function also returns the following tensors.
+    If boxes, masks, keypoints, keypoint_visibilities, and
+    keypoint_flip_permutation are not None,the function also returns the
+    following tensors.
 
     boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].
            Boxes are in normalized form meaning their coordinates vary
@@ -607,6 +612,8 @@ def random_horizontal_flip(image,
            containing instance masks.
     keypoints: rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]
+    keypoint_visibilities: rank 2 bool tensor with shape
+                           [num_instances, num_keypoints].
 
   Raises:
     ValueError: if keypoints are provided but keypoint_flip_permutation is not.
@@ -656,6 +663,17 @@ def random_horizontal_flip(image,
           lambda: keypoints)
       result.append(keypoints)
 
+    # flip keypoint visibilities
+    if (keypoint_visibilities is not None and
+        keypoint_flip_permutation is not None):
+      permutation = keypoint_flip_permutation
+      kpt_flip_perm = keypoint_flip_permutation
+      keypoint_visibilities = tf.cond(
+          do_a_flip_random,
+          lambda: tf.gather(keypoint_visibilities, kpt_flip_perm, axis=1),
+          lambda: keypoint_visibilities)
+      result.append(keypoint_visibilities)
+
     return tuple(result)
 
 
@@ -1259,6 +1277,7 @@ def _strict_random_crop_image(image,
                               multiclass_scores=None,
                               masks=None,
                               keypoints=None,
+                              keypoint_visibilities=None,
                               min_object_covered=1.0,
                               aspect_ratio_range=(0.75, 1.33),
                               area_range=(0.1, 1.0),
@@ -1294,6 +1313,8 @@ def _strict_random_crop_image(image,
     keypoints: (optional) rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]. The keypoints are in y-x
                normalized coordinates.
+    keypoint_visibilities: (optional) rank 2 bool tensor with shape
+               [num_instances, num_keypoints].
     min_object_covered: the cropped image must cover at least this fraction of
                         at least one of the input bounding boxes.
     aspect_ratio_range: allowed range for aspect ratio of cropped image.
@@ -1313,8 +1334,8 @@ def _strict_random_crop_image(image,
            Boxes are in normalized form.
     labels: new labels.
 
-    If label_weights, multiclass_scores, masks, or keypoints is not None, the
-    function also returns:
+    If label_weights, multiclass_scores, masks, keypoints, or
+    keypoint_visibilities is not None, the function also returns:
     label_weights: rank 1 float32 tensor with shape [num_instances].
     multiclass_scores: rank 2 float32 tensor with shape
                        [num_instances, num_classes]
@@ -1322,6 +1343,8 @@ def _strict_random_crop_image(image,
            containing instance masks.
     keypoints: rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]
+    keypoint_visibilities: rank 2 bool tensor with shape
+               [num_instances, num_keypoints]
   """
   with tf.name_scope('RandomCropImage', values=[image, boxes]):
     image_shape = tf.shape(image)
@@ -1349,8 +1372,9 @@ def _strict_random_crop_image(image,
         preprocess_vars_cache, key=min_object_covered)
 
     im_box_begin, im_box_size, im_box = sample_distorted_bounding_box
-
-    new_image = tf.slice(image, im_box_begin, im_box_size)
+    im_box_end = im_box_begin + im_box_size
+    new_image = image[im_box_begin[0]:im_box_end[0],
+                      im_box_begin[1]:im_box_end[1], :]
     new_image.set_shape([None, None, image.get_shape()[2]])
 
     # [1, 4]
@@ -1407,11 +1431,8 @@ def _strict_random_crop_image(image,
       masks_of_boxes_inside_window = tf.gather(masks, inside_window_ids)
       masks_of_boxes_completely_inside_window = tf.gather(
           masks_of_boxes_inside_window, keep_ids)
-      masks_box_begin = [0, im_box_begin[0], im_box_begin[1]]
-      masks_box_size = [-1, im_box_size[0], im_box_size[1]]
-      new_masks = tf.slice(
-          masks_of_boxes_completely_inside_window,
-          masks_box_begin, masks_box_size)
+      new_masks = masks_of_boxes_completely_inside_window[:, im_box_begin[
+          0]:im_box_end[0], im_box_begin[1]:im_box_end[1]]
       result.append(new_masks)
 
     if keypoints is not None:
@@ -1425,6 +1446,17 @@ def _strict_random_crop_image(image,
                                                           [0.0, 0.0, 1.0, 1.0])
       result.append(new_keypoints)
 
+    if keypoint_visibilities is not None:
+      kpt_vis_of_boxes_inside_window = tf.gather(keypoint_visibilities,
+                                                 inside_window_ids)
+      kpt_vis_of_boxes_completely_inside_window = tf.gather(
+          kpt_vis_of_boxes_inside_window, keep_ids)
+      if clip_boxes:
+        # Set any keypoints with NaN coordinates to invisible.
+        new_kpt_visibilities = keypoint_ops.set_keypoint_visibilities(
+            new_keypoints, kpt_vis_of_boxes_completely_inside_window)
+        result.append(new_kpt_visibilities)
+
     return tuple(result)
 
 
@@ -1436,6 +1468,7 @@ def random_crop_image(image,
                       multiclass_scores=None,
                       masks=None,
                       keypoints=None,
+                      keypoint_visibilities=None,
                       min_object_covered=1.0,
                       aspect_ratio_range=(0.75, 1.33),
                       area_range=(0.1, 1.0),
@@ -1457,6 +1490,7 @@ def random_crop_image(image,
 
   Note: Keypoint coordinates that are outside the crop will be set to NaN, which
   is consistent with the original keypoint encoding for non-existing keypoints.
+  Also, the keypoint visibility will be set to False.
 
   Args:
     image: rank 3 float32 tensor contains 1 image -> [height, width, channels]
@@ -1480,6 +1514,8 @@ def random_crop_image(image,
     keypoints: (optional) rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]. The keypoints are in y-x
                normalized coordinates.
+    keypoint_visibilities: (optional) rank 2 bool tensor with shape
+                           [num_instances, num_keypoints].
     min_object_covered: the cropped image must cover at least this fraction of
                         at least one of the input bounding boxes.
     aspect_ratio_range: allowed range for aspect ratio of cropped image.
@@ -1504,8 +1540,8 @@ def random_crop_image(image,
            form.
     labels: new labels.
 
-    If label_weights, multiclass_scores, masks, or keypoints is not None, the
-    function also returns:
+    If label_weights, multiclass_scores, masks, keypoints, keypoint_visibilities
+    is not None, the function also returns:
     label_weights: rank 1 float32 tensor with shape [num_instances].
     multiclass_scores: rank 2 float32 tensor with shape
                        [num_instances, num_classes]
@@ -1513,6 +1549,8 @@ def random_crop_image(image,
            containing instance masks.
     keypoints: rank 3 float32 tensor with shape
                [num_instances, num_keypoints, 2]
+    keypoint_visibilities: rank 2 bool tensor with shape
+               [num_instances, num_keypoints]
   """
 
   def strict_random_crop_image_fn():
@@ -1525,6 +1563,7 @@ def random_crop_image(image,
         multiclass_scores=multiclass_scores,
         masks=masks,
         keypoints=keypoints,
+        keypoint_visibilities=keypoint_visibilities,
         min_object_covered=min_object_covered,
         aspect_ratio_range=aspect_ratio_range,
         area_range=area_range,
@@ -1554,6 +1593,8 @@ def random_crop_image(image,
       outputs.append(masks)
     if keypoints is not None:
       outputs.append(keypoints)
+    if keypoint_visibilities is not None:
+      outputs.append(keypoint_visibilities)
 
     result = tf.cond(do_a_crop_random, strict_random_crop_image_fn,
                      lambda: tuple(outputs))
@@ -1696,8 +1737,9 @@ def random_pad_image(image,
 
 def random_absolute_pad_image(image,
                               boxes,
-                              max_height_padding,
-                              max_width_padding,
+                              keypoints=None,
+                              max_height_padding=None,
+                              max_width_padding=None,
                               pad_color=None,
                               seed=None,
                               preprocess_vars_cache=None):
@@ -1714,6 +1756,9 @@ def random_absolute_pad_image(image,
            Boxes are in normalized form meaning their coordinates vary
            between [0, 1].
            Each row is in the form of [ymin, xmin, ymax, xmax].
+    keypoints: (optional) rank 3 float32 tensor with shape
+               [N, num_keypoints, 2]. The keypoints are in y-x normalized
+               coordinates.
     max_height_padding: a scalar tf.int32 tensor denoting the maximum amount of
                         height padding. The padding will be chosen uniformly at
                         random from [0, max_height_padding).
@@ -1737,10 +1782,15 @@ def random_absolute_pad_image(image,
   min_image_size = tf.shape(image)[:2]
   max_image_size = min_image_size + tf.cast(
       [max_height_padding, max_width_padding], dtype=tf.int32)
-  return random_pad_image(image, boxes, min_image_size=min_image_size,
-                          max_image_size=max_image_size, pad_color=pad_color,
-                          seed=seed,
-                          preprocess_vars_cache=preprocess_vars_cache)
+  return random_pad_image(
+      image,
+      boxes,
+      keypoints=keypoints,
+      min_image_size=min_image_size,
+      max_image_size=max_image_size,
+      pad_color=pad_color,
+      seed=seed,
+      preprocess_vars_cache=preprocess_vars_cache)
 
 
 def random_crop_pad_image(image,
@@ -2869,6 +2919,49 @@ def resize_to_max_dimension(image, masks=None, max_dimension=600,
     return result
 
 
+def resize_pad_to_multiple(image, masks=None, multiple=1):
+  """Resize an image by zero padding it to the specified multiple.
+
+  For example, with an image of size (101, 199, 3) and multiple=4,
+  the returned image will have shape (104, 200, 3).
+
+  Args:
+    image: a tensor of shape [height, width, channels]
+    masks: (optional) a tensor of shape [num_instances, height, width]
+    multiple: int, the multiple to which the height and width of the input
+      will be padded.
+
+  Returns:
+    resized_image: The image with 0 padding applied, such that output
+      dimensions are divisible by `multiple`
+    resized_masks: If masks are given, they are resized to the same
+      spatial dimensions as the image.
+    resized_image_shape: An integer tensor of shape [3] which holds
+      the shape of the input image.
+
+  """
+
+  if len(image.get_shape()) != 3:
+    raise ValueError('Image should be 3D tensor')
+
+  with tf.name_scope('ResizePadToMultiple', values=[image, multiple]):
+    image_height, image_width, num_channels = _get_image_info(image)
+    image = image[tf.newaxis, :, :, :]
+    image = ops.pad_to_multiple(image, multiple)[0, :, :, :]
+
+    if masks is not None:
+      masks = tf.transpose(masks, (1, 2, 0))
+      masks = masks[tf.newaxis, :, :, :]
+
+      masks = ops.pad_to_multiple(masks, multiple)[0, :, :, :]
+      masks = tf.transpose(masks, (2, 0, 1))
+
+  if masks is None:
+    return image, tf.stack([image_height, image_width, num_channels])
+  else:
+    return image, masks, tf.stack([image_height, image_width, num_channels])
+
+
 def scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):
   """Scales boxes from normalized to pixel coordinates.
 
@@ -3695,18 +3788,189 @@ def convert_class_logits_to_softmax(multiclass_scores, temperature=1.0):
   """
 
   # Multiclass scores must be stored as logits. Apply temp and softmax.
-  multiclass_scores_scaled = tf.divide(
-      multiclass_scores, temperature, name='scale_logits')
+  multiclass_scores_scaled = tf.multiply(
+      multiclass_scores, 1.0 / temperature, name='scale_logits')
   multiclass_scores = tf.nn.softmax(multiclass_scores_scaled, name='softmax')
 
   return multiclass_scores
 
 
+def _get_crop_border(border, size):
+  border = tf.cast(border, tf.float32)
+  size = tf.cast(size, tf.float32)
+
+  i = tf.ceil(tf.log(2.0 * border / size) / tf.log(2.0))
+  divisor = tf.pow(2.0, i)
+  divisor = tf.clip_by_value(divisor, 1, border)
+  divisor = tf.cast(divisor, tf.int32)
+
+  return tf.cast(border, tf.int32) // divisor
+
+
+def random_square_crop_by_scale(image, boxes, labels, label_weights,
+                                masks=None, keypoints=None, max_border=128,
+                                scale_min=0.6, scale_max=1.3, num_scales=8,
+                                seed=None, preprocess_vars_cache=None):
+  """Randomly crop a square in proportion to scale and image size.
+
+   Extract a square sized crop from an image whose side length is sampled by
+   randomly scaling the maximum spatial dimension of the image. If part of
+   the crop falls outside the image, it is filled with zeros.
+   The augmentation is borrowed from [1]
+   [1]: https://arxiv.org/abs/1904.07850
+
+  Args:
+    image: rank 3 float32 tensor containing 1 image ->
+           [height, width,channels].
+    boxes: rank 2 float32 tensor containing the bounding boxes -> [N, 4].
+           Boxes are in normalized form meaning their coordinates vary
+           between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax].
+           Boxes on the crop boundary are clipped to the boundary and boxes
+           falling outside the crop are ignored.
+    labels: rank 1 int32 tensor containing the object classes.
+    label_weights: float32 tensor of shape [num_instances] representing the
+      weight for each box.
+    masks: (optional) rank 3 float32 tensor with shape
+           [num_instances, height, width] containing instance masks. The masks
+           are of the same height, width as the input `image`.
+    keypoints: (optional) rank 3 float32 tensor with shape
+      [num_instances, num_keypoints, 2]. The keypoints are in y-x normalized
+      coordinates.
+    max_border: The maximum size of the border. The border defines distance in
+      pixels to the image boundaries that will not be considered as a center of
+      a crop. To make sure that the border does not go over the center of the
+      image, we chose the border value by computing the minimum k, such that
+      (max_border / (2**k)) < image_dimension/2.
+    scale_min: float, the minimum value for scale.
+    scale_max: float, the maximum value for scale.
+    num_scales: int, the number of discrete scale values to sample between
+      [scale_min, scale_max]
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+
+
+  Returns:
+    image: image which is the same rank as input image.
+    boxes: boxes which is the same rank as input boxes.
+           Boxes are in normalized form.
+    labels: new labels.
+    label_weights: rank 1 float32 tensor with shape [num_instances].
+    masks: rank 3 float32 tensor with shape [num_instances, height, width]
+           containing instance masks.
+
+  """
+
+  img_shape = tf.shape(image)
+  height, width = img_shape[0], img_shape[1]
+  scales = tf.linspace(scale_min, scale_max, num_scales)
+
+  scale = _get_or_create_preprocess_rand_vars(
+      lambda: scales[_random_integer(0, num_scales, seed)],
+      preprocessor_cache.PreprocessorCache.SQUARE_CROP_BY_SCALE,
+      preprocess_vars_cache, 'scale')
+
+  image_size = scale * tf.cast(tf.maximum(height, width), tf.float32)
+  image_size = tf.cast(image_size, tf.int32)
+  h_border = _get_crop_border(max_border, height)
+  w_border = _get_crop_border(max_border, width)
+
+  def y_function():
+    y = _random_integer(h_border,
+                        tf.cast(height, tf.int32) - h_border + 1,
+                        seed)
+    return y
+
+  def x_function():
+    x = _random_integer(w_border,
+                        tf.cast(width, tf.int32) - w_border + 1,
+                        seed)
+    return x
+
+  y_center = _get_or_create_preprocess_rand_vars(
+      y_function,
+      preprocessor_cache.PreprocessorCache.SQUARE_CROP_BY_SCALE,
+      preprocess_vars_cache, 'y_center')
+
+  x_center = _get_or_create_preprocess_rand_vars(
+      x_function,
+      preprocessor_cache.PreprocessorCache.SQUARE_CROP_BY_SCALE,
+      preprocess_vars_cache, 'x_center')
+
+  half_size = tf.cast(image_size / 2, tf.int32)
+  crop_ymin, crop_ymax = y_center - half_size, y_center + half_size
+  crop_xmin, crop_xmax = x_center - half_size, x_center + half_size
+
+  ymin = tf.maximum(crop_ymin, 0)
+  xmin = tf.maximum(crop_xmin, 0)
+  ymax = tf.minimum(crop_ymax, height - 1)
+  xmax = tf.minimum(crop_xmax, width - 1)
+
+  cropped_image = image[ymin:ymax, xmin:xmax]
+  offset_y = tf.maximum(0, ymin - crop_ymin)
+  offset_x = tf.maximum(0, xmin - crop_xmin)
+
+  oy_i = offset_y
+  ox_i = offset_x
+
+  output_image = tf.image.pad_to_bounding_box(
+      cropped_image, offset_height=oy_i, offset_width=ox_i,
+      target_height=image_size, target_width=image_size)
+
+  if ymin == 0:
+    # We might be padding the image.
+    box_ymin = -offset_y
+  else:
+    box_ymin = crop_ymin
+
+  if xmin == 0:
+    # We might be padding the image.
+    box_xmin = -offset_x
+  else:
+    box_xmin = crop_xmin
+
+  box_ymax = box_ymin + image_size
+  box_xmax = box_xmin + image_size
+
+  image_box = [box_ymin / height, box_xmin / width,
+               box_ymax / height, box_xmax / width]
+  boxlist = box_list.BoxList(boxes)
+  boxlist = box_list_ops.change_coordinate_frame(boxlist, image_box)
+  boxlist, indices = box_list_ops.prune_completely_outside_window(
+      boxlist, [0.0, 0.0, 1.0, 1.0])
+  boxlist = box_list_ops.clip_to_window(boxlist, [0.0, 0.0, 1.0, 1.0],
+                                        filter_nonoverlapping=False)
+
+  return_values = [output_image, boxlist.get(),
+                   tf.gather(labels, indices),
+                   tf.gather(label_weights, indices)]
+
+  if masks is not None:
+    new_masks = tf.expand_dims(masks, -1)
+    new_masks = new_masks[:, ymin:ymax, xmin:xmax]
+    new_masks = tf.image.pad_to_bounding_box(
+        new_masks, oy_i, ox_i, image_size, image_size)
+    new_masks = tf.squeeze(new_masks, [-1])
+    return_values.append(tf.gather(new_masks, indices))
+
+  if keypoints is not None:
+    keypoints = tf.gather(keypoints, indices)
+    keypoints = keypoint_ops.change_coordinate_frame(keypoints, image_box)
+    keypoints = keypoint_ops.prune_outside_window(keypoints,
+                                                  [0.0, 0.0, 1.0, 1.0])
+    return_values.append(keypoints)
+
+  return return_values
+
+
 def get_default_func_arg_map(include_label_weights=True,
                              include_label_confidences=False,
                              include_multiclass_scores=False,
                              include_instance_masks=False,
-                             include_keypoints=False):
+                             include_keypoints=False,
+                             include_keypoint_visibilities=False):
   """Returns the default mapping from a preprocessor function to its args.
 
   Args:
@@ -3720,6 +3984,8 @@ def get_default_func_arg_map(include_label_weights=True,
       instance masks, too.
     include_keypoints: If True, preprocessing functions will modify the
       keypoints, too.
+    include_keypoint_visibilities: If True, preprocessing functions will modify
+      the keypoint visibilities, too.
 
   Returns:
     A map from preprocessing functions to the arguments they receive.
@@ -3747,6 +4013,11 @@ def get_default_func_arg_map(include_label_weights=True,
   if include_keypoints:
     groundtruth_keypoints = fields.InputDataFields.groundtruth_keypoints
 
+  groundtruth_keypoint_visibilities = None
+  if include_keypoint_visibilities:
+    groundtruth_keypoint_visibilities = (
+        fields.InputDataFields.groundtruth_keypoint_visibilities)
+
   prep_func_arg_map = {
       normalize_image: (fields.InputDataFields.image,),
       random_horizontal_flip: (
@@ -3754,6 +4025,7 @@ def get_default_func_arg_map(include_label_weights=True,
           fields.InputDataFields.groundtruth_boxes,
           groundtruth_instance_masks,
           groundtruth_keypoints,
+          groundtruth_keypoint_visibilities,
       ),
       random_vertical_flip: (
           fields.InputDataFields.image,
@@ -3783,21 +4055,20 @@ def get_default_func_arg_map(include_label_weights=True,
                           fields.InputDataFields.groundtruth_boxes,
                           fields.InputDataFields.groundtruth_classes,
                           groundtruth_label_weights,
-                          groundtruth_label_confidences,
-                          multiclass_scores,
-                          groundtruth_instance_masks,
-                          groundtruth_keypoints),
-      random_pad_image: (fields.InputDataFields.image,
-                         fields.InputDataFields.groundtruth_boxes,
-                         groundtruth_keypoints),
-      random_absolute_pad_image: (fields.InputDataFields.image,
-                                  fields.InputDataFields.groundtruth_boxes),
+                          groundtruth_label_confidences, multiclass_scores,
+                          groundtruth_instance_masks, groundtruth_keypoints,
+                          groundtruth_keypoint_visibilities),
+      random_pad_image:
+          (fields.InputDataFields.image,
+           fields.InputDataFields.groundtruth_boxes, groundtruth_keypoints),
+      random_absolute_pad_image:
+          (fields.InputDataFields.image,
+           fields.InputDataFields.groundtruth_boxes, groundtruth_keypoints),
       random_crop_pad_image: (fields.InputDataFields.image,
                               fields.InputDataFields.groundtruth_boxes,
                               fields.InputDataFields.groundtruth_classes,
                               groundtruth_label_weights,
-                              groundtruth_label_confidences,
-                              multiclass_scores),
+                              groundtruth_label_confidences, multiclass_scores),
       random_crop_to_aspect_ratio: (
           fields.InputDataFields.image,
           fields.InputDataFields.groundtruth_boxes,
@@ -3821,8 +4092,10 @@ def get_default_func_arg_map(include_label_weights=True,
           groundtruth_instance_masks,
       ),
       random_patch_gaussian: (fields.InputDataFields.image,),
-      autoaugment_image: (fields.InputDataFields.image,
-                          fields.InputDataFields.groundtruth_boxes,),
+      autoaugment_image: (
+          fields.InputDataFields.image,
+          fields.InputDataFields.groundtruth_boxes,
+      ),
       retain_boxes_above_threshold: (
           fields.InputDataFields.groundtruth_boxes,
           fields.InputDataFields.groundtruth_classes,
@@ -3864,35 +4137,30 @@ def get_default_func_arg_map(include_label_weights=True,
       subtract_channel_mean: (fields.InputDataFields.image,),
       one_hot_encoding: (fields.InputDataFields.groundtruth_image_classes,),
       rgb_to_gray: (fields.InputDataFields.image,),
-      random_self_concat_image: (fields.InputDataFields.image,
-                                 fields.InputDataFields.groundtruth_boxes,
-                                 fields.InputDataFields.groundtruth_classes,
-                                 groundtruth_label_weights,
-                                 groundtruth_label_confidences,
-                                 multiclass_scores),
+      random_self_concat_image:
+          (fields.InputDataFields.image,
+           fields.InputDataFields.groundtruth_boxes,
+           fields.InputDataFields.groundtruth_classes,
+           groundtruth_label_weights, groundtruth_label_confidences,
+           multiclass_scores),
       ssd_random_crop: (fields.InputDataFields.image,
                         fields.InputDataFields.groundtruth_boxes,
                         fields.InputDataFields.groundtruth_classes,
                         groundtruth_label_weights,
-                        groundtruth_label_confidences,
-                        multiclass_scores,
-                        groundtruth_instance_masks,
-                        groundtruth_keypoints),
+                        groundtruth_label_confidences, multiclass_scores,
+                        groundtruth_instance_masks, groundtruth_keypoints),
       ssd_random_crop_pad: (fields.InputDataFields.image,
                             fields.InputDataFields.groundtruth_boxes,
                             fields.InputDataFields.groundtruth_classes,
                             groundtruth_label_weights,
-                            groundtruth_label_confidences,
-                            multiclass_scores),
-      ssd_random_crop_fixed_aspect_ratio: (
-          fields.InputDataFields.image,
-          fields.InputDataFields.groundtruth_boxes,
-          fields.InputDataFields.groundtruth_classes,
-          groundtruth_label_weights,
-          groundtruth_label_confidences,
-          multiclass_scores,
-          groundtruth_instance_masks,
-          groundtruth_keypoints),
+                            groundtruth_label_confidences, multiclass_scores),
+      ssd_random_crop_fixed_aspect_ratio:
+          (fields.InputDataFields.image,
+           fields.InputDataFields.groundtruth_boxes,
+           fields.InputDataFields.groundtruth_classes,
+           groundtruth_label_weights, groundtruth_label_confidences,
+           multiclass_scores, groundtruth_instance_masks, groundtruth_keypoints
+          ),
       ssd_random_crop_pad_fixed_aspect_ratio: (
           fields.InputDataFields.image,
           fields.InputDataFields.groundtruth_boxes,
@@ -3904,6 +4172,12 @@ def get_default_func_arg_map(include_label_weights=True,
           groundtruth_keypoints,
       ),
       convert_class_logits_to_softmax: (multiclass_scores,),
+      random_square_crop_by_scale:
+          (fields.InputDataFields.image,
+           fields.InputDataFields.groundtruth_boxes,
+           fields.InputDataFields.groundtruth_classes,
+           groundtruth_label_weights, groundtruth_instance_masks,
+           groundtruth_keypoints),
   }
 
   return prep_func_arg_map
diff --git a/research/object_detection/core/preprocessor_cache.py b/research/object_detection/core/preprocessor_cache.py
index 706d44cd..94871056 100644
--- a/research/object_detection/core/preprocessor_cache.py
+++ b/research/object_detection/core/preprocessor_cache.py
@@ -22,7 +22,7 @@ PreprocessorCache object, that function will perform the same augmentation
 on all calls.
 """
 
-from collections import defaultdict
+import collections
 
 
 class PreprocessorCache(object):
@@ -57,6 +57,7 @@ class PreprocessorCache(object):
   JPEG_QUALITY = 'jpeg_quality'
   DOWNSCALE_TO_TARGET_PIXELS = 'downscale_to_target_pixels'
   PATCH_GAUSSIAN = 'patch_gaussian'
+  SQUARE_CROP_BY_SCALE = 'square_crop_scale'
 
   # 27 permitted function ids
   _VALID_FNS = [ROTATION90, HORIZONTAL_FLIP, VERTICAL_FLIP, PIXEL_VALUE_SCALE,
@@ -66,14 +67,15 @@ class PreprocessorCache(object):
                 PAD_TO_ASPECT_RATIO, BLACK_PATCHES, ADD_BLACK_PATCH, SELECTOR,
                 SELECTOR_TUPLES, SELF_CONCAT_IMAGE, SSD_CROP_SELECTOR_ID,
                 SSD_CROP_PAD_SELECTOR_ID, JPEG_QUALITY,
-                DOWNSCALE_TO_TARGET_PIXELS, PATCH_GAUSSIAN]
+                DOWNSCALE_TO_TARGET_PIXELS, PATCH_GAUSSIAN,
+                SQUARE_CROP_BY_SCALE]
 
   def __init__(self):
-    self._history = defaultdict(dict)
+    self._history = collections.defaultdict(dict)
 
   def clear(self):
     """Resets cache."""
-    self._history = defaultdict(dict)
+    self._history = collections.defaultdict(dict)
 
   def get(self, function_id, key):
     """Gets stored value given a function id and key.
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 39167742..3b4e2b13 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -29,6 +29,7 @@ import tensorflow as tf
 from object_detection.core import preprocessor
 from object_detection.core import preprocessor_cache
 from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
 
 if six.PY2:
   import mock  # pylint: disable=g-import-not-at-top
@@ -36,7 +37,7 @@ else:
   from unittest import mock  # pylint: disable=g-import-not-at-top
 
 
-class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
+class PreprocessorTest(test_case.TestCase, parameterized.TestCase):
 
   def createColorfulTestImage(self):
     ch255 = tf.fill([1, 100, 200, 1], tf.constant(255, dtype=tf.uint8))
@@ -90,11 +91,17 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     return tf.constant(mask, dtype=tf.float32)
 
   def createTestKeypoints(self):
-    keypoints = np.array([
+    keypoints_np = np.array([
         [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]],
         [[0.4, 0.4], [0.5, 0.5], [0.6, 0.6]],
     ])
-    return tf.constant(keypoints, dtype=tf.float32)
+    keypoints = tf.constant(keypoints_np, dtype=tf.float32)
+    keypoint_visibilities = tf.constant(
+        [
+            [True, True, False],
+            [False, True, True]
+        ])
+    return keypoints, keypoint_visibilities
 
   def createTestKeypointsInsideCrop(self):
     keypoints = np.array([
@@ -329,369 +336,348 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     return tf.constant([3, 3, 4], dtype=tf.float32)
 
   def testRgbToGrayscale(self):
-    images = self.createTestImages()
-    grayscale_images = preprocessor._rgb_to_grayscale(images)
-    expected_images = tf.image.rgb_to_grayscale(images)
-    with self.test_session() as sess:
-      (grayscale_images, expected_images) = sess.run(
-          [grayscale_images, expected_images])
-      self.assertAllEqual(expected_images, grayscale_images)
+    def graph_fn():
+      images = self.createTestImages()
+      grayscale_images = preprocessor._rgb_to_grayscale(images)
+      expected_images = tf.image.rgb_to_grayscale(images)
+      return grayscale_images, expected_images
+    (grayscale_images, expected_images) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(expected_images, grayscale_images)
 
   def testNormalizeImage(self):
-    preprocess_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 256,
-        'target_minval': -1,
-        'target_maxval': 1
-    })]
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    images_expected = self.expectedImagesAfterNormalization()
-
-    with self.test_session() as sess:
-      (images_, images_expected_) = sess.run(
-          [images, images_expected])
-      images_shape_ = images_.shape
-      images_expected_shape_ = images_expected_.shape
-      expected_shape = [1, 4, 4, 3]
-      self.assertAllEqual(images_expected_shape_, images_shape_)
-      self.assertAllEqual(images_shape_, expected_shape)
-      self.assertAllClose(images_, images_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 256,
+          'target_minval': -1,
+          'target_maxval': 1
+      })]
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      images_expected = self.expectedImagesAfterNormalization()
+      return images, images_expected
+    images_, images_expected_ = self.execute_cpu(graph_fn, [])
+    images_shape_ = images_.shape
+    images_expected_shape_ = images_expected_.shape
+    expected_shape = [1, 4, 4, 3]
+    self.assertAllEqual(images_expected_shape_, images_shape_)
+    self.assertAllEqual(images_shape_, expected_shape)
+    self.assertAllClose(images_, images_expected_)
 
   def testRetainBoxesAboveThreshold(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    (retained_boxes, retained_labels,
-     retained_weights) = preprocessor.retain_boxes_above_threshold(
-         boxes, labels, weights, threshold=0.6)
-    with self.test_session() as sess:
-      (retained_boxes_, retained_labels_, retained_weights_,
-       expected_retained_boxes_, expected_retained_labels_,
-       expected_retained_weights_) = sess.run([
-           retained_boxes, retained_labels, retained_weights,
-           self.expectedBoxesAfterThresholding(),
-           self.expectedLabelsAfterThresholding(),
-           self.expectedLabelScoresAfterThresholding()])
-      self.assertAllClose(
-          retained_boxes_, expected_retained_boxes_)
-      self.assertAllClose(
-          retained_labels_, expected_retained_labels_)
-      self.assertAllClose(
-          retained_weights_, expected_retained_weights_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      (retained_boxes, retained_labels,
+       retained_weights) = preprocessor.retain_boxes_above_threshold(
+           boxes, labels, weights, threshold=0.6)
+      return [
+          retained_boxes, retained_labels, retained_weights,
+          self.expectedBoxesAfterThresholding(),
+          self.expectedLabelsAfterThresholding(),
+          self.expectedLabelScoresAfterThresholding()
+      ]
+
+    (retained_boxes_, retained_labels_, retained_weights_,
+     expected_retained_boxes_, expected_retained_labels_,
+     expected_retained_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(
+        retained_boxes_, expected_retained_boxes_)
+    self.assertAllClose(
+        retained_labels_, expected_retained_labels_)
+    self.assertAllClose(
+        retained_weights_, expected_retained_weights_)
 
   def testRetainBoxesAboveThresholdWithMultiClassScores(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    multiclass_scores = self.createTestMultiClassScores()
-    (_, _, _,
-     retained_multiclass_scores) = preprocessor.retain_boxes_above_threshold(
-         boxes,
-         labels,
-         weights,
-         multiclass_scores=multiclass_scores,
-         threshold=0.6)
-    with self.test_session() as sess:
-      (retained_multiclass_scores_,
-       expected_retained_multiclass_scores_) = sess.run([
-           retained_multiclass_scores,
-           self.expectedMultiClassScoresAfterThresholding()
-       ])
-
-      self.assertAllClose(retained_multiclass_scores_,
-                          expected_retained_multiclass_scores_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      multiclass_scores = self.createTestMultiClassScores()
+      (_, _, _,
+       retained_multiclass_scores) = preprocessor.retain_boxes_above_threshold(
+           boxes,
+           labels,
+           weights,
+           multiclass_scores=multiclass_scores,
+           threshold=0.6)
+      return [
+          retained_multiclass_scores,
+          self.expectedMultiClassScoresAfterThresholding()
+      ]
+
+    (retained_multiclass_scores_,
+     expected_retained_multiclass_scores_) = self.execute(graph_fn, [])
+    self.assertAllClose(retained_multiclass_scores_,
+                        expected_retained_multiclass_scores_)
 
   def testRetainBoxesAboveThresholdWithMasks(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = self.createTestMasks()
-    _, _, _, retained_masks = preprocessor.retain_boxes_above_threshold(
-        boxes, labels, weights, masks, threshold=0.6)
-    with self.test_session() as sess:
-      retained_masks_, expected_retained_masks_ = sess.run([
-          retained_masks, self.expectedMasksAfterThresholding()])
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = self.createTestMasks()
+      _, _, _, retained_masks = preprocessor.retain_boxes_above_threshold(
+          boxes, labels, weights, masks, threshold=0.6)
+      return [
+          retained_masks, self.expectedMasksAfterThresholding()]
+    retained_masks_, expected_retained_masks_ = self.execute_cpu(graph_fn, [])
 
-      self.assertAllClose(
-          retained_masks_, expected_retained_masks_)
+    self.assertAllClose(
+        retained_masks_, expected_retained_masks_)
 
   def testRetainBoxesAboveThresholdWithKeypoints(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypoints()
-    (_, _, _, retained_keypoints) = preprocessor.retain_boxes_above_threshold(
-        boxes, labels, weights, keypoints=keypoints, threshold=0.6)
-    with self.test_session() as sess:
-      (retained_keypoints_,
-       expected_retained_keypoints_) = sess.run([
-           retained_keypoints,
-           self.expectedKeypointsAfterThresholding()])
-
-      self.assertAllClose(
-          retained_keypoints_, expected_retained_keypoints_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints, _ = self.createTestKeypoints()
+      (_, _, _, retained_keypoints) = preprocessor.retain_boxes_above_threshold(
+          boxes, labels, weights, keypoints=keypoints, threshold=0.6)
+      return [retained_keypoints, self.expectedKeypointsAfterThresholding()]
+
+    (retained_keypoints_,
+     expected_retained_keypoints_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_keypoints_, expected_retained_keypoints_)
 
   def testDropLabelProbabilistically(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    (retained_boxes, retained_labels,
-     retained_weights) = preprocessor.drop_label_probabilistically(
-         boxes, labels, weights, dropped_label=1, drop_probability=1.0)
-    with self.test_session() as sess:
-      (retained_boxes_, retained_labels_, retained_weights_,
-       expected_retained_boxes_, expected_retained_labels_,
-       expected_retained_weights_) = sess.run([
-           retained_boxes, retained_labels, retained_weights,
-           self.expectedBoxesAfterDropping(),
-           self.expectedLabelsAfterDropping(),
-           self.expectedLabelScoresAfterDropping()
-       ])
-      self.assertAllClose(retained_boxes_, expected_retained_boxes_)
-      self.assertAllClose(retained_labels_, expected_retained_labels_)
-      self.assertAllClose(retained_weights_, expected_retained_weights_)
-
-  def testDropLabelProbabilisticallyWithProbabilityHalf(self):
-    # Boxes contain one box of label 2 and one box of label 1 which should be
-    # dropped ~50% of the time.
-    num_tests = 100
-    total = 0
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    (_, retained_labels, _) = preprocessor.drop_label_probabilistically(
-        boxes, labels, weights, dropped_label=1, drop_probability=0.5)
-    for _ in range(num_tests):
-      with self.test_session() as sess:
-        retained_labels_ = sess.run(retained_labels)
-        total += len(retained_labels_)
-        self.assertIn(2, retained_labels_)
-    av = total * 1.0 / num_tests
-    self.assertGreater(av, 1.40)
-    self.assertLess(av, 1.50)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      (retained_boxes, retained_labels,
+       retained_weights) = preprocessor.drop_label_probabilistically(
+           boxes, labels, weights, dropped_label=1, drop_probability=1.0)
+      return [
+          retained_boxes, retained_labels, retained_weights,
+          self.expectedBoxesAfterDropping(),
+          self.expectedLabelsAfterDropping(),
+          self.expectedLabelScoresAfterDropping()
+      ]
+
+    (retained_boxes_, retained_labels_, retained_weights_,
+     expected_retained_boxes_, expected_retained_labels_,
+     expected_retained_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_boxes_, expected_retained_boxes_)
+    self.assertAllClose(retained_labels_, expected_retained_labels_)
+    self.assertAllClose(retained_weights_, expected_retained_weights_)
 
   def testDropLabelProbabilisticallyWithMultiClassScores(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    multiclass_scores = self.createTestMultiClassScores()
-    (_, _, _,
-     retained_multiclass_scores) = preprocessor.drop_label_probabilistically(
-         boxes,
-         labels,
-         weights,
-         multiclass_scores=multiclass_scores,
-         dropped_label=1,
-         drop_probability=1.0)
-    with self.test_session() as sess:
-      (retained_multiclass_scores_,
-       expected_retained_multiclass_scores_) = sess.run([
-           retained_multiclass_scores,
-           self.expectedMultiClassScoresAfterDropping()
-       ])
-      self.assertAllClose(retained_multiclass_scores_,
-                          expected_retained_multiclass_scores_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      multiclass_scores = self.createTestMultiClassScores()
+      (_, _, _,
+       retained_multiclass_scores) = preprocessor.drop_label_probabilistically(
+           boxes,
+           labels,
+           weights,
+           multiclass_scores=multiclass_scores,
+           dropped_label=1,
+           drop_probability=1.0)
+      return [retained_multiclass_scores,
+              self.expectedMultiClassScoresAfterDropping()]
+    (retained_multiclass_scores_,
+     expected_retained_multiclass_scores_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_multiclass_scores_,
+                        expected_retained_multiclass_scores_)
 
   def testDropLabelProbabilisticallyWithMasks(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = self.createTestMasks()
-    (_, _, _, retained_masks) = preprocessor.drop_label_probabilistically(
-        boxes,
-        labels,
-        weights,
-        masks=masks,
-        dropped_label=1,
-        drop_probability=1.0)
-    with self.test_session() as sess:
-      (retained_masks_, expected_retained_masks_) = sess.run(
-          [retained_masks, self.expectedMasksAfterDropping()])
-      self.assertAllClose(retained_masks_, expected_retained_masks_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = self.createTestMasks()
+      (_, _, _, retained_masks) = preprocessor.drop_label_probabilistically(
+          boxes,
+          labels,
+          weights,
+          masks=masks,
+          dropped_label=1,
+          drop_probability=1.0)
+      return [retained_masks, self.expectedMasksAfterDropping()]
+    (retained_masks_, expected_retained_masks_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_masks_, expected_retained_masks_)
 
   def testDropLabelProbabilisticallyWithKeypoints(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypoints()
-    (_, _, _, retained_keypoints) = preprocessor.drop_label_probabilistically(
-        boxes,
-        labels,
-        weights,
-        keypoints=keypoints,
-        dropped_label=1,
-        drop_probability=1.0)
-    with self.test_session() as sess:
-      (retained_keypoints_, expected_retained_keypoints_) = sess.run(
-          [retained_keypoints,
-           self.expectedKeypointsAfterDropping()])
-      self.assertAllClose(retained_keypoints_, expected_retained_keypoints_)
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints, _ = self.createTestKeypoints()
+      (_, _, _, retained_keypoints) = preprocessor.drop_label_probabilistically(
+          boxes,
+          labels,
+          weights,
+          keypoints=keypoints,
+          dropped_label=1,
+          drop_probability=1.0)
+      return [retained_keypoints, self.expectedKeypointsAfterDropping()]
+
+    (retained_keypoints_,
+     expected_retained_keypoints_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_keypoints_, expected_retained_keypoints_)
 
   def testRemapLabels(self):
-    labels = self.createTestLabelsLong()
-    remapped_labels = preprocessor.remap_labels(labels, [1, 2], 3)
-    with self.test_session() as sess:
-      (remapped_labels_, expected_remapped_labels_) = sess.run(
-          [remapped_labels, self.expectedLabelsAfterRemapping()])
-      self.assertAllClose(remapped_labels_, expected_remapped_labels_)
+    def graph_fn():
+      labels = self.createTestLabelsLong()
+      remapped_labels = preprocessor.remap_labels(labels, [1, 2], 3)
+      return [remapped_labels, self.expectedLabelsAfterRemapping()]
+
+    (remapped_labels_, expected_remapped_labels_) = self.execute_cpu(graph_fn,
+                                                                     [])
+    self.assertAllClose(remapped_labels_, expected_remapped_labels_)
 
   def testFlipBoxesLeftRight(self):
-    boxes = self.createTestBoxes()
-    flipped_boxes = preprocessor._flip_boxes_left_right(boxes)
-    expected_boxes = self.expectedBoxesAfterLeftRightFlip()
-    with self.test_session() as sess:
-      flipped_boxes, expected_boxes = sess.run([flipped_boxes, expected_boxes])
-      self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      flipped_boxes = preprocessor._flip_boxes_left_right(boxes)
+      expected_boxes = self.expectedBoxesAfterLeftRightFlip()
+      return flipped_boxes, expected_boxes
+    flipped_boxes, expected_boxes = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())
 
   def testFlipBoxesUpDown(self):
-    boxes = self.createTestBoxes()
-    flipped_boxes = preprocessor._flip_boxes_up_down(boxes)
-    expected_boxes = self.expectedBoxesAfterUpDownFlip()
-    with self.test_session() as sess:
-      flipped_boxes, expected_boxes = sess.run([flipped_boxes, expected_boxes])
-      self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      flipped_boxes = preprocessor._flip_boxes_up_down(boxes)
+      expected_boxes = self.expectedBoxesAfterUpDownFlip()
+      return flipped_boxes, expected_boxes
+    flipped_boxes, expected_boxes = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(flipped_boxes.flatten(), expected_boxes.flatten())
 
   def testRot90Boxes(self):
-    boxes = self.createTestBoxes()
-    rotated_boxes = preprocessor._rot90_boxes(boxes)
-    expected_boxes = self.expectedBoxesAfterRot90()
-    with self.test_session() as sess:
-      rotated_boxes, expected_boxes = sess.run([rotated_boxes, expected_boxes])
-      self.assertAllEqual(rotated_boxes.flatten(), expected_boxes.flatten())
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      rotated_boxes = preprocessor._rot90_boxes(boxes)
+      expected_boxes = self.expectedBoxesAfterRot90()
+      return rotated_boxes, expected_boxes
+    rotated_boxes, expected_boxes = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(rotated_boxes.flatten(), expected_boxes.flatten())
 
   def testFlipMasksLeftRight(self):
-    test_mask = self.createTestMasks()
-    flipped_mask = preprocessor._flip_masks_left_right(test_mask)
-    expected_mask = self.expectedMasksAfterLeftRightFlip()
-    with self.test_session() as sess:
-      flipped_mask, expected_mask = sess.run([flipped_mask, expected_mask])
-      self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())
+    def graph_fn():
+      test_mask = self.createTestMasks()
+      flipped_mask = preprocessor._flip_masks_left_right(test_mask)
+      expected_mask = self.expectedMasksAfterLeftRightFlip()
+      return flipped_mask, expected_mask
+    flipped_mask, expected_mask = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())
 
   def testFlipMasksUpDown(self):
-    test_mask = self.createTestMasks()
-    flipped_mask = preprocessor._flip_masks_up_down(test_mask)
-    expected_mask = self.expectedMasksAfterUpDownFlip()
-    with self.test_session() as sess:
-      flipped_mask, expected_mask = sess.run([flipped_mask, expected_mask])
-      self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())
+    def graph_fn():
+      test_mask = self.createTestMasks()
+      flipped_mask = preprocessor._flip_masks_up_down(test_mask)
+      expected_mask = self.expectedMasksAfterUpDownFlip()
+      return  flipped_mask, expected_mask
+    flipped_mask, expected_mask = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(flipped_mask.flatten(), expected_mask.flatten())
 
   def testRot90Masks(self):
-    test_mask = self.createTestMasks()
-    rotated_mask = preprocessor._rot90_masks(test_mask)
-    expected_mask = self.expectedMasksAfterRot90()
-    with self.test_session() as sess:
-      rotated_mask, expected_mask = sess.run([rotated_mask, expected_mask])
-      self.assertAllEqual(rotated_mask.flatten(), expected_mask.flatten())
+    def graph_fn():
+      test_mask = self.createTestMasks()
+      rotated_mask = preprocessor._rot90_masks(test_mask)
+      expected_mask = self.expectedMasksAfterRot90()
+      return [rotated_mask, expected_mask]
+    rotated_mask, expected_mask = self.execute(graph_fn, [])
+    self.assertAllEqual(rotated_mask.flatten(), expected_mask.flatten())
 
   def _testPreprocessorCache(self,
                              preprocess_options,
                              test_boxes=False,
                              test_masks=False,
-                             test_keypoints=False,
-                             num_runs=4):
-    cache = preprocessor_cache.PreprocessorCache()
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    weights = self.createTestGroundtruthWeights()
-    classes = self.createTestLabels()
-    masks = self.createTestMasks()
-    keypoints = self.createTestKeypoints()
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=test_masks, include_keypoints=test_keypoints)
-    out = []
-    for i in range(num_runs):
-      tensor_dict = {
-          fields.InputDataFields.image: images,
-          fields.InputDataFields.groundtruth_weights: weights
-      }
-      num_outputs = 1
-      if test_boxes:
-        tensor_dict[fields.InputDataFields.groundtruth_boxes] = boxes
-        tensor_dict[fields.InputDataFields.groundtruth_classes] = classes
-        num_outputs += 1
-      if test_masks:
-        tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks
-        num_outputs += 1
-      if test_keypoints:
-        tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints
-        num_outputs += 1
-      out.append(preprocessor.preprocess(
-          tensor_dict, preprocess_options, preprocessor_arg_map, cache))
-
-    with self.test_session() as sess:
-      to_run = []
-      for i in range(num_runs):
-        to_run.append(out[i][fields.InputDataFields.image])
+                             test_keypoints=False):
+    if self.is_tf2(): return
+    def graph_fn():
+      cache = preprocessor_cache.PreprocessorCache()
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      weights = self.createTestGroundtruthWeights()
+      classes = self.createTestLabels()
+      masks = self.createTestMasks()
+      keypoints, _ = self.createTestKeypoints()
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=test_masks, include_keypoints=test_keypoints)
+      out = []
+      for _ in range(2):
+        tensor_dict = {
+            fields.InputDataFields.image: images,
+            fields.InputDataFields.groundtruth_weights: weights
+        }
         if test_boxes:
-          to_run.append(out[i][fields.InputDataFields.groundtruth_boxes])
+          tensor_dict[fields.InputDataFields.groundtruth_boxes] = boxes
+          tensor_dict[fields.InputDataFields.groundtruth_classes] = classes
         if test_masks:
-          to_run.append(
-              out[i][fields.InputDataFields.groundtruth_instance_masks])
+          tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks
         if test_keypoints:
-          to_run.append(out[i][fields.InputDataFields.groundtruth_keypoints])
+          tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints
+        out.append(
+            preprocessor.preprocess(tensor_dict, preprocess_options,
+                                    preprocessor_arg_map, cache))
+      return out
 
-      out_array = sess.run(to_run)
-      for i in range(num_outputs, len(out_array)):
-        self.assertAllClose(out_array[i], out_array[i - num_outputs])
+    out1, out2 = self.execute_cpu_tf1(graph_fn, [])
+    for (_, v1), (_, v2) in zip(out1.items(), out2.items()):
+      self.assertAllClose(v1, v2)
 
   def testRandomHorizontalFlip(self):
-    preprocess_options = [(preprocessor.random_horizontal_flip, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterLeftRightFlip()
-    boxes_expected1 = self.expectedBoxesAfterLeftRightFlip()
-    images_expected2 = images
-    boxes_expected2 = boxes
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
-    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
-    boxes_diff_expected = tf.zeros_like(boxes_diff)
-
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
-
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_diff_,
-       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,
-                                         boxes_diff, boxes_diff_expected])
-      self.assertAllClose(boxes_diff_, boxes_diff_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_horizontal_flip, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createTestBoxes()
+      tensor_dict = {fields.InputDataFields.image: images,
+                     fields.InputDataFields.groundtruth_boxes: boxes}
+      images_expected1 = self.expectedImagesAfterLeftRightFlip()
+      boxes_expected1 = self.expectedBoxesAfterLeftRightFlip()
+      images_expected2 = images
+      boxes_expected2 = boxes
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
+      boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+      boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
+      boxes_diff_expected = tf.zeros_like(boxes_diff)
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [images_diff, images_diff_expected, boxes_diff,
+              boxes_diff_expected]
+    (images_diff_, images_diff_expected_, boxes_diff_,
+     boxes_diff_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_diff_, boxes_diff_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomHorizontalFlipWithEmptyBoxes(self):
-    preprocess_options = [(preprocessor.random_horizontal_flip, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createEmptyTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterLeftRightFlip()
-    boxes_expected = self.createEmptyTestBoxes()
-    images_expected2 = images
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
-
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_,
-       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,
-                                    boxes_expected])
-      self.assertAllClose(boxes_, boxes_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_horizontal_flip, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createEmptyTestBoxes()
+      tensor_dict = {fields.InputDataFields.image: images,
+                     fields.InputDataFields.groundtruth_boxes: boxes}
+      images_expected1 = self.expectedImagesAfterLeftRightFlip()
+      boxes_expected = self.createEmptyTestBoxes()
+      images_expected2 = images
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [images_diff, images_diff_expected, boxes, boxes_expected]
+    (images_diff_, images_diff_expected_, boxes_,
+     boxes_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_, boxes_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomHorizontalFlipWithCache(self):
     keypoint_flip_permutation = self.createKeypointFlipPermutation()
@@ -704,91 +690,114 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=True)
 
   def testRunRandomHorizontalFlipWithMaskAndKeypoints(self):
-    preprocess_options = [(preprocessor.random_horizontal_flip, {})]
-    image_height = 3
-    image_width = 3
-    images = tf.random_uniform([1, image_height, image_width, 3])
-    boxes = self.createTestBoxes()
-    masks = self.createTestMasks()
-    keypoints = self.createTestKeypoints()
-    keypoint_flip_permutation = self.createKeypointFlipPermutation()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_instance_masks: masks,
-        fields.InputDataFields.groundtruth_keypoints: keypoints
-    }
-    preprocess_options = [
-        (preprocessor.random_horizontal_flip,
-         {'keypoint_flip_permutation': keypoint_flip_permutation})]
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=True, include_keypoints=True)
-    tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocess_options, func_arg_map=preprocessor_arg_map)
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
-    keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]
-    with self.test_session() as sess:
-      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])
-      self.assertTrue(boxes is not None)
-      self.assertTrue(masks is not None)
-      self.assertTrue(keypoints is not None)
-
-  def testRandomVerticalFlip(self):
-    preprocess_options = [(preprocessor.random_vertical_flip, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterUpDownFlip()
-    boxes_expected1 = self.expectedBoxesAfterUpDownFlip()
-    images_expected2 = images
-    boxes_expected2 = boxes
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
-    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
-    boxes_diff_expected = tf.zeros_like(boxes_diff)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_horizontal_flip, {})]
+      image_height = 3
+      image_width = 3
+      images = tf.random_uniform([1, image_height, image_width, 3])
+      boxes = self.createTestBoxes()
+      masks = self.createTestMasks()
+      keypoints, keypoint_visibilities = self.createTestKeypoints()
+      keypoint_flip_permutation = self.createKeypointFlipPermutation()
+      tensor_dict = {
+          fields.InputDataFields.image:
+              images,
+          fields.InputDataFields.groundtruth_boxes:
+              boxes,
+          fields.InputDataFields.groundtruth_instance_masks:
+              masks,
+          fields.InputDataFields.groundtruth_keypoints:
+              keypoints,
+          fields.InputDataFields.groundtruth_keypoint_visibilities:
+              keypoint_visibilities
+      }
+      preprocess_options = [(preprocessor.random_horizontal_flip, {
+          'keypoint_flip_permutation': keypoint_flip_permutation
+      })]
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True,
+          include_keypoints=True,
+          include_keypoint_visibilities=True)
+      tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocess_options, func_arg_map=preprocessor_arg_map)
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+      masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
+      keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]
+      keypoint_visibilities = tensor_dict[
+          fields.InputDataFields.groundtruth_keypoint_visibilities]
+      return [boxes, masks, keypoints, keypoint_visibilities]
+
+    boxes, masks, keypoints, keypoint_visibilities = self.execute_cpu(
+        graph_fn, [])
+    self.assertIsNotNone(boxes)
+    self.assertIsNotNone(masks)
+    self.assertIsNotNone(keypoints)
+    self.assertIsNotNone(keypoint_visibilities)
 
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
+  def testRandomVerticalFlip(self):
 
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_diff_,
-       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,
-                                         boxes_diff, boxes_diff_expected])
-      self.assertAllClose(boxes_diff_, boxes_diff_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_vertical_flip, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createTestBoxes()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes
+      }
+      images_expected1 = self.expectedImagesAfterUpDownFlip()
+      boxes_expected1 = self.expectedBoxesAfterUpDownFlip()
+      images_expected2 = images
+      boxes_expected2 = boxes
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
+      boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+      boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
+      boxes_diff_expected = tf.zeros_like(boxes_diff)
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [
+          images_diff, images_diff_expected, boxes_diff, boxes_diff_expected
+      ]
+
+    (images_diff_, images_diff_expected_, boxes_diff_,
+     boxes_diff_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_diff_, boxes_diff_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomVerticalFlipWithEmptyBoxes(self):
-    preprocess_options = [(preprocessor.random_vertical_flip, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createEmptyTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterUpDownFlip()
-    boxes_expected = self.createEmptyTestBoxes()
-    images_expected2 = images
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
 
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_,
-       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,
-                                    boxes_expected])
-      self.assertAllClose(boxes_, boxes_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_vertical_flip, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createEmptyTestBoxes()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes
+      }
+      images_expected1 = self.expectedImagesAfterUpDownFlip()
+      boxes_expected = self.createEmptyTestBoxes()
+      images_expected2 = images
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [images_diff, images_diff_expected, boxes, boxes_expected]
+
+    (images_diff_, images_diff_expected_, boxes_,
+     boxes_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_, boxes_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomVerticalFlipWithCache(self):
     keypoint_flip_permutation = self.createKeypointFlipPermutation()
@@ -807,7 +816,7 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     images = tf.random_uniform([1, image_height, image_width, 3])
     boxes = self.createTestBoxes()
     masks = self.createTestMasks()
-    keypoints = self.createTestKeypoints()
+    keypoints, _ = self.createTestKeypoints()
     keypoint_flip_permutation = self.createKeypointFlipPermutation()
     tensor_dict = {
         fields.InputDataFields.image: images,
@@ -825,67 +834,73 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
     masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
     keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]
-    with self.test_session() as sess:
-      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])
-      self.assertTrue(boxes is not None)
-      self.assertTrue(masks is not None)
-      self.assertTrue(keypoints is not None)
+    self.assertIsNotNone(boxes)
+    self.assertIsNotNone(masks)
+    self.assertIsNotNone(keypoints)
 
   def testRandomRotation90(self):
-    preprocess_options = [(preprocessor.random_rotation90, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterRot90()
-    boxes_expected1 = self.expectedBoxesAfterRot90()
-    images_expected2 = images
-    boxes_expected2 = boxes
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
 
-    boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
-    boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
-    boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
-    boxes_diff_expected = tf.zeros_like(boxes_diff)
-
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
-
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_diff_,
-       boxes_diff_expected_) = sess.run([images_diff, images_diff_expected,
-                                         boxes_diff, boxes_diff_expected])
-      self.assertAllClose(boxes_diff_, boxes_diff_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_rotation90, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createTestBoxes()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes
+      }
+      images_expected1 = self.expectedImagesAfterRot90()
+      boxes_expected1 = self.expectedBoxesAfterRot90()
+      images_expected2 = images
+      boxes_expected2 = boxes
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      boxes_diff1 = tf.squared_difference(boxes, boxes_expected1)
+      boxes_diff2 = tf.squared_difference(boxes, boxes_expected2)
+      boxes_diff = tf.multiply(boxes_diff1, boxes_diff2)
+      boxes_diff_expected = tf.zeros_like(boxes_diff)
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [
+          images_diff, images_diff_expected, boxes_diff, boxes_diff_expected
+      ]
+
+    (images_diff_, images_diff_expected_, boxes_diff_,
+     boxes_diff_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_diff_, boxes_diff_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomRotation90WithEmptyBoxes(self):
-    preprocess_options = [(preprocessor.random_rotation90, {})]
-    images = self.expectedImagesAfterNormalization()
-    boxes = self.createEmptyTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    images_expected1 = self.expectedImagesAfterRot90()
-    boxes_expected = self.createEmptyTestBoxes()
-    images_expected2 = images
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images = tensor_dict[fields.InputDataFields.image]
-    boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-
-    images_diff1 = tf.squared_difference(images, images_expected1)
-    images_diff2 = tf.squared_difference(images, images_expected2)
-    images_diff = tf.multiply(images_diff1, images_diff2)
-    images_diff_expected = tf.zeros_like(images_diff)
 
-    with self.test_session() as sess:
-      (images_diff_, images_diff_expected_, boxes_,
-       boxes_expected_) = sess.run([images_diff, images_diff_expected, boxes,
-                                    boxes_expected])
-      self.assertAllClose(boxes_, boxes_expected_)
-      self.assertAllClose(images_diff_, images_diff_expected_)
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_rotation90, {})]
+      images = self.expectedImagesAfterNormalization()
+      boxes = self.createEmptyTestBoxes()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes
+      }
+      images_expected1 = self.expectedImagesAfterRot90()
+      boxes_expected = self.createEmptyTestBoxes()
+      images_expected2 = images
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images = tensor_dict[fields.InputDataFields.image]
+      boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+
+      images_diff1 = tf.squared_difference(images, images_expected1)
+      images_diff2 = tf.squared_difference(images, images_expected2)
+      images_diff = tf.multiply(images_diff1, images_diff2)
+      images_diff_expected = tf.zeros_like(images_diff)
+      return [images_diff, images_diff_expected, boxes, boxes_expected]
+
+    (images_diff_, images_diff_expected_, boxes_,
+     boxes_expected_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(boxes_, boxes_expected_)
+    self.assertAllClose(images_diff_, images_diff_expected_)
 
   def testRandomRotation90WithCache(self):
     preprocess_options = [(preprocessor.random_rotation90, {})]
@@ -901,7 +916,7 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     images = tf.random_uniform([1, image_height, image_width, 3])
     boxes = self.createTestBoxes()
     masks = self.createTestMasks()
-    keypoints = self.createTestKeypoints()
+    keypoints, _ = self.createTestKeypoints()
     tensor_dict = {
         fields.InputDataFields.image: images,
         fields.InputDataFields.groundtruth_boxes: boxes,
@@ -915,35 +930,36 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
     masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
     keypoints = tensor_dict[fields.InputDataFields.groundtruth_keypoints]
-    with self.test_session() as sess:
-      boxes, masks, keypoints = sess.run([boxes, masks, keypoints])
-      self.assertTrue(boxes is not None)
-      self.assertTrue(masks is not None)
-      self.assertTrue(keypoints is not None)
+    self.assertIsNotNone(boxes)
+    self.assertIsNotNone(masks)
+    self.assertIsNotNone(keypoints)
 
   def testRandomPixelValueScale(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_pixel_value_scale, {}))
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_min = tf.cast(images, dtype=tf.float32) * 0.9 / 255.0
-    images_max = tf.cast(images, dtype=tf.float32) * 1.1 / 255.0
-    images = tensor_dict[fields.InputDataFields.image]
-    values_greater = tf.greater_equal(images, images_min)
-    values_less = tf.less_equal(images, images_max)
-    values_true = tf.fill([1, 4, 4, 3], True)
-    with self.test_session() as sess:
-      (values_greater_, values_less_, values_true_) = sess.run(
-          [values_greater, values_less, values_true])
-      self.assertAllClose(values_greater_, values_true_)
-      self.assertAllClose(values_less_, values_true_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_pixel_value_scale, {}))
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images_min = tf.cast(images, dtype=tf.float32) * 0.9 / 255.0
+      images_max = tf.cast(images, dtype=tf.float32) * 1.1 / 255.0
+      images = tensor_dict[fields.InputDataFields.image]
+      values_greater = tf.greater_equal(images, images_min)
+      values_less = tf.less_equal(images, images_max)
+      values_true = tf.fill([1, 4, 4, 3], True)
+      return [values_greater, values_less, values_true]
+
+    (values_greater_, values_less_,
+     values_true_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(values_greater_, values_true_)
+    self.assertAllClose(values_less_, values_true_)
 
   def testRandomPixelValueScaleWithCache(self):
     preprocess_options = []
@@ -960,24 +976,27 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomImageScale(self):
-    preprocess_options = [(preprocessor.random_image_scale, {})]
-    images_original = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images_scaled = tensor_dict[fields.InputDataFields.image]
-    images_original_shape = tf.shape(images_original)
-    images_scaled_shape = tf.shape(images_scaled)
-    with self.test_session() as sess:
-      (images_original_shape_, images_scaled_shape_) = sess.run(
-          [images_original_shape, images_scaled_shape])
-      self.assertTrue(
-          images_original_shape_[1] * 0.5 <= images_scaled_shape_[1])
-      self.assertTrue(
-          images_original_shape_[1] * 2.0 >= images_scaled_shape_[1])
-      self.assertTrue(
-          images_original_shape_[2] * 0.5 <= images_scaled_shape_[2])
-      self.assertTrue(
-          images_original_shape_[2] * 2.0 >= images_scaled_shape_[2])
+
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_image_scale, {})]
+      images_original = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images_scaled = tensor_dict[fields.InputDataFields.image]
+      images_original_shape = tf.shape(images_original)
+      images_scaled_shape = tf.shape(images_scaled)
+      return [images_original_shape, images_scaled_shape]
+
+    (images_original_shape_,
+     images_scaled_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertLessEqual(images_original_shape_[1] * 0.5,
+                         images_scaled_shape_[1])
+    self.assertGreaterEqual(images_original_shape_[1] * 2.0,
+                            images_scaled_shape_[1])
+    self.assertLessEqual(images_original_shape_[2] * 0.5,
+                         images_scaled_shape_[2])
+    self.assertGreaterEqual(images_original_shape_[2] * 2.0,
+                            images_scaled_shape_[2])
 
   def testRandomImageScaleWithCache(self):
     preprocess_options = [(preprocessor.random_image_scale, {})]
@@ -987,43 +1006,46 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomRGBtoGray(self):
-    preprocess_options = [(preprocessor.random_rgb_to_gray, {})]
-    images_original = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
-    images_gray = tensor_dict[fields.InputDataFields.image]
-    images_gray_r, images_gray_g, images_gray_b = tf.split(
-        value=images_gray, num_or_size_splits=3, axis=3)
-    images_r, images_g, images_b = tf.split(
-        value=images_original, num_or_size_splits=3, axis=3)
-    images_r_diff1 = tf.squared_difference(
-        tf.cast(images_r, dtype=tf.float32),
-        tf.cast(images_gray_r, dtype=tf.float32))
-    images_r_diff2 = tf.squared_difference(
-        tf.cast(images_gray_r, dtype=tf.float32),
-        tf.cast(images_gray_g, dtype=tf.float32))
-    images_r_diff = tf.multiply(images_r_diff1, images_r_diff2)
-    images_g_diff1 = tf.squared_difference(
-        tf.cast(images_g, dtype=tf.float32),
-        tf.cast(images_gray_g, dtype=tf.float32))
-    images_g_diff2 = tf.squared_difference(
-        tf.cast(images_gray_g, dtype=tf.float32),
-        tf.cast(images_gray_b, dtype=tf.float32))
-    images_g_diff = tf.multiply(images_g_diff1, images_g_diff2)
-    images_b_diff1 = tf.squared_difference(
-        tf.cast(images_b, dtype=tf.float32),
-        tf.cast(images_gray_b, dtype=tf.float32))
-    images_b_diff2 = tf.squared_difference(
-        tf.cast(images_gray_b, dtype=tf.float32),
-        tf.cast(images_gray_r, dtype=tf.float32))
-    images_b_diff = tf.multiply(images_b_diff1, images_b_diff2)
-    image_zero1 = tf.constant(0, dtype=tf.float32, shape=[1, 4, 4, 1])
-    with self.test_session() as sess:
-      (images_r_diff_, images_g_diff_, images_b_diff_, image_zero1_) = sess.run(
-          [images_r_diff, images_g_diff, images_b_diff, image_zero1])
-      self.assertAllClose(images_r_diff_, image_zero1_)
-      self.assertAllClose(images_g_diff_, image_zero1_)
-      self.assertAllClose(images_b_diff_, image_zero1_)
+
+    def graph_fn():
+      preprocess_options = [(preprocessor.random_rgb_to_gray, {})]
+      images_original = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocess_options)
+      images_gray = tensor_dict[fields.InputDataFields.image]
+      images_gray_r, images_gray_g, images_gray_b = tf.split(
+          value=images_gray, num_or_size_splits=3, axis=3)
+      images_r, images_g, images_b = tf.split(
+          value=images_original, num_or_size_splits=3, axis=3)
+      images_r_diff1 = tf.squared_difference(
+          tf.cast(images_r, dtype=tf.float32),
+          tf.cast(images_gray_r, dtype=tf.float32))
+      images_r_diff2 = tf.squared_difference(
+          tf.cast(images_gray_r, dtype=tf.float32),
+          tf.cast(images_gray_g, dtype=tf.float32))
+      images_r_diff = tf.multiply(images_r_diff1, images_r_diff2)
+      images_g_diff1 = tf.squared_difference(
+          tf.cast(images_g, dtype=tf.float32),
+          tf.cast(images_gray_g, dtype=tf.float32))
+      images_g_diff2 = tf.squared_difference(
+          tf.cast(images_gray_g, dtype=tf.float32),
+          tf.cast(images_gray_b, dtype=tf.float32))
+      images_g_diff = tf.multiply(images_g_diff1, images_g_diff2)
+      images_b_diff1 = tf.squared_difference(
+          tf.cast(images_b, dtype=tf.float32),
+          tf.cast(images_gray_b, dtype=tf.float32))
+      images_b_diff2 = tf.squared_difference(
+          tf.cast(images_gray_b, dtype=tf.float32),
+          tf.cast(images_gray_r, dtype=tf.float32))
+      images_b_diff = tf.multiply(images_b_diff1, images_b_diff2)
+      image_zero1 = tf.constant(0, dtype=tf.float32, shape=[1, 4, 4, 1])
+      return [images_r_diff, images_g_diff, images_b_diff, image_zero1]
+
+    (images_r_diff_, images_g_diff_, images_b_diff_,
+     image_zero1_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(images_r_diff_, image_zero1_)
+    self.assertAllClose(images_g_diff_, image_zero1_)
+    self.assertAllClose(images_b_diff_, image_zero1_)
 
   def testRandomRGBtoGrayWithCache(self):
     preprocess_options = [(
@@ -1034,24 +1056,27 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomAdjustBrightness(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_adjust_brightness, {}))
-    images_original = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_bright = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_bright_shape = tf.shape(images_bright)
-    with self.test_session() as sess:
-      (image_original_shape_, image_bright_shape_) = sess.run(
-          [image_original_shape, image_bright_shape])
-      self.assertAllEqual(image_original_shape_, image_bright_shape_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_adjust_brightness, {}))
+      images_original = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images_bright = tensor_dict[fields.InputDataFields.image]
+      image_original_shape = tf.shape(images_original)
+      image_bright_shape = tf.shape(images_bright)
+      return [image_original_shape, image_bright_shape]
+
+    (image_original_shape_,
+     image_bright_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(image_original_shape_, image_bright_shape_)
 
   def testRandomAdjustBrightnessWithCache(self):
     preprocess_options = []
@@ -1068,24 +1093,27 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomAdjustContrast(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_adjust_contrast, {}))
-    images_original = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_contrast = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_contrast_shape = tf.shape(images_contrast)
-    with self.test_session() as sess:
-      (image_original_shape_, image_contrast_shape_) = sess.run(
-          [image_original_shape, image_contrast_shape])
-      self.assertAllEqual(image_original_shape_, image_contrast_shape_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_adjust_contrast, {}))
+      images_original = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images_contrast = tensor_dict[fields.InputDataFields.image]
+      image_original_shape = tf.shape(images_original)
+      image_contrast_shape = tf.shape(images_contrast)
+      return [image_original_shape, image_contrast_shape]
+
+    (image_original_shape_,
+     image_contrast_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(image_original_shape_, image_contrast_shape_)
 
   def testRandomAdjustContrastWithCache(self):
     preprocess_options = []
@@ -1102,24 +1130,26 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomAdjustHue(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_adjust_hue, {}))
-    images_original = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_hue = tensor_dict[fields.InputDataFields.image]
-    image_original_shape = tf.shape(images_original)
-    image_hue_shape = tf.shape(images_hue)
-    with self.test_session() as sess:
-      (image_original_shape_, image_hue_shape_) = sess.run(
-          [image_original_shape, image_hue_shape])
-      self.assertAllEqual(image_original_shape_, image_hue_shape_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_adjust_hue, {}))
+      images_original = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images_hue = tensor_dict[fields.InputDataFields.image]
+      image_original_shape = tf.shape(images_original)
+      image_hue_shape = tf.shape(images_hue)
+      return [image_original_shape, image_hue_shape]
+
+    (image_original_shape_, image_hue_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(image_original_shape_, image_hue_shape_)
 
   def testRandomAdjustHueWithCache(self):
     preprocess_options = []
@@ -1136,24 +1166,27 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomDistortColor(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_distort_color, {}))
-    images_original = self.createTestImages()
-    images_original_shape = tf.shape(images_original)
-    tensor_dict = {fields.InputDataFields.image: images_original}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_distorted_color = tensor_dict[fields.InputDataFields.image]
-    images_distorted_color_shape = tf.shape(images_distorted_color)
-    with self.test_session() as sess:
-      (images_original_shape_, images_distorted_color_shape_) = sess.run(
-          [images_original_shape, images_distorted_color_shape])
-      self.assertAllEqual(images_original_shape_, images_distorted_color_shape_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_distort_color, {}))
+      images_original = self.createTestImages()
+      images_original_shape = tf.shape(images_original)
+      tensor_dict = {fields.InputDataFields.image: images_original}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images_distorted_color = tensor_dict[fields.InputDataFields.image]
+      images_distorted_color_shape = tf.shape(images_distorted_color)
+      return [images_original_shape, images_distorted_color_shape]
+
+    (images_original_shape_,
+     images_distorted_color_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_original_shape_, images_distorted_color_shape_)
 
   def testRandomDistortColorWithCache(self):
     preprocess_options = []
@@ -1170,57 +1203,59 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomJitterBoxes(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.random_jitter_boxes, {}))
-    boxes = self.createTestBoxes()
-    boxes_shape = tf.shape(boxes)
-    tensor_dict = {fields.InputDataFields.groundtruth_boxes: boxes}
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    distorted_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-    distorted_boxes_shape = tf.shape(distorted_boxes)
 
-    with self.test_session() as sess:
-      (boxes_shape_, distorted_boxes_shape_) = sess.run(
-          [boxes_shape, distorted_boxes_shape])
-      self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.random_jitter_boxes, {}))
+      boxes = self.createTestBoxes()
+      boxes_shape = tf.shape(boxes)
+      tensor_dict = {fields.InputDataFields.groundtruth_boxes: boxes}
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      distorted_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
+      distorted_boxes_shape = tf.shape(distorted_boxes)
+      return [boxes_shape, distorted_boxes_shape]
+
+    (boxes_shape_, distorted_boxes_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)
 
   def testRandomCropImage(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_crop_image, {}))
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    self.assertEqual(3, distorted_images.get_shape()[3])
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = sess.run([
-           boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
-       ])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_crop_image, {}))
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      return [
+          boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
+      ]
+
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testRandomCropImageWithCache(self):
     preprocess_options = [(preprocessor.random_rgb_to_gray,
@@ -1238,171 +1273,175 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRandomCropImageGrayscale(self):
-    preprocessing_options = [(preprocessor.rgb_to_gray, {}),
-                             (preprocessor.normalize_image, {
-                                 'original_minval': 0,
-                                 'original_maxval': 255,
-                                 'target_minval': 0,
-                                 'target_maxval': 1,
-                             }),
-                             (preprocessor.random_crop_image, {})]
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    self.assertEqual(1, distorted_images.get_shape()[3])
-
-    with self.test_session() as sess:
-      session_results = sess.run([
+
+    def graph_fn():
+      preprocessing_options = [(preprocessor.rgb_to_gray, {}),
+                               (preprocessor.normalize_image, {
+                                   'original_minval': 0,
+                                   'original_maxval': 255,
+                                   'target_minval': 0,
+                                   'target_maxval': 1,
+                               }), (preprocessor.random_crop_image, {})]
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      return [
           boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
-      ])
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = session_results
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+      ]
+
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testRandomCropImageWithBoxOutOfImage(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_crop_image, {}))
-    images = self.createTestImages()
-    boxes = self.createTestBoxesOutOfImage()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        }
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = sess.run(
-           [boxes_rank, distorted_boxes_rank, images_rank,
-            distorted_images_rank])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_crop_image, {}))
+      images = self.createTestImages()
+      boxes = self.createTestBoxesOutOfImage()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      return [
+          boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
+      ]
+
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testRandomCropImageWithRandomCoefOne(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
 
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images = tensor_dict[fields.InputDataFields.image]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images = tensor_dict[fields.InputDataFields.image]
 
-    preprocessing_options = [(preprocessor.random_crop_image, {
-        'random_coef': 1.0
-    })]
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
+      preprocessing_options = [(preprocessor.random_crop_image, {
+          'random_coef': 1.0
+      })]
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
 
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_labels = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_classes]
-    distorted_weights = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_weights]
-    boxes_shape = tf.shape(boxes)
-    distorted_boxes_shape = tf.shape(distorted_boxes)
-    images_shape = tf.shape(images)
-    distorted_images_shape = tf.shape(distorted_images)
-
-    with self.test_session() as sess:
-      (boxes_shape_, distorted_boxes_shape_, images_shape_,
-       distorted_images_shape_, images_, distorted_images_,
-       boxes_, distorted_boxes_, labels_, distorted_labels_,
-       weights_, distorted_weights_) = sess.run(
-           [boxes_shape, distorted_boxes_shape, images_shape,
-            distorted_images_shape, images, distorted_images,
-            boxes, distorted_boxes, labels, distorted_labels,
-            weights, distorted_weights])
-      self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)
-      self.assertAllEqual(images_shape_, distorted_images_shape_)
-      self.assertAllClose(images_, distorted_images_)
-      self.assertAllClose(boxes_, distorted_boxes_)
-      self.assertAllEqual(labels_, distorted_labels_)
-      self.assertAllEqual(weights_, distorted_weights_)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_labels = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_classes]
+      distorted_weights = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_weights]
+      boxes_shape = tf.shape(boxes)
+      distorted_boxes_shape = tf.shape(distorted_boxes)
+      images_shape = tf.shape(images)
+      distorted_images_shape = tf.shape(distorted_images)
+      return [
+          boxes_shape, distorted_boxes_shape, images_shape,
+          distorted_images_shape, images, distorted_images, boxes,
+          distorted_boxes, labels, distorted_labels, weights, distorted_weights
+      ]
+
+    (boxes_shape_, distorted_boxes_shape_, images_shape_,
+     distorted_images_shape_, images_, distorted_images_, boxes_,
+     distorted_boxes_, labels_, distorted_labels_, weights_,
+     distorted_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_shape_, distorted_boxes_shape_)
+    self.assertAllEqual(images_shape_, distorted_images_shape_)
+    self.assertAllClose(images_, distorted_images_)
+    self.assertAllClose(boxes_, distorted_boxes_)
+    self.assertAllEqual(labels_, distorted_labels_)
+    self.assertAllEqual(weights_, distorted_weights_)
 
   def testRandomCropWithMockSampleDistortedBoundingBox(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createColorfulTestImage()
+      boxes = tf.constant([[0.1, 0.1, 0.8, 0.3], [0.2, 0.4, 0.75, 0.75],
+                           [0.3, 0.1, 0.4, 0.7]],
+                          dtype=tf.float32)
+      labels = tf.constant([1, 7, 11], dtype=tf.int32)
+      weights = tf.constant([1.0, 0.5, 0.6], dtype=tf.float32)
 
-    images = self.createColorfulTestImage()
-    boxes = tf.constant([[0.1, 0.1, 0.8, 0.3],
-                         [0.2, 0.4, 0.75, 0.75],
-                         [0.3, 0.1, 0.4, 0.7]], dtype=tf.float32)
-    labels = tf.constant([1, 7, 11], dtype=tf.int32)
-    weights = tf.constant([1.0, 0.5, 0.6], dtype=tf.float32)
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict,
+                                            preprocessing_options)
+      images = tensor_dict[fields.InputDataFields.image]
 
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images = tensor_dict[fields.InputDataFields.image]
-
-    preprocessing_options = [(preprocessor.random_crop_image, {})]
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box') as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (tf.constant(
-          [6, 143, 0], dtype=tf.int32), tf.constant(
-              [190, 237, -1], dtype=tf.int32), tf.constant(
-                  [[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+      preprocessing_options = [(preprocessor.random_crop_image, {})]
 
-      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                      preprocessing_options)
+      with mock.patch.object(tf.image, 'sample_distorted_bounding_box'
+                            ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (tf.constant(
+            [6, 143, 0], dtype=tf.int32), tf.constant(
+                [190, 237, -1], dtype=tf.int32), tf.constant(
+                    [[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                        preprocessing_options)
 
       distorted_boxes = distorted_tensor_dict[
           fields.InputDataFields.groundtruth_boxes]
@@ -1410,66 +1449,68 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
           fields.InputDataFields.groundtruth_classes]
       distorted_weights = distorted_tensor_dict[
           fields.InputDataFields.groundtruth_weights]
-      expected_boxes = tf.constant([[0.178947, 0.07173, 0.75789469, 0.66244733],
-                                    [0.28421, 0.0, 0.38947365, 0.57805908]],
-                                   dtype=tf.float32)
+      expected_boxes = tf.constant(
+          [[0.178947, 0.07173, 0.75789469, 0.66244733],
+           [0.28421, 0.0, 0.38947365, 0.57805908]],
+          dtype=tf.float32)
       expected_labels = tf.constant([7, 11], dtype=tf.int32)
       expected_weights = tf.constant([0.5, 0.6], dtype=tf.float32)
+      return [
+          distorted_boxes, distorted_labels, distorted_weights,
+          expected_boxes, expected_labels, expected_weights
+      ]
 
-      with self.test_session() as sess:
-        (distorted_boxes_, distorted_labels_, distorted_weights_,
-         expected_boxes_, expected_labels_, expected_weights_) = sess.run(
-             [distorted_boxes, distorted_labels, distorted_weights,
-              expected_boxes, expected_labels, expected_weights])
-        self.assertAllClose(distorted_boxes_, expected_boxes_)
-        self.assertAllEqual(distorted_labels_, expected_labels_)
-        self.assertAllEqual(distorted_weights_, expected_weights_)
+    (distorted_boxes_, distorted_labels_, distorted_weights_, expected_boxes_,
+     expected_labels_, expected_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(distorted_boxes_, expected_boxes_)
+    self.assertAllEqual(distorted_labels_, expected_labels_)
+    self.assertAllEqual(distorted_weights_, expected_weights_)
 
   def testRandomCropWithoutClipBoxes(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
 
-    images = self.createColorfulTestImage()
-    boxes = tf.constant([[0.1, 0.1, 0.8, 0.3],
-                         [0.2, 0.4, 0.75, 0.75],
-                         [0.3, 0.1, 0.4, 0.7]], dtype=tf.float32)
-    keypoints = tf.constant([
-        [[0.1, 0.1], [0.8, 0.3]],
-        [[0.2, 0.4], [0.75, 0.75]],
-        [[0.3, 0.1], [0.4, 0.7]],
-    ], dtype=tf.float32)
-    labels = tf.constant([1, 7, 11], dtype=tf.int32)
-    weights = tf.constant([1.0, 0.5, 0.6], dtype=tf.float32)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createColorfulTestImage()
+      boxes = tf.constant([[0.1, 0.1, 0.8, 0.3],
+                           [0.2, 0.4, 0.75, 0.75],
+                           [0.3, 0.1, 0.4, 0.7]], dtype=tf.float32)
+      keypoints = tf.constant([
+          [[0.1, 0.1], [0.8, 0.3]],
+          [[0.2, 0.4], [0.75, 0.75]],
+          [[0.3, 0.1], [0.4, 0.7]],
+      ], dtype=tf.float32)
+      labels = tf.constant([1, 7, 11], dtype=tf.int32)
+      weights = tf.constant([1.0, 0.5, 0.6], dtype=tf.float32)
 
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_keypoints: keypoints,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-
-    preprocessing_options = [(preprocessor.random_crop_image, {
-        'clip_boxes': False,
-    })]
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box') as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (tf.constant(
-          [6, 143, 0], dtype=tf.int32), tf.constant(
-              [190, 237, -1], dtype=tf.int32), tf.constant(
-                  [[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_keypoints: keypoints,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
 
+      preprocessing_options = [(preprocessor.random_crop_image, {
+          'clip_boxes': False,
+      })]
       preprocessor_arg_map = preprocessor.get_default_func_arg_map(
           include_keypoints=True)
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-
+      with mock.patch.object(tf.image, 'sample_distorted_bounding_box'
+                            ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (tf.constant(
+            [6, 143, 0], dtype=tf.int32), tf.constant(
+                [190, 237, -1], dtype=tf.int32), tf.constant(
+                    [[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict, preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
       distorted_boxes = distorted_tensor_dict[
           fields.InputDataFields.groundtruth_boxes]
       distorted_keypoints = distorted_tensor_dict[
@@ -1488,444 +1529,450 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
           dtype=tf.float32)
       expected_labels = tf.constant([7, 11], dtype=tf.int32)
       expected_weights = tf.constant([0.5, 0.6], dtype=tf.float32)
-
-      with self.test_session() as sess:
-        (distorted_boxes_, distorted_keypoints_, distorted_labels_,
-         distorted_weights_, expected_boxes_, expected_keypoints_,
-         expected_labels_, expected_weights_) = sess.run(
-             [distorted_boxes, distorted_keypoints, distorted_labels,
+      return [distorted_boxes, distorted_keypoints, distorted_labels,
               distorted_weights, expected_boxes, expected_keypoints,
-              expected_labels, expected_weights])
-        self.assertAllClose(distorted_boxes_, expected_boxes_)
-        self.assertAllClose(distorted_keypoints_, expected_keypoints_)
-        self.assertAllEqual(distorted_labels_, expected_labels_)
-        self.assertAllEqual(distorted_weights_, expected_weights_)
+              expected_labels, expected_weights]
+
+    (distorted_boxes_, distorted_keypoints_, distorted_labels_,
+     distorted_weights_, expected_boxes_, expected_keypoints_, expected_labels_,
+     expected_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(distorted_boxes_, expected_boxes_)
+    self.assertAllClose(distorted_keypoints_, expected_keypoints_)
+    self.assertAllEqual(distorted_labels_, expected_labels_)
+    self.assertAllEqual(distorted_weights_, expected_weights_)
 
   def testRandomCropImageWithMultiClassScores(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_crop_image, {}))
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    multiclass_scores = self.createTestMultiClassScores()
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_crop_image, {}))
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      multiclass_scores = self.createTestMultiClassScores()
 
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.multiclass_scores: multiclass_scores
-    }
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_multiclass_scores = distorted_tensor_dict[
-        fields.InputDataFields.multiclass_scores]
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    multiclass_scores_rank = tf.rank(multiclass_scores)
-    distorted_multiclass_scores_rank = tf.rank(distorted_multiclass_scores)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_, multiclass_scores_rank_,
-       distorted_multiclass_scores_rank_,
-       distorted_multiclass_scores_) = sess.run([
-           boxes_rank, distorted_boxes, distorted_boxes_rank, images_rank,
-           distorted_images_rank, multiclass_scores_rank,
-           distorted_multiclass_scores_rank, distorted_multiclass_scores
-       ])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
-      self.assertAllEqual(multiclass_scores_rank_,
-                          distorted_multiclass_scores_rank_)
-      self.assertAllEqual(distorted_boxes_.shape[0],
-                          distorted_multiclass_scores_.shape[0])
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.multiclass_scores: multiclass_scores
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_multiclass_scores = distorted_tensor_dict[
+          fields.InputDataFields.multiclass_scores]
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      multiclass_scores_rank = tf.rank(multiclass_scores)
+      distorted_multiclass_scores_rank = tf.rank(distorted_multiclass_scores)
+      return [
+          boxes_rank, distorted_boxes, distorted_boxes_rank, images_rank,
+          distorted_images_rank, multiclass_scores_rank,
+          distorted_multiclass_scores_rank, distorted_multiclass_scores
+      ]
+
+    (boxes_rank_, distorted_boxes_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_, multiclass_scores_rank_,
+     distorted_multiclass_scores_rank_,
+     distorted_multiclass_scores_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
+    self.assertAllEqual(multiclass_scores_rank_,
+                        distorted_multiclass_scores_rank_)
+    self.assertAllEqual(distorted_boxes_.shape[0],
+                        distorted_multiclass_scores_.shape[0])
 
   def testStrictRandomCropImageWithGroundtruthWeights(self):
-    image = self.createColorfulTestImage()[0]
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      new_image, new_boxes, new_labels, new_groundtruth_weights = (
-          preprocessor._strict_random_crop_image(
-              image, boxes, labels, weights))
-      with self.test_session() as sess:
+    def graph_fn():
+      image = self.createColorfulTestImage()[0]
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
         new_image, new_boxes, new_labels, new_groundtruth_weights = (
-            sess.run(
-                [new_image, new_boxes, new_labels, new_groundtruth_weights])
-        )
-
-        expected_boxes = np.array(
-            [[0.0, 0.0, 0.75789469, 1.0],
-             [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)
-        self.assertAllEqual(new_image.shape, [190, 237, 3])
-        self.assertAllEqual(new_groundtruth_weights, [1.0, 0.5])
-        self.assertAllClose(
-            new_boxes.flatten(), expected_boxes.flatten())
+            preprocessor._strict_random_crop_image(
+                image, boxes, labels, weights))
+        return [new_image, new_boxes, new_labels, new_groundtruth_weights]
+    (new_image, new_boxes, _,
+     new_groundtruth_weights) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array(
+        [[0.0, 0.0, 0.75789469, 1.0],
+         [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)
+    self.assertAllEqual(new_image.shape, [190, 237, 3])
+    self.assertAllEqual(new_groundtruth_weights, [1.0, 0.5])
+    self.assertAllClose(
+        new_boxes.flatten(), expected_boxes.flatten())
 
   def testStrictRandomCropImageWithMasks(self):
-    image = self.createColorfulTestImage()[0]
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      new_image, new_boxes, new_labels, new_weights, new_masks = (
-          preprocessor._strict_random_crop_image(
-              image, boxes, labels, weights, masks=masks))
-      with self.test_session() as sess:
-        new_image, new_boxes, new_labels, new_weights, new_masks = sess.run(
-            [new_image, new_boxes, new_labels, new_weights, new_masks])
-        expected_boxes = np.array(
-            [[0.0, 0.0, 0.75789469, 1.0],
-             [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)
-        self.assertAllEqual(new_image.shape, [190, 237, 3])
-        self.assertAllEqual(new_masks.shape, [2, 190, 237])
-        self.assertAllClose(
-            new_boxes.flatten(), expected_boxes.flatten())
+    def graph_fn():
+      image = self.createColorfulTestImage()[0]
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        new_image, new_boxes, new_labels, new_weights, new_masks = (
+            preprocessor._strict_random_crop_image(
+                image, boxes, labels, weights, masks=masks))
+        return [new_image, new_boxes, new_labels, new_weights, new_masks]
+    (new_image, new_boxes, _, _,
+     new_masks) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array(
+        [[0.0, 0.0, 0.75789469, 1.0],
+         [0.23157893, 0.24050637, 0.75789469, 1.0]], dtype=np.float32)
+    self.assertAllEqual(new_image.shape, [190, 237, 3])
+    self.assertAllEqual(new_masks.shape, [2, 190, 237])
+    self.assertAllClose(
+        new_boxes.flatten(), expected_boxes.flatten())
 
   def testStrictRandomCropImageWithKeypoints(self):
-    image = self.createColorfulTestImage()[0]
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypoints()
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      new_image, new_boxes, new_labels, new_weights, new_keypoints = (
-          preprocessor._strict_random_crop_image(
-              image, boxes, labels, weights, keypoints=keypoints))
-      with self.test_session() as sess:
-        new_image, new_boxes, new_labels, new_weights, new_keypoints = sess.run(
-            [new_image, new_boxes, new_labels, new_weights, new_keypoints])
-
-        expected_boxes = np.array([
-            [0.0, 0.0, 0.75789469, 1.0],
-            [0.23157893, 0.24050637, 0.75789469, 1.0],], dtype=np.float32)
-        expected_keypoints = np.array([
-            [[np.nan, np.nan],
-             [np.nan, np.nan],
-             [np.nan, np.nan]],
-            [[0.38947368, 0.07173],
-             [0.49473682, 0.24050637],
-             [0.60000002, 0.40928277]]
-        ], dtype=np.float32)
-        self.assertAllEqual(new_image.shape, [190, 237, 3])
-        self.assertAllClose(
-            new_boxes.flatten(), expected_boxes.flatten())
-        self.assertAllClose(
-            new_keypoints.flatten(), expected_keypoints.flatten())
+    def graph_fn():
+      image = self.createColorfulTestImage()[0]
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints, keypoint_visibilities = self.createTestKeypoints()
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        (new_image, new_boxes, new_labels, new_weights, new_keypoints,
+         new_keypoint_visibilities) = preprocessor._strict_random_crop_image(
+             image, boxes, labels, weights, keypoints=keypoints,
+             keypoint_visibilities=keypoint_visibilities)
+        return [new_image, new_boxes, new_labels, new_weights, new_keypoints,
+                new_keypoint_visibilities]
+    (new_image, new_boxes, _, _, new_keypoints,
+     new_keypoint_visibilities) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array([
+        [0.0, 0.0, 0.75789469, 1.0],
+        [0.23157893, 0.24050637, 0.75789469, 1.0],], dtype=np.float32)
+    expected_keypoints = np.array([
+        [[np.nan, np.nan],
+         [np.nan, np.nan],
+         [np.nan, np.nan]],
+        [[0.38947368, 0.07173],
+         [0.49473682, 0.24050637],
+         [0.60000002, 0.40928277]]
+    ], dtype=np.float32)
+    expected_keypoint_visibilities = [
+        [False, False, False],
+        [False, True, True]
+    ]
+    self.assertAllEqual(new_image.shape, [190, 237, 3])
+    self.assertAllClose(
+        new_boxes, expected_boxes)
+    self.assertAllClose(
+        new_keypoints, expected_keypoints)
+    self.assertAllEqual(
+        new_keypoint_visibilities, expected_keypoint_visibilities)
 
   def testRunRandomCropImageWithMasks(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_instance_masks: masks,
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_instance_masks: masks,
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=True)
-
-    preprocessing_options = [(preprocessor.random_crop_image, {})]
-
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-      distorted_boxes = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_boxes]
-      distorted_labels = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_classes]
-      distorted_masks = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_instance_masks]
-      with self.test_session() as sess:
-        (distorted_image_, distorted_boxes_, distorted_labels_,
-         distorted_masks_) = sess.run(
-             [distorted_image, distorted_boxes, distorted_labels,
-              distorted_masks])
-
-        expected_boxes = np.array([
-            [0.0, 0.0, 0.75789469, 1.0],
-            [0.23157893, 0.24050637, 0.75789469, 1.0],
-        ], dtype=np.float32)
-        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
-        self.assertAllEqual(distorted_masks_.shape, [2, 190, 237])
-        self.assertAllEqual(distorted_labels_, [1, 2])
-        self.assertAllClose(
-            distorted_boxes_.flatten(), expected_boxes.flatten())
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True)
+
+      preprocessing_options = [(preprocessor.random_crop_image, {})]
+
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict,
+            preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
+        distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+        distorted_boxes = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_boxes]
+        distorted_labels = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_classes]
+        distorted_masks = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_instance_masks]
+        return [distorted_image, distorted_boxes, distorted_labels,
+                distorted_masks]
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_masks_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array([
+        [0.0, 0.0, 0.75789469, 1.0],
+        [0.23157893, 0.24050637, 0.75789469, 1.0],
+    ], dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
+    self.assertAllEqual(distorted_masks_.shape, [2, 190, 237])
+    self.assertAllEqual(distorted_labels_, [1, 2])
+    self.assertAllClose(
+        distorted_boxes_.flatten(), expected_boxes.flatten())
 
   def testRunRandomCropImageWithKeypointsInsideCrop(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypointsInsideCrop()
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints = self.createTestKeypointsInsideCrop()
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_keypoints: keypoints,
-        fields.InputDataFields.groundtruth_weights: weights
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_keypoints: keypoints,
+          fields.InputDataFields.groundtruth_weights: weights
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_keypoints=True)
-
-    preprocessing_options = [(preprocessor.random_crop_image, {})]
-
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-      distorted_boxes = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_boxes]
-      distorted_labels = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_classes]
-      distorted_keypoints = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_keypoints]
-      with self.test_session() as sess:
-        (distorted_image_, distorted_boxes_, distorted_labels_,
-         distorted_keypoints_) = sess.run(
-             [distorted_image, distorted_boxes, distorted_labels,
-              distorted_keypoints])
-
-        expected_boxes = np.array([
-            [0.0, 0.0, 0.75789469, 1.0],
-            [0.23157893, 0.24050637, 0.75789469, 1.0],
-        ], dtype=np.float32)
-        expected_keypoints = np.array([
-            [[0.38947368, 0.07173],
-             [0.49473682, 0.24050637],
-             [0.60000002, 0.40928277]],
-            [[0.38947368, 0.07173],
-             [0.49473682, 0.24050637],
-             [0.60000002, 0.40928277]]
-        ])
-        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
-        self.assertAllEqual(distorted_labels_, [1, 2])
-        self.assertAllClose(
-            distorted_boxes_.flatten(), expected_boxes.flatten())
-        self.assertAllClose(
-            distorted_keypoints_.flatten(), expected_keypoints.flatten())
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_keypoints=True)
+
+      preprocessing_options = [(preprocessor.random_crop_image, {})]
+
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict,
+            preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
+        distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+        distorted_boxes = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_boxes]
+        distorted_labels = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_classes]
+        distorted_keypoints = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_keypoints]
+        return [distorted_image, distorted_boxes, distorted_labels,
+                distorted_keypoints]
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_keypoints_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array([
+        [0.0, 0.0, 0.75789469, 1.0],
+        [0.23157893, 0.24050637, 0.75789469, 1.0],
+    ], dtype=np.float32)
+    expected_keypoints = np.array([
+        [[0.38947368, 0.07173],
+         [0.49473682, 0.24050637],
+         [0.60000002, 0.40928277]],
+        [[0.38947368, 0.07173],
+         [0.49473682, 0.24050637],
+         [0.60000002, 0.40928277]]
+    ])
+    self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
+    self.assertAllEqual(distorted_labels_, [1, 2])
+    self.assertAllClose(
+        distorted_boxes_.flatten(), expected_boxes.flatten())
+    self.assertAllClose(
+        distorted_keypoints_.flatten(), expected_keypoints.flatten())
 
   def testRunRandomCropImageWithKeypointsOutsideCrop(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypointsOutsideCrop()
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints = self.createTestKeypointsOutsideCrop()
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_keypoints: keypoints
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_keypoints: keypoints
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_keypoints=True)
-
-    preprocessing_options = [(preprocessor.random_crop_image, {})]
-
-    with mock.patch.object(
-        tf.image,
-        'sample_distorted_bounding_box'
-    ) as mock_sample_distorted_bounding_box:
-      mock_sample_distorted_bounding_box.return_value = (
-          tf.constant([6, 143, 0], dtype=tf.int32),
-          tf.constant([190, 237, -1], dtype=tf.int32),
-          tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-      distorted_boxes = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_boxes]
-      distorted_labels = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_classes]
-      distorted_keypoints = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_keypoints]
-      with self.test_session() as sess:
-        (distorted_image_, distorted_boxes_, distorted_labels_,
-         distorted_keypoints_) = sess.run(
-             [distorted_image, distorted_boxes, distorted_labels,
-              distorted_keypoints])
-
-        expected_boxes = np.array([
-            [0.0, 0.0, 0.75789469, 1.0],
-            [0.23157893, 0.24050637, 0.75789469, 1.0],
-        ], dtype=np.float32)
-        expected_keypoints = np.array([
-            [[np.nan, np.nan],
-             [np.nan, np.nan],
-             [np.nan, np.nan]],
-            [[np.nan, np.nan],
-             [np.nan, np.nan],
-             [np.nan, np.nan]],
-        ])
-        self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
-        self.assertAllEqual(distorted_labels_, [1, 2])
-        self.assertAllClose(
-            distorted_boxes_.flatten(), expected_boxes.flatten())
-        self.assertAllClose(
-            distorted_keypoints_.flatten(), expected_keypoints.flatten())
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_keypoints=True)
+
+      preprocessing_options = [(preprocessor.random_crop_image, {})]
+
+      with mock.patch.object(
+          tf.image,
+          'sample_distorted_bounding_box'
+      ) as mock_sample_distorted_bounding_box:
+        mock_sample_distorted_bounding_box.return_value = (
+            tf.constant([6, 143, 0], dtype=tf.int32),
+            tf.constant([190, 237, -1], dtype=tf.int32),
+            tf.constant([[[0.03, 0.3575, 0.98, 0.95]]], dtype=tf.float32))
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict,
+            preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
+        distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+        distorted_boxes = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_boxes]
+        distorted_labels = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_classes]
+        distorted_keypoints = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_keypoints]
+        return [distorted_image, distorted_boxes, distorted_labels,
+                distorted_keypoints]
+      (distorted_image_, distorted_boxes_, distorted_labels_,
+       distorted_keypoints_) = self.execute_cpu(graph_fn, [])
+
+      expected_boxes = np.array([
+          [0.0, 0.0, 0.75789469, 1.0],
+          [0.23157893, 0.24050637, 0.75789469, 1.0],
+      ], dtype=np.float32)
+      expected_keypoints = np.array([
+          [[np.nan, np.nan],
+           [np.nan, np.nan],
+           [np.nan, np.nan]],
+          [[np.nan, np.nan],
+           [np.nan, np.nan],
+           [np.nan, np.nan]],
+      ])
+      self.assertAllEqual(distorted_image_.shape, [1, 190, 237, 3])
+      self.assertAllEqual(distorted_labels_, [1, 2])
+      self.assertAllClose(
+          distorted_boxes_.flatten(), expected_boxes.flatten())
+      self.assertAllClose(
+          distorted_keypoints_.flatten(), expected_keypoints.flatten())
 
   def testRunRetainBoxesAboveThreshold(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
 
-    tensor_dict = {
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
+      tensor_dict = {
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
 
-    preprocessing_options = [
-        (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
-    ]
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map()
-    retained_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    retained_boxes = retained_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    retained_labels = retained_tensor_dict[
-        fields.InputDataFields.groundtruth_classes]
-    retained_weights = retained_tensor_dict[
-        fields.InputDataFields.groundtruth_weights]
-
-    with self.test_session() as sess:
-      (retained_boxes_, retained_labels_,
-       retained_weights_, expected_retained_boxes_,
-       expected_retained_labels_, expected_retained_weights_) = sess.run(
-           [retained_boxes, retained_labels, retained_weights,
-            self.expectedBoxesAfterThresholding(),
-            self.expectedLabelsAfterThresholding(),
-            self.expectedLabelScoresAfterThresholding()])
-
-      self.assertAllClose(retained_boxes_, expected_retained_boxes_)
-      self.assertAllClose(retained_labels_, expected_retained_labels_)
-      self.assertAllClose(
-          retained_weights_, expected_retained_weights_)
+      preprocessing_options = [
+          (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
+      ]
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map()
+      retained_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      retained_boxes = retained_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      retained_labels = retained_tensor_dict[
+          fields.InputDataFields.groundtruth_classes]
+      retained_weights = retained_tensor_dict[
+          fields.InputDataFields.groundtruth_weights]
+      return [retained_boxes, retained_labels, retained_weights,
+              self.expectedBoxesAfterThresholding(),
+              self.expectedLabelsAfterThresholding(),
+              self.expectedLabelScoresAfterThresholding()]
+
+    (retained_boxes_, retained_labels_, retained_weights_,
+     expected_retained_boxes_, expected_retained_labels_,
+     expected_retained_weights_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_boxes_, expected_retained_boxes_)
+    self.assertAllClose(retained_labels_, expected_retained_labels_)
+    self.assertAllClose(
+        retained_weights_, expected_retained_weights_)
 
   def testRunRetainBoxesAboveThresholdWithMasks(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = self.createTestMasks()
-
-    tensor_dict = {
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_instance_masks: masks
-    }
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = self.createTestMasks()
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_label_weights=True,
-        include_instance_masks=True)
+      tensor_dict = {
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_instance_masks: masks
+      }
 
-    preprocessing_options = [
-        (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
-    ]
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_label_weights=True,
+          include_instance_masks=True)
 
-    retained_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    retained_masks = retained_tensor_dict[
-        fields.InputDataFields.groundtruth_instance_masks]
+      preprocessing_options = [
+          (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
+      ]
 
-    with self.test_session() as sess:
-      (retained_masks_, expected_masks_) = sess.run(
-          [retained_masks,
-           self.expectedMasksAfterThresholding()])
-      self.assertAllClose(retained_masks_, expected_masks_)
+      retained_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      retained_masks = retained_tensor_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
+      return [retained_masks, self.expectedMasksAfterThresholding()]
+    (retained_masks_, expected_masks_) = self.execute(graph_fn, [])
+    self.assertAllClose(retained_masks_, expected_masks_)
 
   def testRunRetainBoxesAboveThresholdWithKeypoints(self):
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypoints()
+    def graph_fn():
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints, _ = self.createTestKeypoints()
 
-    tensor_dict = {
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_keypoints: keypoints
-    }
-
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_keypoints=True)
+      tensor_dict = {
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_keypoints: keypoints
+      }
 
-    preprocessing_options = [
-        (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
-    ]
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_keypoints=True)
 
-    retained_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    retained_keypoints = retained_tensor_dict[
-        fields.InputDataFields.groundtruth_keypoints]
+      preprocessing_options = [
+          (preprocessor.retain_boxes_above_threshold, {'threshold': 0.6})
+      ]
 
-    with self.test_session() as sess:
-      (retained_keypoints_, expected_keypoints_) = sess.run(
-          [retained_keypoints,
-           self.expectedKeypointsAfterThresholding()])
-      self.assertAllClose(retained_keypoints_, expected_keypoints_)
+      retained_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      retained_keypoints = retained_tensor_dict[
+          fields.InputDataFields.groundtruth_keypoints]
+      return [retained_keypoints, self.expectedKeypointsAfterThresholding()]
+    (retained_keypoints_, expected_keypoints_) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(retained_keypoints_, expected_keypoints_)
 
   def testRandomCropToAspectRatioWithCache(self):
     preprocess_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
@@ -1935,98 +1982,101 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testRunRandomCropToAspectRatioWithMasks(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
-
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_instance_masks: masks
-    }
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=True)
-
-    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_instance_masks: masks
+      }
 
-    with mock.patch.object(preprocessor,
-                           '_random_integer') as mock_random_integer:
-      mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-      distorted_boxes = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_boxes]
-      distorted_labels = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_classes]
-      distorted_masks = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_instance_masks]
-      with self.test_session() as sess:
-        (distorted_image_, distorted_boxes_, distorted_labels_,
-         distorted_masks_) = sess.run([
-             distorted_image, distorted_boxes, distorted_labels, distorted_masks
-         ])
-
-        expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)
-        self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])
-        self.assertAllEqual(distorted_labels_, [1])
-        self.assertAllClose(distorted_boxes_.flatten(),
-                            expected_boxes.flatten())
-        self.assertAllEqual(distorted_masks_.shape, [1, 200, 200])
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True)
+
+      preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
+
+      with mock.patch.object(preprocessor,
+                             '_random_integer') as mock_random_integer:
+        mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict,
+            preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
+        distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+        distorted_boxes = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_boxes]
+        distorted_labels = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_classes]
+        distorted_masks = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_instance_masks]
+        return [
+            distorted_image, distorted_boxes, distorted_labels, distorted_masks
+        ]
+
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_masks_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])
+    self.assertAllEqual(distorted_labels_, [1])
+    self.assertAllClose(distorted_boxes_.flatten(),
+                        expected_boxes.flatten())
+    self.assertAllEqual(distorted_masks_.shape, [1, 200, 200])
 
   def testRunRandomCropToAspectRatioWithKeypoints(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    keypoints = self.createTestKeypoints()
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      keypoints, _ = self.createTestKeypoints()
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_keypoints: keypoints
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_keypoints: keypoints
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_keypoints=True)
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_keypoints=True)
 
-    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
-
-    with mock.patch.object(preprocessor,
-                           '_random_integer') as mock_random_integer:
-      mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)
-      distorted_tensor_dict = preprocessor.preprocess(
-          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-      distorted_boxes = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_boxes]
-      distorted_labels = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_classes]
-      distorted_keypoints = distorted_tensor_dict[
-          fields.InputDataFields.groundtruth_keypoints]
-      with self.test_session() as sess:
-        (distorted_image_, distorted_boxes_, distorted_labels_,
-         distorted_keypoints_) = sess.run([
-             distorted_image, distorted_boxes, distorted_labels,
-             distorted_keypoints
-         ])
-
-        expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)
-        expected_keypoints = np.array(
-            [[0.1, 0.2], [0.2, 0.4], [0.3, 0.6]], dtype=np.float32)
-        self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])
-        self.assertAllEqual(distorted_labels_, [1])
-        self.assertAllClose(distorted_boxes_.flatten(),
-                            expected_boxes.flatten())
-        self.assertAllClose(distorted_keypoints_.flatten(),
-                            expected_keypoints.flatten())
+      preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
+
+      with mock.patch.object(preprocessor,
+                             '_random_integer') as mock_random_integer:
+        mock_random_integer.return_value = tf.constant(0, dtype=tf.int32)
+        distorted_tensor_dict = preprocessor.preprocess(
+            tensor_dict,
+            preprocessing_options,
+            func_arg_map=preprocessor_arg_map)
+        distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+        distorted_boxes = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_boxes]
+        distorted_labels = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_classes]
+        distorted_keypoints = distorted_tensor_dict[
+            fields.InputDataFields.groundtruth_keypoints]
+        return [distorted_image, distorted_boxes, distorted_labels,
+                distorted_keypoints]
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_keypoints_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array([0.0, 0.5, 0.75, 1.0], dtype=np.float32)
+    expected_keypoints = np.array(
+        [[0.1, 0.2], [0.2, 0.4], [0.3, 0.6]], dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 200, 200, 3])
+    self.assertAllEqual(distorted_labels_, [1])
+    self.assertAllClose(distorted_boxes_.flatten(),
+                        expected_boxes.flatten())
+    self.assertAllClose(distorted_keypoints_.flatten(),
+                        expected_keypoints.flatten())
 
   def testRandomPadToAspectRatioWithCache(self):
     preprocess_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
@@ -2036,127 +2086,130 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=True)
 
   def testRunRandomPadToAspectRatioWithMinMaxPaddedSizeRatios(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map()
-    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio,
-                              {'min_padded_size_ratio': (4.0, 4.0),
-                               'max_padded_size_ratio': (4.0, 4.0)})]
-
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_labels = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_classes]
-    with self.test_session() as sess:
-      distorted_image_, distorted_boxes_, distorted_labels_ = sess.run([
-          distorted_image, distorted_boxes, distorted_labels])
-
-      expected_boxes = np.array(
-          [[0.0, 0.125, 0.1875, 0.5], [0.0625, 0.25, 0.1875, 0.5]],
-          dtype=np.float32)
-      self.assertAllEqual(distorted_image_.shape, [1, 800, 800, 3])
-      self.assertAllEqual(distorted_labels_, [1, 2])
-      self.assertAllClose(distorted_boxes_.flatten(),
-                          expected_boxes.flatten())
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map()
+      preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio,
+                                {'min_padded_size_ratio': (4.0, 4.0),
+                                 'max_padded_size_ratio': (4.0, 4.0)})]
+
+      distorted_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_labels = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_classes]
+      return [distorted_image, distorted_boxes, distorted_labels]
+
+    distorted_image_, distorted_boxes_, distorted_labels_ = self.execute_cpu(
+        graph_fn, [])
+    expected_boxes = np.array(
+        [[0.0, 0.125, 0.1875, 0.5], [0.0625, 0.25, 0.1875, 0.5]],
+        dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 800, 800, 3])
+    self.assertAllEqual(distorted_labels_, [1, 2])
+    self.assertAllClose(distorted_boxes_.flatten(),
+                        expected_boxes.flatten())
 
   def testRunRandomPadToAspectRatioWithMasks(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      masks = tf.random_uniform([2, 200, 400], dtype=tf.float32)
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_instance_masks: masks
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_instance_masks: masks
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=True)
-
-    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
-
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_labels = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_classes]
-    distorted_masks = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_instance_masks]
-    with self.test_session() as sess:
-      (distorted_image_, distorted_boxes_, distorted_labels_,
-       distorted_masks_) = sess.run([
-           distorted_image, distorted_boxes, distorted_labels, distorted_masks
-       ])
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True)
 
-      expected_boxes = np.array(
-          [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)
-      self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])
-      self.assertAllEqual(distorted_labels_, [1, 2])
-      self.assertAllClose(distorted_boxes_.flatten(),
-                          expected_boxes.flatten())
-      self.assertAllEqual(distorted_masks_.shape, [2, 400, 400])
+      preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
+
+      distorted_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_labels = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_classes]
+      distorted_masks = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
+      return [
+          distorted_image, distorted_boxes, distorted_labels, distorted_masks
+      ]
+
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_masks_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array(
+        [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])
+    self.assertAllEqual(distorted_labels_, [1, 2])
+    self.assertAllClose(distorted_boxes_.flatten(),
+                        expected_boxes.flatten())
+    self.assertAllEqual(distorted_masks_.shape, [2, 400, 400])
 
   def testRunRandomPadToAspectRatioWithKeypoints(self):
-    image = self.createColorfulTestImage()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    keypoints = self.createTestKeypoints()
+    def graph_fn():
+      image = self.createColorfulTestImage()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      keypoints, _ = self.createTestKeypoints()
 
-    tensor_dict = {
-        fields.InputDataFields.image: image,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_keypoints: keypoints
-    }
+      tensor_dict = {
+          fields.InputDataFields.image: image,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_keypoints: keypoints
+      }
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_keypoints=True)
-
-    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
-
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_labels = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_classes]
-    distorted_keypoints = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_keypoints]
-    with self.test_session() as sess:
-      (distorted_image_, distorted_boxes_, distorted_labels_,
-       distorted_keypoints_) = sess.run([
-           distorted_image, distorted_boxes, distorted_labels,
-           distorted_keypoints
-       ])
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_keypoints=True)
 
-      expected_boxes = np.array(
-          [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)
-      expected_keypoints = np.array([
-          [[0.05, 0.1], [0.1, 0.2], [0.15, 0.3]],
-          [[0.2, 0.4], [0.25, 0.5], [0.3, 0.6]],
-      ], dtype=np.float32)
-      self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])
-      self.assertAllEqual(distorted_labels_, [1, 2])
-      self.assertAllClose(distorted_boxes_.flatten(),
-                          expected_boxes.flatten())
-      self.assertAllClose(distorted_keypoints_.flatten(),
-                          expected_keypoints.flatten())
+      preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
+
+      distorted_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      distorted_image = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_labels = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_classes]
+      distorted_keypoints = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_keypoints]
+      return [
+          distorted_image, distorted_boxes, distorted_labels,
+          distorted_keypoints
+      ]
+
+    (distorted_image_, distorted_boxes_, distorted_labels_,
+     distorted_keypoints_) = self.execute_cpu(graph_fn, [])
+    expected_boxes = np.array(
+        [[0.0, 0.25, 0.375, 1.0], [0.125, 0.5, 0.375, 1.0]], dtype=np.float32)
+    expected_keypoints = np.array([
+        [[0.05, 0.1], [0.1, 0.2], [0.15, 0.3]],
+        [[0.2, 0.4], [0.25, 0.5], [0.3, 0.6]],
+    ], dtype=np.float32)
+    self.assertAllEqual(distorted_image_.shape, [1, 400, 400, 3])
+    self.assertAllEqual(distorted_labels_, [1, 2])
+    self.assertAllClose(distorted_boxes_.flatten(),
+                        expected_boxes.flatten())
+    self.assertAllClose(distorted_keypoints_.flatten(),
+                        expected_keypoints.flatten())
 
   def testRandomPadImageWithCache(self):
     preprocess_options = [(preprocessor.normalize_image, {
@@ -2170,140 +2223,176 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=True)
 
   def testRandomPadImage(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images = tensor_dict[fields.InputDataFields.image]
 
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images = tensor_dict[fields.InputDataFields.image]
-
-    preprocessing_options = [(preprocessor.random_pad_image, {})]
-    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                 preprocessing_options)
-
-    padded_images = padded_tensor_dict[fields.InputDataFields.image]
-    padded_boxes = padded_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
-
-    with self.test_session() as sess:
-      (boxes_shape_, padded_boxes_shape_, images_shape_,
-       padded_images_shape_, boxes_, padded_boxes_) = sess.run(
-           [boxes_shape, padded_boxes_shape, images_shape,
-            padded_images_shape, boxes, padded_boxes])
-      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
-      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
-      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
-      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
-      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
-      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
-          padded_boxes_[:, 2] - padded_boxes_[:, 0])))
-      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
-          padded_boxes_[:, 3] - padded_boxes_[:, 1])))
+      preprocessing_options = [(preprocessor.random_pad_image, {})]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
+
+      padded_images = padded_tensor_dict[fields.InputDataFields.image]
+      padded_boxes = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_shape = tf.shape(boxes)
+      padded_boxes_shape = tf.shape(padded_boxes)
+      images_shape = tf.shape(images)
+      padded_images_shape = tf.shape(padded_images)
+      return [boxes_shape, padded_boxes_shape, images_shape,
+              padded_images_shape, boxes, padded_boxes]
+    (boxes_shape_, padded_boxes_shape_, images_shape_,
+     padded_images_shape_, boxes_, padded_boxes_) = self.execute_cpu(graph_fn,
+                                                                     [])
+    self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
+    self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
+    self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
+    self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
+    self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
+    self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
+        padded_boxes_[:, 2] - padded_boxes_[:, 0])))
+    self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
+        padded_boxes_[:, 3] - padded_boxes_[:, 1])))
 
   def testRandomPadImageWithKeypoints(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      keypoints, _ = self.createTestKeypoints()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_keypoints: keypoints,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images = tensor_dict[fields.InputDataFields.image]
 
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    keypoints = self.createTestKeypoints()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_keypoints: keypoints,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images = tensor_dict[fields.InputDataFields.image]
-
-    preprocessing_options = [(preprocessor.random_pad_image, {})]
-    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                 preprocessing_options)
-
-    padded_images = padded_tensor_dict[fields.InputDataFields.image]
-    padded_boxes = padded_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    padded_keypoints = padded_tensor_dict[
-        fields.InputDataFields.groundtruth_keypoints]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    keypoints_shape = tf.shape(keypoints)
-    padded_keypoints_shape = tf.shape(padded_keypoints)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
-
-    with self.test_session() as sess:
-      (boxes_shape_, padded_boxes_shape_, keypoints_shape_,
-       padded_keypoints_shape_, images_shape_, padded_images_shape_, boxes_,
-       padded_boxes_, keypoints_, padded_keypoints_) = sess.run(
-           [boxes_shape, padded_boxes_shape, keypoints_shape,
-            padded_keypoints_shape, images_shape, padded_images_shape, boxes,
-            padded_boxes, keypoints, padded_keypoints])
-      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
-      self.assertAllEqual(keypoints_shape_, padded_keypoints_shape_)
-      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
-      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
-      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
-      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
-      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
-          padded_boxes_[:, 2] - padded_boxes_[:, 0])))
-      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
-          padded_boxes_[:, 3] - padded_boxes_[:, 1])))
-      self.assertTrue(np.all((keypoints_[1, :, 0] - keypoints_[0, :, 0]) >= (
-          padded_keypoints_[1, :, 0] - padded_keypoints_[0, :, 0])))
-      self.assertTrue(np.all((keypoints_[1, :, 1] - keypoints_[0, :, 1]) >= (
-          padded_keypoints_[1, :, 1] - padded_keypoints_[0, :, 1])))
+      preprocessing_options = [(preprocessor.random_pad_image, {})]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
 
-  def testRandomAbsolutePadImage(self):
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    tensor_dict = {
-        fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-    }
+      padded_images = padded_tensor_dict[fields.InputDataFields.image]
+      padded_boxes = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      padded_keypoints = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_keypoints]
+      boxes_shape = tf.shape(boxes)
+      padded_boxes_shape = tf.shape(padded_boxes)
+      keypoints_shape = tf.shape(keypoints)
+      padded_keypoints_shape = tf.shape(padded_keypoints)
+      images_shape = tf.shape(images)
+      padded_images_shape = tf.shape(padded_images)
+      return [boxes_shape, padded_boxes_shape, keypoints_shape,
+              padded_keypoints_shape, images_shape, padded_images_shape, boxes,
+              padded_boxes, keypoints, padded_keypoints]
+
+    (boxes_shape_, padded_boxes_shape_, keypoints_shape_,
+     padded_keypoints_shape_, images_shape_, padded_images_shape_, boxes_,
+     padded_boxes_, keypoints_, padded_keypoints_) = self.execute_cpu(graph_fn,
+                                                                      [])
+    self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
+    self.assertAllEqual(keypoints_shape_, padded_keypoints_shape_)
+    self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
+    self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
+    self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
+    self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
+    self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
+        padded_boxes_[:, 2] - padded_boxes_[:, 0])))
+    self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
+        padded_boxes_[:, 3] - padded_boxes_[:, 1])))
+    self.assertTrue(np.all((keypoints_[1, :, 0] - keypoints_[0, :, 0]) >= (
+        padded_keypoints_[1, :, 0] - padded_keypoints_[0, :, 0])))
+    self.assertTrue(np.all((keypoints_[1, :, 1] - keypoints_[0, :, 1]) >= (
+        padded_keypoints_[1, :, 1] - padded_keypoints_[0, :, 1])))
 
+  def testRandomAbsolutePadImage(self):
     height_padding = 10
     width_padding = 20
-    preprocessing_options = [(preprocessor.random_absolute_pad_image, {
-        'max_height_padding': height_padding,
-        'max_width_padding': width_padding})]
-    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                 preprocessing_options)
-
-    original_shape = tf.shape(images)
-    final_shape = tf.shape(padded_tensor_dict[fields.InputDataFields.image])
-
-    with self.test_session() as sess:
-      _, height, width, _ = sess.run(original_shape)
-      for _ in range(100):
-        output_shape = sess.run(final_shape)
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      tensor_dict = {
+          fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+      }
+      preprocessing_options = [(preprocessor.random_absolute_pad_image, {
+          'max_height_padding': height_padding,
+          'max_width_padding': width_padding})]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
+      original_shape = tf.shape(images)
+      final_shape = tf.shape(padded_tensor_dict[fields.InputDataFields.image])
+      return original_shape, final_shape
+    for _ in range(100):
+      original_shape, output_shape = self.execute_cpu(graph_fn, [])
+      _, height, width, _ = original_shape
+      self.assertGreaterEqual(output_shape[1], height)
+      self.assertLess(output_shape[1], height + height_padding)
+      self.assertGreaterEqual(output_shape[2], width)
+      self.assertLess(output_shape[2], width + width_padding)
+
+  def testRandomAbsolutePadImageWithKeypoints(self):
+    height_padding = 10
+    width_padding = 20
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      keypoints, _ = self.createTestKeypoints()
+      tensor_dict = {
+          fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_keypoints: keypoints,
+      }
 
-        self.assertTrue(output_shape[1] >= height)
-        self.assertTrue(output_shape[1] < height + height_padding)
-        self.assertTrue(output_shape[2] >= width)
-        self.assertTrue(output_shape[2] < width + width_padding)
+      preprocessing_options = [(preprocessor.random_absolute_pad_image, {
+          'max_height_padding': height_padding,
+          'max_width_padding': width_padding
+      })]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
+      original_shape = tf.shape(images)
+      final_shape = tf.shape(padded_tensor_dict[fields.InputDataFields.image])
+      padded_keypoints = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_keypoints]
+      return (original_shape, final_shape, padded_keypoints)
+    for _ in range(100):
+      original_shape, output_shape, padded_keypoints_ = self.execute_cpu(
+          graph_fn, [])
+      _, height, width, _ = original_shape
+      self.assertGreaterEqual(output_shape[1], height)
+      self.assertLess(output_shape[1], height + height_padding)
+      self.assertGreaterEqual(output_shape[2], width)
+      self.assertLess(output_shape[2], width + width_padding)
+      # Verify the keypoints are populated. The correctness of the keypoint
+      # coordinates are already tested in random_pad_image function.
+      self.assertEqual(padded_keypoints_.shape, (2, 3, 2))
 
   def testRandomCropPadImageWithCache(self):
     preprocess_options = [(preprocessor.normalize_image, {
@@ -2317,126 +2406,128 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=True)
 
   def testRandomCropPadImageWithRandomCoefOne(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    })]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      })]
+
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+      images = tensor_dict[fields.InputDataFields.image]
 
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images = tensor_dict[fields.InputDataFields.image]
+      preprocessing_options = [(preprocessor.random_crop_pad_image, {
+          'random_coef': 1.0
+      })]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
 
-    preprocessing_options = [(preprocessor.random_crop_pad_image, {
-        'random_coef': 1.0
-    })]
-    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                 preprocessing_options)
-
-    padded_images = padded_tensor_dict[fields.InputDataFields.image]
-    padded_boxes = padded_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
-
-    with self.test_session() as sess:
-      (boxes_shape_, padded_boxes_shape_, images_shape_,
-       padded_images_shape_, boxes_, padded_boxes_) = sess.run(
-           [boxes_shape, padded_boxes_shape, images_shape,
-            padded_images_shape, boxes, padded_boxes])
-      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
-      self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
-      self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
-      self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
-      self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
-      self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
-          padded_boxes_[:, 2] - padded_boxes_[:, 0])))
-      self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
-          padded_boxes_[:, 3] - padded_boxes_[:, 1])))
+      padded_images = padded_tensor_dict[fields.InputDataFields.image]
+      padded_boxes = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_shape = tf.shape(boxes)
+      padded_boxes_shape = tf.shape(padded_boxes)
+      images_shape = tf.shape(images)
+      padded_images_shape = tf.shape(padded_images)
+      return [boxes_shape, padded_boxes_shape, images_shape,
+              padded_images_shape, boxes, padded_boxes]
+    (boxes_shape_, padded_boxes_shape_, images_shape_,
+     padded_images_shape_, boxes_, padded_boxes_) = self.execute_cpu(graph_fn,
+                                                                     [])
+    self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
+    self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
+    self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
+    self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
+    self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
+    self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
+        padded_boxes_[:, 2] - padded_boxes_[:, 0])))
+    self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
+        padded_boxes_[:, 3] - padded_boxes_[:, 1])))
 
   def testRandomCropToAspectRatio(self):
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, [])
-    images = tensor_dict[fields.InputDataFields.image]
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, [])
+      images = tensor_dict[fields.InputDataFields.image]
 
-    preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {
-        'aspect_ratio': 2.0
-    })]
-    cropped_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                  preprocessing_options)
-
-    cropped_images = cropped_tensor_dict[fields.InputDataFields.image]
-    cropped_boxes = cropped_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    cropped_boxes_shape = tf.shape(cropped_boxes)
-    images_shape = tf.shape(images)
-    cropped_images_shape = tf.shape(cropped_images)
+      preprocessing_options = [(preprocessor.random_crop_to_aspect_ratio, {
+          'aspect_ratio': 2.0
+      })]
+      cropped_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
 
-    with self.test_session() as sess:
-      (boxes_shape_, cropped_boxes_shape_, images_shape_,
-       cropped_images_shape_) = sess.run([
-           boxes_shape, cropped_boxes_shape, images_shape, cropped_images_shape
-       ])
-      self.assertAllEqual(boxes_shape_, cropped_boxes_shape_)
-      self.assertEqual(images_shape_[1], cropped_images_shape_[1] * 2)
-      self.assertEqual(images_shape_[2], cropped_images_shape_[2])
+      cropped_images = cropped_tensor_dict[fields.InputDataFields.image]
+      cropped_boxes = cropped_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_shape = tf.shape(boxes)
+      cropped_boxes_shape = tf.shape(cropped_boxes)
+      images_shape = tf.shape(images)
+      cropped_images_shape = tf.shape(cropped_images)
+      return [
+          boxes_shape, cropped_boxes_shape, images_shape, cropped_images_shape
+      ]
+
+    (boxes_shape_, cropped_boxes_shape_, images_shape_,
+     cropped_images_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_shape_, cropped_boxes_shape_)
+    self.assertEqual(images_shape_[1], cropped_images_shape_[1] * 2)
+    self.assertEqual(images_shape_[2], cropped_images_shape_[2])
 
   def testRandomPadToAspectRatio(self):
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-    }
-    tensor_dict = preprocessor.preprocess(tensor_dict, [])
-    images = tensor_dict[fields.InputDataFields.image]
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+      }
+      tensor_dict = preprocessor.preprocess(tensor_dict, [])
+      images = tensor_dict[fields.InputDataFields.image]
 
-    preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {
-        'aspect_ratio': 2.0
-    })]
-    padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                 preprocessing_options)
-
-    padded_images = padded_tensor_dict[fields.InputDataFields.image]
-    padded_boxes = padded_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    boxes_shape = tf.shape(boxes)
-    padded_boxes_shape = tf.shape(padded_boxes)
-    images_shape = tf.shape(images)
-    padded_images_shape = tf.shape(padded_images)
+      preprocessing_options = [(preprocessor.random_pad_to_aspect_ratio, {
+          'aspect_ratio': 2.0
+      })]
+      padded_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                   preprocessing_options)
 
-    with self.test_session() as sess:
-      (boxes_shape_, padded_boxes_shape_, images_shape_,
-       padded_images_shape_) = sess.run([
-           boxes_shape, padded_boxes_shape, images_shape, padded_images_shape
-       ])
-      self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
-      self.assertEqual(images_shape_[1], padded_images_shape_[1])
-      self.assertEqual(2 * images_shape_[2], padded_images_shape_[2])
+      padded_images = padded_tensor_dict[fields.InputDataFields.image]
+      padded_boxes = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      boxes_shape = tf.shape(boxes)
+      padded_boxes_shape = tf.shape(padded_boxes)
+      images_shape = tf.shape(images)
+      padded_images_shape = tf.shape(padded_images)
+      return [
+          boxes_shape, padded_boxes_shape, images_shape, padded_images_shape
+      ]
+
+    (boxes_shape_, padded_boxes_shape_, images_shape_,
+     padded_images_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
+    self.assertEqual(images_shape_[1], padded_images_shape_[1])
+    self.assertEqual(2 * images_shape_[2], padded_images_shape_[2])
 
   def testRandomBlackPatchesWithCache(self):
     preprocess_options = []
@@ -2455,46 +2546,44 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=True)
 
   def testRandomBlackPatches(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_black_patches, {
-        'size_to_image_ratio': 0.5
-    }))
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    blacked_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                  preprocessing_options)
-    blacked_images = blacked_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    blacked_images_shape = tf.shape(blacked_images)
-
-    with self.test_session() as sess:
-      (images_shape_, blacked_images_shape_) = sess.run(
-          [images_shape, blacked_images_shape])
-      self.assertAllEqual(images_shape_, blacked_images_shape_)
-
-  def testRandomJpegQuality(self):
-    preprocessing_options = [(preprocessor.random_jpeg_quality, {
-        'min_jpeg_quality': 0,
-        'max_jpeg_quality': 100
-    })]
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_black_patches, {
+          'size_to_image_ratio': 0.5
+      }))
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      blacked_tensor_dict = preprocessor.preprocess(tensor_dict,
                                                     preprocessing_options)
-    encoded_images = processed_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    encoded_images_shape = tf.shape(encoded_images)
+      blacked_images = blacked_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      blacked_images_shape = tf.shape(blacked_images)
+      return [images_shape, blacked_images_shape]
+    (images_shape_, blacked_images_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_, blacked_images_shape_)
 
-    with self.test_session() as sess:
-      images_shape_out, encoded_images_shape_out = sess.run(
-          [images_shape, encoded_images_shape])
-      self.assertAllEqual(images_shape_out, encoded_images_shape_out)
+  def testRandomJpegQuality(self):
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_jpeg_quality, {
+          'min_jpeg_quality': 0,
+          'max_jpeg_quality': 100
+      })]
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      encoded_images = processed_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      encoded_images_shape = tf.shape(encoded_images)
+      return [images_shape, encoded_images_shape]
+    images_shape_out, encoded_images_shape_out = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_out, encoded_images_shape_out)
 
   def testRandomJpegQualityKeepsStaticChannelShape(self):
     # Set at least three weeks past the forward compatibility horizon for
@@ -2502,7 +2591,6 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     # https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/compat/compat.py#L30
     if not tf.compat.forward_compatible(year=2019, month=12, day=1):
       self.skipTest('Skipping test for future functionality.')
-
     preprocessing_options = [(preprocessor.random_jpeg_quality, {
         'min_jpeg_quality': 0,
         'max_jpeg_quality': 100
@@ -2524,67 +2612,71 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     self._testPreprocessorCache(preprocessing_options)
 
   def testRandomJpegQualityWithRandomCoefOne(self):
-    preprocessing_options = [(preprocessor.random_jpeg_quality, {
-        'random_coef': 1.0
-    })]
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    encoded_images = processed_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    encoded_images_shape = tf.shape(encoded_images)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_jpeg_quality, {
+          'random_coef': 1.0
+      })]
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      encoded_images = processed_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      encoded_images_shape = tf.shape(encoded_images)
+      return [images, encoded_images, images_shape, encoded_images_shape]
 
-    with self.test_session() as sess:
-      (images_out, encoded_images_out, images_shape_out,
-       encoded_images_shape_out) = sess.run(
-           [images, encoded_images, images_shape, encoded_images_shape])
-      self.assertAllEqual(images_shape_out, encoded_images_shape_out)
-      self.assertAllEqual(images_out, encoded_images_out)
+    (images_out, encoded_images_out, images_shape_out,
+     encoded_images_shape_out) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_out, encoded_images_shape_out)
+    self.assertAllEqual(images_out, encoded_images_out)
 
   def testRandomDownscaleToTargetPixels(self):
-    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
-        'min_target_pixels': 100,
-        'max_target_pixels': 101
-    })]
-    images = tf.random_uniform([1, 25, 100, 3])
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
-    downscaled_shape = tf.shape(downscaled_images)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_downscale_to_target_pixels,
+                                {
+                                    'min_target_pixels': 100,
+                                    'max_target_pixels': 101
+                                })]
+      images = tf.random_uniform([1, 25, 100, 3])
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+      downscaled_shape = tf.shape(downscaled_images)
+      return downscaled_shape
     expected_shape = [1, 5, 20, 3]
-    with self.test_session() as sess:
-      downscaled_shape_out = sess.run(downscaled_shape)
-      self.assertAllEqual(downscaled_shape_out, expected_shape)
+    downscaled_shape_out = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(downscaled_shape_out, expected_shape)
 
   def testRandomDownscaleToTargetPixelsWithMasks(self):
-    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
-        'min_target_pixels': 100,
-        'max_target_pixels': 101
-    })]
-    images = tf.random_uniform([1, 25, 100, 3])
-    masks = tf.random_uniform([10, 25, 100])
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_instance_masks: masks
-    }
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_instance_masks=True)
-    processed_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
-    downscaled_masks = processed_tensor_dict[
-        fields.InputDataFields.groundtruth_instance_masks]
-    downscaled_images_shape = tf.shape(downscaled_images)
-    downscaled_masks_shape = tf.shape(downscaled_masks)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_downscale_to_target_pixels,
+                                {
+                                    'min_target_pixels': 100,
+                                    'max_target_pixels': 101
+                                })]
+      images = tf.random_uniform([1, 25, 100, 3])
+      masks = tf.random_uniform([10, 25, 100])
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_instance_masks: masks
+      }
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True)
+      processed_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+      downscaled_masks = processed_tensor_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
+      downscaled_images_shape = tf.shape(downscaled_images)
+      downscaled_masks_shape = tf.shape(downscaled_masks)
+      return [downscaled_images_shape, downscaled_masks_shape]
     expected_images_shape = [1, 5, 20, 3]
     expected_masks_shape = [10, 5, 20]
-    with self.test_session() as sess:
-      downscaled_images_shape_out, downscaled_masks_shape_out = sess.run(
-          [downscaled_images_shape, downscaled_masks_shape])
-      self.assertAllEqual(downscaled_images_shape_out, expected_images_shape)
-      self.assertAllEqual(downscaled_masks_shape_out, expected_masks_shape)
+    (downscaled_images_shape_out,
+     downscaled_masks_shape_out) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(downscaled_images_shape_out, expected_images_shape)
+    self.assertAllEqual(downscaled_masks_shape_out, expected_masks_shape)
 
   @parameterized.parameters(
       {'test_masks': False},
@@ -2598,44 +2690,45 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     self._testPreprocessorCache(preprocessing_options, test_masks=test_masks)
 
   def testRandomDownscaleToTargetPixelsWithRandomCoefOne(self):
-    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
-        'random_coef': 1.0,
-        'min_target_pixels': 10,
-        'max_target_pixels': 20,
-    })]
-    images = tf.random_uniform([1, 25, 100, 3])
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    downscaled_images_shape = tf.shape(downscaled_images)
-
-    with self.test_session() as sess:
-      (images_out, downscaled_images_out, images_shape_out,
-       downscaled_images_shape_out) = sess.run(
-           [images, downscaled_images, images_shape, downscaled_images_shape])
-      self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
-      self.assertAllEqual(images_out, downscaled_images_out)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_downscale_to_target_pixels,
+                                {
+                                    'random_coef': 1.0,
+                                    'min_target_pixels': 10,
+                                    'max_target_pixels': 20,
+                                })]
+      images = tf.random_uniform([1, 25, 100, 3])
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      downscaled_images_shape = tf.shape(downscaled_images)
+      return [images, downscaled_images, images_shape, downscaled_images_shape]
+    (images_out, downscaled_images_out, images_shape_out,
+     downscaled_images_shape_out) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
+    self.assertAllEqual(images_out, downscaled_images_out)
 
   def testRandomDownscaleToTargetPixelsIgnoresSmallImages(self):
-    preprocessing_options = [(preprocessor.random_downscale_to_target_pixels, {
-        'min_target_pixels': 1000,
-        'max_target_pixels': 1001
-    })]
-    images = tf.random_uniform([1, 10, 10, 3])
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    downscaled_images_shape = tf.shape(downscaled_images)
-    with self.test_session() as sess:
-      (images_out, downscaled_images_out, images_shape_out,
-       downscaled_images_shape_out) = sess.run(
-           [images, downscaled_images, images_shape, downscaled_images_shape])
-      self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
-      self.assertAllEqual(images_out, downscaled_images_out)
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_downscale_to_target_pixels,
+                                {
+                                    'min_target_pixels': 1000,
+                                    'max_target_pixels': 1001
+                                })]
+      images = tf.random_uniform([1, 10, 10, 3])
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      downscaled_images = processed_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      downscaled_images_shape = tf.shape(downscaled_images)
+      return [images, downscaled_images, images_shape, downscaled_images_shape]
+    (images_out, downscaled_images_out, images_shape_out,
+     downscaled_images_shape_out) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_out, downscaled_images_shape_out)
+    self.assertAllEqual(images_out, downscaled_images_out)
 
   def testRandomPatchGaussianShape(self):
     preprocessing_options = [(preprocessor.random_patch_gaussian, {
@@ -2654,31 +2747,37 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     self.assertAllEqual(images_shape, patched_images_shape)
 
   def testRandomPatchGaussianClippedToLowerBound(self):
-    preprocessing_options = [(preprocessor.random_patch_gaussian, {
-        'min_patch_size': 20,
-        'max_patch_size': 40,
-        'min_gaussian_stddev': 50,
-        'max_gaussian_stddev': 100
-    })]
-    images = tf.zeros([1, 5, 4, 3])
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_patch_gaussian, {
+          'min_patch_size': 20,
+          'max_patch_size': 40,
+          'min_gaussian_stddev': 50,
+          'max_gaussian_stddev': 100
+      })]
+      images = tf.zeros([1, 5, 4, 3])
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      patched_images = processed_tensor_dict[fields.InputDataFields.image]
+      return patched_images
+    patched_images = self.execute_cpu(graph_fn, [])
     self.assertAllGreaterEqual(patched_images, 0.0)
 
   def testRandomPatchGaussianClippedToUpperBound(self):
-    preprocessing_options = [(preprocessor.random_patch_gaussian, {
-        'min_patch_size': 20,
-        'max_patch_size': 40,
-        'min_gaussian_stddev': 50,
-        'max_gaussian_stddev': 100
-    })]
-    images = tf.constant(255.0, shape=[1, 5, 4, 3])
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    patched_images = processed_tensor_dict[fields.InputDataFields.image]
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_patch_gaussian, {
+          'min_patch_size': 20,
+          'max_patch_size': 40,
+          'min_gaussian_stddev': 50,
+          'max_gaussian_stddev': 100
+      })]
+      images = tf.constant(255.0, shape=[1, 5, 4, 3])
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      patched_images = processed_tensor_dict[fields.InputDataFields.image]
+      return patched_images
+    patched_images = self.execute_cpu(graph_fn, [])
     self.assertAllLessEqual(patched_images, 255.0)
 
   def testRandomPatchGaussianWithCache(self):
@@ -2691,46 +2790,48 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     self._testPreprocessorCache(preprocessing_options)
 
   def testRandomPatchGaussianWithRandomCoefOne(self):
-    preprocessing_options = [(preprocessor.random_patch_gaussian, {
-        'random_coef': 1.0
-    })]
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    processed_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    patched_images = processed_tensor_dict[fields.InputDataFields.image]
-    images_shape = tf.shape(images)
-    patched_images_shape = tf.shape(patched_images)
-
+    def graph_fn():
+      preprocessing_options = [(preprocessor.random_patch_gaussian, {
+          'random_coef': 1.0
+      })]
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      processed_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      patched_images = processed_tensor_dict[fields.InputDataFields.image]
+      images_shape = tf.shape(images)
+      patched_images_shape = tf.shape(patched_images)
+      return patched_images_shape, patched_images, images_shape, images
+    (patched_images_shape, patched_images, images_shape,
+     images) = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(images_shape, patched_images_shape)
     self.assertAllEqual(images, patched_images)
 
   def testAutoAugmentImage(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.autoaugment_image, {
-        'policy_name': 'v1'
-    }))
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    tensor_dict = {fields.InputDataFields.image: images,
-                   fields.InputDataFields.groundtruth_boxes: boxes}
-    autoaugment_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options)
-    augmented_images = autoaugment_tensor_dict[fields.InputDataFields.image]
-    augmented_boxes = autoaugment_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    images_shape = tf.shape(images)
-    boxes_shape = tf.shape(boxes)
-    augmented_images_shape = tf.shape(augmented_images)
-    augmented_boxes_shape = tf.shape(augmented_boxes)
-
-    with self.test_session() as sess:
-      (images_shape_, boxes_shape_,
-       augmented_images_shape_, augmented_boxes_shape_) = sess.run(
-           [images_shape, boxes_shape,
-            augmented_images_shape, augmented_boxes_shape])
-      self.assertAllEqual(images_shape_, augmented_images_shape_)
-      self.assertAllEqual(boxes_shape_, augmented_boxes_shape_)
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.autoaugment_image, {
+          'policy_name': 'v1'
+      }))
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      tensor_dict = {fields.InputDataFields.image: images,
+                     fields.InputDataFields.groundtruth_boxes: boxes}
+      autoaugment_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options)
+      augmented_images = autoaugment_tensor_dict[fields.InputDataFields.image]
+      augmented_boxes = autoaugment_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      images_shape = tf.shape(images)
+      boxes_shape = tf.shape(boxes)
+      augmented_images_shape = tf.shape(augmented_images)
+      augmented_boxes_shape = tf.shape(augmented_boxes)
+      return [images_shape, boxes_shape, augmented_images_shape,
+              augmented_boxes_shape]
+    (images_shape_, boxes_shape_, augmented_images_shape_,
+     augmented_boxes_shape_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(images_shape_, augmented_images_shape_)
+    self.assertAllEqual(boxes_shape_, augmented_boxes_shape_)
 
   def testRandomResizeMethodWithCache(self):
     preprocess_options = []
@@ -2748,31 +2849,31 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_masks=True,
                                 test_keypoints=True)
 
-  def testRandomResizeMethod(self):
-    preprocessing_options = []
-    preprocessing_options.append((preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }))
-    preprocessing_options.append((preprocessor.random_resize_method, {
-        'target_size': (75, 150)
-    }))
-    images = self.createTestImages()
-    tensor_dict = {fields.InputDataFields.image: images}
-    resized_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                  preprocessing_options)
-    resized_images = resized_tensor_dict[fields.InputDataFields.image]
-    resized_images_shape = tf.shape(resized_images)
-    expected_images_shape = tf.constant([1, 75, 150, 3], dtype=tf.int32)
-
-    with self.test_session() as sess:
-      (expected_images_shape_, resized_images_shape_) = sess.run(
-          [expected_images_shape, resized_images_shape])
-      self.assertAllEqual(expected_images_shape_,
-                          resized_images_shape_)
-
+  def testRandomResizeMethod(self):
+    def graph_fn():
+      preprocessing_options = []
+      preprocessing_options.append((preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }))
+      preprocessing_options.append((preprocessor.random_resize_method, {
+          'target_size': (75, 150)
+      }))
+      images = self.createTestImages()
+      tensor_dict = {fields.InputDataFields.image: images}
+      resized_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                    preprocessing_options)
+      resized_images = resized_tensor_dict[fields.InputDataFields.image]
+      resized_images_shape = tf.shape(resized_images)
+      expected_images_shape = tf.constant([1, 75, 150, 3], dtype=tf.int32)
+      return [expected_images_shape, resized_images_shape]
+    (expected_images_shape_, resized_images_shape_) = self.execute_cpu(graph_fn,
+                                                                       [])
+    self.assertAllEqual(expected_images_shape_,
+                        resized_images_shape_)
+
   def testResizeImageWithMasks(self):
     """Tests image resizing, checking output sizes."""
     in_image_shape_list = [[60, 40, 3], [15, 30, 3]]
@@ -2781,51 +2882,55 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     width = 100
     expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]
     expected_masks_shape_list = [[15, 50, 100], [10, 50, 100]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
+    def graph_fn(in_image_shape, in_masks_shape):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return out_image_shape, out_masks_shape
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      (out_image_shape,
+       out_masks_shape) = self.execute_cpu(graph_fn, [
+           np.array(in_image_shape, np.int32),
+           np.array(in_masks_shape, np.int32)
+       ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeImageWithMasksTensorInputHeightAndWidth(self):
     """Tests image resizing, checking output sizes."""
     in_image_shape_list = [[60, 40, 3], [15, 30, 3]]
     in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]
-    height = tf.constant(50, dtype=tf.int32)
-    width = tf.constant(100, dtype=tf.int32)
     expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]
     expected_masks_shape_list = [[15, 50, 100], [10, 50, 100]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
+    def graph_fn(in_image_shape, in_masks_shape):
+      height = tf.constant(50, dtype=tf.int32)
+      width = tf.constant(100, dtype=tf.int32)
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return out_image_shape, out_masks_shape
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      (out_image_shape,
+       out_masks_shape) = self.execute_cpu(graph_fn, [
+           np.array(in_image_shape, np.int32),
+           np.array(in_masks_shape, np.int32)
+       ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeImageWithNoInstanceMask(self):
     """Tests image resizing, checking output sizes."""
@@ -2835,24 +2940,26 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     width = 100
     expected_image_shape_list = [[50, 100, 3], [50, 100, 3]]
     expected_masks_shape_list = [[0, 50, 100], [0, 50, 100]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
+    def graph_fn(in_image_shape, in_masks_shape):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return out_image_shape, out_masks_shape
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      (out_image_shape,
+       out_masks_shape) = self.execute_cpu(graph_fn, [
+           np.array(in_image_shape, np.int32),
+           np.array(in_masks_shape, np.int32)
+       ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToRangePreservesStaticSpatialShape(self):
     """Tests image resizing, checking output sizes."""
@@ -2873,37 +2980,33 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     min_dim = 50
     max_dim = 100
     expected_shape_list = [[75, 50, 3], [50, 100, 3], [30, 100, 3]]
-
-    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+    def graph_fn(in_image_shape):
+      in_image = tf.random_uniform(in_image_shape)
       out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
-      with self.test_session() as sess:
-        out_image_shape = sess.run(out_image_shape,
-                                   feed_dict={in_image:
-                                              np.random.randn(*in_shape)})
-        self.assertAllEqual(out_image_shape, expected_shape)
+      return out_image_shape
+    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
+      out_image_shape = self.execute_cpu(graph_fn, [np.array(in_shape,
+                                                             np.int32)])
+      self.assertAllEqual(out_image_shape, expected_shape)
 
   def testResizeToRangeWithPadToMaxDimensionReturnsCorrectShapes(self):
     in_shape_list = [[60, 40, 3], [15, 30, 3], [15, 50, 3]]
     min_dim = 50
     max_dim = 100
     expected_shape_list = [[100, 100, 3], [100, 100, 3], [100, 100, 3]]
-
-    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+    def graph_fn(in_image):
       out_image, _ = preprocessor.resize_to_range(
           in_image,
           min_dimension=min_dim,
           max_dimension=max_dim,
           pad_to_max_dimension=True)
-      self.assertAllEqual(out_image.shape.as_list(), expected_shape)
-      out_image_shape = tf.shape(out_image)
-      with self.test_session() as sess:
-        out_image_shape = sess.run(
-            out_image_shape, feed_dict={in_image: np.random.randn(*in_shape)})
-        self.assertAllEqual(out_image_shape, expected_shape)
+      return tf.shape(out_image)
+    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
+      out_image_shape = self.execute_cpu(
+          graph_fn, [np.random.rand(*in_shape).astype('f')])
+      self.assertAllEqual(out_image_shape, expected_shape)
 
   def testResizeToRangeWithPadToMaxDimensionReturnsCorrectTensor(self):
     in_image_np = np.array([[[0, 1, 2]]], np.float32)
@@ -2912,18 +3015,16 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
          [[123.68, 116.779, 103.939], [123.68, 116.779, 103.939]]], np.float32)
     min_dim = 1
     max_dim = 2
-
-    in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-    out_image, _ = preprocessor.resize_to_range(
-        in_image,
-        min_dimension=min_dim,
-        max_dimension=max_dim,
-        pad_to_max_dimension=True,
-        per_channel_pad_value=(123.68, 116.779, 103.939))
-
-    with self.test_session() as sess:
-      out_image_np = sess.run(out_image, feed_dict={in_image: in_image_np})
-      self.assertAllClose(ex_image_np, out_image_np)
+    def graph_fn(in_image):
+      out_image, _ = preprocessor.resize_to_range(
+          in_image,
+          min_dimension=min_dim,
+          max_dimension=max_dim,
+          pad_to_max_dimension=True,
+          per_channel_pad_value=(123.68, 116.779, 103.939))
+      return out_image
+    out_image_np = self.execute_cpu(graph_fn, [in_image_np])
+    self.assertAllClose(ex_image_np, out_image_np)
 
   def testResizeToRangeWithMasksPreservesStaticSpatialShape(self):
     """Tests image resizing, checking output sizes."""
@@ -2954,31 +3055,25 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     max_dim = 100
     expected_image_shape_list = [[100, 100, 3], [100, 100, 3]]
     expected_masks_shape_list = [[15, 100, 100], [10, 100, 100]]
-
-    for (in_image_shape,
-         expected_image_shape, in_masks_shape, expected_mask_shape) in zip(
-             in_image_shape_list, expected_image_shape_list,
-             in_masks_shape_list, expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+    def graph_fn(in_image, in_masks):
       out_image, out_masks, _ = preprocessor.resize_to_range(
-          in_image,
-          in_masks,
-          min_dimension=min_dim,
-          max_dimension=max_dim,
-          pad_to_max_dimension=True)
+          in_image, in_masks, min_dimension=min_dim,
+          max_dimension=max_dim, pad_to_max_dimension=True)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape],
-            feed_dict={
-                in_image: np.random.randn(*in_image_shape),
-                in_masks: np.random.randn(*in_masks_shape)
-            })
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return [out_image_shape, out_masks_shape]
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.random.rand(*in_image_shape).astype('f'),
+              np.random.rand(*in_masks_shape).astype('f'),
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToRangeWithMasksAndDynamicSpatialShape(self):
     """Tests image resizing, checking output sizes."""
@@ -2988,29 +3083,24 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     max_dim = 100
     expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]
     expected_masks_shape_list = [[15, 75, 50], [10, 50, 100]]
-
+    def graph_fn(in_image, in_masks):
+      out_image, out_masks, _ = preprocessor.resize_to_range(
+          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
+      out_image_shape = tf.shape(out_image)
+      out_masks_shape = tf.shape(out_masks)
+      return [out_image_shape, out_masks_shape]
     for (in_image_shape, expected_image_shape, in_masks_shape,
          expected_mask_shape) in zip(in_image_shape_list,
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
-      in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks, _ = preprocessor.resize_to_range(
-          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape],
-            feed_dict={
-                in_image: np.random.randn(*in_image_shape),
-                in_masks: np.random.randn(*in_masks_shape)
-            })
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.random.rand(*in_image_shape).astype('f'),
+              np.random.rand(*in_masks_shape).astype('f'),
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToRangeWithInstanceMasksTensorOfSizeZero(self):
     """Tests image resizing, checking output sizes."""
@@ -3020,24 +3110,24 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     max_dim = 100
     expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]
     expected_masks_shape_list = [[0, 75, 50], [0, 50, 100]]
-
+    def graph_fn(in_image, in_masks):
+      out_image, out_masks, _ = preprocessor.resize_to_range(
+          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
+      out_image_shape = tf.shape(out_image)
+      out_masks_shape = tf.shape(out_masks)
+      return [out_image_shape, out_masks_shape]
     for (in_image_shape, expected_image_shape, in_masks_shape,
          expected_mask_shape) in zip(in_image_shape_list,
                                      expected_image_shape_list,
                                      in_masks_shape_list,
                                      expected_masks_shape_list):
-      in_image = tf.random_uniform(in_image_shape)
-      in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks, _ = preprocessor.resize_to_range(
-          in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
-      out_image_shape = tf.shape(out_image)
-      out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.random.rand(*in_image_shape).astype('f'),
+              np.random.rand(*in_masks_shape).astype('f'),
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToRange4DImageTensor(self):
     image = tf.random_uniform([1, 200, 300, 3])
@@ -3050,16 +3140,16 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     min_dim = 320
     max_dim = 320
     expected_shape_list = [[320, 320, 3], [320, 320, 3]]
-
-    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
+    def graph_fn(in_shape):
       in_image = tf.random_uniform(in_shape)
       out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
-
-      with self.test_session() as sess:
-        out_image_shape = sess.run(out_image_shape)
-        self.assertAllEqual(out_image_shape, expected_shape)
+      return out_image_shape
+    for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
+      out_image_shape = self.execute_cpu(graph_fn, [np.array(in_shape,
+                                                             np.int32)])
+      self.assertAllEqual(out_image_shape, expected_shape)
 
   def testResizeToMaxDimensionTensorShapes(self):
     """Tests both cases where image should and shouldn't be resized."""
@@ -3068,29 +3158,26 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     max_dim = 50
     expected_image_shape_list = [[50, 25, 3], [15, 30, 3]]
     expected_masks_shape_list = [[15, 50, 25], [10, 15, 30]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+    def graph_fn(in_image_shape, in_masks_shape):
+      in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_max_dimension(
           in_image, in_masks, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape],
-            feed_dict={
-                in_image: np.random.randn(*in_image_shape),
-                in_masks: np.random.randn(*in_masks_shape)
-            })
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return [out_image_shape, out_masks_shape]
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.array(in_image_shape, np.int32),
+              np.array(in_masks_shape, np.int32)
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToMaxDimensionWithInstanceMasksTensorOfSizeZero(self):
     """Tests both cases where image should and shouldn't be resized."""
@@ -3100,23 +3187,27 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     expected_image_shape_list = [[50, 25, 3], [15, 30, 3]]
     expected_masks_shape_list = [[0, 50, 25], [0, 15, 30]]
 
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
+    def graph_fn(in_image_shape, in_masks_shape):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_max_dimension(
           in_image, in_masks, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
+      return [out_image_shape, out_masks_shape]
 
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.array(in_image_shape, np.int32),
+              np.array(in_masks_shape, np.int32)
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToMaxDimensionRaisesErrorOn4DImage(self):
     image = tf.random_uniform([1, 200, 300, 3])
@@ -3129,29 +3220,26 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     min_dim = 50
     expected_image_shape_list = [[60, 55, 3], [50, 100, 3]]
     expected_masks_shape_list = [[15, 60, 55], [10, 50, 100]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
-      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+    def graph_fn(in_image_shape, in_masks_shape):
+      in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape],
-            feed_dict={
-                in_image: np.random.randn(*in_image_shape),
-                in_masks: np.random.randn(*in_masks_shape)
-            })
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return [out_image_shape, out_masks_shape]
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.array(in_image_shape, np.int32),
+              np.array(in_masks_shape, np.int32)
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToMinDimensionWithInstanceMasksTensorOfSizeZero(self):
     """Tests image resizing, checking output sizes."""
@@ -3160,177 +3248,230 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
     min_dim = 50
     expected_image_shape_list = [[75, 50, 3], [50, 100, 3]]
     expected_masks_shape_list = [[0, 75, 50], [0, 50, 100]]
-
-    for (in_image_shape, expected_image_shape, in_masks_shape,
-         expected_mask_shape) in zip(in_image_shape_list,
-                                     expected_image_shape_list,
-                                     in_masks_shape_list,
-                                     expected_masks_shape_list):
+    def graph_fn(in_image_shape, in_masks_shape):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
       out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
-
-      with self.test_session() as sess:
-        out_image_shape, out_masks_shape = sess.run(
-            [out_image_shape, out_masks_shape])
-        self.assertAllEqual(out_image_shape, expected_image_shape)
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      return [out_image_shape, out_masks_shape]
+    for (in_image_shape, expected_image_shape, in_masks_shape,
+         expected_mask_shape) in zip(in_image_shape_list,
+                                     expected_image_shape_list,
+                                     in_masks_shape_list,
+                                     expected_masks_shape_list):
+      out_image_shape, out_masks_shape = self.execute_cpu(
+          graph_fn, [
+              np.array(in_image_shape, np.int32),
+              np.array(in_masks_shape, np.int32)
+          ])
+      self.assertAllEqual(out_image_shape, expected_image_shape)
+      self.assertAllEqual(out_masks_shape, expected_mask_shape)
 
   def testResizeToMinDimensionRaisesErrorOn4DImage(self):
     image = tf.random_uniform([1, 200, 300, 3])
     with self.assertRaises(ValueError):
       preprocessor.resize_to_min_dimension(image, 500)
 
+  def testResizePadToMultipleNoMasks(self):
+    """Tests resizing when padding to multiple without masks."""
+    def graph_fn():
+      image = tf.ones((200, 100, 3), dtype=tf.float32)
+      out_image, out_shape = preprocessor.resize_pad_to_multiple(
+          image, multiple=32)
+      return out_image, out_shape
+
+    out_image, out_shape = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_image.sum(), 200 * 100 * 3)
+    self.assertAllEqual(out_shape, (200, 100, 3))
+    self.assertAllEqual(out_image.shape, (224, 128, 3))
+
+  def testResizePadToMultipleWithMasks(self):
+    """Tests resizing when padding to multiple with masks."""
+    def graph_fn():
+      image = tf.ones((200, 100, 3), dtype=tf.float32)
+      masks = tf.ones((10, 200, 100), dtype=tf.float32)
+
+      _, out_masks, out_shape = preprocessor.resize_pad_to_multiple(
+          image, multiple=32, masks=masks)
+      return [out_masks, out_shape]
+
+    out_masks, out_shape = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_masks.sum(), 200 * 100 * 10)
+    self.assertAllEqual(out_shape, (200, 100, 3))
+    self.assertAllEqual(out_masks.shape, (10, 224, 128))
+
+  def testResizePadToMultipleEmptyMasks(self):
+    """Tests resizing when padding to multiple with an empty mask."""
+    def graph_fn():
+      image = tf.ones((200, 100, 3), dtype=tf.float32)
+      masks = tf.ones((0, 200, 100), dtype=tf.float32)
+      _, out_masks, out_shape = preprocessor.resize_pad_to_multiple(
+          image, multiple=32, masks=masks)
+      return [out_masks, out_shape]
+    out_masks, out_shape = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(out_shape, (200, 100, 3))
+    self.assertAllEqual(out_masks.shape, (0, 224, 128))
+
   def testScaleBoxesToPixelCoordinates(self):
     """Tests box scaling, checking scaled values."""
-    in_shape = [60, 40, 3]
-    in_boxes = [[0.1, 0.2, 0.4, 0.6],
-                [0.5, 0.3, 0.9, 0.7]]
-
+    def graph_fn():
+      in_shape = [60, 40, 3]
+      in_boxes = [[0.1, 0.2, 0.4, 0.6],
+                  [0.5, 0.3, 0.9, 0.7]]
+      in_image = tf.random_uniform(in_shape)
+      in_boxes = tf.constant(in_boxes)
+      _, out_boxes = preprocessor.scale_boxes_to_pixel_coordinates(
+          in_image, boxes=in_boxes)
+      return out_boxes
     expected_boxes = [[6., 8., 24., 24.],
                       [30., 12., 54., 28.]]
-
-    in_image = tf.random_uniform(in_shape)
-    in_boxes = tf.constant(in_boxes)
-    _, out_boxes = preprocessor.scale_boxes_to_pixel_coordinates(
-        in_image, boxes=in_boxes)
-    with self.test_session() as sess:
-      out_boxes = sess.run(out_boxes)
-      self.assertAllClose(out_boxes, expected_boxes)
+    out_boxes = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_boxes, expected_boxes)
 
   def testScaleBoxesToPixelCoordinatesWithKeypoints(self):
     """Tests box and keypoint scaling, checking scaled values."""
-    in_shape = [60, 40, 3]
-    in_boxes = self.createTestBoxes()
-    in_keypoints = self.createTestKeypoints()
-
+    def graph_fn():
+      in_shape = [60, 40, 3]
+      in_boxes = self.createTestBoxes()
+      in_keypoints, _ = self.createTestKeypoints()
+      in_image = tf.random_uniform(in_shape)
+      (_, out_boxes,
+       out_keypoints) = preprocessor.scale_boxes_to_pixel_coordinates(
+           in_image, boxes=in_boxes, keypoints=in_keypoints)
+      return out_boxes, out_keypoints
     expected_boxes = [[0., 10., 45., 40.],
                       [15., 20., 45., 40.]]
     expected_keypoints = [
         [[6., 4.], [12., 8.], [18., 12.]],
         [[24., 16.], [30., 20.], [36., 24.]],
     ]
-
-    in_image = tf.random_uniform(in_shape)
-    _, out_boxes, out_keypoints = preprocessor.scale_boxes_to_pixel_coordinates(
-        in_image, boxes=in_boxes, keypoints=in_keypoints)
-    with self.test_session() as sess:
-      out_boxes_, out_keypoints_ = sess.run([out_boxes, out_keypoints])
-      self.assertAllClose(out_boxes_, expected_boxes)
-      self.assertAllClose(out_keypoints_, expected_keypoints)
+    out_boxes_, out_keypoints_ = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_boxes_, expected_boxes)
+    self.assertAllClose(out_keypoints_, expected_keypoints)
 
   def testSubtractChannelMean(self):
     """Tests whether channel means have been subtracted."""
-    with self.test_session():
+    def graph_fn():
       image = tf.zeros((240, 320, 3))
       means = [1, 2, 3]
       actual = preprocessor.subtract_channel_mean(image, means=means)
-      actual = actual.eval()
-
-      self.assertTrue((actual[:, :, 0] == -1).all())
-      self.assertTrue((actual[:, :, 1] == -2).all())
-      self.assertTrue((actual[:, :, 2] == -3).all())
+      return actual
+    actual = self.execute_cpu(graph_fn, [])
+    self.assertTrue((actual[:, :, 0], -1))
+    self.assertTrue((actual[:, :, 1], -2))
+    self.assertTrue((actual[:, :, 2], -3))
 
   def testOneHotEncoding(self):
     """Tests one hot encoding of multiclass labels."""
-    with self.test_session():
+    def graph_fn():
       labels = tf.constant([1, 4, 2], dtype=tf.int32)
       one_hot = preprocessor.one_hot_encoding(labels, num_classes=5)
-      one_hot = one_hot.eval()
+      return one_hot
+    one_hot = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual([0, 1, 1, 0, 1], one_hot)
 
-      self.assertAllEqual([0, 1, 1, 0, 1], one_hot)
+  def testRandomSelfConcatImageVertically(self):
 
-  def testRandomSelfConcatImage(self):
-    tf.set_random_seed(24601)
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      confidences = weights
+      scores = self.createTestMultiClassScores()
 
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    confidences = weights
-    scores = self.createTestMultiClassScores()
+      tensor_dict = {
+          fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_confidences: confidences,
+          fields.InputDataFields.multiclass_scores: scores,
+      }
 
-    tensor_dict = {
-        fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-        fields.InputDataFields.groundtruth_confidences: confidences,
-        fields.InputDataFields.multiclass_scores: scores,
-    }
+      preprocessing_options = [(preprocessor.random_self_concat_image, {
+          'concat_vertical_probability': 1.0,
+          'concat_horizontal_probability': 0.0,
+      })]
+      func_arg_map = preprocessor.get_default_func_arg_map(
+          True, True, True)
+      output_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=func_arg_map)
+
+      original_shape = tf.shape(images)[1:3]
+      final_shape = tf.shape(output_tensor_dict[fields.InputDataFields.image])[
+          1:3]
+      return [
+          original_shape,
+          boxes,
+          labels,
+          confidences,
+          scores,
+          final_shape,
+          output_tensor_dict[fields.InputDataFields.groundtruth_boxes],
+          output_tensor_dict[fields.InputDataFields.groundtruth_classes],
+          output_tensor_dict[fields.InputDataFields.groundtruth_confidences],
+          output_tensor_dict[fields.InputDataFields.multiclass_scores],
+      ]
+    (original_shape, boxes, labels, confidences, scores, final_shape, new_boxes,
+     new_labels, new_confidences, new_scores) = self.execute(graph_fn, [])
+    self.assertAllEqual(final_shape, original_shape * np.array([2, 1]))
+    self.assertAllEqual(2 * boxes.size, new_boxes.size)
+    self.assertAllEqual(2 * labels.size, new_labels.size)
+    self.assertAllEqual(2 * confidences.size, new_confidences.size)
+    self.assertAllEqual(2 * scores.size, new_scores.size)
+
+  def testRandomSelfConcatImageHorizontally(self):
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      confidences = weights
+      scores = self.createTestMultiClassScores()
 
-    preprocessing_options = [(preprocessor.random_self_concat_image, {
-        'concat_vertical_probability': 0.5,
-        'concat_horizontal_probability': 0.5,
-        'seed': 24601,
-    })]
-    func_arg_map = preprocessor.get_default_func_arg_map(
-        True, True, True)
-    output_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=func_arg_map)
-
-    final_shape = tf.shape(output_tensor_dict[fields.InputDataFields.image])[
-        1:3]
-
-    with self.test_session() as sess:
-      outputs = []
-
-      augment_height_only = False
-      augment_width_only = False
-
-      for _ in range(50):
-        original_boxes = sess.run(boxes)
-        shape, new_boxes, new_labels, new_confidences, new_scores = sess.run(
-            [final_shape,
-             output_tensor_dict[fields.InputDataFields.groundtruth_boxes],
-             output_tensor_dict[fields.InputDataFields.groundtruth_classes],
-             output_tensor_dict[fields.InputDataFields.groundtruth_confidences],
-             output_tensor_dict[fields.InputDataFields.multiclass_scores],
-            ])
-        shape = np.array(shape)
-        outputs.append(shape)
-
-        if np.array_equal(shape, [8, 4]):
-          augment_height_only = True
-          self.assertEqual(
-              new_boxes.shape[0], 2 * boxes.shape[0])
-
-          self.assertAllClose(new_boxes[:2, :] * [2.0, 1.0, 2.0, 1.0],
-                              original_boxes)
-          self.assertAllClose(
-              (new_boxes[2:, :] - [0.5, 0.0, 0.5, 0.0]) * [
-                  2.0, 1.0, 2.0, 1.0],
-              original_boxes)
-        elif np.array_equal(shape, [4, 8]):
-          augment_width_only = True
-          self.assertEqual(
-              new_boxes.shape[0], 2 * boxes.shape[0])
-
-          self.assertAllClose(new_boxes[:2, :] * [1.0, 2.0, 1.0, 2.0],
-                              original_boxes)
-          self.assertAllClose(
-              (new_boxes[2:, :] - [0.0, 0.5, 0.0, 0.5]) * [
-                  1.0, 2.0, 1.0, 2.0],
-              original_boxes)
-
-        augmentation_factor = new_boxes.shape[0] / boxes.shape[0].value
-        self.assertEqual(new_labels.shape[0],
-                         labels.shape[0].value * augmentation_factor)
-        self.assertEqual(new_confidences.shape[0],
-                         confidences.shape[0].value * augmentation_factor)
-        self.assertEqual(new_scores.shape[0],
-                         scores.shape[0].value * augmentation_factor)
-
-      max_height = max(x[0] for x in outputs)
-      max_width = max(x[1] for x in outputs)
-
-      self.assertEqual(max_height, 8)
-      self.assertEqual(max_width, 8)
-      self.assertEqual(augment_height_only, True)
-      self.assertEqual(augment_width_only, True)
+      tensor_dict = {
+          fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+          fields.InputDataFields.groundtruth_confidences: confidences,
+          fields.InputDataFields.multiclass_scores: scores,
+      }
+
+      preprocessing_options = [(preprocessor.random_self_concat_image, {
+          'concat_vertical_probability': 0.0,
+          'concat_horizontal_probability': 1.0,
+      })]
+      func_arg_map = preprocessor.get_default_func_arg_map(
+          True, True, True)
+      output_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=func_arg_map)
+
+      original_shape = tf.shape(images)[1:3]
+      final_shape = tf.shape(output_tensor_dict[fields.InputDataFields.image])[
+          1:3]
+      return [
+          original_shape,
+          boxes,
+          labels,
+          confidences,
+          scores,
+          final_shape,
+          output_tensor_dict[fields.InputDataFields.groundtruth_boxes],
+          output_tensor_dict[fields.InputDataFields.groundtruth_classes],
+          output_tensor_dict[fields.InputDataFields.groundtruth_confidences],
+          output_tensor_dict[fields.InputDataFields.multiclass_scores],
+      ]
+    (original_shape, boxes, labels, confidences, scores, final_shape, new_boxes,
+     new_labels, new_confidences, new_scores) = self.execute(graph_fn, [])
+    self.assertAllEqual(final_shape, original_shape * np.array([1, 2]))
+    self.assertAllEqual(2 * boxes.size, new_boxes.size)
+    self.assertAllEqual(2 * labels.size, new_labels.size)
+    self.assertAllEqual(2 * confidences.size, new_confidences.size)
+    self.assertAllEqual(2 * scores.size, new_scores.size)
 
   def testSSDRandomCropWithCache(self):
     preprocess_options = [
@@ -3347,133 +3488,133 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                 test_keypoints=False)
 
   def testSSDRandomCrop(self):
-    preprocessing_options = [
-        (preprocessor.normalize_image, {
-            'original_minval': 0,
-            'original_maxval': 255,
-            'target_minval': 0,
-            'target_maxval': 1
-        }),
-        (preprocessor.ssd_random_crop, {})]
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = sess.run(
-           [boxes_rank, distorted_boxes_rank, images_rank,
-            distorted_images_rank])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+    def graph_fn():
+      preprocessing_options = [
+          (preprocessor.normalize_image, {
+              'original_minval': 0,
+              'original_maxval': 255,
+              'target_minval': 0,
+              'target_maxval': 1
+          }),
+          (preprocessor.ssd_random_crop, {})]
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      return [boxes_rank, distorted_boxes_rank, images_rank,
+              distorted_images_rank]
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testSSDRandomCropWithMultiClassScores(self):
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }), (preprocessor.ssd_random_crop, {})]
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    multiclass_scores = self.createTestMultiClassScores()
+    def graph_fn():
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }), (preprocessor.ssd_random_crop, {})]
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      multiclass_scores = self.createTestMultiClassScores()
 
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.multiclass_scores: multiclass_scores,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_multiclass_scores=True)
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    distorted_multiclass_scores = distorted_tensor_dict[
-        fields.InputDataFields.multiclass_scores]
-
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-    multiclass_scores_rank = tf.rank(multiclass_scores)
-    distorted_multiclass_scores_rank = tf.rank(distorted_multiclass_scores)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_, multiclass_scores_rank_,
-       distorted_multiclass_scores_,
-       distorted_multiclass_scores_rank_) = sess.run([
-           boxes_rank, distorted_boxes, distorted_boxes_rank, images_rank,
-           distorted_images_rank, multiclass_scores_rank,
-           distorted_multiclass_scores, distorted_multiclass_scores_rank
-       ])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
-      self.assertAllEqual(multiclass_scores_rank_,
-                          distorted_multiclass_scores_rank_)
-      self.assertAllEqual(distorted_boxes_.shape[0],
-                          distorted_multiclass_scores_.shape[0])
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.multiclass_scores: multiclass_scores,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_multiclass_scores=True)
+      distorted_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      distorted_multiclass_scores = distorted_tensor_dict[
+          fields.InputDataFields.multiclass_scores]
+
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      multiclass_scores_rank = tf.rank(multiclass_scores)
+      distorted_multiclass_scores_rank = tf.rank(distorted_multiclass_scores)
+      return [
+          boxes_rank, distorted_boxes, distorted_boxes_rank, images_rank,
+          distorted_images_rank, multiclass_scores_rank,
+          distorted_multiclass_scores, distorted_multiclass_scores_rank
+      ]
+
+    (boxes_rank_, distorted_boxes_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_, multiclass_scores_rank_,
+     distorted_multiclass_scores_,
+     distorted_multiclass_scores_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
+    self.assertAllEqual(multiclass_scores_rank_,
+                        distorted_multiclass_scores_rank_)
+    self.assertAllEqual(distorted_boxes_.shape[0],
+                        distorted_multiclass_scores_.shape[0])
 
   def testSSDRandomCropPad(self):
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    preprocessing_options = [
-        (preprocessor.normalize_image, {
-            'original_minval': 0,
-            'original_maxval': 255,
-            'target_minval': 0,
-            'target_maxval': 1
-        }),
-        (preprocessor.ssd_random_crop_pad, {})]
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights,
-    }
-    distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                    preprocessing_options)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = sess.run([
-           boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
-       ])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      preprocessing_options = [
+          (preprocessor.normalize_image, {
+              'original_minval': 0,
+              'original_maxval': 255,
+              'target_minval': 0,
+              'target_maxval': 1
+          }),
+          (preprocessor.ssd_random_crop_pad, {})]
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights,
+      }
+      distorted_tensor_dict = preprocessor.preprocess(tensor_dict,
+                                                      preprocessing_options)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      return [
+          boxes_rank, distorted_boxes_rank, images_rank, distorted_images_rank
+      ]
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testSSDRandomCropFixedAspectRatioWithCache(self):
     preprocess_options = [
@@ -3493,54 +3634,54 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                          include_multiclass_scores,
                                          include_instance_masks,
                                          include_keypoints):
-    images = self.createTestImages()
-    boxes = self.createTestBoxes()
-    labels = self.createTestLabels()
-    weights = self.createTestGroundtruthWeights()
-    preprocessing_options = [(preprocessor.normalize_image, {
-        'original_minval': 0,
-        'original_maxval': 255,
-        'target_minval': 0,
-        'target_maxval': 1
-    }), (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]
-    tensor_dict = {
-        fields.InputDataFields.image: images,
-        fields.InputDataFields.groundtruth_boxes: boxes,
-        fields.InputDataFields.groundtruth_classes: labels,
-        fields.InputDataFields.groundtruth_weights: weights
-    }
-    if include_multiclass_scores:
-      multiclass_scores = self.createTestMultiClassScores()
-      tensor_dict[fields.InputDataFields.multiclass_scores] = (
-          multiclass_scores)
-    if include_instance_masks:
-      masks = self.createTestMasks()
-      tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks
-    if include_keypoints:
-      keypoints = self.createTestKeypoints()
-      tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints
+    def graph_fn():
+      images = self.createTestImages()
+      boxes = self.createTestBoxes()
+      labels = self.createTestLabels()
+      weights = self.createTestGroundtruthWeights()
+      preprocessing_options = [(preprocessor.normalize_image, {
+          'original_minval': 0,
+          'original_maxval': 255,
+          'target_minval': 0,
+          'target_maxval': 1
+      }), (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+          fields.InputDataFields.groundtruth_boxes: boxes,
+          fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_weights: weights
+      }
+      if include_multiclass_scores:
+        multiclass_scores = self.createTestMultiClassScores()
+        tensor_dict[fields.InputDataFields.multiclass_scores] = (
+            multiclass_scores)
+      if include_instance_masks:
+        masks = self.createTestMasks()
+        tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks
+      if include_keypoints:
+        keypoints, _ = self.createTestKeypoints()
+        tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints
 
-    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        include_multiclass_scores=include_multiclass_scores,
-        include_instance_masks=include_instance_masks,
-        include_keypoints=include_keypoints)
-    distorted_tensor_dict = preprocessor.preprocess(
-        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-    distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
-    distorted_boxes = distorted_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes]
-    images_rank = tf.rank(images)
-    distorted_images_rank = tf.rank(distorted_images)
-    boxes_rank = tf.rank(boxes)
-    distorted_boxes_rank = tf.rank(distorted_boxes)
-
-    with self.test_session() as sess:
-      (boxes_rank_, distorted_boxes_rank_, images_rank_,
-       distorted_images_rank_) = sess.run(
-           [boxes_rank, distorted_boxes_rank, images_rank,
-            distorted_images_rank])
-      self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
-      self.assertAllEqual(images_rank_, distorted_images_rank_)
+      preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+          include_multiclass_scores=include_multiclass_scores,
+          include_instance_masks=include_instance_masks,
+          include_keypoints=include_keypoints)
+      distorted_tensor_dict = preprocessor.preprocess(
+          tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
+      distorted_images = distorted_tensor_dict[fields.InputDataFields.image]
+      distorted_boxes = distorted_tensor_dict[
+          fields.InputDataFields.groundtruth_boxes]
+      images_rank = tf.rank(images)
+      distorted_images_rank = tf.rank(distorted_images)
+      boxes_rank = tf.rank(boxes)
+      distorted_boxes_rank = tf.rank(distorted_boxes)
+      return [boxes_rank, distorted_boxes_rank, images_rank,
+              distorted_images_rank]
+
+    (boxes_rank_, distorted_boxes_rank_, images_rank_,
+     distorted_images_rank_) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
+    self.assertAllEqual(images_rank_, distorted_images_rank_)
 
   def testSSDRandomCropFixedAspectRatio(self):
     self._testSSDRandomCropFixedAspectRatio(include_multiclass_scores=False,
@@ -3563,22 +3704,66 @@ class PreprocessorTest(tf.test.TestCase, parameterized.TestCase):
                                             include_keypoints=True)
 
   def testConvertClassLogitsToSoftmax(self):
-    multiclass_scores = tf.constant(
-        [[1.0, 0.0], [0.5, 0.5], [1000, 1]], dtype=tf.float32)
-    temperature = 2.0
-
-    converted_multiclass_scores = (
-        preprocessor.convert_class_logits_to_softmax(
-            multiclass_scores=multiclass_scores, temperature=temperature))
-
-    expected_converted_multiclass_scores = [[[0.62245935, 0.37754068],
-                                             [0.5, 0.5], [1, 0]]]
-
-    with self.test_session() as sess:
-      (converted_multiclass_scores_) = sess.run([converted_multiclass_scores])
-
-      self.assertAllClose(converted_multiclass_scores_,
-                          expected_converted_multiclass_scores)
+    def graph_fn():
+      multiclass_scores = tf.constant(
+          [[1.0, 0.0], [0.5, 0.5], [1000, 1]], dtype=tf.float32)
+      temperature = 2.0
+
+      converted_multiclass_scores = (
+          preprocessor.convert_class_logits_to_softmax(
+              multiclass_scores=multiclass_scores, temperature=temperature))
+      return converted_multiclass_scores
+    converted_multiclass_scores_ = self.execute_cpu(graph_fn, [])
+    expected_converted_multiclass_scores = [[0.62245935, 0.37754068],
+                                            [0.5, 0.5],
+                                            [1, 0]]
+    self.assertAllClose(converted_multiclass_scores_,
+                        expected_converted_multiclass_scores)
+
+  @parameterized.named_parameters(
+      ('scale_1', 1.0),
+      ('scale_1.5', 1.5),
+      ('scale_0.5', 0.5)
+  )
+  def test_square_crop_by_scale(self, scale):
+    def graph_fn():
+      image = np.random.randn(256, 256, 1)
+
+      masks = tf.constant(image[:, :, 0].reshape(1, 256, 256))
+      image = tf.constant(image)
+      keypoints = tf.constant([[[0.25, 0.25], [0.75, 0.75]]])
+
+      boxes = tf.constant([[0.25, .25, .75, .75]])
+      labels = tf.constant([[1]])
+      label_weights = tf.constant([[1.]])
+
+      (new_image, new_boxes, _, _, new_masks,
+       new_keypoints) = preprocessor.random_square_crop_by_scale(
+           image,
+           boxes,
+           labels,
+           label_weights,
+           masks=masks,
+           keypoints=keypoints,
+           max_border=256,
+           scale_min=scale,
+           scale_max=scale)
+      return new_image, new_boxes, new_masks, new_keypoints
+    image, boxes, masks, keypoints = self.execute_cpu(graph_fn, [])
+    ymin, xmin, ymax, xmax = boxes[0]
+    self.assertAlmostEqual(ymax - ymin, 0.5 / scale)
+    self.assertAlmostEqual(xmax - xmin, 0.5 / scale)
+
+    k1 = keypoints[0, 0]
+    k2 = keypoints[0, 1]
+    self.assertAlmostEqual(k2[0] - k1[0], 0.5 / scale)
+    self.assertAlmostEqual(k2[1] - k1[1], 0.5 / scale)
+
+    size = max(image.shape)
+    self.assertAlmostEqual(scale * 256.0, size)
+
+    self.assertAllClose(image[:, :, 0],
+                        masks[0, :, :])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/region_similarity_calculator_test.py b/research/object_detection/core/region_similarity_calculator_test.py
index 1d0c26bb..64f6ff39 100644
--- a/research/object_detection/core/region_similarity_calculator_test.py
+++ b/research/object_detection/core/region_similarity_calculator_test.py
@@ -19,76 +19,79 @@ import tensorflow as tf
 from object_detection.core import box_list
 from object_detection.core import region_similarity_calculator
 from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
 
 
-class RegionSimilarityCalculatorTest(tf.test.TestCase):
+class RegionSimilarityCalculatorTest(test_case.TestCase):
 
   def test_get_correct_pairwise_similarity_based_on_iou(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      iou_similarity_calculator = region_similarity_calculator.IouSimilarity()
+      iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)
+      return iou_similarity
     exp_output = [[2.0 / 16.0, 0, 6.0 / 400.0], [1.0 / 16.0, 0.0, 5.0 / 400.0]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    iou_similarity_calculator = region_similarity_calculator.IouSimilarity()
-    iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)
-    with self.test_session() as sess:
-      iou_output = sess.run(iou_similarity)
-      self.assertAllClose(iou_output, exp_output)
+    iou_output = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output, exp_output)
 
   def test_get_correct_pairwise_similarity_based_on_squared_distances(self):
-    corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],
-                            [1.0, 1.0, 0.0, 2.0]])
-    corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],
-                            [-4.0, 0.0, 0.0, 3.0],
-                            [0.0, 0.0, 0.0, 0.0]])
+    def graph_fn():
+      corners1 = tf.constant([[0.0, 0.0, 0.0, 0.0],
+                              [1.0, 1.0, 0.0, 2.0]])
+      corners2 = tf.constant([[3.0, 4.0, 1.0, 0.0],
+                              [-4.0, 0.0, 0.0, 3.0],
+                              [0.0, 0.0, 0.0, 0.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      dist_similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
+      dist_similarity = dist_similarity_calc.compare(boxes1, boxes2)
+      return dist_similarity
     exp_output = [[-26, -25, 0], [-18, -27, -6]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    dist_similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    dist_similarity = dist_similarity_calc.compare(boxes1, boxes2)
-    with self.test_session() as sess:
-      dist_output = sess.run(dist_similarity)
-      self.assertAllClose(dist_output, exp_output)
+    iou_output = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output, exp_output)
 
   def test_get_correct_pairwise_similarity_based_on_ioa(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      boxes1 = box_list.BoxList(corners1)
+      boxes2 = box_list.BoxList(corners2)
+      ioa_similarity_calculator = region_similarity_calculator.IoaSimilarity()
+      ioa_similarity_1 = ioa_similarity_calculator.compare(boxes1, boxes2)
+      ioa_similarity_2 = ioa_similarity_calculator.compare(boxes2, boxes1)
+      return ioa_similarity_1, ioa_similarity_2
     exp_output_1 = [[2.0 / 12.0, 0, 6.0 / 400.0],
                     [1.0 / 12.0, 0.0, 5.0 / 400.0]]
     exp_output_2 = [[2.0 / 6.0, 1.0 / 5.0],
                     [0, 0],
                     [6.0 / 6.0, 5.0 / 5.0]]
-    boxes1 = box_list.BoxList(corners1)
-    boxes2 = box_list.BoxList(corners2)
-    ioa_similarity_calculator = region_similarity_calculator.IoaSimilarity()
-    ioa_similarity_1 = ioa_similarity_calculator.compare(boxes1, boxes2)
-    ioa_similarity_2 = ioa_similarity_calculator.compare(boxes2, boxes1)
-    with self.test_session() as sess:
-      iou_output_1, iou_output_2 = sess.run(
-          [ioa_similarity_1, ioa_similarity_2])
-      self.assertAllClose(iou_output_1, exp_output_1)
-      self.assertAllClose(iou_output_2, exp_output_2)
+    iou_output_1, iou_output_2 = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output_1, exp_output_1)
+    self.assertAllClose(iou_output_2, exp_output_2)
 
   def test_get_correct_pairwise_similarity_based_on_thresholded_iou(self):
-    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
-    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
-                            [0.0, 0.0, 20.0, 20.0]])
-    scores = tf.constant([.3, .6])
-    iou_threshold = .013
-
+    def graph_fn():
+      corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+      corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                              [0.0, 0.0, 20.0, 20.0]])
+      scores = tf.constant([.3, .6])
+      iou_threshold = .013
+      boxes1 = box_list.BoxList(corners1)
+      boxes1.add_field(fields.BoxListFields.scores, scores)
+      boxes2 = box_list.BoxList(corners2)
+      iou_similarity_calculator = (
+          region_similarity_calculator.ThresholdedIouSimilarity(
+              iou_threshold=iou_threshold))
+      iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)
+      return iou_similarity
     exp_output = tf.constant([[0.3, 0., 0.3], [0.6, 0., 0.]])
-    boxes1 = box_list.BoxList(corners1)
-    boxes1.add_field(fields.BoxListFields.scores, scores)
-    boxes2 = box_list.BoxList(corners2)
-    iou_similarity_calculator = (
-        region_similarity_calculator.ThresholdedIouSimilarity(
-            iou_threshold=iou_threshold))
-    iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)
-    with self.test_session() as sess:
-      iou_output = sess.run(iou_similarity)
-      self.assertAllClose(iou_output, exp_output)
+    iou_output = self.execute(graph_fn, [])
+    self.assertAllClose(iou_output, exp_output)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index 628902eb..d1fe3628 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -42,6 +42,8 @@ class InputDataFields(object):
     filename: original filename of the dataset (without common path).
     groundtruth_image_classes: image-level class labels.
     groundtruth_image_confidences: image-level class confidences.
+    groundtruth_labeled_classes: image-level annotation that indicates the
+      classes for which an image has been labeled.
     groundtruth_boxes: coordinates of the ground truth boxes in the image.
     groundtruth_classes: box-level class labels.
     groundtruth_confidences: box-level class confidences. The shape should be
@@ -61,6 +63,7 @@ class InputDataFields(object):
     groundtruth_instance_classes: instance mask-level class labels.
     groundtruth_keypoints: ground truth keypoints.
     groundtruth_keypoint_visibilities: ground truth keypoint visibilities.
+    groundtruth_keypoint_weights: groundtruth weight factor for keypoints.
     groundtruth_label_weights: groundtruth label weights.
     groundtruth_weights: groundtruth weight factor for bounding boxes.
     num_groundtruth_boxes: number of groundtruth boxes.
@@ -68,6 +71,11 @@ class InputDataFields(object):
     true_image_shapes: true shapes of images in the resized images, as resized
       images can be padded with zeros.
     multiclass_scores: the label score per class for each box.
+    context_features: a flattened list of contextual features.
+    context_feature_length: the fixed length of each feature in
+      context_features, used for reshaping.
+    valid_context_size: the valid context size, used in filtering the padded
+      context features.
   """
   image = 'image'
   image_additional_channels = 'image_additional_channels'
@@ -78,6 +86,7 @@ class InputDataFields(object):
   filename = 'filename'
   groundtruth_image_classes = 'groundtruth_image_classes'
   groundtruth_image_confidences = 'groundtruth_image_confidences'
+  groundtruth_labeled_classes = 'groundtruth_labeled_classes'
   groundtruth_boxes = 'groundtruth_boxes'
   groundtruth_classes = 'groundtruth_classes'
   groundtruth_confidences = 'groundtruth_confidences'
@@ -93,12 +102,16 @@ class InputDataFields(object):
   groundtruth_instance_classes = 'groundtruth_instance_classes'
   groundtruth_keypoints = 'groundtruth_keypoints'
   groundtruth_keypoint_visibilities = 'groundtruth_keypoint_visibilities'
+  groundtruth_keypoint_weights = 'groundtruth_keypoint_weights'
   groundtruth_label_weights = 'groundtruth_label_weights'
   groundtruth_weights = 'groundtruth_weights'
   num_groundtruth_boxes = 'num_groundtruth_boxes'
   is_annotated = 'is_annotated'
   true_image_shape = 'true_image_shape'
   multiclass_scores = 'multiclass_scores'
+  context_features = 'context_features'
+  context_feature_length = 'context_feature_length'
+  valid_context_size = 'valid_context_size'
 
 
 class DetectionResultFields(object):
@@ -115,6 +128,7 @@ class DetectionResultFields(object):
     detection_masks: contains a segmentation mask for each detection box.
     detection_boundaries: contains an object boundary for each detection box.
     detection_keypoints: contains detection keypoints for each detection box.
+    detection_keypoint_scores: contains detection keypoint scores.
     num_detections: number of detections in the batch.
     raw_detection_boxes: contains decoded detection boxes without Non-Max
       suppression.
@@ -134,6 +148,7 @@ class DetectionResultFields(object):
   detection_masks = 'detection_masks'
   detection_boundaries = 'detection_boundaries'
   detection_keypoints = 'detection_keypoints'
+  detection_keypoint_scores = 'detection_keypoint_scores'
   num_detections = 'num_detections'
   raw_detection_boxes = 'raw_detection_boxes'
   raw_detection_scores = 'raw_detection_scores'
@@ -164,6 +179,7 @@ class BoxListFields(object):
   masks = 'masks'
   boundaries = 'boundaries'
   keypoints = 'keypoints'
+  keypoint_visibilities = 'keypoint_visibilities'
   keypoint_heatmaps = 'keypoint_heatmaps'
   is_crowd = 'is_crowd'
 
@@ -201,6 +217,7 @@ class TfExampleFields(object):
     source_id: original source of the image
     image_class_text: image-level label in text format
     image_class_label: image-level label in numerical format
+    image_class_confidence: image-level confidence of the label
     object_class_text: labels in text format, e.g. ["person", "cat"]
     object_class_label: labels in numbers, e.g. [16, 8]
     object_bbox_xmin: xmin coordinates of groundtruth box, e.g. 10, 30
@@ -237,6 +254,7 @@ class TfExampleFields(object):
   source_id = 'image/source_id'
   image_class_text = 'image/class/text'
   image_class_label = 'image/class/label'
+  image_class_confidence = 'image/class/confidence'
   object_class_text = 'image/object/class/text'
   object_class_label = 'image/object/class/label'
   object_bbox_ymin = 'image/object/bbox/ymin'
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 3e3ba1ae..fdbc36e1 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -44,12 +44,17 @@ from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_coder
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
+from object_detection.core import keypoint_ops
 from object_detection.core import matcher as mat
 from object_detection.core import region_similarity_calculator as sim_calc
 from object_detection.core import standard_fields as fields
 from object_detection.matchers import argmax_matcher
 from object_detection.matchers import bipartite_matcher
 from object_detection.utils import shape_utils
+from object_detection.utils import target_assigner_utils as ta_utils
+
+
+_DEFAULT_KEYPOINT_OFFSET_STD_DEV = 1.0
 
 
 class TargetAssigner(object):
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index 1ac67f2b..a17c4c34 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -25,6 +25,7 @@ from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner as targetassigner
 from object_detection.matchers import argmax_matcher
 from object_detection.matchers import bipartite_matcher
+from object_detection.utils import np_box_ops
 from object_detection.utils import test_case
 
 
@@ -65,10 +66,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_class_agnostic_with_ignored_matches(self):
     # Note: test is very similar to above. The third box matched with an IOU
@@ -108,10 +109,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_agnostic_with_keypoints(self):
     def graph_fn(anchor_means, groundtruth_box_corners,
@@ -158,10 +159,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_class_agnostic_with_keypoints_and_ignored_matches(self):
     # Note: test is very similar to above. The third box matched with an IOU
@@ -213,10 +214,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_multiclass(self):
 
@@ -271,10 +272,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_multiclass_with_groundtruth_weights(self):
 
@@ -379,10 +380,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_assign_empty_groundtruth(self):
 
@@ -431,10 +432,10 @@ class TargetAssignerTest(test_case.TestCase):
     self.assertAllClose(cls_weights_out, exp_cls_weights)
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
-    self.assertEquals(cls_targets_out.dtype, np.float32)
-    self.assertEquals(cls_weights_out.dtype, np.float32)
-    self.assertEquals(reg_targets_out.dtype, np.float32)
-    self.assertEquals(reg_weights_out.dtype, np.float32)
+    self.assertEqual(cls_targets_out.dtype, np.float32)
+    self.assertEqual(cls_weights_out.dtype, np.float32)
+    self.assertEqual(reg_targets_out.dtype, np.float32)
+    self.assertEqual(reg_weights_out.dtype, np.float32)
 
   def test_raises_error_on_incompatible_groundtruth_boxes_and_labels(self):
     similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
@@ -1228,5 +1229,7 @@ class CreateTargetAssignerTest(tf.test.TestCase):
                                             stage='invalid_stage')
 
 
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/data/face_person_with_keypoints_label_map.pbtxt b/research/object_detection/data/face_person_with_keypoints_label_map.pbtxt
new file mode 100644
index 00000000..181f11b2
--- /dev/null
+++ b/research/object_detection/data/face_person_with_keypoints_label_map.pbtxt
@@ -0,0 +1,102 @@
+item: {
+ id: 1
+ name: 'face'
+ display_name: 'face'
+ keypoints {
+   id: 0
+   label: "left_eye_center"
+ }
+ keypoints {
+   id: 1
+   label: "right_eye_center"
+ }
+ keypoints {
+   id: 2
+   label: "nose_tip"
+ }
+ keypoints {
+   id: 3
+   label: "mouth_center"
+ }
+ keypoints {
+   id: 4
+   label: "left_ear_tragion"
+ }
+ keypoints {
+   id: 5
+   label: "right_ear_tragion"
+ }
+}
+item: {
+ id: 2
+ name: 'Person'
+ display_name: 'PERSON'
+ keypoints {
+   id: 6
+   label: "NOSE_TIP"
+ }
+ keypoints {
+   id: 7
+   label: "LEFT_EYE"
+ }
+ keypoints {
+   id: 8
+   label: "RIGHT_EYE"
+ }
+ keypoints {
+   id: 9
+   label: "LEFT_EAR_TRAGION"
+ }
+ keypoints {
+   id: 10
+   label: "RIGHT_EAR_TRAGION"
+ }
+ keypoints {
+   id: 11
+   label: "LEFT_SHOULDER"
+ }
+ keypoints {
+   id: 12
+   label: "RIGHT_SHOULDER"
+ }
+ keypoints {
+   id: 13
+   label: "LEFT_ELBOW"
+ }
+ keypoints {
+   id: 14
+   label: "RIGHT_ELBOW"
+ }
+ keypoints {
+   id: 15
+   label: "LEFT_WRIST"
+ }
+ keypoints {
+   id: 16
+   label: "RIGHT_WRIST"
+ }
+ keypoints {
+   id: 17
+   label: "LEFT_HIP"
+ }
+ keypoints {
+   id: 18
+   label: "RIGHT_HIP"
+ }
+ keypoints {
+   id: 19
+   label: "LEFT_KNEE"
+ }
+ keypoints {
+   id: 20
+   label: "RIGHT_KNEE"
+ }
+ keypoints {
+   id: 21
+   label: "LEFT_ANKLE"
+ }
+ keypoints {
+   id: 22
+   label: "RIGHT_ANKLE"
+ }
+}
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index acd112d3..c159eeac 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -21,6 +21,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import enum
+import numpy as np
 from six.moves import zip
 import tensorflow as tf
 
@@ -29,7 +31,27 @@ from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import label_map_util
 
-slim_example_decoder = tf.contrib.slim.tfexample_decoder
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import lookup as contrib_lookup
+  from tensorflow.contrib.slim import tfexample_decoder as slim_example_decoder
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
+
+class Visibility(enum.Enum):
+  """Visibility definitions.
+
+  This follows the MS Coco convention (http://cocodataset.org/#format-data).
+  """
+  # Keypoint is not labeled.
+  UNLABELED = 0
+  # Keypoint is labeled but falls outside the object segment (e.g. occluded).
+  NOT_VISIBLE = 1
+  # Keypoint is labeled and visible.
+  VISIBLE = 2
 
 
 class _ClassTensorHandler(slim_example_decoder.Tensor):
@@ -69,8 +91,8 @@ class _ClassTensorHandler(slim_example_decoder.Tensor):
       lookup = tf.compat.v2.lookup
       hash_table_class = tf.compat.v2.lookup.StaticHashTable
     except AttributeError:
-      lookup = tf.contrib.lookup
-      hash_table_class = tf.contrib.lookup.HashTable
+      lookup = contrib_lookup
+      hash_table_class = contrib_lookup.HashTable
     name_to_id_table = hash_table_class(
         initializer=lookup.KeyValueTensorInitializer(
             keys=tf.constant(list(name_to_id.keys())),
@@ -144,7 +166,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
                dct_method='',
                num_keypoints=0,
                num_additional_channels=0,
-               load_multiclass_scores=False):
+               load_multiclass_scores=False,
+               load_context_features=False):
     """Constructor sets keys_to_features and items_to_handlers.
 
     Args:
@@ -168,6 +191,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       num_additional_channels: how many additional channels to use.
       load_multiclass_scores: Whether to load multiclass scores associated with
         boxes.
+      load_context_features: Whether to load information from context_features,
+        to provide additional context to a detection model for training and/or
+        inference
 
     Raises:
       ValueError: If `instance_mask_type` option is not one of
@@ -197,6 +223,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             tf.VarLenFeature(tf.string),
         'image/class/label':
             tf.VarLenFeature(tf.int64),
+        'image/class/confidence':
+            tf.VarLenFeature(tf.float32),
         # Object boxes and classes.
         'image/object/bbox/xmin':
             tf.VarLenFeature(tf.float32),
@@ -253,6 +281,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             slim_example_decoder.Tensor('image/key/sha256')),
         fields.InputDataFields.filename: (
             slim_example_decoder.Tensor('image/filename')),
+        # Image-level labels.
+        fields.InputDataFields.groundtruth_image_confidences: (
+            slim_example_decoder.Tensor('image/class/confidence')),
         # Object boxes and classes.
         fields.InputDataFields.groundtruth_boxes: (
             slim_example_decoder.BoundingBox(['ymin', 'xmin', 'ymax', 'xmax'],
@@ -274,6 +305,20 @@ class TfExampleDecoder(data_decoder.DataDecoder):
           'image/object/class/multiclass_scores'] = tf.VarLenFeature(tf.float32)
       self.items_to_handlers[fields.InputDataFields.multiclass_scores] = (
           slim_example_decoder.Tensor('image/object/class/multiclass_scores'))
+
+    if load_context_features:
+      self.keys_to_features[
+          'image/context_features'] = tf.VarLenFeature(tf.float32)
+      self.items_to_handlers[fields.InputDataFields.context_features] = (
+          slim_example_decoder.ItemHandlerCallback(
+              ['image/context_features', 'image/context_feature_length'],
+              self._reshape_context_features))
+
+      self.keys_to_features[
+          'image/context_feature_length'] = tf.FixedLenFeature((), tf.int64)
+      self.items_to_handlers[fields.InputDataFields.context_feature_length] = (
+          slim_example_decoder.Tensor('image/context_feature_length'))
+
     if num_additional_channels > 0:
       self.keys_to_features[
           'image/additional_channels/encoded'] = tf.FixedLenFeature(
@@ -287,10 +332,17 @@ class TfExampleDecoder(data_decoder.DataDecoder):
           tf.VarLenFeature(tf.float32))
       self.keys_to_features['image/object/keypoint/y'] = (
           tf.VarLenFeature(tf.float32))
+      self.keys_to_features['image/object/keypoint/visibility'] = (
+          tf.VarLenFeature(tf.int64))
       self.items_to_handlers[fields.InputDataFields.groundtruth_keypoints] = (
           slim_example_decoder.ItemHandlerCallback(
               ['image/object/keypoint/y', 'image/object/keypoint/x'],
               self._reshape_keypoints))
+      kpt_vis_field = fields.InputDataFields.groundtruth_keypoint_visibilities
+      self.items_to_handlers[kpt_vis_field] = (
+          slim_example_decoder.ItemHandlerCallback(
+              ['image/object/keypoint/x', 'image/object/keypoint/visibility'],
+              self._reshape_keypoint_visibilities))
     if load_instance_masks:
       if instance_mask_type in (input_reader_pb2.DEFAULT,
                                 input_reader_pb2.NUMERICAL_MASKS):
@@ -363,6 +415,9 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         [None] indicating if the boxes enclose a crowd.
 
     Optional:
+      fields.InputDataFields.groundtruth_image_confidences - 1D float tensor of
+        shape [None] indicating if a class is present in the image (1.0) or
+        a class is not present in the image (0.0).
       fields.InputDataFields.image_additional_channels - 3D uint8 tensor of
         shape [None, None, num_additional_channels]. 1st dim is height; 2nd dim
         is width; 3rd dim is the number of additional channels.
@@ -371,8 +426,10 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       fields.InputDataFields.groundtruth_group_of - 1D bool tensor of shape
         [None] indicating if the boxes represent `group_of` instances.
       fields.InputDataFields.groundtruth_keypoints - 3D float32 tensor of
-        shape [None, None, 2] containing keypoints, where the coordinates of
-        the keypoints are ordered (y, x).
+        shape [None, num_keypoints, 2] containing keypoints, where the
+        coordinates of the keypoints are ordered (y, x).
+      fields.InputDataFields.groundtruth_keypoint_visibilities - 2D bool
+        tensor of shape [None, num_keypoints] containing keypoint visibilites.
       fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
         shape [None, None, None] containing instance masks.
       fields.InputDataFields.groundtruth_image_classes - 1D uint64 of shape
@@ -380,6 +437,10 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       fields.InputDataFields.multiclass_scores - 1D float32 tensor of shape
         [None * num_classes] containing flattened multiclass scores for
         groundtruth boxes.
+      fields.InputDataFields.context_features - 1D float32 tensor of shape
+        [context_feature_length * num_context_features]
+      fields.InputDataFields.context_feature_length - int32 tensor specifying
+        the length of each feature in context_features
     """
     serialized_example = tf.reshape(tf_example_string_tensor, shape=[])
     decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,
@@ -410,20 +471,34 @@ class TfExampleDecoder(data_decoder.DataDecoder):
                 tensor_dict[fields.InputDataFields.groundtruth_weights])[0],
             0), lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],
         default_groundtruth_weights)
+
+    if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
+      # Set all keypoints that are not labeled to NaN.
+      gt_kpt_fld = fields.InputDataFields.groundtruth_keypoints
+      gt_kpt_vis_fld = fields.InputDataFields.groundtruth_keypoint_visibilities
+      visibilities_tiled = tf.tile(
+          tf.expand_dims(tensor_dict[gt_kpt_vis_fld], -1),
+          [1, 1, 2])
+      tensor_dict[gt_kpt_fld] = tf.where(
+          visibilities_tiled,
+          tensor_dict[gt_kpt_fld],
+          np.nan * tf.ones_like(tensor_dict[gt_kpt_fld]))
+
     return tensor_dict
 
   def _reshape_keypoints(self, keys_to_tensors):
     """Reshape keypoints.
 
-    The instance segmentation masks are reshaped to [num_instances,
-    num_keypoints, 2].
+    The keypoints are reshaped to [num_instances, num_keypoints, 2].
 
     Args:
-      keys_to_tensors: a dictionary from keys to tensors.
+      keys_to_tensors: a dictionary from keys to tensors. Expected keys are:
+        'image/object/keypoint/x'
+        'image/object/keypoint/y'
 
     Returns:
       A 3-D float tensor of shape [num_instances, num_keypoints, 2] with values
-        in {0, 1}.
+        in [0, 1].
     """
     y = keys_to_tensors['image/object/keypoint/y']
     if isinstance(y, tf.SparseTensor):
@@ -437,6 +512,54 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     keypoints = tf.reshape(keypoints, [-1, self._num_keypoints, 2])
     return keypoints
 
+  def _reshape_keypoint_visibilities(self, keys_to_tensors):
+    """Reshape keypoint visibilities.
+
+    The keypoint visibilities are reshaped to [num_instances,
+    num_keypoints].
+
+    The raw keypoint visibilities are expected to conform to the
+    MSCoco definition. See Visibility enum.
+
+    The returned boolean is True for the labeled case (either
+    Visibility.NOT_VISIBLE or Visibility.VISIBLE). These are the same categories
+    that COCO uses to evaluate keypoint detection performance:
+    http://cocodataset.org/#keypoints-eval
+
+    If image/object/keypoint/visibility is not provided, visibilities will be
+    set to True for finite keypoint coordinate values, and 0 if the coordinates
+    are NaN.
+
+    Args:
+      keys_to_tensors: a dictionary from keys to tensors. Expected keys are:
+        'image/object/keypoint/x'
+        'image/object/keypoint/visibility'
+
+    Returns:
+      A 2-D bool tensor of shape [num_instances, num_keypoints] with values
+        in {0, 1}. 1 if the keypoint is labeled, 0 otherwise.
+    """
+    x = keys_to_tensors['image/object/keypoint/x']
+    vis = keys_to_tensors['image/object/keypoint/visibility']
+    if isinstance(vis, tf.SparseTensor):
+      vis = tf.sparse_tensor_to_dense(vis)
+    if isinstance(x, tf.SparseTensor):
+      x = tf.sparse_tensor_to_dense(x)
+
+    default_vis = tf.where(
+        tf.math.is_nan(x),
+        Visibility.UNLABELED.value * tf.ones_like(x, dtype=tf.int64),
+        Visibility.VISIBLE.value * tf.ones_like(x, dtype=tf.int64))
+    # Use visibility if provided, otherwise use the default visibility.
+    vis = tf.cond(tf.equal(tf.size(x), tf.size(vis)),
+                  true_fn=lambda: vis,
+                  false_fn=lambda: default_vis)
+    vis = tf.math.logical_or(
+        tf.math.equal(vis, Visibility.NOT_VISIBLE.value),
+        tf.math.equal(vis, Visibility.VISIBLE.value))
+    vis = tf.reshape(vis, [-1, self._num_keypoints])
+    return vis
+
   def _reshape_instance_masks(self, keys_to_tensors):
     """Reshape instance segmentation masks.
 
@@ -460,6 +583,26 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         tf.cast(tf.greater(masks, 0.0), dtype=tf.float32), to_shape)
     return tf.cast(masks, tf.float32)
 
+  def _reshape_context_features(self, keys_to_tensors):
+    """Reshape context features.
+
+    The instance context_features are reshaped to
+      [num_context_features, context_feature_length]
+
+    Args:
+      keys_to_tensors: a dictionary from keys to tensors.
+
+    Returns:
+      A 2-D float tensor of shape [num_context_features, context_feature_length]
+    """
+    context_feature_length = keys_to_tensors['image/context_feature_length']
+    to_shape = tf.cast(tf.stack([-1, context_feature_length]), tf.int32)
+    context_features = keys_to_tensors['image/context_features']
+    if isinstance(context_features, tf.SparseTensor):
+      context_features = tf.sparse_tensor_to_dense(context_features)
+    context_features = tf.reshape(context_features, to_shape)
+    return context_features
+
   def _decode_png_instance_masks(self, keys_to_tensors):
     """Decode PNG instance segmentation masks and stack into dense tensor.
 
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 9ed8df60..0f3ccce6 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -24,8 +24,6 @@ from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
-slim_example_decoder = tf.contrib.slim.tfexample_decoder
-
 
 class TfExampleDecoderTest(tf.test.TestCase):
 
@@ -265,6 +263,68 @@ class TfExampleDecoderTest(tf.test.TestCase):
                         tensor_dict[fields.InputDataFields.groundtruth_boxes])
 
   def testDecodeKeypoint(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    keypoint_ys = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
+    keypoint_xs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
+    keypoint_visibility = [1, 2, 0, 1, 0, 2]
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature(six.b('jpeg')),
+                'image/object/bbox/ymin':
+                    dataset_util.float_list_feature(bbox_ymins),
+                'image/object/bbox/xmin':
+                    dataset_util.float_list_feature(bbox_xmins),
+                'image/object/bbox/ymax':
+                    dataset_util.float_list_feature(bbox_ymaxs),
+                'image/object/bbox/xmax':
+                    dataset_util.float_list_feature(bbox_xmaxs),
+                'image/object/keypoint/y':
+                    dataset_util.float_list_feature(keypoint_ys),
+                'image/object/keypoint/x':
+                    dataset_util.float_list_feature(keypoint_xs),
+                'image/object/keypoint/visibility':
+                    dataset_util.int64_list_feature(keypoint_visibility),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes]
+                         .get_shape().as_list()), [None, 4])
+    self.assertAllEqual(
+        (tensor_dict[fields.InputDataFields.groundtruth_keypoints].get_shape()
+         .as_list()), [2, 3, 2])
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    expected_boxes = np.vstack([bbox_ymins, bbox_xmins, bbox_ymaxs,
+                                bbox_xmaxs]).transpose()
+    self.assertAllEqual(expected_boxes,
+                        tensor_dict[fields.InputDataFields.groundtruth_boxes])
+
+    expected_keypoints = [
+        [[0.0, 1.0], [1.0, 2.0], [np.nan, np.nan]],
+        [[3.0, 4.0], [np.nan, np.nan], [5.0, 6.0]]]
+    self.assertAllClose(
+        expected_keypoints,
+        tensor_dict[fields.InputDataFields.groundtruth_keypoints])
+
+    expected_visibility = (
+        (np.array(keypoint_visibility) > 0).reshape((2, 3)))
+    self.assertAllEqual(
+        expected_visibility,
+        tensor_dict[fields.InputDataFields.groundtruth_keypoint_visibilities])
+
+  def testDecodeKeypointNoVisibilities(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     bbox_ymins = [0.0, 4.0]
@@ -316,6 +376,11 @@ class TfExampleDecoderTest(tf.test.TestCase):
         expected_keypoints,
         tensor_dict[fields.InputDataFields.groundtruth_keypoints])
 
+    expected_visibility = np.ones((2, 3))
+    self.assertAllEqual(
+        expected_visibility,
+        tensor_dict[fields.InputDataFields.groundtruth_keypoint_visibilities])
+
   def testDecodeDefaultGroundtruthWeights(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -841,6 +906,34 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(object_weights,
                         tensor_dict[fields.InputDataFields.groundtruth_weights])
 
+  def testDecodeClassConfidence(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    class_confidence = [0.0, 1.0, 0.0]
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature(six.b('jpeg')),
+                'image/class/confidence':
+                    dataset_util.float_list_feature(class_confidence),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder()
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual(
+        (tensor_dict[fields.InputDataFields.groundtruth_image_confidences]
+         .get_shape().as_list()), [3])
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(
+        class_confidence,
+        tensor_dict[fields.InputDataFields.groundtruth_image_confidences])
+
   def testDecodeInstanceSegmentation(self):
     num_instances = 4
     image_height = 5
@@ -992,6 +1085,87 @@ class TfExampleDecoderTest(tf.test.TestCase):
         tensor_dict[fields.InputDataFields.groundtruth_image_classes],
         np.array([1, 3]))
 
+  def testDecodeContextFeatures(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    num_features = 8
+    context_feature_length = 10
+    context_features = np.random.random(num_features*context_feature_length)
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature(six.b('jpeg')),
+                'image/context_features':
+                    dataset_util.float_list_feature(context_features),
+                'image/context_feature_length':
+                    dataset_util.int64_feature(context_feature_length),
+                'image/object/bbox/ymin':
+                    dataset_util.float_list_feature(bbox_ymins),
+                'image/object/bbox/xmin':
+                    dataset_util.float_list_feature(bbox_xmins),
+                'image/object/bbox/ymax':
+                    dataset_util.float_list_feature(bbox_ymaxs),
+                'image/object/bbox/xmax':
+                    dataset_util.float_list_feature(bbox_xmaxs),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        load_context_features=True)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+    self.assertAllClose(
+        context_features.reshape(num_features, context_feature_length),
+        tensor_dict[fields.InputDataFields.context_features])
+    self.assertAllEqual(
+        context_feature_length,
+        tensor_dict[fields.InputDataFields.context_feature_length])
+
+  def testContextFeaturesNotAvailableByDefault(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    num_features = 10
+    context_feature_length = 10
+    context_features = np.random.random(num_features*context_feature_length)
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    dataset_util.bytes_feature(encoded_jpeg),
+                'image/format':
+                    dataset_util.bytes_feature(six.b('jpeg')),
+                'image/context_features':
+                    dataset_util.float_list_feature(context_features),
+                'image/context_feature_length':
+                    dataset_util.int64_feature(context_feature_length),
+                'image/object/bbox/ymin':
+                    dataset_util.float_list_feature(bbox_ymins),
+                'image/object/bbox/xmin':
+                    dataset_util.float_list_feature(bbox_xmins),
+                'image/object/bbox/ymax':
+                    dataset_util.float_list_feature(bbox_ymaxs),
+                'image/object/bbox/xmax':
+                    dataset_util.float_list_feature(bbox_xmaxs),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder()
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+    self.assertNotIn(fields.InputDataFields.context_features,
+                     tensor_dict)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index 7f2bd1fb..aee40f62 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 r"""Convert raw COCO dataset to TFRecord for object_detection.
 
 Please note that this tool creates sharded output files.
@@ -34,6 +33,7 @@ from __future__ import print_function
 import hashlib
 import io
 import json
+import logging
 import os
 import contextlib2
 import numpy as np
@@ -46,57 +46,69 @@ from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 
-
 flags = tf.app.flags
-tf.flags.DEFINE_boolean('include_masks', False,
-                        'Whether to include instance segmentations masks '
-                        '(PNG encoded) in the result. default: False.')
-tf.flags.DEFINE_string('train_image_dir', '',
-                       'Training image directory.')
-tf.flags.DEFINE_string('val_image_dir', '',
-                       'Validation image directory.')
-tf.flags.DEFINE_string('test_image_dir', '',
-                       'Test image directory.')
+tf.flags.DEFINE_boolean(
+    'include_masks', False, 'Whether to include instance segmentations masks '
+    '(PNG encoded) in the result. default: False.')
+tf.flags.DEFINE_string('train_image_dir', '', 'Training image directory.')
+tf.flags.DEFINE_string('val_image_dir', '', 'Validation image directory.')
+tf.flags.DEFINE_string('test_image_dir', '', 'Test image directory.')
 tf.flags.DEFINE_string('train_annotations_file', '',
                        'Training annotations JSON file.')
 tf.flags.DEFINE_string('val_annotations_file', '',
                        'Validation annotations JSON file.')
 tf.flags.DEFINE_string('testdev_annotations_file', '',
                        'Test-dev annotations JSON file.')
+tf.flags.DEFINE_string('train_keypoint_annotations_file', '',
+                       'Training annotations JSON file.')
+tf.flags.DEFINE_string('val_keypoint_annotations_file', '',
+                       'Validation annotations JSON file.')
 tf.flags.DEFINE_string('output_dir', '/tmp/', 'Output data directory.')
 
 FLAGS = flags.FLAGS
 
-tf.logging.set_verbosity(tf.logging.INFO)
+logger = tf.get_logger()
+logger.setLevel(logging.INFO)
+
+_COCO_KEYPOINT_NAMES = [
+    b'nose', b'left_eye', b'right_eye', b'left_ear', b'right_ear',
+    b'left_shoulder', b'right_shoulder', b'left_elbow', b'right_elbow',
+    b'left_wrist', b'right_wrist', b'left_hip', b'right_hip',
+    b'left_knee', b'right_knee', b'left_ankle', b'right_ankle'
+]
 
 
 def create_tf_example(image,
                       annotations_list,
                       image_dir,
                       category_index,
-                      include_masks=False):
+                      include_masks=False,
+                      keypoint_annotations_dict=None):
   """Converts image and annotations to a tf.Example proto.
 
   Args:
-    image: dict with keys:
-      [u'license', u'file_name', u'coco_url', u'height', u'width',
-      u'date_captured', u'flickr_url', u'id']
+    image: dict with keys: [u'license', u'file_name', u'coco_url', u'height',
+      u'width', u'date_captured', u'flickr_url', u'id']
     annotations_list:
-      list of dicts with keys:
-      [u'segmentation', u'area', u'iscrowd', u'image_id',
-      u'bbox', u'category_id', u'id']
-      Notice that bounding box coordinates in the official COCO dataset are
-      given as [x, y, width, height] tuples using absolute coordinates where
-      x, y represent the top-left (0-indexed) corner.  This function converts
-      to the format expected by the Tensorflow Object Detection API (which is
-      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative
-      to image size).
+      list of dicts with keys: [u'segmentation', u'area', u'iscrowd',
+        u'image_id', u'bbox', u'category_id', u'id'] Notice that bounding box
+        coordinates in the official COCO dataset are given as [x, y, width,
+        height] tuples using absolute coordinates where x, y represent the
+        top-left (0-indexed) corner.  This function converts to the format
+        expected by the Tensorflow Object Detection API (which is which is
+        [ymin, xmin, ymax, xmax] with coordinates normalized relative to image
+        size).
     image_dir: directory containing the image files.
-    category_index: a dict containing COCO category information keyed
-      by the 'id' field of each category.  See the
-      label_map_util.create_category_index function.
+    category_index: a dict containing COCO category information keyed by the
+      'id' field of each category.  See the label_map_util.create_category_index
+      function.
     include_masks: Whether to include instance segmentations masks
       (PNG encoded) in the result. default: False.
+    keypoint_annotations_dict: A dictionary that maps from annotation_id to a
+      dictionary with keys: [u'keypoints', u'num_keypoints'] represeting the
+      keypoint information for this person object annotation. If None, then
+      no keypoint annotations will be populated.
+
   Returns:
     example: The converted tf.Example
     num_annotations_skipped: Number of (invalid) annotations that were ignored.
@@ -125,7 +137,15 @@ def create_tf_example(image,
   category_ids = []
   area = []
   encoded_mask_png = []
+  keypoints_x = []
+  keypoints_y = []
+  keypoints_visibility = []
+  keypoints_name = []
+  num_keypoints = []
+  include_keypoint = keypoint_annotations_dict is not None
   num_annotations_skipped = 0
+  num_keypoint_annotation_used = 0
+  num_keypoint_annotation_skipped = 0
   for object_annotations in annotations_list:
     (x, y, width, height) = tuple(object_annotations['bbox'])
     if width <= 0 or height <= 0:
@@ -154,6 +174,29 @@ def create_tf_example(image,
       output_io = io.BytesIO()
       pil_image.save(output_io, format='PNG')
       encoded_mask_png.append(output_io.getvalue())
+
+    if include_keypoint:
+      annotation_id = object_annotations['id']
+      if annotation_id in keypoint_annotations_dict:
+        num_keypoint_annotation_used += 1
+        keypoint_annotations = keypoint_annotations_dict[annotation_id]
+        keypoints = keypoint_annotations['keypoints']
+        num_kpts = keypoint_annotations['num_keypoints']
+        keypoints_x_abs = keypoints[::3]
+        keypoints_x.extend(
+            [float(x_abs) / image_width for x_abs in keypoints_x_abs])
+        keypoints_y_abs = keypoints[1::3]
+        keypoints_y.extend(
+            [float(y_abs) / image_height for y_abs in keypoints_y_abs])
+        keypoints_visibility.extend(keypoints[2::3])
+        keypoints_name.extend(_COCO_KEYPOINT_NAMES)
+        num_keypoints.append(num_kpts)
+      else:
+        keypoints_x.extend([0.0] * len(_COCO_KEYPOINT_NAMES))
+        keypoints_y.extend([0.0] * len(_COCO_KEYPOINT_NAMES))
+        keypoints_visibility.extend([0] * len(_COCO_KEYPOINT_NAMES))
+        keypoints_name.extend(_COCO_KEYPOINT_NAMES)
+        num_keypoints.append(0)
   feature_dict = {
       'image/height':
           dataset_util.int64_feature(image_height),
@@ -187,12 +230,28 @@ def create_tf_example(image,
   if include_masks:
     feature_dict['image/object/mask'] = (
         dataset_util.bytes_list_feature(encoded_mask_png))
+  if include_keypoint:
+    feature_dict['image/object/keypoint/x'] = (
+        dataset_util.float_list_feature(keypoints_x))
+    feature_dict['image/object/keypoint/y'] = (
+        dataset_util.float_list_feature(keypoints_y))
+    feature_dict['image/object/keypoint/num'] = (
+        dataset_util.int64_list_feature(num_keypoints))
+    feature_dict['image/object/keypoint/visibility'] = (
+        dataset_util.int64_list_feature(keypoints_visibility))
+    feature_dict['image/object/keypoint/text'] = (
+        dataset_util.bytes_list_feature(keypoints_name))
+    num_keypoint_annotation_skipped = (
+        len(keypoint_annotations_dict) - num_keypoint_annotation_used)
+
   example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
-  return key, example, num_annotations_skipped
+  return key, example, num_annotations_skipped, num_keypoint_annotation_skipped
 
 
-def _create_tf_record_from_coco_annotations(
-    annotations_file, image_dir, output_path, include_masks, num_shards):
+def _create_tf_record_from_coco_annotations(annotations_file, image_dir,
+                                            output_path, include_masks,
+                                            num_shards,
+                                            keypoint_annotations_file=''):
   """Loads COCO annotation json files and converts to tf.Record format.
 
   Args:
@@ -202,6 +261,9 @@ def _create_tf_record_from_coco_annotations(
     include_masks: Whether to include instance segmentations masks
       (PNG encoded) in the result. default: False.
     num_shards: number of output file shards.
+    keypoint_annotations_file: JSON file containing the person keypoint
+      annotations. If empty, then no person keypoint annotations will be
+      generated.
   """
   with contextlib2.ExitStack() as tf_record_close_stack, \
       tf.gfile.GFile(annotations_file, 'r') as fid:
@@ -214,8 +276,7 @@ def _create_tf_record_from_coco_annotations(
 
     annotations_index = {}
     if 'annotations' in groundtruth_data:
-      tf.logging.info(
-          'Found groundtruth annotations. Building annotations index.')
+      logging.info('Found groundtruth annotations. Building annotations index.')
       for annotation in groundtruth_data['annotations']:
         image_id = annotation['image_id']
         if image_id not in annotations_index:
@@ -227,21 +288,43 @@ def _create_tf_record_from_coco_annotations(
       if image_id not in annotations_index:
         missing_annotation_count += 1
         annotations_index[image_id] = []
-    tf.logging.info('%d images are missing annotations.',
-                    missing_annotation_count)
+    logging.info('%d images are missing annotations.', missing_annotation_count)
+
+    keypoint_annotations_index = {}
+    if keypoint_annotations_file:
+      with tf.gfile.GFile(keypoint_annotations_file, 'r') as kid:
+        keypoint_groundtruth_data = json.load(kid)
+      if 'annotations' in keypoint_groundtruth_data:
+        for annotation in keypoint_groundtruth_data['annotations']:
+          image_id = annotation['image_id']
+          if image_id not in keypoint_annotations_index:
+            keypoint_annotations_index[image_id] = {}
+          keypoint_annotations_index[image_id][annotation['id']] = annotation
 
     total_num_annotations_skipped = 0
+    total_num_keypoint_annotations_skipped = 0
     for idx, image in enumerate(images):
       if idx % 100 == 0:
-        tf.logging.info('On image %d of %d', idx, len(images))
+        logging.info('On image %d of %d', idx, len(images))
       annotations_list = annotations_index[image['id']]
-      _, tf_example, num_annotations_skipped = create_tf_example(
-          image, annotations_list, image_dir, category_index, include_masks)
+      keypoint_annotations_dict = None
+      if keypoint_annotations_file:
+        keypoint_annotations_dict = {}
+        if image['id'] in keypoint_annotations_index:
+          keypoint_annotations_dict = keypoint_annotations_index[image['id']]
+      (_, tf_example, num_annotations_skipped,
+       num_keypoint_annotations_skipped) = create_tf_example(
+           image, annotations_list, image_dir, category_index, include_masks,
+           keypoint_annotations_dict)
       total_num_annotations_skipped += num_annotations_skipped
+      total_num_keypoint_annotations_skipped += num_keypoint_annotations_skipped
       shard_idx = idx % num_shards
       output_tfrecords[shard_idx].write(tf_example.SerializeToString())
-    tf.logging.info('Finished writing, skipped %d annotations.',
-                    total_num_annotations_skipped)
+    logging.info('Finished writing, skipped %d annotations.',
+                 total_num_annotations_skipped)
+    if keypoint_annotations_file:
+      logging.info('Finished writing, skipped %d keypoint annotations.',
+                   total_num_keypoint_annotations_skipped)
 
 
 def main(_):
@@ -263,13 +346,15 @@ def main(_):
       FLAGS.train_image_dir,
       train_output_path,
       FLAGS.include_masks,
-      num_shards=100)
+      num_shards=100,
+      keypoint_annotations_file=FLAGS.train_keypoint_annotations_file)
   _create_tf_record_from_coco_annotations(
       FLAGS.val_annotations_file,
       FLAGS.val_image_dir,
       val_output_path,
       FLAGS.include_masks,
-      num_shards=10)
+      num_shards=100,
+      keypoint_annotations_file=FLAGS.val_keypoint_annotations_file)
   _create_tf_record_from_coco_annotations(
       FLAGS.testdev_annotations_file,
       FLAGS.test_image_dir,
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record_test.py b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
index b99fd12b..756fb61b 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
@@ -20,6 +20,7 @@ import os
 
 import numpy as np
 import PIL.Image
+import six
 import tensorflow as tf
 
 from object_detection.dataset_tools import create_coco_tf_record
@@ -37,6 +38,16 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
     proto_list = [p for p in proto_field]
     self.assertListEqual(proto_list, expectation)
 
+  def _assertProtoClose(self, proto_field, expectation):
+    """Helper function to assert if a proto field nearly equals some value.
+
+    Args:
+      proto_field: The protobuf field to compare.
+      expectation: The expected value of the protobuf field.
+    """
+    proto_list = [p for p in proto_field]
+    self.assertAllClose(proto_list, expectation)
+
   def test_create_tf_example(self):
     image_file_name = 'tmp_image.jpg'
     image_data = np.random.rand(256, 256, 3)
@@ -78,7 +89,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
     }
 
     (_, example,
-     num_annotations_skipped) = create_coco_tf_record.create_tf_example(
+     num_annotations_skipped, _) = create_coco_tf_record.create_tf_example(
          image, annotations_list, image_dir, category_index)
 
     self.assertEqual(num_annotations_skipped, 0)
@@ -88,12 +99,13 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         example.features.feature['image/width'].int64_list.value, [256])
     self._assertProtoEqual(
         example.features.feature['image/filename'].bytes_list.value,
-        [image_file_name])
+        [six.b(image_file_name)])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [str(image['id'])])
+        [six.b(str(image['id']))])
     self._assertProtoEqual(
-        example.features.feature['image/format'].bytes_list.value, ['jpeg'])
+        example.features.feature['image/format'].bytes_list.value,
+        [six.b('jpeg')])
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/xmin'].float_list.value,
         [0.25])
@@ -108,7 +120,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         [0.75])
     self._assertProtoEqual(
         example.features.feature['image/object/class/text'].bytes_list.value,
-        ['cat'])
+        [six.b('cat')])
 
   def test_create_tf_example_with_instance_masks(self):
     image_file_name = 'tmp_image.jpg'
@@ -144,7 +156,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
     }
 
     (_, example,
-     num_annotations_skipped) = create_coco_tf_record.create_tf_example(
+     num_annotations_skipped, _) = create_coco_tf_record.create_tf_example(
          image, annotations_list, image_dir, category_index, include_masks=True)
 
     self.assertEqual(num_annotations_skipped, 0)
@@ -154,12 +166,13 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         example.features.feature['image/width'].int64_list.value, [8])
     self._assertProtoEqual(
         example.features.feature['image/filename'].bytes_list.value,
-        [image_file_name])
+        [six.b(image_file_name)])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [str(image['id'])])
+        [six.b(str(image['id']))])
     self._assertProtoEqual(
-        example.features.feature['image/format'].bytes_list.value, ['jpeg'])
+        example.features.feature['image/format'].bytes_list.value,
+        [six.b('jpeg')])
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/xmin'].float_list.value,
         [0])
@@ -174,7 +187,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         [1])
     self._assertProtoEqual(
         example.features.feature['image/object/class/text'].bytes_list.value,
-        ['dog'])
+        [six.b('dog')])
     encoded_mask_pngs = [
         io.BytesIO(encoded_masks) for encoded_masks in example.features.feature[
             'image/object/mask'].bytes_list.value
@@ -183,13 +196,120 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         np.array(PIL.Image.open(encoded_mask_png))
         for encoded_mask_png in encoded_mask_pngs
     ]
-    self.assertTrue(len(pil_masks) == 1)
+    self.assertEqual(len(pil_masks), 1)
     self.assertAllEqual(pil_masks[0],
                         [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],
                          [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1],
                          [0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]])
 
+  def test_create_tf_example_with_keypoints(self):
+    image_dir = self.get_temp_dir()
+    image_file_name = 'tmp_image.jpg'
+    image_data = np.random.randint(low=0, high=256, size=(256, 256, 3)).astype(
+        np.uint8)
+    save_path = os.path.join(image_dir, image_file_name)
+    image = PIL.Image.fromarray(image_data, 'RGB')
+    image.save(save_path)
+
+    image = {
+        'file_name': image_file_name,
+        'height': 256,
+        'width': 256,
+        'id': 11,
+    }
+
+    min_x, min_y = 64, 64
+    max_x, max_y = 128, 128
+    keypoints = []
+    num_visible_keypoints = 0
+    xv = []
+    yv = []
+    vv = []
+    for _ in range(17):
+      xc = min_x + int(np.random.rand()*(max_x - min_x))
+      yc = min_y + int(np.random.rand()*(max_y - min_y))
+      vis = np.random.randint(0, 3)
+      xv.append(xc)
+      yv.append(yc)
+      vv.append(vis)
+      keypoints.extend([xc, yc, vis])
+      num_visible_keypoints += (vis > 0)
+
+    annotations_list = [{
+        'area': 0.5,
+        'iscrowd': False,
+        'image_id': 11,
+        'bbox': [64, 64, 128, 128],
+        'category_id': 1,
+        'id': 1000
+    }]
+
+    keypoint_annotations_dict = {
+        1000: {
+            'keypoints': keypoints,
+            'num_keypoints': num_visible_keypoints
+        }
+    }
+
+    category_index = {
+        1: {
+            'name': 'person',
+            'id': 1
+        }
+    }
+
+    (_, example, _,
+     num_keypoint_annotation_skipped) = create_coco_tf_record.create_tf_example(
+         image,
+         annotations_list,
+         image_dir,
+         category_index,
+         include_masks=False,
+         keypoint_annotations_dict=keypoint_annotations_dict)
+
+    self.assertEqual(num_keypoint_annotation_skipped, 0)
+    self._assertProtoEqual(
+        example.features.feature['image/height'].int64_list.value, [256])
+    self._assertProtoEqual(
+        example.features.feature['image/width'].int64_list.value, [256])
+    self._assertProtoEqual(
+        example.features.feature['image/filename'].bytes_list.value,
+        [six.b(image_file_name)])
+    self._assertProtoEqual(
+        example.features.feature['image/source_id'].bytes_list.value,
+        [six.b(str(image['id']))])
+    self._assertProtoEqual(
+        example.features.feature['image/format'].bytes_list.value,
+        [six.b('jpeg')])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmin'].float_list.value,
+        [0.25])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymin'].float_list.value,
+        [0.25])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmax'].float_list.value,
+        [0.75])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymax'].float_list.value,
+        [0.75])
+    self._assertProtoEqual(
+        example.features.feature['image/object/class/text'].bytes_list.value,
+        [six.b('person')])
+    self._assertProtoClose(
+        example.features.feature['image/object/keypoint/x'].float_list.value,
+        np.array(xv, dtype=np.float32) / 256)
+    self._assertProtoClose(
+        example.features.feature['image/object/keypoint/y'].float_list.value,
+        np.array(yv, dtype=np.float32) / 256)
+    self._assertProtoEqual(
+        example.features.feature['image/object/keypoint/text'].bytes_list.value,
+        create_coco_tf_record._COCO_KEYPOINT_NAMES)
+    self._assertProtoEqual(
+        example.features.feature[
+            'image/object/keypoint/visibility'].int64_list.value, vv)
+
   def test_create_sharded_tf_record(self):
     tmp_dir = self.get_temp_dir()
     image_paths = ['tmp1_image.jpg', 'tmp2_image.jpg']
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
index 37ac4b8b..1e3097a0 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
@@ -19,6 +19,7 @@ import os
 
 import numpy as np
 import PIL.Image
+import six
 import tensorflow as tf
 
 from object_detection.dataset_tools import create_kitti_tf_record
@@ -75,12 +76,13 @@ class CreateKittiTFRecordTest(tf.test.TestCase):
         example.features.feature['image/width'].int64_list.value, [256])
     self._assertProtoEqual(
         example.features.feature['image/filename'].bytes_list.value,
-        [save_path])
+        [six.b(save_path)])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [save_path])
+        [six.b(save_path)])
     self._assertProtoEqual(
-        example.features.feature['image/format'].bytes_list.value, ['png'])
+        example.features.feature['image/format'].bytes_list.value,
+        [six.b('png')])
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/xmin'].float_list.value,
         [0.25])
@@ -95,7 +97,7 @@ class CreateKittiTFRecordTest(tf.test.TestCase):
         [0.75])
     self._assertProtoEqual(
         example.features.feature['image/object/class/text'].bytes_list.value,
-        ['car'])
+        [six.b('car')])
     self._assertProtoEqual(
         example.features.feature['image/object/class/label'].int64_list.value,
         [1])
diff --git a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
index 66929bd4..982652ed 100644
--- a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
@@ -19,6 +19,7 @@ import os
 
 import numpy as np
 import PIL.Image
+import six
 import tensorflow as tf
 
 from object_detection.dataset_tools import create_pascal_tf_record
@@ -80,12 +81,13 @@ class CreatePascalTFRecordTest(tf.test.TestCase):
         example.features.feature['image/width'].int64_list.value, [256])
     self._assertProtoEqual(
         example.features.feature['image/filename'].bytes_list.value,
-        [image_file_name])
+        [six.b(image_file_name)])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [image_file_name])
+        [six.b(image_file_name)])
     self._assertProtoEqual(
-        example.features.feature['image/format'].bytes_list.value, ['jpeg'])
+        example.features.feature['image/format'].bytes_list.value,
+        [six.b('jpeg')])
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/xmin'].float_list.value,
         [0.25])
@@ -100,7 +102,7 @@ class CreatePascalTFRecordTest(tf.test.TestCase):
         [0.75])
     self._assertProtoEqual(
         example.features.feature['image/object/class/text'].bytes_list.value,
-        ['person'])
+        [six.b('person')])
     self._assertProtoEqual(
         example.features.feature['image/object/class/label'].int64_list.value,
         [1])
@@ -111,7 +113,8 @@ class CreatePascalTFRecordTest(tf.test.TestCase):
         example.features.feature['image/object/truncated'].int64_list.value,
         [0])
     self._assertProtoEqual(
-        example.features.feature['image/object/view'].bytes_list.value, [''])
+        example.features.feature['image/object/view'].bytes_list.value,
+        [six.b('')])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
index aa96c399..b3fcf143 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -32,12 +33,15 @@ oid_hierarchical_labels_expansion.py \
 --annotation_type=<1 (for boxes and segments) or 2 (for image-level labels)>
 """
 
+from __future__ import absolute_import
+from __future__ import division
 from __future__ import print_function
 
 import copy
 import json
 from absl import app
 from absl import flags
+import six
 
 flags.DEFINE_string(
     'json_hierarchy_file', None,
@@ -136,7 +140,7 @@ class OIDHierarchicalLabelsExpansion(object):
     # Row header is expected to be the following for segments:
     # ImageID,LabelName,ImageWidth,ImageHeight,XMin,XMax,YMin,YMax,
     # IsGroupOf,Mask
-    split_csv_row = csv_row.split(',')
+    split_csv_row = six.ensure_str(csv_row).split(',')
     result = [csv_row]
     assert split_csv_row[
         labelname_column_index] in self._hierarchy_keyed_child
@@ -165,7 +169,7 @@ class OIDHierarchicalLabelsExpansion(object):
     """
     # Row header is expected to be exactly:
     # ImageID,Source,LabelName,Confidence
-    split_csv_row = csv_row.split(',')
+    split_csv_row = six.ensure_str(csv_row).split(',')
     result = [csv_row]
     if int(split_csv_row[confidence_column_index]) == 1:
       assert split_csv_row[
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation.py b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
index 70628098..12f9b6e0 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
@@ -18,6 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import six
 import tensorflow as tf
 
 from object_detection.core import standard_fields
@@ -61,18 +62,21 @@ def tf_example_from_annotations_data_frame(annotations_data_frame, label_map,
           dataset_util.float_list_feature(
               filtered_data_frame_boxes.XMax.as_matrix()),
       standard_fields.TfExampleFields.object_class_text:
-          dataset_util.bytes_list_feature(
-              filtered_data_frame_boxes.LabelName.as_matrix()),
+          dataset_util.bytes_list_feature([
+              six.ensure_binary(label_text)
+              for label_text in filtered_data_frame_boxes.LabelName.as_matrix()
+          ]),
       standard_fields.TfExampleFields.object_class_label:
           dataset_util.int64_list_feature(
-              filtered_data_frame_boxes.LabelName.map(lambda x: label_map[x])
-              .as_matrix()),
+              filtered_data_frame_boxes.LabelName.map(
+                  lambda x: label_map[x]).as_matrix()),
       standard_fields.TfExampleFields.filename:
-          dataset_util.bytes_feature('{}.jpg'.format(image_id)),
+          dataset_util.bytes_feature(
+              six.ensure_binary('{}.jpg'.format(image_id))),
       standard_fields.TfExampleFields.source_id:
-          dataset_util.bytes_feature(image_id),
+          dataset_util.bytes_feature(six.ensure_binary(image_id)),
       standard_fields.TfExampleFields.image_encoded:
-          dataset_util.bytes_feature(encoded_image),
+          dataset_util.bytes_feature(six.ensure_binary(encoded_image)),
   }
 
   if 'IsGroupOf' in filtered_data_frame.columns:
@@ -100,7 +104,9 @@ def tf_example_from_annotations_data_frame(annotations_data_frame, label_map,
                 image_class_label] = dataset_util.int64_list_feature(
                     filtered_data_frame_labels.LabelName.map(
                         lambda x: label_map[x]).as_matrix())
-    feature_map[standard_fields.TfExampleFields.
-                image_class_text] = dataset_util.bytes_list_feature(
-                    filtered_data_frame_labels.LabelName.as_matrix()),
+    feature_map[standard_fields.TfExampleFields
+                .image_class_text] = dataset_util.bytes_list_feature([
+                    six.ensure_binary(label_text) for label_text in
+                    filtered_data_frame_labels.LabelName.as_matrix()
+                ]),
   return tf.train.Example(features=tf.train.Features(feature=feature_map))
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
index 44ef8521..e0520fb0 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
@@ -15,6 +15,7 @@
 """Tests for oid_tfrecord_creation.py."""
 
 import pandas as pd
+import six
 import tensorflow as tf
 
 from object_detection.dataset_tools import oid_tfrecord_creation
@@ -46,8 +47,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
 
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i1'], label_map, 'encoded_image_test')
-    self.assertProtoEquals(
-        """
+    self.assertProtoEquals(six.ensure_str("""
         features {
           feature {
             key: "image/encoded"
@@ -94,7 +94,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
           feature {
             key: "image/class/text"
             value { bytes_list { value: ["c"] } } } }
-    """, tf_example)
+    """), tf_example)
 
   def test_no_attributes(self):
     label_map, df = create_test_data()
@@ -107,7 +107,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
 
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i2'], label_map, 'encoded_image_test')
-    self.assertProtoEquals("""
+    self.assertProtoEquals(six.ensure_str("""
         features {
           feature {
             key: "image/encoded"
@@ -136,7 +136,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
           feature {
             key: "image/source_id"
            value { bytes_list { value: "i2" } } } }
-    """, tf_example)
+    """), tf_example)
 
   def test_label_filtering(self):
     label_map, df = create_test_data()
@@ -146,7 +146,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
     tf_example = oid_tfrecord_creation.tf_example_from_annotations_data_frame(
         df[df.ImageID == 'i1'], label_map, 'encoded_image_test')
     self.assertProtoEquals(
-        """
+        six.ensure_str("""
         features {
           feature {
             key: "image/encoded"
@@ -193,7 +193,7 @@ class TfExampleFromAnnotationsDataFrameTests(tf.test.TestCase):
           feature {
             key: "image/class/text"
             value { bytes_list { } } } }
-    """, tf_example)
+    """), tf_example)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util.py b/research/object_detection/dataset_tools/tf_record_creation_util.py
index e8da2291..781a0640 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -18,6 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from six.moves import range
 import tensorflow as tf
 
 
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util_test.py b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
index f1231f8b..d37bcbe8 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util_test.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,8 +15,14 @@
 # ==============================================================================
 """Tests for tf_record_creation_util.py."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import os
 import contextlib2
+import six
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.dataset_tools import tf_record_creation_util
@@ -29,7 +36,7 @@ class OpenOutputTfrecordsTests(tf.test.TestCase):
           tf_record_close_stack,
           os.path.join(tf.test.get_temp_dir(), 'test.tfrec'), 10)
       for idx in range(10):
-        output_tfrecords[idx].write('test_{}'.format(idx))
+        output_tfrecords[idx].write(six.ensure_binary('test_{}'.format(idx)))
 
     for idx in range(10):
       tf_record_path = '{}-{:05d}-of-00010'.format(
diff --git a/research/object_detection/dockerfiles/android/Dockerfile b/research/object_detection/dockerfiles/android/Dockerfile
index 472ca1d5..470f669d 100644
--- a/research/object_detection/dockerfiles/android/Dockerfile
+++ b/research/object_detection/dockerfiles/android/Dockerfile
@@ -24,7 +24,8 @@ RUN git clone --depth 1 https://github.com/tensorflow/models.git && \
 
 # Install gcloud and gsutil commands
 # https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu
-RUN apt-get -y update && apt-get install -y gpg-agent && export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)" && \
+RUN apt-get -y update && apt-get install -y gpg-agent && \
+    export CLOUD_SDK_REPO="cloud-sdk-$(lsb_release -c -s)" && \
     echo "deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \
     apt-get update -y && apt-get install google-cloud-sdk -y
@@ -33,8 +34,10 @@ RUN apt-get -y update && apt-get install -y gpg-agent && export CLOUD_SDK_REPO="
 # Install the Tensorflow Object Detection API from here
 # https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md
 
-# Install object detection api dependencies - use non-interactive mode to set default tzdata config during installation
-RUN export DEBIAN_FRONTEND=noninteractive && apt-get install -y protobuf-compiler python-pil python-lxml python-tk && \
+# Install object detection api dependencies - use non-interactive mode to set
+# default tzdata config during installation.
+RUN export DEBIAN_FRONTEND=noninteractive && \
+    apt-get install -y protobuf-compiler python-pil python-lxml python-tk && \
     pip install Cython && \
     pip install contextlib2 && \
     pip install jupyter && \
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index d43bb8cb..08b7f3ad 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -26,18 +26,20 @@ import numpy as np
 from six.moves import range
 import tensorflow as tf
 
+from tensorflow.contrib import slim
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import keypoint_ops
 from object_detection.core import standard_fields as fields
 from object_detection.metrics import coco_evaluation
+from object_detection.protos import eval_pb2
 from object_detection.utils import label_map_util
 from object_detection.utils import object_detection_evaluation
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from object_detection.utils import visualization_utils as vis_utils
 
-slim = tf.contrib.slim
+EVAL_KEYPOINT_METRIC = 'coco_keypoint_metrics'
 
 # A dictionary of metric names to classes that implement the metric. The classes
 # in the dictionary must implement
@@ -45,6 +47,8 @@ slim = tf.contrib.slim
 EVAL_METRICS_CLASS_DICT = {
     'coco_detection_metrics':
         coco_evaluation.CocoDetectionEvaluator,
+    'coco_keypoint_metrics':
+        coco_evaluation.CocoKeypointEvaluator,
     'coco_mask_metrics':
         coco_evaluation.CocoMaskEvaluator,
     'oid_challenge_detection_metrics':
@@ -324,7 +328,7 @@ def _run_checkpoint_once(tensor_dict,
 
   counters = {'skipped': 0, 'success': 0}
   aggregate_result_losses_dict = collections.defaultdict(list)
-  with tf.contrib.slim.queues.QueueRunners(sess):
+  with slim.queues.QueueRunners(sess):
     try:
       for batch in range(int(num_batches)):
         if (batch + 1) % 100 == 0:
@@ -591,6 +595,8 @@ def result_dict_for_single_example(image,
       'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)
       'groundtruth_instance_masks': 3D int64 tensor of instance masks
         (Optional).
+      'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with
+        keypoints (Optional).
     class_agnostic: Boolean indicating whether the detections are class-agnostic
       (i.e. binary). Default False.
     scale_to_absolute: Boolean indicating whether boxes and keypoints should be
@@ -620,7 +626,8 @@ def result_dict_for_single_example(image,
     'groundtruth_group_of': [num_boxes] int64 tensor. (Optional)
     'groundtruth_instance_masks': 3D int64 tensor of instance masks
       (Optional).
-
+    'groundtruth_keypoints': [num_boxes, num_keypoints, 2] float32 tensor with
+      keypoints (Optional).
   """
 
   if groundtruth:
@@ -675,6 +682,10 @@ def result_dict_for_batched_example(images,
   Note that evaluation tools require classes that are 1-indexed, and so this
   function performs the offset. If `class_agnostic` is True, all output classes
   have label 1.
+  The groundtruth coordinates of boxes/keypoints in 'groundtruth' dictionary are
+  normalized relative to the (potentially padded) input image, while the
+  coordinates in 'detection' dictionary are normalized relative to the true
+  image shape.
 
   Args:
     images: A single 4D uint8 image tensor of shape [batch_size, H, W, C].
@@ -696,6 +707,10 @@ def result_dict_for_batched_example(images,
         tensor. (Optional)
       'groundtruth_instance_masks': 4D int64 tensor of instance
         masks (Optional).
+      'groundtruth_keypoints': [batch_size, max_number_of_boxes, num_keypoints,
+        2] float32 tensor with keypoints (Optional).
+      'groundtruth_keypoint_visibilities': [batch_size, max_number_of_boxes,
+        num_keypoints] bool tensor with keypoint visibilities (Optional).
     class_agnostic: Boolean indicating whether the detections are class-agnostic
       (i.e. binary). Default False.
     scale_to_absolute: Boolean indicating whether boxes and keypoints should be
@@ -724,7 +739,11 @@ def result_dict_for_batched_example(images,
     'detection_classes': [batch_size, max_detections] int64 tensor of 1-indexed
       classes.
     'detection_masks': [batch_size, max_detections, H, W] float32 tensor of
-      binarized masks, reframed to full image masks.
+      binarized masks, reframed to full image masks. (Optional)
+    'detection_keypoints': [batch_size, max_detections, num_keypoints, 2]
+      float32 tensor containing keypoint coordinates. (Optional)
+    'detection_keypoint_scores': [batch_size, max_detections, num_keypoints]
+      float32 tensor containing keypoint scores. (Optional)
     'num_detections': [batch_size] int64 tensor containing number of valid
       detections.
     'groundtruth_boxes': [batch_size, num_boxes, 4] float32 tensor of boxes, in
@@ -739,6 +758,10 @@ def result_dict_for_batched_example(images,
     'groundtruth_group_of': [batch_size, num_boxes] int64 tensor. (Optional)
     'groundtruth_instance_masks': 4D int64 tensor of instance masks
       (Optional).
+    'groundtruth_keypoints': [batch_size, num_boxes, num_keypoints, 2] float32
+      tensor with keypoints (Optional).
+    'groundtruth_keypoint_visibilities': [batch_size, num_boxes, num_keypoints]
+      bool tensor with keypoint visibilities (Optional).
     'num_groundtruth_boxes': [batch_size] tensor containing the maximum number
       of groundtruth boxes per image.
 
@@ -828,6 +851,12 @@ def result_dict_for_batched_example(images,
               _scale_keypoint_to_absolute,
               elems=[detection_keypoints, original_image_spatial_shapes],
               dtype=tf.float32))
+    if detection_fields.detection_keypoint_scores in detections:
+      output_dict[detection_fields.detection_keypoint_scores] = detections[
+          detection_fields.detection_keypoint_scores]
+    else:
+      output_dict[detection_fields.detection_keypoint_scores] = tf.ones_like(
+          detections[detection_fields.detection_keypoints][:, :, :, 0])
 
   if groundtruth:
     if max_gt_boxes is None:
@@ -866,6 +895,28 @@ def result_dict_for_batched_example(images,
         elems=[groundtruth_boxes, true_image_shapes], dtype=tf.float32)
     output_dict[input_data_fields.groundtruth_boxes] = groundtruth_boxes
 
+    if input_data_fields.groundtruth_keypoints in groundtruth:
+      # If groundtruth_keypoints is in the groundtruth dictionary. Update the
+      # coordinates to conform with the true image shape.
+      def _scale_keypoints_to_normalized_true_image(args):
+        """Scale the box coordinates to be relative to the true image shape."""
+        keypoints, true_image_shape = args
+        true_image_shape = tf.cast(true_image_shape, tf.float32)
+        true_height, true_width = true_image_shape[0], true_image_shape[1]
+        normalized_window = tf.stack(
+            [0.0, 0.0, true_height / image_height, true_width / image_width])
+        return keypoint_ops.change_coordinate_frame(keypoints,
+                                                    normalized_window)
+
+      groundtruth_keypoints = groundtruth[
+          input_data_fields.groundtruth_keypoints]
+      groundtruth_keypoints = shape_utils.static_or_dynamic_map_fn(
+          _scale_keypoints_to_normalized_true_image,
+          elems=[groundtruth_keypoints, true_image_shapes],
+          dtype=tf.float32)
+      output_dict[
+          input_data_fields.groundtruth_keypoints] = groundtruth_keypoints
+
     if scale_to_absolute:
       groundtruth_boxes = output_dict[input_data_fields.groundtruth_boxes]
       output_dict[input_data_fields.groundtruth_boxes] = (
@@ -873,6 +924,14 @@ def result_dict_for_batched_example(images,
               _scale_box_to_absolute,
               elems=[groundtruth_boxes, original_image_spatial_shapes],
               dtype=tf.float32))
+      if input_data_fields.groundtruth_keypoints in groundtruth:
+        groundtruth_keypoints = output_dict[
+            input_data_fields.groundtruth_keypoints]
+        output_dict[input_data_fields.groundtruth_keypoints] = (
+            shape_utils.static_or_dynamic_map_fn(
+                _scale_keypoint_to_absolute,
+                elems=[groundtruth_keypoints, original_image_spatial_shapes],
+                dtype=tf.float32))
 
     # For class-agnostic models, groundtruth classes all become 1.
     if class_agnostic:
@@ -893,6 +952,8 @@ def get_evaluators(eval_config, categories, evaluator_options=None):
     categories: A list of dicts, each of which has the following keys -
         'id': (required) an integer id uniquely identifying this category.
         'name': (required) string representing category name e.g., 'cat', 'dog'.
+        'keypoints': (optional) dict mapping this category's keypoints to unique
+          ids.
     evaluator_options: A dictionary of metric names (see
       EVAL_METRICS_CLASS_DICT) to `DetectionEvaluator` initialization
       keyword arguments. For example:
@@ -919,6 +980,32 @@ def get_evaluators(eval_config, categories, evaluator_options=None):
     evaluators_list.append(EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](
         categories,
         **kwargs_dict))
+
+  if isinstance(eval_config, eval_pb2.EvalConfig):
+    parameterized_metrics = eval_config.parameterized_metric
+    for parameterized_metric in parameterized_metrics:
+      assert parameterized_metric.HasField('parameterized_metric')
+      if parameterized_metric.WhichOneof(
+          'parameterized_metric') == EVAL_KEYPOINT_METRIC:
+        keypoint_metrics = parameterized_metric.coco_keypoint_metrics
+        # Create category to keypoints mapping dict.
+        category_keypoints = {}
+        class_label = keypoint_metrics.class_label
+        category = None
+        for cat in categories:
+          if cat['name'] == class_label:
+            category = cat
+            break
+        if not category:
+          continue
+        keypoints_for_this_class = category['keypoints']
+        category_keypoints = [{
+            'id': keypoints_for_this_class[kp_name], 'name': kp_name
+        } for kp_name in keypoints_for_this_class]
+        # Create keypoint evaluator for this category.
+        evaluators_list.append(EVAL_METRICS_CLASS_DICT[EVAL_KEYPOINT_METRIC](
+            category['id'], category_keypoints, class_label,
+            keypoint_metrics.keypoint_label_to_sigmas))
   return evaluators_list
 
 
diff --git a/research/object_detection/eval_util_test.py b/research/object_detection/eval_util_test.py
index 833aaa98..7006e13e 100644
--- a/research/object_detection/eval_util_test.py
+++ b/research/object_detection/eval_util_test.py
@@ -27,6 +27,7 @@ import tensorflow as tf
 
 from object_detection import eval_util
 from object_detection.core import standard_fields as fields
+from object_detection.metrics import coco_evaluation
 from object_detection.protos import eval_pb2
 from object_detection.utils import test_case
 
@@ -38,6 +39,26 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
             {'id': 2, 'name': 'dog'},
             {'id': 3, 'name': 'cat'}]
 
+  def _get_categories_list_with_keypoints(self):
+    return [{
+        'id': 1,
+        'name': 'person',
+        'keypoints': {
+            'left_eye': 0,
+            'right_eye': 3
+        }
+    }, {
+        'id': 2,
+        'name': 'dog',
+        'keypoints': {
+            'tail_start': 1,
+            'mouth': 2
+        }
+    }, {
+        'id': 3,
+        'name': 'cat'
+    }]
+
   def _make_evaluation_dict(self,
                             resized_groundtruth_masks=False,
                             batch_size=1,
@@ -61,6 +82,7 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
     groundtruth_boxes = tf.constant([[0., 0., 1., 1.]])
     groundtruth_classes = tf.constant([1])
     groundtruth_instance_masks = tf.ones(shape=[1, 20, 20], dtype=tf.uint8)
+    groundtruth_keypoints = tf.constant([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])
     if resized_groundtruth_masks:
       groundtruth_instance_masks = tf.ones(shape=[1, 10, 10], dtype=tf.uint8)
 
@@ -72,6 +94,9 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
       groundtruth_instance_masks = tf.tile(
           tf.expand_dims(groundtruth_instance_masks, 0),
           multiples=[batch_size, 1, 1, 1])
+      groundtruth_keypoints = tf.tile(
+          tf.expand_dims(groundtruth_keypoints, 0),
+          multiples=[batch_size, 1, 1])
 
     detections = {
         detection_fields.detection_boxes: detection_boxes,
@@ -83,6 +108,7 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
     groundtruth = {
         input_data_fields.groundtruth_boxes: groundtruth_boxes,
         input_data_fields.groundtruth_classes: groundtruth_classes,
+        input_data_fields.groundtruth_keypoints: groundtruth_keypoints,
         input_data_fields.groundtruth_instance_masks: groundtruth_instance_masks
     }
     if batch_size > 1:
@@ -255,6 +281,49 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
     self.assertAlmostEqual(evaluator[1]._recall_lower_bound, 0.0)
     self.assertAlmostEqual(evaluator[1]._recall_upper_bound, 1.0)
 
+  def test_get_evaluator_with_keypoint_metrics(self):
+    eval_config = eval_pb2.EvalConfig()
+    person_keypoints_metric = eval_config.parameterized_metric.add()
+    person_keypoints_metric.coco_keypoint_metrics.class_label = 'person'
+    person_keypoints_metric.coco_keypoint_metrics.keypoint_label_to_sigmas[
+        'left_eye'] = 0.1
+    person_keypoints_metric.coco_keypoint_metrics.keypoint_label_to_sigmas[
+        'right_eye'] = 0.2
+    dog_keypoints_metric = eval_config.parameterized_metric.add()
+    dog_keypoints_metric.coco_keypoint_metrics.class_label = 'dog'
+    dog_keypoints_metric.coco_keypoint_metrics.keypoint_label_to_sigmas[
+        'tail_start'] = 0.3
+    dog_keypoints_metric.coco_keypoint_metrics.keypoint_label_to_sigmas[
+        'mouth'] = 0.4
+    categories = self._get_categories_list_with_keypoints()
+
+    evaluator = eval_util.get_evaluators(
+        eval_config, categories, evaluator_options=None)
+
+    # Verify keypoint evaluator class variables.
+    self.assertLen(evaluator, 3)
+    self.assertFalse(evaluator[0]._include_metrics_per_category)
+    self.assertEqual(evaluator[1]._category_name, 'person')
+    self.assertEqual(evaluator[2]._category_name, 'dog')
+    self.assertAllEqual(evaluator[1]._keypoint_ids, [0, 3])
+    self.assertAllEqual(evaluator[2]._keypoint_ids, [1, 2])
+    self.assertAllClose([0.1, 0.2], evaluator[1]._oks_sigmas)
+    self.assertAllClose([0.3, 0.4], evaluator[2]._oks_sigmas)
+
+  def test_get_evaluator_with_unmatched_label(self):
+    eval_config = eval_pb2.EvalConfig()
+    person_keypoints_metric = eval_config.parameterized_metric.add()
+    person_keypoints_metric.coco_keypoint_metrics.class_label = 'unmatched'
+    person_keypoints_metric.coco_keypoint_metrics.keypoint_label_to_sigmas[
+        'kpt'] = 0.1
+    categories = self._get_categories_list_with_keypoints()
+
+    evaluator = eval_util.get_evaluators(
+        eval_config, categories, evaluator_options=None)
+    self.assertLen(evaluator, 1)
+    self.assertNotIsInstance(
+        evaluator[0], coco_evaluation.CocoKeypointEvaluator)
+
   def test_padded_image_result_dict(self):
 
     input_data_fields = fields.InputDataFields
@@ -263,6 +332,8 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
 
     detection_boxes = np.array([[[0., 0., 1., 1.]], [[0.0, 0.0, 0.5, 0.5]]],
                                dtype=np.float32)
+    detection_keypoints = np.array([[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]],
+                                   dtype=np.float32)
     detections = {
         detection_fields.detection_boxes:
             tf.constant(detection_boxes),
@@ -271,7 +342,12 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
         detection_fields.detection_classes:
             tf.constant([[1], [2]]),
         detection_fields.num_detections:
-            tf.constant([1, 1])
+            tf.constant([1, 1]),
+        detection_fields.detection_keypoints:
+            tf.tile(
+                tf.reshape(
+                    tf.constant(detection_keypoints), shape=[1, 1, 3, 2]),
+                multiples=[2, 1, 1, 1])
     }
 
     gt_boxes = detection_boxes
@@ -280,6 +356,11 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
             tf.constant(gt_boxes),
         input_data_fields.groundtruth_classes:
             tf.constant([[1.], [1.]]),
+        input_data_fields.groundtruth_keypoints:
+            tf.tile(
+                tf.reshape(
+                    tf.constant(detection_keypoints), shape=[1, 1, 3, 2]),
+                multiples=[2, 1, 1, 1])
     }
 
     image = tf.zeros((2, 100, 100, 3), dtype=tf.float32)
@@ -299,11 +380,17 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
       self.assertAllEqual(
           [[[0., 0., 200., 200.]], [[0.0, 0.0, 150., 150.]]],
           result[input_data_fields.groundtruth_boxes])
+      self.assertAllClose([[[[0., 0.], [100., 100.], [200., 200.]]],
+                           [[[0., 0.], [150., 150.], [300., 300.]]]],
+                          result[input_data_fields.groundtruth_keypoints])
 
       # Predictions from the model are not scaled.
       self.assertAllEqual(
           [[[0., 0., 200., 200.]], [[0.0, 0.0, 75., 150.]]],
           result[detection_fields.detection_boxes])
+      self.assertAllClose([[[[0., 0.], [100., 100.], [200., 200.]]],
+                           [[[0., 0.], [75., 150.], [150., 300.]]]],
+                          result[detection_fields.detection_keypoints])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 666c491a..662cf1f1 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -108,7 +109,6 @@ from google.protobuf import text_format
 from object_detection import exporter
 from object_detection.protos import pipeline_pb2
 
-slim = tf.contrib.slim
 flags = tf.app.flags
 
 flags.DEFINE_string('input_type', 'image_tensor', 'Type of input node. Can be '
diff --git a/research/object_detection/export_tflite_ssd_graph.py b/research/object_detection/export_tflite_ssd_graph.py
index 6fe2ba9a..88fe1812 100644
--- a/research/object_detection/export_tflite_ssd_graph.py
+++ b/research/object_detection/export_tflite_ssd_graph.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index cf276fd4..548818c6 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/research/object_detection/export_tflite_ssd_graph_lib_test.py b/research/object_detection/export_tflite_ssd_graph_lib_test.py
index b469d595..cb83e7b7 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib_test.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -30,10 +31,18 @@ from object_detection.protos import graph_rewriter_pb2
 from object_detection.protos import pipeline_pb2
 from object_detection.protos import post_processing_pb2
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+
 if six.PY2:
-  import mock  # pylint: disable=g-import-not-at-top
+  import mock
 else:
-  from unittest import mock  # pylint: disable=g-import-not-at-top
+  from unittest import mock  # pylint: disable=g-importing-member
+# pylint: enable=g-import-not-at-top
 
 
 class FakeModel(model.DetectionModel):
@@ -45,7 +54,7 @@ class FakeModel(model.DetectionModel):
     pass
 
   def predict(self, preprocessed_inputs, true_image_shapes):
-    features = tf.contrib.slim.conv2d(preprocessed_inputs, 3, 1)
+    features = contrib_slim.conv2d(preprocessed_inputs, 3, 1)
     with tf.control_dependencies([features]):
       prediction_tensors = {
           'box_encodings':
@@ -105,17 +114,17 @@ class ExportTfliteGraphTest(tf.test.TestCase):
         saver.save(sess, checkpoint_path)
 
   def _assert_quant_vars_exists(self, tflite_graph_file):
-    with tf.gfile.Open(tflite_graph_file) as f:
+    with tf.gfile.Open(tflite_graph_file, mode='rb') as f:
       graph_string = f.read()
       print(graph_string)
-      self.assertTrue('quant' in graph_string)
+      self.assertIn(six.ensure_binary('quant'), graph_string)
 
   def _import_graph_and_run_inference(self, tflite_graph_file, num_channels=3):
     """Imports a tflite graph, runs single inference and returns outputs."""
     graph = tf.Graph()
     with graph.as_default():
       graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      with tf.gfile.Open(tflite_graph_file, mode='rb') as f:
         graph_def.ParseFromString(f.read())
       tf.import_graph_def(graph_def, name='')
       input_tensor = graph.get_tensor_by_name('normalized_input_image_tensor:0')
@@ -330,21 +339,21 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     graph = tf.Graph()
     with graph.as_default():
       graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      with tf.gfile.Open(tflite_graph_file, mode='rb') as f:
         graph_def.ParseFromString(f.read())
       all_op_names = [node.name for node in graph_def.node]
       self.assertIn('TFLite_Detection_PostProcess', all_op_names)
       self.assertNotIn('UnattachedTensor', all_op_names)
       for node in graph_def.node:
         if node.name == 'TFLite_Detection_PostProcess':
-          self.assertTrue(node.attr['_output_quantized'].b is True)
+          self.assertTrue(node.attr['_output_quantized'].b)
           self.assertTrue(
-              node.attr['_support_output_type_float_in_quantized_op'].b is True)
-          self.assertTrue(node.attr['y_scale'].f == 10.0)
-          self.assertTrue(node.attr['x_scale'].f == 10.0)
-          self.assertTrue(node.attr['h_scale'].f == 5.0)
-          self.assertTrue(node.attr['w_scale'].f == 5.0)
-          self.assertTrue(node.attr['num_classes'].i == 2)
+              node.attr['_support_output_type_float_in_quantized_op'].b)
+          self.assertEqual(node.attr['y_scale'].f, 10.0)
+          self.assertEqual(node.attr['x_scale'].f, 10.0)
+          self.assertEqual(node.attr['h_scale'].f, 5.0)
+          self.assertEqual(node.attr['w_scale'].f, 5.0)
+          self.assertEqual(node.attr['num_classes'].i, 2)
           self.assertTrue(
               all([
                   t == types_pb2.DT_FLOAT
@@ -362,7 +371,7 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     graph = tf.Graph()
     with graph.as_default():
       graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      with tf.gfile.Open(tflite_graph_file, mode='rb') as f:
         graph_def.ParseFromString(f.read())
       all_op_names = [node.name for node in graph_def.node]
       self.assertIn('UnattachedTensor', all_op_names)
@@ -381,7 +390,7 @@ class ExportTfliteGraphTest(tf.test.TestCase):
     graph = tf.Graph()
     with graph.as_default():
       graph_def = tf.GraphDef()
-      with tf.gfile.Open(tflite_graph_file) as f:
+      with tf.gfile.Open(tflite_graph_file, mode='rb') as f:
         graph_def.ParseFromString(f.read())
       all_op_names = [node.name for node in graph_def.node]
       self.assertIn('TFLite_Detection_PostProcess', all_op_names)
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index a109d3b9..62c0ee4a 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -17,7 +17,6 @@
 import os
 import tempfile
 import tensorflow as tf
-from tensorflow.contrib.quantize.python import graph_matcher
 from tensorflow.core.protobuf import saver_pb2
 from tensorflow.python.tools import freeze_graph  # pylint: disable=g-direct-tensorflow-import
 from object_detection.builders import graph_rewriter_builder
@@ -27,7 +26,15 @@ from object_detection.data_decoders import tf_example_decoder
 from object_detection.utils import config_util
 from object_detection.utils import shape_utils
 
-slim = tf.contrib.slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim
+  from tensorflow.contrib import tfprof as contrib_tfprof
+  from tensorflow.contrib.quantize.python import graph_matcher
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 freeze_graph_with_def_protos = freeze_graph.freeze_graph_with_def_protos
 
@@ -41,7 +48,7 @@ def rewrite_nn_resize_op(is_quantized=False):
     is_quantized: True if the default graph is quantized.
   """
   def remove_nn():
-    """Remove nearest neighbor upsampling structure and replace with TF op."""
+    """Remove nearest neighbor upsampling structures and replace with TF op."""
     input_pattern = graph_matcher.OpTypePattern(
         'FakeQuantWithMinMaxVars' if is_quantized else '*')
     stack_1_pattern = graph_matcher.OpTypePattern(
@@ -50,27 +57,37 @@ def rewrite_nn_resize_op(is_quantized=False):
         'Pack', inputs=[stack_1_pattern, stack_1_pattern], ordered_inputs=False)
     reshape_pattern = graph_matcher.OpTypePattern(
         'Reshape', inputs=[stack_2_pattern, 'Const'], ordered_inputs=False)
-    consumer_pattern = graph_matcher.OpTypePattern(
+    consumer_pattern1 = graph_matcher.OpTypePattern(
         'Add|AddV2|Max|Mul', inputs=[reshape_pattern, '*'],
         ordered_inputs=False)
+    consumer_pattern2 = graph_matcher.OpTypePattern(
+        'StridedSlice', inputs=[reshape_pattern, '*', '*', '*'],
+        ordered_inputs=False)
 
-    match_counter = 0
-    matcher = graph_matcher.GraphMatcher(consumer_pattern)
-    for match in matcher.match_graph(tf.get_default_graph()):
-      match_counter += 1
-      projection_op = match.get_op(input_pattern)
-      reshape_op = match.get_op(reshape_pattern)
-      consumer_op = match.get_op(consumer_pattern)
-      nn_resize = tf.image.resize_nearest_neighbor(
-          projection_op.outputs[0],
-          reshape_op.outputs[0].shape.dims[1:3],
-          align_corners=False,
-          name=os.path.split(reshape_op.name)[0] + '/resize_nearest_neighbor')
-
-      for index, op_input in enumerate(consumer_op.inputs):
-        if op_input == reshape_op.outputs[0]:
-          consumer_op._update_input(index, nn_resize)  # pylint: disable=protected-access
-          break
+    def replace_matches(consumer_pattern):
+      """Search for nearest neighbor pattern and replace with TF op."""
+      match_counter = 0
+      matcher = graph_matcher.GraphMatcher(consumer_pattern)
+      for match in matcher.match_graph(tf.get_default_graph()):
+        match_counter += 1
+        projection_op = match.get_op(input_pattern)
+        reshape_op = match.get_op(reshape_pattern)
+        consumer_op = match.get_op(consumer_pattern)
+        nn_resize = tf.image.resize_nearest_neighbor(
+            projection_op.outputs[0],
+            reshape_op.outputs[0].shape.dims[1:3],
+            align_corners=False,
+            name=os.path.split(reshape_op.name)[0] + '/resize_nearest_neighbor')
+
+        for index, op_input in enumerate(consumer_op.inputs):
+          if op_input == reshape_op.outputs[0]:
+            consumer_op._update_input(index, nn_resize)  # pylint: disable=protected-access
+            break
+
+      return match_counter
+
+    match_counter = replace_matches(consumer_pattern1)
+    match_counter += replace_matches(consumer_pattern2)
 
     tf.logging.info('Found and fixed {} matches'.format(match_counter))
     return match_counter
@@ -524,8 +541,8 @@ def profile_inference_graph(graph):
     graph: the inference graph.
   """
   tfprof_vars_option = (
-      tf.contrib.tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
-  tfprof_flops_option = tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS
+      contrib_tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
+  tfprof_flops_option = contrib_tfprof.model_analyzer.FLOAT_OPS_OPTIONS
 
   # Batchnorm is usually folded during inference.
   tfprof_vars_option['trim_name_regexes'] = ['.*BatchNorm.*']
@@ -534,10 +551,8 @@ def profile_inference_graph(graph):
       '.*BatchNorm.*', '.*Initializer.*', '.*Regularizer.*', '.*BiasAdd.*'
   ]
 
-  tf.contrib.tfprof.model_analyzer.print_model_analysis(
-      graph,
-      tfprof_options=tfprof_vars_option)
+  contrib_tfprof.model_analyzer.print_model_analysis(
+      graph, tfprof_options=tfprof_vars_option)
 
-  tf.contrib.tfprof.model_analyzer.print_model_analysis(
-      graph,
-      tfprof_options=tfprof_flops_option)
+  contrib_tfprof.model_analyzer.print_model_analysis(
+      graph, tfprof_options=tfprof_flops_option)
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index 66c39636..e26e164e 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,6 +15,9 @@
 # ==============================================================================
 
 """Tests for object_detection.export_inference_graph."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 import os
 import numpy as np
 import six
@@ -36,7 +40,13 @@ if six.PY2:
 else:
   from unittest import mock  # pylint: disable=g-import-not-at-top
 
-slim = tf.contrib.slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 class FakeModel(model.DetectionModel):
@@ -55,7 +65,7 @@ class FakeModel(model.DetectionModel):
     return {'image': tf.layers.conv2d(preprocessed_inputs, 3, 1)}
 
   def postprocess(self, prediction_dict, true_image_shapes):
-    with tf.control_dependencies(prediction_dict.values()):
+    with tf.control_dependencies(list(prediction_dict.values())):
       postprocessed_tensors = {
           'detection_boxes': tf.constant([[[0.0, 0.0, 0.5, 0.5],
                                            [0.5, 0.5, 0.8, 0.8]],
@@ -135,7 +145,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     od_graph = tf.Graph()
     with od_graph.as_default():
       od_graph_def = tf.GraphDef()
-      with tf.gfile.GFile(inference_graph_path) as fid:
+      with tf.gfile.GFile(inference_graph_path, mode='rb') as fid:
         if is_binary:
           od_graph_def.ParseFromString(fid.read())
         else:
@@ -147,7 +157,9 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     with self.test_session():
       encoded_image = tf.image.encode_jpeg(tf.constant(image_array)).eval()
     def _bytes_feature(value):
-      return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
+      return tf.train.Feature(
+          bytes_list=tf.train.BytesList(value=[six.ensure_binary(value)]))
+
     example = tf.train.Example(features=tf.train.Features(feature={
         'image/encoded': _bytes_feature(encoded_image),
         'image/format': _bytes_feature('jpg'),
@@ -401,7 +413,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     self._load_inference_graph(inference_graph_path, is_binary=False)
     has_quant_nodes = False
     for v in variables_helper.get_global_variables_safely():
-      if v.op.name.endswith('act_quant/min'):
+      if six.ensure_str(v.op.name).endswith('act_quant/min'):
         has_quant_nodes = True
         break
     self.assertTrue(has_quant_nodes)
@@ -724,7 +736,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           input_shape=None,
           output_collection_name='inference_op',
           graph_hook_fn=None)
-      output_node_names = ','.join(outputs.keys())
+      output_node_names = ','.join(list(outputs.keys()))
       saver = tf.train.Saver()
       input_saver_def = saver.as_saver_def()
       exporter.freeze_graph_with_def_protos(
@@ -877,7 +889,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           input_shape=None,
           output_collection_name='inference_op',
           graph_hook_fn=None)
-      output_node_names = ','.join(outputs.keys())
+      output_node_names = ','.join(list(outputs.keys()))
       saver = tf.train.Saver()
       input_saver_def = saver.as_saver_def()
       frozen_graph_def = exporter.freeze_graph_with_def_protos(
@@ -1080,7 +1092,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     g = tf.Graph()
     with g.as_default():
       x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
-      x_conv = tf.contrib.slim.conv2d(x, 8, 1)
+      x_conv = contrib_slim.conv2d(x, 8, 1)
       y = array_ops.placeholder(dtypes.float32, shape=(8, 20, 20, 8))
       s = ops.nearest_neighbor_upsampling(x_conv, 2)
       t = s + y
@@ -1103,6 +1115,50 @@ class ExportInferenceGraphTest(tf.test.TestCase):
 
     self.assertTrue(resize_op_found)
 
+  def test_rewrite_nn_resize_op_odd_size(self):
+    g = tf.Graph()
+    with g.as_default():
+      x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
+      s = ops.nearest_neighbor_upsampling(x, 2)
+      t = s[:, :19, :19, :]
+      exporter.rewrite_nn_resize_op()
+
+    resize_op_found = False
+    for op in g.get_operations():
+      if op.type == 'ResizeNearestNeighbor':
+        resize_op_found = True
+        self.assertEqual(op.inputs[0], x)
+        self.assertEqual(op.outputs[0].consumers()[0], t.op)
+        break
+
+    self.assertTrue(resize_op_found)
+
+  def test_rewrite_nn_resize_op_quantized_odd_size(self):
+    g = tf.Graph()
+    with g.as_default():
+      x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
+      x_conv = contrib_slim.conv2d(x, 8, 1)
+      s = ops.nearest_neighbor_upsampling(x_conv, 2)
+      t = s[:, :19, :19, :]
+
+      graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
+      graph_rewriter_config.quantization.delay = 500000
+      graph_rewriter_fn = graph_rewriter_builder.build(
+          graph_rewriter_config, is_training=False)
+      graph_rewriter_fn()
+
+      exporter.rewrite_nn_resize_op(is_quantized=True)
+
+    resize_op_found = False
+    for op in g.get_operations():
+      if op.type == 'ResizeNearestNeighbor':
+        resize_op_found = True
+        self.assertEqual(op.inputs[0].op.type, 'FakeQuantWithMinMaxVars')
+        self.assertEqual(op.outputs[0].consumers()[0], t.op)
+        break
+
+    self.assertTrue(resize_op_found)
+
   def test_rewrite_nn_resize_op_multiple_path(self):
     g = tf.Graph()
     with g.as_default():
@@ -1136,7 +1192,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       self.assertNotEqual(node.op, 'Pack')
       if node.op == 'ResizeNearestNeighbor':
         counter_resize_op += 1
-        self.assertIn(node.name + ':0', t_input_ops)
+        self.assertIn(six.ensure_str(node.name) + ':0', t_input_ops)
     self.assertEqual(counter_resize_op, 2)
 
 
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index eeba4880..686f6f8c 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -1,9 +1,10 @@
 # Tensorflow detection model zoo
 
 We provide a collection of detection models pre-trained on the [COCO
-dataset](http://cocodataset.org/), the [Kitti dataset](http://www.cvlibs.net/datasets/kitti/),
-the [Open Images dataset](https://storage.googleapis.com/openimages/web/index.html), the
-[AVA v2.1 dataset](https://research.google.com/ava/) and the
+dataset](http://cocodataset.org), the [Kitti dataset](http://www.cvlibs.net/datasets/kitti/),
+the
+[Open Images dataset](https://storage.googleapis.com/openimages/web/index.html),
+the [AVA v2.1 dataset](https://research.google.com/ava/) and the
 [iNaturalist Species Detection Dataset](https://github.com/visipedia/inat_comp/blob/master/2017/README.md#bounding-boxes).
 These models can be useful for out-of-the-box inference if you are interested in
 categories already in those datasets. They are also useful for initializing your
@@ -106,10 +107,11 @@ Note: If you download the tar.gz file of quantized models and un-tar, you will g
 
 ### Mobile models
 
-Model name                                                                                                                          | Pixel 1 Latency (ms) | COCO mAP | Outputs
------------------------------------------------------------------------------------------------------------------------------------ | :------------------: | :------: | :-----:
-[ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz) | 119                  | 22.6     | Boxes
-[ssd_mobilenet_v3_small_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz) | 43                   | 15.4     | Boxes
+Model name                                                                                                                                                                | Pixel 1 Latency (ms) | COCO mAP | Outputs
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :------: | :-----:
+[ssd_mobilenet_v2_mnasfpn_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_06.tar.gz) | 183                  | 26.6     | Boxes
+[ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz)                                       | 119                  | 22.6     | Boxes
+[ssd_mobilenet_v3_small_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz)                                       | 43                   | 15.4     | Boxes
 
 ### Pixel4 Edge TPU models
 Model name                                                                                                                          | Pixel 4  Edge TPU Latency (ms) | COCO mAP | Outputs
diff --git a/research/object_detection/g3doc/img/dataset_explorer.png b/research/object_detection/g3doc/img/dataset_explorer.png
deleted file mode 100644
index 173003e8..00000000
Binary files a/research/object_detection/g3doc/img/dataset_explorer.png and /dev/null differ
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index c793ab13..fb13ca02 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -11,7 +11,7 @@ Tensorflow Object Detection API depends on the following libraries:
 *   tf Slim (which is included in the "tensorflow/models/research/" checkout)
 *   Jupyter notebook
 *   Matplotlib
-*   Tensorflow (>=1.12.0)
+*   Tensorflow (1.15.0)
 *   Cython
 *   contextlib2
 *   cocoapi
@@ -59,7 +59,9 @@ If that is your case, try the [manual](#Manual-protobuf-compiler-installation-an
 git clone https://github.com/tensorflow/models.git
 ```
 
-To use this library, you need to download this repository, whenever it says `<path-to-tensorflow>` it will be referring to the folder that you downloaded this repository into.
+To use this library, you need to download this repository, whenever it says
+`<path-to-tensorflow>` it will be referring to the folder that you downloaded
+this repository into.
 
 ## COCO API installation
 
@@ -80,18 +82,20 @@ make
 cp -r pycocotools <path_to_tensorflow>/models/research/
 ```
 
- Alternatively, users can install `pycocotools` using pip:
+Alternatively, users can install `pycocotools` using pip:
 
- ```bash
+```bash
 pip install --user pycocotools
- ```
+```
 
 ## Protobuf Compilation
 
 The Tensorflow Object Detection API uses Protobufs to configure model and
 training parameters. Before the framework can be used, the Protobuf libraries
 must be compiled. This should be done by running the following command from
-the [tensorflow/models/research/](https://github.com/tensorflow/models/tree/master/research/) directory:
+the [tensorflow/models/research/
+](https://github.com/tensorflow/models/tree/master/research/)
+directory:
 
 
 ``` bash
@@ -154,7 +158,8 @@ export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
 Note: This command needs to run from every new terminal you start. If you wish
 to avoid running this manually, you can add it as a new line to the end of your
 ~/.bashrc file, replacing \`pwd\` with the absolute path of
-tensorflow/models/research on your system. After updating ~/.bashrc file you can run the following command:
+tensorflow/models/research on your system. After updating ~/.bashrc file you
+can run the following command:
 
 ``` bash
 source ~/.bashrc
@@ -165,6 +170,8 @@ source ~/.bashrc
 You can test that you have correctly installed the Tensorflow Object Detection\
 API by running the following command:
 
+
 ```bash
-python object_detection/builders/model_builder_test.py
+# If using Tensorflow 1.X:
+python object_detection/builders/model_builder_tf1_test.py
 ```
diff --git a/research/object_detection/g3doc/oid_inference_and_evaluation.md b/research/object_detection/g3doc/oid_inference_and_evaluation.md
index 97313f9b..4babf10a 100644
--- a/research/object_detection/g3doc/oid_inference_and_evaluation.md
+++ b/research/object_detection/g3doc/oid_inference_and_evaluation.md
@@ -142,11 +142,11 @@ python -m object_detection/inference/infer_detections \
 
 Inference preserves all fields of the input TFExamples, and adds new fields to
 store the inferred detections. This allows [computing evaluation
-measures](#computing-evaluation-measures) on the output TFRecord alone, as ground
-truth boxes are preserved as well. Since measure computations don't require
-access to the images, `infer_detections` can optionally discard them with the
-`--discard_image_pixels` flag. Discarding the images drastically reduces the
-size of the output TFRecord.
+measures](#computing-evaluation-measures) on the output TFRecord alone, as
+groundtruth boxes are preserved as well. Since measure computations don't
+require access to the images, `infer_detections` can optionally discard them
+with the `--discard_image_pixels` flag. Discarding the images drastically
+reduces the size of the output TFRecord.
 
 ### Accelerating inference
 
diff --git a/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md b/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
index 2135c312..db166bcd 100644
--- a/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
+++ b/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md
@@ -56,7 +56,7 @@ via the following command. For a quantized model, run this from the tensorflow/
 directory:
 
 ```shell
-bazel run --config=opt tensorflow/lite/toco:toco -- \
+bazel run -c opt tensorflow/lite/toco:toco -- \
 --input_file=$OUTPUT_DIR/tflite_graph.pb \
 --output_file=$OUTPUT_DIR/detect.tflite \
 --input_shapes=1,300,300,3 \
@@ -82,7 +82,7 @@ parameters and can be run via the TensorFlow Lite interpreter on the Android
 device. For a floating point model, run this from the tensorflow/ directory:
 
 ```shell
-bazel run --config=opt tensorflow/lite/toco:toco -- \
+bazel run -c opt tensorflow/lite/toco:toco -- \
 --input_file=$OUTPUT_DIR/tflite_graph.pb \
 --output_file=$OUTPUT_DIR/detect.tflite \
 --input_shapes=1,300,300,3 \
diff --git a/research/object_detection/g3doc/tpu_compatibility.md b/research/object_detection/g3doc/tpu_compatibility.md
index f64a5001..0eb0c7a2 100644
--- a/research/object_detection/g3doc/tpu_compatibility.md
+++ b/research/object_detection/g3doc/tpu_compatibility.md
@@ -46,26 +46,16 @@ have static shape:
 
 *   **Groundtruth tensors with static shape** - Images in a typical detection
     dataset have variable number of groundtruth boxes and associated classes.
-    Setting `max_number_of_boxes` to a large enough number in the
-    `train_input_reader` and `eval_input_reader` pads the groundtruth tensors
-    with zeros to a static shape. Padded groundtruth tensors are correctly
-    handled internally within the model.
+    Setting `max_number_of_boxes` to a large enough number in `train_config`
+    pads the groundtruth tensors with zeros to a static shape. Padded
+    groundtruth tensors are correctly handled internally within the model.
 
     ```
-    train_input_reader: {
-      tf_record_input_reader {
-        input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
-      }
-      label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
-      max_number_of_boxes: 200
-    }
-
-    eval_input_reader: {
-      tf_record_input_reader {
-        input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-0010"
-      }
-      label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+    train_config: {
+      fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+      batch_size: 64
       max_number_of_boxes: 200
+      unpad_groundtruth_tensors: false
     }
     ```
 
diff --git a/research/object_detection/inference/detection_inference_test.py b/research/object_detection/inference/detection_inference_test.py
index eabb6b47..31cd3e9b 100644
--- a/research/object_detection/inference/detection_inference_test.py
+++ b/research/object_detection/inference/detection_inference_test.py
@@ -15,11 +15,12 @@
 r"""Tests for detection_inference.py."""
 
 import os
-import StringIO
 
 import numpy as np
 from PIL import Image
+import six
 import tensorflow as tf
+from google.protobuf import text_format
 
 from object_detection.core import standard_fields
 from object_detection.inference import detection_inference
@@ -32,7 +33,7 @@ def get_mock_tfrecord_path():
 
 def create_mock_tfrecord():
   pil_image = Image.fromarray(np.array([[[123, 0, 0]]], dtype=np.uint8), 'RGB')
-  image_output_stream = StringIO.StringIO()
+  image_output_stream = six.BytesIO()
   pil_image.save(image_output_stream, format='png')
   encoded_image = image_output_stream.getvalue()
 
@@ -46,6 +47,7 @@ def create_mock_tfrecord():
   tf_example = tf.train.Example(features=tf.train.Features(feature=feature_map))
   with tf.python_io.TFRecordWriter(get_mock_tfrecord_path()) as writer:
     writer.write(tf_example.SerializeToString())
+  return encoded_image
 
 
 def get_mock_graph_path():
@@ -76,7 +78,7 @@ class InferDetectionsTests(tf.test.TestCase):
 
   def test_simple(self):
     create_mock_graph()
-    create_mock_tfrecord()
+    encoded_image = create_mock_tfrecord()
 
     serialized_example_tensor, image_tensor = detection_inference.build_input(
         [get_mock_tfrecord_path()])
@@ -94,8 +96,8 @@ class InferDetectionsTests(tf.test.TestCase):
       tf_example = detection_inference.infer_detections_and_add_to_example(
           serialized_example_tensor, detected_boxes_tensor,
           detected_scores_tensor, detected_labels_tensor, False)
-
-    self.assertProtoEquals(r"""
+    expected_example = tf.train.Example()
+    text_format.Merge(r"""
         features {
           feature {
             key: "image/detection/bbox/ymin"
@@ -115,17 +117,14 @@ class InferDetectionsTests(tf.test.TestCase):
           feature {
             key: "image/detection/score"
             value { float_list { value: [0.1, 0.2] } } }
-          feature {
-            key: "image/encoded"
-            value { bytes_list { value:
-              "\211PNG\r\n\032\n\000\000\000\rIHDR\000\000\000\001\000\000"
-              "\000\001\010\002\000\000\000\220wS\336\000\000\000\022IDATx"
-              "\234b\250f`\000\000\000\000\377\377\003\000\001u\000|gO\242"
-              "\213\000\000\000\000IEND\256B`\202" } } }
           feature {
             key: "test_field"
-            value { float_list { value: [1.0, 2.0, 3.0, 4.0] } } } }
-    """, tf_example)
+            value { float_list { value: [1.0, 2.0, 3.0, 4.0] } } } }""",
+                      expected_example)
+    expected_example.features.feature[
+        standard_fields.TfExampleFields
+        .image_encoded].CopyFrom(dataset_util.bytes_feature(encoded_image))
+    self.assertProtoEquals(expected_example, tf_example)
 
   def test_discard_image(self):
     create_mock_graph()
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index fd807795..74b2c61c 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -43,6 +43,7 @@ from object_detection.utils import shape_utils
 HASH_KEY = 'hash'
 HASH_BINS = 1 << 31
 SERVING_FED_EXAMPLE_KEY = 'serialized_example'
+_LABEL_OFFSET = 1
 
 # A map of names to methods that help build the input pipeline.
 INPUT_BUILDER_UTIL_MAP = {
@@ -67,6 +68,64 @@ def _multiclass_scores_or_one_hot_labels(multiclass_scores,
   return tf.cond(tf.size(multiclass_scores) > 0, true_fn, false_fn)
 
 
+def _convert_labeled_classes_to_k_hot(groundtruth_labeled_classes, num_classes):
+  """Returns k-hot encoding of the labeled classes."""
+
+  # If the input labeled_classes is empty, it assumes all classes are
+  # exhaustively labeled, thus returning an all-one encoding.
+  def true_fn():
+    return tf.sparse_to_dense(
+        groundtruth_labeled_classes - _LABEL_OFFSET, [num_classes],
+        tf.constant(1, dtype=tf.float32),
+        validate_indices=False)
+
+  def false_fn():
+    return tf.ones(num_classes, dtype=tf.float32)
+
+  return tf.cond(tf.size(groundtruth_labeled_classes) > 0, true_fn, false_fn)
+
+
+def _remove_unrecognized_classes(class_ids, unrecognized_label):
+  """Returns class ids with unrecognized classes filtered out."""
+
+  recognized_indices = tf.where(tf.greater(class_ids, unrecognized_label))
+  return tf.gather(class_ids, recognized_indices)
+
+
+def assert_or_prune_invalid_boxes(boxes):
+  """Makes sure boxes have valid sizes (ymax >= ymin, xmax >= xmin).
+
+  When the hardware supports assertions, the function raises an error when
+  boxes have an invalid size. If assertions are not supported (e.g. on TPU),
+  boxes with invalid sizes are filtered out.
+
+  Args:
+    boxes: float tensor of shape [num_boxes, 4]
+
+  Returns:
+    boxes: float tensor of shape [num_valid_boxes, 4] with invalid boxes
+      filtered out.
+
+  Raises:
+    tf.errors.InvalidArgumentError: When we detect boxes with invalid size.
+      This is not supported on TPUs.
+  """
+
+  ymin, xmin, ymax, xmax = tf.split(
+      boxes, num_or_size_splits=4, axis=1)
+
+  height_check = tf.Assert(tf.reduce_all(ymax >= ymin), [ymin, ymax])
+  width_check = tf.Assert(tf.reduce_all(xmax >= xmin), [xmin, xmax])
+
+  with tf.control_dependencies([height_check, width_check]):
+    boxes_tensor = tf.concat([ymin, xmin, ymax, xmax], axis=1)
+    boxlist = box_list.BoxList(boxes_tensor)
+    # TODO(b/149221748) Remove pruning when XLA supports assertions.
+    boxlist = box_list_ops.prune_small_boxes(boxlist, 0)
+
+  return boxlist.get()
+
+
 def transform_input_data(tensor_dict,
                          model_preprocess_fn,
                          image_resizer_fn,
@@ -76,7 +135,8 @@ def transform_input_data(tensor_dict,
                          retain_original_image=False,
                          use_multiclass_scores=False,
                          use_bfloat16=False,
-                         retain_original_image_additional_channels=False):
+                         retain_original_image_additional_channels=False,
+                         keypoint_type_weight=None):
   """A single function that is responsible for all input data transformations.
 
   Data transformation functions are applied in the following order.
@@ -85,10 +145,15 @@ def transform_input_data(tensor_dict,
      fields.InputDataFields.image.
   2. data_augmentation_fn (optional): applied on tensor_dict.
   3. model_preprocess_fn: applied only on image tensor in tensor_dict.
-  4. image_resizer_fn: applied on original image and instance mask tensor in
+  4. keypoint_type_weight (optional): If groundtruth keypoints are in
+     the tensor dictionary, per-keypoint weights are produced. These weights are
+     initialized by `keypoint_type_weight` (or ones if left None).
+     Then, for all keypoints that are not visible, the weights are set to 0 (to
+     avoid penalizing the model in a loss function).
+  5. image_resizer_fn: applied on original image and instance mask tensor in
      tensor_dict.
-  5. one_hot_encoding: applied to classes tensor in tensor_dict.
-  6. merge_multiple_boxes (optional): when groundtruth boxes are exactly the
+  6. one_hot_encoding: applied to classes tensor in tensor_dict.
+  7. merge_multiple_boxes (optional): when groundtruth boxes are exactly the
      same they can be merged into a single box with an associated k-hot class
      label.
 
@@ -117,12 +182,25 @@ def transform_input_data(tensor_dict,
     use_bfloat16: (optional) a bool, whether to use bfloat16 in training.
     retain_original_image_additional_channels: (optional) Whether to retain
       original image additional channels in the output dictionary.
+    keypoint_type_weight: A list (of length num_keypoints) containing
+      groundtruth loss weights to use for each keypoint. If None, will use a
+      weight of 1.
 
   Returns:
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
     after applying all the transformations.
   """
   out_tensor_dict = tensor_dict.copy()
+
+  labeled_classes_field = fields.InputDataFields.groundtruth_labeled_classes
+  if labeled_classes_field in out_tensor_dict:
+    # tf_example_decoder casts unrecognized labels to -1. Remove these
+    # unrecognized labels before converting labeled_classes to k-hot vector.
+    out_tensor_dict[labeled_classes_field] = _remove_unrecognized_classes(
+        out_tensor_dict[labeled_classes_field], unrecognized_label=-1)
+    out_tensor_dict[labeled_classes_field] = _convert_labeled_classes_to_k_hot(
+        out_tensor_dict[labeled_classes_field], num_classes)
+
   if fields.InputDataFields.multiclass_scores in out_tensor_dict:
     out_tensor_dict[
         fields.InputDataFields
@@ -173,8 +251,11 @@ def transform_input_data(tensor_dict,
     bboxes = out_tensor_dict[fields.InputDataFields.groundtruth_boxes]
     boxlist = box_list.BoxList(bboxes)
     realigned_bboxes = box_list_ops.change_coordinate_frame(boxlist, im_box)
+
+    realigned_boxes_tensor = realigned_bboxes.get()
+    valid_boxes_tensor = assert_or_prune_invalid_boxes(realigned_boxes_tensor)
     out_tensor_dict[
-        fields.InputDataFields.groundtruth_boxes] = realigned_bboxes.get()
+        fields.InputDataFields.groundtruth_boxes] = valid_boxes_tensor
 
   if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
     keypoints = out_tensor_dict[fields.InputDataFields.groundtruth_keypoints]
@@ -182,10 +263,24 @@ def transform_input_data(tensor_dict,
                                                                im_box)
     out_tensor_dict[
         fields.InputDataFields.groundtruth_keypoints] = realigned_keypoints
+    flds_gt_kpt = fields.InputDataFields.groundtruth_keypoints
+    flds_gt_kpt_vis = fields.InputDataFields.groundtruth_keypoint_visibilities
+    flds_gt_kpt_weights = fields.InputDataFields.groundtruth_keypoint_weights
+    if flds_gt_kpt_vis not in out_tensor_dict:
+      out_tensor_dict[flds_gt_kpt_vis] = tf.ones_like(
+          out_tensor_dict[flds_gt_kpt][:, :, 0],
+          dtype=tf.bool)
+    out_tensor_dict[flds_gt_kpt_weights] = (
+        keypoint_ops.keypoint_weights_from_visibilities(
+            out_tensor_dict[flds_gt_kpt_vis],
+            keypoint_type_weight))
 
   if use_bfloat16:
     preprocessed_resized_image = tf.cast(
         preprocessed_resized_image, tf.bfloat16)
+    if fields.InputDataFields.context_features in out_tensor_dict:
+      out_tensor_dict[fields.InputDataFields.context_features] = tf.cast(
+          out_tensor_dict[fields.InputDataFields.context_features], tf.bfloat16)
   out_tensor_dict[fields.InputDataFields.image] = tf.squeeze(
       preprocessed_resized_image, axis=0)
   out_tensor_dict[fields.InputDataFields.true_image_shape] = tf.squeeze(
@@ -198,9 +293,8 @@ def transform_input_data(tensor_dict,
     out_tensor_dict[
         fields.InputDataFields.groundtruth_instance_masks] = resized_masks
 
-  label_offset = 1
   zero_indexed_groundtruth_classes = out_tensor_dict[
-      fields.InputDataFields.groundtruth_classes] - label_offset
+      fields.InputDataFields.groundtruth_classes] - _LABEL_OFFSET
   if use_multiclass_scores:
     out_tensor_dict[
         fields.InputDataFields.groundtruth_classes] = out_tensor_dict[
@@ -242,8 +336,12 @@ def transform_input_data(tensor_dict,
   return out_tensor_dict
 
 
-def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
-                                    spatial_image_shape=None):
+def pad_input_data_to_static_shapes(tensor_dict,
+                                    max_num_boxes,
+                                    num_classes,
+                                    spatial_image_shape=None,
+                                    max_num_context_features=None,
+                                    context_feature_length=None):
   """Pads input tensors to static shapes.
 
   In case num_additional_channels > 0, we assume that the additional channels
@@ -257,6 +355,9 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
       padding.
     spatial_image_shape: A list of two integers of the form [height, width]
       containing expected spatial shape of the image.
+    max_num_context_features (optional): The maximum number of context
+      features needed to compute shapes padding.
+    context_feature_length (optional): The length of the context feature.
 
   Returns:
     A dictionary keyed by fields.InputDataFields containing padding shapes for
@@ -264,7 +365,9 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
 
   Raises:
     ValueError: If groundtruth classes is neither rank 1 nor rank 2, or if we
-      detect that additional channels have not been concatenated yet.
+      detect that additional channels have not been concatenated yet, or if
+      max_num_context_features is not specified and context_features is in the
+      tensor dict.
   """
 
   if not spatial_image_shape or spatial_image_shape == [-1, -1]:
@@ -296,10 +399,14 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
       raise ValueError(
           'Image must be already concatenated with additional channels.')
 
+  if fields.InputDataFields.context_features in tensor_dict and (
+      max_num_context_features is None):
+    raise ValueError('max_num_context_features must be specified in the model '
+                     'config if include_context is specified in the input '
+                     'config')
+
   padding_shapes = {
-      fields.InputDataFields.image: [
-          height, width, num_channels
-      ],
+      fields.InputDataFields.image: [height, width, num_channels],
       fields.InputDataFields.original_image_spatial_shape: [2],
       fields.InputDataFields.image_additional_channels: [
           height, width, num_additional_channels
@@ -326,6 +433,7 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
       fields.InputDataFields.true_image_shape: [3],
       fields.InputDataFields.groundtruth_image_classes: [num_classes],
       fields.InputDataFields.groundtruth_image_confidences: [num_classes],
+      fields.InputDataFields.groundtruth_labeled_classes: [num_classes],
   }
 
   if fields.InputDataFields.original_image in tensor_dict:
@@ -348,6 +456,25 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
     padding_shapes[fields.InputDataFields.
                    groundtruth_keypoint_visibilities] = padding_shape
 
+  if fields.InputDataFields.groundtruth_keypoint_weights in tensor_dict:
+    tensor_shape = (
+        tensor_dict[fields.InputDataFields.groundtruth_keypoint_weights].shape)
+    padding_shape = [max_num_boxes, shape_utils.get_dim_as_int(tensor_shape[1])]
+    padding_shapes[fields.InputDataFields.
+                   groundtruth_keypoint_weights] = padding_shape
+
+  # Prepare for ContextRCNN related fields.
+  if fields.InputDataFields.context_features in tensor_dict:
+    padding_shape = [max_num_context_features, context_feature_length]
+    padding_shapes[fields.InputDataFields.context_features] = padding_shape
+
+    tensor_shape = tf.shape(
+        tensor_dict[fields.InputDataFields.context_features])
+    tensor_dict[fields.InputDataFields.valid_context_size] = tensor_shape[0]
+    padding_shapes[fields.InputDataFields.valid_context_size] = []
+  if fields.InputDataFields.context_feature_length in tensor_dict:
+    padding_shapes[fields.InputDataFields.context_feature_length] = []
+
   padded_tensor_dict = {}
   for tensor_name in tensor_dict:
     padded_tensor_dict[tensor_name] = shape_utils.pad_or_clip_nd(
@@ -383,6 +510,8 @@ def augment_input_data(tensor_dict, data_augmentation_options):
                             in tensor_dict)
   include_keypoints = (fields.InputDataFields.groundtruth_keypoints
                        in tensor_dict)
+  include_keypoint_visibilities = (
+      fields.InputDataFields.groundtruth_keypoint_visibilities in tensor_dict)
   include_label_weights = (fields.InputDataFields.groundtruth_weights
                            in tensor_dict)
   include_label_confidences = (fields.InputDataFields.groundtruth_confidences
@@ -396,7 +525,8 @@ def augment_input_data(tensor_dict, data_augmentation_options):
           include_label_confidences=include_label_confidences,
           include_multiclass_scores=include_multiclass_scores,
           include_instance_masks=include_instance_masks,
-          include_keypoints=include_keypoints))
+          include_keypoints=include_keypoints,
+          include_keypoint_visibilities=include_keypoint_visibilities))
   tensor_dict[fields.InputDataFields.image] = tf.squeeze(
       tensor_dict[fields.InputDataFields.image], axis=0)
   return tensor_dict
@@ -416,11 +546,14 @@ def _get_labels_dict(input_dict):
 
   optional_label_keys = [
       fields.InputDataFields.groundtruth_confidences,
+      fields.InputDataFields.groundtruth_labeled_classes,
       fields.InputDataFields.groundtruth_keypoints,
       fields.InputDataFields.groundtruth_instance_masks,
       fields.InputDataFields.groundtruth_area,
       fields.InputDataFields.groundtruth_is_crowd,
-      fields.InputDataFields.groundtruth_difficult
+      fields.InputDataFields.groundtruth_difficult,
+      fields.InputDataFields.groundtruth_keypoint_visibilities,
+      fields.InputDataFields.groundtruth_keypoint_weights,
   ]
 
   for key in optional_label_keys:
@@ -461,7 +594,7 @@ def _replace_empty_string_with_random_number(string_tensor):
   return out_string
 
 
-def _get_features_dict(input_dict):
+def _get_features_dict(input_dict, include_source_id=False):
   """Extracts features dict from input dict."""
 
   source_id = _replace_empty_string_with_random_number(
@@ -477,12 +610,20 @@ def _get_features_dict(input_dict):
       fields.InputDataFields.original_image_spatial_shape:
           input_dict[fields.InputDataFields.original_image_spatial_shape]
   }
+  if include_source_id:
+    features[fields.InputDataFields.source_id] = source_id
   if fields.InputDataFields.original_image in input_dict:
     features[fields.InputDataFields.original_image] = input_dict[
         fields.InputDataFields.original_image]
   if fields.InputDataFields.image_additional_channels in input_dict:
     features[fields.InputDataFields.image_additional_channels] = input_dict[
         fields.InputDataFields.image_additional_channels]
+  if fields.InputDataFields.context_features in input_dict:
+    features[fields.InputDataFields.context_features] = input_dict[
+        fields.InputDataFields.context_features]
+  if fields.InputDataFields.valid_context_size in input_dict:
+    features[fields.InputDataFields.valid_context_size] = input_dict[
+        fields.InputDataFields.valid_context_size]
   return features
 
 
@@ -507,7 +648,7 @@ def create_train_input_fn(train_config, train_input_config,
 
 
 def train_input(train_config, train_input_config,
-                model_config, model=None, params=None):
+                model_config, model=None, params=None, input_context=None):
   """Returns `features` and `labels` tensor dictionaries for training.
 
   Args:
@@ -517,6 +658,9 @@ def train_input(train_config, train_input_config,
     model: A pre-constructed Detection Model.
       If None, one will be created from the config.
     params: Parameter dictionary passed from the estimator.
+    input_context: optional, A tf.distribute.InputContext object used to
+      shard filenames and compute per-replica batch_size when this function
+      is being called per-replica.
 
   Returns:
     A tf.data.Dataset that holds (features, labels) tuple.
@@ -550,6 +694,12 @@ def train_input(train_config, train_input_config,
       labels[fields.InputDataFields.groundtruth_keypoints] is a
         [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing
         keypoints for each box.
+      labels[fields.InputDataFields.groundtruth_weights] is a
+        [batch_size, num_boxes, num_keypoints] float32 tensor containing
+        groundtruth weights for the keypoints.
+      labels[fields.InputDataFields.groundtruth_visibilities] is a
+        [batch_size, num_boxes, num_keypoints] bool tensor containing
+        groundtruth visibilities for each keypoint.
 
   Raises:
     TypeError: if the `train_config`, `train_input_config` or `model_config`
@@ -571,6 +721,8 @@ def train_input(train_config, train_input_config,
   else:
     model_preprocess_fn = model.preprocess
 
+  num_classes = config_util.get_number_of_classes(model_config)
+
   def transform_and_pad_input_data_fn(tensor_dict):
     """Combines transform and pad operation."""
     data_augmentation_options = [
@@ -583,28 +735,37 @@ def train_input(train_config, train_input_config,
 
     image_resizer_config = config_util.get_image_resizer_config(model_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+    keypoint_type_weight = train_input_config.keypoint_type_weight or None
     transform_data_fn = functools.partial(
         transform_input_data, model_preprocess_fn=model_preprocess_fn,
         image_resizer_fn=image_resizer_fn,
-        num_classes=config_util.get_number_of_classes(model_config),
+        num_classes=num_classes,
         data_augmentation_fn=data_augmentation_fn,
         merge_multiple_boxes=train_config.merge_multiple_label_boxes,
         retain_original_image=train_config.retain_original_images,
         use_multiclass_scores=train_config.use_multiclass_scores,
-        use_bfloat16=train_config.use_bfloat16)
+        use_bfloat16=train_config.use_bfloat16,
+        keypoint_type_weight=keypoint_type_weight)
 
     tensor_dict = pad_input_data_to_static_shapes(
         tensor_dict=transform_data_fn(tensor_dict),
         max_num_boxes=train_input_config.max_number_of_boxes,
-        num_classes=config_util.get_number_of_classes(model_config),
+        num_classes=num_classes,
         spatial_image_shape=config_util.get_spatial_image_size(
-            image_resizer_config))
-    return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
+            image_resizer_config),
+        max_num_context_features=config_util.get_max_num_context_features(
+            model_config),
+        context_feature_length=config_util.get_context_feature_length(
+            model_config))
+    include_source_id = train_input_config.include_source_id
+    return (_get_features_dict(tensor_dict, include_source_id),
+            _get_labels_dict(tensor_dict))
 
   dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
       train_input_config,
       transform_input_data_fn=transform_and_pad_input_data_fn,
-      batch_size=params['batch_size'] if params else train_config.batch_size)
+      batch_size=params['batch_size'] if params else train_config.batch_size,
+      input_context=input_context)
   return dataset
 
 
@@ -667,6 +828,12 @@ def eval_input(eval_config, eval_input_config, model_config,
       labels[fields.InputDataFields.groundtruth_instance_masks] is a
         [1, num_boxes, H, W] float32 tensor containing only binary values,
         which represent instance masks for objects.
+      labels[fields.InputDataFields.groundtruth_weights] is a
+        [batch_size, num_boxes, num_keypoints] float32 tensor containing
+        groundtruth weights for the keypoints.
+      labels[fields.InputDataFields.groundtruth_visibilities] is a
+        [batch_size, num_boxes, num_keypoints] bool tensor containing
+        groundtruth visibilities for each keypoint.
 
   Raises:
     TypeError: if the `eval_config`, `eval_input_config` or `model_config`
@@ -703,6 +870,7 @@ def eval_input(eval_config, eval_input_config, model_config,
 
     image_resizer_config = config_util.get_image_resizer_config(model_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+    keypoint_type_weight = eval_input_config.keypoint_type_weight or None
 
     transform_data_fn = functools.partial(
         transform_input_data, model_preprocess_fn=model_preprocess_fn,
@@ -711,14 +879,21 @@ def eval_input(eval_config, eval_input_config, model_config,
         data_augmentation_fn=None,
         retain_original_image=eval_config.retain_original_images,
         retain_original_image_additional_channels=
-        eval_config.retain_original_image_additional_channels)
+        eval_config.retain_original_image_additional_channels,
+        keypoint_type_weight=keypoint_type_weight)
     tensor_dict = pad_input_data_to_static_shapes(
         tensor_dict=transform_data_fn(tensor_dict),
         max_num_boxes=eval_input_config.max_number_of_boxes,
         num_classes=config_util.get_number_of_classes(model_config),
         spatial_image_shape=config_util.get_spatial_image_size(
-            image_resizer_config))
-    return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
+            image_resizer_config),
+        max_num_context_features=config_util.get_max_num_context_features(
+            model_config),
+        context_feature_length=config_util.get_context_feature_length(
+            model_config))
+    include_source_id = eval_input_config.include_source_id
+    return (_get_features_dict(tensor_dict, include_source_id),
+            _get_labels_dict(tensor_dict))
   dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
       eval_input_config,
       batch_size=params['batch_size'] if params else eval_config.batch_size,
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index 3629bb28..a450fcbb 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 
 import functools
 import os
+from absl import logging
 from absl.testing import parameterized
 
 import numpy as np
@@ -484,16 +485,18 @@ class InputsTest(test_case.TestCase, parameterized.TestCase):
 
     empty_string = ''
     feed_dict = {string_placeholder: empty_string}
-
-    tf.set_random_seed(0)
-
     with self.test_session() as sess:
       out_string = sess.run(replaced_string, feed_dict=feed_dict)
 
-    # Test whether out_string is a string which represents an integer.
-    int(out_string)  # throws an error if out_string is not castable to int.
+    is_integer = True
+    try:
+      # Test whether out_string is a string which represents an integer, the
+      # casting below will throw an error if out_string is not castable to int.
+      int(out_string)
+    except ValueError:
+      is_integer = False
 
-    self.assertEqual(out_string, b'2798129067578209328')
+    self.assertTrue(is_integer)
 
   def test_force_no_resize(self):
     """Tests the functionality of force_no_reisze option."""
@@ -681,7 +684,7 @@ def _fake_resize50_preprocess_fn(image):
   return tf.expand_dims(image, 0), tf.expand_dims(shape, axis=0)
 
 
-class DataTransformationFnTest(test_case.TestCase):
+class DataTransformationFnTest(test_case.TestCase, parameterized.TestCase):
 
   def test_combine_additional_channels_if_present(self):
     image = np.random.rand(4, 4, 3).astype(np.float32)
@@ -766,6 +769,54 @@ class DataTransformationFnTest(test_case.TestCase):
         np.array([[0, 1, 0], [0, 0, 1]], np.float32),
         transformed_inputs[fields.InputDataFields.groundtruth_classes])
 
+  @parameterized.parameters(
+      {'labeled_classes': [1, 2]},
+      {'labeled_classes': []},
+      {'labeled_classes': [1, -1, 2]}  # -1 denotes an unrecognized class
+  )
+  def test_use_labeled_classes(self, labeled_classes):
+
+    def compute_fn(image, groundtruth_boxes, groundtruth_classes,
+                   groundtruth_labeled_classes):
+      tensor_dict = {
+          fields.InputDataFields.image:
+              image,
+          fields.InputDataFields.groundtruth_boxes:
+              groundtruth_boxes,
+          fields.InputDataFields.groundtruth_classes:
+              groundtruth_classes,
+          fields.InputDataFields.groundtruth_labeled_classes:
+              groundtruth_labeled_classes
+      }
+
+      input_transformation_fn = functools.partial(
+          inputs.transform_input_data,
+          model_preprocess_fn=_fake_model_preprocessor_fn,
+          image_resizer_fn=_fake_image_resizer_fn,
+          num_classes=3)
+      return input_transformation_fn(tensor_dict=tensor_dict)
+
+    image = np.random.rand(4, 4, 3).astype(np.float32)
+    groundtruth_boxes = np.array([[.5, .5, 1, 1], [.5, .5, 1, 1]], np.float32)
+    groundtruth_classes = np.array([1, 2], np.int32)
+    groundtruth_labeled_classes = np.array(labeled_classes, np.int32)
+
+    transformed_inputs = self.execute_cpu(compute_fn, [
+        image, groundtruth_boxes, groundtruth_classes,
+        groundtruth_labeled_classes
+    ])
+
+    if labeled_classes == [1, 2] or labeled_classes == [1, -1, 2]:
+      transformed_labeled_classes = [1, 1, 0]
+    elif not labeled_classes:
+      transformed_labeled_classes = [1, 1, 1]
+    else:
+      logging.exception('Unexpected labeled_classes %r', labeled_classes)
+
+    self.assertAllEqual(
+        np.array(transformed_labeled_classes, np.float32),
+        transformed_inputs[fields.InputDataFields.groundtruth_labeled_classes])
+
   def test_returns_correct_class_label_encodings(self):
     tensor_dict = {
         fields.InputDataFields.image:
@@ -809,7 +860,7 @@ class DataTransformationFnTest(test_case.TestCase):
                 np.array([[[.1, .1]], [[.2, .2]], [[.5, .5]]],
                          np.float32)),
         fields.InputDataFields.groundtruth_keypoint_visibilities:
-            tf.constant([True, False, True]),
+            tf.constant([[True, True], [False, False], [True, True]]),
         fields.InputDataFields.groundtruth_instance_masks:
             tf.constant(np.random.rand(3, 4, 4).astype(np.float32)),
         fields.InputDataFields.groundtruth_is_crowd:
@@ -847,7 +898,7 @@ class DataTransformationFnTest(test_case.TestCase):
     self.assertAllEqual(
         transformed_inputs[
             fields.InputDataFields.groundtruth_keypoint_visibilities],
-        [True, True])
+        [[True, True], [True, True]])
     self.assertAllEqual(
         transformed_inputs[
             fields.InputDataFields.groundtruth_instance_masks].shape, [2, 4, 4])
@@ -1060,7 +1111,7 @@ class DataTransformationFnTest(test_case.TestCase):
         fields.InputDataFields.groundtruth_classes:
             tf.constant(np.array([1, 2], np.int32)),
         fields.InputDataFields.groundtruth_keypoints:
-            tf.constant([[0.1, 0.2], [0.3, 0.4]]),
+            tf.constant([[[0.1, 0.2]], [[0.3, 0.4]]]),
     }
 
     num_classes = 3
@@ -1078,7 +1129,75 @@ class DataTransformationFnTest(test_case.TestCase):
         [[.5, .25, 1., .5], [.0, .0, .5, .25]])
     self.assertAllClose(
         transformed_inputs[fields.InputDataFields.groundtruth_keypoints],
-        [[[.1, .1], [.3, .2]]])
+        [[[.1, .1]], [[.3, .2]]])
+
+  def test_groundtruth_keypoint_weights(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(100, 50, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[.5, .5, 1, 1], [.0, .0, .5, .5]],
+                                 np.float32)),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([1, 2], np.int32)),
+        fields.InputDataFields.groundtruth_keypoints:
+            tf.constant([[[0.1, 0.2], [0.3, 0.4]],
+                         [[0.5, 0.6], [0.7, 0.8]]]),
+        fields.InputDataFields.groundtruth_keypoint_visibilities:
+            tf.constant([[True, False], [True, True]]),
+    }
+
+    num_classes = 3
+    keypoint_type_weight = [1.0, 2.0]
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_resize50_preprocess_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes,
+        keypoint_type_weight=keypoint_type_weight)
+
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_keypoints],
+        [[[0.1, 0.1], [0.3, 0.2]],
+         [[0.5, 0.3], [0.7, 0.4]]])
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_keypoint_weights],
+        [[1.0, 0.0], [1.0, 2.0]])
+
+  def test_groundtruth_keypoint_weights_default(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(100, 50, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[.5, .5, 1, 1], [.0, .0, .5, .5]],
+                                 np.float32)),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([1, 2], np.int32)),
+        fields.InputDataFields.groundtruth_keypoints:
+            tf.constant([[[0.1, 0.2], [0.3, 0.4]],
+                         [[0.5, 0.6], [0.7, 0.8]]]),
+    }
+
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_resize50_preprocess_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes)
+
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_keypoints],
+        [[[0.1, 0.1], [0.3, 0.2]],
+         [[0.5, 0.3], [0.7, 0.4]]])
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_keypoint_weights],
+        [[1.0, 1.0], [1.0, 1.0]])
 
 
 class PadInputDataToStaticShapesFnTest(test_case.TestCase):
@@ -1272,6 +1391,44 @@ class PadInputDataToStaticShapesFnTest(test_case.TestCase):
             fields.InputDataFields.groundtruth_keypoint_visibilities]
         .shape.as_list(), [3, 16])
 
+  def test_context_features(self):
+    context_memory_size = 8
+    context_feature_length = 10
+    max_num_context_features = 20
+    input_tensor_dict = {
+        fields.InputDataFields.context_features:
+            tf.placeholder(tf.float32,
+                           [context_memory_size, context_feature_length]),
+        fields.InputDataFields.context_feature_length:
+            tf.placeholder(tf.float32, [])
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[5, 6],
+        max_num_context_features=max_num_context_features,
+        context_feature_length=context_feature_length)
+
+    self.assertAllEqual(
+        padded_tensor_dict[
+            fields.InputDataFields.context_features].shape.as_list(),
+        [max_num_context_features, context_feature_length])
+
+    with self.test_session() as sess:
+      feed_dict = {
+          input_tensor_dict[fields.InputDataFields.context_features]:
+              np.ones([context_memory_size, context_feature_length],
+                      dtype=np.float32),
+          input_tensor_dict[fields.InputDataFields.context_feature_length]:
+              context_feature_length
+      }
+      padded_tensor_dict_out = sess.run(padded_tensor_dict, feed_dict=feed_dict)
+
+    self.assertEqual(
+        padded_tensor_dict_out[fields.InputDataFields.valid_context_size],
+        context_memory_size)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/matchers/bipartite_matcher_test.py b/research/object_detection/matchers/bipartite_matcher_test.py
index c0f693a4..9dabaaa9 100644
--- a/research/object_detection/matchers/bipartite_matcher_test.py
+++ b/research/object_detection/matchers/bipartite_matcher_test.py
@@ -15,66 +15,73 @@
 
 """Tests for object_detection.core.bipartite_matcher."""
 
+import numpy as np
 import tensorflow as tf
 
 from object_detection.matchers import bipartite_matcher
+from object_detection.utils import test_case
 
 
-class GreedyBipartiteMatcherTest(tf.test.TestCase):
+class GreedyBipartiteMatcherTest(test_case.TestCase):
 
   def test_get_expected_matches_when_all_rows_are_valid(self):
-    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])
-    valid_rows = tf.ones([2], dtype=tf.bool)
+    similarity_matrix = np.array([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]],
+                                 dtype=np.float32)
+    valid_rows = np.ones([2], dtype=np.bool)
     expected_match_results = [-1, 1, 0]
-
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    match = matcher.match(similarity_matrix, valid_rows=valid_rows)
-    with self.test_session() as sess:
-      match_results_out = sess.run(match._match_results)
-      self.assertAllEqual(match_results_out, expected_match_results)
+    def graph_fn(similarity_matrix, valid_rows):
+      matcher = bipartite_matcher.GreedyBipartiteMatcher()
+      match = matcher.match(similarity_matrix, valid_rows=valid_rows)
+      return match._match_results
+    match_results_out = self.execute(graph_fn, [similarity_matrix, valid_rows])
+    self.assertAllEqual(match_results_out, expected_match_results)
 
   def test_get_expected_matches_with_all_rows_be_default(self):
-    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])
+    similarity_matrix = np.array([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]],
+                                 dtype=np.float32)
     expected_match_results = [-1, 1, 0]
-
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    match = matcher.match(similarity_matrix)
-    with self.test_session() as sess:
-      match_results_out = sess.run(match._match_results)
-      self.assertAllEqual(match_results_out, expected_match_results)
+    def graph_fn(similarity_matrix):
+      matcher = bipartite_matcher.GreedyBipartiteMatcher()
+      match = matcher.match(similarity_matrix)
+      return match._match_results
+    match_results_out = self.execute(graph_fn, [similarity_matrix])
+    self.assertAllEqual(match_results_out, expected_match_results)
 
   def test_get_no_matches_with_zero_valid_rows(self):
-    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])
-    valid_rows = tf.zeros([2], dtype=tf.bool)
+    similarity_matrix = np.array([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]],
+                                 dtype=np.float32)
+    valid_rows = np.zeros([2], dtype=np.bool)
     expected_match_results = [-1, -1, -1]
-
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    match = matcher.match(similarity_matrix, valid_rows)
-    with self.test_session() as sess:
-      match_results_out = sess.run(match._match_results)
-      self.assertAllEqual(match_results_out, expected_match_results)
+    def graph_fn(similarity_matrix, valid_rows):
+      matcher = bipartite_matcher.GreedyBipartiteMatcher()
+      match = matcher.match(similarity_matrix, valid_rows=valid_rows)
+      return match._match_results
+    match_results_out = self.execute(graph_fn, [similarity_matrix, valid_rows])
+    self.assertAllEqual(match_results_out, expected_match_results)
 
   def test_get_expected_matches_with_only_one_valid_row(self):
-    similarity_matrix = tf.constant([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]])
-    valid_rows = tf.constant([True, False], dtype=tf.bool)
+    similarity_matrix = np.array([[0.50, 0.1, 0.8], [0.15, 0.2, 0.3]],
+                                 dtype=np.float32)
+    valid_rows = np.array([True, False], dtype=np.bool)
     expected_match_results = [-1, -1, 0]
-
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    match = matcher.match(similarity_matrix, valid_rows)
-    with self.test_session() as sess:
-      match_results_out = sess.run(match._match_results)
-      self.assertAllEqual(match_results_out, expected_match_results)
+    def graph_fn(similarity_matrix, valid_rows):
+      matcher = bipartite_matcher.GreedyBipartiteMatcher()
+      match = matcher.match(similarity_matrix, valid_rows=valid_rows)
+      return match._match_results
+    match_results_out = self.execute(graph_fn, [similarity_matrix, valid_rows])
+    self.assertAllEqual(match_results_out, expected_match_results)
 
   def test_get_expected_matches_with_only_one_valid_row_at_bottom(self):
-    similarity_matrix = tf.constant([[0.15, 0.2, 0.3], [0.50, 0.1, 0.8]])
-    valid_rows = tf.constant([False, True], dtype=tf.bool)
+    similarity_matrix = np.array([[0.15, 0.2, 0.3], [0.50, 0.1, 0.8]],
+                                 dtype=np.float32)
+    valid_rows = np.array([False, True], dtype=np.bool)
     expected_match_results = [-1, -1, 0]
-
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    match = matcher.match(similarity_matrix, valid_rows)
-    with self.test_session() as sess:
-      match_results_out = sess.run(match._match_results)
-      self.assertAllEqual(match_results_out, expected_match_results)
+    def graph_fn(similarity_matrix, valid_rows):
+      matcher = bipartite_matcher.GreedyBipartiteMatcher()
+      match = matcher.match(similarity_matrix, valid_rows=valid_rows)
+      return match._match_results
+    match_results_out = self.execute(graph_fn, [similarity_matrix, valid_rows])
+    self.assertAllEqual(match_results_out, expected_match_results)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 56d83a80..a4fb1580 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -92,11 +92,11 @@ configured in the meta architecture:
   non-max suppression and normalize them. In this case, the `postprocess` method
   skips both `_postprocess_rpn` and `_postprocess_box_classifier`.
 """
+
+from __future__ import print_function
 import abc
 import functools
 import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.builders import box_predictor_builder
@@ -112,7 +112,14 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from object_detection.utils import variables_helper
 
-slim = contrib_slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import framework as contrib_framework
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 _UNINITIALIZED_FEATURE_EXTRACTOR = '__uninitialized__'
 
@@ -141,7 +148,7 @@ class FasterRCNNFeatureExtractor(object):
     self._is_training = is_training
     self._first_stage_features_stride = first_stage_features_stride
     self._train_batch_norm = (batch_norm_trainable and is_training)
-    self._reuse_weights = reuse_weights
+    self._reuse_weights = tf.AUTO_REUSE if reuse_weights else None
     self._weight_decay = weight_decay
 
   @abc.abstractmethod
@@ -329,7 +336,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                use_static_shapes=False,
                resize_masks=True,
                freeze_batchnorm=False,
-               return_raw_detections_during_predict=False):
+               return_raw_detections_during_predict=False,
+               output_final_box_features=False):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -461,6 +469,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
       return_raw_detections_during_predict: Whether to return raw detection
         boxes in the predict() method. These are decoded boxes that have not
         been through postprocessing (i.e. NMS). Default False.
+      output_final_box_features: Whether to output final box features. If true,
+        it crops the feauture map based on the final box prediction and returns
+        in the dict as detection_features.
+
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
         training time.
@@ -554,13 +566,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
       self._first_stage_box_predictor_arg_scope_fn = (
           first_stage_box_predictor_arg_scope_fn)
       def rpn_box_predictor_feature_extractor(rpn_features_to_crop):
-        with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):
+        with contrib_slim.arg_scope(
+            self._first_stage_box_predictor_arg_scope_fn()):
           reuse = tf.get_variable_scope().reuse
-          return slim.conv2d(
+          return contrib_slim.conv2d(
               rpn_features_to_crop,
               self._first_stage_box_predictor_depth,
-              kernel_size=[self._first_stage_box_predictor_kernel_size,
-                           self._first_stage_box_predictor_kernel_size],
+              kernel_size=[
+                  self._first_stage_box_predictor_kernel_size,
+                  self._first_stage_box_predictor_kernel_size
+              ],
               rate=self._first_stage_atrous_rate,
               activation_fn=tf.nn.relu6,
               scope='Conv',
@@ -630,6 +645,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._batched_prediction_tensor_names = []
     self._return_raw_detections_during_predict = (
         return_raw_detections_during_predict)
+    self._output_final_box_features = output_final_box_features
 
   @property
   def first_stage_feature_extractor_scope(self):
@@ -678,6 +694,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
                          'tensor names.')
     return self._batched_prediction_tensor_names
 
+  @property
+  def feature_extractor(self):
+    return self._feature_extractor
+
   def preprocess(self, inputs):
     """Feature-extractor specific preprocessing.
 
@@ -746,7 +766,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
             anchors, image_shape_2d, true_image_shapes)
     return proposal_boxes_normalized, num_proposals
 
-  def predict(self, preprocessed_inputs, true_image_shapes):
+  def predict(self, preprocessed_inputs, true_image_shapes, **side_inputs):
     """Predicts unpostprocessed tensors from input tensor.
 
     This function takes an input batch of images and runs it through the
@@ -771,6 +791,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         of the form [height, width, channels] indicating the shapes
         of true images in the resized images, as resized images can be padded
         with zeros.
+      **side_inputs: additional tensors that are required by the network.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -841,9 +862,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
               prediction_dict['rpn_box_encodings'],
               prediction_dict['rpn_objectness_predictions_with_background'],
               prediction_dict['rpn_features_to_crop'],
-              prediction_dict['anchors'],
-              prediction_dict['image_shape'],
-              true_image_shapes))
+              prediction_dict['anchors'], prediction_dict['image_shape'],
+              true_image_shapes, **side_inputs))
 
     if self._number_of_stages == 3:
       prediction_dict = self._predict_third_stage(prediction_dict,
@@ -948,14 +968,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
   def _predict_second_stage(self, rpn_box_encodings,
                             rpn_objectness_predictions_with_background,
-                            rpn_features_to_crop,
-                            anchors,
-                            image_shape,
-                            true_image_shapes):
+                            rpn_features_to_crop, anchors, image_shape,
+                            true_image_shapes, **side_inputs):
     """Predicts the output tensors from second stage of Faster R-CNN.
 
     Args:
-      rpn_box_encodings: 4-D float tensor of shape
+      rpn_box_encodings: 3-D float tensor of shape
         [batch_size, num_valid_anchors, self._box_coder.code_size] containing
         predicted boxes.
       rpn_objectness_predictions_with_background: 2-D float tensor of shape
@@ -972,6 +990,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         of the form [height, width, channels] indicating the shapes
         of true images in the resized images, as resized images can be padded
         with zeros.
+      **side_inputs: additional tensors that are required by the network.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -1016,12 +1035,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
         image_shape, true_image_shapes)
     prediction_dict = self._box_prediction(rpn_features_to_crop,
                                            proposal_boxes_normalized,
-                                           image_shape, true_image_shapes)
+                                           image_shape, true_image_shapes,
+                                           **side_inputs)
     prediction_dict['num_proposals'] = num_proposals
     return prediction_dict
 
   def _box_prediction(self, rpn_features_to_crop, proposal_boxes_normalized,
-                      image_shape, true_image_shapes):
+                      image_shape, true_image_shapes, **side_inputs):
     """Predicts the output tensors from second stage of Faster R-CNN.
 
     Args:
@@ -1037,6 +1057,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         of the form [height, width, channels] indicating the shapes
         of true images in the resized images, as resized images can be padded
         with zeros.
+      **side_inputs: additional tensors that are required by the network.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -1076,7 +1097,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     flattened_proposal_feature_maps = (
         self._compute_second_stage_input_feature_maps(
-            rpn_features_to_crop, proposal_boxes_normalized))
+            rpn_features_to_crop, proposal_boxes_normalized, **side_inputs))
 
     box_classifier_features = self._extract_box_classifier_features(
         flattened_proposal_feature_maps)
@@ -1508,16 +1529,21 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     Raises:
       ValueError: If `predict` is called before `preprocess`.
+      ValueError: If `_output_final_box_features` is true but
+        rpn_features_to_crop is not in the prediction_dict.
     """
 
     with tf.name_scope('FirstStagePostprocessor'):
       if self._number_of_stages == 1:
+
+        image_shapes = self._image_batch_shape_2d(
+            prediction_dict['image_shape'])
         (proposal_boxes, proposal_scores, proposal_multiclass_scores,
          num_proposals, raw_proposal_boxes,
          raw_proposal_scores) = self._postprocess_rpn(
              prediction_dict['rpn_box_encodings'],
              prediction_dict['rpn_objectness_predictions_with_background'],
-             prediction_dict['anchors'], true_image_shapes, true_image_shapes)
+             prediction_dict['anchors'], image_shapes, true_image_shapes)
         return {
             fields.DetectionResultFields.detection_boxes:
                 proposal_boxes,
@@ -1546,7 +1572,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
             true_image_shapes,
             mask_predictions=mask_predictions)
 
-      if 'rpn_features_to_crop' in prediction_dict and self._initial_crop_size:
+      if self._output_final_box_features:
+        if 'rpn_features_to_crop' not in prediction_dict:
+          raise ValueError(
+              'Please make sure rpn_features_to_crop is in the prediction_dict.'
+          )
         detections_dict[
             'detection_features'] = self._add_detection_features_output_node(
                 detections_dict[fields.DetectionResultFields.detection_boxes],
@@ -1679,7 +1709,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     rpn_objectness_softmax = tf.nn.softmax(
         rpn_objectness_predictions_with_background_batch)
     rpn_objectness_softmax_without_background = rpn_objectness_softmax[:, :, 1]
-    clip_window = self._compute_clip_window(image_shapes)
+    clip_window = self._compute_clip_window(true_image_shapes)
     additional_fields = {'multiclass_scores': rpn_objectness_softmax}
     (proposal_boxes, proposal_scores, _, _, nmsed_additional_fields,
      num_proposals) = self._first_stage_nms_fn(
@@ -1692,7 +1722,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       if not self._hard_example_miner:
         (groundtruth_boxlists, groundtruth_classes_with_background_list, _,
          groundtruth_weights_list
-        ) = self._format_groundtruth_data(true_image_shapes)
+        ) = self._format_groundtruth_data(image_shapes)
         (proposal_boxes, proposal_scores,
          num_proposals) = self._sample_box_classifier_batch(
              proposal_boxes, proposal_scores, num_proposals,
@@ -1798,7 +1828,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
             tf.stack(single_image_proposal_score_sample),
             tf.stack(single_image_num_proposals_sample))
 
-  def _format_groundtruth_data(self, true_image_shapes):
+  def _format_groundtruth_data(self, image_shapes):
     """Helper function for preparing groundtruth data for target assignment.
 
     In order to be consistent with the model.DetectionModel interface,
@@ -1811,10 +1841,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
        image_shape.
 
     Args:
-      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
-        of the form [height, width, channels] indicating the shapes
-        of true images in the resized images, as resized images can be padded
-        with zeros.
+      image_shapes: a 2-D int32 tensor of shape [batch_size, 3] containing
+        shapes of input image in the batch.
 
     Returns:
       groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates
@@ -1826,10 +1854,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         shape [num_boxes, image_height, image_width] containing instance masks.
         This is set to None if no masks exist in the provided groundtruth.
     """
+    # pylint: disable=g-complex-comprehension
     groundtruth_boxlists = [
         box_list_ops.to_absolute_coordinates(
-            box_list.BoxList(boxes), true_image_shapes[i, 0],
-            true_image_shapes[i, 1])
+            box_list.BoxList(boxes), image_shapes[i, 0], image_shapes[i, 1])
         for i, boxes in enumerate(
             self.groundtruth_lists(fields.BoxListFields.boxes))
     ]
@@ -1934,7 +1962,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                        if self._use_static_shapes else None))
 
   def _compute_second_stage_input_feature_maps(self, features_to_crop,
-                                               proposal_boxes_normalized):
+                                               proposal_boxes_normalized,
+                                               **side_inputs):
     """Crops to a set of proposals from the feature map for a batch of images.
 
     Helper function for self._postprocess_rpn. This function calls
@@ -1947,6 +1976,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       proposal_boxes_normalized: A float32 tensor with shape [batch_size,
         num_proposals, box_code_size] containing proposal boxes in
         normalized coordinates.
+      **side_inputs: additional tensors that are required by the network.
 
     Returns:
       A float32 tensor with shape [K, new_height, new_width, depth].
@@ -2189,7 +2219,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
     with tf.name_scope(scope, 'Loss', prediction_dict.values()):
       (groundtruth_boxlists, groundtruth_classes_with_background_list,
        groundtruth_masks_list, groundtruth_weights_list
-      ) = self._format_groundtruth_data(true_image_shapes)
+      ) = self._format_groundtruth_data(
+          self._image_batch_shape_2d(prediction_dict['image_shape']))
       loss_dict = self._loss_rpn(
           prediction_dict['rpn_box_encodings'],
           prediction_dict['rpn_objectness_predictions_with_background'],
@@ -2222,7 +2253,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     participate in the loss computation, and returns the RPN losses.
 
     Args:
-      rpn_box_encodings: A 4-D float tensor of shape
+      rpn_box_encodings: A 3-D float tensor of shape
         [batch_size, num_anchors, self._box_coder.code_size] containing
         predicted proposal box encodings.
       rpn_objectness_predictions_with_background: A 2-D float tensor of shape
@@ -2765,7 +2796,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           self.second_stage_feature_extractor_scope)
 
     variables_to_restore = variables_helper.get_global_variables_safely()
-    variables_to_restore.append(slim.get_or_create_global_step())
+    variables_to_restore.append(tf.train.get_or_create_global_step())
     # Only load feature extractor variables to be consistent with loading from
     # a classification checkpoint.
     include_patterns = None
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index fef51502..c061b1a7 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,14 @@
 
 """Tests for object_detection.meta_architectures.faster_rcnn_meta_arch."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from absl.testing import parameterized
 import numpy as np
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch_test_lib
@@ -488,8 +495,8 @@ class FasterRCNNMetaArchTest(
     batch_size = 2
     initial_crop_size = 3
     maxpool_stride = 1
-    height = initial_crop_size/maxpool_stride
-    width = initial_crop_size/maxpool_stride
+    height = initial_crop_size // maxpool_stride
+    width = initial_crop_size // maxpool_stride
     depth = 3
     image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
     for (num_proposals_shape, refined_box_encoding_shape,
@@ -574,9 +581,102 @@ class FasterRCNNMetaArchTest(
                                          maxpool_stride,
                                          num_features):
     return (batch_size * max_num_proposals,
-            initial_crop_size/maxpool_stride,
-            initial_crop_size/maxpool_stride,
+            initial_crop_size // maxpool_stride,
+            initial_crop_size // maxpool_stride,
             num_features)
 
+  @parameterized.parameters({'use_keras': True}, {'use_keras': False})
+  def test_output_final_box_features(self, use_keras):
+    model = self._build_model(
+        is_training=False,
+        use_keras=use_keras,
+        number_of_stages=2,
+        second_stage_batch_size=6,
+        output_final_box_features=True)
+
+    batch_size = 2
+    total_num_padded_proposals = batch_size * model.max_num_proposals
+    proposal_boxes = tf.constant([[[1, 1, 2, 3], [0, 0, 1, 1], [.5, .5, .6, .6],
+                                   4 * [0], 4 * [0], 4 * [0], 4 * [0], 4 * [0]],
+                                  [[2, 3, 6, 8], [1, 2, 5, 3], 4 * [0], 4 * [0],
+                                   4 * [0], 4 * [0], 4 * [0], 4 * [0]]],
+                                 dtype=tf.float32)
+    num_proposals = tf.constant([3, 2], dtype=tf.int32)
+    refined_box_encodings = tf.zeros(
+        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
+    class_predictions_with_background = tf.ones(
+        [total_num_padded_proposals, model.num_classes + 1], dtype=tf.float32)
+    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+    mask_height = 2
+    mask_width = 2
+    mask_predictions = 30. * tf.ones([
+        total_num_padded_proposals, model.num_classes, mask_height, mask_width
+    ],
+                                     dtype=tf.float32)
+    exp_detection_masks = np.array([[[[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]]],
+                                    [[[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[0, 0], [0, 0]]]])
+
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+
+    # It should fail due to no rpn_features_to_crop in the input dict.
+    with self.assertRaises(ValueError):
+      detections = model.postprocess(
+          {
+              'refined_box_encodings':
+                  refined_box_encodings,
+              'class_predictions_with_background':
+                  class_predictions_with_background,
+              'num_proposals':
+                  num_proposals,
+              'proposal_boxes':
+                  proposal_boxes,
+              'image_shape':
+                  image_shape,
+              'mask_predictions':
+                  mask_predictions
+          }, true_image_shapes)
+
+    rpn_features_to_crop = tf.ones((batch_size, mask_height, mask_width, 3),
+                                   tf.float32)
+    detections = model.postprocess(
+        {
+            'refined_box_encodings':
+                refined_box_encodings,
+            'class_predictions_with_background':
+                class_predictions_with_background,
+            'num_proposals':
+                num_proposals,
+            'proposal_boxes':
+                proposal_boxes,
+            'image_shape':
+                image_shape,
+            'mask_predictions':
+                mask_predictions,
+            'rpn_features_to_crop':
+                rpn_features_to_crop
+        }, true_image_shapes)
+
+    with self.test_session() as sess:
+      init_op = tf.global_variables_initializer()
+      sess.run(init_op)
+      detections_out = sess.run(detections)
+      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
+      self.assertAllClose(detections_out['detection_scores'],
+                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
+      self.assertAllClose(detections_out['detection_classes'],
+                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+      self.assertAllClose(detections_out['num_detections'], [5, 4])
+      self.assertAllClose(detections_out['detection_masks'],
+                          exp_detection_masks)
+      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
+      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
+      self.assertIn('detection_features', detections_out)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index e067e028..99297262 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -18,10 +18,10 @@ import functools
 from absl.testing import parameterized
 
 import numpy as np
+import six
 import tensorflow as tf
 
 from google.protobuf import text_format
-from tensorflow.contrib import slim as contrib_slim
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.builders import box_predictor_builder
 from object_detection.builders import hyperparams_builder
@@ -38,7 +38,14 @@ from object_detection.utils import ops
 from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
-slim = contrib_slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 BOX_CODE_SIZE = 4
 
 
@@ -58,14 +65,14 @@ class FakeFasterRCNNFeatureExtractor(
 
   def _extract_proposal_features(self, preprocessed_inputs, scope):
     with tf.variable_scope('mock_model'):
-      proposal_features = 0 * slim.conv2d(
+      proposal_features = 0 * contrib_slim.conv2d(
           preprocessed_inputs, num_outputs=3, kernel_size=1, scope='layer1')
       return proposal_features, {}
 
   def _extract_box_classifier_features(self, proposal_feature_maps, scope):
     with tf.variable_scope('mock_model'):
-      return 0 * slim.conv2d(proposal_feature_maps,
-                             num_outputs=3, kernel_size=1, scope='layer2')
+      return 0 * contrib_slim.conv2d(
+          proposal_feature_maps, num_outputs=3, kernel_size=1, scope='layer2')
 
 
 class FakeFasterRCNNKerasFeatureExtractor(
@@ -226,7 +233,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    use_static_shapes=False,
                    calibration_mapping_value=None,
                    share_box_across_classes=False,
-                   return_raw_detections_during_predict=False):
+                   return_raw_detections_during_predict=False,
+                   output_final_box_features=False):
 
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
@@ -372,47 +380,70 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         ops.matmul_crop_and_resize
         if use_matmul_crop_and_resize else ops.native_crop_and_resize)
     common_kwargs = {
-        'is_training': is_training,
-        'num_classes': num_classes,
-        'image_resizer_fn': image_resizer_fn,
-        'feature_extractor': fake_feature_extractor,
-        'number_of_stages': number_of_stages,
-        'first_stage_anchor_generator': first_stage_anchor_generator,
-        'first_stage_target_assigner': first_stage_target_assigner,
-        'first_stage_atrous_rate': first_stage_atrous_rate,
+        'is_training':
+            is_training,
+        'num_classes':
+            num_classes,
+        'image_resizer_fn':
+            image_resizer_fn,
+        'feature_extractor':
+            fake_feature_extractor,
+        'number_of_stages':
+            number_of_stages,
+        'first_stage_anchor_generator':
+            first_stage_anchor_generator,
+        'first_stage_target_assigner':
+            first_stage_target_assigner,
+        'first_stage_atrous_rate':
+            first_stage_atrous_rate,
         'first_stage_box_predictor_arg_scope_fn':
-        first_stage_box_predictor_arg_scope_fn,
+            first_stage_box_predictor_arg_scope_fn,
         'first_stage_box_predictor_kernel_size':
-        first_stage_box_predictor_kernel_size,
-        'first_stage_box_predictor_depth': first_stage_box_predictor_depth,
-        'first_stage_minibatch_size': first_stage_minibatch_size,
-        'first_stage_sampler': first_stage_sampler,
+            first_stage_box_predictor_kernel_size,
+        'first_stage_box_predictor_depth':
+            first_stage_box_predictor_depth,
+        'first_stage_minibatch_size':
+            first_stage_minibatch_size,
+        'first_stage_sampler':
+            first_stage_sampler,
         'first_stage_non_max_suppression_fn':
-        first_stage_non_max_suppression_fn,
-        'first_stage_max_proposals': first_stage_max_proposals,
+            first_stage_non_max_suppression_fn,
+        'first_stage_max_proposals':
+            first_stage_max_proposals,
         'first_stage_localization_loss_weight':
-        first_stage_localization_loss_weight,
+            first_stage_localization_loss_weight,
         'first_stage_objectness_loss_weight':
-        first_stage_objectness_loss_weight,
-        'second_stage_target_assigner': second_stage_target_assigner,
-        'second_stage_batch_size': second_stage_batch_size,
-        'second_stage_sampler': second_stage_sampler,
+            first_stage_objectness_loss_weight,
+        'second_stage_target_assigner':
+            second_stage_target_assigner,
+        'second_stage_batch_size':
+            second_stage_batch_size,
+        'second_stage_sampler':
+            second_stage_sampler,
         'second_stage_non_max_suppression_fn':
-        second_stage_non_max_suppression_fn,
-        'second_stage_score_conversion_fn': second_stage_score_conversion_fn,
+            second_stage_non_max_suppression_fn,
+        'second_stage_score_conversion_fn':
+            second_stage_score_conversion_fn,
         'second_stage_localization_loss_weight':
-        second_stage_localization_loss_weight,
+            second_stage_localization_loss_weight,
         'second_stage_classification_loss_weight':
-        second_stage_classification_loss_weight,
+            second_stage_classification_loss_weight,
         'second_stage_classification_loss':
-        second_stage_classification_loss,
-        'hard_example_miner': hard_example_miner,
-        'crop_and_resize_fn': crop_and_resize_fn,
-        'clip_anchors_to_image': clip_anchors_to_image,
-        'use_static_shapes': use_static_shapes,
-        'resize_masks': True,
+            second_stage_classification_loss,
+        'hard_example_miner':
+            hard_example_miner,
+        'crop_and_resize_fn':
+            crop_and_resize_fn,
+        'clip_anchors_to_image':
+            clip_anchors_to_image,
+        'use_static_shapes':
+            use_static_shapes,
+        'resize_masks':
+            True,
         'return_raw_detections_during_predict':
-            return_raw_detections_during_predict
+            return_raw_detections_during_predict,
+        'output_final_box_features':
+            output_final_box_features
     }
 
     return self._get_model(
@@ -866,12 +897,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           use_matmul_gather_in_matcher=use_static_shapes,
           first_stage_max_proposals=first_stage_max_proposals,
           pad_to_max_dimension=pad_to_max_dimension)
-      _, true_image_shapes = model.preprocess(images)
+      preprocessed_images, true_image_shapes = model.preprocess(images)
       proposals = model.postprocess({
           'rpn_box_encodings': rpn_box_encodings,
           'rpn_objectness_predictions_with_background':
           rpn_objectness_predictions_with_background,
           'rpn_features_to_crop': rpn_features_to_crop,
+          'image_shape': tf.shape(preprocessed_images),
           'anchors': anchors}, true_image_shapes)
       return (proposals['num_detections'], proposals['detection_boxes'],
               proposals['detection_scores'], proposals['raw_detection_boxes'],
@@ -925,6 +957,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_raw_scores = [[[0., 1.], [1., 0.], [1., 0.], [0., 1.]],
                            [[1., 0.], [0., 1.], [0., 1.], [1., 0.]]]
 
+    if pad_to_max_dimension is not None:
+      expected_raw_proposal_boxes = (np.array(expected_raw_proposal_boxes) *
+                                     32 / pad_to_max_dimension)
+      expected_proposal_boxes = (np.array(expected_proposal_boxes) *
+                                 32 / pad_to_max_dimension)
+
     self.assertAllClose(results[0], expected_num_proposals)
     for indx, num_proposals in enumerate(expected_num_proposals):
       self.assertAllClose(results[1][indx][0:num_proposals],
@@ -982,7 +1020,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         'rpn_objectness_predictions_with_background':
         rpn_objectness_predictions_with_background,
         'rpn_features_to_crop': rpn_features_to_crop,
-        'anchors': anchors}, true_image_shapes)
+        'anchors': anchors,
+        'image_shape': image_shape}, true_image_shapes)
     expected_proposal_boxes = [
         [[0, 0, .5, .5], [.5, .5, 1, 1]], [[0, .5, .5, 1], [.5, 0, 1, .5]]]
     expected_proposal_scores = [[1, 1],
@@ -1933,8 +1972,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     with test_graph_classification.as_default():
       image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
       with tf.variable_scope('mock_model'):
-        net = slim.conv2d(image, num_outputs=3, kernel_size=1, scope='layer1')
-        slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
+        net = contrib_slim.conv2d(
+            image, num_outputs=3, kernel_size=1, scope='layer1')
+        contrib_slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
@@ -2012,10 +2052,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       with self.test_session(graph=test_graph_detection2) as sess:
         saver.restore(sess, saved_model_path)
         uninitialized_vars_list = sess.run(tf.report_uninitialized_variables())
-        self.assertIn('another_variable', uninitialized_vars_list)
+        self.assertIn(six.b('another_variable'), uninitialized_vars_list)
         for var in uninitialized_vars_list:
-          self.assertNotIn(model2.first_stage_feature_extractor_scope, var)
-          self.assertNotIn(model2.second_stage_feature_extractor_scope, var)
+          self.assertNotIn(
+              six.b(model2.first_stage_feature_extractor_scope), var)
+          self.assertNotIn(
+              six.b(model2.second_stage_feature_extractor_scope), var)
 
   @parameterized.parameters(
       {'use_keras': True},
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 850140f0..bd692cd4 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -83,7 +83,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                use_static_shapes=False,
                resize_masks=False,
                freeze_batchnorm=False,
-               return_raw_detections_during_predict=False):
+               return_raw_detections_during_predict=False,
+               output_final_box_features=False):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -192,6 +193,9 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       return_raw_detections_during_predict: Whether to return raw detection
         boxes in the predict() method. These are decoded boxes that have not
         been through postprocessing (i.e. NMS). Default False.
+      output_final_box_features: Whether to output final box features. If true,
+        it crops the feauture map based on the final box prediction and returns
+        in the dict as detection_features.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
@@ -240,7 +244,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         resize_masks,
         freeze_batchnorm=freeze_batchnorm,
         return_raw_detections_during_predict=(
-            return_raw_detections_during_predict))
+            return_raw_detections_during_predict),
+        output_final_box_features=output_final_box_features)
 
     self._rfcn_box_predictor = second_stage_rfcn_box_predictor
 
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 4f2678ab..4b518135 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -19,9 +19,7 @@ models.
 """
 import abc
 import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-from tensorflow.contrib import tpu as contrib_tpu
-
+from tensorflow.python.util.deprecation import deprecated_args
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import matcher
@@ -33,7 +31,14 @@ from object_detection.utils import shape_utils
 from object_detection.utils import variables_helper
 from object_detection.utils import visualization_utils
 
-slim = contrib_slim
+
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 class SSDFeatureExtractor(object):
@@ -278,6 +283,9 @@ class SSDKerasFeatureExtractor(tf.keras.Model):
 class SSDMetaArch(model.DetectionModel):
   """SSD Meta-architecture definition."""
 
+  @deprecated_args(None,
+                   'NMS is always placed on TPU; do not use nms_on_host '
+                   'as it has no effect.', 'nms_on_host')
   def __init__(self,
                is_training,
                anchor_generator,
@@ -457,7 +465,10 @@ class SSDMetaArch(model.DetectionModel):
 
     self._return_raw_detections_during_predict = (
         return_raw_detections_during_predict)
-    self._nms_on_host = nms_on_host
+
+  @property
+  def feature_extractor(self):
+    return self._feature_extractor
 
   @property
   def anchors(self):
@@ -590,10 +601,10 @@ class SSDMetaArch(model.DetectionModel):
     if self._feature_extractor.is_keras_model:
       feature_maps = self._feature_extractor(preprocessed_inputs)
     else:
-      with slim.arg_scope([slim.batch_norm],
-                          is_training=(self._is_training and
-                                       not self._freeze_batchnorm),
-                          updates_collections=batchnorm_updates_collections):
+      with contrib_slim.arg_scope(
+          [contrib_slim.batch_norm],
+          is_training=(self._is_training and not self._freeze_batchnorm),
+          updates_collections=batchnorm_updates_collections):
         with tf.variable_scope(None, self._extract_features_scope,
                                [preprocessed_inputs]):
           feature_maps = self._feature_extractor.extract_features(
@@ -611,10 +622,10 @@ class SSDMetaArch(model.DetectionModel):
     if self._box_predictor.is_keras_model:
       predictor_results_dict = self._box_predictor(feature_maps)
     else:
-      with slim.arg_scope([slim.batch_norm],
-                          is_training=(self._is_training and
-                                       not self._freeze_batchnorm),
-                          updates_collections=batchnorm_updates_collections):
+      with contrib_slim.arg_scope(
+          [contrib_slim.batch_norm],
+          is_training=(self._is_training and not self._freeze_batchnorm),
+          updates_collections=batchnorm_updates_collections):
         predictor_results_dict = self._box_predictor.predict(
             feature_maps, self._anchor_generator.num_anchors_per_location())
     predictions_dict = {
@@ -780,36 +791,16 @@ class SSDMetaArch(model.DetectionModel):
             detection_keypoints, 'raw_keypoint_locations')
         additional_fields[fields.BoxListFields.keypoints] = detection_keypoints
 
-      with tf.init_scope():
-        if tf.executing_eagerly():
-          # soft device placement in eager mode will automatically handle
-          # outside compilation.
-          def _non_max_suppression_wrapper(kwargs):
-            return self._non_max_suppression_fn(**kwargs)
-        else:
-          def _non_max_suppression_wrapper(kwargs):
-            if self._nms_on_host:
-              # Note: NMS is not memory efficient on TPU. This force the NMS
-              # to run outside of TPU.
-              return contrib_tpu.outside_compilation(
-                  lambda x: self._non_max_suppression_fn(**x), kwargs)
-            else:
-              return self._non_max_suppression_fn(**kwargs)
-
       (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
        nmsed_additional_fields,
-       num_detections) = _non_max_suppression_wrapper({
-           'boxes':
+       num_detections) = self._non_max_suppression_fn(
            detection_boxes,
-           'scores':
            detection_scores,
-           'clip_window':
-           self._compute_clip_window(preprocessed_images, true_image_shapes),
-           'additional_fields':
-           additional_fields,
-           'masks':
-           prediction_dict.get('mask_predictions')
-       })
+           clip_window=self._compute_clip_window(
+               preprocessed_images, true_image_shapes),
+           additional_fields=additional_fields,
+           masks=prediction_dict.get('mask_predictions'))
+
       detection_dict = {
           fields.DetectionResultFields.detection_boxes:
               nmsed_boxes,
@@ -817,9 +808,6 @@ class SSDMetaArch(model.DetectionModel):
               nmsed_scores,
           fields.DetectionResultFields.detection_classes:
               nmsed_classes,
-          fields.DetectionResultFields.detection_multiclass_scores:
-              nmsed_additional_fields.get(
-                  'multiclass_scores') if nmsed_additional_fields else None,
           fields.DetectionResultFields.num_detections:
               tf.cast(num_detections, dtype=tf.float32),
           fields.DetectionResultFields.raw_detection_boxes:
@@ -827,6 +815,12 @@ class SSDMetaArch(model.DetectionModel):
           fields.DetectionResultFields.raw_detection_scores:
               detection_scores_with_background
       }
+      if (nmsed_additional_fields is not None and
+          fields.InputDataFields.multiclass_scores in nmsed_additional_fields):
+        detection_dict[
+            fields.DetectionResultFields.detection_multiclass_scores] = (
+                nmsed_additional_fields[
+                    fields.InputDataFields.multiclass_scores])
       if (nmsed_additional_fields is not None and
           'anchor_indices' in nmsed_additional_fields):
         detection_dict.update({
@@ -907,6 +901,8 @@ class SSDMetaArch(model.DetectionModel):
       if self.groundtruth_has_field(fields.InputDataFields.is_annotated):
         losses_mask = tf.stack(self.groundtruth_lists(
             fields.InputDataFields.is_annotated))
+
+
       location_losses = self._localization_loss(
           prediction_dict['box_encodings'],
           batch_reg_targets,
@@ -1068,10 +1064,14 @@ class SSDMetaArch(model.DetectionModel):
       batch_reg_targets: a tensor with shape [batch_size, num_anchors,
         box_code_dimension]
       batch_reg_weights: a tensor with shape [batch_size, num_anchors],
-      match_list: a list of matcher.Match objects encoding the match between
-        anchors and groundtruth boxes for each image of the batch,
-        with rows of the Match objects corresponding to groundtruth boxes
-        and columns corresponding to anchors.
+      match: an int32 tensor of shape [batch_size, num_anchors], containing
+        result of anchor groundtruth matching. Each position in the tensor
+        indicates an anchor and holds the following meaning:
+        (1) if match[x, i] >= 0, anchor i is matched with groundtruth
+            match[x, i].
+        (2) if match[x, i]=-1, anchor i is marked to be background .
+        (3) if match[x, i]=-2, anchor i is ignored since it is not background
+            and does not have sufficient overlap to call it a foreground.
     """
     groundtruth_boxlists = [
         box_list.BoxList(boxes) for boxes in groundtruth_boxes_list
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 7052ba8b..8eef1ee7 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,18 +16,30 @@
 
 """Tests for object_detection.meta_architectures.ssd_meta_arch."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from absl.testing import parameterized
 
 import numpy as np
+import six
+from six.moves import range
 import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch_test_lib
 from object_detection.protos import model_pb2
 from object_detection.utils import test_utils
 
-slim = contrib_slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 keras = tf.keras.layers
 
 
@@ -681,9 +694,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
           layer_two(net)
       else:
         with tf.variable_scope('mock_model'):
-          net = slim.conv2d(image, num_outputs=32, kernel_size=1,
-                            scope='layer1')
-          slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
+          net = contrib_slim.conv2d(
+              image, num_outputs=32, kernel_size=1, scope='layer1')
+          contrib_slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
@@ -711,7 +724,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       with self.test_session(graph=test_graph_detection) as sess:
         saver.restore(sess, saved_model_path)
         for var in sess.run(tf.report_uninitialized_variables()):
-          self.assertNotIn('FeatureExtractor', var)
+          self.assertNotIn(six.ensure_binary('FeatureExtractor'), var)
 
   def test_load_all_det_checkpoint_vars(self, use_keras):
     test_graph_detection = tf.Graph()
@@ -776,5 +789,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
index 9cb0a8af..1e9b1e84 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
@@ -17,7 +17,6 @@
 import functools
 import tensorflow as tf
 from google.protobuf import text_format
-from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.builders import post_processing_builder
 from object_detection.core import anchor_generator
@@ -34,7 +33,14 @@ from object_detection.utils import ops
 from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
-slim = contrib_slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import slim as contrib_slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 keras = tf.keras.layers
 
 
@@ -54,7 +60,7 @@ class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
 
   def extract_features(self, preprocessed_inputs):
     with tf.variable_scope('mock_model'):
-      features = slim.conv2d(
+      features = contrib_slim.conv2d(
           inputs=preprocessed_inputs,
           num_outputs=32,
           kernel_size=1,
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index 3749b270..bf2f0b1f 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -82,17 +82,39 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
           [num_boxes] containing 1-indexed groundtruth classes for the boxes.
         InputDataFields.groundtruth_is_crowd (optional): integer numpy array of
           shape [num_boxes] containing iscrowd flag for groundtruth boxes.
+        InputDataFields.groundtruth_area (optional): float numpy array of
+          shape [num_boxes] containing the area (in the original absolute
+          coordinates) of the annotated object.
+        InputDataFields.groundtruth_keypoints (optional): float numpy array of
+          keypoints with shape [num_boxes, num_keypoints, 2].
+        InputDataFields.groundtruth_keypoint_visibilities (optional): integer
+          numpy array of keypoint visibilities with shape [num_gt_boxes,
+          num_keypoints]. Integer is treated as an enum with 0=not labeled,
+          1=labeled but not visible and 2=labeled and visible.
     """
     if image_id in self._image_ids:
       tf.logging.warning('Ignoring ground truth with image id %s since it was '
                          'previously added', image_id)
       return
 
+    # Drop optional fields if empty tensor.
     groundtruth_is_crowd = groundtruth_dict.get(
         standard_fields.InputDataFields.groundtruth_is_crowd)
-    # Drop groundtruth_is_crowd if empty tensor.
+    groundtruth_area = groundtruth_dict.get(
+        standard_fields.InputDataFields.groundtruth_area)
+    groundtruth_keypoints = groundtruth_dict.get(
+        standard_fields.InputDataFields.groundtruth_keypoints)
+    groundtruth_keypoint_visibilities = groundtruth_dict.get(
+        standard_fields.InputDataFields.groundtruth_keypoint_visibilities)
     if groundtruth_is_crowd is not None and not groundtruth_is_crowd.shape[0]:
       groundtruth_is_crowd = None
+    if groundtruth_area is not None and not groundtruth_area.shape[0]:
+      groundtruth_area = None
+    if groundtruth_keypoints is not None and not groundtruth_keypoints.shape[0]:
+      groundtruth_keypoints = None
+    if groundtruth_keypoint_visibilities is not None and not groundtruth_keypoint_visibilities.shape[
+        0]:
+      groundtruth_keypoint_visibilities = None
 
     self._groundtruth_list.extend(
         coco_tools.ExportSingleImageGroundtruthToCoco(
@@ -103,7 +125,12 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
                 standard_fields.InputDataFields.groundtruth_boxes],
             groundtruth_classes=groundtruth_dict[
                 standard_fields.InputDataFields.groundtruth_classes],
-            groundtruth_is_crowd=groundtruth_is_crowd))
+            groundtruth_is_crowd=groundtruth_is_crowd,
+            groundtruth_area=groundtruth_area,
+            groundtruth_keypoints=groundtruth_keypoints,
+            groundtruth_keypoint_visibilities=groundtruth_keypoint_visibilities)
+    )
+
     self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.
                                             groundtruth_boxes].shape[0]
     # Boolean to indicate whether a detection has been added for this image.
@@ -127,7 +154,8 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
           [num_boxes] containing detection scores for the boxes.
         DetectionResultFields.detection_classes: integer numpy array of shape
           [num_boxes] containing 1-indexed detection classes for the boxes.
-
+        DetectionResultFields.detection_keypoints (optional): float numpy array
+          of keypoints with shape [num_boxes, num_keypoints, 2].
     Raises:
       ValueError: If groundtruth for the image_id is not available.
     """
@@ -139,19 +167,22 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
                          'previously added', image_id)
       return
 
+    # Drop optional fields if empty tensor.
+    detection_keypoints = detections_dict.get(
+        standard_fields.DetectionResultFields.detection_keypoints)
+    if detection_keypoints is not None and not detection_keypoints.shape[0]:
+      detection_keypoints = None
     self._detection_boxes_list.extend(
         coco_tools.ExportSingleImageDetectionBoxesToCoco(
             image_id=image_id,
             category_id_set=self._category_id_set,
-            detection_boxes=detections_dict[standard_fields.
-                                            DetectionResultFields
-                                            .detection_boxes],
-            detection_scores=detections_dict[standard_fields.
-                                             DetectionResultFields.
-                                             detection_scores],
-            detection_classes=detections_dict[standard_fields.
-                                              DetectionResultFields.
-                                              detection_classes]))
+            detection_boxes=detections_dict[
+                standard_fields.DetectionResultFields.detection_boxes],
+            detection_scores=detections_dict[
+                standard_fields.DetectionResultFields.detection_scores],
+            detection_classes=detections_dict[
+                standard_fields.DetectionResultFields.detection_classes],
+            detection_keypoints=detection_keypoints))
     self._image_ids[image_id] = True
 
   def dump_detections_to_json_file(self, json_output_path):
@@ -410,6 +441,460 @@ def _check_mask_type_and_value(array_name, masks):
         array_name))
 
 
+class CocoKeypointEvaluator(CocoDetectionEvaluator):
+  """Class to evaluate COCO keypoint metrics."""
+
+  def __init__(self,
+               category_id,
+               category_keypoints,
+               class_text,
+               oks_sigmas=None):
+    """Constructor.
+
+    Args:
+      category_id: An integer id uniquely identifying this category.
+      category_keypoints: A list specifying keypoint mappings, with items:
+          'id': (required) an integer id identifying the keypoint.
+          'name': (required) a string representing the keypoint name.
+      class_text: A string representing the category name for which keypoint
+        metrics are to be computed.
+      oks_sigmas: A dict of keypoint name to standard deviation values for OKS
+        metrics. If not provided, default value of 0.05 will be used.
+    """
+    self._category_id = category_id
+    self._category_name = class_text
+    self._keypoint_ids = sorted(
+        [keypoint['id'] for keypoint in category_keypoints])
+    kpt_id_to_name = {kpt['id']: kpt['name'] for kpt in category_keypoints}
+    if oks_sigmas:
+      self._oks_sigmas = np.array([
+          oks_sigmas[kpt_id_to_name[idx]] for idx in self._keypoint_ids
+      ])
+    else:
+      # Default all per-keypoint sigmas to 0.
+      self._oks_sigmas = np.full((len(self._keypoint_ids)), 0.05)
+      tf.logging.warning('No default keypoint OKS sigmas provided. Will use '
+                         '0.05')
+    tf.logging.info('Using the following keypoint OKS sigmas: {}'.format(
+        self._oks_sigmas))
+    self._metrics = None
+    super(CocoKeypointEvaluator, self).__init__([{
+        'id': self._category_id,
+        'name': class_text
+    }])
+
+  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
+    """Adds groundtruth for a single image with keypoints.
+
+    If the image has already been added, a warning is logged, and groundtruth
+    is ignored.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      groundtruth_dict: A dictionary containing -
+        InputDataFields.groundtruth_boxes: float32 numpy array of shape
+          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format
+          [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        InputDataFields.groundtruth_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed groundtruth classes for the boxes.
+        InputDataFields.groundtruth_is_crowd (optional): integer numpy array of
+          shape [num_boxes] containing iscrowd flag for groundtruth boxes.
+        InputDataFields.groundtruth_area (optional): float numpy array of
+          shape [num_boxes] containing the area (in the original absolute
+          coordinates) of the annotated object.
+        InputDataFields.groundtruth_keypoints: float numpy array of
+          keypoints with shape [num_boxes, num_keypoints, 2].
+        InputDataFields.groundtruth_keypoint_visibilities (optional): integer
+          numpy array of keypoint visibilities with shape [num_gt_boxes,
+          num_keypoints]. Integer is treated as an enum with 0=not labels,
+          1=labeled but not visible and 2=labeled and visible.
+    """
+
+    # Keep only the groundtruth for our category and its keypoints.
+    groundtruth_classes = groundtruth_dict[
+        standard_fields.InputDataFields.groundtruth_classes]
+    groundtruth_boxes = groundtruth_dict[
+        standard_fields.InputDataFields.groundtruth_boxes]
+    groundtruth_keypoints = groundtruth_dict[
+        standard_fields.InputDataFields.groundtruth_keypoints]
+    class_indices = [
+        idx for idx, gt_class_id in enumerate(groundtruth_classes)
+        if gt_class_id == self._category_id
+    ]
+    filtered_groundtruth_classes = np.take(
+        groundtruth_classes, class_indices, axis=0)
+    filtered_groundtruth_boxes = np.take(
+        groundtruth_boxes, class_indices, axis=0)
+    filtered_groundtruth_keypoints = np.take(
+        groundtruth_keypoints, class_indices, axis=0)
+    filtered_groundtruth_keypoints = np.take(
+        filtered_groundtruth_keypoints, self._keypoint_ids, axis=1)
+
+    filtered_groundtruth_dict = {}
+    filtered_groundtruth_dict[
+        standard_fields.InputDataFields
+        .groundtruth_classes] = filtered_groundtruth_classes
+    filtered_groundtruth_dict[standard_fields.InputDataFields
+                              .groundtruth_boxes] = filtered_groundtruth_boxes
+    filtered_groundtruth_dict[
+        standard_fields.InputDataFields
+        .groundtruth_keypoints] = filtered_groundtruth_keypoints
+
+    if (standard_fields.InputDataFields.groundtruth_is_crowd in
+        groundtruth_dict.keys()):
+      groundtruth_is_crowd = groundtruth_dict[
+          standard_fields.InputDataFields.groundtruth_is_crowd]
+      filtered_groundtruth_is_crowd = np.take(groundtruth_is_crowd,
+                                              class_indices, 0)
+      filtered_groundtruth_dict[
+          standard_fields.InputDataFields
+          .groundtruth_is_crowd] = filtered_groundtruth_is_crowd
+    if (standard_fields.InputDataFields.groundtruth_area in
+        groundtruth_dict.keys()):
+      groundtruth_area = groundtruth_dict[
+          standard_fields.InputDataFields.groundtruth_area]
+      filtered_groundtruth_area = np.take(groundtruth_area, class_indices, 0)
+      filtered_groundtruth_dict[
+          standard_fields.InputDataFields
+          .groundtruth_area] = filtered_groundtruth_area
+    if (standard_fields.InputDataFields.groundtruth_keypoint_visibilities in
+        groundtruth_dict.keys()):
+      groundtruth_keypoint_visibilities = groundtruth_dict[
+          standard_fields.InputDataFields.groundtruth_keypoint_visibilities]
+      filtered_groundtruth_keypoint_visibilities = np.take(
+          groundtruth_keypoint_visibilities, class_indices, axis=0)
+      filtered_groundtruth_keypoint_visibilities = np.take(
+          filtered_groundtruth_keypoint_visibilities,
+          self._keypoint_ids,
+          axis=1)
+      filtered_groundtruth_dict[
+          standard_fields.InputDataFields.
+          groundtruth_keypoint_visibilities] = filtered_groundtruth_keypoint_visibilities
+
+    super(CocoKeypointEvaluator,
+          self).add_single_ground_truth_image_info(image_id,
+                                                   filtered_groundtruth_dict)
+
+  def add_single_detected_image_info(self, image_id, detections_dict):
+    """Adds detections for a single image and the specific category for which keypoints are evaluated.
+
+    If a detection has already been added for this image id, a warning is
+    logged, and the detection is skipped.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      detections_dict: A dictionary containing -
+        DetectionResultFields.detection_boxes: float32 numpy array of shape
+          [num_boxes, 4] containing `num_boxes` detection boxes of the format
+          [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        DetectionResultFields.detection_scores: float32 numpy array of shape
+          [num_boxes] containing detection scores for the boxes.
+        DetectionResultFields.detection_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed detection classes for the boxes.
+        DetectionResultFields.detection_keypoints: float numpy array of
+          keypoints with shape [num_boxes, num_keypoints, 2].
+
+    Raises:
+      ValueError: If groundtruth for the image_id is not available.
+    """
+
+    # Keep only the detections for our category and its keypoints.
+    detection_classes = detections_dict[
+        standard_fields.DetectionResultFields.detection_classes]
+    detection_boxes = detections_dict[
+        standard_fields.DetectionResultFields.detection_boxes]
+    detection_scores = detections_dict[
+        standard_fields.DetectionResultFields.detection_scores]
+    detection_keypoints = detections_dict[
+        standard_fields.DetectionResultFields.detection_keypoints]
+    class_indices = [
+        idx for idx, class_id in enumerate(detection_classes)
+        if class_id == self._category_id
+    ]
+    filtered_detection_classes = np.take(
+        detection_classes, class_indices, axis=0)
+    filtered_detection_boxes = np.take(detection_boxes, class_indices, axis=0)
+    filtered_detection_scores = np.take(detection_scores, class_indices, axis=0)
+    filtered_detection_keypoints = np.take(
+        detection_keypoints, class_indices, axis=0)
+    filtered_detection_keypoints = np.take(
+        filtered_detection_keypoints, self._keypoint_ids, axis=1)
+
+    filtered_detections_dict = {}
+    filtered_detections_dict[standard_fields.DetectionResultFields
+                             .detection_classes] = filtered_detection_classes
+    filtered_detections_dict[standard_fields.DetectionResultFields
+                             .detection_boxes] = filtered_detection_boxes
+    filtered_detections_dict[standard_fields.DetectionResultFields
+                             .detection_scores] = filtered_detection_scores
+    filtered_detections_dict[standard_fields.DetectionResultFields.
+                             detection_keypoints] = filtered_detection_keypoints
+
+    super(CocoKeypointEvaluator,
+          self).add_single_detected_image_info(image_id,
+                                               filtered_detections_dict)
+
+  def evaluate(self):
+    """Evaluates the keypoints and returns a dictionary of coco metrics.
+
+    Returns:
+      A dictionary holding -
+
+      1. summary_metrics:
+      'Keypoints_Precision/mAP': mean average precision over classes
+        averaged over OKS thresholds ranging from .5 to .95 with .05
+        increments.
+      'Keypoints_Precision/mAP@.50IOU': mean average precision at 50% OKS
+      'Keypoints_Precision/mAP@.75IOU': mean average precision at 75% OKS
+      'Keypoints_Precision/mAP (medium)': mean average precision for medium
+        sized objects (32^2 pixels < area < 96^2 pixels).
+      'Keypoints_Precision/mAP (large)': mean average precision for large
+        objects (96^2 pixels < area < 10000^2 pixels).
+      'Keypoints_Recall/AR@1': average recall with 1 detection.
+      'Keypoints_Recall/AR@10': average recall with 10 detections.
+      'Keypoints_Recall/AR@100': average recall with 100 detections.
+      'Keypoints_Recall/AR@100 (medium)': average recall for medium objects with
+        100.
+      'Keypoints_Recall/AR@100 (large)': average recall for large objects with
+        100 detections.
+    """
+    tf.logging.info('Performing evaluation on %d images.', len(self._image_ids))
+    groundtruth_dict = {
+        'annotations': self._groundtruth_list,
+        'images': [{'id': image_id} for image_id in self._image_ids],
+        'categories': self._categories
+    }
+    coco_wrapped_groundtruth = coco_tools.COCOWrapper(
+        groundtruth_dict, detection_type='bbox')
+    coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(
+        self._detection_boxes_list)
+    keypoint_evaluator = coco_tools.COCOEvalWrapper(
+        coco_wrapped_groundtruth,
+        coco_wrapped_detections,
+        agnostic_mode=False,
+        iou_type='keypoints',
+        oks_sigmas=self._oks_sigmas)
+    keypoint_metrics, _ = keypoint_evaluator.ComputeMetrics(
+        include_metrics_per_category=False, all_metrics_per_category=False)
+    keypoint_metrics = {
+        'Keypoints_' + key: value
+        for key, value in iter(keypoint_metrics.items())
+    }
+    return keypoint_metrics
+
+  def add_eval_dict(self, eval_dict):
+    """Observes an evaluation result dict for a single example.
+
+    When executing eagerly, once all observations have been observed by this
+    method you can use `.evaluate()` to get the final metrics.
+
+    When using `tf.estimator.Estimator` for evaluation this function is used by
+    `get_estimator_eval_metric_ops()` to construct the metric update op.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating an object
+        detection model, returned from
+        eval_util.result_dict_for_single_example().
+
+    Returns:
+      None when executing eagerly, or an update_op that can be used to update
+      the eval metrics in `tf.estimator.EstimatorSpec`.
+    """
+    def update_op(
+        image_id_batched,
+        groundtruth_boxes_batched,
+        groundtruth_classes_batched,
+        groundtruth_is_crowd_batched,
+        groundtruth_area_batched,
+        groundtruth_keypoints_batched,
+        groundtruth_keypoint_visibilities_batched,
+        num_gt_boxes_per_image,
+        detection_boxes_batched,
+        detection_scores_batched,
+        detection_classes_batched,
+        detection_keypoints_batched,
+        num_det_boxes_per_image,
+        is_annotated_batched):
+      """Update operation for adding batch of images to Coco evaluator."""
+
+      for (image_id, gt_box, gt_class, gt_is_crowd, gt_area, gt_keyp,
+           gt_keyp_vis, num_gt_box, det_box, det_score, det_class, det_keyp,
+           num_det_box, is_annotated) in zip(
+               image_id_batched, groundtruth_boxes_batched,
+               groundtruth_classes_batched, groundtruth_is_crowd_batched,
+               groundtruth_area_batched, groundtruth_keypoints_batched,
+               groundtruth_keypoint_visibilities_batched,
+               num_gt_boxes_per_image, detection_boxes_batched,
+               detection_scores_batched, detection_classes_batched,
+               detection_keypoints_batched, num_det_boxes_per_image,
+               is_annotated_batched):
+        if is_annotated:
+          self.add_single_ground_truth_image_info(
+              image_id, {
+                  'groundtruth_boxes': gt_box[:num_gt_box],
+                  'groundtruth_classes': gt_class[:num_gt_box],
+                  'groundtruth_is_crowd': gt_is_crowd[:num_gt_box],
+                  'groundtruth_area': gt_area[:num_gt_box],
+                  'groundtruth_keypoints': gt_keyp[:num_gt_box],
+                  'groundtruth_keypoint_visibilities': gt_keyp_vis[:num_gt_box]
+              })
+          self.add_single_detected_image_info(
+              image_id, {
+                  'detection_boxes': det_box[:num_det_box],
+                  'detection_scores': det_score[:num_det_box],
+                  'detection_classes': det_class[:num_det_box],
+                  'detection_keypoints': det_keyp[:num_det_box],
+              })
+
+    # Unpack items from the evaluation dictionary.
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    image_id = eval_dict[input_data_fields.key]
+    groundtruth_boxes = eval_dict[input_data_fields.groundtruth_boxes]
+    groundtruth_classes = eval_dict[input_data_fields.groundtruth_classes]
+    groundtruth_is_crowd = eval_dict.get(input_data_fields.groundtruth_is_crowd,
+                                         None)
+    groundtruth_area = eval_dict.get(input_data_fields.groundtruth_area, None)
+    groundtruth_keypoints = eval_dict[input_data_fields.groundtruth_keypoints]
+    groundtruth_keypoint_visibilities = eval_dict.get(
+        input_data_fields.groundtruth_keypoint_visibilities, None)
+    detection_boxes = eval_dict[detection_fields.detection_boxes]
+    detection_scores = eval_dict[detection_fields.detection_scores]
+    detection_classes = eval_dict[detection_fields.detection_classes]
+    detection_keypoints = eval_dict[detection_fields.detection_keypoints]
+    num_gt_boxes_per_image = eval_dict.get(
+        'num_groundtruth_boxes_per_image', None)
+    num_det_boxes_per_image = eval_dict.get('num_det_boxes_per_image', None)
+    is_annotated = eval_dict.get('is_annotated', None)
+
+    if groundtruth_is_crowd is None:
+      groundtruth_is_crowd = tf.zeros_like(groundtruth_classes, dtype=tf.bool)
+
+    if groundtruth_area is None:
+      groundtruth_area = tf.zeros_like(groundtruth_classes, dtype=tf.float32)
+
+    if not image_id.shape.as_list():
+      # Apply a batch dimension to all tensors.
+      image_id = tf.expand_dims(image_id, 0)
+      groundtruth_boxes = tf.expand_dims(groundtruth_boxes, 0)
+      groundtruth_classes = tf.expand_dims(groundtruth_classes, 0)
+      groundtruth_is_crowd = tf.expand_dims(groundtruth_is_crowd, 0)
+      groundtruth_area = tf.expand_dims(groundtruth_area, 0)
+      groundtruth_keypoints = tf.expand_dims(groundtruth_keypoints, 0)
+      detection_boxes = tf.expand_dims(detection_boxes, 0)
+      detection_scores = tf.expand_dims(detection_scores, 0)
+      detection_classes = tf.expand_dims(detection_classes, 0)
+      detection_keypoints = tf.expand_dims(detection_keypoints, 0)
+
+      if num_gt_boxes_per_image is None:
+        num_gt_boxes_per_image = tf.shape(groundtruth_boxes)[1:2]
+      else:
+        num_gt_boxes_per_image = tf.expand_dims(num_gt_boxes_per_image, 0)
+
+      if num_det_boxes_per_image is None:
+        num_det_boxes_per_image = tf.shape(detection_boxes)[1:2]
+      else:
+        num_det_boxes_per_image = tf.expand_dims(num_det_boxes_per_image, 0)
+
+      if is_annotated is None:
+        is_annotated = tf.constant([True])
+      else:
+        is_annotated = tf.expand_dims(is_annotated, 0)
+
+      if groundtruth_keypoint_visibilities is None:
+        groundtruth_keypoint_visibilities = tf.fill([
+            tf.shape(groundtruth_boxes)[1],
+            tf.shape(groundtruth_keypoints)[2]
+        ], tf.constant(2, dtype=tf.int32))
+      groundtruth_keypoint_visibilities = tf.expand_dims(
+          groundtruth_keypoint_visibilities, 0)
+    else:
+      if num_gt_boxes_per_image is None:
+        num_gt_boxes_per_image = tf.tile(
+            tf.shape(groundtruth_boxes)[1:2],
+            multiples=tf.shape(groundtruth_boxes)[0:1])
+      if num_det_boxes_per_image is None:
+        num_det_boxes_per_image = tf.tile(
+            tf.shape(detection_boxes)[1:2],
+            multiples=tf.shape(detection_boxes)[0:1])
+      if is_annotated is None:
+        is_annotated = tf.ones_like(image_id, dtype=tf.bool)
+      if groundtruth_keypoint_visibilities is None:
+        groundtruth_keypoint_visibilities = tf.fill([
+            tf.shape(groundtruth_keypoints)[1],
+            tf.shape(groundtruth_keypoints)[2]
+        ], tf.constant(2, dtype=tf.int32))
+        groundtruth_keypoint_visibilities = tf.tile(
+            tf.expand_dims(groundtruth_keypoint_visibilities, 0),
+            multiples=[tf.shape(groundtruth_keypoints)[0], 1, 1])
+
+    return tf.py_func(update_op, [
+        image_id, groundtruth_boxes, groundtruth_classes, groundtruth_is_crowd,
+        groundtruth_area, groundtruth_keypoints,
+        groundtruth_keypoint_visibilities, num_gt_boxes_per_image,
+        detection_boxes, detection_scores, detection_classes,
+        detection_keypoints, num_det_boxes_per_image, is_annotated
+    ], [])
+
+  def get_estimator_eval_metric_ops(self, eval_dict):
+    """Returns a dictionary of eval metric ops.
+
+    Note that once value_op is called, the detections and groundtruth added via
+    update_op are cleared.
+
+    This function can take in groundtruth and detections for a batch of images,
+    or for a single image. For the latter case, the batch dimension for input
+    tensors need not be present.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating object detection
+        performance. For single-image evaluation, this dictionary may be
+        produced from eval_util.result_dict_for_single_example(). If multi-image
+        evaluation, `eval_dict` should contain the fields
+        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
+        properly unpad the tensors from the batch.
+
+    Returns:
+      a dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
+      update ops must be run together and similarly all value ops must be run
+      together to guarantee correct behaviour.
+    """
+    update_op = self.add_eval_dict(eval_dict)
+    category = self._category_name
+    metric_names = [
+        'Keypoints_Precision/mAP ByCategory/{}'.format(category),
+        'Keypoints_Precision/mAP@.50IOU ByCategory/{}'.format(category),
+        'Keypoints_Precision/mAP@.75IOU ByCategory/{}'.format(category),
+        'Keypoints_Precision/mAP (large) ByCategory/{}'.format(category),
+        'Keypoints_Precision/mAP (medium) ByCategory/{}'.format(category),
+        'Keypoints_Recall/AR@1 ByCategory/{}'.format(category),
+        'Keypoints_Recall/AR@10 ByCategory/{}'.format(category),
+        'Keypoints_Recall/AR@100 ByCategory/{}'.format(category),
+        'Keypoints_Recall/AR@100 (large) ByCategory/{}'.format(category),
+        'Keypoints_Recall/AR@100 (medium) ByCategory/{}'.format(category)
+    ]
+
+    def first_value_func():
+      self._metrics = self.evaluate()
+      self.clear()
+      return np.float32(self._metrics[metric_names[0]])
+
+    def value_func_factory(metric_name):
+      def value_func():
+        return np.float32(self._metrics[metric_name])
+      return value_func
+
+    # Ensure that the metrics are only evaluated once.
+    first_value_op = tf.py_func(first_value_func, [], tf.float32)
+    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}
+    with tf.control_dependencies([first_value_op]):
+      for metric_name in metric_names[1:]:
+        eval_metric_ops[metric_name] = (tf.py_func(
+            value_func_factory(metric_name), [], np.float32), update_op)
+    return eval_metric_ops
+
+
 class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
   """Class to evaluate COCO detection metrics."""
 
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
index 0a567c51..c23e44d6 100644
--- a/research/object_detection/metrics/coco_evaluation_test.py
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -37,6 +37,25 @@ def _get_categories_list():
   }]
 
 
+def _get_category_keypoints_dict():
+  return {
+      'person': [{
+          'id': 0,
+          'name': 'left_eye'
+      }, {
+          'id': 3,
+          'name': 'right_eye'
+      }],
+      'dog': [{
+          'id': 1,
+          'name': 'tail_start'
+      }, {
+          'id': 2,
+          'name': 'mouth'
+      }]
+  }
+
+
 class CocoDetectionEvaluationTest(tf.test.TestCase):
 
   def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
@@ -287,7 +306,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    detection_classes: np.array([2])
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -380,7 +399,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    detection_classes: np.array([1, 2, 2, 3])
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -476,7 +495,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                   np.array([2, 2])
           })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -538,7 +557,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
                    detection_classes: np.array([[1], [3], [2]])
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -625,7 +644,7 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
     self.assertEqual(len(coco_evaluator._detection_boxes_list), 5)
 
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
@@ -647,6 +666,696 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
     self.assertFalse(coco_evaluator._image_ids)
 
 
+class CocoKeypointEvaluationTest(tf.test.TestCase):
+
+  def testGetOneMAPWithMatchingKeypoints(self):
+    """Tests that correct mAP for keypoints is calculated."""
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [float('nan'),
+                                          float('nan')],
+                           [float('nan'), float('nan')], [170., 180.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[2, 0, 0, 2]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]])
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image2',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[50., 50., 100., 100.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[75., 76.], [float('nan'),
+                                        float('nan')],
+                           [float('nan'), float('nan')], [77., 78.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[2, 0, 0, 2]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image2',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[50., 50., 100., 100.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[75., 76.], [5., 6.], [7., 8.], [77., 78.]]])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+
+  def testGroundtruthListValues(self):
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [float('nan'), float('nan')],
+                           [float('nan'), float('nan')], [170., 180.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[2, 0, 0, 2]]),
+            standard_fields.InputDataFields.groundtruth_area: np.array([15.])
+        })
+    gt_dict = coco_evaluator._groundtruth_list[0]
+    self.assertEqual(gt_dict['id'], 1)
+    self.assertAlmostEqual(gt_dict['bbox'], [100.0, 100.0, 100.0, 100.0])
+    self.assertAlmostEqual(
+        gt_dict['keypoints'], [160.0, 150.0, 2, 180.0, 170.0, 2])
+    self.assertEqual(gt_dict['num_keypoints'], 2)
+    self.assertAlmostEqual(gt_dict['area'], 15.0)
+
+  def testKeypointVisibilitiesAreOptional(self):
+    """Tests that evaluator works when visibilities aren't provided."""
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [float('nan'),
+                                          float('nan')],
+                           [float('nan'), float('nan')], [170., 180.]]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]])
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image2',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[50., 50., 100., 100.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[75., 76.], [float('nan'),
+                                        float('nan')],
+                           [float('nan'), float('nan')], [77., 78.]]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image2',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[50., 50., 100., 100.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[75., 76.], [5., 6.], [7., 8.], [77., 78.]]])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+
+  def testFiltersDetectionsFromOtherCategories(self):
+    """Tests that the evaluator ignores detections from other categories."""
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=2, category_keypoints=category_keypoint_dict['person'],
+        class_text='dog')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [170., 180.], [110., 120.],
+                           [130., 140.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[2, 2, 2, 2]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.9]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[150., 160.], [170., 180.], [110., 120.],
+                           [130., 140.]]])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/dog'],
+                           -1.0)
+
+  def testHandlesUnlabeledKeypointData(self):
+    """Tests that the evaluator handles missing keypoints GT."""
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [float('nan'),
+                                          float('nan')],
+                           [float('nan'), float('nan')], [170., 180.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[0, 0, 0, 2]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[50., 60.], [1., 2.], [3., 4.], [170., 180.]]])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+
+  def testIgnoresCrowdAnnotations(self):
+    """Tests that the evaluator ignores GT marked as crowd."""
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_is_crowd:
+                np.array([1]),
+            standard_fields.InputDataFields.groundtruth_keypoints:
+                np.array([[[150., 160.], [float('nan'),
+                                          float('nan')],
+                           [float('nan'), float('nan')], [170., 180.]]]),
+            standard_fields.InputDataFields.groundtruth_keypoint_visibilities:
+                np.array([[2, 0, 0, 2]])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+                np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+                np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+                np.array([1]),
+            standard_fields.DetectionResultFields.detection_keypoints:
+                np.array([[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           -1.0)
+
+
+class CocoKeypointEvaluationPyFuncTest(tf.test.TestCase):
+
+  def testGetOneMAPWithMatchingKeypoints(self):
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_keypoint_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    image_id = tf.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
+    groundtruth_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    detection_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    eval_dict = {
+        input_data_fields.key: image_id,
+        input_data_fields.groundtruth_boxes: groundtruth_boxes,
+        input_data_fields.groundtruth_classes: groundtruth_classes,
+        input_data_fields.groundtruth_keypoints: groundtruth_keypoints,
+        detection_fields.detection_boxes: detection_boxes,
+        detection_fields.detection_scores: detection_scores,
+        detection_fields.detection_classes: detection_classes,
+        detection_fields.detection_keypoints: detection_keypoints,
+    }
+
+    eval_metric_ops = coco_keypoint_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)
+
+    _, update_op = eval_metric_ops['Keypoints_Precision/mAP ByCategory/person']
+
+    with self.test_session() as sess:
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image1',
+              groundtruth_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              groundtruth_classes:
+                  np.array([1]),
+              groundtruth_keypoints:
+                  np.array([[[150., 160.], [float('nan'),
+                                            float('nan')],
+                             [float('nan'), float('nan')], [170., 180.]]]),
+              detection_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              detection_scores:
+                  np.array([.8]),
+              detection_classes:
+                  np.array([1]),
+              detection_keypoints:
+                  np.array([[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]])
+          })
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image2',
+              groundtruth_boxes:
+                  np.array([[50., 50., 100., 100.]]),
+              groundtruth_classes:
+                  np.array([1]),
+              groundtruth_keypoints:
+                  np.array([[[75., 76.], [float('nan'),
+                                          float('nan')],
+                             [float('nan'), float('nan')], [77., 78.]]]),
+              detection_boxes:
+                  np.array([[50., 50., 100., 100.]]),
+              detection_scores:
+                  np.array([.7]),
+              detection_classes:
+                  np.array([1]),
+              detection_keypoints:
+                  np.array([[[75., 76.], [5., 6.], [7., 8.], [77., 78.]]])
+          })
+    metrics = {}
+    for key, (value_op, _) in eval_metric_ops.items():
+      metrics[key] = value_op
+    metrics = sess.run(metrics)
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.50IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.75IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (medium) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@1 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@10 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@100 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (medium) ByCategory/person'], 1.0)
+    self.assertFalse(coco_keypoint_evaluator._groundtruth_list)
+    self.assertFalse(coco_keypoint_evaluator._detection_boxes_list)
+    self.assertFalse(coco_keypoint_evaluator._image_ids)
+
+  def testGetOneMAPWithMatchingKeypointsAndVisibilities(self):
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_keypoint_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    image_id = tf.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
+    groundtruth_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+    groundtruth_keypoint_visibilities = tf.placeholder(
+        tf.float32, shape=(None, 4))
+    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    detection_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    eval_dict = {
+        input_data_fields.key:
+            image_id,
+        input_data_fields.groundtruth_boxes:
+            groundtruth_boxes,
+        input_data_fields.groundtruth_classes:
+            groundtruth_classes,
+        input_data_fields.groundtruth_keypoints:
+            groundtruth_keypoints,
+        input_data_fields.groundtruth_keypoint_visibilities:
+            groundtruth_keypoint_visibilities,
+        detection_fields.detection_boxes:
+            detection_boxes,
+        detection_fields.detection_scores:
+            detection_scores,
+        detection_fields.detection_classes:
+            detection_classes,
+        detection_fields.detection_keypoints:
+            detection_keypoints,
+    }
+
+    eval_metric_ops = coco_keypoint_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)
+
+    _, update_op = eval_metric_ops['Keypoints_Precision/mAP ByCategory/person']
+
+    with self.test_session() as sess:
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image1',
+              groundtruth_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              groundtruth_classes:
+                  np.array([1]),
+              groundtruth_keypoints:
+                  np.array([[[150., 160.], [float('nan'),
+                                            float('nan')],
+                             [float('nan'), float('nan')], [170., 180.]]]),
+              groundtruth_keypoint_visibilities:
+                  np.array([[0, 0, 0, 2]]),
+              detection_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              detection_scores:
+                  np.array([.8]),
+              detection_classes:
+                  np.array([1]),
+              detection_keypoints:
+                  np.array([[[50., 60.], [1., 2.], [3., 4.], [170., 180.]]])
+          })
+    metrics = {}
+    for key, (value_op, _) in eval_metric_ops.items():
+      metrics[key] = value_op
+    metrics = sess.run(metrics)
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.50IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.75IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (medium) ByCategory/person'], -1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@1 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@10 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@100 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (medium) ByCategory/person'], -1.0)
+    self.assertFalse(coco_keypoint_evaluator._groundtruth_list)
+    self.assertFalse(coco_keypoint_evaluator._detection_boxes_list)
+    self.assertFalse(coco_keypoint_evaluator._image_ids)
+
+  def testGetOneMAPWithMatchingKeypointsIsAnnotated(self):
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_keypoint_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    image_id = tf.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
+    groundtruth_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+    is_annotated = tf.placeholder(tf.bool, shape=())
+    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    detection_keypoints = tf.placeholder(tf.float32, shape=(None, 4, 2))
+
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    eval_dict = {
+        input_data_fields.key: image_id,
+        input_data_fields.groundtruth_boxes: groundtruth_boxes,
+        input_data_fields.groundtruth_classes: groundtruth_classes,
+        input_data_fields.groundtruth_keypoints: groundtruth_keypoints,
+        'is_annotated': is_annotated,
+        detection_fields.detection_boxes: detection_boxes,
+        detection_fields.detection_scores: detection_scores,
+        detection_fields.detection_classes: detection_classes,
+        detection_fields.detection_keypoints: detection_keypoints,
+    }
+
+    eval_metric_ops = coco_keypoint_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)
+
+    _, update_op = eval_metric_ops['Keypoints_Precision/mAP ByCategory/person']
+
+    with self.test_session() as sess:
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image1',
+              groundtruth_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              groundtruth_classes:
+                  np.array([1]),
+              groundtruth_keypoints:
+                  np.array([[[150., 160.], [float('nan'),
+                                            float('nan')],
+                             [float('nan'), float('nan')], [170., 180.]]]),
+              is_annotated:
+                  True,
+              detection_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              detection_scores:
+                  np.array([.8]),
+              detection_classes:
+                  np.array([1]),
+              detection_keypoints:
+                  np.array([[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]])
+          })
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image2',
+              groundtruth_boxes:
+                  np.array([[50., 50., 100., 100.]]),
+              groundtruth_classes:
+                  np.array([1]),
+              groundtruth_keypoints:
+                  np.array([[[75., 76.], [float('nan'),
+                                          float('nan')],
+                             [float('nan'), float('nan')], [77., 78.]]]),
+              is_annotated:
+                  True,
+              detection_boxes:
+                  np.array([[50., 50., 100., 100.]]),
+              detection_scores:
+                  np.array([.7]),
+              detection_classes:
+                  np.array([1]),
+              detection_keypoints:
+                  np.array([[[75., 76.], [5., 6.], [7., 8.], [77., 78.]]])
+          })
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image3',
+              groundtruth_boxes:
+                  np.zeros((0, 4)),
+              groundtruth_classes:
+                  np.zeros((0)),
+              groundtruth_keypoints:
+                  np.zeros((0, 4, 2)),
+              is_annotated:
+                  False,  # Note that this image isn't annotated.
+              detection_boxes:
+                  np.array([[25., 25., 50., 50.], [25., 25., 70., 50.],
+                            [25., 25., 80., 50.], [25., 25., 90., 50.]]),
+              detection_scores:
+                  np.array([0.6, 0.7, 0.8, 0.9]),
+              detection_classes:
+                  np.array([1, 2, 2, 3]),
+              detection_keypoints:
+                  np.array([[[0., 0.], [0., 0.], [0., 0.], [0., 0.]]])
+          })
+    metrics = {}
+    for key, (value_op, _) in eval_metric_ops.items():
+      metrics[key] = value_op
+    metrics = sess.run(metrics)
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.50IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.75IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (medium) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@1 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@10 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@100 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (medium) ByCategory/person'], 1.0)
+    self.assertFalse(coco_keypoint_evaluator._groundtruth_list)
+    self.assertFalse(coco_keypoint_evaluator._detection_boxes_list)
+    self.assertFalse(coco_keypoint_evaluator._image_ids)
+
+  def testGetOneMAPWithMatchingKeypointsBatched(self):
+    category_keypoint_dict = _get_category_keypoints_dict()
+    coco_keypoint_evaluator = coco_evaluation.CocoKeypointEvaluator(
+        category_id=1, category_keypoints=category_keypoint_dict['person'],
+        class_text='person')
+    batch_size = 2
+    image_id = tf.placeholder(tf.string, shape=(batch_size))
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
+    groundtruth_keypoints = tf.placeholder(
+        tf.float32, shape=(batch_size, None, 4, 2))
+    detection_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))
+    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))
+    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))
+    detection_keypoints = tf.placeholder(
+        tf.float32, shape=(batch_size, None, 4, 2))
+
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    eval_dict = {
+        input_data_fields.key: image_id,
+        input_data_fields.groundtruth_boxes: groundtruth_boxes,
+        input_data_fields.groundtruth_classes: groundtruth_classes,
+        input_data_fields.groundtruth_keypoints: groundtruth_keypoints,
+        detection_fields.detection_boxes: detection_boxes,
+        detection_fields.detection_scores: detection_scores,
+        detection_fields.detection_classes: detection_classes,
+        detection_fields.detection_keypoints: detection_keypoints
+    }
+
+    eval_metric_ops = coco_keypoint_evaluator.get_estimator_eval_metric_ops(
+        eval_dict)
+
+    _, update_op = eval_metric_ops['Keypoints_Precision/mAP ByCategory/person']
+
+    with self.test_session() as sess:
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id: ['image1', 'image2'],
+              groundtruth_boxes:
+                  np.array([[[100., 100., 200., 200.]], [[50., 50., 100.,
+                                                          100.]]]),
+              groundtruth_classes:
+                  np.array([[1], [3]]),
+              groundtruth_keypoints:
+                  np.array([[[[150., 160.], [float('nan'),
+                                             float('nan')],
+                              [float('nan'), float('nan')], [170., 180.]]],
+                            [[[75., 76.], [float('nan'),
+                                           float('nan')],
+                              [float('nan'), float('nan')], [77., 78.]]]]),
+              detection_boxes:
+                  np.array([[[100., 100., 200., 200.]], [[50., 50., 100.,
+                                                          100.]]]),
+              detection_scores:
+                  np.array([[.8], [.7]]),
+              detection_classes:
+                  np.array([[1], [3]]),
+              detection_keypoints:
+                  np.array([[[[150., 160.], [1., 2.], [3., 4.], [170., 180.]]],
+                            [[[75., 76.], [5., 6.], [7., 8.], [77., 78.]]]])
+          })
+    metrics = {}
+    for key, (value_op, _) in eval_metric_ops.items():
+      metrics[key] = value_op
+    metrics = sess.run(metrics)
+    self.assertAlmostEqual(metrics['Keypoints_Precision/mAP ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.50IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP@.75IOU ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Precision/mAP (medium) ByCategory/person'], -1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@1 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@10 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(metrics['Keypoints_Recall/AR@100 ByCategory/person'],
+                           1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (large) ByCategory/person'], 1.0)
+    self.assertAlmostEqual(
+        metrics['Keypoints_Recall/AR@100 (medium) ByCategory/person'], -1.0)
+    self.assertFalse(coco_keypoint_evaluator._groundtruth_list)
+    self.assertFalse(coco_keypoint_evaluator._detection_boxes_list)
+    self.assertFalse(coco_keypoint_evaluator._image_ids)
+
+
 class CocoMaskEvaluationTest(tf.test.TestCase):
 
   def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
@@ -824,7 +1533,7 @@ class CocoMaskEvaluationPyFuncTest(tf.test.TestCase):
                                            mode='constant')
                })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionMasks_Precision/mAP'], 1.0)
@@ -924,7 +1633,7 @@ class CocoMaskEvaluationPyFuncTest(tf.test.TestCase):
                            axis=0)
           })
     metrics = {}
-    for key, (value_op, _) in eval_metric_ops.iteritems():
+    for key, (value_op, _) in eval_metric_ops.items():
       metrics[key] = value_op
     metrics = sess.run(metrics)
     self.assertAlmostEqual(metrics['DetectionMasks_Precision/mAP'], 1.0)
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 7e7bb164..60b70f38 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -157,7 +157,7 @@ class COCOEvalWrapper(cocoeval.COCOeval):
   """
 
   def __init__(self, groundtruth=None, detections=None, agnostic_mode=False,
-               iou_type='bbox'):
+               iou_type='bbox', oks_sigmas=None):
     """COCOEvalWrapper constructor.
 
     Note that for the area-based metrics to be meaningful, detection and
@@ -170,12 +170,16 @@ class COCOEvalWrapper(cocoeval.COCOeval):
         detections
       agnostic_mode: boolean (default: False).  If True, evaluation ignores
         class labels, treating all detections as proposals.
-      iou_type: IOU type to use for evaluation. Supports `bbox` or `segm`.
+      iou_type: IOU type to use for evaluation. Supports `bbox', `segm`,
+        `keypoints`.
+      oks_sigmas: Float numpy array holding the OKS variances for keypoints.
     """
-    cocoeval.COCOeval.__init__(self, groundtruth, detections,
-                               iouType=iou_type)
+    cocoeval.COCOeval.__init__(self, groundtruth, detections, iouType=iou_type)
+    if oks_sigmas is not None:
+      self.params.kpt_oks_sigmas = oks_sigmas
     if agnostic_mode:
       self.params.useCats = 0
+    self._iou_type = iou_type
 
   def GetCategory(self, category_id):
     """Fetches dictionary holding category information given category id.
@@ -198,7 +202,7 @@ class COCOEvalWrapper(cocoeval.COCOeval):
   def ComputeMetrics(self,
                      include_metrics_per_category=False,
                      all_metrics_per_category=False):
-    """Computes detection metrics.
+    """Computes detection/keypoint metrics.
 
     Args:
       include_metrics_per_category: If True, will include metrics per category.
@@ -214,7 +218,7 @@ class COCOEvalWrapper(cocoeval.COCOeval):
         'Precision/mAP@.50IOU': mean average precision at 50% IOU
         'Precision/mAP@.75IOU': mean average precision at 75% IOU
         'Precision/mAP (small)': mean average precision for small objects
-                        (area < 32^2 pixels)
+                        (area < 32^2 pixels). NOTE: not present for 'keypoints'
         'Precision/mAP (medium)': mean average precision for medium sized
                         objects (32^2 pixels < area < 96^2 pixels)
         'Precision/mAP (large)': mean average precision for large objects
@@ -223,7 +227,7 @@ class COCOEvalWrapper(cocoeval.COCOeval):
         'Recall/AR@10': average recall with 10 detections
         'Recall/AR@100': average recall with 100 detections
         'Recall/AR@100 (small)': average recall for small objects with 100
-          detections
+          detections. NOTE: not present for 'keypoints'
         'Recall/AR@100 (medium)': average recall for medium objects with 100
           detections
         'Recall/AR@100 (large)': average recall for large objects with 100
@@ -243,20 +247,44 @@ class COCOEvalWrapper(cocoeval.COCOeval):
     self.accumulate()
     self.summarize()
 
-    summary_metrics = OrderedDict([
-        ('Precision/mAP', self.stats[0]),
-        ('Precision/mAP@.50IOU', self.stats[1]),
-        ('Precision/mAP@.75IOU', self.stats[2]),
-        ('Precision/mAP (small)', self.stats[3]),
-        ('Precision/mAP (medium)', self.stats[4]),
-        ('Precision/mAP (large)', self.stats[5]),
-        ('Recall/AR@1', self.stats[6]),
-        ('Recall/AR@10', self.stats[7]),
-        ('Recall/AR@100', self.stats[8]),
-        ('Recall/AR@100 (small)', self.stats[9]),
-        ('Recall/AR@100 (medium)', self.stats[10]),
-        ('Recall/AR@100 (large)', self.stats[11])
-    ])
+    summary_metrics = {}
+    if self._iou_type in ['bbox', 'segm']:
+      summary_metrics = OrderedDict([('Precision/mAP', self.stats[0]),
+                                     ('Precision/mAP@.50IOU', self.stats[1]),
+                                     ('Precision/mAP@.75IOU', self.stats[2]),
+                                     ('Precision/mAP (small)', self.stats[3]),
+                                     ('Precision/mAP (medium)', self.stats[4]),
+                                     ('Precision/mAP (large)', self.stats[5]),
+                                     ('Recall/AR@1', self.stats[6]),
+                                     ('Recall/AR@10', self.stats[7]),
+                                     ('Recall/AR@100', self.stats[8]),
+                                     ('Recall/AR@100 (small)', self.stats[9]),
+                                     ('Recall/AR@100 (medium)', self.stats[10]),
+                                     ('Recall/AR@100 (large)', self.stats[11])])
+    elif self._iou_type == 'keypoints':
+      category_id = self.GetCategoryIdList()[0]
+      category_name = self.GetCategory(category_id)['name']
+      summary_metrics = OrderedDict([])
+      summary_metrics['Precision/mAP ByCategory/{}'.format(
+          category_name)] = self.stats[0]
+      summary_metrics['Precision/mAP@.50IOU ByCategory/{}'.format(
+          category_name)] = self.stats[1]
+      summary_metrics['Precision/mAP@.75IOU ByCategory/{}'.format(
+          category_name)] = self.stats[2]
+      summary_metrics['Precision/mAP (medium) ByCategory/{}'.format(
+          category_name)] = self.stats[3]
+      summary_metrics['Precision/mAP (large) ByCategory/{}'.format(
+          category_name)] = self.stats[4]
+      summary_metrics['Recall/AR@1 ByCategory/{}'.format(
+          category_name)] = self.stats[5]
+      summary_metrics['Recall/AR@10 ByCategory/{}'.format(
+          category_name)] = self.stats[6]
+      summary_metrics['Recall/AR@100 ByCategory/{}'.format(
+          category_name)] = self.stats[7]
+      summary_metrics['Recall/AR@100 (medium) ByCategory/{}'.format(
+          category_name)] = self.stats[8]
+      summary_metrics['Recall/AR@100 (large) ByCategory/{}'.format(
+          category_name)] = self.stats[9]
     if not include_metrics_per_category:
       return summary_metrics, {}
     if not hasattr(self, 'category_stats'):
@@ -333,8 +361,11 @@ def ExportSingleImageGroundtruthToCoco(image_id,
                                        category_id_set,
                                        groundtruth_boxes,
                                        groundtruth_classes,
+                                       groundtruth_keypoints=None,
+                                       groundtruth_keypoint_visibilities=None,
                                        groundtruth_masks=None,
-                                       groundtruth_is_crowd=None):
+                                       groundtruth_is_crowd=None,
+                                       groundtruth_area=None):
   """Export groundtruth of a single image to COCO format.
 
   This function converts groundtruth detection annotations represented as numpy
@@ -356,10 +387,19 @@ def ExportSingleImageGroundtruthToCoco(image_id,
       category_id_set are dropped.
     groundtruth_boxes: numpy array (float32) with shape [num_gt_boxes, 4]
     groundtruth_classes: numpy array (int) with shape [num_gt_boxes]
+    groundtruth_keypoints: optional float numpy array of keypoints
+      with shape [num_gt_boxes, num_keypoints, 2].
+    groundtruth_keypoint_visibilities: optional integer numpy array of keypoint
+      visibilities with shape [num_gt_boxes, num_keypoints]. Integer is treated
+      as an enum with 0=not labels, 1=labeled but not visible and 2=labeled and
+      visible.
     groundtruth_masks: optional uint8 numpy array of shape [num_detections,
       image_height, image_width] containing detection_masks.
     groundtruth_is_crowd: optional numpy array (int) with shape [num_gt_boxes]
       indicating whether groundtruth boxes are crowd.
+    groundtruth_area: numpy array (float32) with shape [num_gt_boxes]. If
+      provided, then the area values (in the original absolute coordinates) will
+      be populated instead of calculated from bounding box coordinates.
 
   Returns:
     a list of groundtruth annotations for a single image in the COCO format.
@@ -390,10 +430,20 @@ def ExportSingleImageGroundtruthToCoco(image_id,
   has_is_crowd = groundtruth_is_crowd is not None
   if has_is_crowd and len(groundtruth_is_crowd.shape) != 1:
     raise ValueError('groundtruth_is_crowd is expected to be of rank 1.')
+  has_keypoints = groundtruth_keypoints is not None
+  has_keypoint_visibilities = groundtruth_keypoint_visibilities is not None
+  if has_keypoints and not has_keypoint_visibilities:
+    groundtruth_keypoint_visibilities = np.full(
+        (num_boxes, groundtruth_keypoints.shape[1]), 2)
   groundtruth_list = []
   for i in range(num_boxes):
     if groundtruth_classes[i] in category_id_set:
       iscrowd = groundtruth_is_crowd[i] if has_is_crowd else 0
+      if groundtruth_area is not None and groundtruth_area[i] > 0:
+        area = float(groundtruth_area[i])
+      else:
+        area = float((groundtruth_boxes[i, 2] - groundtruth_boxes[i, 0]) *
+                     (groundtruth_boxes[i, 3] - groundtruth_boxes[i, 1]))
       export_dict = {
           'id':
               next_annotation_id + i,
@@ -403,14 +453,27 @@ def ExportSingleImageGroundtruthToCoco(image_id,
               int(groundtruth_classes[i]),
           'bbox':
               list(_ConvertBoxToCOCOFormat(groundtruth_boxes[i, :])),
-          'area':
-              float((groundtruth_boxes[i, 2] - groundtruth_boxes[i, 0]) *
-                    (groundtruth_boxes[i, 3] - groundtruth_boxes[i, 1])),
+          'area': area,
           'iscrowd':
               iscrowd
       }
       if groundtruth_masks is not None:
         export_dict['segmentation'] = _RleCompress(groundtruth_masks[i])
+      if has_keypoints:
+        keypoints = groundtruth_keypoints[i]
+        visibilities = np.reshape(groundtruth_keypoint_visibilities[i], [-1])
+        coco_keypoints = []
+        num_valid_keypoints = 0
+        for keypoint, visibility in zip(keypoints, visibilities):
+          # Convert from [y, x] to [x, y] as mandated by COCO.
+          coco_keypoints.append(float(keypoint[1]))
+          coco_keypoints.append(float(keypoint[0]))
+          coco_keypoints.append(int(visibility))
+          if int(visibility) > 0:
+            num_valid_keypoints = num_valid_keypoints + 1
+        export_dict['keypoints'] = coco_keypoints
+        export_dict['num_keypoints'] = num_valid_keypoints
+
       groundtruth_list.append(export_dict)
   return groundtruth_list
 
@@ -494,7 +557,9 @@ def ExportSingleImageDetectionBoxesToCoco(image_id,
                                           category_id_set,
                                           detection_boxes,
                                           detection_scores,
-                                          detection_classes):
+                                          detection_classes,
+                                          detection_keypoints=None,
+                                          detection_keypoint_visibilities=None):
   """Export detections of a single image to COCO format.
 
   This function converts detections represented as numpy arrays to dictionaries
@@ -514,6 +579,12 @@ def ExportSingleImageDetectionBoxesToCoco(image_id,
       scored for the detection boxes.
     detection_classes: integer numpy array of shape [num_detections] containing
       the classes for detection boxes.
+    detection_keypoints: optional float numpy array of keypoints
+      with shape [num_detections, num_keypoints, 2].
+    detection_keypoint_visibilities: optional integer numpy array of keypoint
+      visibilities with shape [num_detections, num_keypoints]. Integer is
+      treated as an enum with 0=not labels, 1=labeled but not visible and
+      2=labeled and visible.
 
   Returns:
     a list of detection annotations for a single image in the COCO format.
@@ -546,12 +617,33 @@ def ExportSingleImageDetectionBoxesToCoco(image_id,
   detections_list = []
   for i in range(num_boxes):
     if detection_classes[i] in category_id_set:
-      detections_list.append({
-          'image_id': image_id,
-          'category_id': int(detection_classes[i]),
-          'bbox': list(_ConvertBoxToCOCOFormat(detection_boxes[i, :])),
-          'score': float(detection_scores[i])
-      })
+      export_dict = {
+          'image_id':
+              image_id,
+          'category_id':
+              int(detection_classes[i]),
+          'bbox':
+              list(_ConvertBoxToCOCOFormat(detection_boxes[i, :])),
+          'score':
+              float(detection_scores[i]),
+      }
+      if detection_keypoints is not None:
+        keypoints = detection_keypoints[i]
+        num_keypoints = keypoints.shape[0]
+        if detection_keypoint_visibilities is None:
+          detection_keypoint_visibilities = np.full((num_boxes, num_keypoints),
+                                                    2)
+        visibilities = np.reshape(detection_keypoint_visibilities[i], [-1])
+        coco_keypoints = []
+        for keypoint, visibility in zip(keypoints, visibilities):
+          # Convert from [y, x] to [x, y] as mandated by COCO.
+          coco_keypoints.append(float(keypoint[1]))
+          coco_keypoints.append(float(keypoint[0]))
+          coco_keypoints.append(int(visibility))
+        export_dict['keypoints'] = coco_keypoints
+        export_dict['num_keypoints'] = num_keypoints
+      detections_list.append(export_dict)
+
   return detections_list
 
 
diff --git a/research/object_detection/metrics/coco_tools_test.py b/research/object_detection/metrics/coco_tools_test.py
index cfb73d8c..269147c2 100644
--- a/research/object_detection/metrics/coco_tools_test.py
+++ b/research/object_detection/metrics/coco_tools_test.py
@@ -290,6 +290,116 @@ class CocoToolsTest(tf.test.TestCase):
       self.assertEqual(annotation['iscrowd'], is_crowd[i])
       self.assertEqual(annotation['id'], i + next_annotation_id)
 
+  def testSingleImageGroundtruthExportWithKeypoints(self):
+    boxes = np.array([[0, 0, 1, 1],
+                      [0, 0, .5, .5],
+                      [.5, .5, 1, 1]], dtype=np.float32)
+    coco_boxes = np.array([[0, 0, 1, 1],
+                           [0, 0, .5, .5],
+                           [.5, .5, .5, .5]], dtype=np.float32)
+    keypoints = np.array([[[0, 0], [0.25, 0.25], [0.75, 0.75]],
+                          [[0, 0], [0.125, 0.125], [0.375, 0.375]],
+                          [[0.5, 0.5], [0.75, 0.75], [1.0, 1.0]]],
+                         dtype=np.float32)
+    visibilities = np.array([[2, 2, 2],
+                             [2, 2, 0],
+                             [2, 0, 0]], dtype=np.int32)
+    areas = np.array([15., 16., 17.])
+
+    classes = np.array([1, 2, 3], dtype=np.int32)
+    is_crowd = np.array([0, 1, 0], dtype=np.int32)
+    next_annotation_id = 1
+
+    # Tests exporting without passing in is_crowd (for backward compatibility).
+    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        next_annotation_id=next_annotation_id,
+        groundtruth_boxes=boxes,
+        groundtruth_classes=classes,
+        groundtruth_keypoints=keypoints,
+        groundtruth_keypoint_visibilities=visibilities,
+        groundtruth_area=areas)
+    for i, annotation in enumerate(coco_annotations):
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+      self.assertEqual(annotation['image_id'], 'first_image')
+      self.assertEqual(annotation['category_id'], classes[i])
+      self.assertEqual(annotation['id'], i + next_annotation_id)
+      self.assertEqual(annotation['num_keypoints'], 3 - i)
+      self.assertEqual(annotation['area'], 15.0 + i)
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][0::3], keypoints[i, :, 1])))
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][1::3], keypoints[i, :, 0])))
+      self.assertTrue(
+          np.all(np.equal(annotation['keypoints'][2::3], visibilities[i])))
+
+    # Tests exporting with is_crowd.
+    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        next_annotation_id=next_annotation_id,
+        groundtruth_boxes=boxes,
+        groundtruth_classes=classes,
+        groundtruth_keypoints=keypoints,
+        groundtruth_keypoint_visibilities=visibilities,
+        groundtruth_is_crowd=is_crowd)
+    for i, annotation in enumerate(coco_annotations):
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+      self.assertEqual(annotation['image_id'], 'first_image')
+      self.assertEqual(annotation['category_id'], classes[i])
+      self.assertEqual(annotation['iscrowd'], is_crowd[i])
+      self.assertEqual(annotation['id'], i + next_annotation_id)
+      self.assertEqual(annotation['num_keypoints'], 3 - i)
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][0::3], keypoints[i, :, 1])))
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][1::3], keypoints[i, :, 0])))
+      self.assertTrue(
+          np.all(np.equal(annotation['keypoints'][2::3], visibilities[i])))
+      # Testing the area values are derived from the bounding boxes.
+      if i == 0:
+        self.assertAlmostEqual(annotation['area'], 1.0)
+      else:
+        self.assertAlmostEqual(annotation['area'], 0.25)
+
+  def testSingleImageDetectionBoxesExportWithKeypoints(self):
+    boxes = np.array([[0, 0, 1, 1], [0, 0, .5, .5], [.5, .5, 1, 1]],
+                     dtype=np.float32)
+    coco_boxes = np.array([[0, 0, 1, 1], [0, 0, .5, .5], [.5, .5, .5, .5]],
+                          dtype=np.float32)
+    keypoints = np.array([[[0, 0], [0.25, 0.25], [0.75, 0.75]],
+                          [[0, 0], [0.125, 0.125], [0.375, 0.375]],
+                          [[0.5, 0.5], [0.75, 0.75], [1.0, 1.0]]],
+                         dtype=np.float32)
+    visibilities = np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]], dtype=np.int32)
+
+    classes = np.array([1, 2, 3], dtype=np.int32)
+    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)
+
+    # Tests exporting without passing in is_crowd (for backward compatibility).
+    coco_annotations = coco_tools.ExportSingleImageDetectionBoxesToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        detection_boxes=boxes,
+        detection_scores=scores,
+        detection_classes=classes,
+        detection_keypoints=keypoints,
+        detection_keypoint_visibilities=visibilities)
+    for i, annotation in enumerate(coco_annotations):
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+      self.assertEqual(annotation['image_id'], 'first_image')
+      self.assertEqual(annotation['category_id'], classes[i])
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+      self.assertEqual(annotation['score'], scores[i])
+      self.assertEqual(annotation['num_keypoints'], 3)
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][0::3], keypoints[i, :, 1])))
+      self.assertTrue(
+          np.all(np.isclose(annotation['keypoints'][1::3], keypoints[i, :, 0])))
+      self.assertTrue(
+          np.all(np.equal(annotation['keypoints'][2::3], visibilities[i])))
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
index f3894cb6..de68461c 100644
--- a/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
+++ b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
@@ -24,6 +24,7 @@ import zlib
 import numpy as np
 import pandas as pd
 from pycocotools import mask as coco_mask
+import six
 import tensorflow as tf
 
 from object_detection.core import standard_fields
@@ -50,7 +51,8 @@ def encode_mask(mask_to_encode):
   mask_to_encode = mask_to_encode.astype(np.uint8)
   mask_to_encode = np.asfortranarray(mask_to_encode)
   encoded_mask = coco_mask.encode(mask_to_encode)[0]['counts']
-  compressed_mask = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)
+  compressed_mask = zlib.compress(six.ensure_binary(encoded_mask),
+                                  zlib.Z_BEST_COMPRESSION)
   base64_mask = base64.b64encode(compressed_mask)
   return base64_mask
 
diff --git a/research/object_detection/metrics/tf_example_parser.py b/research/object_detection/metrics/tf_example_parser.py
index 9a5f130f..cb1535f8 100644
--- a/research/object_detection/metrics/tf_example_parser.py
+++ b/research/object_detection/metrics/tf_example_parser.py
@@ -44,9 +44,9 @@ class StringParser(data_parser.DataToNumpyParser):
     self.field_name = field_name
 
   def parse(self, tf_example):
-    return "".join(tf_example.features.feature[self.field_name]
-                   .bytes_list.value) if tf_example.features.feature[
-                       self.field_name].HasField("bytes_list") else None
+    return b"".join(tf_example.features.feature[
+        self.field_name].bytes_list.value) if tf_example.features.feature[
+            self.field_name].HasField("bytes_list") else None
 
 
 class Int64Parser(data_parser.DataToNumpyParser):
diff --git a/research/object_detection/metrics/tf_example_parser_test.py b/research/object_detection/metrics/tf_example_parser_test.py
index 7d265cc2..57792b69 100644
--- a/research/object_detection/metrics/tf_example_parser_test.py
+++ b/research/object_detection/metrics/tf_example_parser_test.py
@@ -34,7 +34,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
 
   def testParseDetectionsAndGT(self):
-    source_id = 'abc.jpg'
+    source_id = b'abc.jpg'
     # y_min, x_min, y_max, x_max
     object_bb = np.array([[0.0, 0.5, 0.3], [0.0, 0.1, 0.6], [1.0, 0.6, 0.8],
                           [1.0, 0.6, 0.7]]).transpose()
@@ -129,7 +129,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         results_dict[fields.InputDataFields.groundtruth_image_classes])
 
   def testParseString(self):
-    string_val = 'abc'
+    string_val = b'abc'
     features = {'string': self._BytesFeature(string_val)}
     example = tf.train.Example(features=tf.train.Features(feature=features))
 
diff --git a/research/object_detection/model_hparams.py b/research/object_detection/model_hparams.py
index b0d12fce..12b043e9 100644
--- a/research/object_detection/model_hparams.py
+++ b/research/object_detection/model_hparams.py
@@ -21,7 +21,13 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import training as contrib_training
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 def create_hparams(hparams_overrides=None):
@@ -34,7 +40,7 @@ def create_hparams(hparams_overrides=None):
   Returns:
     The hyperparameters as a tf.HParams object.
   """
-  hparams = tf.contrib.training.HParams(
+  hparams = contrib_training.HParams(
       # Whether a fine tuning checkpoint (provided in the pipeline config)
       # should be loaded for training.
       load_pretrained=True)
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index e58be719..ba0f0b4b 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -38,6 +38,18 @@ from object_detection.utils import shape_utils
 from object_detection.utils import variables_helper
 from object_detection.utils import visualization_utils as vis_utils
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import framework as contrib_framework
+  from tensorflow.contrib import layers as contrib_layers
+  from tensorflow.contrib import learn as contrib_learn
+  from tensorflow.contrib import tpu as contrib_tpu
+  from tensorflow.contrib import training as contrib_training
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 # A map of names to methods that help build the model.
 MODEL_BUILD_UTIL_MAP = {
     'get_configs_from_pipeline_file':
@@ -76,8 +88,13 @@ def _prepare_groundtruth_for_eval(detection_model, class_agnostic,
         groundtruth)
       'groundtruth_is_crowd': [batch_size, num_boxes] bool tensor indicating
         is_crowd annotations (if provided in groundtruth).
+      'groundtruth_area': [batch_size, num_boxes] float32 tensor indicating
+        the area (in the original absolute coordinates) of annotations (if
+        provided in groundtruth).
       'num_groundtruth_boxes': [batch_size] tensor containing the maximum number
         of groundtruth boxes per image..
+      'groundtruth_keypoints': [batch_size, num_boxes, num_keypoints, 2] float32
+        tensor of keypoints (if provided in groundtruth).
     class_agnostic: Boolean indicating whether detections are class agnostic.
   """
   input_data_fields = fields.InputDataFields()
@@ -107,6 +124,20 @@ def _prepare_groundtruth_for_eval(detection_model, class_agnostic,
     groundtruth[input_data_fields.groundtruth_is_crowd] = tf.stack(
         detection_model.groundtruth_lists(fields.BoxListFields.is_crowd))
 
+  if detection_model.groundtruth_has_field(input_data_fields.groundtruth_area):
+    groundtruth[input_data_fields.groundtruth_area] = tf.stack(
+        detection_model.groundtruth_lists(input_data_fields.groundtruth_area))
+
+  if detection_model.groundtruth_has_field(fields.BoxListFields.keypoints):
+    groundtruth[input_data_fields.groundtruth_keypoints] = tf.stack(
+        detection_model.groundtruth_lists(fields.BoxListFields.keypoints))
+
+  if detection_model.groundtruth_has_field(
+      fields.BoxListFields.keypoint_visibilities):
+    groundtruth[input_data_fields.groundtruth_keypoint_visibilities] = tf.stack(
+        detection_model.groundtruth_lists(
+            fields.BoxListFields.keypoint_visibilities))
+
   groundtruth[input_data_fields.num_groundtruth_boxes] = (
       tf.tile([max_number_of_boxes], multiples=[groundtruth_boxes_shape[0]]))
   return groundtruth
@@ -161,6 +192,7 @@ def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
         fields.InputDataFields.groundtruth_classes,
         fields.InputDataFields.groundtruth_boxes,
         fields.InputDataFields.groundtruth_keypoints,
+        fields.InputDataFields.groundtruth_keypoint_visibilities,
         fields.InputDataFields.groundtruth_group_of,
         fields.InputDataFields.groundtruth_difficult,
         fields.InputDataFields.groundtruth_is_crowd,
@@ -206,6 +238,10 @@ def provide_groundtruth(model, labels):
   gt_keypoints_list = None
   if fields.InputDataFields.groundtruth_keypoints in labels:
     gt_keypoints_list = labels[fields.InputDataFields.groundtruth_keypoints]
+  gt_keypoint_visibilities_list = None
+  if fields.InputDataFields.groundtruth_keypoint_visibilities in labels:
+    gt_keypoint_visibilities_list = labels[
+        fields.InputDataFields.groundtruth_keypoint_visibilities]
   gt_weights_list = None
   if fields.InputDataFields.groundtruth_weights in labels:
     gt_weights_list = labels[fields.InputDataFields.groundtruth_weights]
@@ -216,14 +252,24 @@ def provide_groundtruth(model, labels):
   gt_is_crowd_list = None
   if fields.InputDataFields.groundtruth_is_crowd in labels:
     gt_is_crowd_list = labels[fields.InputDataFields.groundtruth_is_crowd]
+  gt_area_list = None
+  if fields.InputDataFields.groundtruth_area in labels:
+    gt_area_list = labels[fields.InputDataFields.groundtruth_area]
+  gt_labeled_classes = None
+  if fields.InputDataFields.groundtruth_labeled_classes in labels:
+    gt_labeled_classes = labels[
+        fields.InputDataFields.groundtruth_labeled_classes]
   model.provide_groundtruth(
       groundtruth_boxes_list=gt_boxes_list,
       groundtruth_classes_list=gt_classes_list,
       groundtruth_confidences_list=gt_confidences_list,
+      groundtruth_labeled_classes=gt_labeled_classes,
       groundtruth_masks_list=gt_masks_list,
       groundtruth_keypoints_list=gt_keypoints_list,
+      groundtruth_keypoint_visibilities_list=gt_keypoint_visibilities_list,
       groundtruth_weights_list=gt_weights_list,
-      groundtruth_is_crowd_list=gt_is_crowd_list)
+      groundtruth_is_crowd_list=gt_is_crowd_list,
+      groundtruth_area_list=gt_area_list)
 
 
 def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
@@ -296,23 +342,26 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       provide_groundtruth(detection_model, labels)
 
     preprocessed_images = features[fields.InputDataFields.image]
+
+    side_inputs = detection_model.get_side_inputs(features)
+
     if use_tpu and train_config.use_bfloat16:
-      with tf.contrib.tpu.bfloat16_scope():
+      with contrib_tpu.bfloat16_scope():
         prediction_dict = detection_model.predict(
             preprocessed_images,
-            features[fields.InputDataFields.true_image_shape])
+            features[fields.InputDataFields.true_image_shape], **side_inputs)
         prediction_dict = ops.bfloat16_to_float32_nested(prediction_dict)
     else:
       prediction_dict = detection_model.predict(
           preprocessed_images,
-          features[fields.InputDataFields.true_image_shape])
+          features[fields.InputDataFields.true_image_shape], **side_inputs)
 
     def postprocess_wrapper(args):
       return detection_model.postprocess(args[0], args[1])
 
     if mode in (tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT):
       if use_tpu and postprocess_on_cpu:
-        detections = tf.contrib.tpu.outside_compilation(
+        detections = contrib_tpu.outside_compilation(
             postprocess_wrapper,
             (prediction_dict,
              features[fields.InputDataFields.true_image_shape]))
@@ -354,21 +403,26 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
                                         available_var_map)
 
     if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
-      losses_dict = detection_model.loss(
-          prediction_dict, features[fields.InputDataFields.true_image_shape])
-      losses = [loss_tensor for loss_tensor in losses_dict.values()]
-      if train_config.add_regularization_loss:
-        regularization_losses = detection_model.regularization_losses()
-        if use_tpu and train_config.use_bfloat16:
-          regularization_losses = ops.bfloat16_to_float32_nested(
-              regularization_losses)
-        if regularization_losses:
-          regularization_loss = tf.add_n(
-              regularization_losses, name='regularization_loss')
-          losses.append(regularization_loss)
-          losses_dict['Loss/regularization_loss'] = regularization_loss
-      total_loss = tf.add_n(losses, name='total_loss')
-      losses_dict['Loss/total_loss'] = total_loss
+      if (mode == tf.estimator.ModeKeys.EVAL and
+          eval_config.use_dummy_loss_in_eval):
+        total_loss = tf.constant(1.0)
+        losses_dict = {'Loss/total_loss': total_loss}
+      else:
+        losses_dict = detection_model.loss(
+            prediction_dict, features[fields.InputDataFields.true_image_shape])
+        losses = [loss_tensor for loss_tensor in losses_dict.values()]
+        if train_config.add_regularization_loss:
+          regularization_losses = detection_model.regularization_losses()
+          if use_tpu and train_config.use_bfloat16:
+            regularization_losses = ops.bfloat16_to_float32_nested(
+                regularization_losses)
+          if regularization_losses:
+            regularization_loss = tf.add_n(
+                regularization_losses, name='regularization_loss')
+            losses.append(regularization_loss)
+            losses_dict['Loss/regularization_loss'] = regularization_loss
+        total_loss = tf.add_n(losses, name='total_loss')
+        losses_dict['Loss/total_loss'] = total_loss
 
       if 'graph_rewriter_config' in configs:
         graph_rewriter_fn = graph_rewriter_builder.build(
@@ -383,8 +437,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
 
     if mode == tf.estimator.ModeKeys.TRAIN:
       if use_tpu:
-        training_optimizer = tf.contrib.tpu.CrossShardOptimizer(
-            training_optimizer)
+        training_optimizer = contrib_tpu.CrossShardOptimizer(training_optimizer)
 
       # Optionally freeze some layers by setting their gradients to be zero.
       trainable_variables = None
@@ -394,7 +447,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       exclude_variables = (
           train_config.freeze_variables
           if train_config.freeze_variables else None)
-      trainable_variables = tf.contrib.framework.filter_variables(
+      trainable_variables = contrib_framework.filter_variables(
           tf.trainable_variables(),
           include_patterns=include_variables,
           exclude_patterns=exclude_variables)
@@ -409,7 +462,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       summaries = [] if use_tpu else None
       if train_config.summarize_gradients:
         summaries = ['gradients', 'gradient_norm', 'global_gradient_norm']
-      train_op = tf.contrib.layers.optimize_loss(
+      train_op = contrib_layers.optimize_loss(
           loss=total_loss,
           global_step=global_step,
           learning_rate=None,
@@ -468,12 +521,16 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
             eval_input_config.label_map_path)
       vis_metric_ops = None
       if not use_tpu and use_original_images:
+        keypoint_edges = [
+            (kp.start, kp.end) for kp in eval_config.keypoint_edge]
+
         eval_metric_op_vis = vis_utils.VisualizeSingleFrameDetections(
             category_index,
             max_examples_to_draw=eval_config.num_visualizations,
             max_boxes_to_draw=eval_config.max_num_boxes_to_visualize,
             min_score_thresh=eval_config.min_score_threshold,
-            use_normalized_coordinates=False)
+            use_normalized_coordinates=False,
+            keypoint_edges=keypoint_edges or None)
         vis_metric_ops = eval_metric_op_vis.get_estimator_eval_metric_ops(
             eval_dict)
 
@@ -500,7 +557,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
 
     # EVAL executes on CPU, so use regular non-TPU EstimatorSpec.
     if use_tpu and mode != tf.estimator.ModeKeys.EVAL:
-      return tf.contrib.tpu.TPUEstimatorSpec(
+      return contrib_tpu.TPUEstimatorSpec(
           mode=mode,
           scaffold_fn=scaffold_fn,
           predictions=detections,
@@ -535,7 +592,7 @@ def create_estimator_and_inputs(run_config,
                                 pipeline_config_path,
                                 config_override=None,
                                 train_steps=None,
-                                sample_1_of_n_eval_examples=None,
+                                sample_1_of_n_eval_examples=1,
                                 sample_1_of_n_eval_on_train_examples=1,
                                 model_fn_creator=create_model_fn,
                                 use_tpu_estimator=False,
@@ -681,7 +738,7 @@ def create_estimator_and_inputs(run_config,
   model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu,
                               postprocess_on_cpu)
   if use_tpu_estimator:
-    estimator = tf.contrib.tpu.TPUEstimator(
+    estimator = contrib_tpu.TPUEstimator(
         model_fn=model_fn,
         train_batch_size=train_config.batch_size,
         # For each core, only batch size 1 is supported for eval.
@@ -785,7 +842,7 @@ def continuous_eval(estimator, model_dir, input_fn, train_steps, name):
     tf.logging.info('Terminating eval after 180 seconds of no checkpoints')
     return True
 
-  for ckpt in tf.contrib.training.checkpoints_iterator(
+  for ckpt in contrib_training.checkpoints_iterator(
       model_dir, min_interval_secs=180, timeout=None,
       timeout_fn=terminate_eval):
 
@@ -862,11 +919,11 @@ def populate_experiment(run_config,
   train_steps = train_and_eval_dict['train_steps']
 
   export_strategies = [
-      tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(
+      contrib_learn.utils.saved_model_export_utils.make_export_strategy(
           serving_input_fn=predict_input_fn)
   ]
 
-  return tf.contrib.learn.Experiment(
+  return contrib_learn.Experiment(
       estimator=estimator,
       train_input_fn=train_input_fn,
       eval_input_fn=eval_input_fns[0],
diff --git a/research/object_detection/model_lib_test.py b/research/object_detection/model_lib_test.py
index c61fbb6e..c64b202c 100644
--- a/research/object_detection/model_lib_test.py
+++ b/research/object_detection/model_lib_test.py
@@ -39,6 +39,9 @@ from object_detection.utils import config_util
 # 'ssd_inception_v2_pets', 'faster_rcnn_resnet50_pets'
 MODEL_NAME_FOR_TEST = 'ssd_inception_v2_pets'
 
+# Model for testing keypoints.
+MODEL_NAME_FOR_KEYPOINTS_TEST = 'ssd_mobilenet_v1_fpp'
+
 
 def _get_data_path():
   """Returns an absolute path to TFRecord file."""
@@ -48,8 +51,12 @@ def _get_data_path():
 
 def get_pipeline_config_path(model_name):
   """Returns path to the local pipeline config file."""
-  return os.path.join(tf.resource_loader.get_data_files_path(), 'samples',
-                      'configs', model_name + '.config')
+  if model_name == MODEL_NAME_FOR_KEYPOINTS_TEST:
+    return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
+                        model_name + '.config')
+  else:
+    return os.path.join(tf.resource_loader.get_data_files_path(), 'samples',
+                        'configs', model_name + '.config')
 
 
 def _get_labelmap_path():
@@ -58,11 +65,20 @@ def _get_labelmap_path():
                       'pet_label_map.pbtxt')
 
 
+def _get_keypoints_labelmap_path():
+  """Returns an absolute path to label map file."""
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'data',
+                      'face_person_with_keypoints_label_map.pbtxt')
+
+
 def _get_configs_for_model(model_name):
   """Returns configurations for model."""
   filename = get_pipeline_config_path(model_name)
   data_path = _get_data_path()
-  label_map_path = _get_labelmap_path()
+  if model_name == MODEL_NAME_FOR_KEYPOINTS_TEST:
+    label_map_path = _get_keypoints_labelmap_path()
+  else:
+    label_map_path = _get_labelmap_path()
   configs = config_util.get_configs_from_pipeline_file(filename)
   override_dict = {
       'train_input_path': data_path,
@@ -213,6 +229,17 @@ class ModelLibTest(tf.test.TestCase):
     configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
     self._assert_model_fn_for_train_eval(configs, 'eval')
 
+  def test_model_fn_in_keypoints_eval_mode(self):
+    """Tests the model function in EVAL mode with keypoints config."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_KEYPOINTS_TEST)
+    estimator_spec = self._assert_model_fn_for_train_eval(configs, 'eval')
+    metric_ops = estimator_spec.eval_metric_ops
+    self.assertIn('Keypoints_Precision/mAP ByCategory/face', metric_ops)
+    self.assertIn('Keypoints_Precision/mAP ByCategory/PERSON', metric_ops)
+    detection_keypoints = estimator_spec.predictions['detection_keypoints']
+    self.assertEqual(1, detection_keypoints.shape.as_list()[0])
+    self.assertEqual(tf.float32, detection_keypoints.dtype)
+
   def test_model_fn_in_eval_on_train_mode(self):
     """Tests the model function in EVAL mode with train data."""
     configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
diff --git a/research/object_detection/model_lib_v2.py b/research/object_detection/model_lib_v2.py
index d433ebc5..b99f0666 100644
--- a/research/object_detection/model_lib_v2.py
+++ b/research/object_detection/model_lib_v2.py
@@ -19,6 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import copy
+import os
 import time
 
 import tensorflow as tf
@@ -29,10 +30,20 @@ from object_detection import model_lib
 from object_detection.builders import model_builder
 from object_detection.builders import optimizer_builder
 from object_detection.core import standard_fields as fields
+from object_detection.protos import train_pb2
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
 from object_detection.utils import ops
 from object_detection.utils import variables_helper
+from object_detection.utils import visualization_utils as vutils
+
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import tpu as contrib_tpu
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 MODEL_BUILD_UTIL_MAP = model_lib.MODEL_BUILD_UTIL_MAP
 
@@ -44,6 +55,12 @@ MODEL_BUILD_UTIL_MAP = model_lib.MODEL_BUILD_UTIL_MAP
 #### & verify the loss output from the eval_loop method.
 ### TODO(kaftan): Make sure the unit tests run in TAP presubmits or Kokoro
 
+RESTORE_MAP_ERROR_TEMPLATE = (
+    'Since we are restoring a v2 style checkpoint'
+    ' restore_map was expected to return a (str -> Model) mapping,'
+    ' but we received a ({} -> {}) mapping instead.'
+)
+
 
 def _compute_losses_and_predictions_dicts(
     model, features, labels,
@@ -233,12 +250,34 @@ def eager_train_step(detection_model,
     gradients, _ = tf.clip_by_global_norm(gradients, clip_gradients_value)
   optimizer.apply_gradients(zip(gradients, trainable_variables))
   tf.compat.v2.summary.scalar('learning_rate', learning_rate, step=global_step)
-
+  tf.compat.v2.summary.image(
+      name='train_input_images',
+      step=global_step,
+      data=features[fields.InputDataFields.image],
+      max_outputs=3)
   return total_loss
 
 
+def validate_tf_v2_checkpoint_restore_map(checkpoint_restore_map):
+  """Ensure that given dict is a valid TF v2 style restore map.
+
+  Args:
+    checkpoint_restore_map: A dict mapping strings to tf.keras.Model objects.
+
+  Raises:
+    ValueError: If they keys in checkpoint_restore_map are not strings or if
+      the values are not keras Model objects.
+
+  """
+
+  for key, value in checkpoint_restore_map.items():
+    if not (isinstance(key, str) and isinstance(value, tf.Module)):
+      raise TypeError(RESTORE_MAP_ERROR_TEMPLATE.format(
+          key.__class__.__name__, value.__class__.__name__))
+
+
 def load_fine_tune_checkpoint(
-    model, checkpoint_path, checkpoint_type,
+    model, checkpoint_path, checkpoint_type, checkpoint_version,
     load_all_detection_checkpoint_vars, input_dataset,
     unpad_groundtruth_tensors):
   """Load a fine tuning classification or detection checkpoint.
@@ -260,6 +299,8 @@ def load_fine_tune_checkpoint(
       checkpoint (with compatible variable names) or to restore from a
       classification checkpoint for initialization prior to training.
       Valid values: `detection`, `classification`.
+    checkpoint_version: train_pb2.CheckpointVersion.V1 or V2 enum indicating
+      whether to load checkpoints in V1 style or V2 style.
     load_all_detection_checkpoint_vars: whether to load all variables (when
       `fine_tune_checkpoint_type` is `detection`). If False, only variables
       within the feature extractor scopes are included. Default False.
@@ -269,6 +310,7 @@ def load_fine_tune_checkpoint(
   """
   features, labels = iter(input_dataset).next()
 
+  @tf.function
   def _dummy_computation_fn(features, labels):
     model._is_training = False  # pylint: disable=protected-access
     tf.keras.backend.set_learning_phase(False)
@@ -282,21 +324,65 @@ def load_fine_tune_checkpoint(
         labels)
 
   strategy = tf.compat.v2.distribute.get_strategy()
-  strategy.experimental_run_v2(
+  strategy.run(
       _dummy_computation_fn, args=(
           features,
           labels,
       ))
-  var_map = model.restore_map(
-      fine_tune_checkpoint_type=checkpoint_type,
-      load_all_detection_checkpoint_vars=(
-          load_all_detection_checkpoint_vars))
-  available_var_map = variables_helper.get_variables_available_in_checkpoint(
-      var_map,
-      checkpoint_path,
-      include_global_step=False)
-  tf.train.init_from_checkpoint(checkpoint_path,
-                                available_var_map)
+
+  if checkpoint_version == train_pb2.CheckpointVersion.V1:
+    var_map = model.restore_map(
+        fine_tune_checkpoint_type=checkpoint_type,
+        load_all_detection_checkpoint_vars=(
+            load_all_detection_checkpoint_vars))
+    available_var_map = variables_helper.get_variables_available_in_checkpoint(
+        var_map,
+        checkpoint_path,
+        include_global_step=False)
+    tf.train.init_from_checkpoint(checkpoint_path,
+                                  available_var_map)
+  elif checkpoint_version == train_pb2.CheckpointVersion.V2:
+    restore_map = model.restore_map(
+        fine_tune_checkpoint_type=checkpoint_type,
+        load_all_detection_checkpoint_vars=(
+            load_all_detection_checkpoint_vars))
+    validate_tf_v2_checkpoint_restore_map(restore_map)
+
+    ckpt = tf.train.Checkpoint(**restore_map)
+    ckpt.restore(checkpoint_path).assert_existing_objects_matched()
+
+
+def _get_filepath(strategy, filepath):
+  """Get appropriate filepath for worker.
+
+  Args:
+    strategy: A tf.distribute.Strategy object.
+    filepath: A path to where the Checkpoint object is stored.
+
+  Returns:
+    A temporary filepath for non-chief workers to use or the original filepath
+    for the chief.
+  """
+  if strategy.extended.should_checkpoint:
+    return filepath
+  else:
+    # TODO(vighneshb) Replace with the public API when TF exposes it.
+    task_id = strategy.extended._task_id  # pylint:disable=protected-access
+    return os.path.join(filepath, 'temp_worker_{:03d}'.format(task_id))
+
+
+def _clean_temporary_directories(strategy, filepath):
+  """Temporary directory clean up for MultiWorker Mirrored Strategy.
+
+  This is needed for all non-chief workers.
+
+  Args:
+    strategy: A tf.distribute.Strategy object.
+    filepath: The filepath for the temporary directory.
+  """
+  if not strategy.extended.should_checkpoint:
+    if tf.io.gfile.exists(filepath) and tf.io.gfile.isdir(filepath):
+      tf.io.gfile.rmtree(filepath)
 
 
 def train_loop(
@@ -308,7 +394,9 @@ def train_loop(
     use_tpu=False,
     save_final_config=False,
     export_to_tpu=None,
-    checkpoint_every_n=1000, **kwargs):
+    checkpoint_every_n=1000,
+    checkpoint_max_to_keep=7,
+    **kwargs):
   """Trains a model using eager + functions.
 
   This method:
@@ -340,6 +428,8 @@ def train_loop(
       hparams too.
     checkpoint_every_n:
       Checkpoint every n training steps.
+    checkpoint_max_to_keep:
+      int, the number of most recent checkpoints to keep in the model directory.
     **kwargs: Additional keyword arguments for configuration override.
   """
   ## Parse the configs
@@ -400,6 +490,7 @@ def train_loop(
     else:
       train_config.fine_tune_checkpoint_type = 'classification'
   fine_tune_checkpoint_type = train_config.fine_tune_checkpoint_type
+  fine_tune_checkpoint_version = train_config.fine_tune_checkpoint_version
 
   # Write the as-run pipeline config to disk.
   if save_final_config:
@@ -412,18 +503,25 @@ def train_loop(
     detection_model = model_builder.build(
         model_config=model_config, is_training=True)
 
-    # Create the inputs.
-    train_input = inputs.train_input(
-        train_config=train_config,
-        train_input_config=train_input_config,
-        model_config=model_config,
-        model=detection_model)
-
-    train_input = strategy.experimental_distribute_dataset(
-        train_input.repeat())
-
-    global_step = tf.compat.v2.Variable(
-        0, trainable=False, dtype=tf.compat.v2.dtypes.int64, name='global_step')
+    def train_dataset_fn(input_context):
+      """Callable to create train input."""
+      # Create the inputs.
+      train_input = inputs.train_input(
+          train_config=train_config,
+          train_input_config=train_input_config,
+          model_config=model_config,
+          model=detection_model,
+          input_context=input_context)
+      train_input = train_input.repeat()
+      return train_input
+
+    train_input = strategy.experimental_distribute_datasets_from_function(
+        train_dataset_fn)
+
+
+    global_step = tf.Variable(
+        0, trainable=False, dtype=tf.compat.v2.dtypes.int64, name='global_step',
+        aggregation=tf.compat.v2.VariableAggregation.ONLY_FIRST_REPLICA)
     optimizer, (learning_rate,) = optimizer_builder.build(
         train_config.optimizer, global_step=global_step)
 
@@ -433,69 +531,117 @@ def train_loop(
       learning_rate_fn = lambda: learning_rate
 
   ## Train the model
-  summary_writer = tf.compat.v2.summary.create_file_writer(model_dir + '/train')
+  # Get the appropriate filepath (temporary or not) based on whether the worker
+  # is the chief.
+  summary_writer_filepath = _get_filepath(strategy,
+                                          os.path.join(model_dir, 'train'))
+  summary_writer = tf.compat.v2.summary.create_file_writer(
+      summary_writer_filepath)
+
+  if use_tpu:
+    num_steps_per_iteration = 100
+  else:
+    # TODO(b/135933080) Explore setting to 100 when GPU performance issues
+    # are fixed.
+    num_steps_per_iteration = 1
+
   with summary_writer.as_default():
     with strategy.scope():
-      # Load a fine-tuning checkpoint.
-      if fine_tune_checkpoint_path:
-        load_fine_tune_checkpoint(detection_model, fine_tune_checkpoint_path,
-                                  fine_tune_checkpoint_type,
-                                  load_all_detection_checkpoint_vars,
-                                  train_input,
-                                  unpad_groundtruth_tensors)
-
-      ckpt = tf.compat.v2.train.Checkpoint(
-          step=global_step, model=detection_model, optimizer=optimizer)
-      manager = tf.compat.v2.train.CheckpointManager(
-          ckpt, model_dir, max_to_keep=7)
-      ckpt.restore(manager.latest_checkpoint)
-
-      def train_step_fn(features, labels):
-        return eager_train_step(
-            detection_model,
-            features,
-            labels,
-            unpad_groundtruth_tensors,
-            optimizer,
-            learning_rate=learning_rate_fn(),
-            add_regularization_loss=add_regularization_loss,
-            clip_gradients_value=clip_gradients_value,
-            global_step=global_step,
-            num_replicas=strategy.num_replicas_in_sync)
-
-      @tf.function
-      def _dist_train_step(data_iterator):
-        """A distributed train step."""
-        features, labels = data_iterator.next()
-        per_replica_losses = strategy.experimental_run_v2(
-            train_step_fn, args=(
-                features,
-                labels,
-            ))
-        # TODO(anjalisridhar): explore if it is safe to remove the
-        ## num_replicas scaling of the loss and switch this to a ReduceOp.Mean
-        mean_loss = strategy.reduce(
-            tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)
-        return mean_loss
-
-      train_input_iter = iter(train_input)
-      for _ in range(train_steps - global_step.value()):
-        start_time = time.time()
-
-        loss = _dist_train_step(train_input_iter)
-        global_step.assign_add(1)
-        end_time = time.time()
-
-        tf.compat.v2.summary.scalar(
-            'steps_per_sec', 1.0 / (end_time - start_time),
-            step=global_step)
-        if (int(global_step.value()) % 100) == 0:
-          tf.logging.info(
-              'Step {} time taken {:.3f}s loss={:.3f}'.format(
-                  global_step.value(), end_time - start_time, loss))
-
-        if int(global_step.value()) % checkpoint_every_n == 0:
-          manager.save()
+      with tf.compat.v2.summary.record_if(
+          lambda: global_step % num_steps_per_iteration == 0):
+        # Load a fine-tuning checkpoint.
+        if fine_tune_checkpoint_path:
+          load_fine_tune_checkpoint(detection_model, fine_tune_checkpoint_path,
+                                    fine_tune_checkpoint_type,
+                                    fine_tune_checkpoint_version,
+                                    load_all_detection_checkpoint_vars,
+                                    train_input,
+                                    unpad_groundtruth_tensors)
+
+        ckpt = tf.compat.v2.train.Checkpoint(
+            step=global_step, model=detection_model, optimizer=optimizer)
+
+        manager_dir = _get_filepath(strategy, model_dir)
+        if not strategy.extended.should_checkpoint:
+          checkpoint_max_to_keep = 1
+        manager = tf.compat.v2.train.CheckpointManager(
+            ckpt, manager_dir, max_to_keep=checkpoint_max_to_keep)
+
+        # We use the following instead of manager.latest_checkpoint because
+        # manager_dir does not point to the model directory when we are running
+        # in a worker.
+        latest_checkpoint = tf.train.latest_checkpoint(model_dir)
+        ckpt.restore(latest_checkpoint)
+
+        def train_step_fn(features, labels):
+          """Single train step."""
+          loss = eager_train_step(
+              detection_model,
+              features,
+              labels,
+              unpad_groundtruth_tensors,
+              optimizer,
+              learning_rate=learning_rate_fn(),
+              add_regularization_loss=add_regularization_loss,
+              clip_gradients_value=clip_gradients_value,
+              global_step=global_step,
+              num_replicas=strategy.num_replicas_in_sync)
+          global_step.assign_add(1)
+          return loss
+
+        def _sample_and_train(strategy, train_step_fn, data_iterator):
+          features, labels = data_iterator.next()
+          per_replica_losses = strategy.run(
+              train_step_fn, args=(features, labels))
+          # TODO(anjalisridhar): explore if it is safe to remove the
+          ## num_replicas scaling of the loss and switch this to a ReduceOp.Mean
+          return strategy.reduce(tf.distribute.ReduceOp.SUM,
+                                 per_replica_losses, axis=None)
+
+        @tf.function
+        def _dist_train_step(data_iterator):
+          """A distributed train step."""
+
+          if num_steps_per_iteration > 1:
+            for _ in tf.range(num_steps_per_iteration - 1):
+              _sample_and_train(strategy, train_step_fn, data_iterator)
+
+          return _sample_and_train(strategy, train_step_fn, data_iterator)
+
+        train_input_iter = iter(train_input)
+        checkpointed_step = int(global_step.value())
+        logged_step = global_step.value()
+
+        last_step_time = time.time()
+        for _ in range(global_step.value(), train_steps,
+                       num_steps_per_iteration):
+
+          loss = _dist_train_step(train_input_iter)
+
+          time_taken = time.time() - last_step_time
+          last_step_time = time.time()
+
+          tf.compat.v2.summary.scalar(
+              'steps_per_sec', num_steps_per_iteration * 1.0 / time_taken,
+              step=global_step)
+
+          if global_step.value() - logged_step >= 100:
+            tf.logging.info(
+                'Step {} per-step time {:.3f}s loss={:.3f}'.format(
+                    global_step.value(), time_taken / num_steps_per_iteration,
+                    loss))
+            logged_step = global_step.value()
+
+          if ((int(global_step.value()) - checkpointed_step) >=
+              checkpoint_every_n):
+            manager.save()
+            checkpointed_step = int(global_step.value())
+
+  # Remove the checkpoint directories of the non-chief workers that
+  # MultiWorkerMirroredStrategy forces us to save during sync distributed
+  # training.
+  _clean_temporary_directories(strategy, manager_dir)
+  _clean_temporary_directories(strategy, summary_writer_filepath)
 
 
 def eager_eval_loop(
@@ -509,7 +655,7 @@ def eager_eval_loop(
 
   This method will compute the evaluation metrics specified in the configs on
   the entire evaluation dataset, then return the metrics. It will also log
-  the metrics to TensorBoard
+  the metrics to TensorBoard.
 
   Args:
     detection_model: A DetectionModel (based on Keras) to evaluate.
@@ -578,7 +724,7 @@ def eager_eval_loop(
     # TODO(kaftan): Depending on how postprocessing will work for TPUS w/
     ## TPUStrategy, may be good to move wrapping to a utility method
     if use_tpu and postprocess_on_cpu:
-      detections = tf.contrib.tpu.outside_compilation(
+      detections = contrib_tpu.outside_compilation(
           postprocess_wrapper,
           (prediction_dict, features[fields.InputDataFields.true_image_shape]))
     else:
@@ -621,6 +767,36 @@ def eager_eval_loop(
     if i % 100 == 0:
       tf.logging.info('Finished eval step %d', i)
 
+    use_original_images = fields.InputDataFields.original_image in features
+    if not use_tpu and use_original_images:
+      # Summary for input images.
+      tf.compat.v2.summary.image(
+          name='eval_input_images',
+          step=global_step,
+          data=eval_dict['original_image'],
+          max_outputs=1)
+      # Summary for prediction/groundtruth side-by-side images.
+      if class_agnostic:
+        category_index = label_map_util.create_class_agnostic_category_index()
+      else:
+        category_index = label_map_util.create_category_index_from_labelmap(
+            eval_input_config.label_map_path)
+      keypoint_edges = [
+          (kp.start, kp.end) for kp in eval_config.keypoint_edge]
+      sbys_image_list = vutils.draw_side_by_side_evaluation_image(
+          eval_dict,
+          category_index=category_index,
+          max_boxes_to_draw=eval_config.max_num_boxes_to_visualize,
+          min_score_thresh=eval_config.min_score_threshold,
+          use_normalized_coordinates=False,
+          keypoint_edges=keypoint_edges or None)
+      sbys_images = tf.concat(sbys_image_list, axis=0)
+      tf.compat.v2.summary.image(
+          name='eval_side_by_side',
+          step=global_step,
+          data=sbys_images,
+          max_outputs=eval_config.num_visualizations)
+
     if evaluators is None:
       if class_agnostic:
         evaluators = class_agnostic_evaluators
@@ -633,6 +809,11 @@ def eager_eval_loop(
     for loss_key, loss_tensor in iter(losses_dict.items()):
       if loss_key not in loss_metrics:
         loss_metrics[loss_key] = tf.keras.metrics.Mean()
+      # Skip the loss with value equal or lower than 0.0 when calculating the
+      # average loss since they don't usually reflect the normal loss values
+      # causing spurious average loss value.
+      if loss_tensor <= 0.0:
+        continue
       loss_metrics[loss_key].update_state(loss_tensor)
 
   eval_metrics = {}
@@ -663,6 +844,7 @@ def eval_continuously(
     model_dir=None,
     checkpoint_dir=None,
     wait_interval=180,
+    timeout=3600,
     **kwargs):
   """Run continuous evaluation of a detection model eagerly.
 
@@ -691,13 +873,13 @@ def eval_continuously(
       `export_savedmodel()` exports a metagraph for serving on TPU besides the
       one on CPU. If export_to_tpu is not provided, we will look for it in
       hparams too.
-    model_dir:
-      Directory to output resulting evaluation summaries to.
-    checkpoint_dir:
-      Directory that contains the training checkpoints.
-    wait_interval:
-      Terminate evaluation in no new checkpoints arrive within this wait
-      interval (in seconds).
+    model_dir: Directory to output resulting evaluation summaries to.
+    checkpoint_dir: Directory that contains the training checkpoints.
+    wait_interval: The mimmum number of seconds to wait before checking for a
+      new checkpoint.
+    timeout: The maximum number of seconds to wait for a checkpoint. Execution
+      will terminate if no new checkpoints are found after these many seconds.
+
     **kwargs: Additional keyword arguments for configuration override.
   """
   get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[
@@ -759,45 +941,21 @@ def eval_continuously(
   global_step = tf.compat.v2.Variable(
       0, trainable=False, dtype=tf.compat.v2.dtypes.int64)
 
-  prev_checkpoint = None
-  waiting = False
-  while True:
+  for latest_checkpoint in tf.train.checkpoints_iterator(
+      checkpoint_dir, timeout=timeout, min_interval_secs=wait_interval):
     ckpt = tf.compat.v2.train.Checkpoint(
         step=global_step, model=detection_model)
-    manager = tf.compat.v2.train.CheckpointManager(
-        ckpt, checkpoint_dir, max_to_keep=3)
-
-    latest_checkpoint = manager.latest_checkpoint
-    if prev_checkpoint == latest_checkpoint:
-      if prev_checkpoint is None:
-        tf.logging.info('No checkpoints found yet. Trying again in %s seconds.'
-                        % wait_interval)
-        time.sleep(wait_interval)
-      else:
-        if waiting:
-          tf.logging.info('Terminating eval after %s seconds of no new '
-                          'checkpoints.' % wait_interval)
-          break
-        else:
-          tf.logging.info('No new checkpoint found. Will try again '
-                          'in %s seconds and terminate if no checkpoint '
-                          'appears.' % wait_interval)
-          waiting = True
-          time.sleep(wait_interval)
-    else:
-      tf.logging.info('New checkpoint found. Starting evaluation.')
-      waiting = False
-      prev_checkpoint = latest_checkpoint
-      ckpt.restore(latest_checkpoint)
-
-      for eval_name, eval_input in eval_inputs:
-        summary_writer = tf.compat.v2.summary.create_file_writer(
-            model_dir + '/eval' + eval_name)
-        with summary_writer.as_default():
-          eager_eval_loop(
-              detection_model,
-              configs,
-              eval_input,
-              use_tpu=use_tpu,
-              postprocess_on_cpu=postprocess_on_cpu,
-              global_step=global_step)
+
+    ckpt.restore(latest_checkpoint).expect_partial()
+
+    for eval_name, eval_input in eval_inputs:
+      summary_writer = tf.compat.v2.summary.create_file_writer(
+          model_dir + '/eval' + eval_name)
+      with summary_writer.as_default():
+        eager_eval_loop(
+            detection_model,
+            configs,
+            eval_input,
+            use_tpu=use_tpu,
+            postprocess_on_cpu=postprocess_on_cpu,
+            global_step=global_step)
diff --git a/research/object_detection/model_lib_v2_test.py b/research/object_detection/model_lib_v2_test.py
index af650a32..20fce37a 100644
--- a/research/object_detection/model_lib_v2_test.py
+++ b/research/object_detection/model_lib_v2_test.py
@@ -19,13 +19,24 @@ from __future__ import division
 from __future__ import print_function
 
 import os
+import tempfile
 
+import numpy as np
+import six
 import tensorflow as tf
 
+from object_detection import inputs
 from object_detection import model_hparams
 from object_detection import model_lib_v2
+from object_detection.builders import model_builder
+from object_detection.core import model
+from object_detection.protos import train_pb2
 from object_detection.utils import config_util
 
+if six.PY2:
+  import mock  # pylint: disable=g-importing-member,g-import-not-at-top
+else:
+  from unittest import mock  # pylint: disable=g-importing-member,g-import-not-at-top
 
 # Model for test. Current options are:
 # 'ssd_mobilenet_v2_pets_keras'
@@ -61,19 +72,10 @@ def _get_config_kwarg_overrides():
   }
 
 
-def _get_configs_for_model(model_name):
-  """Returns configurations for model."""
-  filename = get_pipeline_config_path(model_name)
-  configs = config_util.get_configs_from_pipeline_file(filename)
-  configs = config_util.merge_external_params_with_configs(
-      configs, kwargs_dict=_get_config_kwarg_overrides())
-  return configs
-
-
 class ModelLibTest(tf.test.TestCase):
 
   @classmethod
-  def setUpClass(cls):
+  def setUpClass(cls):  # pylint:disable=g-missing-super-call
     tf.keras.backend.clear_session()
 
   def test_train_loop_then_eval_loop(self):
@@ -99,6 +101,119 @@ class ModelLibTest(tf.test.TestCase):
         model_dir=model_dir,
         checkpoint_dir=model_dir,
         train_steps=train_steps,
-        wait_interval=10,
+        wait_interval=1,
+        timeout=10,
         **config_kwarg_overrides)
 
+
+class SimpleModel(model.DetectionModel):
+  """A model with a single weight vector."""
+
+  def __init__(self, num_classes=1):
+    super(SimpleModel, self).__init__(num_classes)
+    self.weight = tf.keras.backend.variable(np.ones(10), name='weight')
+
+  def postprocess(self, prediction_dict, true_image_shapes):
+    return {}
+
+  def updates(self):
+    return []
+
+  def restore_map(self, *args, **kwargs):
+    return {'model': self}
+
+  def preprocess(self, _):
+    return tf.zeros((1, 128, 128, 3)), tf.constant([[128, 128, 3]])
+
+  def provide_groundtruth(self, *args, **kwargs):
+    pass
+
+  def predict(self, pred_inputs, true_image_shapes):
+    return {'prediction':
+            tf.abs(tf.reduce_sum(self.weight) * tf.reduce_sum(pred_inputs))}
+
+  def loss(self, prediction_dict, _):
+    return {'loss': tf.reduce_sum(prediction_dict['prediction'])}
+
+  def regularization_losses(self):
+    return []
+
+
+class ModelCheckpointTest(tf.test.TestCase):
+  """Test for model checkpoint related functionality."""
+
+  def test_checkpoint_max_to_keep(self):
+    """Test that only the most recent checkpoints are kept."""
+
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = SimpleModel()
+
+      hparams = model_hparams.create_hparams(
+          hparams_overrides='load_pretrained=false')
+      pipeline_config_path = get_pipeline_config_path(MODEL_NAME_FOR_TEST)
+      config_kwarg_overrides = _get_config_kwarg_overrides()
+      model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())
+
+      model_lib_v2.train_loop(
+          hparams, pipeline_config_path, model_dir=model_dir,
+          train_steps=20, checkpoint_every_n=2, checkpoint_max_to_keep=3,
+          **config_kwarg_overrides
+      )
+      ckpt_files = tf.io.gfile.glob(os.path.join(model_dir, 'ckpt-*.index'))
+      self.assertEqual(len(ckpt_files), 3,
+                       '{} not of length 3.'.format(ckpt_files))
+
+
+class IncompatibleModel(SimpleModel):
+
+  def restore_map(self, *args, **kwargs):
+    return {'weight': self.weight}
+
+
+class CheckpointV2Test(tf.test.TestCase):
+
+  def setUp(self):
+    super(CheckpointV2Test, self).setUp()
+
+    self._model = SimpleModel()
+    tf.keras.backend.set_value(self._model.weight, np.ones(10) * 42)
+    ckpt = tf.train.Checkpoint(model=self._model)
+
+    self._test_dir = tf.test.get_temp_dir()
+    self._ckpt_path = ckpt.save(os.path.join(self._test_dir, 'ckpt'))
+    tf.keras.backend.set_value(self._model.weight, np.ones(10))
+
+    pipeline_config_path = get_pipeline_config_path(MODEL_NAME_FOR_TEST)
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, kwargs_dict=_get_config_kwarg_overrides())
+    self._train_input_fn = inputs.create_train_input_fn(
+        configs['train_config'],
+        configs['train_input_config'],
+        configs['model'])
+
+  def test_restore_v2(self):
+    """Test that restoring a v2 style checkpoint works."""
+
+    model_lib_v2.load_fine_tune_checkpoint(
+        self._model, self._ckpt_path, checkpoint_type='',
+        checkpoint_version=train_pb2.CheckpointVersion.V2,
+        load_all_detection_checkpoint_vars=True,
+        input_dataset=self._train_input_fn(),
+        unpad_groundtruth_tensors=True)
+    np.testing.assert_allclose(self._model.weight.numpy(), 42)
+
+  def test_restore_map_incompatible_error(self):
+    """Test that restoring an incompatible restore map causes an error."""
+
+    with self.assertRaisesRegex(TypeError,
+                                r'.*received a \(str -> ResourceVariable\).*'):
+      model_lib_v2.load_fine_tune_checkpoint(
+          IncompatibleModel(), self._ckpt_path, checkpoint_type='',
+          checkpoint_version=train_pb2.CheckpointVersion.V2,
+          load_all_detection_checkpoint_vars=True,
+          input_dataset=self._train_input_fn(),
+          unpad_groundtruth_tensors=True)
+
+
diff --git a/research/object_detection/model_tpu_main.py b/research/object_detection/model_tpu_main.py
index 67a62fe3..94d7e9f1 100644
--- a/research/object_detection/model_tpu_main.py
+++ b/research/object_detection/model_tpu_main.py
@@ -29,6 +29,15 @@ import tensorflow as tf
 from object_detection import model_hparams
 from object_detection import model_lib
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import cluster_resolver as contrib_cluster_resolver
+  from tensorflow.contrib import tpu as contrib_tpu
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 tf.flags.DEFINE_bool('use_tpu', True, 'Use TPUs rather than plain CPUs')
 
 # Cloud TPU Cluster Resolvers
@@ -85,17 +94,15 @@ def main(unused_argv):
   flags.mark_flag_as_required('pipeline_config_path')
 
   tpu_cluster_resolver = (
-      tf.contrib.cluster_resolver.TPUClusterResolver(
-          tpu=[FLAGS.tpu_name],
-          zone=FLAGS.tpu_zone,
-          project=FLAGS.gcp_project))
+      contrib_cluster_resolver.TPUClusterResolver(
+          tpu=[FLAGS.tpu_name], zone=FLAGS.tpu_zone, project=FLAGS.gcp_project))
   tpu_grpc_url = tpu_cluster_resolver.get_master()
 
-  config = tf.contrib.tpu.RunConfig(
+  config = contrib_tpu.RunConfig(
       master=tpu_grpc_url,
       evaluation_master=tpu_grpc_url,
       model_dir=FLAGS.model_dir,
-      tpu_config=tf.contrib.tpu.TPUConfig(
+      tpu_config=contrib_tpu.TPUConfig(
           iterations_per_loop=FLAGS.iterations_per_loop,
           num_shards=FLAGS.num_shards))
 
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index b3eeb672..8e272720 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -163,4 +164,4 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             insert_1x1_conv=True,
             image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
index 5a9ff48c..a9f15306 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
@@ -58,6 +58,7 @@ class FasterRCNNInceptionResnetV2KerasFeatureExtractor(
     super(FasterRCNNInceptionResnetV2KerasFeatureExtractor, self).__init__(
         is_training, first_stage_features_stride, batch_norm_trainable,
         weight_decay)
+    self._variable_dict = {}
 
   def preprocess(self, resized_inputs):
     """Faster R-CNN with Inception Resnet v2 preprocessing.
@@ -105,9 +106,12 @@ class FasterRCNNInceptionResnetV2KerasFeatureExtractor(
               include_top=False)
         proposal_features = model.get_layer(
             name='block17_20_ac').output
-        return tf.keras.Model(
+        keras_model = tf.keras.Model(
             inputs=model.inputs,
             outputs=proposal_features)
+        for variable in keras_model.variables:
+          self._variable_dict[variable.name[:-2]] = variable
+        return keras_model
 
   def get_box_classifier_feature_extractor_model(self, name=None):
     """Returns a model that extracts second stage box classifier features.
@@ -143,10 +147,13 @@ class FasterRCNNInceptionResnetV2KerasFeatureExtractor(
         proposal_classifier_features = model.get_layer(
             name='conv_7b_ac').output
 
-        return model_util.extract_submodel(
+        keras_model = model_util.extract_submodel(
             model=model,
             inputs=proposal_feature_maps,
             outputs=proposal_classifier_features)
+        for variable in keras_model.variables:
+          self._variable_dict[variable.name[:-2]] = variable
+        return keras_model
 
   def restore_from_classification_checkpoint_fn(
       self,
@@ -1071,9 +1078,16 @@ class FasterRCNNInceptionResnetV2KerasFeatureExtractor(
     }
 
     variables_to_restore = {}
-    for variable in variables_helper.get_global_variables_safely():
-      var_name = keras_to_slim_name_mapping.get(variable.op.name)
-      if var_name:
-        variables_to_restore[var_name] = variable
+    if tf.executing_eagerly():
+      for key in self._variable_dict:
+        # variable.name includes ":0" at the end, but the names in the
+        # checkpoint do not have the suffix ":0". So, we strip it here.
+        var_name = keras_to_slim_name_mapping.get(key)
+        if var_name:
+          variables_to_restore[var_name] = self._variable_dict[key]
+    else:
+      for variable in variables_helper.get_global_variables_safely():
+        var_name = keras_to_slim_name_mapping.get(variable.op.name)
+        if var_name:
+          variables_to_restore[var_name] = variable
     return variables_to_restore
-
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
index aec57a1f..e765af11 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -20,6 +21,11 @@ Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le
 https://arxiv.org/abs/1707.07012
 """
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import framework as contrib_framework
 from tensorflow.contrib import slim as contrib_slim
diff --git a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
index 60016ed0..5263839b 100644
--- a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -18,6 +19,11 @@
 Based on PNASNet model: https://arxiv.org/abs/1712.00559
 """
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import framework as contrib_framework
 from tensorflow.contrib import slim as contrib_slim
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 0421c8f9..2af3ba66 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -23,8 +24,13 @@ Object detection feature extractors usually are built by stacking two components
 Feature map generators build on the base feature extractors and produce a list
 of final feature maps.
 """
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 import collections
 import functools
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 from object_detection.utils import ops
@@ -221,8 +227,8 @@ class KerasMultiResolutionFeatureMaps(tf.keras.Model):
       else:
         if insert_1x1_conv:
           layer_name = '{}_1_Conv2d_{}_1x1_{}'.format(
-              base_from_layer, index, depth_fn(layer_depth / 2))
-          net.append(tf.keras.layers.Conv2D(depth_fn(layer_depth / 2),
+              base_from_layer, index, depth_fn(layer_depth // 2))
+          net.append(tf.keras.layers.Conv2D(depth_fn(layer_depth // 2),
                                             [1, 1],
                                             padding='SAME',
                                             strides=1,
@@ -431,10 +437,10 @@ def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
       intermediate_layer = pre_layer
       if insert_1x1_conv:
         layer_name = '{}_1_Conv2d_{}_1x1_{}'.format(
-            base_from_layer, index, depth_fn(layer_depth / 2))
+            base_from_layer, index, depth_fn(layer_depth // 2))
         intermediate_layer = slim.conv2d(
             pre_layer,
-            depth_fn(layer_depth / 2), [1, 1],
+            depth_fn(layer_depth // 2), [1, 1],
             padding='SAME',
             stride=1,
             scope=layer_name)
@@ -547,7 +553,7 @@ class KerasFpnTopDownFeatureMaps(tf.keras.Model):
       self.top_layers.append(tf.keras.layers.Lambda(
           clip_by_value, name='clip_by_value'))
 
-    for level in reversed(range(num_levels - 1)):
+    for level in reversed(list(range(num_levels - 1))):
       # to generate residual from image features
       residual_net = []
       # to preprocess top_down (the image feature map from last layer)
@@ -636,7 +642,7 @@ class KerasFpnTopDownFeatureMaps(tf.keras.Model):
       output_feature_map_keys.append('top_down_%s' % image_features[-1][0])
 
       num_levels = len(image_features)
-      for index, level in enumerate(reversed(range(num_levels - 1))):
+      for index, level in enumerate(reversed(list(range(num_levels - 1)))):
         residual = image_features[level][1]
         top_down = output_feature_maps_list[-1]
         for layer in self.residual_blocks[index]:
@@ -703,7 +709,7 @@ def fpn_top_down_feature_maps(image_features,
       output_feature_map_keys.append(
           'top_down_%s' % image_features[-1][0])
 
-      for level in reversed(range(num_levels - 1)):
+      for level in reversed(list(range(num_levels - 1))):
         if use_native_resize_op:
           with tf.name_scope('nearest_neighbor_upsampling'):
             top_down_shape = shape_utils.combined_static_and_dynamic_shape(
@@ -731,10 +737,11 @@ def fpn_top_down_feature_maps(image_features,
           conv_op = functools.partial(slim.separable_conv2d, depth_multiplier=1)
         else:
           conv_op = slim.conv2d
+        pre_output = top_down
         if use_explicit_padding:
-          top_down = ops.fixed_padding(top_down, kernel_size)
+          pre_output = ops.fixed_padding(pre_output, kernel_size)
         output_feature_maps_list.append(conv_op(
-            top_down,
+            pre_output,
             depth, [kernel_size, kernel_size],
             scope='smoothing_%d' % (level + 1)))
         output_feature_map_keys.append('top_down_%s' % image_features[level][0])
@@ -778,7 +785,7 @@ def pooling_pyramid_feature_maps(base_feature_map_depth, num_layers,
   """
   if len(image_features) != 1:
     raise ValueError('image_features should be a dictionary of length 1.')
-  image_features = image_features[image_features.keys()[0]]
+  image_features = image_features[list(image_features.keys())[0]]
 
   feature_map_keys = []
   feature_maps = []
diff --git a/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
index 28e5b69c..beaabb81 100644
--- a/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
@@ -1,4 +1,3 @@
-
 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
diff --git a/research/object_detection/models/keras_models/hourglass_network.py b/research/object_detection/models/keras_models/hourglass_network.py
new file mode 100644
index 00000000..d216b166
--- /dev/null
+++ b/research/object_detection/models/keras_models/hourglass_network.py
@@ -0,0 +1,376 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""The Hourglass[1] network.
+
+[1]: https://arxiv.org/abs/1603.06937
+"""
+
+
+import tensorflow.compat.v2 as tf
+
+
+BATCH_NORM_EPSILON = 1e-5
+BATCH_NORM_MOMENTUM = 0.1
+BATCH_NORM_FUSED = True
+
+
+class IdentityLayer(tf.keras.layers.Layer):
+  """A layer which passes through the input as it is."""
+
+  def call(self, inputs):
+    return inputs
+
+
+def _get_padding_for_kernel_size(kernel_size):
+  if kernel_size == 7:
+    return (3, 3)
+  elif kernel_size == 3:
+    return (1, 1)
+  else:
+    raise ValueError('Padding for kernel size {} not known.'.format(
+        kernel_size))
+
+
+class ConvolutionalBlock(tf.keras.layers.Layer):
+  """Block that aggregates Convolution + Norm layer + ReLU."""
+
+  def __init__(self, kernel_size, out_channels, stride=1, relu=True,
+               padding='same'):
+    """Initializes the Convolutional block.
+
+    Args:
+      kernel_size: int, convolution kernel size.
+      out_channels: int, the desired number of output channels.
+      stride: Integer, stride used in the convolution.
+      relu: bool, whether to use relu at the end of the layer.
+      padding: str, the padding scheme to use when kernel_size <= 1
+    """
+    super(ConvolutionalBlock, self).__init__()
+
+    if kernel_size > 1:
+      padding = 'valid'
+      padding_size = _get_padding_for_kernel_size(kernel_size)
+
+      # TODO(vighneshb) Explore if removing and using padding option in conv
+      # layer works.
+      self.pad = tf.keras.layers.ZeroPadding2D(padding_size)
+    else:
+      self.pad = IdentityLayer()
+
+    self.conv = tf.keras.layers.Conv2D(
+        filters=out_channels, kernel_size=kernel_size, use_bias=False,
+        strides=stride, padding=padding)
+
+    self.norm = tf.keras.layers.experimental.SyncBatchNormalization(
+        name='batchnorm', epsilon=1e-5, momentum=0.1)
+
+    if relu:
+      self.relu = tf.keras.layers.ReLU()
+    else:
+      self.relu = IdentityLayer()
+
+  def call(self, inputs):
+    net = self.pad(inputs)
+    net = self.conv(net)
+    net = self.norm(net)
+    return self.relu(net)
+
+
+class SkipConvolution(ConvolutionalBlock):
+  """The skip connection layer for a ResNet."""
+
+  def __init__(self, out_channels, stride):
+    """Initializes the skip convolution layer.
+
+    Args:
+      out_channels: int, the desired number of output channels.
+      stride: int, the stride for the layer.
+    """
+    super(SkipConvolution, self).__init__(
+        out_channels=out_channels, kernel_size=1, stride=stride, relu=False)
+
+
+class ResidualBlock(tf.keras.layers.Layer):
+  """A Residual block."""
+
+  def __init__(self, out_channels, skip_conv=False, kernel_size=3, stride=1,
+               padding='same'):
+    """Initializes the Residual block.
+
+    Args:
+      out_channels: int, the desired number of output channels.
+      skip_conv: bool, whether to use a conv layer for skip connections.
+      kernel_size: int, convolution kernel size.
+      stride: Integer, stride used in the convolution.
+      padding: str, the type of padding to use.
+    """
+
+    super(ResidualBlock, self).__init__()
+    self.conv_block = ConvolutionalBlock(
+        kernel_size=kernel_size, out_channels=out_channels, stride=stride)
+
+    self.conv = tf.keras.layers.Conv2D(
+        filters=out_channels, kernel_size=kernel_size, use_bias=False,
+        strides=1, padding=padding)
+    self.norm = tf.keras.layers.experimental.SyncBatchNormalization(
+        name='batchnorm', epsilon=1e-5, momentum=0.1)
+
+    if skip_conv:
+      self.skip = SkipConvolution(out_channels=out_channels,
+                                  stride=stride)
+    else:
+      self.skip = IdentityLayer()
+
+    self.relu = tf.keras.layers.ReLU()
+
+  def call(self, inputs):
+    net = self.conv_block(inputs)
+    net = self.conv(net)
+    net = self.norm(net)
+    net_skip = self.skip(inputs)
+    return self.relu(net + net_skip)
+
+
+class InputDownsampleBlock(tf.keras.layers.Layer):
+  """Block for the initial feature downsampling."""
+
+  def __init__(self, out_channels_initial_conv, out_channels_residual_block):
+    """Initializes the downsample block.
+
+    Args:
+      out_channels_initial_conv: int, the desired number of output channels
+        in the initial conv layer.
+      out_channels_residual_block: int, the desired number of output channels
+        in the underlying residual block.
+    """
+
+    super(InputDownsampleBlock, self).__init__()
+    self.conv_block = ConvolutionalBlock(
+        kernel_size=7, out_channels=out_channels_initial_conv, stride=2,
+        padding='valid')
+    self.residual_block = ResidualBlock(
+        out_channels=out_channels_residual_block, stride=2, skip_conv=True)
+
+  def call(self, inputs):
+    return self.residual_block(self.conv_block(inputs))
+
+
+def _make_repeated_residual_blocks(out_channels, num_blocks,
+                                   initial_stride=1, residual_channels=None):
+  """Stack Residual blocks one after the other.
+
+  Args:
+    out_channels: int, the desired number of output channels.
+    num_blocks: int, the number of residual blocks to be stacked.
+    initial_stride: int, the stride of the initial residual block.
+    residual_channels: int, the desired number of output channels in the
+      intermediate residual blocks. If not specifed, we use out_channels.
+
+  Returns:
+    blocks: A list of residual blocks to be applied in sequence.
+
+  """
+
+  blocks = []
+
+  if residual_channels is None:
+    residual_channels = out_channels
+
+  for i in range(num_blocks - 1):
+    stride = initial_stride if i == 0 else 1
+    skip_conv = stride > 1
+
+    blocks.append(
+        ResidualBlock(out_channels=residual_channels, stride=stride,
+                      skip_conv=skip_conv)
+    )
+
+  skip_conv = residual_channels != out_channels
+  blocks.append(ResidualBlock(out_channels=out_channels, skip_conv=skip_conv))
+
+  return blocks
+
+
+def _apply_blocks(inputs, blocks):
+  net = inputs
+
+  for block in blocks:
+    net = block(net)
+
+  return net
+
+
+class EncoderDecoderBlock(tf.keras.layers.Layer):
+  """An encoder-decoder block which recursively defines the hourglass network."""
+
+  def __init__(self, num_stages, channel_dims, blocks_per_stage):
+    """Initializes the encoder-decoder block.
+
+    Args:
+      num_stages: int, Number of stages in the network. At each stage we have 2
+        encoder and 1 decoder blocks. The second encoder block downsamples the
+        input.
+      channel_dims: int list, the output channels dimensions of stages in
+        the network. `channel_dims[0]` is used to define the number of
+        channels in the first encoder block and `channel_dims[1]` is used to
+        define the number of channels in the second encoder block. The channels
+        in the recursive inner layers are defined using `channel_dims[1:]`
+      blocks_per_stage: int list, number of residual blocks to use at each
+        stage. `blocks_per_stage[0]` defines the number of blocks at the
+        current stage and `blocks_per_stage[1:]` is used at further stages.
+    """
+
+    super(EncoderDecoderBlock, self).__init__()
+
+    out_channels = channel_dims[0]
+    out_channels_downsampled = channel_dims[1]
+
+    self.encoder_block1 = _make_repeated_residual_blocks(
+        out_channels=out_channels, num_blocks=blocks_per_stage[0],
+        initial_stride=1)
+    self.encoder_block2 = _make_repeated_residual_blocks(
+        out_channels=out_channels_downsampled,
+        num_blocks=blocks_per_stage[0], initial_stride=2)
+
+    if num_stages > 1:
+      self.inner_block = [
+          EncoderDecoderBlock(num_stages - 1, channel_dims[1:],
+                              blocks_per_stage[1:])
+      ]
+    else:
+      self.inner_block = _make_repeated_residual_blocks(
+          out_channels=out_channels_downsampled,
+          num_blocks=blocks_per_stage[1])
+
+    self.decoder_block = _make_repeated_residual_blocks(
+        residual_channels=out_channels_downsampled,
+        out_channels=out_channels, num_blocks=blocks_per_stage[0])
+    self.upsample = tf.keras.layers.UpSampling2D(2)
+
+    self.merge_features = tf.keras.layers.Add()
+
+  def call(self, inputs):
+
+    encoded_outputs = _apply_blocks(inputs, self.encoder_block1)
+    encoded_downsampled_outputs = _apply_blocks(inputs, self.encoder_block2)
+    inner_block_outputs = _apply_blocks(
+        encoded_downsampled_outputs, self.inner_block)
+
+    decoded_outputs = _apply_blocks(inner_block_outputs, self.decoder_block)
+    upsampled_outputs = self.upsample(decoded_outputs)
+
+    return self.merge_features([encoded_outputs, upsampled_outputs])
+
+
+class HourglassNetwork(tf.keras.Model):
+  """The hourglass network."""
+
+  def __init__(self, num_stages, channel_dims, blocks_per_stage,
+               num_hourglasses):
+    """Intializes the feature extractor.
+
+    Args:
+      num_stages: int, Number of stages in the network. At each stage we have 2
+        encoder and 1 decoder blocks. The second encoder block downsamples the
+        input.
+      channel_dims: int list, the output channel dimensions of stages in
+        the network. `channel_dims[0]` and `channel_dims[1]` are used to define
+        the initial downsampling block. `channel_dims[1:]` is used to define
+        the hourglass network(s) which follow(s).
+      blocks_per_stage: int list, number of residual blocks to use at each
+        stage in the hourglass network
+      num_hourglasses: int, number of hourglas networks to stack
+        sequentially.
+    """
+
+    super(HourglassNetwork, self).__init__()
+
+    self.num_hourglasses = num_hourglasses
+    self.downsample_input = InputDownsampleBlock(
+        out_channels_initial_conv=channel_dims[0],
+        out_channels_residual_block=channel_dims[1]
+    )
+
+    self.hourglass_network = []
+    self.output_conv = []
+    for _ in range(self.num_hourglasses):
+      self.hourglass_network.append(
+          EncoderDecoderBlock(
+              num_stages=num_stages, channel_dims=channel_dims[1:],
+              blocks_per_stage=blocks_per_stage)
+      )
+      self.output_conv.append(
+          ConvolutionalBlock(kernel_size=3, out_channels=channel_dims[1])
+      )
+
+    self.intermediate_conv1 = []
+    self.intermediate_conv2 = []
+    self.intermediate_residual = []
+
+    for _ in range(self.num_hourglasses - 1):
+      self.intermediate_conv1.append(
+          ConvolutionalBlock(
+              kernel_size=1, out_channels=channel_dims[1], relu=False)
+      )
+      self.intermediate_conv2.append(
+          ConvolutionalBlock(
+              kernel_size=1, out_channels=channel_dims[1], relu=False)
+      )
+      self.intermediate_residual.append(
+          ResidualBlock(out_channels=channel_dims[1])
+      )
+
+    self.intermediate_relu = tf.keras.layers.ReLU()
+
+  def call(self, inputs):
+
+    inputs = self.downsample_input(inputs)
+    outputs = []
+
+    for i in range(self.num_hourglasses):
+
+      hourglass_output = self.hourglass_network[i](inputs)
+
+      output = self.output_conv[i](hourglass_output)
+      outputs.append(output)
+
+      if i < self.num_hourglasses - 1:
+        secondary_output = (self.intermediate_conv1[i](inputs) +
+                            self.intermediate_conv2[i](output))
+        secondary_output = self.intermediate_relu(secondary_output)
+        inputs = self.intermediate_residual[i](secondary_output)
+
+    return outputs
+
+  @property
+  def out_stride(self):
+    """The stride in the output image of the network."""
+    return 4
+
+  @property
+  def num_feature_outputs(self):
+    """Ther number of feature outputs returned by the feature extractor."""
+    return self.num_hourglasses
+
+
+def hourglass_104():
+  """The Hourglass-104 backbone."""
+
+  return HourglassNetwork(
+      channel_dims=[128, 256, 256, 384, 384, 384, 512],
+      num_hourglasses=2,
+      num_stages=5,
+      blocks_per_stage=[2, 2, 2, 2, 2, 4],
+  )
diff --git a/research/object_detection/models/keras_models/hourglass_network_test.py b/research/object_detection/models/keras_models/hourglass_network_test.py
new file mode 100644
index 00000000..b168feae
--- /dev/null
+++ b/research/object_detection/models/keras_models/hourglass_network_test.py
@@ -0,0 +1,99 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Testing the Hourglass network."""
+
+from absl.testing import parameterized
+import numpy as np
+import tensorflow as tf
+
+from object_detection.models.keras_models import hourglass_network as hourglass
+
+
+class HourglassFeatureExtractorTest(tf.test.TestCase, parameterized.TestCase):
+
+  def test_identity_layer(self):
+
+    layer = hourglass.IdentityLayer()
+    output = layer(np.zeros((2, 32, 32, 3), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 32, 32, 3))
+
+  def test_skip_conv_layer_stride_1(self):
+
+    layer = hourglass.SkipConvolution(out_channels=8, stride=1)
+    output = layer(np.zeros((2, 32, 32, 3), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 32, 32, 8))
+
+  def test_skip_conv_layer_stride_2(self):
+
+    layer = hourglass.SkipConvolution(out_channels=8, stride=2)
+    output = layer(np.zeros((2, 32, 32, 3), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 16, 16, 8))
+
+  @parameterized.parameters([{'kernel_size': 1},
+                             {'kernel_size': 3},
+                             {'kernel_size': 7}])
+  def test_conv_block(self, kernel_size):
+
+    layer = hourglass.ConvolutionalBlock(
+        out_channels=8, kernel_size=kernel_size, stride=1)
+    output = layer(np.zeros((2, 32, 32, 3), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 32, 32, 8))
+
+    layer = hourglass.ConvolutionalBlock(
+        out_channels=8, kernel_size=kernel_size, stride=2)
+    output = layer(np.zeros((2, 32, 32, 3), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 16, 16, 8))
+
+  def test_residual_block_stride_1(self):
+
+    layer = hourglass.ResidualBlock(out_channels=8, stride=1)
+    output = layer(np.zeros((2, 32, 32, 8), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 32, 32, 8))
+
+  def test_residual_block_stride_2(self):
+
+    layer = hourglass.ResidualBlock(out_channels=8, stride=2,
+                                    skip_conv=True)
+    output = layer(np.zeros((2, 32, 32, 8), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 16, 16, 8))
+
+  def test_input_downsample_block(self):
+
+    layer = hourglass.InputDownsampleBlock(
+        out_channels_initial_conv=4, out_channels_residual_block=8)
+    output = layer(np.zeros((2, 32, 32, 8), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 8, 8, 8))
+
+  def test_encoder_decoder_block(self):
+
+    layer = hourglass.EncoderDecoderBlock(
+        num_stages=4, blocks_per_stage=[2, 3, 4, 5, 6],
+        channel_dims=[4, 6, 8, 10, 12])
+    output = layer(np.zeros((2, 64, 64, 4), dtype=np.float32))
+    self.assertEqual(output.shape, (2, 64, 64, 4))
+
+  def test_hourglass_feature_extractor(self):
+
+    model = hourglass.HourglassNetwork(
+        num_stages=4, blocks_per_stage=[2, 3, 4, 5, 6],
+        channel_dims=[4, 6, 8, 10, 12, 14], num_hourglasses=2)
+    outputs = model(np.zeros((2, 64, 64, 3), dtype=np.float32))
+    self.assertEqual(outputs[0].shape, (2, 16, 16, 6))
+    self.assertEqual(outputs[1].shape, (2, 16, 16, 6))
+
+
+if __name__ == '__main__':
+  tf.enable_v2_behavior()
+  tf.test.main()
diff --git a/research/object_detection/models/keras_models/inception_resnet_v2_test.py b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
index 18801b3b..e3bcaded 100644
--- a/research/object_detection/models/keras_models/inception_resnet_v2_test.py
+++ b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -26,8 +27,12 @@ layout and the parameters of each Op to make sure the two implementations are
 consistent.
 """
 
-import itertools
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import numpy as np
+from six.moves import zip
 import tensorflow as tf
 
 from object_detection.models.keras_models import inception_resnet_v2
@@ -151,8 +156,7 @@ class InceptionResnetV2Test(test_case.TestCase):
                                   _NUM_CHANNELS).astype(np.float32)
     feature_maps = model(image_tensor)
 
-    for feature_map, layer_name in itertools.izip(
-        feature_maps, layer_names):
+    for feature_map, layer_name in zip(feature_maps, layer_names):
       endpoint_name = _KERAS_TO_SLIM_ENDPOINT_NAMES[layer_name]
       expected_shape = expected_feature_map_shape[endpoint_name]
       self.assertAllEqual(feature_map.shape, expected_shape)
diff --git a/research/object_detection/models/keras_models/mobilenet_v1.py b/research/object_detection/models/keras_models/mobilenet_v1.py
index e1bfb328..bac9b852 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1.py
@@ -118,7 +118,8 @@ class _LayersOverride(object):
     Args:
       filters: The number of filters to use for the convolution.
       kernel_size: The kernel size to specify the height and width of the 2D
-        convolution window.
+        convolution window. In this function, the kernel size is expected to
+        be pair of numbers and the numbers must be equal for this function.
       **kwargs: Keyword args specified by the Keras application for
         constructing the convolution.
 
@@ -126,7 +127,17 @@ class _LayersOverride(object):
       A one-arg callable that will either directly apply a Keras Conv2D layer to
       the input argument, or that will first pad the input then apply a Conv2D
       layer.
+
+    Raises:
+      ValueError: if kernel size is not a pair of equal
+        integers (representing a square kernel).
     """
+    if not isinstance(kernel_size, tuple):
+      raise ValueError('kernel is expected to be a tuple.')
+    if len(kernel_size) != 2:
+      raise ValueError('kernel is expected to be length two.')
+    if kernel_size[0] != kernel_size[1]:
+      raise ValueError('kernel is expected to be square.')
     layer_name = kwargs['name']
     if self._conv_defs:
       conv_filters = model_utils.get_conv_def(self._conv_defs, layer_name)
@@ -144,7 +155,7 @@ class _LayersOverride(object):
       kwargs['kernel_initializer'] = self.initializer
 
     kwargs['padding'] = 'same'
-    if self._use_explicit_padding and kernel_size > 1:
+    if self._use_explicit_padding and kernel_size[0] > 1:
       kwargs['padding'] = 'valid'
       def padded_conv(features):  # pylint: disable=invalid-name
         padded_features = self._FixedPaddingLayer(kernel_size)(features)
diff --git a/research/object_detection/models/keras_models/mobilenet_v1_test.py b/research/object_detection/models/keras_models/mobilenet_v1_test.py
index 9e1d3498..7563ff00 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -25,8 +26,12 @@ layout and the parameters of each Op to make sure the two implementations are
 consistent.
 """
 
-import itertools
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import numpy as np
+from six.moves import zip
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -127,8 +132,8 @@ class MobilenetV1Test(test_case.TestCase):
                                   _NUM_CHANNELS).astype(np.float32)
     feature_maps = self.execute(graph_fn, [image_tensor])
 
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shape):
+    for feature_map, expected_shape in zip(feature_maps,
+                                           expected_feature_map_shape):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
   def _check_returns_correct_shapes_with_dynamic_inputs(
@@ -150,8 +155,8 @@ class MobilenetV1Test(test_case.TestCase):
         np.array(image_width, dtype=np.int32)
     ])
 
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shape):
+    for feature_map, expected_shape in zip(feature_maps,
+                                           expected_feature_map_shape):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
   def _get_variables(self, depth_multiplier, layer_names=None):
diff --git a/research/object_detection/models/keras_models/mobilenet_v2_test.py b/research/object_detection/models/keras_models/mobilenet_v2_test.py
index 5ec8aae5..e64f08a6 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,8 +15,12 @@
 # ==============================================================================
 
 """Tests for mobilenet_v2."""
-import itertools
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import numpy as np
+from six.moves import zip
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -116,8 +121,8 @@ class MobilenetV2Test(test_case.TestCase):
                                   3).astype(np.float32)
     feature_maps = self.execute(graph_fn, [image_tensor])
 
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shapes):
+    for feature_map, expected_shape in zip(feature_maps,
+                                           expected_feature_map_shapes):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
   def _check_returns_correct_shapes_with_dynamic_inputs(
@@ -138,8 +143,8 @@ class MobilenetV2Test(test_case.TestCase):
         np.array(image_width, dtype=np.int32)
     ])
 
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shapes):
+    for feature_map, expected_shape in zip(feature_maps,
+                                           expected_feature_map_shapes):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
   def _get_variables(self, depth_multiplier, layer_names=None):
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index 10b28190..d1332e47 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,10 @@
 
 """Base test class SSDFeatureExtractors."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from abc import abstractmethod
 
 import numpy as np
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index d40da534..d8ce0798 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -135,4 +136,4 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             insert_1x1_conv=True,
             image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index 51b99551..f62cd34e 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -135,4 +136,4 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             insert_1x1_conv=True,
             image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor.py
index 40786668..6de4cae3 100644
--- a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor.py
@@ -14,13 +14,9 @@
 # ==============================================================================
 """SSDFeatureExtractor for MobileNetEdgeTPU features."""
 
-import tensorflow as tf
-
 from object_detection.models import ssd_mobilenet_v3_feature_extractor
 from nets.mobilenet import mobilenet_v3
 
-slim = tf.contrib.slim
-
 
 class SSDMobileNetEdgeTPUFeatureExtractor(
     ssd_mobilenet_v3_feature_extractor.SSDMobileNetV3FeatureExtractorBase):
diff --git a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
index c2ab4759..10c607bc 100644
--- a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
@@ -22,9 +22,6 @@ import tensorflow as tf
 from object_detection.models import ssd_feature_extractor_test
 
 
-slim = tf.contrib.slim
-
-
 class _SsdMobilenetEdgeTPUFeatureExtractorTestBase(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
   """Base class for MobilenetEdgeTPU tests."""
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 810af5bb..4b173c17 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -136,4 +137,4 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             insert_1x1_conv=True,
             image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
index 8d269de9..32b5261e 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,13 @@
 
 """SSD MobilenetV1 FPN Feature Extractor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import copy
 import functools
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
index d2d27606..9357b7ce 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,11 @@
 
 """SSD Keras-based MobilenetV1 FPN Feature Extractor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
diff --git a/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
index f3b095ae..4455d012 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -164,4 +165,4 @@ class SSDMobileNetV1KerasFeatureExtractor(
         'Conv2d_11_pointwise': image_features[0],
         'Conv2d_13_pointwise': image_features[1]})
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
index 42ef1328..0910acd7 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -82,4 +83,4 @@ class SSDMobileNetV1PpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             image_features={
                 'image_features': image_features['Conv2d_11_pointwise']
             })
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
index 74725e09..4629b5fb 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -138,4 +139,4 @@ class SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               insert_1x1_conv=True,
               image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
index 086cf32d..dd24731f 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,13 @@
 
 """SSD MobilenetV2 FPN Feature Extractor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import copy
 import functools
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
index d2927b42..1d528dcf 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,11 @@
 
 """SSD Keras-based MobilenetV2 FPN Feature Extractor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
diff --git a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
index 1cefc74a..d3c80588 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -164,4 +165,4 @@ class SSDMobileNetV2KerasFeatureExtractor(
         'layer_15/expansion_output': image_features[0],
         'layer_19': image_features[1]})
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py
new file mode 100644
index 00000000..96b91a73
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py
@@ -0,0 +1,413 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""SSD MobilenetV2 NAS-FPN Feature Extractor."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+from six.moves import range
+import tensorflow as tf
+
+from tensorflow.contrib import slim as contrib_slim
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets.mobilenet import mobilenet
+from nets.mobilenet import mobilenet_v2
+
+slim = contrib_slim
+
+Block = collections.namedtuple(
+    'Block', ['inputs', 'output_level', 'kernel_size', 'expansion_size'])
+
+_MNASFPN_CELL_CONFIG = [
+    Block(inputs=(1, 2), output_level=4, kernel_size=3, expansion_size=256),
+    Block(inputs=(0, 4), output_level=3, kernel_size=3, expansion_size=128),
+    Block(inputs=(5, 4), output_level=4, kernel_size=3, expansion_size=128),
+    Block(inputs=(4, 3), output_level=5, kernel_size=5, expansion_size=128),
+    Block(inputs=(4, 3), output_level=6, kernel_size=3, expansion_size=96),
+]
+
+MNASFPN_DEF = dict(
+    feature_levels=[3, 4, 5, 6],
+    spec=[_MNASFPN_CELL_CONFIG] * 4,
+)
+
+
+def _maybe_pad(feature, use_explicit_padding, kernel_size=3):
+  return ops.fixed_padding(feature,
+                           kernel_size) if use_explicit_padding else feature
+
+
+# Wrapper around mobilenet.depth_multiplier
+def _apply_multiplier(d, multiplier, min_depth):
+  p = {'num_outputs': d}
+  mobilenet.depth_multiplier(
+      p, multiplier=multiplier, divisible_by=8, min_depth=min_depth)
+  return p['num_outputs']
+
+
+def _apply_size_dependent_ordering(input_feature, feature_level, block_level,
+                                   expansion_size, use_explicit_padding,
+                                   use_native_resize_op):
+  """Applies Size-Dependent-Ordering when resizing feature maps.
+
+     See https://arxiv.org/abs/1912.01106
+
+  Args:
+    input_feature: input feature map to be resized.
+    feature_level: the level of the input feature.
+    block_level: the desired output level for the block.
+    expansion_size: the expansion size for the block.
+    use_explicit_padding: Whether to use explicit padding.
+    use_native_resize_op: Whether to use native resize op.
+
+  Returns:
+    A transformed feature at the desired resolution and expansion size.
+  """
+  padding = 'VALID' if use_explicit_padding else 'SAME'
+  if feature_level >= block_level:  # Perform 1x1 then upsampling.
+    node = slim.conv2d(
+        input_feature,
+        expansion_size, [1, 1],
+        activation_fn=None,
+        normalizer_fn=slim.batch_norm,
+        padding=padding,
+        scope='Conv1x1')
+    if feature_level == block_level:
+      return node
+    scale = 2**(feature_level - block_level)
+    if use_native_resize_op:
+      input_shape = shape_utils.combined_static_and_dynamic_shape(node)
+      node = tf.image.resize_nearest_neighbor(
+          node, [input_shape[1] * scale, input_shape[2] * scale])
+    else:
+      node = ops.nearest_neighbor_upsampling(node, scale=scale)
+  else:  # Perform downsampling then 1x1.
+    stride = 2**(block_level - feature_level)
+    node = slim.max_pool2d(
+        _maybe_pad(input_feature, use_explicit_padding), [3, 3],
+        stride=[stride, stride],
+        padding=padding,
+        scope='Downsample')
+    node = slim.conv2d(
+        node,
+        expansion_size, [1, 1],
+        activation_fn=None,
+        normalizer_fn=slim.batch_norm,
+        padding=padding,
+        scope='Conv1x1')
+  return node
+
+
+def _mnasfpn_cell(feature_maps,
+                  feature_levels,
+                  cell_spec,
+                  output_channel=48,
+                  use_explicit_padding=False,
+                  use_native_resize_op=False,
+                  multiplier_func=None):
+  """Create a MnasFPN cell.
+
+  Args:
+    feature_maps: input feature maps.
+    feature_levels: levels of the feature maps.
+    cell_spec: A list of Block configs.
+    output_channel: Number of features for the input, output and intermediate
+      feature maps.
+    use_explicit_padding: Whether to use explicit padding.
+    use_native_resize_op: Whether to use native resize op.
+    multiplier_func: Depth-multiplier function. If None, use identity function.
+
+  Returns:
+    A transformed list of feature maps at the same resolutions as the inputs.
+  """
+  # This is the level where multipliers are realized.
+  if multiplier_func is None:
+    multiplier_func = lambda x: x
+  num_outputs = len(feature_maps)
+  cell_features = list(feature_maps)
+  cell_levels = list(feature_levels)
+  padding = 'VALID' if use_explicit_padding else 'SAME'
+  for bi, block in enumerate(cell_spec):
+    with tf.variable_scope('block_{}'.format(bi)):
+      block_level = block.output_level
+      intermediate_feature = None
+      for i, inp in enumerate(block.inputs):
+        with tf.variable_scope('input_{}'.format(i)):
+          input_level = cell_levels[inp]
+          node = _apply_size_dependent_ordering(
+              cell_features[inp], input_level, block_level,
+              multiplier_func(block.expansion_size), use_explicit_padding,
+              use_native_resize_op)
+        # Add features incrementally to avoid producing AddN, which doesn't
+        # play well with TfLite.
+        if intermediate_feature is None:
+          intermediate_feature = node
+        else:
+          intermediate_feature += node
+      node = tf.nn.relu6(intermediate_feature)
+      node = slim.separable_conv2d(
+          _maybe_pad(node, use_explicit_padding, block.kernel_size),
+          multiplier_func(output_channel),
+          block.kernel_size,
+          activation_fn=None,
+          normalizer_fn=slim.batch_norm,
+          padding=padding,
+          scope='SepConv')
+    cell_features.append(node)
+    cell_levels.append(block_level)
+
+  # Cell-wide residuals.
+  out_idx = range(len(cell_features) - num_outputs, len(cell_features))
+  for in_i, out_i in enumerate(out_idx):
+    if cell_features[out_i].shape.as_list(
+    ) == cell_features[in_i].shape.as_list():
+      cell_features[out_i] += cell_features[in_i]
+
+  return cell_features[-num_outputs:]
+
+
+def mnasfpn(feature_maps,
+            head_def,
+            output_channel=48,
+            use_explicit_padding=False,
+            use_native_resize_op=False,
+            multiplier_func=None):
+  """Create the MnasFPN head given head_def."""
+  features = feature_maps
+  for ci, cell_spec in enumerate(head_def['spec']):
+    with tf.variable_scope('cell_{}'.format(ci)):
+      features = _mnasfpn_cell(features, head_def['feature_levels'], cell_spec,
+                               output_channel, use_explicit_padding,
+                               use_native_resize_op, multiplier_func)
+  return features
+
+
+def training_scope(l2_weight_decay=1e-4, is_training=None):
+  """Arg scope for training MnasFPN."""
+  with slim.arg_scope(
+      [slim.conv2d],
+      weights_initializer=tf.initializers.he_normal(),
+      weights_regularizer=slim.l2_regularizer(l2_weight_decay)), \
+      slim.arg_scope(
+          [slim.separable_conv2d],
+          weights_initializer=tf.initializers.truncated_normal(
+              stddev=0.536),  # He_normal for 3x3 depthwise kernel.
+          weights_regularizer=slim.l2_regularizer(l2_weight_decay)), \
+      slim.arg_scope([slim.batch_norm],
+                     is_training=is_training,
+                     epsilon=0.01,
+                     decay=0.99,
+                     center=True,
+                     scale=True) as s:
+    return s
+
+
+class SSDMobileNetV2MnasFPNFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD Feature Extractor using MobilenetV2 MnasFPN features."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               fpn_min_level=3,
+               fpn_max_level=6,
+               additional_layer_depth=48,
+               head_def=None,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               use_native_resize_op=False,
+               override_base_feature_extractor_hyperparams=False,
+               data_format='channels_last'):
+    """SSD MnasFPN feature extractor based on Mobilenet v2 architecture.
+
+    See https://arxiv.org/abs/1912.01106
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the base
+        feature extractor.
+      fpn_min_level: the highest resolution feature map to use in MnasFPN.
+        Currently the only valid value is 3.
+      fpn_max_level: the smallest resolution feature map to construct or use in
+        MnasFPN. Currentl the only valid value is 6.
+      additional_layer_depth: additional feature map layer channel depth for
+        NAS-FPN.
+      head_def: A dictionary specifying the MnasFPN head architecture. Default
+        uses MNASFPN_DEF.
+      reuse_weights: whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      use_native_resize_op: Whether to use native resize op. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+      data_format: The ordering of the dimensions in the inputs, The valid
+        values are {'channels_first', 'channels_last').
+    """
+    super(SSDMobileNetV2MnasFPNFeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=(
+            override_base_feature_extractor_hyperparams))
+    if fpn_min_level != 3 or fpn_max_level != 6:
+      raise ValueError('Min and max levels of MnasFPN must be 3 and 6 for now.')
+    self._fpn_min_level = fpn_min_level
+    self._fpn_max_level = fpn_max_level
+    self._fpn_layer_depth = additional_layer_depth
+    self._head_def = head_def if head_def else MNASFPN_DEF
+    self._data_format = data_format
+    self._use_native_resize_op = use_native_resize_op
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def _verify_config(self, inputs):
+    """Verify that MnasFPN config and its inputs."""
+    num_inputs = len(inputs)
+    assert len(self._head_def['feature_levels']) == num_inputs
+
+    base_width = inputs[0].shape.as_list(
+    )[1] * 2**self._head_def['feature_levels'][0]
+    for i in range(1, num_inputs):
+      width = inputs[i].shape.as_list()[1]
+      level = self._head_def['feature_levels'][i]
+      expected_width = base_width // 2**level
+      if width != expected_width:
+        raise ValueError(
+            'Resolution of input {} does not match its level {}.'.format(
+                i, level))
+
+    for cell_spec in self._head_def['spec']:
+      # The last K nodes in a cell are the inputs to the next cell. Assert that
+      # their feature maps are at the right level.
+      for i in range(num_inputs):
+        if cell_spec[-num_inputs +
+                     i].output_level != self._head_def['feature_levels'][i]:
+          raise ValueError(
+              'Mismatch between node level {} and desired output level {}.'
+              .format(cell_spec[-num_inputs + i].output_level,
+                      self._head_def['feature_levels'][i]))
+      # Assert that each block only uses precending blocks.
+      for bi, block_spec in enumerate(cell_spec):
+        for inp in block_spec.inputs:
+          if inp >= bi + num_inputs:
+            raise ValueError(
+                'Block {} is trying to access uncreated block {}.'.format(
+                    bi, inp))
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+    with tf.variable_scope('MobilenetV2', reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(
+          mobilenet_v2.training_scope(is_training=None, bn_decay=0.99)), \
+          slim.arg_scope(
+              [mobilenet.depth_multiplier], min_depth=self._min_depth):
+        with slim.arg_scope(
+            training_scope(l2_weight_decay=4e-5,
+                           is_training=self._is_training)):
+
+          _, image_features = mobilenet_v2.mobilenet_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='layer_18',
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+
+    multiplier_func = functools.partial(
+        _apply_multiplier,
+        multiplier=self._depth_multiplier,
+        min_depth=self._min_depth)
+    with tf.variable_scope('MnasFPN', reuse=self._reuse_weights):
+      with slim.arg_scope(
+          training_scope(l2_weight_decay=1e-4, is_training=self._is_training)):
+        # Create C6 by downsampling C5.
+        c6 = slim.max_pool2d(
+            _maybe_pad(image_features['layer_18'], self._use_explicit_padding),
+            [3, 3],
+            stride=[2, 2],
+            padding='VALID' if self._use_explicit_padding else 'SAME',
+            scope='C6_downsample')
+        c6 = slim.conv2d(
+            c6,
+            multiplier_func(self._fpn_layer_depth),
+            [1, 1],
+            activation_fn=tf.identity,
+            normalizer_fn=slim.batch_norm,
+            weights_regularizer=None,  # this 1x1 has no kernel regularizer.
+            padding='VALID',
+            scope='C6_Conv1x1')
+        image_features['C6'] = tf.identity(c6)  # Needed for quantization.
+        for k in sorted(image_features.keys()):
+          tf.logging.error('{}: {}'.format(k, image_features[k]))
+
+        mnasfpn_inputs = [
+            image_features['layer_7'],  # C3
+            image_features['layer_14'],  # C4
+            image_features['layer_18'],  # C5
+            image_features['C6']  # C6
+        ]
+        self._verify_config(mnasfpn_inputs)
+        feature_maps = mnasfpn(
+            mnasfpn_inputs,
+            head_def=self._head_def,
+            output_channel=self._fpn_layer_depth,
+            use_explicit_padding=self._use_explicit_padding,
+            use_native_resize_op=self._use_native_resize_op,
+            multiplier_func=multiplier_func)
+    return feature_maps
diff --git a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py
new file mode 100644
index 00000000..222eb549
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py
@@ -0,0 +1,87 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for ssd_mobilenet_v2_nas_fpn_feature_extractor."""
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.contrib import slim as contrib_slim
+from object_detection.models import ssd_feature_extractor_test
+from object_detection.models import ssd_mobilenet_v2_mnasfpn_feature_extractor as mnasfpn_feature_extractor
+
+slim = contrib_slim
+
+
+class SsdMobilenetV2MnasFPNFeatureExtractorTest(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+
+  def _create_feature_extractor(self,
+                                depth_multiplier,
+                                pad_to_multiple,
+                                use_explicit_padding=False):
+    min_depth = 16
+    is_training = True
+    fpn_num_filters = 48
+    return mnasfpn_feature_extractor.SSDMobileNetV2MnasFPNFeatureExtractor(
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        self.conv_hyperparams_fn,
+        additional_layer_depth=fpn_num_filters,
+        use_explicit_padding=use_explicit_padding)
+
+  def test_extract_features_returns_correct_shapes_320_256(self):
+    image_height = 320
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 40, 32, 48), (2, 20, 16, 48),
+                                  (2, 10, 8, 48), (2, 5, 4, 48)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 0.5**12
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 16), (2, 16, 16, 16),
+                                  (2, 8, 8, 16), (2, 4, 4, 16)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 320
+    image_width = 320
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(2, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
index 6c87b718..39e4d4d8 100644
--- a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
@@ -157,7 +157,7 @@ class SSDMobileNetV3FeatureExtractorBase(ssd_meta_arch.SSDFeatureExtractor):
               insert_1x1_conv=True,
               image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
 
 
 class SSDMobileNetV3LargeFeatureExtractor(SSDMobileNetV3FeatureExtractorBase):
diff --git a/research/object_detection/models/ssd_pnasnet_feature_extractor.py b/research/object_detection/models/ssd_pnasnet_feature_extractor.py
index d0475b2d..232602e1 100644
--- a/research/object_detection/models/ssd_pnasnet_feature_extractor.py
+++ b/research/object_detection/models/ssd_pnasnet_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -154,7 +155,7 @@ class SSDPNASNetFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             insert_1x1_conv=True,
             image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
 
   def restore_from_classification_checkpoint_fn(self, feature_extractor_scope):
     """Returns a map of variables to load from a foreign checkpoint.
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 4e7618fb..d7cbd245 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -17,6 +18,11 @@
 See https://arxiv.org/abs/1708.02002 for details.
 """
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index 65d0d963..00356c45 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -13,10 +14,14 @@
 # limitations under the License.
 # ==============================================================================
 """Tests for ssd resnet v1 FPN feature extractors."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import abc
-import itertools
 from absl.testing import parameterized
 import numpy as np
+from six.moves import zip
 import tensorflow as tf
 
 from object_detection.models import ssd_feature_extractor_test
@@ -112,8 +117,8 @@ class SSDResnetFPNFeatureExtractorTestBase(
     image_tensor = np.random.rand(2, image_height, image_width,
                                   3).astype(np.float32)
     feature_maps = self.execute(graph_fn, [image_tensor])
-    for feature_map, expected_shape in itertools.izip(
-        feature_maps, expected_feature_map_shape):
+    for feature_map, expected_shape in zip(feature_maps,
+                                           expected_feature_map_shape):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
index ef91d283..0f4f1fa2 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,6 +16,12 @@
 
 """SSD Keras-based ResnetV1 FPN Feature Extractor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
@@ -121,7 +128,7 @@ class SSDResNetV1FpnKerasFeatureExtractor(
     self._resnet_v1_base_model = resnet_v1_base_model
     self._resnet_v1_base_model_name = resnet_v1_base_model_name
     self._resnet_block_names = ['block1', 'block2', 'block3', 'block4']
-    self._resnet_v1 = None
+    self.classification_backbone = None
     self._fpn_features_generator = None
     self._coarse_feature_layers = []
 
@@ -139,7 +146,7 @@ class SSDResNetV1FpnKerasFeatureExtractor(
     output_layers = _RESNET_MODEL_OUTPUT_LAYERS[self._resnet_v1_base_model_name]
     outputs = [full_resnet_v1_model.get_layer(output_layer_name).output
                for output_layer_name in output_layers]
-    self._resnet_v1 = tf.keras.Model(
+    self.classification_backbone = tf.keras.Model(
         inputs=full_resnet_v1_model.inputs,
         outputs=outputs)
     # pylint:disable=g-long-lambda
@@ -214,13 +221,14 @@ class SSDResNetV1FpnKerasFeatureExtractor(
     preprocessed_inputs = shape_utils.check_min_image_dim(
         129, preprocessed_inputs)
 
-    image_features = self._resnet_v1(
+    image_features = self.classification_backbone(
         ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple))
 
     feature_block_list = []
     for level in range(self._fpn_min_level, self._base_fpn_max_level + 1):
       feature_block_list.append('block{}'.format(level - 1))
-    feature_block_map = dict(zip(self._resnet_block_names, image_features))
+    feature_block_map = dict(
+        list(zip(self._resnet_block_names, image_features)))
     fpn_input_image_features = [
         (feature_block, feature_block_map[feature_block])
         for feature_block in feature_block_list]
@@ -238,6 +246,17 @@ class SSDResNetV1FpnKerasFeatureExtractor(
       feature_maps.append(last_feature_map)
     return feature_maps
 
+  def restore_from_classification_checkpoint_fn(self, feature_extractor_scope):
+    """Returns a map for restoring from an (object-based) checkpoint.
+
+    Args:
+      feature_extractor_scope: A scope name for the feature extractor (unused).
+
+    Returns:
+      A dict mapping keys to Keras models
+    """
+    return {'feature_extractor': self.classification_backbone}
+
 
 class SSDResNet50V1FpnKerasFeatureExtractor(
     SSDResNetV1FpnKerasFeatureExtractor):
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
index 06851bc3..a13794c3 100644
--- a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -162,7 +163,7 @@ class _SSDResnetPpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             image_features={
                 'image_features': self._filter_features(activations)['block3']
             })
-    return feature_maps.values()
+    return list(feature_maps.values())
 
 
 class SSDResnet50V1PpnFeatureExtractor(_SSDResnetPpnFeatureExtractor):
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index 827dfa95..d710a124 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,13 +15,19 @@
 # ==============================================================================
 
 """Convolutional Box Predictors with and without weight sharing."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 import functools
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 from object_detection.core import box_predictor
 from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
diff --git a/research/object_detection/predictors/convolutional_box_predictor_test.py b/research/object_detection/predictors/convolutional_box_predictor_test.py
index 9941731d..a5edb55e 100644
--- a/research/object_detection/predictors/convolutional_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_box_predictor_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -15,8 +16,14 @@
 
 """Tests for object_detection.predictors.convolutional_box_predictor."""
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 from absl.testing import parameterized
 import numpy as np
+from six.moves import range
+from six.moves import zip
 import tensorflow as tf
 
 from google.protobuf import text_format
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor.py b/research/object_detection/predictors/convolutional_keras_box_predictor.py
index 96fd7b7d..4aeabbfb 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -14,8 +15,13 @@
 # ==============================================================================
 
 """Convolutional Box Predictors with and without weight sharing."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import collections
 
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.core import box_predictor
@@ -400,7 +406,7 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.KerasBoxPredictor):
         self._head_scope_conv_layers[tower_name_scope] = conv_layers
       return base_tower_layers
 
-    for feature_index, input_shape in enumerate(input_shapes):
+    for feature_index in range(len(input_shapes)):
       # Additional projection layers should not be shared as input channels
       # (and thus weight shapes) are different
       inserted_layer_counter, projection_layers = (
diff --git a/research/object_detection/predictors/heads/box_head.py b/research/object_detection/predictors/heads/box_head.py
index 41f63554..95c03cc1 100644
--- a/research/object_detection/predictors/heads/box_head.py
+++ b/research/object_detection/predictors/heads/box_head.py
@@ -107,6 +107,7 @@ class MaskRCNNBoxHead(head.Head):
       box_encodings = slim.fully_connected(
           flattened_roi_pooled_features,
           number_of_boxes * self._box_code_size,
+          reuse=tf.AUTO_REUSE,
           activation_fn=None,
           scope='BoxEncodingPredictor')
     box_encodings = tf.reshape(box_encodings,
diff --git a/research/object_detection/predictors/heads/class_head.py b/research/object_detection/predictors/heads/class_head.py
index 64b2df92..e779d33d 100644
--- a/research/object_detection/predictors/heads/class_head.py
+++ b/research/object_detection/predictors/heads/class_head.py
@@ -98,6 +98,7 @@ class MaskRCNNClassHead(head.Head):
       class_predictions_with_background = slim.fully_connected(
           flattened_roi_pooled_features,
           self._num_class_slots,
+          reuse=tf.AUTO_REUSE,
           activation_fn=None,
           scope=self._scope)
     class_predictions_with_background = tf.reshape(
diff --git a/research/object_detection/predictors/heads/keras_mask_head.py b/research/object_detection/predictors/heads/keras_mask_head.py
index 86cc48e3..e8c5973b 100644
--- a/research/object_detection/predictors/heads/keras_mask_head.py
+++ b/research/object_detection/predictors/heads/keras_mask_head.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -19,7 +20,12 @@ Contains Mask prediction head classes for different meta architectures.
 All the mask prediction heads have a predict function that receives the
 `features` as the first argument and returns `mask_predictions`.
 """
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import math
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.predictors.heads import head
@@ -255,9 +261,9 @@ class MaskRCNNMaskHead(head.KerasHead):
     if self._convolve_then_upsample:
       # Replace Transposed Convolution with a Nearest Neighbor upsampling step
       # followed by 3x3 convolution.
-      height_scale = self._mask_height / shape_utils.get_dim_as_int(
+      height_scale = self._mask_height // shape_utils.get_dim_as_int(
           input_shapes[1])
-      width_scale = self._mask_width / shape_utils.get_dim_as_int(
+      width_scale = self._mask_width // shape_utils.get_dim_as_int(
           input_shapes[2])
       # pylint: disable=g-long-lambda
       self._mask_predictor_layers.append(tf.keras.layers.Lambda(
diff --git a/research/object_detection/predictors/heads/keypoint_head.py b/research/object_detection/predictors/heads/keypoint_head.py
index 18cac4aa..d72e561a 100644
--- a/research/object_detection/predictors/heads/keypoint_head.py
+++ b/research/object_detection/predictors/heads/keypoint_head.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -22,6 +23,11 @@ Keypoints could be used to represent the human body joint locations as in
 Mask RCNN paper. Or they could be used to represent different part locations of
 objects.
 """
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
diff --git a/research/object_detection/predictors/heads/mask_head.py b/research/object_detection/predictors/heads/mask_head.py
index d30a5234..ee13d68b 100644
--- a/research/object_detection/predictors/heads/mask_head.py
+++ b/research/object_detection/predictors/heads/mask_head.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -19,7 +20,12 @@ Contains Mask prediction head classes for different meta architectures.
 All the mask prediction heads have a predict function that receives the
 `features` as the first argument and returns `mask_predictions`.
 """
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import math
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
@@ -155,8 +161,8 @@ class MaskRCNNMaskHead(head.Head):
       if self._convolve_then_upsample:
         # Replace Transposed Convolution with a Nearest Neighbor upsampling step
         # followed by 3x3 convolution.
-        height_scale = self._mask_height / features.shape[1].value
-        width_scale = self._mask_width / features.shape[2].value
+        height_scale = self._mask_height // features.shape[1].value
+        width_scale = self._mask_width // features.shape[2].value
         features = ops.nearest_neighbor_upsampling(
             features, height_scale=height_scale, width_scale=width_scale)
         features = slim.conv2d(
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor.py b/research/object_detection/predictors/mask_rcnn_box_predictor.py
index 2fd1b692..b7ee156f 100644
--- a/research/object_detection/predictors/mask_rcnn_box_predictor.py
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor.py
@@ -14,11 +14,11 @@
 # ==============================================================================
 
 """Mask R-CNN Box Predictor."""
-import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.core import box_predictor
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
diff --git a/research/object_detection/predictors/rfcn_box_predictor.py b/research/object_detection/predictors/rfcn_box_predictor.py
index a63ce203..a10c5f9d 100644
--- a/research/object_detection/predictors/rfcn_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_box_predictor.py
@@ -15,10 +15,11 @@
 
 """RFCN Box Predictor."""
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 from object_detection.core import box_predictor
 from object_detection.utils import ops
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
diff --git a/research/object_detection/protos/eval.proto b/research/object_detection/protos/eval.proto
index 357edc91..b1b99881 100644
--- a/research/object_detection/protos/eval.proto
+++ b/research/object_detection/protos/eval.proto
@@ -3,7 +3,7 @@ syntax = "proto2";
 package object_detection.protos;
 
 // Message for configuring DetectionModel evaluation jobs (eval.py).
-// Next id - 30
+// Next id - 33
 message EvalConfig {
   optional uint32 batch_size = 25 [default = 1];
   // Number of visualization images to generate.
@@ -31,6 +31,10 @@ message EvalConfig {
   // Type of metrics to use for evaluation.
   repeated string metrics_set = 8;
 
+  // Type of metrics to use for evaluation. Unlike `metrics_set` above, this
+  // field allows configuring evaluation metric through config files.
+  repeated ParameterizedMetric parameterized_metric = 31;
+
   // Path to export detections to COCO compatible JSON format.
   optional string export_path = 9 [default =''];
 
@@ -45,7 +49,7 @@ message EvalConfig {
 
   // Whether to evaluate instance masks.
   // Note that since there is no evaluation code currently for instance
-  // segmenation this option is unused.
+  // segmentation this option is unused.
   optional bool eval_instance_masks = 12 [default = false];
 
   // Minimum score threshold for a detected object box to be visualized
@@ -90,5 +94,59 @@ message EvalConfig {
   // When this flag is set, images are not resized during evaluation.
   // When this flag is not set (default case), image are resized according
   // to the image_resizer config in the model during evaluation.
-  optional bool force_no_resize = 29 [default=false];
+  optional bool force_no_resize = 29 [default = false];
+
+  // Whether to use a dummy loss in eval so model.loss() is not executed.
+  optional bool use_dummy_loss_in_eval = 30 [default = false];
+
+  // Specifies which keypoints should be connected by an edge, which may improve
+  // visualization. An example would be human pose estimation where certain
+  // joints can be connected.
+  repeated KeypointEdge keypoint_edge = 32;
+}
+
+// A message to configure parameterized evaluation metric.
+message ParameterizedMetric {
+  oneof parameterized_metric {
+    CocoKeypointMetrics coco_keypoint_metrics = 1;
+  }
+}
+
+// A message to evaluate COCO keypoint metrics for a specific class.
+message CocoKeypointMetrics {
+  // Identifies the class of object to which keypoints belong. By default this
+  // should use the class's "display_name" in the label map.
+  optional string class_label = 1;
+  // Keypoint specific standard deviations for COCO keypoint metrics, which
+  // controls how OKS is computed.
+  // See http://cocodataset.org/#keypoints-eval for details.
+  // If your keypoints are similar to the COCO keypoints use the precomputed
+  // standard deviations below:
+  // "nose": 0.026
+  // "left_eye": 0.025
+  // "right_eye": 0.025
+  // "left_ear": 0.035
+  // "right_ear": 0.035
+  // "left_shoulder": 0.079
+  // "right_shoulder": 0.079
+  // "left_elbow": 0.072
+  // "right_elbow": 0.072
+  // "left_wrist": 0.062
+  // "right_wrist": 0.062
+  // "left_hip": 0.107
+  // "right_hip": 0.107
+  // "left_knee": 0.087
+  // "right_knee": 0.087
+  // "left_ankle": 0.089
+  // "right_ankle": 0.089
+  map<string, float> keypoint_label_to_sigmas = 2;
+}
+
+// Defines an edge that should be drawn between two keypoints.
+message KeypointEdge {
+  // Index of the keypoint where the edge starts from. Index starts at 0.
+  optional int32 start = 1;
+
+  // Index of the keypoint where the edge ends. Index starts at 0.
+  optional int32 end = 2;
 }
diff --git a/research/object_detection/protos/faster_rcnn.proto b/research/object_detection/protos/faster_rcnn.proto
index 95324b9f..7e06fbcf 100644
--- a/research/object_detection/protos/faster_rcnn.proto
+++ b/research/object_detection/protos/faster_rcnn.proto
@@ -18,9 +18,8 @@ import "object_detection/protos/post_processing.proto";
 // `first_stage_` and `second_stage_` to indicate the stage to which each
 // parameter pertains when relevant.
 message FasterRcnn {
-
   // Whether to construct only the Region Proposal Network (RPN).
-  optional int32 number_of_stages = 1 [default=2];
+  optional int32 number_of_stages = 1 [default = 2];
 
   // Number of classes to predict.
   optional int32 num_classes = 3;
@@ -31,7 +30,6 @@ message FasterRcnn {
   // Feature extractor config.
   optional FasterRcnnFeatureExtractor feature_extractor = 5;
 
-
   // (First stage) region proposal network (RPN) parameters.
 
   // Anchor generator to compute RPN anchors.
@@ -39,40 +37,39 @@ message FasterRcnn {
 
   // Atrous rate for the convolution op applied to the
   // `first_stage_features_to_crop` tensor to obtain box predictions.
-  optional int32 first_stage_atrous_rate = 7 [default=1];
+  optional int32 first_stage_atrous_rate = 7 [default = 1];
 
   // Hyperparameters for the convolutional RPN box predictor.
   optional Hyperparams first_stage_box_predictor_conv_hyperparams = 8;
 
   // Kernel size to use for the convolution op just prior to RPN box
   // predictions.
-  optional int32 first_stage_box_predictor_kernel_size = 9 [default=3];
+  optional int32 first_stage_box_predictor_kernel_size = 9 [default = 3];
 
   // Output depth for the convolution op just prior to RPN box predictions.
-  optional int32 first_stage_box_predictor_depth = 10 [default=512];
+  optional int32 first_stage_box_predictor_depth = 10 [default = 512];
 
   // The batch size to use for computing the first stage objectness and
   // location losses.
-  optional int32 first_stage_minibatch_size = 11 [default=256];
+  optional int32 first_stage_minibatch_size = 11 [default = 256];
 
   // Fraction of positive examples per image for the RPN.
-  optional float first_stage_positive_balance_fraction = 12 [default=0.5];
+  optional float first_stage_positive_balance_fraction = 12 [default = 0.5];
 
   // Non max suppression score threshold applied to first stage RPN proposals.
-  optional float first_stage_nms_score_threshold = 13 [default=0.0];
+  optional float first_stage_nms_score_threshold = 13 [default = 0.0];
 
   // Non max suppression IOU threshold applied to first stage RPN proposals.
-  optional float first_stage_nms_iou_threshold = 14 [default=0.7];
+  optional float first_stage_nms_iou_threshold = 14 [default = 0.7];
 
   // Maximum number of RPN proposals retained after first stage postprocessing.
-  optional int32 first_stage_max_proposals = 15 [default=300];
+  optional int32 first_stage_max_proposals = 15 [default = 300];
 
   // First stage RPN localization loss weight.
-  optional float first_stage_localization_loss_weight = 16 [default=1.0];
+  optional float first_stage_localization_loss_weight = 16 [default = 1.0];
 
   // First stage RPN objectness loss weight.
-  optional float first_stage_objectness_loss_weight = 17 [default=1.0];
-
+  optional float first_stage_objectness_loss_weight = 17 [default = 1.0];
 
   // Per-region cropping parameters.
   // Note that if a R-FCN model is constructed the per region cropping
@@ -89,21 +86,20 @@ message FasterRcnn {
   // Stride of the max pool op on the cropped feature map during ROI pooling.
   optional int32 maxpool_stride = 20;
 
-
   // (Second stage) box classifier parameters
 
   // Hyperparameters for the second stage box predictor. If box predictor type
   // is set to rfcn_box_predictor, a R-FCN model is constructed, otherwise a
   // Faster R-CNN model is constructed.
-  optional BoxPredictor second_stage_box_predictor  = 21;
+  optional BoxPredictor second_stage_box_predictor = 21;
 
   // The batch size per image used for computing the classification and refined
   // location loss of the box classifier.
   // Note that this field is ignored if `hard_example_miner` is configured.
-  optional int32 second_stage_batch_size = 22 [default=64];
+  optional int32 second_stage_batch_size = 22 [default = 64];
 
   // Fraction of positive examples to use per image for the box classifier.
-  optional float second_stage_balance_fraction = 23 [default=0.25];
+  optional float second_stage_balance_fraction = 23 [default = 0.25];
 
   // Post processing to apply on the second stage box classifier predictions.
   // Note: the `score_converter` provided to the FasterRCNNMetaArch constructor
@@ -111,15 +107,15 @@ message FasterRcnn {
   optional PostProcessing second_stage_post_processing = 24;
 
   // Second stage refined localization loss weight.
-  optional float second_stage_localization_loss_weight = 25 [default=1.0];
+  optional float second_stage_localization_loss_weight = 25 [default = 1.0];
 
   // Second stage classification loss weight
-  optional float second_stage_classification_loss_weight = 26 [default=1.0];
+  optional float second_stage_classification_loss_weight = 26 [default = 1.0];
 
   // Second stage instance mask loss weight. Note that this is only applicable
   // when `MaskRCNNBoxPredictor` is selected for second stage and configured to
   // predict instance masks.
-  optional float second_stage_mask_prediction_loss_weight = 27 [default=1.0];
+  optional float second_stage_mask_prediction_loss_weight = 27 [default = 1.0];
 
   // If not left to default, applies hard example mining only to classification
   // and localization loss..
@@ -178,6 +174,30 @@ message FasterRcnn {
 
   // Whether to use tf.image.combined_non_max_suppression.
   optional bool use_combined_nms_in_first_stage = 40 [default = false];
+
+  // Whether to output final box feature. If true, it will crop the feature map
+  // in the postprocess() method based on the final predictions.
+  optional bool output_final_box_features = 42 [default = false];
+
+  // Configs for context model.
+  optional Context context_config = 41;
+}
+
+message Context {
+  // Configuration proto for Context .
+  // Next id: 4
+
+  // The maximum number of contextual features per-image, used for padding
+  optional int32 max_num_context_features = 1 [default = 8500];
+
+  // The bottleneck feature dimension of the attention block.
+  optional int32 attention_bottleneck_dimension = 2 [default = 2048];
+
+  // The attention temperature.
+  optional float attention_temperature = 3 [default = 0.01];
+
+  // The context feature length.
+  optional int32 context_feature_length = 4 [default = 2057];
 }
 
 message FasterRcnnFeatureExtractor {
@@ -186,10 +206,10 @@ message FasterRcnnFeatureExtractor {
   optional string type = 1;
 
   // Output stride of extracted RPN feature map.
-  optional int32 first_stage_features_stride = 2 [default=16];
+  optional int32 first_stage_features_stride = 2 [default = 16];
 
   // Whether to update batch norm parameters during training or not.
   // When training with a relative large batch size (e.g. 8), it could be
   // desirable to enable batch norm update.
-  optional bool batch_norm_trainable = 3 [default=false];
+  optional bool batch_norm_trainable = 3 [default = false];
 }
diff --git a/research/object_detection/protos/hyperparams.proto b/research/object_detection/protos/hyperparams.proto
index eb8a2252..2b105387 100644
--- a/research/object_detection/protos/hyperparams.proto
+++ b/research/object_detection/protos/hyperparams.proto
@@ -32,6 +32,9 @@ message Hyperparams {
 
     // Use tf.nn.relu6
     RELU_6 = 2;
+
+    // Use tf.nn.swish
+    SWISH = 3;
   }
   optional Activation activation = 4 [default = RELU];
 
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index 8132d9cf..94a6ea95 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -10,6 +10,7 @@ message ImageResizer {
     FixedShapeResizer fixed_shape_resizer = 2;
     IdentityResizer identity_resizer = 3;
     ConditionalShapeResizer conditional_shape_resizer = 4;
+    PadToMultipleResizer pad_to_multiple_resizer = 5;
   }
 }
 
@@ -90,3 +91,18 @@ message ConditionalShapeResizer {
   optional bool convert_to_grayscale = 4 [default = false];
 
 }
+
+
+// An image resizer which resizes inputs by zero padding them such that their
+// spatial dimensions are divisible by a specified multiple. This is useful
+// when you want to concatenate or compare the input to an output of a
+// fully convolutional network.
+message PadToMultipleResizer {
+
+  // The multiple to which the spatial dimensions will be padded to.
+  optional int32 multiple = 1 [default = 1];
+
+  // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
+  optional bool convert_to_grayscale = 4 [default = false];
+
+}
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index 701dacae..f3048888 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -2,6 +2,8 @@ syntax = "proto2";
 
 package object_detection.protos;
 
+import "object_detection/protos/image_resizer.proto";
+
 // Configuration proto for defining input readers that generate Object Detection
 // Examples from input sources. Input readers are expected to generate a
 // dictionary of tensors, with the following fields populated:
@@ -22,19 +24,19 @@ enum InstanceMaskType {
   PNG_MASKS = 2;        // Encoded PNG masks.
 }
 
-// Next id: 25
+// Next id: 29
 message InputReader {
   // Name of input reader. Typically used to describe the dataset that is read
   // by this input reader.
-  optional string name = 23 [default=""];
+  optional string name = 23 [default = ""];
 
   // Path to StringIntLabelMap pbtxt file specifying the mapping from string
   // labels to integer ids.
-  optional string label_map_path = 1 [default=""];
+  optional string label_map_path = 1 [default = ""];
 
   // Whether data should be processed in the order they are read in, or
   // shuffled randomly.
-  optional bool shuffle = 2 [default=true];
+  optional bool shuffle = 2 [default = true];
 
   // Buffer size to be used when shuffling.
   optional uint32 shuffle_buffer_size = 11 [default = 2048];
@@ -44,43 +46,43 @@ message InputReader {
 
   // The number of times a data source is read. If set to zero, the data source
   // will be reused indefinitely.
-  optional uint32 num_epochs = 5 [default=0];
+  optional uint32 num_epochs = 5 [default = 0];
 
   // Integer representing how often an example should be sampled. To feed
   // only 1/3 of your data into your model, set `sample_1_of_n_examples` to 3.
   // This is particularly useful for evaluation, where you might not prefer to
   // evaluate all of your samples.
-  optional uint32 sample_1_of_n_examples = 22 [default=1];
+  optional uint32 sample_1_of_n_examples = 22 [default = 1];
 
   // Number of file shards to read in parallel.
-  optional uint32 num_readers = 6 [default=64];
+  optional uint32 num_readers = 6 [default = 64];
 
   // Number of batches to produce in parallel. If this is run on a 2x2 TPU set
   // this to 8.
-  optional uint32 num_parallel_batches = 19 [default=8];
+  optional uint32 num_parallel_batches = 19 [default = 8];
 
   // Number of batches to prefetch. Prefetch decouples input pipeline and
   // model so they can be pipelined resulting in higher throughput. Set this
   // to a small constant and increment linearly until the improvements become
   // marginal or you exceed your cpu memory budget. Setting this to -1,
   // automatically tunes this value for you.
-  optional int32 num_prefetch_batches = 20 [default=2];
+  optional int32 num_prefetch_batches = 20 [default = 2];
 
   // Maximum number of records to keep in reader queue.
-  optional uint32 queue_capacity = 3 [default=2000, deprecated=true];
+  optional uint32 queue_capacity = 3 [default = 2000, deprecated = true];
 
   // Minimum number of records to keep in reader queue. A large value is needed
   // to generate a good random shuffle.
-  optional uint32 min_after_dequeue = 4 [default=1000, deprecated=true];
+  optional uint32 min_after_dequeue = 4 [default = 1000, deprecated = true];
 
   // Number of records to read from each reader at once.
-  optional uint32 read_block_length = 15 [default=32];
+  optional uint32 read_block_length = 15 [default = 32];
 
   // Number of decoded records to prefetch before batching.
-  optional uint32 prefetch_size = 13 [default = 512, deprecated=true];
+  optional uint32 prefetch_size = 13 [default = 512, deprecated = true];
 
   // Number of parallel decode ops to apply.
-  optional uint32 num_parallel_map_calls = 14 [default = 64, deprecated=true];
+  optional uint32 num_parallel_map_calls = 14 [default = 64, deprecated = true];
 
   // If positive, TfExampleDecoder will try to decode rasters of additional
   // channels from tf.Examples.
@@ -89,14 +91,21 @@ message InputReader {
   // Number of groundtruth keypoints per object.
   optional uint32 num_keypoints = 16 [default = 0];
 
+  // Keypoint weights. These weights can be used to apply per-keypoint loss
+  // multipliers. The size of this field should agree with `num_keypoints`.
+  repeated float keypoint_type_weight = 26;
+
   // Maximum number of boxes to pad to during training / evaluation.
   // Set this to at least the maximum amount of boxes in the input data,
   // otherwise some groundtruth boxes may be clipped.
-  optional int32 max_number_of_boxes = 21 [default=100];
+  optional int32 max_number_of_boxes = 21 [default = 100];
 
   // Whether to load multiclass scores from the dataset.
   optional bool load_multiclass_scores = 24 [default = false];
 
+  // Whether to load context features from the dataset.
+  optional bool load_context_features = 25 [default = false];
+
   // Whether to load groundtruth instance masks.
   optional bool load_instance_masks = 7 [default = false];
 
@@ -107,10 +116,15 @@ message InputReader {
   // when mapping class text strings to integers.
   optional bool use_display_name = 17 [default = false];
 
+  // Whether to include the source_id string in the input features.
+  optional bool include_source_id = 27 [default = false];
+
   oneof input_reader {
     TFRecordInputReader tf_record_input_reader = 8;
     ExternalInputReader external_input_reader = 9;
   }
+
+
 }
 
 // An input reader that reads TF Example protos from local TFRecord files.
diff --git a/research/object_detection/protos/losses.proto b/research/object_detection/protos/losses.proto
index ed856588..2342fb24 100644
--- a/research/object_detection/protos/losses.proto
+++ b/research/object_detection/protos/losses.proto
@@ -69,6 +69,7 @@ message LocalizationLoss {
     WeightedL2LocalizationLoss weighted_l2 = 1;
     WeightedSmoothL1LocalizationLoss weighted_smooth_l1 = 2;
     WeightedIOULocalizationLoss weighted_iou = 3;
+    L1LocalizationLoss l1_localization_loss = 4;
   }
 }
 
@@ -96,6 +97,10 @@ message WeightedSmoothL1LocalizationLoss {
 message WeightedIOULocalizationLoss {
 }
 
+// L1 Localization Loss.
+message L1LocalizationLoss {
+}
+
 // Configuration for class prediction loss function.
 message ClassificationLoss {
   oneof classification_loss {
@@ -104,6 +109,7 @@ message ClassificationLoss {
     WeightedSoftmaxClassificationAgainstLogitsLoss weighted_logits_softmax = 5;
     BootstrappedSigmoidClassificationLoss bootstrapped_sigmoid = 3;
     SigmoidFocalClassificationLoss weighted_sigmoid_focal = 4;
+    PenaltyReducedLogisticFocalLoss penalty_reduced_logistic_focal_loss = 6;
   }
 }
 
@@ -162,6 +168,17 @@ message BootstrappedSigmoidClassificationLoss {
   optional bool anchorwise_output = 3 [default=false];
 }
 
+// Pixelwise logistic focal loss with pixels near the target having a reduced
+// penalty.
+message PenaltyReducedLogisticFocalLoss {
+
+  // Focussing parameter of the focal loss.
+  optional float alpha = 1;
+
+  // Penalty reduction factor.
+  optional float beta = 2;
+}
+
 // Configuration for hard example miner.
 message HardExampleMiner {
   // Maximum number of hard examples to be selected per image (prior to
diff --git a/research/object_detection/protos/model.proto b/research/object_detection/protos/model.proto
index 90b8cdb3..9333f2df 100644
--- a/research/object_detection/protos/model.proto
+++ b/research/object_detection/protos/model.proto
@@ -13,9 +13,10 @@ message DetectionModel {
 
     // This can be used to define experimental models. To define your own
     // experimental meta architecture, populate a key in the
-    // model_builder.EXPERIMENTAL_META_ARCHITECURE_BUILDER_MAP dict and set its
+    // model_builder.EXPERIMENTAL_META_ARCH_BUILDER_MAP dict and set its
     // value to a function that builds your model.
     ExperimentalModel experimental_model = 3;
+
   }
 }
 
diff --git a/research/object_detection/protos/optimizer.proto b/research/object_detection/protos/optimizer.proto
index 006648c9..7ce2302b 100644
--- a/research/object_detection/protos/optimizer.proto
+++ b/research/object_detection/protos/optimizer.proto
@@ -36,6 +36,10 @@ message MomentumOptimizer {
 // See: https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer
 message AdamOptimizer {
   optional LearningRate learning_rate = 1;
+  // Default value for epsilon (1e-8) matches default value in
+  // tf.compat.v1.train.AdamOptimizer. This differs from tf2 default of 1e-7
+  // in tf.keras.optimizers.Adam .
+  optional float epsilon = 2 [default = 1e-8];
 }
 
 
diff --git a/research/object_detection/protos/pipeline.proto b/research/object_detection/protos/pipeline.proto
index 0b3cd150..fc8be7b7 100644
--- a/research/object_detection/protos/pipeline.proto
+++ b/research/object_detection/protos/pipeline.proto
@@ -10,7 +10,7 @@ import "object_detection/protos/train.proto";
 
 // Convenience message for configuring a training and eval pipeline. Allows all
 // of the pipeline parameters to be configured from one file.
-// Next id: 7
+// Next id: 8
 message TrainEvalPipelineConfig {
   optional DetectionModel model = 1;
   optional TrainConfig train_config = 2;
diff --git a/research/object_detection/protos/post_processing.proto b/research/object_detection/protos/post_processing.proto
index a286d931..ab12bb53 100644
--- a/research/object_detection/protos/post_processing.proto
+++ b/research/object_detection/protos/post_processing.proto
@@ -45,6 +45,10 @@ message BatchNonMaxSuppression {
 
   // Whether to use tf.image.combined_non_max_suppression.
   optional bool use_combined_nms = 11 [default = false];
+
+  // Whether to change coordinate frame of the boxlist to be relative to
+  // window's frame.
+  optional bool change_coordinate_frame = 12 [default = true];
 }
 
 // Configuration proto for post-processing predicted boxes and
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index 1937decf..aa83939f 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -4,6 +4,7 @@ package object_detection.protos;
 
 // Message for defining a preprocessing operation on input data.
 // See: //third_party/tensorflow_models/object_detection/core/preprocessor.py
+// Next ID: 38
 message PreprocessingStep {
   oneof preprocessing_step {
     NormalizeImage normalize_image = 1;
@@ -42,6 +43,7 @@ message PreprocessingStep {
     RandomJpegQuality random_jpeg_quality = 34;
     RandomDownscaleToTargetPixels random_downscale_to_target_pixels = 35;
     RandomPatchGaussian random_patch_gaussian = 36;
+    RandomSquareCropByScale random_square_crop_by_scale = 37;
   }
 }
 
@@ -533,3 +535,26 @@ message RandomPatchGaussian {
   optional float min_gaussian_stddev = 4 [default = 0.0];
   optional float max_gaussian_stddev = 5 [default = 1.0];
 }
+
+// Extract a square sized crop from an image whose side length is sampled by
+// randomly scaling the maximum spatial dimension of the image. If part of the
+// crop falls outside the image, it is filled with zeros.
+// The augmentation is borrowed from [1]
+// [1]: https://arxiv.org/abs/1904.07850
+message RandomSquareCropByScale {
+
+  // The maximum size of the border. The border defines distance in pixels to
+  // the image boundaries that will not be considered as a center of a crop.
+  // To make sure that the border does not go over the center of the image,
+  // we chose the border value by computing the minimum k, such that
+  // (max_border / (2**k)) < image_dimension/2
+  optional int32 max_border = 1 [default = 128];
+
+  // The minimum and maximum values of scale.
+  optional float scale_min = 2 [default=0.6];
+  optional float scale_max = 3 [default=1.3];
+
+  // The number of discrete scale values to randomly sample between
+  // [min_scale, max_scale]
+  optional int32 num_scales = 4 [default=8];
+}
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index cae019ed..1fd9324b 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -145,6 +145,7 @@ message Ssd {
   optional MaskHead mask_head_config = 25;
 }
 
+// Next id: 18.
 message SsdFeatureExtractor {
   reserved 6;
 
diff --git a/research/object_detection/protos/string_int_label_map.proto b/research/object_detection/protos/string_int_label_map.proto
index 0894183b..3b79e624 100644
--- a/research/object_detection/protos/string_int_label_map.proto
+++ b/research/object_detection/protos/string_int_label_map.proto
@@ -17,6 +17,22 @@ message StringIntLabelMapItem {
 
   // Human readable string label.
   optional string display_name = 3;
+
+  // Name of class specific keypoints for each class object and their respective
+  // keypoint IDs.
+  message KeypointMap {
+    // Id for the keypoint. Id must be unique within a given class, however, it
+    // could be shared across classes. For example "nose" keypoint can occur
+    // in both "face" and "person" classes. Hence they can be mapped to the same
+    // id.
+    //
+    // Note: It is advised to assign ids in range [1, num_unique_keypoints] to
+    // encode keypoint targets efficiently.
+    optional int32 id = 1;
+    // Label for the keypoint.
+    optional string label = 2;
+  }
+  repeated KeypointMap keypoints = 4;
 };
 
 message StringIntLabelMap {
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index 009a3c10..0da8b2ed 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -5,8 +5,16 @@ package object_detection.protos;
 import "object_detection/protos/optimizer.proto";
 import "object_detection/protos/preprocessor.proto";
 
+
+enum CheckpointVersion {
+  UNKNOWN  = 0;
+  V1 = 1;
+  V2 = 2;
+}
+
+
 // Message for configuring DetectionModel training jobs (train.py).
-// Next id: 28
+// Next id: 30
 message TrainConfig {
   // Effective batch size to use for training.
   // For TPU (or sync SGD jobs), the batch size per core (or GPU) is going to be
@@ -37,6 +45,11 @@ message TrainConfig {
   // Typically used to load feature extractor variables from trained models.
   optional string fine_tune_checkpoint_type = 22 [default=""];
 
+  // Either "v1" or "v2". If v1, restores the checkpoint using the tensorflow
+  // v1 style of restoring checkpoints. If v2, uses the eager mode checkpoint
+  // restoration API.
+  optional CheckpointVersion fine_tune_checkpoint_version = 28 [default=V1];
+
   // [Deprecated]: use fine_tune_checkpoint_type instead.
   // Specifies if the finetune checkpoint is from an object detection model.
   // If from an object detection model, the model being trained should have
@@ -119,4 +132,6 @@ message TrainConfig {
 
   // Whether to summarize gradients.
   optional bool summarize_gradients = 27 [default=false];
+
 }
+
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync.config
new file mode 100644
index 00000000..62bd7a72
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync.config
@@ -0,0 +1,198 @@
+# SSD with MnasFPN feature extractor, shared box predictor
+# See Chen et al, https://arxiv.org/abs/1912.01106
+# Trained on COCO, initialized from scratch.
+#
+# 0.92B MulAdds, 2.5M Parameters. Latency is 193ms on Pixel 1.
+# Achieves 26.6 mAP on COCO14 minival dataset.
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      multiscale_anchor_generator {
+        min_level: 3
+        max_level: 6
+        anchor_scale: 3.0
+        aspect_ratios: [1.0, 2.0, 0.5]
+        scales_per_octave: 3
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        depth: 64
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            decay: 0.997,
+            epsilon: 0.001,
+          }
+        }
+        num_layers_before_predictor: 4
+        share_prediction_tower: true
+        use_depthwise: true
+        kernel_size: 3
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v2_mnasfpn'
+      fpn {
+        min_level: 3
+        max_level: 6
+        additional_layer_depth: 48
+      }
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          random_normal_initializer {
+            stddev: 0.01
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.25
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 1024
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 50000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    random_crop_image {
+      min_object_covered: 0.0
+      min_aspect_ratio: 0.75
+      max_aspect_ratio: 3.0
+      min_area: 0.75
+      max_area: 1.0
+      overlap_thresh: 0.0
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 4.
+          total_steps: 50000
+          warmup_learning_rate: .026666
+          warmup_steps: 5000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/test_ckpt/ssd_inception_v2.pb b/research/object_detection/test_ckpt/ssd_inception_v2.pb
deleted file mode 100644
index 090d15ce..00000000
Binary files a/research/object_detection/test_ckpt/ssd_inception_v2.pb and /dev/null differ
diff --git a/research/object_detection/test_data/ssd_mobilenet_v1_fpp.config b/research/object_detection/test_data/ssd_mobilenet_v1_fpp.config
new file mode 100644
index 00000000..09c961ad
--- /dev/null
+++ b/research/object_detection/test_data/ssd_mobilenet_v1_fpp.config
@@ -0,0 +1,251 @@
+model {
+  ssd {
+    num_classes: 2
+    box_coder {
+      keypoint_box_coder {
+        num_keypoints: 23
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+        height_stride: 16
+        height_stride: 32
+        height_stride: 64
+        height_stride: 128
+        height_stride: 256
+        height_stride: 512
+        width_stride: 16
+        width_stride: 32
+        width_stride: 64
+        width_stride: 128
+        width_stride: 256
+        width_stride: 512
+        height_offset: 0
+        height_offset: 0
+        height_offset: 0
+        height_offset: 0
+        height_offset: 0
+        height_offset: 0
+        width_offset: 0
+        width_offset: 0
+        width_offset: 0
+        width_offset: 0
+        width_offset: 0
+        width_offset: 0
+      }
+    }
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 320
+        max_dimension: 640
+        convert_to_grayscale: true
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        kernel_size: 3
+        box_code_size: 50
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true
+            scale: true
+            center: true
+            decay: 0.9997
+            epsilon: 0.001
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: "ssd_mobilenet_v1"
+      min_depth: 16
+      depth_multiplier: 0.25
+      use_explicit_padding: true
+      conv_hyperparams {
+        activation: RELU_6
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true
+          scale: true
+          center: true
+          decay: 0.9997
+          epsilon: 0.001
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid {
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      hard_example_miner {
+        num_hard_examples: 3000
+        iou_threshold: 0.99
+        loss_type: CLASSIFICATION
+        max_negatives_per_positive: 3
+        min_negatives_per_image: 10
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.5
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: ""
+  num_steps: 10000000
+  batch_size: 32
+  data_augmentation_options {
+    random_horizontal_flip {
+      keypoint_flip_permutation: 1
+      keypoint_flip_permutation: 0
+      keypoint_flip_permutation: 2
+      keypoint_flip_permutation: 3
+      keypoint_flip_permutation: 5
+      keypoint_flip_permutation: 4
+      keypoint_flip_permutation: 6
+      keypoint_flip_permutation: 8
+      keypoint_flip_permutation: 7
+      keypoint_flip_permutation: 10
+      keypoint_flip_permutation: 9
+      keypoint_flip_permutation: 12
+      keypoint_flip_permutation: 11
+      keypoint_flip_permutation: 14
+      keypoint_flip_permutation: 13
+      keypoint_flip_permutation: 16
+      keypoint_flip_permutation: 15
+      keypoint_flip_permutation: 18
+      keypoint_flip_permutation: 17
+      keypoint_flip_permutation: 20
+      keypoint_flip_permutation: 19
+      keypoint_flip_permutation: 22
+      keypoint_flip_permutation: 21
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop_fixed_aspect_ratio {
+    }
+  }
+  optimizer {
+    rms_prop_optimizer {
+      learning_rate {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.0004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+}
+
+train_input_reader {
+  label_map_path: "PATH_TO_BE_CONFIGURED/face_person_with_keypoints_label_map.pbtxt"
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/faces_train.record-?????-of-00010"
+  }
+  num_keypoints: 23
+}
+
+eval_config {
+  num_visualizations: 10
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: true
+  parameterized_metric {
+    coco_keypoint_metrics {
+      class_label: "face"
+    }
+  }
+  parameterized_metric {
+    coco_keypoint_metrics {
+      class_label: "PERSON"
+    }
+  }
+}
+
+eval_input_reader {
+  label_map_path: "PATH_TO_BE_CONFIGURED/face_person_with_keypoints_label_map.pbtxt"
+  shuffle: true
+  num_epochs: 1
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/faces_val.record-?????-of-00010"
+  }
+  num_keypoints: 23
+}
+
+graph_rewriter {
+  quantization {
+    delay: 2000000
+    activation_bits: 8
+    weight_bits: 8
+  }
+}
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
index bb3218cb..147d09ad 100644
--- a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
@@ -24,6 +24,7 @@ from google.protobuf import text_format
 from tensorflow.python.saved_model import loader
 from tensorflow.python.saved_model import signature_constants
 from tensorflow.python.saved_model import tag_constants
+from tensorflow.python.tpu import tpu
 # pylint: enable=g-direct-tensorflow-import
 from object_detection.protos import pipeline_pb2
 from object_detection.tpu_exporters import faster_rcnn
@@ -160,7 +161,7 @@ def run_inference(inputs,
     saver = tf.train.Saver()
     init_op = tf.global_variables_initializer()
 
-    sess.run(tf.contrib.tpu.initialize_system())
+    sess.run(tpu.initialize_system())
 
     sess.run(init_op)
     if ckpt_path is not None:
@@ -170,7 +171,7 @@ def run_inference(inputs,
       tensor_dict_out = sess.run(
           result_tensor_dict, feed_dict={placeholder_tensor: [inputs]})
 
-    sess.run(tf.contrib.tpu.shutdown_system())
+    sess.run(tpu.shutdown_system())
 
     return tensor_dict_out
 
@@ -194,7 +195,7 @@ def run_inference_from_saved_model(inputs,
     meta_graph = loader.load(sess, [tag_constants.SERVING, tag_constants.TPU],
                              saved_model_dir)
 
-    sess.run(tf.contrib.tpu.initialize_system())
+    sess.run(tpu.initialize_system())
 
     key_prediction = signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
 
@@ -210,6 +211,6 @@ def run_inference_from_saved_model(inputs,
       tensor_dict_out = sess.run(
           tensor_name_output, feed_dict={tensor_name_input: [inputs]})
 
-    sess.run(tf.contrib.tpu.shutdown_system())
+    sess.run(tpu.shutdown_system())
 
     return tensor_dict_out
diff --git a/research/object_detection/tpu_exporters/faster_rcnn.py b/research/object_detection/tpu_exporters/faster_rcnn.py
index 2ea32809..854a3e8c 100644
--- a/research/object_detection/tpu_exporters/faster_rcnn.py
+++ b/research/object_detection/tpu_exporters/faster_rcnn.py
@@ -31,6 +31,8 @@ if int(major) < 1 or (int(major == 1) and int(minor) < 14):
 
 from tensorflow.python.framework import function
 from tensorflow.python.tpu import functional as tpu_functional
+from tensorflow.python.tpu import tpu
+from tensorflow.python.tpu.bfloat16 import bfloat16_scope
 from tensorflow.python.tpu.ops import tpu_ops
 from object_detection import exporter
 from object_detection.builders import model_builder
@@ -169,12 +171,12 @@ def build_graph(pipeline_config,
   @function.Defun(capture_resource_var_by_value=False)
   def tpu_subgraph_predict():
     if use_bfloat16:
-      with tf.contrib.tpu.bfloat16_scope():
-        return tf.contrib.tpu.rewrite(tpu_subgraph_predict_fn,
-                                      [preprocessed_inputs, true_image_shapes])
+      with bfloat16_scope():
+        return tpu.rewrite(tpu_subgraph_predict_fn,
+                           [preprocessed_inputs, true_image_shapes])
     else:
-      return tf.contrib.tpu.rewrite(tpu_subgraph_predict_fn,
-                                    [preprocessed_inputs, true_image_shapes])
+      return tpu.rewrite(tpu_subgraph_predict_fn,
+                         [preprocessed_inputs, true_image_shapes])
 
   (rpn_box_encodings, rpn_objectness_predictions_with_background, anchors,
    refined_box_encodings, class_predictions_with_background, num_proposals,
diff --git a/research/object_detection/tpu_exporters/ssd.py b/research/object_detection/tpu_exporters/ssd.py
index 6d434794..c36d8695 100644
--- a/research/object_detection/tpu_exporters/ssd.py
+++ b/research/object_detection/tpu_exporters/ssd.py
@@ -30,6 +30,8 @@ if int(major) < 1 or (int(major == 1) and int(minor) < 14):
 
 from tensorflow.python.framework import function
 from tensorflow.python.tpu import functional as tpu_functional
+from tensorflow.python.tpu import tpu
+from tensorflow.python.tpu.bfloat16 import bfloat16_scope
 from tensorflow.python.tpu.ops import tpu_ops
 from object_detection import exporter
 from object_detection.builders import model_builder
@@ -171,7 +173,7 @@ def build_graph(pipeline_config,
     # Dimshuffle: (b, c, h, w) -> (b, h, w, c)
     preprocessed_inputs = tf.transpose(preprocessed_inputs, perm=[0, 2, 3, 1])
     if use_bfloat16:
-      with tf.contrib.tpu.bfloat16_scope():
+      with bfloat16_scope():
         prediction_dict = detection_model.predict(preprocessed_inputs,
                                                   true_image_shapes)
     else:
@@ -188,8 +190,8 @@ def build_graph(pipeline_config,
 
   @function.Defun(capture_resource_var_by_value=False)
   def predict_tpu():
-    return tf.contrib.tpu.rewrite(predict_tpu_subgraph,
-                                  [preprocessed_inputs, true_image_shapes])
+    return tpu.rewrite(predict_tpu_subgraph,
+                       [preprocessed_inputs, true_image_shapes])
 
   prediction_outputs = tpu_functional.TPUPartitionedCall(
       args=predict_tpu.captured_inputs,
diff --git a/research/object_detection/tpu_exporters/utils_test.py b/research/object_detection/tpu_exporters/utils_test.py
index 74877290..ec8922c3 100644
--- a/research/object_detection/tpu_exporters/utils_test.py
+++ b/research/object_detection/tpu_exporters/utils_test.py
@@ -1,3 +1,4 @@
+# Lint as: python2, python3
 # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -18,6 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+from six.moves import range
 import tensorflow as tf
 
 from object_detection.tpu_exporters import utils
diff --git a/research/object_detection/utils/autoaugment_utils.py b/research/object_detection/utils/autoaugment_utils.py
index a0a901db..1060550b 100644
--- a/research/object_detection/utils/autoaugment_utils.py
+++ b/research/object_detection/utils/autoaugment_utils.py
@@ -24,6 +24,14 @@ import math
 import six
 import tensorflow as tf
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import image as contrib_image
+  from tensorflow.contrib import training as contrib_training
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 # This signifies the max integer that the controller RNN could predict for the
 # augmentation scheme.
@@ -315,7 +323,7 @@ def rotate(image, degrees, replace):
   # In practice, we should randomize the rotation degrees by flipping
   # it negatively half the time, but that's done on 'degrees' outside
   # of the function.
-  image = tf.contrib.image.rotate(wrap(image), radians)
+  image = contrib_image.rotate(wrap(image), radians)
   return unwrap(image, replace)
 
 
@@ -870,13 +878,13 @@ def rotate_with_bboxes(image, bboxes, degrees, replace):
 
 def translate_x(image, pixels, replace):
   """Equivalent of PIL Translate in X dimension."""
-  image = tf.contrib.image.translate(wrap(image), [-pixels, 0])
+  image = contrib_image.translate(wrap(image), [-pixels, 0])
   return unwrap(image, replace)
 
 
 def translate_y(image, pixels, replace):
   """Equivalent of PIL Translate in Y dimension."""
-  image = tf.contrib.image.translate(wrap(image), [0, -pixels])
+  image = contrib_image.translate(wrap(image), [0, -pixels])
   return unwrap(image, replace)
 
 
@@ -961,7 +969,7 @@ def shear_x(image, level, replace):
   # with a matrix form of:
   # [1  level
   #  0  1].
-  image = tf.contrib.image.transform(
+  image = contrib_image.transform(
       wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])
   return unwrap(image, replace)
 
@@ -972,7 +980,7 @@ def shear_y(image, level, replace):
   # with a matrix form of:
   # [1  0
   #  level  1].
-  image = tf.contrib.image.transform(
+  image = contrib_image.transform(
       wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])
   return unwrap(image, replace)
 
@@ -1628,9 +1636,12 @@ def distort_image_with_autoaugment(image, bboxes, augmentation_name):
 
   policy = available_policies[augmentation_name]()
   # Hparams that will be used for AutoAugment.
-  augmentation_hparams = tf.contrib.training.HParams(
-      cutout_max_pad_fraction=0.75, cutout_bbox_replace_with_mean=False,
-      cutout_const=100, translate_const=250, cutout_bbox_const=50,
+  augmentation_hparams = contrib_training.HParams(
+      cutout_max_pad_fraction=0.75,
+      cutout_bbox_replace_with_mean=False,
+      cutout_const=100,
+      translate_const=250,
+      cutout_bbox_const=50,
       translate_bbox_const=120)
 
   augmented_image, augmented_bbox = (
diff --git a/research/object_detection/utils/bifpn_utils.py b/research/object_detection/utils/bifpn_utils.py
new file mode 100644
index 00000000..b4b24435
--- /dev/null
+++ b/research/object_detection/utils/bifpn_utils.py
@@ -0,0 +1,350 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Functions to manipulate feature map pyramids, such as for FPNs and BiFPNs.
+
+Includes utility functions to facilitate feature pyramid map manipulations,
+such as combining multiple feature maps, upsampling or downsampling feature
+maps, and applying blocks of convolution, batchnorm, and activation layers.
+"""
+from six.moves import range
+import tensorflow as tf
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+
+
+def create_conv_block(name, num_filters, kernel_size, strides, padding,
+                      use_separable, apply_batchnorm, apply_activation,
+                      conv_hyperparams, is_training, freeze_batchnorm):
+  """Create Keras layers for regular or separable convolutions.
+
+  Args:
+    name: String. The name of the layer.
+    num_filters: Number of filters (channels) for the output feature maps.
+    kernel_size: A list of length 2: [kernel_height, kernel_width] of the
+      filters, or a single int if both values are the same.
+    strides: A list of length 2: [stride_height, stride_width], specifying the
+      convolution stride, or a single int if both strides are the same.
+    padding: One of 'VALID' or 'SAME'.
+    use_separable: Bool. Whether to use depthwise separable convolution instead
+      of regular convolution.
+    apply_batchnorm: Bool. Whether to apply a batch normalization layer after
+      convolution, constructed according to the conv_hyperparams.
+    apply_activation: Bool. Whether to apply an activation layer after
+      convolution, constructed according to the conv_hyperparams.
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    is_training: Bool. Whether the feature generator is in training mode.
+    freeze_batchnorm: Bool. Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+
+  Returns:
+    A list of keras layers, including (regular or seperable) convolution, and
+    optionally batch normalization and activation layers.
+  """
+  layers = []
+  if use_separable:
+    kwargs = conv_hyperparams.params()
+    # Both the regularizer and initializer apply to the depthwise layer,
+    # so we remap the kernel_* to depthwise_* here.
+    kwargs['depthwise_regularizer'] = kwargs['kernel_regularizer']
+    kwargs['depthwise_initializer'] = kwargs['kernel_initializer']
+    # TODO(aom): Verify that the pointwise regularizer/initializer should be set
+    # here, since this is not the case in feature_map_generators.py
+    kwargs['pointwise_regularizer'] = kwargs['kernel_regularizer']
+    kwargs['pointwise_initializer'] = kwargs['kernel_initializer']
+    layers.append(
+        tf.keras.layers.SeparableConv2D(
+            filters=num_filters,
+            kernel_size=kernel_size,
+            depth_multiplier=1,
+            padding=padding,
+            strides=strides,
+            name=name + '_separable_conv',
+            **kwargs))
+  else:
+    layers.append(
+        tf.keras.layers.Conv2D(
+            filters=num_filters,
+            kernel_size=kernel_size,
+            padding=padding,
+            strides=strides,
+            name=name + '_conv',
+            **conv_hyperparams.params()))
+
+  if apply_batchnorm:
+    layers.append(
+        conv_hyperparams.build_batch_norm(
+            training=(is_training and not freeze_batchnorm),
+            name=name + '_batchnorm'))
+
+  if apply_activation:
+    layers.append(
+        conv_hyperparams.build_activation_layer(name=name + '_activation'))
+
+  return layers
+
+
+def create_downsample_feature_map_ops(scale, downsample_method,
+                                      conv_hyperparams, is_training,
+                                      freeze_batchnorm, name):
+  """Creates Keras layers for downsampling feature maps.
+
+  Args:
+    scale: Int. The scale factor by which to downsample input feature maps. For
+      example, in the case of a typical feature map pyramid, the scale factor
+      between level_i and level_i+1 is 2.
+    downsample_method: String. The method used for downsampling. Currently
+      supported methods include 'max_pooling', 'avg_pooling', and
+      'depthwise_conv'.
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    is_training: Bool. Whether the feature generator is in training mode.
+    freeze_batchnorm: Bool. Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+    name: String. The name used to prefix the constructed layers.
+
+  Returns:
+    A list of Keras layers which will downsample input feature maps by the
+    desired scale factor.
+  """
+  layers = []
+  padding = 'SAME'
+  stride = int(scale)
+  kernel_size = stride + 1
+  if downsample_method == 'max_pooling':
+    layers.append(
+        tf.keras.layers.MaxPooling2D(
+            pool_size=kernel_size,
+            strides=stride,
+            padding=padding,
+            name=name + '_downsample_max_x{}'.format(stride)))
+  elif downsample_method == 'avg_pooling':
+    layers.append(
+        tf.keras.layers.AveragePooling2D(
+            pool_size=kernel_size,
+            strides=stride,
+            padding=padding,
+            name=name + '_downsample_avg_x{}'.format(stride)))
+  elif downsample_method == 'depthwise_conv':
+    layers.append(
+        tf.keras.layers.DepthwiseConv2D(
+            kernel_size=kernel_size,
+            strides=stride,
+            padding=padding,
+            name=name + '_downsample_depthwise_x{}'.format(stride)))
+    layers.append(
+        conv_hyperparams.build_batch_norm(
+            training=(is_training and not freeze_batchnorm),
+            name=name + '_downsample_batchnorm'))
+    layers.append(
+        conv_hyperparams.build_activation_layer(name=name +
+                                                '_downsample_activation'))
+  else:
+    raise ValueError('Unknown downsample method: {}'.format(downsample_method))
+
+  return layers
+
+
+def create_upsample_feature_map_ops(scale, use_native_resize_op, name):
+  """Creates Keras layers for upsampling feature maps.
+
+  Args:
+    scale: Int. The scale factor by which to upsample input feature maps. For
+      example, in the case of a typical feature map pyramid, the scale factor
+      between level_i and level_i-1 is 2.
+    use_native_resize_op: If True, uses tf.image.resize_nearest_neighbor op for
+      the upsampling process instead of reshape and broadcasting implementation.
+    name: String. The name used to prefix the constructed layers.
+
+  Returns:
+    A list of Keras layers which will upsample input feature maps by the
+    desired scale factor.
+  """
+  layers = []
+  if use_native_resize_op:
+
+    def resize_nearest_neighbor(image):
+      image_shape = shape_utils.combined_static_and_dynamic_shape(image)
+      return tf.image.resize_nearest_neighbor(
+          image, [image_shape[1] * scale, image_shape[2] * scale])
+
+    layers.append(
+        tf.keras.layers.Lambda(
+            resize_nearest_neighbor,
+            name=name + 'nearest_neighbor_upsampling_x{}'.format(scale)))
+  else:
+
+    def nearest_neighbor_upsampling(image):
+      return ops.nearest_neighbor_upsampling(image, scale=scale)
+
+    layers.append(
+        tf.keras.layers.Lambda(
+            nearest_neighbor_upsampling,
+            name=name + 'nearest_neighbor_upsampling_x{}'.format(scale)))
+
+  return layers
+
+
+def create_resample_feature_map_ops(input_scale_factor, output_scale_factor,
+                                    downsample_method, use_native_resize_op,
+                                    conv_hyperparams, is_training,
+                                    freeze_batchnorm, name):
+  """Creates Keras layers for downsampling or upsampling feature maps.
+
+  Args:
+    input_scale_factor: Int. Scale factor of the input feature map. For example,
+      for a feature pyramid where each successive level halves its spatial
+      resolution, the scale factor of a level is 2^level. The input and output
+      scale factors are used to compute the scale for upsampling or downsamling,
+      so they should be evenly divisible.
+    output_scale_factor: Int. Scale factor of the output feature map. See
+      input_scale_factor for additional details.
+    downsample_method: String. The method used for downsampling. See
+      create_downsample_feature_map_ops for details on supported methods.
+    use_native_resize_op: If True, uses tf.image.resize_nearest_neighbor op for
+      the upsampling process instead of reshape and broadcasting implementation.
+      See create_upsample_feature_map_ops for details.
+    conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object
+      containing hyperparameters for convolution ops.
+    is_training: Bool. Whether the feature generator is in training mode.
+    freeze_batchnorm: Bool. Whether to freeze batch norm parameters during
+      training or not. When training with a small batch size (e.g. 1), it is
+      desirable to freeze batch norm update and use pretrained batch norm
+      params.
+    name: String. The name used to prefix the constructed layers.
+
+  Returns:
+    A list of Keras layers which will downsample or upsample input feature maps
+    to match the desired output feature map scale.
+  """
+  if input_scale_factor < output_scale_factor:
+    if output_scale_factor % input_scale_factor != 0:
+      raise ValueError('Invalid scale factor: input scale 1/{} not divisible by'
+                       'output scale 1/{}'.format(input_scale_factor,
+                                                  output_scale_factor))
+    scale = output_scale_factor // input_scale_factor
+    return create_downsample_feature_map_ops(scale, downsample_method,
+                                             conv_hyperparams, is_training,
+                                             freeze_batchnorm, name)
+  elif input_scale_factor > output_scale_factor:
+    if input_scale_factor % output_scale_factor != 0:
+      raise ValueError('Invalid scale factor: input scale 1/{} not a divisor of'
+                       'output scale 1/{}'.format(input_scale_factor,
+                                                  output_scale_factor))
+    scale = input_scale_factor // output_scale_factor
+    return create_upsample_feature_map_ops(scale, use_native_resize_op, name)
+  else:
+    return []
+
+
+# TODO(aom): Add tests for this module in a followup CL.
+class BiFPNCombineLayer(tf.keras.layers.Layer):
+  """Combines multiple input feature maps into a single output feature map.
+
+  A Keras layer which combines multiple input feature maps into a single output
+  feature map, according to the desired combination method. Options for
+  combining feature maps include simple summation, or several types of weighted
+  sums using learned weights for each input feature map. These include
+  'weighted_sum', 'attention', and 'fast_attention'. For more details, see the
+  EfficientDet paper by Tan et al, see arxiv.org/abs/1911.09070.
+
+  Specifically, this layer takes a list of tensors as input, all of the same
+  shape, and returns a single tensor, also of the same shape.
+  """
+
+  def __init__(self, combine_method, **kwargs):
+    """Constructor.
+
+    Args:
+      combine_method: String. The method used to combine the input feature maps
+        into a single output feature map. One of 'sum', 'weighted_sum',
+        'attention', or 'fast_attention'.
+      **kwargs: Additional Keras layer arguments.
+    """
+    super(BiFPNCombineLayer, self).__init__(**kwargs)
+    self.combine_method = combine_method
+
+  def _combine_weighted_sum(self, inputs):
+    return tf.squeeze(
+        tf.linalg.matmul(tf.stack(inputs, axis=-1), self.per_input_weights),
+        axis=[-1])
+
+  def _combine_attention(self, inputs):
+    normalized_weights = tf.nn.softmax(self.per_input_weights)
+    return tf.squeeze(
+        tf.linalg.matmul(tf.stack(inputs, axis=-1), normalized_weights),
+        axis=[-1])
+
+  def _combine_fast_attention(self, inputs):
+    weights_non_neg = tf.nn.relu(self.per_input_weights)
+    normalizer = tf.reduce_sum(weights_non_neg) + 0.0001
+    normalized_weights = weights_non_neg / normalizer
+    return tf.squeeze(
+        tf.linalg.matmul(tf.stack(inputs, axis=-1), normalized_weights),
+        axis=[-1])
+
+  def build(self, input_shape):
+    if not isinstance(input_shape, list):
+      raise ValueError('A BiFPN combine layer should be called '
+                       'on a list of inputs.')
+    if len(input_shape) < 2:
+      raise ValueError('A BiFPN combine layer should be called '
+                       'on a list of at least 2 inputs. '
+                       'Got ' + str(len(input_shape)) + ' inputs.')
+    if self.combine_method == 'sum':
+      self._combine_op = tf.keras.layers.Add()
+    elif self.combine_method == 'weighted_sum':
+      self._combine_op = self._combine_weighted_sum
+    elif self.combine_method == 'attention':
+      self._combine_op = self._combine_attention
+    elif self.combine_method == 'fast_attention':
+      self._combine_op = self._combine_fast_attention
+    else:
+      raise ValueError('Unknown combine type: {}'.format(self.combine_method))
+    if self.combine_method in {'weighted_sum', 'attention', 'fast_attention'}:
+      self.per_input_weights = self.add_weight(
+          name='bifpn_combine_weights',
+          shape=(len(input_shape), 1),
+          initializer='ones',
+          trainable=True)
+    super(BiFPNCombineLayer, self).build(input_shape)
+
+  def call(self, inputs):
+    """Combines multiple input feature maps into a single output feature map.
+
+    Executed when calling the `.__call__` method on input.
+
+    Args:
+      inputs: A list of tensors where all tensors have the same shape, [batch,
+        height_i, width_i, depth_i].
+
+    Returns:
+      A single tensor, with the same shape as the input tensors,
+        [batch, height_i, width_i, depth_i].
+    """
+    return self._combine_op(inputs)
+
+  def compute_output_shape(self, input_shape):
+    output_shape = input_shape[0]
+    for i in range(1, len(input_shape)):
+      if input_shape[i] != output_shape:
+        raise ValueError(
+            'Inputs could not be combined. Shapes should match, '
+            'but input_shape[0] is {} while input_shape[{}] is {}'.format(
+                output_shape, i, input_shape[i]))
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 0c4e6bd9..a4215496 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -46,12 +46,13 @@ def get_image_resizer_config(model_config):
     ValueError: If the model type is not recognized.
   """
   meta_architecture = model_config.WhichOneof("model")
-  if meta_architecture == "faster_rcnn":
-    return model_config.faster_rcnn.image_resizer
-  if meta_architecture == "ssd":
-    return model_config.ssd.image_resizer
+  meta_architecture_config = getattr(model_config, meta_architecture)
 
-  raise ValueError("Unknown model type: {}".format(meta_architecture))
+  if hasattr(meta_architecture_config, "image_resizer"):
+    return getattr(meta_architecture_config, "image_resizer")
+  else:
+    raise ValueError("{} has no image_reszier_config".format(
+        meta_architecture))
 
 
 def get_spatial_image_size(image_resizer_config):
@@ -84,6 +85,40 @@ def get_spatial_image_size(image_resizer_config):
   raise ValueError("Unknown image resizer type.")
 
 
+def get_max_num_context_features(model_config):
+  """Returns maximum number of context features from a given config.
+
+  Args:
+    model_config: A model config file.
+
+  Returns:
+    An integer specifying the max number of context features if the model
+      config contains context_config, None otherwise
+
+  """
+  meta_architecture = model_config.WhichOneof("model")
+  meta_architecture_config = getattr(model_config, meta_architecture)
+
+  if hasattr(meta_architecture_config, "context_config"):
+    return meta_architecture_config.context_config.max_num_context_features
+
+
+def get_context_feature_length(model_config):
+  """Returns context feature length from a given config.
+
+  Args:
+    model_config: A model config file.
+
+  Returns:
+    An integer specifying the fixed length of each feature in context_features.
+  """
+  meta_architecture = model_config.WhichOneof("model")
+  meta_architecture_config = getattr(model_config, meta_architecture)
+
+  if hasattr(meta_architecture_config, "context_config"):
+    return meta_architecture_config.context_config.context_feature_length
+
+
 def get_configs_from_pipeline_file(pipeline_config_path, config_override=None):
   """Reads config from a file containing pipeline_pb2.TrainEvalPipelineConfig.
 
@@ -263,12 +298,12 @@ def get_number_of_classes(model_config):
     ValueError: If the model type is not recognized.
   """
   meta_architecture = model_config.WhichOneof("model")
-  if meta_architecture == "faster_rcnn":
-    return model_config.faster_rcnn.num_classes
-  if meta_architecture == "ssd":
-    return model_config.ssd.num_classes
+  meta_architecture_config = getattr(model_config, meta_architecture)
 
-  raise ValueError("Expected the model to be one of 'faster_rcnn' or 'ssd'.")
+  if hasattr(meta_architecture_config, "num_classes"):
+    return meta_architecture_config.num_classes
+  else:
+    raise ValueError("{} does not have num_classes.".format(meta_architecture))
 
 
 def get_optimizer_type(train_config):
@@ -555,6 +590,8 @@ def _maybe_update_config_with_key_value(configs, key, value):
   elif field_name == "retain_original_image_additional_channels_in_eval":
     _update_retain_original_image_additional_channels(configs["eval_config"],
                                                       value)
+  elif field_name == "num_classes":
+    _update_num_classes(configs["model"], value)
   else:
     return False
   return True
@@ -885,6 +922,8 @@ def _update_label_map_path(configs, label_map_path):
   _update_all_eval_input_configs(configs, "label_map_path", label_map_path)
 
 
+
+
 def _update_mask_type(configs, mask_type):
   """Updates the mask type for both train and eval input readers.
 
@@ -997,3 +1036,11 @@ def remove_unecessary_ema(variables_to_restore, no_ema_collection=None):
                                            "")] = variables_to_restore[key]
           del variables_to_restore[key]
   return variables_to_restore
+
+
+def _update_num_classes(model_config, num_classes):
+  meta_architecture = model_config.WhichOneof("model")
+  if meta_architecture == "faster_rcnn":
+    model_config.faster_rcnn.num_classes = num_classes
+  if meta_architecture == "ssd":
+    model_config.ssd.num_classes = num_classes
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index 245c9f3f..725551ca 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -33,6 +33,14 @@ from object_detection.protos import pipeline_pb2
 from object_detection.protos import train_pb2
 from object_detection.utils import config_util
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import training as contrib_training
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 
 def _write_config(config, config_path):
   """Writes a config object to disk."""
@@ -209,7 +217,7 @@ class ConfigUtilTest(tf.test.TestCase):
     original_learning_rate = 0.7
     learning_rate_scaling = 0.1
     warmup_learning_rate = 0.07
-    hparams = tf.contrib.training.HParams(learning_rate=0.15)
+    hparams = contrib_training.HParams(learning_rate=0.15)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     # Constant learning rate.
@@ -302,7 +310,7 @@ class ConfigUtilTest(tf.test.TestCase):
 
     # Override each of the parameters:
     configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
-    hparams = tf.contrib.training.HParams(
+    hparams = contrib_training.HParams(
         **{
             "model.ssd.num_classes": 2,
             "train_config.batch_size": 2,
@@ -324,7 +332,7 @@ class ConfigUtilTest(tf.test.TestCase):
   def testNewBatchSize(self):
     """Tests that batch size is updated appropriately."""
     original_batch_size = 2
-    hparams = tf.contrib.training.HParams(batch_size=16)
+    hparams = contrib_training.HParams(batch_size=16)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -339,7 +347,7 @@ class ConfigUtilTest(tf.test.TestCase):
   def testNewBatchSizeWithClipping(self):
     """Tests that batch size is clipped to 1 from below."""
     original_batch_size = 2
-    hparams = tf.contrib.training.HParams(batch_size=0.5)
+    hparams = contrib_training.HParams(batch_size=0.5)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -356,7 +364,7 @@ class ConfigUtilTest(tf.test.TestCase):
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
     pipeline_config.train_config.batch_size = 2
     configs = self._create_and_load_test_configs(pipeline_config)
-    hparams = tf.contrib.training.HParams(**{"train_config.batch_size": 10})
+    hparams = contrib_training.HParams(**{"train_config.batch_size": 10})
     configs = config_util.merge_external_params_with_configs(configs, hparams)
     new_batch_size = configs["train_config"].batch_size
     self.assertEqual(10, new_batch_size)
@@ -365,7 +373,7 @@ class ConfigUtilTest(tf.test.TestCase):
     """Tests that overwriting with a bad key causes an exception."""
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
     configs = self._create_and_load_test_configs(pipeline_config)
-    hparams = tf.contrib.training.HParams(**{"train_config.no_such_field": 10})
+    hparams = contrib_training.HParams(**{"train_config.no_such_field": 10})
     with self.assertRaises(ValueError):
       config_util.merge_external_params_with_configs(configs, hparams)
 
@@ -375,14 +383,14 @@ class ConfigUtilTest(tf.test.TestCase):
     pipeline_config.train_config.batch_size = 2
     configs = self._create_and_load_test_configs(pipeline_config)
     # Type should be an integer, but we're passing a string "10".
-    hparams = tf.contrib.training.HParams(**{"train_config.batch_size": "10"})
+    hparams = contrib_training.HParams(**{"train_config.batch_size": "10"})
     with self.assertRaises(TypeError):
       config_util.merge_external_params_with_configs(configs, hparams)
 
   def testNewMomentumOptimizerValue(self):
     """Tests that new momentum value is updated appropriately."""
     original_momentum_value = 0.4
-    hparams = tf.contrib.training.HParams(momentum_optimizer_value=1.1)
+    hparams = contrib_training.HParams(momentum_optimizer_value=1.1)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -401,7 +409,7 @@ class ConfigUtilTest(tf.test.TestCase):
     original_localization_weight = 0.1
     original_classification_weight = 0.2
     new_weight_ratio = 5.0
-    hparams = tf.contrib.training.HParams(
+    hparams = contrib_training.HParams(
         classification_localization_weight_ratio=new_weight_ratio)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
@@ -424,7 +432,7 @@ class ConfigUtilTest(tf.test.TestCase):
     original_gamma = 1.0
     new_alpha = 0.3
     new_gamma = 2.0
-    hparams = tf.contrib.training.HParams(
+    hparams = contrib_training.HParams(
         focal_loss_alpha=new_alpha, focal_loss_gamma=new_gamma)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
@@ -623,6 +631,20 @@ class ConfigUtilTest(tf.test.TestCase):
     image_shape = config_util.get_spatial_image_size(image_resizer_config)
     self.assertAllEqual(image_shape, [-1, -1])
 
+  def testGetMaxNumContextFeaturesFromModelConfig(self):
+    model_config = model_pb2.DetectionModel()
+    model_config.faster_rcnn.context_config.max_num_context_features = 10
+    max_num_context_features = config_util.get_max_num_context_features(
+        model_config)
+    self.assertAllEqual(max_num_context_features, 10)
+
+  def testGetContextFeatureLengthFromModelConfig(self):
+    model_config = model_pb2.DetectionModel()
+    model_config.faster_rcnn.context_config.context_feature_length = 100
+    context_feature_length = config_util.get_context_feature_length(
+        model_config)
+    self.assertAllEqual(context_feature_length, 100)
+
   def testEvalShuffle(self):
     """Tests that `eval_shuffle` keyword arguments are applied correctly."""
     original_shuffle = True
@@ -895,6 +917,22 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(desired_retain_original_image_additional_channels,
                      retain_original_image_additional_channels)
 
+  def testUpdateNumClasses(self):
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.model.faster_rcnn.num_classes = 10
+
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+
+    self.assertEqual(config_util.get_number_of_classes(configs["model"]), 10)
+
+    config_util.merge_external_params_with_configs(
+        configs, kwargs_dict={"num_classes": 2})
+
+    self.assertEqual(config_util.get_number_of_classes(configs["model"]), 2)
+
   def testRemoveUnecessaryEma(self):
     input_dict = {
         "expanded_conv_10/project/act_quant/min":
diff --git a/research/object_detection/utils/json_utils.py b/research/object_detection/utils/json_utils.py
index c8d09eb0..a620d86c 100644
--- a/research/object_detection/utils/json_utils.py
+++ b/research/object_detection/utils/json_utils.py
@@ -18,7 +18,15 @@ json_utils wraps json.dump and json.dumps so that they can be used to safely
 control the precision of floats when writing to json strings or files.
 """
 import json
-from json import encoder
+import re
+
+
+def FormatFloat(json_str, float_digits):
+  pattern = re.compile(r'\d+\.\d+')
+  float_repr = '{:.' + '{}'.format(float_digits) + 'f}'
+  def MRound(match):
+    return float_repr.format(float(match.group()))
+  return re.sub(pattern, MRound, json_str)
 
 
 def Dump(obj, fid, float_digits=-1, **params):
@@ -30,13 +38,8 @@ def Dump(obj, fid, float_digits=-1, **params):
     float_digits: The number of digits of precision when writing floats out.
     **params: Additional parameters to pass to json.dumps.
   """
-  original_encoder = encoder.FLOAT_REPR
-  if float_digits >= 0:
-    encoder.FLOAT_REPR = lambda o: format(o, '.%df' % float_digits)
-  try:
-    json.dump(obj, fid, **params)
-  finally:
-    encoder.FLOAT_REPR = original_encoder
+  json_str = Dumps(obj, float_digits, **params)
+  fid.write(json_str)
 
 
 def Dumps(obj, float_digits=-1, **params):
@@ -50,18 +53,10 @@ def Dumps(obj, float_digits=-1, **params):
   Returns:
     output: JSON string representation of obj.
   """
-  original_encoder = encoder.FLOAT_REPR
-  original_c_make_encoder = encoder.c_make_encoder
-  if float_digits >= 0:
-    encoder.FLOAT_REPR = lambda o: format(o, '.%df' % float_digits)
-    encoder.c_make_encoder = None
-  try:
-    output = json.dumps(obj, **params)
-  finally:
-    encoder.FLOAT_REPR = original_encoder
-    encoder.c_make_encoder = original_c_make_encoder
-
-  return output
+  json_str = json.dumps(obj, **params)
+  if float_digits > -1:
+    json_str = FormatFloat(json_str, float_digits)
+  return json_str
 
 
 def PrettyParams(**params):
diff --git a/research/object_detection/utils/json_utils_test.py b/research/object_detection/utils/json_utils_test.py
index 5e379043..bd68ef99 100644
--- a/research/object_detection/utils/json_utils_test.py
+++ b/research/object_detection/utils/json_utils_test.py
@@ -32,9 +32,9 @@ class JsonUtilsTest(tf.test.TestCase):
   def testDumpPassExtraParams(self):
     output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
     with tf.gfile.GFile(output_path, 'w') as f:
-      json_utils.Dump([1.0], f, float_digits=2, indent=3)
+      json_utils.Dump([1.12345], f, float_digits=2, indent=3)
     with tf.gfile.GFile(output_path, 'r') as f:
-      self.assertEqual(f.read(), '[\n   1.00\n]')
+      self.assertEqual(f.read(), '[\n   1.12\n]')
 
   def testDumpZeroPrecision(self):
     output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
@@ -51,8 +51,8 @@ class JsonUtilsTest(tf.test.TestCase):
       self.assertEqual(f.read(), '1.012345')
 
   def testDumpsReasonablePrecision(self):
-    s = json_utils.Dumps(1.0, float_digits=2)
-    self.assertEqual(s, '1.00')
+    s = json_utils.Dumps(1.12545, float_digits=2)
+    self.assertEqual(s, '1.13')
 
   def testDumpsPassExtraParams(self):
     s = json_utils.Dumps([1.0], float_digits=2, indent=3)
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index e52b7d1a..516744bc 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -85,6 +85,8 @@ def convert_label_map_to_categories(label_map,
     'id': (required) an integer id uniquely identifying this category.
     'name': (required) string representing category name
       e.g., 'cat', 'dog', 'pizza'.
+    'keypoints': (optional) a dictionary of keypoint string 'label' to integer
+      'id'.
   We only allow class into the list if its id-label_id_offset is
   between 0 (inclusive) and max_num_classes (exclusive).
   If there are several items mapping to the same id in the label map,
@@ -123,7 +125,18 @@ def convert_label_map_to_categories(label_map,
       name = item.name
     if item.id not in list_of_ids_already_added:
       list_of_ids_already_added.append(item.id)
-      categories.append({'id': item.id, 'name': name})
+      category = {'id': item.id, 'name': name}
+      if item.keypoints:
+        keypoints = {}
+        list_of_keypoint_ids = []
+        for kv in item.keypoints:
+          if kv.id in list_of_keypoint_ids:
+            raise ValueError('Duplicate keypoint ids are not allowed. '
+                             'Found {} more than once'.format(kv.id))
+          keypoints[kv.label] = kv.id
+          list_of_keypoint_ids.append(kv.id)
+        category['keypoints'] = keypoints
+      categories.append(category)
   return categories
 
 
@@ -135,7 +148,7 @@ def load_labelmap(path):
   Returns:
     a StringIntLabelMapProto
   """
-  with tf.gfile.GFile(path, 'r') as fid:
+  with tf.io.gfile.GFile(path, 'r') as fid:
     label_map_string = fid.read()
     label_map = string_int_label_map_pb2.StringIntLabelMap()
     try:
@@ -210,6 +223,8 @@ def create_categories_from_labelmap(label_map_path, use_display_name=True):
   which  has the following keys:
     'id': an integer id uniquely identifying this category.
     'name': string representing category name e.g., 'cat', 'dog'.
+    'keypoints': a dictionary of keypoint string label to integer id. It is only
+      returned when available in label map proto.
 
   Args:
     label_map_path: Path to `StringIntLabelMap` proto text file.
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index 08f2f207..b0de44e2 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -226,6 +226,59 @@ class LabelMapUtilTest(tf.test.TestCase):
     }]
     self.assertListEqual(expected_categories_list, categories)
 
+  def test_convert_label_map_with_keypoints_to_categories(self):
+    label_map_str = """
+      item {
+        id: 1
+        name: 'person'
+        keypoints: {
+          id: 1
+          label: 'nose'
+        }
+        keypoints: {
+          id: 2
+          label: 'ear'
+        }
+      }
+    """
+    label_map_proto = string_int_label_map_pb2.StringIntLabelMap()
+    text_format.Merge(label_map_str, label_map_proto)
+    categories = label_map_util.convert_label_map_to_categories(
+        label_map_proto, max_num_classes=1)
+    self.assertEqual('person', categories[0]['name'])
+    self.assertEqual(1, categories[0]['id'])
+    self.assertEqual(1, categories[0]['keypoints']['nose'])
+    self.assertEqual(2, categories[0]['keypoints']['ear'])
+
+  def test_disallow_duplicate_keypoint_ids(self):
+    label_map_str = """
+      item {
+        id: 1
+        name: 'person'
+        keypoints: {
+          id: 1
+          label: 'right_elbow'
+        }
+        keypoints: {
+          id: 1
+          label: 'left_elbow'
+        }
+      }
+      item {
+        id: 2
+        name: 'face'
+        keypoints: {
+          id: 3
+          label: 'ear'
+        }
+      }
+    """
+    label_map_proto = string_int_label_map_pb2.StringIntLabelMap()
+    text_format.Merge(label_map_str, label_map_proto)
+    with self.assertRaises(ValueError):
+      label_map_util.convert_label_map_to_categories(
+          label_map_proto, max_num_classes=2)
+
   def test_convert_label_map_to_categories_with_few_classes(self):
     label_map_proto = self._generate_label_map(num_classes=4)
     cat_no_offset = label_map_util.convert_label_map_to_categories(
diff --git a/research/object_detection/utils/model_util.py b/research/object_detection/utils/model_util.py
index 7196f7b4..b2378993 100644
--- a/research/object_detection/utils/model_util.py
+++ b/research/object_detection/utils/model_util.py
@@ -54,8 +54,8 @@ def extract_submodel(model, inputs, outputs, name=None):
   for layer in model.layers:
     layer_output = layer.output
     layer_inputs = layer.input
-    output_to_layer[layer_output] = layer
-    output_to_layer_input[layer_output] = layer_inputs
+    output_to_layer[layer_output.ref()] = layer
+    output_to_layer_input[layer_output.ref()] = layer_inputs
 
   model_inputs_dict = {}
   memoized_results = {}
@@ -63,20 +63,21 @@ def extract_submodel(model, inputs, outputs, name=None):
   # Relies on recursion, very low limit in python
   def _recurse_in_model(tensor):
     """Walk the existing model recursively to copy a submodel."""
-    if tensor in memoized_results:
-      return memoized_results[tensor]
-    if (tensor == inputs) or (isinstance(inputs, list) and tensor in inputs):
-      if tensor not in model_inputs_dict:
-        model_inputs_dict[tensor] = tf.keras.layers.Input(tensor=tensor)
-      out = model_inputs_dict[tensor]
+    if tensor.ref() in memoized_results:
+      return memoized_results[tensor.ref()]
+    if (tensor.ref() == inputs.ref()) or (
+        isinstance(inputs, list) and tensor in inputs):
+      if tensor.ref() not in model_inputs_dict:
+        model_inputs_dict[tensor.ref()] = tf.keras.layers.Input(tensor=tensor)
+      out = model_inputs_dict[tensor.ref()]
     else:
-      cur_inputs = output_to_layer_input[tensor]
-      cur_layer = output_to_layer[tensor]
+      cur_inputs = output_to_layer_input[tensor.ref()]
+      cur_layer = output_to_layer[tensor.ref()]
       if isinstance(cur_inputs, list):
         out = cur_layer([_recurse_in_model(inp) for inp in cur_inputs])
       else:
         out = cur_layer(_recurse_in_model(cur_inputs))
-    memoized_results[tensor] = out
+    memoized_results[tensor.ref()] = out
     return out
 
   if isinstance(outputs, list):
@@ -85,8 +86,8 @@ def extract_submodel(model, inputs, outputs, name=None):
     model_outputs = _recurse_in_model(outputs)
 
   if isinstance(inputs, list):
-    model_inputs = [model_inputs_dict[tensor] for tensor in inputs]
+    model_inputs = [model_inputs_dict[tensor.ref()] for tensor in inputs]
   else:
-    model_inputs = model_inputs_dict[inputs]
+    model_inputs = model_inputs_dict[inputs.ref()]
 
   return tf.keras.Model(inputs=model_inputs, outputs=model_outputs, name=name)
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index c4b6101a..5bdf866a 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -31,6 +31,13 @@ from object_detection.utils import shape_utils
 from object_detection.utils import spatial_transform_ops as spatial_ops
 from object_detection.utils import static_shape
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import framework as contrib_framework
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 matmul_crop_and_resize = spatial_ops.matmul_crop_and_resize
 multilevel_roi_align = spatial_ops.multilevel_roi_align
@@ -588,8 +595,9 @@ def normalize_to_target(inputs,
       initial_norm = depth * [target_norm_value]
     else:
       initial_norm = target_norm_value
-    target_norm = tf.contrib.framework.model_variable(
-        name='weights', dtype=tf.float32,
+    target_norm = contrib_framework.model_variable(
+        name='weights',
+        dtype=tf.float32,
         initializer=tf.constant(initial_norm, dtype=tf.float32),
         trainable=trainable)
     if summarize:
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 7b41dcda..a903fecc 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -27,7 +27,14 @@ from object_detection.core import standard_fields as fields
 from object_detection.utils import ops
 from object_detection.utils import test_case
 
-slim = tf.contrib.slim
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import framework as contrib_framework
+  from tensorflow.contrib import slim
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
 
 
 class NormalizedToImageCoordinatesTest(tf.test.TestCase):
@@ -760,7 +767,7 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
     with self.test_session():
       output = ops.normalize_to_target(inputs, target_norm_value, dim)
       self.assertEqual(output.op.name, 'NormalizeToTarget/mul')
-      var_name = tf.contrib.framework.get_variables()[0].name
+      var_name = contrib_framework.get_variables()[0].name
       self.assertEqual(var_name, 'NormalizeToTarget/weights:0')
 
   def test_invalid_dim(self):
diff --git a/research/object_detection/utils/patch_ops_test.py b/research/object_detection/utils/patch_ops_test.py
index caf3567a..2dc57251 100644
--- a/research/object_detection/utils/patch_ops_test.py
+++ b/research/object_detection/utils/patch_ops_test.py
@@ -24,9 +24,10 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.utils import patch_ops
+from object_detection.utils import test_case
 
 
-class GetPatchMaskTest(tf.test.TestCase, parameterized.TestCase):
+class GetPatchMaskTest(test_case.TestCase, parameterized.TestCase):
 
   def testMaskShape(self):
     image_shape = [15, 10]
@@ -108,13 +109,17 @@ class GetPatchMaskTest(tf.test.TestCase, parameterized.TestCase):
       patch_ops.get_patch_mask(y, x, patch_size=3, image_shape=image_shape)
 
   def testDynamicCoordinatesOutsideImageRaisesError(self):
-    image_shape = [15, 10]
-    x = tf.random_uniform([], minval=-2, maxval=-1, dtype=tf.int32)
-    y = tf.random_uniform([], minval=0, maxval=1, dtype=tf.int32)
-    mask = patch_ops.get_patch_mask(
-        y, x, patch_size=3, image_shape=image_shape)
+
+    def graph_fn():
+      image_shape = [15, 10]
+      x = tf.random_uniform([], minval=-2, maxval=-1, dtype=tf.int32)
+      y = tf.random_uniform([], minval=0, maxval=1, dtype=tf.int32)
+      mask = patch_ops.get_patch_mask(
+          y, x, patch_size=3, image_shape=image_shape)
+      return mask
+
     with self.assertRaises(tf.errors.InvalidArgumentError):
-      self.evaluate(mask)
+      self.execute(graph_fn, [])
 
   @parameterized.parameters(
       {'patch_size': 0},
@@ -127,12 +132,17 @@ class GetPatchMaskTest(tf.test.TestCase, parameterized.TestCase):
           0, 0, patch_size=patch_size, image_shape=image_shape)
 
   def testDynamicNonPositivePatchSizeRaisesError(self):
-    image_shape = [6, 7]
-    patch_size = -1 * tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
-    mask = patch_ops.get_patch_mask(
-        0, 0, patch_size=patch_size, image_shape=image_shape)
+
+    def graph_fn():
+      image_shape = [6, 7]
+      patch_size = -1 * tf.random_uniform([], minval=0, maxval=3,
+                                          dtype=tf.int32)
+      mask = patch_ops.get_patch_mask(
+          0, 0, patch_size=patch_size, image_shape=image_shape)
+      return mask
+
     with self.assertRaises(tf.errors.InvalidArgumentError):
-      self.evaluate(mask)
+      self.execute(graph_fn, [])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index 5506da99..8372b5b4 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -24,6 +24,14 @@ import tensorflow as tf
 
 from object_detection.utils import shape_utils
 
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import framework as contrib_framework
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
 
 class UtilTest(tf.test.TestCase):
 
@@ -124,7 +132,7 @@ class UtilTest(tf.test.TestCase):
     tensor = tf.placeholder(tf.float32, shape=(None, 2, 3))
     combined_shape = shape_utils.combined_static_and_dynamic_shape(
         tensor)
-    self.assertTrue(tf.contrib.framework.is_tensor(combined_shape[0]))
+    self.assertTrue(contrib_framework.is_tensor(combined_shape[0]))
     self.assertListEqual(combined_shape[1:], [2, 3])
 
   def test_pad_or_clip_nd_tensor(self):
diff --git a/research/object_detection/utils/spatial_transform_ops.py b/research/object_detection/utils/spatial_transform_ops.py
index a029a4ac..b0dee876 100644
--- a/research/object_detection/utils/spatial_transform_ops.py
+++ b/research/object_detection/utils/spatial_transform_ops.py
@@ -217,11 +217,20 @@ def pad_to_max_size(features):
     true_feature_shapes: A 2D int32 tensor of shape [num_levels, 2] containing
       height and width of the feature maps before padding.
   """
-  heights = [tf.shape(feature)[1] for feature in features]
-  widths = [tf.shape(feature)[2] for feature in features]
-  max_height = tf.reduce_max(heights)
-  max_width = tf.reduce_max(widths)
-
+  if len(features) == 1:
+    return tf.expand_dims(features[0],
+                          1), tf.expand_dims(tf.shape(features[0])[1:3], 0)
+
+  if all([feature.shape.is_fully_defined() for feature in features]):
+    heights = [feature.shape[1] for feature in features]
+    widths = [feature.shape[2] for feature in features]
+    max_height = max(heights)
+    max_width = max(widths)
+  else:
+    heights = [tf.shape(feature)[1] for feature in features]
+    widths = [tf.shape(feature)[2] for feature in features]
+    max_height = tf.reduce_max(heights)
+    max_width = tf.reduce_max(widths)
   features_all = [
       tf.image.pad_to_bounding_box(feature, 0, 0, max_height,
                                    max_width) for feature in features
@@ -405,7 +414,7 @@ def multilevel_roi_align(features, boxes, box_levels, output_size,
 def native_crop_and_resize(image, boxes, crop_size, scope=None):
   """Same as `matmul_crop_and_resize` but uses tf.image.crop_and_resize."""
   def get_box_inds(proposals):
-    proposals_shape = proposals.get_shape().as_list()
+    proposals_shape = proposals.shape.as_list()
     if any(dim is None for dim in proposals_shape):
       proposals_shape = tf.shape(proposals)
     ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)
diff --git a/research/object_detection/utils/spatial_transform_ops_test.py b/research/object_detection/utils/spatial_transform_ops_test.py
index 816115d4..6d12968e 100644
--- a/research/object_detection/utils/spatial_transform_ops_test.py
+++ b/research/object_detection/utils/spatial_transform_ops_test.py
@@ -360,7 +360,7 @@ class MultiLevelRoIAlignTest(test_case.TestCase):
     self.assertAllClose(roi_features[0][4], 5 * np.ones((2, 2, 1)))
 
   def test_large_input(self):
-    if test_case.FLAGS.tpu_test:
+    if self.has_tpu():
       input_size = 1408
       min_level = 2
       max_level = 6
@@ -368,36 +368,31 @@ class MultiLevelRoIAlignTest(test_case.TestCase):
       num_boxes = 512
       num_filters = 256
       output_size = [7, 7]
-      with self.test_session() as sess:
-        features = []
-        for level in range(min_level, max_level + 1):
-          feat_size = int(input_size / 2**level)
-          features.append(tf.constant(
-              np.reshape(
-                  np.arange(
-                      batch_size * feat_size * feat_size * num_filters,
-                      dtype=np.float32),
-                  [batch_size, feat_size, feat_size, num_filters]),
-              dtype=tf.bfloat16))
-        boxes = np.array([
-            [[0, 0, 256, 256]]*num_boxes,
-        ], dtype=np.float32) / input_size
-        boxes = np.tile(boxes, [batch_size, 1, 1])
-        tf_boxes = tf.constant(boxes)
-        tf_levels = tf.random_uniform([batch_size, num_boxes], maxval=5,
-                                      dtype=tf.int32)
-        def crop_and_resize_fn():
-          return spatial_ops.multilevel_roi_align(
-              features, tf_boxes, tf_levels, output_size)
-
-        tpu_crop_and_resize_fn = tf.contrib.tpu.rewrite(crop_and_resize_fn)
-        sess.run(tf.contrib.tpu.initialize_system())
-        sess.run(tf.global_variables_initializer())
-        roi_features = sess.run(tpu_crop_and_resize_fn)
-        self.assertEqual(roi_features[0].shape,
-                         (batch_size, num_boxes, output_size[0], output_size[1],
-                          num_filters))
-        sess.run(tf.contrib.tpu.shutdown_system())
+      features = []
+      for level in range(min_level, max_level + 1):
+        feat_size = int(input_size / 2**level)
+        features.append(
+            np.reshape(
+                np.arange(
+                    batch_size * feat_size * feat_size * num_filters,
+                    dtype=np.float32),
+                [batch_size, feat_size, feat_size, num_filters]))
+      boxes = np.array([
+          [[0, 0, 256, 256]]*num_boxes,
+      ], dtype=np.float32) / input_size
+      boxes = np.tile(boxes, [batch_size, 1, 1])
+      levels = np.random.randint(5, size=[batch_size, num_boxes],
+                                 dtype=np.int32)
+      def crop_and_resize_fn():
+        tf_features = [
+            tf.constant(feature, dtype=tf.bfloat16) for feature in features
+        ]
+        return spatial_ops.multilevel_roi_align(
+            tf_features, tf.constant(boxes), tf.constant(levels), output_size)
+      roi_features = self.execute_tpu(crop_and_resize_fn, [])
+      self.assertEqual(roi_features.shape,
+                       (batch_size, num_boxes, output_size[0],
+                        output_size[1], num_filters))
 
 
 class MatMulCropAndResizeTest(test_case.TestCase):
@@ -517,13 +512,6 @@ class MatMulCropAndResizeTest(test_case.TestCase):
     crop_output = self.execute(graph_fn, [image, boxes])
     self.assertAllClose(crop_output, expected_output)
 
-  def testInvalidInputShape(self):
-    image = tf.constant([[[1], [2]], [[3], [4]]], dtype=tf.float32)
-    boxes = tf.constant([[-1, -1, 1, 1]], dtype=tf.float32)
-    crop_size = [4, 4]
-    with self.assertRaises(ValueError):
-      spatial_ops.matmul_crop_and_resize(image, boxes, crop_size)
-
 
 class NativeCropAndResizeTest(test_case.TestCase):
 
diff --git a/research/object_detection/utils/target_assigner_utils.py b/research/object_detection/utils/target_assigner_utils.py
new file mode 100644
index 00000000..9a2707d6
--- /dev/null
+++ b/research/object_detection/utils/target_assigner_utils.py
@@ -0,0 +1,259 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Utility functions used by target assigner."""
+
+import tensorflow as tf
+
+from object_detection.utils import shape_utils
+
+
+def image_shape_to_grids(height, width):
+  """Computes xy-grids given the shape of the image.
+
+  Args:
+    height: The height of the image.
+    width: The width of the image.
+
+  Returns:
+    A tuple of two tensors:
+      y_grid: A float tensor with shape [height, width] representing the
+        y-coordinate of each pixel grid.
+      x_grid: A float tensor with shape [height, width] representing the
+        x-coordinate of each pixel grid.
+  """
+  out_height = tf.cast(height, tf.float32)
+  out_width = tf.cast(width, tf.float32)
+  x_range = tf.range(out_width, dtype=tf.float32)
+  y_range = tf.range(out_height, dtype=tf.float32)
+  x_grid, y_grid = tf.meshgrid(x_range, y_range, indexing='xy')
+  return (y_grid, x_grid)
+
+
+def coordinates_to_heatmap(y_grid,
+                           x_grid,
+                           y_coordinates,
+                           x_coordinates,
+                           sigma,
+                           channel_onehot,
+                           channel_weights=None):
+  """Returns the heatmap targets from a set of point coordinates.
+
+  This function maps a set of point coordinates to the output heatmap image
+  applied using a Gaussian kernel. Note that this function be can used by both
+  object detection and keypoint estimation tasks. For object detection, the
+  "channel" refers to the object class. For keypoint estimation, the "channel"
+  refers to the number of keypoint types.
+
+  Args:
+    y_grid: A 2D tensor with shape [height, width] which contains the grid
+      y-coordinates given in the (output) image dimensions.
+    x_grid: A 2D tensor with shape [height, width] which contains the grid
+      x-coordinates given in the (output) image dimensions.
+    y_coordinates: A 1D tensor with shape [num_instances] representing the
+      y-coordinates of the instances in the output space coordinates.
+    x_coordinates: A 1D tensor with shape [num_instances] representing the
+      x-coordinates of the instances in the output space coordinates.
+    sigma: A 1D tensor with shape [num_instances] representing the standard
+      deviation of the Gaussian kernel to be applied to the point.
+    channel_onehot: A 2D tensor with shape [num_instances, num_channels]
+      representing the one-hot encoded channel labels for each point.
+    channel_weights: A 1D tensor with shape [num_instances] corresponding to the
+      weight of each instance.
+
+  Returns:
+    heatmap: A tensor of size [height, width, num_channels] representing the
+      heatmap. Output (height, width) match the dimensions of the input grids.
+  """
+  num_instances, num_channels = (
+      shape_utils.combined_static_and_dynamic_shape(channel_onehot))
+
+  x_grid = tf.expand_dims(x_grid, 2)
+  y_grid = tf.expand_dims(y_grid, 2)
+  # The raw center coordinates in the output space.
+  x_diff = x_grid - tf.math.floor(x_coordinates)
+  y_diff = y_grid - tf.math.floor(y_coordinates)
+  squared_distance = x_diff**2 + y_diff**2
+
+  gaussian_map = tf.exp(-squared_distance / (2 * sigma * sigma))
+
+  reshaped_gaussian_map = tf.expand_dims(gaussian_map, axis=-1)
+  reshaped_channel_onehot = tf.reshape(channel_onehot,
+                                       (1, 1, num_instances, num_channels))
+  gaussian_per_box_per_class_map = (
+      reshaped_gaussian_map * reshaped_channel_onehot)
+
+  if channel_weights is not None:
+    reshaped_weights = tf.reshape(channel_weights, (1, 1, num_instances, 1))
+    gaussian_per_box_per_class_map *= reshaped_weights
+
+  # Take maximum along the "instance" dimension so that all per-instance
+  # heatmaps of the same class are merged together.
+  heatmap = tf.reduce_max(gaussian_per_box_per_class_map, axis=2)
+
+  # Maximum of an empty tensor is -inf, the following is to avoid that.
+  heatmap = tf.maximum(heatmap, 0)
+
+  return heatmap
+
+
+def compute_floor_offsets_with_indices(y_source,
+                                       x_source,
+                                       y_target=None,
+                                       x_target=None):
+  """Computes offsets from floored source(floored) to target coordinates.
+
+  This function computes the offsets from source coordinates ("floored" as if
+  they were put on the grids) to target coordinates. Note that the input
+  coordinates should be the "absolute" coordinates in terms of the output image
+  dimensions as opposed to the normalized coordinates (i.e. values in [0, 1]).
+
+  Args:
+    y_source: A tensor with shape [num_points] representing the absolute
+      y-coordinates (in the output image space) of the source points.
+    x_source: A tensor with shape [num_points] representing the absolute
+      x-coordinates (in the output image space) of the source points.
+    y_target: A tensor with shape [num_points] representing the absolute
+      y-coordinates (in the output image space) of the target points. If not
+      provided, then y_source is used as the targets.
+    x_target: A tensor with shape [num_points] representing the absolute
+      x-coordinates (in the output image space) of the target points. If not
+      provided, then x_source is used as the targets.
+
+  Returns:
+    A tuple of two tensors:
+      offsets: A tensor with shape [num_points, 2] representing the offsets of
+        each input point.
+      indices: A tensor with shape [num_points, 2] representing the indices of
+        where the offsets should be retrieved in the output image dimension
+        space.
+  """
+  y_source_floored = tf.floor(y_source)
+  x_source_floored = tf.floor(x_source)
+  if y_target is None:
+    y_target = y_source
+  if x_target is None:
+    x_target = x_source
+
+  y_offset = y_target - y_source_floored
+  x_offset = x_target - x_source_floored
+
+  y_source_indices = tf.cast(y_source_floored, tf.int32)
+  x_source_indices = tf.cast(x_source_floored, tf.int32)
+
+  indices = tf.stack([y_source_indices, x_source_indices], axis=1)
+  offsets = tf.stack([y_offset, x_offset], axis=1)
+
+  return offsets, indices
+
+
+def get_valid_keypoint_mask_for_class(keypoint_coordinates,
+                                      class_id,
+                                      class_onehot,
+                                      class_weights=None,
+                                      keypoint_indices=None):
+  """Mask keypoints by their class ids and indices.
+
+  For a given task, we may want to only consider a subset of instances or
+  keypoints. This function is used to provide the mask (in terms of weights) to
+  mark those elements which should be considered based on the classes of the
+  instances and optionally, their keypoint indices. Note that the NaN values
+  in the keypoints will also be masked out.
+
+  Args:
+    keypoint_coordinates: A float tensor with shape [num_instances,
+      num_keypoints, 2] which contains the coordinates of each keypoint.
+    class_id: An integer representing the target class id to be selected.
+    class_onehot: A 2D tensor of shape [num_instances, num_classes] repesents
+      the onehot (or k-hot) encoding of the class for each instance.
+    class_weights: A 1D tensor of shape [num_instances] repesents the weight of
+      each instance. If not provided, all instances are weighted equally.
+    keypoint_indices: A list of integers representing the keypoint indices used
+      to select the values on the keypoint dimension. If provided, the output
+      dimension will be [num_instances, len(keypoint_indices)]
+
+  Returns:
+    A tuple of tensors:
+      mask: A float tensor of shape [num_instances, K], where K is num_keypoints
+        or len(keypoint_indices) if provided. The tensor has values either 0 or
+        1 indicating whether an element in the input keypoints should be used.
+      keypoints_nan_to_zeros: Same as input keypoints with the NaN values
+        replaced by zeros and selected columns corresponding to the
+        keypoint_indices (if provided). The shape of this tensor will always be
+        the same as the output mask.
+  """
+  num_keypoints = tf.shape(keypoint_coordinates)[1]
+  class_mask = class_onehot[:, class_id]
+  reshaped_class_mask = tf.tile(
+      tf.expand_dims(class_mask, axis=-1), multiples=[1, num_keypoints])
+  not_nan = tf.math.logical_not(tf.math.is_nan(keypoint_coordinates))
+  mask = reshaped_class_mask * tf.cast(not_nan[:, :, 0], dtype=tf.float32)
+  keypoints_nan_to_zeros = tf.where(not_nan, keypoint_coordinates,
+                                    tf.zeros_like(keypoint_coordinates))
+  if class_weights is not None:
+    reshaped_class_weight = tf.tile(
+        tf.expand_dims(class_weights, axis=-1), multiples=[1, num_keypoints])
+    mask = mask * reshaped_class_weight
+
+  if keypoint_indices is not None:
+    mask = tf.gather(mask, indices=keypoint_indices, axis=1)
+    keypoints_nan_to_zeros = tf.gather(
+        keypoints_nan_to_zeros, indices=keypoint_indices, axis=1)
+  return mask, keypoints_nan_to_zeros
+
+
+def blackout_pixel_weights_by_box_regions(height, width, boxes, blackout):
+  """Blackout the pixel weights in the target box regions.
+
+  This function is used to generate the pixel weight mask (usually in the output
+  image dimension). The mask is to ignore some regions when computing loss.
+
+  Args:
+    height: int, height of the (output) image.
+    width: int, width of the (output) image.
+    boxes: A float tensor with shape [num_instances, 4] indicating the
+      coordinates of the four corners of the boxes.
+    blackout: A boolean tensor with shape [num_instances] indicating whether to
+      blackout (zero-out) the weights within the box regions.
+
+  Returns:
+    A float tensor with shape [height, width] where all values within the
+    regions of the blackout boxes are 0.0 and 1.0 else where.
+  """
+  (y_grid, x_grid) = image_shape_to_grids(height, width)
+  y_grid = tf.expand_dims(y_grid, axis=0)
+  x_grid = tf.expand_dims(x_grid, axis=0)
+  y_min = tf.expand_dims(boxes[:, 0:1], axis=-1)
+  x_min = tf.expand_dims(boxes[:, 1:2], axis=-1)
+  y_max = tf.expand_dims(boxes[:, 2:3], axis=-1)
+  x_max = tf.expand_dims(boxes[:, 3:], axis=-1)
+
+  # Make the mask with all 1.0 in the box regions.
+  # Shape: [num_instances, height, width]
+  in_boxes = tf.cast(
+      tf.logical_and(
+          tf.logical_and(y_grid >= y_min, y_grid <= y_max),
+          tf.logical_and(x_grid >= x_min, x_grid <= x_max)),
+      dtype=tf.float32)
+
+  # Shape: [num_instances, height, width]
+  blackout = tf.tile(
+      tf.expand_dims(tf.expand_dims(blackout, axis=-1), axis=-1),
+      [1, height, width])
+
+  # Select only the boxes specified by blackout.
+  selected_in_boxes = tf.where(blackout, in_boxes, tf.zeros_like(in_boxes))
+  out_boxes = tf.reduce_max(selected_in_boxes, axis=0)
+  out_boxes = tf.ones_like(out_boxes) - out_boxes
+  return out_boxes
diff --git a/research/object_detection/utils/target_assigner_utils_test.py b/research/object_detection/utils/target_assigner_utils_test.py
new file mode 100644
index 00000000..6fca5510
--- /dev/null
+++ b/research/object_detection/utils/target_assigner_utils_test.py
@@ -0,0 +1,179 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for utils.target_assigner_utils."""
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import target_assigner_utils as ta_utils
+from object_detection.utils import test_case
+
+
+class TargetUtilTest(test_case.TestCase):
+
+  def test_image_shape_to_grids(self):
+    def graph_fn():
+      (y_grid, x_grid) = ta_utils.image_shape_to_grids(height=2, width=3)
+      return y_grid, x_grid
+
+    expected_y_grid = np.array([[0, 0, 0], [1, 1, 1]])
+    expected_x_grid = np.array([[0, 1, 2], [0, 1, 2]])
+
+    y_grid, x_grid = self.execute(graph_fn, [])
+
+    np.testing.assert_array_equal(y_grid, expected_y_grid)
+    np.testing.assert_array_equal(x_grid, expected_x_grid)
+
+  def test_coordinates_to_heatmap(self):
+    def graph_fn():
+      (y_grid, x_grid) = ta_utils.image_shape_to_grids(height=3, width=5)
+      y_coordinates = tf.constant([1.5, 0.5], dtype=tf.float32)
+      x_coordinates = tf.constant([2.5, 4.5], dtype=tf.float32)
+      sigma = tf.constant([0.1, 0.5], dtype=tf.float32)
+      channel_onehot = tf.constant([[1, 0, 0], [0, 1, 0]], dtype=tf.float32)
+      channel_weights = tf.constant([1, 1], dtype=tf.float32)
+      heatmap = ta_utils.coordinates_to_heatmap(y_grid, x_grid, y_coordinates,
+                                                x_coordinates, sigma,
+                                                channel_onehot, channel_weights)
+      return heatmap
+
+    heatmap = self.execute(graph_fn, [])
+    # Peak at (1, 2) for the first class.
+    self.assertAlmostEqual(1.0, heatmap[1, 2, 0])
+    # Peak at (0, 4) for the second class.
+    self.assertAlmostEqual(1.0, heatmap[0, 4, 1])
+
+  def test_compute_floor_offsets_with_indices_onlysource(self):
+
+    def graph_fn():
+      y_source = tf.constant([1.5, 0.3], dtype=tf.float32)
+      x_source = tf.constant([2.5, 4.2], dtype=tf.float32)
+      (offsets, indices) = ta_utils.compute_floor_offsets_with_indices(
+          y_source, x_source)
+
+      return offsets, indices
+
+    offsets, indices = self.execute(graph_fn, [])
+
+    np.testing.assert_array_almost_equal(offsets,
+                                         np.array([[0.5, 0.5], [0.3, 0.2]]))
+    np.testing.assert_array_almost_equal(indices,
+                                         np.array([[1, 2], [0, 4]]))
+
+  def test_compute_floor_offsets_with_indices_and_targets(self):
+
+    def graph_fn():
+      y_source = tf.constant([1.5, 0.3], dtype=tf.float32)
+      x_source = tf.constant([2.5, 4.2], dtype=tf.float32)
+      y_target = tf.constant([2.1, 0.1], dtype=tf.float32)
+      x_target = tf.constant([1.2, 4.5], dtype=tf.float32)
+      (offsets, indices) = ta_utils.compute_floor_offsets_with_indices(
+          y_source, x_source, y_target, x_target)
+      return offsets, indices
+
+    offsets, indices = self.execute(graph_fn, [])
+
+    np.testing.assert_array_almost_equal(offsets,
+                                         np.array([[1.1, -0.8], [0.1, 0.5]]))
+    np.testing.assert_array_almost_equal(indices,
+                                         np.array([[1, 2], [0, 4]]))
+
+  def test_get_valid_keypoints_mask(self):
+
+    def graph_fn():
+      class_onehot = tf.constant(
+          [[0, 0, 1, 0, 0],
+           [0, 1, 0, 0, 0],
+           [0, 0, 1, 0, 1]], dtype=tf.float32)
+      keypoints = tf.constant(
+          [[0.1, float('nan'), 0.2, 0.0],
+           [0.0, 0.0, 0.1, 0.9],
+           [3.2, 4.3, float('nan'), 0.2]],
+          dtype=tf.float32)
+      keypoint_coordinates = tf.stack([keypoints, keypoints], axis=2)
+      mask, keypoints_nan_to_zeros = ta_utils.get_valid_keypoint_mask_for_class(
+          keypoint_coordinates=keypoint_coordinates,
+          class_id=2,
+          class_onehot=class_onehot,
+          keypoint_indices=[1, 2])
+
+      return mask, keypoints_nan_to_zeros
+
+    keypoints = np.array([[0.0, 0.2],
+                          [0.0, 0.1],
+                          [4.3, 0.0]])
+    expected_mask = np.array([[0, 1], [0, 0], [1, 0]])
+    expected_keypoints = np.stack([keypoints, keypoints], axis=2)
+    mask, keypoints_nan_to_zeros = self.execute(graph_fn, [])
+
+    np.testing.assert_array_equal(mask, expected_mask)
+    np.testing.assert_array_almost_equal(keypoints_nan_to_zeros,
+                                         expected_keypoints)
+
+  def test_get_valid_keypoints_with_mask(self):
+    def graph_fn():
+      class_onehot = tf.constant(
+          [[0, 0, 1, 0, 0],
+           [0, 1, 0, 0, 0],
+           [0, 0, 1, 0, 1]], dtype=tf.float32)
+      keypoints = tf.constant(
+          [[0.1, float('nan'), 0.2, 0.0],
+           [0.0, 0.0, 0.1, 0.9],
+           [3.2, 4.3, float('nan'), 0.2]],
+          dtype=tf.float32)
+      keypoint_coordinates = tf.stack([keypoints, keypoints], axis=2)
+      weights = tf.constant([0.0, 0.0, 1.0])
+      mask, keypoints_nan_to_zeros = ta_utils.get_valid_keypoint_mask_for_class(
+          keypoint_coordinates=keypoint_coordinates,
+          class_id=2,
+          class_onehot=class_onehot,
+          class_weights=weights,
+          keypoint_indices=[1, 2])
+      return mask, keypoints_nan_to_zeros
+
+    expected_mask = np.array([[0, 0], [0, 0], [1, 0]])
+    keypoints = np.array([[0.0, 0.2],
+                          [0.0, 0.1],
+                          [4.3, 0.0]])
+    expected_keypoints = np.stack([keypoints, keypoints], axis=2)
+    mask, keypoints_nan_to_zeros = self.execute(graph_fn, [])
+
+    np.testing.assert_array_equal(mask, expected_mask)
+    np.testing.assert_array_almost_equal(keypoints_nan_to_zeros,
+                                         expected_keypoints)
+
+  def test_blackout_pixel_weights_by_box_regions(self):
+    def graph_fn():
+      boxes = tf.constant(
+          [[0.0, 0.0, 5, 5], [0.0, 0.0, 10.0, 20.0], [6.0, 12.0, 8.0, 18.0]],
+          dtype=tf.float32)
+      blackout = tf.constant([True, False, True], dtype=tf.bool)
+      blackout_pixel_weights_by_box_regions = tf.function(
+          ta_utils.blackout_pixel_weights_by_box_regions)
+      output = blackout_pixel_weights_by_box_regions(10, 20, boxes, blackout)
+      return output
+
+    output = self.execute(graph_fn, [])
+    # All zeros in region [0:6, 0:6].
+    self.assertAlmostEqual(np.sum(output[0:6, 0:6]), 0.0)
+    # All zeros in region [12:19, 6:9].
+    self.assertAlmostEqual(np.sum(output[6:9, 12:19]), 0.0)
+    # All other pixel weights should be 1.0.
+    # 20 * 10 - 6 * 6 - 3 * 7 = 143.0
+    self.assertAlmostEqual(np.sum(output), 143.0)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
index d9b921c3..4cc6978b 100644
--- a/research/object_detection/utils/test_case.py
+++ b/research/object_detection/utils/test_case.py
@@ -12,97 +12,261 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""A convenience wrapper around tf.test.TestCase to enable TPU tests."""
+"""A convenience wrapper around tf.test.TestCase to test with TPU, TF1, TF2."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-import os
 from six.moves import zip
 import tensorflow as tf
-from tensorflow.contrib import tpu
+from tensorflow.python import tf2  # pylint: disable=import-outside-toplevel
+if not tf2.enabled():
+  from tensorflow.contrib import tpu as contrib_tpu  # pylint: disable=g-import-not-at-top, line-too-long
 
 flags = tf.app.flags
 
-flags.DEFINE_bool('tpu_test', False, 'Whether to configure test for TPU.')
+flags.DEFINE_bool('tpu_test', False, 'Deprecated Flag.')
 FLAGS = flags.FLAGS
 
 
+class TestCase(tf.test.TestCase):
+  """Base Test class to handle execution under {TF1.X, TF2.X} x {TPU, CPU}.
 
+  This class determines the TF version and availability of TPUs to set up
+  tests appropriately.
+  """
 
-class TestCase(tf.test.TestCase):
-  """Extends tf.test.TestCase to optionally allow running tests on TPU."""
+  def maybe_extract_single_output(self, outputs):
+    if isinstance(outputs, list) or isinstance(outputs, tuple):
+      if isinstance(outputs[0], tf.Tensor):
+        outputs_np = [output.numpy() for output in outputs]
+      else:
+        outputs_np = outputs
+      if len(outputs_np) == 1:
+        return outputs_np[0]
+      else:
+        return outputs_np
+    else:
+      if isinstance(outputs, tf.Tensor):
+        return outputs.numpy()
+      else:
+        return outputs
+
+  def has_tpu(self):
+    """Returns whether there are any logical TPU devices."""
+    return bool(tf.config.experimental.list_logical_devices(device_type='TPU'))
 
-  def execute_tpu(self, graph_fn, inputs):
-    """Constructs the graph, executes it on TPU and returns the result.
+  def is_tf2(self):
+    """Returns whether TF2 is enabled."""
+    return tf2.enabled()
+
+  def execute_tpu_tf1(self, compute_fn, inputs):
+    """Executes compute_fn on TPU with Tensorflow 1.X.
 
     Args:
-      graph_fn: a callable that constructs the tensorflow graph to test. The
-        arguments of this function should correspond to `inputs`.
-      inputs: a list of numpy arrays to feed input to the computation graph.
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
 
     Returns:
-      A list of numpy arrays or a scalar returned from executing the tensorflow
-      graph.
+      A list of numpy arrays or a single numpy array.
     """
     with self.test_session(graph=tf.Graph()) as sess:
       placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
-      tpu_computation = tpu.rewrite(graph_fn, placeholders)
-      sess.run(tpu.initialize_system())
+      def wrap_graph_fn(*args, **kwargs):
+        results = compute_fn(*args, **kwargs)
+        if (not (isinstance(results, dict) or isinstance(results, tf.Tensor))
+            and hasattr(results, '__iter__')):
+          results = list(results)
+        return results
+      tpu_computation = contrib_tpu.rewrite(wrap_graph_fn, placeholders)
+      sess.run(contrib_tpu.initialize_system())
       sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
                 tf.local_variables_initializer()])
       materialized_results = sess.run(tpu_computation,
                                       feed_dict=dict(zip(placeholders, inputs)))
-      sess.run(tpu.shutdown_system())
-      if (hasattr(materialized_results, '__len__') and
-          len(materialized_results) == 1 and
-          (isinstance(materialized_results, list) or
-           isinstance(materialized_results, tuple))):
-        materialized_results = materialized_results[0]
-    return materialized_results
+      sess.run(contrib_tpu.shutdown_system())
+    return self.maybe_extract_single_output(materialized_results)
 
-  def execute_cpu(self, graph_fn, inputs):
-    """Constructs the graph, executes it on CPU and returns the result.
+  def execute_tpu_tf2(self, compute_fn, inputs):
+    """Executes compute_fn on TPU with Tensorflow 2.X.
 
     Args:
-      graph_fn: a callable that constructs the tensorflow graph to test. The
-        arguments of this function should correspond to `inputs`.
-      inputs: a list of numpy arrays to feed input to the computation graph.
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
 
     Returns:
-      A list of numpy arrays or a scalar returned from executing the tensorflow
-      graph.
+      A list of numpy arrays or a single numpy array.
     """
+    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
+    tf.config.experimental_connect_to_cluster(resolver)
+    topology = tf.tpu.experimental.initialize_tpu_system(resolver)
+    device_assignment = tf.tpu.experimental.DeviceAssignment.build(
+        topology, num_replicas=1)
+    strategy = tf.distribute.experimental.TPUStrategy(
+        resolver, device_assignment=device_assignment)
+
+    @tf.function
+    def run():
+      tf_inputs = [tf.constant(input_t) for input_t in inputs]
+      return strategy.run(compute_fn, args=tf_inputs)
+    outputs = run()
+    tf.tpu.experimental.shutdown_tpu_system()
+    return self.maybe_extract_single_output(outputs)
+
+  def execute_cpu_tf1(self, compute_fn, inputs):
+    """Executes compute_fn on CPU with Tensorflow 1.X.
+
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single numpy array.
+    """
+    if self.is_tf2():
+      raise ValueError('Required version Tenforflow 1.X is not available.')
     with self.test_session(graph=tf.Graph()) as sess:
       placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
-      results = graph_fn(*placeholders)
+      results = compute_fn(*placeholders)
+      if (not (isinstance(results, dict) or isinstance(results, tf.Tensor)) and
+          hasattr(results, '__iter__')):
+        results = list(results)
       sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
                 tf.local_variables_initializer()])
       materialized_results = sess.run(results, feed_dict=dict(zip(placeholders,
                                                                   inputs)))
+    return self.maybe_extract_single_output(materialized_results)
+
+  def execute_cpu_tf2(self, compute_fn, inputs):
+    """Executes compute_fn on CPU with Tensorflow 2.X.
 
-      if (hasattr(materialized_results, '__len__') and
-          len(materialized_results) == 1 and
-          (isinstance(materialized_results, list) or
-           isinstance(materialized_results, tuple))):
-        materialized_results = materialized_results[0]
-    return materialized_results
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single numpy array.
+    """
+    if not self.is_tf2():
+      raise ValueError('Required version TensorFlow 2.0 is not available.')
+    @tf.function
+    def run():
+      tf_inputs = [tf.constant(input_t) for input_t in inputs]
+      return compute_fn(*tf_inputs)
+    return self.maybe_extract_single_output(run())
+
+  def execute_cpu(self, compute_fn, inputs):
+    """Executes compute_fn on CPU.
+
+    Depending on the underlying TensorFlow installation (build deps) runs in
+    either TF 1.X or TF 2.X style.
+
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single tensor.
+    """
+    if self.is_tf2():
+      return self.execute_cpu_tf2(compute_fn, inputs)
+    else:
+      return self.execute_cpu_tf1(compute_fn, inputs)
+
+  def execute_tpu(self, compute_fn, inputs):
+    """Executes compute_fn on TPU.
+
+    Depending on the underlying TensorFlow installation (build deps) runs in
+    either TF 1.X or TF 2.X style.
+
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single tensor.
+    """
+    if not self.has_tpu():
+      raise ValueError('No TPU Device found.')
+    if self.is_tf2():
+      return self.execute_tpu_tf2(compute_fn, inputs)
+    else:
+      return self.execute_tpu_tf1(compute_fn, inputs)
+
+  def execute_tf2(self, compute_fn, inputs):
+    """Runs compute_fn with TensorFlow 2.0.
+
+    Executes on TPU if available, otherwise executes on CPU.
+
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single tensor.
+    """
+    if not self.is_tf2():
+      raise ValueError('Required version TensorFlow 2.0 is not available.')
+    if self.has_tpu():
+      return self.execute_tpu_tf2(compute_fn, inputs)
+    else:
+      return self.execute_cpu_tf2(compute_fn, inputs)
+
+  def execute_tf1(self, compute_fn, inputs):
+    """Runs compute_fn with TensorFlow 1.X.
+
+    Executes on TPU if available, otherwise executes on CPU.
+
+    Args:
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
+
+    Returns:
+      A list of numpy arrays or a single tensor.
+    """
+    if self.is_tf2():
+      raise ValueError('Required version Tenforflow 1.X is not available.')
+    if self.has_tpu():
+      return self.execute_tpu_tf1(compute_fn, inputs)
+    else:
+      return self.execute_cpu_tf1(compute_fn, inputs)
 
-  def execute(self, graph_fn, inputs):
-    """Constructs the graph, creates a test session and returns the results.
+  def execute(self, compute_fn, inputs):
+    """Runs compute_fn with inputs and returns results.
 
-    The graph is executed either on TPU or CPU based on the `tpu_test` flag.
+    * Executes in either TF1.X or TF2.X style based on the TensorFlow version.
+    * Executes on TPU if available, otherwise executes on CPU.
 
     Args:
-      graph_fn: a callable that constructs the tensorflow graph to test. The
-        arguments of this function should correspond to `inputs`.
-      inputs: a list of numpy arrays to feed input to the computation graph.
+      compute_fn: a function containing Tensorflow computation that takes a list
+        of input numpy tensors, performs computation and returns output numpy
+        tensors.
+      inputs: a list of numpy arrays to feed input to the `compute_fn`.
 
     Returns:
-      A list of numpy arrays or a scalar returned from executing the tensorflow
-      graph.
+      A list of numpy arrays or a single tensor.
     """
-    if FLAGS.tpu_test:
-      return self.execute_tpu(graph_fn, inputs)
+    if self.has_tpu() and tf2.enabled():
+      return self.execute_tpu_tf2(compute_fn, inputs)
+    elif not self.has_tpu() and tf2.enabled():
+      return self.execute_cpu_tf2(compute_fn, inputs)
+    elif self.has_tpu() and not tf2.enabled():
+      return self.execute_tpu_tf1(compute_fn, inputs)
     else:
-      return self.execute_cpu(graph_fn, inputs)
+      return self.execute_cpu_tf1(compute_fn, inputs)
diff --git a/research/object_detection/utils/test_case_test.py b/research/object_detection/utils/test_case_test.py
new file mode 100644
index 00000000..a7a8d84b
--- /dev/null
+++ b/research/object_detection/utils/test_case_test.py
@@ -0,0 +1,73 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for google3.third_party.tensorflow_models.object_detection.utils.test_case."""
+
+import numpy as np
+import tensorflow as tf
+from object_detection.utils import test_case
+
+
+class TestCaseTest(test_case.TestCase):
+
+  def test_simple(self):
+    def graph_fn(tensora, tensorb):
+      return tf.tensordot(tensora, tensorb, axes=1)
+
+    tensora_np = np.ones(20)
+    tensorb_np = tensora_np * 2
+    output = self.execute(graph_fn, [tensora_np, tensorb_np])
+    self.assertAllClose(output, 40.0)
+
+  def test_two_outputs(self):
+    def graph_fn(tensora, tensorb):
+      return tensora + tensorb, tensora - tensorb
+    tensora_np = np.ones(20)
+    tensorb_np = tensora_np * 2
+    output = self.execute(graph_fn, [tensora_np, tensorb_np])
+    self.assertAllClose(output[0], tensora_np + tensorb_np)
+    self.assertAllClose(output[1], tensora_np - tensorb_np)
+
+  def test_function_with_tf_assert(self):
+    def compute_fn(image):
+      return tf.image.pad_to_bounding_box(image, 0, 0, 40, 40)
+
+    image_np = np.random.rand(2, 20, 30, 3)
+    output = self.execute(compute_fn, [image_np])
+    self.assertAllEqual(output.shape, [2, 40, 40, 3])
+
+  def test_tf2_only_test(self):
+    """Set up tests only to run with TF2."""
+    if self.is_tf2():
+      def graph_fn(tensora, tensorb):
+        return tensora + tensorb, tensora - tensorb
+      tensora_np = np.ones(20)
+      tensorb_np = tensora_np * 2
+      output = self.execute_tf2(graph_fn, [tensora_np, tensorb_np])
+      self.assertAllClose(output[0], tensora_np + tensorb_np)
+      self.assertAllClose(output[1], tensora_np - tensorb_np)
+
+  def test_tpu_only_test(self):
+    """Set up tests only to run with TPU."""
+    if self.has_tpu():
+      def graph_fn(tensora, tensorb):
+        return tensora + tensorb, tensora - tensorb
+      tensora_np = np.ones(20)
+      tensorb_np = tensora_np * 2
+      output = self.execute_tpu(graph_fn, [tensora_np, tensorb_np])
+      self.assertAllClose(output[0], tensora_np + tensorb_np)
+      self.assertAllClose(output[1], tensora_np - tensorb_np)
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/tf_version.py b/research/object_detection/utils/tf_version.py
new file mode 100644
index 00000000..07caef5e
--- /dev/null
+++ b/research/object_detection/utils/tf_version.py
@@ -0,0 +1,27 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Functions to check TensorFlow Version."""
+
+from tensorflow.python import tf2  # pylint: disable=import-outside-toplevel
+
+
+def is_tf1():
+  """Whether current TensorFlow Version is 1.X."""
+  return not tf2.enabled()
+
+
+def is_tf2():
+  """Whether current TensorFlow Version is 2.X."""
+  return tf2.enabled()
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 9d478116..0b1198d4 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -25,10 +25,9 @@ import re
 
 import tensorflow as tf
 
+from tensorflow.contrib import slim
 from tensorflow.python.ops import variables as tf_variables
 
-slim = tf.contrib.slim
-
 
 # TODO(derekjchow): Consider replacing with tf.contrib.filter_variables in
 # tensorflow/contrib/framework/python/ops/variables.py
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index e22be435..eee64c5d 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -38,6 +38,7 @@ from six.moves import range
 from six.moves import zip
 import tensorflow as tf
 
+from object_detection.core import keypoint_ops
 from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
 
@@ -202,8 +203,11 @@ def draw_bounding_box_on_image(image,
                                   ymin * im_height, ymax * im_height)
   else:
     (left, right, top, bottom) = (xmin, xmax, ymin, ymax)
-  draw.line([(left, top), (left, bottom), (right, bottom),
-             (right, top), (left, top)], width=thickness, fill=color)
+  if thickness > 0:
+    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),
+               (left, top)],
+              width=thickness,
+              fill=color)
   try:
     font = ImageFont.truetype('arial.ttf', 24)
   except IOError:
@@ -299,8 +303,11 @@ def draw_bounding_boxes_on_image(image,
                                boxes[i, 3], color, thickness, display_str_list)
 
 
-def create_visualization_fn(category_index, include_masks=False,
-                            include_keypoints=False, include_track_ids=False,
+def create_visualization_fn(category_index,
+                            include_masks=False,
+                            include_keypoints=False,
+                            include_keypoint_scores=False,
+                            include_track_ids=False,
                             **kwargs):
   """Constructs a visualization function that can be wrapped in a py_func.
 
@@ -311,9 +318,10 @@ def create_visualization_fn(category_index, include_masks=False,
   1: boxes
   2: classes
   3: scores
-  [4-6]: masks (optional)
-  [4-6]: keypoints (optional)
-  [4-6]: track_ids (optional)
+  [4]: masks (optional)
+  [4-5]: keypoints (optional)
+  [4-6]: keypoint_scores (optional)
+  [4-7]: track_ids (optional)
 
   -- Example 1 --
   vis_only_masks_fn = create_visualization_fn(category_index,
@@ -338,6 +346,8 @@ def create_visualization_fn(category_index, include_masks=False,
       the returned function.
     include_keypoints: Whether keypoints should be expected as a positional
       argument in the returned function.
+    include_keypoint_scores: Whether keypoint scores should be expected as a
+      positional argument in the returned function.
     include_track_ids: Whether track ids should be expected as a positional
       argument in the returned function.
     **kwargs: Additional kwargs that will be passed to
@@ -359,6 +369,7 @@ def create_visualization_fn(category_index, include_masks=False,
         -- Optional positional arguments --
         instance_masks - a numpy array of shape [N, image_height, image_width].
         keypoints - a numpy array of shape [N, num_keypoints, 2].
+        keypoint_scores - a numpy array of shape [N, num_keypoints].
         track_ids - a numpy array of shape [N] with unique track ids.
 
     Returns:
@@ -369,7 +380,7 @@ def create_visualization_fn(category_index, include_masks=False,
     boxes = args[1]
     classes = args[2]
     scores = args[3]
-    masks = keypoints = track_ids = None
+    masks = keypoints = keypoint_scores = track_ids = None
     pos_arg_ptr = 4  # Positional argument for first optional tensor (masks).
     if include_masks:
       masks = args[pos_arg_ptr]
@@ -377,6 +388,9 @@ def create_visualization_fn(category_index, include_masks=False,
     if include_keypoints:
       keypoints = args[pos_arg_ptr]
       pos_arg_ptr += 1
+    if include_keypoint_scores:
+      keypoint_scores = args[pos_arg_ptr]
+      pos_arg_ptr += 1
     if include_track_ids:
       track_ids = args[pos_arg_ptr]
 
@@ -388,11 +402,106 @@ def create_visualization_fn(category_index, include_masks=False,
         category_index=category_index,
         instance_masks=masks,
         keypoints=keypoints,
+        keypoint_scores=keypoint_scores,
         track_ids=track_ids,
         **kwargs)
   return visualization_py_func_fn
 
 
+def draw_heatmaps_on_image(image, heatmaps):
+  """Draws heatmaps on an image.
+
+  The heatmaps are handled channel by channel and different colors are used to
+  paint different heatmap channels.
+
+  Args:
+    image: a PIL.Image object.
+    heatmaps: a numpy array with shape [image_height, image_width, channel].
+      Note that the image_height and image_width should match the size of input
+      image.
+  """
+  draw = ImageDraw.Draw(image)
+  channel = heatmaps.shape[2]
+  for c in range(channel):
+    heatmap = heatmaps[:, :, c] * 255
+    heatmap = heatmap.astype('uint8')
+    bitmap = Image.fromarray(heatmap, 'L')
+    bitmap.convert('1')
+    draw.bitmap(
+        xy=[(0, 0)],
+        bitmap=bitmap,
+        fill=STANDARD_COLORS[c])
+
+
+def draw_heatmaps_on_image_array(image, heatmaps):
+  """Overlays heatmaps to an image (numpy array).
+
+  The function overlays the heatmaps on top of image. The heatmap values will be
+  painted with different colors depending on the channels. Similar to
+  "draw_heatmaps_on_image_array" function except the inputs are numpy arrays.
+
+  Args:
+    image: a numpy array with shape [height, width, 3].
+    heatmaps: a numpy array with shape [height, width, channel].
+
+  Returns:
+    An uint8 numpy array representing the input image painted with heatmap
+    colors.
+  """
+  if not isinstance(image, np.ndarray):
+    image = image.numpy()
+  if not isinstance(heatmaps, np.ndarray):
+    heatmaps = heatmaps.numpy()
+  image_pil = Image.fromarray(np.uint8(image)).convert('RGB')
+  draw_heatmaps_on_image(image_pil, heatmaps)
+  return np.array(image_pil)
+
+
+def draw_heatmaps_on_image_tensors(images,
+                                   heatmaps,
+                                   apply_sigmoid=False):
+  """Draws heatmaps on batch of image tensors.
+
+  Args:
+    images: A 4D uint8 image tensor of shape [N, H, W, C]. If C > 3, additional
+      channels will be ignored. If C = 1, then we convert the images to RGB
+      images.
+    heatmaps: [N, h, w, channel] float32 tensor of heatmaps. Note that the
+      heatmaps will be resized to match the input image size before overlaying
+      the heatmaps with input images. Theoretically the heatmap height width
+      should have the same aspect ratio as the input image to avoid potential
+      misalignment introduced by the image resize.
+    apply_sigmoid: Whether to apply a sigmoid layer on top of the heatmaps. If
+      the heatmaps come directly from the prediction logits, then we should
+      apply the sigmoid layer to make sure the values are in between [0.0, 1.0].
+
+  Returns:
+    4D image tensor of type uint8, with heatmaps overlaid on top.
+  """
+  # Additional channels are being ignored.
+  if images.shape[3] > 3:
+    images = images[:, :, :, 0:3]
+  elif images.shape[3] == 1:
+    images = tf.image.grayscale_to_rgb(images)
+
+  _, height, width, _ = shape_utils.combined_static_and_dynamic_shape(images)
+  if apply_sigmoid:
+    heatmaps = tf.math.sigmoid(heatmaps)
+  resized_heatmaps = tf.image.resize(heatmaps, size=[height, width])
+
+  elems = [images, resized_heatmaps]
+
+  def draw_heatmaps(image_and_heatmaps):
+    """Draws heatmaps on image."""
+    image_with_heatmaps = tf.py_function(
+        draw_heatmaps_on_image_array,
+        image_and_heatmaps,
+        tf.uint8)
+    return image_with_heatmaps
+  images = tf.map_fn(draw_heatmaps, elems, dtype=tf.uint8, back_prop=False)
+  return images
+
+
 def _resize_original_image(image, image_shape):
   image = tf.expand_dims(image, 0)
   image = tf.image.resize_images(
@@ -412,6 +521,8 @@ def draw_bounding_boxes_on_image_tensors(images,
                                          true_image_shape=None,
                                          instance_masks=None,
                                          keypoints=None,
+                                         keypoint_scores=None,
+                                         keypoint_edges=None,
                                          track_ids=None,
                                          max_boxes_to_draw=20,
                                          min_score_thresh=0.2,
@@ -436,6 +547,11 @@ def draw_bounding_boxes_on_image_tensors(images,
       instance masks.
     keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]
       with keypoints.
+    keypoint_scores: A 3D float32 tensor of shape [N, max_detection,
+      num_keypoints] with keypoint scores.
+    keypoint_edges: A list of tuples with keypoint indices that specify which
+      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+      edges from keypoint 0 to 1 and from keypoint 2 to 4.
     track_ids: [N, max_detections] int32 tensor of unique tracks ids (i.e.
       instance ids for each object). If provided, the color-coding of boxes is
       dictated by these ids, and not classes.
@@ -458,7 +574,8 @@ def draw_bounding_boxes_on_image_tensors(images,
       'max_boxes_to_draw': max_boxes_to_draw,
       'min_score_thresh': min_score_thresh,
       'agnostic_mode': False,
-      'line_thickness': 4
+      'line_thickness': 4,
+      'keypoint_edges': keypoint_edges
   }
   if true_image_shape is None:
     true_shapes = tf.constant(-1, shape=[images.shape.as_list()[0], 3])
@@ -473,6 +590,7 @@ def draw_bounding_boxes_on_image_tensors(images,
       category_index,
       include_masks=instance_masks is not None,
       include_keypoints=keypoints is not None,
+      include_keypoint_scores=keypoint_scores is not None,
       include_track_ids=track_ids is not None,
       **visualization_keyword_args)
 
@@ -481,6 +599,8 @@ def draw_bounding_boxes_on_image_tensors(images,
     elems.append(instance_masks)
   if keypoints is not None:
     elems.append(keypoints)
+  if keypoint_scores is not None:
+    elems.append(keypoint_scores)
   if track_ids is not None:
     elems.append(track_ids)
 
@@ -506,7 +626,8 @@ def draw_side_by_side_evaluation_image(eval_dict,
                                        category_index,
                                        max_boxes_to_draw=20,
                                        min_score_thresh=0.2,
-                                       use_normalized_coordinates=True):
+                                       use_normalized_coordinates=True,
+                                       keypoint_edges=None):
   """Creates a side-by-side image with detections and groundtruth.
 
   Bounding boxes (and instance masks, if available) are visualized on both
@@ -519,9 +640,12 @@ def draw_side_by_side_evaluation_image(eval_dict,
     category_index: A category index (dictionary) produced from a labelmap.
     max_boxes_to_draw: The maximum number of boxes to draw for detections.
     min_score_thresh: The minimum score threshold for showing detections.
-    use_normalized_coordinates: Whether to assume boxes and kepoints are in
-      normalized coordinates (as opposed to absolute coordiantes).
+    use_normalized_coordinates: Whether to assume boxes and keypoints are in
+      normalized coordinates (as opposed to absolute coordinates).
       Default is True.
+    keypoint_edges: A list of tuples with keypoint indices that specify which
+      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+      edges from keypoint 0 to 1 and from keypoint 2 to 4.
 
   Returns:
     A list of [1, H, 2 * W, C] uint8 tensor. The subimage on the left
@@ -536,7 +660,8 @@ def draw_side_by_side_evaluation_image(eval_dict,
   # Add the batch dimension if the eval_dict is for single example.
   if len(eval_dict[detection_fields.detection_classes].shape) == 1:
     for key in eval_dict:
-      if key != input_data_fields.original_image and key != input_data_fields.image_additional_channels:
+      if (key != input_data_fields.original_image and
+          key != input_data_fields.image_additional_channels):
         eval_dict[key] = tf.expand_dims(eval_dict[key], 0)
 
   for indx in range(eval_dict[input_data_fields.original_image].shape[0]):
@@ -547,15 +672,36 @@ def draw_side_by_side_evaluation_image(eval_dict,
               eval_dict[detection_fields.detection_masks][indx], axis=0),
           tf.uint8)
     keypoints = None
+    keypoint_scores = None
     if detection_fields.detection_keypoints in eval_dict:
       keypoints = tf.expand_dims(
           eval_dict[detection_fields.detection_keypoints][indx], axis=0)
+      if detection_fields.detection_keypoint_scores in eval_dict:
+        keypoint_scores = tf.expand_dims(
+            eval_dict[detection_fields.detection_keypoint_scores][indx], axis=0)
+      else:
+        keypoint_scores = tf.cast(keypoint_ops.set_keypoint_visibilities(
+            keypoints), dtype=tf.float32)
+
     groundtruth_instance_masks = None
     if input_data_fields.groundtruth_instance_masks in eval_dict:
       groundtruth_instance_masks = tf.cast(
           tf.expand_dims(
               eval_dict[input_data_fields.groundtruth_instance_masks][indx],
               axis=0), tf.uint8)
+    groundtruth_keypoints = None
+    groundtruth_keypoint_scores = None
+    gt_kpt_vis_fld = input_data_fields.groundtruth_keypoint_visibilities
+    if input_data_fields.groundtruth_keypoints in eval_dict:
+      groundtruth_keypoints = tf.expand_dims(
+          eval_dict[input_data_fields.groundtruth_keypoints][indx], axis=0)
+      if gt_kpt_vis_fld in eval_dict:
+        groundtruth_keypoint_scores = tf.expand_dims(
+            tf.cast(eval_dict[gt_kpt_vis_fld][indx], dtype=tf.float32), axis=0)
+      else:
+        groundtruth_keypoint_scores = tf.cast(
+            keypoint_ops.set_keypoint_visibilities(
+                groundtruth_keypoints), dtype=tf.float32)
 
     images_with_detections = draw_bounding_boxes_on_image_tensors(
         tf.expand_dims(
@@ -574,6 +720,8 @@ def draw_side_by_side_evaluation_image(eval_dict,
             eval_dict[input_data_fields.true_image_shape][indx], axis=0),
         instance_masks=instance_masks,
         keypoints=keypoints,
+        keypoint_scores=keypoint_scores,
+        keypoint_edges=keypoint_edges,
         max_boxes_to_draw=max_boxes_to_draw,
         min_score_thresh=min_score_thresh,
         use_normalized_coordinates=use_normalized_coordinates)
@@ -596,7 +744,9 @@ def draw_side_by_side_evaluation_image(eval_dict,
         true_image_shape=tf.expand_dims(
             eval_dict[input_data_fields.true_image_shape][indx], axis=0),
         instance_masks=groundtruth_instance_masks,
-        keypoints=None,
+        keypoints=groundtruth_keypoints,
+        keypoint_scores=groundtruth_keypoint_scores,
+        keypoint_edges=keypoint_edges,
         max_boxes_to_draw=None,
         min_score_thresh=0.0,
         use_normalized_coordinates=use_normalized_coordinates)
@@ -628,6 +778,7 @@ def draw_side_by_side_evaluation_image(eval_dict,
                   eval_dict[input_data_fields.true_image_shape][indx], axis=0),
               instance_masks=groundtruth_instance_masks,
               keypoints=None,
+              keypoint_edges=None,
               max_boxes_to_draw=None,
               min_score_thresh=0.0,
               use_normalized_coordinates=use_normalized_coordinates))
@@ -641,51 +792,113 @@ def draw_side_by_side_evaluation_image(eval_dict,
 
 def draw_keypoints_on_image_array(image,
                                   keypoints,
+                                  keypoint_scores=None,
+                                  min_score_thresh=0.5,
                                   color='red',
                                   radius=2,
-                                  use_normalized_coordinates=True):
+                                  use_normalized_coordinates=True,
+                                  keypoint_edges=None,
+                                  keypoint_edge_color='green',
+                                  keypoint_edge_width=2):
   """Draws keypoints on an image (numpy array).
 
   Args:
     image: a numpy array with shape [height, width, 3].
     keypoints: a numpy array with shape [num_keypoints, 2].
+    keypoint_scores: a numpy array with shape [num_keypoints]. If provided, only
+      those keypoints with a score above score_threshold will be visualized.
+    min_score_thresh: A scalar indicating the minimum keypoint score required
+      for a keypoint to be visualized. Note that keypoint_scores must be
+      provided for this threshold to take effect.
     color: color to draw the keypoints with. Default is red.
     radius: keypoint radius. Default value is 2.
     use_normalized_coordinates: if True (default), treat keypoint values as
       relative to the image.  Otherwise treat them as absolute.
+    keypoint_edges: A list of tuples with keypoint indices that specify which
+      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+      edges from keypoint 0 to 1 and from keypoint 2 to 4.
+    keypoint_edge_color: color to draw the keypoint edges with. Default is red.
+    keypoint_edge_width: width of the edges drawn between keypoints. Default
+      value is 2.
   """
   image_pil = Image.fromarray(np.uint8(image)).convert('RGB')
-  draw_keypoints_on_image(image_pil, keypoints, color, radius,
-                          use_normalized_coordinates)
+  draw_keypoints_on_image(image_pil,
+                          keypoints,
+                          keypoint_scores=keypoint_scores,
+                          min_score_thresh=min_score_thresh,
+                          color=color,
+                          radius=radius,
+                          use_normalized_coordinates=use_normalized_coordinates,
+                          keypoint_edges=keypoint_edges,
+                          keypoint_edge_color=keypoint_edge_color,
+                          keypoint_edge_width=keypoint_edge_width)
   np.copyto(image, np.array(image_pil))
 
 
 def draw_keypoints_on_image(image,
                             keypoints,
+                            keypoint_scores=None,
+                            min_score_thresh=0.5,
                             color='red',
                             radius=2,
-                            use_normalized_coordinates=True):
+                            use_normalized_coordinates=True,
+                            keypoint_edges=None,
+                            keypoint_edge_color='green',
+                            keypoint_edge_width=2):
   """Draws keypoints on an image.
 
   Args:
     image: a PIL.Image object.
     keypoints: a numpy array with shape [num_keypoints, 2].
+    keypoint_scores: a numpy array with shape [num_keypoints].
+    min_score_thresh: a score threshold for visualizing keypoints. Only used if
+      keypoint_scores is provided.
     color: color to draw the keypoints with. Default is red.
     radius: keypoint radius. Default value is 2.
     use_normalized_coordinates: if True (default), treat keypoint values as
       relative to the image.  Otherwise treat them as absolute.
+    keypoint_edges: A list of tuples with keypoint indices that specify which
+      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+      edges from keypoint 0 to 1 and from keypoint 2 to 4.
+    keypoint_edge_color: color to draw the keypoint edges with. Default is red.
+    keypoint_edge_width: width of the edges drawn between keypoints. Default
+      value is 2.
   """
   draw = ImageDraw.Draw(image)
   im_width, im_height = image.size
+  keypoints = np.array(keypoints)
   keypoints_x = [k[1] for k in keypoints]
   keypoints_y = [k[0] for k in keypoints]
   if use_normalized_coordinates:
     keypoints_x = tuple([im_width * x for x in keypoints_x])
     keypoints_y = tuple([im_height * y for y in keypoints_y])
-  for keypoint_x, keypoint_y in zip(keypoints_x, keypoints_y):
-    draw.ellipse([(keypoint_x - radius, keypoint_y - radius),
-                  (keypoint_x + radius, keypoint_y + radius)],
-                 outline=color, fill=color)
+  if keypoint_scores is not None:
+    keypoint_scores = np.array(keypoint_scores)
+    valid_kpt = np.greater(keypoint_scores, min_score_thresh)
+  else:
+    valid_kpt = np.where(np.any(np.isnan(keypoints), axis=1),
+                         np.zeros_like(keypoints[:, 0]),
+                         np.ones_like(keypoints[:, 0]))
+  valid_kpt = [v for v in valid_kpt]
+
+  for keypoint_x, keypoint_y, valid in zip(keypoints_x, keypoints_y, valid_kpt):
+    if valid:
+      draw.ellipse([(keypoint_x - radius, keypoint_y - radius),
+                    (keypoint_x + radius, keypoint_y + radius)],
+                   outline=color, fill=color)
+  if keypoint_edges is not None:
+    for keypoint_start, keypoint_end in keypoint_edges:
+      if (keypoint_start < 0 or keypoint_start >= len(keypoints) or
+          keypoint_end < 0 or keypoint_end >= len(keypoints)):
+        continue
+      if not (valid_kpt[keypoint_start] and valid_kpt[keypoint_end]):
+        continue
+      edge_coordinates = [
+          keypoints_x[keypoint_start], keypoints_y[keypoint_start],
+          keypoints_x[keypoint_end], keypoints_y[keypoint_end]
+      ]
+      draw.line(
+          edge_coordinates, fill=keypoint_edge_color, width=keypoint_edge_width)
 
 
 def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):
@@ -730,6 +943,8 @@ def visualize_boxes_and_labels_on_image_array(
     instance_masks=None,
     instance_boundaries=None,
     keypoints=None,
+    keypoint_scores=None,
+    keypoint_edges=None,
     track_ids=None,
     use_normalized_coordinates=False,
     max_boxes_to_draw=20,
@@ -737,6 +952,7 @@ def visualize_boxes_and_labels_on_image_array(
     agnostic_mode=False,
     line_thickness=4,
     groundtruth_box_visualization_color='black',
+    skip_boxes=False,
     skip_scores=False,
     skip_labels=False,
     skip_track_ids=False):
@@ -762,7 +978,11 @@ def visualize_boxes_and_labels_on_image_array(
     instance_boundaries: a numpy array of shape [N, image_height, image_width]
       with values ranging between 0 and 1, can be None.
     keypoints: a numpy array of shape [N, num_keypoints, 2], can
-      be None
+      be None.
+    keypoint_scores: a numpy array of shape [N, num_keypoints], can be None.
+    keypoint_edges: A list of tuples with keypoint indices that specify which
+      keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+      edges from keypoint 0 to 1 and from keypoint 2 to 4.
     track_ids: a numpy array of shape [N] with unique track ids. If provided,
       color-coding of boxes will be determined by these ids, and not the class
       indices.
@@ -770,13 +990,15 @@ def visualize_boxes_and_labels_on_image_array(
       normalized coordinates or not.
     max_boxes_to_draw: maximum number of boxes to visualize.  If None, draw
       all boxes.
-    min_score_thresh: minimum score threshold for a box to be visualized
+    min_score_thresh: minimum score threshold for a box or keypoint to be
+      visualized.
     agnostic_mode: boolean (default: False) controlling whether to evaluate in
       class-agnostic mode or not.  This mode will display scores but ignore
       classes.
     line_thickness: integer (default: 4) controlling line width of the boxes.
     groundtruth_box_visualization_color: box color for visualizing groundtruth
       boxes
+    skip_boxes: whether to skip the drawing of bounding boxes.
     skip_scores: whether to skip score when drawing a single detection
     skip_labels: whether to skip label when drawing a single detection
     skip_track_ids: whether to skip track id when drawing a single detection
@@ -791,10 +1013,13 @@ def visualize_boxes_and_labels_on_image_array(
   box_to_instance_masks_map = {}
   box_to_instance_boundaries_map = {}
   box_to_keypoints_map = collections.defaultdict(list)
+  box_to_keypoint_scores_map = collections.defaultdict(list)
   box_to_track_ids_map = {}
   if not max_boxes_to_draw:
     max_boxes_to_draw = boxes.shape[0]
-  for i in range(min(max_boxes_to_draw, boxes.shape[0])):
+  for i in range(boxes.shape[0]):
+    if max_boxes_to_draw == len(box_to_color_map):
+      break
     if scores is None or scores[i] > min_score_thresh:
       box = tuple(boxes[i].tolist())
       if instance_masks is not None:
@@ -803,6 +1028,8 @@ def visualize_boxes_and_labels_on_image_array(
         box_to_instance_boundaries_map[box] = instance_boundaries[i]
       if keypoints is not None:
         box_to_keypoints_map[box].extend(keypoints[i])
+      if keypoint_scores is not None:
+        box_to_keypoint_scores_map[box].extend(keypoint_scores[i])
       if track_ids is not None:
         box_to_track_ids_map[box] = track_ids[i]
       if scores is None:
@@ -860,16 +1087,24 @@ def visualize_boxes_and_labels_on_image_array(
         ymax,
         xmax,
         color=color,
-        thickness=line_thickness,
+        thickness=0 if skip_boxes else line_thickness,
         display_str_list=box_to_display_str_map[box],
         use_normalized_coordinates=use_normalized_coordinates)
     if keypoints is not None:
+      keypoint_scores_for_box = None
+      if box_to_keypoint_scores_map:
+        keypoint_scores_for_box = box_to_keypoint_scores_map[box]
       draw_keypoints_on_image_array(
           image,
           box_to_keypoints_map[box],
+          keypoint_scores_for_box,
+          min_score_thresh=min_score_thresh,
           color=color,
           radius=line_thickness / 2,
-          use_normalized_coordinates=use_normalized_coordinates)
+          use_normalized_coordinates=use_normalized_coordinates,
+          keypoint_edges=keypoint_edges,
+          keypoint_edge_color=color,
+          keypoint_edge_width=line_thickness // 2)
 
   return image
 
@@ -950,7 +1185,8 @@ class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):
                max_boxes_to_draw=20,
                min_score_thresh=0.2,
                use_normalized_coordinates=True,
-               summary_name_prefix='evaluation_image'):
+               summary_name_prefix='evaluation_image',
+               keypoint_edges=None):
     """Creates an EvalMetricOpsVisualization.
 
     Args:
@@ -958,10 +1194,13 @@ class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):
       max_examples_to_draw: The maximum number of example summaries to produce.
       max_boxes_to_draw: The maximum number of boxes to draw for detections.
       min_score_thresh: The minimum score threshold for showing detections.
-      use_normalized_coordinates: Whether to assume boxes and kepoints are in
-        normalized coordinates (as opposed to absolute coordiantes).
+      use_normalized_coordinates: Whether to assume boxes and keypoints are in
+        normalized coordinates (as opposed to absolute coordinates).
         Default is True.
       summary_name_prefix: A string prefix for each image summary.
+      keypoint_edges: A list of tuples with keypoint indices that specify which
+        keypoints should be connected by an edge, e.g. [(0, 1), (2, 4)] draws
+        edges from keypoint 0 to 1 and from keypoint 2 to 4.
     """
 
     self._category_index = category_index
@@ -970,6 +1209,7 @@ class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):
     self._min_score_thresh = min_score_thresh
     self._use_normalized_coordinates = use_normalized_coordinates
     self._summary_name_prefix = summary_name_prefix
+    self._keypoint_edges = keypoint_edges
     self._images = []
 
   def clear(self):
@@ -1005,6 +1245,12 @@ class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):
           int64 tensor with 1-indexed groundtruth classes.
         fields.InputDataFields.groundtruth_instance_masks - (optional)
           [batch_size, num_boxes, H, W] int64 tensor with instance masks.
+        fields.InputDataFields.groundtruth_keypoints - (optional)
+          [batch_size, num_boxes, num_keypoints, 2] float32 tensor with
+          keypoint coordinates in format [y, x].
+        fields.InputDataFields.groundtruth_keypoint_visibilities - (optional)
+          [batch_size, num_boxes, num_keypoints] bool tensor with
+          keypoint visibilities.
         fields.DetectionResultFields.detection_boxes - [batch_size,
           max_num_boxes, 4] float32 tensor with detection boxes in range [0.0,
           1.0].
@@ -1017,6 +1263,9 @@ class EvalMetricOpsVisualization(six.with_metaclass(abc.ABCMeta, object)):
         fields.DetectionResultFields.detection_keypoints - (optional)
           [batch_size, max_num_boxes, num_keypoints, 2] float32 tensor with
           keypoints.
+        fields.DetectionResultFields.detection_keypoint_scores - (optional)
+          [batch_size, max_num_boxes, num_keypoints] float32 tensor with
+          keypoints scores.
 
     Returns:
       A dictionary of image summary names to tuple of (value_op, update_op). The
@@ -1083,16 +1332,20 @@ class VisualizeSingleFrameDetections(EvalMetricOpsVisualization):
                max_boxes_to_draw=20,
                min_score_thresh=0.2,
                use_normalized_coordinates=True,
-               summary_name_prefix='Detections_Left_Groundtruth_Right'):
+               summary_name_prefix='Detections_Left_Groundtruth_Right',
+               keypoint_edges=None):
     super(VisualizeSingleFrameDetections, self).__init__(
         category_index=category_index,
         max_examples_to_draw=max_examples_to_draw,
         max_boxes_to_draw=max_boxes_to_draw,
         min_score_thresh=min_score_thresh,
         use_normalized_coordinates=use_normalized_coordinates,
-        summary_name_prefix=summary_name_prefix)
+        summary_name_prefix=summary_name_prefix,
+        keypoint_edges=keypoint_edges)
 
   def images_from_evaluation_dict(self, eval_dict):
-    return draw_side_by_side_evaluation_image(
-        eval_dict, self._category_index, self._max_boxes_to_draw,
-        self._min_score_thresh, self._use_normalized_coordinates)
+    return draw_side_by_side_evaluation_image(eval_dict, self._category_index,
+                                              self._max_boxes_to_draw,
+                                              self._min_score_thresh,
+                                              self._use_normalized_coordinates,
+                                              self._keypoint_edges)
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index 0f6abec3..6f017297 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -45,6 +45,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
 
     # Show that with 34 colors, the closest prime number to 34/10 that
     # satisfies the constraints is 5.
+    default_standard_colors = visualization_utils.STANDARD_COLORS
     visualization_utils.STANDARD_COLORS = [
         'color_{}'.format(str(i)) for i in range(34)
     ]
@@ -59,6 +60,8 @@ class VisualizationUtilsTest(tf.test.TestCase):
     multiplier = visualization_utils._get_multiplier_for_color_randomness()
     self.assertEqual(13, multiplier)
 
+    visualization_utils.STANDARD_COLORS = default_standard_colors
+
   def create_colorful_test_image(self):
     """This function creates an image that can be used to test vis functions.
 
@@ -161,6 +164,8 @@ class VisualizationUtilsTest(tf.test.TestCase):
                            [[0.25, 0.25, 0.75, 0.75], [0.1, 0.3, 0.6, 1.0]]])
       classes = tf.constant([[1, 1], [1, 2]], dtype=tf.int64)
       scores = tf.constant([[0.8, 0.1], [0.6, 0.5]])
+      keypoints = tf.random.uniform((2, 2, 4, 2), maxval=1.0, dtype=tf.float32)
+      keypoint_edges = [(0, 1), (1, 2), (2, 3), (3, 0)]
       images_with_boxes = (
           visualization_utils.draw_bounding_boxes_on_image_tensors(
               images_tensor,
@@ -170,7 +175,9 @@ class VisualizationUtilsTest(tf.test.TestCase):
               category_index,
               original_image_spatial_shape=image_shape,
               true_image_shape=image_shape,
-              min_score_thresh=0.2))
+              keypoints=keypoints,
+              min_score_thresh=0.2,
+              keypoint_edges=keypoint_edges))
 
       with self.test_session() as sess:
         sess.run(tf.global_variables_initializer())
@@ -297,8 +304,34 @@ class VisualizationUtilsTest(tf.test.TestCase):
     test_image = Image.fromarray(test_image)
     width_original, height_original = test_image.size
     keypoints = [[0.25, 0.75], [0.4, 0.6], [0.1, 0.1], [0.9, 0.9]]
+    keypoint_scores = [0.8, 0.2, 0.2, 0.7]
+    keypoint_edges = [(0, 1), (1, 2), (2, 3), (3, 0)]
+
+    visualization_utils.draw_keypoints_on_image(
+        test_image,
+        keypoints,
+        keypoint_scores,
+        keypoint_edges=keypoint_edges,
+        keypoint_edge_width=1,
+        keypoint_edge_color='green')
+    width_final, height_final = test_image.size
+
+    self.assertEqual(width_original, width_final)
+    self.assertEqual(height_original, height_final)
 
-    visualization_utils.draw_keypoints_on_image(test_image, keypoints)
+  def test_draw_keypoints_on_image_with_default_keypoint_scores(self):
+    test_image = self.create_colorful_test_image()
+    test_image = Image.fromarray(test_image)
+    width_original, height_original = test_image.size
+    keypoints = [[0.25, np.nan], [0.4, 0.6], [np.nan, np.nan], [0.9, 0.9]]
+    keypoint_edges = [(0, 1), (1, 2), (2, 3), (3, 0)]
+
+    visualization_utils.draw_keypoints_on_image(
+        test_image,
+        keypoints,
+        keypoint_edges=keypoint_edges,
+        keypoint_edge_width=1,
+        keypoint_edge_color='green')
     width_final, height_final = test_image.size
 
     self.assertEqual(width_original, width_final)
@@ -309,8 +342,14 @@ class VisualizationUtilsTest(tf.test.TestCase):
     width_original = test_image.shape[0]
     height_original = test_image.shape[1]
     keypoints = [[0.25, 0.75], [0.4, 0.6], [0.1, 0.1], [0.9, 0.9]]
-
-    visualization_utils.draw_keypoints_on_image_array(test_image, keypoints)
+    keypoint_edges = [(0, 1), (1, 2), (2, 3), (3, 0)]
+
+    visualization_utils.draw_keypoints_on_image_array(
+        test_image,
+        keypoints,
+        keypoint_edges=keypoint_edges,
+        keypoint_edge_width=1,
+        keypoint_edge_color='green')
     width_final = test_image.shape[0]
     height_final = test_image.shape[1]
 
@@ -328,6 +367,67 @@ class VisualizationUtilsTest(tf.test.TestCase):
                                                  color='Blue', alpha=.5)
     self.assertAllEqual(test_image, expected_result)
 
+  def test_draw_heatmaps_on_image(self):
+    test_image = self.create_colorful_test_image()
+    test_image = Image.fromarray(test_image)
+    width_original, height_original = test_image.size
+    heatmaps = np.ones(shape=[10, 20, 1], dtype=float)
+    visualization_utils.draw_heatmaps_on_image(test_image, heatmaps)
+    width_final, height_final = test_image.size
+    pixels = list(test_image.getdata())
+
+    self.assertEqual(width_original, width_final)
+    self.assertEqual(height_original, height_final)
+    # The pixel shoud be painted as AliceBlue with RGB (240, 248, 255).
+    self.assertAllEqual((240, 248, 255), pixels[10])
+
+  def test_draw_heatmaps_on_image_array(self):
+    test_image = np.asarray([[[0, 0, 0], [0, 0, 0]],
+                             [[0, 0, 0], [0, 0, 0]]], dtype=np.uint8)
+    heatmap1 = np.asarray([[1, 0],
+                           [0, 1]], dtype=np.float)
+    heatmap2 = np.asarray([[0, 1],
+                           [1, 0]], dtype=np.float)
+    heatmaps = np.stack([heatmap1, heatmap2], axis=0)
+    output_image = visualization_utils.draw_heatmaps_on_image_array(
+        test_image, heatmaps)
+    # Output image should be painted as "AliceBlue" at (0, 0), (1, 1)
+    # and "Chartreuse" at (0, 1), (1, 0).
+    self.assertAllEqual(
+        output_image,
+        np.array([[[240, 248, 255], [127, 255, 0]],
+                  [[127, 255, 0], [240, 248, 255]]]))
+
+  def test_draw_heatmaps_on_image_tensors(self):
+    test_image = np.asarray([[[0, 0, 0], [0, 0, 0]],
+                             [[0, 0, 0], [0, 0, 0]]], dtype=np.uint8)
+
+    heatmap1 = np.asarray([[1, 0],
+                           [0, 1]], dtype=np.float)
+    heatmap2 = np.asarray([[0, 1],
+                           [1, 0]], dtype=np.float)
+    heatmaps = np.stack([heatmap1, heatmap2], axis=0)
+    with tf.Graph().as_default():
+      image_tensor = tf.constant(test_image, dtype=tf.uint8)
+      image_tensor = tf.expand_dims(image_tensor, axis=0)
+      heatmaps_tensor = tf.expand_dims(
+          tf.constant(heatmaps, dtype=tf.float32), axis=0)
+      output_image = visualization_utils.draw_heatmaps_on_image_tensors(
+          images=image_tensor,
+          heatmaps=heatmaps_tensor,
+          apply_sigmoid=False)
+
+      with self.test_session() as sess:
+        sess.run(tf.global_variables_initializer())
+
+        output_image_np = sess.run(output_image)
+      self.assertAllEqual(
+          output_image_np,
+          np.expand_dims(
+              np.array([[[240, 248, 255], [127, 255, 0]],
+                        [[127, 255, 0], [240, 248, 255]]]),
+              axis=0))
+
   def test_add_cdf_image_summary(self):
     values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
     visualization_utils.add_cdf_image_summary(values, 'PositiveAnchorLoss')
@@ -371,6 +471,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
                                           minval=0.0,
                                           maxval=1.0,
                                           dtype=tf.float32)
+    num_groundtruth_boxes = tf.constant([3, 8, 0, 2], tf.int32)
     groundtruth_classes = tf.random_uniform([4, 8],
                                             minval=1,
                                             maxval=3,
@@ -390,7 +491,9 @@ class VisualizationUtilsTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_boxes:
             groundtruth_boxes,
         fields.InputDataFields.groundtruth_classes:
-            groundtruth_classes
+            groundtruth_classes,
+        fields.InputDataFields.num_groundtruth_boxes:
+            num_groundtruth_boxes
     }
     metric_ops = eval_metric_ops.get_estimator_eval_metric_ops(eval_dict)
     _, update_op = metric_ops[next(six.iterkeys(metric_ops))]
@@ -446,6 +549,28 @@ class VisualizationUtilsTest(tf.test.TestCase):
           six.b(''),
           value_ops_out[metric_op_base + '/' + str(max_examples_to_draw - 1)])
 
+  def test_visualize_boxes_and_labels_on_image_array(self):
+    ori_image = np.ones([360, 480, 3], dtype=np.int32) * 255
+    test_image = np.ones([360, 480, 3], dtype=np.int32) * 255
+    detections = np.array([[0.8, 0.1, 0.9, 0.1, 1., 0.1],
+                           [0.1, 0.3, 0.8, 0.7, 1., 0.6]])
+    keypoints = np.array(np.random.rand(2, 5, 2), dtype=np.float32)
+    labelmap = {1: {'id': 1, 'name': 'cat'}, 2: {'id': 2, 'name': 'dog'}}
+    visualization_utils.visualize_boxes_and_labels_on_image_array(
+        test_image,
+        detections[:, :4],
+        detections[:, 4].astype(np.int32),
+        detections[:, 5],
+        labelmap,
+        keypoints=keypoints,
+        track_ids=None,
+        use_normalized_coordinates=True,
+        max_boxes_to_draw=1,
+        min_score_thresh=0.2,
+        agnostic_mode=False,
+        line_thickness=8)
+    self.assertGreater(np.abs(np.sum(test_image - ori_image)), 0)
+
 
 if __name__ == '__main__':
   tf.test.main()
