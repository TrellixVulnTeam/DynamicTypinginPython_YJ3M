commit 36d27f011bb8adc4d7b3b29b1cd1af8644993139
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon May 11 13:31:54 2020 -0700

    Internal change
    
    PiperOrigin-RevId: 310981542

diff --git a/official/nlp/transformer/attention_layer.py b/official/nlp/transformer/attention_layer.py
index 6ba0f88e..114bd5fa 100644
--- a/official/nlp/transformer/attention_layer.py
+++ b/official/nlp/transformer/attention_layer.py
@@ -54,7 +54,7 @@ class Attention(tf.keras.layers.Layer):
       limit = math.sqrt(6.0 / (fan_in + fan_out))
       return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)
 
-    attention_initializer = _glorot_initializer(input_shape[-1],
+    attention_initializer = _glorot_initializer(input_shape.as_list()[-1],
                                                 self.hidden_size)
     self.query_dense_layer = layers.DenseEinsum(
         output_shape=(self.num_heads, size_per_head),
