commit 56e088266fca09ca07d70e7eed8717b1232b218e
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Sun Aug 16 22:39:48 2015 +0200

    Switch from nydus to rb in most places.

diff --git a/setup.py b/setup.py
index 389df19584..ca97956526 100755
--- a/setup.py
+++ b/setup.py
@@ -110,6 +110,7 @@ install_requires = [
     'toronado>=0.0.4,<0.1.0',
     'ua-parser>=0.3.5',
     'urllib3>=1.7.1,<1.8.0',
+    'rb',
 ]
 
 postgres_requires = [
diff --git a/src/sentry/buffer/redis.py b/src/sentry/buffer/redis.py
index fd21d28e10..566e9024ab 100644
--- a/src/sentry/buffer/redis.py
+++ b/src/sentry/buffer/redis.py
@@ -11,7 +11,7 @@ from django.conf import settings
 from django.db import models
 from django.utils.encoding import smart_str
 from hashlib import md5
-from nydus.db import create_cluster
+from rb import Cluster
 from time import time
 
 from sentry.buffer import Buffer
@@ -33,16 +33,12 @@ class RedisBuffer(Buffer):
         options.setdefault('hosts', {
             0: {},
         })
-        options.setdefault('router', 'nydus.db.routers.keyvalue.PartitionRouter')
-        self.conn = create_cluster({
-            'engine': 'nydus.db.backends.redis.Redis',
-            'router': options['router'],
-            'hosts': options['hosts'],
-        })
+        self.cluster = Cluster(options['hosts'])
 
     def validate(self):
         try:
-            self.conn.ping()
+            with self.cluster.all() as client:
+                client.ping()
         except Exception as e:
             raise InvalidConfiguration(unicode(e))
 
@@ -78,7 +74,7 @@ class RedisBuffer(Buffer):
         key = self._make_key(model, filters)
         # We can't use conn.map() due to wanting to support multiple pending
         # keys (one per Redis shard)
-        conn = self.conn.get_conn(key)
+        conn = self.cluster.get_local_client_for_key(key)
 
         pipe = conn.pipeline()
         pipe.hsetnx(key, 'm', '%s.%s' % (model.__module__, model.__name__))
@@ -94,13 +90,15 @@ class RedisBuffer(Buffer):
         pipe.execute()
 
     def process_pending(self):
+        client = self.cluster.get_routing_client()
         lock_key = self._make_lock_key(self.pending_key)
         # prevent a stampede due to celerybeat + periodic task
-        if not self.conn.set(lock_key, '1', nx=True, ex=60):
+        if not client.set(lock_key, '1', nx=True, ex=60):
             return
 
         try:
-            for conn in self.conn.hosts.itervalues():
+            for host_id in self.cluster.hosts.iterkeys():
+                conn = self.cluster.get_local_client(host_id)
                 keys = conn.zrange(self.pending_key, 0, -1)
                 if not keys:
                     continue
@@ -112,17 +110,18 @@ class RedisBuffer(Buffer):
                 pipe.zrem(self.pending_key, *keys)
                 pipe.execute()
         finally:
-            self.conn.delete(lock_key)
+            client.delete(lock_key)
 
     def process(self, key):
+        client = self.cluster.get_routing_client()
         lock_key = self._make_lock_key(key)
         # prevent a stampede due to the way we use celery etas + duplicate
         # tasks
-        if not self.conn.set(lock_key, '1', nx=True, ex=10):
+        if not client.set(lock_key, '1', nx=True, ex=10):
             self.logger.info('Skipped process on %s; unable to get lock', key)
             return
 
-        with self.conn.map() as conn:
+        with self.cluster.map() as conn:
             values = conn.hgetall(key)
             conn.delete(key)
 
@@ -130,11 +129,11 @@ class RedisBuffer(Buffer):
             self.logger.info('Skipped process on %s; no values found', key)
             return
 
-        model = import_string(values['m'])
-        filters = pickle.loads(values['f'])
+        model = import_string(values.value['m'])
+        filters = pickle.loads(values.value['f'])
         incr_values = {}
         extra_values = {}
-        for k, v in values.iteritems():
+        for k, v in values.value.iteritems():
             if k.startswith('i+'):
                 incr_values[k[2:]] = int(v)
             elif k.startswith('e+'):
diff --git a/src/sentry/cache/redis.py b/src/sentry/cache/redis.py
index 496c429946..68cedf8517 100644
--- a/src/sentry/cache/redis.py
+++ b/src/sentry/cache/redis.py
@@ -9,7 +9,7 @@ sentry.cache.redis
 from __future__ import absolute_import
 
 from django.conf import settings
-from nydus.db import create_cluster
+from rb import Cluster
 from threading import local
 
 from sentry.utils import json
@@ -26,24 +26,21 @@ class RedisCache(local):
         options.setdefault('hosts', {
             0: {},
         })
-        options.setdefault('router', 'nydus.db.routers.keyvalue.PartitionRouter')
-        self.conn = create_cluster({
-            'engine': 'nydus.db.backends.redis.Redis',
-            'router': options['router'],
-            'hosts': options['hosts'],
-        })
+        self.cluster = Cluster(options['hosts'])
+        self.client = self.cluster.get_routing_client()
 
     def set(self, key, value, timeout):
-        with self.conn.map() as conn:
-            conn.set(key, json.dumps(value))
-            if timeout:
-                conn.expire(key, timeout)
+        v = json.dumps(value)
+        if timeout:
+            self.client.setex(key, int(timeout), v)
+        else:
+            self.client.set(key, v)
 
     def delete(self, key):
-        self.conn.delete(key)
+        self.client.delete(key)
 
     def get(self, key):
-        result = self.conn.get(key)
+        result = self.client.get(key)
         if result is not None:
             result = json.loads(result)
         return result
diff --git a/src/sentry/quotas/redis.py b/src/sentry/quotas/redis.py
index 910219599f..bf005046ba 100644
--- a/src/sentry/quotas/redis.py
+++ b/src/sentry/quotas/redis.py
@@ -10,7 +10,7 @@ from __future__ import absolute_import
 import time
 
 from django.conf import settings
-from nydus.db import create_cluster
+from rb import Cluster
 
 from sentry.exceptions import InvalidConfiguration
 from sentry.quotas.base import Quota, RateLimited, NotRateLimited
@@ -25,16 +25,12 @@ class RedisQuota(Quota):
             options = settings.SENTRY_REDIS_OPTIONS
         super(RedisQuota, self).__init__(**options)
         options.setdefault('hosts', {0: {}})
-        options.setdefault('router', 'nydus.db.routers.keyvalue.PartitionRouter')
-        self.conn = create_cluster({
-            'engine': 'nydus.db.backends.redis.Redis',
-            'router': options['router'],
-            'hosts': options['hosts'],
-        })
+        self.cluster = Cluster(options['hosts'])
 
     def validate(self):
         try:
-            self.conn.ping()
+            with self.cluster.all() as client:
+                client.ping()
         except Exception as e:
             raise InvalidConfiguration(unicode(e))
 
@@ -63,7 +59,8 @@ class RedisQuota(Quota):
         return NotRateLimited
 
     def get_time_remaining(self):
-        return int(self.ttl - (time.time() - int(time.time() / self.ttl) * self.ttl))
+        return int(self.ttl - (
+            time.time() - int(time.time() / self.ttl) * self.ttl))
 
     def _get_system_key(self):
         return 'quota:s:%s' % (int(time.time() / self.ttl),)
@@ -79,17 +76,21 @@ class RedisQuota(Quota):
             team_key = self._get_team_key(project.team)
         else:
             team_key = None
-            team_result = 0
+            team_result = None
 
         proj_key = self._get_project_key(project)
         sys_key = self._get_system_key()
-        with self.conn.map() as conn:
-            proj_result = conn.incr(proj_key)
-            conn.expire(proj_key, self.ttl)
-            sys_result = conn.incr(sys_key)
-            conn.expire(sys_key, self.ttl)
+        with self.cluster.map() as client:
+            proj_result = client.incr(proj_key)
+            client.expire(proj_key, self.ttl)
+            sys_result = client.incr(sys_key)
+            client.expire(sys_key, self.ttl)
             if team_key:
-                team_result = conn.incr(team_key)
-                conn.expire(team_key, self.ttl)
-
-        return int(sys_result), int(team_result), int(proj_result)
+                team_result = client.incr(team_key)
+                client.expire(team_key, self.ttl)
+
+        return (
+            int(sys_result.value),
+            int(team_result and team_result.value or 0),
+            int(proj_result.value),
+        )
diff --git a/src/sentry/ratelimits/redis.py b/src/sentry/ratelimits/redis.py
index 26512a3e33..8f698d3dc2 100644
--- a/src/sentry/ratelimits/redis.py
+++ b/src/sentry/ratelimits/redis.py
@@ -1,7 +1,7 @@
 from __future__ import absolute_import
 
 from django.conf import settings
-from nydus.db import create_cluster
+from rb import Cluster
 from time import time
 
 from sentry.exceptions import InvalidConfiguration
@@ -16,17 +16,13 @@ class RedisRateLimiter(RateLimiter):
             # inherit default options from REDIS_OPTIONS
             options = settings.SENTRY_REDIS_OPTIONS
         options.setdefault('hosts', {0: {}})
-        options.setdefault('router', 'nydus.db.routers.keyvalue.PartitionRouter')
 
-        self.conn = create_cluster({
-            'engine': 'nydus.db.backends.redis.Redis',
-            'router': options['router'],
-            'hosts': options['hosts'],
-        })
+        self.cluster = Cluster(options['hosts'])
 
     def validate(self):
         try:
-            self.conn.ping()
+            with self.cluster.all() as client:
+                client.ping()
         except Exception as e:
             raise InvalidConfiguration(unicode(e))
 
@@ -35,10 +31,8 @@ class RedisRateLimiter(RateLimiter):
             key, project.id, int(time() / self.ttl)
         )
 
-        with self.conn.map() as conn:
-            proj_result = conn.incr(key)
-            conn.expire(key, self.ttl)
+        with self.cluster.map() as client:
+            proj_result = client.incr(key)
+            client.expire(key, self.ttl)
 
-        if int(proj_result) > limit:
-            return True
-        return False
+        return proj_result.value > limit
diff --git a/src/sentry/tsdb/redis.py b/src/sentry/tsdb/redis.py
index 2e57dfe7d1..a007dabebb 100644
--- a/src/sentry/tsdb/redis.py
+++ b/src/sentry/tsdb/redis.py
@@ -15,7 +15,8 @@ from datetime import timedelta
 from django.conf import settings
 from django.utils import timezone
 from hashlib import md5
-from nydus.db import create_cluster
+
+from rb import Cluster
 
 from sentry.exceptions import InvalidConfiguration
 from sentry.tsdb.base import BaseTSDB
@@ -47,29 +48,22 @@ class RedisTSDB(BaseTSDB):
     - ``vnodes`` controls the shard distribution and should ideally be set to
       the maximum number of physical hosts.
     """
-    def __init__(self, hosts=None, router=None, prefix='ts:', vnodes=64,
-                 **kwargs):
+    def __init__(self, hosts=None, prefix='ts:', vnodes=64, **kwargs):
         # inherit default options from REDIS_OPTIONS
         defaults = settings.SENTRY_REDIS_OPTIONS
 
         if hosts is None:
             hosts = defaults.get('hosts', {0: {}})
 
-        if router is None:
-            router = defaults.get('router', 'nydus.db.routers.keyvalue.PartitionRouter')
-
-        self.conn = create_cluster({
-            'engine': 'nydus.db.backends.redis.Redis',
-            'router': router,
-            'hosts': hosts,
-        })
+        self.cluster = Cluster(hosts)
         self.prefix = prefix
         self.vnodes = vnodes
         super(RedisTSDB, self).__init__(**kwargs)
 
     def validate(self):
         try:
-            self.conn.ping()
+            with self.cluster.all() as client:
+                client.ping()
         except Exception as e:
             raise InvalidConfiguration(unicode(e))
 
@@ -106,7 +100,7 @@ class RedisTSDB(BaseTSDB):
         if timestamp is None:
             timestamp = timezone.now()
 
-        with self.conn.map() as conn:
+        with self.cluster.map() as client:
             for rollup, max_values in self.rollups:
                 norm_rollup = normalize_to_rollup(timestamp, rollup)
                 expire = rollup * max_values
@@ -114,8 +108,8 @@ class RedisTSDB(BaseTSDB):
                 for model, key in items:
                     model_key = self.get_model_key(key)
                     hash_key = make_key(model, norm_rollup, model_key)
-                    conn.hincrby(hash_key, model_key, count)
-                    conn.expire(hash_key, expire)
+                    client.hincrby(hash_key, model_key, count)
+                    client.expire(hash_key, expire)
 
     def get_range(self, model, keys, start, end, rollup=None):
         """
@@ -137,7 +131,7 @@ class RedisTSDB(BaseTSDB):
 
         results = []
         timestamp = end
-        with self.conn.map() as conn:
+        with self.cluster.map() as client:
             while timestamp >= start:
                 real_epoch = normalize_to_epoch(timestamp, rollup)
                 norm_epoch = normalize_to_rollup(timestamp, rollup)
@@ -145,13 +139,14 @@ class RedisTSDB(BaseTSDB):
                 for key in keys:
                     model_key = self.get_model_key(key)
                     hash_key = make_key(model, norm_epoch, model_key)
-                    results.append((real_epoch, key, conn.hget(hash_key, model_key)))
+                    results.append((real_epoch, key,
+                                    client.hget(hash_key, model_key)))
 
                 timestamp = timestamp - timedelta(seconds=rollup)
 
         results_by_key = defaultdict(dict)
         for epoch, key, count in results:
-            results_by_key[key][epoch] = int(count or 0)
+            results_by_key[key][epoch] = int(count.value or 0)
 
         for key, points in results_by_key.iteritems():
             results_by_key[key] = sorted(points.items())
diff --git a/tests/sentry/buffer/redis/tests.py b/tests/sentry/buffer/redis/tests.py
index 50696ae954..6b15e06020 100644
--- a/tests/sentry/buffer/redis/tests.py
+++ b/tests/sentry/buffer/redis/tests.py
@@ -17,8 +17,8 @@ class RedisBufferTest(TestCase):
 
     def test_default_host_is_local(self):
         buf = RedisBuffer()
-        self.assertEquals(len(buf.conn.hosts), 1)
-        self.assertEquals(buf.conn.hosts[0].host, 'localhost')
+        self.assertEquals(len(buf.cluster.hosts), 1)
+        self.assertEquals(buf.cluster.hosts[0].host, 'localhost')
 
     def test_coerce_val_handles_foreignkeys(self):
         assert self.buf._coerce_val(Project(id=1)) == '1'
@@ -29,18 +29,21 @@ class RedisBufferTest(TestCase):
     @mock.patch('sentry.buffer.redis.RedisBuffer._make_key', mock.Mock(return_value='foo'))
     @mock.patch('sentry.buffer.redis.process_incr')
     def test_process_pending(self, process_incr):
-        self.buf.conn.zadd('b:p', 1, 'foo')
-        self.buf.conn.zadd('b:p', 2, 'bar')
+        with self.buf.cluster.map() as client:
+            client.zadd('b:p', 1, 'foo')
+            client.zadd('b:p', 2, 'bar')
         self.buf.process_pending()
         assert len(process_incr.apply_async.mock_calls) == 2
         process_incr.apply_async.assert_any_call(kwargs={'key': 'foo'})
         process_incr.apply_async.assert_any_call(kwargs={'key': 'bar'})
-        assert self.buf.conn.zrange('b:p', 0, -1) == []
+        client = self.buf.cluster.get_routing_client()
+        assert client.zrange('b:p', 0, -1) == []
 
     @mock.patch('sentry.buffer.redis.RedisBuffer._make_key', mock.Mock(return_value='foo'))
     @mock.patch('sentry.buffer.base.Buffer.process')
     def test_process_does_bubble_up(self, process):
-        self.buf.conn.hmset('foo', {
+        client = self.buf.cluster.get_routing_client()
+        client.hmset('foo', {
             'e+foo': "S'bar'\np1\n.",
             'f': "(dp1\nS'pk'\np2\nI1\ns.",
             'i+times_seen': '2',
@@ -55,27 +58,28 @@ class RedisBufferTest(TestCase):
     @mock.patch('sentry.buffer.redis.RedisBuffer._make_key', mock.Mock(return_value='foo'))
     @mock.patch('sentry.buffer.redis.process_incr', mock.Mock())
     def test_incr_saves_to_redis(self):
+        client = self.buf.cluster.get_routing_client()
         model = mock.Mock()
         model.__name__ = 'Mock'
         columns = {'times_seen': 1}
         filters = {'pk': 1}
         self.buf.incr(model, columns, filters, extra={'foo': 'bar'})
-        result = self.buf.conn.hgetall('foo')
+        result = client.hgetall('foo')
         assert result == {
             'e+foo': "S'bar'\np1\n.",
             'f': "(dp1\nS'pk'\np2\nI1\ns.",
             'i+times_seen': '1',
             'm': 'mock.Mock',
         }
-        pending = self.buf.conn.zrange('b:p', 0, -1)
+        pending = client.zrange('b:p', 0, -1)
         assert pending == ['foo']
         self.buf.incr(model, columns, filters, extra={'foo': 'bar'})
-        result = self.buf.conn.hgetall('foo')
+        result = client.hgetall('foo')
         assert result == {
             'e+foo': "S'bar'\np1\n.",
             'f': "(dp1\nS'pk'\np2\nI1\ns.",
             'i+times_seen': '2',
             'm': 'mock.Mock',
         }
-        pending = self.buf.conn.zrange('b:p', 0, -1)
+        pending = client.zrange('b:p', 0, -1)
         assert pending == ['foo']
diff --git a/tests/sentry/quotas/redis/tests.py b/tests/sentry/quotas/redis/tests.py
index 318df96156..4f333ceda2 100644
--- a/tests/sentry/quotas/redis/tests.py
+++ b/tests/sentry/quotas/redis/tests.py
@@ -44,8 +44,8 @@ class RedisQuotaTest(TestCase):
 
     def test_default_host_is_local(self):
         quota = RedisQuota()
-        self.assertEquals(len(quota.conn.hosts), 1)
-        self.assertEquals(quota.conn.hosts[0].host, 'localhost')
+        self.assertEquals(len(quota.cluster.hosts), 1)
+        self.assertEquals(quota.cluster.hosts[0].host, 'localhost')
 
     def test_bails_immediately_without_any_quota(self):
         self._incr_project.return_value = (0, 0, 0)
diff --git a/tests/sentry/tsdb/test_redis.py b/tests/sentry/tsdb/test_redis.py
index a7b5595464..e16c57df88 100644
--- a/tests/sentry/tsdb/test_redis.py
+++ b/tests/sentry/tsdb/test_redis.py
@@ -18,7 +18,9 @@ class RedisTSDBTest(TestCase):
             (ONE_HOUR, 24),  # 1 days at 1 hour
             (ONE_DAY, 30),  # 30 days at 1 day
         ), vnodes=64)
-        self.db.conn.flushdb()
+
+        with self.db.cluster.all() as client:
+            client.flushdb()
 
     def test_make_key(self):
         result = self.db.make_key(TSDBModel.project, 1368889980, 1)
