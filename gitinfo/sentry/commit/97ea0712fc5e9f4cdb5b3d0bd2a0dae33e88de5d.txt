commit 97ea0712fc5e9f4cdb5b3d0bd2a0dae33e88de5d
Author: ted kaemming <ted@kaemming.com>
Date:   Wed Oct 11 09:59:42 2017 -0700

    feat(tsdb): Add support for environment filtering (#6256)

diff --git a/src/sentry/tsdb/base.py b/src/sentry/tsdb/base.py
index 330001273c..5f8ca9cfeb 100644
--- a/src/sentry/tsdb/base.py
+++ b/src/sentry/tsdb/base.py
@@ -246,7 +246,7 @@ class BaseTSDB(Service):
             rollup,
         )
 
-    def incr(self, model, key, timestamp=None, count=1):
+    def incr(self, model, key, timestamp=None, count=1, environment_id=None):
         """
         Increment project ID=1:
 
@@ -254,28 +254,28 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def incr_multi(self, items, timestamp=None, count=1):
+    def incr_multi(self, items, timestamp=None, count=1, environment_id=None):
         """
         Increment project ID=1 and group ID=5:
 
         >>> incr_multi([(TimeSeriesModel.project, 1), (TimeSeriesModel.group, 5)])
         """
         for model, key in items:
-            self.incr(model, key, timestamp, count)
+            self.incr(model, key, timestamp, count, environment_id=environment_id)
 
-    def merge(self, model, destination, sources, timestamp=None):
+    def merge(self, model, destination, sources, timestamp=None, environment_ids=None):
         """
         Transfer all counters from the source keys to the destination key.
         """
         raise NotImplementedError
 
-    def delete(self, models, keys, start=None, end=None, timestamp=None):
+    def delete(self, models, keys, start=None, end=None, timestamp=None, environment_ids=None):
         """
         Delete all counters.
         """
         raise NotImplementedError
 
-    def get_range(self, model, keys, start, end, rollup=None):
+    def get_range(self, model, keys, start, end, rollup=None, environment_id=None):
         """
         To get a range of data for group ID=[1, 2, 3]:
 
@@ -288,8 +288,8 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def get_sums(self, model, keys, start, end, rollup=None):
-        range_set = self.get_range(model, keys, start, end, rollup)
+    def get_sums(self, model, keys, start, end, rollup=None, environment_id=None):
+        range_set = self.get_range(model, keys, start, end, rollup, environment_id)
         sum_set = dict(
             (key, sum(p for _, p in points)) for (key, points) in six.iteritems(range_set)
         )
@@ -314,52 +314,57 @@ class BaseTSDB(Service):
                     last_new_ts = new_ts
         return result
 
-    def record(self, model, key, values, timestamp=None):
+    def record(self, model, key, values, timestamp=None, environment_id=None):
         """
         Record occurences of items in a single distinct counter.
         """
         raise NotImplementedError
 
-    def record_multi(self, items, timestamp=None):
+    def record_multi(self, items, timestamp=None, environment_id=None):
         """
         Record occurences of items in multiple distinct counters.
         """
         for model, key, values in items:
-            self.record(model, key, values, timestamp)
+            self.record(model, key, values, timestamp, environment_id=environment_id)
 
-    def get_distinct_counts_series(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_series(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         """
         Fetch counts of distinct items for each rollup interval within the range.
         """
         raise NotImplementedError
 
-    def get_distinct_counts_totals(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_totals(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         """
         Count distinct items during a time range.
         """
         raise NotImplementedError
 
-    def get_distinct_counts_union(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_union(self, model, keys, start, end=None,
+                                  rollup=None, environment_id=None):
         """
         Count the total number of distinct items across multiple counters
         during a time range.
         """
         raise NotImplementedError
 
-    def merge_distinct_counts(self, model, destination, sources, timestamp=None):
+    def merge_distinct_counts(self, model, destination, sources,
+                              timestamp=None, environment_ids=None):
         """
         Transfer all distinct counters from the source keys to the
         destination key.
         """
         raise NotImplementedError
 
-    def delete_distinct_counts(self, models, keys, start=None, end=None, timestamp=None):
+    def delete_distinct_counts(self, models, keys, start=None, end=None,
+                               timestamp=None, environment_ids=None):
         """
         Delete all distinct counters.
         """
         raise NotImplementedError
 
-    def record_frequency_multi(self, requests, timestamp=None):
+    def record_frequency_multi(self, requests, timestamp=None, environment_id=None):
         """
         Record items in a frequency table.
 
@@ -368,7 +373,8 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def get_most_frequent(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent(self, model, keys, start, end=None,
+                          rollup=None, limit=None, environment_id=None):
         """
         Retrieve the most frequently seen items in a frequency table.
 
@@ -380,7 +386,8 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def get_most_frequent_series(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent_series(self, model, keys, start, end=None,
+                                 rollup=None, limit=None, environment_id=None):
         """
         Retrieve the most frequently seen items in a frequency table for each
         interval in a series. (This is in contrast with ``get_most_frequent``,
@@ -394,7 +401,7 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def get_frequency_series(self, model, items, start, end=None, rollup=None):
+    def get_frequency_series(self, model, items, start, end=None, rollup=None, environment_id=None):
         """
         Retrieve the frequency of known items in a table over time.
 
@@ -408,7 +415,7 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def get_frequency_totals(self, model, items, start, end=None, rollup=None):
+    def get_frequency_totals(self, model, items, start, end=None, rollup=None, environment_id=None):
         """
         Retrieve the total frequency of known items in a table over time.
 
@@ -422,14 +429,15 @@ class BaseTSDB(Service):
         """
         raise NotImplementedError
 
-    def merge_frequencies(self, model, destination, sources, timestamp=None):
+    def merge_frequencies(self, model, destination, sources, timestamp=None, environment_ids=None):
         """
         Transfer all frequency tables from the source keys to the destination
         key.
         """
         raise NotImplementedError
 
-    def delete_frequencies(self, models, keys, start=None, end=None, timestamp=None):
+    def delete_frequencies(self, models, keys, start=None, end=None,
+                           timestamp=None, environment_ids=None):
         """
         Delete all frequency tables.
         """
diff --git a/src/sentry/tsdb/dummy.py b/src/sentry/tsdb/dummy.py
index 1800b95aaf..295b465e41 100644
--- a/src/sentry/tsdb/dummy.py
+++ b/src/sentry/tsdb/dummy.py
@@ -15,49 +15,56 @@ class DummyTSDB(BaseTSDB):
     A no-op time-series storage.
     """
 
-    def incr(self, model, key, timestamp=None, count=1):
+    def incr(self, model, key, timestamp=None, count=1, environment_id=None):
         pass
 
-    def merge(self, model, destination, sources, timestamp=None):
+    def merge(self, model, destination, sources, timestamp=None, environment_ids=None):
         pass
 
-    def delete(self, models, keys, start=None, end=None, timestamp=None):
+    def delete(self, models, keys, start=None, end=None, timestamp=None, environment_ids=None):
         pass
 
-    def get_range(self, model, keys, start, end, rollup=None):
+    def get_range(self, model, keys, start, end, rollup=None, environment_id=None):
         _, series = self.get_optimal_rollup_series(start, end, rollup)
         return {k: [(ts, 0) for ts in series] for k in keys}
 
-    def record(self, model, key, values, timestamp=None):
+    def record(self, model, key, values, timestamp=None, environment_id=None):
         pass
 
-    def get_distinct_counts_series(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_series(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         _, series = self.get_optimal_rollup_series(start, end, rollup)
         return {k: [(ts, 0) for ts in series] for k in keys}
 
-    def get_distinct_counts_totals(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_totals(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         return {k: 0 for k in keys}
 
-    def get_distinct_counts_union(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_union(self, model, keys, start, end=None,
+                                  rollup=None, environment_id=None):
         return 0
 
-    def merge_distinct_counts(self, model, destination, sources, timestamp=None):
+    def merge_distinct_counts(self, model, destination, sources,
+                              timestamp=None, environment_ids=None):
         pass
 
-    def delete_distinct_counts(self, models, keys, start=None, end=None, timestamp=None):
+    def delete_distinct_counts(self, models, keys, start=None, end=None,
+                               timestamp=None, environment_ids=None):
         pass
 
-    def record_frequency_multi(self, requests, timestamp=None):
+    def record_frequency_multi(self, requests, timestamp=None, environment_id=None):
         pass
 
-    def get_most_frequent(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent(self, model, keys, start, end=None,
+                          rollup=None, limit=None, environment_id=None):
         return {key: [] for key in keys}
 
-    def get_most_frequent_series(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent_series(self, model, keys, start, end=None,
+                                 rollup=None, limit=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
         return {key: [(timestamp, {}) for timestamp in series] for key in keys}
 
-    def get_frequency_series(self, model, items, start, end=None, rollup=None):
+    def get_frequency_series(self, model, items, start, end=None, rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
@@ -68,14 +75,15 @@ class DummyTSDB(BaseTSDB):
 
         return results
 
-    def get_frequency_totals(self, model, items, start, end=None, rollup=None):
+    def get_frequency_totals(self, model, items, start, end=None, rollup=None, environment_id=None):
         results = {}
         for key, members in items.items():
             results[key] = {member: 0.0 for member in members}
         return results
 
-    def merge_frequencies(self, model, destination, sources, timestamp=None):
+    def merge_frequencies(self, model, destination, sources, timestamp=None, environment_ids=None):
         pass
 
-    def delete_frequencies(self, models, keys, start=None, end=None, timestamp=None):
+    def delete_frequencies(self, models, keys, start=None, end=None,
+                           timestamp=None, environment_ids=None):
         pass
diff --git a/src/sentry/tsdb/inmemory.py b/src/sentry/tsdb/inmemory.py
index ae897dd34c..24d164aea8 100644
--- a/src/sentry/tsdb/inmemory.py
+++ b/src/sentry/tsdb/inmemory.py
@@ -27,34 +27,47 @@ class InMemoryTSDB(BaseTSDB):
         super(InMemoryTSDB, self).__init__(*args, **kwargs)
         self.flush()
 
-    def incr(self, model, key, timestamp=None, count=1):
+    def incr(self, model, key, timestamp=None, count=1, environment_id=None):
+        environment_ids = set([environment_id, None])
+
         if timestamp is None:
             timestamp = timezone.now()
 
         for rollup, max_values in six.iteritems(self.rollups):
             norm_epoch = self.normalize_to_rollup(timestamp, rollup)
-            self.data[model][key][norm_epoch] += count
+            for environment_id in environment_ids:
+                self.data[model][(key, environment_id)][norm_epoch] += count
+
+    def merge(self, model, destination, sources, timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
+
+        for environment_id in environment_ids:
+            destination = self.data[model][(destination, environment_id)]
+            for source in sources:
+                for bucket, count in self.data[model].pop((source, environment_id), {}).items():
+                    destination[bucket] += count
 
-    def merge(self, model, destination, sources, timestamp=None):
-        destination = self.data[model][destination]
-        for source in sources:
-            for bucket, count in self.data[model].pop(source, {}).items():
-                destination[bucket] += count
+    def delete(self, models, keys, start=None, end=None, timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-    def delete(self, models, keys, start=None, end=None, timestamp=None):
         rollups = self.get_active_series(start, end, timestamp)
 
         for rollup, series in rollups.items():
             for model in models:
                 for key in keys:
-                    data = self.data[model][key]
-                    for timestamp in series:
-                        data.pop(
-                            self.normalize_to_rollup(timestamp, rollup),
-                            0,
-                        )
-
-    def get_range(self, model, keys, start, end, rollup=None):
+                    for environment_id in environment_ids:
+                        data = self.data[model][(key, environment_id)]
+                        for timestamp in series:
+                            data.pop(
+                                self.normalize_to_rollup(timestamp, rollup),
+                                0,
+                            )
+
+    def get_range(self, model, keys, start, end, rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = []
@@ -62,7 +75,7 @@ class InMemoryTSDB(BaseTSDB):
             norm_epoch = self.normalize_to_rollup(timestamp, rollup)
 
             for key in keys:
-                value = self.data[model][key][norm_epoch]
+                value = self.data[model][(key, environment_id)][norm_epoch]
                 results.append((to_timestamp(timestamp), key, value))
 
         results_by_key = defaultdict(dict)
@@ -73,20 +86,24 @@ class InMemoryTSDB(BaseTSDB):
             results_by_key[key] = sorted(points.items())
         return dict(results_by_key)
 
-    def record(self, model, key, values, timestamp=None):
+    def record(self, model, key, values, timestamp=None, environment_id=None):
+        environment_ids = set([environment_id, None])
+
         if timestamp is None:
             timestamp = timezone.now()
 
         for rollup, max_values in six.iteritems(self.rollups):
             r = self.normalize_to_rollup(timestamp, rollup)
-            self.sets[model][key][r].update(values)
+            for environment_id in environment_ids:
+                self.sets[model][(key, environment_id)][r].update(values)
 
-    def get_distinct_counts_series(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_series(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
         for key in keys:
-            source = self.sets[model][key]
+            source = self.sets[model][(key, environment_id)]
             counts = results[key] = []
             for timestamp in series:
                 r = self.normalize_ts_to_rollup(timestamp, rollup)
@@ -94,12 +111,13 @@ class InMemoryTSDB(BaseTSDB):
 
         return results
 
-    def get_distinct_counts_totals(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_totals(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
         for key in keys:
-            source = self.sets[model][key]
+            source = self.sets[model][(key, environment_id)]
             values = set()
             for timestamp in series:
                 r = self.normalize_ts_to_rollup(timestamp, rollup)
@@ -108,36 +126,49 @@ class InMemoryTSDB(BaseTSDB):
 
         return results
 
-    def get_distinct_counts_union(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_union(self, model, keys, start, end=None,
+                                  rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         values = set()
         for key in keys:
-            source = self.sets[model][key]
+            source = self.sets[model][(key, environment_id)]
             for timestamp in series:
                 r = self.normalize_ts_to_rollup(timestamp, rollup)
                 values.update(source[r])
 
         return len(values)
 
-    def merge_distinct_counts(self, model, destination, sources, timestamp=None):
-        destination = self.sets[model][destination]
-        for source in sources:
-            for bucket, values in self.sets[model].pop(source, {}).items():
-                destination[bucket].update(values)
+    def merge_distinct_counts(self, model, destination, sources,
+                              timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
+
+        for environment_id in environment_ids:
+            destination = self.sets[model][(destination, environment_id)]
+            for source in sources:
+                for bucket, values in self.sets[model].pop((source, environment_id), {}).items():
+                    destination[bucket].update(values)
+
+    def delete_distinct_counts(self, models, keys, start=None, end=None,
+                               timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-    def delete_distinct_counts(self, models, keys, start=None, end=None, timestamp=None):
         rollups = self.get_active_series(start, end, timestamp)
 
         for rollup, series in rollups.items():
             for model in models:
                 for key in keys:
-                    data = self.data[model][key]
-                    for timestamp in series:
-                        data.pop(
-                            self.normalize_to_rollup(timestamp, rollup),
-                            set(),
-                        )
+                    for environment_id in environment_ids:
+                        data = self.data[model][(key, environment_id)]
+                        for timestamp in series:
+                            data.pop(
+                                self.normalize_to_rollup(timestamp, rollup),
+                                set(),
+                            )
 
     def flush(self):
         # self.data[model][key][rollup] = count
@@ -167,24 +198,28 @@ class InMemoryTSDB(BaseTSDB):
             ),
         )
 
-    def record_frequency_multi(self, requests, timestamp=None):
+    def record_frequency_multi(self, requests, timestamp=None, environment_id=None):
+        environment_ids = set([environment_id, None])
+
         if timestamp is None:
             timestamp = timezone.now()
 
         for model, request in requests:
             for key, items in request.items():
                 items = {k: float(v) for k, v in items.items()}
-                source = self.frequencies[model][key]
-                for rollup in self.rollups:
-                    source[self.normalize_to_rollup(timestamp, rollup)].update(items)
+                for environment_id in environment_ids:
+                    source = self.frequencies[model][(key, environment_id)]
+                    for rollup in self.rollups:
+                        source[self.normalize_to_rollup(timestamp, rollup)].update(items)
 
-    def get_most_frequent(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent(self, model, keys, start, end=None,
+                          rollup=None, limit=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
         for key in keys:
             result = results[key] = Counter()
-            source = self.frequencies[model][key]
+            source = self.frequencies[model][(key, environment_id)]
             for timestamp in series:
                 result.update(source[self.normalize_ts_to_rollup(timestamp, rollup)])
 
@@ -193,37 +228,38 @@ class InMemoryTSDB(BaseTSDB):
 
         return results
 
-    def get_most_frequent_series(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent_series(self, model, keys, start, end=None,
+                                 rollup=None, limit=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
         for key in keys:
             result = results[key] = []
-            source = self.frequencies[model][key]
+            source = self.frequencies[model][(key, environment_id)]
             for timestamp in series:
                 data = source[self.normalize_ts_to_rollup(timestamp, rollup)]
                 result.append((timestamp, dict(data.most_common(limit))))
 
         return results
 
-    def get_frequency_series(self, model, items, start, end=None, rollup=None):
+    def get_frequency_series(self, model, items, start, end=None, rollup=None, environment_id=None):
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         results = {}
         for key, members in items.items():
             result = results[key] = []
-            source = self.frequencies[model][key]
+            source = self.frequencies[model][(key, environment_id)]
             for timestamp in series:
                 scores = source[self.normalize_ts_to_rollup(timestamp, rollup)]
                 result.append((timestamp, {k: scores.get(k, 0.0) for k in members}, ))
 
         return results
 
-    def get_frequency_totals(self, model, items, start, end=None, rollup=None):
+    def get_frequency_totals(self, model, items, start, end=None, rollup=None, environment_id=None):
         results = {}
 
         for key, series in six.iteritems(
-            self.get_frequency_series(model, items, start, end, rollup)
+            self.get_frequency_series(model, items, start, end, rollup, environment_id)
         ):
             result = results[key] = {}
             for timestamp, scores in series:
@@ -232,21 +268,32 @@ class InMemoryTSDB(BaseTSDB):
 
         return results
 
-    def merge_frequencies(self, model, destination, sources, timestamp=None):
-        destination = self.frequencies[model][destination]
-        for source in sources:
-            for bucket, counter in self.data[model].pop(source, {}).items():
-                destination[bucket].update(counter)
+    def merge_frequencies(self, model, destination, sources, timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
+
+        for environment_id in environment_ids:
+            destination = self.frequencies[model][(destination, environment_id)]
+            for source in sources:
+                for bucket, counter in self.data[model].pop((source, environment_id), {}).items():
+                    destination[bucket].update(counter)
+
+    def delete_frequencies(self, models, keys, start=None, end=None,
+                           timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-    def delete_frequencies(self, models, keys, start=None, end=None, timestamp=None):
         rollups = self.get_active_series(start, end, timestamp)
 
         for rollup, series in rollups.items():
             for model in models:
                 for key in keys:
-                    data = self.frequencies[model][key]
-                    for timestamp in series:
-                        data.pop(
-                            self.normalize_to_rollup(timestamp, rollup),
-                            Counter(),
-                        )
+                    for environment_id in environment_ids:
+                        data = self.frequencies[model][(key, environment_id)]
+                        for timestamp in series:
+                            data.pop(
+                                self.normalize_to_rollup(timestamp, rollup),
+                                Counter(),
+                            )
diff --git a/src/sentry/tsdb/redis.py b/src/sentry/tsdb/redis.py
index f5a38d2a70..e1c87e1741 100644
--- a/src/sentry/tsdb/redis.py
+++ b/src/sentry/tsdb/redis.py
@@ -110,19 +110,37 @@ class RedisTSDB(BaseTSDB):
             label='TSDB',
         )
 
-    def make_key(self, model, rollup, timestamp, key):
+    def get_cluster(self, environment_id):
+        return self.cluster
+
+    def get_cluster_groups(self, environment_ids):
+        results = defaultdict(list)
+        for environment_id in environment_ids:
+            results[self.get_cluster(environment_id)].append(environment_id)
+        return results.items()
+
+    def add_environment_parameter(self, key, environment_id):
+        if environment_id is not None:
+            return '{}?e={}'.format(key, environment_id)
+        else:
+            return key
+
+    def make_key(self, model, rollup, timestamp, key, environment_id):
         """
         Make a key that is used for distinct counter and frequency table
         values.
         """
-        return '{prefix}{model}:{epoch}:{key}'.format(
-            prefix=self.prefix,
-            model=model.value,
-            epoch=self.normalize_ts_to_rollup(timestamp, rollup),
-            key=self.get_model_key(key),
+        return self.add_environment_parameter(
+            '{prefix}{model}:{epoch}:{key}'.format(
+                prefix=self.prefix,
+                model=model.value,
+                epoch=self.normalize_ts_to_rollup(timestamp, rollup),
+                key=self.get_model_key(key),
+            ),
+            environment_id,
         )
 
-    def make_counter_key(self, model, rollup, timestamp, key):
+    def make_counter_key(self, model, rollup, timestamp, key, environment_id):
         """
         Make a key that is used for counter values.
 
@@ -142,7 +160,7 @@ class RedisTSDB(BaseTSDB):
             model=model.value,
             epoch=self.normalize_to_rollup(timestamp, rollup),
             vnode=vnode,
-        ), model_key
+        ), self.add_environment_parameter(model_key, environment_id)
 
     def get_model_key(self, key):
         # We specialize integers so that a pure int-map can be optimized by
@@ -155,10 +173,10 @@ class RedisTSDB(BaseTSDB):
             return md5(repr(key)).hexdigest()
         return key
 
-    def incr(self, model, key, timestamp=None, count=1):
-        self.incr_multi([(model, key)], timestamp, count)
+    def incr(self, model, key, timestamp=None, count=1, environment_id=None):
+        self.incr_multi([(model, key)], timestamp, count, environment_id)
 
-    def incr_multi(self, items, timestamp=None, count=1):
+    def incr_multi(self, items, timestamp=None, count=1, environment_id=None):
         """
         Increment project ID=1 and group ID=5:
 
@@ -167,17 +185,20 @@ class RedisTSDB(BaseTSDB):
         if timestamp is None:
             timestamp = timezone.now()
 
-        with self.cluster.map() as client:
-            for rollup, max_values in six.iteritems(self.rollups):
-                for model, key in items:
-                    hash_key, hash_field = self.make_counter_key(model, rollup, timestamp, key)
-                    client.hincrby(hash_key, hash_field, count)
-                    client.expireat(
-                        hash_key,
-                        self.calculate_expiry(rollup, max_values, timestamp),
-                    )
+        for cluster, environment_ids in self.get_cluster_groups(set([None, environment_id])):
+            with cluster.map() as client:
+                for rollup, max_values in six.iteritems(self.rollups):
+                    for model, key in items:
+                        for environment_id in environment_ids:
+                            hash_key, hash_field = self.make_counter_key(
+                                model, rollup, timestamp, key, environment_id)
+                            client.hincrby(hash_key, hash_field, count)
+                            client.expireat(
+                                hash_key,
+                                self.calculate_expiry(rollup, max_values, timestamp),
+                            )
 
-    def get_range(self, model, keys, start, end, rollup=None):
+    def get_range(self, model, keys, start, end, rollup=None, environment_id=None):
         """
         To get a range of data for group ID=[1, 2, 3]:
 
@@ -190,10 +211,11 @@ class RedisTSDB(BaseTSDB):
         series = map(to_datetime, series)
 
         results = []
-        with self.cluster.map() as client:
+        with self.get_cluster(environment_id).map() as client:
             for key in keys:
                 for timestamp in series:
-                    hash_key, hash_field = self.make_counter_key(model, rollup, timestamp, key)
+                    hash_key, hash_field = self.make_counter_key(
+                        model, rollup, timestamp, key, environment_id)
                     results.append(
                         (to_timestamp(timestamp), key, client.hget(
                             hash_key, hash_field)))
@@ -206,74 +228,91 @@ class RedisTSDB(BaseTSDB):
             results_by_key[key] = sorted(points.items())
         return dict(results_by_key)
 
-    def merge(self, model, destination, sources, timestamp=None):
-        rollups = self.get_active_series(timestamp=timestamp)
+    def merge(self, model, destination, sources, timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-        with self.cluster.map() as client:
-            data = {}
-            for rollup, series in rollups.items():
-                data[rollup] = {}
-                for timestamp in series:
-                    results = data[rollup][timestamp] = []
-                    for source in sources:
-                        source_hash_key, source_hash_field = self.make_counter_key(
-                            model,
-                            rollup,
-                            timestamp,
-                            source,
-                        )
-                        results.append(client.hget(source_hash_key, source_hash_field))
-                        client.hdel(source_hash_key, source_hash_field)
-
-        with self.cluster.map() as client:
-            for rollup, series in data.items():
-                for timestamp, results in series.items():
-                    total = sum(int(result.value or 0) for result in results)
-                    if total:
-                        destination_hash_key, destination_hash_field = self.make_counter_key(
-                            model,
-                            rollup,
-                            timestamp,
-                            destination,
-                        )
-                        client.hincrby(
-                            destination_hash_key,
-                            destination_hash_field,
-                            total,
-                        )
-                        client.expireat(
-                            destination_hash_key,
-                            self.calculate_expiry(
-                                rollup,
-                                self.rollups[rollup],
-                                timestamp,
-                            ),
-                        )
+        rollups = self.get_active_series(timestamp=timestamp)
 
-    def delete(self, models, keys, start=None, end=None, timestamp=None):
-        rollups = self.get_active_series(start, end, timestamp)
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            with cluster.map() as client:
+                data = {}
+                for rollup, series in rollups.items():
+                    data[rollup] = {}
+                    for timestamp in series:
+                        results = data[rollup][timestamp] = defaultdict(list)
+                        for source in sources:
+                            for environment_id in environment_ids:
+                                source_hash_key, source_hash_field = self.make_counter_key(
+                                    model,
+                                    rollup,
+                                    timestamp,
+                                    source,
+                                    environment_id,
+                                )
+                                results[environment_id].append(
+                                    client.hget(source_hash_key, source_hash_field))
+                                client.hdel(source_hash_key, source_hash_field)
 
-        with self.cluster.map() as client:
-            for rollup, series in rollups.items():
-                for timestamp in series:
-                    for model in models:
-                        for key in keys:
-                            hash_key, hash_field = self.make_counter_key(
-                                model,
-                                rollup,
-                                timestamp,
-                                key,
-                            )
+            with cluster.map() as client:
+                for rollup, series in data.items():
+                    for timestamp, results in series.items():
+                        for environment_id, promises in results.items():
+                            total = sum([int(p.value) for p in promises if p.value])
+                            if total:
+                                destination_hash_key, destination_hash_field = self.make_counter_key(
+                                    model,
+                                    rollup,
+                                    timestamp,
+                                    destination,
+                                    environment_id,
+                                )
+                                client.hincrby(
+                                    destination_hash_key,
+                                    destination_hash_field,
+                                    total,
+                                )
+                                client.expireat(
+                                    destination_hash_key,
+                                    self.calculate_expiry(
+                                        rollup,
+                                        self.rollups[rollup],
+                                        timestamp,
+                                    ),
+                                )
 
-                            client.hdel(
-                                hash_key,
-                                hash_field,
-                            )
+    def delete(self, models, keys, start=None, end=None, timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-    def record(self, model, key, values, timestamp=None):
-        self.record_multi(((model, key, values), ), timestamp)
+        rollups = self.get_active_series(start, end, timestamp)
 
-    def record_multi(self, items, timestamp=None):
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            with cluster.map() as client:
+                for rollup, series in rollups.items():
+                    for timestamp in series:
+                        for model in models:
+                            for key in keys:
+                                for environment_id in environment_ids:
+                                    hash_key, hash_field = self.make_counter_key(
+                                        model,
+                                        rollup,
+                                        timestamp,
+                                        key,
+                                        environment_id,
+                                    )
+
+                                    client.hdel(
+                                        hash_key,
+                                        hash_field,
+                                    )
+
+    def record(self, model, key, values, timestamp=None, environment_id=None):
+        self.record_multi(((model, key, values), ), timestamp, environment_id)
+
+    def record_multi(self, items, timestamp=None, environment_id=None):
         """
         Record an occurence of an item in a distinct counter.
         """
@@ -282,34 +321,38 @@ class RedisTSDB(BaseTSDB):
 
         ts = int(to_timestamp(timestamp))  # ``timestamp`` is not actually a timestamp :(
 
-        with self.cluster.fanout() as client:
-            for model, key, values in items:
-                c = client.target_key(key)
-                for rollup, max_values in six.iteritems(self.rollups):
-                    k = self.make_key(
-                        model,
-                        rollup,
-                        ts,
-                        key,
-                    )
-                    c.pfadd(k, *values)
-                    c.expireat(
-                        k,
-                        self.calculate_expiry(
-                            rollup,
-                            max_values,
-                            timestamp,
-                        ),
-                    )
+        for cluster, environment_ids in self.get_cluster_groups(set([None, environment_id])):
+            with cluster.fanout() as client:
+                for model, key, values in items:
+                    c = client.target_key(key)
+                    for rollup, max_values in six.iteritems(self.rollups):
+                        for environment_id in environment_ids:
+                            k = self.make_key(
+                                model,
+                                rollup,
+                                ts,
+                                key,
+                                environment_id,
+                            )
+                            c.pfadd(k, *values)
+                            c.expireat(
+                                k,
+                                self.calculate_expiry(
+                                    rollup,
+                                    max_values,
+                                    timestamp,
+                                ),
+                            )
 
-    def get_distinct_counts_series(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_series(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         """
         Fetch counts of distinct items for each rollup interval within the range.
         """
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         responses = {}
-        with self.cluster.fanout() as client:
+        with self.get_cluster(environment_id).fanout() as client:
             for key in keys:
                 c = client.target_key(key)
                 r = responses[key] = []
@@ -321,6 +364,7 @@ class RedisTSDB(BaseTSDB):
                                 rollup,
                                 timestamp,
                                 key,
+                                environment_id,
                             ),
                         ), )
                     )
@@ -330,14 +374,15 @@ class RedisTSDB(BaseTSDB):
             for key, value in six.iteritems(responses)
         }
 
-    def get_distinct_counts_totals(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_totals(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
         """
         Count distinct items during a time range.
         """
         rollup, series = self.get_optimal_rollup_series(start, end, rollup)
 
         responses = {}
-        with self.cluster.fanout() as client:
+        with self.get_cluster(environment_id).fanout() as client:
             for key in keys:
                 # XXX: The current versions of the Redis driver don't implement
                 # ``PFCOUNT`` correctly (although this is fixed in the Git
@@ -347,13 +392,14 @@ class RedisTSDB(BaseTSDB):
                 # directly here instead.
                 ks = []
                 for timestamp in series:
-                    ks.append(self.make_key(model, rollup, timestamp, key))
+                    ks.append(self.make_key(model, rollup, timestamp, key, environment_id))
 
                 responses[key] = client.target_key(key).execute_command('PFCOUNT', *ks)
 
         return {key: value.value for key, value in six.iteritems(responses)}
 
-    def get_distinct_counts_union(self, model, keys, start, end=None, rollup=None):
+    def get_distinct_counts_union(self, model, keys, start, end=None,
+                                  rollup=None, environment_id=None):
         if not keys:
             return 0
 
@@ -368,9 +414,11 @@ class RedisTSDB(BaseTSDB):
             """
             Return a list containing all keys for each interval in the series for a key.
             """
-            return [self.make_key(model, rollup, timestamp, key) for timestamp in series]
+            return [self.make_key(model, rollup, timestamp, key, environment_id)
+                    for timestamp in series]
 
-        router = self.cluster.get_router()
+        cluster = self.get_cluster(environment_id)
+        router = cluster.get_router()
 
         def map_key_to_host(hosts, key):
             """
@@ -386,7 +434,7 @@ class RedisTSDB(BaseTSDB):
             """
             (host, keys) = value
             destination = make_temporary_key('p:{}'.format(host))
-            client = self.cluster.get_local_client(host)
+            client = cluster.get_local_client(host)
             with client.pipeline(transaction=False) as pipeline:
                 pipeline.execute_command(
                     'PFMERGE', destination, *itertools.chain.from_iterable(map(expand_key, keys))
@@ -406,7 +454,7 @@ class RedisTSDB(BaseTSDB):
             # here that we've already accessed as part of this process -- this
             # way, we constrain the choices to only hosts that we know are
             # running.)
-            client = self.cluster.get_local_client(random.choice(values)[0])
+            client = cluster.get_local_client(random.choice(values)[0])
             with client.pipeline(transaction=False) as pipeline:
                 pipeline.mset(aggregates)
                 pipeline.execute_command('PFMERGE', destination, *aggregates.keys())
@@ -431,90 +479,109 @@ class RedisTSDB(BaseTSDB):
             ]
         )
 
-    def merge_distinct_counts(self, model, destination, sources, timestamp=None):
+    def merge_distinct_counts(self, model, destination, sources,
+                              timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
+
         rollups = self.get_active_series(timestamp=timestamp)
 
-        temporary_id = uuid.uuid1().hex
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            temporary_id = uuid.uuid1().hex
 
-        def make_temporary_key(key):
-            return '{}{}:{}'.format(self.prefix, temporary_id, key)
+            def make_temporary_key(key):
+                return '{}{}:{}'.format(self.prefix, temporary_id, key)
+
+            data = {}
+            for rollup, series in rollups.items():
+                data[rollup] = {timestamp: {e: [] for e in environment_ids} for timestamp in series}
+
+            with cluster.fanout() as client:
+                for source in sources:
+                    c = client.target_key(source)
+                    for rollup, series in data.items():
+                        for timestamp, results in series.items():
+                            for environment_id in environment_ids:
+                                key = self.make_key(
+                                    model,
+                                    rollup,
+                                    to_timestamp(timestamp),
+                                    source,
+                                    environment_id,
+                                )
+                                results[environment_id].append(c.get(key))
+                                c.delete(key)
 
-        data = {}
-        for rollup, series in rollups.items():
-            data[rollup] = {timestamp: [] for timestamp in series}
+            with cluster.fanout() as client:
+                c = client.target_key(destination)
+
+                temporary_key_sequence = itertools.count()
 
-        with self.cluster.fanout() as client:
-            for source in sources:
-                c = client.target_key(source)
                 for rollup, series in data.items():
                     for timestamp, results in series.items():
-                        key = self.make_key(
-                            model,
-                            rollup,
-                            to_timestamp(timestamp),
-                            source,
-                        )
-                        results.append(c.get(key))
-                        c.delete(key)
-
-        with self.cluster.fanout() as client:
-            c = client.target_key(destination)
-
-            temporary_key_sequence = itertools.count()
-
-            for rollup, series in data.items():
-                for timestamp, results in series.items():
-                    values = {}
-                    for result in results:
-                        if result.value is None:
-                            continue
-                        k = make_temporary_key(next(temporary_key_sequence))
-                        values[k] = result.value
-
-                    if values:
-                        key = self.make_key(
-                            model,
-                            rollup,
-                            to_timestamp(timestamp),
-                            destination,
-                        )
-                        c.mset(values)
-                        c.pfmerge(key, key, *values.keys())
-                        c.delete(*values.keys())
-                        c.expireat(
-                            key,
-                            self.calculate_expiry(
-                                rollup,
-                                self.rollups[rollup],
-                                timestamp,
-                            ),
-                        )
-
-    def delete_distinct_counts(self, models, keys, start=None, end=None, timestamp=None):
-        rollups = self.get_active_series(start, end, timestamp)
-
-        with self.cluster.fanout() as client:
-            for rollup, series in rollups.items():
-                for timestamp in series:
-                    for model in models:
-                        for key in keys:
-                            client.target_key(key).delete(
-                                self.make_key(
+                        for environment_id, promises in results.items():
+                            values = {}
+                            for promise in promises:
+                                if promise.value is None:
+                                    continue
+                                k = make_temporary_key(next(temporary_key_sequence))
+                                values[k] = promise.value
+
+                            if values:
+                                key = self.make_key(
                                     model,
                                     rollup,
                                     to_timestamp(timestamp),
+                                    destination,
+                                    environment_id,
+                                )
+                                c.mset(values)
+                                c.pfmerge(key, key, *values.keys())
+                                c.delete(*values.keys())
+                                c.expireat(
                                     key,
+                                    self.calculate_expiry(
+                                        rollup,
+                                        self.rollups[rollup],
+                                        timestamp,
+                                    ),
                                 )
-                            )
 
-    def make_frequency_table_keys(self, model, rollup, timestamp, key):
-        prefix = self.make_key(model, rollup, timestamp, key)
+    def delete_distinct_counts(self, models, keys, start=None, end=None,
+                               timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
+
+        rollups = self.get_active_series(start, end, timestamp)
+
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            with cluster.fanout() as client:
+                for rollup, series in rollups.items():
+                    for timestamp in series:
+                        for model in models:
+                            for key in keys:
+                                c = client.target_key(key)
+                                for environment_id in environment_ids:
+                                    c.delete(
+                                        self.make_key(
+                                            model,
+                                            rollup,
+                                            to_timestamp(timestamp),
+                                            key,
+                                            environment_id,
+                                        )
+                                    )
+
+    def make_frequency_table_keys(self, model, rollup, timestamp, key, environment_id):
+        prefix = self.make_key(model, rollup, timestamp, key, environment_id)
         return map(
             operator.methodcaller('format', prefix),
             ('{}:i', '{}:e'),
         )
 
-    def record_frequency_multi(self, requests, timestamp=None):
+    def record_frequency_multi(self, requests, timestamp=None, environment_id=None):
         if not self.enable_frequency_sketches:
             return
 
@@ -523,37 +590,41 @@ class RedisTSDB(BaseTSDB):
 
         ts = int(to_timestamp(timestamp))  # ``timestamp`` is not actually a timestamp :(
 
-        commands = {}
-
-        for model, request in requests:
-            for key, items in six.iteritems(request):
-                keys = []
-                expirations = {}
-
-                # Figure out all of the keys we need to be incrementing, as
-                # well as their expiration policies.
-                for rollup, max_values in six.iteritems(self.rollups):
-                    chunk = self.make_frequency_table_keys(model, rollup, ts, key)
-                    keys.extend(chunk)
-
-                    expiry = self.calculate_expiry(rollup, max_values, timestamp)
-                    for k in chunk:
-                        expirations[k] = expiry
-
-                arguments = ['INCR'] + list(self.DEFAULT_SKETCH_PARAMETERS)
-                for member, score in items.items():
-                    arguments.extend((score, member))
-
-                # Since we're essentially merging dictionaries, we need to
-                # append this to any value that already exists at the key.
-                cmds = commands.setdefault(key, [])
-                cmds.append((CountMinScript, keys, arguments))
-                for k, t in expirations.items():
-                    cmds.append(('EXPIREAT', k, t))
-
-        self.cluster.execute_commands(commands)
-
-    def get_most_frequent(self, model, keys, start, end=None, rollup=None, limit=None):
+        for cluster, environment_ids in self.get_cluster_groups(set([None, environment_id])):
+            commands = {}
+
+            for model, request in requests:
+                for key, items in six.iteritems(request):
+                    keys = []
+                    expirations = {}
+
+                    # Figure out all of the keys we need to be incrementing, as
+                    # well as their expiration policies.
+                    for rollup, max_values in six.iteritems(self.rollups):
+                        for environment_id in environment_ids:
+                            chunk = self.make_frequency_table_keys(
+                                model, rollup, ts, key, environment_id)
+                            keys.extend(chunk)
+
+                        expiry = self.calculate_expiry(rollup, max_values, timestamp)
+                        for k in chunk:
+                            expirations[k] = expiry
+
+                    arguments = ['INCR'] + list(self.DEFAULT_SKETCH_PARAMETERS)
+                    for member, score in items.items():
+                        arguments.extend((score, member))
+
+                    # Since we're essentially merging dictionaries, we need to
+                    # append this to any value that already exists at the key.
+                    cmds = commands.setdefault(key, [])
+                    cmds.append((CountMinScript, keys, arguments))
+                    for k, t in expirations.items():
+                        cmds.append(('EXPIREAT', k, t))
+
+            cluster.execute_commands(commands)
+
+    def get_most_frequent(self, model, keys, start, end=None,
+                          rollup=None, limit=None, environment_id=None):
         if not self.enable_frequency_sketches:
             raise NotImplementedError("Frequency sketches are disabled.")
 
@@ -567,16 +638,23 @@ class RedisTSDB(BaseTSDB):
         for key in keys:
             ks = []
             for timestamp in series:
-                ks.extend(self.make_frequency_table_keys(model, rollup, timestamp, key))
+                ks.extend(
+                    self.make_frequency_table_keys(
+                        model,
+                        rollup,
+                        timestamp,
+                        key,
+                        environment_id))
             commands[key] = [(CountMinScript, ks, arguments)]
 
         results = {}
-        for key, responses in self.cluster.execute_commands(commands).items():
+        for key, responses in self.get_cluster(environment_id).execute_commands(commands).items():
             results[key] = [(member, float(score)) for member, score in responses[0].value]
 
         return results
 
-    def get_most_frequent_series(self, model, keys, start, end=None, rollup=None, limit=None):
+    def get_most_frequent_series(self, model, keys, start, end=None,
+                                 rollup=None, limit=None, environment_id=None):
         if not self.enable_frequency_sketches:
             raise NotImplementedError("Frequency sketches are disabled.")
 
@@ -590,7 +668,8 @@ class RedisTSDB(BaseTSDB):
         for key in keys:
             commands[key] = [
                 (
-                    CountMinScript, self.make_frequency_table_keys(model, rollup, timestamp, key),
+                    CountMinScript, self.make_frequency_table_keys(
+                        model, rollup, timestamp, key, environment_id),
                     arguments,
                 ) for timestamp in series
             ]
@@ -599,12 +678,12 @@ class RedisTSDB(BaseTSDB):
             return {item: float(score) for item, score in response.value}
 
         results = {}
-        for key, responses in self.cluster.execute_commands(commands).items():
+        for key, responses in self.get_cluster(environment_id).execute_commands(commands).items():
             results[key] = zip(series, map(unpack_response, responses))
 
         return results
 
-    def get_frequency_series(self, model, items, start, end=None, rollup=None):
+    def get_frequency_series(self, model, items, start, end=None, rollup=None, environment_id=None):
         if not self.enable_frequency_sketches:
             raise NotImplementedError("Frequency sketches are disabled.")
 
@@ -623,13 +702,19 @@ class RedisTSDB(BaseTSDB):
         for key, members in items.items():
             ks = []
             for timestamp in series:
-                ks.extend(self.make_frequency_table_keys(model, rollup, timestamp, key))
+                ks.extend(
+                    self.make_frequency_table_keys(
+                        model,
+                        rollup,
+                        timestamp,
+                        key,
+                        environment_id))
 
             commands[key] = [(CountMinScript, ks, arguments + members)]
 
         results = {}
 
-        for key, responses in self.cluster.execute_commands(commands).items():
+        for key, responses in self.get_cluster(environment_id).execute_commands(commands).items():
             members = items[key]
 
             chunk = results[key] = []
@@ -638,14 +723,14 @@ class RedisTSDB(BaseTSDB):
 
         return results
 
-    def get_frequency_totals(self, model, items, start, end=None, rollup=None):
+    def get_frequency_totals(self, model, items, start, end=None, rollup=None, environment_id=None):
         if not self.enable_frequency_sketches:
             raise NotImplementedError("Frequency sketches are disabled.")
 
         responses = {}
 
         for key, series in six.iteritems(
-            self.get_frequency_series(model, items, start, end, rollup)
+            self.get_frequency_series(model, items, start, end, rollup, environment_id)
         ):
             response = responses[key] = {}
             for timestamp, results in series:
@@ -654,7 +739,11 @@ class RedisTSDB(BaseTSDB):
 
         return responses
 
-    def merge_frequencies(self, model, destination, sources, timestamp=None):
+    def merge_frequencies(self, model, destination, sources, timestamp=None, environment_ids=None):
+        environment_ids = list(
+            (set(environment_ids) if environment_ids is not None else set()).union(
+                [None]))
+
         if not self.enable_frequency_sketches:
             return
 
@@ -667,59 +756,74 @@ class RedisTSDB(BaseTSDB):
             )
             rollups.append((rollup, map(to_datetime, series), ))
 
-        exports = defaultdict(list)
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            exports = defaultdict(list)
 
-        for source in sources:
-            for rollup, series in rollups:
-                for timestamp in series:
-                    keys = self.make_frequency_table_keys(
-                        model,
-                        rollup,
-                        to_timestamp(timestamp),
-                        source,
-                    )
-                    arguments = ['EXPORT'] + list(self.DEFAULT_SKETCH_PARAMETERS)
-                    exports[source].extend(
-                        [
-                            (CountMinScript, keys, arguments),
-                            ('DEL', ) + tuple(keys),
-                        ]
-                    )
+            for source in sources:
+                for rollup, series in rollups:
+                    for timestamp in series:
+                        keys = []
+                        for environment_id in environment_ids:
+                            keys.extend(
+                                self.make_frequency_table_keys(
+                                    model,
+                                    rollup,
+                                    to_timestamp(timestamp),
+                                    source,
+                                    environment_id,
+                                )
+                            )
+                        arguments = ['EXPORT'] + list(self.DEFAULT_SKETCH_PARAMETERS)
+                        exports[source].extend(
+                            [
+                                (CountMinScript, keys, arguments),
+                                ['DEL'] + keys,
+                            ]
+                        )
 
-        imports = []
+            imports = []
+
+            for source, results in cluster.execute_commands(exports).items():
+                results = iter(results)
+                for rollup, series in rollups:
+                    for timestamp in series:
+                        for environment_id, payload in zip(environment_ids, next(results).value):
+                            imports.append(
+                                (
+                                    CountMinScript,
+                                    self.make_frequency_table_keys(
+                                        model,
+                                        rollup,
+                                        to_timestamp(timestamp),
+                                        destination,
+                                        environment_id,
+                                    ),
+                                    ['IMPORT'] + list(self.DEFAULT_SKETCH_PARAMETERS) + [payload],
+                                ),
+                            )
+                        next(results)  # pop off the result of DEL
 
-        for source, results in self.cluster.execute_commands(exports).items():
-            results = iter(results)
-            for rollup, series in rollups:
-                for timestamp in series:
-                    imports.append(
-                        (
-                            CountMinScript, self.make_frequency_table_keys(
-                                model,
-                                rollup,
-                                to_timestamp(timestamp),
-                                destination,
-                            ),
-                            ['IMPORT'] + list(self.DEFAULT_SKETCH_PARAMETERS) +
-                            next(results).value,
-                        )
-                    )
-                    next(results)  # pop off the result of DEL
+            cluster.execute_commands({
+                destination: imports,
+            })
 
-        self.cluster.execute_commands({
-            destination: imports,
-        })
+    def delete_frequencies(self, models, keys, start=None, end=None,
+                           timestamp=None, environment_ids=None):
+        environment_ids = (
+            set(environment_ids) if environment_ids is not None else set()).union(
+            [None])
 
-    def delete_frequencies(self, models, keys, start=None, end=None, timestamp=None):
         rollups = self.get_active_series(start, end, timestamp)
 
-        with self.cluster.fanout() as client:
-            for rollup, series in rollups.items():
-                for timestamp in series:
-                    for model in models:
-                        for key in keys:
-                            c = client.target_key(key)
-                            for k in self.make_frequency_table_keys(
-                                model, rollup, to_timestamp(timestamp), key
-                            ):
-                                c.delete(k)
+        for cluster, environment_ids in self.get_cluster_groups(environment_ids):
+            with cluster.fanout() as client:
+                for rollup, series in rollups.items():
+                    for timestamp in series:
+                        for model in models:
+                            for key in keys:
+                                c = client.target_key(key)
+                                for environment_id in environment_ids:
+                                    for k in self.make_frequency_table_keys(
+                                        model, rollup, to_timestamp(timestamp), key, environment_id
+                                    ):
+                                        c.delete(k)
diff --git a/tests/sentry/tsdb/test_redis.py b/tests/sentry/tsdb/test_redis.py
index cff8193d60..111b848a97 100644
--- a/tests/sentry/tsdb/test_redis.py
+++ b/tests/sentry/tsdb/test_redis.py
@@ -35,12 +35,19 @@ class RedisTSDBTest(TestCase):
             client.flushdb()
 
     def test_make_counter_key(self):
-        result = self.db.make_counter_key(TSDBModel.project, 1, to_datetime(1368889980), 1)
+        result = self.db.make_counter_key(TSDBModel.project, 1, to_datetime(1368889980), 1, None)
         assert result == ('ts:1:1368889980:1', 1)
 
-        result = self.db.make_counter_key(TSDBModel.project, 1, to_datetime(1368889980), 'foo')
+        result = self.db.make_counter_key(
+            TSDBModel.project, 1, to_datetime(1368889980), 'foo', None)
         assert result == ('ts:1:1368889980:46', self.db.get_model_key('foo'))
 
+        result = self.db.make_counter_key(TSDBModel.project, 1, to_datetime(1368889980), 1, 1)
+        assert result == ('ts:1:1368889980:1', '1?e=1')
+
+        result = self.db.make_counter_key(TSDBModel.project, 1, to_datetime(1368889980), 'foo', 1)
+        assert result == ('ts:1:1368889980:46', self.db.get_model_key('foo') + '?e=1')
+
     def test_get_model_key(self):
         result = self.db.get_model_key(1)
         assert result == 1
@@ -57,13 +64,20 @@ class RedisTSDBTest(TestCase):
             return t - (t % 3600)
 
         self.db.incr(TSDBModel.project, 1, dts[0])
-        self.db.incr(TSDBModel.project, 1, dts[1], count=3)
+        self.db.incr(TSDBModel.project, 1, dts[1], count=2)
+        self.db.incr(TSDBModel.project, 1, dts[1], environment_id=1)
         self.db.incr(TSDBModel.project, 1, dts[2])
         self.db.incr_multi(
             [
                 (TSDBModel.project, 1),
                 (TSDBModel.project, 2),
-            ], dts[3], count=4
+            ], dts[3], count=3, environment_id=1
+        )
+        self.db.incr_multi(
+            [
+                (TSDBModel.project, 1),
+                (TSDBModel.project, 2),
+            ], dts[3], count=1, environment_id=2
         )
 
         results = self.db.get_range(TSDBModel.project, [1], dts[0], dts[-1])
@@ -75,6 +89,7 @@ class RedisTSDBTest(TestCase):
                 (timestamp(dts[3]), 4),
             ],
         }
+
         results = self.db.get_range(TSDBModel.project, [2], dts[0], dts[-1])
         assert results == {
             2: [
@@ -85,13 +100,41 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        results = self.db.get_range(TSDBModel.project, [1, 2], dts[0], dts[-1], environment_id=1)
+        assert results == {
+            1: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 1),
+                (timestamp(dts[2]), 0),
+                (timestamp(dts[3]), 3),
+            ],
+            2: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 0),
+                (timestamp(dts[2]), 0),
+                (timestamp(dts[3]), 3),
+            ],
+        }
+
         results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1])
         assert results == {
             1: 9,
             2: 4,
         }
 
-        self.db.merge(TSDBModel.project, 1, [2], now)
+        results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1], environment_id=1)
+        assert results == {
+            1: 4,
+            2: 3,
+        }
+
+        results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1], environment_id=0)
+        assert results == {
+            1: 0,
+            2: 0,
+        }
+
+        self.db.merge(TSDBModel.project, 1, [2], now, environment_ids=[0, 1, 2])
 
         results = self.db.get_range(TSDBModel.project, [1], dts[0], dts[-1])
         assert results == {
@@ -102,6 +145,7 @@ class RedisTSDBTest(TestCase):
                 (timestamp(dts[3]), 8),
             ],
         }
+
         results = self.db.get_range(TSDBModel.project, [2], dts[0], dts[-1])
         assert results == {
             2: [
@@ -112,13 +156,24 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        results = self.db.get_range(TSDBModel.project, [1, 2], dts[0], dts[-1], environment_id=1)
+        assert results == {
+            1: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 1),
+                (timestamp(dts[2]), 0),
+                (timestamp(dts[3]), 6),
+            ],
+            2: [(timestamp(dts[i]), 0) for i in xrange(0, 4)],
+        }
+
         results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1])
         assert results == {
             1: 13,
             2: 0,
         }
 
-        self.db.delete([TSDBModel.project], [1, 2], dts[0], dts[-1])
+        self.db.delete([TSDBModel.project], [1, 2], dts[0], dts[-1], environment_ids=[0, 1, 2])
 
         results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1])
         assert results == {
@@ -126,6 +181,12 @@ class RedisTSDBTest(TestCase):
             2: 0,
         }
 
+        results = self.db.get_sums(TSDBModel.project, [1, 2], dts[0], dts[-1], environment_id=1)
+        assert results == {
+            1: 0,
+            2: 0,
+        }
+
     def test_count_distinct(self):
         now = datetime.utcnow().replace(tzinfo=pytz.UTC) - timedelta(hours=4)
         dts = [now + timedelta(hours=i) for i in range(4)]
@@ -148,10 +209,19 @@ class RedisTSDBTest(TestCase):
             1,
             ('baz', ),
             dts[1],
+            environment_id=1,
         )
 
         self.db.record_multi(
-            ((model, 1, ('foo', 'bar', 'baz'), ), (model, 2, ('bar', ), ), ), dts[2]
+            ((model, 1, ('foo', 'bar'), ), (model, 2, ('bar', ), ), ), dts[2]
+        )
+
+        self.db.record(
+            model,
+            1,
+            ('baz', ),
+            dts[2],
+            environment_id=1,
         )
 
         self.db.record(
@@ -183,16 +253,51 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        assert self.db.get_distinct_counts_series(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=1,
+        ) == {
+            1: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 1),
+                (timestamp(dts[2]), 1),
+                (timestamp(dts[3]), 0),
+            ],
+            2: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 0),
+                (timestamp(dts[2]), 0),
+                (timestamp(dts[3]), 0),
+            ],
+        }
+
         results = self.db.get_distinct_counts_totals(model, [1, 2], dts[0], dts[-1], rollup=3600)
         assert results == {
             1: 3,
             2: 2,
         }
 
+        results = self.db.get_distinct_counts_totals(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=1)
+        assert results == {
+            1: 1,
+            2: 0,
+        }
+
+        results = self.db.get_distinct_counts_totals(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=0)
+        assert results == {
+            1: 0,
+            2: 0,
+        }
+
         assert self.db.get_distinct_counts_union(model, [], dts[0], dts[-1], rollup=3600) == 0
         assert self.db.get_distinct_counts_union(model, [1, 2], dts[0], dts[-1], rollup=3600) == 3
+        assert self.db.get_distinct_counts_union(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=1) == 1
+        assert self.db.get_distinct_counts_union(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=0) == 0
 
-        self.db.merge_distinct_counts(model, 1, [2], dts[0])
+        self.db.merge_distinct_counts(model, 1, [2], dts[0], environment_ids=[0, 1])
 
         assert self.db.get_distinct_counts_series(
             model, [1], dts[0], dts[-1], rollup=3600
@@ -216,6 +321,23 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        assert self.db.get_distinct_counts_series(
+            model, [1, 2], dts[0], dts[-1], rollup=3600, environment_id=1,
+        ) == {
+            1: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 1),
+                (timestamp(dts[2]), 1),
+                (timestamp(dts[3]), 0),
+            ],
+            2: [
+                (timestamp(dts[0]), 0),
+                (timestamp(dts[1]), 0),
+                (timestamp(dts[2]), 0),
+                (timestamp(dts[3]), 0),
+            ],
+        }
+
         results = self.db.get_distinct_counts_totals(model, [1, 2], dts[0], dts[-1], rollup=3600)
         assert results == {
             1: 3,
@@ -227,7 +349,7 @@ class RedisTSDBTest(TestCase):
         assert self.db.get_distinct_counts_union(model, [1, 2], dts[0], dts[-1], rollup=3600) == 3
         assert self.db.get_distinct_counts_union(model, [2], dts[0], dts[-1], rollup=3600) == 0
 
-        self.db.delete_distinct_counts([model], [1, 2], dts[0], dts[-1])
+        self.db.delete_distinct_counts([model], [1, 2], dts[0], dts[-1], environment_ids=[0, 1])
 
         results = self.db.get_distinct_counts_totals(model, [1, 2], dts[0], dts[-1])
         assert results == {
@@ -235,6 +357,13 @@ class RedisTSDBTest(TestCase):
             2: 0,
         }
 
+        results = self.db.get_distinct_counts_totals(
+            model, [1, 2], dts[0], dts[-1], environment_id=1)
+        assert results == {
+            1: 0,
+            2: 0,
+        }
+
     def test_frequency_tables(self):
         now = datetime.utcnow().replace(tzinfo=pytz.UTC)
         model = TSDBModel.frequent_projects_by_organization
@@ -257,17 +386,36 @@ class RedisTSDBTest(TestCase):
                     model, {
                         'organization:1': {
                             "project:1": 1,
-                            "project:2": 2,
-                            "project:3": 3,
-                            "project:4": 4,
+                            "project:2": 1,
+                            "project:3": 1,
+                            "project:4": 1,
+                        },
+                        "organization:2": {
+                            "project:5": 1,
+                        },
+                    }
+                ),
+            ),
+            now - timedelta(hours=1),
+        )
+
+        self.db.record_frequency_multi(
+            (
+                (
+                    model, {
+                        'organization:1': {
+                            "project:2": 1,
+                            "project:3": 2,
+                            "project:4": 3,
                         },
                         "organization:2": {
-                            "project:5": 1.5,
+                            "project:5": 0.5,
                         },
                     }
                 ),
             ),
             now - timedelta(hours=1),
+            environment_id=1,
         )
 
         assert self.db.get_most_frequent(
@@ -284,6 +432,24 @@ class RedisTSDBTest(TestCase):
             'organization:2': [],
         }
 
+        assert self.db.get_most_frequent(
+            model,
+            ('organization:1', 'organization:2'),
+            now - timedelta(hours=1),
+            now,
+            rollup=rollup,
+            environment_id=1,
+        ) == {
+            'organization:1': [
+                ('project:4', 3.0),
+                ('project:3', 2.0),
+                ('project:2', 1.0),
+            ],
+            'organization:2': [
+                ('project:5', 0.5),
+            ],
+        }
+
         assert self.db.get_most_frequent(
             model,
             ('organization:1', 'organization:2'),
@@ -315,6 +481,20 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        assert self.db.get_most_frequent(
+            model,
+            ('organization:1', 'organization:2'),
+            now - timedelta(hours=1),
+            now,
+            rollup=rollup,
+            environment_id=0,
+        ) == {
+            'organization:1': [
+            ],
+            'organization:2': [
+            ],
+        }
+
         timestamp = int(to_timestamp(now) // rollup) * rollup
 
         assert self.db.get_most_frequent_series(
@@ -389,6 +569,45 @@ class RedisTSDBTest(TestCase):
             ],
         }
 
+        assert self.db.get_frequency_series(
+            model,
+            {
+                'organization:1': ("project:1", "project:2", "project:3", "project:4"),
+                'organization:2': ("project:5", ),
+            },
+            now - timedelta(hours=1),
+            now,
+            rollup=rollup,
+            environment_id=1,
+        ) == {
+            'organization:1': [
+                (
+                    timestamp - rollup, {
+                        "project:1": 0.0,
+                        "project:2": 1.0,
+                        "project:3": 2.0,
+                        "project:4": 3.0,
+                    }
+                ),
+                (
+                    timestamp, {
+                        "project:1": 0.0,
+                        "project:2": 0.0,
+                        "project:3": 0.0,
+                        "project:4": 0.0,
+                    }
+                ),
+            ],
+            'organization:2': [
+                (timestamp - rollup, {
+                    "project:5": 0.5,
+                }),
+                (timestamp, {
+                    "project:5": 0.0,
+                }),
+            ],
+        }
+
         assert self.db.get_frequency_totals(
             model,
             {
@@ -420,6 +639,7 @@ class RedisTSDBTest(TestCase):
             'organization:1',
             ['organization:2'],
             now,
+            environment_ids=[0, 1],
         )
 
         assert self.db.get_frequency_totals(
@@ -448,11 +668,39 @@ class RedisTSDBTest(TestCase):
             },
         }
 
+        assert self.db.get_frequency_totals(
+            model,
+            {
+                'organization:1': ("project:1", "project:2", "project:3", "project:4", "project:5"),
+                'organization:2': ("project:1", "project:2", "project:3", "project:4", "project:5"),
+            },
+            now - timedelta(hours=1),
+            now,
+            rollup=rollup,
+            environment_id=1,
+        ) == {
+            'organization:1': {
+                "project:1": 0.0,
+                "project:2": 1.0,
+                "project:3": 2.0,
+                "project:4": 3.0,
+                "project:5": 0.5,
+            },
+            'organization:2': {
+                "project:1": 0.0,
+                "project:2": 0.0,
+                "project:3": 0.0,
+                "project:4": 0.0,
+                "project:5": 0.0,
+            },
+        }
+
         self.db.delete_frequencies(
             [model],
             ['organization:1', 'organization:2'],
             now - timedelta(hours=1),
             now,
+            environment_ids=[0, 1],
         )
 
         assert self.db.get_most_frequent(
@@ -464,6 +712,16 @@ class RedisTSDBTest(TestCase):
             'organization:2': [],
         }
 
+        assert self.db.get_most_frequent(
+            model,
+            ('organization:1', 'organization:2'),
+            now,
+            environment_id=1,
+        ) == {
+            'organization:1': [],
+            'organization:2': [],
+        }
+
     def test_frequency_table_import_export_no_estimators(self):
         client = self.db.cluster.get_local_client_for_key('key')
 
