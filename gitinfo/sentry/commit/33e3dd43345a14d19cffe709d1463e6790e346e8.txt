commit 33e3dd43345a14d19cffe709d1463e6790e346e8
Author: James Cunningham <cunninghamjt09@gmail.com>
Date:   Tue Sep 13 12:55:18 2016 -0700

    Bring merge tasks up to par with deletion logging. (#4129)
    
    * Bring merge tasks up to par with deletion logging.
    
    * Yea that didn't make sense.
    
    * Make tests pass.

diff --git a/src/sentry/api/endpoints/project_group_index.py b/src/sentry/api/endpoints/project_group_index.py
index c5932d05fd..58ec7eae6d 100644
--- a/src/sentry/api/endpoints/project_group_index.py
+++ b/src/sentry/api/endpoints/project_group_index.py
@@ -7,6 +7,7 @@ from django.db import IntegrityError, transaction
 from django.utils import timezone
 from rest_framework import serializers
 from rest_framework.response import Response
+from uuid import uuid4
 
 from sentry.app import search
 from sentry.api.base import DocSection
@@ -585,6 +586,7 @@ class ProjectGroupIndexEndpoint(ProjectEndpoint):
         if result.get('merge') and len(group_list) > 1:
             primary_group = sorted(group_list, key=lambda x: -x.times_seen)[0]
             children = []
+            transaction_id = uuid4().hex
             for group in group_list:
                 if group == primary_group:
                     continue
@@ -593,6 +595,7 @@ class ProjectGroupIndexEndpoint(ProjectEndpoint):
                 merge_group.delay(
                     from_object_id=group.id,
                     to_object_id=primary_group.id,
+                    transaction_id=transaction_id,
                 )
 
             Activity.objects.create(
diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index 9388e293d4..5c68ae776b 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -740,6 +740,7 @@ class EventManager(object):
                 merge_group.delay(
                     from_object_id=hash.group_id,
                     to_object_id=group.id,
+                    transaction_id=uuid4().hex,
                 )
 
         return GroupHash.objects.filter(
diff --git a/src/sentry/tasks/merge.py b/src/sentry/tasks/merge.py
index 6a7403b3c5..a78b87be4f 100644
--- a/src/sentry/tasks/merge.py
+++ b/src/sentry/tasks/merge.py
@@ -16,13 +16,14 @@ from django.db.models import F
 from sentry.tasks.base import instrumented_task, retry
 from sentry.tasks.deletion import delete_group
 
-logger = logging.getLogger('sentry.group.merge')
+logger = logging.getLogger('sentry.merge')
+delete_logger = logging.getLogger('sentry.deletions.async')
 
 
 @instrumented_task(name='sentry.tasks.merge.merge_group', queue='merge',
                    default_retry_delay=60 * 5, max_retries=None)
 @retry
-def merge_group(from_object_id=None, to_object_id=None, **kwargs):
+def merge_group(from_object_id=None, to_object_id=None, transaction_id=None, **kwargs):
     # TODO(mattrobenolt): Write tests for all of this
     from sentry.models import (
         Activity, Group, GroupAssignee, GroupHash, GroupRuleStatus,
@@ -31,13 +32,16 @@ def merge_group(from_object_id=None, to_object_id=None, **kwargs):
     )
 
     if not (from_object_id and to_object_id):
-        logger.error('merge_group.malformed.missing_params')
+        logger.error('group.malformed.missing_params', extra={
+            'transaction_id': transaction_id,
+        })
         return
 
     try:
         group = Group.objects.get(id=from_object_id)
     except Group.DoesNotExist:
-        logger.warn('merge_group.malformed.invalid_id', extra={
+        logger.warn('group.malformed.invalid_id', extra={
+            'transaction_id': transaction_id,
             'old_object_id': from_object_id,
         })
         return
@@ -45,29 +49,48 @@ def merge_group(from_object_id=None, to_object_id=None, **kwargs):
     try:
         new_group = Group.objects.get(id=to_object_id)
     except Group.DoesNotExist:
-        logger.warn('merge_group.malformed.invalid_id', extra={
+        logger.warn('group.malformed.invalid_id', extra={
+            'transaction_id': transaction_id,
             'old_object_id': from_object_id,
         })
         return
 
+    logger.info('merge.queued', extra={
+        'transaction_id': transaction_id,
+        'new_group_id': new_group.id,
+        'old_group_id': group.id,
+    })
+
     model_list = (
         Activity, GroupAssignee, GroupHash, GroupRuleStatus, GroupSubscription,
         GroupTagValue, GroupTagKey, EventMapping, Event, UserReport,
         GroupRedirect, GroupMeta,
     )
 
-    has_more = merge_objects(model_list, group, new_group, logger=logger)
+    has_more = merge_objects(
+        model_list,
+        group,
+        new_group,
+        logger=logger,
+        transaction_id=transaction_id,
+    )
 
     if has_more:
         merge_group.delay(
             from_object_id=from_object_id,
             to_object_id=to_object_id,
+            transaction_id=transaction_id,
         )
         return
 
     previous_group_id = group.id
 
     group.delete()
+    delete_logger.info('object.delete.executed', extra={
+        'object_id': previous_group_id,
+        'transaction_id': transaction_id,
+        'model': Group.__name__,
+    })
 
     try:
         with transaction.atomic():
@@ -96,7 +119,7 @@ def merge_group(from_object_id=None, to_object_id=None, **kwargs):
 @instrumented_task(name='sentry.tasks.merge.rehash_group_events', queue='merge',
                    default_retry_delay=60 * 5, max_retries=None)
 @retry
-def rehash_group_events(group_id, **kwargs):
+def rehash_group_events(group_id, transaction_id=None, **kwargs):
     from sentry.models import Group, GroupHash
 
     group = Group.objects.get(id=group_id)
@@ -105,15 +128,21 @@ def rehash_group_events(group_id, **kwargs):
     # This can cause the new groups to be created before we get to them, but
     # its a tradeoff we're willing to take
     GroupHash.objects.filter(group=group).delete()
-
     has_more = _rehash_group_events(group)
 
     if has_more:
         rehash_group_events.delay(
-            group_id=group.id
+            group_id=group.id,
+            transaction_id=transaction_id,
         )
         return
 
+    delete_logger.info('object.delete.bulk_executed', extra={
+        'group_id': group.id,
+        'transaction_id': transaction_id,
+        'model': GroupHash.__name__,
+    })
+
     delete_group.delay(group.id)
 
 
@@ -163,17 +192,11 @@ def _rehash_group_events(group, limit=100):
 
 
 def merge_objects(models, group, new_group, limit=1000,
-                  logger=None):
+                  logger=None, transaction_id=None):
     from sentry.models import GroupTagKey, GroupTagValue
 
     has_more = False
     for model in models:
-        if logger is not None:
-            logger.info('group.merge', extra={
-                'new_group_id': new_group.id,
-                'old_group_id': group.id,
-                'model': model.__name__,
-            })
         all_fields = model._meta.get_all_field_names()
         has_group = 'group' in all_fields
         if has_group:
@@ -220,7 +243,14 @@ def merge_objects(models, group, new_group, limit=1000,
                 except DataError:
                     # it's possible to hit an out of range value for counters
                     pass
+                obj_id = obj.id
                 obj.delete()
+                if logger is not None:
+                    delete_logger.debug('object.delete.executed', extra={
+                        'object_id': obj_id,
+                        'transaction_id': transaction_id,
+                        'model': model.__name__,
+                    })
             has_more = True
 
         if has_more:
diff --git a/tests/sentry/api/endpoints/test_project_group_index.py b/tests/sentry/api/endpoints/test_project_group_index.py
index 9c66a44de8..47636f57d6 100644
--- a/tests/sentry/api/endpoints/test_project_group_index.py
+++ b/tests/sentry/api/endpoints/test_project_group_index.py
@@ -641,8 +641,13 @@ class GroupUpdateTest(APITestCase):
         r4 = GroupSeen.objects.filter(group=group4, user=self.user)
         assert not r4.exists()
 
+    @patch('sentry.api.endpoints.project_group_index.uuid4')
     @patch('sentry.api.endpoints.project_group_index.merge_group')
-    def test_merge(self, merge_group):
+    def test_merge(self, merge_group, mock_uuid4):
+        class uuid(object):
+            hex = 'abc123'
+
+        mock_uuid4.return_value = uuid
         group1 = self.create_group(checksum='a' * 32, times_seen=1)
         group2 = self.create_group(checksum='b' * 32, times_seen=50)
         group3 = self.create_group(checksum='c' * 32, times_seen=2)
@@ -666,8 +671,16 @@ class GroupUpdateTest(APITestCase):
         ])
 
         assert len(merge_group.mock_calls) == 2
-        merge_group.delay.assert_any_call(from_object_id=group1.id, to_object_id=group2.id)
-        merge_group.delay.assert_any_call(from_object_id=group3.id, to_object_id=group2.id)
+        merge_group.delay.assert_any_call(
+            from_object_id=group1.id,
+            to_object_id=group2.id,
+            transaction_id='abc123',
+        )
+        merge_group.delay.assert_any_call(
+            from_object_id=group3.id,
+            to_object_id=group2.id,
+            transaction_id='abc123',
+        )
 
 
 class GroupDeleteTest(APITestCase):
