commit 74af850aa5de8e5b53c84d9fc0ef8b8a1fd3d17b
Author: evanh <evanh@users.noreply.github.com>
Date:   Tue Mar 3 10:43:25 2020 -0500

    feat(discover/issue search) Move all aggregates and aliases to functions (#17392)
    
    Consolidate all the aggregates and field aliases into functions in event_search,
    and add a shim to discover to translate any old queries into the new syntax.

diff --git a/src/sentry/api/endpoints/organization_events_stats.py b/src/sentry/api/endpoints/organization_events_stats.py
index 24a9d7cf2b..e75804c562 100644
--- a/src/sentry/api/endpoints/organization_events_stats.py
+++ b/src/sentry/api/endpoints/organization_events_stats.py
@@ -8,12 +8,7 @@ from rest_framework.exceptions import ParseError
 
 from sentry import features
 from sentry.api.bases import OrganizationEventsEndpointBase, OrganizationEventsError, NoProjects
-from sentry.api.event_search import (
-    resolve_field_list,
-    InvalidSearchQuery,
-    get_aggregate_alias,
-    AGGREGATE_PATTERN,
-)
+from sentry.api.event_search import resolve_field_list, InvalidSearchQuery, get_function_alias
 from sentry.api.serializers.snuba import SnubaTSResultSerializer
 from sentry.discover.utils import transform_aliases_and_query
 from sentry.snuba import discover
@@ -64,9 +59,7 @@ class OrganizationEventsStatsEndpoint(OrganizationEventsEndpointBase):
         if len(columns) > 1:
             # Return with requested yAxis as the key
             data = {
-                column: serializer.serialize(
-                    result, get_aggregate_alias(AGGREGATE_PATTERN.search(query_column))
-                )
+                column: serializer.serialize(result, get_function_alias(query_column))
                 for column, query_column in zip(columns, query_columns)
             }
         else:
@@ -78,11 +71,11 @@ class OrganizationEventsStatsEndpoint(OrganizationEventsEndpointBase):
         if interval is None:
             interval = timedelta(hours=1)
 
-        date_range = params['end'] - params['start']
+        date_range = params["end"] - params["start"]
         if date_range.total_seconds() / interval.total_seconds() > MAX_POINTS:
             raise InvalidSearchQuery(
-                'Your interval and date range would create too many results. '
-                'Use a larger interval, or a smaller date range.'
+                "Your interval and date range would create too many results. "
+                "Use a larger interval, or a smaller date range."
             )
 
         return int(interval.total_seconds())
diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index d8dbf5d289..1bb001c5bb 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -129,7 +129,7 @@ function_key         = key space? open_paren space? closed_paren
 search_key           = key / quoted_key
 search_value         = quoted_value / value
 value                = ~r"[^()\s]*"
-numeric_value        = ~r"[0-9]+(?=\s|$)"
+numeric_value        = ~r"[-]?[0-9\.]+(?=\s|$)"
 quoted_value         = ~r"\"((?:[^\"]|(?<=\\)[\"])*)?\""s
 key                  = ~r"[a-zA-Z0-9_\.-]+"
 function_arg         = space? key? comma? space?
@@ -391,7 +391,7 @@ class SearchVisitor(NodeVisitor):
         operator = operator[0] if not isinstance(operator, Node) else "="
 
         try:
-            search_value = SearchValue(int(search_value.text))
+            search_value = SearchValue(float(search_value.text))
         except ValueError:
             raise InvalidSearchQuery(u"Invalid aggregate query condition: {}".format(search_key))
         return AggregateFilter(search_key, operator, search_value)
@@ -858,52 +858,11 @@ def get_filter(query=None, params=None):
 # the UI builder stays in sync.
 FIELD_ALIASES = {
     "last_seen": {"aggregations": [["max", "timestamp", "last_seen"]]},
-    "latest_event": {"aggregations": [["argMax", ["id", "timestamp"], "latest_event"]]},
     "project": {"fields": ["project.id"], "column_alias": "project.id"},
     "issue": {"fields": ["issue.id"], "column_alias": "issue.id"},
     "user": {"fields": ["user.id", "user.username", "user.email", "user.ip"]},
-    # Long term these will become more complex functions but these are
-    # field aliases.
-    "apdex": {"result_type": "number", "aggregations": [["apdex(duration, 300)", None, "apdex"]]},
-    "impact": {
-        "result_type": "number",
-        "aggregations": [
-            [
-                # Snuba is not able to parse Clickhouse infix expressions. We should pass aggregations
-                # in a format Snuba can parse so query optimizations can be applied.
-                # It has a minimal prefix parser though to bridge the gap between the current state
-                # and when we will have an easier syntax.
-                "plus(minus(1, divide(plus(countIf(less(duration, 300)),divide(countIf(and(greater(duration, 300),less(duration, 1200))),2)),count())),multiply(minus(1,divide(1,sqrt(uniq(user)))),3))",
-                None,
-                "impact",
-            ]
-        ],
-    },
-    "p75": {"result_type": "duration", "aggregations": [["quantile(0.75)(duration)", None, "p75"]]},
-    "p95": {"result_type": "duration", "aggregations": [["quantile(0.95)(duration)", None, "p95"]]},
-    "p99": {"result_type": "duration", "aggregations": [["quantile(0.99)(duration)", None, "p99"]]},
-    "error_rate": {
-        "result_type": "number",
-        "aggregations": [
-            ["divide(countIf(notEquals(transaction_status, 0)), count())", None, "error_rate"]
-        ],
-    },
 }
 
-# When adding functions to this list please also update
-# static/app/views/eventsV2/eventQueryParams.tsx so that
-# the UI builder stays in sync.
-VALID_AGGREGATES = {
-    "count_unique": {"snuba_name": "uniq", "fields": "*"},
-    "count": {"snuba_name": "count", "fields": "*"},
-    "min": {"snuba_name": "min", "fields": ["time", "timestamp", "transaction.duration"]},
-    "max": {"snuba_name": "max", "fields": ["time", "timestamp", "transaction.duration"]},
-    "avg": {"snuba_name": "avg", "fields": ["transaction.duration"]},
-    "sum": {"snuba_name": "sum", "fields": ["transaction.duration"]},
-}
-
-AGGREGATE_PATTERN = re.compile(r"^(?P<function>[^\(]+)\((?P<column>.*)\)$")
-
 
 def get_json_meta_type(field, snuba_type):
     alias_definition = FIELD_ALIASES.get(field)
@@ -916,19 +875,6 @@ def get_json_meta_type(field, snuba_type):
     return get_json_type(snuba_type)
 
 
-def validate_aggregate(field, match):
-    function_name = match.group("function")
-    if function_name not in VALID_AGGREGATES:
-        raise InvalidSearchQuery(u"Unknown aggregate function '{}'".format(field))
-
-    function_data = VALID_AGGREGATES[function_name]
-    column = match.group("column")
-    if column not in function_data["fields"] and function_data["fields"] != "*":
-        raise InvalidSearchQuery(
-            u"Invalid column '{}' in aggregate function '{}'".format(column, function_name)
-        )
-
-
 FUNCTION_PATTERN = re.compile(r"^(?P<function>[^\(]+)\((?P<columns>[^\)]*)\)$")
 
 
@@ -947,16 +893,53 @@ class FunctionArg(object):
         return False
 
 
+class CountColumn(FunctionArg):
+    def has_default(self, params):
+        return None
+
+    def normalize(self, value):
+        if value is None:
+            return value
+
+        # If we use an alias inside an aggregate, resolve it here
+        if value in FIELD_ALIASES:
+            value = FIELD_ALIASES[value].get("column_alias", value)
+
+        return value
+
+
 class NumericColumn(FunctionArg):
     def normalize(self, value):
         snuba_column = SEARCH_MAP.get(value)
         if not snuba_column:
             raise InvalidFunctionArgument(u"{} is not a valid column".format(value))
-        elif snuba_column != "duration":
+        elif snuba_column not in ["time", "timestamp", "duration"]:
             raise InvalidFunctionArgument(u"{} is not a numeric column".format(value))
         return snuba_column
 
 
+class NumericColumnNoLookup(NumericColumn):
+    def normalize(self, value):
+        super(NumericColumnNoLookup, self).normalize(value)
+        return value
+
+
+class DurationColumn(FunctionArg):
+    def normalize(self, value):
+        snuba_column = SEARCH_MAP.get(value)
+        if not snuba_column:
+            raise InvalidFunctionArgument(u"{} is not a valid column".format(value))
+        elif snuba_column != "duration":
+            raise InvalidFunctionArgument(u"{} is not a duration column".format(value))
+        return snuba_column
+
+
+class DurationColumnNoLookup(DurationColumn):
+    def normalize(self, value):
+        super(DurationColumnNoLookup, self).normalize(value)
+        return value
+
+
 class NumberRange(FunctionArg):
     def __init__(self, name, start, end):
         super(NumberRange, self).__init__(name)
@@ -996,7 +979,7 @@ class IntervalDefault(NumberRange):
 FUNCTIONS = {
     "percentile": {
         "name": "percentile",
-        "args": [NumericColumn("column"), NumberRange("percentile", 0, 1)],
+        "args": [DurationColumn("column"), NumberRange("percentile", 0, 1)],
         "transform": u"quantile({percentile:.2f})({column})",
     },
     "rps": {
@@ -1009,16 +992,79 @@ FUNCTIONS = {
         "args": [IntervalDefault("interval", 60, None)],
         "transform": u"divide(count(), divide({interval:g}, 60))",
     },
+    "last_seen": {"name": "last_seen", "args": [], "aggregate": ["max", "timestamp", "last_seen"]},
+    "latest_event": {
+        "name": "latest_event",
+        "args": [],
+        "aggregate": ["argMax", ["id", "timestamp"], "latest_event"],
+    },
+    "apdex": {
+        "name": "apdex",
+        "args": [DurationColumn("column"), NumberRange("satisfaction", 0, None)],
+        "transform": u"apdex({column}, {satisfaction:g})",
+    },
+    "impact": {
+        "name": "impact",
+        "args": [DurationColumn("column"), NumberRange("satisfaction", 0, None)],
+        "calculated_args": [{"name": "tolerated", "fn": lambda args: args["satisfaction"] * 4.0}],
+        # Snuba is not able to parse Clickhouse infix expressions. We should pass aggregations
+        # in a format Snuba can parse so query optimizations can be applied.
+        # It has a minimal prefix parser though to bridge the gap between the current state
+        # and when we will have an easier syntax.
+        "transform": u"plus(minus(1, divide(plus(countIf(less({column}, {satisfaction:g})),divide(countIf(and(greater({column}, {satisfaction:g}),less({column}, {tolerated:g}))),2)),count())),multiply(minus(1,divide(1,sqrt(uniq(user)))),3))",
+    },
+    "error_rate": {
+        "name": "error_rate",
+        "args": [],
+        "transform": "divide(countIf(notEquals(transaction_status, 0)), count())",
+    },
+    "count_unique": {
+        "name": "count_unique",
+        "args": [CountColumn("column")],
+        "aggregate": ["uniq", u"{column}", None],
+    },
+    # TODO(evanh) Count doesn't accept parameters in the frontend, but we support it here
+    # for backwards compatibility. Once we've migrated existing queries this should get
+    # changed to accept no parameters.
+    "count": {"name": "count", "args": [CountColumn("column")], "aggregate": ["count", None, None]},
+    "min": {
+        "name": "min",
+        "args": [NumericColumnNoLookup("column")],
+        "aggregate": ["min", u"{column}", None],
+    },
+    "max": {
+        "name": "max",
+        "args": [NumericColumnNoLookup("column")],
+        "aggregate": ["max", u"{column}", None],
+    },
+    "avg": {
+        "name": "avg",
+        "args": [DurationColumnNoLookup("column")],
+        "aggregate": ["avg", u"{column}", None],
+    },
+    "sum": {
+        "name": "sum",
+        "args": [DurationColumnNoLookup("column")],
+        "aggregate": ["sum", u"{column}", None],
+    },
 }
 
 
 def is_function(field):
     function_match = FUNCTION_PATTERN.search(field)
-    if function_match and function_match.group("function") in FUNCTIONS:
+    if function_match:
         return function_match
 
+    return None
+
+
+def get_function_alias(field):
+    match = FUNCTION_PATTERN.search(field)
+    columns = [c.strip() for c in match.group("columns").split(",") if len(c.strip()) > 0]
+    return get_function_alias_with_columns(match.group("function"), columns)
+
 
-def get_function_alias(function_name, columns):
+def get_function_alias_with_columns(function_name, columns):
     columns = "_".join(columns).replace(".", "_")
     return u"{}_{}".format(function_name, columns).rstrip("_")
 
@@ -1026,8 +1072,9 @@ def get_function_alias(function_name, columns):
 def resolve_function(field, match=None, params=None):
     if not match:
         match = FUNCTION_PATTERN.search(field)
-        if not match or match.group("function") not in FUNCTIONS:
-            raise InvalidSearchQuery(u"{} is not a valid function".format(field))
+
+    if not match or match.group("function") not in FUNCTIONS:
+        raise InvalidSearchQuery(u"{} is not a valid function".format(field))
 
     function = FUNCTIONS[match.group("function")]
     columns = [c.strip() for c in match.group("columns").split(",") if len(c.strip()) > 0]
@@ -1041,13 +1088,15 @@ def resolve_function(field, match=None, params=None):
         except InvalidFunctionArgument as e:
             raise InvalidSearchQuery(u"{}: invalid arguments: {}".format(field, e))
 
-        if default:
-            # Hacky, but we expect column arguments to be strings so easiest to convert it back
-            columns = [six.text_type(default)]
+        # Hacky, but we expect column arguments to be strings so easiest to convert it back
+        if default is not False:
+            columns = [six.text_type(default) if default else default]
             used_default = True
 
     if len(columns) != len(function["args"]):
-        raise InvalidSearchQuery(u"{}: expected {} arguments".format(field, len(function["args"])))
+        raise InvalidSearchQuery(
+            u"{}: expected {:g} arguments".format(field, len(function["args"]))
+        )
 
     arguments = {}
     for column_value, argument in zip(columns, function["args"]):
@@ -1057,18 +1106,36 @@ def resolve_function(field, match=None, params=None):
         except InvalidFunctionArgument as e:
             raise InvalidSearchQuery(u"{}: {} argument invalid: {}".format(field, argument.name, e))
 
-    snuba_string = function["transform"].format(**arguments)
+    if "calculated_args" in function:
+        for calculation in function["calculated_args"]:
+            arguments[calculation["name"]] = calculation["fn"](arguments)
 
-    return (
-        [],
-        [
+    if "transform" in function:
+        snuba_string = function["transform"].format(**arguments)
+        return (
+            [],
             [
-                snuba_string,
-                None,
-                get_function_alias(function["name"], columns if not used_default else []),
-            ]
-        ],
-    )
+                [
+                    snuba_string,
+                    None,
+                    get_function_alias_with_columns(
+                        function["name"], columns if not used_default else []
+                    ),
+                ]
+            ],
+        )
+    elif "aggregate" in function:
+        aggregate = deepcopy(function["aggregate"])
+
+        if isinstance(aggregate[1], six.string_types):
+            aggregate[1] = aggregate[1].format(**arguments)
+
+        if aggregate[2] is None:
+            aggregate[2] = get_function_alias_with_columns(
+                function["name"], columns if not used_default else []
+            )
+
+        return ([], [aggregate])
 
 
 def resolve_orderby(orderby, fields, aggregations):
@@ -1089,9 +1156,9 @@ def resolve_orderby(orderby, fields, aggregations):
             validated.append(column)
             continue
 
-        match = AGGREGATE_PATTERN.search(bare_column)
-        if match:
-            bare_column = get_aggregate_alias(match)
+        if is_function(bare_column):
+            bare_column = get_function_alias(bare_column)
+
         found = [agg[2] for agg in aggregations if agg[2] == bare_column]
         if found:
             prefix = "-" if column.startswith("-") else ""
@@ -1121,33 +1188,7 @@ def resolve_field(field, params=None):
         special_field = deepcopy(FIELD_ALIASES[sans_parens])
         return (special_field.get("fields", []), special_field.get("aggregations", []))
 
-    # Basic fields don't require additional validation. They could be tag
-    # names which we have no way of validating at this point.
-    match = AGGREGATE_PATTERN.search(field)
-    if not match:
-        return ([field], None)
-
-    validate_aggregate(field, match)
-
-    if match.group("function") == "count":
-        # count() is a special function that ignores its column arguments.
-        return (None, [["count", None, get_aggregate_alias(match)]])
-
-    # If we use an alias inside an aggregate, resolve it here
-    column = match.group("column")
-    if column in FIELD_ALIASES:
-        column = FIELD_ALIASES[column].get("column_alias", column)
-
-    return (
-        None,
-        [
-            [
-                VALID_AGGREGATES[match.group("function")]["snuba_name"],
-                column,
-                get_aggregate_alias(match),
-            ]
-        ],
-    )
+    return ([field], None)
 
 
 def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
@@ -1199,7 +1240,8 @@ def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
             columns.append("project.id")
             project_column = "project_id"
         if aggregations and "latest_event" not in map(lambda a: a[-1], aggregations):
-            aggregations.extend(deepcopy(FIELD_ALIASES["latest_event"]["aggregations"]))
+            _, aggregates = resolve_function("latest_event()")
+            aggregations.extend(aggregates)
         if aggregations and "project.id" not in columns:
             aggregations.append(["argMax", ["project.id", "timestamp"], "projectid"])
             project_column = "projectid"
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 8f11d3f104..630da7ecba 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -9,12 +9,13 @@ from datetime import timedelta
 from sentry import options
 from sentry.api.event_search import (
     get_filter,
+    get_function_alias,
+    is_function,
     resolve_field_list,
     InvalidSearchQuery,
-    AGGREGATE_PATTERN,
     FIELD_ALIASES,
-    VALID_AGGREGATES,
 )
+
 from sentry import eventstore
 
 from sentry.models import Project, ProjectStatus
@@ -56,11 +57,10 @@ def is_real_column(col):
     Return true if col corresponds to an actual column to be fetched
     (not an aggregate function or field alias)
     """
-    if col in FIELD_ALIASES or col.strip("()") in FIELD_ALIASES:
+    if is_function(col):
         return False
 
-    match = AGGREGATE_PATTERN.search(col)
-    if match and match.group("function") in VALID_AGGREGATES:
+    if col in FIELD_ALIASES:
         return False
 
     return True
@@ -271,6 +271,79 @@ def transform_results(result, translated_columns, snuba_args):
     return result
 
 
+# TODO(evanh) This is only here for backwards compatibilty with old queries using these deprecated
+# aliases. Once we migrate the queries these can go away.
+OLD_FUNCTIONS_TO_NEW = {
+    "p75": "percentile(transaction.duration, 0.75)",
+    "p95": "percentile(transaction.duration, 0.95)",
+    "p99": "percentile(transaction.duration, 0.99)",
+    "last_seen": "last_seen()",
+    "latest_event": "latest_event()",
+    "apdex": "apdex(transaction.duration, 300)",
+    "impact": "impact(transaction.duration, 300)",
+}
+
+
+def transform_deprecated_functions_in_columns(columns):
+    new_list = []
+    translations = {}
+    for column in columns:
+        if column in OLD_FUNCTIONS_TO_NEW:
+            new_column = OLD_FUNCTIONS_TO_NEW[column]
+            translations[get_function_alias(new_column)] = column
+            new_list.append(new_column)
+        elif column.replace("()", "") in OLD_FUNCTIONS_TO_NEW:
+            new_column = OLD_FUNCTIONS_TO_NEW[column.replace("()", "")]
+            translations[get_function_alias(new_column)] = column.replace("()", "")
+            new_list.append(new_column)
+        else:
+            new_list.append(column)
+
+    return new_list, translations
+
+
+def transform_deprecated_functions_in_orderby(orderby):
+    if not orderby:
+        return
+
+    orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+    new_orderby = []
+    for order in orderby:
+        has_negative = False
+        column = order
+        if order.startswith("-"):
+            has_negative = True
+            column = order.strip("-")
+
+        new_column = column
+        if column in OLD_FUNCTIONS_TO_NEW:
+            new_column = get_function_alias(OLD_FUNCTIONS_TO_NEW[column])
+        elif column.replace("()", "") in OLD_FUNCTIONS_TO_NEW:
+            new_column = get_function_alias(OLD_FUNCTIONS_TO_NEW[column.replace("()", "")])
+
+        if has_negative:
+            new_column = "-" + new_column
+
+        new_orderby.append(new_column)
+
+    return new_orderby
+
+
+def transform_deprecated_functions_in_query(query):
+    if query is None:
+        return query
+
+    for old_function in OLD_FUNCTIONS_TO_NEW:
+        if old_function + "()" in query:
+            replacement = OLD_FUNCTIONS_TO_NEW[old_function]
+            query = query.replace(old_function + "()", replacement)
+        elif old_function in query:
+            replacement = OLD_FUNCTIONS_TO_NEW[old_function]
+            query = query.replace(old_function, replacement)
+
+    return query
+
+
 def query(
     selected_columns,
     query,
@@ -307,6 +380,14 @@ def query(
     if not selected_columns:
         raise InvalidSearchQuery("No columns selected")
 
+    # TODO(evanh): These can be removed once we migrate the frontend / saved queries
+    # to use the new function values
+    selected_columns, function_translations = transform_deprecated_functions_in_columns(
+        selected_columns
+    )
+    orderby = transform_deprecated_functions_in_orderby(orderby)
+    query = transform_deprecated_functions_in_query(query)
+
     snuba_filter = get_filter(query, params)
 
     # TODO(mark) Refactor the need for this translation shim once all of
@@ -335,6 +416,8 @@ def query(
 
     # Resolve the public aliases into the discover dataset names.
     snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
+    for snuba_name, sentry_name in six.iteritems(function_translations):
+        translated_columns[snuba_name] = sentry_name
 
     # Make sure that any aggregate conditions are also in the selected columns
     for having_clause in snuba_args.get("having"):
@@ -389,6 +472,11 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
                     conditions based on the provided reference.
     referrer (str|None) A referrer string to help locate the origin of this query.
     """
+    # TODO(evanh): These can be removed once we migrate the frontend / saved queries
+    # to use the new function values
+    selected_columns, _ = transform_deprecated_functions_in_columns(selected_columns)
+    query = transform_deprecated_functions_in_query(query)
+
     snuba_filter = get_filter(query, params)
     snuba_args = {
         "start": snuba_filter.start,
@@ -453,6 +541,10 @@ def get_pagination_ids(event, query, params, organization, reference_event=None,
                                     conditions based on the provided reference.
     referrer (str|None) A referrer string to help locate the origin of this query.
     """
+    # TODO(evanh): This can be removed once we migrate the frontend / saved queries
+    # to use the new function values
+    query = transform_deprecated_functions_in_query(query)
+
     snuba_filter = get_filter(query, params)
 
     if reference_event:
@@ -509,6 +601,10 @@ def get_facets(query, params, limit=10, referrer=None):
 
     Returns Sequence[FacetResult]
     """
+    # TODO(evanh): This can be removed once we migrate the frontend / saved queries
+    # to use the new function values
+    query = transform_deprecated_functions_in_query(query)
+
     snuba_filter = get_filter(query, params)
 
     # TODO(mark) Refactor the need for this translation shim.
diff --git a/tests/sentry/api/test_event_search.py b/tests/sentry/api/test_event_search.py
index b0cc0d72af..d372051f8a 100644
--- a/tests/sentry/api/test_event_search.py
+++ b/tests/sentry/api/test_event_search.py
@@ -39,9 +39,6 @@ def test_get_json_meta_type():
     assert get_json_meta_type("transaction", "Char") == "string"
     assert get_json_meta_type("foo", "unknown") == "string"
     assert get_json_meta_type("other", "") == "string"
-    assert get_json_meta_type("p99", "number") == "duration"
-    assert get_json_meta_type("p95", "number") == "duration"
-    assert get_json_meta_type("p75", "number") == "duration"
     assert get_json_meta_type("avg_duration", "number") == "duration"
     assert get_json_meta_type("duration", "number") == "duration"
 
@@ -1157,13 +1154,21 @@ class GetSnubaQueryArgsTest(TestCase):
         assert result.having == [["rpm", ">", 100]]
 
     def test_function_with_alias(self):
-        result = get_filter("p95():>100")
-        assert result.having == [["p95", ">", 100]]
+        result = get_filter("percentile(transaction.duration, 0.95):>100")
+        assert result.having == [["percentile_transaction_duration_0_95", ">", 100]]
 
     def test_function_arguments(self):
         result = get_filter("percentile(transaction.duration, 0.75):>100")
         assert result.having == [["percentile_transaction_duration_0_75", ">", 100]]
 
+    def test_function_with_float_arguments(self):
+        result = get_filter("apdex(transaction.duration, 300):>0.5")
+        assert result.having == [["apdex_transaction_duration_300", ">", 0.5]]
+
+    def test_function_with_negative_arguments(self):
+        result = get_filter("apdex(transaction.duration, 300):>-0.5")
+        assert result.having == [["apdex_transaction_duration_300", ">", -0.5]]
+
     @pytest.mark.xfail(reason="this breaks issue search so needs to be redone")
     def test_trace_id(self):
         result = get_filter("trace:{}".format("a0fa8803753e40fd8124b21eeb2986b5"))
@@ -1199,37 +1204,16 @@ class ResolveFieldListTest(unittest.TestCase):
         ]
         assert result["groupby"] == ["title"]
 
-    def test_field_alias_duration_expansion(self):
-        fields = ["avg(transaction.duration)", "apdex", "impact", "p75", "p95", "p99"]
-        result = resolve_field_list(fields, {})
-        assert result["selected_columns"] == []
-        assert result["aggregations"] == [
-            ["avg", "transaction.duration", "avg_transaction_duration"],
-            ["apdex(duration, 300)", None, "apdex"],
-            [
-                "plus(minus(1, divide(plus(countIf(less(duration, 300)),divide(countIf(and(greater(duration, 300),less(duration, 1200))),2)),count())),multiply(minus(1,divide(1,sqrt(uniq(user)))),3))",
-                None,
-                "impact",
-            ],
-            ["quantile(0.75)(duration)", None, "p75"],
-            ["quantile(0.95)(duration)", None, "p95"],
-            ["quantile(0.99)(duration)", None, "p99"],
-            ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project.id", "timestamp"], "projectid"],
-            ["transform(projectid, [], [], '')", None, "project.name"],
-        ]
-        assert result["groupby"] == []
-
     def test_field_alias_duration_expansion_with_brackets(self):
         fields = [
             "avg(transaction.duration)",
             "latest_event()",
             "last_seen()",
-            "apdex()",
-            "impact()",
-            "p75()",
-            "p95()",
-            "p99()",
+            "apdex(transaction.duration, 300)",
+            "impact(transaction.duration, 300)",
+            "percentile(transaction.duration, 0.75)",
+            "percentile(transaction.duration, 0.95)",
+            "percentile(transaction.duration, 0.99)",
         ]
         result = resolve_field_list(fields, {})
 
@@ -1238,22 +1222,22 @@ class ResolveFieldListTest(unittest.TestCase):
             ["avg", "transaction.duration", "avg_transaction_duration"],
             ["argMax", ["id", "timestamp"], "latest_event"],
             ["max", "timestamp", "last_seen"],
-            ["apdex(duration, 300)", None, "apdex"],
+            ["apdex(duration, 300)", None, "apdex_transaction_duration_300"],
             [
                 "plus(minus(1, divide(plus(countIf(less(duration, 300)),divide(countIf(and(greater(duration, 300),less(duration, 1200))),2)),count())),multiply(minus(1,divide(1,sqrt(uniq(user)))),3))",
                 None,
-                "impact",
+                "impact_transaction_duration_300",
             ],
-            ["quantile(0.75)(duration)", None, "p75"],
-            ["quantile(0.95)(duration)", None, "p95"],
-            ["quantile(0.99)(duration)", None, "p99"],
+            ["quantile(0.75)(duration)", None, "percentile_transaction_duration_0_75"],
+            ["quantile(0.95)(duration)", None, "percentile_transaction_duration_0_95"],
+            ["quantile(0.99)(duration)", None, "percentile_transaction_duration_0_99"],
             ["argMax", ["project.id", "timestamp"], "projectid"],
             ["transform(projectid, [], [], '')", None, "project.name"],
         ]
         assert result["groupby"] == []
 
     def test_field_alias_expansion(self):
-        fields = ["title", "last_seen", "latest_event", "project", "issue", "user", "message"]
+        fields = ["title", "last_seen()", "latest_event()", "project", "issue", "user", "message"]
         result = resolve_field_list(fields, {})
         assert result["selected_columns"] == [
             "title",
@@ -1325,19 +1309,22 @@ class ResolveFieldListTest(unittest.TestCase):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["derp(user)"]
             resolve_field_list(fields, {})
-        assert "Unknown aggregate" in six.text_type(err)
+        assert "derp(user) is not a valid function" in six.text_type(err)
 
     def test_aggregate_function_case_sensitive(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["MAX(user)"]
             resolve_field_list(fields, {})
-        assert "Unknown aggregate" in six.text_type(err)
+        assert "MAX(user) is not a valid function" in six.text_type(err)
 
     def test_aggregate_function_invalid_column(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["min(message)"]
             resolve_field_list(fields, {})
-        assert "Invalid column" in six.text_type(err)
+        assert (
+            "InvalidSearchQuery: min(message): column argument invalid: message is not a numeric column"
+            in six.text_type(err)
+        )
 
     def test_percentile_function(self):
         fields = ["percentile(transaction.duration, 0.75)"]
@@ -1374,7 +1361,7 @@ class ResolveFieldListTest(unittest.TestCase):
             fields = ["percentile(id, 0.75)"]
             resolve_field_list(fields, {})
         assert (
-            "percentile(id, 0.75): column argument invalid: id is not a numeric column"
+            "percentile(id, 0.75): column argument invalid: id is not a duration column"
             in six.text_type(err)
         )
 
diff --git a/tests/sentry/discover/test_utils.py b/tests/sentry/discover/test_utils.py
index 85796651ac..dd1c3d87fc 100644
--- a/tests/sentry/discover/test_utils.py
+++ b/tests/sentry/discover/test_utils.py
@@ -59,7 +59,7 @@ class TransformAliasesAndQueryTest(SnubaTestCase, TestCase):
 
     def test_autoconversion_of_time_column(self):
         result = transform_aliases_and_query(
-            aggregations=[["count", "", "count"]],
+            aggregations=[["count", None, "count"]],
             filter_keys={"project_id": [self.project.id]},
             start=before_now(minutes=10),
             end=before_now(minutes=-1),
diff --git a/tests/sentry/snuba/test_discover.py b/tests/sentry/snuba/test_discover.py
index c1db78a888..46b6530773 100644
--- a/tests/sentry/snuba/test_discover.py
+++ b/tests/sentry/snuba/test_discover.py
@@ -436,8 +436,8 @@ class QueryTransformTest(TestCase):
     @patch("sentry.snuba.discover.raw_query")
     def test_selected_columns_aggregate_alias(self, mock_query):
         mock_query.return_value = {
-            "meta": [{"name": "transaction"}, {"name": "p95"}],
-            "data": [{"transaction": "api.do_things", "p95": 200}],
+            "meta": [{"name": "transaction"}, {"name": "percentile_transaction_duration_0_95"}],
+            "data": [{"transaction": "api.do_things", "percentile_transaction_duration_0_95": 200}],
         }
         discover.query(
             selected_columns=["transaction", "p95", "count_unique(transaction)"],
@@ -448,7 +448,7 @@ class QueryTransformTest(TestCase):
         mock_query.assert_called_with(
             selected_columns=["transaction"],
             aggregations=[
-                ["quantile(0.95)(duration)", None, "p95"],
+                ["quantile(0.95)(duration)", None, "percentile_transaction_duration_0_95"],
                 ["uniq", "transaction", "count_unique_transaction"],
                 ["argMax", ["event_id", "timestamp"], "latest_event"],
                 ["argMax", ["project_id", "timestamp"], "projectid"],
@@ -476,8 +476,8 @@ class QueryTransformTest(TestCase):
     @patch("sentry.snuba.discover.raw_query")
     def test_selected_columns_aggregate_alias_with_brackets(self, mock_query):
         mock_query.return_value = {
-            "meta": [{"name": "transaction"}, {"name": "p95()"}],
-            "data": [{"transaction": "api.do_things", "p95()": 200}],
+            "meta": [{"name": "transaction"}, {"name": "percentile_transaction_duration_0_95"}],
+            "data": [{"transaction": "api.do_things", "percentile_transaction_duration_0_95": 200}],
         }
         discover.query(
             selected_columns=["transaction", "p95()", "count_unique(transaction)"],
@@ -488,7 +488,7 @@ class QueryTransformTest(TestCase):
         mock_query.assert_called_with(
             selected_columns=["transaction"],
             aggregations=[
-                ["quantile(0.95)(duration)", None, "p95"],
+                ["quantile(0.95)(duration)", None, "percentile_transaction_duration_0_95"],
                 ["uniq", "transaction", "count_unique_transaction"],
                 ["argMax", ["event_id", "timestamp"], "latest_event"],
                 ["argMax", ["project_id", "timestamp"], "projectid"],
@@ -893,8 +893,10 @@ class QueryTransformTest(TestCase):
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction"],
             dataset=Dataset.Discover,
-            aggregations=[["quantile(0.95)(duration)", None, "p95"]],
-            having=[["p95", ">", 5]],
+            aggregations=[
+                ["quantile(0.95)(duration)", None, "percentile_transaction_duration_0_95"]
+            ],
+            having=[["percentile_transaction_duration_0_95", ">", 5]],
             end=end_time,
             start=start_time,
             orderby=None,
@@ -924,8 +926,10 @@ class QueryTransformTest(TestCase):
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction"],
             dataset=Dataset.Discover,
-            aggregations=[["quantile(0.95)(duration)", None, "p95"]],
-            having=[["p95", ">", 5]],
+            aggregations=[
+                ["quantile(0.95)(duration)", None, "percentile_transaction_duration_0_95"]
+            ],
+            having=[["percentile_transaction_duration_0_95", ">", 5]],
             end=end_time,
             start=start_time,
             orderby=None,
diff --git a/tests/snuba/api/endpoints/test_organization_event_details.py b/tests/snuba/api/endpoints/test_organization_event_details.py
index adec73612d..6981d04806 100644
--- a/tests/snuba/api/endpoints/test_organization_event_details.py
+++ b/tests/snuba/api/endpoints/test_organization_event_details.py
@@ -489,6 +489,7 @@ class OrganizationEventDetailsEndpointTest(APITestCase, SnubaTestCase):
                     "query": "transaction.duration:>2 p95():>0",
                 },
             )
-        assert response.status_code == 200
+
+        assert response.status_code == 200, response.content
         assert response.data["nextEventID"] == format_project_event(self.project.slug, "d" * 32)
         assert response.data["previousEventID"] == format_project_event(self.project.slug, "f" * 32)
diff --git a/tests/snuba/api/endpoints/test_organization_events_v2.py b/tests/snuba/api/endpoints/test_organization_events_v2.py
index b4f4b76931..830ae90261 100644
--- a/tests/snuba/api/endpoints/test_organization_events_v2.py
+++ b/tests/snuba/api/endpoints/test_organization_events_v2.py
@@ -516,6 +516,61 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
         assert data[1]["count_id"] == 2
         assert data[1]["count_unique_user"] == 2
 
+    def test_aggregate_field_with_dotted_param(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        event1 = self.store_event(
+            data={
+                "event_id": "a" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_1"],
+                "user": {"id": "123", "email": "foo@example.com"},
+            },
+            project_id=project.id,
+        )
+        event2 = self.store_event(
+            data={
+                "event_id": "b" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_2"],
+                "user": {"id": "123", "email": "foo@example.com"},
+            },
+            project_id=project.id,
+        )
+        self.store_event(
+            data={
+                "event_id": "c" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_2"],
+                "user": {"id": "456", "email": "bar@example.com"},
+            },
+            project_id=project.id,
+        )
+
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["issue.id", "issue_title", "count(id)", "count_unique(user.email)"],
+                    "orderby": "issue.id",
+                },
+            )
+
+        assert response.status_code == 200, response.content
+        assert len(response.data["data"]) == 2
+        data = response.data["data"]
+        assert data[0]["issue.id"] == event1.group_id
+        assert data[0]["count_id"] == 1
+        assert data[0]["count_unique_user_email"] == 1
+        assert "latest_event" in data[0]
+        assert "project.name" in data[0]
+        assert "projectid" not in data[0]
+        assert "project.id" not in data[0]
+        assert data[1]["issue.id"] == event2.group_id
+        assert data[1]["count_id"] == 2
+        assert data[1]["count_unique_user_email"] == 2
+
     def test_error_rate_alias_field(self):
         self.login_as(user=self.user)
         project = self.create_project()
@@ -1437,3 +1492,378 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
             assert data[0]["count_id"] == 2
             assert data[0]["count_unique_project_id"] == 2
             assert data[0]["count_unique_project"] == 2
+
+    def test_all_aggregates_in_columns(self):
+        self.login_as(user=self.user)
+
+        project = self.create_project()
+        data = load_data("transaction")
+        data["transaction"] = "/error_rate/1"
+        data["timestamp"] = iso_format(before_now(minutes=2))
+        data["start_timestamp"] = iso_format(before_now(minutes=2, seconds=5))
+        self.store_event(data, project_id=project.id)
+
+        data = load_data("transaction")
+        data["transaction"] = "/error_rate/1"
+        data["timestamp"] = iso_format(before_now(minutes=1))
+        data["start_timestamp"] = iso_format(before_now(minutes=1, seconds=5))
+        data["contexts"]["trace"]["status"] = "unauthenticated"
+        event = self.store_event(data, project_id=project.id)
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": [
+                        "event.type",
+                        "p75",
+                        "p95()",
+                        "percentile(transaction.duration, 0.99)",
+                        "apdex",
+                        "impact()",
+                        "error_rate()",
+                    ],
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["p75"] == 5000
+            assert data[0]["p95"] == 5000
+            assert data[0]["percentile_transaction_duration_0_99"] == 5000
+            assert data[0]["apdex"] == 0.0
+            assert data[0]["impact"] == 1.0
+            assert data[0]["error_rate"] == 0.5
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "last_seen", "latest_event()"],
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert iso_format(before_now(minutes=1))[:-5] in data[0]["last_seen"]
+            assert data[0]["latest_event"] == event.event_id
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": [
+                        "event.type",
+                        "count()",
+                        "count(id)",
+                        "count_unique(project)",
+                        "min(transaction.duration)",
+                        "max(transaction.duration)",
+                        "avg(transaction.duration)",
+                        "sum(transaction.duration)",
+                    ],
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["count"] == 2
+            assert data[0]["count_id"] == 2
+            assert data[0]["count_unique_project"] == 1
+            assert data[0]["min_transaction_duration"] == 5000
+            assert data[0]["max_transaction_duration"] == 5000
+            assert data[0]["avg_transaction_duration"] == 5000
+            assert data[0]["sum_transaction_duration"] == 10000
+
+    def test_all_aggregates_in_query(self):
+        self.login_as(user=self.user)
+
+        project = self.create_project()
+        data = load_data("transaction")
+
+        data["transaction"] = "/error_rate/1"
+        data["timestamp"] = iso_format(before_now(minutes=2))
+        data["start_timestamp"] = iso_format(before_now(minutes=2, seconds=5))
+        self.store_event(data, project_id=project.id)
+
+        data = load_data("transaction")
+        data["transaction"] = "/error_rate/2"
+        data["timestamp"] = iso_format(before_now(minutes=1))
+        data["start_timestamp"] = iso_format(before_now(minutes=1, seconds=5))
+        data["contexts"]["trace"]["status"] = "unauthenticated"
+        self.store_event(data, project_id=project.id)
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": [
+                        "event.type",
+                        "p75",
+                        "p95()",
+                        "percentile(transaction.duration, 0.99)",
+                    ],
+                    "query": "event.type:transaction p75:>1000 p95():>1000 percentile(transaction.duration, 0.99):>1000",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["p75"] == 5000
+            assert data[0]["p95"] == 5000
+            assert data[0]["percentile_transaction_duration_0_99"] == 5000
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "apdex", "impact()", "error_rate()"],
+                    "query": "event.type:transaction apdex:>-1.0 impact():>0.5 error_rate():>0.25",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["apdex"] == 0.0
+            assert data[0]["impact"] == 1.0
+            assert data[0]["error_rate"] == 0.5
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "last_seen", "latest_event()"],
+                    "query": u"event.type:transaction last_seen:>1990-12-01T00:00:00",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 0
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "count()", "count(id)", "count_unique(transaction)"],
+                    "query": "event.type:transaction count():>1 count(id):>1 count_unique(transaction):>1",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["count"] == 2
+            assert data[0]["count_id"] == 2
+            assert data[0]["count_unique_transaction"] == 2
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": [
+                        "event.type",
+                        "min(transaction.duration)",
+                        "max(transaction.duration)",
+                        "avg(transaction.duration)",
+                        "sum(transaction.duration)",
+                    ],
+                    "query": "event.type:transaction min(transaction.duration):>1000 max(transaction.duration):>1000 avg(transaction.duration):>1000 sum(transaction.duration):>1000",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["min_transaction_duration"] == 5000
+            assert data[0]["max_transaction_duration"] == 5000
+            assert data[0]["avg_transaction_duration"] == 5000
+            assert data[0]["sum_transaction_duration"] == 10000
+
+    def test_functions_in_orderby(self):
+        self.login_as(user=self.user)
+
+        project = self.create_project()
+        data = load_data("transaction")
+
+        data["transaction"] = "/error_rate/1"
+        data["timestamp"] = iso_format(before_now(minutes=2))
+        data["start_timestamp"] = iso_format(before_now(minutes=2, seconds=5))
+        self.store_event(data, project_id=project.id)
+
+        data = load_data("transaction")
+        data["transaction"] = "/error_rate/2"
+        data["timestamp"] = iso_format(before_now(minutes=1))
+        data["start_timestamp"] = iso_format(before_now(minutes=1, seconds=5))
+        data["contexts"]["trace"]["status"] = "unauthenticated"
+        event = self.store_event(data, project_id=project.id)
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "p75"],
+                    "sort": "-p75",
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["p75"] == 5000
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "percentile(transaction.duration, 0.99)"],
+                    "sort": "-percentile(transaction.duration, 0.99)",
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["percentile_transaction_duration_0_99"] == 5000
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "apdex()"],
+                    "sort": "-apdex",
+                    "query": "event.type:transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["apdex"] == 0.0
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "latest_event()"],
+                    "query": u"event.type:transaction",
+                    "sort": "latest_event",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["latest_event"] == event.event_id
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "count_unique(transaction)"],
+                    "query": "event.type:transaction",
+                    "sort": "-count_unique_transaction",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["count_unique_transaction"] == 2
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "min(transaction.duration)"],
+                    "query": "event.type:transaction",
+                    "sort": "-min_transaction_duration",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["min_transaction_duration"] == 5000
+
+    def test_issue_alias_in_aggregate(self):
+        self.login_as(user=self.user)
+
+        project = self.create_project()
+        self.store_event(
+            data={"event_id": "a" * 32, "timestamp": self.two_min_ago, "fingerprint": ["group_1"]},
+            project_id=project.id,
+        )
+        self.store_event(
+            data={"event_id": "b" * 32, "timestamp": self.min_ago, "fingerprint": ["group_2"]},
+            project_id=project.id,
+        )
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["event.type", "count_unique(issue)"],
+                    "query": "count_unique(issue):>1",
+                },
+            )
+
+            assert response.status_code == 200, response.content
+            data = response.data["data"]
+            assert len(data) == 1
+            assert data[0]["count_unique_issue"] == 2
