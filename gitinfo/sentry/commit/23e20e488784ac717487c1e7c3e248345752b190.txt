commit 23e20e488784ac717487c1e7c3e248345752b190
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Fri Oct 13 17:00:25 2017 +0200

    feat(symbolic): Switch to symbolic based symbolication (#6301)
    
    This changes from our old symsynd based symbolication over to symbolic based on custom cache files.

diff --git a/requirements-base.txt b/requirements-base.txt
index e6aab14903..0fb36bff68 100644
--- a/requirements-base.txt
+++ b/requirements-base.txt
@@ -47,7 +47,7 @@ setproctitle>=1.1.7,<1.2.0
 statsd>=3.1.0,<3.2.0
 structlog==16.1.0
 sqlparse>=0.1.16,<0.2.0
-symsynd>=3.0.0,<4.0.0
+symbolic>=0.9.8,<1.0.0
 toronado>=0.0.11,<0.1.0
 ua-parser>=0.6.1,<0.8.0
 urllib3>=1.22,<1.23
diff --git a/src/sentry/api/endpoints/dsym_files.py b/src/sentry/api/endpoints/dsym_files.py
index c97c20d5d2..d4d96893b5 100644
--- a/src/sentry/api/endpoints/dsym_files.py
+++ b/src/sentry/api/endpoints/dsym_files.py
@@ -36,7 +36,7 @@ class AssociateDsymSerializer(serializers.Serializer):
     build = serializers.CharField(max_length=40, required=False)
 
 
-def upload_from_request(request, project=None):
+def upload_from_request(request, project):
     if 'file' not in request.FILES:
         return Response({'detail': 'Missing uploaded file'}, status=400)
     fileobj = request.FILES['file']
diff --git a/src/sentry/bgtasks/__init__.py b/src/sentry/bgtasks/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/src/sentry/bgtasks/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/src/sentry/bgtasks/api.py b/src/sentry/bgtasks/api.py
new file mode 100644
index 0000000000..cfad1d8fd0
--- /dev/null
+++ b/src/sentry/bgtasks/api.py
@@ -0,0 +1,104 @@
+from __future__ import absolute_import
+
+import six
+import time
+import random
+import logging
+import threading
+from contextlib import contextmanager
+
+from django.conf import settings
+
+
+logger = logging.getLogger('sentry.bgtasks')
+tasks = {}
+
+
+def bgtask(roles=None, interval=60):
+    def decorator(f):
+        return BgTask(callback=f, roles=roles, interval=interval)
+    return decorator
+
+
+class BgTask(object):
+
+    def __init__(self, callback, roles=None, interval=60):
+        self.callback = callback
+        self.roles = roles or []
+        self.interval = interval
+        self.running = False
+
+    @property
+    def name(self):
+        return '%s:%s' % (
+            self.callback.__module__,
+            self.callback.__name__,
+        )
+
+    def run(self):
+        if self.running:
+            return
+
+        next_run = time.time() + self.interval * random.random()
+        while self.running:
+            started = time.time()
+            if next_run >= started:
+                try:
+                    self.callback()
+                except Exception:
+                    logging.error('bgtask.failed', exc_info=True,
+                                  extra=dict(task_name=self.name))
+                next_run = started + self.interval
+            time.sleep(1.0)
+
+    def reconfigure(self, cfg):
+        if 'roles' in cfg:
+            self.roles = cfg['roles']
+        if 'interval' in cfg:
+            self.interval = cfg['interval']
+
+    def spawn_daemon(self):
+        if self.running:
+            return
+        logger.info('bgtask.spawn', extra=dict(task_name=self.name))
+        t = threading.Thread(target=self.run)
+        t.setDaemon(True)
+        t.start()
+
+    def stop(self):
+        logger.info('bgtask.stop', extra=dict(task_name=self.name))
+        self.running = False
+
+
+def get_task(task_name):
+    module, task_cls = task_name.split(':', 1)
+    mod = __import__(module, None, None, [task_cls])
+    return getattr(mod, task_cls)
+
+
+def spawn_bgtasks(role):
+    for import_name, cfg in six.iteritems(settings.BGTASKS):
+        task = get_task(import_name)
+        # This is already running
+        if task.name in tasks:
+            continue
+        task.reconfigure(cfg)
+        if role not in task.roles:
+            continue
+        task.spawn_daemon()
+        tasks[task.name] = task
+
+
+def shutdown_bgtasks():
+    for task_name, task in list(six.iteritems(tasks)):
+        task.stop()
+        tasks.pop(task_name, None)
+
+
+@contextmanager
+def managed_bgtasks(role):
+    spawn_bgtasks(role)
+    try:
+        yield
+    finally:
+        shutdown_bgtasks()
diff --git a/src/sentry/bgtasks/clean_dsymcache.py b/src/sentry/bgtasks/clean_dsymcache.py
new file mode 100644
index 0000000000..dd4a345eac
--- /dev/null
+++ b/src/sentry/bgtasks/clean_dsymcache.py
@@ -0,0 +1,9 @@
+from __future__ import absolute_import
+
+from sentry.bgtasks.api import bgtask
+from sentry.models import ProjectDSymFile
+
+
+@bgtask()
+def clean_dsymcache():
+    ProjectDSymFile.dsymcache.clear_old_entries()
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 98ea4c0b2c..a97c4f7c69 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -424,10 +424,11 @@ CELERY_IMPORTS = (
     'sentry.tasks.auth', 'sentry.tasks.auto_resolve_issues', 'sentry.tasks.beacon',
     'sentry.tasks.check_auth', 'sentry.tasks.clear_expired_snoozes',
     'sentry.tasks.collect_project_platforms', 'sentry.tasks.commits', 'sentry.tasks.deletion',
-    'sentry.tasks.digests', 'sentry.tasks.dsymcache', 'sentry.tasks.email', 'sentry.tasks.merge',
+    'sentry.tasks.digests', 'sentry.tasks.email', 'sentry.tasks.merge',
     'sentry.tasks.options', 'sentry.tasks.ping', 'sentry.tasks.post_process',
     'sentry.tasks.process_buffer', 'sentry.tasks.reports', 'sentry.tasks.reprocessing',
     'sentry.tasks.scheduler', 'sentry.tasks.store', 'sentry.tasks.unmerge',
+    'sentry.tasks.symcache_update',
 )
 CELERY_QUEUES = [
     Queue('alerts', routing_key='alerts'),
@@ -544,14 +545,6 @@ CELERYBEAT_SCHEDULE = {
             'expires': 300,
         },
     },
-    # Disabled for the time being:
-    # 'clear-old-cached-dsyms': {
-    #     'task': 'sentry.tasks.clear_old_cached_dsyms',
-    #     'schedule': timedelta(minutes=60),
-    #     'options': {
-    #         'expires': 3600,
-    #     },
-    # },
     'collect-project-platforms': {
         'task': 'sentry.tasks.collect_project_platforms',
         'schedule': timedelta(days=1),
@@ -588,6 +581,13 @@ CELERYBEAT_SCHEDULE = {
     },
 }
 
+BGTASKS = {
+    'sentry.bgtasks.clean_dsymcache:clean_dsymcache': {
+        'interval': 5 * 60,
+        'roles': ['worker'],
+    }
+}
+
 # Sentry logs to two major places: stdout, and it's internal project.
 # To disable logging to the internal project, add a logger who's only
 # handler is 'console' and disable propagating upwards.
diff --git a/src/sentry/interfaces/debug_meta.py b/src/sentry/interfaces/debug_meta.py
index ca2067e2eb..209b01aeb6 100644
--- a/src/sentry/interfaces/debug_meta.py
+++ b/src/sentry/interfaces/debug_meta.py
@@ -6,7 +6,8 @@ import uuid
 __all__ = ('DebugMeta', )
 
 from sentry.interfaces.base import Interface, InterfaceValidationError
-from sentry.utils.native import parse_addr
+
+from symbolic import parse_addr
 
 image_types = {}
 
@@ -26,12 +27,13 @@ def process_apple_image(image):
 
     try:
         apple_image = {
-            'cpu_type': image['cpu_type'],
-            'cpu_subtype': image['cpu_subtype'],
-            'image_addr': _addr(image['image_addr']),
+            'arch': image.get('arch'),
+            'cpu_type': image.get('cpu_type'),
+            'cpu_subtype': image.get('cpu_subtype'),
+            'image_addr': _addr(image.get('image_addr')),
             'image_size': image['image_size'],
             'image_vmaddr': _addr(image.get('image_vmaddr') or 0),
-            'name': image['name'],
+            'name': image.get('name'),
             'uuid': six.text_type(uuid.UUID(image['uuid']))
         }
         if image.get('major_version') is not None:
diff --git a/src/sentry/lang/java/plugin.py b/src/sentry/lang/java/plugin.py
index 862d6f9764..e2f2fc3883 100644
--- a/src/sentry/lang/java/plugin.py
+++ b/src/sentry/lang/java/plugin.py
@@ -1,5 +1,8 @@
 from __future__ import absolute_import
 
+import six
+import uuid
+
 from libsourcemap import ProguardView
 from sentry.plugins import Plugin2
 from sentry.stacktraces import StacktraceProcessor
@@ -19,7 +22,7 @@ class JavaStacktraceProcessor(StacktraceProcessor):
             self.debug_meta = debug_meta
             for img in debug_meta['images']:
                 if img['type'] == 'proguard':
-                    self.images.add(img['uuid'])
+                    self.images.add(uuid.UUID(img['uuid']))
         else:
             self.available = False
 
@@ -32,7 +35,7 @@ class JavaStacktraceProcessor(StacktraceProcessor):
             (
                 FRAME_CACHE_VERSION, processable_frame.frame['module'],
                 processable_frame.frame['function'],
-            ) + tuple(sorted(self.images))
+            ) + tuple(sorted(map(six.text_type, self.images)))
         )
 
     def preprocess_step(self, processing_task):
diff --git a/src/sentry/lang/native/applecrashreport.py b/src/sentry/lang/native/applecrashreport.py
index df9df95864..9ac21dc3cf 100644
--- a/src/sentry/lang/native/applecrashreport.py
+++ b/src/sentry/lang/native/applecrashreport.py
@@ -3,9 +3,10 @@ from __future__ import absolute_import
 import posixpath
 
 from sentry.utils.compat import implements_to_string
-from sentry.utils.native import parse_addr
 from sentry.constants import NATIVE_UNKNOWN_STRING
 
+from symbolic import parse_addr
+
 REPORT_VERSION = '104'
 
 
diff --git a/src/sentry/lang/native/plugin.py b/src/sentry/lang/native/plugin.py
index e5ca040876..9b41aeb8c4 100644
--- a/src/sentry/lang/native/plugin.py
+++ b/src/sentry/lang/native/plugin.py
@@ -1,9 +1,11 @@
 from __future__ import absolute_import
 
+import six
 import logging
 import posixpath
 
-from symsynd import find_best_instruction, parse_addr, ImageLookup
+from symbolic import parse_addr, find_best_instruction, arch_get_ip_reg_name, \
+    ObjectLookup
 
 from sentry import options
 from django.db import transaction, IntegrityError
@@ -20,21 +22,21 @@ from sentry.reprocessing import report_processing_issue
 
 logger = logging.getLogger(__name__)
 
-FRAME_CACHE_VERSION = 5
+FRAME_CACHE_VERSION = 6
 
 
 class NativeStacktraceProcessor(StacktraceProcessor):
     def __init__(self, *args, **kwargs):
         StacktraceProcessor.__init__(self, *args, **kwargs)
         debug_meta = self.data.get('debug_meta')
-        self.cpu_name = cpu_name_from_data(self.data)
+        self.arch = cpu_name_from_data(self.data)
         self.sym = None
         self.dsyms_referenced = set()
         if debug_meta:
             self.available = True
             self.debug_meta = debug_meta
             self.sdk_info = get_sdk_from_event(self.data)
-            self.image_lookup = ImageLookup(
+            self.object_lookup = ObjectLookup(
                 [img for img in self.debug_meta['images'] if img['type'] == 'apple']
             )
         else:
@@ -46,21 +48,21 @@ class NativeStacktraceProcessor(StacktraceProcessor):
             metrics.incr(
                 'dsyms.processed', amount=len(self.dsyms_referenced), instance=self.project.id
             )
-        if self.sym is not None:
-            self.sym.close()
-            self.sym = None
 
     def find_best_instruction(self, processable_frame):
         """Given a frame, stacktrace info and frame index this returns the
         interpolated instruction address we then use for symbolication later.
         """
-        if self.cpu_name is None:
+        if self.arch is None:
             return parse_addr(processable_frame['instruction_addr'])
-        meta = None
+
+        crashing_frame = False
+        signal = None
+        ip_reg = None
 
         # We only need to provide meta information for frame zero
         if processable_frame.idx == 0:
-            # The signal is useful information for symsynd in some situations
+            # The signal is useful information for symbolic in some situations
             # to disambiugate the first frame.  If we can get this information
             # from the mechanism we want to pass it onwards.
             signal = None
@@ -70,14 +72,19 @@ class NativeStacktraceProcessor(StacktraceProcessor):
                 if mechanism and 'posix_signal' in mechanism and \
                    'signal' in mechanism['posix_signal']:
                     signal = mechanism['posix_signal']['signal']
-            meta = {
-                'frame_number': 0,
-                'registers': processable_frame.stacktrace_info.stacktrace.get('registers'),
-                'signal': signal,
-            }
+            registers = processable_frame.stacktrace_info.stacktrace.get('registers')
+            if registers:
+                ip_reg_name = arch_get_ip_reg_name(self.arch)
+                if ip_reg_name:
+                    ip_reg = registers.get(ip_reg_name)
+            crashing_frame = True
 
         return find_best_instruction(
-            processable_frame['instruction_addr'], self.cpu_name, meta=meta
+            processable_frame['instruction_addr'],
+            arch=self.arch,
+            crashing_frame=crashing_frame,
+            signal=signal,
+            ip_reg=ip_reg
         )
 
     def handles_frame(self, frame, stacktrace_info):
@@ -86,27 +93,26 @@ class NativeStacktraceProcessor(StacktraceProcessor):
 
     def preprocess_frame(self, processable_frame):
         instr_addr = self.find_best_instruction(processable_frame)
-        img = self.image_lookup.find_image(instr_addr)
+        obj = self.object_lookup.find_object(instr_addr)
 
         processable_frame.data = {
             'instruction_addr': instr_addr,
-            'image': img,
-            'image_uuid': img['uuid'] if img is not None else None,
+            'obj': obj,
+            'obj_uuid': six.text_type(obj.uuid) if obj is not None else None,
             'symbolserver_match': None,
         }
 
-        if img is not None:
+        if obj is not None:
             processable_frame.set_cache_key_from_values(
                 (
                     FRAME_CACHE_VERSION,
                     # Because the images can move around, we want to rebase
                     # the address for the cache key to be within the image
                     # the same way as we do it in the symbolizer.
-                    rebase_addr(instr_addr, img),
-                    img['uuid'].lower(),
-                    img['cpu_type'],
-                    img['cpu_subtype'],
-                    img['image_size'],
+                    rebase_addr(instr_addr, obj),
+                    six.text_type(obj.uuid),
+                    obj.arch,
+                    obj.size,
                 )
             )
 
@@ -115,19 +121,20 @@ class NativeStacktraceProcessor(StacktraceProcessor):
             return False
 
         referenced_images = set(
-            pf.data['image_uuid'] for pf in processing_task.iter_processable_frames(self)
-            if pf.cache_value is None and pf.data['image_uuid'] is not None
+            pf.data['obj_uuid'] for pf in processing_task.iter_processable_frames(self)
+            if pf.cache_value is None and pf.data['obj_uuid'] is not None
         )
 
-        def on_referenced(dsym_file):
-            app_info = version_build_from_data(self.data)
-            if app_info is not None:
+        app_info = version_build_from_data(self.data)
+        if app_info is not None:
+            def on_referenced(dsym_file):
                 dsym_app = DSymApp.objects.create_or_update_app(
                     sync_id=None,
                     app_id=app_info.id,
                     project=self.project,
                     data={'name': app_info.name},
                     platform=DSymPlatform.APPLE,
+                    no_fetch=True
                 )
                 try:
                     with transaction.atomic():
@@ -142,11 +149,13 @@ class NativeStacktraceProcessor(StacktraceProcessor):
                     # support one app per dsym file.  Since this can
                     # happen in some cases anyways we ignore it.
                     pass
+        else:
+            on_referenced = None
 
         self.sym = Symbolizer(
             self.project,
-            self.image_lookup,
-            cpu_name=self.cpu_name,
+            self.object_lookup,
+            arch=self.arch,
             referenced_images=referenced_images,
             on_dsym_file_referenced=on_referenced
         )
@@ -158,15 +167,15 @@ class NativeStacktraceProcessor(StacktraceProcessor):
         to_lookup = []
         pf_list = []
         for pf in processing_task.iter_processable_frames(self):
-            img = pf.data['image']
-            if pf.cache_value is not None or img is None or \
-               self.sym.is_image_from_app_bundle(img):
+            obj = pf.data['obj']
+            if pf.cache_value is not None or obj is None or \
+               self.sym.is_image_from_app_bundle(obj):
                 continue
             to_lookup.append(
                 {
-                    'object_uuid': img['uuid'],
-                    'object_name': img['name'],
-                    'addr': '0x%x' % rebase_addr(pf.data['instruction_addr'], img)
+                    'object_uuid': six.text_type(obj.uuid),
+                    'object_name': obj.name or '<unknown>',
+                    'addr': '0x%x' % rebase_addr(pf.data['instruction_addr'], obj)
                 }
             )
             pf_list.append(pf)
@@ -174,7 +183,7 @@ class NativeStacktraceProcessor(StacktraceProcessor):
         if not to_lookup:
             return
 
-        rv = lookup_system_symbols(to_lookup, self.sdk_info, self.sym.cpu_name)
+        rv = lookup_system_symbols(to_lookup, self.sdk_info, self.sym.arch)
         if rv is not None:
             for symrv, pf in zip(rv, pf_list):
                 if symrv is None:
@@ -196,9 +205,9 @@ class NativeStacktraceProcessor(StacktraceProcessor):
                 in_app = not self.sym.is_internal_function(raw_frame['function'])
             if raw_frame.get('in_app') is None:
                 raw_frame['in_app'] = in_app
-            img_uuid = processable_frame.data['image_uuid']
-            if img_uuid is not None:
-                self.dsyms_referenced.add(img_uuid)
+            obj_uuid = processable_frame.data['obj_uuid']
+            if obj_uuid is not None:
+                self.dsyms_referenced.add(obj_uuid)
             try:
                 symbolicated_frames = self.sym.symbolize_frame(
                     instruction_addr,
@@ -216,12 +225,7 @@ class NativeStacktraceProcessor(StacktraceProcessor):
                         scope='native',
                         object='dsym:%s' % e.image_uuid,
                         type=e.type,
-                        data={
-                            'image_path': e.image_path,
-                            'image_uuid': e.image_uuid,
-                            'image_arch': e.image_arch,
-                            'message': e.message,
-                        }
+                        data=e.get_data()
                     )
 
                 # This in many ways currently does not really do anything.
diff --git a/src/sentry/lang/native/symbolizer.py b/src/sentry/lang/native/symbolizer.py
index 344cfc4445..5b199e819c 100644
--- a/src/sentry/lang/native/symbolizer.py
+++ b/src/sentry/lang/native/symbolizer.py
@@ -3,12 +3,12 @@ from __future__ import absolute_import
 import re
 import six
 
-from symsynd import demangle_symbol, SymbolicationError, get_cpu_name, \
-    ImageLookup, Symbolizer as SymsyndSymbolizer
+from symbolic import SymbolicError, ObjectLookup, Symbol, parse_addr
 
 from sentry.utils.safe import trim
 from sentry.utils.compat import implements_to_string
 from sentry.models import EventError, ProjectDSymFile
+from sentry.lang.native.utils import rebase_addr
 from sentry.constants import MAX_SYM, NATIVE_UNKNOWN_STRING
 
 FATAL_ERRORS = (EventError.NATIVE_MISSING_DSYM, EventError.NATIVE_BAD_DSYM, )
@@ -45,19 +45,20 @@ KNOWN_GARBAGE_SYMBOLS = set([
 class SymbolicationFailed(Exception):
     message = None
 
-    def __init__(self, message=None, type=None, image=None):
+    def __init__(self, message=None, type=None, obj=None):
         Exception.__init__(self)
         self.message = six.text_type(message)
         self.type = type
-        if image is not None:
-            self.image_uuid = image['uuid'].lower()
-            self.image_path = image['name']
-            self.image_name = image['name'].rsplit('/', 1)[-1]
-            self.image_arch = get_cpu_name(image['cpu_type'], image['cpu_subtype'])
+        self.image_name = None
+        self.image_path = None
+        if obj is not None:
+            self.image_uuid = six.text_type(obj.uuid)
+            if obj.name:
+                self.image_path = obj.name
+                self.image_name = obj.name.rsplit('/', 1)[-1]
+            self.image_arch = obj.arch
         else:
             self.image_uuid = None
-            self.image_name = None
-            self.image_path = None
             self.image_arch = None
 
     @property
@@ -75,6 +76,17 @@ class SymbolicationFailed(Exception):
         """An error that most likely happened because of a bad SDK."""
         return self.type == EventError.NATIVE_UNKNOWN_IMAGE
 
+    def get_data(self):
+        """Returns the event data."""
+        rv = {'message': self.message}
+        if self.image_path is not None:
+            rv['image_path'] = self.image_path
+        if self.image_uuid is not None:
+            rv['image_uuid'] = self.image_uuid
+        if self.image_arch is not None:
+            rv['image_arch'] = self.image_arch
+        return rv
+
     def __str__(self):
         rv = []
         if self.type is not None:
@@ -88,41 +100,30 @@ class SymbolicationFailed(Exception):
 
 
 class Symbolizer(object):
-    """This symbolizer dispatches to both symsynd and the system symbols
+    """This symbolizer dispatches to both symbolic and the system symbols
     we have in the database and reports errors slightly differently.
     """
 
-    def __init__(
-        self,
-        project,
-        binary_images,
-        referenced_images=None,
-        cpu_name=None,
-        on_dsym_file_referenced=None
-    ):
-        if isinstance(binary_images, ImageLookup):
-            self.image_lookup = binary_images
-        else:
-            self.image_lookup = ImageLookup(binary_images)
-
-        self._symbolizer = SymsyndSymbolizer()
-
-        to_load = referenced_images
-        if to_load is None:
-            to_load = self.image_lookup.get_uuids()
-
-        self.dsym_paths = ProjectDSymFile.dsymcache.fetch_dsyms(
-            project, to_load, on_dsym_file_referenced=on_dsym_file_referenced
-        )
+    def __init__(self, project, object_lookup, referenced_images,
+                 arch=None, on_dsym_file_referenced=None):
+        if not isinstance(object_lookup, ObjectLookup):
+            object_lookup = ObjectLookup(object_lookup)
+        self.object_lookup = object_lookup
 
-        self.cpu_name = cpu_name
+        self.symcaches = ProjectDSymFile.dsymcache.get_symcaches(
+            project, referenced_images,
+            on_dsym_file_referenced=on_dsym_file_referenced)
 
-    def close(self):
-        self._symbolizer.close()
+        self.arch = arch
 
-    def _process_frame(self, frame, img):
-        symbol = trim(frame['symbol'], MAX_SYM)
-        function = trim(demangle_symbol(frame['symbol'], simplified=True), MAX_SYM)
+    def _process_frame(self, sym, obj, package=None, addr_off=0):
+        frame = {
+            'sym_addr': sym.sym_addr + addr_off,
+            'instruction_addr': sym.instr_addr + addr_off,
+            'lineno': sym.line,
+        }
+        symbol = trim(sym.symbol, MAX_SYM)
+        function = trim(sym.function_name, MAX_SYM)
 
         frame['function'] = function
         if function != symbol:
@@ -130,13 +131,17 @@ class Symbolizer(object):
         else:
             frame['symbol'] = None
 
-        frame['filename'] = trim(frame.get('filename'), 256)
-        frame['abs_path'] = trim(frame.get('abs_path'), 256)
+        frame['filename'] = trim(sym.rel_path, 256)
+        frame['abs_path'] = trim(sym.abs_path, 256)
+        if package is not None:
+            frame['package'] = package
 
         return frame
 
-    def is_image_from_app_bundle(self, img, sdk_info=None):
-        fn = img['name']
+    def is_image_from_app_bundle(self, obj, sdk_info=None):
+        fn = obj.name
+        if not fn:
+            return False
         is_mac_platform = (sdk_info is not None and sdk_info['sdk_name'].lower() == 'macos')
         if not (
             fn.startswith(APP_BUNDLE_PATHS) or (SIM_PATH in fn and SIM_APP_PATH in fn) or
@@ -145,90 +150,82 @@ class Symbolizer(object):
             return False
         return True
 
-    def _is_support_framework(self, img):
+    def _is_support_framework(self, obj):
         """True if the frame is from a framework that is known and app
         bundled.  Those are frameworks which are specifically not frameworks
         that are ever in_app.
         """
-        return _support_framework.search(img['name']) is not None
+        return obj.name and _support_framework.search(obj.name) is not None
 
-    def _is_app_bundled_framework(self, img):
-        fn = img['name']
-        return fn.startswith(APP_BUNDLE_PATHS) and '/Frameworks/' in fn
+    def _is_app_bundled_framework(self, obj):
+        fn = obj.name
+        return fn and fn.startswith(APP_BUNDLE_PATHS) and '/Frameworks/' in fn
 
-    def _is_app_frame(self, instruction_addr, img, sdk_info=None):
+    def _is_app_frame(self, instruction_addr, obj, sdk_info=None):
         """Given a frame derives the value of `in_app` by discarding the
         original value of the frame.
         """
         # Anything that is outside the app bundle is definitely not a
         # frame from out app.
-        if not self.is_image_from_app_bundle(img, sdk_info=sdk_info):
+        if not self.is_image_from_app_bundle(obj, sdk_info=sdk_info):
             return False
 
         # We also do not consider known support frameworks to be part of
         # the app
-        if self._is_support_framework(img):
+        if self._is_support_framework(obj):
             return False
 
         # Otherwise, yeah, let's just say it's in_app
         return True
 
-    def _is_optional_dsym(self, img, sdk_info=None):
+    def _is_optional_dsym(self, obj, sdk_info=None):
         """Checks if this is a dsym that is optional."""
         # Frames that are not in the app are not considered optional.  In
         # theory we should never reach this anyways.
-        if not self.is_image_from_app_bundle(img, sdk_info=sdk_info):
+        if not self.is_image_from_app_bundle(obj, sdk_info=sdk_info):
             return False
 
         # If we're dealing with an app bundled framework that is also
         # considered optional.
-        if self._is_app_bundled_framework(img):
+        if self._is_app_bundled_framework(obj):
             return True
 
         # Frameworks that are known to sentry and bundled helpers are always
         # optional for now.  In theory this should always be False here
         # because we should catch it with the last branch already.
-        if self._is_support_framework(img):
+        if self._is_support_framework(obj):
             return True
 
         return False
 
-    def _is_simulator_frame(self, frame, img):
-        return _sim_platform_re.search(img['name']) is not None
+    def _is_simulator_frame(self, frame, obj):
+        return obj.name and _sim_platform_re.search(obj.name) is not None
 
-    def _symbolize_app_frame(self, instruction_addr, img, sdk_info=None):
-        dsym_path = self.dsym_paths.get(img['uuid'])
-        if dsym_path is None:
-            if self._is_optional_dsym(img, sdk_info=sdk_info):
+    def _symbolize_app_frame(self, instruction_addr, obj, sdk_info=None):
+        symcache = self.symcaches.get(obj.uuid)
+        if symcache is None:
+            if self._is_optional_dsym(obj, sdk_info=sdk_info):
                 type = EventError.NATIVE_MISSING_OPTIONALLY_BUNDLED_DSYM
             else:
                 type = EventError.NATIVE_MISSING_DSYM
-            raise SymbolicationFailed(type=type, image=img)
-
-        # cputype of image might be a variation of self.cpu_name
-        # e.g.: armv7 instead of armv7f
-        # (example error fat file does not contain armv7f)
-        cpu_name = get_cpu_name(img['cpu_type'], img['cpu_subtype'])
+            raise SymbolicationFailed(type=type, obj=obj)
 
         try:
-            rv = self._symbolizer.symbolize(
-                dsym_path,
-                img['image_vmaddr'],
-                img['image_addr'],
-                instruction_addr,
-                cpu_name,
-                symbolize_inlined=True
-            )
-        except SymbolicationError as e:
+            rv = symcache.lookup(rebase_addr(instruction_addr, obj))
+        except SymbolicError as e:
             raise SymbolicationFailed(
-                type=EventError.NATIVE_BAD_DSYM, message=six.text_type(e), image=img
+                type=EventError.NATIVE_BAD_DSYM, message=six.text_type(e), obj=obj
             )
 
         if not rv:
-            raise SymbolicationFailed(type=EventError.NATIVE_MISSING_SYMBOL, image=img)
-        return [self._process_frame(nf, img) for nf in reversed(rv)]
-
-    def _convert_symbolserver_match(self, instruction_addr, symbolserver_match, img):
+            # For some frameworks we are willing to ignore missing symbol
+            # errors.
+            if self._is_optional_dsym(obj, sdk_info=sdk_info):
+                return []
+            raise SymbolicationFailed(type=EventError.NATIVE_MISSING_SYMBOL, obj=obj)
+        return [self._process_frame(s, obj, addr_off=obj.addr) for s in reversed(rv)]
+
+    def _convert_symbolserver_match(self, instruction_addr, symbolserver_match, obj):
         """Symbolizes a frame with system symbols only."""
         if symbolserver_match is None:
             return []
@@ -238,41 +235,37 @@ class Symbolizer(object):
             symbol = symbol[1:]
 
         return [
-            self._process_frame(
-                dict(
-                    symbol=symbol,
-                    filename=None,
-                    abs_path=None,
-                    lineno=0,
-                    colno=0,
-                    package=symbolserver_match['object_name']
-                ), img
-            )
+            self._process_frame(Symbol(
+                sym_addr=parse_addr(symbolserver_match['addr']),
+                instr_addr=parse_addr(instruction_addr),
+                line=None,
+                symbol=symbol,
+            ), obj, package=symbolserver_match['object_name'])
         ]
 
     def symbolize_frame(self, instruction_addr, sdk_info=None, symbolserver_match=None):
         # If we do not have a CPU name we fail.  We currently only support
         # a single cpu architecture.
-        if self.cpu_name is None:
+        if self.arch is None:
             raise SymbolicationFailed(
                 type=EventError.NATIVE_INTERNAL_FAILURE, message='Found multiple architectures.'
             )
 
-        img = self.image_lookup.find_image(instruction_addr)
-        if img is None:
+        obj = self.object_lookup.find_object(instruction_addr)
+        if obj is None:
             raise SymbolicationFailed(type=EventError.NATIVE_UNKNOWN_IMAGE)
 
         # If we are dealing with a frame that is not bundled with the app
         # we look at system symbols.  If that fails, we go to looking for
         # app symbols explicitly.
-        if not self.is_image_from_app_bundle(img, sdk_info=sdk_info):
-            return self._convert_symbolserver_match(instruction_addr, symbolserver_match, img)
+        if not self.is_image_from_app_bundle(obj, sdk_info=sdk_info):
+            return self._convert_symbolserver_match(instruction_addr, symbolserver_match, obj)
 
-        return self._symbolize_app_frame(instruction_addr, img, sdk_info=sdk_info)
+        return self._symbolize_app_frame(instruction_addr, obj, sdk_info=sdk_info)
 
     def is_in_app(self, instruction_addr, sdk_info=None):
-        img = self.image_lookup.find_image(instruction_addr)
-        return img is not None and self._is_app_frame(instruction_addr, img, sdk_info=sdk_info)
+        obj = self.object_lookup.find_object(instruction_addr)
+        return obj is not None and self._is_app_frame(instruction_addr, obj, sdk_info=sdk_info)
 
     def is_internal_function(self, function):
         return _internal_function_re.search(function) is not None
diff --git a/src/sentry/lang/native/utils.py b/src/sentry/lang/native/utils.py
index 78a8add8eb..9ebe11a360 100644
--- a/src/sentry/lang/native/utils.py
+++ b/src/sentry/lang/native/utils.py
@@ -4,7 +4,7 @@ import six
 import logging
 
 from collections import namedtuple
-from symsynd import get_cpu_name, parse_addr
+from symbolic import parse_addr, arch_from_macho, arch_is_known
 
 from sentry.interfaces.contexts import DeviceContextType
 
@@ -92,7 +92,13 @@ def cpu_name_from_data(data):
     unique_cpu_name = None
     images = (data.get('debug_meta') or {}).get('images') or []
     for img in images:
-        cpu_name = get_cpu_name(img['cpu_type'], img['cpu_subtype'])
+        if img.get('arch') and arch_is_known(img['arch']):
+            cpu_name = img['arch']
+        elif img.get('cpu_type') is not None \
+                and img.get('cpu_subtype') is not None:
+            cpu_name = arch_from_macho(img['cpu_type'], img['cpu_subtype'])
+        else:
+            cpu_name = None
         if unique_cpu_name is None:
             unique_cpu_name = cpu_name
         elif unique_cpu_name != cpu_name:
@@ -119,8 +125,8 @@ def version_build_from_data(data):
     return None
 
 
-def rebase_addr(instr_addr, img):
-    return parse_addr(instr_addr) - parse_addr(img['image_addr'])
+def rebase_addr(instr_addr, obj):
+    return parse_addr(instr_addr) - parse_addr(obj.addr)
 
 
 def sdk_info_to_sdk_id(sdk_info):
diff --git a/src/sentry/models/dsymfile.py b/src/sentry/models/dsymfile.py
index 9f39ffff9c..d7c38a07de 100644
--- a/src/sentry/models/dsymfile.py
+++ b/src/sentry/models/dsymfile.py
@@ -16,6 +16,7 @@ import time
 import errno
 import shutil
 import hashlib
+import logging
 import tempfile
 from requests.exceptions import RequestException
 
@@ -24,7 +25,8 @@ from django.db import models, transaction, IntegrityError
 from django.utils import timezone
 from django.utils.translation import ugettext_lazy as _
 
-from symsynd import DebugInfo, DebugInfoError
+from symbolic import FatObject, SymbolicError, UnsupportedObjectFile, \
+    SymCache, SYMCACHE_LATEST_VERSION
 
 from sentry import options
 from sentry.db.models import FlexibleForeignKey, Model, \
@@ -34,6 +36,10 @@ from sentry.utils.zip import safe_extract_zip
 from sentry.constants import KNOWN_DSYM_TYPES
 from sentry.reprocessing import resolve_processing_issue
 
+
+logger = logging.getLogger(__name__)
+
+
 ONE_DAY = 60 * 60 * 24
 ONE_DAY_AND_A_HALF = int(ONE_DAY * 1.5)
 DSYM_MIMETYPES = dict((v, k) for k, v in KNOWN_DSYM_TYPES.items())
@@ -93,11 +99,13 @@ def _auto_enrich_data(data, app_id, platform):
 
 class DSymAppManager(BaseManager):
     def create_or_update_app(
-        self, sync_id, app_id, project, data=None, platform=DSymPlatform.GENERIC
+        self, sync_id, app_id, project, data=None, platform=DSymPlatform.GENERIC,
+        no_fetch=False
     ):
         if data is None:
             data = {}
-        _auto_enrich_data(data, app_id, platform)
+        if not no_fetch:
+            _auto_enrich_data(data, app_id, platform)
         existing_app = DSymApp.objects.filter(app_id=app_id, project=project).first()
         if existing_app is not None:
             now = timezone.now()
@@ -183,6 +191,11 @@ class ProjectDSymFile(Model):
         ct = self.file.headers.get('Content-Type').lower()
         return KNOWN_DSYM_TYPES.get(ct, 'unknown')
 
+    @property
+    def supports_symcache(self):
+        # Only one that supports it so far.
+        return self.dsym_type == 'macho'
+
     def delete(self, *args, **kwargs):
         super(ProjectDSymFile, self).delete(*args, **kwargs)
         self.file.delete()
@@ -283,7 +296,8 @@ def _analyze_progard_filename(filename):
         pass
 
 
-def create_files_from_dsym_zip(fileobj, project=None):
+def create_files_from_dsym_zip(fileobj, project,
+                               update_symcaches=True):
     """Creates all missing dsym files from the given zip file.  This
     returns a list of all files created.
     """
@@ -301,19 +315,22 @@ def create_files_from_dsym_zip(fileobj, project=None):
                 proguard_uuid = _analyze_progard_filename(fn)
                 if proguard_uuid is not None:
                     to_create.append(('proguard', 'any', six.text_type(proguard_uuid), fn, ))
+                    continue
 
                 # macho style debug symbols
                 try:
-                    di = DebugInfo.open_path(fn)
-                except DebugInfoError:
+                    fo = FatObject.from_path(fn)
+                except UnsupportedObjectFile:
+                    pass
+                except SymbolicError:
                     # Whatever was contained there, was probably not a
                     # macho file.
-                    pass
+                    # XXX: log?
+                    logger.warning('dsymfile.bad-fat-object', exc_info=True)
                 else:
-                    for variant in di.get_variants():
-                        to_create.append(
-                            ('macho', variant.cpu_name, six.text_type(variant.uuid), fn, )
-                        )
+                    for obj in fo.iter_objects():
+                        to_create.append((obj.kind, obj.arch,
+                                          six.text_type(obj.uuid), fn))
                     continue
 
         rv = []
@@ -324,6 +341,17 @@ def create_files_from_dsym_zip(fileobj, project=None):
                 )
                 if created:
                     rv.append(dsym)
+
+        # By default we trigger the symcache generation on upload to avoid
+        # some obvious dogpiling.
+        if update_symcaches:
+            from sentry.tasks.symcache_update import symcache_update
+            uuids_to_update = [six.text_type(x.uuid) for x in rv
+                               if x.supports_symcache]
+            if uuids_to_update:
+                symcache_update.delay(project_id=project.id,
+                                      uuids=uuids_to_update)
+
         return rv
     finally:
         shutil.rmtree(scratchpad)
@@ -342,80 +370,168 @@ def find_dsym_file(project, image_uuid):
 
 class DSymCache(object):
     @property
-    def dsym_cache_path(self):
+    def cache_path(self):
         return options.get('dsym.cache-path')
 
     def get_project_path(self, project):
-        return os.path.join(self.dsym_cache_path, six.text_type(project.id))
-
-    def fetch_dsyms(self, project, uuids, on_dsym_file_referenced=None):
+        return os.path.join(self.cache_path, six.text_type(project.id))
+
+    def update_symcaches(self, project, uuids):
+        """Given some uuids of dsyms this will update the symcaches for
+        all of these if a symcache is supported for that symbol.
+        """
+        self._get_symcaches_impl(project, uuids)
+
+    def get_symcaches(self, project, uuids, on_dsym_file_referenced=None):
+        """Given some uuids returns the symcaches loaded for these uuids."""
+        cachefiles = self._get_symcaches_impl(project, uuids,
+                                              on_dsym_file_referenced)
+        return self._load_cachefiles_via_fs(project, cachefiles)
+
+    def fetch_dsyms(self, project, uuids):
+        """Given some uuids returns a uuid to path mapping for where the
+        debug symbol files are on the FS.
+        """
         rv = {}
         for image_uuid in uuids:
-            path = self.fetch_dsym(
-                project, image_uuid, on_dsym_file_referenced=on_dsym_file_referenced
-            )
-            if path is not None:
-                rv[image_uuid] = path
+            image_uuid = six.text_type(image_uuid).lower()
+            dsym_path = os.path.join(self.get_project_path(project), image_uuid)
+
+            try:
+                os.stat(dsym_path)
+            except OSError as e:
+                if e.errno != errno.ENOENT:
+                    raise
+                dsym_file = find_dsym_file(project, image_uuid)
+                if dsym_file is None:
+                    continue
+                dsym_file.file.save_to(dsym_path)
+            rv[uuid.UUID(image_uuid)] = dsym_path
+
         return rv
 
-    def try_bump_timestamp(self, path, old_stat):
-        now = int(time.time())
-        if old_stat.st_ctime < now - ONE_DAY:
-            os.utime(path, (now, now))
-        return path
+    def _get_symcaches_impl(self, project, uuids, on_dsym_file_referenced=None):
+        # Fetch dsym files first and invoke the callback if we need
+        uuid_strings = list(map(six.text_type, uuids))
+        dsym_files = [x for x in ProjectDSymFile.objects.filter(
+            project=project,
+            uuid__in=uuid_strings,
+        ).select_related('file') if x.supports_symcache]
+        if not dsym_files:
+            return {}
+
+        dsym_files_by_uuid = {}
+        for dsym_file in dsym_files:
+            if on_dsym_file_referenced is not None:
+                on_dsym_file_referenced(dsym_file)
+            dsym_files_by_uuid[dsym_file.uuid] = dsym_file
+
+        # Now find all the cache files we already have.
+        q = ProjectSymCacheFile.objects.filter(
+            project=project,
+            dsym_file_id__in=[x.id for x in dsym_files],
+        ).select_related('cache_file', 'dsym_file__uuid')
+
+        cachefiles = []
+        cachefiles_to_update = dict.fromkeys(x.uuid for x in dsym_files)
+        for cache_file in q:
+            dsym_uuid = cache_file.dsym_file.uuid
+            dsym_file = dsym_files_by_uuid[dsym_uuid]
+            if cache_file.version == SYMCACHE_LATEST_VERSION and \
+               cache_file.checksum == dsym_file.file.checksum:
+                cachefiles_to_update.pop(dsym_uuid, None)
+                cachefiles.append((dsym_uuid, cache_file))
+            else:
+                cachefiles_to_update[dsym_uuid] = \
+                    (cache_file, dsym_file)
+
+        # if any cache files need to be updated, do that now.
+        if cachefiles_to_update:
+            to_update = []
+            for dsym_uuid, it in six.iteritems(cachefiles_to_update):
+                if it is None:
+                    dsym_file = dsym_files_by_uuid[dsym_uuid]
+                else:
+                    cache_file, dsym_file = it
+                    cache_file.delete()
+                to_update.append(dsym_file)
+            cachefiles.extend(self._update_cachefiles(project, to_update))
 
-    def fetch_dsym(self, project, image_uuid, on_dsym_file_referenced=None):
-        image_uuid = image_uuid.lower()
-        dsym_path = os.path.join(self.get_project_path(project), image_uuid)
-        try:
-            os.stat(dsym_path)
-        except OSError as e:
-            if e.errno != errno.ENOENT:
-                raise
-        else:
-            return dsym_path
+        return cachefiles
 
-        dsf = find_dsym_file(project, image_uuid)
-        if dsf is None:
-            return None
+    def _update_cachefiles(self, project, dsym_files):
+        rv = []
 
-        if on_dsym_file_referenced is not None:
-            on_dsym_file_referenced(dsf)
+        for dsym_file in dsym_files:
+            dsym_uuid = dsym_file.uuid
+            try:
+                with dsym_file.file.getfile(as_tempfile=True) as tf:
+                    fo = FatObject.from_path(tf.name)
+                    o = fo.get_object(uuid=dsym_file.uuid)
+                    if o is None:
+                        continue
+                    cache = o.make_symcache()
+            except SymbolicError:
+                logger.error('dsymfile.symcache-build-error',
+                             exc_info=True, extra=dict(dsym_uuid=dsym_uuid))
+                continue
 
-        try:
-            os.makedirs(os.path.dirname(dsym_path))
-        except OSError:
-            pass
+            file = File.objects.create(
+                name=dsym_file.uuid,
+                type='project.symcache',
+            )
+            file.putfile(cache.open_stream())
+            try:
+                with transaction.atomic():
+                    rv.append((dsym_uuid, ProjectSymCacheFile.objects.get_or_create(
+                        project=project,
+                        cache_file=file,
+                        dsym_file=dsym_file,
+                        defaults=dict(
+                            checksum=dsym_file.file.checksum,
+                            version=cache.file_format_version,
+                        )
+                    )[0]))
+            except IntegrityError:
+                file.delete()
+                rv.append((dsym_uuid, ProjectSymCacheFile.objects.get(
+                    project=project,
+                    dsym_file=dsym_file,
+                )))
+
+        return rv
 
-        with dsf.file.getfile() as sf:
-            suffix = '_%s' % uuid.uuid4()
-            done = False
+    def _load_cachefiles_via_fs(self, project, cachefiles):
+        rv = {}
+        base = self.get_project_path(project)
+        for dsym_uuid, symcache_file in cachefiles:
+            cachefile_path = os.path.join(base, dsym_uuid + '.symcache')
             try:
-                with open(dsym_path + suffix, 'w') as df:
-                    shutil.copyfileobj(sf, df)
-                os.rename(dsym_path + suffix, dsym_path)
-                done = True
-            finally:
-                # Use finally here because it does not lie about the
-                # error on exit
-                if not done:
-                    try:
-                        os.remove(dsym_path + suffix)
-                    except Exception:
-                        pass
+                stat = os.stat(cachefile_path)
+            except OSError as e:
+                if e.errno != errno.ENOENT:
+                    raise
+                symcache_file.cache_file.save_to(cachefile_path)
+            else:
+                self._try_bump_timestamp(cachefile_path, stat)
+            rv[uuid.UUID(dsym_uuid)] = SymCache.from_path(cachefile_path)
+        return rv
 
-        return dsym_path
+    def _try_bump_timestamp(self, path, old_stat):
+        now = int(time.time())
+        if old_stat.st_ctime < now - ONE_DAY:
+            os.utime(path, (now, now))
 
     def clear_old_entries(self):
         try:
-            cache_folders = os.listdir(self.dsym_cache_path)
+            cache_folders = os.listdir(self.cache_path)
         except OSError:
             return
 
         cutoff = int(time.time()) - ONE_DAY_AND_A_HALF
 
         for cache_folder in cache_folders:
-            cache_folder = os.path.join(self.dsym_cache_path, cache_folder)
+            cache_folder = os.path.join(self.cache_path, cache_folder)
             try:
                 items = os.listdir(cache_folder)
             except OSError:
diff --git a/src/sentry/models/eventerror.py b/src/sentry/models/eventerror.py
index f7041bf911..6d44348c5f 100644
--- a/src/sentry/models/eventerror.py
+++ b/src/sentry/models/eventerror.py
@@ -44,7 +44,6 @@ class EventError(object):
     FETCH_TIMEOUT = 'fetch_timeout'
     NATIVE_NO_CRASHED_THREAD = 'native_no_crashed_thread'
     NATIVE_INTERNAL_FAILURE = 'native_internal_failure'
-    NATIVE_NO_SYMSYND = 'native_no_symsynd'
     NATIVE_BAD_DSYM = 'native_bad_dsym'
     NATIVE_MISSING_OPTIONALLY_BUNDLED_DSYM = 'native_optionally_bundled_dsym'
     NATIVE_MISSING_DSYM = 'native_missing_dsym'
@@ -84,7 +83,6 @@ class EventError(object):
         FETCH_TIMEOUT: u'Remote file took too long to load: ({timeout}s, {url})',
         NATIVE_NO_CRASHED_THREAD: u'No crashed thread found in crash report',
         NATIVE_INTERNAL_FAILURE: u'Internal failure when attempting to symbolicate: {error}',
-        NATIVE_NO_SYMSYND: u'The symbolizer is not configured for this system.',
         NATIVE_BAD_DSYM: u'The debug symbol file used was broken.',
         NATIVE_MISSING_OPTIONALLY_BUNDLED_DSYM: u'An optional debug symbol file was missing.',
         NATIVE_MISSING_DSYM: u'A required debug symbol file was missing.',
diff --git a/src/sentry/models/file.py b/src/sentry/models/file.py
index 61a6a42da7..c433c0ce79 100644
--- a/src/sentry/models/file.py
+++ b/src/sentry/models/file.py
@@ -8,8 +8,9 @@ sentry.models.file
 
 from __future__ import absolute_import
 
+import os
 import six
-import shutil
+import mmap
 import tempfile
 
 from hashlib import sha1
@@ -163,15 +164,61 @@ class File(Model):
         app_label = 'sentry'
         db_table = 'sentry_file'
 
-    def getfile(self, mode=None, prefetch=False):
-        cfbiw = ChunkedFileBlobIndexWrapper(
+    def _get_chunked_blob(self, mode=None, prefetch=False,
+                          prefetch_to=None, delete=True):
+        return ChunkedFileBlobIndexWrapper(
             FileBlobIndex.objects.filter(
                 file=self,
             ).select_related('blob').order_by('offset'),
             mode=mode,
-            prefetch=prefetch
+            prefetch=prefetch,
+            prefetch_to=prefetch_to,
+            delete=delete
         )
-        return FileObj(cfbiw, self.name)
+
+    def getfile(self, mode=None, prefetch=False, as_tempfile=False):
+        """Returns a file object.  By default the file is fetched on
+        demand but if prefetch is enabled the file is fully prefetched
+        into a tempfile before reading can happen.
+
+        Additionally if `as_tempfile` is passed a NamedTemporaryFile is
+        returned instead which can help in certain situations where a
+        tempfile is necessary.
+        """
+        if as_tempfile:
+            prefetch = True
+        impl = self._get_chunked_blob(mode, prefetch)
+        if as_tempfile:
+            return impl.detach_tempfile()
+        return FileObj(impl, self.name)
+
+    def save_to(self, path):
+        """Fetches the file and emplaces it at a certain location.  The
+        write is done atomically to a tempfile first and then moved over.
+        If the directory does not exist it is created.
+        """
+        path = os.path.abspath(path)
+        base = os.path.dirname(path)
+        try:
+            os.makedirs(base)
+        except OSError:
+            pass
+
+        f = None
+        try:
+            f = self._get_chunked_blob(prefetch=True,
+                                       prefetch_to=base,
+                                       delete=False).detach_tempfile()
+            os.rename(f.name, path)
+            f.close()
+            f = None
+        finally:
+            if f is not None:
+                f.close()
+                try:
+                    os.remove(f.name)
+                except Exception:
+                    pass
 
     def putfile(self, fileobj, blob_size=DEFAULT_BLOB_SIZE, commit=True):
         """
@@ -222,14 +269,15 @@ class FileBlobIndex(Model):
 
 
 class ChunkedFileBlobIndexWrapper(object):
-    def __init__(self, indexes, mode=None, prefetch=False):
+    def __init__(self, indexes, mode=None, prefetch=False,
+                 prefetch_to=None, delete=True):
         # eager load from database incase its a queryset
         self._indexes = list(indexes)
         self._curfile = None
         self._curidx = None
         if prefetch:
             self.prefetched = True
-            self._prefetch()
+            self._prefetch(prefetch_to, delete)
         else:
             self.prefetched = False
         self.mode = mode
@@ -241,6 +289,15 @@ class ChunkedFileBlobIndexWrapper(object):
     def __exit__(self, exc_type, exc_value, tb):
         self.close()
 
+    def detach_tempfile(self):
+        if not self.prefetched:
+            raise TypeError('Can only detech tempfiles in prefetch mode')
+        rv = self._curfile
+        self._curfile = None
+        self.close()
+        rv.seek(0)
+        return rv
+
     def _nextidx(self):
         assert not self.prefetched, 'this makes no sense'
         old_file = self._curfile
@@ -263,21 +320,36 @@ class ChunkedFileBlobIndexWrapper(object):
         self.closed = False
         self.seek(0)
 
-    def _prefetch(self):
-        f = tempfile.NamedTemporaryFile()
-        fpath = f.name
+    def _prefetch(self, prefetch_to=None, delete=True):
+        size = self.size
+        f = tempfile.NamedTemporaryFile(prefix='._prefetch-',
+                                        dir=prefetch_to,
+                                        delete=delete)
+        if size == 0:
+            self._curfile = f
+            return
+
+        # Zero out the file
+        f.seek(size - 1)
+        f.write('\x00')
+        f.flush()
+
+        mem = mmap.mmap(f.fileno(), size)
 
         def fetch_file(offset, getfile):
-            with open(fpath, 'r+') as f:
-                f.seek(offset)
-                with getfile() as sf:
-                    shutil.copyfileobj(sf, f)
-                f.flush()
+            with getfile() as sf:
+                while 1:
+                    chunk = sf.read(65535)
+                    if not chunk:
+                        break
+                    mem[offset:offset + len(chunk)] = chunk
+                    offset += len(chunk)
 
         with ThreadPoolExecutor(max_workers=4) as exe:
             for idx in self._indexes:
                 exe.submit(fetch_file, idx.offset, idx.blob.getfile)
 
+        mem.flush()
         self._curfile = f
 
     def close(self):
diff --git a/src/sentry/runner/commands/devserver.py b/src/sentry/runner/commands/devserver.py
index c3d2d6b368..3fa089086c 100644
--- a/src/sentry/runner/commands/devserver.py
+++ b/src/sentry/runner/commands/devserver.py
@@ -83,6 +83,8 @@ def devserver(reload, watchers, workers, browser_reload, styleguide, environment
         'worker-reload-mercy': 2,
         # We need stdin to support pdb in devserver
         'honour-stdin': True,
+        # accept ridiculously large files
+        'limit-post': 1 << 30,
     }
 
     if reload:
diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index a1a667b010..023cd3b6d1 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -13,6 +13,7 @@ from multiprocessing import cpu_count
 import click
 
 from sentry.runner.decorators import configuration, log_options
+from sentry.bgtasks.api import managed_bgtasks
 
 
 class AddressParamType(click.ParamType):
@@ -111,11 +112,12 @@ def web(bind, workers, upgrade, with_lock, noinput):
                 raise
 
     from sentry.services.http import SentryHTTPServer
-    SentryHTTPServer(
-        host=bind[0],
-        port=bind[1],
-        workers=workers,
-    ).run()
+    with managed_bgtasks(role='web'):
+        SentryHTTPServer(
+            host=bind[0],
+            port=bind[1],
+            workers=workers,
+        ).run()
 
 
 @run.command()
@@ -137,10 +139,11 @@ def smtp(bind, upgrade, noinput):
         )
 
     from sentry.services.smtp import SentrySMTPServer
-    SentrySMTPServer(
-        host=bind[0],
-        port=bind[1],
-    ).run()
+    with managed_bgtasks(role='smtp'):
+        SentrySMTPServer(
+            host=bind[0],
+            port=bind[1],
+        ).run()
 
 
 @run.command()
@@ -194,21 +197,22 @@ def worker(**options):
         )
 
     from sentry.celery import app
-    worker = app.Worker(
-        # without_gossip=True,
-        # without_mingle=True,
-        # without_heartbeat=True,
-        pool_cls='processes',
-        **options
-    )
-    worker.start()
-    try:
-        sys.exit(worker.exitcode)
-    except AttributeError:
-        # `worker.exitcode` was added in a newer version of Celery:
-        # https://github.com/celery/celery/commit/dc28e8a5
-        # so this is an attempt to be forwards compatible
-        pass
+    with managed_bgtasks(role='worker'):
+        worker = app.Worker(
+            # without_gossip=True,
+            # without_mingle=True,
+            # without_heartbeat=True,
+            pool_cls='processes',
+            **options
+        )
+        worker.start()
+        try:
+            sys.exit(worker.exitcode)
+        except AttributeError:
+            # `worker.exitcode` was added in a newer version of Celery:
+            # https://github.com/celery/celery/commit/dc28e8a5
+            # so this is an attempt to be forwards compatible
+            pass
 
 
 @run.command()
@@ -240,9 +244,10 @@ def cron(**options):
         )
 
     from sentry.celery import app
-    app.Beat(
-        # without_gossip=True,
-        # without_mingle=True,
-        # without_heartbeat=True,
-        **options
-    ).run()
+    with managed_bgtasks(role='cron'):
+        app.Beat(
+            # without_gossip=True,
+            # without_mingle=True,
+            # without_heartbeat=True,
+            **options
+        ).run()
diff --git a/src/sentry/static/sentry/app/components/events/interfaces/frame.jsx b/src/sentry/static/sentry/app/components/events/interfaces/frame.jsx
index a292de6a38..2668cc0250 100644
--- a/src/sentry/static/sentry/app/components/events/interfaces/frame.jsx
+++ b/src/sentry/static/sentry/app/components/events/interfaces/frame.jsx
@@ -351,7 +351,7 @@ const Frame = React.createClass({
             ? <span className="package" title={data.package}>
                 {trimPackage(data.package)}
               </span>
-            : <span className="package" />}
+            : <span className="package">{'<unknown>'}</span>}
           <span className="address">
             {data.instructionAddr}
           </span>
diff --git a/src/sentry/static/sentry/app/components/events/interfaces/stacktraceContent.jsx b/src/sentry/static/sentry/app/components/events/interfaces/stacktraceContent.jsx
index f9a1f19d74..217776224c 100644
--- a/src/sentry/static/sentry/app/components/events/interfaces/stacktraceContent.jsx
+++ b/src/sentry/static/sentry/app/components/events/interfaces/stacktraceContent.jsx
@@ -74,6 +74,9 @@ const StacktraceContent = React.createClass({
       let repeatedFrame =
         nextFrame &&
         frame.lineNo === nextFrame.lineNo &&
+        frame.instructionAddr === nextFrame.instructionAddr &&
+        frame.package === nextFrame.package &&
+        frame.module === nextFrame.module &&
         frame.function === nextFrame.function;
 
       if (repeatedFrame) {
diff --git a/src/sentry/static/sentry/app/views/projectProcessingIssues.jsx b/src/sentry/static/sentry/app/views/projectProcessingIssues.jsx
index 7196c86f81..fffa20ab91 100644
--- a/src/sentry/static/sentry/app/views/projectProcessingIssues.jsx
+++ b/src/sentry/static/sentry/app/views/projectProcessingIssues.jsx
@@ -13,7 +13,6 @@ import {t, tn} from '../locale';
 const MESSAGES = {
   native_no_crashed_thread: t('No crashed thread found in crash report'),
   native_internal_failure: t('Internal failure when attempting to symbolicate: {error}'),
-  native_no_symsynd: t('The symbolizer is not configured for this system.'),
   native_bad_dsym: t('The debug symbol file used was broken.'),
   native_missing_optionally_bundled_dsym: t('An optional debug symbol file was missing.'),
   native_missing_dsym: t('A required debug symbol file was missing.'),
diff --git a/src/sentry/tasks/dsymcache.py b/src/sentry/tasks/dsymcache.py
deleted file mode 100644
index 368529043a..0000000000
--- a/src/sentry/tasks/dsymcache.py
+++ /dev/null
@@ -1,9 +0,0 @@
-from __future__ import absolute_import, print_function
-
-from sentry.tasks.base import instrumented_task
-from sentry.models import ProjectDSymFile
-
-
-@instrumented_task(name='sentry.tasks.clear_old_cached_dsyms', time_limit=15, soft_time_limit=10)
-def clear_old_cached_dsyms():
-    ProjectDSymFile.dsymcache.clear_old_entries()
diff --git a/src/sentry/tasks/symcache_update.py b/src/sentry/tasks/symcache_update.py
new file mode 100644
index 0000000000..ead2c22242
--- /dev/null
+++ b/src/sentry/tasks/symcache_update.py
@@ -0,0 +1,20 @@
+from __future__ import absolute_import
+
+import uuid
+from sentry.tasks.base import instrumented_task
+from sentry.models import Project, ProjectDSymFile
+
+
+@instrumented_task(
+    name='sentry.tasks.symcache_update',
+    time_limit=65,
+    soft_time_limit=60,
+)
+def symcache_update(project_id, uuids, **kwargs):
+    try:
+        project = Project.objects.get(id=project_id)
+    except Project.DoesNotExist:
+        return
+
+    uuids = list(map(uuid.UUID, uuids))
+    ProjectDSymFile.dsymcache.update_symcaches(project, uuids)
diff --git a/src/sentry/utils/native.py b/src/sentry/utils/native.py
deleted file mode 100644
index 064a3283b1..0000000000
--- a/src/sentry/utils/native.py
+++ /dev/null
@@ -1,15 +0,0 @@
-from __future__ import absolute_import
-
-import six
-
-
-def parse_addr(x):
-    if x is None:
-        return 0
-    if isinstance(x, six.integer_types):
-        return x
-    if isinstance(x, six.string_types):
-        if x[:2] == '0x':
-            return int(x[2:], 16)
-        return int(x)
-    raise ValueError('Unsupported address format %r' % (x, ))
diff --git a/tests/sentry/lang/native/fixtures/hello.dsym b/tests/sentry/lang/native/fixtures/hello.dsym
new file mode 100644
index 0000000000..6d0e88c1b0
Binary files /dev/null and b/tests/sentry/lang/native/fixtures/hello.dsym differ
diff --git a/tests/sentry/lang/native/test_plugin.py b/tests/sentry/lang/native/test_plugin.py
index af7651a4d6..795acd4e08 100644
--- a/tests/sentry/lang/native/test_plugin.py
+++ b/tests/sentry/lang/native/test_plugin.py
@@ -1,12 +1,18 @@
 from __future__ import absolute_import
 
+import os
+import zipfile
 from mock import patch
+from six import BytesIO
+
+from django.core.files.uploadedfile import SimpleUploadedFile
+from django.core.urlresolvers import reverse
 
 from sentry.models import Event
 from sentry.testutils import TestCase
 from sentry.lang.native.symbolizer import Symbolizer
 
-from symsynd import parse_addr
+from symbolic import parse_addr
 
 
 class BasicResolvingIntegrationTest(TestCase):
@@ -1028,3 +1034,84 @@ class InAppHonoringResolvingIntegrationTest(TestCase):
         bt = event.interfaces['sentry.interfaces.Exception'].values[0].stacktrace
         frames = bt.frames
         assert frames[0].in_app
+
+
+class RealResolvingIntegrationTest(TestCase):
+    def test_real_resolving(self):
+        url = reverse(
+            'sentry-api-0-dsym-files',
+            kwargs={
+                'organization_slug': self.project.organization.slug,
+                'project_slug': self.project.slug,
+            }
+        )
+
+        self.login_as(user=self.user)
+
+        out = BytesIO()
+        f = zipfile.ZipFile(out, 'w')
+        f.write(os.path.join(os.path.dirname(__file__), 'fixtures', 'hello.dsym'),
+                'dSYM/hello')
+        f.close()
+
+        response = self.client.post(
+            url, {
+                'file':
+                SimpleUploadedFile('symbols.zip', out.getvalue(), content_type='application/zip'),
+            },
+            format='multipart'
+        )
+        assert response.status_code == 201, response.content
+        assert len(response.data) == 1
+
+        event_data = {
+            "project": self.project.id,
+            "platform": "cocoa",
+            "debug_meta": {
+                "images": [{
+                    "type": "apple",
+                    "arch": "x86_64",
+                    "uuid": "502fc0a5-1ec1-3e47-9998-684fa139dca7",
+                    "image_vmaddr": "0x0000000100000000",
+                    "image_size": 4096,
+                    "image_addr": "0x0000000100000000",
+                    "name": "Foo.app/Contents/Foo"
+                }],
+                "sdk_info": {
+                    "dsym_type": "macho",
+                    "sdk_name": "macOS",
+                    "version_major": 10,
+                    "version_minor": 12,
+                    "version_patchlevel": 4,
+                }
+            },
+            "sentry.interfaces.Exception": {
+                "values": [
+                    {
+                        'stacktrace': {
+                            "frames": [
+                                {
+                                    "function": "unknown",
+                                    "instruction_addr": "0x0000000100000fa0"
+                                },
+                            ]
+                        },
+                        "type": "Fail",
+                        "value": "fail"
+                    }
+                ]
+            },
+        }
+
+        resp = self._postWithHeader(event_data)
+        assert resp.status_code == 200
+
+        event = Event.objects.get()
+
+        bt = event.interfaces['sentry.interfaces.Exception'].values[0].stacktrace
+        frames = bt.frames
+
+        assert frames[0].function == 'main'
+        assert frames[0].filename == 'hello.c'
+        assert frames[0].abs_path == '/tmp/hello.c'
+        assert frames[0].lineno == 1
diff --git a/tests/sentry/lang/native/test_processor.py b/tests/sentry/lang/native/test_processor.py
index 2374f942b3..c49c2f7607 100644
--- a/tests/sentry/lang/native/test_processor.py
+++ b/tests/sentry/lang/native/test_processor.py
@@ -240,7 +240,7 @@ class BasicInAppTest(TestCase):
                     '2DA67FF5-2643-44D6-8FFF-1B6BC78C9912',
                 ]
             ),
-            cpu_name='arm64'
+            arch='arm64'
         )
 
         assert sym.is_in_app(4295121764)
diff --git a/tests/sentry/models/test_dsymfile.py b/tests/sentry/models/test_dsymfile.py
new file mode 100644
index 0000000000..bb058569a4
--- /dev/null
+++ b/tests/sentry/models/test_dsymfile.py
@@ -0,0 +1,84 @@
+from __future__ import absolute_import
+
+import os
+import time
+import uuid
+import zipfile
+from six import BytesIO, text_type
+
+from django.core.files.uploadedfile import SimpleUploadedFile
+from django.core.urlresolvers import reverse
+
+from sentry.testutils import APITestCase
+from sentry.models import ProjectDSymFile
+
+# This is obviously a freely generated UUID and not the checksum UUID.
+# This is permissible if users want to send different UUIDs
+PROGUARD_UUID = uuid.UUID('6dc7fdb0-d2fb-4c8e-9d6b-bb1aa98929b1')
+PROGUARD_SOURCE = b'''\
+org.slf4j.helpers.Util$ClassContextSecurityManager -> org.a.b.g$a:
+65:65:void <init>() -> <init>
+67:67:java.lang.Class[] getClassContext() -> getClassContext
+65:65:void <init>(org.slf4j.helpers.Util$1) -> <init>
+'''
+
+
+class DSymFilesClearTest(APITestCase):
+    def test_simple_cache_clear(self):
+        project = self.create_project(name='foo')
+
+        url = reverse(
+            'sentry-api-0-dsym-files',
+            kwargs={
+                'organization_slug': project.organization.slug,
+                'project_slug': project.slug,
+            }
+        )
+
+        self.login_as(user=self.user)
+
+        out = BytesIO()
+        f = zipfile.ZipFile(out, 'w')
+        f.writestr('proguard/%s.txt' % PROGUARD_UUID, PROGUARD_SOURCE)
+        f.writestr('ignored-file.txt', b'This is just some stuff')
+        f.close()
+
+        response = self.client.post(
+            url, {
+                'file':
+                SimpleUploadedFile('symbols.zip', out.getvalue(),
+                                   content_type='application/zip'),
+            },
+            format='multipart'
+        )
+
+        assert response.status_code == 201, response.content
+        assert len(response.data) == 1
+        assert response.data[0]['headers'] == {
+            'Content-Type': 'text/x-proguard+plain'}
+        assert response.data[0]['sha1'] == 'e6d3c5185dac63eddfdc1a5edfffa32d46103b44'
+        assert response.data[0]['uuid'] == text_type(PROGUARD_UUID)
+        assert response.data[0]['objectName'] == 'proguard-mapping'
+        assert response.data[0]['cpuName'] == 'any'
+        assert response.data[0]['symbolType'] == 'proguard'
+
+        dsyms = ProjectDSymFile.dsymcache.fetch_dsyms(
+            project=project,
+            uuids=[PROGUARD_UUID])
+        assert len(dsyms) == 1
+        assert os.path.isfile(dsyms[PROGUARD_UUID])
+
+        # if we clear now, nothing happens
+        ProjectDSymFile.dsymcache.clear_old_entries()
+        assert os.path.isfile(dsyms[PROGUARD_UUID])
+
+        # Put the time into the future
+        real_time = time.time
+        time.time = lambda: real_time() + 60 * 60 * 48
+        try:
+            ProjectDSymFile.dsymcache.clear_old_entries()
+        finally:
+            time.time = real_time
+
+        # But it's gone now
+        assert not os.path.isfile(dsyms[PROGUARD_UUID])
diff --git a/tests/sentry/models/test_file.py b/tests/sentry/models/test_file.py
index 876e7a26c6..d6ac31e529 100644
--- a/tests/sentry/models/test_file.py
+++ b/tests/sentry/models/test_file.py
@@ -65,7 +65,7 @@ class FileTest(TestCase):
             fp.read()
 
     def test_multi_chunk_prefetch(self):
-        random_data = os.urandom(1 << 16)
+        random_data = os.urandom(1 << 25)
 
         fileobj = ContentFile(random_data)
         file = File.objects.create(
@@ -73,7 +73,7 @@ class FileTest(TestCase):
             type='default',
             size=len(random_data),
         )
-        file.putfile(fileobj, 128)
+        file.putfile(fileobj)
 
         f = file.getfile(prefetch=True)
         assert f.read() == random_data
