commit 6ed88d512cb25dd68775fa9e339856a01eec9850
Author: Dan Fuller <dfuller@sentry.io>
Date:   Tue Mar 3 18:57:38 2020 -0800

    feat(subscriptions): Explicitly store environment relations
    
    We typically want to have the `QuerySubscription` able to completely recreate a snuba subscription
    if needed, without looking at `AlertRules`. This helps out with a task I'm working on to move
    creation of subscriptions to tasks.

diff --git a/migrations_lockfile.txt b/migrations_lockfile.txt
index 2e165e1218..072d6a356c 100644
--- a/migrations_lockfile.txt
+++ b/migrations_lockfile.txt
@@ -10,7 +10,7 @@ auth: 0008_alter_user_username_max_length
 contenttypes: 0002_remove_content_type_name
 jira_ac: 0001_initial
 nodestore: 0001_initial
-sentry: 0048_auto_20200302_1825
+sentry: 0049_auto_20200304_0254
 sessions: 0001_initial
 sites: 0002_alter_domain_unique
 social_auth: 0001_initial
diff --git a/src/sentry/incidents/logic.py b/src/sentry/incidents/logic.py
index ac97469de3..4e2fa6da3a 100644
--- a/src/sentry/incidents/logic.py
+++ b/src/sentry/incidents/logic.py
@@ -366,10 +366,6 @@ def bulk_get_incident_event_stats(incidents, query_params_list, data_points=50):
     ]
 
 
-def get_alert_rule_environment_names(alert_rule):
-    return [x.environment.name for x in AlertRuleEnvironment.objects.filter(alert_rule=alert_rule)]
-
-
 def get_incident_aggregates(incident, start=None, end=None, prewindow=False):
     """
     Calculates aggregate stats across the life of an incident, or the provided range.
@@ -667,7 +663,7 @@ def update_alert_rule(
                 QueryAggregations(alert_rule.aggregation),
                 timedelta(minutes=alert_rule.time_window),
                 timedelta(minutes=DEFAULT_ALERT_RULE_RESOLUTION),
-                get_alert_rule_environment_names(alert_rule),
+                list(alert_rule.environment.all()),
             )
 
     return alert_rule
@@ -686,7 +682,7 @@ def subscribe_projects_to_alert_rule(alert_rule, projects):
         QueryAggregations(alert_rule.aggregation),
         timedelta(minutes=alert_rule.time_window),
         timedelta(minutes=alert_rule.resolution),
-        get_alert_rule_environment_names(alert_rule),
+        list(alert_rule.environment.all()),
     )
     subscription_links = [
         AlertRuleQuerySubscription(query_subscription=subscription, alert_rule=alert_rule)
diff --git a/src/sentry/migrations/0049_auto_20200304_0254.py b/src/sentry/migrations/0049_auto_20200304_0254.py
new file mode 100644
index 0000000000..3bdbd42e64
--- /dev/null
+++ b/src/sentry/migrations/0049_auto_20200304_0254.py
@@ -0,0 +1,57 @@
+# -*- coding: utf-8 -*-
+# Generated by Django 1.11.28 on 2020-03-04 02:54
+from __future__ import unicode_literals
+
+from django.db import migrations, models
+import django.db.models.deletion
+import django.utils.timezone
+import sentry.db.models.fields.bounded
+import sentry.db.models.fields.foreignkey
+
+
+class Migration(migrations.Migration):
+    # This flag is used to mark that a migration shouldn't be automatically run in
+    # production. We set this to True for operations that we think are risky and want
+    # someone from ops to run manually and monitor.
+    # General advice is that if in doubt, mark your migration as `is_dangerous`.
+    # Some things you should always mark as dangerous:
+    # - Large data migrations. Typically we want these to be run manually by ops so that
+    #   they can be monitored. Since data migrations will now hold a transaction open
+    #   this is even more important.
+    # - Adding columns to highly active tables, even ones that are NULL.
+    is_dangerous = False
+
+    # This flag is used to decide whether to run this migration in a transaction or not.
+    # By default we prefer to run in a transaction, but for migrations where you want
+    # to `CREATE INDEX CONCURRENTLY` this needs to be set to False. Typically you'll
+    # want to create an index concurrently when adding one to an existing table.
+    atomic = True
+
+
+    dependencies = [
+        ('sentry', '0048_auto_20200302_1825'),
+    ]
+
+    operations = [
+        migrations.CreateModel(
+            name='QuerySubscriptionEnvironment',
+            fields=[
+                ('id', sentry.db.models.fields.bounded.BoundedBigAutoField(primary_key=True, serialize=False)),
+                ('date_added', models.DateTimeField(default=django.utils.timezone.now)),
+                ('environment', sentry.db.models.fields.foreignkey.FlexibleForeignKey(on_delete=django.db.models.deletion.CASCADE, to='sentry.Environment')),
+                ('query_subscription', sentry.db.models.fields.foreignkey.FlexibleForeignKey(on_delete=django.db.models.deletion.CASCADE, to='sentry.QuerySubscription')),
+            ],
+            options={
+                'db_table': 'sentry_querysubscriptionenvironment',
+            },
+        ),
+        migrations.AddField(
+            model_name='querysubscription',
+            name='environments',
+            field=models.ManyToManyField(through='sentry.QuerySubscriptionEnvironment', to='sentry.Environment'),
+        ),
+        migrations.AlterUniqueTogether(
+            name='querysubscriptionenvironment',
+            unique_together=set([('query_subscription', 'environment')]),
+        ),
+    ]
diff --git a/src/sentry/snuba/models.py b/src/sentry/snuba/models.py
index a105179639..a5fcd78443 100644
--- a/src/sentry/snuba/models.py
+++ b/src/sentry/snuba/models.py
@@ -19,10 +19,26 @@ class QueryDatasets(Enum):
     EVENTS = "events"
 
 
+class QuerySubscriptionEnvironment(Model):
+    __core__ = True
+
+    query_subscription = FlexibleForeignKey("sentry.QuerySubscription")
+    environment = FlexibleForeignKey("sentry.Environment")
+    date_added = models.DateTimeField(default=timezone.now)
+
+    class Meta:
+        app_label = "sentry"
+        db_table = "sentry_querysubscriptionenvironment"
+        unique_together = (("query_subscription", "environment"),)
+
+
 class QuerySubscription(Model):
     __core__ = True
 
     project = FlexibleForeignKey("sentry.Project", db_constraint=False)
+    environments = models.ManyToManyField(
+        "sentry.Environment", through=QuerySubscriptionEnvironment
+    )
     type = models.TextField()
     subscription_id = models.TextField(unique=True)
     dataset = models.TextField()
diff --git a/src/sentry/snuba/subscriptions.py b/src/sentry/snuba/subscriptions.py
index 5327659674..0d9526b738 100644
--- a/src/sentry/snuba/subscriptions.py
+++ b/src/sentry/snuba/subscriptions.py
@@ -7,7 +7,12 @@ from django.db import transaction
 
 from sentry.api.event_search import get_filter
 from sentry.snuba.discover import resolve_discover_aliases
-from sentry.snuba.models import QueryAggregations, QueryDatasets, QuerySubscription
+from sentry.snuba.models import (
+    QueryAggregations,
+    QueryDatasets,
+    QuerySubscription,
+    QuerySubscriptionEnvironment,
+)
 from sentry.utils.snuba import _snuba_pool, SnubaError
 
 query_aggregation_to_snuba = {
@@ -18,14 +23,7 @@ logger = logging.getLogger(__name__)
 
 
 def bulk_create_snuba_subscriptions(
-    projects,
-    subscription_type,
-    dataset,
-    query,
-    aggregation,
-    time_window,
-    resolution,
-    environment_names,
+    projects, subscription_type, dataset, query, aggregation, time_window, resolution, environments
 ):
     """
     Creates a subscription to a snuba query for each project.
@@ -39,7 +37,7 @@ def bulk_create_snuba_subscriptions(
     :param aggregation: An aggregation to calculate over the time window
     :param time_window: The time window to aggregate over
     :param resolution: How often to receive updates/bucket size
-    :param environment_names: List of environment names to filter by
+    :param environments: List of environments to filter by
     :return: A list of QuerySubscriptions
     """
     subscriptions = []
@@ -54,21 +52,14 @@ def bulk_create_snuba_subscriptions(
                 aggregation,
                 time_window,
                 resolution,
-                environment_names,
+                environments,
             )
         )
     return subscriptions
 
 
 def create_snuba_subscription(
-    project,
-    subscription_type,
-    dataset,
-    query,
-    aggregation,
-    time_window,
-    resolution,
-    environment_names,
+    project, subscription_type, dataset, query, aggregation, time_window, resolution, environments
 ):
     """
     Creates a subscription to a snuba query.
@@ -82,17 +73,17 @@ def create_snuba_subscription(
     :param aggregation: An aggregation to calculate over the time window
     :param time_window: The time window to aggregate over
     :param resolution: How often to receive updates/bucket size
-    :param environment_names: List of environment names to filter by
+    :param environments: List of environments to filter by
     :return: The QuerySubscription representing the subscription
     """
     # TODO: Move this call to snuba into a task. This lets us successfully create a
     # subscription in postgres and rollback as needed without having to create/delete
     # from Snuba
     subscription_id = _create_in_snuba(
-        project, dataset, query, aggregation, time_window, resolution, environment_names
+        project, dataset, query, aggregation, time_window, resolution, environments
     )
 
-    return QuerySubscription.objects.create(
+    subscription = QuerySubscription.objects.create(
         project=project,
         type=subscription_type,
         subscription_id=subscription_id,
@@ -102,10 +93,17 @@ def create_snuba_subscription(
         time_window=int(time_window.total_seconds()),
         resolution=int(resolution.total_seconds()),
     )
+    sub_envs = [
+        QuerySubscriptionEnvironment(query_subscription=subscription, environment=env)
+        for env in environments
+    ]
+    QuerySubscriptionEnvironment.objects.bulk_create(sub_envs)
+
+    return subscription
 
 
 def bulk_update_snuba_subscriptions(
-    subscriptions, query, aggregation, time_window, resolution, environment_names
+    subscriptions, query, aggregation, time_window, resolution, environments
 ):
     """
     Updates a list of query subscriptions.
@@ -116,7 +114,7 @@ def bulk_update_snuba_subscriptions(
     :param aggregation: An aggregation to calculate over the time window
     :param time_window: The time window to aggregate over
     :param resolution: How often to receive updates/bucket size
-    :param environment_names: List of environment names to filter by
+    :param environments: List of environments to filter by
     :return: A list of QuerySubscriptions
     """
     updated_subscriptions = []
@@ -124,14 +122,14 @@ def bulk_update_snuba_subscriptions(
     for subscription in subscriptions:
         updated_subscriptions.append(
             update_snuba_subscription(
-                subscription, query, aggregation, time_window, resolution, environment_names
+                subscription, query, aggregation, time_window, resolution, environments
             )
         )
     return subscriptions
 
 
 def update_snuba_subscription(
-    subscription, query, aggregation, time_window, resolution, environment_names
+    subscription, query, aggregation, time_window, resolution, environments
 ):
     """
     Updates a subscription to a snuba query.
@@ -141,7 +139,7 @@ def update_snuba_subscription(
     :param aggregation: An aggregation to calculate over the time window
     :param time_window: The time window to aggregate over
     :param resolution: How often to receive updates/bucket size
-    :param environment_names: List of environment names to filter by
+    :param environments: List of environments to filter by
     :return: The QuerySubscription representing the subscription
     """
     # TODO: Move this call to snuba into a task. This lets us successfully update a
@@ -150,13 +148,7 @@ def update_snuba_subscription(
     dataset = QueryDatasets(subscription.dataset)
     _delete_from_snuba(dataset, subscription.subscription_id)
     subscription_id = _create_in_snuba(
-        subscription.project,
-        dataset,
-        query,
-        aggregation,
-        time_window,
-        resolution,
-        environment_names,
+        subscription.project, dataset, query, aggregation, time_window, resolution, environments
     )
     subscription.update(
         subscription_id=subscription_id,
@@ -165,6 +157,14 @@ def update_snuba_subscription(
         time_window=int(time_window.total_seconds()),
         resolution=int(resolution.total_seconds()),
     )
+    QuerySubscriptionEnvironment.objects.filter(query_subscription=subscription).exclude(
+        environment__in=environments
+    ).delete()
+    for e in environments:
+        QuerySubscriptionEnvironment.objects.get_or_create(
+            query_subscription=subscription, environment=e
+        )
+
     return subscription
 
 
@@ -193,14 +193,12 @@ def delete_snuba_subscription(subscription):
         _delete_from_snuba(QueryDatasets(subscription.dataset), subscription.subscription_id)
 
 
-def _create_in_snuba(
-    project, dataset, query, aggregation, time_window, resolution, environment_names
-):
+def _create_in_snuba(project, dataset, query, aggregation, time_window, resolution, environments):
     conditions = resolve_discover_aliases({"conditions": get_filter(query).conditions})[0][
         "conditions"
     ]
-    if environment_names:
-        conditions.append(["environment", "IN", environment_names])
+    if environments:
+        conditions.append(["environment", "IN", [env.name for env in environments]])
     response = _snuba_pool.urlopen(
         "POST",
         "/%s/subscriptions" % (dataset.value,),
