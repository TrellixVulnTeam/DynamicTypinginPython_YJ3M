commit c765dee0cc13db1bff6f27a069d821f3fa015033
Author: David Cramer <dcramer@gmail.com>
Date:   Sun Oct 20 14:42:10 2013 -0700

    Implement search backend abstraction

diff --git a/setup.py b/setup.py
index f82f8e3616..f5def091e1 100755
--- a/setup.py
+++ b/setup.py
@@ -62,17 +62,19 @@ tests_require = [
 
 
 install_requires = [
-    'cssutils>0.9.9,<0.9.11',
     'BeautifulSoup>=3.2.1,<3.3.0',
-    'django-celery>=3.0.11,<3.1.0',
     'celery>=3.0.15,<3.1.0',
-    'django-crispy-forms>=1.2.3,<1.3.0',
+    'cssutils>=0.9.9,<0.10.0',
     'Django>=1.5.4,<1.6',
-    'django-paging>=0.2.5,<0.3.0',
+    'django-celery>=3.0.11,<3.1.0',
+    'django-crispy-forms>=1.2.3,<1.3.0',
+    'django-paging>=0.2.4,<0.3.0',
     'django-picklefield>=0.3.0,<0.4.0',
+    'django-social-auth>=0.7.28,<0.8.0',
     'django-static-compiler>=0.3.0,<0.4.0',
     'django-templatetag-sugar>=0.1.0,<0.2.0',
     'gunicorn>=0.17.2,<0.18.0',
+    'httpagentparser>=1.2.1,<1.3.0',
     'logan>=0.5.8.2,<0.6.0',
     'nydus>=0.10.0,<0.11.0',
     'Pygments>=1.6.0,<1.7.0',
@@ -82,10 +84,9 @@ install_requires = [
     'raven>=3.3.8',
     'redis>=2.7.0,<2.9.0',
     'simplejson>=3.1.0,<3.4.0',
-    'South>=0.8.0,<0.9.0',
-    'httpagentparser>=1.2.1,<1.3.0',
-    'django-social-auth>=0.7.28,<0.8.0',
     'setproctitle>=1.1.7,<1.2.0',
+    'South>=0.8.0,<0.9.0',
+    'urllib3>=1.7.1,<1.8.0',
 ]
 
 postgres_requires = [
diff --git a/src/sentry/app.py b/src/sentry/app.py
index 28e0894845..8d6583fa11 100644
--- a/src/sentry/app.py
+++ b/src/sentry/app.py
@@ -15,11 +15,14 @@ class State(local):
     request = None
     data = {}
 
+env = State()
+
 
 def get_instance(path, options):
     cls = import_string(path)
     return cls(**options)
 
+
 buffer = get_instance(settings.SENTRY_BUFFER, settings.SENTRY_BUFFER_OPTIONS)
 quotas = get_instance(settings.SENTRY_QUOTAS, settings.SENTRY_QUOTA_OPTIONS)
-env = State()
+search = get_instance(settings.SENTRY_SEARCH, settings.SENTRY_SEARCH_OPTIONS)
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 0e9222d6fb..b3ab5dec3f 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -150,6 +150,7 @@ INSTALLED_APPS = (
     'kombu.transport.django',
     'raven.contrib.django.raven_compat',
     'sentry',
+    'sentry.search',
     'sentry.plugins.sentry_interface_types',
     'sentry.plugins.sentry_mail',
     'sentry.plugins.sentry_urls',
@@ -494,10 +495,6 @@ SENTRY_ALLOW_PUBLIC_PROJECTS = True
 # manually.
 SENTRY_ALLOW_REGISTRATION = True
 
-# Instructs Sentry to utilize it's internal search indexer on all incoming
-# events..
-SENTRY_USE_SEARCH = True
-
 # Enable trend results. These can be expensive and are calculated in real-time.
 # When disabled they will be replaced w/ a default priority sort.
 SENTRY_USE_TRENDING = True
@@ -517,11 +514,24 @@ SENTRY_BUFFER_OPTIONS = {}
 
 SENTRY_QUOTAS = 'sentry.quotas.Quota'
 SENTRY_QUOTA_OPTIONS = {}
+
 # The default value for project-level quotas
 SENTRY_DEFAULT_MAX_EVENTS_PER_MINUTE = '90%'
+
 # The maximum number of events per minute the system should accept.
 SENTRY_SYSTEM_MAX_EVENTS_PER_MINUTE = 0
 
+# Search backend
+SENTRY_SEARCH = 'sentry.search.django.DjangoSearchBackend'
+SENTRY_SEARCH_OPTIONS = {}
+# SENTRY_SEARCH_OPTIONS = {
+#     'urls': ['http://localhost:9200/'],
+#     'timeout': 5,
+# }
+
+# Enable search within the frontend
+SENTRY_USE_SEARCH = True
+
 SENTRY_RAVEN_JS_URL = 'd3nslu0hdya83q.cloudfront.net/dist/1.0/raven.min.js'
 
 # URI Prefixes for generating DSN URLs
diff --git a/src/sentry/manager.py b/src/sentry/manager.py
index 52855e8f74..7992695f53 100644
--- a/src/sentry/manager.py
+++ b/src/sentry/manager.py
@@ -8,12 +8,9 @@ sentry.manager
 
 from __future__ import with_statement
 
-from collections import defaultdict
 import datetime
 import hashlib
-import itertools
 import logging
-import re
 import time
 import warnings
 import uuid
@@ -26,7 +23,6 @@ from django.db import models, transaction, IntegrityError
 from django.db.models import Sum
 from django.utils import timezone
 from django.utils.datastructures import SortedDict
-from django.utils.encoding import force_unicode
 
 from raven.utils.encoding import to_string
 from sentry import app
@@ -949,135 +945,6 @@ class UserOptionManager(BaseManager):
         self.__metadata = {}
 
 
-class SearchDocumentManager(BaseManager):
-    # Words which should not be indexed
-    STOP_WORDS = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', 'you', 'that'])
-
-    # Do not index any words shorter than this
-    MIN_WORD_LENGTH = 3
-
-    # Consider these characters to be punctuation (they will be replaced with spaces prior to word extraction)
-    PUNCTUATION_CHARS = re.compile('[%s]' % re.escape(".,;:!?@$%^&*()-<>[]{}\\|/`~'\""))
-
-    def _tokenize(self, text):
-        """
-        Given a string, returns a list of tokens.
-        """
-        if not text:
-            return []
-
-        text = self.PUNCTUATION_CHARS.sub(' ', text)
-
-        words = [t[:128].lower() for t in text.split() if len(t) >= self.MIN_WORD_LENGTH and t.lower() not in self.STOP_WORDS]
-
-        return words
-
-    def search(self, project, query, sort_by='score', offset=0, limit=100):
-        tokens = self._tokenize(query)
-
-        if sort_by == 'score':
-            order_by = 'SUM(st.times_seen) / sd.total_events DESC'
-        elif sort_by == 'new':
-            order_by = 'sd.date_added DESC'
-        elif sort_by == 'date':
-            order_by = 'sd.date_changed DESC'
-        else:
-            raise ValueError('sort_by: %r' % sort_by)
-
-        if tokens:
-            token_sql = ' st.token IN (%s) AND ' % \
-                ', '.join('%s' for i in range(len(tokens)))
-        else:
-            token_sql = ' '
-
-        sql = """
-            SELECT sd.*,
-                   SUM(st.times_seen) / sd.total_events as score
-            FROM sentry_searchdocument as sd
-            INNER JOIN sentry_searchtoken as st
-                ON st.document_id = sd.id
-            WHERE %s
-                sd.project_id = %s
-            GROUP BY sd.id, sd.group_id, sd.total_events, sd.date_changed, sd.date_added, sd.project_id, sd.status
-            ORDER BY %s
-            LIMIT %d OFFSET %d
-        """ % (
-            token_sql,
-            project.id,
-            order_by,
-            limit,
-            offset,
-        )
-        params = tokens
-
-        return self.raw(sql, params)
-
-    def index(self, event):
-        from sentry.models import SearchToken
-
-        group = event.group
-        document, created = self.get_or_create(
-            project=event.project,
-            group=group,
-            defaults={
-                'status': group.status,
-                'total_events': 1,
-                'date_added': group.first_seen,
-                'date_changed': group.last_seen,
-            }
-        )
-        if not created:
-            app.buffer.incr(self.model, {
-                'total_events': 1,
-            }, {
-                'id': document.id,
-            }, {
-                'date_changed': group.last_seen,
-                'status': group.status,
-            })
-
-            document.total_events += 1
-            document.date_changed = group.last_seen
-            document.status = group.status
-
-        context = defaultdict(list)
-        for interface in event.interfaces.itervalues():
-            for k, v in interface.get_search_context(event).iteritems():
-                context[k].extend(v)
-
-        context['text'].extend([
-            event.message,
-            event.logger,
-            event.server_name,
-            event.culprit,
-        ])
-
-        token_counts = defaultdict(lambda: defaultdict(int))
-        for field, values in context.iteritems():
-            field = field.lower()
-            if field == 'text':
-                # we only tokenize the base text field
-                values = itertools.chain(*[self._tokenize(force_unicode(v)) for v in values])
-            else:
-                values = [v.lower() for v in values]
-            for value in values:
-                if not value:
-                    continue
-                token_counts[field][value] += 1
-
-        for field, tokens in token_counts.iteritems():
-            for token, count in tokens.iteritems():
-                app.buffer.incr(SearchToken, {
-                    'times_seen': count,
-                }, {
-                    'document': document,
-                    'token': token,
-                    'field': field,
-                })
-
-        return document
-
-
 class TagKeyManager(BaseManager):
     def _get_cache_key(self, project_id):
         return 'filterkey:all:%s' % project_id
diff --git a/src/sentry/models.py b/src/sentry/models.py
index 0ebe89e371..b8b087d3df 100644
--- a/src/sentry/models.py
+++ b/src/sentry/models.py
@@ -37,14 +37,16 @@ from sentry.constants import (
     MEMBER_OWNER, MEMBER_USER, PLATFORM_TITLES, PLATFORM_LIST,
     STATUS_UNRESOLVED, STATUS_RESOLVED, STATUS_VISIBLE, STATUS_HIDDEN,
     MINUTE_NORMALIZATION, STATUS_MUTED, RESERVED_TEAM_SLUGS,
-    LOG_LEVELS, MAX_CULPRIT_LENGTH, MAX_TAG_KEY_LENGTH, MAX_TAG_VALUE_LENGTH)
+    LOG_LEVELS, MAX_CULPRIT_LENGTH, MAX_TAG_KEY_LENGTH, MAX_TAG_VALUE_LENGTH
+)
 from sentry.db.models import (
     Model, GzippedDictField, BoundedIntegerField, BoundedPositiveIntegerField,
-    update, sane_repr)
+    update, sane_repr
+)
 from sentry.manager import (
-    GroupManager, ProjectManager,
-    MetaManager, InstanceMetaManager, SearchDocumentManager, BaseManager,
-    UserOptionManager, TagKeyManager, TeamManager, UserManager)
+    GroupManager, ProjectManager, MetaManager, InstanceMetaManager, BaseManager,
+    UserOptionManager, TagKeyManager, TeamManager, UserManager
+)
 from sentry.signals import buffer_incr_complete, regression_signal
 from sentry.utils.cache import memoize
 from sentry.utils.db import has_trending
@@ -53,7 +55,7 @@ from sentry.utils.imports import import_string
 from sentry.utils.safe import safe_execute
 from sentry.utils.strings import truncatechars, strip
 
-__all__ = ('Event', 'Group', 'Project', 'SearchDocument')
+__all__ = ('Event', 'Group', 'Project')
 
 
 def slugify_instance(inst, label, reserved=(), **kwargs):
@@ -975,36 +977,6 @@ class ProjectCountByMinute(Model):
     __repr__ = sane_repr('project_id', 'date')
 
 
-class SearchDocument(Model):
-    project = models.ForeignKey(Project)
-    group = models.ForeignKey(Group)
-    total_events = BoundedPositiveIntegerField(default=1)
-    status = BoundedPositiveIntegerField(default=0)
-    date_added = models.DateTimeField(default=timezone.now)
-    date_changed = models.DateTimeField(default=timezone.now)
-
-    objects = SearchDocumentManager()
-
-    class Meta:
-        unique_together = (('project', 'group'),)
-
-    __repr__ = sane_repr('project_id', 'group_id')
-
-
-class SearchToken(Model):
-    document = models.ForeignKey(SearchDocument, related_name="token_set")
-    field = models.CharField(max_length=64, default='text')
-    token = models.CharField(max_length=128)
-    times_seen = BoundedPositiveIntegerField(default=1)
-
-    objects = BaseManager()
-
-    class Meta:
-        unique_together = (('document', 'field', 'token'),)
-
-    __repr__ = sane_repr('document_id', 'field', 'token')
-
-
 class UserOption(Model):
     """
     User options apply only to a user, and optionally a project.
@@ -1349,16 +1321,6 @@ def create_team_member_for_owner(instance, created, **kwargs):
     )
 
 
-def update_document(instance, created, **kwargs):
-    if created:
-        return
-
-    SearchDocument.objects.filter(
-        project=instance.project,
-        group=instance,
-    ).update(status=instance.status)
-
-
 def remove_key_for_team_member(instance, **kwargs):
     for project in instance.team.project_set.all():
         ProjectKey.objects.filter(
@@ -1447,12 +1409,6 @@ post_save.connect(
     dispatch_uid="create_team_member_for_owner",
     weak=False,
 )
-post_save.connect(
-    update_document,
-    sender=Group,
-    dispatch_uid="update_document",
-    weak=False,
-)
 pre_delete.connect(
     remove_key_for_team_member,
     sender=TeamMember,
diff --git a/src/sentry/search/__init__.py b/src/sentry/search/__init__.py
new file mode 100644
index 0000000000..ff22cf3d93
--- /dev/null
+++ b/src/sentry/search/__init__.py
@@ -0,0 +1,9 @@
+"""
+sentry.search
+~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from .base import *  # NOQA
diff --git a/src/sentry/search/base.py b/src/sentry/search/base.py
new file mode 100644
index 0000000000..3dd9544b52
--- /dev/null
+++ b/src/sentry/search/base.py
@@ -0,0 +1,23 @@
+"""
+sentry.search.base
+~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+
+class SearchBackend(object):
+    def __init__(self, **options):
+        pass
+
+    def index(self, group, event):
+        pass
+
+    def remove(self, group):
+        pass
+
+    def query(self, **kwargs):
+        raise NotImplementedError
diff --git a/src/sentry/search/django/__init__.py b/src/sentry/search/django/__init__.py
new file mode 100644
index 0000000000..e70970fc15
--- /dev/null
+++ b/src/sentry/search/django/__init__.py
@@ -0,0 +1,9 @@
+"""
+sentry.search.django
+~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from .backend import *  # NOQA
diff --git a/src/sentry/search/django/backend.py b/src/sentry/search/django/backend.py
new file mode 100644
index 0000000000..b4d0466c2b
--- /dev/null
+++ b/src/sentry/search/django/backend.py
@@ -0,0 +1,152 @@
+"""
+sentry.search.django.backend
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+import re
+import itertools
+
+from collections import defaultdict
+
+from django.utils.encoding import force_unicode
+
+from sentry.search.base import SearchBackend
+
+# Words which should not be indexed
+STOP_WORDS = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it', 'you', 'that'])
+
+# Do not index any words shorter than this
+MIN_WORD_LENGTH = 3
+
+# Consider these characters to be punctuation (they will be replaced with spaces prior to word extraction)
+PUNCTUATION_CHARS = re.compile('[%s]' % re.escape(".,;:!?@$%^&*()-<>[]{}\\|/`~'\""))
+
+
+class DjangoSearchBackend(SearchBackend):
+    def _tokenize(self, text):
+        """
+        Given a string, returns a list of tokens.
+        """
+        if not text:
+            return []
+
+        text = PUNCTUATION_CHARS.sub(' ', text)
+
+        words = [
+            t[:128].lower() for t in text.split()
+            if len(t) >= MIN_WORD_LENGTH and t.lower() not in STOP_WORDS
+        ]
+
+        return words
+
+    def index(self, group, event):
+        from sentry import app
+        from sentry.search.django.models import SearchDocument, SearchToken
+
+        document, created = SearchDocument.objects.get_or_create(
+            project=event.project,
+            group=group,
+            defaults={
+                'status': group.status,
+                'total_events': 1,
+                'date_added': group.first_seen,
+                'date_changed': group.last_seen,
+            }
+        )
+        if not created:
+            app.buffer.incr(SearchDocument, {
+                'total_events': 1,
+            }, {
+                'id': document.id,
+            }, {
+                'date_changed': group.last_seen,
+                'status': group.status,
+            })
+
+            document.total_events += 1
+            document.date_changed = group.last_seen
+            document.status = group.status
+
+        context = defaultdict(list)
+        for interface in event.interfaces.itervalues():
+            for k, v in interface.get_search_context(event).iteritems():
+                context[k].extend(v)
+
+        context['text'].extend([
+            event.message,
+            event.logger,
+            event.server_name,
+            event.culprit,
+        ])
+
+        token_counts = defaultdict(lambda: defaultdict(int))
+        for field, values in context.iteritems():
+            field = field.lower()
+            if field == 'text':
+                # we only tokenize the base text field
+                values = itertools.chain(*[self._tokenize(force_unicode(v)) for v in values])
+            else:
+                values = [v.lower() for v in values]
+            for value in values:
+                if not value:
+                    continue
+                token_counts[field][value] += 1
+
+        for field, tokens in token_counts.iteritems():
+            for token, count in tokens.iteritems():
+                app.buffer.incr(SearchToken, {
+                    'times_seen': count,
+                }, {
+                    'document': document,
+                    'token': token,
+                    'field': field,
+                })
+
+        return document
+
+    def query(self, project, query, sort_by='score', offset=0, limit=100):
+        from sentry.search.django.models import SearchDocument
+
+        tokens = self._tokenize(query)
+
+        if sort_by == 'score':
+            order_by = 'SUM(st.times_seen) / sd.total_events DESC'
+        elif sort_by == 'new':
+            order_by = 'sd.date_added DESC'
+        elif sort_by == 'date':
+            order_by = 'sd.date_changed DESC'
+        else:
+            raise ValueError('sort_by: %r' % sort_by)
+
+        if tokens:
+            token_sql = ' st.token IN (%s) AND ' % \
+                ', '.join('%s' for i in range(len(tokens)))
+        else:
+            token_sql = ' '
+
+        sql = """
+            SELECT sd.*,
+                   SUM(st.times_seen) / sd.total_events as score
+            FROM sentry_searchdocument as sd
+            INNER JOIN sentry_searchtoken as st
+                ON st.document_id = sd.id
+            WHERE %s
+                sd.project_id = %s
+            GROUP BY sd.id, sd.group_id, sd.total_events, sd.date_changed, sd.date_added, sd.project_id, sd.status
+            ORDER BY %s
+            LIMIT %d OFFSET %d
+        """ % (
+            token_sql,
+            project.id,
+            order_by,
+            limit,
+            offset,
+        )
+        params = tokens
+
+        return list(SearchDocument.objects.raw(sql, params))
diff --git a/src/sentry/search/django/models.py b/src/sentry/search/django/models.py
new file mode 100644
index 0000000000..0c2369dc16
--- /dev/null
+++ b/src/sentry/search/django/models.py
@@ -0,0 +1,44 @@
+"""
+sentry.search.django.models
+~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+from django.db import models
+from django.utils import timezone
+
+from sentry.db.models import (
+    Model, BoundedPositiveIntegerField, sane_repr
+)
+
+
+class SearchDocument(Model):
+    project = models.ForeignKey('sentry.Project')
+    group = models.ForeignKey('sentry.Group')
+    total_events = BoundedPositiveIntegerField(default=1)
+    status = BoundedPositiveIntegerField(default=0)
+    date_added = models.DateTimeField(default=timezone.now)
+    date_changed = models.DateTimeField(default=timezone.now)
+
+    class Meta:
+        db_table = 'sentry_searchdocument'
+        unique_together = (('project', 'group'),)
+
+    __repr__ = sane_repr('project_id', 'group_id')
+
+
+class SearchToken(Model):
+    document = models.ForeignKey(SearchDocument, related_name="token_set")
+    field = models.CharField(max_length=64, default='text')
+    token = models.CharField(max_length=128)
+    times_seen = BoundedPositiveIntegerField(default=1)
+
+    class Meta:
+        db_table = 'sentry_searchtoken'
+        unique_together = (('document', 'field', 'token'),)
+
+    __repr__ = sane_repr('document_id', 'field', 'token')
diff --git a/src/sentry/search/models.py b/src/sentry/search/models.py
new file mode 100644
index 0000000000..c1dee90da4
--- /dev/null
+++ b/src/sentry/search/models.py
@@ -0,0 +1,13 @@
+"""
+sentry.search.models
+~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+# HACK(dcramer): Django doesn't play well with our naming schemes, and we prefer
+# our methods ways over Django's limited scoping
+from .django.models import *  # NOQA
diff --git a/src/sentry/search/solr/__init__.py b/src/sentry/search/solr/__init__.py
new file mode 100644
index 0000000000..263f98d3c1
--- /dev/null
+++ b/src/sentry/search/solr/__init__.py
@@ -0,0 +1,7 @@
+"""
+sentry.search.solr
+~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
diff --git a/src/sentry/search/solr/backend.py b/src/sentry/search/solr/backend.py
new file mode 100644
index 0000000000..8b804ef276
--- /dev/null
+++ b/src/sentry/search/solr/backend.py
@@ -0,0 +1,72 @@
+"""
+sentry.search.solr.backend
+~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+from collections import defaultdict
+from nydus.db import create_cluster
+
+from sentry.search.base import SearchBackend
+
+# TODO: ensure upgrade creates search schemas
+# TODO: optimize group indexing so it only happens when a group is updated
+# TODO: only index an event after a group is indexed??
+# TODO: confirm replication=async is a good idea
+# TODO: determine TTL
+# TODO: index.routing.allocation.include.tag = project_id?
+
+
+class SolrBackend(SearchBackend):
+    def __init__(self, servers, **options):
+        self.backend = create_cluster({
+            'engine': 'sentry.search.solr.client.Solr',
+            'router': 'nydus.db.routers.base.RoundRobinRouter',
+            'hosts': [{'url': u} for u in servers],
+        })
+
+    def index(self, group, event):
+        self.backend.add([
+            self._make_document(event),
+        ])
+
+    def remove(self, group):
+        self.backend.delete(group.id)
+
+    def _make_document(self, event):
+        group = event.group
+
+        context = {
+            'text': [
+                event.message,
+                event.culprit
+            ],
+            'filters': defaultdict(list),
+        }
+        for interface in event.interfaces.itervalues():
+            for k, v in interface.get_search_context(event).iteritems():
+                if k == 'text':
+                    context[k].extend(v)
+                elif k == 'filters':
+                    for f_k, f_v in v.iteritems():
+                        context[k][f_k].extend(f_v)
+
+        tags = []
+        for k, v in context['filters'].iteritems():
+            tags.extend('%s=%s' % (k, f_v) for f_v in v)
+
+        doc = {
+            'id': '%s%s' % (event.project_id, event.event_id),
+            'group': group.id,
+            'project': group.project.id,
+            'team': group.team.id,
+            'datetime': event.datetime,
+            'text': filter(bool, context['text']),
+            'tags': tags,
+        }
+
+        return doc
diff --git a/src/sentry/search/solr/client.py b/src/sentry/search/solr/client.py
new file mode 100644
index 0000000000..e4a6ff0c5b
--- /dev/null
+++ b/src/sentry/search/solr/client.py
@@ -0,0 +1,214 @@
+"""
+sentry.search.solr.client
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2013 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+import urllib3
+
+try:
+    # Prefer lxml, if installed.
+    from lxml import etree as ET
+except ImportError:
+    try:
+        from xml.etree import cElementTree as ET
+    except ImportError:
+        raise ImportError("No suitable ElementTree implementation was found.")
+
+from urlparse import urljoin
+
+from nydus.db.backends import BaseConnection
+
+
+class SolrError(Exception):
+    pass
+
+
+class SolrClient(object):
+    """
+    Inspired by Pysolr, but retrofitted to support a limited scope of features
+    and remove the ``requests`` dependency.
+
+    See also:
+    https://github.com/toastdriven/pysolr
+    """
+    def __init__(self, url, timeout=60):
+        self.url = url
+        self.timeout = timeout
+        self.http = urllib3.connection_from_url(self.url)
+
+    def _send_request(self, method, path='', body=None, headers=None):
+        url = urljoin(self.url, path.lstrip('/'))
+        method = method.lower()
+        log_body = body
+
+        if headers is None:
+            headers = {}
+
+        if log_body is None:
+            log_body = ''
+        elif not isinstance(log_body, str):
+            log_body = repr(body)
+
+        if not 'content-type' in [key.lower() for key in headers.keys()]:
+            headers['Content-type'] = 'application/xml; charset=UTF-8'
+
+        resp = self.http.urlopen(
+            method, url, body=body, headers=headers, timeout=self.timeout)
+
+        if resp.status != 200:
+            raise SolrError(resp.data)
+
+        return resp
+
+    def _is_null_value(self, value):
+        if value is None:
+            return True
+
+        if isinstance(value, basestring) and len(value) == 0:
+            return True
+
+        return False
+
+    def _add_doc_field(self, doc, key, value):
+        if not isinstance(value, dict):
+            return self._add_doc_field(doc, key, {None: value})
+
+        # dict is expected to be something like
+        # {key: {'add': [value]}}
+        for action, action_value in value.iteritems():
+            # To avoid multiple code-paths we'd like to treat all of our values
+            # as iterables:
+            if isinstance(action_value, (list, tuple)):
+                action_value = action_value
+            else:
+                action_value = (action_value, )
+
+            for bit in action_value:
+                if self._is_null_value(bit):
+                    continue
+
+                attrs = {
+                    'name': key,
+                }
+                if action:
+                    attrs['update'] = action
+                field = ET.Element('field', **attrs)
+                field.text = self._from_python(bit)
+                doc.append(field)
+
+    def _from_python(self, value):
+        """
+        Converts python values to a form suitable for insertion into the xml
+        we send to solr.
+        """
+        if hasattr(value, 'strftime'):
+            if hasattr(value, 'hour'):
+                value = "%sZ" % value.isoformat()
+            else:
+                value = "%sT00:00:00Z" % value.isoformat()
+        elif isinstance(value, bool):
+            if value:
+                value = 'true'
+            else:
+                value = 'false'
+        else:
+            if isinstance(value, str):
+                value = unicode(value, errors='replace')
+
+            value = "{0}".format(value)
+
+        return value
+
+    def _build_doc(self, doc):
+        doc_elem = ET.Element('doc')
+
+        for key, value in doc.items():
+            self._add_doc_field(doc_elem, key, value)
+
+        return doc_elem
+
+    def _update(self, message, commit=True, waitFlush=None, waitSearcher=None,
+                softCommit=None):
+        """
+        Posts the given xml message to http://<self.url>/update and
+        returns the result.
+
+        Passing `sanitize` as False will prevent the message from being cleaned
+        of control characters (default True). This is done by default because
+        these characters would cause Solr to fail to parse the XML. Only pass
+        False if you're positive your data is clean.
+        """
+        path = '/update'
+
+        # Per http://wiki.apache.org/solr/UpdateXmlMessages, we can append a
+        # ``commit=true`` to the URL and have the commit happen without a
+        # second request.
+        query_vars = []
+
+        if commit is not None:
+            query_vars.append('commit=%s' % str(bool(commit)).lower())
+
+        if waitFlush is not None:
+            query_vars.append('waitFlush=%s' % str(bool(waitFlush)).lower())
+
+        if waitSearcher is not None:
+            query_vars.append('waitSearcher=%s' % str(bool(waitSearcher)).lower())
+
+        if query_vars:
+            path = '%s?%s' % (path, '&'.join(query_vars))
+
+        return self._send_request('post', path, message, {
+            'Content-type': 'text/xml; charset=utf-8'
+        })
+
+    def add(self, docs, commit=True, commitWithin=None, waitFlush=None,
+            waitSearcher=None):
+        """
+        >>> solr.add([
+        >>>     {
+        >>>         "id": "doc_1",
+        >>>         "title": "A test document",
+        >>>     },
+        >>>     {
+        >>>         "id": "doc_2",
+        >>>         "title": "The Banana: Tasty or Dangerous?",
+        >>>         "tags": {
+        >>>             "add": ["foo", "bar"],
+        >>>         },
+        >>>     },
+        >>> ])
+        """
+        message = ET.Element('add')
+
+        if commitWithin:
+            message.set('commitWithin', commitWithin)
+
+        for doc in docs:
+            message.append(self._build_doc(doc))
+
+        m = ET.tostring(message, encoding='utf-8')
+
+        return self._update(m, commit=commit, waitFlush=waitFlush,
+                            waitSearcher=waitSearcher)
+
+
+class Solr(BaseConnection):
+    retryable_exceptions = frozenset([urllib3.Timeout])
+
+    def __init__(self, num, url, timeout=60):
+        self.url = url
+        self.timeout = timeout
+        super(Solr, self).__init__(num)
+
+    @property
+    def identifier(self):
+        return 'solr+%(url)s' % vars(self)
+
+    def connect(self):
+        return SolrClient(self.url, timeout=self.timeout)
+
+    def disconnect(self):
+        pass
diff --git a/src/sentry/search/solr/data/schema.xml b/src/sentry/search/solr/data/schema.xml
new file mode 100644
index 0000000000..c37500a8f9
--- /dev/null
+++ b/src/sentry/search/solr/data/schema.xml
@@ -0,0 +1,359 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+ This is the Solr schema file. This file should be named "schema.xml" and
+ should be in the conf directory under the solr home
+ (i.e. ./solr/conf/schema.xml by default)
+ or located where the classloader for the Solr webapp can find it.
+
+ This example schema is the recommended starting point for users.
+ It should be kept correct and concise, usable out-of-the-box.
+
+ For more information, on how to customize this file, please see
+ http://wiki.apache.org/solr/SchemaXml
+
+ PERFORMANCE NOTE: this schema includes many optional features and should not
+ be used for benchmarking.  To improve performance one could
+  - set stored="false" for all fields possible (esp large fields) when you
+    only need to search on the field but don't need to return the original
+    value.
+  - set indexed="false" if you don't need to search on the field, but only
+    return the field as a result of searching on other indexed fields.
+  - remove all unneeded copyField statements
+  - for best index size and searching performance, set "index" to false
+    for all general text fields, use copyField to copy them to the
+    catchall "text" field, and use that for searching.
+  - For maximum indexing performance, use the StreamingUpdateSolrServer
+    java client.
+  - Remember to run the JVM in server mode, and use a higher logging level
+    that avoids logging every request
+-->
+
+<schema name="sentry_group" version="1.5">
+  <!-- attribute "name" is the name of this schema and is only used for display purposes.
+       version="x.y" is Solr's version number for the schema syntax and
+       semantics.  It should not normally be changed by applications.
+
+       1.0: multiValued attribute did not exist, all fields are multiValued
+            by nature
+       1.1: multiValued attribute introduced, false by default
+       1.2: omitTermFreqAndPositions attribute introduced, true by default
+            except for text fields.
+       1.3: removed optional field compress feature
+       1.4: autoGeneratePhraseQueries attribute introduced to drive QueryParser
+            behavior when a single string produces multiple tokens.  Defaults
+            to off for version >= 1.4
+       1.5: omitNorms defaults to true for primitive field types
+            (int, float, boolean, string...)
+     -->
+
+  <fields>
+    <!-- Valid attributes for fields:
+     name: mandatory - the name for the field
+     type: mandatory - the name of a field type from the
+       <types> fieldType section
+     indexed: true if this field should be indexed (searchable or sortable)
+     stored: true if this field should be retrievable
+     docValues: true if this field should have doc values. Doc values are
+       useful for faceting, grouping, sorting and function queries. Although not
+       required, doc values will make the index faster to load, more
+       NRT-friendly and more memory-efficient. They however come with some
+       limitations: they are currently only supported by StrField, UUIDField
+       and all Trie*Fields, and depending on the field type, they might
+       require the field to be single-valued, be required or have a default
+       value (check the documentation of the field type you're interested in
+       for more information)
+     multiValued: true if this field may contain multiple values per document
+     omitNorms: (expert) set to true to omit the norms associated with
+       this field (this disables length normalization and index-time
+       boosting for the field, and saves some memory).  Only full-text
+       fields or fields that need an index-time boost need norms.
+       Norms are omitted for primitive (non-analyzed) types by default.
+     termVectors: [false] set to true to store the term vector for a
+       given field.
+       When using MoreLikeThis, fields used for similarity should be
+       stored for best performance.
+     termPositions: Store position information with the term vector.
+       This will increase storage costs.
+     termOffsets: Store offset information with the term vector. This
+       will increase storage costs.
+     required: The field is required.  It will throw an error if the
+       value does not exist
+     default: a value that should be used if no value is specified
+       when adding a document.
+   -->
+
+    <!-- field names should consist of alphanumeric or underscore characters only and
+      not start with a digit.  This is not currently strictly enforced,
+      but other field names will not have first class support from all components
+      and back compatibility is not guaranteed.  Names with both leading and
+      trailing underscores (e.g. _version_) are reserved.
+   -->
+
+    <!-- In this "schemaless" example, only two fields are pre-declared: id and _version_.
+         All other fields will be type guessed and added via the
+         "add-unknown-fields-to-the-schema" update request processor chain declared
+         in solrconfig.xml.
+      -->
+    <field name="_version_" type="long" indexed="true" stored="true"/>
+
+    <field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" />
+
+    <field name="group" type="string" indexed="true" stored="true" required="true" multiValued="false" />
+    <field name="text" type="text" indexed="true" stored="false" required="true" multiValued="true" />
+    <field name="project" type="int" indexed="true" stored="false" multiValued="false" />
+    <field name="team" type="int" indexed="true" stored="false" multiValued="false" />
+
+    <field name="status" type="int" indexed="true" stored="false" multiValued="false" />
+    <field name="tags" type="string" indexed="true" stored="false" multiValued="true" />
+
+    <field name="last_seen" type="tdate" indexed="true" stored="false" multiValued="false" docValues="true" />
+    <field name="first_seen" type="tdate" indexed="true" stored="false" multiValued="false" docValues="true" />
+  </fields>
+
+
+  <!-- Field to use to determine and enforce document uniqueness.
+      Unless this field is marked with required="false", it will be a required field
+   -->
+  <uniqueKey>id</uniqueKey>
+
+  <types>
+    <!-- field type definitions. The "name" attribute is
+       just a label to be used by field definitions.  The "class"
+       attribute and any other attributes determine the real
+       behavior of the fieldType.
+         Class names starting with "solr" refer to java classes in a
+       standard package such as org.apache.solr.analysis
+    -->
+
+    <!-- The StrField type is not analyzed, but indexed/stored verbatim.
+       It supports doc values but in that case the field needs to be
+       single-valued and either required or have a default value.
+      -->
+    <fieldType name="string" class="solr.StrField" sortMissingLast="true" />
+
+    <!-- boolean type: "true" or "false" -->
+    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true"/>
+
+    <fieldType name="booleans" class="solr.BoolField" sortMissingLast="true" multiValued="true"/>
+
+    <!-- sortMissingLast and sortMissingFirst attributes are optional attributes are
+         currently supported on types that are sorted internally as strings
+         and on numeric types.
+	     This includes "string","boolean", and, as of 3.5 (and 4.x),
+	     int, float, long, date, double, including the "Trie" variants.
+       - If sortMissingLast="true", then a sort on this field will cause documents
+         without the field to come after documents with the field,
+         regardless of the requested sort order (asc or desc).
+       - If sortMissingFirst="true", then a sort on this field will cause documents
+         without the field to come before documents with the field,
+         regardless of the requested sort order.
+       - If sortMissingLast="false" and sortMissingFirst="false" (the default),
+         then default lucene sorting will be used which places docs without the
+         field first in an ascending sort and last in a descending sort.
+    -->
+
+    <!--
+      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.
+
+      These fields support doc values, but they require the field to be
+      single-valued and either be required or have a default value.
+    -->
+    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" positionIncrementGap="0"/>
+    <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" positionIncrementGap="0"/>
+    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" positionIncrementGap="0"/>
+    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" positionIncrementGap="0"/>
+
+    <!--
+     Numeric field types that index each value at various levels of precision
+     to accelerate range queries when the number of values between the range
+     endpoints is large. See the javadoc for NumericRangeQuery for internal
+     implementation details.
+
+     Smaller precisionStep values (specified in bits) will lead to more tokens
+     indexed per value, slightly larger index size, and faster range queries.
+     A precisionStep of 0 disables indexing at different precision levels.
+    -->
+    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0"/>
+    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" positionIncrementGap="0"/>
+    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0"/>
+    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0"/>
+
+    <fieldType name="tints" class="solr.TrieIntField" precisionStep="8" positionIncrementGap="0" multiValued="true"/>
+    <fieldType name="tfloats" class="solr.TrieFloatField" precisionStep="8" positionIncrementGap="0" multiValued="true"/>
+    <fieldType name="tlongs" class="solr.TrieLongField" precisionStep="8" positionIncrementGap="0" multiValued="true"/>
+    <fieldType name="tdoubles" class="solr.TrieDoubleField" precisionStep="8" positionIncrementGap="0" multiValued="true"/>
+
+    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
+         is a more restricted form of the canonical representation of dateTime
+         http://www.w3.org/TR/xmlschema-2/#dateTime
+         The trailing "Z" designates UTC time and is mandatory.
+         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
+         All other components are mandatory.
+
+         Expressions can also be used to denote calculations that should be
+         performed relative to "NOW" to determine the value, ie...
+
+               NOW/HOUR
+                  ... Round to the start of the current hour
+               NOW-1DAY
+                  ... Exactly 1 day prior to now
+               NOW/DAY+6MONTHS+3DAYS
+                  ... 6 months and 3 days in the future from the start of
+                      the current day
+
+         Consult the DateField javadocs for more information.
+
+         Note: For faster range queries, consider the tdate type
+      -->
+    <fieldType name="date" class="solr.TrieDateField" precisionStep="0" positionIncrementGap="0"/>
+
+    <!-- A Trie based date field for faster date range queries and date faceting. -->
+    <fieldType name="tdate" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0"/>
+
+    <fieldType name="tdates" class="solr.TrieDateField" precisionStep="6" positionIncrementGap="0" multiValued="true"/>
+
+
+    <!--Binary data type. The data should be sent/retrieved in as Base64 encoded Strings -->
+    <fieldtype name="binary" class="solr.BinaryField"/>
+
+    <!--
+      Note:
+      These should only be used for compatibility with existing indexes (created with lucene or older Solr versions).
+      Use Trie based fields instead. As of Solr 3.5 and 4.x, Trie based fields support sortMissingFirst/Last
+
+      Plain numeric field types that store and index the text
+      value verbatim (and hence don't correctly support range queries, since the
+      lexicographic ordering isn't equal to the numeric ordering)
+    -->
+    <fieldType name="pint" class="solr.IntField"/>
+    <fieldType name="plong" class="solr.LongField"/>
+    <fieldType name="pfloat" class="solr.FloatField"/>
+    <fieldType name="pdouble" class="solr.DoubleField"/>
+    <fieldType name="pdate" class="solr.DateField" sortMissingLast="true"/>
+
+    <!-- The "RandomSortField" is not used to store or search any
+         data.  You can declare fields of this type it in your schema
+         to generate pseudo-random orderings of your docs for sorting
+         or function purposes.  The ordering is generated based on the field
+         name and the version of the index. As long as the index version
+         remains unchanged, and the same field name is reused,
+         the ordering of the docs will be consistent.
+         If you want different psuedo-random orderings of documents,
+         for the same version of the index, use a dynamicField and
+         change the field name in the request.
+     -->
+    <fieldType name="random" class="solr.RandomSortField" indexed="true" />
+
+    <!-- solr.TextField allows the specification of custom text analyzers
+         specified as a tokenizer and a list of token filters. Different
+         analyzers may be specified for indexing and querying.
+
+         The optional positionIncrementGap puts space between multiple fields of
+         this type on the same document, with the purpose of preventing false phrase
+         matching across fields.
+
+         For more info on customizing your analyzer chain, please see
+         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters
+     -->
+
+    <!-- A text field that only splits on whitespace for exact matching of words -->
+    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
+      <analyzer>
+        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <!-- A general text field that has reasonable, generic
+         cross-language defaults: it tokenizes with StandardTokenizer,
+	 removes stop words from case-insensitive "stopwords.txt"
+	 (empty by default), and down cases.  At query time only, it
+	 also applies synonyms. -->
+    <fieldType name="text" class="solr.TextField" positionIncrementGap="100" multiValued="true">
+      <analyzer type="index">
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <!-- Just like text_general except it reverses the characters of
+	 each token, to enable more efficient leading wildcard queries. -->
+    <fieldType name="text_general_rev" class="solr.TextField" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.ReversedWildcardFilterFactory" withOriginal="true"
+                maxPosAsterisk="3" maxPosQuestion="2" maxFractionAsterisk="0.33"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" />
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <!-- charFilter + WhitespaceTokenizer  -->
+    <!--
+    <fieldType name="text_char_norm" class="solr.TextField" positionIncrementGap="100" >
+      <analyzer>
+        <charFilter class="solr.MappingCharFilterFactory" mapping="mapping-ISOLatin1Accent.txt"/>
+        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+    -->
+
+    <fieldtype name="phonetic" stored="false" indexed="true" class="solr.TextField" >
+      <analyzer>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.DoubleMetaphoneFilterFactory" inject="false"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- lowercases the entire field value, keeping it as a single token.  -->
+    <fieldType name="lowercase" class="solr.TextField" positionIncrementGap="100">
+      <analyzer>
+        <tokenizer class="solr.KeywordTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory" />
+      </analyzer>
+    </fieldType>
+
+    <!-- since fields of this type are by default not stored or indexed,
+         any data added to them will be ignored outright.  -->
+    <fieldtype name="ignored" stored="false" indexed="false" multiValued="true" class="solr.StrField" />
+  </types>
+
+  <!-- Similarity is the scoring routine for each document vs. a query.
+       A custom Similarity or SimilarityFactory may be specified here, but
+       the default is fine for most applications.
+       For more info: http://wiki.apache.org/solr/SchemaXml#Similarity
+    -->
+  <!--
+     <similarity class="com.example.solr.CustomSimilarityFactory">
+       <str name="paramkey">param value</str>
+     </similarity>
+    -->
+
+</schema>
diff --git a/src/sentry/search/solr/data/solrconfig.xml b/src/sentry/search/solr/data/solrconfig.xml
new file mode 100644
index 0000000000..2a64ef972a
--- /dev/null
+++ b/src/sentry/search/solr/data/solrconfig.xml
@@ -0,0 +1,948 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<config>
+  <!-- Controls what version of Lucene various components of Solr
+       adhere to.  Generally, you want to use the latest version to
+       get all bug fixes and improvements. It is highly recommended
+       that you fully re-index after changing this setting as it can
+       affect both how text is indexed and queried.
+  -->
+  <luceneMatchVersion>4.5</luceneMatchVersion>
+
+  <!-- Data Directory
+
+       Used to specify an alternate directory to hold all index data
+       other than the default ./data under the Solr home.  If
+       replication is in use, this should match the replication
+       configuration.
+    -->
+  <dataDir>${solr.data.dir:}</dataDir>
+
+  <!-- The DirectoryFactory to use for indexes.
+
+       solr.StandardDirectoryFactory is filesystem
+       based and tries to pick the best implementation for the current
+       JVM and platform.  solr.NRTCachingDirectoryFactory, the default,
+       wraps solr.StandardDirectoryFactory and caches small files in memory
+       for better NRT performance.
+
+       One can force a particular implementation via solr.MMapDirectoryFactory,
+       solr.NIOFSDirectoryFactory, or solr.SimpleFSDirectoryFactory.
+
+       solr.RAMDirectoryFactory is memory based, not
+       persistent, and doesn't work with replication.
+    -->
+  <directoryFactory name="DirectoryFactory"
+                    class="${solr.directoryFactory:solr.NRTCachingDirectoryFactory}"/>
+
+  <!-- The CodecFactory for defining the format of the inverted index.
+       The default implementation is SchemaCodecFactory, which is the official Lucene
+       index format, but hooks into the schema to provide per-field customization of
+       the postings lists and per-document values in the fieldType element
+       (postingsFormat/docValuesFormat). Note that most of the alternative implementations
+       are experimental, so if you choose to customize the index format, its a good
+       idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
+       before upgrading to a newer version to avoid unnecessary reindexing.
+  -->
+  <codecFactory class="solr.SchemaCodecFactory"/>
+
+  <!-- To enable dynamic schema REST APIs, use the following for <schemaFactory>:
+
+       <schemaFactory class="ManagedIndexSchemaFactory">
+         <bool name="mutable">true</bool>
+         <str name="managedSchemaResourceName">managed-schema</str>
+       </schemaFactory>
+
+       When ManagedIndexSchemaFactory is specified, Solr will load the schema from
+       he resource named in 'managedSchemaResourceName', rather than from schema.xml.
+       Note that the managed schema resource CANNOT be named schema.xml.  If the managed
+       schema does not exist, Solr will create it after reading schema.xml, then rename
+       'schema.xml' to 'schema.xml.bak'.
+
+       Do NOT hand edit the managed schema - external modifications will be ignored and
+       overwritten as a result of schema modification REST API calls.
+
+       When ManagedIndexSchemaFactory is specified with mutable = true, schema
+       modification REST API calls will be allowed; otherwise, error responses will be
+       sent back for these requests.
+  -->
+  <schemaFactory class="ClassicIndexSchemaFactory"/>
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Index Config - These settings control low-level behavior of indexing
+       Most example settings here show the default value, but are commented
+       out, to more easily see where customizations have been made.
+
+       Note: This replaces <indexDefaults> and <mainIndex> from older versions
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <indexConfig>
+    <!-- maxFieldLength was removed in 4.0. To get similar behavior, include a
+         LimitTokenCountFilterFactory in your fieldType definition. E.g.
+     <filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="10000"/>
+    -->
+    <!-- Maximum time to wait for a write lock (ms) for an IndexWriter. Default: 1000 -->
+    <!-- <writeLockTimeout>1000</writeLockTimeout>  -->
+
+    <!-- The maximum number of simultaneous threads that may be
+         indexing documents at once in IndexWriter; if more than this
+         many threads arrive they will wait for others to finish.
+         Default in Solr/Lucene is 8. -->
+    <!-- <maxIndexingThreads>8</maxIndexingThreads>  -->
+
+    <!-- Expert: Enabling compound file will use less files for the index,
+         using fewer file descriptors on the expense of performance decrease.
+         Default in Lucene is "true". Default in Solr is "false" (since 3.6) -->
+    <!-- <useCompoundFile>false</useCompoundFile> -->
+
+    <!-- ramBufferSizeMB sets the amount of RAM that may be used by Lucene
+         indexing for buffering added documents and deletions before they are
+         flushed to the Directory.
+         maxBufferedDocs sets a limit on the number of documents buffered
+         before flushing.
+         If both ramBufferSizeMB and maxBufferedDocs is set, then
+         Lucene will flush based on whichever limit is hit first.
+         The default is 100 MB.  -->
+    <!-- <ramBufferSizeMB>100</ramBufferSizeMB> -->
+    <!-- <maxBufferedDocs>1000</maxBufferedDocs> -->
+
+    <!-- Expert: Merge Policy
+         The Merge Policy in Lucene controls how merging of segments is done.
+         The default since Solr/Lucene 3.3 is TieredMergePolicy.
+         The default since Lucene 2.3 was the LogByteSizeMergePolicy,
+         Even older versions of Lucene used LogDocMergePolicy.
+      -->
+    <!--
+        <mergePolicy class="org.apache.lucene.index.TieredMergePolicy">
+          <int name="maxMergeAtOnce">10</int>
+          <int name="segmentsPerTier">10</int>
+        </mergePolicy>
+      -->
+
+    <!-- Merge Factor
+         The merge factor controls how many segments will get merged at a time.
+         For TieredMergePolicy, mergeFactor is a convenience parameter which
+         will set both MaxMergeAtOnce and SegmentsPerTier at once.
+         For LogByteSizeMergePolicy, mergeFactor decides how many new segments
+         will be allowed before they are merged into one.
+         Default is 10 for both merge policies.
+      -->
+    <!--
+    <mergeFactor>10</mergeFactor>
+      -->
+
+    <!-- Expert: Merge Scheduler
+         The Merge Scheduler in Lucene controls how merges are
+         performed.  The ConcurrentMergeScheduler (Lucene 2.3 default)
+         can perform merges in the background using separate threads.
+         The SerialMergeScheduler (Lucene 2.2 default) does not.
+     -->
+    <!--
+       <mergeScheduler class="org.apache.lucene.index.ConcurrentMergeScheduler"/>
+       -->
+
+    <!-- LockFactory
+
+         This option specifies which Lucene LockFactory implementation
+         to use.
+
+         single = SingleInstanceLockFactory - suggested for a
+                  read-only index or when there is no possibility of
+                  another process trying to modify the index.
+         native = NativeFSLockFactory - uses OS native file locking.
+                  Do not use when multiple solr webapps in the same
+                  JVM are attempting to share a single index.
+         simple = SimpleFSLockFactory  - uses a plain file for locking
+
+         Defaults: 'native' is default for Solr3.6 and later, otherwise
+                   'simple' is the default
+
+         More details on the nuances of each LockFactory...
+         http://wiki.apache.org/lucene-java/AvailableLockFactories
+    -->
+    <lockType>${solr.lock.type:native}</lockType>
+
+    <!-- Unlock On Startup
+
+         If true, unlock any held write or commit locks on startup.
+         This defeats the locking mechanism that allows multiple
+         processes to safely access a lucene index, and should be used
+         with care. Default is "false".
+
+         This is not needed if lock type is 'single'
+     -->
+    <!--
+    <unlockOnStartup>false</unlockOnStartup>
+      -->
+
+    <!-- Expert: Controls how often Lucene loads terms into memory
+         Default is 128 and is likely good for most everyone.
+      -->
+    <!-- <termIndexInterval>128</termIndexInterval> -->
+
+    <!-- If true, IndexReaders will be opened/reopened from the IndexWriter
+         instead of from the Directory. Hosts in a master/slave setup
+         should have this set to false while those in a SolrCloud
+         cluster need to be set to true. Default: true
+      -->
+    <!--
+    <nrtMode>true</nrtMode>
+      -->
+
+    <!-- Commit Deletion Policy
+         Custom deletion policies can be specified here. The class must
+         implement org.apache.lucene.index.IndexDeletionPolicy.
+
+         The default Solr IndexDeletionPolicy implementation supports
+         deleting index commit points on number of commits, age of
+         commit point and optimized status.
+
+         The latest commit point should always be preserved regardless
+         of the criteria.
+    -->
+    <!--
+    <deletionPolicy class="solr.SolrDeletionPolicy">
+    -->
+      <!-- The number of commit points to be kept -->
+      <!-- <str name="maxCommitsToKeep">1</str> -->
+      <!-- The number of optimized commit points to be kept -->
+      <!-- <str name="maxOptimizedCommitsToKeep">0</str> -->
+      <!--
+          Delete all commit points once they have reached the given age.
+          Supports DateMathParser syntax e.g.
+        -->
+      <!--
+         <str name="maxCommitAge">30MINUTES</str>
+         <str name="maxCommitAge">1DAY</str>
+      -->
+    <!--
+    </deletionPolicy>
+    -->
+
+    <!-- Lucene Infostream
+
+         To aid in advanced debugging, Lucene provides an "InfoStream"
+         of detailed information when indexing.
+
+         Setting the value to true will instruct the underlying Lucene
+         IndexWriter to write its info stream to solr's log. By default,
+         this is enabled here, and controlled through log4j.properties.
+      -->
+     <infoStream>true</infoStream>
+  </indexConfig>
+
+
+  <!-- JMX
+
+       This example enables JMX if and only if an existing MBeanServer
+       is found, use this if you want to configure JMX through JVM
+       parameters. Remove this to disable exposing Solr configuration
+       and statistics to JMX.
+
+       For more details see http://wiki.apache.org/solr/SolrJmx
+    -->
+  <jmx />
+  <!-- If you want to connect to a particular server, specify the
+       agentId
+    -->
+  <!-- <jmx agentId="myAgent" /> -->
+  <!-- If you want to start a new MBeanServer, specify the serviceUrl -->
+  <!-- <jmx serviceUrl="service:jmx:rmi:///jndi/rmi://localhost:9999/solr"/>
+    -->
+
+  <!-- The default high-performance update handler -->
+  <updateHandler class="solr.DirectUpdateHandler2">
+
+    <!-- Enables a transaction log, used for real-time get, durability, and
+         and solr cloud replica recovery.  The log can grow as big as
+         uncommitted changes to the index, so use of a hard autoCommit
+         is recommended (see below).
+         "dir" - the target directory for transaction logs, defaults to the
+                solr data directory.  -->
+    <updateLog>
+      <str name="dir">${solr.ulog.dir:}</str>
+    </updateLog>
+
+    <!-- AutoCommit
+
+         Perform a hard commit automatically under certain conditions.
+         Instead of enabling autoCommit, consider using "commitWithin"
+         when adding documents.
+
+         http://wiki.apache.org/solr/UpdateXmlMessages
+
+         maxDocs - Maximum number of documents to add since the last
+                   commit before automatically triggering a new commit.
+
+         maxTime - Maximum amount of time in ms that is allowed to pass
+                   since a document was added before automatically
+                   triggering a new commit.
+         openSearcher - if false, the commit causes recent index changes
+           to be flushed to stable storage, but does not cause a new
+           searcher to be opened to make those changes visible.
+
+         If the updateLog is enabled, then it's highly recommended to
+         have some sort of hard autoCommit to limit the log size.
+      -->
+     <autoCommit>
+       <maxTime>${solr.autoCommit.maxTime:15000}</maxTime>
+       <openSearcher>false</openSearcher>
+     </autoCommit>
+
+    <!-- softAutoCommit is like autoCommit except it causes a
+         'soft' commit which only ensures that changes are visible
+         but does not ensure that data is synced to disk.  This is
+         faster and more near-realtime friendly than a hard commit.
+      -->
+
+     <autoSoftCommit>
+       <maxTime>${solr.autoSoftCommit.maxTime:-1}</maxTime>
+     </autoSoftCommit>
+
+    <!-- Update Related Event Listeners
+
+         Various IndexWriter related events can trigger Listeners to
+         take actions.
+
+         postCommit - fired after every commit or optimize command
+         postOptimize - fired after every optimize command
+      -->
+    <!-- The RunExecutableListener executes an external command from a
+         hook such as postCommit or postOptimize.
+
+         exe - the name of the executable to run
+         dir - dir to use as the current working directory. (default=".")
+         wait - the calling thread waits until the executable returns.
+                (default="true")
+         args - the arguments to pass to the program.  (default is none)
+         env - environment variables to set.  (default is none)
+      -->
+    <!-- This example shows how RunExecutableListener could be used
+         with the script based replication...
+         http://wiki.apache.org/solr/CollectionDistribution
+      -->
+    <!--
+       <listener event="postCommit" class="solr.RunExecutableListener">
+         <str name="exe">solr/bin/snapshooter</str>
+         <str name="dir">.</str>
+         <bool name="wait">true</bool>
+         <arr name="args"> <str>arg1</str> <str>arg2</str> </arr>
+         <arr name="env"> <str>MYVAR=val1</str> </arr>
+       </listener>
+      -->
+
+  </updateHandler>
+
+  <!-- IndexReaderFactory
+
+       Use the following format to specify a custom IndexReaderFactory,
+       which allows for alternate IndexReader implementations.
+
+       ** Experimental Feature **
+
+       Please note - Using a custom IndexReaderFactory may prevent
+       certain other features from working. The API to
+       IndexReaderFactory may change without warning or may even be
+       removed from future releases if the problems cannot be
+       resolved.
+
+
+       ** Features that may not work with custom IndexReaderFactory **
+
+       The ReplicationHandler assumes a disk-resident index. Using a
+       custom IndexReader implementation may cause incompatibility
+       with ReplicationHandler and may cause replication to not work
+       correctly. See SOLR-1366 for details.
+
+    -->
+  <!--
+  <indexReaderFactory name="IndexReaderFactory" class="package.class">
+    <str name="someArg">Some Value</str>
+  </indexReaderFactory >
+  -->
+  <!-- By explicitly declaring the Factory, the termIndexDivisor can
+       be specified.
+    -->
+  <!--
+     <indexReaderFactory name="IndexReaderFactory"
+                         class="solr.StandardIndexReaderFactory">
+       <int name="setTermIndexDivisor">12</int>
+     </indexReaderFactory >
+    -->
+
+  <!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+       Query section - these settings control query time things like caches
+       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+  <query>
+    <!-- Max Boolean Clauses
+
+         Maximum number of clauses in each BooleanQuery,  an exception
+         is thrown if exceeded.
+
+         ** WARNING **
+
+         This option actually modifies a global Lucene property that
+         will affect all SolrCores.  If multiple solrconfig.xml files
+         disagree on this property, the value at any given moment will
+         be based on the last SolrCore to be initialized.
+
+      -->
+    <maxBooleanClauses>1024</maxBooleanClauses>
+
+
+    <!-- Solr Internal Query Caches
+
+         There are two implementations of cache available for Solr,
+         LRUCache, based on a synchronized LinkedHashMap, and
+         FastLRUCache, based on a ConcurrentHashMap.
+
+         FastLRUCache has faster gets and slower puts in single
+         threaded operation and thus is generally faster than LRUCache
+         when the hit ratio of the cache is high (> 75%), and may be
+         faster under other scenarios on multi-cpu systems.
+    -->
+
+    <!-- Filter Cache
+
+         Cache used by SolrIndexSearcher for filters (DocSets),
+         unordered sets of *all* documents that match a query.  When a
+         new searcher is opened, its caches may be prepopulated or
+         "autowarmed" using data from caches in the old searcher.
+         autowarmCount is the number of items to prepopulate.  For
+         LRUCache, the autowarmed items will be the most recently
+         accessed items.
+
+         Parameters:
+           class - the SolrCache implementation LRUCache or
+               (LRUCache or FastLRUCache)
+           size - the maximum number of entries in the cache
+           initialSize - the initial capacity (number of entries) of
+               the cache.  (see java.util.HashMap)
+           autowarmCount - the number of entries to prepopulate from
+               and old cache.
+      -->
+    <filterCache class="solr.FastLRUCache"
+                 size="512"
+                 initialSize="512"
+                 autowarmCount="0"/>
+
+    <!-- Query Result Cache
+
+         Caches results of searches - ordered lists of document ids
+         (DocList) based on a query, a sort, and the range of documents requested.
+      -->
+    <queryResultCache class="solr.LRUCache"
+                     size="512"
+                     initialSize="512"
+                     autowarmCount="0"/>
+
+    <!-- Document Cache
+
+         Caches Lucene Document objects (the stored fields for each
+         document).  Since Lucene internal document ids are transient,
+         this cache will not be autowarmed.
+      -->
+    <documentCache class="solr.LRUCache"
+                   size="512"
+                   initialSize="512"
+                   autowarmCount="0"/>
+
+    <!-- custom cache currently used by block join -->
+    <cache name="perSegFilter"
+      class="solr.search.LRUCache"
+      size="10"
+      initialSize="0"
+      autowarmCount="10"
+      regenerator="solr.NoOpRegenerator" />
+
+    <!-- Field Value Cache
+
+         Cache used to hold field values that are quickly accessible
+         by document id.  The fieldValueCache is created by default
+         even if not configured here.
+      -->
+    <!--
+       <fieldValueCache class="solr.FastLRUCache"
+                        size="512"
+                        autowarmCount="128"
+                        showItems="32" />
+      -->
+
+    <!-- Custom Cache
+
+         Example of a generic cache.  These caches may be accessed by
+         name through SolrIndexSearcher.getCache(),cacheLookup(), and
+         cacheInsert().  The purpose is to enable easy caching of
+         user/application level data.  The regenerator argument should
+         be specified as an implementation of solr.CacheRegenerator
+         if autowarming is desired.
+      -->
+    <!--
+       <cache name="myUserCache"
+              class="solr.LRUCache"
+              size="4096"
+              initialSize="1024"
+              autowarmCount="1024"
+              regenerator="com.mycompany.MyRegenerator"
+              />
+      -->
+
+
+    <!-- Lazy Field Loading
+
+         If true, stored fields that are not requested will be loaded
+         lazily.  This can result in a significant speed improvement
+         if the usual case is to not load all stored fields,
+         especially if the skipped fields are large compressed text
+         fields.
+    -->
+    <enableLazyFieldLoading>true</enableLazyFieldLoading>
+
+   <!-- Use Filter For Sorted Query
+
+        A possible optimization that attempts to use a filter to
+        satisfy a search.  If the requested sort does not include
+        score, then the filterCache will be checked for a filter
+        matching the query. If found, the filter will be used as the
+        source of document ids, and then the sort will be applied to
+        that.
+
+        For most situations, this will not be useful unless you
+        frequently get the same search repeatedly with different sort
+        options, and none of them ever use "score"
+     -->
+   <!--
+      <useFilterForSortedQuery>true</useFilterForSortedQuery>
+     -->
+
+   <!-- Result Window Size
+
+        An optimization for use with the queryResultCache.  When a search
+        is requested, a superset of the requested number of document ids
+        are collected.  For example, if a search for a particular query
+        requests matching documents 10 through 19, and queryWindowSize is 50,
+        then documents 0 through 49 will be collected and cached.  Any further
+        requests in that range can be satisfied via the cache.
+     -->
+   <queryResultWindowSize>20</queryResultWindowSize>
+
+   <!-- Maximum number of documents to cache for any entry in the
+        queryResultCache.
+     -->
+   <queryResultMaxDocsCached>200</queryResultMaxDocsCached>
+
+   <!-- Query Related Event Listeners
+
+        Various IndexSearcher related events can trigger Listeners to
+        take actions.
+
+        newSearcher - fired whenever a new searcher is being prepared
+        and there is a current searcher handling requests (aka
+        registered).  It can be used to prime certain caches to
+        prevent long request times for certain requests.
+
+        firstSearcher - fired whenever a new searcher is being
+        prepared but there is no current registered searcher to handle
+        requests or to gain autowarming data from.
+
+
+     -->
+    <!-- QuerySenderListener takes an array of NamedList and executes a
+         local query request for each NamedList in sequence.
+      -->
+    <listener event="newSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <!--
+           <lst><str name="q">solr</str><str name="sort">price asc</str></lst>
+           <lst><str name="q">rocks</str><str name="sort">weight asc</str></lst>
+          -->
+      </arr>
+    </listener>
+    <listener event="firstSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <lst>
+          <str name="q">static firstSearcher warming in solrconfig.xml</str>
+        </lst>
+      </arr>
+    </listener>
+
+    <!-- Use Cold Searcher
+
+         If a search request comes in and there is no current
+         registered searcher, then immediately register the still
+         warming searcher and use it.  If "false" then all requests
+         will block until the first searcher is done warming.
+      -->
+    <useColdSearcher>false</useColdSearcher>
+
+    <!-- Max Warming Searchers
+
+         Maximum number of searchers that may be warming in the
+         background concurrently.  An error is returned if this limit
+         is exceeded.
+
+         Recommend values of 1-2 for read-only slaves, higher for
+         masters w/o cache warming.
+      -->
+    <maxWarmingSearchers>2</maxWarmingSearchers>
+
+  </query>
+
+
+  <!-- Request Dispatcher
+
+       This section contains instructions for how the SolrDispatchFilter
+       should behave when processing requests for this SolrCore.
+
+       handleSelect is a legacy option that affects the behavior of requests
+       such as /select?qt=XXX
+
+       handleSelect="true" will cause the SolrDispatchFilter to process
+       the request and dispatch the query to a handler specified by the
+       "qt" param, assuming "/select" isn't already registered.
+
+       handleSelect="false" will cause the SolrDispatchFilter to
+       ignore "/select" requests, resulting in a 404 unless a handler
+       is explicitly registered with the name "/select"
+
+       handleSelect="true" is not recommended for new users, but is the default
+       for backwards compatibility
+    -->
+  <requestDispatcher handleSelect="false" >
+    <!-- Request Parsing
+
+         These settings indicate how Solr Requests may be parsed, and
+         what restrictions may be placed on the ContentStreams from
+         those requests
+
+         enableRemoteStreaming - enables use of the stream.file
+         and stream.url parameters for specifying remote streams.
+
+         multipartUploadLimitInKB - specifies the max size (in KiB) of
+         Multipart File Uploads that Solr will allow in a Request.
+
+         formdataUploadLimitInKB - specifies the max size (in KiB) of
+         form data (application/x-www-form-urlencoded) sent via
+         POST. You can use POST to pass request parameters not
+         fitting into the URL.
+
+         addHttpRequestToContext - if set to true, it will instruct
+         the requestParsers to include the original HttpServletRequest
+         object in the context map of the SolrQueryRequest under the
+         key "httpRequest". It will not be used by any of the existing
+         Solr components, but may be useful when developing custom
+         plugins.
+
+         *** WARNING ***
+         The settings below authorize Solr to fetch remote files, You
+         should make sure your system has some authentication before
+         using enableRemoteStreaming="true"
+
+      -->
+    <requestParsers enableRemoteStreaming="true"
+                    multipartUploadLimitInKB="2048000"
+                    formdataUploadLimitInKB="2048"
+                    addHttpRequestToContext="false"/>
+
+    <!-- HTTP Caching
+
+         Set HTTP caching related parameters (for proxy caches and clients).
+
+         The options below instruct Solr not to output any HTTP Caching
+         related headers
+      -->
+    <httpCaching never304="true" />
+    <!-- If you include a <cacheControl> directive, it will be used to
+         generate a Cache-Control header (as well as an Expires header
+         if the value contains "max-age=")
+
+         By default, no Cache-Control header is generated.
+
+         You can use the <cacheControl> option even if you have set
+         never304="true"
+      -->
+    <!--
+       <httpCaching never304="true" >
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+    <!-- To enable Solr to respond with automatically generated HTTP
+         Caching headers, and to response to Cache Validation requests
+         correctly, set the value of never304="false"
+
+         This will cause Solr to generate Last-Modified and ETag
+         headers based on the properties of the Index.
+
+         The following options can also be specified to affect the
+         values of these headers...
+
+         lastModFrom - the default value is "openTime" which means the
+         Last-Modified value (and validation against If-Modified-Since
+         requests) will all be relative to when the current Searcher
+         was opened.  You can change it to lastModFrom="dirLastMod" if
+         you want the value to exactly correspond to when the physical
+         index was last modified.
+
+         etagSeed="..." is an option you can change to force the ETag
+         header (and validation against If-None-Match requests) to be
+         different even if the index has not changed (ie: when making
+         significant changes to your config file)
+
+         (lastModifiedFrom and etagSeed are both ignored if you use
+         the never304="true" option)
+      -->
+    <!--
+       <httpCaching lastModifiedFrom="openTime"
+                    etagSeed="Solr">
+         <cacheControl>max-age=30, public</cacheControl>
+       </httpCaching>
+      -->
+  </requestDispatcher>
+
+  <!-- Request Handlers
+
+       http://wiki.apache.org/solr/SolrRequestHandler
+
+       Incoming queries will be dispatched to a specific handler by name
+       based on the path specified in the request.
+
+       Legacy behavior: If the request path uses "/select" but no Request
+       Handler has that name, and if handleSelect="true" has been specified in
+       the requestDispatcher, then the Request Handler is dispatched based on
+       the qt parameter.  Handlers without a leading '/' are accessed this way
+       like so: http://host/app/[core/]select?qt=name  If no qt is
+       given, then the requestHandler that declares default="true" will be
+       used or the one named "standard".
+
+       If a Request Handler is declared with startup="lazy", then it will
+       not be initialized until the first request that uses it.
+
+    -->
+  <!-- SearchHandler
+
+       http://wiki.apache.org/solr/SearchHandler
+
+       For processing Search Queries, the primary Request Handler
+       provided with Solr is "SearchHandler" It delegates to a sequent
+       of SearchComponents (see below) and supports distributed
+       queries across multiple shards
+    -->
+  <requestHandler name="/select" class="solr.SearchHandler">
+    <!-- default values for query parameters can be specified, these
+         will be overridden by parameters in the request
+      -->
+     <lst name="defaults">
+       <str name="echoParams">explicit</str>
+       <int name="rows">10</int>
+       <str name="df">text</str>
+     </lst>
+    <!-- In addition to defaults, "appends" params can be specified
+         to identify values which should be appended to the list of
+         multi-val params from the query (or the existing "defaults").
+      -->
+    <!-- In this example, the param "fq=instock:true" would be appended to
+         any query time fq params the user may specify, as a mechanism for
+         partitioning the index, independent of any user selected filtering
+         that may also be desired (perhaps as a result of faceted searching).
+
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "appends" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="appends">
+         <str name="fq">inStock:true</str>
+       </lst>
+      -->
+    <!-- "invariants" are a way of letting the Solr maintainer lock down
+         the options available to Solr clients.  Any params values
+         specified here are used regardless of what values may be specified
+         in either the query, the "defaults", or the "appends" params.
+
+         In this example, the facet.field and facet.query params would
+         be fixed, limiting the facets clients can use.  Faceting is
+         not turned on by default - but if the client does specify
+         facet=true in the request, these are the only facets they
+         will be able to see counts for; regardless of what other
+         facet.field or facet.query params they may specify.
+
+         NOTE: there is *absolutely* nothing a client can do to prevent these
+         "invariants" values from being used, so don't use this mechanism
+         unless you are sure you always want it.
+      -->
+    <!--
+       <lst name="invariants">
+         <str name="facet.field">cat</str>
+         <str name="facet.field">manu_exact</str>
+         <str name="facet.query">price:[* TO 500]</str>
+         <str name="facet.query">price:[500 TO *]</str>
+       </lst>
+      -->
+    <!-- If the default list of SearchComponents is not desired, that
+         list can either be overridden completely, or components can be
+         prepended or appended to the default list.  (see below)
+      -->
+    <!--
+       <arr name="components">
+         <str>nameOfCustomComponent1</str>
+         <str>nameOfCustomComponent2</str>
+       </arr>
+      -->
+    </requestHandler>
+
+  <!-- A request handler that returns indented JSON by default -->
+  <requestHandler name="/query" class="solr.SearchHandler">
+     <lst name="defaults">
+       <str name="echoParams">explicit</str>
+       <str name="wt">json</str>
+       <str name="indent">true</str>
+       <str name="df">text</str>
+     </lst>
+  </requestHandler>
+
+
+  <!-- realtime get handler, guaranteed to return the latest stored fields of
+       any document, without the need to commit or open a new searcher.  The
+       current implementation relies on the updateLog feature being enabled. -->
+  <requestHandler name="/get" class="solr.RealTimeGetHandler">
+     <lst name="defaults">
+       <str name="omitHeader">true</str>
+       <str name="wt">json</str>
+       <str name="indent">true</str>
+     </lst>
+  </requestHandler>
+
+  <!-- Update Request Handler.
+
+       http://wiki.apache.org/solr/UpdateXmlMessages
+
+       The canonical Request Handler for Modifying the Index through
+       commands specified using XML, JSON, CSV, or JAVABIN
+
+       Note: Since solr1.1 requestHandlers requires a valid content
+       type header if posted in the body. For example, curl now
+       requires: -H 'Content-type:text/xml; charset=utf-8'
+
+       To override the request content type and force a specific
+       Content-type, use the request parameter:
+         ?update.contentType=text/csv
+
+       This handler will pick a response format to match the input
+       if the 'wt' parameter is not explicit
+    -->
+  <requestHandler name="/update" class="solr.UpdateRequestHandler">
+    <!-- See below for information on defining
+         updateRequestProcessorChains that can be used by name
+         on each Update Request
+      -->
+    <!--
+       <lst name="defaults">
+         <str name="update.chain">dedupe</str>
+       </lst>
+       -->
+  </requestHandler>
+
+  <!-- for back compat with clients using /update/json and /update/csv -->
+  <requestHandler name="/update/json" class="solr.JsonUpdateRequestHandler">
+        <lst name="defaults">
+         <str name="stream.contentType">application/json</str>
+       </lst>
+  </requestHandler>
+  <requestHandler name="/update/csv" class="solr.CSVRequestHandler">
+        <lst name="defaults">
+         <str name="stream.contentType">application/csv</str>
+       </lst>
+  </requestHandler>
+
+  <!-- Admin Handlers
+
+       Admin Handlers - This will register all the standard admin
+       RequestHandlers.
+    -->
+  <requestHandler name="/admin/"
+                  class="solr.admin.AdminHandlers" />
+
+  <!-- ping/healthcheck -->
+  <requestHandler name="/admin/ping" class="solr.PingRequestHandler">
+    <lst name="invariants">
+      <str name="q">solrpingquery</str>
+    </lst>
+    <lst name="defaults">
+      <str name="echoParams">all</str>
+    </lst>
+    <!-- An optional feature of the PingRequestHandler is to configure the
+         handler with a "healthcheckFile" which can be used to enable/disable
+         the PingRequestHandler.
+         relative paths are resolved against the data dir
+      -->
+    <!-- <str name="healthcheckFile">server-enabled.txt</str> -->
+  </requestHandler>
+
+  <!-- Solr Replication
+
+       The SolrReplicationHandler supports replicating indexes from a
+       "master" used for indexing and "slaves" used for queries.
+
+       http://wiki.apache.org/solr/SolrReplication
+
+       It is also necessary for SolrCloud to function (in Cloud mode, the
+       replication handler is used to bulk transfer segments when nodes
+       are added or need to recover).
+
+       https://wiki.apache.org/solr/SolrCloud/
+    -->
+  <requestHandler name="/replication" class="solr.ReplicationHandler" >
+    <!--
+       To enable simple master/slave replication, uncomment one of the
+       sections below, depending on whether this solr instance should be
+       the "master" or a "slave".  If this instance is a "slave" you will
+       also need to fill in the masterUrl to point to a real machine.
+    -->
+    <!--
+       <lst name="master">
+         <str name="replicateAfter">commit</str>
+         <str name="replicateAfter">startup</str>
+         <str name="confFiles">schema.xml,stopwords.txt</str>
+       </lst>
+    -->
+    <!--
+       <lst name="slave">
+         <str name="masterUrl">http://your-master-hostname:8983/solr</str>
+         <str name="pollInterval">00:00:60</str>
+       </lst>
+    -->
+  </requestHandler>
+
+  <!-- Update Processors
+
+       Chains of Update Processor Factories for dealing with Update
+       Requests can be declared, and then used by name in Update
+       Request Processors
+
+       http://wiki.apache.org/solr/UpdateRequestProcessor
+
+    -->
+
+  <queryResponseWriter name="json" class="solr.JSONResponseWriter" />
+
+  <!-- Legacy config for the admin interface -->
+  <admin>
+    <defaultQuery>*:*</defaultQuery>
+  </admin>
+
+</config>
diff --git a/src/sentry/search/solr/data/stopwords.txt b/src/sentry/search/solr/data/stopwords.txt
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/src/sentry/tasks/cleanup.py b/src/sentry/tasks/cleanup.py
index 09a82cf894..6bdd8d42b0 100644
--- a/src/sentry/tasks/cleanup.py
+++ b/src/sentry/tasks/cleanup.py
@@ -27,7 +27,8 @@ def cleanup(days=30, project=None, chunk_size=1000, **kwargs):
     from sentry.models import (
         Group, Event, GroupCountByMinute, EventMapping,
         GroupTag, TagValue, ProjectCountByMinute, Alert,
-        SearchDocument, Activity, LostPasswordHash)
+        Activity, LostPasswordHash)
+    from sentry.search.django.models import SearchDocument
 
     GENERIC_DELETES = (
         (SearchDocument, 'date_changed'),
diff --git a/src/sentry/tasks/index.py b/src/sentry/tasks/index.py
index 167d8260f9..261d7713d6 100644
--- a/src/sentry/tasks/index.py
+++ b/src/sentry/tasks/index.py
@@ -6,11 +6,13 @@ sentry.tasks.index
 :license: BSD, see LICENSE for more details.
 """
 
+from __future__ import absolute_import
+
 from celery.task import task
 
 
 @task(name='sentry.tasks.index.index_event', queue='search')
 def index_event(event, **kwargs):
-    from sentry.models import SearchDocument
+    from sentry import app
 
-    SearchDocument.objects.index(event)
+    app.search.index(event.group, event)
diff --git a/src/sentry/web/frontend/groups.py b/src/sentry/web/frontend/groups.py
index 8f8fa43c57..3498b6cb8a 100644
--- a/src/sentry/web/frontend/groups.py
+++ b/src/sentry/web/frontend/groups.py
@@ -21,18 +21,20 @@ from django.http import HttpResponse, HttpResponseRedirect, Http404
 from django.shortcuts import get_object_or_404
 from django.utils import timezone
 
+from sentry import app
 from sentry.constants import (
     SORT_OPTIONS, SEARCH_SORT_OPTIONS, SORT_CLAUSES,
     MYSQL_SORT_CLAUSES, SQLITE_SORT_CLAUSES, MEMBER_USER,
     SCORE_CLAUSES, MYSQL_SCORE_CLAUSES, SQLITE_SCORE_CLAUSES,
     ORACLE_SORT_CLAUSES, ORACLE_SCORE_CLAUSES,
     MSSQL_SORT_CLAUSES, MSSQL_SCORE_CLAUSES, DEFAULT_SORT_OPTION,
-    SEARCH_DEFAULT_SORT_OPTION, MAX_JSON_RESULTS)
+    SEARCH_DEFAULT_SORT_OPTION, MAX_JSON_RESULTS
+)
 from sentry.db.models import create_or_update
 from sentry.filters import get_filters
 from sentry.models import (
-    Project, Group, Event, SearchDocument, Activity, EventMapping, TagKey,
-    GroupSeen)
+    Project, Group, Event, Activity, EventMapping, TagKey, GroupSeen
+)
 from sentry.permissions import can_admin_group, can_create_projects
 from sentry.plugins import plugins
 from sentry.utils import json
@@ -320,7 +322,7 @@ def search(request, team, project):
         #         'project': project,
         #     }, request)
     else:
-        documents = list(SearchDocument.objects.search(project, query, sort_by=sort))
+        documents = list(app.search.query(project, query, sort_by=sort))
         groups = Group.objects.in_bulk([d.group_id for d in documents])
 
         event_list = []
diff --git a/tests/sentry/manager/tests.py b/tests/sentry/manager/tests.py
index 2bfff17f03..6ca2993a28 100644
--- a/tests/sentry/manager/tests.py
+++ b/tests/sentry/manager/tests.py
@@ -12,7 +12,8 @@ from sentry.interfaces import Interface
 from sentry.manager import get_checksum_from_event
 from sentry.models import (
     Event, Group, Project, GroupCountByMinute, ProjectCountByMinute,
-    SearchDocument, Team, EventMapping, User, AccessGroup)
+    Team, EventMapping, User, AccessGroup
+)
 from sentry.utils.db import has_trending  # NOQA
 from sentry.testutils import TestCase
 
@@ -23,14 +24,6 @@ class DummyInterface(Interface):
 
 
 class SentryManagerTest(TestCase):
-    @mock.patch('sentry.models.SearchDocument.objects.index')
-    def test_broken_search_index(self, index):
-        index.side_effect = Exception()
-
-        event = Group.objects.from_kwargs(1, message='foo')
-        self.assertEquals(event.message, 'foo')
-        self.assertEquals(event.project_id, 1)
-
     @mock.patch('sentry.signals.regression_signal.send')
     def test_broken_regression_signal(self, send):
         send.side_effect = Exception()
@@ -224,29 +217,6 @@ class SentryManagerTest(TestCase):
         self.assertEquals(res.times_seen, 1)
 
 
-class SearchManagerTest(TestCase):
-    def test_search(self):
-        project = Project.objects.all()[0]
-        group = Group.objects.create(project=project, message='foo', checksum='a' * 32)
-        doc = SearchDocument.objects.create(
-            project=project,
-            group=group,
-            status=group.status,
-            total_events=1,
-            date_added=group.first_seen,
-            date_changed=group.last_seen,
-        )
-        doc.token_set.create(
-            field='text',
-            token='foo',
-        )
-
-        results = list(SearchDocument.objects.search(project, query='foo'))
-        self.assertEquals(len(results), 1)
-        # This uses a raw query set so we have to check the id
-        self.assertEquals(results[0].id, doc.id)
-
-
 @pytest.mark.skipif('not has_trending()')
 class TrendsTest(TestCase):
     def test_accelerated_works_at_all(self):
diff --git a/tests/sentry/search/django/__init__.py b/tests/sentry/search/django/__init__.py
new file mode 100644
index 0000000000..e69de29bb2
diff --git a/tests/sentry/search/tests.py b/tests/sentry/search/django/tests.py
similarity index 71%
rename from tests/sentry/search/tests.py
rename to tests/sentry/search/django/tests.py
index b2393e58c1..acabeabdf7 100644
--- a/tests/sentry/search/tests.py
+++ b/tests/sentry/search/django/tests.py
@@ -2,7 +2,9 @@
 
 from __future__ import absolute_import
 
-from sentry.models import SearchDocument
+from exam import fixture
+
+from sentry.search.django.backend import DjangoSearchBackend
 from sentry.testutils import TestCase
 
 
@@ -12,17 +14,21 @@ def norm_date(dt):
 
 
 class SearchIndexTest(TestCase):
+    @fixture
+    def backend(self):
+        return DjangoSearchBackend()
+
     def test_index_behavior(self):
         event = self.event
 
-        doc = SearchDocument.objects.index(event)
+        doc = self.backend.index(event.group, event)
         assert doc.project == event.project
         assert doc.group == event.group
         assert doc.total_events == 1
         assert norm_date(doc.date_added) == norm_date(event.group.first_seen)
         assert norm_date(doc.date_changed) == norm_date(event.group.last_seen)
 
-        doc = SearchDocument.objects.index(event)
+        doc = self.backend.index(event.group, event)
         assert doc.project == event.project
         assert doc.group == event.group
         assert doc.total_events == 2
@@ -31,9 +37,9 @@ class SearchIndexTest(TestCase):
 
     def test_search(self):
         event = self.event
-        doc = SearchDocument.objects.index(event)
+        doc = self.backend.index(event.group, event)
 
-        results = list(SearchDocument.objects.search(event.project, event.message.upper()))
+        results = self.backend.query(event.project, event.message.upper())
         assert len(results) == 1
         [res] = results
         assert res.id == doc.id
