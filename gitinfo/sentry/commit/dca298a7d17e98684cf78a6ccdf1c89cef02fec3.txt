commit dca298a7d17e98684cf78a6ccdf1c89cef02fec3
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Tue Dec 6 17:20:34 2016 +0100

    Add basic support for reprocessing internally

diff --git a/src/sentry/reprocessing.py b/src/sentry/reprocessing.py
index eeb3e8c445..6a8111e0cc 100644
--- a/src/sentry/reprocessing.py
+++ b/src/sentry/reprocessing.py
@@ -1,6 +1,9 @@
 from __future__ import absolute_import
 
-from sentry.models import ProcessingIssue, ProcessingIssueGroup
+from sentry.models import ProcessingIssue, ProcessingIssueGroup, GroupStatus, \
+    Event
+from sentry.utils.query import batched_queryset_iter
+from sentry.tasks.store import preprocess_event
 
 
 def record_processing_issue(event_data, type, key, release_bound=True,
@@ -34,3 +37,37 @@ def store_processing_issues(issues, group, release=None):
             issue=issue,
             defaults={'data': d['group_data']},
         )[0]
+
+
+def trigger_reprocessing(project, type, key=None):
+    """Triggers reprocessing of issues for the given project and type.  If
+    a key is given only groups matching that key are reprocessed.
+    """
+    q = ProcessingIssueGroup.objects.filter(
+        issue__project=project,
+        issue__type=type,
+        group__status=GroupStatus.ON_HOLD
+    )
+    if key is not None:
+        q = q.filter(issue__key=key)
+
+    for pig in q.select_related('group'):
+        pig.group.project = project
+        trigger_group_reprocessing(pig.group)
+
+
+def trigger_group_reprocessing(group):
+    """Triggers reprocessing for an entire group."""
+    # Sanity check just in case
+    if group.status != GroupStatus.ON_HOLD:
+        return
+
+    q = Event.objects.filter(group=group)
+
+    for events in batched_queryset_iter(q):
+        Event.objects.bind_nodes(events, 'data')
+        for event in events:
+            preprocess_event.delay(
+                data=event.data,
+                reprocesses_event_id=event.id
+            )
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index d6fe356c85..6b98f10218 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -13,6 +13,7 @@ import logging
 from raven.contrib.django.models import client as Raven
 from time import time
 
+from sentry.models import Event
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
@@ -27,7 +28,8 @@ error_logger = logging.getLogger('sentry.errors.events')
     time_limit=65,
     soft_time_limit=60,
 )
-def preprocess_event(cache_key=None, data=None, start_time=None, **kwargs):
+def preprocess_event(cache_key=None, data=None, start_time=None,
+                     reprocesses_event_id=None, **kwargs):
     from sentry.plugins import plugins
 
     if cache_key:
@@ -51,14 +53,16 @@ def preprocess_event(cache_key=None, data=None, start_time=None, **kwargs):
         for processor in (processors or ()):
             # On the first processor found, we just defer to the process_event
             # queue to handle the actual work.
-            process_event.delay(cache_key=cache_key, start_time=start_time)
+            process_event.delay(cache_key=cache_key, start_time=start_time,
+                                reprocesses_event_id=reprocesses_event_id)
             return
 
     # If we get here, that means the event had no preprocessing needed to be done
     # so we can jump directly to save_event
     if cache_key:
         data = None
-    save_event.delay(cache_key=cache_key, data=data, start_time=start_time)
+    save_event.delay(cache_key=cache_key, data=data, start_time=start_time,
+                     reprocesses_event_id=reprocesses_event_id)
 
 
 @instrumented_task(
@@ -67,7 +71,8 @@ def preprocess_event(cache_key=None, data=None, start_time=None, **kwargs):
     time_limit=65,
     soft_time_limit=60,
 )
-def process_event(cache_key, start_time=None, **kwargs):
+def process_event(cache_key, start_time=None, reprocesses_event_id=None,
+                  **kwargs):
     from sentry.plugins import plugins
 
     data = default_cache.get(cache_key)
@@ -103,7 +108,8 @@ def process_event(cache_key, start_time=None, **kwargs):
 @instrumented_task(
     name='sentry.tasks.store.save_event',
     queue='events.save_event')
-def save_event(cache_key=None, data=None, start_time=None, **kwargs):
+def save_event(cache_key=None, data=None, start_time=None,
+               reprocesses_event_id=None, **kwargs):
     """
     Saves an event to the database.
     """
@@ -112,6 +118,16 @@ def save_event(cache_key=None, data=None, start_time=None, **kwargs):
     if cache_key:
         data = default_cache.get(cache_key)
 
+    # If we are reprocessing an old event we just delete it here.
+    # XXX(mitsuhiko): this is most likely completely wrong.
+    if reprocesses_event_id is not None:
+        try:
+            event = Event.objects.get(pk=reprocesses_event_id)
+        except Event.DoesNotExist:
+            pass
+        else:
+            event.delete()
+
     if data is None:
         metrics.incr('events.failed', tags={'reason': 'cache', 'stage': 'post'})
         return
diff --git a/src/sentry/utils/query.py b/src/sentry/utils/query.py
index da7f56fa00..67ffde24a4 100644
--- a/src/sentry/utils/query.py
+++ b/src/sentry/utils/query.py
@@ -22,6 +22,20 @@ class InvalidQuerySetError(ValueError):
     pass
 
 
+def batched_queryset_iter(*args, **kwargs):
+    batch_size = kwargs.pop('batch_size', 100)
+    qsw = RangeQuerySetWrapper(*args, **kwargs)
+    batch = []
+    for item in qsw:
+        if len(batch) < batch_size:
+            batch.append(item)
+        else:
+            yield batch
+            batch = []
+    if batch:
+        yield batch
+
+
 class RangeQuerySetWrapper(object):
     """
     Iterates through a queryset by chunking results by ``step`` and using GREATER THAN
