commit 905564ac47938160c547af0f6ed86f248eaccd36
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Wed May 29 16:50:56 2019 +0200

    fix: Normalize minidumps after processing (#13358)
    
    I want to try this out because processing plugins might create too old timestamps or too big data.
    
    This will possibly enable us to disable trimming in renormalization again.

diff --git a/src/sentry/constants.py b/src/sentry/constants.py
index 6631d54d33..f4978f460c 100644
--- a/src/sentry/constants.py
+++ b/src/sentry/constants.py
@@ -13,6 +13,7 @@ from __future__ import absolute_import, print_function
 import logging
 import os.path
 import six
+from datetime import timedelta
 
 from collections import OrderedDict, namedtuple
 from django.conf import settings
@@ -20,6 +21,7 @@ from django.utils.translation import ugettext_lazy as _
 from operator import attrgetter
 
 from sentry.utils.integrationdocs import load_doc
+from sentry.utils.geo import rust_geoip
 
 
 def get_all_languages():
@@ -395,3 +397,20 @@ class SentryAppStatus(object):
 StatsPeriod = namedtuple('StatsPeriod', ('segments', 'interval'))
 
 LEGACY_RATE_LIMIT_OPTIONS = frozenset(('sentry:project-rate-limit', 'sentry:account-rate-limit'))
+
+
+# We need to limit the range of valid timestamps of an event because that
+# timestamp is used to control data retention.
+MAX_SECS_IN_FUTURE = 60
+MAX_SECS_IN_PAST = 2592000  # 30 days
+ALLOWED_FUTURE_DELTA = timedelta(seconds=MAX_SECS_IN_FUTURE)
+
+DEFAULT_STORE_NORMALIZER_ARGS = dict(
+    geoip_lookup=rust_geoip,
+    stacktrace_frames_hard_limit=settings.SENTRY_STACKTRACE_FRAMES_HARD_LIMIT,
+    max_stacktrace_frames=settings.SENTRY_MAX_STACKTRACE_FRAMES,
+    valid_platforms=list(VALID_PLATFORMS),
+    max_secs_in_future=MAX_SECS_IN_FUTURE,
+    max_secs_in_past=MAX_SECS_IN_PAST,
+    enable_trimming=True,
+)
diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index 396ced53ae..29b023f477 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -20,7 +20,8 @@ from django.utils.encoding import force_text
 
 from sentry import buffer, eventtypes, eventstream, features, tagstore, tsdb, filters
 from sentry.constants import (
-    LOG_LEVELS, LOG_LEVELS_MAP, VALID_PLATFORMS, MAX_TAG_VALUE_LENGTH,
+    DEFAULT_STORE_NORMALIZER_ARGS, LOG_LEVELS, LOG_LEVELS_MAP,
+    MAX_TAG_VALUE_LENGTH, MAX_SECS_IN_FUTURE, MAX_SECS_IN_PAST
 )
 from sentry.grouping.api import get_grouping_config_dict_for_project, \
     get_grouping_config_dict_for_event_data, load_grouping_config, \
@@ -57,7 +58,6 @@ from sentry.utils.data_filters import (
 )
 from sentry.utils.dates import to_timestamp
 from sentry.utils.db import is_postgres
-from sentry.utils.geo import rust_geoip
 from sentry.utils.safe import safe_execute, trim, get_path, setdefault_path
 from sentry.stacktraces.processing import normalize_stacktraces_for_grouping
 from sentry.culprit import generate_culprit
@@ -66,9 +66,6 @@ from sentry.culprit import generate_culprit
 logger = logging.getLogger("sentry.events")
 
 
-MAX_SECS_IN_FUTURE = 60
-ALLOWED_FUTURE_DELTA = timedelta(seconds=MAX_SECS_IN_FUTURE)
-MAX_SECS_IN_PAST = 2592000  # 30 days
 SECURITY_REPORT_INTERFACES = (
     "csp",
     "hpkp",
@@ -355,21 +352,15 @@ class EventManager(object):
 
         from semaphore.processing import StoreNormalizer
         rust_normalizer = StoreNormalizer(
-            geoip_lookup=rust_geoip,
             project_id=self._project.id if self._project else None,
             client_ip=self._client_ip,
             client=self._auth.client if self._auth else None,
             key_id=six.text_type(self._key.id) if self._key else None,
             grouping_config=self._grouping_config,
             protocol_version=six.text_type(self.version) if self.version is not None else None,
-            stacktrace_frames_hard_limit=settings.SENTRY_STACKTRACE_FRAMES_HARD_LIMIT,
-            max_stacktrace_frames=settings.SENTRY_MAX_STACKTRACE_FRAMES,
-            valid_platforms=list(VALID_PLATFORMS),
-            max_secs_in_future=MAX_SECS_IN_FUTURE,
-            max_secs_in_past=MAX_SECS_IN_PAST,
-            enable_trimming=True,
             is_renormalize=self._is_renormalize,
             remove_other=self._remove_other,
+            **DEFAULT_STORE_NORMALIZER_ARGS
         )
 
         self._data = CanonicalKeyDict(
diff --git a/src/sentry/options/defaults.py b/src/sentry/options/defaults.py
index 2bde7e7187..3c4231315a 100644
--- a/src/sentry/options/defaults.py
+++ b/src/sentry/options/defaults.py
@@ -181,3 +181,7 @@ register('store.empty-interface-sample-rate', default=0.0)
 register('symbolicator.minidump-refactor-projects-opt-in', type=Sequence, default=[])  # unused
 register('symbolicator.minidump-refactor-projects-opt-out', type=Sequence, default=[])  # unused
 register('symbolicator.minidump-refactor-random-sampling', default=0.0)  # unused
+
+
+# Normalization after processors
+register('store.normalize-after-processing', default=0.0)
diff --git a/src/sentry/search/snuba/backend.py b/src/sentry/search/snuba/backend.py
index 2f6a2e200d..cafa6c1cc2 100644
--- a/src/sentry/search/snuba/backend.py
+++ b/src/sentry/search/snuba/backend.py
@@ -18,7 +18,7 @@ from sentry.api.event_search import (
     InvalidSearchQuery,
 )
 from sentry.api.paginator import DateTimePaginator, SequencePaginator, Paginator
-from sentry.event_manager import ALLOWED_FUTURE_DELTA
+from sentry.constants import ALLOWED_FUTURE_DELTA
 from sentry.models import Group
 from sentry.search.base import SearchBackend
 from sentry.utils import snuba, metrics
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index df5764cba3..6c1acdf695 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -10,13 +10,17 @@ from __future__ import absolute_import
 
 import logging
 from datetime import datetime
+import random
 import six
 
 from time import time
 from django.conf import settings
 from django.utils import timezone
 
-from sentry import features, reprocessing
+from semaphore.processing import StoreNormalizer
+
+from sentry import features, reprocessing, options
+from sentry.constants import DEFAULT_STORE_NORMALIZER_ARGS
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
@@ -40,6 +44,11 @@ REPROCESSING_DEFAULT = False
 CRASH_REPORT_TYPES = ('event.minidump', )
 
 
+def _should_normalize_after_processing():
+    value = options.get('store.normalize-after-processing')
+    return value and random.random() < value
+
+
 class RetryProcessing(Exception):
     pass
 
@@ -264,7 +273,20 @@ def _do_process_event(cache_key, start_time, event_id, process_task,
         data = dict(data.items())
 
     if has_changed:
+        if _should_normalize_after_processing():
+            # Run some of normalization again such that we don't:
+            # - persist e.g. incredibly large stacktraces from minidumps
+            # - store event timestamps that are older than our retention window
+            #   (also happening with minidumps)
+            normalizer = StoreNormalizer(
+                remove_other=False,
+                is_renormalize=True,
+                **DEFAULT_STORE_NORMALIZER_ARGS
+            )
+            data = normalizer.normalize_event(dict(data))
+
         issues = data.get('processing_issues')
+
         try:
             if issues and create_failed_event(
                 cache_key, project_id, list(issues.values()),
diff --git a/tests/sentry/tasks/test_store.py b/tests/sentry/tasks/test_store.py
index a8e568883b..de595f9bac 100644
--- a/tests/sentry/tasks/test_store.py
+++ b/tests/sentry/tasks/test_store.py
@@ -26,7 +26,7 @@ class BasicPreprocessorPlugin(Plugin2):
             return [remove_extra, lambda x: None]
 
         if data.get('platform') == 'noop':
-            return [lambda data: data]
+            return [lambda data: None]
 
         if data.get('platform') == 'holdmeclose':
             return [put_on_hold]
@@ -103,17 +103,11 @@ class StoreTasksTest(PluginTestCase):
         process_event(cache_key='e:1', start_time=1)
 
         # The event mutated, so make sure we save it back
-        mock_default_cache.set.assert_called_once_with(
-            'e:1',
-            {
-                'project': project.id,
-                'platform': 'mattlang',
-                'logentry': {
-                    'formatted': 'test',
-                },
-            },
-            3600,
-        )
+        (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+
+        assert key == 'e:1'
+        assert 'extra' not in event
+        assert duration == 3600
 
         mock_save_event.delay.assert_called_once_with(
             cache_key='e:1', data=None, start_time=1, event_id=None,
@@ -141,7 +135,7 @@ class StoreTasksTest(PluginTestCase):
         process_event(cache_key='e:1', start_time=1)
 
         # The event did not mutate, so we shouldn't reset it in cache
-        mock_default_cache.set.call_count == 0
+        assert mock_default_cache.set.call_count == 0
 
         mock_save_event.delay.assert_called_once_with(
             cache_key='e:1', data=None, start_time=1, event_id=None,
@@ -168,19 +162,10 @@ class StoreTasksTest(PluginTestCase):
 
         process_event(cache_key='e:1', start_time=1)
 
-        mock_default_cache.set.assert_called_once_with(
-            'e:1', {
-                'project': project.id,
-                'platform': 'holdmeclose',
-                'logentry': {
-                    'formatted': 'test',
-                },
-                'extra': {
-                    'foo': 'bar'
-                },
-                'unprocessed': True,
-            }, 3600
-        )
+        (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+        assert key == 'e:1'
+        assert event['unprocessed'] is True
+        assert duration == 3600
 
         mock_save_event.delay.assert_called_once_with(
             cache_key='e:1', data=None, start_time=1, event_id=None,
