commit 37447344a796caa1359ec20e361ad7cb57cb5721
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Tue Mar 5 16:05:27 2019 -0600

    feat: Add options to use Kafka for preprocess/process/event_save tasks (#12159)

diff --git a/requirements-optional.txt b/requirements-optional.txt
index e1020cf725..194728c7b6 100644
--- a/requirements-optional.txt
+++ b/requirements-optional.txt
@@ -1,3 +1,4 @@
+batching-kafka-consumer==0.0.3
 confluent-kafka==0.11.5
 GeoIP==1.3.2
 google-cloud-pubsub>=0.35.4,<0.36.0
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index aeebc1be55..d650ea2209 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -867,6 +867,8 @@ SENTRY_FEATURES = {
     'projects:sample-events': True,
     # Enable functionality to trigger service hooks upon event ingestion.
     'projects:servicehooks': False,
+    # Use Kafka (instead of Celery) for ingestion pipeline.
+    'projects:kafka-ingest': False,
 
     # Don't add feature defaults down here! Please add them in their associated
     # group sorted alphabetically.
@@ -1548,3 +1550,34 @@ INVALID_EMAIL_ADDRESS_PATTERN = re.compile(r'\@qq\.com$', re.I)
 SENTRY_USER_PERMISSIONS = (
     'broadcasts.admin',
 )
+
+KAFKA_CLUSTERS = {
+    'default': {
+        'bootstrap.servers': 'localhost:9092',
+        'message.max.bytes': 50000000,  # 50MB, default is 1MB
+    }
+}
+
+KAFKA_PREPROCESS = 'events-preprocess'
+KAFKA_PROCESS = 'events-process'
+KAFKA_SAVE = 'events-save'
+KAFKA_EVENTS = 'events'
+
+KAFKA_TOPICS = {
+    KAFKA_PREPROCESS: {
+        'cluster': 'default',
+        'topic': KAFKA_PREPROCESS,
+    },
+    KAFKA_PROCESS: {
+        'cluster': 'default',
+        'topic': KAFKA_PROCESS,
+    },
+    KAFKA_SAVE: {
+        'cluster': 'default',
+        'topic': KAFKA_SAVE,
+    },
+    KAFKA_EVENTS: {
+        'cluster': 'default',
+        'topic': KAFKA_EVENTS,
+    },
+}
diff --git a/src/sentry/consumer.py b/src/sentry/consumer.py
new file mode 100644
index 0000000000..44243242e4
--- /dev/null
+++ b/src/sentry/consumer.py
@@ -0,0 +1,69 @@
+from __future__ import absolute_import, print_function
+
+from batching_kafka_consumer import AbstractBatchWorker
+
+from django.conf import settings
+
+import sentry.tasks.store as store_tasks
+from sentry.utils import json
+
+
+class ConsumerWorker(AbstractBatchWorker):
+    def __init__(self):
+        self.dispatch = {}
+        for key, handler in (
+            (settings.KAFKA_PREPROCESS, self.handle_preprocess),
+            (settings.KAFKA_PROCESS, self.handle_process),
+            (settings.KAFKA_SAVE, self.handle_save)
+        ):
+            topic = settings.KAFKA_TOPICS[key]['topic']
+            self.dispatch[topic] = handler
+
+    def handle_preprocess(self, message):
+        data = message['data']
+        event_id = data['event_id']
+        cache_key = message['cache_key']
+        start_time = message['start_time']
+        process_task = (
+            store_tasks.process_event_from_reprocessing
+            if message['from_reprocessing']
+            else store_tasks.process_event
+        )
+
+        store_tasks._do_preprocess_event(cache_key, data, start_time, event_id, process_task)
+
+    def handle_process(self, message):
+        data = message['data']
+        event_id = data['event_id']
+        cache_key = message['cache_key']
+        start_time = message['start_time']
+
+        if message['from_reprocessing']:
+            task = store_tasks.process_event_from_reprocessing
+        else:
+            task = store_tasks.process_event
+
+        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
+
+    def handle_save(self, message):
+        data = message['data']
+        event_id = data['event_id']
+        cache_key = message['cache_key']
+        start_time = message['start_time']
+        project_id = data['project']
+
+        store_tasks._do_save_event(cache_key, data, start_time, event_id, project_id)
+
+    def process_message(self, message):
+        topic = message.topic()
+        return self._handle(topic, json.loads(message.value()))
+
+    def _handle(self, topic, message):
+        handler = self.dispatch[topic]
+        return handler(message)
+
+    def flush_batch(self, batch):
+        pass
+
+    def shutdown(self):
+        pass
diff --git a/src/sentry/coreapi.py b/src/sentry/coreapi.py
index 9edef871b5..cdc437134f 100644
--- a/src/sentry/coreapi.py
+++ b/src/sentry/coreapi.py
@@ -17,18 +17,20 @@ import re
 import six
 import zlib
 
+from django.conf import settings
 from django.core.exceptions import SuspiciousOperation
 from django.utils.crypto import constant_time_compare
 from gzip import GzipFile
 from six import BytesIO
 from time import time
 
+from sentry import features
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.models import ProjectKey
 from sentry.tasks.store import preprocess_event, \
     preprocess_event_from_reprocessing
-from sentry.utils import json
+from sentry.utils import kafka, json
 from sentry.utils.auth import parse_auth_header
 from sentry.utils.http import origin_from_request
 from sentry.utils.strings import decompress
@@ -166,7 +168,7 @@ class ClientApiHelper(object):
         if start_time is None:
             start_time = time()
 
-        # we might be passed some sublcasses of dict that fail dumping
+        # we might be passed some subclasses of dict that fail dumping
         if isinstance(data, CANONICAL_TYPES):
             data = dict(data.items())
 
@@ -180,10 +182,25 @@ class ClientApiHelper(object):
         if attachments is not None:
             attachment_cache.set(cache_key, attachments, cache_timeout)
 
-        task = from_reprocessing and \
-            preprocess_event_from_reprocessing or preprocess_event
-        task.delay(cache_key=cache_key, start_time=start_time,
-                   event_id=data['event_id'])
+        # NOTE: Project is bound to the context in most cases in production, which
+        # is enough for us to do `projects:kafka-ingest` testing.
+        project = self.context and self.context.project
+
+        if project and features.has('projects:kafka-ingest', project=project):
+            kafka.produce_sync(
+                settings.KAFKA_PREPROCESS,
+                value=json.dumps({
+                    'cache_key': cache_key,
+                    'start_time': start_time,
+                    'from_reprocessing': from_reprocessing,
+                    'data': data,
+                }),
+            )
+        else:
+            task = from_reprocessing and \
+                preprocess_event_from_reprocessing or preprocess_event
+            task.delay(cache_key=cache_key, start_time=start_time,
+                       event_id=data['event_id'])
 
 
 @six.add_metaclass(abc.ABCMeta)
diff --git a/src/sentry/eventstream/kafka/backend.py b/src/sentry/eventstream/kafka/backend.py
index 9e031baf85..6cfbe82f44 100644
--- a/src/sentry/eventstream/kafka/backend.py
+++ b/src/sentry/eventstream/kafka/backend.py
@@ -3,29 +3,26 @@ from __future__ import absolute_import
 import logging
 import six
 
-from confluent_kafka import OFFSET_INVALID, Producer, TopicPartition
+from confluent_kafka import OFFSET_INVALID, TopicPartition
+from django.conf import settings
 from django.utils.functional import cached_property
 
 from sentry.eventstream.kafka.consumer import SynchronizedConsumer
 from sentry.eventstream.kafka.protocol import get_task_kwargs_for_message
 from sentry.eventstream.snuba import SnubaProtocolEventStream
-from sentry.utils import json
+from sentry.utils import json, kafka
 
 
 logger = logging.getLogger(__name__)
 
 
 class KafkaEventStream(SnubaProtocolEventStream):
-    def __init__(self, publish_topic='events', producer_configuration=None, **options):
-        if producer_configuration is None:
-            producer_configuration = {}
-
-        self.publish_topic = publish_topic
-        self.producer_configuration = producer_configuration
+    def __init__(self, **options):
+        self.topic = settings.KAFKA_TOPICS[settings.KAFKA_EVENTS]['topic']
 
     @cached_property
     def producer(self):
-        return Producer(self.producer_configuration)
+        return kafka.producers.get(settings.KAFKA_EVENTS)
 
     def delivery_callback(self, error, message):
         if error is not None:
@@ -49,7 +46,7 @@ class KafkaEventStream(SnubaProtocolEventStream):
 
         try:
             self.producer.produce(
-                topic=self.publish_topic,
+                topic=self.topic,
                 key=key.encode('utf-8'),
                 value=json.dumps(
                     (self.EVENT_PROTOCOL_VERSION, _type) + extra_data
diff --git a/src/sentry/features/__init__.py b/src/sentry/features/__init__.py
index 3ff04fb4ae..73af1745d4 100644
--- a/src/sentry/features/__init__.py
+++ b/src/sentry/features/__init__.py
@@ -92,6 +92,7 @@ default_manager.add('projects:sample-events', ProjectFeature)  # NOQA
 default_manager.add('projects:servicehooks', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-view', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-indexing', ProjectFeature)  # NOQA
+default_manager.add('projects:kafka-ingest', ProjectFeature)  # NOQA
 
 # Project plugin features
 default_manager.add('projects:plugins', ProjectPluginFeature)  # NOQA
diff --git a/src/sentry/runner/__init__.py b/src/sentry/runner/__init__.py
index 802d0247d1..5d84ec7c19 100644
--- a/src/sentry/runner/__init__.py
+++ b/src/sentry/runner/__init__.py
@@ -56,7 +56,7 @@ list(
         lambda cmd: cli.add_command(import_string(cmd)), (
             'sentry.runner.commands.backup.export', 'sentry.runner.commands.backup.import_',
             'sentry.runner.commands.cleanup.cleanup', 'sentry.runner.commands.config.config',
-            'sentry.runner.commands.createuser.createuser',
+            'sentry.runner.commands.consumer.consumer', 'sentry.runner.commands.createuser.createuser',
             'sentry.runner.commands.devserver.devserver', 'sentry.runner.commands.django.django',
             'sentry.runner.commands.exec.exec_', 'sentry.runner.commands.files.files',
             'sentry.runner.commands.help.help', 'sentry.runner.commands.init.init',
diff --git a/src/sentry/runner/commands/consumer.py b/src/sentry/runner/commands/consumer.py
new file mode 100644
index 0000000000..29e450044f
--- /dev/null
+++ b/src/sentry/runner/commands/consumer.py
@@ -0,0 +1,49 @@
+from __future__ import absolute_import, print_function
+
+import click
+import signal
+from django.conf import settings
+
+from sentry.runner.decorators import configuration
+
+
+@click.command()
+@click.option('--topic', multiple=True, required=True,
+              help='Topic(s) to consume from.')
+@click.option('--consumer-group', default='sentry-consumers',
+              help='Consumer group name.')
+@click.option('--bootstrap-server', default=['localhost:9092'], multiple=True,
+              help='Kafka bootstrap server(s) to use.')
+@click.option('--max-batch-size', default=10000,
+              help='Max number of messages to batch in memory before committing offsets to Kafka.')
+@click.option('--max-batch-time-ms', default=60000,
+              help='Max length of time to buffer messages in memory before committing offsets to Kafka.')
+@click.option('--auto-offset-reset', default='error', type=click.Choice(['error', 'earliest', 'latest']),
+              help='Kafka consumer auto offset reset.')
+@configuration
+def consumer(**options):
+    from batching_kafka_consumer import BatchingKafkaConsumer
+    from sentry.consumer import ConsumerWorker
+
+    known_topics = {x['topic'] for x in settings.KAFKA_TOPICS.values()}
+    topics = options['topic']
+    for topic in topics:
+        if topic not in known_topics:
+            raise RuntimeError("topic '%s' is not one of: %s" % (topic, known_topics))
+
+    consumer = BatchingKafkaConsumer(
+        topics=options['topic'],
+        worker=ConsumerWorker(),
+        max_batch_size=options['max_batch_size'],
+        max_batch_time=options['max_batch_time_ms'],
+        bootstrap_servers=options['bootstrap_server'],
+        group_id=options['consumer_group'],
+        auto_offset_reset=options['auto_offset_reset'],
+    )
+
+    def handler(signum, frame):
+        consumer.signal_shutdown()
+
+    signal.signal(signal.SIGINT, handler)
+
+    consumer.run()
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 4de2825261..d1992a7223 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -13,13 +13,14 @@ from datetime import datetime
 import six
 
 from time import time
+from django.conf import settings
 from django.utils import timezone
 
 from sentry import features, reprocessing
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
-from sentry.utils import metrics
+from sentry.utils import json, kafka, metrics
 from sentry.utils.safe import safe_execute
 from sentry.stacktraces import process_stacktraces, \
     should_process_for_stacktraces
@@ -59,8 +60,44 @@ def should_process(data):
     return False
 
 
-def _do_preprocess_event(cache_key, data, start_time, event_id, process_event):
-    if cache_key:
+def submit_process(project, from_reprocessing, cache_key, event_id, start_time, data):
+    if features.has('projects:kafka-ingest', project=project):
+        kafka.produce_sync(
+            settings.KAFKA_PROCESS,
+            value=json.dumps({
+                'cache_key': cache_key,
+                'start_time': start_time,
+                'from_reprocessing': from_reprocessing,
+                'data': data,
+            }),
+        )
+    else:
+        task = process_event_from_reprocessing if from_reprocessing else process_event
+        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
+
+
+def submit_save_event(project, cache_key, event_id, start_time, data):
+    if features.has('projects:kafka-ingest', project=project):
+        kafka.produce_sync(
+            settings.KAFKA_SAVE,
+            value=json.dumps({
+                'cache_key': cache_key,
+                'start_time': start_time,
+                'data': data,
+            }),
+        )
+    else:
+        if cache_key:
+            data = None
+
+        save_event.delay(
+            cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
+            project_id=project.id
+        )
+
+
+def _do_preprocess_event(cache_key, data, start_time, event_id, process_task):
+    if cache_key and data is None:
         data = default_cache.get(cache_key)
 
     if data is None:
@@ -68,24 +105,21 @@ def _do_preprocess_event(cache_key, data, start_time, event_id, process_event):
         error_logger.error('preprocess.failed.empty', extra={'cache_key': cache_key})
         return
 
+    original_data = data
     data = CanonicalKeyDict(data)
-    project = data['project']
+    project_id = data['project']
 
     with configure_scope() as scope:
-        scope.set_tag("project", project)
+        scope.set_tag("project", project_id)
+
+    project = Project.objects.get_from_cache(id=project_id)
 
     if should_process(data):
-        process_event.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
+        from_reprocessing = process_task is process_event_from_reprocessing
+        submit_process(project, from_reprocessing, cache_key, event_id, start_time, original_data)
         return
 
-    # If we get here, that means the event had no preprocessing needed to be done
-    # so we can jump directly to save_event
-    if cache_key:
-        data = None
-    save_event.delay(
-        cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
-        project_id=project
-    )
+    submit_save_event(project, cache_key, event_id, start_time, original_data)
 
 
 @instrumented_task(
@@ -112,10 +146,11 @@ def preprocess_event_from_reprocessing(
     )
 
 
-def _do_process_event(cache_key, start_time, event_id, process_task):
+def _do_process_event(cache_key, start_time, event_id, process_task, data=None):
     from sentry.plugins import plugins
 
-    data = default_cache.get(cache_key)
+    if data is None:
+        data = default_cache.get(cache_key)
 
     if data is None:
         metrics.incr(
@@ -128,15 +163,15 @@ def _do_process_event(cache_key, start_time, event_id, process_task):
         return
 
     data = CanonicalKeyDict(data)
-    project = data['project']
+    project_id = data['project']
 
     with configure_scope() as scope:
-        scope.set_tag("project", project)
+        scope.set_tag("project", project_id)
 
     has_changed = False
 
     # Fetch the reprocessing revision
-    reprocessing_rev = reprocessing.get_reprocessing_revision(project)
+    reprocessing_rev = reprocessing.get_reprocessing_revision(project_id)
 
     # Event enhancers.  These run before anything else.
     for plugin in plugins.all(version=2):
@@ -165,13 +200,19 @@ def _do_process_event(cache_key, start_time, event_id, process_task):
                 data = result
                 has_changed = True
 
-    assert data['project'] == project, 'Project cannot be mutated by preprocessor'
+    assert data['project'] == project_id, 'Project cannot be mutated by preprocessor'
+    project = Project.objects.get_from_cache(id=project_id)
+
+    # We cannot persist canonical types in the cache, so we need to
+    # downgrade this.
+    if isinstance(data, CANONICAL_TYPES):
+        data = dict(data.items())
 
     if has_changed:
         issues = data.get('processing_issues')
         try:
             if issues and create_failed_event(
-                cache_key, project, list(issues.values()),
+                cache_key, project_id, list(issues.values()),
                 event_id=event_id, start_time=start_time,
                 reprocessing_rev=reprocessing_rev
             ):
@@ -180,20 +221,15 @@ def _do_process_event(cache_key, start_time, event_id, process_task):
             # If `create_failed_event` indicates that we need to retry we
             # invoke outselves again.  This happens when the reprocessing
             # revision changed while we were processing.
+            from_reprocessing = process_task is process_event_from_reprocessing
+            submit_process(project, from_reprocessing, cache_key, event_id, start_time, data)
             process_task.delay(cache_key, start_time=start_time,
                                event_id=event_id)
             return
 
-        # We cannot persist canonical types in the cache, so we need to
-        # downgrade this.
-        if isinstance(data, CANONICAL_TYPES):
-            data = dict(data.items())
         default_cache.set(cache_key, data, 3600)
 
-    save_event.delay(
-        cache_key=cache_key, data=None, start_time=start_time, event_id=event_id,
-        project_id=project
-    )
+    submit_save_event(project, cache_key, event_id, start_time, data)
 
 
 @instrumented_task(
@@ -343,9 +379,8 @@ def save_attachment(event, attachment):
     )
 
 
-@instrumented_task(name='sentry.tasks.store.save_event', queue='events.save_event')
-def save_event(cache_key=None, data=None, start_time=None, event_id=None,
-               project_id=None, **kwargs):
+def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
+                   project_id=None, **kwargs):
     """
     Saves an event to the database.
     """
@@ -353,7 +388,7 @@ def save_event(cache_key=None, data=None, start_time=None, event_id=None,
     from sentry import quotas, tsdb
     from sentry.models import ProjectKey
 
-    if cache_key:
+    if cache_key and data is None:
         data = default_cache.get(cache_key)
 
     if data is not None:
@@ -453,3 +488,9 @@ def save_event(cache_key=None, data=None, start_time=None, event_id=None,
                 'events.time-to-process',
                 time() - start_time,
                 instance=data['platform'])
+
+
+@instrumented_task(name='sentry.tasks.store.save_event', queue='events.save_event')
+def save_event(cache_key=None, data=None, start_time=None, event_id=None,
+               project_id=None, **kwargs):
+    _do_save_event(cache_key, data, start_time, event_id, project_id, **kwargs)
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
new file mode 100644
index 0000000000..2b78369986
--- /dev/null
+++ b/src/sentry/utils/kafka.py
@@ -0,0 +1,56 @@
+from __future__ import absolute_import
+
+import logging
+
+from django.conf import settings
+
+
+logger = logging.getLogger(__name__)
+
+
+class ProducerManager(object):
+    """\
+    Manages one `confluent_kafka.Producer` per Kafka cluster.
+
+    See `KAFKA_CLUSTERS` and `KAFKA_TOPICS` in settings.
+    """
+
+    def __init__(self):
+        self.__producers = {}
+
+    def get(self, key):
+        cluster_name = settings.KAFKA_TOPICS[key]['cluster']
+        producer = self.__producers.get(cluster_name)
+
+        if producer:
+            return producer
+
+        from confluent_kafka import Producer
+
+        cluster_options = settings.KAFKA_CLUSTERS[cluster_name]
+        producer = self.__producers[cluster_name] = Producer(cluster_options)
+        return producer
+
+
+producers = ProducerManager()
+
+
+def delivery_callback(error, message):
+    if error is not None:
+        logger.error('Could not publish message (error: %s): %r', error, message)
+
+
+def produce_sync(topic_key, **kwargs):
+    producer = producers.get(topic_key)
+
+    try:
+        producer.produce(
+            topic=settings.KAFKA_TOPICS[topic_key]['topic'],
+            on_delivery=delivery_callback,
+            **kwargs
+        )
+    except Exception as error:
+        logger.error('Could not publish message: %s', error, exc_info=True)
+        return
+
+    producer.flush()
diff --git a/tests/sentry/consumer/__init__.py b/tests/sentry/consumer/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/sentry/consumer/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/sentry/consumer/test_consumer.py b/tests/sentry/consumer/test_consumer.py
new file mode 100644
index 0000000000..da77b9b4a0
--- /dev/null
+++ b/tests/sentry/consumer/test_consumer.py
@@ -0,0 +1,102 @@
+from __future__ import absolute_import, print_function
+
+from mock import patch
+
+from sentry.consumer import ConsumerWorker
+from sentry.coreapi import ClientApiHelper
+from sentry.models import Event
+from sentry.plugins import Plugin2
+from sentry.testutils import PluginTestCase
+from sentry.utils import json
+
+
+class BasicPreprocessorPlugin(Plugin2):
+    def add_foo(self, data):
+        data['foo'] = 'bar'
+        return data
+
+    def get_event_preprocessors(self, data):
+        if data.get('platform') == 'needs_process':
+            return [self.add_foo]
+
+        return []
+
+    def is_enabled(self, project=None):
+        return True
+
+
+class TestConsumer(PluginTestCase):
+    plugin = BasicPreprocessorPlugin
+
+    def _call_consumer(self, mock_produce):
+        args, kwargs = list(mock_produce.call_args)
+        mock_produce.call_args = None
+        topic = args[0]
+        value = json.loads(kwargs['value'])
+
+        consumer = ConsumerWorker()
+        consumer._handle(topic, value)
+
+    def _create_event_with_platform(self, project, platform):
+        from sentry.event_manager import EventManager
+        em = EventManager({}, project=project)
+        em.normalize()
+        data = em.get_data()
+        data['platform'] = platform
+        return data
+
+    @patch('sentry.tasks.store.save_event')
+    @patch('sentry.tasks.store.preprocess_event')
+    @patch('sentry.utils.kafka.produce_sync')
+    def test_process_path(self, mock_produce, mock_preprocess_event, mock_save_event):
+        with self.feature('projects:kafka-ingest'):
+            project = self.create_project()
+            data = self._create_event_with_platform(project, 'needs_process')
+
+            helper = ClientApiHelper(project_id=self.project.id)
+            helper.context.bind_project(project)
+            helper.insert_data_to_database(data)
+
+            # preprocess
+            self._call_consumer(mock_produce)
+            # process
+            with self.tasks():
+                self._call_consumer(mock_produce)
+            # save
+            self._call_consumer(mock_produce)
+
+            assert mock_preprocess_event.delay.call_count == 0
+            assert mock_save_event.delay.call_count == 0
+
+            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
+            saved_data = event.get_raw_data()
+            assert saved_data['foo'] == 'bar'
+            assert saved_data['platform'] == 'needs_process'
+
+    @patch('sentry.tasks.store.save_event')
+    @patch('sentry.tasks.store.process_event')
+    @patch('sentry.tasks.store.preprocess_event')
+    @patch('sentry.utils.kafka.produce_sync')
+    def test_save_path(self, mock_produce, mock_preprocess_event,
+                       mock_process_event, mock_save_event):
+        with self.feature('projects:kafka-ingest'):
+            project = self.create_project()
+            data = self._create_event_with_platform(project, 'doesnt_need_process')
+
+            helper = ClientApiHelper(project_id=self.project.id)
+            helper.context.bind_project(project)
+            helper.insert_data_to_database(data)
+
+            # preprocess
+            self._call_consumer(mock_produce)
+            # save
+            self._call_consumer(mock_produce)
+
+            assert mock_preprocess_event.delay.call_count == 0
+            assert mock_process_event.delay.call_count == 0
+            assert mock_save_event.delay.call_count == 0
+
+            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
+            saved_data = event.get_raw_data()
+            assert 'foo' not in saved_data
+            assert saved_data['platform'] == 'doesnt_need_process'
