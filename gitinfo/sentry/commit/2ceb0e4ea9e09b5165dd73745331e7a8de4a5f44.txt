commit 2ceb0e4ea9e09b5165dd73745331e7a8de4a5f44
Author: Alex Hofsteede <alex@hofsteede.com>
Date:   Fri May 3 16:08:34 2019 -0700

    feat: Add event_id to outcome payload if it exists (#13044)

diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index e5df19ca3d..e1c1fafd33 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -500,7 +500,9 @@ def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
             key_id,
             Outcome.ACCEPTED,
             None,
-            timestamp)
+            timestamp,
+            event_id
+        )
 
     except HashDiscarded:
         project = Project.objects.get_from_cache(id=project_id)
@@ -519,7 +521,8 @@ def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
             key_id,
             Outcome.FILTERED,
             reason,
-            timestamp
+            timestamp,
+            event_id
         )
 
     finally:
diff --git a/src/sentry/utils/outcomes.py b/src/sentry/utils/outcomes.py
index 978b2ec221..032d07db9c 100644
--- a/src/sentry/utils/outcomes.py
+++ b/src/sentry/utils/outcomes.py
@@ -31,7 +31,7 @@ outcomes = settings.KAFKA_TOPICS[settings.KAFKA_OUTCOMES]
 outcomes_publisher = None
 
 
-def track_outcome(org_id, project_id, key_id, outcome, reason=None, timestamp=None):
+def track_outcome(org_id, project_id, key_id, outcome, reason=None, timestamp=None, event_id=None):
     """
     This is a central point to track org/project counters per incoming event.
     NB: This should only ever be called once per incoming event, which means
@@ -92,6 +92,7 @@ def track_outcome(org_id, project_id, key_id, outcome, reason=None, timestamp=No
                 'key_id': key_id,
                 'outcome': outcome.value,
                 'reason': reason,
+                'event_id': event_id,
             })
         )
 
diff --git a/src/sentry/web/api.py b/src/sentry/web/api.py
index cb2fa3ea79..2631006c2a 100644
--- a/src/sentry/web/api.py
+++ b/src/sentry/web/api.py
@@ -97,6 +97,7 @@ def api(func):
 def process_event(event_manager, project, key, remote_addr, helper, attachments):
     event_received.send_robust(ip=remote_addr, project=project, sender=process_event)
 
+    event_id = event_manager.get_data().get('event_id')
     start_time = time()
     should_filter, filter_reason = event_manager.should_filter()
     if should_filter:
@@ -106,6 +107,7 @@ def process_event(event_manager, project, key, remote_addr, helper, attachments)
             key.id,
             Outcome.FILTERED,
             filter_reason,
+            event_id=event_id
         )
         metrics.incr(
             'events.blacklisted', tags={'reason': filter_reason}, skip_internal=False
@@ -137,6 +139,7 @@ def process_event(event_manager, project, key, remote_addr, helper, attachments)
             key.id,
             Outcome.RATE_LIMITED,
             reason,
+            event_id=event_id
         )
         metrics.incr(
             'events.dropped',
@@ -173,6 +176,7 @@ def process_event(event_manager, project, key, remote_addr, helper, attachments)
             key.id,
             Outcome.INVALID,
             'duplicate',
+            event_id=event_id
         )
         raise APIForbidden(
             'An event with the same ID already exists (%s)' % (event_id, ))
@@ -544,6 +548,7 @@ class StoreView(APIView):
                 key.id,
                 Outcome.INVALID,
                 'too_large',
+                event_id=dict_data.get('event_id')
             )
             raise APIForbidden("Event size exceeded 10MB after normalization.")
 
