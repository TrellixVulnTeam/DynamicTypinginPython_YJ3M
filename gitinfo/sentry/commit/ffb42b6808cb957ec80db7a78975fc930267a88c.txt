commit ffb42b6808cb957ec80db7a78975fc930267a88c
Author: ted kaemming <ted@kaemming.com>
Date:   Fri Aug 11 10:31:54 2017 -0700

    Move digest implementation entirely to a Lua implementation. (#5846)
    
    This consolidates all of the digest storage logic to Lua scripts, drastically simplifying the implementation (there is less optimistic transaction recovery code and less locking required) while maintaining fundamentally the same behavior. All keys remain the same, so moving from the previous implementation to this one should not cause any data loss.
    
    Here are the notable changes:
    
    - Scheduling and schedule maintenance no longer operate in chunks, and just move all items as part of a single operation.
    - Schedule maintenance no longer explicitly deletes timelines that have exceeded the TTL. The existing implementation was exceedingly complex, and the only drawback with the changed implementation is that timelines that are moved back to the "waiting" set during schedule maintenance that are currently being digested may be sent with a shorter interval than they otherwise would have. (This is solvable by forcing an upgrade to Redis 3.0.2+, or rewriting this to operate in a `ZSCORE`/`ZADD` loop if needed.) No data is lost.
    - Tests are less exhaustive about data being cleaned up and depend more on behavior (but I think good enough to ensure the backend is working correctly.)

diff --git a/src/sentry/digests/backends/redis.py b/src/sentry/digests/backends/redis.py
index fd781cb47b..70902ae49d 100644
--- a/src/sentry/digests/backends/redis.py
+++ b/src/sentry/digests/backends/redis.py
@@ -1,13 +1,11 @@
 from __future__ import absolute_import
 
-import itertools
 import logging
-import random
 import six
 import time
 
 from contextlib import contextmanager
-from redis.exceptions import ResponseError, WatchError
+from redis.client import ResponseError
 
 from sentry.digests import Record, ScheduleEntry
 from sentry.digests.backends.base import Backend, InvalidState
@@ -18,57 +16,7 @@ from sentry.utils.versioning import Version
 
 logger = logging.getLogger('sentry.digests')
 
-SCHEDULE_PATH_COMPONENT = 's'
-SCHEDULE_STATE_WAITING = 'w'
-SCHEDULE_STATE_READY = 'r'
-
-TIMELINE_DIGEST_PATH_COMPONENT = 'd'
-TIMELINE_LAST_PROCESSED_TIMESTAMP_PATH_COMPONENT = 'l'
-TIMELINE_PATH_COMPONENT = 't'
-TIMELINE_RECORD_PATH_COMPONENT = 'r'
-
-
-def ilen(iterator):
-    i = 0
-    for i, _ in enumerate(iterator):
-        pass
-    return i
-
-
-def make_schedule_key(namespace, state):
-    return '{0}:{1}:{2}'.format(namespace, SCHEDULE_PATH_COMPONENT, state)
-
-
-def make_timeline_key(namespace, key):
-    return '{0}:{1}:{2}'.format(namespace, TIMELINE_PATH_COMPONENT, key)
-
-
-def make_last_processed_timestamp_key(timeline_key):
-    return '{0}:{1}'.format(timeline_key, TIMELINE_LAST_PROCESSED_TIMESTAMP_PATH_COMPONENT)
-
-
-def make_digest_key(timeline_key):
-    return '{0}:{1}'.format(timeline_key, TIMELINE_DIGEST_PATH_COMPONENT)
-
-
-def make_record_key(timeline_key, record):
-    return '{0}:{1}:{2}'.format(timeline_key, TIMELINE_RECORD_PATH_COMPONENT, record)
-
-
-ensure_timeline_scheduled = load_script('digests/ensure_timeline_scheduled.lua')
-truncate_timeline = load_script('digests/truncate_timeline.lua')
-
-
-def clear_timeline_contents(pipeline, timeline_key):
-    """
-    Removes all keys associated with a timeline key. This does not remove the
-    timeline from schedules.
-
-    This assumes the timeline lock has already been acquired.
-    """
-    truncate_timeline(pipeline, (timeline_key, ), (0, timeline_key))
-    truncate_timeline(pipeline, (make_digest_key(timeline_key), ), (0, timeline_key))
-    pipeline.delete(make_last_processed_timestamp_key(timeline_key))
+script = load_script('digests/digests.lua')
 
 
 class RedisBackend(Backend):
@@ -146,126 +94,73 @@ class RedisBackend(Backend):
             label='Digests',
         )
 
-    def add(self, key, record, increment_delay=None, maximum_delay=None):
+    def _get_connection(self, key):
+        return self.cluster.get_local_client_for_key(
+            '{}:t:{}'.format(self.namespace, key),
+        )
+
+    def _get_timeline_lock(self, key, duration):
+        lock_key = '{}:t:{}'.format(self.namespace, key)
+        return self.locks.get(
+            lock_key,
+            duration=duration,
+            routing_key=lock_key,
+        )
+
+    def add(self, key, record, increment_delay=None, maximum_delay=None, timestamp=None):
+        if timestamp is None:
+            timestamp = time.time()
+
         if increment_delay is None:
             increment_delay = self.increment_delay
 
         if maximum_delay is None:
             maximum_delay = self.maximum_delay
 
-        timeline_key = make_timeline_key(self.namespace, key)
-        record_key = make_record_key(timeline_key, record.key)
-
-        connection = self.cluster.get_local_client_for_key(timeline_key)
-        with connection.pipeline() as pipeline:
-            pipeline.multi()
-
-            pipeline.set(
-                record_key,
-                self.codec.encode(record.value),
-                ex=self.ttl,
-            )
-
-            # In the future, it might make sense to prefix the entry with the
-            # timestamp (lexicographically sortable) to ensure that we can
-            # maintain the correct sort order with abitrary precision:
-            # http://redis.io/commands/ZADD#elements-with-the-same-score
-            pipeline.zadd(timeline_key, record.timestamp, record.key)
-            pipeline.expire(timeline_key, self.ttl)
-
-            ensure_timeline_scheduled(
-                pipeline,
-                (
-                    make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                    make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                    make_last_processed_timestamp_key(timeline_key),
-                ),
-                (key, record.timestamp, increment_delay, maximum_delay, ),
+        # Redis returns "true" and "false" as "1" and "None", so we just cast
+        # them back to the appropriate boolean here.
+        return bool(
+            script(
+                self._get_connection(key),
+                [key],
+                [
+                    'ADD',
+                    self.namespace,
+                    self.ttl,
+                    timestamp,
+                    key,
+                    record.key,
+                    self.codec.encode(record.value),
+                    record.timestamp,  # TODO: check type
+                    increment_delay,
+                    maximum_delay,
+                    self.capacity if self.capacity else -1,
+                    self.truncation_chance,
+                ],
             )
-
-            should_truncate = random.random() < self.truncation_chance
-            if should_truncate:
-                truncate_timeline(
-                    pipeline,
-                    (timeline_key, ),
-                    (self.capacity, timeline_key),
-                )
-
-            results = pipeline.execute()
-            if should_truncate:
-                logger.debug('Removed %s extra records from %s.', results[-1], key)
-
-            return results[-2 if should_truncate else -1]
-
-    def __schedule_partition(self, host, deadline, chunk):
-        connection = self.cluster.get_local_client(host)
-
-        lock = self.locks.get(
-            '{0}:s:{1}'.format(self.namespace, host),
-            duration=30,
-            routing_key=host,
         )
 
-        with lock.acquire():
-            # Prevent a runaway loop by setting a maximum number of
-            # iterations. Note that this limits the total number of
-            # expected items in any specific scheduling interval to chunk *
-            # maximum_iterations.
-            maximum_iterations = 1000
-            for i in range(maximum_iterations):
-                items = connection.zrangebyscore(
-                    make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                    min=0,
-                    max=deadline,
-                    withscores=True,
-                    start=0,
-                    num=chunk,
-                )
-
-                # XXX: Redis will error if we try and execute an empty
-                # transaction. If there are no items to move between states, we
-                # need to exit the loop now. (This can happen on the first
-                # iteration of the loop if there is nothing to do, or on a
-                # subsequent iteration if there was exactly the same number of
-                # items to change states as the chunk size.)
-                if not items:
-                    break
-
-                with connection.pipeline() as pipeline:
-                    pipeline.multi()
-
-                    pipeline.zrem(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                        * [key for key, timestamp in items]
-                    )
-
-                    pipeline.zadd(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                        *itertools.chain.from_iterable(
-                            [(timestamp, key) for (key, timestamp) in items]
-                        )
-                    )
-
-                    for key, timestamp in items:
-                        yield ScheduleEntry(key, timestamp)
-
-                    pipeline.execute()
+    def __schedule_partition(self, host, deadline, timestamp):
+        return script(
+            self.cluster.get_local_client(host),
+            ['-'],
+            [
+                'SCHEDULE',
+                self.namespace,
+                self.ttl,
+                timestamp,
+                deadline,
+            ],
+        )
 
-                # If we retrieved less than the chunk size of items, we don't
-                # need try to retrieve more items.
-                if len(items) < chunk:
-                    break
-            else:
-                raise RuntimeError('loop exceeded maximum iterations (%s)' % (maximum_iterations, ))
+    def schedule(self, deadline, timestamp=None):
+        if timestamp is None:
+            timestamp = time.time()
 
-    def schedule(self, deadline, chunk=1000):
-        # TODO: This doesn't lead to a fair balancing of workers, ideally each
-        # scheduling task would be executed by a different process for each
-        # host.
         for host in self.cluster.hosts:
             try:
-                for entry in self.__schedule_partition(host, deadline, chunk):
-                    yield entry
+                for key, timestamp in self.__schedule_partition(host, deadline, timestamp):
+                    yield ScheduleEntry(key, float(timestamp))
             except Exception as error:
                 logger.error(
                     'Failed to perform scheduling for partition %r due to error: %r',
@@ -274,171 +169,26 @@ class RedisBackend(Backend):
                     exc_info=True
                 )
 
-    def __maintenance_partition(self, host, deadline, chunk):
-        connection = self.cluster.get_local_client(host)
-
-        extra = 0
-        start = 0
-        maximum_iterations = 1000
-        for i in range(maximum_iterations):
-            fetch_size = chunk + extra
-            entries = [
-                ScheduleEntry(*x)
-                for x in (
-                    connection.zrangebyscore(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                        min=start,
-                        max=deadline,
-                        withscores=True,
-                        start=0,
-                        num=fetch_size,
-                    )
-                )
-            ]
-
-            def try_lock(entry):
-                """
-                Attempt to immedately acquire a lock on the timeline at
-                key, returning the lock if it can be acquired, otherwise
-                returning ``None``.
-                """
-                timeline_key = make_timeline_key(self.namespace, entry.key),
-                lock = self.locks.get(
-                    timeline_key,
-                    duration=5,
-                    routing_key=timeline_key,
-                )
-                try:
-                    lock.acquire()
-                except Exception:
-                    lock = None
-                return lock, entry
-
-            # Try to take out a lock on each entry. If we can't acquire the
-            # lock, that means this is currently being digested and cannot be
-            # rescheduled.
-            can_reschedule = ([], [])  # indexed by True and False
-            for result in map(try_lock, entries):
-                can_reschedule[result[0] is not None].append(result)
-
-            logger.debug(
-                'Fetched %s items, able to reschedule %s.', len(entries), len(can_reschedule[True])
-            )
-
-            # Set the start position for the next query. (If there are no
-            # items, we don't need to worry about this, since there won't
-            # be a next query.) If all items share the same score and are
-            # locked, the iterator will never advance (we will keep trying
-            # to schedule the same locked items over and over) and either
-            # eventually progress slowly as items are unlocked, or hit the
-            # maximum iterations boundary. A possible solution to this
-            # would be to count the number of items that have the maximum
-            # score in this page that we assume we can't acquire (since we
-            # couldn't acquire the lock this iteration) and add that count
-            # to the next query limit. (This unfortunately could also
-            # lead to unbounded growth too, so we have to limit it as well.)
-            if entries:
-                start = entries[-1].key
-                extra = min(
-                    ilen(
-                        itertools.takewhile(
-                            # (lock, entry)
-                            lambda x: x[1].timestamp == start,
-                            can_reschedule[False][::-1],
-                        ),
-                    ),
-                    chunk,
-                )
-
-            # XXX: We need to perform this check before the transaction to
-            # ensure that we don't execute an empty transaction. (We'll
-            # need to perform a similar check after the completion of the
-            # transaction as well.)
-            if not can_reschedule[True]:
-                if len(entries) == fetch_size:
-                    # There is nothing to reschedule in this chunk, but we
-                    # need check if there are others after this chunk.
-                    continue
-                else:
-                    # There is nothing to unlock, and we've exhausted all items.
-                    break
-
-            try:
-                with connection.pipeline() as pipeline:
-                    pipeline.multi()
-
-                    pipeline.zrem(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                        * [entry.key for (lock, entry) in can_reschedule[True]]
-                    )
-
-                    should_reschedule = ([], [])  # indexed by True and False
-                    timeout = time.time() - self.ttl
-                    for lock, entry in can_reschedule[True]:
-                        should_reschedule[entry.timestamp > timeout].append(entry)
+    def __maintenance_partition(self, host, deadline, timestamp):
+        return script(
+            self.cluster.get_local_client(host),
+            ['-'],
+            [
+                'MAINTENANCE',
+                self.namespace,
+                self.ttl,
+                timestamp,
+                deadline,
+            ],
+        )
 
-                    logger.debug(
-                        'Identified %s items that should be rescheduled, and %s that will be removed.',
-                        len(should_reschedule[True]),
-                        len(should_reschedule[False]),
-                    )
+    def maintenance(self, deadline, timestamp=None):
+        if timestamp is None:
+            timestamp = time.time()
 
-                    # Move items that should be rescheduled to the waiting state.
-                    if should_reschedule[True]:
-                        pipeline.zadd(
-                            make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                            *itertools.chain.from_iterable(
-                                [(entry.timestamp, entry.key) for entry in should_reschedule[True]]
-                            )
-                        )
-
-                    # Clear out timelines that should not be rescheduled.
-                    # Ideally this path is never actually hit, but this can
-                    # happen if the queue is **extremely** backlogged, or if a
-                    # cluster size change caused partition ownership to change
-                    # and timelines are now stuck within partitions that they
-                    # no longer should be. (For more details, see GH-2479.)
-                    for entry in should_reschedule[False]:
-                        logger.warning(
-                            'Clearing expired timeline %r from host %s, schedule timestamp was %s.',
-                            entry.key,
-                            host,
-                            entry.timestamp,
-                        )
-                        clear_timeline_contents(
-                            pipeline,
-                            make_timeline_key(self.namespace, entry.key),
-                        )
-
-                    pipeline.execute()
-            finally:
-                # Regardless of the outcome of the transaction, we should
-                # try to unlock the items for processing.
-                for lock, entry in can_reschedule[True]:
-                    try:
-                        lock.release()
-                    except Exception as error:
-                        # XXX: This shouldn't be hit (the ``Lock`` code
-                        # should swallow the exception) but this is here
-                        # for safety anyway.
-                        logger.warning('Could not unlock %r: %s', entry, error)
-
-            # If we retrieved less than the chunk size of items, we don't
-            # need try to retrieve more items.
-            if len(entries) < fetch_size:
-                break
-        else:
-            raise RuntimeError('loop exceeded maximum iterations (%s)' % (maximum_iterations, ))
-
-    def maintenance(self, deadline, chunk=1000):
-        # TODO: Ideally, this would also return the number of items that were
-        # rescheduled (and possibly even how late they were at the point of
-        # rescheduling) but that causes a bit of an API issue since in the case
-        # of an error, this can be considered a partial success (but still
-        # should raise an exception.)
         for host in self.cluster.hosts:
             try:
-                self.__maintenance_partition(host, deadline, chunk)
+                self.__maintenance_partition(host, deadline, timestamp)
             except Exception as error:
                 logger.error(
                     'Failed to perform maintenance on digest partition %r due to error: %r',
@@ -448,132 +198,56 @@ class RedisBackend(Backend):
                 )
 
     @contextmanager
-    def digest(self, key, minimum_delay=None):
+    def digest(self, key, minimum_delay=None, timestamp=None):
         if minimum_delay is None:
             minimum_delay = self.minimum_delay
 
-        timeline_key = make_timeline_key(self.namespace, key)
-        digest_key = make_digest_key(timeline_key)
-
-        connection = self.cluster.get_local_client_for_key(timeline_key)
-
-        lock = self.locks.get(timeline_key, duration=30, routing_key=timeline_key)
-        with lock.acquire():
-            # Check to ensure the timeline is in the correct state ("ready")
-            # before sending. This acts as a throttling mechanism to prevent
-            # sending a digest before it's next scheduled delivery time in a
-            # race condition scenario.
-            if connection.zscore(make_schedule_key(self.namespace, SCHEDULE_STATE_READY), key
-                                 ) is None:
-                raise InvalidState('Timeline is not in the ready state.')
-
-            with connection.pipeline() as pipeline:
-                # This shouldn't be necessary, but better safe than sorry?
-                pipeline.watch(digest_key)
-
-                if pipeline.exists(digest_key):
-                    pipeline.multi()
-                    pipeline.zunionstore(digest_key, (timeline_key, digest_key), aggregate='max')
-                    pipeline.delete(timeline_key)
-                    pipeline.expire(digest_key, self.ttl)
-                    pipeline.execute()
-                else:
-                    pipeline.multi()
-                    pipeline.rename(timeline_key, digest_key)
-                    pipeline.expire(digest_key, self.ttl)
-                    try:
-                        pipeline.execute()
-                    except ResponseError as error:
-                        if 'no such key' in six.text_type(error):
-                            logger.debug(
-                                'Could not move timeline for digestion (likely has no contents.)'
-                            )
-                        else:
-                            raise
-
-            # XXX: This must select all records, even though not all of them will
-            # be returned if they exceed the capacity, to ensure that all records
-            # will be garbage collected.
-            records = connection.zrevrange(digest_key, 0, -1, withscores=True)
-            if not records:
-                logger.debug('Retrieved timeline containing no records.')
-
-            def get_records_for_digest():
-                with connection.pipeline(transaction=False) as pipeline:
-                    for record_key, timestamp in records:
-                        pipeline.get(make_record_key(timeline_key, record_key))
-
-                    for (record_key, timestamp), value in zip(records, pipeline.execute()):
-                        # We have to handle failures if the key does not exist --
-                        # this could happen due to evictions or race conditions
-                        # where the record was added to a timeline while it was
-                        # already being digested.
-                        if value is None:
-                            logger.warning('Could not retrieve event for timeline.')
-                        else:
-                            yield Record(record_key, self.codec.decode(value), timestamp)
-
-            yield itertools.islice(get_records_for_digest(), self.capacity)
-
-            def cleanup_records(pipeline):
-                record_keys = [
-                    make_record_key(timeline_key, record_key) for record_key, score in records
-                ]
-                pipeline.delete(digest_key, *record_keys)
-
-            def reschedule():
-                with connection.pipeline() as pipeline:
-                    # This shouldn't be necessary, but better safe than sorry?
-                    pipeline.watch(digest_key)
-                    pipeline.multi()
-
-                    cleanup_records(pipeline)
-                    pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_READY), key)
-                    pipeline.zadd(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                        time.time() + minimum_delay, key
-                    )
-                    pipeline.setex(
-                        make_last_processed_timestamp_key(timeline_key), self.ttl, int(time.time())
+        if timestamp is None:
+            timestamp = time.time()
+
+        connection = self._get_connection(key)
+        with self._get_timeline_lock(key, duration=30).acquire():
+            try:
+                response = script(
+                    connection, [key], ['DIGEST_OPEN', self.namespace, self.ttl, timestamp, key]
+                )
+            except ResponseError as e:
+                if 'err(invalid_state):' in e.message:
+                    six.raise_from(
+                        InvalidState('Timeline is not in the ready state.'),
+                        e,
                     )
-                    pipeline.execute()
-
-            def unschedule():
-                with connection.pipeline() as pipeline:
-                    # Watch the timeline to ensure that no other transactions add
-                    # events to the timeline while we are trying to delete it.
-                    pipeline.watch(timeline_key)
-                    pipeline.multi()
-                    if connection.zcard(timeline_key) == 0:
-                        cleanup_records(pipeline)
-                        pipeline.delete(make_last_processed_timestamp_key(timeline_key))
-                        pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_READY), key)
-                        pipeline.zrem(
-                            make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING), key
-                        )
-                        pipeline.execute()
-
-            # If there were records in the digest, we need to schedule it so
-            # that we schedule any records that were added during digestion. If
-            # there were no items, we can try to remove the timeline from the
-            # digestion schedule.
-            if records:
-                reschedule()
-            else:
-                try:
-                    unschedule()
-                except WatchError:
-                    logger.debug('Could not remove timeline from schedule, rescheduling instead')
-                    reschedule()
-
-    def delete(self, key):
-        timeline_key = make_timeline_key(self.namespace, key)
-
-        connection = self.cluster.get_local_client_for_key(timeline_key)
-
-        lock = self.locks.get(timeline_key, duration=30, routing_key=timeline_key)
-        with lock.acquire(), connection.pipeline() as pipeline:
-            clear_timeline_contents(pipeline, timeline_key)
-            pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_READY), key)
-            pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING), key)
-            pipeline.execute()
+                else:
+                    raise
+
+            records = map(
+                lambda (key, value, timestamp): Record(
+                    key,
+                    self.codec.decode(value) if value is not None else None,
+                    float(timestamp),
+                ),
+                response,
+            )
+
+            # If the record value is `None`, this means the record data was
+            # missing (it was presumably evicted by Redis) so we don't need to
+            # return it here.
+            yield filter(
+                lambda record: record.value is not None,
+                records,
+            )
+
+            script(
+                connection,
+                [key],
+                ['DIGEST_CLOSE', self.namespace, self.ttl, timestamp, key, minimum_delay] +
+                [record.key for record in records],
+            )
+
+    def delete(self, key, timestamp=None):
+        if timestamp is None:
+            timestamp = time.time()
+
+        connection = self._get_connection(key)
+        with self._get_timeline_lock(key, duration=30).acquire():
+            script(connection, [key], ['DELETE', self.namespace, self.ttl, timestamp, key])
diff --git a/src/sentry/scripts/digests/digests.lua b/src/sentry/scripts/digests/digests.lua
new file mode 100644
index 0000000000..b40ce89c2f
--- /dev/null
+++ b/src/sentry/scripts/digests/digests.lua
@@ -0,0 +1,319 @@
+function table.extend(t, items)
+    for _, item in ipairs(items) do
+        table.insert(t, item)
+    end
+end
+
+function table.slice(t, ...)
+    local start, stop = ...
+    if stop == nil then
+        stop = #t
+    end
+    local result = {}
+    for i = start, stop do
+        table.insert(result, t[i])
+    end
+    return result
+end
+
+local noop = function ()
+    return
+end
+
+local function zrange_scored_iterator(result)
+    local i = -1
+    return function ()
+        i = i + 2
+        return result[i], result[i+1]
+    end
+end
+
+local function zrange_move_slice(source, destination, threshold, callback)
+    local callback = callback
+    if callback == nil then
+        callback = noop
+    end
+
+    local keys = redis.call('ZRANGEBYSCORE', source, 0, threshold, 'WITHSCORES')
+    if #keys == 0 then
+        return
+    end
+
+    local zadd_args = {}
+    local zrem_args = {}
+    for key, score in zrange_scored_iterator(keys) do
+        table.insert(zrem_args, key)
+        table.extend(zadd_args, {score, key})
+        callback(key, score)
+    end
+
+    -- TODO: This should support modifiers, and maintenance ZADD should include
+    -- the "NX" modifier to avoid resetting schedules during a race conditions
+    -- between a digest task and the maintenance task.
+    redis.call('ZADD', destination, unpack(zadd_args))
+    redis.call('ZREM', source, unpack(zrem_args))
+end
+
+local function zset_trim(key, capacity, callback)
+    local callback = callback
+    if callback == nil then
+        callback = noop
+    end
+
+    local n = 0
+
+    -- ZCARD is O(1) while ZREVRANGE is O(log(N)+M) so as long as the set is
+    -- generally smaller than the limit (which seems like a safe assumption)
+    -- then its cheaper just to check here and exit if there's nothing to do.
+    if redis.call('ZCARD', key) <= capacity then
+        return n
+    end
+
+    local items = redis.call('ZREVRANGE', key, capacity, -1)
+    for _, item in ipairs(items) do
+        redis.call('ZREM', key, item)
+        callback(item)
+        n = n + 1
+    end
+
+    return n
+end
+
+local function schedule(configuration, deadline)
+    local response = {}
+    zrange_move_slice(
+        configuration:get_schedule_waiting_key(),
+        configuration:get_schedule_ready_key(),
+        deadline,
+        function (timeline_id, timestamp)
+            table.insert(response, {timeline_id, timestamp})
+        end
+    )
+    return response
+end
+
+local function maintenance(configuration, deadline)
+    zrange_move_slice(
+        configuration:get_schedule_ready_key(),
+        configuration:get_schedule_waiting_key(),
+        deadline
+    )
+end
+
+local function add_timeline_to_schedule(configuration, timeline_id, timestamp, increment, maximum)
+    -- If the timeline is already in the "ready" set, this is a noop.
+    if redis.call('ZSCORE', configuration:get_schedule_ready_key(), timeline_id) ~= false then
+        return false
+    end
+
+    local score = redis.call('ZSCORE', configuration:get_schedule_waiting_key(), timeline_id)
+    if score == false then
+        -- If the timeline isn't already in either set, add it to the "ready" set with
+        -- the provided timestamp. This allows for immediate scheduling, bypassing the
+        -- imposed delay of the "waiting" state. (This should also be ZADD NX, but
+        -- like above, this allows us to still work with older Redis.)
+        redis.call('ZADD', configuration:get_schedule_ready_key(), timestamp, timeline_id)
+        return true
+    end
+
+    -- If the timeline is already in the "waiting" set, increase the delay by
+    -- min(current schedule + increment value, maximum delay after last
+    -- processing time).
+    local last_processed = tonumber(redis.call('GET', configuration:get_timeline_last_processed_timestamp_key(timeline_id)))
+    local update = nil
+    if last_processed == nil then
+        -- If the last processed timestamp is missing for some reason (possibly
+        -- evicted), be conservative and allow the timeline to be scheduled
+        -- with either the current schedule time or provided timestamp,
+        -- whichever is smaller.
+        update = math.min(score, timestamp)
+    else
+        update = math.min(
+            score + increment,
+            last_processed + maximum
+        )
+    end
+
+    if update ~= score then
+        -- This should technically be ZADD XX for correctness (this item
+        -- should always exist, and we established that above) but not
+        -- using that here doesn't break anything and allows use to use
+        -- older Redis versions.
+        redis.call('ZADD', configuration:get_schedule_waiting_key(), update, timeline_id)
+    end
+
+    return false
+end
+
+local function truncate_timeline(configuration, timeline_id, capacity)
+    return zset_trim(
+        configuration:get_timeline_key(timeline_id),
+        capacity,
+        function (record_id)
+            redis.call('DEL', configuration:get_timeline_record_key(timeline_id, record_id))
+        end
+    )
+end
+
+local function truncate_digest(configuration, timeline_id, capacity)
+    return zset_trim(
+        configuration:get_timeline_digest_key(timeline_id),
+        capacity,
+        function (record_id)
+            redis.call('DEL', configuration:get_timeline_record_key(timeline_id, record_id))
+        end
+    )
+end
+
+local function add_record_to_timeline(configuration, timeline_id, record_id, value, timestamp, delay_increment, delay_maximum, timeline_capacity, truncation_chance)
+    redis.call('SETEX', configuration:get_timeline_record_key(timeline_id, record_id), configuration.ttl, value)
+    redis.call('ZADD', configuration:get_timeline_key(timeline_id), timestamp, record_id)
+    redis.call('EXPIRE', configuration:get_timeline_key(timeline_id), configuration.ttl)
+
+    local ready = add_timeline_to_schedule(configuration, timeline_id, timestamp, delay_increment, delay_maximum)
+
+    -- TODO: Validating `timeline_capacity` and casting to number should happen upstream.
+    local timeline_capacity = tonumber(timeline_capacity)
+    -- TODO: Validating `truncation_chance` and casting to number should happen upstream.
+    if timeline_capacity > 0 and math.random() < tonumber(truncation_chance) then
+        truncate_timeline(configuration, timeline_id, timeline_capacity)
+    end
+
+    return ready
+end
+
+local function digest_timeline(configuration, timeline_id)
+    -- Check to ensure that the timeline is in the correct state.
+    if redis.call('ZSCORE', configuration:get_schedule_ready_key(), timeline_id) == false then
+        error('err(invalid_state): timeline is not in the ready state, cannot be digested')
+    end
+
+    local digest_key = configuration:get_timeline_digest_key(timeline_id)
+    local timeline_key = configuration:get_timeline_key(timeline_id)
+    if redis.call('EXISTS', timeline_key) == 1 then
+        if redis.call('EXISTS', digest_key) == 1 then
+            -- If the digest set already exists (possibly because we already tried
+            -- to send it and failed for some reason), merge any new data into it.
+            -- TODO: It might make sense to trim here to avoid returning capacity *
+            -- 2 if timeline was full when it was previously digested.
+            redis.call('ZUNIONSTORE', digest_key, 2, timeline_key, digest_key, 'AGGREGATE', 'MAX')
+            redis.call('DEL', timeline_key)
+        else
+            -- Otherwise, we can just move the timeline contents to the digest key.
+            redis.call('RENAME', timeline_key, digest_key)
+        end
+        redis.call('EXPIRE', digest_key, configuration.ttl)
+    end
+
+    local results = {}
+    local records = redis.call('ZREVRANGE', digest_key, 0, -1, 'WITHSCORES')
+    for key, score in zrange_scored_iterator(records) do
+        table.insert(results, {
+            key,
+            redis.call('GET', configuration:get_timeline_record_key(timeline_id, key)),
+            score
+        })
+    end
+
+    return results
+end
+
+local function close_digest(configuration, timeline_id, delay_minimum, ...)
+    local record_ids = {...}
+    local timeline_key = configuration:get_timeline_key(timeline_id)
+    local digest_key = configuration:get_timeline_digest_key(timeline_id)
+
+    if #record_ids > 0 then
+        redis.call('ZREM', digest_key, unpack(record_ids))
+        for _, record_id in ipairs(record_ids) do
+            redis.call('DEL', configuration:get_timeline_record_key(timeline_id, record_id))
+        end
+    end
+
+    -- If this digest didn't contain any data (no record IDs) and there isn't
+    -- any data left in the timeline or digest sets, we can safely remove this
+    -- timeline reference from all schedule sets.
+    if #record_ids > 0 or redis.call('ZCARD', timeline_key) > 0 or redis.call('ZCARD', digest_key) > 0 then
+        redis.call('SETEX', configuration:get_timeline_last_processed_timestamp_key(timeline_id), configuration.ttl, configuration.timestamp)
+        redis.call('ZREM', configuration:get_schedule_ready_key(), timeline_id)
+        redis.call('ZADD', configuration:get_schedule_waiting_key(), configuration.timestamp + delay_minimum, timeline_id)
+    else
+        redis.call('DEL', configuration:get_timeline_last_processed_timestamp_key(timeline_id))
+        redis.call('ZREM', configuration:get_schedule_ready_key(), timeline_id)
+        redis.call('ZREM', configuration:get_schedule_waiting_key(), timeline_id)
+    end
+end
+
+local function delete_timeline(configuration, timeline_id)
+    truncate_timeline(configuration, timeline_id, 0)
+    truncate_digest(configuration, timeline_id, 0)
+    redis.call('DEL', configuration:get_timeline_last_processed_timestamp_key(timeline_id))
+    redis.call('ZREM', configuration:get_schedule_ready_key(), timeline_id)
+    redis.call('ZREM', configuration:get_schedule_waiting_key(), timeline_id)
+end
+
+local function parse_arguments(arguments)
+    -- TODO: These need validation!
+    local configuration = {
+        namespace = arguments[1],
+        ttl = tonumber(arguments[2]),
+        timestamp = tonumber(arguments[3]),
+    }
+
+    math.randomseed(configuration.timestamp)
+
+    function configuration:get_schedule_waiting_key()
+        return string.format('%s:s:w', self.namespace)
+    end
+
+    function configuration:get_schedule_ready_key()
+        return string.format('%s:s:r', self.namespace)
+    end
+
+    function configuration:get_timeline_key(timeline_id)
+        return string.format('%s:t:%s', self.namespace, timeline_id)
+    end
+
+    function configuration:get_timeline_digest_key(timeline_id)
+        return string.format('%s:t:%s:d', self.namespace, timeline_id)
+    end
+
+    function configuration:get_timeline_last_processed_timestamp_key(timeline_id)
+        return string.format('%s:t:%s:l', self.namespace, timeline_id)
+    end
+
+    function configuration:get_timeline_record_key(timeline_id, record_id)
+        return string.format('%s:t:%s:r:%s', self.namespace, timeline_id, record_id)
+    end
+
+    return configuration, table.slice(arguments, 4)
+end
+
+local commands = {
+    SCHEDULE = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return schedule(configuration, unpack(arguments))
+    end,
+    MAINTENANCE = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return maintenance(configuration, unpack(arguments))
+    end,
+    ADD = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return add_record_to_timeline(configuration, unpack(arguments))
+    end,
+    DELETE = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return delete_timeline(configuration, unpack(arguments))
+    end,
+    DIGEST_OPEN = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return digest_timeline(configuration, unpack(arguments))
+    end,
+    DIGEST_CLOSE = function (arguments)
+        local configuration, arguments = parse_arguments(arguments)
+        return close_digest(configuration, unpack(arguments))
+    end,
+}
+
+return commands[ARGV[1]](table.slice(ARGV, 2))
diff --git a/src/sentry/scripts/digests/ensure_timeline_scheduled.lua b/src/sentry/scripts/digests/ensure_timeline_scheduled.lua
deleted file mode 100644
index 47fa3d7f3c..0000000000
--- a/src/sentry/scripts/digests/ensure_timeline_scheduled.lua
+++ /dev/null
@@ -1,57 +0,0 @@
--- Ensures a timeline is scheduled to be digested, adjusting the schedule time
--- if necessary.
--- KEYS: {WAITING, READY, LAST_PROCESSED_TIMESTAMP}
--- ARGV: {
---  TIMELINE,   -- timeline key
---  TIMESTAMP,  --
---  INCREMENT,  -- amount of time (in seconds) that an event addition delays
---              -- scheduling
---  MAXIMUM     -- maximum amount of time (in seconds) between a timeline
---              -- being digested, and the same timeline being scheduled for
---              -- the next digestion
--- }
-local WAITING = KEYS[1] or error("incorrect number of keys provided")
-local READY = KEYS[2] or error("incorrect number of keys provided")
-local LAST_PROCESSED_TIMESTAMP = KEYS[3] or error("incorrect number of keys provided")
-
-local TIMELINE = ARGV[1] or error("incorrect number of arguments provided")
-local TIMESTAMP = ARGV[2] or error("incorrect number of arguments provided")
-local INCREMENT = ARGV[3] or error("incorrect number of arguments provided")
-local MAXIMUM = ARGV[4] or error("incorrect number of arguments provided")
-
--- If the timeline is already in the "ready" set, this is a noop.
-if tonumber(redis.call('ZSCORE', READY, TIMELINE)) ~= nil then
-    return false
-end
-
--- Otherwise, check to see if the timeline is in the "waiting" set.
-local score = tonumber(redis.call('ZSCORE', WAITING, TIMELINE))
-if score ~= nil then
-    -- If the timeline is already in the "waiting" set, increase the delay by
-    -- min(current schedule + increment value, maximum delay after last processing time).
-    local last = tonumber(redis.call('GET', LAST_PROCESSED_TIMESTAMP))
-    local update = nil;
-    if last == nil then
-        -- If the last processed timestamp is missing for some reason (possibly
-        -- evicted), be conservative and allow the timeline to be scheduled
-        -- with either the current schedule time or provided timestamp,
-        -- whichever is smaller.
-        update = math.min(score, TIMESTAMP)
-    else
-        update = math.min(
-            score + tonumber(INCREMENT),
-            last + tonumber(MAXIMUM)
-        )
-    end
-
-    if update ~= score then
-        redis.call('ZADD', WAITING, update, TIMELINE)
-    end
-    return false
-end
-
--- If the timeline isn't already in either set, add it to the "ready" set with
--- the provided timestamp. This allows for immediate scheduling, bypassing the
--- imposed delay of the "waiting" state.
-redis.call('ZADD', READY, TIMESTAMP, TIMELINE)
-return true
diff --git a/src/sentry/scripts/digests/truncate_timeline.lua b/src/sentry/scripts/digests/truncate_timeline.lua
deleted file mode 100644
index 1d1631112a..0000000000
--- a/src/sentry/scripts/digests/truncate_timeline.lua
+++ /dev/null
@@ -1,12 +0,0 @@
--- Trims a timeline to a maximum number of records.
--- Returns the number of keys that were deleted.
--- KEYS: {TIMELINE}
--- ARGV: {LIMIT, PREFIX}
-local keys = redis.call('ZREVRANGE', KEYS[1], ARGV[1], -1)
-local prefix = ARGV[2]
-local separator = ARGV[3]
-for i, record in pairs(keys) do
-    redis.call('DEL', prefix .. ':r:' .. record)
-    redis.call('ZREM', KEYS[1], record)
-end
-return table.getn(keys)
diff --git a/tests/sentry/digests/backends/test_redis.py b/tests/sentry/digests/backends/test_redis.py
index 32412c1dec..fb0f2c0072 100644
--- a/tests/sentry/digests/backends/test_redis.py
+++ b/tests/sentry/digests/backends/test_redis.py
@@ -1,388 +1,109 @@
 from __future__ import absolute_import
 
-import functools
-import itertools
-import mock
-import six
+import pytest
 import time
 
-from exam import fixture
-
-from sentry.digests import (
-    Record,
-)
-from sentry.digests.backends.redis import (
-    SCHEDULE_STATE_READY,
-    SCHEDULE_STATE_WAITING,
-    RedisBackend,
-    ensure_timeline_scheduled,
-    make_digest_key,
-    make_last_processed_timestamp_key,
-    make_record_key,
-    make_schedule_key,
-    make_timeline_key,
-    truncate_timeline,
-)
-from sentry.utils.redis import clusters
+from sentry.digests import Record
+from sentry.digests.backends.base import InvalidState
+from sentry.digests.backends.redis import RedisBackend
 from sentry.testutils import TestCase
 
 
-def get_set_size(cluster, key):
-    results = []
-    with cluster.all() as client:
-        results = client.zcard(key)
-    return sum(results.value.values())
-
-
-class BaseRedisBackendTestCase(TestCase):
-    @fixture
-    def records(self):
-        for i in itertools.count():
-            yield Record(six.text_type(i), six.text_type(i), float(i))
-
-
-class RedisScriptTestCase(BaseRedisBackendTestCase):
-    def test_ensure_timeline_scheduled_script(self):
-        cluster = clusters.get('default')
-        client = cluster.get_local_client(six.next(iter(cluster.hosts)))
-
-        timeline = 'timeline'
-        timestamp = 100.0
-
-        waiting_set_size = functools.partial(client.zcard, 'waiting')
-        ready_set_size = functools.partial(client.zcard, 'ready')
-
-        timeline_score_in_waiting_set = functools.partial(client.zscore, 'waiting', timeline)
-        timeline_score_in_ready_set = functools.partial(client.zscore, 'ready', timeline)
-
-        keys = ('waiting', 'ready', 'last-processed')
-
-        # The first addition should cause the timeline to be added to the ready set.
-        with self.assertChanges(ready_set_size, before=0, after=1), \
-                self.assertChanges(timeline_score_in_ready_set, before=None, after=timestamp):
-            assert ensure_timeline_scheduled(client, keys, (timeline, timestamp, 1, 10)) == 1
-
-        # Adding it again with a timestamp in the future should not change the schedule time.
-        with self.assertDoesNotChange(waiting_set_size), \
-                self.assertDoesNotChange(ready_set_size), \
-                self.assertDoesNotChange(timeline_score_in_ready_set):
-            assert ensure_timeline_scheduled(client, keys,
-                                             (timeline, timestamp + 50, 1, 10)) is None
-
-        # Move the timeline from the ready set to the waiting set.
-        client.zrem('ready', timeline)
-        client.zadd('waiting', timestamp, timeline)
-        client.set('last-processed', timestamp)
-
-        increment = 1
-        with self.assertDoesNotChange(waiting_set_size), \
-                self.assertChanges(timeline_score_in_waiting_set, before=timestamp, after=timestamp + increment):
-            assert ensure_timeline_scheduled(client, keys,
-                                             (timeline, timestamp, increment, 10)) is None
-
-        # Make sure the schedule respects the maximum value.
-        with self.assertDoesNotChange(waiting_set_size), \
-                self.assertChanges(timeline_score_in_waiting_set, before=timestamp + 1, after=timestamp):
-            assert ensure_timeline_scheduled(client, keys,
-                                             (timeline, timestamp, increment, 0)) is None
-
-        # Test to ensure a missing last processed timestamp can be handled
-        # correctly (chooses minimum of schedule value and record timestamp.)
-        client.zadd('waiting', timestamp, timeline)
-        client.delete('last-processed')
-        with self.assertDoesNotChange(waiting_set_size), \
-                self.assertDoesNotChange(timeline_score_in_waiting_set):
-            assert ensure_timeline_scheduled(
-                client, keys, (timeline, timestamp + 100, increment, 10)
-            ) is None
-
-        with self.assertDoesNotChange(waiting_set_size), \
-                self.assertChanges(timeline_score_in_waiting_set, before=timestamp, after=timestamp - 100):
-            assert ensure_timeline_scheduled(
-                client, keys, (timeline, timestamp - 100, increment, 10)
-            ) is None
-
-    def test_truncate_timeline_script(self):
-        cluster = clusters.get('default')
-        client = cluster.get_local_client(six.next(iter(cluster.hosts)))
-
-        timeline = 'timeline'
-
-        # Preload some fake records (the contents don't matter.)
-        records = list(itertools.islice(self.records, 10))
-        for record in records:
-            client.zadd(timeline, record.timestamp, record.key)
-            client.set(make_record_key(timeline, record.key), 'data')
-
-        with self.assertChanges(lambda: client.zcard(timeline), before=10, after=5):
-            truncate_timeline(client, (timeline, ), (5, timeline))
-
-            # Ensure the early records don't exist.
-            for record in records[:5]:
-                assert not client.zscore(timeline, record.key)
-                assert not client.exists(make_record_key(timeline, record.key))
-
-            # Ensure the later records do exist.
-            for record in records[-5:]:
-                assert client.zscore(timeline, record.key) == float(record.timestamp)
-                assert client.exists(make_record_key(timeline, record.key))
-
-
-class RedisBackendTestCase(BaseRedisBackendTestCase):
-    def test_add_record(self):
-        timeline = 'timeline'
+class RedisBackendTestCase(TestCase):
+    def test_basic(self):
         backend = RedisBackend()
 
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        connection = backend.cluster.get_local_client_for_key(timeline_key)
+        # The first item should return "true", indicating that this timeline
+        # can be immediately dispatched to be digested.
+        record_1 = Record('record:1', 'value', time.time())
+        assert backend.add('timeline', record_1) is True
 
-        record = next(self.records)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
-        record_key = make_record_key(timeline_key, record.key)
+        # The second item should return "false", since it's ready to be
+        # digested but dispatching again would cause it to be sent twice.
+        record_2 = Record('record:2', 'value', time.time())
+        assert backend.add('timeline', record_2) is False
 
-        get_timeline_score_in_ready_set = functools.partial(
-            connection.zscore, ready_set_key, timeline
-        )
-        get_record_score_in_timeline_set = functools.partial(
-            connection.zscore, timeline_key, record.key
-        )
+        # There's nothing to move between sets, so scheduling should return nothing.
+        assert set(backend.schedule(time.time())) == set()
 
-        def get_record_value():
-            value = connection.get(record_key)
-            return backend.codec.decode(value) if value is not None else None
+        with backend.digest('timeline', 0) as records:
+            assert set(records) == set([record_1, record_2])
 
-        with self.assertChanges(get_timeline_score_in_ready_set, before=None, after=record.timestamp), \
-                self.assertChanges(get_record_score_in_timeline_set, before=None, after=record.timestamp), \
-                self.assertChanges(get_record_value, before=None, after=record.value):
-            backend.add(timeline, record)
-
-    def test_truncation(self):
-        timeline = 'timeline'
-        capacity = 5
-        backend = RedisBackend(capacity=capacity, truncation_chance=0.5)
+        # The schedule should now contain the timeline.
+        assert set(entry.key for entry in backend.schedule(time.time())) == set(['timeline'])
 
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        connection = backend.cluster.get_local_client_for_key(timeline_key)
+        # We didn't add any new records so there's nothing to do here.
+        with backend.digest('timeline', 0) as records:
+            assert set(records) == set([])
 
-        get_timeline_size = functools.partial(connection.zcard, timeline_key)
+        # There's nothing to move between sets since the timeline contents no
+        # longer exist at this point.
+        assert set(backend.schedule(time.time())) == set()
 
-        fill = 10
+    def test_truncation(self):
+        backend = RedisBackend(capacity=2, truncation_chance=1.0)
 
-        with mock.patch('random.random', return_value=1.0):
-            with self.assertChanges(get_timeline_size, before=0, after=fill):
-                for _ in range(fill):
-                    backend.add(timeline, next(self.records))
+        records = [Record('record:{}'.format(i), 'value', time.time()) for i in xrange(4)]
+        for record in records:
+            backend.add('timeline', record)
 
-        with mock.patch('random.random', return_value=0.0):
-            with self.assertChanges(get_timeline_size, before=fill, after=capacity):
-                backend.add(timeline, next(self.records))
+        with backend.digest('timeline', 0) as records:
+            assert set(records) == set(records[-2:])
 
-    def test_scheduling(self):
+    def test_maintenance_failure_recovery(self):
         backend = RedisBackend()
 
-        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
-
-        n = 10
-
-        for i in range(n):
-            with backend.cluster.map() as client:
-                client.zadd(waiting_set_key, i, 'timelines:{0}'.format(i))
-
-        for i in range(n, n * 2):
-            with backend.cluster.map() as client:
-                client.zadd(ready_set_key, i, 'timelines:{0}'.format(i))
-
-        get_waiting_set_size = functools.partial(get_set_size, backend.cluster, waiting_set_key)
-        get_ready_set_size = functools.partial(get_set_size, backend.cluster, ready_set_key)
-
-        with self.assertChanges(get_waiting_set_size, before=n, after=0), \
-                self.assertChanges(get_ready_set_size, before=n, after=n * 2):
-            results = list(zip(range(n), list(backend.schedule(n, chunk=5))))
-            assert len(results) is n
-
-            # Ensure scheduled entries are returned earliest first.
-            for i, entry in results:
-                assert entry.key == 'timelines:{0}'.format(i)
-                assert entry.timestamp == float(i)
-
-    def test_maintenance(self):
-        timeline = 'timeline'
-        backend = RedisBackend(ttl=3600)
+        record_1 = Record('record:1', 'value', time.time())
+        backend.add('timeline', record_1)
 
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        digest_key = make_digest_key(timeline_key)
-        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
-
-        now = time.time()
+        try:
+            with backend.digest('timeline', 0) as records:
+                raise Exception('This causes the digest to not be closed.')
+        except Exception:
+            pass
 
-        connection = backend.cluster.get_local_client_for_key(timeline_key)
-        schedule_time = now - 60
-        connection.zadd(ready_set_key, schedule_time, timeline)
-        connection.zadd(timeline_key, 0, '1')
-        connection.set(make_record_key(timeline_key, '1'), 'data')
-        connection.zadd(digest_key, 0, '2')
-        connection.set(make_record_key(timeline_key, '2'), 'data')
+        # Maintenance should move the timeline back to the waiting state, ...
+        backend.maintenance(time.time())
 
-        # Move the digest from the ready set to the waiting set.
-        backend.maintenance(now)
-        assert connection.zcard(ready_set_key) == 0
-        assert connection.zrange(
-            waiting_set_key, 0, -1, withscores=True
-        ) == [(timeline, schedule_time)]
+        # ...and you can't send a digest in the waiting state.
+        with pytest.raises(InvalidState):
+            with backend.digest('timeline', 0) as records:
+                pass
 
-        connection.zrem(waiting_set_key, timeline)
-        connection.zadd(ready_set_key, schedule_time, timeline)
+        record_2 = Record('record:2', 'value', time.time())
+        backend.add('timeline', record_2)
 
-        # Delete the digest from the ready set.
-        with mock.patch('time.time', return_value=now + (backend.ttl + 1)):
-            backend.maintenance(now)
+        # The schedule should now contain the timeline.
+        assert set(entry.key for entry in backend.schedule(time.time())) == set(['timeline'])
 
-        keys = (
-            ready_set_key, waiting_set_key, timeline_key, digest_key,
-            make_record_key(timeline_key, '1'), make_record_key(timeline_key, '2'),
-        )
-        for key in keys:
-            assert connection.exists(key) is False
+        # The existing and new record should be there because the timeline
+        # contents were merged back into the digest.
+        with backend.digest('timeline', 0) as records:
+            assert set(records) == set([record_1, record_2])
 
     def test_delete(self):
-        timeline = 'timeline'
-        backend = RedisBackend()
-
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        digest_key = make_digest_key(timeline_key)
-        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
-
-        connection = backend.cluster.get_local_client_for_key(timeline_key)
-        connection.zadd(waiting_set_key, 0, timeline)
-        connection.zadd(ready_set_key, 0, timeline)
-        connection.zadd(timeline_key, 0, '1')
-        connection.set(make_record_key(timeline_key, '1'), 'data')
-        connection.zadd(digest_key, 0, '2')
-        connection.set(make_record_key(timeline_key, '2'), 'data')
-
-        keys = (
-            waiting_set_key, ready_set_key, digest_key, timeline_key,
-            make_record_key(timeline_key, '1'), make_record_key(timeline_key, '2')
-        )
-
-        def check_keys_exist():
-            return map(connection.exists, keys)
-
-        with self.assertChanges(
-            check_keys_exist, before=[True] * len(keys), after=[False] * len(keys)
-        ):
-            backend.delete(timeline)
-
-
-class ExpectedError(Exception):
-    pass
-
-
-class DigestTestCase(BaseRedisBackendTestCase):
-    def test_digesting(self):
         backend = RedisBackend()
+        backend.add('timeline', Record('record:1', 'value', time.time()))
+        backend.delete('timeline')
 
-        # XXX: This assumes the that adding records and scheduling are working
-        # correctly to set up the state needed for this test!
-
-        timeline = 'timeline'
-        n = 10
-        records = list(itertools.islice(self.records, n))
-        for record in records:
-            backend.add(timeline, record)
-
-        for entry in backend.schedule(time.time()):
-            pass
-
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        client = backend.cluster.get_local_client_for_key(timeline_key)
+        with pytest.raises(InvalidState):
+            with backend.digest('timeline', 0) as records:
+                assert set(records) == set([])
 
-        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
+        assert set(backend.schedule(time.time())) == set()
+        assert len(backend._get_connection('timeline').keys('d:*')) == 0
 
-        get_timeline_size = functools.partial(client.zcard, timeline_key)
-        get_waiting_set_size = functools.partial(get_set_size, backend.cluster, waiting_set_key)
-        get_ready_set_size = functools.partial(get_set_size, backend.cluster, ready_set_key)
-
-        with self.assertChanges(get_timeline_size, before=n, after=0), \
-                self.assertChanges(get_waiting_set_size, before=0, after=1), \
-                self.assertChanges(get_ready_set_size, before=1, after=0):
-
-            timestamp = time.time()
-            with mock.patch('time.time', return_value=timestamp), \
-                    backend.digest(timeline) as entries:
-                entries = list(entries)
-                assert entries == records[::-1]
-
-            next_scheduled_delivery = timestamp + backend.minimum_delay
-            assert client.zscore(waiting_set_key, timeline) == next_scheduled_delivery
-            assert int(client.get(make_last_processed_timestamp_key(timeline_key))
-                       ) == int(timestamp)
-
-        # Move the timeline back to the ready set.
-        for entry in backend.schedule(next_scheduled_delivery):
-            pass
-
-        # The digest should be removed from the schedule if it is empty.
-        with self.assertDoesNotChange(get_waiting_set_size), \
-                self.assertChanges(get_ready_set_size, before=1, after=0):
-            with backend.digest(timeline) as entries:
-                assert list(entries) == []
-
-        assert client.get(make_last_processed_timestamp_key(timeline_key)) is None
-
-    def test_digesting_failure_recovery(self):
+    def test_missing_record_contents(self):
         backend = RedisBackend()
 
-        # XXX: This assumes the that adding records and scheduling are working
-        # correctly to set up the state needed for this test!
-
-        timeline = 'timeline'
-        n = 10
-        records = list(itertools.islice(self.records, n))
-        for record in records:
-            backend.add(timeline, record)
-
-        for entry in backend.schedule(time.time()):
-            pass
-
-        timeline_key = make_timeline_key(backend.namespace, timeline)
-        client = backend.cluster.get_local_client_for_key(timeline_key)
-
-        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
-        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
-
-        get_waiting_set_size = functools.partial(get_set_size, backend.cluster, waiting_set_key)
-        get_ready_set_size = functools.partial(get_set_size, backend.cluster, ready_set_key)
-        get_timeline_size = functools.partial(client.zcard, timeline_key)
-        get_digest_size = functools.partial(client.zcard, make_digest_key(timeline_key))
-
-        with self.assertChanges(get_timeline_size, before=n, after=0), \
-                self.assertChanges(get_digest_size, before=0, after=n), \
-                self.assertDoesNotChange(get_waiting_set_size), \
-                self.assertDoesNotChange(get_ready_set_size):
-            try:
-                with backend.digest(timeline) as entries:
-                    raise ExpectedError
-            except ExpectedError:
-                pass
-
-        # Add another few records to the timeline to ensure they end up in the digest.
-        extra = list(itertools.islice(self.records, 5))
-        for record in extra:
-            backend.add(timeline, record)
+        record_1 = Record('record:1', 'value', time.time())
+        backend.add('timeline', record_1)
+        backend._get_connection('timeline').delete('d:t:timeline:r:record:1')
 
-        with self.assertChanges(get_timeline_size, before=len(extra), after=0), \
-                self.assertChanges(get_digest_size, before=len(records), after=0), \
-                self.assertChanges(get_waiting_set_size, before=0, after=1), \
-                self.assertChanges(get_ready_set_size, before=1, after=0):
-            timestamp = time.time()
-            with mock.patch('time.time', return_value=timestamp), \
-                    backend.digest(timeline) as entries:
-                entries = list(entries)
-                assert entries == (records + extra)[::-1]
+        record_2 = Record('record:2', 'value', time.time())
+        backend.add('timeline', record_2)
 
-            assert client.zscore(waiting_set_key, timeline) == timestamp + backend.minimum_delay
+        # The existing and new record should be there because the timeline
+        # contents were merged back into the digest.
+        with backend.digest('timeline', 0) as records:
+            assert set(records) == set([record_2])
