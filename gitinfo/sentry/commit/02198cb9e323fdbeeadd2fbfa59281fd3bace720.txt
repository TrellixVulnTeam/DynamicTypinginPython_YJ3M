commit 02198cb9e323fdbeeadd2fbfa59281fd3bace720
Author: Nathan Heskia <nathan.heskia@gmail.com>
Date:   Mon Mar 9 12:20:15 2020 -0700

    feat(exclude_filters): Add signal_only option for buffers (#17491)
    
    * Exclude a set of filters from model create/update
    
    * Use pickle only
    
    * Add exclude_filters explanation
    
    * Use json.loads
    
    * Use signal_only and update tests
    
    * Update incr doc string
    
    * Signal only in local buffers defaul val
    
    * Skip update kwargs/extra if signal only
    
    * Add more tests

diff --git a/src/sentry/buffer/base.py b/src/sentry/buffer/base.py
index 88e99c7239..604ce27600 100644
--- a/src/sentry/buffer/base.py
+++ b/src/sentry/buffer/base.py
@@ -34,35 +34,49 @@ class Buffer(Service):
 
     __all__ = ("incr", "process", "process_pending", "validate")
 
-    def incr(self, model, columns, filters, extra=None):
+    def incr(self, model, columns, filters, extra=None, signal_only=None):
         """
         >>> incr(Group, columns={'times_seen': 1}, filters={'pk': group.pk})
+        signal_only - added to indicate that `process` should only call the complete
+        signal handler with the updated model and skip creates/updates in the database. this
+        is useful in cases where we need to do additional processing before writing to the
+        database and opt to do it in a `buffer_incr_complete` receiver.
         """
         process_incr.apply_async(
-            kwargs={"model": model, "columns": columns, "filters": filters, "extra": extra}
+            kwargs={
+                "model": model,
+                "columns": columns,
+                "filters": filters,
+                "extra": extra,
+                "signal_only": signal_only,
+            }
         )
 
     def process_pending(self, partition=None):
         return []
 
-    def process(self, model, columns, filters, extra=None):
+    def process(self, model, columns, filters, extra=None, signal_only=None):
         from sentry.models import Group
         from sentry.event_manager import ScoreClause
 
-        update_kwargs = dict((c, F(c) + v) for c, v in six.iteritems(columns))
-        if extra:
-            update_kwargs.update(extra)
+        created = False
 
-        # HACK(dcramer): this is gross, but we dont have a good hook to compute this property today
-        # XXX(dcramer): remove once we can replace 'priority' with something reasonable via Snuba
-        if model is Group and "last_seen" in update_kwargs and "times_seen" in update_kwargs:
-            update_kwargs["score"] = ScoreClause(
-                group=None,
-                times_seen=update_kwargs["times_seen"],
-                last_seen=update_kwargs["last_seen"],
-            )
+        if not signal_only:
+            update_kwargs = dict((c, F(c) + v) for c, v in six.iteritems(columns))
 
-        _, created = model.objects.create_or_update(values=update_kwargs, **filters)
+            if extra:
+                update_kwargs.update(extra)
+
+            # HACK(dcramer): this is gross, but we dont have a good hook to compute this property today
+            # XXX(dcramer): remove once we can replace 'priority' with something reasonable via Snuba
+            if model is Group and "last_seen" in update_kwargs and "times_seen" in update_kwargs:
+                update_kwargs["score"] = ScoreClause(
+                    group=None,
+                    times_seen=update_kwargs["times_seen"],
+                    last_seen=update_kwargs["last_seen"],
+                )
+
+            _, created = model.objects.create_or_update(values=update_kwargs, **filters)
 
         buffer_incr_complete.send_robust(
             model=model,
diff --git a/src/sentry/buffer/inprocess.py b/src/sentry/buffer/inprocess.py
index 1cfc7544bb..2fdf0b9fe8 100644
--- a/src/sentry/buffer/inprocess.py
+++ b/src/sentry/buffer/inprocess.py
@@ -11,5 +11,5 @@ class InProcessBuffer(Buffer):
               in development and testing environments.
     """
 
-    def incr(self, model, columns, filters, extra=None):
-        self.process(model, columns, filters, extra)
+    def incr(self, model, columns, filters, extra=None, signal_only=None):
+        self.process(model, columns, filters, extra, signal_only)
diff --git a/src/sentry/buffer/redis.py b/src/sentry/buffer/redis.py
index 4a6917cc06..1fade10e98 100644
--- a/src/sentry/buffer/redis.py
+++ b/src/sentry/buffer/redis.py
@@ -172,13 +172,14 @@ class RedisBuffer(Buffer):
         else:
             raise TypeError("invalid type: {}".format(type_))
 
-    def incr(self, model, columns, filters, extra=None):
+    def incr(self, model, columns, filters, extra=None, signal_only=None):
         """
         Increment the key by doing the following:
 
         - Insert/update a hashmap based on (model, columns)
             - Perform an incrby on counters
             - Perform a set (last write wins) on extra
+            - Perform a set on signal_only (only if True)
         - Add hashmap key to pending flushes
         """
 
@@ -188,7 +189,9 @@ class RedisBuffer(Buffer):
                     frozen_filters = tuple(sorted(filters.items()))
                     key = (frozen_filters, model)
 
-                    stored_columns, stored_extra = _local_buffers.get(key, ({}, None))
+                    stored_columns, stored_extra, stored_signal_only = _local_buffers.get(
+                        key, ({}, None, None)
+                    )
 
                     for k, v in columns.items():
                         stored_columns[k] = stored_columns.get(k, 0) + v
@@ -196,7 +199,10 @@ class RedisBuffer(Buffer):
                     if extra is not None:
                         stored_extra = extra
 
-                    _local_buffers[key] = stored_columns, stored_extra
+                    if signal_only is not None:
+                        stored_signal_only = signal_only
+
+                    _local_buffers[key] = stored_columns, stored_extra, stored_signal_only
                     return
 
         # TODO(dcramer): longer term we'd rather not have to serialize values
@@ -225,6 +231,10 @@ class RedisBuffer(Buffer):
                 # (this is to ensure a zero downtime deploy where we can transition event processing)
                 pipe.hset(key, "e+" + column, pickle.dumps(value))
                 # pipe.hset(key, 'e+' + column, json.dumps(self._dump_value(value)))
+
+        if signal_only is True:
+            pipe.hset(key, "s", "1")
+
         pipe.expire(key, self.key_expire)
         pipe.zadd(pending_key, time(), key)
         pipe.execute()
@@ -322,6 +332,7 @@ class RedisBuffer(Buffer):
 
             incr_values = {}
             extra_values = {}
+            signal_only = None
             for k, v in six.iteritems(values):
                 if k.startswith("i+"):
                     incr_values[k[2:]] = int(v)
@@ -331,7 +342,9 @@ class RedisBuffer(Buffer):
                     else:
                         # TODO(dcramer): legacy pickle support - remove in Sentry 9.1
                         extra_values[k[2:]] = pickle.loads(v)
+                elif k == "s":
+                    signal_only = bool(int(v))  # Should be 1 if set
 
-            super(RedisBuffer, self).process(model, incr_values, filters, extra_values)
+            super(RedisBuffer, self).process(model, incr_values, filters, extra_values, signal_only)
         finally:
             client.delete(lock_key)
diff --git a/tests/sentry/buffer/base/tests.py b/tests/sentry/buffer/base/tests.py
index de05ec76c1..4007c29799 100644
--- a/tests/sentry/buffer/base/tests.py
+++ b/tests/sentry/buffer/base/tests.py
@@ -21,7 +21,7 @@ class BufferTest(TestCase):
         columns = {"times_seen": 1}
         filters = {"id": 1}
         self.buf.incr(model, columns, filters)
-        kwargs = dict(model=model, columns=columns, filters=filters, extra=None)
+        kwargs = dict(model=model, columns=columns, filters=filters, extra=None, signal_only=None)
         process_incr.apply_async.assert_called_once_with(kwargs=kwargs)
 
     def test_process_saves_data(self):
@@ -65,3 +65,14 @@ class BufferTest(TestCase):
         self.buf.process(ReleaseProject, columns, filters)
         release_project_ = ReleaseProject.objects.get(id=release_project.id)
         assert release_project_.new_groups == 1
+
+    @mock.patch("sentry.models.Group.objects.create_or_update")
+    def test_signal_only(self, create_or_update):
+        group = Group.objects.create(project=Project(id=1))
+        columns = {"times_seen": 1}
+        filters = {"id": group.id, "project_id": 1}
+        the_date = timezone.now() + timedelta(days=5)
+        prev_times_seen = group.times_seen
+        self.buf.process(Group, columns, filters, {"last_seen": the_date}, signal_only=True)
+        group.refresh_from_db()
+        assert group.times_seen == prev_times_seen
diff --git a/tests/sentry/buffer/redis/tests.py b/tests/sentry/buffer/redis/tests.py
index 0052bfb39d..4fc126dc13 100644
--- a/tests/sentry/buffer/redis/tests.py
+++ b/tests/sentry/buffer/redis/tests.py
@@ -67,8 +67,9 @@ class RedisBufferTest(TestCase):
         columns = {"times_seen": 2}
         filters = {"pk": 1}
         extra = {"foo": "bar", "datetime": datetime(2017, 5, 3, 6, 6, 6, tzinfo=timezone.utc)}
+        signal_only = None
         self.buf.process("foo")
-        process.assert_called_once_with(Group, columns, filters, extra)
+        process.assert_called_once_with(Group, columns, filters, extra, signal_only)
 
     @mock.patch("sentry.buffer.redis.RedisBuffer._make_key", mock.Mock(return_value="foo"))
     @mock.patch("sentry.buffer.base.Buffer.process")
@@ -86,8 +87,9 @@ class RedisBufferTest(TestCase):
         columns = {"times_seen": 2}
         filters = {"pk": 1}
         extra = {"foo": "bar"}
+        signal_only = None
         self.buf.process("foo")
-        process.assert_called_once_with(Group, columns, filters, extra)
+        process.assert_called_once_with(Group, columns, filters, extra, signal_only)
 
     # this test should be passing once we no longer serialize using pickle
     @pytest.mark.xfail
@@ -168,3 +170,61 @@ class RedisBufferTest(TestCase):
 
         # Make sure we didn't queue up more
         assert len(process_pending.apply_async.mock_calls) == 2
+
+    @mock.patch("sentry.buffer.redis.RedisBuffer._make_key", mock.Mock(return_value="foo"))
+    @mock.patch("sentry.buffer.base.Buffer.process")
+    def test_process_uses_signal_only(self, process):
+        client = self.buf.cluster.get_routing_client()
+        client.hmset(
+            "foo",
+            {
+                "f": '{"pk": ["i","1"]}',
+                "i+times_seen": "1",
+                "m": "sentry.utils.compat.mock.Mock",
+                "s": "1",
+            },
+        )
+        self.buf.process("foo")
+        process.assert_called_once_with(mock.Mock, {"times_seen": 1}, {"pk": 1}, {}, True)
+
+    """
+    @mock.patch("sentry.buffer.redis.RedisBuffer._make_key", mock.Mock(return_value="foo"))
+    def test_incr_uses_signal_only(self):
+        now = datetime(2017, 5, 3, 6, 6, 6, tzinfo=timezone.utc)
+        client = self.buf.cluster.get_routing_client()
+        model = mock.Mock()
+        model.__name__ = "Mock"
+        columns = {"times_seen": 1}
+        filters = {"pk": 1, "datetime": now}
+        self.buf.incr(model, columns, filters, extra={"foo": "bar", "datetime": now}, signal_only=True)
+        result = client.hgetall("foo")
+        assert result == {
+            "e+foo": '["s","bar"]',
+            "e+datetime": '["d","1493791566.000000"]',
+            "f": '{"pk":["i","1"],"datetime":["d","1493791566.000000"]}',
+            "i+times_seen": "1",
+            "m": "mock.mock.Mock",
+            "s": "1"
+        }
+    """
+
+    @mock.patch("sentry.buffer.redis.RedisBuffer._make_key", mock.Mock(return_value="foo"))
+    @mock.patch("sentry.buffer.redis._local_buffers", dict())
+    def test_signal_only_saved_local_buffs(self):
+        now = datetime(2017, 5, 3, 6, 6, 6, tzinfo=timezone.utc)
+        model = mock.Mock()
+        model.__name__ = "Mock"
+        columns = {"times_seen": 1}
+        filters = {"pk": 1, "datetime": now}
+
+        self.buf.incr(
+            model, columns, filters, extra={"foo": "bar", "datetime": now}, signal_only=True
+        )
+
+        from sentry.buffer.redis import _local_buffers
+
+        frozen_filters = tuple(sorted(filters.items()))
+        key = (frozen_filters, model)
+        values = _local_buffers[key]
+
+        assert values[-1]  # signal_only stored last
