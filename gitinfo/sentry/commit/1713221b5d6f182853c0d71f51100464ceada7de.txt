commit 1713221b5d6f182853c0d71f51100464ceada7de
Author: Lyn Nagara <lyn.nagara@gmail.com>
Date:   Fri Jan 24 15:13:03 2020 -0800

    perf(nodestore): Add the ability to cache node data (#16599)
    
    Add the ability to cache node data at a rate based on the sample rate option.
    If the "nodedata.cache-on-save" flag is set to True, we will also apply the
    caching to node data on initial save. Note these options are only
    intended to support gradual rollout of this feature, they are not a part
    of the design.
    
    Node data will only be cached if a "nodedata" cache is defined, otherwise
    caching will be skipped, the default cache will never be used.
    
    Missing items are not cached.

diff --git a/src/sentry/nodestore/base.py b/src/sentry/nodestore/base.py
index 2da4725ecd..b3d4016ade 100644
--- a/src/sentry/nodestore/base.py
+++ b/src/sentry/nodestore/base.py
@@ -1,11 +1,15 @@
 from __future__ import absolute_import
 
 import six
+from hashlib import md5
 
 from base64 import b64encode
 from threading import local
 from uuid import uuid4
 
+from django.core.cache import caches, InvalidCacheBackendError
+
+from sentry.utils.cache import memoize
 from sentry.utils.services import Service
 
 
@@ -21,6 +25,11 @@ class NodeStorage(local, Service):
         "generate_id",
         "cleanup",
         "validate",
+        "_get_cache_item",
+        "_get_cache_items",
+        "_set_cache_item",
+        "_delete_cache_item",
+        "_delete_cache_items",
     )
 
     def create(self, data):
@@ -85,3 +94,47 @@ class NodeStorage(local, Service):
 
     def cleanup(self, cutoff_timestamp):
         raise NotImplementedError
+
+    def _get_cache_item(self, id):
+        if self.cache and self.should_cache(id):
+            return self.cache.get(id)
+
+    def _get_cache_items(self, id_list):
+        cacheable_ids = [id for id in id_list if self.should_cache(id)]
+        if self.cache and self.sample_rate != 0.0:
+            return self.cache.get_many(cacheable_ids)
+        return {}
+
+    def _set_cache_item(self, id, data):
+        if self.cache and data:
+            if self.should_cache(id):
+                self.cache.set(id, data)
+
+    def _set_cache_items(self, items):
+        cacheable_items = {k: v for k, v in six.iteritems(items) if v and self.should_cache(k)}
+        if self.cache:
+            self.cache.set_many(cacheable_items)
+
+    def _delete_cache_item(self, id):
+        if self.cache:
+            self.cache.delete(id)
+
+    def _delete_cache_items(self, id_list):
+        if self.cache:
+            self.cache.delete_many(id_list)
+
+    @memoize
+    def cache(self):
+        try:
+            return caches["nodedata"]
+        except InvalidCacheBackendError:
+            return None
+
+    @memoize
+    def sample_rate(self):
+        from sentry import options
+
+        return options.get("nodedata.cache-sample-rate", 0.0)
+
+    def should_cache(self, id):
+        return (int(md5(id).hexdigest(), 16) % 1000) / 1000.0 < self.sample_rate
diff --git a/src/sentry/nodestore/bigtable/backend.py b/src/sentry/nodestore/bigtable/backend.py
index 5f0125de44..56f37bd5dc 100644
--- a/src/sentry/nodestore/bigtable/backend.py
+++ b/src/sentry/nodestore/bigtable/backend.py
@@ -10,8 +10,10 @@ from google.cloud.bigtable.row_set import RowSet
 from simplejson import JSONEncoder, _default_decoder
 from django.utils import timezone
 
+from sentry import options
 from sentry.nodestore.base import NodeStorage
 
+
 # Cache an instance of the encoder we want to use
 json_dumps = JSONEncoder(
     separators=(",", ":"),
@@ -97,22 +99,34 @@ class BigtableNodeStorage(NodeStorage):
         return get_connection(self.project, self.instance, self.table, self.options)
 
     def get(self, id):
-        return self.decode_row(self.connection.read_row(id))
+        item_from_cache = self._get_cache_item(id)
+        if item_from_cache:
+            return item_from_cache
+
+        data = self.decode_row(self.connection.read_row(id))
+        self._set_cache_item(id, data)
+        return data
 
     def get_multi(self, id_list):
         if len(id_list) == 1:
-            id = id_list[0]
-            return {id: self.get(id)}
+            return {id_list[0]: self.get(id_list[0])}
+
+        cache_items = self._get_cache_items(id_list)
 
+        if len(cache_items) == len(id_list):
+            return cache_items
+
+        uncached_ids = [id for id in id_list if id not in cache_items]
         rv = {}
         rows = RowSet()
-        for id in id_list:
+        for id in uncached_ids:
             rows.add_row_key(id)
             rv[id] = None
 
         for row in self.connection.read_rows(row_set=rows):
             rv[row.row_key] = self.decode_row(row)
-
+        self._set_cache_items(rv)
+        rv.update(cache_items)
         return rv
 
     def decode_row(self, row):
@@ -154,6 +168,9 @@ class BigtableNodeStorage(NodeStorage):
     def set(self, id, data, ttl=None):
         row = self.encode_row(id, data, ttl)
         row.commit()
+        cache_on_save = options.get("nodedata.cache-on-save")
+        if cache_on_save:
+            self._set_cache_item(id, data)
 
     def encode_row(self, id, data, ttl=None):
         data = json_dumps(data)
@@ -215,6 +232,7 @@ class BigtableNodeStorage(NodeStorage):
         row = self.connection.row(id)
         row.delete()
         row.commit()
+        self._delete_cache_item(id)
 
     def delete_multi(self, id_list):
         if self.skip_deletes:
@@ -231,6 +249,7 @@ class BigtableNodeStorage(NodeStorage):
             rows.append(row)
 
         self.connection.mutate_rows(rows)
+        self._delete_cache_items(id_list)
 
     def cleanup(self, cutoff_timestamp):
         raise NotImplementedError
diff --git a/src/sentry/nodestore/django/backend.py b/src/sentry/nodestore/django/backend.py
index 890569264d..bfbb1642b5 100644
--- a/src/sentry/nodestore/django/backend.py
+++ b/src/sentry/nodestore/django/backend.py
@@ -4,6 +4,7 @@ import math
 
 from django.utils import timezone
 
+from sentry import options
 from sentry.db.models import create_or_update
 from sentry.nodestore.base import NodeStorage
 
@@ -13,21 +14,39 @@ from .models import Node
 class DjangoNodeStorage(NodeStorage):
     def delete(self, id):
         Node.objects.filter(id=id).delete()
+        self._delete_cache_item(id)
 
     def get(self, id):
+        item_from_cache = self._get_cache_item(id)
+        if item_from_cache:
+            return item_from_cache
         try:
-            return Node.objects.get(id=id).data
+            data = Node.objects.get(id=id).data
+            self._set_cache_item(id, data)
+            return data
         except Node.DoesNotExist:
             return None
 
     def get_multi(self, id_list):
-        return {n.id: n.data for n in Node.objects.filter(id__in=id_list)}
+        cache_items = self._get_cache_items(id_list)
+        if len(cache_items) == len(id_list):
+            return cache_items
+
+        uncached_ids = [id for id in id_list if id not in cache_items]
+        items = {n.id: n.data for n in Node.objects.filter(id__in=uncached_ids)}
+        self._set_cache_items(items)
+        items.update(cache_items)
+        return items
 
     def delete_multi(self, id_list):
         Node.objects.filter(id__in=id_list).delete()
+        self._delete_cache_items(id_list)
 
     def set(self, id, data, ttl=None):
         create_or_update(Node, id=id, values={"data": data, "timestamp": timezone.now()})
+        cache_on_save = options.get("nodedata.cache-on-save")
+        if cache_on_save:
+            self._set_cache_item(id, data)
 
     def cleanup(self, cutoff_timestamp):
         from sentry.db.deletion import BulkDeleteQuery
@@ -36,3 +55,5 @@ class DjangoNodeStorage(NodeStorage):
         days = math.floor(total_seconds / 86400)
 
         BulkDeleteQuery(model=Node, dtfield="timestamp", days=days).execute()
+        if self.cache:
+            self.cache.clear()
diff --git a/src/sentry/utils/pytest/sentry.py b/src/sentry/utils/pytest/sentry.py
index 71c79a6e27..8928c42c01 100644
--- a/src/sentry/utils/pytest/sentry.py
+++ b/src/sentry/utils/pytest/sentry.py
@@ -94,7 +94,10 @@ def pytest_configure(config):
 
     settings.DISABLE_RAVEN = True
 
-    settings.CACHES = {"default": {"BACKEND": "django.core.cache.backends.locmem.LocMemCache"}}
+    settings.CACHES = {
+        "default": {"BACKEND": "django.core.cache.backends.locmem.LocMemCache"},
+        "nodedata": {"BACKEND": "django.core.cache.backends.locmem.LocMemCache"},
+    }
 
     if os.environ.get("USE_SNUBA", False):
         settings.SENTRY_SEARCH = "sentry.search.snuba.EventsDatasetSnubaSearchBackend"
diff --git a/tests/sentry/nodestore/bigtable/__init__.py b/tests/sentry/nodestore/bigtable/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/sentry/nodestore/bigtable/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/sentry/nodestore/bigtable/backend/__init__.py b/tests/sentry/nodestore/bigtable/backend/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/sentry/nodestore/bigtable/backend/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/sentry/nodestore/bigtable/backend/tests.py b/tests/sentry/nodestore/bigtable/backend/tests.py
new file mode 100644
index 0000000000..077746c309
--- /dev/null
+++ b/tests/sentry/nodestore/bigtable/backend/tests.py
@@ -0,0 +1,112 @@
+from __future__ import absolute_import
+
+import pytest
+
+from sentry.nodestore.bigtable.backend import BigtableNodeStorage
+from sentry.testutils import TestCase
+from sentry.utils.compat import mock
+
+
+@pytest.mark.skip(reason="Bigtable is not available in CI")
+class BigtableNodeStorageTest(TestCase):
+    def setUp(self):
+        self.ns = BigtableNodeStorage()
+        self.ns.bootstrap()
+
+    def test_get(self):
+        node_id = "node_id"
+        data = {"foo": "bar"}
+        self.ns.set(node_id, data)
+        assert self.ns.get(node_id) == data
+
+    def test_get_multi(self):
+        nodes = [("a" * 32, {"foo": "a"}), ("b" * 32, {"foo": "b"})]
+
+        self.ns.set(nodes[0][0], nodes[0][1])
+        self.ns.set(nodes[1][0], nodes[1][1])
+
+        result = self.ns.get_multi([nodes[0][0], nodes[1][0]])
+        assert result == dict((n[0], n[1]) for n in nodes)
+
+    def test_set(self):
+        node_id = "d2502ebbd7df41ceba8d3275595cac33"
+        data = {"foo": "bar"}
+        self.ns.set(node_id, data)
+        assert self.ns.get(node_id) == data
+
+    def test_delete(self):
+        node_id = "d2502ebbd7df41ceba8d3275595cac33"
+        data = {"foo": "bar"}
+        self.ns.set(node_id, data)
+        assert self.ns.get(node_id) == data
+        self.ns.delete(node_id)
+        assert not self.ns.get(node_id)
+
+    def test_delete_multi(self):
+        nodes = [("node_1", {"foo": "a"}), ("node_2", {"foo": "b"})]
+
+        for n in nodes:
+            self.ns.set(n[0], n[1])
+
+        self.ns.delete_multi([nodes[0][0], nodes[1][0]])
+        assert not self.ns.get(nodes[0][0])
+        assert not self.ns.get(nodes[1][0])
+
+    def test_cache(self):
+        with self.options({"nodedata.cache-sample-rate": 1.0, "nodedata.cache-on-save": True}):
+            node_1 = ("a" * 32, {"foo": "a"})
+            node_2 = ("b" * 32, {"foo": "b"})
+            node_3 = ("c" * 32, {"foo": "c"})
+
+            for node_id, data in [node_1, node_2, node_3]:
+                self.ns.set(node_id, data)
+
+            # Get / get multi populates cache
+            assert self.ns.get(node_1[0]) == node_1[1]
+            assert self.ns.get_multi([node_2[0], node_3[0]]) == {
+                node_2[0]: node_2[1],
+                node_3[0]: node_3[1],
+            }
+            with mock.patch.object(self.ns.connection, "read_row") as mock_read_row:
+                assert self.ns.get(node_1[0]) == node_1[1]
+                assert self.ns.get(node_2[0]) == node_2[1]
+                assert self.ns.get(node_3[0]) == node_3[1]
+                assert mock_read_row.call_count == 0
+
+            with mock.patch.object(self.ns.connection, "read_rows") as mock_read_rows:
+                assert self.ns.get_multi([node_1[0], node_2[0], node_3[0]])
+                assert mock_read_rows.call_count == 0
+
+            # Manually deleted item should still retreivable from cache
+            row = self.ns.connection.row(node_1[0])
+            row.delete()
+            row.commit()
+            assert self.ns.get(node_1[0]) == node_1[1]
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {
+                node_1[0]: node_1[1],
+                node_2[0]: node_2[1],
+            }
+
+            # Deletion clars cache
+            self.ns.delete(node_1[0])
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {
+                node_1[0]: None,
+                node_2[0]: node_2[1],
+            }
+            self.ns.delete_multi([node_1[0], node_2[0]])
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {node_1[0]: None, node_2[0]: None}
+
+            # Setting the item updates cache
+            new_value = {"event_id": "d" * 32}
+            self.ns.set(node_1[0], new_value)
+            with mock.patch.object(self.ns.connection, "read_row") as mock_read_row:
+                assert self.ns.get(node_1[0]) == new_value
+                assert mock_read_row.call_count == 0
+
+            # Missing rows are never cached
+            assert self.ns.get("node_4") is None
+            with mock.patch.object(self.ns.connection, "read_row") as mock_read_row:
+                mock_read_row.return_value = None
+                self.ns.get("node_4")
+                self.ns.get("node_4")
+                assert mock_read_row.call_count == 2
diff --git a/tests/sentry/nodestore/django/backend/tests.py b/tests/sentry/nodestore/django/backend/tests.py
index b6e7c630fa..65211d6d66 100644
--- a/tests/sentry/nodestore/django/backend/tests.py
+++ b/tests/sentry/nodestore/django/backend/tests.py
@@ -8,6 +8,7 @@ from django.utils import timezone
 from sentry.nodestore.django.models import Node
 from sentry.nodestore.django.backend import DjangoNodeStorage
 from sentry.testutils import TestCase
+from sentry.utils.compat import mock
 
 
 class DjangoNodeStorageTest(TestCase):
@@ -77,3 +78,57 @@ class DjangoNodeStorageTest(TestCase):
 
         assert Node.objects.filter(id=node.id).exists()
         assert not Node.objects.filter(id=node2.id).exists()
+
+    def test_cache(self):
+        with self.options({"nodedata.cache-sample-rate": 1.0, "nodedata.cache-on-save": True}):
+            node_1 = ("a" * 32, {"foo": "a"})
+            node_2 = ("b" * 32, {"foo": "b"})
+            node_3 = ("c" * 32, {"foo": "c"})
+
+            for node_id, data in [node_1, node_2, node_3]:
+                Node.objects.create(id=node_id, data=data)
+
+            # Get / get multi populates cache
+            assert self.ns.get(node_1[0]) == node_1[1]
+            assert self.ns.get_multi([node_2[0], node_3[0]]) == {
+                node_2[0]: node_2[1],
+                node_3[0]: node_3[1],
+            }
+            with mock.patch.object(Node.objects, "get") as mock_get:
+                assert self.ns.get(node_1[0]) == node_1[1]
+                assert self.ns.get(node_2[0]) == node_2[1]
+                assert self.ns.get(node_3[0]) == node_3[1]
+                assert mock_get.call_count == 0
+
+            with mock.patch.object(Node.objects, "filter") as mock_filter:
+                assert self.ns.get_multi([node_1[0], node_2[0], node_3[0]])
+                assert mock_filter.call_count == 0
+
+            # Manually deleted item should still retreivable from cache
+            Node.objects.get(id=node_1[0]).delete()
+            assert self.ns.get(node_1[0]) == node_1[1]
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {
+                node_1[0]: node_1[1],
+                node_2[0]: node_2[1],
+            }
+
+            # Deletion clars cache
+            self.ns.delete(node_1[0])
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {node_2[0]: node_2[1]}
+            self.ns.delete_multi([node_1[0], node_2[0]])
+            assert self.ns.get_multi([node_1[0], node_2[0]]) == {}
+
+            # Setting the item updates cache
+            new_value = {"event_id": "d" * 32}
+            self.ns.set(node_1[0], new_value)
+            with mock.patch.object(Node.objects, "get") as mock_get:
+                assert self.ns.get(node_1[0]) == new_value
+                assert mock_get.call_count == 0
+
+            # Missing rows are never cached
+            assert self.ns.get("node_4") is None
+            with mock.patch.object(Node.objects, "get") as mock_get:
+                mock_get.side_effect = Node.DoesNotExist
+                self.ns.get("node_4")
+                self.ns.get("node_4")
+                assert mock_get.call_count == 2
