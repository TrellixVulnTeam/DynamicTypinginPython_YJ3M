commit e76c759cf4b2fcea0f647896abef09d15b8a85fb
Author: Ted Kaemming <ted@kaemming.com>
Date:   Mon Oct 12 15:32:44 2015 -0700

    Minor improvements (updated timeouts, comments, etc.)

diff --git a/src/sentry/digests/backends/redis.py b/src/sentry/digests/backends/redis.py
index 10d8cd97f5..665d6b826d 100644
--- a/src/sentry/digests/backends/redis.py
+++ b/src/sentry/digests/backends/redis.py
@@ -137,8 +137,9 @@ class RedisBackend(Backend):
 
             pipeline.set(make_iteration_key(timeline_key), 0, nx=True)
 
-            # TODO: Prefix the entry with the timestamp (lexicographically
-            # sortable) to ensure that we can maintain abitrary precision:
+            # In the future, it might make sense to prefix the entry with the
+            # timestamp (lexicographically sortable) to ensure that we can
+            # maintain the correct sort order with abitrary precision:
             # http://redis.io/commands/ZADD#elements-with-the-same-score
             pipeline.zadd(timeline_key, record.timestamp, record.key)
 
@@ -222,7 +223,11 @@ class RedisBackend(Backend):
         # TODO: This needs tests!
 
         # TODO: This suffers from the same shard isolation issues as
-        # ``schedule``.
+        # ``schedule``. Ideally, this would also return the number of items
+        # that were rescheduled (and possibly even how late they were at the
+        # point of rescheduling) but that causes a bit of an API issue since in
+        # the case of an error, this can be considered a partial success (but
+        # still should raise an exception.)
         for host in self.cluster.hosts:
             connection = self.cluster.get_local_client(host)
 
diff --git a/src/sentry/plugins/bases/notify.py b/src/sentry/plugins/bases/notify.py
index 04fc6c27f8..95483fe240 100644
--- a/src/sentry/plugins/bases/notify.py
+++ b/src/sentry/plugins/bases/notify.py
@@ -54,6 +54,10 @@ class NotificationPlugin(Plugin):
         event = notification.event
         return self.notify_users(event.group, event)
 
+    def __can_be_digested(self, event):
+        return hasattr(self, 'notify_digest') and \
+            features.has('projects:digests', event.group.project)
+
     def rule_notify(self, event, futures):
         rules = []
         for future in futures:
@@ -62,10 +66,9 @@ class NotificationPlugin(Plugin):
                 continue
             raise NotImplementedError('The default behavior for notification de-duplication does not support args')
 
-        # TODO: Encapsulate this better, maybe make it an option on the plugin?
-        if features.has('projects:digests', event.group.project) and hasattr(self, 'notify_digest'):
+        if self.__can_be_digested(event):
             digests.add(
-                unsplit_key(self, event.group.project),
+                unsplit_key(self, event.group.project),  # TODO: Improve this abstraction.
                 event_to_record(event, rules),
             )
         else:
@@ -107,7 +110,7 @@ class NotificationPlugin(Plugin):
     def should_notify(self, group, event):
         # If digests are enabled for this project, we always want to add the
         # notification to the digest (even if it may be filtered out later.)
-        if features.has('projects:digests', group.project):
+        if self.__can_be_digested(event):
             return True
 
         if group.is_muted():
diff --git a/src/sentry/tasks/digests.py b/src/sentry/tasks/digests.py
index 2b3f07844d..5e20d9052a 100644
--- a/src/sentry/tasks/digests.py
+++ b/src/sentry/tasks/digests.py
@@ -7,7 +7,6 @@ from sentry.digests.notifications import (
     split_key,
 )
 from sentry.tasks.base import instrumented_task
-from sentry.utils import metrics
 
 
 @instrumented_task(
@@ -17,15 +16,20 @@ def schedule_digests():
     from sentry.app import digests
 
     deadline = time.time()
-    timeout = 30  # TODO: Make this a setting, it also should match task expiration.
 
-    # TODO: This might make sense to make probabilistic instead?
+    # The maximum (but hopefully not typical) expected delay can be roughly
+    # calculated by adding together the schedule interval, the # of shards *
+    # schedule timeout (at least until these are able to be processed in
+    # parallel), the expected duration of time an item spends waiting in the
+    # queue to be processed for delivery and the expected duration of time an
+    # item takes to be processed for delivery, so this timeout should be
+    # relatively high to avoid requeueing items before they even had a chance
+    # to be processed.
+    timeout = 300
     digests.maintenance(deadline - timeout)
 
-    deadline = time.time()
     for entry in digests.schedule(deadline):
         deliver_digest.delay(entry.key, entry.timestamp)
-        metrics.timing('digests.schedule_latency', time.time() - entry.timestamp)
 
 
 @instrumented_task(
@@ -40,6 +44,3 @@ def deliver_digest(key, schedule_timestamp):
 
     if digest:
         plugin.notify_digest(project, digest)
-
-    # TODO: This should probably report, no matter the outcome of the task?
-    metrics.timing('digests.delivery_latency', time.time() - schedule_timestamp)
