commit abf3f51e5820cb98fd0e3af9cd01882b0c92731b
Author: Michal Kuffa <beezz@users.noreply.github.com>
Date:   Mon Jun 22 10:12:11 2020 +0200

    ref(cache): Introduce configurable tmp store for events (#19162)
    
    Tmp event store acts by default just as facade to default_cache but
    allow to be configured to use different redis cache or any other
    compatible backend class.

diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 9cf5eb6c8c..b7087813cf 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -1074,6 +1074,10 @@ SENTRY_CACHE_OPTIONS = {}
 SENTRY_ATTACHMENTS = "sentry.attachments.default.DefaultAttachmentCache"
 SENTRY_ATTACHMENTS_OPTIONS = {}
 
+# Events blobs processing backend
+SENTRY_EVENT_PROCESSING_STORE = "sentry.eventstore.processing.default.DefaultEventProcessingStore"
+SENTRY_EVENT_PROCESSING_STORE_OPTIONS = {}
+
 # The internal Django cache is still used in many places
 # TODO(dcramer): convert uses over to Sentry's backend
 CACHES = {"default": {"BACKEND": "django.core.cache.backends.dummy.DummyCache"}}
diff --git a/src/sentry/eventstore/processing/__init__.py b/src/sentry/eventstore/processing/__init__.py
new file mode 100644
index 0000000000..dc58f8a1ab
--- /dev/null
+++ b/src/sentry/eventstore/processing/__init__.py
@@ -0,0 +1,12 @@
+from __future__ import absolute_import
+
+from sentry.utils.imports import import_string
+from django.conf import settings
+
+
+event_processing_store = import_string(settings.SENTRY_EVENT_PROCESSING_STORE)(
+    **settings.SENTRY_EVENT_PROCESSING_STORE_OPTIONS
+)
+
+
+__all__ = ["event_processing_store"]
diff --git a/src/sentry/eventstore/processing/base.py b/src/sentry/eventstore/processing/base.py
new file mode 100644
index 0000000000..24a99b3d4a
--- /dev/null
+++ b/src/sentry/eventstore/processing/base.py
@@ -0,0 +1,39 @@
+from __future__ import absolute_import
+from sentry.utils.cache import cache_key_for_event
+
+DEFAULT_TIMEOUT = 3600
+
+
+class BaseEventProcessingStore(object):
+    """
+    Store for event blobs during processing
+
+    Ingest processing pipeline tasks are passing event payload through this
+    backend instead of the message broker. Tasks are submitted with a key so
+    the payload can be retrieved and in case of change saved back to the
+    processing store.
+
+    Separating processing store from the cache allows use of different
+    implementations.
+    """
+
+    def __init__(self, inner, timeout=DEFAULT_TIMEOUT):
+        self.inner = inner
+        self.timeout = timeout
+
+    def _key_for_event(self, event):
+        return cache_key_for_event(event)
+
+    def store(self, event):
+        key = self._key_for_event(event)
+        self.inner.set(key, event, self.timeout)
+        return key
+
+    def get(self, key):
+        return self.inner.get(key)
+
+    def delete_by_key(self, key):
+        return self.inner.delete(key)
+
+    def delete(self, event):
+        return self.inner.delete(self._key_for_event(event))
diff --git a/src/sentry/eventstore/processing/default.py b/src/sentry/eventstore/processing/default.py
new file mode 100644
index 0000000000..e79d374d77
--- /dev/null
+++ b/src/sentry/eventstore/processing/default.py
@@ -0,0 +1,15 @@
+from __future__ import absolute_import
+
+from sentry.cache import default_cache
+
+from .base import BaseEventProcessingStore
+
+
+class DefaultEventProcessingStore(BaseEventProcessingStore):
+    """
+    Default implementation of processing store which uses the `default_cache`
+    as backend.
+    """
+
+    def __init__(self, **options):
+        super(DefaultEventProcessingStore, self).__init__(inner=default_cache, **options)
diff --git a/src/sentry/eventstore/processing/redis.py b/src/sentry/eventstore/processing/redis.py
new file mode 100644
index 0000000000..93381e6b34
--- /dev/null
+++ b/src/sentry/eventstore/processing/redis.py
@@ -0,0 +1,17 @@
+from __future__ import absolute_import
+
+import logging
+
+from .base import BaseEventProcessingStore
+from sentry.cache.redis import RedisClusterCache
+
+logger = logging.getLogger(__name__)
+
+
+class RedisClusterEventProcessingStore(BaseEventProcessingStore):
+    """
+    Processing store implementation using the redis cluster cache as a backend.
+    """
+
+    def __init__(self, **options):
+        super(RedisClusterEventProcessingStore, self).__init__(inner=RedisClusterCache(**options))
diff --git a/src/sentry/ingest/ingest_consumer.py b/src/sentry/ingest/ingest_consumer.py
index 82946a4c44..6329c37690 100644
--- a/src/sentry/ingest/ingest_consumer.py
+++ b/src/sentry/ingest/ingest_consumer.py
@@ -16,7 +16,7 @@ from django.core.cache import cache
 import sentry_sdk
 
 from sentry import eventstore, features, options
-from sentry.cache import default_cache
+
 from sentry.models import Project, File, EventAttachment
 from sentry.signals import event_accepted
 from sentry.tasks.store import preprocess_event
@@ -30,6 +30,7 @@ from sentry.attachments import CachedAttachment, MissingAttachmentChunks, attach
 from sentry.ingest.types import ConsumerType
 from sentry.ingest.userreport import Conflict, save_userreport
 from sentry.event_manager import save_transaction_events
+from sentry.eventstore.processing import event_processing_store
 
 logger = logging.getLogger(__name__)
 
@@ -191,8 +192,7 @@ def _do_process_event(message, projects):
     # which assumes that data passed in is a raw dictionary.
     data = json.loads(payload)
 
-    cache_key = cache_key_for_event(data)
-    default_cache.set(cache_key, data, CACHE_TIMEOUT)
+    cache_key = event_processing_store.store(data)
 
     if attachments:
         attachment_objects = [
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index e52c343fc2..655fe363ed 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -17,7 +17,6 @@ from sentry.relay.config import get_project_config
 from sentry.datascrubbing import scrub_data
 from sentry.constants import DEFAULT_STORE_NORMALIZER_ARGS
 from sentry.attachments import attachment_cache
-from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
@@ -26,6 +25,7 @@ from sentry.utils.canonical import CanonicalKeyDict, CANONICAL_TYPES
 from sentry.utils.dates import to_datetime
 from sentry.utils.sdk import set_current_project
 from sentry.models import ProjectOption, Activity, Project, Organization
+from sentry.eventstore.processing import event_processing_store
 
 error_logger = logging.getLogger("sentry.errors.events")
 info_logger = logging.getLogger("sentry.store")
@@ -110,7 +110,7 @@ def _do_preprocess_event(cache_key, data, start_time, event_id, process_task, pr
     from sentry.lang.native.processing import should_process_with_symbolicator
 
     if cache_key and data is None:
-        data = default_cache.get(cache_key)
+        data = event_processing_store.get(cache_key)
 
     if data is None:
         metrics.incr("events.failed", tags={"reason": "cache", "stage": "pre"}, skip_internal=False)
@@ -192,7 +192,7 @@ def _do_symbolicate_event(cache_key, start_time, event_id, symbolicate_task, dat
     from sentry.lang.native.processing import get_symbolication_function
 
     if data is None:
-        data = default_cache.get(cache_key)
+        data = event_processing_store.get(cache_key)
 
     if data is None:
         metrics.incr(
@@ -269,7 +269,7 @@ def _do_symbolicate_event(cache_key, start_time, event_id, symbolicate_task, dat
         data = dict(data.items())
 
     if has_changed:
-        default_cache.set(cache_key, data, 3600)
+        cache_key = event_processing_store.store(data)
 
     process_task = process_event_from_reprocessing if from_reprocessing else process_event
     _do_process_event(
@@ -396,7 +396,7 @@ def _do_process_event(
     from sentry.plugins.base import plugins
 
     if data is None:
-        data = default_cache.get(cache_key)
+        data = event_processing_store.get(cache_key)
 
     if data is None:
         metrics.incr(
@@ -536,7 +536,7 @@ def _do_process_event(
             _do_preprocess_event(cache_key, data, start_time, event_id, process_task, project)
             return
 
-        default_cache.set(cache_key, data, 3600)
+        cache_key = event_processing_store.store(data)
 
     submit_save_event(project, cache_key, event_id, start_time, data)
 
@@ -678,7 +678,7 @@ def create_failed_event(
     # from the last processing step because we do not want any
     # modifications to take place.
     delete_raw_event(project_id, event_id)
-    data = default_cache.get(cache_key)
+    data = event_processing_store.get(cache_key)
     if data is None:
         metrics.incr("events.failed", tags={"reason": "cache", "stage": "raw"}, skip_internal=False)
         error_logger.error("process.failed_raw.empty", extra={"cache_key": cache_key})
@@ -703,7 +703,7 @@ def create_failed_event(
             data=issue["data"],
         )
 
-    default_cache.delete(cache_key)
+    event_processing_store.delete_by_key(cache_key)
 
     return True
 
@@ -723,7 +723,7 @@ def _do_save_event(
 
     if cache_key and data is None:
         with metrics.timer("tasks.store.do_save_event.get_cache") as metric_tags:
-            data = default_cache.get(cache_key)
+            data = event_processing_store.get(cache_key)
             if data is not None:
                 metric_tags["event_type"] = event_type = data.get("type") or "none"
 
@@ -778,7 +778,7 @@ def _do_save_event(
         finally:
             if cache_key:
                 with metrics.timer("tasks.store.do_save_event.delete_cache"):
-                    default_cache.delete(cache_key)
+                    event_processing_store.delete_by_key(cache_key)
 
                 with metrics.timer("tasks.store.do_save_event.delete_attachment_cache"):
                     # For the unlikely case that we did not manage to persist the
diff --git a/tests/sentry/tasks/test_store.py b/tests/sentry/tasks/test_store.py
index 96d4aff122..ec3fd869e8 100644
--- a/tests/sentry/tasks/test_store.py
+++ b/tests/sentry/tasks/test_store.py
@@ -76,8 +76,8 @@ def mock_get_symbolication_function():
 
 
 @pytest.fixture
-def mock_default_cache():
-    with mock.patch("sentry.tasks.store.default_cache") as m:
+def mock_event_processing_store():
+    with mock.patch("sentry.tasks.store.event_processing_store") as m:
         yield m
 
 
@@ -130,7 +130,7 @@ def test_move_to_symbolicate_event(
 @pytest.mark.django_db
 def test_symbolicate_event_call_process_inline(
     default_project,
-    mock_default_cache,
+    mock_event_processing_store,
     mock_process_event,
     mock_save_event,
     mock_get_symbolication_function,
@@ -143,7 +143,8 @@ def test_symbolicate_event_call_process_inline(
         "event_id": EVENT_ID,
         "extra": {"foo": "bar"},
     }
-    mock_default_cache.get.return_value = data
+    mock_event_processing_store.get.return_value = data
+    mock_event_processing_store.store.return_value = "e:1"
 
     symbolicated_data = {"type": "error"}
 
@@ -153,11 +154,9 @@ def test_symbolicate_event_call_process_inline(
         symbolicate_event(cache_key="e:1", start_time=1)
 
     # The event mutated, so make sure we save it back
-    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
+    ((_, (event,), _),) = mock_event_processing_store.store.mock_calls
 
-    assert key == "e:1"
     assert event == symbolicated_data
-    assert duration == 3600
 
     assert mock_save_event.delay.call_count == 0
     assert mock_process_event.delay.call_count == 0
@@ -194,7 +193,7 @@ def test_move_to_save_event(
 
 @pytest.mark.django_db
 def test_process_event_mutate_and_save(
-    default_project, mock_default_cache, mock_save_event, register_plugin
+    default_project, mock_event_processing_store, mock_save_event, register_plugin
 ):
     register_plugin(BasicPreprocessorPlugin)
 
@@ -206,16 +205,15 @@ def test_process_event_mutate_and_save(
         "extra": {"foo": "bar"},
     }
 
-    mock_default_cache.get.return_value = data
+    mock_event_processing_store.get.return_value = data
+    mock_event_processing_store.store.return_value = "e:1"
 
     process_event(cache_key="e:1", start_time=1)
 
     # The event mutated, so make sure we save it back
-    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
+    ((_, (event,), _),) = mock_event_processing_store.store.mock_calls
 
-    assert key == "e:1"
     assert "extra" not in event
-    assert duration == 3600
 
     mock_save_event.delay.assert_called_once_with(
         cache_key="e:1", data=None, start_time=1, event_id=EVENT_ID, project_id=default_project.id
@@ -224,7 +222,7 @@ def test_process_event_mutate_and_save(
 
 @pytest.mark.django_db
 def test_process_event_no_mutate_and_save(
-    default_project, mock_default_cache, mock_save_event, register_plugin
+    default_project, mock_event_processing_store, mock_save_event, register_plugin
 ):
     register_plugin(BasicPreprocessorPlugin)
 
@@ -236,12 +234,12 @@ def test_process_event_no_mutate_and_save(
         "extra": {"foo": "bar"},
     }
 
-    mock_default_cache.get.return_value = data
+    mock_event_processing_store.get.return_value = data
 
     process_event(cache_key="e:1", start_time=1)
 
     # The event did not mutate, so we shouldn't reset it in cache
-    assert mock_default_cache.set.call_count == 0
+    assert mock_event_processing_store.store.call_count == 0
 
     mock_save_event.delay.assert_called_once_with(
         cache_key="e:1", data=None, start_time=1, event_id=EVENT_ID, project_id=default_project.id
@@ -250,7 +248,7 @@ def test_process_event_no_mutate_and_save(
 
 @pytest.mark.django_db
 def test_process_event_unprocessed(
-    default_project, mock_default_cache, mock_save_event, register_plugin
+    default_project, mock_event_processing_store, mock_save_event, register_plugin
 ):
     register_plugin(BasicPreprocessorPlugin)
 
@@ -262,14 +260,13 @@ def test_process_event_unprocessed(
         "extra": {"foo": "bar"},
     }
 
-    mock_default_cache.get.return_value = data
+    mock_event_processing_store.get.return_value = data
+    mock_event_processing_store.store.return_value = "e:1"
 
     process_event(cache_key="e:1", start_time=1)
 
-    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
-    assert key == "e:1"
+    ((_, (event,), _),) = mock_event_processing_store.store.mock_calls
     assert event["unprocessed"] is True
-    assert duration == 3600
 
     mock_save_event.delay.assert_called_once_with(
         cache_key="e:1", data=None, start_time=1, event_id=EVENT_ID, project_id=default_project.id
@@ -313,7 +310,7 @@ def test_scrubbing_after_processing(
     default_organization,
     mock_save_event,
     register_plugin,
-    mock_default_cache,
+    mock_event_processing_store,
     setting_method,
     options_model,
 ):
@@ -348,17 +345,16 @@ def test_scrubbing_after_processing(
         "extra": {"aaa": "remove me"},
     }
 
-    mock_default_cache.get.return_value = data
+    mock_event_processing_store.get.return_value = data
+    mock_event_processing_store.store.return_value = "e:1"
 
     with Feature({"organizations:datascrubbers-v2": True}):
         # We pass data_has_changed=True to pretend that we've added "extra" attribute
         # to "data" shortly before (e.g. during symbolication).
         process_event(cache_key="e:1", start_time=1, data_has_changed=True)
 
-    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
-    assert key == "e:1"
+    ((_, (event,), _),) = mock_event_processing_store.store.mock_calls
     assert event["extra"] == {u"aaa": u"[Filtered]", u"aaa2": u"event preprocessor"}
-    assert duration == 3600
 
     mock_save_event.delay.assert_called_once_with(
         cache_key="e:1", data=None, start_time=1, event_id=EVENT_ID, project_id=default_project.id
