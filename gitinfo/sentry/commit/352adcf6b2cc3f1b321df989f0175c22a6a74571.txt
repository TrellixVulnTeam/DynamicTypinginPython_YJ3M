commit 352adcf6b2cc3f1b321df989f0175c22a6a74571
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Tue Jan 31 13:07:19 2017 +0100

    Added new reprocessing models

diff --git a/src/sentry/models/processingissue.py b/src/sentry/models/processingissue.py
new file mode 100644
index 0000000000..6c0abe9644
--- /dev/null
+++ b/src/sentry/models/processingissue.py
@@ -0,0 +1,91 @@
+"""
+sentry.models.processingissue
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2017 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+from __future__ import absolute_import
+
+from hashlib import sha1
+from django.db import models
+
+from sentry.db.models import (
+    BaseManager, Model, FlexibleForeignKey, GzippedDictField, sane_repr
+)
+
+
+def get_processing_issue_checksum(scope, object, type):
+    h = sha1()
+    h.update(type.encode('utf-8') + '\x00')
+    h.update(scope.encode('utf-8') + '\x00')
+    h.update(scope.encode('utf-8') + '\x00')
+    return h.hexdigest()
+
+
+class ProcessingIssueManager(BaseManager):
+
+    def resolve_processing_issue(self, project, scope, object, type):
+        """Given scope, object and type this marks all issues as resolved
+        and returns a list of events that now require reprocessing.
+        """
+
+    def record_processing_issue(self, project, raw_event, scope, object,
+                                type, data=None):
+        data = dict(data or {})
+        checksum = get_processing_issue_checksum(scope, object, type)
+        data['_scope'] = scope
+        data['_object'] = object
+        data['_type'] = type
+        issue = ProcessingIssue.objects.get_or_create(
+            project=project,
+            checksum=checksum,
+            data=data,
+        )
+        EventProcessingIssue.objects.get_or_create(
+            raw_event=raw_event,
+            issue=issue,
+        )
+
+
+class ProcessingIssue(Model):
+    __core__ = False
+
+    project = FlexibleForeignKey('sentry.Project')
+    checksum = models.CharField(max_length=40)
+    data = GzippedDictField()
+
+    objects = BaseManager()
+
+    class Meta:
+        app_label = 'sentry'
+        db_table = 'sentry_processingissue'
+        unique_together = (('project', 'checksum'),)
+
+    __repr__ = sane_repr('project_id')
+
+    @property
+    def scope(self):
+        return self.data['_scope']
+
+    @property
+    def object(self):
+        return self.data['_object']
+
+    @property
+    def type(self):
+        return self.data['_type']
+
+
+class EventProcessingIssue(Model):
+    __core__ = False
+
+    raw_event = FlexibleForeignKey('sentry.RawEvent')
+    processing_issue = FlexibleForeignKey('sentry.ProcessingIssue')
+
+    class Meta:
+        app_label = 'sentry'
+        db_table = 'sentry_eventprocessingissue'
+        unique_together = (('raw_event', 'processing_issue'),)
+
+    __repr__ = sane_repr('raw_event', 'processing_issue')
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 0ee4955281..8e73a8d829 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -116,7 +116,7 @@ def process_event(cache_key, start_time=None, **kwargs):
     if has_changed:
         issues = data.get('processing_issues')
         if issues:
-            create_failed_event(cache_key, project, issues)
+            create_failed_event(cache_key, project, list(issues.values()))
             return
 
         default_cache.set(cache_key, data, 3600)
@@ -137,8 +137,8 @@ def create_failed_event(cache_key, project, issues):
         error_logger.error('process.failed_raw.empty', extra={'cache_key': cache_key})
         return
 
-    from sentry.models import RawEvent
-    RawEvent.objects.create(
+    from sentry.models import RawEvent, ProcessingIssue
+    raw_event = RawEvent.objects.create(
         project_id=project,
         event_id=data['event_id'],
         datetime=datetime.utcfromtimestamp(
@@ -146,7 +146,15 @@ def create_failed_event(cache_key, project, issues):
         data=data
     )
 
-    # TODO: store associated issues
+    for issue in issues:
+        ProcessingIssue.objects.record_processing_issue(
+            project=project,
+            raw_event=raw_event,
+            scope=issue['scope'],
+            object=issue['object'],
+            type=issue['type'],
+            data=issue['data'],
+        )
 
     default_cache.delete(cache_key)
 
