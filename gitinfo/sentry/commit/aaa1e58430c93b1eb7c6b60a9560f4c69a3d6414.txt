commit aaa1e58430c93b1eb7c6b60a9560f4c69a3d6414
Author: evanh <evanh@users.noreply.github.com>
Date:   Fri May 1 14:15:32 2020 -0400

    feat(discover) add tracing spans to the discover backend code (#18554)
    
    Add some more spans to the backend code to breakup the traces into digestible
    chunks.

diff --git a/src/sentry/api/bases/organization_events.py b/src/sentry/api/bases/organization_events.py
index ab14779f61..193e49bef8 100644
--- a/src/sentry/api/bases/organization_events.py
+++ b/src/sentry/api/bases/organization_events.py
@@ -1,5 +1,6 @@
 from __future__ import absolute_import
 
+import sentry_sdk
 import six
 from rest_framework.exceptions import PermissionDenied
 from rest_framework.exceptions import ParseError
@@ -88,19 +89,20 @@ class OrganizationEventsEndpointBase(OrganizationEndpoint):
 
 class OrganizationEventsV2EndpointBase(OrganizationEventsEndpointBase):
     def handle_results_with_meta(self, request, organization, project_ids, results):
-        data = self.handle_data(request, organization, project_ids, results.get("data"))
-        if not data:
-            return {"data": [], "meta": {}}
-
-        meta = {
-            value["name"]: get_json_meta_type(value["name"], value["type"])
-            for value in results["meta"]
-        }
-        # Ensure all columns in the result have types.
-        for key in data[0]:
-            if key not in meta:
-                meta[key] = "string"
-        return {"meta": meta, "data": data}
+        with sentry_sdk.start_span(op="discover.endpoint", description="base.handle_results"):
+            data = self.handle_data(request, organization, project_ids, results.get("data"))
+            if not data:
+                return {"data": [], "meta": {}}
+
+            meta = {
+                value["name"]: get_json_meta_type(value["name"], value["type"])
+                for value in results["meta"]
+            }
+            # Ensure all columns in the result have types.
+            for key in data[0]:
+                if key not in meta:
+                    meta[key] = "string"
+            return {"meta": meta, "data": data}
 
     def handle_data(self, request, organization, project_ids, results):
         if not results:
@@ -135,55 +137,60 @@ class OrganizationEventsV2EndpointBase(OrganizationEventsEndpointBase):
 
     def get_event_stats_data(self, request, organization, get_event_stats, top_events=False):
         try:
-            columns = request.GET.getlist("yAxis", ["count()"])
-            query = request.GET.get("query")
-            params = self.get_filter_params(request, organization)
-            rollup = get_rollup_from_request(
-                request,
-                params,
-                "1h",
-                InvalidSearchQuery(
-                    "Your interval and date range would create too many results. "
-                    "Use a larger interval, or a smaller date range."
-                ),
-            )
-            # Backwards compatibility for incidents which uses the old
-            # column aliases as it straddles both versions of events/discover.
-            # We will need these aliases until discover2 flags are enabled for all
-            # users.
-            column_map = {
-                "user_count": "count_unique(user)",
-                "event_count": "count()",
-                "rpm()": "rpm(%d)" % rollup,
-                "rps()": "rps(%d)" % rollup,
-            }
-            query_columns = [column_map.get(column, column) for column in columns]
-            reference_event = self.reference_event(
-                request, organization, params.get("start"), params.get("end")
-            )
-
-            result = get_event_stats(query_columns, query, params, rollup, reference_event)
+            with sentry_sdk.start_span(
+                op="discover.endpoint", description="base.stats_query_creation"
+            ):
+                columns = request.GET.getlist("yAxis", ["count()"])
+                query = request.GET.get("query")
+                params = self.get_filter_params(request, organization)
+                rollup = get_rollup_from_request(
+                    request,
+                    params,
+                    "1h",
+                    InvalidSearchQuery(
+                        "Your interval and date range would create too many results. "
+                        "Use a larger interval, or a smaller date range."
+                    ),
+                )
+                # Backwards compatibility for incidents which uses the old
+                # column aliases as it straddles both versions of events/discover.
+                # We will need these aliases until discover2 flags are enabled for all
+                # users.
+                column_map = {
+                    "user_count": "count_unique(user)",
+                    "event_count": "count()",
+                    "rpm()": "rpm(%d)" % rollup,
+                    "rps()": "rps(%d)" % rollup,
+                }
+                query_columns = [column_map.get(column, column) for column in columns]
+                reference_event = self.reference_event(
+                    request, organization, params.get("start"), params.get("end")
+                )
+
+            with sentry_sdk.start_span(op="discover.endpoint", description="base.stats_query"):
+                result = get_event_stats(query_columns, query, params, rollup, reference_event)
         except (discover.InvalidSearchQuery, snuba.QueryOutsideRetentionError) as error:
             raise ParseError(detail=six.text_type(error))
         serializer = SnubaTSResultSerializer(organization, None, request.user)
 
-        if top_events:
-            results = {}
-            for key, event_result in six.iteritems(result):
-                if len(query_columns) > 1:
-                    results[key] = self.serialize_multiple_axis(
-                        serializer, event_result, columns, query_columns
-                    )
-                else:
-                    # Need to get function alias if count is a field, but not the axis
-                    results[key] = serializer.serialize(
-                        event_result, get_function_alias(query_columns[0])
-                    )
-            return results
-        elif len(query_columns) > 1:
-            return self.serialize_multiple_axis(serializer, result, columns, query_columns)
-        else:
-            return serializer.serialize(result)
+        with sentry_sdk.start_span(op="discover.endpoint", description="base.stats_serialization"):
+            if top_events:
+                results = {}
+                for key, event_result in six.iteritems(result):
+                    if len(query_columns) > 1:
+                        results[key] = self.serialize_multiple_axis(
+                            serializer, event_result, columns, query_columns
+                        )
+                    else:
+                        # Need to get function alias if count is a field, but not the axis
+                        results[key] = serializer.serialize(
+                            event_result, get_function_alias(query_columns[0])
+                        )
+                return results
+            elif len(query_columns) > 1:
+                return self.serialize_multiple_axis(serializer, result, columns, query_columns)
+            else:
+                return serializer.serialize(result)
 
     def serialize_multiple_axis(self, serializer, event_result, columns, query_columns):
         # Return with requested yAxis as the key
diff --git a/src/sentry/api/endpoints/organization_events.py b/src/sentry/api/endpoints/organization_events.py
index 2d42a2d0f1..a8fec188a7 100644
--- a/src/sentry/api/endpoints/organization_events.py
+++ b/src/sentry/api/endpoints/organization_events.py
@@ -119,18 +119,20 @@ class OrganizationEventsV2Endpoint(OrganizationEventsV2EndpointBase):
         if not features.has("organizations:discover-basic", organization, actor=request.user):
             return Response(status=404)
 
-        try:
-            params = self.get_filter_params(request, organization)
-        except NoProjects:
-            return Response([])
+        with sentry_sdk.start_span(op="discover.endpoint", description="filter_params") as span:
+            span.set_tag("organization", organization)
+            try:
+                params = self.get_filter_params(request, organization)
+            except NoProjects:
+                return Response([])
 
-        params["organization_id"] = organization.id
+            params["organization_id"] = organization.id
 
-        has_global_views = features.has(
-            "organizations:global-views", organization, actor=request.user
-        )
-        if not has_global_views and len(params.get("project_id", [])) > 1:
-            raise ParseError(detail="You cannot view events from multiple projects.")
+            has_global_views = features.has(
+                "organizations:global-views", organization, actor=request.user
+            )
+            if not has_global_views and len(params.get("project_id", [])) > 1:
+                raise ParseError(detail="You cannot view events from multiple projects.")
 
         def data_fn(offset, limit):
             return discover.query(
diff --git a/src/sentry/api/endpoints/organization_events_facets.py b/src/sentry/api/endpoints/organization_events_facets.py
index a5d78c020b..0865031066 100644
--- a/src/sentry/api/endpoints/organization_events_facets.py
+++ b/src/sentry/api/endpoints/organization_events_facets.py
@@ -1,8 +1,9 @@
 from __future__ import absolute_import
 
-from collections import defaultdict
+import sentry_sdk
 import six
 
+from collections import defaultdict
 from rest_framework.response import Response
 from rest_framework.exceptions import ParseError
 
@@ -14,46 +15,51 @@ from sentry import features, tagstore
 
 class OrganizationEventsFacetsEndpoint(OrganizationEventsEndpointBase):
     def get(self, request, organization):
-        if not features.has("organizations:discover-basic", organization, actor=request.user):
-            return Response(status=404)
-        try:
-            params = self.get_filter_params(request, organization)
-        except NoProjects:
-            raise ParseError(detail="A valid project must be included.")
-        self._validate_project_ids(request, organization, params)
+        with sentry_sdk.start_span(op="discover.endpoint", description="filter_params") as span:
+            span.set_data("organization", organization)
+            if not features.has("organizations:discover-basic", organization, actor=request.user):
+                return Response(status=404)
+            try:
+                params = self.get_filter_params(request, organization)
+            except NoProjects:
+                raise ParseError(detail="A valid project must be included.")
+            self._validate_project_ids(request, organization, params)
 
-        try:
-            facets = discover.get_facets(
-                query=request.GET.get("query"),
-                params=params,
-                referrer="api.organization-events-facets.top-tags",
-            )
-        except (discover.InvalidSearchQuery, snuba.QueryOutsideRetentionError) as error:
-            raise ParseError(detail=six.text_type(error))
+        with sentry_sdk.start_span(op="discover.endpoint", description="discover_query"):
+            try:
+                facets = discover.get_facets(
+                    query=request.GET.get("query"),
+                    params=params,
+                    referrer="api.organization-events-facets.top-tags",
+                )
+            except (discover.InvalidSearchQuery, snuba.QueryOutsideRetentionError) as error:
+                raise ParseError(detail=six.text_type(error))
 
-        resp = defaultdict(lambda: {"key": "", "topValues": []})
-        for row in facets:
-            values = resp[row.key]
-            values["key"] = tagstore.get_standardized_key(row.key)
-            values["topValues"].append(
-                {
-                    "name": tagstore.get_tag_value_label(row.key, row.value),
-                    "value": row.value,
-                    "count": row.count,
-                }
-            )
-        if "project" in resp:
-            # Replace project ids with slugs as that is what we generally expose to users
-            # and filter out projects that the user doesn't have access too.
-            projects = {p.id: p.slug for p in self.get_projects(request, organization)}
-            filtered_values = []
-            for v in resp["project"]["topValues"]:
-                if v["value"] in projects:
-                    name = projects[v["value"]]
-                    v.update({"name": name})
-                    filtered_values.append(v)
+        with sentry_sdk.start_span(op="discover.endpoint", description="populate_results") as span:
+            span.set_data("facet_count", len(facets or []))
+            resp = defaultdict(lambda: {"key": "", "topValues": []})
+            for row in facets:
+                values = resp[row.key]
+                values["key"] = tagstore.get_standardized_key(row.key)
+                values["topValues"].append(
+                    {
+                        "name": tagstore.get_tag_value_label(row.key, row.value),
+                        "value": row.value,
+                        "count": row.count,
+                    }
+                )
+            if "project" in resp:
+                # Replace project ids with slugs as that is what we generally expose to users
+                # and filter out projects that the user doesn't have access too.
+                projects = {p.id: p.slug for p in self.get_projects(request, organization)}
+                filtered_values = []
+                for v in resp["project"]["topValues"]:
+                    if v["value"] in projects:
+                        name = projects[v["value"]]
+                        v.update({"name": name})
+                        filtered_values.append(v)
 
-            resp["project"]["topValues"] = filtered_values
+                resp["project"]["topValues"] = filtered_values
 
         return Response(resp.values())
 
diff --git a/src/sentry/api/endpoints/organization_events_meta.py b/src/sentry/api/endpoints/organization_events_meta.py
index ecee1c304a..1263d82497 100644
--- a/src/sentry/api/endpoints/organization_events_meta.py
+++ b/src/sentry/api/endpoints/organization_events_meta.py
@@ -1,6 +1,7 @@
 from __future__ import absolute_import
 
 import re
+import sentry_sdk
 import six
 
 from rest_framework.response import Response
@@ -19,10 +20,12 @@ from sentry.utils import snuba
 
 class OrganizationEventsMetaEndpoint(OrganizationEventsEndpointBase):
     def get(self, request, organization):
-        try:
-            params = self.get_filter_params(request, organization)
-        except NoProjects:
-            return Response({"count": 0})
+        with sentry_sdk.start_span(op="discover.endpoint", description="filter_params") as span:
+            span.set_data("organization", organization)
+            try:
+                params = self.get_filter_params(request, organization)
+            except NoProjects:
+                return Response({"count": 0})
 
         try:
             result = discover.query(
@@ -42,50 +45,59 @@ UNESCAPED_QUOTE_RE = re.compile('(?<!\\\\)"')
 
 class OrganizationEventsRelatedIssuesEndpoint(OrganizationEventsEndpointBase, EnvironmentMixin):
     def get(self, request, organization):
-        try:
-            params = self.get_filter_params(request, organization)
-        except NoProjects:
-            return Response([])
-
-        possible_keys = ["transaction"]
-        lookup_keys = {key: request.query_params.get(key) for key in possible_keys}
-
-        if not any(lookup_keys.values()):
-            return Response(
-                {
-                    "detail": "Must provide one of {} in order to find related events".format(
-                        possible_keys
-                    )
-                },
-                status=400,
-            )
+        with sentry_sdk.start_span(op="discover.endpoint", description="filter_params") as span:
+            span.set_data("organization", organization)
+            try:
+                params = self.get_filter_params(request, organization)
+            except NoProjects:
+                return Response([])
+
+            possible_keys = ["transaction"]
+            lookup_keys = {key: request.query_params.get(key) for key in possible_keys}
+
+            if not any(lookup_keys.values()):
+                return Response(
+                    {
+                        "detail": "Must provide one of {} in order to find related events".format(
+                            possible_keys
+                        )
+                    },
+                    status=400,
+                )
 
         try:
-            projects = self.get_projects(request, organization)
-            query_kwargs = build_query_params_from_request(
-                request, organization, projects, params.get("environment")
-            )
-            query_kwargs["limit"] = 5
-            try:
-                # Need to escape quotes in case some "joker" has a transaction with quotes
-                transaction_name = UNESCAPED_QUOTE_RE.sub('\\"', lookup_keys["transaction"])
-                parsed_terms = parse_search_query('transaction:"{}"'.format(transaction_name))
-            except ParseError:
-                return Response({"detail": "Invalid transaction search"}, status=400)
-
-            if query_kwargs.get("search_filters"):
-                query_kwargs["search_filters"].extend(parsed_terms)
-            else:
-                query_kwargs["search_filters"] = parsed_terms
-
-            results = search.query(**query_kwargs)
+            with sentry_sdk.start_span(op="discover.endpoint", description="filter_creation"):
+                projects = self.get_projects(request, organization)
+                query_kwargs = build_query_params_from_request(
+                    request, organization, projects, params.get("environment")
+                )
+                query_kwargs["limit"] = 5
+                try:
+                    # Need to escape quotes in case some "joker" has a transaction with quotes
+                    transaction_name = UNESCAPED_QUOTE_RE.sub('\\"', lookup_keys["transaction"])
+                    parsed_terms = parse_search_query('transaction:"{}"'.format(transaction_name))
+                except ParseError:
+                    return Response({"detail": "Invalid transaction search"}, status=400)
+
+                if query_kwargs.get("search_filters"):
+                    query_kwargs["search_filters"].extend(parsed_terms)
+                else:
+                    query_kwargs["search_filters"] = parsed_terms
+
+            with sentry_sdk.start_span(op="discover.endpoint", description="issue_search"):
+                results = search.query(**query_kwargs)
         except discover.InvalidSearchQuery as err:
             raise ParseError(detail=six.text_type(err))
 
-        context = serialize(
-            list(results),
-            request.user,
-            GroupSerializer(environment_func=self._get_environment_func(request, organization.id)),
-        )
+        with sentry_sdk.start_span(op="discover.endpoint", description="serialize_results") as span:
+            results = list(results)
+            span.set_data("result_length", len(results))
+            context = serialize(
+                results,
+                request.user,
+                GroupSerializer(
+                    environment_func=self._get_environment_func(request, organization.id)
+                ),
+            )
 
         return Response(context)
diff --git a/src/sentry/api/endpoints/organization_events_stats.py b/src/sentry/api/endpoints/organization_events_stats.py
index 8d75527c62..d8a40ca8b7 100644
--- a/src/sentry/api/endpoints/organization_events_stats.py
+++ b/src/sentry/api/endpoints/organization_events_stats.py
@@ -1,5 +1,6 @@
 from __future__ import absolute_import
 
+import sentry_sdk
 import six
 
 from rest_framework.response import Response
@@ -18,24 +19,27 @@ from sentry.utils.dates import get_rollup_from_request
 
 class OrganizationEventsStatsEndpoint(OrganizationEventsV2EndpointBase):
     def get(self, request, organization):
-        if not features.has("organizations:discover-basic", organization, actor=request.user):
-            return self.get_v1_results(request, organization)
+        with sentry_sdk.start_span(op="discover.endpoint", description="filter_params") as span:
+            span.set_data("organization", organization)
+            if not features.has("organizations:discover-basic", organization, actor=request.user):
+                span.set_data("using_v1_results", True)
+                return self.get_v1_results(request, organization)
 
-        top_events = "topEvents" in request.GET
-        limit = None
+            top_events = "topEvents" in request.GET
+            limit = None
 
-        if top_events:
-            try:
-                limit = int(request.GET.get("topEvents", 0))
-            except ValueError:
-                return Response({"detail": "topEvents must be an integer"}, status=400)
-            if limit > MAX_TOP_EVENTS:
-                return Response(
-                    {"detail": "Can only get up to {} top events".format(MAX_TOP_EVENTS)},
-                    status=400,
-                )
-            elif limit <= 0:
-                return Response({"detail": "If topEvents needs to be at least 1"}, status=400)
+            if top_events:
+                try:
+                    limit = int(request.GET.get("topEvents", 0))
+                except ValueError:
+                    return Response({"detail": "topEvents must be an integer"}, status=400)
+                if limit > MAX_TOP_EVENTS:
+                    return Response(
+                        {"detail": "Can only get up to {} top events".format(MAX_TOP_EVENTS)},
+                        status=400,
+                    )
+                elif limit <= 0:
+                    return Response({"detail": "If topEvents needs to be at least 1"}, status=400)
 
         def get_event_stats(query_columns, query, params, rollup, reference_event):
             if top_events:
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index f4f36693c5..7e85ab8aac 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -1,6 +1,7 @@
 from __future__ import absolute_import
 
 import math
+import sentry_sdk
 import six
 import logging
 
@@ -450,22 +451,30 @@ def transform_results(result, translated_columns, snuba_filter, selected_columns
 
     rollup = snuba_filter.rollup
     if rollup and rollup > 0:
-        result["data"] = zerofill(
-            result["data"], snuba_filter.start, snuba_filter.end, rollup, snuba_filter.orderby
-        )
+        with sentry_sdk.start_span(
+            op="discover.discover", description="transform_results.zerofill"
+        ) as span:
+            span.set_data("result_count", len(result.get("data", [])))
+            result["data"] = zerofill(
+                result["data"], snuba_filter.start, snuba_filter.end, rollup, snuba_filter.orderby
+            )
 
     for col in result["meta"]:
         if col["name"].startswith("histogram"):
             # The column name here has been translated, we need the original name
             for snuba_name, sentry_name in six.iteritems(translated_columns):
                 if sentry_name == col["name"]:
-                    result["data"] = zerofill_histogram(
-                        result["data"],
-                        result["meta"],
-                        snuba_filter.orderby,
-                        sentry_name,
-                        snuba_name,
-                    )
+                    with sentry_sdk.start_span(
+                        op="discover.discover", description="transform_results.histogram_zerofill",
+                    ) as span:
+                        span.set_data("histogram_function", snuba_name)
+                        result["data"] = zerofill_histogram(
+                            result["data"],
+                            result["meta"],
+                            snuba_filter.orderby,
+                            sentry_name,
+                            snuba_name,
+                        )
             break
 
     return result
@@ -556,16 +565,20 @@ def query(
     if not selected_columns:
         raise InvalidSearchQuery("No columns selected")
 
-    # TODO(evanh): These can be removed once we migrate the frontend / saved queries
-    # to use the new function values
-    selected_columns, function_translations = transform_deprecated_functions_in_columns(
-        selected_columns
-    )
-    query = transform_deprecated_functions_in_query(query)
+    with sentry_sdk.start_span(
+        op="discover.discover", description="query.filter_transform"
+    ) as span:
+        span.set_data("query", query)
+        # TODO(evanh): These can be removed once we migrate the frontend / saved queries
+        # to use the new function values
+        selected_columns, function_translations = transform_deprecated_functions_in_columns(
+            selected_columns
+        )
+        query = transform_deprecated_functions_in_query(query)
 
-    snuba_filter = get_filter(query, params)
-    if not use_aggregate_conditions:
-        snuba_filter.having = []
+        snuba_filter = get_filter(query, params)
+        if not use_aggregate_conditions:
+            snuba_filter.having = []
 
     # We need to run a separate query to be able to properly bucket the values for the histogram
     # Do that here, and format the bucket number in to the columns before passing it through
@@ -573,72 +586,89 @@ def query(
     idx = 0
     for col in selected_columns:
         if col.startswith("histogram("):
-            histogram_column = find_histogram_buckets(col, params, snuba_filter.conditions)
-            selected_columns[idx] = histogram_column
-            function_translations[get_function_alias(histogram_column)] = get_function_alias(col)
+            with sentry_sdk.start_span(
+                op="discover.discover", description="query.histogram_calculation"
+            ) as span:
+                span.set_data("histogram", col)
+                histogram_column = find_histogram_buckets(col, params, snuba_filter.conditions)
+                selected_columns[idx] = histogram_column
+                function_translations[get_function_alias(histogram_column)] = get_function_alias(
+                    col
+                )
+
             break
 
         idx += 1
 
-    # Check to see if we are ordering by any functions and convert the orderby to be the correct alias.
-    if orderby:
-        orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
-        new_orderby = []
-        for ordering in orderby:
-            is_reversed = ordering.startswith("-")
-            ordering = ordering.lstrip("-")
-            for snuba_name, sentry_name in six.iteritems(function_translations):
-                if sentry_name == ordering:
-                    ordering = snuba_name
-                    break
-
-            ordering = "{}{}".format("-" if is_reversed else "", ordering)
-            new_orderby.append(ordering)
+    with sentry_sdk.start_span(op="discover.discover", description="query.field_translations"):
+        # Check to see if we are ordering by any functions and convert the orderby to be the correct alias.
+        if orderby:
+            orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+            new_orderby = []
+            for ordering in orderby:
+                is_reversed = ordering.startswith("-")
+                ordering = ordering.lstrip("-")
+                for snuba_name, sentry_name in six.iteritems(function_translations):
+                    if sentry_name == ordering:
+                        ordering = snuba_name
+                        break
+
+                ordering = "{}{}".format("-" if is_reversed else "", ordering)
+                new_orderby.append(ordering)
+
+            snuba_filter.orderby = new_orderby
+
+        snuba_filter.update_with(
+            resolve_field_list(selected_columns, snuba_filter, auto_fields=auto_fields)
+        )
 
-        snuba_filter.orderby = new_orderby
+        if reference_event:
+            ref_conditions = create_reference_event_conditions(reference_event)
+            if ref_conditions:
+                snuba_filter.conditions.extend(ref_conditions)
 
-    snuba_filter.update_with(
-        resolve_field_list(selected_columns, snuba_filter, auto_fields=auto_fields)
-    )
-
-    if reference_event:
-        ref_conditions = create_reference_event_conditions(reference_event)
-        if ref_conditions:
-            snuba_filter.conditions.extend(ref_conditions)
+        # Resolve the public aliases into the discover dataset names.
+        snuba_filter, translated_columns = resolve_discover_aliases(
+            snuba_filter, function_translations
+        )
 
-    # Resolve the public aliases into the discover dataset names.
-    snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter, function_translations)
-
-    # Make sure that any aggregate conditions are also in the selected columns
-    for having_clause in snuba_filter.having:
-        found = any(having_clause[0] == agg_clause[-1] for agg_clause in snuba_filter.aggregations)
-        if not found:
-            raise InvalidSearchQuery(
-                u"Aggregate {} used in a condition but is not a selected column.".format(
-                    having_clause[0]
-                )
+        # Make sure that any aggregate conditions are also in the selected columns
+        for having_clause in snuba_filter.having:
+            found = any(
+                having_clause[0] == agg_clause[-1] for agg_clause in snuba_filter.aggregations
             )
+            if not found:
+                raise InvalidSearchQuery(
+                    u"Aggregate {} used in a condition but is not a selected column.".format(
+                        having_clause[0]
+                    )
+                )
 
-    if conditions is not None:
-        snuba_filter.conditions.extend(conditions)
-
-    result = raw_query(
-        start=snuba_filter.start,
-        end=snuba_filter.end,
-        groupby=snuba_filter.groupby,
-        conditions=snuba_filter.conditions,
-        aggregations=snuba_filter.aggregations,
-        selected_columns=snuba_filter.selected_columns,
-        filter_keys=snuba_filter.filter_keys,
-        having=snuba_filter.having,
-        orderby=snuba_filter.orderby,
-        dataset=Dataset.Discover,
-        limit=limit,
-        offset=offset,
-        referrer=referrer,
-    )
+        if conditions is not None:
+            snuba_filter.conditions.extend(conditions)
 
-    return transform_results(result, translated_columns, snuba_filter, selected_columns)
+    with sentry_sdk.start_span(op="discover.discover", description="query.snuba_query"):
+        result = raw_query(
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            groupby=snuba_filter.groupby,
+            conditions=snuba_filter.conditions,
+            aggregations=snuba_filter.aggregations,
+            selected_columns=snuba_filter.selected_columns,
+            filter_keys=snuba_filter.filter_keys,
+            having=snuba_filter.having,
+            orderby=snuba_filter.orderby,
+            dataset=Dataset.Discover,
+            limit=limit,
+            offset=offset,
+            referrer=referrer,
+        )
+
+    with sentry_sdk.start_span(
+        op="discover.discover", description="query.transform_results"
+    ) as span:
+        span.set_data("result_count", len(result.get("data", [])))
+        return transform_results(result, translated_columns, snuba_filter, selected_columns)
 
 
 def key_transaction_conditions(queryset):
@@ -727,30 +757,39 @@ def key_transaction_timeseries_query(selected_columns, query, params, rollup, re
         referrer (str|None) A referrer string to help locate the origin of this query.
         queryset (QuerySet) Filtered QuerySet of KeyTransactions
     """
-    snuba_filter, _ = get_timeseries_snuba_filter(selected_columns, query, params, rollup)
+    with sentry_sdk.start_span(
+        op="discover.discover", description="kt_timeseries.filter_transform"
+    ) as span:
+        span.set_data("query", query)
+        snuba_filter, _ = get_timeseries_snuba_filter(selected_columns, query, params, rollup)
 
     if queryset.exists():
         snuba_filter.conditions.extend(key_transaction_conditions(queryset))
 
-        result = raw_query(
-            aggregations=snuba_filter.aggregations,
-            conditions=snuba_filter.conditions,
-            filter_keys=snuba_filter.filter_keys,
-            start=snuba_filter.start,
-            end=snuba_filter.end,
-            rollup=rollup,
-            orderby="time",
-            groupby=["time"],
-            dataset=Dataset.Discover,
-            limit=10000,
-            referrer=referrer,
-        )
+        with sentry_sdk.start_span(op="discover.discover", description="kt_timeseries.snuba_query"):
+            result = raw_query(
+                aggregations=snuba_filter.aggregations,
+                conditions=snuba_filter.conditions,
+                filter_keys=snuba_filter.filter_keys,
+                start=snuba_filter.start,
+                end=snuba_filter.end,
+                rollup=rollup,
+                orderby="time",
+                groupby=["time"],
+                dataset=Dataset.Discover,
+                limit=10000,
+                referrer=referrer,
+            )
     else:
         result = {"data": []}
 
-    result = zerofill(result["data"], snuba_filter.start, snuba_filter.end, rollup, "time")
+    with sentry_sdk.start_span(
+        op="discover.discover", description="kt_timeseries.transform_results"
+    ) as span:
+        span.set_data("result_count", len(result.get("data", [])))
+        result = zerofill(result["data"], snuba_filter.start, snuba_filter.end, rollup, "time")
 
-    return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
+        return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
 
 
 def timeseries_query(selected_columns, query, params, rollup, reference_event=None, referrer=None):
@@ -775,26 +814,36 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
                     conditions based on the provided reference.
     referrer (str|None) A referrer string to help locate the origin of this query.
     """
-    snuba_filter, _ = get_timeseries_snuba_filter(
-        selected_columns, query, params, rollup, reference_event
-    )
+    with sentry_sdk.start_span(
+        op="discover.discover", description="timeseries.filter_transform"
+    ) as span:
+        span.set_data("query", query)
+        snuba_filter, _ = get_timeseries_snuba_filter(
+            selected_columns, query, params, rollup, reference_event
+        )
 
-    result = raw_query(
-        aggregations=snuba_filter.aggregations,
-        conditions=snuba_filter.conditions,
-        filter_keys=snuba_filter.filter_keys,
-        start=snuba_filter.start,
-        end=snuba_filter.end,
-        rollup=rollup,
-        orderby="time",
-        groupby=["time"],
-        dataset=Dataset.Discover,
-        limit=10000,
-        referrer=referrer,
-    )
-    result = zerofill(result["data"], snuba_filter.start, snuba_filter.end, rollup, "time")
+    with sentry_sdk.start_span(op="discover.discover", description="timeseries.snuba_query"):
+        result = raw_query(
+            aggregations=snuba_filter.aggregations,
+            conditions=snuba_filter.conditions,
+            filter_keys=snuba_filter.filter_keys,
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            rollup=rollup,
+            orderby="time",
+            groupby=["time"],
+            dataset=Dataset.Discover,
+            limit=10000,
+            referrer=referrer,
+        )
+
+    with sentry_sdk.start_span(
+        op="discover.discover", description="timeseries.transform_results"
+    ) as span:
+        span.set_data("result_count", len(result.get("data", [])))
+        result = zerofill(result["data"], snuba_filter.start, snuba_filter.end, rollup, "time")
 
-    return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
+        return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
 
 
 def create_result_key(result_row, fields, issues):
@@ -843,119 +892,129 @@ def top_events_timeseries(
     organization (Organization) Used to map group ids to short ids
     referrer (str|None) A referrer string to help locate the origin of this query.
     """
-    top_events = query(
-        selected_columns,
-        query=user_query,
-        params=params,
-        orderby=orderby,
-        limit=limit,
-        referrer=referrer,
-    )
+    with sentry_sdk.start_span(op="discover.discover", description="top_events.fetch_events"):
+        top_events = query(
+            selected_columns,
+            query=user_query,
+            params=params,
+            orderby=orderby,
+            limit=limit,
+            referrer=referrer,
+        )
 
-    snuba_filter, translated_columns = get_timeseries_snuba_filter(
-        timeseries_columns + selected_columns, user_query, params, rollup
-    )
+    with sentry_sdk.start_span(
+        op="discover.discover", description="top_events.filter_transform"
+    ) as span:
+        span.set_data("query", user_query)
+        snuba_filter, translated_columns = get_timeseries_snuba_filter(
+            timeseries_columns + selected_columns, user_query, params, rollup
+        )
 
-    user_fields = FIELD_ALIASES["user"]["fields"]
+        user_fields = FIELD_ALIASES["user"]["fields"]
+
+        for field in selected_columns:
+            # project is handled by filter_keys already
+            if field in ["project", "project.id"]:
+                continue
+            if field == "issue":
+                field = FIELD_ALIASES["issue"]["column_alias"]
+            # Note that because orderby shouldn't be an array field its not included in the values
+            values = list(
+                {
+                    event.get(field)
+                    for event in top_events["data"]
+                    if field in event and not isinstance(event.get(field), list)
+                }
+            )
+            if values:
+                # timestamp needs special handling, creating a big OR instead
+                if field == "timestamp":
+                    snuba_filter.conditions.append([["timestamp", "=", value] for value in values])
+                # A user field can be any of its field aliases, do an OR across all the user fields
+                elif field == "user":
+                    snuba_filter.conditions.append(
+                        [[resolve_column(user_field), "IN", values] for user_field in user_fields]
+                    )
+                elif None in values:
+                    non_none_values = [value for value in values if value is not None]
+                    condition = [[["isNull", [resolve_column(field)]], "=", 1]]
+                    if non_none_values:
+                        condition.append([resolve_column(field), "IN", non_none_values])
+                    snuba_filter.conditions.append(condition)
+                else:
+                    snuba_filter.conditions.append([resolve_column(field), "IN", values])
 
-    for field in selected_columns:
-        # project is handled by filter_keys already
-        if field in ["project", "project.id"]:
-            continue
-        if field == "issue":
-            field = FIELD_ALIASES["issue"]["column_alias"]
-        # Note that because orderby shouldn't be an array field its not included in the values
-        values = list(
-            {
-                event.get(field)
-                for event in top_events["data"]
-                if field in event and not isinstance(event.get(field), list)
-            }
+    with sentry_sdk.start_span(op="discover.discover", description="top_events.snuba_query"):
+        result = raw_query(
+            aggregations=snuba_filter.aggregations,
+            conditions=snuba_filter.conditions,
+            filter_keys=snuba_filter.filter_keys,
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            rollup=rollup,
+            orderby="time",
+            groupby=["time"] + snuba_filter.groupby,
+            dataset=Dataset.Discover,
+            limit=10000,
+            referrer=referrer,
         )
-        if values:
-            # timestamp needs special handling, creating a big OR instead
-            if field == "timestamp":
-                snuba_filter.conditions.append([["timestamp", "=", value] for value in values])
-            # A user field can be any of its field aliases, do an OR across all the user fields
-            elif field == "user":
-                snuba_filter.conditions.append(
-                    [[resolve_column(user_field), "IN", values] for user_field in user_fields]
-                )
-            elif None in values:
-                non_none_values = [value for value in values if value is not None]
-                condition = [[["isNull", [resolve_column(field)]], "=", 1]]
-                if non_none_values:
-                    condition.append([resolve_column(field), "IN", non_none_values])
-                snuba_filter.conditions.append(condition)
-            else:
-                snuba_filter.conditions.append([resolve_column(field), "IN", values])
-
-    result = raw_query(
-        aggregations=snuba_filter.aggregations,
-        conditions=snuba_filter.conditions,
-        filter_keys=snuba_filter.filter_keys,
-        start=snuba_filter.start,
-        end=snuba_filter.end,
-        rollup=rollup,
-        orderby="time",
-        groupby=["time"] + snuba_filter.groupby,
-        dataset=Dataset.Discover,
-        limit=10000,
-        referrer=referrer,
-    )
 
-    result = transform_results(result, translated_columns, snuba_filter, selected_columns)
+    with sentry_sdk.start_span(
+        op="discover.discover", description="top_events.transform_results"
+    ) as span:
+        span.set_data("result_count", len(result.get("data", [])))
+        result = transform_results(result, translated_columns, snuba_filter, selected_columns)
 
-    translated_columns["project_id"] = "project"
-    translated_groupby = [
-        translated_columns.get(groupby, groupby) for groupby in snuba_filter.groupby
-    ]
+        translated_columns["project_id"] = "project"
+        translated_groupby = [
+            translated_columns.get(groupby, groupby) for groupby in snuba_filter.groupby
+        ]
 
-    if "user" in selected_columns:
-        # Determine user related fields to prune based on what wasn't selected, since transform_results does the same
-        for field in user_fields:
-            if field not in selected_columns:
-                translated_groupby.remove(field)
-        translated_groupby.append("user")
-    issues = {}
-    if "issue" in selected_columns:
-        issues = Group.issues_mapping(
-            set([event["issue.id"] for event in top_events["data"]]),
-            params["project_id"],
-            organization,
-        )
-    # so the result key is consistent
-    translated_groupby.sort()
-
-    results = {}
-    # Using the top events add the order to the results
-    for index, item in enumerate(top_events["data"]):
-        result_key = create_result_key(item, translated_groupby, issues)
-        results[result_key] = {
-            "order": index,
-            "data": [],
-        }
-    for row in result["data"]:
-        result_key = create_result_key(row, translated_groupby, issues)
-        if result_key in results:
-            results[result_key]["data"].append(row)
-        else:
-            logger.warning(
-                "discover.top-events.timeseries.key-mismatch",
-                extra={"result_key": result_key, "top_event_keys": results.keys()},
+        if "user" in selected_columns:
+            # Determine user related fields to prune based on what wasn't selected, since transform_results does the same
+            for field in user_fields:
+                if field not in selected_columns:
+                    translated_groupby.remove(field)
+            translated_groupby.append("user")
+        issues = {}
+        if "issue" in selected_columns:
+            issues = Group.issues_mapping(
+                set([event["issue.id"] for event in top_events["data"]]),
+                params["project_id"],
+                organization,
+            )
+        # so the result key is consistent
+        translated_groupby.sort()
+
+        results = {}
+        # Using the top events add the order to the results
+        for index, item in enumerate(top_events["data"]):
+            result_key = create_result_key(item, translated_groupby, issues)
+            results[result_key] = {
+                "order": index,
+                "data": [],
+            }
+        for row in result["data"]:
+            result_key = create_result_key(row, translated_groupby, issues)
+            if result_key in results:
+                results[result_key]["data"].append(row)
+            else:
+                logger.warning(
+                    "discover.top-events.timeseries.key-mismatch",
+                    extra={"result_key": result_key, "top_event_keys": results.keys()},
+                )
+        for key, item in six.iteritems(results):
+            results[key] = SnubaTSResult(
+                {
+                    "data": zerofill(
+                        item["data"], snuba_filter.start, snuba_filter.end, rollup, "time"
+                    ),
+                    "order": item["order"],
+                },
+                snuba_filter.start,
+                snuba_filter.end,
+                rollup,
             )
-    for key, item in six.iteritems(results):
-        results[key] = SnubaTSResult(
-            {
-                "data": zerofill(
-                    item["data"], snuba_filter.start, snuba_filter.end, rollup, "time"
-                ),
-                "order": item["order"],
-            },
-            snuba_filter.start,
-            snuba_filter.end,
-            rollup,
-        )
 
     return results
 
@@ -1039,14 +1098,18 @@ def get_facets(query, params, limit=10, referrer=None):
 
     Returns Sequence[FacetResult]
     """
-    # TODO(evanh): This can be removed once we migrate the frontend / saved queries
-    # to use the new function values
-    query = transform_deprecated_functions_in_query(query)
+    with sentry_sdk.start_span(
+        op="discover.discover", description="facets.filter_transform"
+    ) as span:
+        span.set_data("query", query)
+        # TODO(evanh): This can be removed once we migrate the frontend / saved queries
+        # to use the new function values
+        query = transform_deprecated_functions_in_query(query)
 
-    snuba_filter = get_filter(query, params)
+        snuba_filter = get_filter(query, params)
 
-    # Resolve the public aliases into the discover dataset names.
-    snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter)
+        # Resolve the public aliases into the discover dataset names.
+        snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter)
 
     # Exclude tracing tags as they are noisy and generally not helpful.
     excluded_tags = ["tags_key", "NOT IN", ["trace", "trace.ctx", "trace.span", "project"]]
@@ -1055,24 +1118,25 @@ def get_facets(query, params, limit=10, referrer=None):
     # with that much data.
     sample = len(snuba_filter.filter_keys["project_id"]) > 2
 
-    # Get the most frequent tag keys
-    key_names = raw_query(
-        aggregations=[["count", None, "count"]],
-        start=snuba_filter.start,
-        end=snuba_filter.end,
-        conditions=snuba_filter.conditions,
-        filter_keys=snuba_filter.filter_keys,
-        orderby=["-count", "tags_key"],
-        groupby="tags_key",
-        having=[excluded_tags],
-        dataset=Dataset.Discover,
-        limit=limit,
-        referrer=referrer,
-        turbo=sample,
-    )
-    top_tags = [r["tags_key"] for r in key_names["data"]]
-    if not top_tags:
-        return []
+    with sentry_sdk.start_span(op="discover.discover", description="facets.frequent_tags"):
+        # Get the most frequent tag keys
+        key_names = raw_query(
+            aggregations=[["count", None, "count"]],
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            conditions=snuba_filter.conditions,
+            filter_keys=snuba_filter.filter_keys,
+            orderby=["-count", "tags_key"],
+            groupby="tags_key",
+            having=[excluded_tags],
+            dataset=Dataset.Discover,
+            limit=limit,
+            referrer=referrer,
+            turbo=sample,
+        )
+        top_tags = [r["tags_key"] for r in key_names["data"]]
+        if not top_tags:
+            return []
 
     # TODO(mark) Make the sampling rate scale based on the result size and scaling factor in
     # sentry.options. To test the lowest acceptable sampling rate, we use 0.1 which
@@ -1090,26 +1154,27 @@ def get_facets(query, params, limit=10, referrer=None):
 
     results = []
     if fetch_projects:
-        project_values = raw_query(
-            aggregations=[["count", None, "count"]],
-            start=snuba_filter.start,
-            end=snuba_filter.end,
-            conditions=snuba_filter.conditions,
-            filter_keys=snuba_filter.filter_keys,
-            groupby="project_id",
-            orderby="-count",
-            dataset=Dataset.Discover,
-            referrer=referrer,
-            sample=sample_rate,
-            # Ensures Snuba will not apply FINAL
-            turbo=sample_rate is not None,
-        )
-        results.extend(
-            [
-                FacetResult("project", r["project_id"], int(r["count"]) * multiplier)
-                for r in project_values["data"]
-            ]
-        )
+        with sentry_sdk.start_span(op="discover.discover", description="facets.projects"):
+            project_values = raw_query(
+                aggregations=[["count", None, "count"]],
+                start=snuba_filter.start,
+                end=snuba_filter.end,
+                conditions=snuba_filter.conditions,
+                filter_keys=snuba_filter.filter_keys,
+                groupby="project_id",
+                orderby="-count",
+                dataset=Dataset.Discover,
+                referrer=referrer,
+                sample=sample_rate,
+                # Ensures Snuba will not apply FINAL
+                turbo=sample_rate is not None,
+            )
+            results.extend(
+                [
+                    FacetResult("project", r["project_id"], int(r["count"]) * multiplier)
+                    for r in project_values["data"]
+                ]
+            )
 
     # Get tag counts for our top tags. Fetching them individually
     # allows snuba to leverage promoted tags better and enables us to get
@@ -1126,53 +1191,58 @@ def get_facets(query, params, limit=10, referrer=None):
         else:
             individual_tags.append(tag)
 
-    for tag_name in individual_tags:
-        tag = u"tags[{}]".format(tag_name)
-        tag_values = raw_query(
-            aggregations=[["count", None, "count"]],
-            conditions=snuba_filter.conditions,
-            start=snuba_filter.start,
-            end=snuba_filter.end,
-            filter_keys=snuba_filter.filter_keys,
-            orderby=["-count"],
-            groupby=[tag],
-            limit=TOP_VALUES_DEFAULT_LIMIT,
-            dataset=Dataset.Discover,
-            referrer=referrer,
-            sample=sample_rate,
-            # Ensures Snuba will not apply FINAL
-            turbo=sample_rate is not None,
-        )
-        results.extend(
-            [
-                FacetResult(tag_name, r[tag], int(r["count"]) * multiplier)
-                for r in tag_values["data"]
-            ]
-        )
+    with sentry_sdk.start_span(
+        op="discover.discover", description="facets.individual_tags"
+    ) as span:
+        span.set_data("tag_count", len(individual_tags))
+        for tag_name in individual_tags:
+            tag = u"tags[{}]".format(tag_name)
+            tag_values = raw_query(
+                aggregations=[["count", None, "count"]],
+                conditions=snuba_filter.conditions,
+                start=snuba_filter.start,
+                end=snuba_filter.end,
+                filter_keys=snuba_filter.filter_keys,
+                orderby=["-count"],
+                groupby=[tag],
+                limit=TOP_VALUES_DEFAULT_LIMIT,
+                dataset=Dataset.Discover,
+                referrer=referrer,
+                sample=sample_rate,
+                # Ensures Snuba will not apply FINAL
+                turbo=sample_rate is not None,
+            )
+            results.extend(
+                [
+                    FacetResult(tag_name, r[tag], int(r["count"]) * multiplier)
+                    for r in tag_values["data"]
+                ]
+            )
 
     if aggregate_tags:
-        conditions = snuba_filter.conditions
-        conditions.append(["tags_key", "IN", aggregate_tags])
-        tag_values = raw_query(
-            aggregations=[["count", None, "count"]],
-            conditions=conditions,
-            start=snuba_filter.start,
-            end=snuba_filter.end,
-            filter_keys=snuba_filter.filter_keys,
-            orderby=["tags_key", "-count"],
-            groupby=["tags_key", "tags_value"],
-            dataset=Dataset.Discover,
-            referrer=referrer,
-            sample=sample_rate,
-            # Ensures Snuba will not apply FINAL
-            turbo=sample_rate is not None,
-            limitby=[TOP_VALUES_DEFAULT_LIMIT, "tags_key"],
-        )
-        results.extend(
-            [
-                FacetResult(r["tags_key"], r["tags_value"], int(r["count"]) * multiplier)
-                for r in tag_values["data"]
-            ]
-        )
+        with sentry_sdk.start_span(op="discover.discover", description="facets.aggregate_tags"):
+            conditions = snuba_filter.conditions
+            conditions.append(["tags_key", "IN", aggregate_tags])
+            tag_values = raw_query(
+                aggregations=[["count", None, "count"]],
+                conditions=conditions,
+                start=snuba_filter.start,
+                end=snuba_filter.end,
+                filter_keys=snuba_filter.filter_keys,
+                orderby=["tags_key", "-count"],
+                groupby=["tags_key", "tags_value"],
+                dataset=Dataset.Discover,
+                referrer=referrer,
+                sample=sample_rate,
+                # Ensures Snuba will not apply FINAL
+                turbo=sample_rate is not None,
+                limitby=[TOP_VALUES_DEFAULT_LIMIT, "tags_key"],
+            )
+            results.extend(
+                [
+                    FacetResult(r["tags_key"], r["tags_value"], int(r["count"]) * multiplier)
+                    for r in tag_values["data"]
+                ]
+            )
 
     return results
