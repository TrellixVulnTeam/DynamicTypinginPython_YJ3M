commit 0f3386f929557d409e4713aa9a9d20e9f0a1354e
Author: David Cramer <dcramer@gmail.com>
Date:   Mon May 15 16:48:21 2017 -0700

    [deletions] new task abstraction and scheduler
    
    This implements a new extensible layer for deletions allowing us to:
    
    - Schedule deletions via a standard means (ScheduledDeletion)
    - Abstraction to manage cascade deletes between both db models and other objects
    
    It also includes:
    
    - Compatibility shims so old tasks remain operable
    - Change to Event deletions (less in-memory per task run)
    - Moves several models to use bulk_delete_objects that are capable and previously werent
    - Remove unlimited retries on deletions in development

diff --git a/CHANGES b/CHANGES
index f94097092f..646108d2fc 100644
--- a/CHANGES
+++ b/CHANGES
@@ -8,6 +8,7 @@ Version 8.17 (Unreleased)
 - The threads interface now contributes to grouping if it contains a single thread.
 - Added per-key (DSN) rate limits (``project:rate-limits`` feature).
 - Added tsdb statistics for events per-key.
+- Added ``sentry.deletions`` abstraction to improve bulk deletions.
 
 Schema Changes
 ~~~~~~~~~~~~~~
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index ce0963566b..af02ad62b0 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -594,6 +594,13 @@ CELERYBEAT_SCHEDULE = {
             'expires': 60 * 25,
         },
     },
+    'schedule-deletions': {
+        'task': 'sentry.tasks.deletion.run_scheduled_deletions',
+        'schedule': timedelta(minutes=15),
+        'options': {
+            'expires': 60 * 25,
+        },
+    },
     'schedule-weekly-organization-reports': {
         'task': 'sentry.tasks.reports.prepare_reports',
         'schedule': crontab(
diff --git a/src/sentry/deletions/__init__.py b/src/sentry/deletions/__init__.py
new file mode 100644
index 0000000000..388bcd9cd5
--- /dev/null
+++ b/src/sentry/deletions/__init__.py
@@ -0,0 +1,81 @@
+"""
+The deletions subsystem managers bulk deletes as well as cascades. It attempts
+to optimize around various patterns while using a standard approach to do so.
+
+For example, let's say you want to delete an organization:
+
+>>> from sentry import deletions
+>>> task = deletions.get(model=Organization)
+>>> work = True
+>>> while work:
+>>>    work = task.chunk()
+
+The system has a default task implementation to handle Organization which will
+efficiently cascade deletes. This behavior varies based on the input object,
+as the task can override the behavior for it's children.
+
+For example, when you delete a Group, it will cascade in a more traditional
+manner. It will batch each child (such as Event). However, when you delete a
+project, it won't actually cascade to the registered Group task. It will instead
+take a more efficient approach of batch deleting its indirect descedancts, such
+as Event, so it can more efficiently bulk delete rows.
+"""
+
+from __future__ import absolute_import
+
+from .base import BulkModelDeletionTask, ModelDeletionTask, ModelRelation  # NOQA
+from .manager import DeletionTaskManager
+
+default_manager = DeletionTaskManager(default_task=ModelDeletionTask)
+
+
+def load_defaults():
+    from sentry import models
+    from . import defaults
+
+    default_manager.register(models.Activity, BulkModelDeletionTask)
+    default_manager.register(models.ApiApplication, defaults.ApiApplicationDeletionTask)
+    default_manager.register(models.ApiKey, BulkModelDeletionTask)
+    default_manager.register(models.ApiGrant, BulkModelDeletionTask)
+    default_manager.register(models.ApiToken, BulkModelDeletionTask)
+    default_manager.register(models.CommitAuthor, BulkModelDeletionTask)
+    default_manager.register(models.CommitFileChange, BulkModelDeletionTask)
+    default_manager.register(models.EnvironmentProject, BulkModelDeletionTask)
+    default_manager.register(models.Event, defaults.EventDeletionTask)
+    default_manager.register(models.EventMapping, BulkModelDeletionTask)
+    default_manager.register(models.EventTag, BulkModelDeletionTask)
+    default_manager.register(models.EventUser, BulkModelDeletionTask)
+    default_manager.register(models.Group, defaults.GroupDeletionTask)
+    default_manager.register(models.GroupAssignee, BulkModelDeletionTask)
+    default_manager.register(models.GroupBookmark, BulkModelDeletionTask)
+    default_manager.register(models.GroupCommitResolution, BulkModelDeletionTask)
+    default_manager.register(models.GroupEmailThread, BulkModelDeletionTask)
+    default_manager.register(models.GroupHash, BulkModelDeletionTask)
+    default_manager.register(models.GroupMeta, BulkModelDeletionTask)
+    default_manager.register(models.GroupRedirect, BulkModelDeletionTask)
+    default_manager.register(models.GroupRelease, BulkModelDeletionTask)
+    default_manager.register(models.GroupResolution, BulkModelDeletionTask)
+    default_manager.register(models.GroupRuleStatus, BulkModelDeletionTask)
+    default_manager.register(models.GroupSeen, BulkModelDeletionTask)
+    default_manager.register(models.GroupSnooze, BulkModelDeletionTask)
+    default_manager.register(models.GroupSubscription, BulkModelDeletionTask)
+    default_manager.register(models.GroupTagKey, BulkModelDeletionTask)
+    default_manager.register(models.GroupTagValue, BulkModelDeletionTask)
+    default_manager.register(models.Organization, defaults.OrganizationDeletionTask)
+    default_manager.register(models.OrganizationMemberTeam, BulkModelDeletionTask)
+    default_manager.register(models.Project, defaults.ProjectDeletionTask)
+    default_manager.register(models.ProjectBookmark, BulkModelDeletionTask)
+    default_manager.register(models.ProjectKey, BulkModelDeletionTask)
+    default_manager.register(models.Repository, defaults.RepositoryDeletionTask)
+    default_manager.register(models.SavedSearch, BulkModelDeletionTask)
+    default_manager.register(models.SavedSearchUserDefault, BulkModelDeletionTask)
+    default_manager.register(models.TagKey, defaults.TagKeyDeletionTask)
+    default_manager.register(models.TagValue, BulkModelDeletionTask)
+    default_manager.register(models.Team, defaults.TeamDeletionTask)
+    default_manager.register(models.UserReport, BulkModelDeletionTask)
+
+
+load_defaults()
+
+get = default_manager.get
+register = default_manager.register
diff --git a/src/sentry/deletions/base.py b/src/sentry/deletions/base.py
new file mode 100644
index 0000000000..87812d3187
--- /dev/null
+++ b/src/sentry/deletions/base.py
@@ -0,0 +1,218 @@
+from __future__ import absolute_import, print_function
+
+import logging
+
+from sentry.constants import ObjectStatus
+from sentry.utils.query import bulk_delete_objects
+
+
+class BaseRelation(object):
+    def __init__(self, params, task):
+        self.task = task
+        self.params = params
+
+    def __repr__(self):
+        return '<%s: task=%s params=%s>' % (
+            type(self),
+            self.task,
+            self.params,
+        )
+
+
+class ModelRelation(BaseRelation):
+    def __init__(self, model, query, task=None):
+        params = {
+            'model': model,
+            'query': query,
+        }
+        super(ModelRelation, self).__init__(params=params, task=task)
+
+
+class BaseDeletionTask(object):
+    logger = logging.getLogger('sentry.deletions.async')
+
+    DEFAULT_CHUNK_SIZE = 100
+
+    def __init__(self, manager, transaction_id=None,
+                 actor_id=None, chunk_size=DEFAULT_CHUNK_SIZE):
+        self.manager = manager
+        self.transaction_id = transaction_id
+        self.actor_id = actor_id
+        self.chunk_size = chunk_size
+
+    def __repr__(self):
+        return '<%s: transaction_id=%s actor_id=%s>' % (
+            type(self),
+            self.transaction_id,
+            self.actor_id,
+        )
+
+    def chunk(self):
+        """
+        Deletes a chunk of this instance's data. Return ``True`` if there is
+        more work, or ``False`` if the entity has been removed.
+        """
+        raise NotImplementedError
+
+    def get_child_relations(self, instance):
+        # TODO(dcramer): it'd be nice if we collected the default relationships
+        return [
+            # ModelRelation(Model, {'parent_id': instance.id})
+        ]
+
+    def get_child_relations_bulk(self, instance_list):
+        return [
+            # ModelRelation(Model, {'parent_id__in': [i.id for id in instance_list]})
+        ]
+
+    def delete_bulk(self, instance_list):
+        """
+        Delete a batch of objects bound to this task.
+
+        This **should** not be called with arbitrary types, but rather should
+        be used for only the base type this task was instantiated against.
+        """
+        self.mark_deletion_in_progress(instance_list)
+
+        child_relations = self.get_child_relations_bulk(instance_list)
+        if child_relations:
+            has_more = self.delete_children(child_relations)
+            if has_more:
+                return has_more
+
+        for instance in instance_list:
+            child_relations = self.get_child_relations(instance)
+            if child_relations:
+                has_more = self.delete_children(child_relations)
+                if has_more:
+                    return has_more
+
+        return self.delete_instance_bulk(instance_list)
+
+    def delete_instance(self, instance):
+        raise NotImplementedError
+
+    def delete_instance_bulk(self, instance_list):
+        for instance in instance_list:
+            self.delete_instance(instance)
+
+    def delete_children(self, relations):
+        # Ideally this runs through the deletion manager
+        has_more = False
+        for relation in relations:
+            task = self.manager.get(
+                transaction_id=self.transaction_id,
+                actor_id=self.actor_id,
+                chunk_size=self.chunk_size,
+                task=relation.task,
+                **relation.params
+            )
+            has_more = task.chunk()
+            if has_more:
+                return has_more
+        return has_more
+
+    def mark_deletion_in_progress(self, instance_list):
+        pass
+
+
+class ModelDeletionTask(BaseDeletionTask):
+    DEFAULT_QUERY_LIMIT = None
+
+    def __init__(self, manager, model, query, query_limit=None, **kwargs):
+        super(ModelDeletionTask, self).__init__(manager, **kwargs)
+        self.model = model
+        self.query = query
+        self.query_limit = (
+            query_limit or
+            self.DEFAULT_QUERY_LIMIT or
+            self.chunk_size
+        )
+
+    def __repr__(self):
+        return '<%s: model=%s query=%s transaction_id=%s actor_id=%s>' % (
+            type(self),
+            self.model,
+            self.query,
+            self.transaction_id,
+            self.actor_id,
+        )
+
+    def chunk(self):
+        """
+        Deletes a chunk of this instance's data. Return ``True`` if there is
+        more work, or ``False`` if the entity has been removed.
+        """
+        query_limit = self.query_limit
+        remaining = self.chunk_size
+        while remaining > 0:
+            queryset = list(self.model.objects.filter(
+                **self.query
+            )[:query_limit])
+            if not queryset:
+                return False
+
+            self.delete_bulk(queryset)
+            remaining -= query_limit
+        return True
+
+    def delete_instance_bulk(self, instance_list):
+        # slow, but ensures Django cascades are handled
+        for instance in instance_list:
+            self.delete_instance(instance)
+
+    def delete_instance(self, instance):
+        instance_id = instance.id
+        try:
+            instance.delete()
+        finally:
+            self.logger.info('object.delete.executed', extra={
+                'object_id': instance_id,
+                'transaction_id': self.transaction_id,
+                'app_label': instance._meta.app_label,
+                'model': type(instance).__name__,
+            })
+
+    def get_actor(self):
+        from sentry.models import User
+
+        if self.actor_id:
+            try:
+                return User.objects.get_from_cache(id=self.actor_id)
+            except User.DoesNotExist:
+                pass
+        return None
+
+    def mark_deletion_in_progress(self, instance_list):
+        for instance in instance_list:
+            status = getattr(instance, 'status', None)
+            if status not in (ObjectStatus.DELETION_IN_PROGRESS, None):
+                instance.update(status=ObjectStatus.DELETION_IN_PROGRESS)
+
+
+class BulkModelDeletionTask(ModelDeletionTask):
+    """
+    An efficient mechanism for deleting larger volumes of rows in one pass,
+    but will hard fail if the relations have resident foreign relations.
+
+    Note: Does NOT support child relations.
+    """
+    DEFAULT_CHUNK_SIZE = 10000
+
+    def chunk(self):
+        return self.delete_instance_bulk()
+
+    def delete_instance_bulk(self):
+        try:
+            return bulk_delete_objects(
+                model=self.model,
+                limit=self.chunk_size,
+                transaction_id=self.transaction_id,
+                **self.query
+            )
+        finally:
+            self.logger.info('object.delete.bulk_executed', extra=dict({
+                'transaction_id': self.transaction_id,
+                'app_label': self.model._meta.app_label,
+                'model': self.model.__name__,
+            }, **self.query))
diff --git a/src/sentry/deletions/defaults/__init__.py b/src/sentry/deletions/defaults/__init__.py
new file mode 100644
index 0000000000..1422fb52ef
--- /dev/null
+++ b/src/sentry/deletions/defaults/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import absolute_import
+
+from sentry.utils.imports import import_submodules
+
+import_submodules(globals(), __name__, __path__)
diff --git a/src/sentry/deletions/defaults/apiapplication.py b/src/sentry/deletions/defaults/apiapplication.py
new file mode 100644
index 0000000000..f098d7a31d
--- /dev/null
+++ b/src/sentry/deletions/defaults/apiapplication.py
@@ -0,0 +1,23 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class ApiApplicationDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry.models import ApiGrant, ApiToken
+
+        # in bulk
+        model_list = (
+            ApiToken, ApiGrant
+        )
+        return [
+            ModelRelation(m, {'application_id': instance.id}) for m in model_list
+        ]
+
+    def mark_deletion_in_progress(self, instance_list):
+        from sentry.models import ApiApplicationStatus
+
+        for instance in instance_list:
+            if instance.status != ApiApplicationStatus.deletion_in_progress:
+                instance.update(status=ApiApplicationStatus.deletion_in_progress)
diff --git a/src/sentry/deletions/defaults/event.py b/src/sentry/deletions/defaults/event.py
new file mode 100644
index 0000000000..14ee9c7475
--- /dev/null
+++ b/src/sentry/deletions/defaults/event.py
@@ -0,0 +1,32 @@
+from __future__ import absolute_import, print_function
+
+from sentry import nodestore
+
+from ..base import (
+    BaseDeletionTask, BaseRelation, ModelDeletionTask, ModelRelation
+)
+
+
+class NodeDeletionTask(BaseDeletionTask):
+    def __init__(self, manager, nodes, **kwargs):
+        self.nodes = nodes
+        super(NodeDeletionTask, self).__init__(manager, **kwargs)
+
+    def chunk(self):
+        nodestore.delete_multi(self.nodes)
+        return False
+
+
+class EventDeletionTask(ModelDeletionTask):
+    def get_child_relations_bulk(self, instance_list):
+        from sentry.models import EventTag
+
+        node_ids = [i.data.id for i in instance_list]
+        event_ids = [i.id for i in instance_list]
+
+        return [
+            BaseRelation({'nodes': node_ids}, NodeDeletionTask),
+            ModelRelation(EventTag, {
+                'event_id__in': event_ids,
+            }, ModelDeletionTask),
+        ]
diff --git a/src/sentry/deletions/defaults/group.py b/src/sentry/deletions/defaults/group.py
new file mode 100644
index 0000000000..01e336a42e
--- /dev/null
+++ b/src/sentry/deletions/defaults/group.py
@@ -0,0 +1,52 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class GroupDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry import models
+
+        relations = []
+
+        model_list = (
+            # prioritize GroupHash
+            models.GroupHash,
+            models.EventTag,
+            models.EventMapping,
+            models.GroupAssignee,
+            models.GroupCommitResolution,
+            models.GroupBookmark,
+            models.GroupMeta,
+            models.GroupRelease,
+            models.GroupRedirect,
+            models.GroupResolution,
+            models.GroupRuleStatus,
+            models.GroupSnooze,
+            models.GroupTagValue,
+            models.GroupTagKey,
+            models.GroupEmailThread,
+            models.GroupSubscription,
+            models.UserReport,
+            # Event is last as its the most time consuming
+            models.Event,
+        )
+        relations.extend([
+            ModelRelation(m, {'group_id': instance.id}) for m in model_list
+        ])
+
+        return relations
+
+    def delete_instance(self, instance):
+        from sentry.similarity import features
+
+        features.delete(instance)
+
+        return super(GroupDeletionTask, self).delete_instance(instance)
+
+    def mark_deletion_in_progress(self, instance_list):
+        from sentry.models import GroupStatus
+
+        for instance in instance_list:
+            if instance.status != GroupStatus.DELETION_IN_PROGRESS:
+                instance.update(status=GroupStatus.DELETION_IN_PROGRESS)
diff --git a/src/sentry/deletions/defaults/organization.py b/src/sentry/deletions/defaults/organization.py
new file mode 100644
index 0000000000..1ec783aa83
--- /dev/null
+++ b/src/sentry/deletions/defaults/organization.py
@@ -0,0 +1,36 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class OrganizationDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry.models import (
+            OrganizationMember,
+            Commit, CommitAuthor, CommitFileChange, Environment, Release,
+            ReleaseCommit, ReleaseEnvironment, ReleaseFile, Distribution,
+            ReleaseHeadCommit, Repository, Team
+        )
+
+        # Team must come first
+        relations = [
+            ModelRelation(Team, {'organization_id': instance.id}),
+        ]
+
+        model_list = (
+            OrganizationMember, CommitFileChange, Commit, CommitAuthor,
+            Environment, Repository, Release, ReleaseCommit,
+            ReleaseEnvironment, ReleaseFile, Distribution, ReleaseHeadCommit
+        )
+        relations.extend([
+            ModelRelation(m, {'organization_id': instance.id}) for m in model_list
+        ])
+
+        return relations
+
+    def mark_deletion_in_progress(self, instance_list):
+        from sentry.models import OrganizationStatus
+
+        for instance in instance_list:
+            if instance.status != OrganizationStatus.DELETION_IN_PROGRESS:
+                instance.update(status=OrganizationStatus.DELETION_IN_PROGRESS)
diff --git a/src/sentry/deletions/defaults/project.py b/src/sentry/deletions/defaults/project.py
new file mode 100644
index 0000000000..7757461455
--- /dev/null
+++ b/src/sentry/deletions/defaults/project.py
@@ -0,0 +1,71 @@
+from __future__ import absolute_import, print_function
+
+from ..base import (
+    BulkModelDeletionTask, ModelDeletionTask, ModelRelation
+)
+
+
+class ProjectDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry import models
+
+        relations = [
+            # ProjectKey gets revoked immediately, in bulk
+            ModelRelation(models.ProjectKey, {'project_id': instance.id})
+        ]
+
+        # in bulk
+        model_list = (
+            models.Activity,
+            models.EnvironmentProject,
+            models.EventMapping,
+            models.EventUser,
+            models.EventTag,
+            models.GroupAssignee,
+            models.GroupBookmark,
+            models.GroupEmailThread,
+            models.GroupHash,
+            models.GroupRelease,
+            models.GroupRuleStatus,
+            models.GroupSeen,
+            models.GroupSubscription,
+            models.GroupTagKey,
+            models.GroupTagValue,
+            models.ProjectBookmark,
+            models.ProjectKey,
+            models.SavedSearchUserDefault,
+            models.SavedSearch,
+            models.TagKey,
+            models.TagValue,
+            models.UserReport,
+        )
+        relations.extend([
+            ModelRelation(m, {'project_id': instance.id}, BulkModelDeletionTask)
+            for m in model_list
+        ])
+
+        model_list = (
+            models.GroupMeta,
+            models.GroupResolution,
+            models.GroupSnooze,
+        )
+        relations.extend([
+            ModelRelation(m, {'group__project': instance.id}, ModelDeletionTask)
+            for m in model_list
+        ])
+
+        # special case event due to nodestore
+        relations.extend([
+            ModelRelation(models.Event, {'project_id': instance.id})
+        ])
+
+        # in bulk
+        # Release needs to handle deletes after Group is cleaned up as the foreign
+        # key is protected
+        model_list = (models.Group, models.ReleaseProject)
+        relations.extend([
+            ModelRelation(m, {'project_id': instance.id}, ModelDeletionTask)
+            for m in model_list
+        ])
+
+        return relations
diff --git a/src/sentry/deletions/defaults/repository.py b/src/sentry/deletions/defaults/repository.py
new file mode 100644
index 0000000000..3b14f2808a
--- /dev/null
+++ b/src/sentry/deletions/defaults/repository.py
@@ -0,0 +1,12 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class RepositoryDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry.models import Commit
+
+        return [
+            ModelRelation(Commit, {'repository_id': instance.id}),
+        ]
diff --git a/src/sentry/deletions/defaults/tagkey.py b/src/sentry/deletions/defaults/tagkey.py
new file mode 100644
index 0000000000..f502bdf2b0
--- /dev/null
+++ b/src/sentry/deletions/defaults/tagkey.py
@@ -0,0 +1,35 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class TagKeyDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry.models import (
+            EventTag, GroupTagKey, GroupTagValue, TagValue
+        )
+
+        # in bulk
+        model_list = (
+            GroupTagValue, GroupTagKey, TagValue
+        )
+        relations = [
+            ModelRelation(m, {
+                'project_id': instance.project_id,
+                'key': instance.key,
+            }) for m in model_list
+        ]
+        relations.append(
+            ModelRelation(EventTag, {
+                'project_id': instance.project_id,
+                'key_id': instance.id,
+            })
+        )
+        return relations
+
+    def mark_deletion_in_progress(self, instance_list):
+        from sentry.models import TagKeyStatus
+
+        for instance in instance_list:
+            if instance.status != TagKeyStatus.DELETION_IN_PROGRESS:
+                instance.update(status=TagKeyStatus.DELETION_IN_PROGRESS)
diff --git a/src/sentry/deletions/defaults/team.py b/src/sentry/deletions/defaults/team.py
new file mode 100644
index 0000000000..4ec8db547b
--- /dev/null
+++ b/src/sentry/deletions/defaults/team.py
@@ -0,0 +1,19 @@
+from __future__ import absolute_import, print_function
+
+from ..base import ModelDeletionTask, ModelRelation
+
+
+class TeamDeletionTask(ModelDeletionTask):
+    def get_child_relations(self, instance):
+        from sentry.models import Project
+
+        return [
+            ModelRelation(Project, {'team_id': instance.id}),
+        ]
+
+    def mark_deletion_in_progress(self, instance_list):
+        from sentry.models import TeamStatus
+
+        for instance in instance_list:
+            if instance.status != TeamStatus.DELETION_IN_PROGRESS:
+                instance.update(status=TeamStatus.DELETION_IN_PROGRESS)
diff --git a/src/sentry/deletions/manager.py b/src/sentry/deletions/manager.py
new file mode 100644
index 0000000000..0d8673b9bd
--- /dev/null
+++ b/src/sentry/deletions/manager.py
@@ -0,0 +1,24 @@
+from __future__ import absolute_import, print_function
+
+__all__ = ['DeletionTaskManager']
+
+
+class DeletionTaskManager(object):
+    def __init__(self, default_task=None):
+        self.tasks = {}
+        self.default_task = default_task
+
+    def get(self, task=None, **kwargs):
+        if task is None:
+            model = kwargs.get('model')
+            try:
+                task = self.tasks[model]
+            except KeyError:
+                task = self.default_task
+        return task(
+            manager=self,
+            **kwargs
+        )
+
+    def register(self, model, task):
+        self.tasks[model] = task
diff --git a/src/sentry/models/repository.py b/src/sentry/models/repository.py
index b7b84d88e0..c73e7e036d 100644
--- a/src/sentry/models/repository.py
+++ b/src/sentry/models/repository.py
@@ -43,7 +43,7 @@ class Repository(Model):
         return provider_cls(self.provider)
 
 
-def on_delete(instance, actor, **kwargs):
+def on_delete(instance, actor=None, **kwargs):
     instance.get_provider().delete_repository(
         repo=instance,
         actor=actor,
diff --git a/src/sentry/models/scheduledeletion.py b/src/sentry/models/scheduledeletion.py
new file mode 100644
index 0000000000..1aea796130
--- /dev/null
+++ b/src/sentry/models/scheduledeletion.py
@@ -0,0 +1,61 @@
+from __future__ import absolute_import
+
+from datetime import timedelta
+from django.db import models
+from django.db.models import get_model
+from django.utils import timezone
+from jsonfield import JSONField
+from uuid import uuid4
+
+from sentry.db.models import BoundedBigIntegerField, Model
+
+
+class ScheduledDeletion(Model):
+    __core__ = False
+
+    guid = models.CharField(max_length=32, unique=True,
+                            default=lambda: uuid4().hex)
+    app_label = models.CharField(max_length=64)
+    model_name = models.CharField(max_length=64)
+    object_id = BoundedBigIntegerField()
+    date_added = models.DateTimeField(default=timezone.now)
+    date_scheduled = models.DateTimeField(default=lambda: (
+        timezone.now() + timedelta(days=30)
+    ))
+    actor_id = BoundedBigIntegerField(null=True)
+    data = JSONField(default={})
+    in_progress = models.BooleanField(default=False)
+    aborted = models.BooleanField(default=False)
+
+    class Meta:
+        unique_together = (('app_label', 'model_name', 'object_id'),)
+        app_label = 'sentry'
+        db_table = 'sentry_scheduleddeletion'
+
+    @classmethod
+    def schedule(cls, instance, days=30, data={}, actor=None):
+        return cls.objects.create(
+            app_label=instance._meta.app_label,
+            model_name=type(instance).__name__,
+            object_id=instance.pk,
+            date_scheduled=timezone.now() + timedelta(days=days),
+            data=data,
+            actor_id=actor.id if actor else None,
+        )
+
+    def get_model(self):
+        return get_model(self.app_label, self.model_name)
+
+    def get_instance(self):
+        return self.get_model().objects.get(pk=self.object_id)
+
+    def get_actor(self):
+        from sentry.models import User
+
+        if not self.actor_id:
+            return None
+
+        try:
+            return User.objects.get(id=self.actor_id)
+        except User.DoesNotExist:
+            return None
diff --git a/src/sentry/nodestore/README.rst b/src/sentry/nodestore/README.rst
index 047ed8f3f6..cdcc239b79 100644
--- a/src/sentry/nodestore/README.rst
+++ b/src/sentry/nodestore/README.rst
@@ -59,7 +59,8 @@ A developer need not worry about directly setting data via ``nodestore`` methods
 
 Deleting Node Data
 ``````````````````
-Node data is deleted as part of the ``delete_events()`` in ``sentry.tasks.deletion``. The
+
+Node data is deleted as part of ``sentry.tasks.deletion``. The
 deletion process of node data is different from other Node Storage operations because it is
 entirely optional for a backend to actually delete the data. This concept can be leveraged by
 backends that support time-to-live(TTL) fields, taking the responsibility of deleting data
diff --git a/src/sentry/tasks/deletion.py b/src/sentry/tasks/deletion.py
index bb09a2026c..0f28440d07 100644
--- a/src/sentry/tasks/deletion.py
+++ b/src/sentry/tasks/deletion.py
@@ -1,416 +1,295 @@
-"""
-sentry.tasks.deletion
-~~~~~~~~~~~~~~~~~~~~~
-
-:copyright: (c) 2010-2014 by the Sentry Team, see AUTHORS for more details.
-:license: BSD, see LICENSE for more details.
-"""
-
 from __future__ import absolute_import
 
-import logging
+from uuid import uuid4
 
+from django.conf import settings
+from django.db import transaction
 from django.db.models import get_model
+from django.utils import timezone
 
-from sentry import nodestore
 from sentry.constants import ObjectStatus
 from sentry.exceptions import DeleteAborted
 from sentry.signals import pending_delete
-from sentry.similarity import features
 from sentry.tasks.base import instrumented_task, retry
-from sentry.utils.query import bulk_delete_objects
 
-logger = logging.getLogger('sentry.deletions.async')
+# in prod we run with infinite retries to recover from errors
+# in debug/development, we assume these tasks generally shouldn't fail
+MAX_RETRIES = 1 if settings.DEBUG else None
+MAX_RETRIES = 1
 
 
-@instrumented_task(name='sentry.tasks.deletion.delete_organization', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
-@retry(exclude=(DeleteAborted,))
-def delete_organization(object_id, transaction_id=None, continuous=True, **kwargs):
-    from sentry.models import (
-        Organization, OrganizationMember, OrganizationStatus, Team, TeamStatus,
-        Commit, CommitAuthor, CommitFileChange, Environment, Release, ReleaseCommit,
-        ReleaseEnvironment, ReleaseFile, Distribution, ReleaseHeadCommit, Repository
+@instrumented_task(name='sentry.tasks.deletion.run_scheduled_deletions', queue='cleanup')
+def run_scheduled_deletions():
+    from sentry.models import ScheduledDeletion
+
+    queryset = ScheduledDeletion.objects.filter(
+        in_progress=False,
+        aborted=False,
+        date_scheduled__lte=timezone.now(),
     )
+    for item in queryset:
+        with transaction.atomic():
+            affected = ScheduledDeletion.objects.filter(
+                id=item.id,
+                in_progress=False,
+                aborted=False,
+            ).update(
+                in_progress=True,
+            )
+            if not affected:
+                continue
+
+            run_deletion.delay(deletion_id=item.id)
+
+
+@instrumented_task(name='sentry.tasks.deletion.run_deletion', queue='cleanup',
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
+@retry(exclude=(DeleteAborted,))
+def run_deletion(deletion_id):
+    from sentry import deletions
+    from sentry.models import ScheduledDeletion
 
     try:
-        o = Organization.objects.get(id=object_id)
-    except Organization.DoesNotExist:
+        deletion = ScheduledDeletion.objects.get(
+            id=deletion_id,
+        )
+    except ScheduledDeletion.DoesNotExist:
         return
 
-    if o.status == OrganizationStatus.VISIBLE:
-        raise DeleteAborted('Aborting organization deletion as status is invalid')
-
-    if o.status != OrganizationStatus.DELETION_IN_PROGRESS:
-        o.update(status=OrganizationStatus.DELETION_IN_PROGRESS)
-        pending_delete.send(sender=Organization, instance=o)
+    if deletion.aborted:
+        raise DeleteAborted
 
-    for team in Team.objects.filter(organization=o).order_by('id')[:1]:
-        team.update(status=TeamStatus.DELETION_IN_PROGRESS)
-        delete_team(team.id, transaction_id=transaction_id, continuous=False)
-        if continuous:
-            delete_organization.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
+    if not deletion.in_progress:
+        actor = deletion.get_actor()
+        instance = deletion.get_instance()
+        with transaction.atomc():
+            deletion.update(in_progress=True)
+            pending_delete.send(
+                sender=type(instance),
+                instance=instance,
+                actor=actor,
             )
-        return
 
-    model_list = (
-        OrganizationMember, CommitFileChange, Commit, CommitAuthor,
-        Environment, Repository, Release, ReleaseCommit,
-        ReleaseEnvironment, ReleaseFile, Distribution, ReleaseHeadCommit
+    task = deletions.get(
+        model=deletion.get_model(),
+        query={
+            'id': deletion.object_id,
+        },
+        transaction_id=deletion.guid,
+        actor_id=deletion.actor_id,
     )
+    has_more = task.chunk()
+    if has_more:
+        run_deletion.apply_async(
+            kwargs={'deletion_id': deletion_id},
+            countdown=15,
+        )
+    deletion.delete()
+
 
-    has_more = delete_objects(
-        model_list,
-        transaction_id=transaction_id,
-        relation={'organization_id': o.id},
-        logger=logger,
+@instrumented_task(name='sentry.tasks.deletion.revoke_api_tokens', queue='cleanup',
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
+@retry(exclude=(DeleteAborted,))
+def revoke_api_tokens(object_id, transaction_id=None, continuous=True,
+                      timestamp=None, **kwargs):
+    from sentry.models import ApiToken
+
+    queryset = ApiToken.objects.filter(
+        application=object_id,
     )
-    if has_more:
-        if continuous:
-            delete_organization.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
-            )
+    if timestamp:
+        queryset = queryset.filter(date_added__lte=timestamp)
+
+    # we're using a slow deletion strategy to avoid a lot of custom code for
+    # mysql/postgres
+    has_more = False
+    for obj in queryset[:1000]:
+        obj.delete()
+        has_more = True
+
+    if has_more and continuous:
+        revoke_api_tokens.apply_async(
+            kwargs={
+                'object_id': object_id,
+                'transaction_id': transaction_id,
+                'timestamp': timestamp,
+            },
+            countdown=15,
+        )
+    return has_more
+
+
+@instrumented_task(name='sentry.tasks.deletion.delete_organization', queue='cleanup',
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
+@retry(exclude=(DeleteAborted,))
+def delete_organization(object_id, transaction_id=None, continuous=True, **kwargs):
+    from sentry import deletions
+    from sentry.models import Organization, OrganizationStatus
+
+    try:
+        instance = Organization.objects.get(id=object_id)
+    except Organization.DoesNotExist:
         return
-    o_id = o.id
-    o.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': o_id,
-        'transaction_id': transaction_id,
-        'model': Organization.__name__,
-    })
+
+    if instance.status == OrganizationStatus.VISIBLE:
+        raise DeleteAborted
+
+    task = deletions.get(
+        model=Organization,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
+    )
+    has_more = task.chunk()
+    if has_more:
+        delete_organization.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_team', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_team(object_id, transaction_id=None, continuous=True, **kwargs):
-    from sentry.models import Team, TeamStatus, Project, ProjectStatus
+    from sentry import deletions
+    from sentry.models import Team, TeamStatus
 
     try:
-        t = Team.objects.get(id=object_id)
+        instance = Team.objects.get(id=object_id)
     except Team.DoesNotExist:
         return
 
-    if t.status == TeamStatus.VISIBLE:
-        raise DeleteAborted('Aborting team deletion as status is invalid')
-
-    if t.status != TeamStatus.DELETION_IN_PROGRESS:
-        pending_delete.send(sender=Team, instance=t)
-        t.update(status=TeamStatus.DELETION_IN_PROGRESS)
-
-    # Delete 1 project at a time since this is expensive by itself
-    for project in Project.objects.filter(team=t).order_by('id')[:1]:
-        project.update(status=ProjectStatus.DELETION_IN_PROGRESS)
-        delete_project(project.id, transaction_id=transaction_id, continuous=False)
-        if continuous:
-            delete_team.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
-            )
-        return
+    if instance.status == TeamStatus.VISIBLE:
+        raise DeleteAborted
 
-    t_id = t.id
-    t.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': t_id,
-        'transaction_id': transaction_id,
-        'model': Team.__name__,
-    })
+    task = deletions.get(
+        model=Team,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
+    )
+    has_more = task.chunk()
+    if has_more:
+        delete_team.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_project', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_project(object_id, transaction_id=None, continuous=True, **kwargs):
-    from sentry.models import (
-        Activity, EventMapping, EventUser, Group, GroupAssignee, GroupBookmark,
-        GroupEmailThread, GroupHash, GroupMeta, GroupRelease, GroupResolution,
-        GroupRuleStatus, GroupSeen, GroupSubscription, GroupSnooze, GroupTagKey,
-        GroupTagValue, Project, ProjectBookmark, ProjectKey, ProjectStatus,
-        ReleaseProject, SavedSearchUserDefault, SavedSearch,
-        TagKey, TagValue, UserReport, EnvironmentProject
-    )
+    from sentry import deletions
+    from sentry.models import Project, ProjectStatus
 
     try:
-        p = Project.objects.get(id=object_id)
+        instance = Project.objects.get(id=object_id)
     except Project.DoesNotExist:
         return
 
-    if p.status == ProjectStatus.VISIBLE:
-        raise DeleteAborted('Aborting project deletion as status is invalid')
-
-    if p.status != ProjectStatus.DELETION_IN_PROGRESS:
-        pending_delete.send(sender=Project, instance=p)
-        p.update(status=ProjectStatus.DELETION_IN_PROGRESS)
-
-    # Immediately revoke keys
-    project_keys = list(ProjectKey.objects.filter(project_id=object_id).values_list('id', flat=True))
-    ProjectKey.objects.filter(project_id=object_id).delete()
-    for key_id in project_keys:
-        logger.info('object.delete.executed', extra={
-            'object_id': key_id,
-            'transaction_id': transaction_id,
-            'model': ProjectKey.__name__,
-        })
-
-    model_list = (
-        Activity, EventMapping, EventUser, GroupAssignee, GroupBookmark,
-        GroupEmailThread, GroupHash, GroupRelease, GroupRuleStatus, GroupSeen,
-        GroupSubscription, GroupTagKey, GroupTagValue, ProjectBookmark,
-        ProjectKey, TagKey, TagValue, SavedSearchUserDefault, SavedSearch,
-        UserReport, EnvironmentProject
-    )
-    for model in model_list:
-        has_more = bulk_delete_objects(model, project_id=p.id, transaction_id=transaction_id, logger=logger)
-        if has_more:
-            if continuous:
-                delete_project.apply_async(
-                    kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                    countdown=15,
-                )
-            return
-
-    # TODO(dcramer): no project relation so we cant easily bulk
-    # delete today
-    has_more = delete_objects([GroupMeta, GroupResolution, GroupSnooze],
-                              relation={'group__project': p},
-                              transaction_id=transaction_id,
-                              logger=logger)
-    if has_more:
-        if continuous:
-            delete_project.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
-            )
-        return
+    if instance.status == ProjectStatus.VISIBLE:
+        raise DeleteAborted
 
-    has_more = delete_events(relation={'project_id': p.id}, transaction_id=transaction_id, logger=logger)
+    task = deletions.get(
+        model=Project,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
+    )
+    has_more = task.chunk()
     if has_more:
-        if continuous:
-            delete_project.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-            )
-        return
-
-    # Release needs to handle deletes after Group is cleaned up as the foreign
-    # key is protected
-    model_list = (Group, ReleaseProject)
-    for model in model_list:
-        has_more = bulk_delete_objects(model, project_id=p.id, transaction_id=transaction_id, logger=logger)
-        if has_more:
-            if continuous:
-                delete_project.apply_async(
-                    kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                    countdown=15,
-                )
-            return
-
-    p_id = p.id
-    p.delete()
-    logger.info('object.delete.queued', extra={
-        'object_id': p_id,
-        'transaction_id': transaction_id,
-        'model': Project.__name__,
-    })
+        delete_project.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_group', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_group(object_id, transaction_id=None, continuous=True, **kwargs):
-    from sentry.models import (
-        EventMapping, Group, GroupAssignee, GroupBookmark, GroupCommitResolution,
-        GroupHash, GroupMeta, GroupRelease, GroupResolution, GroupRuleStatus,
-        GroupSnooze, GroupSubscription, GroupStatus, GroupTagKey, GroupTagValue,
-        GroupEmailThread, GroupRedirect, UserReport
-    )
-
-    try:
-        group = Group.objects.get(id=object_id)
-    except Group.DoesNotExist:
-        return
-
-    if group.status != GroupStatus.DELETION_IN_PROGRESS:
-        group.update(status=GroupStatus.DELETION_IN_PROGRESS)
-
-    bulk_model_list = (
-        # prioritize GroupHash
-        GroupHash, GroupAssignee, GroupCommitResolution, GroupBookmark,
-        GroupMeta, GroupRelease, GroupResolution, GroupRuleStatus, GroupSnooze,
-        GroupTagValue, GroupTagKey, EventMapping, GroupEmailThread, GroupRedirect,
-        GroupSubscription, UserReport,
+    from sentry import deletions
+    from sentry.models import Group
+
+    task = deletions.get(
+        model=Group,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
     )
-    for model in bulk_model_list:
-        has_more = bulk_delete_objects(model, group_id=object_id, logger=logger)
-        if has_more:
-            if continuous:
-                delete_group.apply_async(
-                    kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                    countdown=15,
-                )
-            return
-
-    has_more = delete_events(relation={'group_id': object_id}, logger=logger)
+    has_more = task.chunk()
     if has_more:
-        if continuous:
-            delete_group.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-            )
-        return
-
-    features.delete(group)
-    g_id = group.id
-    group.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': g_id,
-        'transaction_id': transaction_id,
-        'model': Group.__name__,
-    })
+        delete_group.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_tag_key', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_tag_key(object_id, transaction_id=None, continuous=True, **kwargs):
-    from sentry.models import (
-        EventTag, GroupTagKey, GroupTagValue, TagKey, TagKeyStatus, TagValue
-    )
-
-    try:
-        tagkey = TagKey.objects.get(id=object_id)
-    except TagKey.DoesNotExist:
-        return
-
-    if tagkey.status != TagKeyStatus.DELETION_IN_PROGRESS:
-        tagkey.update(status=TagKeyStatus.DELETION_IN_PROGRESS)
-
-    bulk_model_list = (
-        GroupTagValue, GroupTagKey, TagValue
+    from sentry import deletions
+    from sentry.models import TagKey
+
+    task = deletions.get(
+        model=TagKey,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
     )
-    for model in bulk_model_list:
-        has_more = bulk_delete_objects(model, project_id=tagkey.project_id,
-                                       key=tagkey.key, logger=logger)
-        if has_more:
-            if continuous:
-                delete_tag_key.apply_async(
-                    kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                    countdown=15,
-                )
-            return
-
-    has_more = bulk_delete_objects(EventTag, project_id=tagkey.project_id,
-                                   key_id=tagkey.id, logger=logger)
+    has_more = task.chunk()
     if has_more:
-        if continuous:
-            delete_tag_key.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
-            )
-        return
-
-    tagkey_id = tagkey.id
-    tagkey.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': tagkey_id,
-        'transaction_id': transaction_id,
-        'model': TagKey.__name__,
-    })
+        delete_tag_key.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_api_application', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_api_application(object_id, transaction_id=None, continuous=True,
                            **kwargs):
-    from sentry.models import ApiApplication, ApiApplicationStatus, ApiGrant
+    from sentry import deletions
+    from sentry.models import ApiApplication, ApiApplicationStatus
 
     try:
-        app = ApiApplication.objects.get(id=object_id)
+        instance = ApiApplication.objects.get(id=object_id)
     except ApiApplication.DoesNotExist:
         return
 
-    if app.status == ApiApplicationStatus.active:
+    if instance.status == ApiApplicationStatus.active:
         raise DeleteAborted
 
-    if app.status != ApiApplicationStatus.deletion_in_progress:
-        app.update(status=ApiApplicationStatus.deletion_in_progress)
-
-    has_more = revoke_api_tokens(object_id)
-    if has_more:
-        if continuous:
-            delete_api_application.apply_async(
-                kwargs={
-                    'object_id': object_id,
-                    'transaction_id': transaction_id,
-                },
-                countdown=15,
-            )
-        return
-
-    bulk_model_list = (ApiGrant,)
-    for model in bulk_model_list:
-        has_more = bulk_delete_objects(model, application_id=app.id,
-                                       logger=logger)
-        if has_more:
-            if continuous:
-                delete_api_application.apply_async(
-                    kwargs={
-                        'object_id': object_id,
-                        'transaction_id': transaction_id,
-                    },
-                    countdown=15,
-                )
-            return
-
-    app.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': object_id,
-        'transaction_id': transaction_id,
-        'model': ApiApplication.__name__,
-    })
-
-
-@instrumented_task(name='sentry.tasks.deletion.revoke_api_tokens', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
-@retry(exclude=(DeleteAborted,))
-def revoke_api_tokens(object_id, transaction_id=None, continuous=True,
-                      timestamp=None, **kwargs):
-    from sentry.models import ApiToken
-
-    queryset = ApiToken.objects.filter(
-        application=object_id,
+    task = deletions.get(
+        model=ApiApplication,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
     )
-    if timestamp:
-        queryset = queryset.filter(date_added__lte=timestamp)
-
-    # we're using a slow deletion strategy to avoid a lot of custom code for
-    # mysql/postgres
-    has_more = False
-    for obj in queryset[:1000]:
-        obj.delete()
-        has_more = True
-
-    if has_more and continuous:
-        revoke_api_tokens.apply_async(
-            kwargs={
-                'object_id': object_id,
-                'transaction_id': transaction_id,
-                'timestamp': timestamp,
-            },
+    has_more = task.chunk()
+    if has_more:
+        delete_api_application.apply_async(
+            kwargs={'object_id': object_id, 'transaction_id': transaction_id},
             countdown=15,
         )
-    return has_more
 
 
 @instrumented_task(name='sentry.tasks.deletion.generic_delete', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def generic_delete(app_label, model_name, object_id, transaction_id=None,
                    continuous=True, actor_id=None, **kwargs):
-    from sentry.models import User
+    from sentry import deletions
 
     model = get_model(app_label, model_name)
 
@@ -422,131 +301,59 @@ def generic_delete(app_label, model_name, object_id, transaction_id=None,
     if instance.status == ObjectStatus.VISIBLE:
         raise DeleteAborted
 
-    if instance.status == ObjectStatus.PENDING_DELETION:
-        if actor_id:
-            actor = User.objects.get(id=actor_id)
-        else:
-            actor = None
-        instance.update(status=ObjectStatus.DELETION_IN_PROGRESS)
-        pending_delete.send(sender=model, instance=instance, actor=actor)
-
-    # TODO(dcramer): it'd be nice if we could collect relations here and
-    # cascade efficiently
-    instance_id = instance.id
-    instance.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': instance_id,
-        'transaction_id': transaction_id,
-        'model': model.__name__,
-    })
-
-
-def delete_events(relation, transaction_id=None, limit=10000, chunk_limit=100, logger=None):
-    from sentry.models import Event, EventTag
-
-    while limit > 0:
-        result_set = list(Event.objects.filter(**relation)[:chunk_limit])
-        if not bool(result_set):
-            return False
-
-        # delete objects from nodestore first
-        node_ids = set(r.data.id for r in result_set if r.data.id)
-        if node_ids:
-            nodestore.delete_multi(node_ids)
-
-        event_ids = [r.id for r in result_set]
-
-        # bulk delete by id
-        EventTag.objects.filter(event_id__in=event_ids).delete()
-        if logger is not None:
-            # The only reason this is a different log statement is that logging every
-            # single event that gets deleted in the relation will destroy disks.
-            logger.info('object.delete.bulk_executed', extra=dict(
-                relation.items() + [
-                    ('transaction_id', transaction_id),
-                    ('model', 'EventTag'),
-                ],
-            ))
-
-        # bulk delete by id
-        Event.objects.filter(id__in=event_ids).delete()
-        if logger is not None:
-            # The only reason this is a different log statement is that logging every
-            # single event that gets deleted in the relation will destroy disks.
-            logger.info('object.delete.bulk_executed', extra=dict(
-                relation.items() + [
-                    ('transaction_id', transaction_id),
-                    ('model', 'Event'),
-                ],
-            ))
-
-        limit -= chunk_limit
-
-    return True
-
-
-def delete_objects(models, relation, transaction_id=None, limit=100, logger=None):
-    # This handles cascades properly
-    has_more = False
-    for model in models:
-        for obj in model.objects.filter(**relation)[:limit]:
-            obj_id = obj.id
-            model_name = type(obj).__name__
-            obj.delete()
-            if logger is not None:
-                logger.info('object.delete.executed', extra={
-                    'object_id': obj_id,
-                    'transaction_id': transaction_id,
-                    'model': model_name,
-                })
-            has_more = True
-
-        if has_more:
-            return True
-    return has_more
+    task = deletions.get(
+        model=model,
+        actor_id=actor_id,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
+    )
+    has_more = task.chunk()
+    if has_more:
+        generic_delete.apply_async(
+            kwargs={
+                'app_label': app_label,
+                'model_name': model_name,
+                'object_id': object_id,
+                'transaction_id': transaction_id,
+                'actor_id': actor_id,
+            },
+            countdown=15,
+        )
 
 
 @instrumented_task(name='sentry.tasks.deletion.delete_repository', queue='cleanup',
-                   default_retry_delay=60 * 5, max_retries=None)
+                   default_retry_delay=60 * 5, max_retries=MAX_RETRIES)
 @retry(exclude=(DeleteAborted,))
 def delete_repository(object_id, transaction_id=None, continuous=True,
                       actor_id=None, **kwargs):
-    from sentry.models import Commit, Repository, User
+    from sentry import deletions
+    from sentry.models import Repository
 
     try:
-        repo = Repository.objects.get(id=object_id)
+        instance = Repository.objects.get(id=object_id)
     except Repository.DoesNotExist:
         return
 
-    if repo.status == ObjectStatus.VISIBLE:
+    if instance.status == ObjectStatus.VISIBLE:
         raise DeleteAborted
 
-    if repo.status == ObjectStatus.PENDING_DELETION:
-        if actor_id:
-            actor = User.objects.get(id=actor_id)
-        else:
-            actor = None
-        repo.update(status=ObjectStatus.DELETION_IN_PROGRESS)
-        pending_delete.send(sender=Repository, instance=repo, actor=actor)
-
-    has_more = delete_objects(
-        (Commit,),
-        transaction_id=transaction_id,
-        relation={'repository_id': repo.id},
-        logger=logger,
+    task = deletions.get(
+        model=Repository,
+        actor_id=actor_id,
+        query={
+            'id': object_id,
+        },
+        transaction_id=transaction_id or uuid4().hex,
     )
+    has_more = task.chunk()
     if has_more:
-        if continuous:
-            delete_repository.apply_async(
-                kwargs={'object_id': object_id, 'transaction_id': transaction_id},
-                countdown=15,
-            )
-        return
-
-    repo_id = repo.id
-    repo.delete()
-    logger.info('object.delete.executed', extra={
-        'object_id': repo_id,
-        'transaction_id': transaction_id,
-        'model': Repository.__name__,
-    })
+        delete_repository.apply_async(
+            kwargs={
+                'object_id': object_id,
+                'transaction_id': transaction_id,
+                'actor_id': actor_id,
+            },
+            countdown=15,
+        )
diff --git a/tests/sentry/deletions/test_apiapplication.py b/tests/sentry/deletions/test_apiapplication.py
new file mode 100644
index 0000000000..47a0549343
--- /dev/null
+++ b/tests/sentry/deletions/test_apiapplication.py
@@ -0,0 +1,33 @@
+from __future__ import absolute_import
+
+from sentry.models import ApiApplication, ApiGrant, ApiToken, ScheduledDeletion
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteApiApplicationTest(TestCase):
+    def test_simple(self):
+        app = ApiApplication.objects.create(
+            owner=self.user,
+        )
+        ApiToken.objects.create(
+            application=app,
+            user=self.user,
+            scopes=0,
+        )
+        ApiGrant.objects.create(
+            application=app,
+            user=self.user,
+            scopes=0,
+            redirect_uri='http://example.com',
+        )
+
+        deletion = ScheduledDeletion.schedule(app, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not ApiApplication.objects.filter(id=app.id).exists()
+        assert not ApiGrant.objects.filter(application=app).exists()
+        assert not ApiToken.objects.filter(application=app).exists()
diff --git a/tests/sentry/deletions/test_group.py b/tests/sentry/deletions/test_group.py
new file mode 100644
index 0000000000..15d7ae76e2
--- /dev/null
+++ b/tests/sentry/deletions/test_group.py
@@ -0,0 +1,65 @@
+from __future__ import absolute_import
+
+from uuid import uuid4
+
+from sentry.models import (
+    Event, EventMapping, EventTag, Group, GroupAssignee, GroupHash, GroupMeta,
+    GroupRedirect, ScheduledDeletion
+)
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteGroupTest(TestCase):
+    def test_simple(self):
+        project = self.create_project()
+        group = self.create_group(
+            project=project,
+        )
+        event = self.create_event(group=group)
+        EventMapping.objects.create(
+            project_id=project.id,
+            event_id='a' * 32,
+            group_id=group.id,
+        )
+        EventTag.objects.create(
+            event_id=event.id,
+            project_id=project.id,
+            key_id=1,
+            value_id=1,
+        )
+        GroupAssignee.objects.create(
+            group=group,
+            project=project,
+            user=self.user,
+        )
+        GroupHash.objects.create(
+            project=project,
+            group=group,
+            hash=uuid4().hex,
+        )
+        GroupMeta.objects.create(
+            group=group,
+            key='foo',
+            value='bar',
+        )
+        GroupRedirect.objects.create(
+            group_id=group.id,
+            previous_group_id=1,
+        )
+
+        deletion = ScheduledDeletion.schedule(group, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not Event.objects.filter(id=event.id).exists()
+        assert not EventMapping.objects.filter(
+            event_id='a' * 32,
+            group_id=group.id,
+        ).exists()
+        assert not EventTag.objects.filter(event_id=event.id).exists()
+        assert not GroupRedirect.objects.filter(group_id=group.id).exists()
+        assert not GroupHash.objects.filter(group_id=group.id).exists()
+        assert not Group.objects.filter(id=group.id).exists()
diff --git a/tests/sentry/deletions/test_organization.py b/tests/sentry/deletions/test_organization.py
new file mode 100644
index 0000000000..7a131acfcf
--- /dev/null
+++ b/tests/sentry/deletions/test_organization.py
@@ -0,0 +1,70 @@
+from __future__ import absolute_import
+
+from sentry.models import (
+    Commit, CommitAuthor, Environment, Organization, Release, ReleaseCommit,
+    ReleaseEnvironment, Repository, ScheduledDeletion
+)
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteOrganizationTest(TestCase):
+    def test_simple(self):
+        org = self.create_organization(
+            name='test',
+        )
+        org2 = self.create_organization(name='test2')
+        self.create_team(organization=org, name='test1')
+        self.create_team(organization=org, name='test2')
+        release = Release.objects.create(version='a' * 32,
+                                         organization_id=org.id)
+        repo = Repository.objects.create(
+            organization_id=org.id,
+            name=org.name,
+        )
+        commit_author = CommitAuthor.objects.create(
+            organization_id=org.id,
+            name='foo',
+            email='foo@example.com',
+        )
+        commit = Commit.objects.create(
+            repository_id=repo.id,
+            organization_id=org.id,
+            author=commit_author,
+            key='a' * 40,
+        )
+        ReleaseCommit.objects.create(
+            organization_id=org.id,
+            release=release,
+            commit=commit,
+            order=0,
+        )
+
+        env = Environment.objects.create(
+            organization_id=org.id,
+            project_id=4,
+            name='foo'
+        )
+        release_env = ReleaseEnvironment.objects.create(
+            organization_id=org.id,
+            project_id=4,
+            release_id=release.id,
+            environment_id=env.id
+        )
+
+        deletion = ScheduledDeletion.schedule(org, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert Organization.objects.filter(id=org2.id).exists()
+
+        assert not Organization.objects.filter(id=org.id).exists()
+        assert not Environment.objects.filter(id=env.id).exists()
+        assert not ReleaseEnvironment.objects.filter(id=release_env.id).exists()
+        assert not Repository.objects.filter(id=repo.id).exists()
+        assert not ReleaseCommit.objects.filter(organization_id=org.id).exists()
+        assert not Release.objects.filter(organization_id=org.id).exists()
+        assert not CommitAuthor.objects.filter(id=commit_author.id).exists()
+        assert not Commit.objects.filter(id=commit.id).exists()
diff --git a/tests/sentry/deletions/test_project.py b/tests/sentry/deletions/test_project.py
new file mode 100644
index 0000000000..94cd78b9e7
--- /dev/null
+++ b/tests/sentry/deletions/test_project.py
@@ -0,0 +1,67 @@
+from __future__ import absolute_import
+
+from sentry.models import (
+    Commit, CommitAuthor, Environment, EnvironmentProject, GroupAssignee,
+    GroupMeta, GroupResolution, Project, Release, ReleaseCommit, Repository,
+    ScheduledDeletion
+)
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteProjectTest(TestCase):
+    def test_simple(self):
+        project = self.create_project(
+            name='test',
+        )
+        group = self.create_group(project=project)
+        GroupAssignee.objects.create(group=group, project=project, user=self.user)
+        GroupMeta.objects.create(group=group, key='foo', value='bar')
+        release = Release.objects.create(version='a' * 32,
+                                         organization_id=project.organization_id)
+        release.add_project(project)
+        GroupResolution.objects.create(group=group, release=release)
+        env = Environment.objects.create(
+            organization_id=project.organization_id,
+            project_id=project.id,
+            name='foo'
+        )
+        env.add_project(project)
+        repo = Repository.objects.create(
+            organization_id=project.organization_id,
+            name=project.name,
+        )
+        commit_author = CommitAuthor.objects.create(
+            organization_id=project.organization_id,
+            name='foo',
+            email='foo@example.com',
+        )
+        commit = Commit.objects.create(
+            repository_id=repo.id,
+            organization_id=project.organization_id,
+            author=commit_author,
+            key='a' * 40,
+        )
+        ReleaseCommit.objects.create(
+            organization_id=project.organization_id,
+            project_id=project.id,
+            release=release,
+            commit=commit,
+            order=0,
+        )
+
+        deletion = ScheduledDeletion.schedule(project, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not Project.objects.filter(id=project.id).exists()
+        assert not EnvironmentProject.objects.filter(
+            project_id=project.id,
+            environment_id=env.id
+        ).exists()
+        assert Environment.objects.filter(id=env.id).exists()
+        assert Release.objects.filter(id=release.id).exists()
+        assert ReleaseCommit.objects.filter(release_id=release.id).exists()
+        assert Commit.objects.filter(id=commit.id).exists()
diff --git a/tests/sentry/deletions/test_repository.py b/tests/sentry/deletions/test_repository.py
new file mode 100644
index 0000000000..251389cdfe
--- /dev/null
+++ b/tests/sentry/deletions/test_repository.py
@@ -0,0 +1,40 @@
+from __future__ import absolute_import
+
+from sentry.models import Commit, Repository, ScheduledDeletion
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteRepositoryTest(TestCase):
+    def test_simple(self):
+        org = self.create_organization()
+        repo = Repository.objects.create(
+            organization_id=org.id,
+            provider='dummy',
+            name='example/example',
+        )
+        repo2 = Repository.objects.create(
+            organization_id=org.id,
+            provider='dummy',
+            name='example/example2',
+        )
+        commit = Commit.objects.create(
+            repository_id=repo.id,
+            organization_id=org.id,
+            key='1234abcd',
+        )
+        commit2 = Commit.objects.create(
+            repository_id=repo2.id,
+            organization_id=org.id,
+            key='1234abcd',
+        )
+
+        deletion = ScheduledDeletion.schedule(repo, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not Repository.objects.filter(id=repo.id).exists()
+        assert not Commit.objects.filter(id=commit.id).exists()
+        assert Commit.objects.filter(id=commit2.id).exists()
diff --git a/tests/sentry/deletions/test_tagkey.py b/tests/sentry/deletions/test_tagkey.py
new file mode 100644
index 0000000000..c11ff6d209
--- /dev/null
+++ b/tests/sentry/deletions/test_tagkey.py
@@ -0,0 +1,49 @@
+from __future__ import absolute_import
+
+from sentry.models import (
+    EventTag, GroupTagKey, GroupTagValue, ScheduledDeletion, TagKey, TagValue
+)
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteTagKeyTest(TestCase):
+    def test_simple(self):
+        team = self.create_team(name='test', slug='test')
+        project = self.create_project(team=team, name='test1', slug='test1')
+        group = self.create_group(project=project)
+        tk = TagKey.objects.create(key='foo', project=project)
+        TagValue.objects.create(key='foo', value='bar', project=project)
+        GroupTagKey.objects.create(key='foo', group=group, project=project)
+        GroupTagValue.objects.create(key='foo', value='bar', group_id=group.id, project_id=project.id)
+        EventTag.objects.create(
+            key_id=tk.id, group_id=group.id, value_id=1, project_id=project.id,
+            event_id=1,
+        )
+
+        project2 = self.create_project(team=team, name='test2')
+        group2 = self.create_group(project=project2)
+        tk2 = TagKey.objects.create(key='foo', project=project2)
+        gtk2 = GroupTagKey.objects.create(key='foo', group=group2, project=project2)
+        gtv2 = GroupTagValue.objects.create(key='foo', value='bar', group_id=group2.id, project_id=project2.id)
+        EventTag.objects.create(
+            key_id=tk2.id, group_id=group2.id, value_id=1, project_id=project.id,
+            event_id=1,
+        )
+
+        deletion = ScheduledDeletion.schedule(tk, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not GroupTagValue.objects.filter(key=tk.key, project_id=project.id).exists()
+        assert not GroupTagKey.objects.filter(key=tk.key, project=project).exists()
+        assert not TagValue.objects.filter(key=tk.key, project=project).exists()
+        assert not TagKey.objects.filter(id=tk.id).exists()
+        assert not EventTag.objects.filter(key_id=tk.id).exists()
+
+        assert TagKey.objects.filter(id=tk2.id).exists()
+        assert GroupTagKey.objects.filter(id=gtk2.id).exists()
+        assert GroupTagValue.objects.filter(id=gtv2.id).exists()
+        assert EventTag.objects.filter(key_id=tk2.id).exists()
diff --git a/tests/sentry/deletions/test_team.py b/tests/sentry/deletions/test_team.py
new file mode 100644
index 0000000000..5d00168a82
--- /dev/null
+++ b/tests/sentry/deletions/test_team.py
@@ -0,0 +1,22 @@
+from __future__ import absolute_import
+
+from sentry.models import ScheduledDeletion, Team
+from sentry.tasks.deletion import run_deletion
+from sentry.testutils import TestCase
+
+
+class DeleteTeamTest(TestCase):
+    def test_simple(self):
+        team = self.create_team(
+            name='test',
+        )
+        self.create_project(team=team, name='test1')
+        self.create_project(team=team, name='test2')
+
+        deletion = ScheduledDeletion.schedule(team, days=0)
+        deletion.update(in_progress=True)
+
+        with self.tasks():
+            run_deletion(deletion.id)
+
+        assert not Team.objects.filter(id=team.id).exists()
diff --git a/tests/sentry/tasks/test_deletion.py b/tests/sentry/tasks/test_deletion.py
index 39ac365866..3c06117b0b 100644
--- a/tests/sentry/tasks/test_deletion.py
+++ b/tests/sentry/tasks/test_deletion.py
@@ -1,6 +1,7 @@
 from __future__ import absolute_import
 
 from datetime import datetime, timedelta
+from uuid import uuid4
 
 import pytest
 
@@ -9,14 +10,15 @@ from sentry.exceptions import DeleteAborted
 from sentry.models import (
     ApiApplication, ApiApplicationStatus, ApiGrant, ApiToken, Commit,
     CommitAuthor, Environment, EnvironmentProject, Event, EventMapping,
-    EventTag, Group, GroupAssignee, GroupMeta, GroupRedirect, GroupResolution,
-    GroupStatus, GroupTagKey, GroupTagValue, Organization, OrganizationStatus,
-    Project, ProjectStatus, Release, ReleaseCommit, ReleaseEnvironment,
-    Repository, TagKey, TagValue, Team, TeamStatus
+    EventTag, Group, GroupAssignee, GroupHash, GroupMeta, GroupRedirect,
+    GroupResolution, GroupStatus, GroupTagKey, GroupTagValue, Organization,
+    OrganizationStatus, Project, ProjectStatus, Release, ReleaseCommit,
+    ReleaseEnvironment, Repository, TagKey, TagValue, Team, TeamStatus
 )
 from sentry.tasks.deletion import (
     delete_api_application, delete_group, delete_organization, delete_project,
-    delete_repository, delete_tag_key, delete_team, generic_delete, revoke_api_tokens
+    delete_repository, delete_tag_key, delete_team, generic_delete,
+    revoke_api_tokens
 )
 from sentry.testutils import TestCase
 
@@ -218,8 +220,8 @@ class DeleteTagKeyTest(TestCase):
             assert not GroupTagValue.objects.filter(key=tk.key, project_id=project.id).exists()
             assert not GroupTagKey.objects.filter(key=tk.key, project=project).exists()
             assert not TagValue.objects.filter(key=tk.key, project=project).exists()
-            assert not TagKey.objects.filter(id=tk.id).exists()
             assert not EventTag.objects.filter(key_id=tk.id).exists()
+            assert not TagKey.objects.filter(id=tk.id).exists()
 
         assert TagKey.objects.filter(id=tk2.id).exists()
         assert GroupTagKey.objects.filter(id=gtk2.id).exists()
@@ -251,6 +253,11 @@ class DeleteGroupTest(TestCase):
             project=project,
             user=self.user,
         )
+        GroupHash.objects.create(
+            project=project,
+            group=group,
+            hash=uuid4().hex,
+        )
         GroupMeta.objects.create(
             group=group,
             key='foo',
@@ -264,7 +271,6 @@ class DeleteGroupTest(TestCase):
         with self.tasks():
             delete_group(object_id=group.id)
 
-        assert not Group.objects.filter(id=group.id).exists()
         assert not Event.objects.filter(id=event.id).exists()
         assert not EventMapping.objects.filter(
             event_id='a' * 32,
@@ -272,6 +278,8 @@ class DeleteGroupTest(TestCase):
         ).exists()
         assert not EventTag.objects.filter(event_id=event.id).exists()
         assert not GroupRedirect.objects.filter(group_id=group.id).exists()
+        assert not GroupHash.objects.filter(group_id=group.id).exists()
+        assert not Group.objects.filter(id=group.id).exists()
 
 
 class DeleteApplicationTest(TestCase):
