commit 65509f9a53b33d53bdf93cbd062304138974f39b
Author: Ted Kaemming <ted@kaemming.com>
Date:   Mon Sep 28 18:26:06 2015 -0700

    Add simple implementations of digest tasks.

diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 4ac4a8afd4..8ca1170098 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -368,6 +368,7 @@ CELERY_IMPORTS = (
     'sentry.tasks.beacon',
     'sentry.tasks.check_auth',
     'sentry.tasks.deletion',
+    'sentry.tasks.digests',
     'sentry.tasks.email',
     'sentry.tasks.index',
     'sentry.tasks.merge',
@@ -388,6 +389,8 @@ CELERY_QUEUES = [
     Queue('update', routing_key='update'),
     Queue('email', routing_key='email'),
     Queue('options', routing_key='options'),
+    Queue('digests.delivery', routing_key='digests.delivery'),
+    Queue('digests.scheduling', routing_key='digests.scheduling'),
 ]
 
 CELERY_ROUTES = ('sentry.queue.routers.SplitQueueRouter',)
@@ -452,6 +455,13 @@ CELERYBEAT_SCHEDULE = {
             'queue': 'options',
         }
     },
+    'schedule-digests': {
+        'task': 'sentry.tasks.digests.schedule_digests',
+        'schedule': timedelta(seconds=15),
+        'options': {
+            'expires': 15,
+        },
+    }
 }
 
 LOGGING = {
diff --git a/src/sentry/digests/backends/redis.py b/src/sentry/digests/backends/redis.py
index fef82a554b..72c7a597bf 100644
--- a/src/sentry/digests/backends/redis.py
+++ b/src/sentry/digests/backends/redis.py
@@ -213,8 +213,56 @@ class RedisBackend(Backend):
                 else:
                     raise RuntimeError('loop exceeded maximum iterations (%s)' % (maximum_iterations,))
 
-    def maintenance(self, deadline):
-        raise NotImplementedError
+    def maintenance(self, deadline, chunk=1000):
+        # XXX: This is not the final iteration of this, and will need changes
+        # before actual use!
+
+        # TODO: Balancing.
+        for host in self.cluster.hosts:
+            connection = self.cluster.get_local_client(host)
+
+            # TODO: This needs to respect locks!
+            maximum_iterations = 1000
+            for i in xrange(maximum_iterations):
+                items = connection.zrangebyscore(
+                    make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
+                    min=0,
+                    max=deadline,
+                    withscores=True,
+                    start=0,
+                    num=chunk,
+                )
+
+                # XXX: Redis will error if we try and execute an empty
+                # transaction. If there are no items to move between states, we
+                # need to exit the loop now. (This can happen on the first
+                # iteration of the loop if there is nothing to do, or on a
+                # subsequent iteration if there was exactly the same number of
+                # items to change states as the chunk size.)
+                if not items:
+                    break
+
+                with connection.pipeline() as pipeline:
+                    pipeline.multi()
+
+                    pipeline.zrem(
+                        make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
+                        *[key for key, timestamp in items]
+                    )
+
+                    pipeline.zadd(
+                        make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
+                        *itertools.chain.from_iterable([(timestamp, key) for (key, timestamp) in items])
+                    )
+
+                    pipeline.execute()
+
+                # If we retrieved less than the chunk size of items, we don't
+                # need try to retrieve more items.
+                if len(items) < chunk:
+                    break
+            else:
+                raise RuntimeError('loop exceeded maximum iterations (%s)' % (maximum_iterations,))
 
     @contextmanager
     def digest(self, key):
diff --git a/src/sentry/tasks/digests.py b/src/sentry/tasks/digests.py
new file mode 100644
index 0000000000..1c74f958ef
--- /dev/null
+++ b/src/sentry/tasks/digests.py
@@ -0,0 +1,40 @@
+from __future__ import absolute_import
+
+import time
+
+from sentry.digests.notifications import (
+    build_digest,
+    split_key,
+)
+from sentry.tasks.base import instrumented_task
+
+
+@instrumented_task(
+    name='sentry.tasks.digests.schedule_digests')
+def schedule_digests():
+    from sentry.app import digests
+
+    deadline = time.time()
+    timeout = 30  # TODO: Make this a setting, it also should match task expiration.
+
+    # TODO: This might make sense to make probabilistic instead?
+    digests.maintenance(deadline - timeout)
+
+    # TODO: Monitor schedule latency (deadline - schedule time).
+    deadline = time.time()
+    for entry in digests.schedule(deadline):
+        # TODO: Pass through schedule time so we can monitor total lateny.
+        deliver_digest.delay(entry.key)
+
+
+@instrumented_task(
+    name='sentry.tasks.digests.deliver_digest')
+def deliver_digest(key):
+    from sentry.app import digests
+
+    plugin, project = split_key(key)
+    with digests.digest(key) as records:
+        digest = build_digest(project, records)
+
+    # XXX: This won't actually work right now.
+    plugin.notify(digest)
