commit 80db27ff55cddb5730a66d2569335a0efa205fe0
Author: David Cramer <dcramer@gmail.com>
Date:   Sun Dec 21 10:12:19 2014 -0800

    Optimize bulk deletion to use LIMIT

diff --git a/src/sentry/tasks/deletion.py b/src/sentry/tasks/deletion.py
index 771199b96d..57e558f401 100644
--- a/src/sentry/tasks/deletion.py
+++ b/src/sentry/tasks/deletion.py
@@ -10,6 +10,9 @@ from __future__ import absolute_import
 
 from sentry.tasks.base import instrumented_task, retry
 
+from django.db import connections
+from sentry.utils import db
+
 
 @instrumented_task(name='sentry.tasks.deletion.delete_organization', queue='cleanup',
                    default_retry_delay=60 * 5, max_retries=None)
@@ -96,9 +99,17 @@ def delete_project(object_id, **kwargs):
 
     logger = delete_project.get_logger()
 
+    bulk_model_list = (
+        TagKey, TagValue, GroupTagKey, GroupTagValue, EventMapping
+    )
+    for model in bulk_model_list:
+        has_more = bulk_delete_objects(model, project_id=p.id, logger=logger)
+        if has_more:
+            delete_project.delay(object_id=object_id, countdown=15)
+            return
+
     model_list = (
-        ProjectKey, TagKey, TagValue, GroupTagKey, GroupTagValue,
-        Activity, EventMapping, Event, Group
+        Activity, EventMapping, Event, Group, ProjectKey
     )
 
     has_more = delete_objects(model_list, relation={'project': p}, logger=logger)
@@ -124,8 +135,17 @@ def delete_group(object_id, **kwargs):
 
     logger = delete_group.get_logger()
 
+    bulk_model_list = (
+        GroupHash, GroupRuleStatus, GroupTagValue, GroupTagKey, EventMapping
+    )
+    for model in bulk_model_list:
+        has_more = bulk_delete_objects(model, group_id=object_id, logger=logger)
+        if has_more:
+            delete_group.delay(object_id=object_id, countdown=15)
+            return
+
     model_list = (
-        GroupHash, GroupRuleStatus, GroupTagValue, GroupTagKey, EventMapping, Event
+        Event,
     )
 
     has_more = delete_objects(model_list, relation={'group': group}, logger=logger)
@@ -137,7 +157,6 @@ def delete_group(object_id, **kwargs):
 
 def delete_objects(models, relation, limit=1000, logger=None):
     # This handles cascades properly
-    # TODO: this doesn't clean up the index
     has_more = False
     for model in models:
         if logger is not None:
@@ -149,3 +168,61 @@ def delete_objects(models, relation, limit=1000, logger=None):
         if has_more:
             return True
     return has_more
+
+
+def bulk_delete_objects(model, group_id=None, project_id=None, limit=10000,
+                        logger=None):
+    assert group_id or project_id, 'Must pass either project_id or group_id'
+
+    if group_id:
+        column = 'group_id'
+        value = group_id
+
+    elif project_id:
+        column = 'project_id'
+        value = project_id
+
+    connection = connections['default']
+    quote_name = connection.ops.quote_name
+
+    if logger is not None:
+        logger.info('Removing %r objects where %s=%r', model, column, value)
+
+    if db.is_postgres():
+        query = """
+            delete from %(table)s
+            where id = any(array(
+                select id
+                from %(table)s
+                where %(column)s = %%s
+                limit %(limit)d
+            ))
+        """ % dict(
+            table=model._meta.db_table,
+            column=quote_name(column),
+            limit=limit,
+        )
+        print query
+        params = [value]
+    elif db.is_mysql():
+        query = """
+            delete from %(table)s
+            where %(column)s = %%s
+            limit %(limit)d
+        """ % dict(
+            table=model._meta.db_table,
+            column=quote_name(column),
+            limit=limit,
+        )
+        params = [value]
+    else:
+        logger.warning('Using slow deletion strategy due to unknown database')
+        has_more = False
+        for obj in model.objects.filter(project=project_id)[:limit]:
+            obj.delete()
+            has_more = True
+        return has_more
+
+    cursor = connection.cursor()
+    cursor.execute(query, params)
+    return cursor.rowcount > 0
diff --git a/src/sentry/utils/db.py b/src/sentry/utils/db.py
index 0ba58425f1..a9145ee2f1 100644
--- a/src/sentry/utils/db.py
+++ b/src/sentry/utils/db.py
@@ -7,25 +7,32 @@ sentry.utils.db
 """
 from __future__ import absolute_import
 
-import django
-
 from django.conf import settings
 from django.db import connections, DEFAULT_DB_ALIAS
 from django.db.models.fields.related import SingleRelatedObjectDescriptor
 
 
 def get_db_engine(alias='default'):
-    has_multidb = django.VERSION >= (1, 2)
-    if has_multidb:
-        value = settings.DATABASES[alias]['ENGINE']
-    else:
-        assert alias == 'default', 'You cannot fetch a database engine other than the default on Django < 1.2'
-        value = settings.DATABASE_ENGINE
+    value = settings.DATABASES[alias]['ENGINE']
     if value == 'mysql.connector.django':
         return 'mysql'
     return value.rsplit('.', 1)[-1]
 
 
+def is_postgres(alias='default'):
+    engine = get_db_engine(alias)
+    return 'postgres' in engine
+
+
+def is_mysql(alias='default'):
+    engine = get_db_engine(alias)
+    return 'mysql' in engine
+
+
+def is_sqlite(alias='sqlite'):
+    engine = get_db_engine(alias)
+
+
 def has_charts(db):
     engine = get_db_engine(db)
     if engine.startswith('sqlite'):
