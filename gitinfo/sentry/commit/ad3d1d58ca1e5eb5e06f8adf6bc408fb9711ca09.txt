commit ad3d1d58ca1e5eb5e06f8adf6bc408fb9711ca09
Author: Radu Woinaroski <5281987+RaduW@users.noreply.github.com>
Date:   Fri Mar 6 13:37:05 2020 +0100

    test(relay): Add E2E tests with Relay (#17241)
    
    Convert tests to use the Relay store endpoint
    Created travis job for Relay E2E test
    Started unification of test configurations in Travis (now each container is started using the same configuration across all tests).

diff --git a/.travis.yml b/.travis.yml
index 6e0b577692..e0029833f5 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -40,12 +40,43 @@ env:
     - SENTRY_ZOOKEEPER_HOSTS=localhost:2181
     - PYTEST_ADDOPTS="--reruns 5"
 
-base_install: &before_install_default |-
+base_install: &base_install |-
   pip install --no-cache-dir "pip>=20.0.2"
-  docker run -d --network host --name sentry_zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:4.1.0
-  docker run -d --network host --name sentry_kafka -e KAFKA_ZOOKEEPER_CONNECT=localhost:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 confluentinc/cp-kafka:4.1.0
+
+  docker run \
+    --name sentry_zookeeper \
+    -d --network host \
+    -e ZOOKEEPER_CLIENT_PORT=2181 \
+    confluentinc/cp-zookeeper:4.1.0
+
+  docker run \
+    --name sentry_kafka \
+    -d --network host \
+    -e KAFKA_ZOOKEEPER_CONNECT=127.0.0.1:2181 \
+    -e KAFKA_LISTENERS=INTERNAL://0.0.0.0:9093,EXTERNAL://0.0.0.0:9092 \
+    -e KAFKA_ADVERTISED_LISTENERS=INTERNAL://127.0.0.1:9093,EXTERNAL://127.0.0.1:9092 \
+    -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT \
+    -e KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL \
+    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
+    confluentinc/cp-kafka:5.1.2
+
   [ "$TRAVIS_PULL_REQUEST" != "false" ] || export PYTEST_SENTRY_ALWAYS_REPORT=1
 
+start_snuba: &start_snuba |-
+  docker run \
+    --name sentry_clickhouse \
+    -d --network host \
+    --ulimit nofile=262144:262144 \
+    yandex/clickhouse-server:19.11 \
+
+  docker run \
+    --name sentry_snuba \
+    -d --network host \
+    -e SNUBA_SETTINGS=test \
+    -e CLICKHOUSE_HOST=127.0.0.1 \
+    -e CLICKHOUSE_PORT=9000 \
+    getsentry/snuba
+
 script:
   # certain commands require sentry init to be run, but this is only true for
   # running things within Travis
@@ -80,9 +111,8 @@ base_postgres: &postgres_default
     - redis-server
     - postgresql
   before_install:
-    - *before_install_default
-    - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.11
-    - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
+    - *base_install
+    - *start_snuba
     - docker ps -a
   install:
     - python setup.py install_egg_info
@@ -98,11 +128,10 @@ base_acceptance: &acceptance_default
     - redis-server
     - postgresql
   before_install:
-    - *before_install_default
+    - *base_install
+    - *start_snuba
     - find "$NODE_DIR" -type d -empty -delete
     - nvm install
-    - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.11
-    - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
     - docker ps -a
   install:
     - ./bin/yarn install --frozen-lockfile
@@ -161,7 +190,7 @@ matrix:
       name: 'Frontend'
       env: TEST_SUITE=js
       before_install:
-        - *before_install_default
+        - *base_install
         - find "$NODE_DIR" -type d -empty -delete
         - nvm install
       install:
@@ -183,12 +212,25 @@ matrix:
       name: 'Symbolicator Integration'
       env: TEST_SUITE=symbolicator
       before_install:
-        - *before_install_default
-        - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.11
-        - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
+        - *base_install
+        - *start_snuba
         - docker run -d --network host --name symbolicator us.gcr.io/sentryio/symbolicator:latest run
         - docker ps -a
 
+    - <<: *postgres_default
+      name: 'Sentry-Relay integration tests'
+      env: TEST_SUITE=relay-integration DB=postgres
+      services:
+        - docker
+        - memcached
+        - redis-server
+        - postgresql
+      before_install:
+        - *base_install
+        - *start_snuba
+        - docker pull us.gcr.io/sentryio/relay:latest # pull relay we'll run and kill it for each test
+        - docker ps -a
+
     - python: 2.7
       name: 'Snuba Integration with migrations'
       env: TEST_SUITE=snuba USE_SNUBA=1 MIGRATIONS_TEST_MIGRATE=1
@@ -198,9 +240,8 @@ matrix:
         - redis-server
         - postgresql
       before_install:
-        - *before_install_default
-        - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.11
-        - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
+        - *base_install
+        - *start_snuba
         - docker ps -a
       install:
         - python setup.py install_egg_info
@@ -220,9 +261,8 @@ matrix:
         - redis-server
         - postgresql
       before_install:
-        - *before_install_default
-        - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.11
-        - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
+        - *base_install
+        - *start_snuba
         - docker ps -a
       install:
         - python setup.py install_egg_info
@@ -247,6 +287,7 @@ matrix:
 
   allow_failures:
     - name: 'Python 3.6 backend (no migrations) [Postgres]'
+    - name: 'Sentry-Relay integration tests'
 
 notifications:
   webhooks:
diff --git a/Makefile b/Makefile
index 4c620bb927..ebce4a34f9 100644
--- a/Makefile
+++ b/Makefile
@@ -181,6 +181,11 @@ test-plugins:
 	py.test tests/sentry_plugins -vv --cov . --cov-report="xml:.artifacts/plugins.coverage.xml" --junit-xml=".artifacts/plugins.junit.xml"
 	@echo ""
 
+test-relay-integration:
+	@echo "--> Running Relay integration tests"
+	pytest tests/relay_integration -vv
+	@echo ""
+
 lint: lint-python lint-js
 
 # configuration for flake8 can be found in setup.cfg
@@ -221,7 +226,8 @@ travis-noop:
 .PHONY: travis-test-lint
 travis-test-lint: lint-python lint-js
 
-.PHONY: travis-test-postgres travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js travis-test-cli
+.PHONY: travis-test-postgres travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js
+.PHONY: travis-test-cli travis-test-relay-integration
 travis-test-postgres: test-python
 travis-test-acceptance: test-acceptance
 travis-test-snuba: test-snuba
@@ -229,8 +235,10 @@ travis-test-symbolicator: test-symbolicator
 travis-test-js: test-js
 travis-test-cli: test-cli
 travis-test-plugins: test-plugins
+travis-test-relay-integration: test-relay-integration
 
-.PHONY: scan-python travis-scan-postgres travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator travis-scan-js travis-scan-cli travis-scan-lint
+.PHONY: scan-python travis-scan-postgres travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator
+.PHONY: travis-scan-js travis-scan-cli travis-scan-lint travis-scan-relay-integration
 scan-python:
 	@echo "--> Running Python vulnerability scanner"
 	$(PIP) install safety
@@ -245,3 +253,4 @@ travis-scan-js: travis-noop
 travis-scan-cli: travis-noop
 travis-scan-lint: scan-python
 travis-scan-plugins: travis-noop
+travis-scan-relay-integration: travis-noop
diff --git a/conftest.py b/conftest.py
index 139e2be950..4f6e931a95 100644
--- a/conftest.py
+++ b/conftest.py
@@ -22,6 +22,15 @@ def pytest_configure(config):
     # always install plugins for the tests
     install_sentry_plugins()
 
+    # add custom test markers
+    config.addinivalue_line(
+        "markers",
+        "sentry_store_integration: mark test as using the sentry store endpoint and therefore using legacy code",
+    )
+    config.addinivalue_line(
+        "markers", "relay_store_integration: mark test as using the relay store endpoint"
+    )
+
 
 def install_sentry_plugins():
     # Sentry's pytest plugin explicitly doesn't load plugins, so let's load all of them
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 17252e2f17..29afae6ef3 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -1335,6 +1335,8 @@ SENTRY_WATCHERS = (
 # will split the requests between Relay and Sentry (all store requests will be passed to Relay, and the
 # rest will be forwarded to Sentry)
 SENTRY_USE_RELAY = False
+SENTRY_RELAY_PORT = 3000
+SENTRY_REVERSE_PROXY_PORT = 8000
 
 SENTRY_DEVSERVICES = {
     "redis": {
@@ -1402,13 +1404,13 @@ SENTRY_DEVSERVICES = {
     },
     "reverse_proxy": {
         "image": "nginx:1.16.1",
-        "ports": {"80/tcp": 8000},
+        "ports": {"80/tcp": SENTRY_REVERSE_PROXY_PORT},
         "volumes": {REVERSE_PROXY_CONFIG: {"bind": "/etc/nginx/nginx.conf"}},
     },
     "relay": {
         "image": "us.gcr.io/sentryio/relay:latest",
         "pull": True,
-        "ports": {"3000/tcp": 3000},
+        "ports": {"3000/tcp": SENTRY_RELAY_PORT},
         "volumes": {RELAY_CONFIG_DIR: {"bind": "/etc/relay"}},
         "command": ["run", "--config", "/etc/relay"],
     },
diff --git a/src/sentry/testutils/__init__.py b/src/sentry/testutils/__init__.py
index 3a25c06da7..76dc62c8e5 100644
--- a/src/sentry/testutils/__init__.py
+++ b/src/sentry/testutils/__init__.py
@@ -3,3 +3,4 @@ from __future__ import absolute_import
 from .asserts import *  # NOQA
 from .cases import *  # NOQA
 from .skips import *  # NOQA
+from .relay import *  # NOQA
diff --git a/src/sentry/testutils/relay.py b/src/sentry/testutils/relay.py
new file mode 100644
index 0000000000..e88a0c9b54
--- /dev/null
+++ b/src/sentry/testutils/relay.py
@@ -0,0 +1,125 @@
+from __future__ import absolute_import
+
+import json
+
+import pytest
+import requests
+import responses
+
+from sentry import eventstore
+from sentry.eventtypes import transaction
+from sentry.models.relay import Relay
+from sentry.testutils.helpers import get_auth_header
+
+
+def ensure_relay_is_registered():
+    """
+    Ensure that the test Relay instance is registered
+
+    Note: This is an ugly hack, we need it because we are persisting a Relay instance during the whole
+    test session and the database is cleaned up after each test.
+    Relay will do a security handshake when it is started and this will result in a Relay object
+    being added in the database. After the test is finished the entry will be cleaned up and next
+    time Relay will be used in another test it will not be recognized as an internal relay.
+
+    TODO: A fix for this would be to restart Relay for every test I (RaduW) need to investigate the
+    performance hit for starting relay for every test that uses it.
+    """
+    try:
+        with transaction.atomic():
+            # just check for the Relay object and insert it if it does not exist
+            Relay.objects.create(
+                relay_id="88888888-4444-4444-8444-cccccccccccc",
+                public_key="SMSesqan65THCV6M4qs4kBzPai60LzuDn-xNsvYpuP8",
+                is_internal=True,
+            )
+    except:  # NOQA
+        # relay already registered  probably the first test (registration happened at Relay handshake time)
+        pass  # NOQA
+
+
+def adjust_settings_for_relay_tests(settings):
+    """
+    Adjusts the application settings to accept calls from a Relay instance running inside a
+    docker container.
+
+    :param settings: the app settings
+    """
+    settings.ALLOWED_HOSTS = [
+        "localhost",
+        "testserver",
+        "host.docker.internal",
+        "0.0.0.0",
+        "127.0.0.1",
+    ]
+    settings.KAFKA_CLUSTERS = {
+        "default": {
+            "bootstrap.servers": "127.0.0.1:9092",
+            "compression.type": "lz4",
+            "message.max.bytes": 50000000,  # 50MB, default is 1MB
+        }
+    }
+    settings.SENTRY_RELAY_WHITELIST_PK = ["SMSesqan65THCV6M4qs4kBzPai60LzuDn-xNsvYpuP8"]
+
+
+class SentryStoreHelper(object):
+    """
+    Unit tests that post to the store entry point should use this
+    helper class (together with RelayStoreHelper) to check the functionality
+    with both posting to the Sentry Store and the Relay Store.
+    """
+
+    def use_relay(self):
+        return False
+
+    def post_and_retrieve_event(self, data):
+        resp = self._postWithHeader(data)
+        assert resp.status_code == 200
+        event_id = json.loads(resp.content)["id"]
+
+        event = eventstore.get_event_by_id(self.project.id, event_id)
+        assert event is not None
+        return event
+
+
+class RelayStoreHelper(object):
+    """
+    Unit tests that post to the store entry point should use this
+    helper class (together with RelayStoreHelper) to check the functionality
+    with both posting to the Sentry Store and the Relay Store.
+    """
+
+    def use_relay(self):
+        return True
+
+    def post_and_retrieve_event(self, data):
+        url = self.get_relay_store_url(self.project.id)
+        responses.add_passthru(url)
+        resp = requests.post(
+            url,
+            headers={"x-sentry-auth": self.auth_header, "content-type": "application/json"},
+            json=data,
+        )
+
+        assert resp.ok
+        resp_body = resp.json()
+        event_id = resp_body["id"]
+
+        event = self.wait_for_ingest_consumer(
+            lambda: eventstore.get_event_by_id(self.project.id, event_id)
+        )
+        # check that we found it in Snuba
+        assert event is not None
+        return event
+
+    def setUp(self):  # NOQA
+        self.auth_header = get_auth_header(
+            "TEST_USER_AGENT/0.0.0", self.projectkey.public_key, self.projectkey.secret_key, "7"
+        )
+        adjust_settings_for_relay_tests(self.settings)
+
+    @pytest.fixture(autouse=True)
+    def setup_fixtures(self, settings, live_server, get_relay_store_url, wait_for_ingest_consumer):
+        self.settings = settings
+        self.get_relay_store_url = get_relay_store_url  # noqa
+        self.wait_for_ingest_consumer = wait_for_ingest_consumer(settings)  # noqa
diff --git a/src/sentry/testutils/skips.py b/src/sentry/testutils/skips.py
index a1e37bdf56..b800d2a7b2 100644
--- a/src/sentry/testutils/skips.py
+++ b/src/sentry/testutils/skips.py
@@ -35,3 +35,36 @@ def xfail_if_not_postgres(reason):
         )
 
     return decorator
+
+
+def skip_for_relay_store(reason):
+    """
+    Decorator factory will skip marked tests if Relay is enabled.
+    A test decorated with @skip_for_relay_store("this test has been moved in relay")
+    Will not be executed when the settings SENTRY_USE_RELAY = True
+    :param reason: the reason the test should be skipped
+
+    Note: Eventually, when Relay becomes compulsory, tests marked with this decorator will be deleted.
+    """
+
+    def decorator(function):
+        return pytest.mark.skipif(settings.SENTRY_USE_RELAY, reason=reason)(function)
+
+    return decorator
+
+
+def relay_is_available():
+    if "relay" in _service_status:
+        return _service_status["relay"]
+    try:
+        socket.create_connection(("127.0.0.1", settings.SENTRY_RELAY_PORT), 1.0)
+    except socket.error:
+        _service_status["relay"] = False
+    else:
+        _service_status["relay"] = True
+    return _service_status["relay"]
+
+
+requires_relay = pytest.mark.skipif(
+    not relay_is_available(), reason="requires relay server running"
+)
diff --git a/src/sentry/utils/pytest/__init__.py b/src/sentry/utils/pytest/__init__.py
index d9bd552be1..27372e7601 100644
--- a/src/sentry/utils/pytest/__init__.py
+++ b/src/sentry/utils/pytest/__init__.py
@@ -6,4 +6,5 @@ pytest_plugins = [
     "sentry.utils.pytest.fixtures",
     "sentry.utils.pytest.unittest",
     "sentry.utils.pytest.kafka",
+    "sentry.utils.pytest.relay",
 ]
diff --git a/src/sentry/utils/pytest/kafka.py b/src/sentry/utils/pytest/kafka.py
index f348f16069..6abceae522 100644
--- a/src/sentry/utils/pytest/kafka.py
+++ b/src/sentry/utils/pytest/kafka.py
@@ -6,6 +6,12 @@ import pytest
 import six
 from confluent_kafka.admin import AdminClient
 from confluent_kafka import Producer
+import time
+import logging
+
+_log = logging.getLogger(__name__)
+
+MAX_SECONDS_WAITING_FOR_EVENT = 16
 
 
 @pytest.fixture
@@ -34,7 +40,7 @@ class _KafkaAdminWrapper:
             futures_dict = self.admin_client.delete_topics([topic_name])
             self._sync_wait_on_result(futures_dict)
         except Exception:  # noqa
-            pass  # noqa nothing to do (probably there was no topic to start with)
+            _log.warning("Could not delete topic %s", topic_name)
 
     def _sync_wait_on_result(self, futures_dict):
         """
@@ -60,9 +66,174 @@ def kafka_admin(request):
     return inner
 
 
+@pytest.fixture
+def kafka_topics_setter():
+    """
+    Returns a function that given a Django settings objects will setup the
+    kafka topics names to test names.
+
+    :return: a function that given a settings object changes all kafka topic names
+    to "test-<normal_topic_name>"
+    """
+
+    def set_test_kafka_settings(settings):
+        ingest_events = "ingest-events"
+        settings.KAFKA_INGEST_EVENTS = ingest_events
+        settings.KAFKA_TOPICS[ingest_events] = {"cluster": "default", "topic": ingest_events}
+
+        ingest_transactions = "ingest-transactions"
+        settings.INGEST_TRANSACTIONS = ingest_transactions
+        settings.KAFKA_TOPICS[ingest_transactions] = {
+            "cluster": "default",
+            "topic": ingest_transactions,
+        }
+
+        ingest_attachments = "ingest-attachments"
+        settings.KAFKA_INGEST_ATTACHMENTS = ingest_attachments
+        settings.KAFKA_TOPICS[ingest_attachments] = {
+            "cluster": "default",
+            "topic": ingest_attachments,
+        }
+
+        outcomes = "outcomes"
+        settings.KAFKA_OUTCOMES = outcomes
+        settings.KAFKA_TOPICS[outcomes] = {"cluster": "default", "topic": outcomes}
+
+    return set_test_kafka_settings
+
+
 @pytest.fixture
 def requires_kafka():
     pytest.importorskip("confluent_kafka")
 
     if "SENTRY_KAFKA_HOSTS" not in os.environ:
         pytest.xfail("test requires SENTRY_KAFKA_HOSTS environment variable which is not set")
+
+
+@pytest.fixture(scope="session")
+def scope_consumers():
+    """
+      Sets up an object to keep track of the scope consumers ( consumers that will only
+      be created once per test session).
+
+    """
+    all_consumers = {
+        "ingest_events": None,
+        "ingest_transactions": None,
+        "ingest_attachments": None,
+        "outcomes": None,
+    }
+    yield all_consumers
+
+    for (consumer, consumer_name) in (
+        (all_consumers.get(consumer_name), consumer_name)
+        for consumer_name in (
+            "ingest_events",
+            "ingest_transactions",
+            "ingest_attachments",
+            "outcomes",
+        )
+    ):
+        if consumer is not None:
+            try:
+                # stop the consumer
+                consumer.signal_shutdown()
+                consumer.run()
+            except:  # noqa:
+                _log.warning("Failed to cleanup consumer %s", consumer_name)
+
+
+@pytest.fixture(scope="function")
+def session_ingest_consumer(scope_consumers, kafka_admin, task_runner):
+    """
+    Returns a factory for a session ingest consumer.
+
+    Note/Warning: Once an inject consumer is created it will be reused by all tests in the session.
+    The ingest consumer is created the first time with the provided settings and then reused.
+    If you don't want this behaviour DO NOT USE this fixture (create a fixture, similar with this one,
+    that returns a new consumer at each invocation rather then reusing it)
+
+    :return: a function factory that creates a consumer at first invocation and returns the cached consumer afterwards.
+    """
+
+    def ingest_consumer(settings):
+        from sentry.ingest.ingest_consumer import ConsumerType, get_ingest_consumer
+
+        if scope_consumers["ingest_events"] is not None:
+            return scope_consumers[
+                "ingest_events"
+            ]  # reuse whatever was already created (will ignore the settings)
+
+        # first time the consumer is requested, create it using settings
+        topic_event_name = ConsumerType.get_topic_name(ConsumerType.Events)
+        admin = kafka_admin(settings)
+        admin.delete_topic(topic_event_name)
+
+        # simulate the event ingestion task
+        group_id = "test-consumer"
+
+        consumer = get_ingest_consumer(
+            max_batch_size=1,
+            max_batch_time=10,
+            group_id=group_id,
+            consumer_type=ConsumerType.Events,
+            auto_offset_reset="earliest",
+        )
+
+        scope_consumers["ingest_events"] = consumer
+
+        return consumer
+
+    return ingest_consumer
+
+
+@pytest.fixture(scope="function")
+def wait_for_ingest_consumer(session_ingest_consumer, task_runner):
+    """
+    Returns a function that can be used to create a wait loop for the ingest consumer
+
+    The ingest consumer will be called in a loop followed by a query to the supplied
+    predicate. If the predicate returns a non None value the wait will be ended and
+    the waiter will return whatever the predicate returned.
+    If the max_time passes the waiter will be terminated and the waiter will return None
+
+    Note: The reason there we return a factory and not directly the waiter is that we
+    need to configure the consumer with the test settings (settings are typically available
+    in the test) so a test would typically first create the waiter and the use it to wait for
+    the required condition:
+
+    waiter = wait_for_ingest_consumer( test_settings_derived_from_the_project_settings)
+    result = waiter( my_predicate, SOME_TIMEOUT)
+    assert result == expected_result
+    """
+
+    def factory(settings):
+        consumer = session_ingest_consumer(settings)
+
+        def waiter(exit_predicate, max_time=MAX_SECONDS_WAITING_FOR_EVENT):
+            """
+            Implements a wait loop for the ingest consumer
+            :param exit_predicate:  A Callable[(),Any] that will be called in a loop after each call
+                to the KafkaConsumer _run_once()
+            :param max_time: maximum time in seconds to wait
+            :return: the first non None result returned by the exit predicate or None if the
+                max time has expired without the exit predicate returning a non None value
+            """
+
+            start_wait = time.time()
+            with task_runner():
+                while time.time() - start_wait < max_time:
+                    consumer._run_once()  # noqa
+                    # check if the condition is satisfied
+                    val = exit_predicate()
+                    if val is not None:
+                        return val  # we got what we were waiting for stop looping
+
+            _log.warning(
+                "Ingest consumer waiter timed-out after %d seconds", time.time() - start_wait
+            )
+            return None  # timout without any success
+
+        return waiter
+
+    return factory
diff --git a/src/sentry/utils/pytest/relay.py b/src/sentry/utils/pytest/relay.py
new file mode 100644
index 0000000000..152c88c8b5
--- /dev/null
+++ b/src/sentry/utils/pytest/relay.py
@@ -0,0 +1,150 @@
+# Fixutres used to interact with a test Relay server
+
+from __future__ import absolute_import
+import pytest
+from os import path
+import six
+from six.moves.urllib.parse import urlparse
+import sys
+import datetime
+import shutil
+
+from sentry.runner.commands.devservices import get_docker_client
+
+
+def _relay_server_container_name():
+    return "sentry_test_relay_server"
+
+
+def _get_template_dir():
+    return path.abspath(path.join(path.dirname(__file__), "template"))
+
+
+def _remove_container_if_exists(docker_client, container_name):
+    try:
+        container = docker_client.containers.get(container_name)
+    except Exception:
+        pass  # container not found
+    else:
+        try:
+            container.kill()
+        except Exception:
+            pass  # maybe the container is already stopped
+        try:
+            container.remove()
+        except Exception:
+            pass  # could not remove the container nothing to do about it
+
+
+@pytest.fixture(scope="session")
+def relay_server_setup(live_server, tmpdir_factory):
+    prefix = "test_relay_config_{}_".format(
+        datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S_%f")
+    )
+    config_path = tmpdir_factory.mktemp(prefix)
+    config_path = six.text_type(config_path)
+
+    parsed_live_server_url = urlparse(live_server.url)
+    if parsed_live_server_url.port is not None:
+        port = six.text_type(parsed_live_server_url.port)
+    else:
+        port = "80"
+
+    if sys.platform.startswith("linux"):
+        upstream_host = "http://127.0.0.1:%s/" % port
+        kafka_host = "127.0.0.1"
+        redis_host = "127.0.0.1"
+        network = "host"
+    else:
+        upstream_host = "http://host.docker.internal:%s/" % port
+        kafka_host = "sentry_kafka"
+        redis_host = "sentry_redis"
+        network = "sentry"
+
+    template_path = _get_template_dir()
+    sources = ["config.yml", "credentials.json"]
+
+    # NOTE: if we ever need to start the test relay server at various ports here's where we need to change
+    relay_port = 33331
+
+    template_vars = {
+        "SENTRY_HOST": upstream_host,
+        "RELAY_PORT": relay_port,
+        "KAFKA_HOST": kafka_host,
+        "REDIS_HOST": redis_host,
+    }
+
+    for source in sources:
+        source_path = path.join(template_path, source)
+        dest_path = path.join(config_path, source)
+        with open(source_path, "rt") as input:
+            content = input.read()
+
+        for var_name, var_val in six.iteritems(template_vars):
+            content = content.replace("${%s}" % var_name, six.text_type(var_val))
+
+        with open(dest_path, "wt") as output:
+            output.write(content)
+
+    # we have a config path for relay that is set up with the current live serve as upstream
+    # check if we have the test relay docker container
+    docker_client = get_docker_client()
+    container_name = _relay_server_container_name()
+    _remove_container_if_exists(docker_client, container_name)
+    options = {
+        "image": "us.gcr.io/sentryio/relay:latest",
+        "ports": {"%s/tcp" % relay_port: relay_port},
+        "network": network,
+        "detach": True,
+        "name": container_name,
+        "volumes": {config_path: {"bind": "/etc/relay"}},
+        "command": ["run", "--config", "/etc/relay"],
+    }
+
+    # Some structure similar to what the live_server fixture returns
+    server_info = {
+        "url": "http://127.0.0.1:{}".format(relay_port),
+        "is_started": False,
+        "options": options,
+    }
+
+    yield server_info
+
+    # cleanup
+    shutil.rmtree(config_path)
+    _remove_container_if_exists(docker_client, container_name)
+
+
+@pytest.fixture(scope="function")
+def relay_server(relay_server_setup):
+    options = relay_server_setup["options"]
+    docker_client = get_docker_client()
+    container_name = _relay_server_container_name()
+    _remove_container_if_exists(docker_client, container_name)
+    docker_client.containers.run(**options)
+    relay_server_setup["is_started"] = True
+
+    return {"url": relay_server_setup["url"]}
+
+
+@pytest.fixture
+def get_relay_store_url(relay_server):
+    def relay_store_url(project_id):
+        return "{}/api/{}/store/".format(relay_server["url"], project_id)
+
+    return relay_store_url
+
+
+@pytest.fixture(scope="function")
+def persistent_relay_server(relay_server_setup):
+    options = relay_server_setup["options"]
+
+    if not relay_server_setup["is_started"]:
+        # first time we use it in a test, everything should be
+        # already setup, sentry should be running and configured,
+        # just run relay
+        docker_client = get_docker_client()
+        docker_client.containers.run(**options)
+        relay_server_setup["is_started"] = True
+
+    return {"url": relay_server_setup["url"]}
diff --git a/src/sentry/utils/pytest/template/config.yml b/src/sentry/utils/pytest/template/config.yml
new file mode 100644
index 0000000000..6a9c563a36
--- /dev/null
+++ b/src/sentry/utils/pytest/template/config.yml
@@ -0,0 +1,14 @@
+---
+relay:
+  mode: managed
+  upstream: ${SENTRY_HOST}
+  host: 0.0.0.0
+  port: ${RELAY_PORT}
+logging:
+  level: TRACE
+  enable_backtraces: true
+processing:
+  enabled: true
+  kafka_config:
+    - {name: "bootstrap.servers", value: "${KAFKA_HOST}:9093"}
+  redis: redis://${REDIS_HOST}:6379
diff --git a/src/sentry/utils/pytest/template/credentials.json b/src/sentry/utils/pytest/template/credentials.json
new file mode 100644
index 0000000000..1d37e82fd6
--- /dev/null
+++ b/src/sentry/utils/pytest/template/credentials.json
@@ -0,0 +1,5 @@
+{
+  "secret_key": "OxE6Du8quMxWj19f7YDCpIxm6XyU9nWGQJkMWFlkchA",
+  "public_key": "SMSesqan65THCV6M4qs4kBzPai60LzuDn-xNsvYpuP8",
+  "id": "88888888-4444-4444-8444-cccccccccccc"
+}
diff --git a/tests/integration/tests.py b/tests/integration/tests.py
index 2418ae2a1b..71fbeda185 100644
--- a/tests/integration/tests.py
+++ b/tests/integration/tests.py
@@ -6,11 +6,11 @@ import os
 import datetime
 import json
 import logging
-from sentry.utils.compat import mock
 import six
 from time import sleep
 import zlib
 
+from sentry.utils.compat import mock
 from sentry import eventstore, tagstore
 from django.conf import settings
 from django.core.urlresolvers import reverse
@@ -33,7 +33,6 @@ from sentry.utils.sdk import configure_scope
 from sentry.web.api import disable_transaction_events
 from sentry.wsgi import application
 
-
 DEPENDENCY_TEST_DATA = {
     "postgresql": (
         "DATABASES",
@@ -564,7 +563,6 @@ class DependencyTest(TestCase):
     def validate_dependency(
         self, key, package, dependency_type, dependency, setting_value, import_string
     ):
-
         import_string.side_effect = self.raise_import_error(package)
 
         with self.settings(**{key: setting_value}):
diff --git a/tests/relay_integration/__init__.py b/tests/relay_integration/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/relay_integration/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/relay_integration/lang/__init__.py b/tests/relay_integration/lang/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/relay_integration/lang/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/relay_integration/lang/java/__init__.py b/tests/relay_integration/lang/java/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/relay_integration/lang/java/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/relay_integration/lang/java/test_plugin.py b/tests/relay_integration/lang/java/test_plugin.py
new file mode 100644
index 0000000000..d439557254
--- /dev/null
+++ b/tests/relay_integration/lang/java/test_plugin.py
@@ -0,0 +1,12 @@
+from __future__ import absolute_import
+import pytest
+
+from tests.sentry.lang.java.test_plugin import BasicResolvingIntegrationTest
+from sentry.testutils import RelayStoreHelper, TransactionTestCase
+
+
+@pytest.mark.relay_store_integration
+class BasicResolvingIntegrationTestRelay(
+    RelayStoreHelper, TransactionTestCase, BasicResolvingIntegrationTest
+):
+    pass
diff --git a/tests/relay_integration/lang/javascript/__init__.py b/tests/relay_integration/lang/javascript/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/relay_integration/lang/javascript/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/relay_integration/lang/javascript/test_example.py b/tests/relay_integration/lang/javascript/test_example.py
new file mode 100644
index 0000000000..2162769666
--- /dev/null
+++ b/tests/relay_integration/lang/javascript/test_example.py
@@ -0,0 +1,11 @@
+from __future__ import absolute_import
+
+import pytest
+
+from ....sentry.lang.javascript.test_example import ExampleTestCase
+from sentry.testutils import RelayStoreHelper, TransactionTestCase
+
+
+@pytest.mark.relay_store_integration
+class ExampleTestCaseRelay(RelayStoreHelper, TransactionTestCase, ExampleTestCase):
+    pass
diff --git a/tests/relay_integration/lang/javascript/test_plugin.py b/tests/relay_integration/lang/javascript/test_plugin.py
new file mode 100644
index 0000000000..4d08755380
--- /dev/null
+++ b/tests/relay_integration/lang/javascript/test_plugin.py
@@ -0,0 +1,15 @@
+from __future__ import absolute_import
+import pytest
+
+from ....sentry.lang.javascript.test_plugin import JavascriptIntegrationTest
+from sentry.testutils import RelayStoreHelper, TransactionTestCase
+from sentry.testutils.helpers.datetime import iso_format, before_now
+
+
+@pytest.mark.relay_store_integration
+class JavascriptIntegrationTestRelay(
+    RelayStoreHelper, TransactionTestCase, JavascriptIntegrationTest
+):
+    def setUp(self):
+        super(JavascriptIntegrationTestRelay, self).setUp()
+        self.min_ago = iso_format(before_now(minutes=1))
diff --git a/tests/sentry/lang/java/test_plugin.py b/tests/sentry/lang/java/test_plugin.py
index c60a2e4d0a..69dbfe0e68 100644
--- a/tests/sentry/lang/java/test_plugin.py
+++ b/tests/sentry/lang/java/test_plugin.py
@@ -1,16 +1,14 @@
 from __future__ import absolute_import
 
 import zipfile
+import pytest
 from six import BytesIO
 
 from django.core.urlresolvers import reverse
 from django.core.files.uploadedfile import SimpleUploadedFile
 
-from sentry import eventstore
-from sentry.testutils import TestCase
+from sentry.testutils import SentryStoreHelper, TestCase
 from sentry.testutils.helpers.datetime import before_now, iso_format
-from sentry.utils import json
-
 
 PROGUARD_UUID = "6dc7fdb0-d2fb-4c8e-9d6b-bb1aa98929b1"
 PROGUARD_SOURCE = b"""\
@@ -24,7 +22,12 @@ PROGUARD_BUG_UUID = "071207ac-b491-4a74-957c-2c94fd9594f2"
 PROGUARD_BUG_SOURCE = b"x"
 
 
-class BasicResolvingIntegrationTest(TestCase):
+class BasicResolvingIntegrationTest(object):
+    def post_and_retrieve_event(self, data):
+        raise NotImplementedError(
+            "post_and_retrieve_event should be implemented in a dervied test class"
+        )
+
     def test_basic_resolving(self):
         url = reverse(
             "sentry-api-0-dsym-files",
@@ -90,22 +93,21 @@ class BasicResolvingIntegrationTest(TestCase):
             "timestamp": iso_format(before_now(seconds=1)),
         }
 
-        # We do a preflight post, because there are many queries polluting the array
-        # before the actual "processing" happens (like, auth_user)
-        resp = self._postWithHeader(event_data)
-        with self.assertWriteQueries(
-            {
-                "nodestore_node": 2,
-                "sentry_eventuser": 1,
-                "sentry_groupedmessage": 1,
-                "sentry_userreport": 1,
-            }
-        ):
-            self._postWithHeader(event_data)
-        assert resp.status_code == 200
-        event_id = json.loads(resp.content)["id"]
-
-        event = eventstore.get_event_by_id(self.project.id, event_id)
+        event = self.post_and_retrieve_event(event_data)
+        if not self.use_relay():
+            # We measure the number of queries after an initial post,
+            # because there are many queries polluting the array
+            # before the actual "processing" happens (like, auth_user)
+            with self.assertWriteQueries(
+                {
+                    "nodestore_node": 2,
+                    "sentry_eventuser": 1,
+                    "sentry_groupedmessage": 1,
+                    "sentry_userreport": 1,
+                }
+            ):
+                self.post_and_retrieve_event(event_data)
+
         exc = event.interfaces["exception"].values[0]
         bt = exc.stacktrace
         frames = bt.frames
@@ -184,14 +186,17 @@ class BasicResolvingIntegrationTest(TestCase):
             "timestamp": iso_format(before_now(seconds=1)),
         }
 
-        resp = self._postWithHeader(event_data)
-        assert resp.status_code == 200
-        event_id = json.loads(resp.content)["id"]
-
-        event = eventstore.get_event_by_id(self.project.id, event_id)
+        event = self.post_and_retrieve_event(event_data)
 
         assert len(event.data["errors"]) == 1
         assert event.data["errors"][0] == {
             "mapping_uuid": u"071207ac-b491-4a74-957c-2c94fd9594f2",
             "type": "proguard_missing_lineno",
         }
+
+
+@pytest.mark.sentry_store_integration
+class BasicResolvingIntegrationTestLegacy(
+    SentryStoreHelper, TestCase, BasicResolvingIntegrationTest
+):
+    pass
diff --git a/tests/sentry/lang/javascript/test_example.py b/tests/sentry/lang/javascript/test_example.py
index 7815f9a8bb..2f7c0c44bd 100644
--- a/tests/sentry/lang/javascript/test_example.py
+++ b/tests/sentry/lang/javascript/test_example.py
@@ -4,10 +4,10 @@ from __future__ import absolute_import
 
 import os
 import json
+import pytest
 import responses
 
-from sentry import eventstore
-from sentry.testutils import TestCase
+from sentry.testutils import TransactionTestCase, SentryStoreHelper
 from sentry.testutils.helpers.datetime import iso_format, before_now
 
 
@@ -20,7 +20,12 @@ def load_fixture(name):
         return f.read()
 
 
-class ExampleTestCase(TestCase):
+class ExampleTestCase(object):
+    def post_and_retrieve_event(self, data):
+        raise NotImplementedError(
+            "post_and_retrieve_event should be implemented in dervied test class"
+        )
+
     @responses.activate
     def test_sourcemap_expansion(self):
         responses.add(
@@ -61,11 +66,7 @@ class ExampleTestCase(TestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code == 200
-        event_id = json.loads(resp.content)["id"]
-
-        event = eventstore.get_event_by_id(self.project.id, event_id)
+        event = self.post_and_retrieve_event(data)
 
         exception = event.interfaces["exception"]
         frame_list = exception.values[0].stacktrace.frames
@@ -91,3 +92,8 @@ class ExampleTestCase(TestCase):
         assert frame_list[3].function == "onFailure"
         assert frame_list[3].lineno == 5
         assert frame_list[3].filename == "test.js"
+
+
+@pytest.mark.sentry_store_integration
+class ExampleTestCaseLegacy(SentryStoreHelper, TransactionTestCase, ExampleTestCase):
+    pass
diff --git a/tests/sentry/lang/javascript/test_plugin.py b/tests/sentry/lang/javascript/test_plugin.py
index 1a73d8e37c..e4bd132972 100644
--- a/tests/sentry/lang/javascript/test_plugin.py
+++ b/tests/sentry/lang/javascript/test_plugin.py
@@ -3,21 +3,26 @@
 from __future__ import absolute_import
 
 import os.path
+import pytest
 import responses
+
 from base64 import b64encode
 
 from sentry.utils.compat.mock import patch
-from sentry import eventstore
 from sentry.models import File, Release, ReleaseFile
-from sentry.testutils import TestCase, SnubaTestCase
-from sentry.testutils.helpers.datetime import iso_format, before_now
-from sentry.utils import json
+from sentry.testutils import TestCase, SnubaTestCase, SentryStoreHelper
 
+from sentry.testutils.helpers.datetime import iso_format, before_now
 
 # TODO(joshuarli): six 1.12.0 adds ensure_binary
 # might also want to put this in utils since we pretty much expect the result to be py3 str and not bytes
 BASE64_SOURCEMAP = "data:application/json;base64," + (
-    b64encode(u'{"version":3,"file":"generated.js","sources":["/test.js"],"names":[],"mappings":"AAAA","sourcesContent":["console.log(\\"hello, World!\\")"]}'.encode("utf-8")).decode("utf-8").replace("\n", "")
+    b64encode(
+        u'{"version":3,"file":"generated.js","sources":["/test.js"],"names":[],"mappings":"AAAA","sourcesContent":['
+        '"console.log(\\"hello, World!\\")"]}'.encode("utf-8")
+    )
+    .decode("utf-8")
+    .replace("\n", "")
 )
 
 
@@ -30,14 +35,11 @@ def load_fixture(name):
         return fp.read()
 
 
-class JavascriptIntegrationTest(TestCase, SnubaTestCase):
+class JavascriptIntegrationTest(SnubaTestCase):
     def setUp(self):
         super(JavascriptIntegrationTest, self).setUp()
         self.min_ago = iso_format(before_now(minutes=1))
 
-    def get_event(self, event_id):
-        return eventstore.get_event_by_id(self.project.id, event_id)
-
     def test_adds_contexts_without_device(self):
         data = {
             "timestamp": self.min_ago,
@@ -48,28 +50,28 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
                 "headers": [
                     [
                         "User-Agent",
-                        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.72 Safari/537.36",
+                        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) "
+                        "Chrome/28.0.1500.72 Safari/537.36",
                     ]
                 ],
             },
         }
 
-        # We do a preflight post, because there are many queries polluting the array
-        # before the actual "processing" happens (like, auth_user)
-        self._postWithHeader(data)
-        with self.assertWriteQueries(
-            {
-                "nodestore_node": 2,
-                "sentry_eventuser": 1,
-                "sentry_groupedmessage": 1,
-                "sentry_userreport": 1,
-            },
-            debug=True,
-        ):  # debug=True is for coverage
-            resp = self._postWithHeader(data)
-        assert resp.status_code, 200
-
-        event = self.get_event(json.loads(resp.content)["id"])
+        event = self.post_and_retrieve_event(data)
+        if not self.use_relay():
+            # We measure the number of queries after an initial post,
+            # because there are many queries polluting the array
+            # before the actual "processing" happens (like, auth_user)
+            with self.assertWriteQueries(
+                {
+                    "nodestore_node": 2,
+                    "sentry_eventuser": 1,
+                    "sentry_groupedmessage": 1,
+                    "sentry_userreport": 1,
+                },
+                debug=True,
+            ):  # debug=True is for coverage
+                self._postWithHeader(data)
 
         contexts = event.interfaces["contexts"].to_json()
         assert contexts.get("os") == {"name": "Windows", "version": "8", "type": "os"}
@@ -85,16 +87,15 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
                 "headers": [
                     [
                         "User-Agent",
-                        "Mozilla/5.0 (Linux; U; Android 4.3; en-us; SCH-R530U Build/JSS15J) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 USCC-R530U",
+                        "Mozilla/5.0 (Linux; U; Android 4.3; en-us; SCH-R530U Build/JSS15J) AppleWebKit/534.30 ("
+                        "KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 USCC-R530U",
                     ]
                 ],
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         contexts = event.interfaces["contexts"].to_json()
         assert contexts.get("os") == {"name": "Android", "type": "os", "version": "4.3"}
         assert contexts.get("browser") == {"name": "Android", "type": "browser", "version": "4.3"}
@@ -121,10 +122,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         contexts = event.interfaces["contexts"].to_json()
         assert contexts.get("os") is None
         assert contexts.get("browser") is None
@@ -170,8 +169,7 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         mock_fetch_file.return_value.encoding = None
         mock_fetch_file.return_value.headers = {}
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
         mock_fetch_file.assert_called_once_with(
             "http://example.com/foo.js",
@@ -181,7 +179,6 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             allow_scraping=True,
         )
 
-        event = self.get_event(json.loads(resp.content)["id"])
         exception = event.interfaces["exception"]
         frame_list = exception.values[0].stacktrace.frames
 
@@ -230,8 +227,7 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         mock_fetch_file.return_value.body = "\n".join("<generated source>")
         mock_fetch_file.return_value.encoding = None
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
         mock_fetch_file.assert_called_once_with(
             "http://example.com/test.min.js",
@@ -241,7 +237,6 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             allow_scraping=True,
         )
 
-        event = self.get_event(json.loads(resp.content)["id"])
         exception = event.interfaces["exception"]
         frame_list = exception.values[0].stacktrace.frames
 
@@ -258,23 +253,22 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             "message": "hello",
             "platform": "javascript",
             "logentry": {
-                "formatted": u"ReferenceError: Impossible de d\xe9finir une propri\xe9t\xe9 \xab foo \xbb : objet non extensible"
+                "formatted": u"ReferenceError: Impossible de d\xe9finir une propri\xe9t\xe9 \xab foo \xbb : objet non "
+                u"extensible"
             },
             "exception": {
                 "values": [
                     {"type": "Error", "value": u"P\u0159\xedli\u0161 mnoho soubor\u016f"},
                     {
                         "type": "Error",
-                        "value": u"foo: wyst\u0105pi\u0142 nieoczekiwany b\u0142\u0105d podczas pr\xf3by uzyskania informacji o metadanych",
+                        "value": u"foo: wyst\u0105pi\u0142 nieoczekiwany b\u0142\u0105d podczas pr\xf3by uzyskania "
+                        u"informacji o metadanych",
                     },
                 ]
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
-
-        event = self.get_event(json.loads(resp.content)["id"])
+        event = self.post_and_retrieve_event(data)
 
         message = event.interfaces["logentry"]
         assert (
@@ -349,10 +343,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [
             {"type": "js_no_source", "url": "http//example.com/index.html"}
         ]
@@ -371,7 +363,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         assert not raw_frame.pre_context
         assert (
             raw_frame.context_line
-            == 'function add(a,b){"use strict";return a+b}function multiply(a,b){"use strict";return a*b}function divide(a,b){"use strict";try{return multip {snip}'
+            == 'function add(a,b){"use strict";return a+b}function multiply(a,b){"use strict";return a*b}function '
+            'divide(a,b){"use strict";try{return multip {snip}'
         )
         assert raw_frame.post_context == ["//@ sourceMappingURL=file.sourcemap.js"]
         assert raw_frame.lineno == 1
@@ -428,10 +421,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [
             {"type": "js_no_source", "url": "http//example.com/index.html"}
         ]
@@ -490,10 +481,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert "errors" not in event.data
 
         exception = event.interfaces["exception"]
@@ -567,10 +556,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert "errors" not in event.data
 
         exception = event.interfaces["exception"]
@@ -588,7 +575,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         assert not raw_frame.pre_context
         assert raw_frame.context_line == 'function add(a,b){"use strict";return a+b}'
         assert raw_frame.post_context == [
-            'function multiply(a,b){"use strict";return a*b}function divide(a,b){"use strict";try{return multiply(add(a,b),a,b)/c}catch(e){Raven.captureE {snip}',
+            'function multiply(a,b){"use strict";return a*b}function divide(a,b){"use strict";try{return multiply('
+            "add(a,b),a,b)/c}catch(e){Raven.captureE {snip}",
             "//# sourceMappingURL=indexed.sourcemap.js",
             "",
         ]
@@ -609,7 +597,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         assert raw_frame.pre_context == ['function add(a,b){"use strict";return a+b}']
         assert (
             raw_frame.context_line
-            == 'function multiply(a,b){"use strict";return a*b}function divide(a,b){"use strict";try{return multiply(add(a,b),a,b)/c}catch(e){Raven.captureE {snip}'
+            == 'function multiply(a,b){"use strict";return a*b}function divide(a,b){"use strict";try{return multiply('
+            "add(a,b),a,b)/c}catch(e){Raven.captureE {snip}"
         )
         assert raw_frame.post_context == ["//# sourceMappingURL=indexed.sourcemap.js", ""]
         assert raw_frame.lineno == 2
@@ -728,10 +717,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert "errors" not in event.data
 
         exception = event.interfaces["exception"]
@@ -874,10 +861,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert "errors" not in event.data
 
         exception = event.interfaces["exception"]
@@ -950,10 +935,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [
             {"url": u"http://example.com/file1.js", "type": "fetch_invalid_http_code", "value": 404}
         ]
@@ -1015,10 +998,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [
             {"url": u"http://example.com/unsupported.sourcemap.js", "type": "js_invalid_source"}
         ]
@@ -1047,10 +1028,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [{"url": u"<data url>", "type": "js_no_source"}]
 
     @responses.activate
@@ -1088,10 +1067,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code == 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert "errors" not in event.data
 
     @responses.activate
@@ -1149,10 +1126,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         assert event.data["errors"] == [
             {"url": u"http://example.com/file1.js", "type": "js_invalid_content"},
             {"url": u"http://example.com/file2.js", "type": "js_invalid_content"},
@@ -1245,10 +1220,7 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
-
-        event = self.get_event(json.loads(resp.content)["id"])
+        event = self.post_and_retrieve_event(data)
 
         exception = event.interfaces["exception"]
         frame_list = exception.values[0].stacktrace.frames
@@ -1356,10 +1328,8 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
             },
         }
 
-        resp = self._postWithHeader(data)
-        assert resp.status_code, 200
+        event = self.post_and_retrieve_event(data)
 
-        event = self.get_event(json.loads(resp.content)["id"])
         exception = event.interfaces["exception"]
         frame_list = exception.values[0].stacktrace.frames
 
@@ -1369,3 +1339,10 @@ class JavascriptIntegrationTest(TestCase, SnubaTestCase):
         # None of the in app should update
         for x in range(6):
             assert not frame_list[x].in_app
+
+
+@pytest.mark.sentry_store_integration
+class JavascriptIntegrationTestLegacy(SentryStoreHelper, TestCase, JavascriptIntegrationTest):
+    def setUp(self):
+        super(JavascriptIntegrationTestLegacy, self).setUp()
+        self.min_ago = iso_format(before_now(minutes=1))
