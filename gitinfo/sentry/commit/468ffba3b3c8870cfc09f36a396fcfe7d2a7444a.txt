commit 468ffba3b3c8870cfc09f36a396fcfe7d2a7444a
Author: Alex Hofsteede <alex@hofsteede.com>
Date:   Fri Mar 30 11:57:48 2018 -0700

    feat: Snuba tsdb implementation. (#7834)
    
    feat(snuba): Add basic TSDB (and TagStore) backend for snuba service

diff --git a/src/sentry/tagstore/snuba/__init__.py b/src/sentry/tagstore/snuba/__init__.py
new file mode 100644
index 0000000000..6ca20d578e
--- /dev/null
+++ b/src/sentry/tagstore/snuba/__init__.py
@@ -0,0 +1,10 @@
+"""
+sentry.tagstore.snuba
+~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2017 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+from __future__ import absolute_import
+
+from .backend import SnubaTagStorage  # NOQA
diff --git a/src/sentry/tagstore/snuba/backend.py b/src/sentry/tagstore/snuba/backend.py
new file mode 100644
index 0000000000..2a84090917
--- /dev/null
+++ b/src/sentry/tagstore/snuba/backend.py
@@ -0,0 +1,108 @@
+"""
+sentry.tagstore.snuba.backend
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+:copyright: (c) 2010-2018 by the Sentry Team, see AUTHORS for more details.
+:license: BSD, see LICENSE for more details.
+"""
+
+from __future__ import absolute_import
+
+from datetime import datetime, timedelta
+import pytz
+import six
+
+from sentry.tagstore import TagKeyStatus
+from sentry.tagstore.base import TagStorage
+from sentry.utils import snuba
+
+
+class SnubaTagStorage(TagStorage):
+
+    # Tag keys and values
+    def get_tag_key(self, project_id, environment_id, key, status=TagKeyStatus.VISIBLE):
+        pass
+
+    def get_tag_keys(self, project_id, environment_id, status=TagKeyStatus.VISIBLE):
+        pass
+
+    def get_tag_value(self, project_id, environment_id, key, value):
+        pass
+
+    def get_tag_values(self, project_id, environment_id, key):
+        pass
+
+    def get_group_tag_key(self, project_id, group_id, environment_id, key):
+        pass
+
+    def get_group_tag_keys(self, project_id, group_id, environment_id, limit=None):
+        pass
+
+    def get_group_tag_value(self, project_id, group_id, environment_id, key, value):
+        pass
+
+    def get_group_tag_values(self, project_id, group_id, environment_id, key):
+        pass
+
+    def get_group_list_tag_value(self, project_id, group_id_list, environment_id, key, value):
+        pass
+
+    def get_group_tag_value_count(self, project_id, group_id, environment_id, key):
+        pass
+
+    def get_group_tag_values_for_users(self, event_users, limit=100):
+        pass
+
+    def get_top_group_tag_values(self, project_id, group_id, environment_id, key, limit=3):
+        pass
+
+    # Releases
+    def get_first_release(self, project_id, group_id):
+        pass
+
+    def get_last_release(self, project_id, group_id):
+        pass
+
+    def get_release_tags(self, project_ids, environment_id, versions):
+        pass
+
+    def get_group_event_ids(self, project_id, group_id, environment_id, tags):
+        pass
+
+    def get_group_ids_for_users(self, project_ids, event_users, limit=100):
+        pass
+
+    def get_groups_user_counts(self, project_id, group_ids, environment_id):
+        pass
+
+    # Search
+    def get_group_ids_for_search_filter(self, project_id, environment_id, tags):
+        from sentry.search.base import ANY, EMPTY
+
+        # Any EMPTY value means there can be no results for this query so
+        # return an empty list immediately.
+        if any(val == EMPTY for _, val in six.iteritems(tags)):
+            return []
+
+        filters = {
+            'environment': [environment_id],
+            'project_id': [project_id],
+        }
+
+        conditions = []
+        for tag, val in six.iteritems(tags):
+            col = 'tags[{}]'.format(tag)
+            if val == ANY:
+                conditions.append((col, 'IS NOT NULL', None))
+            else:
+                conditions.append((col, '=', val))
+
+        end = datetime.utcnow().replace(tzinfo=pytz.UTC)
+        start = end - timedelta(days=90)
+        issues = snuba.query(start, end, ['issue'], conditions, filters)
+
+        # convert
+        #    {issue1: count, ...}
+        # into
+        #    [issue1, ...]
+        return issues.keys()
diff --git a/src/sentry/tsdb/snuba.py b/src/sentry/tsdb/snuba.py
new file mode 100644
index 0000000000..147cec0166
--- /dev/null
+++ b/src/sentry/tsdb/snuba.py
@@ -0,0 +1,167 @@
+from __future__ import absolute_import
+
+import six
+
+from sentry.tsdb.base import BaseTSDB, TSDBModel
+from sentry.utils import snuba
+
+
+class SnubaTSDB(BaseTSDB):
+    """
+    A time series query interface to Snuba
+
+    Write methods are not supported, as the raw data from which we generate our
+    time series is assumed to already exist in snuba.
+
+    Read methods are supported only for models based on group/event data and
+    will return empty results for unsupported models.
+    """
+
+    def __init__(self, **options):
+        super(SnubaTSDB, self).__init__(**options)
+
+    def model_columns(self, model):
+        """
+        Translates TSDB models into the required columns for querying snuba.
+        Returns a tuple of (groupby_column, aggregateby_column)
+        """
+        return {
+            TSDBModel.project: ('project_id', None),
+            TSDBModel.group: ('issue', None),
+            TSDBModel.release: ('release', None),
+            TSDBModel.users_affected_by_group: ('issue', 'user_id'),
+            TSDBModel.users_affected_by_project: ('project_id', 'user_id'),
+            TSDBModel.users_affected_by_project: ('project_id', 'user_id'),
+            TSDBModel.frequent_environments_by_group: ('issue', 'environment'),
+            TSDBModel.frequent_releases_by_group: ('issue', 'release'),
+            TSDBModel.frequent_issues_by_project: ('project_id', 'issue'),
+        }.get(model, None)
+
+    def get_data(self, model, keys, start, end, rollup=None, environment_id=None,
+                 aggregation='count', group_on_model=True, group_on_time=False):
+        """
+        Normalizes all the TSDB parameters and sends a query to snuba.
+
+        `group_on_time`: whether to add a GROUP BY clause on the 'time' field.
+        `group_on_model`: whether to add a GROUP BY clause on the primary model.
+        """
+        model_columns = self.model_columns(model)
+
+        if model_columns is None:
+            raise Exception("Unsupported TSDBModel: {}".format(model.name))
+
+        model_group, model_aggregate = model_columns
+
+        groupby = []
+        if group_on_model and model_group is not None:
+            groupby.append(model_group)
+        if group_on_time:
+            groupby.append('time')
+        if aggregation == 'count' and model_aggregate is not None:
+            # Special case, because count has different semantics, we change:
+            # `COUNT(model_aggregate)` to `COUNT() GROUP BY model_aggregate`
+            groupby.append(model_aggregate)
+            model_aggregate = None
+
+        keys_map = dict(zip(model_columns, self.flatten_keys(keys)))
+        keys_map = {k: v for k, v in six.iteritems(keys_map) if k is not None and v is not None}
+        if environment_id is not None:
+            keys_map['environment'] = [environment_id]
+
+        return snuba.query(start, end, groupby, None, keys_map,
+                           aggregation, model_aggregate, rollup)
+
+    def get_range(self, model, keys, start, end, rollup=None, environment_id=None):
+        result = self.get_data(model, keys, start, end, rollup, environment_id,
+                               aggregation='count', group_on_time=True)
+        # convert
+        #    {group:{timestamp:count, ...}}
+        # into
+        #    {group: [(timestamp, count), ...]}
+        return {k: sorted(result[k].items()) for k in result}
+
+    def get_distinct_counts_series(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
+        result = self.get_data(model, keys, start, end, rollup, environment_id,
+                               aggregation='uniq', group_on_time=True)
+        # convert
+        #    {group:{timestamp:count, ...}}
+        # into
+        #    {group: [(timestamp, count), ...]}
+        return {k: sorted(result[k].items()) for k in result}
+
+    def get_distinct_counts_totals(self, model, keys, start, end=None,
+                                   rollup=None, environment_id=None):
+        return self.get_data(model, keys, start, end, rollup, environment_id,
+                             aggregation='uniq')
+
+    def get_distinct_counts_union(self, model, keys, start, end=None,
+                                  rollup=None, environment_id=None):
+        return self.get_data(model, keys, start, end, rollup, environment_id,
+                             aggregation='uniq', group_on_model=False)
+
+    def get_most_frequent(self, model, keys, start, end=None,
+                          rollup=None, limit=10, environment_id=None):
+        aggregation = 'topK({})'.format(limit)
+        result = self.get_data(model, keys, start, end, rollup, environment_id,
+                               aggregation=aggregation)
+        # convert
+        #    {group:[top1, ...]}
+        # into
+        #    {group: [(top1, score), ...]}
+        for k in result:
+            item_scores = [(v, float(i + 1)) for i, v in enumerate(reversed(result[k]))]
+            result[k] = list(reversed(item_scores))
+
+        return result
+
+    def get_most_frequent_series(self, model, keys, start, end=None,
+                                 rollup=None, limit=10, environment_id=None):
+        aggregation = 'topK({})'.format(limit)
+        result = self.get_data(model, keys, start, end, rollup, environment_id,
+                               aggregation=aggregation, group_on_time=True)
+        # convert
+        #    {group:{timestamp:[top1, ...]}}
+        # into
+        #    {group: [(timestamp, {top1: score, ...}), ...]}
+        for k in result:
+            result[k] = sorted([
+                (timestamp, {v: float(i + 1) for i, v in enumerate(reversed(topk))})
+                for (timestamp, topk) in result[k].items()
+            ])
+
+        return result
+
+    def get_frequency_series(self, model, items, start, end=None,
+                             rollup=None, environment_id=None):
+        result = self.get_data(model, items, start, end, rollup, environment_id,
+                               aggregation='count', group_on_time=True)
+        # convert
+        #    {group:{timestamp:{agg:count}}}
+        # into
+        #    {group: [(timestamp, {agg: count, ...}), ...]}
+        return {k: result[k].items() for k in result}
+
+    def get_frequency_totals(self, model, items, start, end=None, rollup=None, environment_id=None):
+        return self.get_data(model, items, start, end, rollup, environment_id,
+                             aggregation='count')
+
+    def get_optimal_rollup(self, start):
+        """
+        Always return the smallest rollup as we can bucket on any granularity.
+        """
+        return self.rollups.keys()[0]
+
+    def flatten_keys(self, items):
+        """
+        Returns a normalized set of keys based on the various formats accepted
+        by TSDB methods. The input is either just a plain list of keys for the
+        top level or a `{level1_key: [level2_key, ...]}` dictionary->list map.
+        """
+        # Flatten keys
+        if isinstance(items, list):
+            return (items, None)
+        elif isinstance(items, dict):
+            return (items.keys(), list(set.union(*(set(v) for v in items.values()))))
+        else:
+            return (None, None)
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
new file mode 100644
index 0000000000..873d2fda73
--- /dev/null
+++ b/src/sentry/utils/snuba.py
@@ -0,0 +1,172 @@
+from __future__ import absolute_import
+
+from dateutil.parser import parse as parse_datetime
+import json
+import requests
+import six
+
+from sentry.models import Group, GroupHash, Environment, Release, ReleaseProject
+from sentry.utils.dates import to_timestamp
+
+SNUBA = 'http://localhost:5000'
+
+
+def query(start, end, groupby, conditions=None, filter_keys=None,
+          aggregation=None, aggregateby=None, rollup=None, arrayjoin=None):
+    """
+    Sends a query to snuba.
+
+    `conditions`: A list of (column, operator, literal) conditions to be passed
+    to the query. Conditions that we know will not have to be translated should
+    be passed this way (eg tag[foo] = bar).
+
+    `filter_keys`: A dictionary of {col: [key, ...]} that will be converted
+    into "col IN (key, ...)" conditions. These are used to restrict the query to
+    known sets of project/issue/environment/release etc. Appropriate
+    translations (eg. from environment model ID to environment name) are
+    performed on the query, and the inverse translation performed on the
+    result. The project_id(s) to restrict the query to will also be
+    automatically inferred from these keys.
+    """
+    groupby = groupby or []
+    conditions = conditions or []
+    filter_keys = filter_keys or {}
+
+    # Forward and reverse translation maps from model ids to snuba keys, per column
+    snuba_map = {col: get_snuba_map(col, keys) for col, keys in six.iteritems(filter_keys)}
+    snuba_map = {k: v for k, v in six.iteritems(snuba_map) if k is not None and v is not None}
+    rev_snuba_map = {col: dict(reversed(i) for i in keys.items())
+                     for col, keys in six.iteritems(snuba_map)}
+
+    for col, keys in six.iteritems(filter_keys):
+        if col in snuba_map:
+            keys = [snuba_map[col][k] for k in keys]
+        conditions.append((col, 'IN', keys))
+
+    if 'project_id' in filter_keys:
+        # If we are given a set of project ids, use those directly.
+        project_ids = filter_keys['project_id']
+    elif filter_keys:
+        # Otherwise infer the project_ids from any related models
+        ids = [get_related_project_ids(k, filter_keys[k]) for k in filter_keys]
+        project_ids = list(set.union(*map(set, ids)))
+    else:
+        project_ids = []
+
+    if not project_ids:
+        raise Exception("No project_id filter, or none could be inferred from other filters.")
+
+    # If the grouping, aggregation, or any of the conditions reference `issue`
+    # we need to fetch the issue definitions (issue -> fingerprint hashes)
+    get_issues = 'issue' in groupby + [aggregateby] + [c[0] for c in conditions]
+    issues = get_project_issues(project_ids, filter_keys.get('issue')) if get_issues else None
+
+    url = '{0}/query'.format(SNUBA)
+    request = {k: v for k, v in six.iteritems({
+        'from_date': start.isoformat(),
+        'to_date': end.isoformat(),
+        'conditions': conditions,
+        'groupby': groupby,
+        'project': project_ids,
+        'aggregation': aggregation,
+        'aggregateby': aggregateby,
+        'granularity': rollup,
+        'issues': issues,
+        'arrayjoin': arrayjoin,
+    }) if v is not None}
+
+    response = requests.post(url, data=json.dumps(request))
+    # TODO handle error responses
+    response = json.loads(response.text)
+
+    # Validate and scrub response, and translate snuba keys back to IDs
+    expected_cols = groupby + ['aggregate']
+    assert all(c['name'] in expected_cols for c in response['meta'])
+    for d in response['data']:
+        if 'time' in d:
+            d['time'] = int(to_timestamp(parse_datetime(d['time'])))
+        if d['aggregate'] is None:
+            d['aggregate'] = 0
+        for col in rev_snuba_map:
+            if col in d:
+                d[col] = rev_snuba_map[col][d[col]]
+
+    return nest_groups(response['data'], groupby)
+
+
+def nest_groups(data, groups):
+    """
+    Build a nested mapping from query response rows. Each group column
+    gives a new level of nesting and the leaf result is the aggregate
+    """
+    if not groups:
+        # If no groups, just return the aggregate value from the first row
+        return data[0]['aggregate'] if data else None
+    else:
+        g, rest = groups[0], groups[1:]
+        inter = {}
+        for d in data:
+            inter.setdefault(d[g], []).append(d)
+        return {k: nest_groups(v, rest) for k, v in six.iteritems(inter)}
+
+# The following are functions for resolving information from sentry models
+# about projects, environments, and issues (groups). Having this snuba
+# implementation have to know about these relationships is not ideal, and
+# many of these relationships (eg environment id->name) will have already
+# been queried and exist somewhere in the call stack, but for now, lookup
+# is implemented here for simplicity.
+
+
+def get_snuba_map(column, ids):
+    """
+    Some models are stored differently in snuba, eg. as the environment
+    name instead of the the environment ID. Here we look up a set of keys
+    for a given model and return a lookup dictionary from those keys to the
+    equivalent ones in snuba.
+    """
+    mappings = {
+        'environment': (Environment, 'name'),
+        'release': (Release, 'version'),
+    }
+    if column in mappings and ids:
+        model, field = mappings[column]
+        return dict(model.objects.filter(id__in=ids).values_list('id', field))
+    return None
+
+
+def get_project_issues(project_ids, issue_ids=None):
+    """
+    Get a list of issues and associated fingerprint hashes for a list of
+    project ids. If issue_ids is also set, then also restrict to only those
+    issues as well.
+
+    Returns a list: [(issue_id: [hash1, hash2, ...]), ...]
+    """
+    hashes = GroupHash.objects.filter(project__in=project_ids)
+    if issue_ids:
+        hashes = hashes.filter(group_id__in=issue_ids)
+    result = {}
+    for gid, hsh in hashes.values_list('group_id', 'hash'):
+        result.setdefault(gid, []).append(hsh)
+    return list(result.items())
+
+
+def get_related_project_ids(column, ids):
+    """
+    Get the project_ids from a model that has a foreign key to project.
+    """
+    mappings = {
+        'environment': (Environment, 'id', 'project_id'),
+        'issue': (Group, 'id', 'project_id'),
+        'release': (ReleaseProject, 'release_id', 'project_id'),
+    }
+    if ids:
+        if column == "project_id":
+            return ids
+        elif column in mappings:
+            model, id_field, project_field = mappings[column]
+            return model.objects.filter(**{
+                id_field + '__in': ids,
+                project_field + '__isnull': False,
+            }).values_list(project_field, flat=True)
+    return []
diff --git a/tests/sentry/tagstore/snuba/test_backend.py b/tests/sentry/tagstore/snuba/test_backend.py
new file mode 100644
index 0000000000..c4089c2389
--- /dev/null
+++ b/tests/sentry/tagstore/snuba/test_backend.py
@@ -0,0 +1,71 @@
+from __future__ import absolute_import
+
+
+import json
+import responses
+
+from sentry.utils import snuba
+from sentry.models import GroupHash
+from sentry.testutils import TestCase
+from sentry.tagstore.snuba.backend import SnubaTagStorage
+
+
+class TagStorage(TestCase):
+    def setUp(self):
+        self.ts = SnubaTagStorage()
+
+        self.proj1 = self.create_project()
+
+        self.proj1env1 = self.create_environment(project=self.proj1, name='test')
+        self.proj1env2 = self.create_environment(project=self.proj1, name='prod')
+
+        self.proj1group1 = self.create_group(self.proj1)
+        self.proj1group2 = self.create_group(self.proj1)
+
+        GroupHash.objects.create(project=self.proj1, group=self.proj1group1, hash='1' * 32)
+        GroupHash.objects.create(project=self.proj1, group=self.proj1group2, hash='2' * 32)
+
+    @responses.activate
+    def test_get_group_ids_for_search_filter(self):
+        from sentry.search.base import ANY
+        tags = {
+            'foo': 'bar',
+            'baz': 'quux',
+        }
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response(request):
+                body = json.loads(request.body)
+                assert body['project'] == [self.proj1.id]
+                assert body['groupby'] == ['issue']
+                assert body['issues']
+                assert ['tags[foo]', '=', 'bar'] in body['conditions']
+                assert ['tags[baz]', '=', 'quux'] in body['conditions']
+                return (200, {}, json.dumps({
+                    'meta': [{'name': 'issue'}, {'name': 'aggregate'}],
+                    'data': [{'issue': self.proj1group1.id, 'aggregate': 1}],
+                }))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
+            result = self.ts.get_group_ids_for_search_filter(self.proj1.id, self.proj1env1.id, tags)
+            assert result == [self.proj1group1.id]
+
+        tags = {
+            'foo': ANY,
+        }
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response_2(request):
+                body = json.loads(request.body)
+                assert body['project'] == [self.proj1.id]
+                assert body['groupby'] == ['issue']
+                assert body['issues']
+                assert ['tags[foo]', 'IS NOT NULL', None] in body['conditions']
+                return (200, {}, json.dumps({
+                    'meta': [{'name': 'issue'}, {'name': 'aggregate'}],
+                    'data': [{'issue': self.proj1group2.id, 'aggregate': 1}],
+                }))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response_2)
+            result = self.ts.get_group_ids_for_search_filter(self.proj1.id, self.proj1env1.id, tags)
+            assert result == [self.proj1group2.id]
diff --git a/tests/sentry/tsdb/test_snuba.py b/tests/sentry/tsdb/test_snuba.py
new file mode 100644
index 0000000000..cfd3bbf781
--- /dev/null
+++ b/tests/sentry/tsdb/test_snuba.py
@@ -0,0 +1,193 @@
+from __future__ import absolute_import
+
+from datetime import timedelta
+from dateutil.parser import parse as parse_datetime
+import json
+import pytest
+import responses
+
+from sentry.utils import snuba
+from sentry.models import GroupHash, Release
+from sentry.testutils import TestCase
+from sentry.tsdb.base import TSDBModel
+from sentry.tsdb.snuba import SnubaTSDB
+from sentry.utils.dates import to_timestamp
+
+
+def has_shape(data, shape, allow_empty=False):
+    """
+    Determine if a data object has the provided shape
+
+    At any level, the object in `data` and in `shape` must have the same type.
+    A dict is the same shape if all its keys and values have the same shape as the
+    key/value in `shape`. The number of keys/values is not relevant.
+    A list is the same shape if all its items have the same shape as the value
+    in `shape`
+    A tuple is the same shape if it has the same length as `shape` and all the
+    values have the same shape as the corresponding value in `shape`
+    Any other object simply has to have the same type.
+    If `allow_empty` is set, lists and dicts in `data` will pass even if they are empty.
+    """
+    if type(data) != type(shape):
+        return False
+    if isinstance(data, dict):
+        return (allow_empty or len(data) > 0) and\
+            all(has_shape(k, shape.keys()[0]) for k in data.keys()) and\
+            all(has_shape(v, shape.values()[0]) for v in data.values())
+    elif isinstance(data, list):
+        return (allow_empty or len(data) > 0) and\
+            all(has_shape(v, shape[0]) for v in data)
+    elif isinstance(data, tuple):
+        return len(data) == len(shape) and all(
+            has_shape(data[i], shape[i]) for i in range(len(data)))
+    else:
+        return True
+
+
+class SnubaTSDBTest(TestCase):
+    def setUp(self):
+        self.db = SnubaTSDB()
+
+    @responses.activate
+    def test_result_shape(self):
+        """
+        Tests that the results from the different TSDB methods have the
+        expected format.
+        """
+        now = parse_datetime('2018-03-09T01:00:00Z')
+        project_id = 194503
+        dts = [now + timedelta(hours=i) for i in range(4)]
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response(request):
+                body = json.loads(request.body)
+                meta = [{'name': col} for col in body['groupby'] + ['aggregate']]
+                datum = {col['name']: 1 for col in meta}
+                if 'time' in datum:
+                    datum['time'] = '2018-03-09T01:00:00Z'
+                if body['aggregation'].startswith('topK'):
+                    datum['aggregate'] = [1]
+                return (200, {}, json.dumps({'data': [datum], 'meta': meta}))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
+
+            results = self.db.get_most_frequent(TSDBModel.frequent_issues_by_project,
+                                                [project_id], dts[0], dts[-1])
+            assert has_shape(results, {1: [(1, 1.0)]})
+
+            results = self.db.get_most_frequent_series(TSDBModel.frequent_issues_by_project,
+                                                       [project_id], dts[0], dts[-1])
+            assert has_shape(results, {1: [(1, {1: 1.0})]})
+
+            items = {
+                project_id: (0, 1, 2)  # {project_id: (issue_id, issue_id, ...)}
+            }
+            results = self.db.get_frequency_series(TSDBModel.frequent_issues_by_project,
+                                                   items, dts[0], dts[-1])
+            assert has_shape(results, {1: [(1, {1: 1})]})
+
+            results = self.db.get_frequency_totals(TSDBModel.frequent_issues_by_project,
+                                                   items, dts[0], dts[-1])
+            assert has_shape(results, {1: {1: 1}})
+
+            results = self.db.get_range(TSDBModel.project, [project_id], dts[0], dts[-1])
+            assert has_shape(results, {1: [(1, 1)]})
+
+            results = self.db.get_distinct_counts_series(TSDBModel.users_affected_by_project,
+                                                         [project_id], dts[0], dts[-1])
+            assert has_shape(results, {1: [(1, 1)]})
+
+            results = self.db.get_distinct_counts_totals(TSDBModel.users_affected_by_project,
+                                                         [project_id], dts[0], dts[-1])
+            assert has_shape(results, {1: 1})
+
+            results = self.db.get_distinct_counts_union(TSDBModel.users_affected_by_project,
+                                                        [project_id], dts[0], dts[-1])
+            assert has_shape(results, 1)
+
+    @responses.activate
+    def test_groups(self):
+        now = parse_datetime('2018-03-09T01:00:00Z')
+        dts = [now + timedelta(hours=i) for i in range(4)]
+        project = self.create_project()
+        group = self.create_group(project=project)
+        GroupHash.objects.create(project=project, group=group, hash='0' * 32)
+        group2 = self.create_group(project=project)
+        GroupHash.objects.create(project=project, group=group2, hash='1' * 32)
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response(request):
+                body = json.loads(request.body)
+                assert body['aggregation'] == 'count'
+                assert body['project'] == [project.id]
+                assert body['groupby'] == ['issue', 'time']
+
+                # Assert issue->hash map is generated, but only for referenced issues
+                assert [group.id, ['0' * 32]] in body['issues']
+                assert [group2.id, ['1' * 32]] not in body['issues']
+
+                return (200, {}, json.dumps({
+                    'data': [{'time': '2018-03-09T01:00:00Z', 'issue': 1, 'aggregate': 100}],
+                    'meta': [{'name': 'time'}, {'name': 'issue'}, {'name': 'aggregate'}]
+                }))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
+            results = self.db.get_range(TSDBModel.group, [group.id], dts[0], dts[-1])
+            assert results is not None
+
+    @responses.activate
+    def test_releases(self):
+        now = parse_datetime('2018-03-09T01:00:00Z')
+        project = self.create_project()
+        release = Release.objects.create(
+            organization_id=self.organization.id,
+            version='version X',
+            date_added=now,
+        )
+        release.add_project(project)
+        dts = [now + timedelta(hours=i) for i in range(4)]
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response(request):
+                body = json.loads(request.body)
+                assert body['aggregation'] == 'count'
+                assert body['project'] == [project.id]
+                assert body['groupby'] == ['release', 'time']
+                assert ['release', 'IN', ['version X']] in body['conditions']
+                return (200, {}, json.dumps({
+                    'data': [{'release': 'version X', 'time': '2018-03-09T01:00:00Z', 'aggregate': 100}],
+                    'meta': [{'name': 'release'}, {'name': 'time'}, {'name': 'aggregate'}]
+                }))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
+            results = self.db.get_range(TSDBModel.release, [release.id], dts[0], dts[-1])
+            assert results == {release.id: [(to_timestamp(now), 100)]}
+
+    @responses.activate
+    def test_environment(self):
+        now = parse_datetime('2018-03-09T01:00:00Z')
+        project = self.create_project()
+        env = self.create_environment(project=project, name="prod")
+        dts = [now + timedelta(hours=i) for i in range(4)]
+
+        with responses.RequestsMock() as rsps:
+            def snuba_response(request):
+                body = json.loads(request.body)
+                assert body['aggregation'] == 'count'
+                assert body['project'] == [project.id]
+                assert body['groupby'] == ['project_id', 'time']
+                assert ['environment', 'IN', ['prod']] in body['conditions']
+                return (200, {}, json.dumps({
+                    'data': [{'project_id': project.id, 'time': '2018-03-09T01:00:00Z', 'aggregate': 100}],
+                    'meta': [{'name': 'project_id'}, {'name': 'time'}, {'name': 'aggregate'}]
+                }))
+
+            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
+            results = self.db.get_range(TSDBModel.project, [project.id],
+                                        dts[0], dts[-1], environment_id=env.id)
+            assert results == {project.id: [(to_timestamp(now), 100)]}
+
+    def test_invalid_model(self):
+        with pytest.raises(Exception) as ex:
+            self.db.get_range(TSDBModel.project_total_received_discarded, [], None, None)
+        assert "Unsupported TSDBModel" in ex.value.message
