commit 249944108eb1d324ee9056cc72fd69a58e3f2d0c
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Tue Oct 22 16:29:29 2019 +0200

    feat(outcomes_consumer): Make TSDB processing faster (#15216)
    
    * feat(outcomes_consumer): Use multithreading for processing outcomes
    
    Admittedly a lot of this is CPU-bound, but it still calls out to
    memcached.
    
    * fix: Batch memcached calls instead
    
    * ref: Skip much earlier over outcomes that are not relevant for tsdb
    
    * trigger bot
    
    * fix: Fix wrong if-statement

diff --git a/src/sentry/ingest/outcomes_consumer.py b/src/sentry/ingest/outcomes_consumer.py
index 2bac664ed3..646468b398 100644
--- a/src/sentry/ingest/outcomes_consumer.py
+++ b/src/sentry/ingest/outcomes_consumer.py
@@ -37,9 +37,9 @@ from sentry.utils.kafka import create_batching_kafka_consumer
 from sentry.utils import json, metrics
 from sentry.utils.outcomes import (
     Outcome,
-    mark_tsdb_incremented,
-    is_tsdb_incremented,
+    mark_tsdb_incremented_many,
     tsdb_increments_from_outcome,
+    _get_tsdb_cache_key,
 )
 from sentry.utils.dates import to_datetime, parse_timestamp
 from sentry.buffer.redis import batch_buffers_incr
@@ -126,39 +126,55 @@ def _process_signal_with_timer(message):
         return _process_signal(message)
 
 
-def _process_tsdb_batch(batch):
+def _process_tsdb_batch(messages):
     tsdb_increments = []
+    messages_to_process = []
+    is_tsdb_incremented_requests = []
 
-    for msg in batch:
+    for msg in messages:
         project_id = int(msg.get("project_id") or 0) or None
         event_id = msg.get("event_id")
 
         if not project_id or not event_id:
             continue
 
-        if is_tsdb_incremented(project_id, event_id):
+        to_increment = [
+            (
+                model,
+                key,
+                {
+                    "timestamp": parse_timestamp(msg["timestamp"])
+                    if msg.get("timestamp") is not None
+                    else to_datetime(time.time())
+                },
+            )
+            for model, key in tsdb_increments_from_outcome(
+                org_id=int(msg.get("org_id") or 0) or None,
+                project_id=project_id,
+                key_id=int(msg.get("key_id") or 0) or None,
+                outcome=int(msg.get("outcome", -1)),
+                reason=msg.get("reason") or None,
+            )
+        ]
+
+        if not to_increment:
             continue
 
-        for model, key in tsdb_increments_from_outcome(
-            org_id=int(msg.get("org_id") or 0) or None,
-            project_id=project_id,
-            key_id=int(msg.get("key_id") or 0) or None,
-            outcome=int(msg.get("outcome", -1)),
-            reason=msg.get("reason") or None,
-        ):
-            tsdb_increments.append(
-                (
-                    model,
-                    key,
-                    {
-                        "timestamp": parse_timestamp(msg["timestamp"])
-                        if msg.get("timestamp") is not None
-                        else to_datetime(time.time())
-                    },
-                )
-            )
+        messages_to_process.append((msg, to_increment))
+        is_tsdb_incremented_requests.append(_get_tsdb_cache_key(project_id, event_id))
+
+    is_tsdb_incremented_results = cache.get_many(is_tsdb_incremented_requests)
 
-        mark_tsdb_incremented(project_id, event_id)
+    mark_tsdb_incremented_requests = []
+
+    for (msg, to_increment), should_increment in zip(
+        messages_to_process, is_tsdb_incremented_results
+    ):
+        if should_increment is not None:
+            continue
+
+        tsdb_increments.extend(to_increment)
+        mark_tsdb_incremented_requests.append((project_id, event_id))
         metrics.incr("outcomes_consumer.tsdb_incremented")
 
     metrics.timing("outcomes_consumer.tsdb_incr_multi_size", len(tsdb_increments))
@@ -166,6 +182,9 @@ def _process_tsdb_batch(batch):
     if tsdb_increments:
         tsdb.incr_multi(tsdb_increments)
 
+    if mark_tsdb_incremented_requests:
+        mark_tsdb_incremented_many(mark_tsdb_incremented_requests)
+
 
 class OutcomesConsumerWorker(AbstractBatchWorker):
     def __init__(self, concurrency):
diff --git a/src/sentry/utils/outcomes.py b/src/sentry/utils/outcomes.py
index 097e6d5aa3..c913541a0f 100644
--- a/src/sentry/utils/outcomes.py
+++ b/src/sentry/utils/outcomes.py
@@ -45,7 +45,7 @@ def _get_tsdb_cache_key(project_id, event_id):
     return "is-tsdb-incremented:{}:{}".format(project_id, event_id)
 
 
-def mark_tsdb_incremented(project_id, event_id):
+def mark_tsdb_incremented_many(items):
     """
     Remembers that TSDB was already called for an outcome.
 
@@ -55,13 +55,14 @@ def mark_tsdb_incremented(project_id, event_id):
 
     This is used by the outcomes consumer to avoid double-emission.
     """
-    key = _get_tsdb_cache_key(project_id, event_id)
-    cache.set(key, True, 3600)
+    cache.set_many(
+        dict((_get_tsdb_cache_key(project_id, event_id), True) for project_id, event_id in items),
+        3600,
+    )
 
 
-def is_tsdb_incremented(project_id, event_id):
-    key = _get_tsdb_cache_key(project_id, event_id)
-    return cache.get(key, None) is not None
+def mark_tsdb_incremented(project_id, event_id):
+    mark_tsdb_incremented_many([(project_id, event_id)])
 
 
 def tsdb_increments_from_outcome(org_id, project_id, key_id, outcome, reason):
