commit e2b3cc3aab11b21367d2f909cdb8bd3ec7e594cf
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Wed Sep 6 12:55:55 2017 -0500

    ref: Use deletions code path for cleanup script. (#6020)

diff --git a/src/sentry/deletions/base.py b/src/sentry/deletions/base.py
index 5a43a15bc5..06c9d4314b 100644
--- a/src/sentry/deletions/base.py
+++ b/src/sentry/deletions/base.py
@@ -112,18 +112,19 @@ class BaseDeletionTask(object):
 class ModelDeletionTask(BaseDeletionTask):
     DEFAULT_QUERY_LIMIT = None
 
-    def __init__(self, manager, model, query, query_limit=None, **kwargs):
+    def __init__(self, manager, model, query, query_limit=None, order_by=None, **kwargs):
         super(ModelDeletionTask, self).__init__(manager, **kwargs)
         self.model = model
         self.query = query
         self.query_limit = (query_limit or self.DEFAULT_QUERY_LIMIT or self.chunk_size)
+        self.order_by = order_by
 
     def __repr__(self):
-        return '<%s: model=%s query=%s transaction_id=%s actor_id=%s>' % (
-            type(self), self.model, self.query, self.transaction_id, self.actor_id,
+        return '<%s: model=%s query=%s order_by=%s transaction_id=%s actor_id=%s>' % (
+            type(self), self.model, self.query, self.order_by, self.transaction_id, self.actor_id,
         )
 
-    def chunk(self):
+    def chunk(self, num_shards=None, shard_id=None):
         """
         Deletes a chunk of this instance's data. Return ``True`` if there is
         more work, or ``False`` if the entity has been removed.
@@ -131,7 +132,23 @@ class ModelDeletionTask(BaseDeletionTask):
         query_limit = self.query_limit
         remaining = self.chunk_size
         while remaining > 0:
-            queryset = list(self.model.objects.filter(**self.query)[:query_limit])
+            queryset = self.model.objects.filter(**self.query)
+            if self.order_by:
+                queryset = queryset.order_by(self.order_by)
+
+            if num_shards:
+                assert num_shards > 1
+                assert shard_id < num_shards
+                queryset = queryset.extra(
+                    where=[
+                        'id %% {num_shards} = {shard_id}'.format(
+                            num_shards=num_shards,
+                            shard_id=shard_id,
+                        )
+                    ]
+                )
+
+            queryset = list(queryset[:query_limit])
             if not queryset:
                 return False
 
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index a29e0f6cc9..6ab958d48e 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -7,9 +7,10 @@ sentry.runner.commands.cleanup
 """
 from __future__ import absolute_import, print_function
 
-import click
-
 from datetime import timedelta
+from uuid import uuid4
+
+import click
 from django.utils import timezone
 
 from sentry.runner.decorators import configuration
@@ -72,6 +73,7 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
     from django.db import router as db_router
     from sentry.app import nodestore
     from sentry.db.deletion import BulkDeleteQuery
+    from sentry import deletions
     from sentry import models
 
     if timed:
@@ -89,8 +91,9 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
             return False
         return model.__name__.lower() not in model_list
 
-    # these models should be safe to delete without cascades, in order
-    BULK_DELETES = (
+    # Deletions that use `BulkDeleteQuery` (and don't need to worry about child relations)
+    # (model, datetime_field, order_by)
+    BULK_QUERY_DELETES = (
         (models.GroupEmailThread, 'date', None),
         (models.GroupRuleStatus, 'date_added', None),
         (models.GroupTagValue, 'last_seen', None),
@@ -98,7 +101,12 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
         (models.EventTag, 'date_added', '-date_added'),
     )
 
-    GENERIC_DELETES = ((models.Event, 'datetime'), (models.Group, 'last_seen'), )
+    # Deletions that use the `deletions` code path (which handles their child relations)
+    # (model, datetime_field)
+    DELETES = (
+        (models.Event, 'datetime'),
+        (models.Group, 'last_seen'),
+    )
 
     if not silent:
         click.echo('Removing expired values for LostPasswordHash')
@@ -123,7 +131,8 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
 
     project_id = None
     if project:
-        click.echo("Bulk NodeStore deletion not available for project selection", err=True)
+        click.echo(
+            "Bulk NodeStore deletion not available for project selection", err=True)
         project_id = get_project(project)
         if project_id is None:
             click.echo('Error: Project not found', err=True)
@@ -136,9 +145,10 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
             try:
                 nodestore.cleanup(cutoff)
             except NotImplementedError:
-                click.echo("NodeStore backend does not support cleanup operation", err=True)
+                click.echo(
+                    "NodeStore backend does not support cleanup operation", err=True)
 
-    for model, dtfield, order_by in BULK_DELETES:
+    for model, dtfield, order_by in BULK_QUERY_DELETES:
         if not silent:
             click.echo(
                 "Removing {model} for days={days} project={project}".format(
@@ -158,33 +168,8 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
                 project_id=project_id,
                 order_by=order_by,
             ).execute()
-    # EventMapping is fairly expensive and is special cased as it's likely you
-    # won't need a reference to an event for nearly as long
-    if not silent:
-        click.echo("Removing expired values for EventMapping")
-    if is_filtered(models.EventMapping):
-        if not silent:
-            click.echo('>> Skipping EventMapping')
-    else:
-        BulkDeleteQuery(
-            model=models.EventMapping,
-            dtfield='date_added',
-            days=min(days, 7),
-            project_id=project_id,
-            order_by='-date_added'
-        ).execute()
-
-    # Clean up FileBlob instances which are no longer used and aren't super
-    # recent (as there could be a race between blob creation and reference)
-    if not silent:
-        click.echo("Cleaning up unused FileBlob references")
-    if is_filtered(models.FileBlob):
-        if not silent:
-            click.echo('>> Skipping FileBlob')
-    else:
-        cleanup_unused_files(silent)
 
-    for model, dtfield in GENERIC_DELETES:
+    for model, dtfield in DELETES:
         if not silent:
             click.echo(
                 "Removing {model} for days={days} project={project}".format(
@@ -193,22 +178,39 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
                     project=project or '*',
                 )
             )
+
         if is_filtered(model):
             if not silent:
                 click.echo('>> Skipping %s' % model.__name__)
         else:
-            query = BulkDeleteQuery(
+            query = {
+                '{}__lte'.format(dtfield): (timezone.now() - timedelta(days=days)),
+            }
+
+            if project_id:
+                if 'project' in model._meta.get_all_field_names():
+                    query['project'] = project_id
+                else:
+                    query['project_id'] = project_id
+
+            task = deletions.get(
                 model=model,
-                dtfield=dtfield,
-                days=days,
-                project_id=project_id,
+                query=query,
+                transaction_id=uuid4().hex,
             )
+
+            def _chunk_until_complete(num_shards=None, shard_id=None):
+                has_more = True
+                while has_more:
+                    has_more = task.chunk(num_shards=num_shards, shard_id=shard_id)
+
             if concurrency > 1:
                 threads = []
                 for shard_id in range(concurrency):
                     t = Thread(
                         target=(
-                            lambda shard_id=shard_id: query.execute_sharded(concurrency, shard_id)
+                            lambda shard_id=shard_id: _chunk_until_complete(
+                                num_shards=concurrency, shard_id=shard_id)
                         )
                     )
                     t.start()
@@ -217,7 +219,33 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
                 for t in threads:
                     t.join()
             else:
-                query.execute_generic()
+                _chunk_until_complete()
+
+    # EventMapping is fairly expensive and is special cased as it's likely you
+    # won't need a reference to an event for nearly as long
+    if not silent:
+        click.echo("Removing expired values for EventMapping")
+    if is_filtered(models.EventMapping):
+        if not silent:
+            click.echo('>> Skipping EventMapping')
+    else:
+        BulkDeleteQuery(
+            model=models.EventMapping,
+            dtfield='date_added',
+            days=min(days, 7),
+            project_id=project_id,
+            order_by='-date_added'
+        ).execute()
+
+    # Clean up FileBlob instances which are no longer used and aren't super
+    # recent (as there could be a race between blob creation and reference)
+    if not silent:
+        click.echo("Cleaning up unused FileBlob references")
+    if is_filtered(models.FileBlob):
+        if not silent:
+            click.echo('>> Skipping FileBlob')
+    else:
+        cleanup_unused_files(silent)
 
     if timed:
         duration = int(time.time() - start_time)
