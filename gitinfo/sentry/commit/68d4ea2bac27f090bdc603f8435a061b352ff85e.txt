commit 68d4ea2bac27f090bdc603f8435a061b352ff85e
Author: evanh <evanh@users.noreply.github.com>
Date:   Thu Jan 16 10:48:28 2020 -0500

    feat(discover) Add having aggregate conditions support to backend (#16391)
    
    Allow users to filter their queries using aggregates e.g. count(id):>2i / p95

diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index 17a033dd1e..939d2874e9 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -95,7 +95,7 @@ boolean_term         = (paren_term / search_term) space? (boolean_operator space
 paren_term           = space? open_paren space? (paren_term / boolean_term)+ space? closed_paren space?
 search_term          = key_val_term / quoted_raw_search / raw_search
 key_val_term         = space? (tag_filter / time_filter / rel_time_filter / specific_time_filter
-                       / numeric_filter / has_filter / is_filter / basic_filter)
+                       / numeric_filter / aggregate_filter / aggregate_date_filter / has_filter / is_filter / basic_filter)
                        space?
 raw_search           = (!key_val_term ~r"\ *([^\ ^\n ()]+)\ *" )*
 quoted_raw_search    = spaces quoted_value spaces
@@ -109,16 +109,21 @@ rel_time_filter      = search_key sep rel_date_format
 # exact time filter for dates
 specific_time_filter = search_key sep date_format
 # Numeric comparison filter
-numeric_filter       = search_key sep operator? ~r"[0-9]+(?=\s|$)"
+numeric_filter       = search_key sep operator? numeric_value
+# Aggregate numeric filter
+aggregate_filter        = aggregate_key sep operator? numeric_value
+aggregate_date_filter   = aggregate_key sep operator? (date_format / rel_date_format)
 
 # has filter for not null type checks
 has_filter           = negation? "has" sep (search_key / search_value)
 is_filter            = negation? "is" sep search_value
-tag_filter            = negation? "tags[" search_key "]" sep search_value
+tag_filter           = negation? "tags[" search_key "]" sep search_value
 
+aggregate_key        = key open_paren key closed_paren
 search_key           = key / quoted_key
 search_value         = quoted_value / value
 value                = ~r"[^()\s]*"
+numeric_value        = ~r"[0-9]+(?=\s|$)"
 quoted_value         = ~r"\"((?:[^\"]|(?<=\\)[\"])*)?\""s
 key                  = ~r"[a-zA-Z0-9_\.-]+"
 # only allow colons in quoted keys
@@ -194,6 +199,15 @@ class SearchKey(namedtuple("SearchKey", "name")):
         return TAG_KEY_RE.match(self.name) or self.name not in SEARCH_MAP
 
 
+class AggregateFilter(namedtuple("AggregateFilter", "key operator value")):
+    def __str__(self):
+        return "".join(map(six.text_type, (self.key.name, self.operator, self.value.raw_value)))
+
+
+class AggregateKey(namedtuple("AggregateKey", "name")):
+    pass
+
+
 class SearchValue(namedtuple("SearchValue", "raw_value")):
     @property
     def value(self):
@@ -226,8 +240,11 @@ class SearchVisitor(NodeVisitor):
             "stack.lineno",
             "stack.stack_level",
             "transaction.duration",
-            # TODO(mark) figure out how to safelist aggregate functions/field aliases
-            # so they can be used in conditions
+            "apdex",
+            "impact",
+            "p75",
+            "p95",
+            "p99",
         ]
     )
     date_keys = set(
@@ -360,6 +377,31 @@ class SearchVisitor(NodeVisitor):
             )
             return self._handle_basic_filter(search_key, "=", search_value)
 
+    def visit_aggregate_filter(self, node, children):
+        (search_key, _, operator, search_value) = children
+        operator = operator[0] if not isinstance(operator, Node) else "="
+        try:
+            search_value = SearchValue(int(search_value.text))
+        except ValueError:
+            raise InvalidSearchQuery("Invalid aggregate query condition: %s" % (search_key,))
+        return AggregateFilter(search_key, operator, search_value)
+
+    def visit_aggregate_date_filter(self, node, children):
+        (search_key, _, operator, search_value) = children
+
+        operator = operator[0] if not isinstance(operator, Node) else "="
+        is_date_aggregate = any(key in search_key.name for key in self.date_keys)
+
+        if is_date_aggregate:
+            try:
+                search_value = parse_datetime_string(search_value)
+            except InvalidQuery as exc:
+                raise InvalidSearchQuery(six.text_type(exc))
+            return AggregateFilter(search_key, operator, SearchValue(search_value))
+        else:
+            search_value = operator + search_value if operator != "=" else search_value
+            return AggregateFilter(search_key, "=", SearchValue(search_value))
+
     def visit_time_filter(self, node, children):
         (search_key, _, operator, search_value) = children
         if search_key.name in self.date_keys:
@@ -467,6 +509,10 @@ class SearchVisitor(NodeVisitor):
         key = children[0]
         return SearchKey(self.key_mappings_lookup.get(key, key))
 
+    def visit_aggregate_key(self, node, children):
+        key = "".join(children)
+        return AggregateKey(self.key_mappings_lookup.get(key, key))
+
     def visit_search_value(self, node, children):
         return SearchValue(children[0])
 
@@ -513,6 +559,8 @@ def convert_search_boolean_to_snuba_query(search_boolean):
     def convert_term(term):
         if isinstance(term, SearchFilter):
             return convert_search_filter_to_snuba_query(term)
+        elif isinstance(term, AggregateFilter):
+            return convert_aggregate_filter_to_snuba_query(term, False)
         elif isinstance(term, SearchBoolean):
             return convert_search_boolean_to_snuba_query(term)
         else:
@@ -531,6 +579,27 @@ def convert_search_boolean_to_snuba_query(search_boolean):
     return [operator, [left, right]]
 
 
+def convert_aggregate_filter_to_snuba_query(aggregate_filter, is_alias):
+    name = aggregate_filter.key.name
+    value = aggregate_filter.value.value
+
+    value = (
+        int(to_timestamp(value)) * 1000
+        if isinstance(value, datetime) and name != "timestamp"
+        else value
+    )
+
+    if aggregate_filter.operator in ("=", "!=") and aggregate_filter.value.value == "":
+        return [["isNull", [name]], aggregate_filter.operator, 1]
+
+    _, agg_additions = resolve_field(name)
+    if len(agg_additions) > 0:
+        name = agg_additions[0][-1]
+
+    condition = [name, aggregate_filter.operator, value]
+    return condition
+
+
 def convert_search_filter_to_snuba_query(search_filter):
     name = search_filter.key.name
     value = search_filter.value.value
@@ -658,7 +727,14 @@ def get_filter(query=None, params=None):
         except ParseError as e:
             raise InvalidSearchQuery(u"Parse error: %r (column %d)" % (e.expr.name, e.column()))
 
-    kwargs = {"start": None, "end": None, "conditions": [], "project_ids": [], "group_ids": []}
+    kwargs = {
+        "start": None,
+        "end": None,
+        "conditions": [],
+        "having": [],
+        "project_ids": [],
+        "group_ids": [],
+    }
 
     def get_projects(params):
         return {
@@ -684,10 +760,18 @@ def get_filter(query=None, params=None):
                 kwargs["conditions"].append(condition)
             elif name == "issue.id":
                 kwargs["group_ids"].extend(to_list(term.value.value))
+            elif name in FIELD_ALIASES:
+                converted_filter = convert_aggregate_filter_to_snuba_query(term, True)
+                if converted_filter:
+                    kwargs["having"].append(converted_filter)
             else:
                 converted_filter = convert_search_filter_to_snuba_query(term)
                 if converted_filter:
                     kwargs["conditions"].append(converted_filter)
+        elif isinstance(term, AggregateFilter):
+            converted_filter = convert_aggregate_filter_to_snuba_query(term, False)
+            if converted_filter:
+                kwargs["having"].append(converted_filter)
 
     # Keys included as url params take precedent if same key is included in search
     # They are also considered safe and to have had access rules applied unlike conditions
@@ -805,6 +889,38 @@ def get_aggregate_alias(match):
     return u"{}_{}".format(match.group("function"), column).rstrip("_")
 
 
+def resolve_field(field):
+    if not isinstance(field, six.string_types):
+        raise InvalidSearchQuery("Field names must be strings")
+
+    if field in FIELD_ALIASES:
+        special_field = deepcopy(FIELD_ALIASES[field])
+        return (special_field.get("fields", []), special_field.get("aggregations", []))
+
+    # Basic fields don't require additional validation. They could be tag
+    # names which we have no way of validating at this point.
+    match = AGGREGATE_PATTERN.search(field)
+    if not match:
+        return ([field], None)
+
+    validate_aggregate(field, match)
+
+    if match.group("function") == "count":
+        # count() is a special function that ignores its column arguments.
+        return (None, [["count", None, get_aggregate_alias(match)]])
+
+    return (
+        None,
+        [
+            [
+                VALID_AGGREGATES[match.group("function")]["snuba_name"],
+                match.group("column"),
+                get_aggregate_alias(match),
+            ]
+        ],
+    )
+
+
 def resolve_field_list(fields, snuba_args, auto_fields=True):
     """
     Expand a list of fields based on aliases and aggregate functions.
@@ -824,35 +940,12 @@ def resolve_field_list(fields, snuba_args, auto_fields=True):
     groupby = []
     columns = []
     for field in fields:
-        if not isinstance(field, six.string_types):
-            raise InvalidSearchQuery("Field names must be strings")
-
-        if field in FIELD_ALIASES:
-            special_field = deepcopy(FIELD_ALIASES[field])
-            columns.extend(special_field.get("fields", []))
-            aggregations.extend(special_field.get("aggregations", []))
-            continue
-
-        # Basic fields don't require additional validation. They could be tag
-        # names which we have no way of validating at this point.
-        match = AGGREGATE_PATTERN.search(field)
-        if not match:
-            columns.append(field)
-            continue
+        column_additions, agg_additions = resolve_field(field)
+        if column_additions:
+            columns.extend(column_additions)
 
-        validate_aggregate(field, match)
-
-        if match.group("function") == "count":
-            # count() is a special function that ignores its column arguments.
-            aggregations.append(["count", None, get_aggregate_alias(match)])
-        else:
-            aggregations.append(
-                [
-                    VALID_AGGREGATES[match.group("function")]["snuba_name"],
-                    match.group("column"),
-                    get_aggregate_alias(match),
-                ]
-            )
+        if agg_additions:
+            aggregations.extend(agg_additions)
 
     rollup = snuba_args.get("rollup")
     if not rollup and auto_fields:
diff --git a/src/sentry/eventstore/base.py b/src/sentry/eventstore/base.py
index aafa80384a..bea94a158f 100644
--- a/src/sentry/eventstore/base.py
+++ b/src/sentry/eventstore/base.py
@@ -16,6 +16,7 @@ class Filter(object):
     start (DateTime): Start datetime - default None
     end (DateTime): Start datetime - default None
     conditions (Sequence[Sequence[str, str, Any]]): List of conditions to fetch - default None
+    having (Sequence[str, str, Any]]): List of having conditions to filter by - default None
     project_ids (Sequence[int]): List of project IDs to fetch - default None
     group_ids (Sequence[int]): List of group IDs to fetch - defualt None
     event_ids (Sequence[int]): List of event IDs to fetch - default None
@@ -26,6 +27,7 @@ class Filter(object):
         start=None,
         end=None,
         conditions=None,
+        having=None,
         project_ids=None,
         group_ids=None,
         event_ids=None,
@@ -33,6 +35,7 @@ class Filter(object):
         self.start = start
         self.end = end
         self.conditions = conditions
+        self.having = having
         self.project_ids = project_ids
         self.group_ids = group_ids
         self.event_ids = event_ids
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 359c3f6a3d..b69ba6d9f7 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -343,6 +343,9 @@ def query(
     referrer (str|None) A referrer string to help locate the origin of this query.
     auto_fields (bool) Set to true to have project + eventid fields automatically added.
     """
+    if not selected_columns:
+        raise InvalidSearchQuery("No fields provided")
+
     snuba_filter = get_filter(query, params)
 
     # TODO(mark) Refactor the need for this translation shim once all of
@@ -353,10 +356,10 @@ def query(
         "end": snuba_filter.end,
         "conditions": snuba_filter.conditions,
         "filter_keys": snuba_filter.filter_keys,
+        "having": snuba_filter.having,
         "orderby": orderby,
     }
-    if not selected_columns:
-        raise InvalidSearchQuery("No fields provided")
+
     snuba_args.update(resolve_field_list(selected_columns, snuba_args, auto_fields=auto_fields))
 
     if reference_event:
@@ -367,6 +370,16 @@ def query(
     # Resolve the public aliases into the discover dataset names.
     snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
 
+    # Make sure that any aggregate conditions are also in the selected columns
+    for having_clause in snuba_args.get("having"):
+        found = any(
+            having_clause[0] == agg_clause[-1] for agg_clause in snuba_args.get("aggregations")
+        )
+        if not found:
+            raise InvalidSearchQuery(
+                "Aggregates used in a condition must also be in the selected columns."
+            )
+
     result = raw_query(
         start=snuba_args.get("start"),
         end=snuba_args.get("end"),
@@ -375,6 +388,7 @@ def query(
         aggregations=snuba_args.get("aggregations"),
         selected_columns=snuba_args.get("selected_columns"),
         filter_keys=snuba_args.get("filter_keys"),
+        having=snuba_args.get("having"),
         orderby=snuba_args.get("orderby"),
         dataset=Dataset.Discover,
         limit=limit,
@@ -413,6 +427,7 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
         "end": snuba_filter.end,
         "conditions": snuba_filter.conditions,
         "filter_keys": snuba_filter.filter_keys,
+        "having": snuba_filter.having,
     }
     if not snuba_args["start"] and not snuba_args["end"]:
         raise InvalidSearchQuery("Cannot get timeseries result without a start and end.")
@@ -437,6 +452,7 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
         aggregations=snuba_args.get("aggregations"),
         conditions=snuba_args.get("conditions"),
         filter_keys=snuba_args.get("filter_keys"),
+        having=snuba_args.get("having"),
         start=snuba_args.get("start"),
         end=snuba_args.get("end"),
         rollup=rollup,
@@ -508,6 +524,7 @@ def get_facets(query, params, limit=10, referrer=None):
         "end": snuba_filter.end,
         "conditions": snuba_filter.conditions,
         "filter_keys": snuba_filter.filter_keys,
+        "having": snuba_filter.having,
     }
     # Resolve the public aliases into the discover dataset names.
     snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
@@ -528,7 +545,7 @@ def get_facets(query, params, limit=10, referrer=None):
         filter_keys=snuba_args.get("filter_keys"),
         orderby=["-count", "tags_key"],
         groupby="tags_key",
-        having=[excluded_tags],
+        having=[excluded_tags] + snuba_args.get("having"),
         dataset=Dataset.Discover,
         limit=limit,
         referrer=referrer,
diff --git a/tests/sentry/snuba/test_discover.py b/tests/sentry/snuba/test_discover.py
index 61404177fa..6622f0bb30 100644
--- a/tests/sentry/snuba/test_discover.py
+++ b/tests/sentry/snuba/test_discover.py
@@ -245,6 +245,7 @@ class QueryTransformTest(TestCase):
             start=None,
             conditions=[],
             groupby=[],
+            having=[],
             orderby=None,
             limit=50,
             offset=None,
@@ -272,6 +273,7 @@ class QueryTransformTest(TestCase):
             start=None,
             conditions=[],
             groupby=[],
+            having=[],
             orderby=None,
             limit=50,
             offset=None,
@@ -307,6 +309,7 @@ class QueryTransformTest(TestCase):
             start=None,
             conditions=[],
             groupby=["transaction", "duration"],
+            having=[],
             orderby=None,
             limit=50,
             offset=None,
@@ -340,6 +343,7 @@ class QueryTransformTest(TestCase):
             end=None,
             start=None,
             orderby=None,
+            having=[],
             limit=50,
             offset=None,
             referrer=None,
@@ -369,6 +373,7 @@ class QueryTransformTest(TestCase):
             start=None,
             conditions=[],
             groupby=[],
+            having=[],
             limit=200,
             offset=100,
             referrer=None,
@@ -407,6 +412,7 @@ class QueryTransformTest(TestCase):
             start=None,
             conditions=[],
             groupby=["project_id", "event_id"],
+            having=[],
             limit=50,
             offset=None,
             referrer=None,
@@ -434,6 +440,7 @@ class QueryTransformTest(TestCase):
             ],
             filter_keys={"project_id": [self.project.id]},
             groupby=["timestamp", "transaction", "duration"],
+            having=[],
             orderby=["-timestamp", "-count"],
             dataset=Dataset.Discover,
             end=None,
@@ -465,6 +472,7 @@ class QueryTransformTest(TestCase):
             filter_keys={"project_id": [self.project.id]},
             dataset=Dataset.Discover,
             groupby=["transaction"],
+            having=[],
             orderby=None,
             end=None,
             start=None,
@@ -491,6 +499,7 @@ class QueryTransformTest(TestCase):
             groupby=[],
             dataset=Dataset.Discover,
             aggregations=[],
+            having=[],
             orderby=None,
             end=None,
             start=None,
@@ -519,6 +528,7 @@ class QueryTransformTest(TestCase):
             groupby=[],
             dataset=Dataset.Discover,
             aggregations=[],
+            having=[],
             orderby=None,
             end=None,
             start=None,
@@ -549,6 +559,7 @@ class QueryTransformTest(TestCase):
             groupby=[],
             dataset=Dataset.Discover,
             aggregations=[],
+            having=[],
             orderby=None,
             end=None,
             start=None,
@@ -577,6 +588,7 @@ class QueryTransformTest(TestCase):
             groupby=[],
             dataset=Dataset.Discover,
             aggregations=[],
+            having=[],
             end=end_time,
             start=start_time,
             orderby=None,
@@ -585,6 +597,114 @@ class QueryTransformTest(TestCase):
             referrer=None,
         )
 
+    @patch("sentry.snuba.discover.raw_query")
+    def test_aggregate_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        start_time = before_now(minutes=10)
+        end_time = before_now(seconds=1)
+        discover.query(
+            selected_columns=["transaction", "avg(transaction.duration)"],
+            query="http.method:GET avg(transaction.duration):>5",
+            params={"project_id": [self.project.id], "start": start_time, "end": end_time},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction"],
+            conditions=[["http_method", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction"],
+            dataset=Dataset.Discover,
+            aggregations=[["avg", "duration", "avg_transaction_duration"]],
+            having=[["avg_transaction_duration", ">", 5]],
+            end=end_time,
+            start=start_time,
+            orderby=None,
+            limit=50,
+            offset=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_alias_aggregate_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        start_time = before_now(minutes=10)
+        end_time = before_now(seconds=1)
+        discover.query(
+            selected_columns=["transaction", "p95"],
+            query="http.method:GET p95:>5",
+            params={"project_id": [self.project.id], "start": start_time, "end": end_time},
+        )
+
+        mock_query.assert_called_with(
+            selected_columns=["transaction"],
+            conditions=[["http_method", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction"],
+            dataset=Dataset.Discover,
+            aggregations=[["quantile(0.95)(duration)", None, "p95"]],
+            having=[["p95", ">", 5]],
+            end=end_time,
+            start=start_time,
+            orderby=None,
+            limit=50,
+            offset=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_aggregate_date_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        start_time = before_now(minutes=10)
+        end_time = before_now(seconds=1)
+
+        discover.query(
+            selected_columns=["transaction", "avg(transaction.duration)", "max(time)"],
+            query="http.method:GET max(time):>5",
+            params={"project_id": [self.project.id], "start": start_time, "end": end_time},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction"],
+            conditions=[["http_method", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction"],
+            dataset=Dataset.Discover,
+            aggregations=[
+                ["avg", "duration", "avg_transaction_duration"],
+                ["max", "time", "max_time"],
+            ],
+            having=[["max_time", ">", 5]],
+            end=end_time,
+            start=start_time,
+            orderby=None,
+            limit=50,
+            offset=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_aggregate_condition_missing_selected_column(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        start_time = before_now(minutes=10)
+        end_time = before_now(seconds=1)
+
+        with pytest.raises(InvalidSearchQuery):
+            discover.query(
+                selected_columns=["transaction"],
+                query="http.method:GET max(time):>5",
+                params={"project_id": [self.project.id], "start": start_time, "end": end_time},
+            )
+
 
 class TimeseriesQueryTest(SnubaTestCase, TestCase):
     def setUp(self):
diff --git a/tests/snuba/api/endpoints/test_organization_events_v2.py b/tests/snuba/api/endpoints/test_organization_events_v2.py
index e2f48d72c4..e99208a9bf 100644
--- a/tests/snuba/api/endpoints/test_organization_events_v2.py
+++ b/tests/snuba/api/endpoints/test_organization_events_v2.py
@@ -6,7 +6,6 @@ from django.core.urlresolvers import reverse
 from sentry.testutils import APITestCase, SnubaTestCase
 from sentry.testutils.helpers.datetime import before_now, iso_format
 from sentry.utils.samples import load_data
-import pytest
 
 
 class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
@@ -59,7 +58,9 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
         )
 
         with self.feature("organizations:events-v2"):
-            response = self.client.get(self.url, {"query": "hi \n there"}, format="json")
+            response = self.client.get(
+                self.url, {"field": ["id"], "query": "hi \n there"}, format="json"
+            )
 
         assert response.status_code == 400, response.content
         assert (
@@ -321,7 +322,6 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
         assert data[1]["count_id"] == 2
         assert data[1]["count_unique_user"] == 2
 
-    @pytest.mark.xfail(reason="aggregate comparisons need parser improvements")
     def test_aggregation_comparison(self):
         self.login_as(user=self.user)
         project = self.create_project()
@@ -376,21 +376,51 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 self.url,
                 format="json",
                 data={
-                    "field": ["issue_title", "count(id)", "count_unique(user)"],
-                    "query": "count_id:>1 count_unique_user:>1",
-                    "orderby": "issue_title",
+                    "field": ["issue.id", "count(id)", "count_unique(user)"],
+                    "query": "count(id):>1 count_unique(user):>1",
+                    "orderby": "issue.id",
                 },
             )
 
         assert response.status_code == 200, response.content
-
         assert len(response.data["data"]) == 1
         data = response.data["data"]
         assert data[0]["issue.id"] == event.group_id
         assert data[0]["count_id"] == 2
         assert data[0]["count_unique_user"] == 2
 
-    @pytest.mark.xfail(reason="aggregate comparisons need parser improvements")
+    def test_aggregation_alias_comparison(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        data = load_data("transaction")
+        data["transaction"] = "/aggregates/1"
+        data["timestamp"] = iso_format(before_now(minutes=1))
+        data["start_timestamp"] = iso_format(before_now(minutes=1, seconds=5))
+        self.store_event(data, project_id=project.id)
+
+        data = load_data("transaction")
+        data["transaction"] = "/aggregates/2"
+        data["timestamp"] = iso_format(before_now(minutes=1))
+        data["start_timestamp"] = iso_format(before_now(minutes=1, seconds=3))
+        event = self.store_event(data, project_id=project.id)
+
+        with self.feature("organizations:events-v2"):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["transaction", "p95"],
+                    "query": "event.type:transaction p95:<4000",
+                    "orderby": ["transaction"],
+                },
+            )
+
+        assert response.status_code == 200, response.content
+        assert len(response.data["data"]) == 1
+        data = response.data["data"]
+        assert data[0]["transaction"] == event.transaction
+        assert data[0]["p95"] == 3000
+
     def test_aggregation_comparison_with_conditions(self):
         self.login_as(user=self.user)
         project = self.create_project()
@@ -404,7 +434,7 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
             },
             project_id=project.id,
         )
-        event = self.store_event(
+        self.store_event(
             data={
                 "event_id": "b" * 32,
                 "timestamp": self.min_ago,
@@ -414,7 +444,7 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
             },
             project_id=project.id,
         )
-        self.store_event(
+        event = self.store_event(
             data={
                 "event_id": "c" * 32,
                 "timestamp": self.min_ago,
@@ -440,9 +470,9 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 self.url,
                 format="json",
                 data={
-                    "field": ["issue_title", "count(id)"],
-                    "query": "count_id:>1 user.email:foo@example.com environment:prod",
-                    "orderby": "issue_title",
+                    "field": ["issue.id", "count(id)"],
+                    "query": "count(id):>1 user.email:foo@example.com environment:prod",
+                    "orderby": "issue.id",
                 },
             )
 
@@ -453,6 +483,66 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
         assert data[0]["issue.id"] == event.group_id
         assert data[0]["count_id"] == 2
 
+    def test_aggregation_date_comparison_with_conditions(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        event = self.store_event(
+            data={
+                "event_id": "a" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_1"],
+                "user": {"email": "foo@example.com"},
+                "environment": "prod",
+            },
+            project_id=project.id,
+        )
+        self.store_event(
+            data={
+                "event_id": "b" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_2"],
+                "user": {"email": "foo@example.com"},
+                "environment": "staging",
+            },
+            project_id=project.id,
+        )
+        self.store_event(
+            data={
+                "event_id": "c" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_2"],
+                "user": {"email": "foo@example.com"},
+                "environment": "prod",
+            },
+            project_id=project.id,
+        )
+        self.store_event(
+            data={
+                "event_id": "d" * 32,
+                "timestamp": self.min_ago,
+                "fingerprint": ["group_2"],
+                "user": {"email": "foo@example.com"},
+                "environment": "prod",
+            },
+            project_id=project.id,
+        )
+
+        with self.feature("organizations:events-v2"):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["issue.id", "max(timestamp)"],
+                    "query": "max(timestamp):>1 user.email:foo@example.com environment:prod",
+                    "orderby": "issue.id",
+                },
+            )
+
+        assert response.status_code == 200, response.content
+        assert len(response.data["data"]) == 2
+        data = response.data["data"]
+        assert data[0]["issue.id"] == event.group_id
+
     def test_nonexistent_fields(self):
         self.login_as(user=self.user)
 
