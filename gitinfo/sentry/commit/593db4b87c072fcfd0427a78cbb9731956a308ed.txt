commit 593db4b87c072fcfd0427a78cbb9731956a308ed
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Tue Mar 5 17:39:58 2019 -0600

    Revert "feat: Add options to use Kafka for preprocess/process/event_save tasks (#12159)"
    
    This reverts commit 37447344a796caa1359ec20e361ad7cb57cb5721.

diff --git a/requirements-optional.txt b/requirements-optional.txt
index 194728c7b6..e1020cf725 100644
--- a/requirements-optional.txt
+++ b/requirements-optional.txt
@@ -1,4 +1,3 @@
-batching-kafka-consumer==0.0.3
 confluent-kafka==0.11.5
 GeoIP==1.3.2
 google-cloud-pubsub>=0.35.4,<0.36.0
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index d650ea2209..aeebc1be55 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -867,8 +867,6 @@ SENTRY_FEATURES = {
     'projects:sample-events': True,
     # Enable functionality to trigger service hooks upon event ingestion.
     'projects:servicehooks': False,
-    # Use Kafka (instead of Celery) for ingestion pipeline.
-    'projects:kafka-ingest': False,
 
     # Don't add feature defaults down here! Please add them in their associated
     # group sorted alphabetically.
@@ -1550,34 +1548,3 @@ INVALID_EMAIL_ADDRESS_PATTERN = re.compile(r'\@qq\.com$', re.I)
 SENTRY_USER_PERMISSIONS = (
     'broadcasts.admin',
 )
-
-KAFKA_CLUSTERS = {
-    'default': {
-        'bootstrap.servers': 'localhost:9092',
-        'message.max.bytes': 50000000,  # 50MB, default is 1MB
-    }
-}
-
-KAFKA_PREPROCESS = 'events-preprocess'
-KAFKA_PROCESS = 'events-process'
-KAFKA_SAVE = 'events-save'
-KAFKA_EVENTS = 'events'
-
-KAFKA_TOPICS = {
-    KAFKA_PREPROCESS: {
-        'cluster': 'default',
-        'topic': KAFKA_PREPROCESS,
-    },
-    KAFKA_PROCESS: {
-        'cluster': 'default',
-        'topic': KAFKA_PROCESS,
-    },
-    KAFKA_SAVE: {
-        'cluster': 'default',
-        'topic': KAFKA_SAVE,
-    },
-    KAFKA_EVENTS: {
-        'cluster': 'default',
-        'topic': KAFKA_EVENTS,
-    },
-}
diff --git a/src/sentry/consumer.py b/src/sentry/consumer.py
deleted file mode 100644
index 44243242e4..0000000000
--- a/src/sentry/consumer.py
+++ /dev/null
@@ -1,69 +0,0 @@
-from __future__ import absolute_import, print_function
-
-from batching_kafka_consumer import AbstractBatchWorker
-
-from django.conf import settings
-
-import sentry.tasks.store as store_tasks
-from sentry.utils import json
-
-
-class ConsumerWorker(AbstractBatchWorker):
-    def __init__(self):
-        self.dispatch = {}
-        for key, handler in (
-            (settings.KAFKA_PREPROCESS, self.handle_preprocess),
-            (settings.KAFKA_PROCESS, self.handle_process),
-            (settings.KAFKA_SAVE, self.handle_save)
-        ):
-            topic = settings.KAFKA_TOPICS[key]['topic']
-            self.dispatch[topic] = handler
-
-    def handle_preprocess(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-        process_task = (
-            store_tasks.process_event_from_reprocessing
-            if message['from_reprocessing']
-            else store_tasks.process_event
-        )
-
-        store_tasks._do_preprocess_event(cache_key, data, start_time, event_id, process_task)
-
-    def handle_process(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-
-        if message['from_reprocessing']:
-            task = store_tasks.process_event_from_reprocessing
-        else:
-            task = store_tasks.process_event
-
-        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
-
-    def handle_save(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-        project_id = data['project']
-
-        store_tasks._do_save_event(cache_key, data, start_time, event_id, project_id)
-
-    def process_message(self, message):
-        topic = message.topic()
-        return self._handle(topic, json.loads(message.value()))
-
-    def _handle(self, topic, message):
-        handler = self.dispatch[topic]
-        return handler(message)
-
-    def flush_batch(self, batch):
-        pass
-
-    def shutdown(self):
-        pass
diff --git a/src/sentry/coreapi.py b/src/sentry/coreapi.py
index cdc437134f..9edef871b5 100644
--- a/src/sentry/coreapi.py
+++ b/src/sentry/coreapi.py
@@ -17,20 +17,18 @@ import re
 import six
 import zlib
 
-from django.conf import settings
 from django.core.exceptions import SuspiciousOperation
 from django.utils.crypto import constant_time_compare
 from gzip import GzipFile
 from six import BytesIO
 from time import time
 
-from sentry import features
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.models import ProjectKey
 from sentry.tasks.store import preprocess_event, \
     preprocess_event_from_reprocessing
-from sentry.utils import kafka, json
+from sentry.utils import json
 from sentry.utils.auth import parse_auth_header
 from sentry.utils.http import origin_from_request
 from sentry.utils.strings import decompress
@@ -168,7 +166,7 @@ class ClientApiHelper(object):
         if start_time is None:
             start_time = time()
 
-        # we might be passed some subclasses of dict that fail dumping
+        # we might be passed some sublcasses of dict that fail dumping
         if isinstance(data, CANONICAL_TYPES):
             data = dict(data.items())
 
@@ -182,25 +180,10 @@ class ClientApiHelper(object):
         if attachments is not None:
             attachment_cache.set(cache_key, attachments, cache_timeout)
 
-        # NOTE: Project is bound to the context in most cases in production, which
-        # is enough for us to do `projects:kafka-ingest` testing.
-        project = self.context and self.context.project
-
-        if project and features.has('projects:kafka-ingest', project=project):
-            kafka.produce_sync(
-                settings.KAFKA_PREPROCESS,
-                value=json.dumps({
-                    'cache_key': cache_key,
-                    'start_time': start_time,
-                    'from_reprocessing': from_reprocessing,
-                    'data': data,
-                }),
-            )
-        else:
-            task = from_reprocessing and \
-                preprocess_event_from_reprocessing or preprocess_event
-            task.delay(cache_key=cache_key, start_time=start_time,
-                       event_id=data['event_id'])
+        task = from_reprocessing and \
+            preprocess_event_from_reprocessing or preprocess_event
+        task.delay(cache_key=cache_key, start_time=start_time,
+                   event_id=data['event_id'])
 
 
 @six.add_metaclass(abc.ABCMeta)
diff --git a/src/sentry/eventstream/kafka/backend.py b/src/sentry/eventstream/kafka/backend.py
index 6cfbe82f44..9e031baf85 100644
--- a/src/sentry/eventstream/kafka/backend.py
+++ b/src/sentry/eventstream/kafka/backend.py
@@ -3,26 +3,29 @@ from __future__ import absolute_import
 import logging
 import six
 
-from confluent_kafka import OFFSET_INVALID, TopicPartition
-from django.conf import settings
+from confluent_kafka import OFFSET_INVALID, Producer, TopicPartition
 from django.utils.functional import cached_property
 
 from sentry.eventstream.kafka.consumer import SynchronizedConsumer
 from sentry.eventstream.kafka.protocol import get_task_kwargs_for_message
 from sentry.eventstream.snuba import SnubaProtocolEventStream
-from sentry.utils import json, kafka
+from sentry.utils import json
 
 
 logger = logging.getLogger(__name__)
 
 
 class KafkaEventStream(SnubaProtocolEventStream):
-    def __init__(self, **options):
-        self.topic = settings.KAFKA_TOPICS[settings.KAFKA_EVENTS]['topic']
+    def __init__(self, publish_topic='events', producer_configuration=None, **options):
+        if producer_configuration is None:
+            producer_configuration = {}
+
+        self.publish_topic = publish_topic
+        self.producer_configuration = producer_configuration
 
     @cached_property
     def producer(self):
-        return kafka.producers.get(settings.KAFKA_EVENTS)
+        return Producer(self.producer_configuration)
 
     def delivery_callback(self, error, message):
         if error is not None:
@@ -46,7 +49,7 @@ class KafkaEventStream(SnubaProtocolEventStream):
 
         try:
             self.producer.produce(
-                topic=self.topic,
+                topic=self.publish_topic,
                 key=key.encode('utf-8'),
                 value=json.dumps(
                     (self.EVENT_PROTOCOL_VERSION, _type) + extra_data
diff --git a/src/sentry/features/__init__.py b/src/sentry/features/__init__.py
index 73af1745d4..3ff04fb4ae 100644
--- a/src/sentry/features/__init__.py
+++ b/src/sentry/features/__init__.py
@@ -92,7 +92,6 @@ default_manager.add('projects:sample-events', ProjectFeature)  # NOQA
 default_manager.add('projects:servicehooks', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-view', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-indexing', ProjectFeature)  # NOQA
-default_manager.add('projects:kafka-ingest', ProjectFeature)  # NOQA
 
 # Project plugin features
 default_manager.add('projects:plugins', ProjectPluginFeature)  # NOQA
diff --git a/src/sentry/runner/__init__.py b/src/sentry/runner/__init__.py
index 5d84ec7c19..802d0247d1 100644
--- a/src/sentry/runner/__init__.py
+++ b/src/sentry/runner/__init__.py
@@ -56,7 +56,7 @@ list(
         lambda cmd: cli.add_command(import_string(cmd)), (
             'sentry.runner.commands.backup.export', 'sentry.runner.commands.backup.import_',
             'sentry.runner.commands.cleanup.cleanup', 'sentry.runner.commands.config.config',
-            'sentry.runner.commands.consumer.consumer', 'sentry.runner.commands.createuser.createuser',
+            'sentry.runner.commands.createuser.createuser',
             'sentry.runner.commands.devserver.devserver', 'sentry.runner.commands.django.django',
             'sentry.runner.commands.exec.exec_', 'sentry.runner.commands.files.files',
             'sentry.runner.commands.help.help', 'sentry.runner.commands.init.init',
diff --git a/src/sentry/runner/commands/consumer.py b/src/sentry/runner/commands/consumer.py
deleted file mode 100644
index 29e450044f..0000000000
--- a/src/sentry/runner/commands/consumer.py
+++ /dev/null
@@ -1,49 +0,0 @@
-from __future__ import absolute_import, print_function
-
-import click
-import signal
-from django.conf import settings
-
-from sentry.runner.decorators import configuration
-
-
-@click.command()
-@click.option('--topic', multiple=True, required=True,
-              help='Topic(s) to consume from.')
-@click.option('--consumer-group', default='sentry-consumers',
-              help='Consumer group name.')
-@click.option('--bootstrap-server', default=['localhost:9092'], multiple=True,
-              help='Kafka bootstrap server(s) to use.')
-@click.option('--max-batch-size', default=10000,
-              help='Max number of messages to batch in memory before committing offsets to Kafka.')
-@click.option('--max-batch-time-ms', default=60000,
-              help='Max length of time to buffer messages in memory before committing offsets to Kafka.')
-@click.option('--auto-offset-reset', default='error', type=click.Choice(['error', 'earliest', 'latest']),
-              help='Kafka consumer auto offset reset.')
-@configuration
-def consumer(**options):
-    from batching_kafka_consumer import BatchingKafkaConsumer
-    from sentry.consumer import ConsumerWorker
-
-    known_topics = {x['topic'] for x in settings.KAFKA_TOPICS.values()}
-    topics = options['topic']
-    for topic in topics:
-        if topic not in known_topics:
-            raise RuntimeError("topic '%s' is not one of: %s" % (topic, known_topics))
-
-    consumer = BatchingKafkaConsumer(
-        topics=options['topic'],
-        worker=ConsumerWorker(),
-        max_batch_size=options['max_batch_size'],
-        max_batch_time=options['max_batch_time_ms'],
-        bootstrap_servers=options['bootstrap_server'],
-        group_id=options['consumer_group'],
-        auto_offset_reset=options['auto_offset_reset'],
-    )
-
-    def handler(signum, frame):
-        consumer.signal_shutdown()
-
-    signal.signal(signal.SIGINT, handler)
-
-    consumer.run()
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index d1992a7223..4de2825261 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -13,14 +13,13 @@ from datetime import datetime
 import six
 
 from time import time
-from django.conf import settings
 from django.utils import timezone
 
 from sentry import features, reprocessing
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
-from sentry.utils import json, kafka, metrics
+from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
 from sentry.stacktraces import process_stacktraces, \
     should_process_for_stacktraces
@@ -60,44 +59,8 @@ def should_process(data):
     return False
 
 
-def submit_process(project, from_reprocessing, cache_key, event_id, start_time, data):
-    if features.has('projects:kafka-ingest', project=project):
-        kafka.produce_sync(
-            settings.KAFKA_PROCESS,
-            value=json.dumps({
-                'cache_key': cache_key,
-                'start_time': start_time,
-                'from_reprocessing': from_reprocessing,
-                'data': data,
-            }),
-        )
-    else:
-        task = process_event_from_reprocessing if from_reprocessing else process_event
-        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
-
-
-def submit_save_event(project, cache_key, event_id, start_time, data):
-    if features.has('projects:kafka-ingest', project=project):
-        kafka.produce_sync(
-            settings.KAFKA_SAVE,
-            value=json.dumps({
-                'cache_key': cache_key,
-                'start_time': start_time,
-                'data': data,
-            }),
-        )
-    else:
-        if cache_key:
-            data = None
-
-        save_event.delay(
-            cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
-            project_id=project.id
-        )
-
-
-def _do_preprocess_event(cache_key, data, start_time, event_id, process_task):
-    if cache_key and data is None:
+def _do_preprocess_event(cache_key, data, start_time, event_id, process_event):
+    if cache_key:
         data = default_cache.get(cache_key)
 
     if data is None:
@@ -105,21 +68,24 @@ def _do_preprocess_event(cache_key, data, start_time, event_id, process_task):
         error_logger.error('preprocess.failed.empty', extra={'cache_key': cache_key})
         return
 
-    original_data = data
     data = CanonicalKeyDict(data)
-    project_id = data['project']
+    project = data['project']
 
     with configure_scope() as scope:
-        scope.set_tag("project", project_id)
-
-    project = Project.objects.get_from_cache(id=project_id)
+        scope.set_tag("project", project)
 
     if should_process(data):
-        from_reprocessing = process_task is process_event_from_reprocessing
-        submit_process(project, from_reprocessing, cache_key, event_id, start_time, original_data)
+        process_event.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
         return
 
-    submit_save_event(project, cache_key, event_id, start_time, original_data)
+    # If we get here, that means the event had no preprocessing needed to be done
+    # so we can jump directly to save_event
+    if cache_key:
+        data = None
+    save_event.delay(
+        cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
+        project_id=project
+    )
 
 
 @instrumented_task(
@@ -146,11 +112,10 @@ def preprocess_event_from_reprocessing(
     )
 
 
-def _do_process_event(cache_key, start_time, event_id, process_task, data=None):
+def _do_process_event(cache_key, start_time, event_id, process_task):
     from sentry.plugins import plugins
 
-    if data is None:
-        data = default_cache.get(cache_key)
+    data = default_cache.get(cache_key)
 
     if data is None:
         metrics.incr(
@@ -163,15 +128,15 @@ def _do_process_event(cache_key, start_time, event_id, process_task, data=None):
         return
 
     data = CanonicalKeyDict(data)
-    project_id = data['project']
+    project = data['project']
 
     with configure_scope() as scope:
-        scope.set_tag("project", project_id)
+        scope.set_tag("project", project)
 
     has_changed = False
 
     # Fetch the reprocessing revision
-    reprocessing_rev = reprocessing.get_reprocessing_revision(project_id)
+    reprocessing_rev = reprocessing.get_reprocessing_revision(project)
 
     # Event enhancers.  These run before anything else.
     for plugin in plugins.all(version=2):
@@ -200,19 +165,13 @@ def _do_process_event(cache_key, start_time, event_id, process_task, data=None):
                 data = result
                 has_changed = True
 
-    assert data['project'] == project_id, 'Project cannot be mutated by preprocessor'
-    project = Project.objects.get_from_cache(id=project_id)
-
-    # We cannot persist canonical types in the cache, so we need to
-    # downgrade this.
-    if isinstance(data, CANONICAL_TYPES):
-        data = dict(data.items())
+    assert data['project'] == project, 'Project cannot be mutated by preprocessor'
 
     if has_changed:
         issues = data.get('processing_issues')
         try:
             if issues and create_failed_event(
-                cache_key, project_id, list(issues.values()),
+                cache_key, project, list(issues.values()),
                 event_id=event_id, start_time=start_time,
                 reprocessing_rev=reprocessing_rev
             ):
@@ -221,15 +180,20 @@ def _do_process_event(cache_key, start_time, event_id, process_task, data=None):
             # If `create_failed_event` indicates that we need to retry we
             # invoke outselves again.  This happens when the reprocessing
             # revision changed while we were processing.
-            from_reprocessing = process_task is process_event_from_reprocessing
-            submit_process(project, from_reprocessing, cache_key, event_id, start_time, data)
             process_task.delay(cache_key, start_time=start_time,
                                event_id=event_id)
             return
 
+        # We cannot persist canonical types in the cache, so we need to
+        # downgrade this.
+        if isinstance(data, CANONICAL_TYPES):
+            data = dict(data.items())
         default_cache.set(cache_key, data, 3600)
 
-    submit_save_event(project, cache_key, event_id, start_time, data)
+    save_event.delay(
+        cache_key=cache_key, data=None, start_time=start_time, event_id=event_id,
+        project_id=project
+    )
 
 
 @instrumented_task(
@@ -379,8 +343,9 @@ def save_attachment(event, attachment):
     )
 
 
-def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
-                   project_id=None, **kwargs):
+@instrumented_task(name='sentry.tasks.store.save_event', queue='events.save_event')
+def save_event(cache_key=None, data=None, start_time=None, event_id=None,
+               project_id=None, **kwargs):
     """
     Saves an event to the database.
     """
@@ -388,7 +353,7 @@ def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
     from sentry import quotas, tsdb
     from sentry.models import ProjectKey
 
-    if cache_key and data is None:
+    if cache_key:
         data = default_cache.get(cache_key)
 
     if data is not None:
@@ -488,9 +453,3 @@ def _do_save_event(cache_key=None, data=None, start_time=None, event_id=None,
                 'events.time-to-process',
                 time() - start_time,
                 instance=data['platform'])
-
-
-@instrumented_task(name='sentry.tasks.store.save_event', queue='events.save_event')
-def save_event(cache_key=None, data=None, start_time=None, event_id=None,
-               project_id=None, **kwargs):
-    _do_save_event(cache_key, data, start_time, event_id, project_id, **kwargs)
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
deleted file mode 100644
index 2b78369986..0000000000
--- a/src/sentry/utils/kafka.py
+++ /dev/null
@@ -1,56 +0,0 @@
-from __future__ import absolute_import
-
-import logging
-
-from django.conf import settings
-
-
-logger = logging.getLogger(__name__)
-
-
-class ProducerManager(object):
-    """\
-    Manages one `confluent_kafka.Producer` per Kafka cluster.
-
-    See `KAFKA_CLUSTERS` and `KAFKA_TOPICS` in settings.
-    """
-
-    def __init__(self):
-        self.__producers = {}
-
-    def get(self, key):
-        cluster_name = settings.KAFKA_TOPICS[key]['cluster']
-        producer = self.__producers.get(cluster_name)
-
-        if producer:
-            return producer
-
-        from confluent_kafka import Producer
-
-        cluster_options = settings.KAFKA_CLUSTERS[cluster_name]
-        producer = self.__producers[cluster_name] = Producer(cluster_options)
-        return producer
-
-
-producers = ProducerManager()
-
-
-def delivery_callback(error, message):
-    if error is not None:
-        logger.error('Could not publish message (error: %s): %r', error, message)
-
-
-def produce_sync(topic_key, **kwargs):
-    producer = producers.get(topic_key)
-
-    try:
-        producer.produce(
-            topic=settings.KAFKA_TOPICS[topic_key]['topic'],
-            on_delivery=delivery_callback,
-            **kwargs
-        )
-    except Exception as error:
-        logger.error('Could not publish message: %s', error, exc_info=True)
-        return
-
-    producer.flush()
diff --git a/tests/sentry/consumer/__init__.py b/tests/sentry/consumer/__init__.py
deleted file mode 100644
index c3961685ab..0000000000
--- a/tests/sentry/consumer/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from __future__ import absolute_import
diff --git a/tests/sentry/consumer/test_consumer.py b/tests/sentry/consumer/test_consumer.py
deleted file mode 100644
index da77b9b4a0..0000000000
--- a/tests/sentry/consumer/test_consumer.py
+++ /dev/null
@@ -1,102 +0,0 @@
-from __future__ import absolute_import, print_function
-
-from mock import patch
-
-from sentry.consumer import ConsumerWorker
-from sentry.coreapi import ClientApiHelper
-from sentry.models import Event
-from sentry.plugins import Plugin2
-from sentry.testutils import PluginTestCase
-from sentry.utils import json
-
-
-class BasicPreprocessorPlugin(Plugin2):
-    def add_foo(self, data):
-        data['foo'] = 'bar'
-        return data
-
-    def get_event_preprocessors(self, data):
-        if data.get('platform') == 'needs_process':
-            return [self.add_foo]
-
-        return []
-
-    def is_enabled(self, project=None):
-        return True
-
-
-class TestConsumer(PluginTestCase):
-    plugin = BasicPreprocessorPlugin
-
-    def _call_consumer(self, mock_produce):
-        args, kwargs = list(mock_produce.call_args)
-        mock_produce.call_args = None
-        topic = args[0]
-        value = json.loads(kwargs['value'])
-
-        consumer = ConsumerWorker()
-        consumer._handle(topic, value)
-
-    def _create_event_with_platform(self, project, platform):
-        from sentry.event_manager import EventManager
-        em = EventManager({}, project=project)
-        em.normalize()
-        data = em.get_data()
-        data['platform'] = platform
-        return data
-
-    @patch('sentry.tasks.store.save_event')
-    @patch('sentry.tasks.store.preprocess_event')
-    @patch('sentry.utils.kafka.produce_sync')
-    def test_process_path(self, mock_produce, mock_preprocess_event, mock_save_event):
-        with self.feature('projects:kafka-ingest'):
-            project = self.create_project()
-            data = self._create_event_with_platform(project, 'needs_process')
-
-            helper = ClientApiHelper(project_id=self.project.id)
-            helper.context.bind_project(project)
-            helper.insert_data_to_database(data)
-
-            # preprocess
-            self._call_consumer(mock_produce)
-            # process
-            with self.tasks():
-                self._call_consumer(mock_produce)
-            # save
-            self._call_consumer(mock_produce)
-
-            assert mock_preprocess_event.delay.call_count == 0
-            assert mock_save_event.delay.call_count == 0
-
-            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
-            saved_data = event.get_raw_data()
-            assert saved_data['foo'] == 'bar'
-            assert saved_data['platform'] == 'needs_process'
-
-    @patch('sentry.tasks.store.save_event')
-    @patch('sentry.tasks.store.process_event')
-    @patch('sentry.tasks.store.preprocess_event')
-    @patch('sentry.utils.kafka.produce_sync')
-    def test_save_path(self, mock_produce, mock_preprocess_event,
-                       mock_process_event, mock_save_event):
-        with self.feature('projects:kafka-ingest'):
-            project = self.create_project()
-            data = self._create_event_with_platform(project, 'doesnt_need_process')
-
-            helper = ClientApiHelper(project_id=self.project.id)
-            helper.context.bind_project(project)
-            helper.insert_data_to_database(data)
-
-            # preprocess
-            self._call_consumer(mock_produce)
-            # save
-            self._call_consumer(mock_produce)
-
-            assert mock_preprocess_event.delay.call_count == 0
-            assert mock_process_event.delay.call_count == 0
-            assert mock_save_event.delay.call_count == 0
-
-            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
-            saved_data = event.get_raw_data()
-            assert 'foo' not in saved_data
-            assert saved_data['platform'] == 'doesnt_need_process'
