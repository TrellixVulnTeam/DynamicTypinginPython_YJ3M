commit 1159d5cda28aec6d723de1d277c4f919533da4fb
Author: Ted Kaemming <ted@kaemming.com>
Date:   Wed Oct 7 14:23:12 2015 -0700

    Add demonstration of distinct counters using HyperLogLog in TSDB.

diff --git a/example.py b/example.py
new file mode 100644
index 0000000000..9157b637fd
--- /dev/null
+++ b/example.py
@@ -0,0 +1,31 @@
+#!/usr/bin/env python
+
+from sentry.utils.runner import configure
+
+configure()
+
+from datetime import timedelta
+
+from django.utils import timezone
+
+from sentry.app import tsdb
+
+
+items = (1, 2, 3)
+tsdb.record_multi((
+    (tsdb.models.users_affected_by_event, 1, items),
+))
+
+end = timezone.now()
+start = end - timedelta(minutes=5)
+
+interval, results = tsdb.get_distinct_counts(
+    tsdb.models.users_affected_by_event,
+    (0, 1,),
+    start,
+    end,
+)
+
+print interval, results
+print 'Extra time included prior to start position:', timedelta(seconds=int(start.strftime('%s')) - interval[0])
+print 'Extra time included after end position:', timedelta(seconds=interval[1] - int(end.strftime('%s')))
diff --git a/src/sentry/tsdb/base.py b/src/sentry/tsdb/base.py
index 49de15fba1..600736656d 100644
--- a/src/sentry/tsdb/base.py
+++ b/src/sentry/tsdb/base.py
@@ -44,6 +44,8 @@ class TSDBModel(Enum):
     # the number of events blocked due to being blacklisted
     organization_total_blacklisted = 202
 
+    users_affected_by_event = 300
+
 
 class BaseTSDB(object):
     models = TSDBModel
diff --git a/src/sentry/tsdb/redis.py b/src/sentry/tsdb/redis.py
index a68e5cb0d1..5e70ae300f 100644
--- a/src/sentry/tsdb/redis.py
+++ b/src/sentry/tsdb/redis.py
@@ -7,6 +7,7 @@ sentry.tsdb.redis
 """
 from __future__ import absolute_import
 
+import functools
 import logging
 import six
 
@@ -173,3 +174,74 @@ class RedisTSDB(BaseTSDB):
         for key, points in results_by_key.iteritems():
             results_by_key[key] = sorted(points.items())
         return dict(results_by_key)
+
+    def record_multi(self, items, timestamp=None):
+        """
+        Record an occurence of an item in a distinct counter.
+        """
+        if timestamp is None:
+            timestamp = timezone.now()
+
+        pipelines = {}
+        for model, key, values in items:
+            client = self.cluster.get_local_client_for_key(key)
+            pipeline = pipelines.get(client, None)
+            if pipeline is None:
+                # This operation is idempotent, so the pipeline does not need
+                # to be transactional.
+                pipeline = pipelines[client] = client.pipeline(transaction=False)
+
+            for rollup, max_values in self.rollups:
+                expire = rollup * max_values  # XXX: This logic can lead to incorrect expiry values.
+
+                m = self.get_model_key(key)
+                k = self.make_key(model, self.normalize_to_rollup(timestamp, rollup), m)
+                pipeline.pfadd(k, m, *values)
+                pipeline.expire(k, expire)
+
+        # NOTE: This isn't concurrent.
+        for pipeline in pipelines.values():
+            pipeline.execute()
+
+    def get_distinct_counts(self, model, keys, start, end):
+        """
+        Count distinct items during a time range.
+        """
+        # NOTE: "optimal" here means "able to most closely reflect the upper
+        # and lower bounds", not "able to construct the most efficient query"
+        rollup = self.get_optimal_rollup(start, end)
+
+        intervals = [self.normalize_to_epoch(start, rollup)]
+        end_ts = int(end.strftime('%s'))  # XXX: HACK
+        while intervals[-1] + rollup < end_ts:
+            intervals.append(intervals[-1] + rollup)
+
+        def get_key(key, timestamp):
+            return self.make_key(
+                model,
+                self.normalize_ts_to_rollup(timestamp, rollup),
+                self.get_model_key(key),
+            )
+
+        pipelines = {}
+        for key in keys:
+            client = self.cluster.get_local_client_for_key(key)
+            result = pipelines.get(client, None)
+            if result is not None:
+                pipeline, requests = result
+            else:
+                pipeline, requests = pipelines[client] = client.pipeline(transaction=False), []
+
+            make_key = functools.partial(get_key, key)
+            pipeline.execute_command('pfcount', *map(make_key, intervals))
+            requests.append(key)
+
+        # NOTE: This isn't concurrent.
+        results = {}
+        for pipeline, requests in pipelines.values():
+            for request, response in zip(requests, pipeline.execute()):
+                results[request] = int(response)
+
+        upper = intervals[0]
+        lower = intervals[-1] + rollup
+        return (upper, lower), results
