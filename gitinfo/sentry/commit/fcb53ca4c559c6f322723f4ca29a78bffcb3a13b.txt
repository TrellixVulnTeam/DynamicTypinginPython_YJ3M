commit fcb53ca4c559c6f322723f4ca29a78bffcb3a13b
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Mon Mar 23 10:05:16 2020 +0100

    ref: Put the ingest consumer in devserver (#17726)
    
    * Allow `--consumer-type` to be specified multiple times to pull from multiple topics at once. (note: I am not particularly happy with that part of the interface but it's backwards compatible). At the same time the option is no longer mandatory: By default it pulls from all three topics at once.
    * devserver checks for `settings.SENTRY_USE_RELAY` and runs the consumer if relay is used.
    
    still open: do we really want to have a singular topic for all? What simplicity do we gain from this given that the consumer can now pull from all three?

diff --git a/src/sentry/ingest/ingest_consumer.py b/src/sentry/ingest/ingest_consumer.py
index 87db9af714..902b3cd845 100644
--- a/src/sentry/ingest/ingest_consumer.py
+++ b/src/sentry/ingest/ingest_consumer.py
@@ -8,7 +8,6 @@ from six import BytesIO
 import multiprocessing.dummy
 import multiprocessing as _multiprocessing
 
-from django.conf import settings
 from django.core.cache import cache
 
 from sentry import eventstore, features, options
@@ -22,6 +21,7 @@ from sentry.utils.cache import cache_key_for_event
 from sentry.utils.kafka import create_batching_kafka_consumer
 from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
 from sentry.attachments import CachedAttachment, MissingAttachmentChunks, attachment_cache
+from sentry.ingest.types import ConsumerType
 from sentry.ingest.userreport import Conflict, save_userreport
 from sentry.event_manager import save_transaction_events
 
@@ -31,26 +31,6 @@ logger = logging.getLogger(__name__)
 CACHE_TIMEOUT = 3600
 
 
-class ConsumerType(object):
-    """
-    Defines the types of ingestion consumers
-    """
-
-    Events = "events"  # consumes simple events ( from the Events topic)
-    Attachments = "attachments"  # consumes events with attachments ( from the Attachments topic)
-    Transactions = "transactions"  # consumes transaction events ( from the Transactions topic)
-
-    @staticmethod
-    def get_topic_name(consumer_type):
-        if consumer_type == ConsumerType.Events:
-            return settings.KAFKA_INGEST_EVENTS
-        elif consumer_type == ConsumerType.Attachments:
-            return settings.KAFKA_INGEST_ATTACHMENTS
-        elif consumer_type == ConsumerType.Transactions:
-            return settings.KAFKA_INGEST_TRANSACTIONS
-        raise ValueError("Invalid consumer type", consumer_type)
-
-
 class IngestConsumerWorker(AbstractBatchWorker):
     def __init__(self, concurrency):
         self.pool = _multiprocessing.dummy.Pool(concurrency)
@@ -279,13 +259,15 @@ def process_userreport(message, projects):
         return False
 
 
-def get_ingest_consumer(consumer_type, once=False, concurrency=None, **options):
+def get_ingest_consumer(consumer_types, once=False, concurrency=None, **options):
     """
     Handles events coming via a kafka queue.
 
     The events should have already been processed (normalized... ) upstream (by Relay).
     """
-    topic_name = ConsumerType.get_topic_name(consumer_type)
+    topic_names = set(
+        ConsumerType.get_topic_name(consumer_type) for consumer_type in consumer_types
+    )
     return create_batching_kafka_consumer(
-        topic_name=topic_name, worker=IngestConsumerWorker(concurrency=concurrency), **options
+        topic_names=topic_names, worker=IngestConsumerWorker(concurrency=concurrency), **options
     )
diff --git a/src/sentry/ingest/outcomes_consumer.py b/src/sentry/ingest/outcomes_consumer.py
index 95d2d007b9..f7168a1e66 100644
--- a/src/sentry/ingest/outcomes_consumer.py
+++ b/src/sentry/ingest/outcomes_consumer.py
@@ -144,7 +144,7 @@ def get_outcomes_consumer(concurrency=None, **options):
     """
 
     return create_batching_kafka_consumer(
-        topic_name=settings.KAFKA_OUTCOMES,
+        topic_names=[settings.KAFKA_OUTCOMES],
         worker=OutcomesConsumerWorker(concurrency=concurrency),
         **options
     )
diff --git a/src/sentry/ingest/types.py b/src/sentry/ingest/types.py
new file mode 100644
index 0000000000..9241ebd000
--- /dev/null
+++ b/src/sentry/ingest/types.py
@@ -0,0 +1,27 @@
+from __future__ import absolute_import
+
+
+class ConsumerType(object):
+    """
+    Defines the types of ingestion consumers
+    """
+
+    Events = "events"  # consumes simple events ( from the Events topic)
+    Attachments = "attachments"  # consumes events with attachments ( from the Attachments topic)
+    Transactions = "transactions"  # consumes transaction events ( from the Transactions topic)
+
+    @staticmethod
+    def all():
+        return (ConsumerType.Events, ConsumerType.Attachments, ConsumerType.Transactions)
+
+    @staticmethod
+    def get_topic_name(consumer_type):
+        from django.conf import settings
+
+        if consumer_type == ConsumerType.Events:
+            return settings.KAFKA_INGEST_EVENTS
+        elif consumer_type == ConsumerType.Attachments:
+            return settings.KAFKA_INGEST_ATTACHMENTS
+        elif consumer_type == ConsumerType.Transactions:
+            return settings.KAFKA_INGEST_TRANSACTIONS
+        raise ValueError("Invalid consumer type", consumer_type)
diff --git a/src/sentry/runner/commands/devserver.py b/src/sentry/runner/commands/devserver.py
index 40ff230268..979fd69b74 100644
--- a/src/sentry/runner/commands/devserver.py
+++ b/src/sentry/runner/commands/devserver.py
@@ -55,7 +55,7 @@ def devserver(reload, watchers, workers, experimental_spa, styleguide, prefix, e
     os.environ["SENTRY_ENVIRONMENT"] = environment
     # NODE_ENV *must* use production for any prod-like environment as third party libraries look
     # for this magic constant
-    os.environ["NODE_ENV"] = 'production' if environment.startswith('prod') else environment
+    os.environ["NODE_ENV"] = "production" if environment.startswith("prod") else environment
 
     from django.conf import settings
     from sentry import options
@@ -171,7 +171,7 @@ def devserver(reload, watchers, workers, experimental_spa, styleguide, prefix, e
         if eventstream.requires_post_process_forwarder():
             daemons += [
                 (
-                    "relay",
+                    "post-process-forwarder",
                     [
                         "sentry",
                         "run",
@@ -182,6 +182,9 @@ def devserver(reload, watchers, workers, experimental_spa, styleguide, prefix, e
                 )
             ]
 
+    if settings.SENTRY_USE_RELAY:
+        daemons += [("ingest", ["sentry", "run", "ingest-consumer", "--all-consumer-types"])]
+
     if needs_https and has_https:
         https_port = six.text_type(parsed_url.port)
         https_host = parsed_url.hostname
diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index 6d09dab82c..339a2a3d35 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -8,6 +8,7 @@ import click
 
 from sentry.runner.decorators import configuration, log_options
 from sentry.bgtasks.api import managed_bgtasks
+from sentry.ingest.types import ConsumerType
 
 
 class AddressParamType(click.ParamType):
@@ -412,11 +413,18 @@ def batching_kafka_options(group):
 @run.command("ingest-consumer")
 @log_options()
 @click.option(
+    "consumer_types",
     "--consumer-type",
-    default=None,
-    required=True,
-    help="Specify which type of consumer to create, i.e. from which topic to consume messages.",
-    type=click.Choice(["events", "transactions", "attachments"]),
+    default=[],
+    multiple=True,
+    help="Specify which type of consumer to create, i.e. from which topic to consume messages. By default all ingest-related topics are consumed ",
+    type=click.Choice(ConsumerType.all()),
+)
+@click.option(
+    "--all-consumer-types",
+    default=False,
+    is_flag=True,
+    help="Listen to all consumer types at once.",
 )
 @batching_kafka_options("ingest-consumer")
 @click.option(
@@ -426,25 +434,31 @@ def batching_kafka_options(group):
     help="Spawn this many threads to process messages. Defaults to 1.",
 )
 @configuration
-def ingest_consumer(consumer_type, **options):
+def ingest_consumer(consumer_types, all_consumer_types, **options):
     """
     Runs an "ingest consumer" task.
 
     The "ingest consumer" tasks read events from a kafka topic (coming from Relay) and schedules
     process event celery tasks for them
     """
-    from sentry.ingest.ingest_consumer import ConsumerType, get_ingest_consumer
+    from sentry.ingest.ingest_consumer import get_ingest_consumer
     from sentry.utils import metrics
 
-    if consumer_type == "events":
-        consumer_type = ConsumerType.Events
-    elif consumer_type == "transactions":
-        consumer_type = ConsumerType.Transactions
-    elif consumer_type == "attachments":
-        consumer_type = ConsumerType.Attachments
+    if all_consumer_types:
+        if consumer_types:
+            raise click.ClickException(
+                "Cannot specify --all-consumer types and --consumer-type at the same time"
+            )
+        else:
+            consumer_types = set(ConsumerType.all())
+
+    if not all_consumer_types and not consumer_types:
+        raise click.ClickException("Need to specify --all-consumer-types or --consumer-type")
 
-    with metrics.global_tags(ingest_consumer_type=consumer_type, _all_threads=True):
-        get_ingest_consumer(consumer_type=consumer_type, **options).run()
+    with metrics.global_tags(
+        ingest_consumer_types=",".join(sorted(consumer_types)), _all_threads=True
+    ):
+        get_ingest_consumer(consumer_types=consumer_types, **options).run()
 
 
 @run.command("outcomes-consumer")
diff --git a/src/sentry/utils/batching_kafka_consumer.py b/src/sentry/utils/batching_kafka_consumer.py
index 78253e770e..7ed2b1b373 100644
--- a/src/sentry/utils/batching_kafka_consumer.py
+++ b/src/sentry/utils/batching_kafka_consumer.py
@@ -139,10 +139,10 @@ class BatchingKafkaConsumer(object):
         # new messages)
         self.__batch_processing_time_ms = 0.0
 
-        if not isinstance(topics, (list, tuple)):
-            topics = [topics]
-        elif isinstance(topics, tuple):
+        if isinstance(topics, (tuple, set)):
             topics = list(topics)
+        elif not isinstance(topics, list):
+            topics = [topics]
 
         self.consumer = self.create_consumer(
             topics,
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
index 0b087181c0..220508df31 100644
--- a/src/sentry/utils/kafka.py
+++ b/src/sentry/utils/kafka.py
@@ -53,18 +53,30 @@ class ProducerManager(object):
 producers = ProducerManager()
 
 
-def create_batching_kafka_consumer(topic_name, worker, **options):
-    cluster_name = settings.KAFKA_TOPICS[topic_name]["cluster"]
+def create_batching_kafka_consumer(topic_names, worker, **options):
+    cluster_names = set(settings.KAFKA_TOPICS[topic_name]["cluster"] for topic_name in topic_names)
+    if len(cluster_names) > 1:
+        raise ValueError(
+            "Cannot launch Kafka consumer listening to multiple topics ({}) on different clusters ({})".format(
+                topic_names, cluster_names
+            )
+        )
+
+    cluster_name, = cluster_names
+
     bootstrap_servers = settings.KAFKA_CLUSTERS[cluster_name]["bootstrap.servers"]
     if not isinstance(bootstrap_servers, (list, tuple)):
         bootstrap_servers = bootstrap_servers.split(",")
 
     consumer = BatchingKafkaConsumer(
-        topics=[topic_name],
+        topics=topic_names,
         bootstrap_servers=bootstrap_servers,
         worker=worker,
         metrics=metrics,
-        metrics_default_tags={"topic": topic_name, "group_id": options.get("group_id")},
+        metrics_default_tags={
+            "topics": ",".join(sorted(topic_names)),
+            "group_id": options.get("group_id"),
+        },
         **options
     )
 
diff --git a/src/sentry/utils/pytest/kafka.py b/src/sentry/utils/pytest/kafka.py
index 6abceae522..0cfc06b9c0 100644
--- a/src/sentry/utils/pytest/kafka.py
+++ b/src/sentry/utils/pytest/kafka.py
@@ -176,7 +176,7 @@ def session_ingest_consumer(scope_consumers, kafka_admin, task_runner):
             max_batch_size=1,
             max_batch_time=10,
             group_id=group_id,
-            consumer_type=ConsumerType.Events,
+            consumer_types=[ConsumerType.Events],
             auto_offset_reset="earliest",
         )
 
diff --git a/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_kafka.py b/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_kafka.py
index 462c8bd135..2674db327a 100644
--- a/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_kafka.py
+++ b/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_kafka.py
@@ -99,7 +99,7 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
         max_batch_size=2,
         max_batch_time=5000,
         group_id=group_id,
-        consumer_type=ConsumerType.Events,
+        consumer_types=set([ConsumerType.Events]),
         auto_offset_reset="earliest",
     )
 
