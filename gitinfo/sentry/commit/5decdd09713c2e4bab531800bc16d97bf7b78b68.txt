commit 5decdd09713c2e4bab531800bc16d97bf7b78b68
Author: Lyn Nagara <lyn.nagara@gmail.com>
Date:   Thu Nov 7 16:37:55 2019 -0800

    feat: Save duplicate events (#15044)
    
    This PR switches from a first write wins to a last write wins in the case
    of events with duplicate IDs.
    
    This is the last place that we read from the Postgres events table. Since
    this will be removed soon, we won't be able to check this for the
    presence of a duplicate event ID anymore.

diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index 954dd0bbe6..322943ce6f 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -15,7 +15,7 @@ from django.db.models import Func
 from django.utils import timezone
 from django.utils.encoding import force_text
 
-from sentry import buffer, eventtypes, eventstream, tsdb
+from sentry import buffer, eventtypes, eventstream, options, tsdb
 from sentry.constants import (
     DEFAULT_STORE_NORMALIZER_ARGS,
     LOG_LEVELS,
@@ -462,6 +462,27 @@ class EventManager(object):
         return trim(message.strip(), settings.SENTRY_MAX_MESSAGE_LENGTH)
 
     def save(self, project_id, raw=False, assume_normalized=False):
+        """
+        We re-insert events with duplicate IDs into Snuba, which is responsible
+        for deduplicating events. Since deduplication in Snuba is on the primary
+        key (based on event ID, project ID and day), events with same IDs are only
+        deduplicated if their timestamps fall on the same day. The latest event
+        always wins and overwrites the value of events received earlier in that day.
+
+        Since we increment counters and frequencies here before events get inserted
+        to eventstream these numbers may be larger than the total number of
+        events if we receive duplicate event IDs that fall on the same day
+        (that do not hit cache first).
+
+        Note that if "store.check-duplicates" is set to True, we instead check for
+        the event in Postgres first and do not proceed to update counters or
+        insert the event into the eventstream if the event ID is found in the
+        database. This option is intended to be temporary, whilst we move towards
+        the previously described last write wins approach.
+        """
+
+        CHECK_DUPLICATES = options.get("store.check-duplicates")
+
         # Normalize if needed
         if not self._normalized:
             if not assume_normalized:
@@ -475,30 +496,31 @@ class EventManager(object):
             id=project.organization_id
         )
 
-        # Check to make sure we're not about to do a bunch of work that's
-        # already been done if we've processed an event with this ID. (This
-        # isn't a perfect solution -- this doesn't handle ``EventMapping`` and
-        # there's a race condition between here and when the event is actually
-        # saved, but it's an improvement. See GH-7677.)
-        try:
-            event = Event.objects.get(project_id=project.id, event_id=data["event_id"])
-        except Event.DoesNotExist:
-            pass
-        else:
-            # Make sure we cache on the project before returning
-            event._project_cache = project
-            logger.info(
-                "duplicate.found",
-                exc_info=True,
-                extra={
-                    "event_uuid": data["event_id"],
-                    "project_id": project.id,
-                    "group_id": event.group_id,
-                    "platform": data.get("platform"),
-                    "model": Event.__name__,
-                },
-            )
-            return event
+        if CHECK_DUPLICATES:
+            # Check to make sure we're not about to do a bunch of work that's
+            # already been done if we've processed an event with this ID. (This
+            # isn't a perfect solution -- this doesn't handle ``EventMapping`` and
+            # there's a race condition between here and when the event is actually
+            # saved, but it's an improvement. See GH-7677.)
+            try:
+                event = Event.objects.get(project_id=project.id, event_id=data["event_id"])
+            except Event.DoesNotExist:
+                pass
+            else:
+                # Make sure we cache on the project before returning
+                event._project_cache = project
+                logger.info(
+                    "duplicate.found",
+                    exc_info=True,
+                    extra={
+                        "event_uuid": data["event_id"],
+                        "project_id": project.id,
+                        "group_id": event.group_id,
+                        "platform": data.get("platform"),
+                        "model": Event.__name__,
+                    },
+                )
+                return event
 
         # Pull out the culprit
         culprit = self.get_culprit()
@@ -757,7 +779,9 @@ class EventManager(object):
                     "model": Event.__name__,
                 },
             )
-            return event
+
+            if CHECK_DUPLICATES:
+                return event
 
         if event_user:
             counters = [
diff --git a/src/sentry/web/frontend/debug/mail.py b/src/sentry/web/frontend/debug/mail.py
index 0ae010f234..9d86d3465b 100644
--- a/src/sentry/web/frontend/debug/mail.py
+++ b/src/sentry/web/frontend/debug/mail.py
@@ -13,6 +13,7 @@ from django.template.defaultfilters import slugify
 from django.utils import timezone
 from django.utils.safestring import mark_safe
 from django.views.generic import View
+from django.db import IntegrityError
 from loremipsum import Generator
 from random import Random
 
@@ -243,7 +244,10 @@ def alert(request):
     event = event_manager.save(project.id)
     # Prevent Percy screenshot from constantly changing
     event.datetime = datetime(2017, 9, 6, 0, 0)
-    event.save()
+    try:
+        event.save()
+    except IntegrityError:
+        pass
     event_type = event_manager.get_event_type()
 
     group.message = event_manager.get_search_message()
