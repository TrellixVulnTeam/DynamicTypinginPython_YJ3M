commit 4c241f82cc641fec19364bf64407dc227f72c947
Author: Leander Rodrigues <leandergrodrigues@gmail.com>
Date:   Tue Feb 18 16:47:38 2020 -0800

    ref(async-csv): Formalize export file deletion (#17108)
    
    Ensure clean file deletion on ExportedData model

diff --git a/migrations_lockfile.txt b/migrations_lockfile.txt
index a14992d8a1..bb917ec86e 100644
--- a/migrations_lockfile.txt
+++ b/migrations_lockfile.txt
@@ -10,7 +10,7 @@ auth: 0008_alter_user_username_max_length
 contenttypes: 0002_remove_content_type_name
 jira_ac: 0001_initial
 nodestore: 0001_initial
-sentry: 0043_auto_20200218_1903
+sentry: 0044_auto_20200219_0018
 sessions: 0001_initial
 sites: 0002_alter_domain_unique
 social_auth: 0001_initial
diff --git a/src/sentry/migrations/0044_auto_20200219_0018.py b/src/sentry/migrations/0044_auto_20200219_0018.py
new file mode 100644
index 0000000000..1dd72c2237
--- /dev/null
+++ b/src/sentry/migrations/0044_auto_20200219_0018.py
@@ -0,0 +1,37 @@
+# -*- coding: utf-8 -*-
+# Generated by Django 1.11.27 on 2020-02-19 00:18
+from __future__ import unicode_literals
+
+from django.db import migrations, models
+
+
+class Migration(migrations.Migration):
+    # This flag is used to mark that a migration shouldn't be automatically run in
+    # production. We set this to True for operations that we think are risky and want
+    # someone from ops to run manually and monitor.
+    # General advice is that if in doubt, mark your migration as `is_dangerous`.
+    # Some things you should always mark as dangerous:
+    # - Large data migrations. Typically we want these to be run manually by ops so that
+    #   they can be monitored. Since data migrations will now hold a transaction open
+    #   this is even more important.
+    # - Adding columns to highly active tables, even ones that are NULL.
+    is_dangerous = False
+
+    # This flag is used to decide whether to run this migration in a transaction or not.
+    # By default we prefer to run in a transaction, but for migrations where you want
+    # to `CREATE INDEX CONCURRENTLY` this needs to be set to False. Typically you'll
+    # want to create an index concurrently when adding one to an existing table.
+    atomic = True
+
+
+    dependencies = [
+        ('sentry', '0043_auto_20200218_1903'),
+    ]
+
+    operations = [
+        migrations.AlterField(
+            model_name='exporteddata',
+            name='date_expired',
+            field=models.DateTimeField(db_index=True, null=True),
+        ),
+    ]
diff --git a/src/sentry/models/exporteddata.py b/src/sentry/models/exporteddata.py
index 029c5d1d26..264acb49eb 100644
--- a/src/sentry/models/exporteddata.py
+++ b/src/sentry/models/exporteddata.py
@@ -39,7 +39,7 @@ class ExportedData(Model):
     file = FlexibleForeignKey("sentry.File", null=True, db_constraint=False)
     date_added = models.DateTimeField(default=timezone.now)
     date_finished = models.DateTimeField(null=True)
-    date_expired = models.DateTimeField(null=True)
+    date_expired = models.DateTimeField(null=True, db_index=True)
     query_type = BoundedPositiveIntegerField(choices=ExportQueryType.as_choices())
     query_info = JSONField()
 
@@ -52,9 +52,18 @@ class ExportedData(Model):
         else:
             return ExportStatus.Valid
 
-    def complete_upload(self, file, expiration=DEFAULT_EXPIRATION):
+    def delete_file(self):
+        if self.file:
+            self.file.delete()
+
+    def delete(self, *args, **kwargs):
+        self.delete_file()
+        super(ExportedData, self).delete(*args, **kwargs)
+
+    def finalize_upload(self, file, expiration=DEFAULT_EXPIRATION):
+        self.delete_file()
         current_time = timezone.now()
-        expire_time = current_time + DEFAULT_EXPIRATION
+        expire_time = current_time + expiration
         self.update(file=file, date_finished=current_time, date_expired=expire_time)
         # TODO(Leander): Implement email notification
 
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index 2c2ceb0f10..6934ca4049 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -226,6 +226,17 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
 
             queryset.delete()
 
+    if not silent:
+        click.echo("Removing expired files associated with ExportedData")
+
+    if is_filtered(models.ExportedData):
+        if not silent:
+            click.echo(">> Skipping ExportedData files")
+    else:
+        queryset = models.ExportedData.objects.filter(date_expired__lt=(timezone.now()))
+        for item in queryset:
+            item.file.delete()
+
     project_id = None
     if project:
         click.echo("Bulk NodeStore deletion not available for project selection", err=True)
diff --git a/src/sentry/tasks/data_export.py b/src/sentry/tasks/data_export.py
index 04f6d21622..847d8bb5d2 100644
--- a/src/sentry/tasks/data_export.py
+++ b/src/sentry/tasks/data_export.py
@@ -31,7 +31,7 @@ def assemble_download(data_export):
         with transaction.atomic():
             file = File.objects.create(name=file_name, type="export.csv")
             file.putfile(tf)
-            data_export.complete_upload(file=file)
+            data_export.finalize_upload(file=file)
 
 
 def process_discover_v2(data_export, file):
diff --git a/tests/sentry/models/test_exporteddata.py b/tests/sentry/models/test_exporteddata.py
new file mode 100644
index 0000000000..cfef3a0fe4
--- /dev/null
+++ b/tests/sentry/models/test_exporteddata.py
@@ -0,0 +1,67 @@
+from __future__ import absolute_import
+
+import tempfile
+from datetime import timedelta
+
+from sentry.models import ExportedData, File
+from sentry.testutils import TestCase
+
+
+class DeleteExportedDataTest(TestCase):
+    def setUp(self):
+        super(DeleteExportedDataTest, self).setUp()
+        self.user = self.create_user()
+        self.organization = self.create_organization()
+        self.data_export = ExportedData.objects.create(
+            user=self.user, organization=self.organization, query_type=2, query_info={"env": "test"}
+        )
+        self.file1 = File.objects.create(
+            name="tempfile-data-export", type="export.csv", headers={"Content-Type": "text/csv"}
+        )
+        self.file2 = File.objects.create(
+            name="tempfile-data-export", type="export.csv", headers={"Content-Type": "text/csv"}
+        )
+
+    def test_delete_file(self):
+        # Empty call should have no effect
+        assert self.data_export.file is None
+        self.data_export.delete_file()
+        assert self.data_export.file is None
+        # Real call should delete the file
+        assert File.objects.filter(id=self.file1.id).exists()
+        self.data_export.update(file=self.file1)
+        assert isinstance(self.data_export.file, File)
+        self.data_export.delete_file()
+        assert not File.objects.filter(id=self.file1.id).exists()
+
+    def test_delete(self):
+        self.data_export.finalize_upload(file=self.file1)
+        assert ExportedData.objects.filter(id=self.data_export.id).exists()
+        assert File.objects.filter(id=self.file1.id).exists()
+        self.data_export.delete()
+        assert not ExportedData.objects.filter(id=self.data_export.id).exists()
+        assert not File.objects.filter(id=self.file1.id).exists()
+
+    def test_finalize_upload(self):
+        TEST_STRING = "A bunch of test data..."
+        DEFAULT_EXPIRATION = timedelta(weeks=4)  # Matches src/sentry/models/exporteddata.py
+        # With default expiration
+        with tempfile.TemporaryFile() as tf:
+            tf.write(TEST_STRING)
+            tf.seek(0)
+            self.file1.putfile(tf)
+        self.data_export.finalize_upload(file=self.file1)
+        assert self.data_export.file.getfile().read() == TEST_STRING
+        assert self.data_export.date_finished is not None
+        assert self.data_export.date_expired is not None
+        assert self.data_export.date_expired == self.data_export.date_finished + DEFAULT_EXPIRATION
+        # With custom expiration
+        with tempfile.TemporaryFile() as tf:
+            tf.write(TEST_STRING + TEST_STRING)
+            tf.seek(0)
+            self.file2.putfile(tf)
+        self.data_export.finalize_upload(file=self.file2, expiration=timedelta(weeks=2))
+        assert self.data_export.file.getfile().read() == TEST_STRING + TEST_STRING
+        # Ensure the first file is deleted
+        assert not File.objects.filter(id=self.file1.id).exists()
+        assert self.data_export.date_expired == self.data_export.date_finished + timedelta(weeks=2)
