commit d13a5bae62724558196da1fe6e40d01b13985681
Author: Mark Story <mark@sentry.io>
Date:   Tue Oct 8 09:59:44 2019 -0400

    feat(discover2) Add dataset selection based on query (#14857)
    
    Add mappings for the transactions columns and add dataset detection.
    This inference will enable us to start querying the transactions dataset
    when it is required. This is a prerequisite to enabling duration
    aggregates. I've  removed the workaround for aggregated fields not
    supporting list arguments as they do now.
    
    These changes add `dataset_query` which is a dataset aware wrapper
    for snuba. Using this new query method allows order, group and condition
    transformations without doing column aliasing. This new function is
    primarily intended to be used by endpoints with fixed column sets.
    
    The search parser resolves aliases automatically which is nice, but can
    result in conditions from one table being sent to the other. When
    `skip_conditions` is True, we need to check that all the columns in the
    conditions reference real columns. If they don't we cast them into
    a tags expression. The `type` condition is special as it needs to be
    entirely dropped when using the transactions dataset.
    
    The environment field exists on the events dataset. Use the promoted
    column instead of digging into tags.key for environment. This could help
    these queries be a bit faster too.
    
    We had a few redundant tests between the snuba integration suite and the
    postgres suite. The removed test cases were 'postgres' tests that were
    not resetting snuba state correctly. I've merged the 'postgres' tests
    methods into the snuba cases.
    
    Another very notable change here is that we're using finish_ts instead
    of start_ts as it is a better cousin for an event's timestamp.
    
    Refs SEN-1043

diff --git a/src/sentry/api/bases/organization_events.py b/src/sentry/api/bases/organization_events.py
index 5a1b93ffc2..ccf816eaa4 100644
--- a/src/sentry/api/bases/organization_events.py
+++ b/src/sentry/api/bases/organization_events.py
@@ -12,6 +12,7 @@ from sentry.api.event_search import (
     get_reference_event_conditions,
 )
 from sentry.models.project import Project
+from sentry.utils import snuba
 
 
 class Direction(object):
@@ -101,6 +102,14 @@ class OrganizationEventsEndpointBase(OrganizationEndpoint):
             raise OrganizationEventsError(
                 "Boolean search operator OR and AND not allowed in this search."
             )
+
+        # 'legacy' endpoints cannot access transactions dataset.
+        # as they often have assumptions about which columns are returned.
+        dataset = snuba.detect_dataset(snuba_args, aliased_conditions=True)
+        if dataset != "events":
+            raise OrganizationEventsError(
+                "Invalid query. You cannot reference non-events data in this endpoint."
+            )
         return snuba_args
 
     def next_event_id(self, snuba_args, event):
@@ -162,15 +171,17 @@ class OrganizationEventsEndpointBase(OrganizationEndpoint):
         conditions = snuba_args["conditions"][:]
         conditions.extend(time_condition)
 
-        result = eventstore.get_events(
+        result = snuba.dataset_query(
+            selected_columns=["event_id"],
             start=snuba_args.get("start", None),
             end=snuba_args.get("end", None),
             conditions=conditions,
+            dataset=snuba.detect_dataset(snuba_args, aliased_conditions=True),
             filter_keys=snuba_args["filter_keys"],
             orderby=orderby,
             limit=1,
         )
-        if not result:
+        if not result or "data" not in result or len(result["data"]) == 0:
             return None
 
-        return result[0].event_id
+        return result["data"][0]["event_id"]
diff --git a/src/sentry/api/endpoints/organization_events_distribution.py b/src/sentry/api/endpoints/organization_events_distribution.py
index 6e8d84cd8b..44c39a8b23 100644
--- a/src/sentry/api/endpoints/organization_events_distribution.py
+++ b/src/sentry/api/endpoints/organization_events_distribution.py
@@ -4,7 +4,7 @@ import six
 
 from rest_framework.response import Response
 from sentry.api.bases import OrganizationEventsEndpointBase, OrganizationEventsError, NoProjects
-from sentry.utils.snuba import get_snuba_column_name, raw_query
+from sentry.utils.snuba import transform_aliases_and_query
 from sentry import features, tagstore
 from sentry.tagstore.base import TOP_VALUES_DEFAULT_LIMIT
 
@@ -36,10 +36,11 @@ class OrganizationEventsDistributionEndpoint(OrganizationEventsEndpointBase):
             colname = "project_id"
             conditions = snuba_args["conditions"]
         else:
-            colname = get_snuba_column_name(key)
+            colname = key
             conditions = snuba_args["conditions"] + [[colname, "IS NOT NULL", None]]
 
-        top_values = raw_query(
+        top_values = transform_aliases_and_query(
+            skip_conditions=True,
             start=snuba_args["start"],
             end=snuba_args["end"],
             conditions=conditions,
diff --git a/src/sentry/api/endpoints/organization_events_meta.py b/src/sentry/api/endpoints/organization_events_meta.py
index 36dd52f8a1..3eae70a25a 100644
--- a/src/sentry/api/endpoints/organization_events_meta.py
+++ b/src/sentry/api/endpoints/organization_events_meta.py
@@ -3,7 +3,7 @@ from __future__ import absolute_import
 from rest_framework.response import Response
 
 from sentry.api.bases import OrganizationEventsEndpointBase, OrganizationEventsError, NoProjects
-from sentry.utils.snuba import raw_query
+from sentry.utils import snuba
 
 
 class OrganizationEventsMetaEndpoint(OrganizationEventsEndpointBase):
@@ -16,7 +16,8 @@ class OrganizationEventsMetaEndpoint(OrganizationEventsEndpointBase):
         except NoProjects:
             return Response({"count": 0})
 
-        data = raw_query(
+        data = snuba.transform_aliases_and_query(
+            skip_conditions=True,
             aggregations=[["count()", "", "count"]],
             referrer="api.organization-event-meta",
             **snuba_args
diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index 21a4d074cc..319f98112a 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -21,7 +21,7 @@ from sentry.search.utils import (
     InvalidQuery,
 )
 from sentry.utils.dates import to_timestamp
-from sentry.utils.snuba import SENTRY_SNUBA_MAP, get_snuba_column_name
+from sentry.utils.snuba import DATASETS, get_snuba_column_name
 
 WILDCARD_CHARS = re.compile(r"[\*]")
 
@@ -138,20 +138,21 @@ spaces               = ~r"\ *"
 )
 
 
-# add valid snuba `raw_query` args
-SEARCH_MAP = dict(
-    {
-        "start": "start",
-        "end": "end",
-        "project_id": "project_id",
-        "first_seen": "first_seen",
-        "last_seen": "last_seen",
-        "times_seen": "times_seen",
-        # TODO(mark) figure out how to safelist aggregate functions/field aliases
-        # so they can be used in conditions
-    },
-    **SENTRY_SNUBA_MAP
-)
+# Create the known set of fields from the issue properties
+# and the transactions and events dataset mapping definitions.
+SEARCH_MAP = {
+    "start": "start",
+    "end": "end",
+    "project_id": "project_id",
+    "first_seen": "first_seen",
+    "last_seen": "last_seen",
+    "times_seen": "times_seen",
+    # TODO(mark) figure out how to safelist aggregate functions/field aliases
+    # so they can be used in conditions
+}
+SEARCH_MAP.update(**DATASETS["transactions"])
+SEARCH_MAP.update(**DATASETS["events"])
+
 no_conversion = set(["project_id", "start", "end"])
 
 PROJECT_KEY = "project.name"
@@ -226,11 +227,23 @@ class SearchVisitor(NodeVisitor):
             "stack.in_app",
             "stack.lineno",
             "stack.stack_level",
+            "transaction.duration",
             # TODO(mark) figure out how to safelist aggregate functions/field aliases
             # so they can be used in conditions
         ]
     )
-    date_keys = set(["start", "end", "first_seen", "last_seen", "time", "timestamp"])
+    date_keys = set(
+        [
+            "start",
+            "end",
+            "first_seen",
+            "last_seen",
+            "time",
+            "timestamp",
+            "transaction.start_time",
+            "transaction.end_time",
+        ]
+    )
 
     unwrapped_exceptions = (InvalidSearchQuery,)
 
diff --git a/src/sentry/api/urls.py b/src/sentry/api/urls.py
index e5896a504b..afaafb6812 100644
--- a/src/sentry/api/urls.py
+++ b/src/sentry/api/urls.py
@@ -735,7 +735,7 @@ urlpatterns = patterns(
                     name="sentry-api-0-organization-eventsv2",
                 ),
                 url(
-                    r"^(?P<organization_slug>[^\/]+)/events/(?P<project_slug>[^\/]+):(?P<event_id>(?:\d+|[A-Fa-f0-9]{32}))/$",
+                    r"^(?P<organization_slug>[^\/]+)/events/(?P<project_slug>[^\/]+):(?P<event_id>(?:\d+|[A-Fa-f0-9-]{32,36}))/$",
                     OrganizationEventDetailsEndpoint.as_view(),
                     name="sentry-api-0-organization-event-details",
                 ),
diff --git a/src/sentry/eventstore/snuba/backend.py b/src/sentry/eventstore/snuba/backend.py
index 6a86e059c9..0ec97af9ab 100644
--- a/src/sentry/eventstore/snuba/backend.py
+++ b/src/sentry/eventstore/snuba/backend.py
@@ -136,11 +136,11 @@ class SnubaEventStorage(EventStorage):
         return [col.value for col in columns]
 
     def __get_next_or_prev_event_id(self, **kwargs):
-
-        result = snuba.raw_query(
+        result = snuba.dataset_query(
             selected_columns=["event_id", "project_id"],
             limit=1,
             referrer="eventstore.get_next_or_prev_event_id",
+            dataset=snuba.detect_dataset(kwargs, aliased_conditions=True),
             **kwargs
         )
 
diff --git a/src/sentry/static/sentry/app/views/eventsV2/eventDetails.tsx b/src/sentry/static/sentry/app/views/eventsV2/eventDetails.tsx
index 09e4e9a5a0..28083b983a 100644
--- a/src/sentry/static/sentry/app/views/eventsV2/eventDetails.tsx
+++ b/src/sentry/static/sentry/app/views/eventsV2/eventDetails.tsx
@@ -26,7 +26,7 @@ const slugValidator = function(
   const value = props[propName];
   // Accept slugs that look like:
   // * project-slug:deadbeef
-  if (value && typeof value === 'string' && !/^(?:[^:]+):(?:[a-f0-9]+)$/.test(value)) {
+  if (value && typeof value === 'string' && !/^(?:[^:]+):(?:[a-f0-9-]+)$/.test(value)) {
     return new Error(`Invalid value for ${propName} provided to ${componentName}.`);
   }
   return null;
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 01f9c1145f..8ce58b6650 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -11,6 +11,7 @@ import re
 import six
 import time
 import urllib3
+import uuid
 
 from concurrent.futures import ThreadPoolExecutor
 from django.conf import settings
@@ -129,6 +130,7 @@ TRANSACTIONS_SENTRY_SNUBA_MAP = {
     "trace_id": "trace_id",
     "span_id": "span_id",
     "title": "transaction_name",
+    "message": "transaction_name",
     "transaction": "transaction_name",
     "transaction.name": "transaction_name",
     "transaction.op": "transaction_op",
@@ -139,7 +141,7 @@ TRANSACTIONS_SENTRY_SNUBA_MAP = {
     # Time related properties
     "transaction.duration": "duration",
     "transaction.start_time": "start_ts",
-    "transaction.end_time": "end_ts",
+    "transaction.end_time": "finish_ts",
     # User
     "user": "user",
     "user.id": "user_id",
@@ -155,12 +157,18 @@ TRANSACTIONS_SENTRY_SNUBA_MAP = {
     "contexts.value": "contexts.value",
     # Shim to make queries that can act on
     # events or transactions work more smoothly.
-    "timestamp": "start_ts",
-    "time": "bucketed_start",
+    "timestamp": "finish_ts",
+    "time": "bucketed_end",
 }
 
 DATASETS = {EVENTS: SENTRY_SNUBA_MAP, TRANSACTIONS: TRANSACTIONS_SENTRY_SNUBA_MAP}
 
+# Store the internal field names to save work later on.
+DATASET_FIELDS = {
+    EVENTS: SENTRY_SNUBA_MAP.values(),
+    TRANSACTIONS: TRANSACTIONS_SENTRY_SNUBA_MAP.values(),
+}
+
 
 class SnubaError(Exception):
     pass
@@ -325,7 +333,7 @@ def zerofill(data, start, end, rollup, orderby):
     return rv
 
 
-def get_snuba_column_name(name):
+def get_snuba_column_name(name, dataset="events"):
     """
     Get corresponding Snuba column name from Sentry snuba map, if not found
     the column is assumed to be a tag. If name is falsy or name is a quoted literal
@@ -339,28 +347,46 @@ def get_snuba_column_name(name):
     if not name or QUOTED_LITERAL_RE.match(name):
         return name
 
-    return SENTRY_SNUBA_MAP.get(name, u"tags[{}]".format(name))
+    return DATASETS[dataset].get(name, u"tags[{}]".format(name))
 
 
-def detect_dataset(query_args):
+def detect_dataset(query_args, aliased_conditions=False):
     """
     Determine the dataset to use based on the conditions, selected_columns,
     groupby clauses.
 
     This function operates on the end user field aliases and not the internal column
     names that have been converted using the field mappings.
+
+    The aliased_conditions parameter switches column detection between
+    the public aliases and the internal names. When query conditions
+    have been pre-parsed by api.event_search set aliased_conditions=True
+    as we need to look for internal names.
     """
     if query_args.get("dataset", None):
         return query_args["dataset"]
 
     dataset = EVENTS
     transaction_fields = set(DATASETS[TRANSACTIONS].keys()) - set(DATASETS[EVENTS].keys())
+    condition_fieldset = transaction_fields
+
+    if aliased_conditions:
+        # Release and user are also excluded as they are present on both
+        # datasets and don't trigger usage of transactions.
+        condition_fieldset = (
+            set(DATASET_FIELDS[TRANSACTIONS])
+            - set(DATASET_FIELDS[EVENTS])
+            - set(["release", "user"])
+        )
+
     for condition in query_args.get("conditions") or []:
-        if isinstance(condition[0], six.string_types) and condition[0] in transaction_fields:
-            return TRANSACTIONS
-        if condition == ["event.type", "=", "transaction"]:
+        if isinstance(condition[0], six.string_types) and condition[0] in condition_fieldset:
             return TRANSACTIONS
-        if condition == ["type", "=", "transaction"]:
+        if condition == ["event.type", "=", "transaction"] or condition == [
+            "type",
+            "=",
+            "transaction",
+        ]:
             return TRANSACTIONS
 
     for field in query_args.get("selected_columns") or []:
@@ -421,7 +447,7 @@ def get_function_index(column_expr, depth=0):
         return None
 
 
-def parse_columns_in_functions(col, context=None, index=None):
+def parse_columns_in_functions(col, context=None, index=None, dataset="events"):
     """
     Checks expressions for arguments that should be considered a column while
     ignoring strings that represent clickhouse function names
@@ -444,23 +470,23 @@ def parse_columns_in_functions(col, context=None, index=None):
         if function_name_index > 0:
             for i in six.moves.xrange(0, function_name_index):
                 if context is not None:
-                    context[i] = get_snuba_column_name(col[i])
+                    context[i] = get_snuba_column_name(col[i], dataset)
 
         args = col[function_name_index + 1]
 
         # check for nested functions in args
         if get_function_index(args):
             # look for columns
-            return parse_columns_in_functions(args, args)
+            return parse_columns_in_functions(args, args, dataset=dataset)
 
         # check each argument for column names
         else:
             for (i, arg) in enumerate(args):
-                parse_columns_in_functions(arg, args, i)
+                parse_columns_in_functions(arg, args, i, dataset=dataset)
     else:
         # probably a column name
         if context is not None and index is not None:
-            context[index] = get_snuba_column_name(col)
+            context[index] = get_snuba_column_name(col, dataset)
 
 
 def get_arrayjoin(column):
@@ -469,7 +495,7 @@ def get_arrayjoin(column):
         return match.groups()[0]
 
 
-def valid_orderby(orderby, custom_fields=None):
+def valid_orderby(orderby, custom_fields=None, dataset="events"):
     """
     Check if a field can be used in sorting. We don't allow
     sorting on fields that would be aliased as tag[foo] because those
@@ -478,9 +504,10 @@ def valid_orderby(orderby, custom_fields=None):
     if custom_fields is None:
         custom_fields = []
     fields = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+    mapping = DATASETS[dataset]
     for field in fields:
         field = field.lstrip("-")
-        if field not in SENTRY_SNUBA_MAP and field not in custom_fields:
+        if field not in mapping and field not in custom_fields:
             return False
     return True
 
@@ -507,6 +534,7 @@ def transform_aliases_and_query(skip_conditions=False, **kwargs):
     rollup = kwargs.get("rollup")
     orderby = kwargs.get("orderby")
     having = kwargs.get("having", [])
+    dataset = detect_dataset(kwargs, aliased_conditions=skip_conditions)
 
     if selected_columns:
         for (idx, col) in enumerate(selected_columns):
@@ -518,14 +546,14 @@ def transform_aliases_and_query(skip_conditions=False, **kwargs):
                 translated_columns[col[2]] = col[2]
                 derived_columns.add(col[2])
             else:
-                name = get_snuba_column_name(col)
+                name = get_snuba_column_name(col, dataset)
                 selected_columns[idx] = name
                 translated_columns[name] = col
 
     if groupby:
         for (idx, col) in enumerate(groupby):
             if col not in derived_columns:
-                name = get_snuba_column_name(col)
+                name = get_snuba_column_name(col, dataset)
             else:
                 name = col
 
@@ -535,13 +563,13 @@ def transform_aliases_and_query(skip_conditions=False, **kwargs):
     for aggregation in aggregations or []:
         derived_columns.add(aggregation[2])
         if isinstance(aggregation[1], six.string_types):
-            aggregation[1] = get_snuba_column_name(aggregation[1])
+            aggregation[1] = get_snuba_column_name(aggregation[1], dataset)
         elif isinstance(aggregation[1], (set, tuple, list)):
-            aggregation[1] = [get_snuba_column_name(col) for col in aggregation[1]]
+            aggregation[1] = [get_snuba_column_name(col, dataset) for col in aggregation[1]]
 
     if not skip_conditions:
-        for (col, _value) in six.iteritems(filter_keys):
-            name = get_snuba_column_name(col)
+        for col in filter_keys.keys():
+            name = get_snuba_column_name(col, dataset)
             filter_keys[name] = filter_keys.pop(col)
 
     def handle_condition(cond):
@@ -550,29 +578,28 @@ def transform_aliases_and_query(skip_conditions=False, **kwargs):
                 cond[0] = handle_condition(cond[0])
             elif len(cond) == 3:
                 # map column name
-                cond[0] = get_snuba_column_name(cond[0])
+                cond[0] = get_snuba_column_name(cond[0], dataset)
             elif len(cond) == 2 and cond[0] == "has":
                 # first function argument is the column if function is "has"
-                cond[1][0] = get_snuba_column_name(cond[1][0])
+                cond[1][0] = get_snuba_column_name(cond[1][0], dataset)
         return cond
 
     if conditions:
-        kwargs["conditions"] = []
+        aliased_conditions = []
         for condition in conditions:
             field = condition[0]
             if not isinstance(field, (list, tuple)) and field in derived_columns:
                 having.append(condition)
             elif skip_conditions:
-                kwargs["conditions"].append(condition)
+                aliased_conditions.append(condition)
             else:
-                kwargs["conditions"].append(handle_condition(condition))
+                aliased_conditions.append(handle_condition(condition))
+        kwargs["conditions"] = aliased_conditions
 
     if having:
         kwargs["having"] = having
 
     if orderby:
-        if orderby is None:
-            orderby = []
         orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
         translated_orderby = []
 
@@ -581,15 +608,16 @@ def transform_aliases_and_query(skip_conditions=False, **kwargs):
             translated_orderby.append(
                 u"{}{}".format(
                     "-" if field_with_order.startswith("-") else "",
-                    field if field in derived_columns else get_snuba_column_name(field),
+                    field if field in derived_columns else get_snuba_column_name(field, dataset),
                 )
             )
 
         kwargs["orderby"] = translated_orderby
 
     kwargs["arrayjoin"] = arrayjoin_map.get(arrayjoin, arrayjoin)
+    kwargs["dataset"] = dataset
 
-    result = raw_query(**kwargs)
+    result = dataset_query(**kwargs)
 
     # Translate back columns that were converted to snuba format
     for col in result["meta"]:
@@ -903,6 +931,137 @@ def nest_groups(data, groups, aggregate_cols):
         )
 
 
+def constrain_column_to_dataset(col, dataset, value=None):
+    """
+    Ensure conditions only reference valid columns on the provided
+    dataset. Return none for conditions to be removed, and convert
+    unknown columns into tags expressions.
+    """
+    if col.startswith("tags["):
+        return col
+    # Special case for the type condition as we only want
+    # to drop it when we are querying transactions.
+    if dataset == TRANSACTIONS and col == "type" and value == "transaction":
+        return None
+    if not col or QUOTED_LITERAL_RE.match(col):
+        return col
+    if col in DATASETS[dataset]:
+        return DATASETS[dataset][col]
+    if col in DATASET_FIELDS[dataset]:
+        return col
+    return u"tags[{}]".format(col)
+
+
+def constrain_condition_to_dataset(cond, dataset):
+    """
+    When conditions have been parsed by the api.event_search module
+    we can end up with conditions that are not valid on the current dataset
+    due to how ap.event_search checks for valid field names without
+    being aware of the dataset.
+
+    We have the dataset context here, so we need to re-scope conditions to the
+    current dataset.
+    """
+    if isinstance(cond, (list, tuple)) and len(cond):
+        if isinstance(cond[0], (list, tuple)):
+            # Nested condition or function expressions
+            cond = [constrain_condition_to_dataset(c, dataset) for c in cond]
+        elif len(cond) == 3:
+            # map column name
+            name = constrain_column_to_dataset(cond[0], dataset, cond[2])
+            if name is None:
+                return None
+            cond[0] = name
+            # Reformat 32 byte uuids to 36 byte variants.
+            # The transactions dataset requires properly formatted uuid values.
+            # But the rest of sentry isn't aware of that requirement.
+            if dataset == TRANSACTIONS and name == "event_id" and len(cond[2]) == 32:
+                cond[2] = six.text_type(uuid.UUID(cond[2]))
+        elif len(cond) == 2 and cond[0] == "has":
+            # first function argument is the column if function is "has"
+            cond[1][0] = constrain_column_to_dataset(cond[1][0], dataset)
+        elif len(cond) == 2 and SAFE_FUNCTION_RE.match(cond[0]):
+            # Function call with column name arguments.
+            if isinstance(cond[1], list):
+                cond[1] = [constrain_column_to_dataset(item, dataset) for item in cond[1]]
+    return cond
+
+
+def dataset_query(
+    start=None,
+    end=None,
+    groupby=None,
+    conditions=None,
+    filter_keys=None,
+    aggregations=None,
+    selected_columns=None,
+    arrayjoin=None,
+    having=None,
+    dataset=None,
+    orderby=None,
+    **kwargs
+):
+    """
+    Wrapper around raw_query that selects the dataset based on the
+    selected_columns, conditions and groupby parameters.
+    Useful for taking arbitrary end user queries and searching
+    either error or transaction events.
+
+    This function will also re-alias columns to match the selected dataset
+    """
+    if dataset is None:
+        dataset = detect_dataset(
+            dict(
+                dataset=dataset,
+                aggregations=aggregations,
+                conditions=conditions,
+                selected_columns=selected_columns,
+                groupby=groupby,
+            )
+        )
+
+    derived_columns = []
+    if selected_columns:
+        for (i, col) in enumerate(selected_columns):
+            if isinstance(col, list):
+                derived_columns.append(col[2])
+            else:
+                selected_columns[i] = constrain_column_to_dataset(col, dataset)
+        selected_columns = list(filter(None, selected_columns))
+
+    if aggregations:
+        for aggregation in aggregations:
+            derived_columns.append(aggregation[2])
+
+    if conditions:
+        for (i, condition) in enumerate(conditions):
+            replacement = constrain_condition_to_dataset(condition, dataset)
+            conditions[i] = replacement
+        conditions = list(filter(None, conditions))
+
+    if orderby:
+        for (i, order) in enumerate(orderby):
+            order_field = order.lstrip("-")
+            if order_field not in derived_columns:
+                order_field = constrain_column_to_dataset(order_field, dataset)
+            orderby[i] = u"{}{}".format("-" if order.startswith("-") else "", order_field)
+
+    return raw_query(
+        start=start,
+        end=end,
+        groupby=groupby,
+        conditions=conditions,
+        aggregations=aggregations,
+        selected_columns=selected_columns,
+        filter_keys=filter_keys,
+        arrayjoin=arrayjoin,
+        having=having,
+        dataset=dataset,
+        orderby=orderby,
+        **kwargs
+    )
+
+
 JSON_TYPE_MAP = {
     "UInt8": "boolean",
     "UInt16": "integer",
@@ -972,6 +1131,7 @@ def get_snuba_translators(filter_keys, is_grouprelease=False):
     map_columns = {
         "environment": (Environment, "name", lambda name: None if name == "" else name),
         "tags[sentry:release]": (Release, "version", identity),
+        "release": (Release, "version", identity),
     }
 
     for col, (model, field, fmt) in six.iteritems(map_columns):
@@ -979,7 +1139,7 @@ def get_snuba_translators(filter_keys, is_grouprelease=False):
         ids = filter_keys.get(col)
         if not ids:
             continue
-        if is_grouprelease and col == "tags[sentry:release]":
+        if is_grouprelease and col in ("release", "tags[sentry:release]"):
             # GroupRelease -> Release translation is a special case because the
             # translation relies on both the Group and Release value in the result row.
             #
@@ -1038,6 +1198,15 @@ def get_snuba_translators(filter_keys, is_grouprelease=False):
         if "time" in row
         else row,
     )
+    # Extra reverse translator for bucketed_start column.
+    reverse = compose(
+        reverse,
+        lambda row: replace(
+            row, "bucketed_start", int(to_timestamp(parse_datetime(row["bucketed_start"])))
+        )
+        if "bucketed_start" in row
+        else row,
+    )
 
     return (forward, reverse)
 
@@ -1049,6 +1218,7 @@ def get_related_project_ids(column, ids):
     mappings = {
         "issue": (Group, "id", "project_id"),
         "tags[sentry:release]": (ReleaseProject, "release_id", "project_id"),
+        "release": (ReleaseProject, "release_id", "project_id"),
     }
     if ids:
         if column == "project_id":
diff --git a/tests/sentry/api/endpoints/test_group_events_latest.py b/tests/sentry/api/endpoints/test_group_events_latest.py
deleted file mode 100644
index 870cf91821..0000000000
--- a/tests/sentry/api/endpoints/test_group_events_latest.py
+++ /dev/null
@@ -1,35 +0,0 @@
-from __future__ import absolute_import
-
-import six
-
-from sentry.models import Group
-from sentry.testutils import APITestCase
-from sentry.testutils.helpers.datetime import iso_format, before_now
-
-
-class GroupEventsLatestTest(APITestCase):
-    def setUp(self):
-        super(GroupEventsLatestTest, self).setUp()
-        self.login_as(user=self.user)
-
-        project = self.create_project()
-        min_ago = iso_format(before_now(minutes=1))
-        two_min_ago = iso_format(before_now(minutes=2))
-
-        self.event1 = self.store_event(
-            data={"environment": "staging", "fingerprint": ["group_1"], "timestamp": two_min_ago},
-            project_id=project.id,
-        )
-
-        self.event2 = self.store_event(
-            data={"environment": "production", "fingerprint": ["group_1"], "timestamp": min_ago},
-            project_id=project.id,
-        )
-
-        self.group = Group.objects.first()
-
-    def test_simple(self):
-        url = u"/api/0/issues/{}/events/latest/".format(self.group.id)
-        response = self.client.get(url, format="json")
-        assert response.status_code == 200
-        assert response.data["eventID"] == six.text_type(self.event2.event_id)
diff --git a/tests/sentry/api/endpoints/test_group_events_oldest.py b/tests/sentry/api/endpoints/test_group_events_oldest.py
deleted file mode 100644
index e3bbc4adac..0000000000
--- a/tests/sentry/api/endpoints/test_group_events_oldest.py
+++ /dev/null
@@ -1,36 +0,0 @@
-from __future__ import absolute_import
-
-import six
-
-from sentry.models import Group
-from sentry.testutils import APITestCase
-from sentry.testutils.helpers.datetime import iso_format, before_now
-
-
-class GroupEventsOldestTest(APITestCase):
-    def setUp(self):
-        super(GroupEventsOldestTest, self).setUp()
-        self.login_as(user=self.user)
-
-        project = self.create_project()
-        min_ago = iso_format(before_now(minutes=1))
-        two_min_ago = iso_format(before_now(minutes=2))
-
-        self.event1 = self.store_event(
-            data={"environment": "staging", "fingerprint": ["group_1"], "timestamp": two_min_ago},
-            project_id=project.id,
-        )
-
-        self.event2 = self.store_event(
-            data={"environment": "production", "fingerprint": ["group_1"], "timestamp": min_ago},
-            project_id=project.id,
-        )
-
-        self.group = Group.objects.first()
-
-    def test_simple(self):
-        url = u"/api/0/issues/{}/events/oldest/".format(self.group.id)
-        response = self.client.get(url, format="json")
-
-        assert response.status_code == 200
-        assert response.data["id"] == six.text_type(self.event1.event_id)
diff --git a/tests/sentry/api/endpoints/test_project_event_details.py b/tests/sentry/api/endpoints/test_project_event_details.py
deleted file mode 100644
index 93d6d1ed7d..0000000000
--- a/tests/sentry/api/endpoints/test_project_event_details.py
+++ /dev/null
@@ -1,88 +0,0 @@
-from __future__ import absolute_import
-
-import six
-
-from django.core.urlresolvers import reverse
-from sentry.testutils import APITestCase
-from sentry.testutils.helpers.datetime import iso_format, before_now
-
-
-class ProjectEventDetailsTest(APITestCase):
-    def setUp(self):
-        super(ProjectEventDetailsTest, self).setUp()
-        self.login_as(user=self.user)
-        project = self.create_project()
-
-        one_min_ago = iso_format(before_now(minutes=1))
-        two_min_ago = iso_format(before_now(minutes=2))
-        three_min_ago = iso_format(before_now(minutes=3))
-
-        self.prev_event = self.store_event(
-            data={"timestamp": three_min_ago, "fingerprint": ["group-1"]}, project_id=project.id
-        )
-        self.cur_event = self.store_event(
-            data={"timestamp": two_min_ago, "fingerprint": ["group-1"]}, project_id=project.id
-        )
-        self.next_event = self.store_event(
-            data={
-                "timestamp": one_min_ago,
-                "fingerprint": ["group-1"],
-                "environment": "production",
-                "tags": {"environment": "production"},
-            },
-            project_id=project.id,
-        )
-
-    def test_simple(self):
-        url = reverse(
-            "sentry-api-0-project-event-details",
-            kwargs={
-                "event_id": self.cur_event.event_id,
-                "project_slug": self.cur_event.project.slug,
-                "organization_slug": self.cur_event.project.organization.slug,
-            },
-        )
-        response = self.client.get(url, format="json")
-        assert response.status_code == 200, response.content
-        assert response.data["id"] == six.text_type(self.cur_event.event_id)
-        assert response.data["nextEventID"] == six.text_type(self.next_event.event_id)
-        assert response.data["previousEventID"] == six.text_type(self.prev_event.event_id)
-        assert response.data["groupID"] == six.text_type(self.cur_event.group.id)
-
-        # Same event can be looked up by primary key
-        url = reverse(
-            "sentry-api-0-project-event-details",
-            kwargs={
-                "event_id": self.cur_event.event_id,
-                "project_slug": self.cur_event.project.slug,
-                "organization_slug": self.cur_event.project.organization.slug,
-            },
-        )
-        response = self.client.get(url, format="json")
-
-        assert response.status_code == 200, response.content
-        assert response.data["id"] == six.text_type(self.cur_event.event_id)
-        assert response.data["nextEventID"] == six.text_type(self.next_event.event_id)
-        assert response.data["previousEventID"] == six.text_type(self.prev_event.event_id)
-        assert response.data["groupID"] == six.text_type(self.cur_event.group.id)
-
-    def test_prev_has_no_prev(self):
-        # Test that the "previous" event does not itself have a "previousEventID"
-        # pointing back to the current event. i.e. test that there is not a redirect
-        # loop between next and previous events that occur within the same second.
-
-        url = reverse(
-            "sentry-api-0-project-event-details",
-            kwargs={
-                "event_id": self.prev_event.event_id,
-                "project_slug": self.prev_event.project.slug,
-                "organization_slug": self.prev_event.project.organization.slug,
-            },
-        )
-        response = self.client.get(url, format="json")
-
-        assert response.status_code == 200, response.content
-        assert response.data["id"] == six.text_type(self.prev_event.event_id)
-        assert response.data["previousEventID"] is None
-        assert response.data["nextEventID"] == self.cur_event.event_id
-        assert response.data["groupID"] == six.text_type(self.prev_event.group.id)
diff --git a/tests/sentry/utils/test_snuba.py b/tests/sentry/utils/test_snuba.py
index 55f7efe748..ba07c894ec 100644
--- a/tests/sentry/utils/test_snuba.py
+++ b/tests/sentry/utils/test_snuba.py
@@ -1,16 +1,19 @@
 from __future__ import absolute_import
 
 from datetime import datetime
+from mock import patch
 import pytz
 
 from sentry.models import GroupRelease, Release
-from sentry.testutils import TestCase
+from sentry.testutils import TestCase, SnubaTestCase
+from sentry.testutils.helpers.datetime import iso_format, before_now
 from sentry.utils.snuba import (
     get_snuba_translators,
     zerofill,
     get_json_type,
     get_snuba_column_name,
     detect_dataset,
+    transform_aliases_and_query,
 )
 
 
@@ -176,16 +179,384 @@ class SnubaUtilsTest(TestCase):
         assert get_snuba_column_name("organization") == "tags[organization]"
 
 
+class TransformAliasesAndQueryTest(SnubaTestCase, TestCase):
+    def setUp(self):
+        super(TransformAliasesAndQueryTest, self).setUp()
+        self.environment = self.create_environment(self.project, name="prod")
+        self.release = self.create_release(self.project, version="first-release")
+
+        self.store_event(
+            data={
+                "message": "oh no",
+                "release": "first-release",
+                "environment": "prod",
+                "platform": "python",
+                "user": {"id": "99", "email": "bruce@example.com", "username": "brucew"},
+                "timestamp": iso_format(before_now(minutes=1)),
+            },
+            project_id=self.project.id,
+        )
+
+    def test_field_aliasing_in_selected_columns(self):
+        result = transform_aliases_and_query(
+            selected_columns=["project.id", "user.email", "release"],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["user.email"] == "bruce@example.com"
+        assert data[0]["release"] == "first-release"
+
+    def test_field_aliasing_in_aggregate_functions_and_groupby(self):
+        result = transform_aliases_and_query(
+            selected_columns=["project.id"],
+            aggregations=[["uniq", "user.email", "uniq_email"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["project.id"],
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["uniq_email"] == 1
+
+    def test_field_aliasing_in_conditions(self):
+        result = transform_aliases_and_query(
+            selected_columns=["project.id", "user.email"],
+            conditions=[["user.email", "=", "bruce@example.com"]],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["user.email"] == "bruce@example.com"
+
+    def test_autoconversion_of_time_column(self):
+        result = transform_aliases_and_query(
+            aggregations=[["count", "", "count"]],
+            filter_keys={"project_id": [self.project.id]},
+            start=before_now(minutes=5),
+            end=before_now(),
+            groupby=["time"],
+            orderby=["time"],
+            rollup=3600,
+        )
+        data = result["data"]
+        assert isinstance(data[-1]["time"], int)
+        assert data[-1]["count"] == 1
+
+    def test_conversion_of_release_filter_key(self):
+        result = transform_aliases_and_query(
+            selected_columns=["id", "message"],
+            filter_keys={
+                "release": [self.create_release(self.project).id],
+                "project_id": [self.project.id],
+            },
+        )
+        assert len(result["data"]) == 0
+
+        result = transform_aliases_and_query(
+            selected_columns=["id", "message"],
+            filter_keys={"release": [self.release.id], "project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 1
+
+    def test_conversion_of_environment_filter_key(self):
+        result = transform_aliases_and_query(
+            selected_columns=["id", "message"],
+            filter_keys={
+                "environment": [self.create_environment(self.project).id],
+                "project_id": [self.project.id],
+            },
+        )
+        assert len(result["data"]) == 0
+
+        result = transform_aliases_and_query(
+            selected_columns=["id", "message"],
+            filter_keys={"environment": [self.environment.id], "project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 1
+
+
+class TransformAliasesAndQueryTransactionsTest(TestCase):
+    """
+    This test mocks snuba.raw_query because there is currently no
+    way to insert data into the transactions dataset during tests.
+    """
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_selected_columns_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            selected_columns=["transaction", "transaction.duration"],
+            aggregations=[
+                ["argMax", ["id", "transaction.duration"], "longest"],
+                ["uniq", "transaction", "uniq_transaction"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            aggregations=[
+                ["argMax", ["event_id", "duration"], "longest"],
+                ["uniq", "transaction_name", "uniq_transaction"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset="transactions",
+            arrayjoin=None,
+            end=None,
+            start=None,
+            conditions=None,
+            groupby=None,
+            having=None,
+            orderby=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_orderby_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            selected_columns=["transaction", "transaction.duration"],
+            filter_keys={"project_id": [self.project.id]},
+            orderby=["timestamp"],
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            filter_keys={"project_id": [self.project.id]},
+            dataset="transactions",
+            orderby=["finish_ts"],
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            conditions=None,
+            groupby=None,
+            having=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_conditions_order_and_groupby_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            selected_columns=["transaction", "transaction.duration"],
+            conditions=[
+                ["transaction.duration", "=", 200],
+                ["time", ">", "2019-09-23"],
+                ["http.method", "=", "GET"],
+            ],
+            aggregations=[["count", "", "count"]],
+            groupby=["transaction.op"],
+            orderby=["-timestamp", "-count"],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            conditions=[
+                ["duration", "=", 200],
+                ["bucketed_end", ">", "2019-09-23"],
+                ["tags[http.method]", "=", "GET"],
+            ],
+            aggregations=[["count", "", "count"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction_op"],
+            orderby=["-finish_ts", "-count"],
+            dataset="transactions",
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_conditions_nested_function_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}],
+            "data": [{"transaction_name": "api.do_things"}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["transaction"],
+            conditions=[
+                ["type", "=", "transaction"],
+                [["positionCaseInsensitive", ["message", "'recent-searches'"]], "!=", 0],
+            ],
+            aggregations=[["count", "", "count"]],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name"],
+            conditions=[
+                [["positionCaseInsensitive", ["transaction_name", "'recent-searches'"]], "!=", 0]
+            ],
+            aggregations=[["count", "", "count"]],
+            filter_keys={"project_id": [self.project.id]},
+            dataset="transactions",
+            groupby=None,
+            orderby=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_condition_removal_skip_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["transaction", "transaction.duration"],
+            conditions=[["type", "=", "transaction"], ["duration", ">", 200]],
+            groupby=["transaction.op"],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            conditions=[["duration", ">", 200]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction_op"],
+            dataset="transactions",
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+            orderby=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_condition_not_remove_type_csp(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["transaction", "transaction.duration"],
+            conditions=[["type", "=", "transaction"], ["type", "=", "csp"], ["duration", ">", 200]],
+            groupby=["transaction.op"],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            conditions=[["tags[type]", "=", "csp"], ["duration", ">", 200]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction_op"],
+            dataset="transactions",
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+            orderby=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_condition_transform_skip_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
+            "data": [{"transaction_name": "api.do_things", "duration": 200}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["transaction", "transaction.duration"],
+            conditions=[["http_method", "=", "GET"]],
+            groupby=["transaction.op"],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction_name", "duration"],
+            conditions=[["tags[http_method]", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["transaction_op"],
+            dataset="transactions",
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+            orderby=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_condition_reformat_event_id_condition(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "id"}, {"name": "duration"}],
+            "data": [{"event_id": "a" * 32, "duration": 200}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["id", "transaction.duration"],
+            conditions=[["id", "=", "a" * 32]],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["event_id", "duration"],
+            conditions=[["event_id", "=", "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa"]],
+            filter_keys={"project_id": [self.project.id]},
+            dataset="transactions",
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+            orderby=None,
+            groupby=None,
+        )
+
+    @patch("sentry.utils.snuba.raw_query")
+    def test_condition_reformat_nested_conditions(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "id"}, {"name": "duration"}],
+            "data": [{"id": "a" * 32, "duration": 200}],
+        }
+        transform_aliases_and_query(
+            skip_conditions=True,
+            selected_columns=["id", "transaction.duration"],
+            conditions=[[["timestamp", ">", "2019-09-26T12:13:14"], ["id", "=", "a" * 32]]],
+            filter_keys={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["event_id", "duration"],
+            conditions=[
+                [
+                    ["finish_ts", ">", "2019-09-26T12:13:14"],
+                    ["event_id", "=", "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa"],
+                ]
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset="transactions",
+            aggregations=None,
+            arrayjoin=None,
+            end=None,
+            start=None,
+            having=None,
+            orderby=None,
+            groupby=None,
+        )
+
+
 class DetectDatasetTest(TestCase):
     def test_dataset_key(self):
         query = {"dataset": "events", "conditions": [["event.type", "=", "transaction"]]}
         assert detect_dataset(query) == "events"
 
     def test_event_type_condition(self):
-        query = {"conditions": [["event.type", "=", "transaction"]]}
+        query = {"conditions": [["type", "=", "transaction"]]}
         assert detect_dataset(query) == "transactions"
 
-        query = {"conditions": [["event.type", "!=", "transaction"]]}
+        query = {"conditions": [["type", "=", "error"]]}
         assert detect_dataset(query) == "events"
 
         query = {"conditions": [["type", "=", "transaction"]]}
@@ -204,6 +575,18 @@ class DetectDatasetTest(TestCase):
         query = {"conditions": [["transaction.duration", ">", "3"]]}
         assert detect_dataset(query) == "transactions"
 
+        # Internal aliases are treated as tags
+        query = {"conditions": [["duration", ">", "3"]]}
+        assert detect_dataset(query) == "events"
+
+    def test_conditions_aliased(self):
+        query = {"conditions": [["duration", ">", "3"]]}
+        assert detect_dataset(query, aliased_conditions=True) == "transactions"
+
+        # Not an internal alias
+        query = {"conditions": [["transaction.duration", ">", "3"]]}
+        assert detect_dataset(query, aliased_conditions=True) == "events"
+
     def test_selected_columns(self):
         query = {"selected_columns": ["id", "message"]}
         assert detect_dataset(query) == "events"
diff --git a/tests/snuba/api/endpoints/test_group_events_latest.py b/tests/snuba/api/endpoints/test_group_events_latest.py
index dcdcf4edfe..9555c5dd6c 100644
--- a/tests/snuba/api/endpoints/test_group_events_latest.py
+++ b/tests/snuba/api/endpoints/test_group_events_latest.py
@@ -51,3 +51,9 @@ class GroupEventsLatestTest(APITestCase, SnubaTestCase):
 
         assert response.status_code == 200
         assert response.data["id"] == six.text_type(self.event2.event_id)
+
+    def test_simple(self):
+        url = u"/api/0/issues/{}/events/latest/".format(self.group.id)
+        response = self.client.get(url, format="json")
+        assert response.status_code == 200
+        assert response.data["eventID"] == six.text_type(self.event2.event_id)
diff --git a/tests/snuba/api/endpoints/test_group_events_oldest.py b/tests/snuba/api/endpoints/test_group_events_oldest.py
index 6cf6b008c3..8af1e54c22 100644
--- a/tests/snuba/api/endpoints/test_group_events_oldest.py
+++ b/tests/snuba/api/endpoints/test_group_events_oldest.py
@@ -50,3 +50,10 @@ class GroupEventsOldestTest(APITestCase, SnubaTestCase):
 
         assert response.status_code == 200
         assert response.data["id"] == six.text_type(self.event2.event_id)
+
+    def test_simple(self):
+        url = u"/api/0/issues/{}/events/oldest/".format(self.group.id)
+        response = self.client.get(url, format="json")
+
+        assert response.status_code == 200
+        assert response.data["id"] == six.text_type(self.event1.event_id)
diff --git a/tests/snuba/api/endpoints/test_organization_event_details.py b/tests/snuba/api/endpoints/test_organization_event_details.py
index 8c5a00b638..e5f7d9b71a 100644
--- a/tests/snuba/api/endpoints/test_organization_event_details.py
+++ b/tests/snuba/api/endpoints/test_organization_event_details.py
@@ -1,5 +1,7 @@
 from __future__ import absolute_import
 
+from datetime import timedelta
+
 from django.core.urlresolvers import reverse
 from sentry.testutils import APITestCase, SnubaTestCase
 from sentry.testutils.helpers.datetime import iso_format, before_now
@@ -243,3 +245,41 @@ class OrganizationEventDetailsEndpointTest(APITestCase, SnubaTestCase):
         assert response.data["oldestEventID"] is None, "no older matching events"
         assert response.data["nextEventID"] == "2" * 32, "2 is older and has matching tags "
         assert response.data["latestEventID"] == "2" * 32, "2 is oldest matching message"
+
+    def test_event_links_with_transaction_events(self):
+        prototype = {
+            "type": "transaction",
+            "transaction": "api.issue.delete",
+            "spans": [],
+            "contexts": {"trace": {"trace_id": "a" * 32, "span_id": "a" * 16}},
+            "tags": {"important": "yes"},
+        }
+        fixtures = (
+            ("d" * 32, before_now(minutes=1)),
+            ("e" * 32, before_now(minutes=2)),
+            ("f" * 32, before_now(minutes=3)),
+        )
+        for fixture in fixtures:
+            data = prototype.copy()
+            data["event_id"] = fixture[0]
+            data["timestamp"] = iso_format(fixture[1])
+            data["start_timestamp"] = iso_format(fixture[1] - timedelta(seconds=5))
+            self.store_event(data=data, project_id=self.project.id)
+
+        url = reverse(
+            "sentry-api-0-organization-event-details",
+            kwargs={
+                "organization_slug": self.project.organization.slug,
+                "project_slug": self.project.slug,
+                "event_id": "e" * 32,
+            },
+        )
+        with self.feature("organizations:events-v2"):
+            response = self.client.get(
+                url,
+                format="json",
+                data={"field": ["important", "count()"], "query": "transaction.duration:>2"},
+            )
+        assert response.status_code == 200
+        assert response.data["nextEventID"].replace("-", "") == "d" * 32
+        assert response.data["previousEventID"].replace("-", "") == "f" * 32
diff --git a/tests/snuba/api/endpoints/test_organization_events.py b/tests/snuba/api/endpoints/test_organization_events.py
index 32ba25252a..394f193ba7 100644
--- a/tests/snuba/api/endpoints/test_organization_events.py
+++ b/tests/snuba/api/endpoints/test_organization_events.py
@@ -137,6 +137,18 @@ class OrganizationEventsEndpointTest(APITestCase, SnubaTestCase):
             == "Parse error: 'search' (column 4). This is commonly caused by unmatched-parentheses. Enclose any text in double quotes."
         )
 
+    def test_invalid_search_referencing_transactions(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        url = reverse(
+            "sentry-api-0-organization-events",
+            kwargs={"organization_slug": project.organization.slug},
+        )
+        response = self.client.get(url, {"query": "transaction.duration:>200"}, format="json")
+
+        assert response.status_code == 400, response.content
+        assert "cannot reference non-events data" in response.data["detail"]
+
     def test_project_filtering(self):
         user = self.create_user(is_staff=False, is_superuser=False)
         org = self.create_organization()
diff --git a/tests/snuba/api/endpoints/test_organization_events_distribution.py b/tests/snuba/api/endpoints/test_organization_events_distribution.py
index 97905dc9b8..32596c3987 100644
--- a/tests/snuba/api/endpoints/test_organization_events_distribution.py
+++ b/tests/snuba/api/endpoints/test_organization_events_distribution.py
@@ -10,7 +10,7 @@ from sentry.testutils import APITestCase, SnubaTestCase
 from sentry.testutils.helpers.datetime import before_now, iso_format
 
 
-class OrganizationEventsDistributionEndpointTest(APITestCase, SnubaTestCase):
+class OrganizationEventsDistributionEndpointTest(SnubaTestCase, APITestCase):
     feature_list = ("organizations:events-v2", "organizations:global-views")
 
     def setUp(self):
@@ -210,6 +210,7 @@ class OrganizationEventsDistributionEndpointTest(APITestCase, SnubaTestCase):
             data={
                 "event_id": uuid4().hex,
                 "timestamp": iso_format(self.day_ago),
+                "message": "very bad",
                 "tags": {"sentry:user": self.user.email},
             },
             project_id=self.project.id,
@@ -218,6 +219,7 @@ class OrganizationEventsDistributionEndpointTest(APITestCase, SnubaTestCase):
             data={
                 "event_id": uuid4().hex,
                 "timestamp": iso_format(self.day_ago),
+                "message": "very bad",
                 "tags": {"sentry:user": self.user2.email},
             },
             project_id=self.project.id,
@@ -226,6 +228,7 @@ class OrganizationEventsDistributionEndpointTest(APITestCase, SnubaTestCase):
             data={
                 "event_id": uuid4().hex,
                 "timestamp": iso_format(self.day_ago),
+                "message": "very bad",
                 "tags": {"sentry:user": self.user2.email},
             },
             project_id=self.project.id,
diff --git a/tests/snuba/api/endpoints/test_organization_events_meta.py b/tests/snuba/api/endpoints/test_organization_events_meta.py
index 575d1f4e4b..5d03f3a0c1 100644
--- a/tests/snuba/api/endpoints/test_organization_events_meta.py
+++ b/tests/snuba/api/endpoints/test_organization_events_meta.py
@@ -3,7 +3,7 @@ from __future__ import absolute_import
 from django.core.urlresolvers import reverse
 
 from sentry.testutils import APITestCase, SnubaTestCase
-from sentry.testutils.helpers.datetime import before_now
+from sentry.testutils.helpers.datetime import before_now, iso_format
 
 
 class OrganizationEventsMetaEndpoint(APITestCase, SnubaTestCase):
@@ -62,3 +62,27 @@ class OrganizationEventsMetaEndpoint(APITestCase, SnubaTestCase):
 
         assert response.status_code == 200, response.content
         assert response.data["count"] == 0
+
+    def test_transaction_event(self):
+        self.login_as(user=self.user)
+
+        project = self.create_project()
+        data = {
+            "event_id": "a" * 32,
+            "type": "transaction",
+            "transaction": "api.issue.delete",
+            "spans": [],
+            "contexts": {"trace": {"trace_id": "a" * 32, "span_id": "a" * 16}},
+            "tags": {"important": "yes"},
+            "timestamp": iso_format(before_now(minutes=1)),
+            "start_timestamp": iso_format(before_now(minutes=1, seconds=3)),
+        }
+        self.store_event(data=data, project_id=project.id)
+        url = reverse(
+            "sentry-api-0-organization-events-meta",
+            kwargs={"organization_slug": project.organization.slug},
+        )
+        response = self.client.get(url, {"query": "transaction.duration:>1"}, format="json")
+
+        assert response.status_code == 200, response.content
+        assert response.data["count"] == 1
diff --git a/tests/snuba/api/endpoints/test_organization_events_stats.py b/tests/snuba/api/endpoints/test_organization_events_stats.py
index f27da3a66d..04813fadd0 100644
--- a/tests/snuba/api/endpoints/test_organization_events_stats.py
+++ b/tests/snuba/api/endpoints/test_organization_events_stats.py
@@ -283,3 +283,41 @@ class OrganizationEventsStatsEndpointTest(APITestCase, SnubaTestCase):
             [{"count": 1}],
             [{"count": 1}],
         ]
+
+    def test_transaction_events(self):
+        prototype = {
+            "type": "transaction",
+            "transaction": "api.issue.delete",
+            "spans": [],
+            "contexts": {"trace": {"trace_id": "a" * 32, "span_id": "a" * 16}},
+            "tags": {"important": "yes"},
+        }
+        fixtures = (
+            ("d" * 32, before_now(minutes=32)),
+            ("e" * 32, before_now(hours=1, minutes=2)),
+            ("f" * 32, before_now(hours=1, minutes=35)),
+        )
+        for fixture in fixtures:
+            data = prototype.copy()
+            data["event_id"] = fixture[0]
+            data["timestamp"] = iso_format(fixture[1])
+            data["start_timestamp"] = iso_format(fixture[1] - timedelta(seconds=1))
+            self.store_event(data=data, project_id=self.project.id)
+
+        with self.feature("organizations:events-v2"):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "end": iso_format(before_now()),
+                    "start": iso_format(before_now(hours=2)),
+                    "query": "event.type:transaction",
+                    "interval": "30m",
+                    "yAxis": "count()",
+                },
+            )
+        assert response.status_code == 200, response.content
+        items = [item for time, item in response.data["data"] if item]
+        # We could get more results depending on where the 30 min
+        # windows land.
+        assert len(items) >= 3
diff --git a/tests/snuba/api/endpoints/test_organization_events_v2.py b/tests/snuba/api/endpoints/test_organization_events_v2.py
index 75ad57f568..cefa0a5ef2 100644
--- a/tests/snuba/api/endpoints/test_organization_events_v2.py
+++ b/tests/snuba/api/endpoints/test_organization_events_v2.py
@@ -480,7 +480,7 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
         assert response.status_code == 400, response.content
         assert response.data["detail"] == "No fields provided"
 
-    def test_condition_on_aggregate_fails(self):
+    def test_condition_on_aggregate_misses(self):
         self.login_as(user=self.user)
         project = self.create_project()
         self.store_event(
diff --git a/tests/snuba/api/endpoints/test_project_event_details.py b/tests/snuba/api/endpoints/test_project_event_details.py
index c682d498eb..56aceb9473 100644
--- a/tests/snuba/api/endpoints/test_project_event_details.py
+++ b/tests/snuba/api/endpoints/test_project_event_details.py
@@ -49,7 +49,7 @@ class ProjectEventDetailsTest(APITestCase, SnubaTestCase):
             project_id=project.id,
         )
 
-    def test_snuba(self):
+    def test_simple(self):
         url = reverse(
             "sentry-api-0-project-event-details",
             kwargs={
