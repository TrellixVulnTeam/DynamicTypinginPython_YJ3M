commit 91cd88e14650c36380e660b99a4470b2db10dceb
Author: Leander Rodrigues <leandergrodrigues@gmail.com>
Date:   Fri Mar 6 11:33:58 2020 -0800

    chore(async-csv): Add metrics and logging to feature (#17483)
    
    Analytics on async export feature

diff --git a/src/sentry/api/endpoints/data_export.py b/src/sentry/api/endpoints/data_export.py
index 65036a2510..fc2484884d 100644
--- a/src/sentry/api/endpoints/data_export.py
+++ b/src/sentry/api/endpoints/data_export.py
@@ -11,6 +11,7 @@ from sentry.api.serializers import serialize
 from sentry.constants import ExportQueryType
 from sentry.models import ExportedData
 from sentry.tasks.data_export import assemble_download
+from sentry.utils import metrics
 
 
 class ExportedDataSerializer(serializers.Serializer):
@@ -53,9 +54,11 @@ class DataExportEndpoint(OrganizationEndpoint):
             )
             status = 200
             if created:
+                metrics.incr("dataexport.start", tags={"query_type": data["query_type"]})
                 assemble_download.delay(data_export_id=data_export.id)
                 status = 201
         except ValidationError as e:
             # This will handle invalid JSON requests
+            metrics.incr("dataexport.invalid", tags={"query_type": data.get("query_type")})
             return Response({"detail": six.text_type(e)}, status=400)
         return Response(serialize(data_export, request.user), status=status)
diff --git a/src/sentry/api/endpoints/data_export_details.py b/src/sentry/api/endpoints/data_export_details.py
index f54e6881f2..e2477460ca 100644
--- a/src/sentry/api/endpoints/data_export_details.py
+++ b/src/sentry/api/endpoints/data_export_details.py
@@ -7,6 +7,7 @@ from sentry import features
 from sentry.api.bases.organization import OrganizationEndpoint, OrganizationDataExportPermission
 from sentry.api.serializers import serialize
 from sentry.models import ExportedData
+from sentry.utils import metrics
 
 
 class DataExportDetailsEndpoint(OrganizationEndpoint):
@@ -31,6 +32,7 @@ class DataExportDetailsEndpoint(OrganizationEndpoint):
             return Response(status=404)
 
     def download(self, data_export):
+        metrics.incr("dataexport.download")
         file = data_export.file
         raw_file = file.getfile()
         response = StreamingHttpResponse(
diff --git a/src/sentry/models/exporteddata.py b/src/sentry/models/exporteddata.py
index 7b95944f1c..0b6e65ce95 100644
--- a/src/sentry/models/exporteddata.py
+++ b/src/sentry/models/exporteddata.py
@@ -1,6 +1,7 @@
 from __future__ import absolute_import
 
 import json
+import logging
 import six
 from enum import Enum
 from datetime import timedelta
@@ -17,8 +18,12 @@ from sentry.db.models import (
     Model,
     sane_repr,
 )
+from sentry.utils import metrics
 from sentry.utils.http import absolute_uri
 
+logger = logging.getLogger(__name__)
+
+
 # Arbitrary, subject to change
 DEFAULT_EXPIRATION = timedelta(weeks=4)
 
@@ -88,7 +93,10 @@ class ExportedData(Model):
 
         # The following condition should never be true, but it's a safeguard in case someone manually calls this method
         if self.date_finished is None or self.date_expired is None or self.file is None:
-            # TODO(Leander): Implement logging here
+            logger.warning(
+                "Notification email attempted on incomplete dataset",
+                extra={"data_export_id": self.id, "organization_id": self.organization_id},
+            )
             return
         url = absolute_uri(
             reverse("sentry-data-export-details", args=[self.organization.slug, self.id])
@@ -101,6 +109,7 @@ class ExportedData(Model):
             html_template="sentry/emails/data-export-success.html",
         )
         msg.send_async([self.user.email])
+        metrics.incr("dataexport.end", instance="success")
 
     def email_failure(self, message):
         from sentry.utils.email import MessageBuilder
@@ -117,6 +126,7 @@ class ExportedData(Model):
             html_template="sentry/emails/data-export-failure.html",
         )
         msg.send_async([self.user.email])
+        metrics.incr("dataexport.end", instance="failure")
         self.delete()
 
     class Meta:
diff --git a/src/sentry/tasks/data_export.py b/src/sentry/tasks/data_export.py
index 8e8c0e6e96..a28df2133c 100644
--- a/src/sentry/tasks/data_export.py
+++ b/src/sentry/tasks/data_export.py
@@ -1,6 +1,8 @@
 from __future__ import absolute_import
 
 import csv
+import logging
+import six
 import tempfile
 from contextlib import contextmanager
 from django.db import transaction, IntegrityError
@@ -9,11 +11,13 @@ from sentry import tagstore
 from sentry.constants import ExportQueryType
 from sentry.models import EventUser, ExportedData, File, Group, Project, get_group_with_redirect
 from sentry.tasks.base import instrumented_task
-from sentry.utils import snuba
+from sentry.utils import metrics, snuba
 from sentry.utils.sdk import capture_exception
 
 SNUBA_MAX_RESULTS = 1000
 
+logger = logging.getLogger(__name__)
+
 
 class DataExportError(Exception):
     pass
@@ -23,6 +27,7 @@ class DataExportError(Exception):
 def assemble_download(data_export_id):
     # Extract the ExportedData object
     try:
+        logger.info("dataexport.start", extra={"data_export_id": data_export_id})
         data_export = ExportedData.objects.get(id=data_export_id)
     except ExportedData.DoesNotExist as error:
         return capture_exception(error)
@@ -44,20 +49,26 @@ def assemble_download(data_export_id):
                     file = File.objects.create(
                         name=file_name, type="export.csv", headers={"Content-Type": "text/csv"}
                     )
-                    file.putfile(tf)
+                    file.putfile(tf, logger=logger)
                     data_export.finalize_upload(file=file)
+                    logger.info("dataexport.end", extra={"data_export_id": data_export_id})
             except IntegrityError as error:
-                capture_exception(error)
+                metrics.incr("dataexport.error", instance=six.text_type(error))
+                logger.error(
+                    "dataexport.error: {}".format(six.text_type(error)),
+                    extra={"query": data_export.payload, "org": data_export.organization_id},
+                )
                 raise DataExportError("Failed to save the assembled file")
     except DataExportError as error:
-        # TODO(Leander): Implement logging
         return data_export.email_failure(message=error)
     except NotImplementedError as error:
-        # TODO(Leander): Implement logging
         return data_export.email_failure(message=error)
     except BaseException as error:
-        # TODO(Leander): Implement logging
-        capture_exception(error)
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error(
+            "dataexport.error: {}".format(six.text_type(error)),
+            extra={"query": data_export.payload, "org": data_export.organization_id},
+        )
         return data_export.email_failure(message="Internal processing failure")
 
 
@@ -81,7 +92,9 @@ def process_issue_by_tag(data_export, file, limit=None):
     try:
         payload = data_export.query_info
         project = Project.objects.get(id=payload["project_id"])
-    except Project.DoesNotExist:
+    except Project.DoesNotExist as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
         raise DataExportError("Requested project does not exist")
 
     # Get the pertaining issue
@@ -89,12 +102,14 @@ def process_issue_by_tag(data_export, file, limit=None):
         group, _ = get_group_with_redirect(
             payload["group_id"], queryset=Group.objects.filter(project=project)
         )
-    except Group.DoesNotExist:
+    except Group.DoesNotExist as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
         raise DataExportError("Requested issue does not exist")
 
     # Get the pertaining key
     key = payload["key"]
-    lookup_key = u"sentry:{}".format(key) if tagstore.is_reserved_key(key) else key
+    lookup_key = six.text_type("sentry:{}").format(key) if tagstore.is_reserved_key(key) else key
 
     # If the key is the 'user' tag, attach the event user
     def attach_eventuser(items):
@@ -120,7 +135,7 @@ def process_issue_by_tag(data_export, file, limit=None):
         fields = ["value", "times_seen", "last_seen", "first_seen"]
 
     # Example file name: ISSUE_BY_TAG-project10-user__721.csv
-    file_details = u"{}-{}__{}".format(project.slug, key, data_export.id)
+    file_details = six.text_type("{}-{}__{}").format(project.slug, key, data_export.id)
     file_name = get_file_name(ExportQueryType.ISSUE_BY_TAG_STR, file_details)
 
     # Iterate through all the GroupTagValues
@@ -158,7 +173,7 @@ def create_writer(file, fields):
 
 
 def get_file_name(export_type, custom_string, extension="csv"):
-    file_name = u"{}-{}.{}".format(export_type, custom_string, extension)
+    file_name = six.text_type("{}-{}.{}").format(export_type, custom_string, extension)
     return file_name
 
 
@@ -167,11 +182,17 @@ def get_file_name(export_type, custom_string, extension="csv"):
 def snuba_error_handler():
     try:
         yield
-    except snuba.QueryOutsideRetentionError:
+    except snuba.QueryOutsideRetentionError as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
         raise DataExportError("Invalid date range. Please try a more recent date range.")
-    except snuba.QueryIllegalTypeOfArgument:
+    except snuba.QueryIllegalTypeOfArgument as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
         raise DataExportError("Invalid query. Argument to function is wrong type.")
     except snuba.SnubaError as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
         message = "Internal error. Please try again."
         if isinstance(
             error,
@@ -186,7 +207,6 @@ def snuba_error_handler():
             error,
             (snuba.UnqualifiedQueryError, snuba.QueryExecutionError, snuba.SchemaValidationError),
         ):
-            capture_exception(error)
             message = "Internal error. Your query failed to run."
         raise DataExportError(message)
 
