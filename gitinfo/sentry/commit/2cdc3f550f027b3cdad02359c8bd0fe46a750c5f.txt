commit 2cdc3f550f027b3cdad02359c8bd0fe46a750c5f
Author: Chris Fuller <cfuller@sentry.io>
Date:   Mon Jun 8 11:16:21 2020 -0400

    ref(app): Refactor snuba column resolver (#19171)

diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 8e8ab0d9a3..45bc969fdd 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -27,12 +27,11 @@ from sentry.tagstore.base import TOP_VALUES_DEFAULT_LIMIT
 from sentry.utils.snuba import (
     Dataset,
     SnubaTSResult,
-    DISCOVER_COLUMN_MAP,
-    QUOTED_LITERAL_RE,
     raw_query,
     to_naive_timestamp,
     naiveify_datetime,
-    resolve_condition,
+    resolve_snuba_aliases,
+    resolve_column,
 )
 
 __all__ = (
@@ -59,6 +58,8 @@ ReferenceEvent.__new__.__defaults__ = (None, None)
 PaginationResult = namedtuple("PaginationResult", ["next", "previous", "oldest", "latest"])
 FacetResult = namedtuple("FacetResult", ["key", "value", "count"])
 
+resolve_discover_column = resolve_column(Dataset.Discover)
+
 
 def is_real_column(col):
     """
@@ -80,7 +81,9 @@ def find_reference_event(reference_event):
     except ValueError:
         raise InvalidSearchQuery("Invalid reference event")
 
-    column_names = [resolve_column(col) for col in reference_event.fields if is_real_column(col)]
+    column_names = [
+        resolve_discover_column(col) for col in reference_event.fields if is_real_column(col)
+    ]
     # We don't need to run a query if there are no columns
     if not column_names:
         return None
@@ -134,7 +137,7 @@ def create_reference_event_conditions(reference_event):
     if event_data is None:
         return conditions
 
-    field_names = [resolve_column(col) for col in reference_event.fields]
+    field_names = [resolve_discover_column(col) for col in reference_event.fields]
     for (i, field) in enumerate(reference_event.fields):
         value = event_data.get(field_names[i], None)
         # If the value is a sequence use the first element as snuba
@@ -272,40 +275,6 @@ def zerofill_histogram(results, column_meta, orderby, sentry_function_alias, snu
     return new_results
 
 
-def resolve_column(col):
-    """
-    Used as a column resolver in discover queries.
-
-    Resolve a public schema name to the discover dataset.
-    unknown columns are converted into tags expressions.
-    """
-    if col is None:
-        return col
-    elif isinstance(col, (list, tuple)):
-        return col
-    # Whilst project_id is not part of the public schema we convert
-    # the project.name field into project_id way before we get here.
-    if col == "project_id":
-        return col
-    if col.startswith("tags[") or QUOTED_LITERAL_RE.match(col):
-        return col
-
-    return DISCOVER_COLUMN_MAP.get(col, u"tags[{}]".format(col))
-
-
-# TODO (evanh) Since we are assuming that all string values are columns,
-# this will get tricky if we ever have complex columns where there are
-# string arguments to the functions that aren't columns
-def resolve_complex_column(col):
-    args = col[1]
-
-    for i in range(len(args)):
-        if isinstance(args[i], (list, tuple)):
-            resolve_complex_column(args[i])
-        elif isinstance(args[i], six.string_types):
-            args[i] = resolve_column(args[i])
-
-
 def resolve_discover_aliases(snuba_filter, function_translations=None):
     """
     Resolve the public schema aliases to the discover dataset.
@@ -314,70 +283,9 @@ def resolve_discover_aliases(snuba_filter, function_translations=None):
     `translated_columns` key containing the selected fields that need to
     be renamed in the result set.
     """
-    resolved = snuba_filter.clone()
-    translated_columns = {}
-    derived_columns = set()
-    if function_translations:
-        for snuba_name, sentry_name in six.iteritems(function_translations):
-            derived_columns.add(snuba_name)
-            translated_columns[snuba_name] = sentry_name
-
-    selected_columns = resolved.selected_columns
-    if selected_columns:
-        for (idx, col) in enumerate(selected_columns):
-            if isinstance(col, (list, tuple)):
-                resolve_complex_column(col)
-            else:
-                name = resolve_column(col)
-                selected_columns[idx] = name
-                translated_columns[name] = col
-
-        resolved.selected_columns = selected_columns
-
-    groupby = resolved.groupby
-    if groupby:
-        for (idx, col) in enumerate(groupby):
-            name = col
-            if isinstance(col, (list, tuple)):
-                if len(col) == 3:
-                    name = col[2]
-            elif col not in derived_columns:
-                name = resolve_column(col)
-
-            groupby[idx] = name
-        resolved.groupby = groupby
-
-    aggregations = resolved.aggregations
-    for aggregation in aggregations or []:
-        derived_columns.add(aggregation[2])
-        if isinstance(aggregation[1], six.string_types):
-            aggregation[1] = resolve_column(aggregation[1])
-        elif isinstance(aggregation[1], (set, tuple, list)):
-            aggregation[1] = [resolve_column(col) for col in aggregation[1]]
-    resolved.aggregations = aggregations
-
-    conditions = resolved.conditions
-    if conditions:
-        for (i, condition) in enumerate(conditions):
-            replacement = resolve_condition(condition, resolve_column)
-            conditions[i] = replacement
-        resolved.conditions = [c for c in conditions if c]
-
-    orderby = resolved.orderby
-    if orderby:
-        orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
-        resolved_orderby = []
-
-        for field_with_order in orderby:
-            field = field_with_order.lstrip("-")
-            resolved_orderby.append(
-                u"{}{}".format(
-                    "-" if field_with_order.startswith("-") else "",
-                    field if field in derived_columns else resolve_column(field),
-                )
-            )
-        resolved.orderby = resolved_orderby
-    return resolved, translated_columns
+    return resolve_snuba_aliases(
+        snuba_filter, resolve_discover_column, function_translations=function_translations
+    )
 
 
 def zerofill(data, start, end, rollup, orderby):
@@ -918,7 +826,6 @@ def top_events_timeseries(
         )
 
         user_fields = FIELD_ALIASES["user"]["fields"]
-
         for field in selected_columns:
             # project is handled by filter_keys already
             if field in ["project", "project.id"]:
@@ -940,16 +847,19 @@ def top_events_timeseries(
                 # A user field can be any of its field aliases, do an OR across all the user fields
                 elif field == "user":
                     snuba_filter.conditions.append(
-                        [[resolve_column(user_field), "IN", values] for user_field in user_fields]
+                        [
+                            [resolve_discover_column(user_field), "IN", values]
+                            for user_field in user_fields
+                        ]
                     )
                 elif None in values:
                     non_none_values = [value for value in values if value is not None]
-                    condition = [[["isNull", [resolve_column(field)]], "=", 1]]
+                    condition = [[["isNull", [resolve_discover_column(field)]], "=", 1]]
                     if non_none_values:
-                        condition.append([resolve_column(field), "IN", non_none_values])
+                        condition.append([resolve_discover_column(field), "IN", non_none_values])
                     snuba_filter.conditions.append(condition)
                 else:
-                    snuba_filter.conditions.append([resolve_column(field), "IN", values])
+                    snuba_filter.conditions.append([resolve_discover_column(field), "IN", values])
 
     with sentry_sdk.start_span(op="discover.discover", description="top_events.snuba_query"):
         result = raw_query(
diff --git a/src/sentry/snuba/tasks.py b/src/sentry/snuba/tasks.py
index 323ac3e1d1..8490b626db 100644
--- a/src/sentry/snuba/tasks.py
+++ b/src/sentry/snuba/tasks.py
@@ -3,11 +3,16 @@ from __future__ import absolute_import
 import json
 
 from sentry.api.event_search import get_filter, resolve_field_list
-from sentry.snuba.discover import resolve_discover_aliases
 from sentry.snuba.models import QueryDatasets, QuerySubscription
 from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
-from sentry.utils.snuba import _snuba_pool, SnubaError
+from sentry.utils.snuba import (
+    _snuba_pool,
+    Dataset,
+    SnubaError,
+    resolve_snuba_aliases,
+    resolve_column,
+)
 
 
 # TODO: If we want to support security events here we'll need a way to
@@ -115,7 +120,7 @@ def delete_subscription_from_snuba(query_subscription_id, **kwargs):
 def build_snuba_filter(dataset, query, aggregate, environment, params=None):
     snuba_filter = get_filter(query, params=params)
     snuba_filter.update_with(resolve_field_list([aggregate], snuba_filter, auto_fields=False))
-    snuba_filter = resolve_discover_aliases(snuba_filter)[0]
+    snuba_filter = resolve_snuba_aliases(snuba_filter, resolve_column(Dataset.Discover))[0]
     if environment:
         snuba_filter.conditions.append(["environment", "=", environment.name])
     snuba_filter.conditions = apply_dataset_conditions(dataset, snuba_filter.conditions)
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index f326642c2d..0f9e71f12a 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -701,25 +701,33 @@ def nest_groups(data, groups, aggregate_cols):
         )
 
 
-def resolve_column(col, dataset):
-    if col.startswith("tags["):
-        return col
+def resolve_column(dataset):
+    def _resolve_column(col):
+        if col is None or col.startswith("tags[") or QUOTED_LITERAL_RE.match(col):
+            return col
+
+        # Some dataset specific logic:
+        if dataset == Dataset.Discover:
+            if isinstance(col, (list, tuple)) or col == "project_id":
+                return col
+        else:
+            if (
+                col in DATASET_FIELDS[dataset]
+            ):  # Discover does not allow internal aliases to be used by customers, so doesn't get this logic.
+                return col
 
-    if not col or QUOTED_LITERAL_RE.match(col):
-        return col
-    if col in DATASETS[dataset]:
-        return DATASETS[dataset][col]
-    if col in DATASET_FIELDS[dataset]:
-        return col
+        if col in DATASETS[dataset]:
+            return DATASETS[dataset][col]
+        return u"tags[{}]".format(col)
 
-    return u"tags[{}]".format(col)
+    return _resolve_column
 
 
 def resolve_condition(cond, column_resolver):
     """
     When conditions have been parsed by the api.event_search module
     we can end up with conditions that are not valid on the current dataset
-    due to how ap.event_search checks for valid field names without
+    due to how api.event_search checks for valid field names without
     being aware of the dataset.
 
     We have the dataset context here, so we need to re-scope conditions to the
@@ -797,12 +805,13 @@ def _aliased_query_impl(
         raise ValueError("A dataset is required, and is no longer automatically detected.")
 
     derived_columns = []
+    resolve_func = resolve_column(dataset)
     if selected_columns:
         for (i, col) in enumerate(selected_columns):
             if isinstance(col, (list, tuple)):
                 derived_columns.append(col[2])
             else:
-                selected_columns[i] = resolve_column(col, dataset)
+                selected_columns[i] = resolve_func(col)
         selected_columns = [c for c in selected_columns if c]
 
     if aggregations:
@@ -810,8 +819,11 @@ def _aliased_query_impl(
             derived_columns.append(aggregation[2])
 
     if conditions:
-        condition_resolver = condition_resolver or resolve_column
-        column_resolver = functools.partial(condition_resolver, dataset=dataset)
+        column_resolver = (
+            functools.partial(condition_resolver, dataset=dataset)
+            if condition_resolver
+            else resolve_func
+        )
         for (i, condition) in enumerate(conditions):
             replacement = resolve_condition(condition, column_resolver)
             conditions[i] = replacement
@@ -823,7 +835,7 @@ def _aliased_query_impl(
         for (i, order) in enumerate(orderby):
             order_field = order.lstrip("-")
             if order_field not in derived_columns:
-                order_field = resolve_column(order_field, dataset)
+                order_field = resolve_func(order_field)
             updated_order.append(u"{}{}".format("-" if order.startswith("-") else "", order_field))
         orderby = updated_order
 
@@ -843,6 +855,86 @@ def _aliased_query_impl(
     )
 
 
+# TODO (evanh) Since we are assuming that all string values are columns,
+# this will get tricky if we ever have complex columns where there are
+# string arguments to the functions that aren't columns
+def resolve_complex_column(col, resolve_func):
+    args = col[1]
+
+    for i in range(len(args)):
+        if isinstance(args[i], (list, tuple)):
+            resolve_complex_column(args[i], resolve_func)
+        elif isinstance(args[i], six.string_types):
+            args[i] = resolve_func(args[i])
+
+
+def resolve_snuba_aliases(snuba_filter, resolve_func, function_translations=None):
+    resolved = snuba_filter.clone()
+    translated_columns = {}
+    derived_columns = set()
+    if function_translations:
+        for snuba_name, sentry_name in six.iteritems(function_translations):
+            derived_columns.add(snuba_name)
+            translated_columns[snuba_name] = sentry_name
+
+    selected_columns = resolved.selected_columns
+    if selected_columns:
+        for (idx, col) in enumerate(selected_columns):
+            if isinstance(col, (list, tuple)):
+                resolve_complex_column(col, resolve_func)
+            else:
+                name = resolve_func(col)
+                selected_columns[idx] = name
+                translated_columns[name] = col
+
+        resolved.selected_columns = selected_columns
+
+    groupby = resolved.groupby
+    if groupby:
+        for (idx, col) in enumerate(groupby):
+            name = col
+            if isinstance(col, (list, tuple)):
+                if len(col) == 3:
+                    name = col[2]
+            elif col not in derived_columns:
+                name = resolve_func(col)
+
+            groupby[idx] = name
+        resolved.groupby = groupby
+
+    aggregations = resolved.aggregations
+    for aggregation in aggregations or []:
+        derived_columns.add(aggregation[2])
+        if isinstance(aggregation[1], six.string_types):
+            aggregation[1] = resolve_func(aggregation[1])
+        elif isinstance(aggregation[1], (set, tuple, list)):
+            aggregation[1] = [resolve_func(col) for col in aggregation[1]]
+    resolved.aggregations = aggregations
+
+    conditions = resolved.conditions
+    if conditions:
+        for (i, condition) in enumerate(conditions):
+            replacement = resolve_condition(condition, resolve_func)
+            conditions[i] = replacement
+        resolved.conditions = [c for c in conditions if c]
+
+    orderby = resolved.orderby
+    if orderby:
+        orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+        resolved_orderby = []
+
+        for field_with_order in orderby:
+            field = field_with_order.lstrip("-")
+            resolved_orderby.append(
+                u"{}{}".format(
+                    "-" if field_with_order.startswith("-") else "",
+                    field if field in derived_columns else resolve_func(field),
+                )
+            )
+        resolved.orderby = resolved_orderby
+    return resolved, translated_columns
+
+
 JSON_TYPE_MAP = {
     "UInt8": "boolean",
     "UInt16": "integer",
diff --git a/tests/sentry/eventstore/snuba/test_backend.py b/tests/sentry/eventstore/snuba/test_backend.py
index 9abb6059c6..50e83e2098 100644
--- a/tests/sentry/eventstore/snuba/test_backend.py
+++ b/tests/sentry/eventstore/snuba/test_backend.py
@@ -161,7 +161,7 @@ class SnubaEventStorageTest(TestCase, SnubaTestCase):
     def test_transaction_get_next_prev_event_id(self):
         _filter = Filter(
             project_ids=[self.project1.id, self.project2.id],
-            conditions=[["type", "=", "transaction"]],
+            conditions=[["event.type", "=", "transaction"]],
         )
 
         event = self.eventstore.get_event_by_id(self.project2.id, "e" * 32)
