commit 97171652aefa613783ae7bd320ec75e542bc0dd5
Author: ted kaemming <ted@kaemming.com>
Date:   Wed Apr 11 12:38:44 2018 -0700

    feat: Add ability to delegate Service method access (#7945)

diff --git a/src/sentry/utils/concurrent.py b/src/sentry/utils/concurrent.py
new file mode 100644
index 0000000000..4566be994d
--- /dev/null
+++ b/src/sentry/utils/concurrent.py
@@ -0,0 +1,125 @@
+from __future__ import absolute_import
+
+import logging
+import threading
+from Queue import Full, Queue
+from concurrent.futures import Future
+
+
+logger = logging.getLogger(__name__)
+
+
+class ThreadedExecutor(object):
+    """\
+    This executor provides a method of executing callables in a threaded worker
+    pool. The number of outstanding requests can be limited by the ``maxsize``
+    parameter, which has the same behavior as the parameter of the same name
+    for the ``Queue`` constructor.
+
+    All threads are daemon threads and will remain alive until the main thread
+    exits. Any items remaining in the queue at this point may not be executed!
+
+    NOTE: This is *not* compatible with the ``concurrent.futures.Executor``
+    API! Rather than ``submit`` accepting the function arguments, the function
+    must already have the argument values bound (via ``functools.partial`` or
+    similar), and ``submit`` passes all additional arguments to ``queue.put``
+    to allow controlling whether or not queue insertion should be blocking.
+    """
+
+    def __init__(self, worker_count=1, maxsize=0):
+        self.__worker_count = worker_count
+        self.__workers = set([])
+        self.__started = False
+        self.__queue = Queue(maxsize)
+
+    def start(self):
+        assert not self.__started
+
+        def worker():
+            queue = self.__queue
+            while True:
+                function, future = queue.get(True)
+                if not future.set_running_or_notify_cancel():
+                    continue
+                try:
+                    result = function()
+                except Exception as error:
+                    future.set_exception(error)
+                else:
+                    future.set_result(result)
+                queue.task_done()
+
+        for i in xrange(self.__worker_count):
+            t = threading.Thread(target=worker)
+            t.daemon = True
+            t.start()
+            self.__workers.add(t)
+
+        self.__started = True
+
+    def submit(self, callable, *args, **kwargs):
+        """\
+        Enqueue a task to be executed, returning a ``Future``.
+
+        If the worker pool has not already been started, calling this method
+        will cause all of the worker threads to start running.
+        """
+        if not self.__started:
+            self.start()
+
+        future = Future()
+        try:
+            self.__queue.put((callable, future), *args, **kwargs)
+        except Full as error:
+            if future.set_running_or_notify_cancel():
+                future.set_exception(error)
+        return future
+
+
+class FutureSet(object):
+    """\
+    Coordinates a set of ``Future`` objects (either from
+    ``concurrent.futures``, or otherwise API compatible), and allows for
+    attaching a callback when all futures have completed execution.
+    """
+
+    def __init__(self, futures):
+        self.__pending = set(futures)
+        self.__completed = set()
+        self.__callbacks = []
+        self.__lock = threading.Lock()
+
+        for future in futures:
+            future.add_done_callback(self.__mark_completed)
+
+    def __iter__(self):
+        with self.__lock:
+            futures = self.__pending | self.__completed
+        return iter(futures)
+
+    def __execute_callback(self, callback):
+        try:
+            callback(self)
+        except Exception as error:
+            logger.warning(
+                'Error when calling callback %r: %s',
+                callback, error, exc_info=True)
+
+    def __mark_completed(self, future):
+        with self.__lock:
+            self.__pending.remove(future)
+            self.__completed.add(future)
+            remaining = len(self.__pending)
+
+        if remaining == 0:
+            for callback in self.__callbacks:
+                self.__execute_callback(callback)
+
+    def add_done_callback(self, callback):
+        with self.__lock:
+            remaining = len(self.__pending)
+            if remaining > 0:
+                self.__callbacks.append(callback)
+
+        if remaining == 0:
+            self.__execute_callback(callback)
diff --git a/src/sentry/utils/services.py b/src/sentry/utils/services.py
index 32d8658c88..d695b7c86f 100644
--- a/src/sentry/utils/services.py
+++ b/src/sentry/utils/services.py
@@ -1,15 +1,22 @@
 from __future__ import absolute_import
 
+import functools
 import inspect
 import itertools
+import logging
 
+from collections import OrderedDict
 from django.utils.functional import empty, LazyObject
 
 from sentry.utils import warnings
+from sentry.utils.concurrent import FutureSet, ThreadedExecutor
 
 from .imports import import_string
 
 
+logger = logging.getLogger(__name__)
+
+
 class Service(object):
     __all__ = ()
 
@@ -76,3 +83,184 @@ class LazyServiceWrapper(LazyObject):
                 context[key] = (lambda f: lambda *a, **k: getattr(self, f)(*a, **k))(key)
             else:
                 context[key] = getattr(base, key)
+
+
+class ServiceDelegator(Service):
+    """\
+    This is backend that coordinates and delegates method execution to multiple
+    backends. It can be used to route requests to different backends based on
+    method arguments, as well as execute the same request against multiple
+    backends in parallel for testing backend performance and data consistency.
+
+    The backends are provided as mapping of backend name to configuration
+    parameters:
+
+        'redis': {
+            'path': 'sentry.tsdb.redis.RedisTSDB',
+            'executor': {
+                'path': 'sentry.utils.services.ThreadedExecutor',
+                'options': {
+                    'worker_count': 1,
+                },
+            },
+        },
+        'dummy': {
+            'path': 'sentry.tsdb.dummy.DummyTSDB',
+            'executor': {
+                'path': 'sentry.utils.services.ThreadedExecutor',
+                'options': {
+                    'worker_count': 4,
+                },
+            },
+        },
+        # ... etc ...
+
+    The backends used for a method call are determined by a selector function
+    which is provided with the method name (as a string) and arguments (in the
+    form returned by ``inspect.getcallargs``) and expected to return a list of
+    strings which correspond to names in the backend mapping. (This list should
+    contain at least one member.) The first item in the result list is
+    considered the "primary backend". The remainder of the items in the result
+    list are considered "secondary backends". The result value of the primary
+    backend will be the result value of the delegated method (to callers, this
+    appears as a synchronous method call.) The secondary backends are called
+    asynchronously in the background.  (To receive the result values of these
+    method calls, provide a callback_func, described below.) If the primary
+    backend name returned by the selector function doesn't correspond to any
+    registered backend, the function will raise a ``InvalidBackend`` exception.
+    If any referenced secondary backends are not registered names, they will be
+    discarded and logged.
+
+    The members and ordering of the selector function result (and thus the
+    primary and secondary backends for a method call) may vary from call to
+    call based on the calling arguments or some other state. For example, some
+    calls may use a different primary backend based on some piece of global
+    state (e.g. some property of a web request), or a secondary backend
+    undergoing testing may be included based on the result of a random number
+    generator (essentially calling it in the background for a sample of calls.)
+
+    The selector function and callback function are both provided as paths to a
+    callable that will be imported at backend instantation.
+
+    Implementation notes:
+
+    - Only method access is delegated to the individual backends. Attribute
+      values are returned from the base backend. Only methods that are defined
+      on the base backend are eligble for delegation (since these methods are
+      considered the public API.)
+    - The backend makes no attempt to synchronize common backend option values
+      between backends (e.g. TSDB rollup configuration) to ensure equivalency
+      of request parameters based on configuration.
+    - Each backend is associated with an executor pool which defaults to a
+      thread pool implementation unless otherwise specified in the backend
+      configuration. If the backend itself is not thread safe (due to socket
+      access, etc.), it's recommended to specify a pool size of 1 to ensure
+      exclusive access to resources. Each executor is started when the first
+      task is submitted.
+    - The request is added to the request queue of the primary backend using a
+      blocking put. The request is added to the request queue(s) of the
+      secondary backend(s) as a non-blocking put (if these queues are full, the
+      request is rejected and the future will raise ``Queue.Full`` when
+      attempting to retrieve the result.)
+    - The ``callback_func`` is called after all futures have completed, either
+      successfully or unsuccessfully. The function is parameters are the method
+      name, calling arguments (as returned by ``inspect.getcallargs``) and a
+      mapping of backend name to ``Future`` objects.
+    """
+
+    class InvalidBackend(Exception):
+        """\
+        Exception raised when an invalid backend is returned by a selector
+        function.
+        """
+
+    def __init__(self, backend_base, backends, selector_func, callback_func=None):
+        self.__backend_base = import_string(backend_base)
+
+        def load_executor(options):
+            path = options.get('path')
+            if path is None:
+                executor_cls = ThreadedExecutor
+            else:
+                executor_cls = import_string(path)
+            return executor_cls(**options.get('options', {}))
+
+        self.__backends = {}
+        for name, options in backends.items():
+            self.__backends[name] = (
+                import_string(options['path'])(**options.get('options', {})),
+                load_executor(options.get('executor', {})),
+            )
+
+        self.__selector_func = import_string(selector_func)
+
+        if callback_func is not None:
+            self.__callback_func = import_string(callback_func)
+        else:
+            self.__callback_func = None
+
+    def validate(self):
+        for backend, executor in self.__backends.values():
+            backend.validate()
+
+    def setup(self):
+        for backend, executor in self.__backends.values():
+            backend.setup()
+
+    def __getattr__(self, attribute_name):
+        # When deciding how to handle attribute accesses, we have three
+        # different possible outcomes:
+        # 1. If this is defined as a method on the base implementation, we are
+        #    able delegate it to the backends based on the selector function.
+        # 2. If this is defined as an attribute on the base implementation, we
+        #    are able to (immediately) return that as the value. (This also
+        #    mirrors the behavior of ``LazyServiceWrapper``, which will cache
+        #    any attribute access during ``expose``, so we can't delegate
+        #    attribute access anyway.)
+        # 3. If this isn't defined at all on the base implementation, we let
+        #    the ``AttributeError`` raised by ``getattr`` propagate (mirroring
+        #    normal attribute access behavior for a missing/invalid name.)
+        base_value = getattr(self.__backend_base, attribute_name)
+        if not inspect.ismethod(base_value):
+            return base_value
+
+        def execute(*args, **kwargs):
+            # Binding the call arguments to named arguments has two benefits:
+            # 1. These values always be passed in the same form to the selector
+            #    function and callback, regardless of how they were passed to
+            #    the method itself (as positional arguments, keyword arguments,
+            #    etc.)
+            # 2. This ensures that the given arguments are those supported by
+            #    the base backend itself, which should be a common subset of
+            #    arguments that are supported by all backends.
+            callargs = inspect.getcallargs(base_value, None, *args, **kwargs)
+
+            results = OrderedDict()
+            for i, backend_name in enumerate(self.__selector_func(attribute_name, callargs)):
+                is_primary = i == 0
+                try:
+                    backend, executor = self.__backends[backend_name]
+                except KeyError:
+                    if is_primary:
+                        raise self.InvalidBackend(
+                            '{!r} is not a registered backend.'.format(backend_name))
+                    else:
+                        logger.warning(
+                            '%r is not a registered backend and will be ignored.',
+                            backend_name,
+                            exc_info=True)
+                        continue
+
+                results[backend_name] = executor.submit(
+                    functools.partial(getattr(backend, attribute_name), *args, **kwargs),
+                    block=is_primary,
+                )
+
+            if self.__callback_func is not None:
+                FutureSet(results.values()).add_done_callback(
+                    lambda *a, **k: self.__callback_func(attribute_name, callargs, results)
+                )
+
+            return results.values()[0].result()
+
+        return execute
diff --git a/tests/sentry/utils/test_concurrent.py b/tests/sentry/utils/test_concurrent.py
new file mode 100644
index 0000000000..5aeaf1d3a4
--- /dev/null
+++ b/tests/sentry/utils/test_concurrent.py
@@ -0,0 +1,69 @@
+from __future__ import absolute_import
+
+import mock
+from concurrent.futures import Future
+from sentry.utils.concurrent import FutureSet
+
+
+def test_future_set_callback_success():
+    future_set = FutureSet([Future() for i in xrange(3)])
+
+    callback = mock.Mock()
+    future_set.add_done_callback(callback)
+
+    for i, future in enumerate(list(future_set)):
+        assert callback.call_count == 0
+        future.set_result(True)
+
+    assert callback.call_count == 1
+    assert callback.call_args == mock.call(future_set)
+
+    other_callback = mock.Mock()
+    future_set.add_done_callback(other_callback)
+
+    assert other_callback.call_count == 1
+    assert other_callback.call_args == mock.call(future_set)
+
+
+def test_future_set_callback_error():
+    future_set = FutureSet([Future() for i in xrange(3)])
+
+    callback = mock.Mock()
+    future_set.add_done_callback(callback)
+
+    for i, future in enumerate(list(future_set)):
+        assert callback.call_count == 0
+        future.set_exception(Exception)
+
+    assert callback.call_count == 1
+    assert callback.call_args == mock.call(future_set)
+
+    other_callback = mock.Mock()
+    future_set.add_done_callback(other_callback)
+
+    assert other_callback.call_count == 1
+    assert other_callback.call_args == mock.call(future_set)
+
+
+def test_future_set_callback_empty():
+    future_set = FutureSet([])
+
+    callback = mock.Mock()
+    future_set.add_done_callback(callback)
+
+    assert callback.call_count == 1
+    assert callback.call_args == mock.call(future_set)
+
+
+def test_future_broken_callback():
+    future_set = FutureSet([])
+
+    callback = mock.Mock(side_effect=Exception('Boom!'))
+
+    try:
+        future_set.add_done_callback(callback)
+    except Exception:
+        assert False, 'should not raise'
+
+    assert callback.call_count == 1
+    assert callback.call_args == mock.call(future_set)
