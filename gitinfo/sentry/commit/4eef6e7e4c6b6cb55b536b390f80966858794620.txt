commit 4eef6e7e4c6b6cb55b536b390f80966858794620
Author: David Cramer <dcramer@gmail.com>
Date:   Tue Feb 17 20:19:26 2015 -0800

    Handle retry logic in RiakNodeStore
    
    We consistently see issues with multiget failing on BadStatusLine. This will ensure we're retrying using the same logic Riak is supposed to be using (though its tough to say if it is or not).

diff --git a/src/sentry/nodestore/riak/backend.py b/src/sentry/nodestore/riak/backend.py
index 1afef29dc8..73200a102d 100644
--- a/src/sentry/nodestore/riak/backend.py
+++ b/src/sentry/nodestore/riak/backend.py
@@ -10,9 +10,12 @@ from __future__ import absolute_import
 
 import riak
 import riak.resolver
-
 import six
 
+# XXX(dcramer): I realize this is a private function. We lock in versions, so
+# we're going to treat it as a public API as it's better than re-implementing
+# the function using the other public APIs.
+from riak.client.transport import _is_retryable
 from time import sleep
 
 from sentry.nodestore.base import NodeStorage
@@ -26,26 +29,38 @@ def retry(attempts, func, *args, **kwargs):
     for _ in range(attempts):
         try:
             return func(*args, **kwargs)
-        except Exception:
-            sleep(0.01)
+        except Exception as err:
+            if _is_retryable(err):
+                sleep(0.01)
+            raise
     raise
 
 
+# TODO(dcramer): ideally we would use Nydus here, but we need to confirm that
+# the Riak backend is in good shape. This would resolve some issues we see with
+# riak-python, and Nydus is a much more lean codebase that removes a lot of the
+# complexities (and thus, things features dont want or plan to use)
 class RiakNodeStorage(NodeStorage):
     """
     A Riak-based backend for storing node data.
 
     >>> RiakNodeStorage(nodes=[{'host':'127.0.0.1','http_port':8098}])
+
+    Due to issues with riak-python, we implement our own retry strategy for the
+    get_multi behavior.
     """
     def __init__(self, nodes, bucket='nodes',
                  resolver=riak.resolver.last_written_resolver,
-                 protocol='http'):
+                 protocol='http',
+                 max_retries=3):
         self._client_options = {
             'nodes': nodes,
             'resolver': resolver,
             'protocol': protocol,
+            'retries': 0,
         }
         self._bucket_name = bucket
+        self.max_retries = max_retries
 
     @memoize
     def conn(self):
@@ -58,35 +73,44 @@ class RiakNodeStorage(NodeStorage):
     def create(self, data):
         node_id = self.generate_id()
         obj = self.bucket.new(data=data, key=node_id)
-        retry(3, obj.store)
+        retry(self.max_retries, obj.store)
         return obj.key
 
     def delete(self, id):
         obj = self.bucket.new(key=id)
-        retry(3, obj.delete)
+        retry(self.max_retries, obj.delete)
 
     def get(self, id):
         # just fetch it from a random backend, we're not aiming for consistency
-        obj = self.bucket.get(key=id, r=1)
+        obj = retry(self.max_retries, self.bucket.get, key=id, r=1)
         if not obj:
             return None
         return obj.data
 
-    def get_multi(self, id_list, r=1):
-        result = self.bucket.multiget(id_list)
-
+    def get_multi(self, id_list):
+        attempt_num = 0
         results = {}
-        for obj in result:
-            # errors return a tuple of (bucket, key, err)
-            if isinstance(obj, tuple):
-                err = obj[3]
-                six.reraise(type(err), err)
-            results[obj.key] = obj.data
+        while id_list and attempt_num < self.max_retries:
+            attempt_num += 1
+            result = self.bucket.multiget(id_list, r=1)
+            id_list = []
+            for obj in result:
+                # errors return a tuple of (bucket, key, err)
+                if isinstance(obj, tuple):
+                    err = obj[3]
+                    if attempt_num == self.max_retries:
+                        six.reraise(type(err), err)
+                    elif _is_retryable(err):
+                        id_list.append(obj.key)
+                    else:
+                        six.reraise(type(err), err)
+                else:
+                    results[obj.key] = obj.data
         return results
 
     def set(self, id, data):
         obj = self.bucket.new(key=id, data=data)
-        retry(3, obj.store)
+        retry(self.max_retries, obj.store)
 
     def cleanup(self, cutoff_timestamp):
         # TODO(dcramer): we should either index timestamps or have this run
