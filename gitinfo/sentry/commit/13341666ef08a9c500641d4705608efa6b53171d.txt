commit 13341666ef08a9c500641d4705608efa6b53171d
Author: Anton Ovchinnikov <anton@tonyo.info>
Date:   Thu Sep 12 15:20:46 2019 +0200

    ref: Rename ingest-consumer argument and minor clean-up (#14648)
    
    * ref: Rename ingest-consumer argument and minor clean-up
    
    * fix: Allow "error" as initial_offset_reset

diff --git a/src/sentry/ingest_consumer.py b/src/sentry/ingest_consumer.py
index 7d64952e48..e14379c388 100644
--- a/src/sentry/ingest_consumer.py
+++ b/src/sentry/ingest_consumer.py
@@ -26,7 +26,7 @@ class ConsumerType(object):
     Transactions = "transactions"  # consumes transaction events ( from the Transactions topic)
 
     @staticmethod
-    def get_topic_name(consumer_type, settings):
+    def get_topic_name(consumer_type):
         if consumer_type == ConsumerType.Events:
             return settings.KAFKA_INGEST_EVENTS
         elif consumer_type == ConsumerType.Attachments:
@@ -36,13 +36,13 @@ class ConsumerType(object):
         raise ValueError("Invalid consumer type", consumer_type)
 
 
-def _create_consumer(consumer_group, consumer_type, settings):
+def _create_consumer(consumer_group, consumer_type, initial_offset_reset):
     """
     Creates a kafka consumer based on the
     :param consumer_group:
     :return:
     """
-    topic_name = ConsumerType.get_topic_name(consumer_type, settings)
+    topic_name = ConsumerType.get_topic_name(consumer_type)
     cluster_name = settings.KAFKA_TOPICS[topic_name]["cluster"]
     bootstrap_servers = settings.KAFKA_CLUSTERS[cluster_name]["bootstrap.servers"]
 
@@ -52,7 +52,7 @@ def _create_consumer(consumer_group, consumer_type, settings):
         "enable.auto.commit": "false",  # we commit manually
         "enable.auto.offset.store": "true",  # we let the broker keep count of the current offset (when committing)
         "enable.partition.eof": "false",  # stop EOF errors when we read all messages in the topic
-        "default.topic.config": {"auto.offset.reset": "earliest"},
+        "default.topic.config": {"auto.offset.reset": initial_offset_reset},
     }
 
     return kafka.Consumer(consumer_configuration)
@@ -76,7 +76,8 @@ def run_ingest_consumer(
     commit_batch_size,
     consumer_group,
     consumer_type,
-    max_batch_time_seconds,
+    max_fetch_time_seconds,
+    initial_offset_reset="latest",
     is_shutdown_requested=lambda: False,
 ):
     """
@@ -87,19 +88,20 @@ def run_ingest_consumer(
     :param commit_batch_size: the number of message the consumer will try to process/commit in one loop
     :param consumer_group: kafka consumer group name
     :param consumer_type: an enumeration defining the types of ingest messages see `ConsumerType`
-    :param max_batch_time_seconds: the maximum number of seconds a consume operation will be blocked waiting
+    :param max_fetch_time_seconds: the maximum number of seconds a consume operation will be blocked waiting
         for the specified commit_batch_size number of messages to appear in the queue before it returns. At the
         end of the specified time the consume operation will return however many messages it has ( including
         an empty array if no new messages are available).
+    :param initial_offset_reset: offset reset policy when there's no available offset for the consumer
     :param is_shutdown_requested: Callable[[],bool] predicate checked after each loop, if it returns
         True the forwarder stops (by default is lambda: False). In normal operation this should be left to default.
         For unit testing it offers a way to cleanly stop the forwarder after some particular condition is achieved.
     """
 
     logger.debug("Starting ingest-consumer...")
-    consumer = _create_consumer(consumer_group, consumer_type, settings)
+    consumer = _create_consumer(consumer_group, consumer_type, initial_offset_reset)
 
-    consumer.subscribe([ConsumerType.get_topic_name(consumer_type, settings)])
+    consumer.subscribe([ConsumerType.get_topic_name(consumer_type)])
     # setup a flag to mark termination signals received, see below why we use an array
     termination_signal_received = [False]
 
@@ -119,7 +121,7 @@ def run_ingest_consumer(
         while not (is_shutdown_requested() or termination_signal_received[0]):
             # get up to commit_batch_size messages
             messages = consumer.consume(
-                num_messages=commit_batch_size, timeout=max_batch_time_seconds
+                num_messages=commit_batch_size, timeout=max_fetch_time_seconds
             )
 
             for message in messages:
diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index 919297fe76..25474e0670 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -374,12 +374,18 @@ def query_subscription_consumer(**options):
     help="How many messages to process before committing offsets.",
 )
 @click.option(
-    "--max-batch-time-ms",
+    "--max-fetch-time-ms",
     default=100,
     type=int,
-    help="Timeout (in milliseconds) for a consume batch operation. Max time the kafka consumer will wait"
+    help="Timeout (in milliseconds) for a consume operation. Max time the kafka consumer will wait "
     "before returning the available messages in the topic.",
 )
+@click.option(
+    "--initial-offset-reset",
+    default="latest",
+    type=click.Choice(["earliest", "latest", "error"]),
+    help="Position in the commit log topic to begin reading from when no prior offset has been recorded.",
+)
 @configuration
 def ingest_consumer(**options):
     """
@@ -398,11 +404,12 @@ def ingest_consumer(**options):
     elif consumer_type == "attachments":
         consumer_type = ConsumerType.Attachments
 
-    max_batch_time_seconds = options["max-batch-time-ms"] / 1000.0
+    max_fetch_time_seconds = options["max-fetch-time-ms"] / 1000.0
 
     run_ingest_consumer(
         commit_batch_size=options["commit_batch_size"],
         consumer_group=options["group"],
         consumer_type=consumer_type,
-        max_batch_time_seconds=max_batch_time_seconds,
+        max_fetch_time_seconds=max_fetch_time_seconds,
+        initial_offset_reset=options["initial_offset_reset"],
     )
diff --git a/tests/relay/test_ingest_consumer.py b/tests/relay/test_ingest_consumer.py
index 0a2ba2e27a..1efebc52cf 100644
--- a/tests/relay/test_ingest_consumer.py
+++ b/tests/relay/test_ingest_consumer.py
@@ -83,10 +83,10 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
     organization = Factories.create_organization()
     project = Factories.create_project(organization=organization)
 
-    topic_event_name = ConsumerType.get_topic_name(ConsumerType.Events, settings)
+    topic_event_name = ConsumerType.get_topic_name(ConsumerType.Events)
 
     event_ids = set()
-    for i in range(1, 4):
+    for _ in range(3):
         message, event_id = _get_test_message(project)
         event_ids.add(event_id)
         producer.produce(topic_event_name, message)
@@ -96,7 +96,8 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
             commit_batch_size=2,
             consumer_group=consumer_group,
             consumer_type=ConsumerType.Events,
-            max_batch_time_seconds=0.1,
+            max_fetch_time_seconds=0.1,
+            initial_offset_reset="earliest",
             is_shutdown_requested=_shutdown_requested(max_secs=10, num_events=3),
         )
 
