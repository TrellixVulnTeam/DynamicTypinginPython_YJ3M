commit 29ea9bbddc5b7e0a5a061ab2df0a3a254237b9fc
Author: Leander Rodrigues <leandergrodrigues@gmail.com>
Date:   Wed Mar 25 16:12:18 2020 -0400

    ref(async-csv): Reorganize into data-export app, and add processors (#17662)
    
    Create data-export app and issues-by-tag processor

diff --git a/src/sentry/api/serializers/models/exporteddata.py b/src/sentry/api/serializers/models/exporteddata.py
index 3344c5daf1..7a60bbe8f7 100644
--- a/src/sentry/api/serializers/models/exporteddata.py
+++ b/src/sentry/api/serializers/models/exporteddata.py
@@ -1,8 +1,9 @@
 from __future__ import absolute_import
 
 from sentry.api.serializers import Serializer, serialize, register
-from sentry.constants import ExportQueryType
-from sentry.models import ExportedData, User
+from sentry.models import User
+from sentry.data_export.base import ExportQueryType
+from sentry.data_export.models import ExportedData
 
 
 @register(ExportedData)
diff --git a/src/sentry/api/urls.py b/src/sentry/api/urls.py
index 2822047160..66dd49b8ab 100644
--- a/src/sentry/api/urls.py
+++ b/src/sentry/api/urls.py
@@ -18,8 +18,6 @@ from .endpoints.broadcast_index import BroadcastIndexEndpoint
 from .endpoints.builtin_symbol_sources import BuiltinSymbolSourcesEndpoint
 from .endpoints.catchall import CatchallEndpoint
 from .endpoints.chunk import ChunkUploadEndpoint
-from .endpoints.data_export import DataExportEndpoint
-from .endpoints.data_export_details import DataExportDetailsEndpoint
 from .endpoints.debug_files import (
     AssociateDSymFilesEndpoint,
     DebugFilesEndpoint,
@@ -284,6 +282,8 @@ from .endpoints.user_social_identity_details import UserSocialIdentityDetailsEnd
 from .endpoints.user_subscriptions import UserSubscriptionsEndpoint
 from .endpoints.useravatar import UserAvatarEndpoint
 
+from sentry.data_export.endpoints.data_export import DataExportEndpoint
+from sentry.data_export.endpoints.data_export_details import DataExportDetailsEndpoint
 from sentry.discover.endpoints.discover_query import DiscoverQueryEndpoint
 from sentry.discover.endpoints.discover_saved_queries import DiscoverSavedQueriesEndpoint
 from sentry.discover.endpoints.discover_saved_query_detail import DiscoverSavedQueryDetailEndpoint
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 88f61824e3..b8dc1c73fe 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -504,6 +504,7 @@ CELERY_CREATE_MISSING_QUEUES = True
 CELERY_REDIRECT_STDOUTS = False
 CELERYD_HIJACK_ROOT_LOGGER = False
 CELERY_IMPORTS = (
+    "sentry.data_export.tasks",
     "sentry.discover.tasks",
     "sentry.incidents.tasks",
     "sentry.tasks.assemble",
@@ -515,7 +516,6 @@ CELERY_IMPORTS = (
     "sentry.tasks.clear_expired_snoozes",
     "sentry.tasks.collect_project_platforms",
     "sentry.tasks.commits",
-    "sentry.tasks.data_export",
     "sentry.tasks.deletion",
     "sentry.tasks.digests",
     "sentry.tasks.email",
@@ -1788,6 +1788,7 @@ SENTRY_REQUEST_METRIC_ALLOWED_PATHS = (
     "sentry.web.api",
     "sentry.web.frontend",
     "sentry.api.endpoints",
+    "sentry.data_export.endpoints",
     "sentry.discover.endpoints",
     "sentry.incidents.endpoints",
 )
diff --git a/src/sentry/data_export/__init__.py b/src/sentry/data_export/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/src/sentry/data_export/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/src/sentry/data_export/base.py b/src/sentry/data_export/base.py
new file mode 100644
index 0000000000..98b2e2e66d
--- /dev/null
+++ b/src/sentry/data_export/base.py
@@ -0,0 +1,50 @@
+from __future__ import absolute_import
+
+import six
+from datetime import timedelta
+from enum import Enum
+
+SNUBA_MAX_RESULTS = 1000
+DEFAULT_EXPIRATION = timedelta(weeks=4)
+
+
+class ExportError(Exception):
+    pass
+
+
+class ExportStatus(six.text_type, Enum):
+    Early = "EARLY"  # The download is being prepared
+    Valid = "VALID"  # The download is ready for the user
+    Expired = "EXPIRED"  # The download has been deleted
+
+
+class ExportQueryType(object):
+    ISSUES_BY_TAG = 0
+    DISCOVER = 1
+    ISSUES_BY_TAG_STR = "Issues-by-Tag"
+    DISCOVER_STR = "Discover"
+
+    @classmethod
+    def as_choices(cls):
+        return ((cls.ISSUES_BY_TAG, cls.ISSUES_BY_TAG_STR), (cls.DISCOVER, cls.DISCOVER_STR))
+
+    @classmethod
+    def as_str_choices(cls):
+        return (
+            (cls.ISSUES_BY_TAG_STR, cls.ISSUES_BY_TAG_STR),
+            (cls.DISCOVER_STR, cls.DISCOVER_STR),
+        )
+
+    @classmethod
+    def as_str(cls, integer):
+        if integer == cls.ISSUES_BY_TAG:
+            return cls.ISSUES_BY_TAG_STR
+        elif integer == cls.DISCOVER:
+            return cls.DISCOVER_STR
+
+    @classmethod
+    def from_str(cls, string):
+        if string == cls.ISSUES_BY_TAG_STR:
+            return cls.ISSUES_BY_TAG
+        elif string == cls.DISCOVER_STR:
+            return cls.DISCOVER
diff --git a/src/sentry/data_export/endpoints/__init__.py b/src/sentry/data_export/endpoints/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/src/sentry/data_export/endpoints/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/src/sentry/api/endpoints/data_export.py b/src/sentry/data_export/endpoints/data_export.py
similarity index 77%
rename from src/sentry/api/endpoints/data_export.py
rename to src/sentry/data_export/endpoints/data_export.py
index fc2484884d..0c8fca91c0 100644
--- a/src/sentry/api/endpoints/data_export.py
+++ b/src/sentry/data_export/endpoints/data_export.py
@@ -6,21 +6,23 @@ from rest_framework import serializers
 from rest_framework.response import Response
 
 from sentry import features
+from sentry.api.base import EnvironmentMixin
 from sentry.api.bases.organization import OrganizationEndpoint, OrganizationDataExportPermission
 from sentry.api.serializers import serialize
-from sentry.constants import ExportQueryType
-from sentry.models import ExportedData
-from sentry.tasks.data_export import assemble_download
+from sentry.models import Environment
 from sentry.utils import metrics
 
+from ..base import ExportQueryType
+from ..models import ExportedData
+from ..tasks import assemble_download
+
 
 class ExportedDataSerializer(serializers.Serializer):
     query_type = serializers.ChoiceField(choices=ExportQueryType.as_str_choices(), required=True)
-    # TODO(Leander): Implement query_info validation with jsonschema
     query_info = serializers.JSONField(required=True)
 
 
-class DataExportEndpoint(OrganizationEndpoint):
+class DataExportEndpoint(OrganizationEndpoint, EnvironmentMixin):
     permission_classes = (OrganizationDataExportPermission,)
 
     def post(self, request, organization):
@@ -32,9 +34,14 @@ class DataExportEndpoint(OrganizationEndpoint):
         if not features.has("organizations:data-export", organization):
             return Response(status=404)
 
+        limit = request.data.get("limit")
         serializer = ExportedDataSerializer(
             data=request.data, context={"organization": organization, "user": request.user}
         )
+        try:
+            environment_id = self._get_environment_id_from_request(request, organization.id)
+        except Environment.DoesNotExist as error:
+            return Response(error, status=400)
 
         if not serializer.is_valid():
             return Response(serializer.errors, status=400)
@@ -55,7 +62,9 @@ class DataExportEndpoint(OrganizationEndpoint):
             status = 200
             if created:
                 metrics.incr("dataexport.start", tags={"query_type": data["query_type"]})
-                assemble_download.delay(data_export_id=data_export.id)
+                assemble_download.delay(
+                    data_export_id=data_export.id, limit=limit, environment_id=environment_id
+                )
                 status = 201
         except ValidationError as e:
             # This will handle invalid JSON requests
diff --git a/src/sentry/api/endpoints/data_export_details.py b/src/sentry/data_export/endpoints/data_export_details.py
similarity index 97%
rename from src/sentry/api/endpoints/data_export_details.py
rename to src/sentry/data_export/endpoints/data_export_details.py
index e2477460ca..5c81e1ceb3 100644
--- a/src/sentry/api/endpoints/data_export_details.py
+++ b/src/sentry/data_export/endpoints/data_export_details.py
@@ -6,9 +6,10 @@ from django.http import StreamingHttpResponse
 from sentry import features
 from sentry.api.bases.organization import OrganizationEndpoint, OrganizationDataExportPermission
 from sentry.api.serializers import serialize
-from sentry.models import ExportedData
 from sentry.utils import metrics
 
+from ..models import ExportedData
+
 
 class DataExportDetailsEndpoint(OrganizationEndpoint):
     permission_classes = (OrganizationDataExportPermission,)
diff --git a/src/sentry/models/exporteddata.py b/src/sentry/data_export/models.py
similarity index 88%
rename from src/sentry/models/exporteddata.py
rename to src/sentry/data_export/models.py
index 0b6e65ce95..a716e9cd3d 100644
--- a/src/sentry/models/exporteddata.py
+++ b/src/sentry/data_export/models.py
@@ -2,15 +2,11 @@ from __future__ import absolute_import
 
 import json
 import logging
-import six
-from enum import Enum
-from datetime import timedelta
 from django.conf import settings
 from django.core.urlresolvers import reverse
 from django.db import models
 from django.utils import timezone
 
-from sentry.constants import ExportQueryType
 from sentry.db.models import (
     BoundedPositiveIntegerField,
     FlexibleForeignKey,
@@ -21,23 +17,14 @@ from sentry.db.models import (
 from sentry.utils import metrics
 from sentry.utils.http import absolute_uri
 
-logger = logging.getLogger(__name__)
-
-
-# Arbitrary, subject to change
-DEFAULT_EXPIRATION = timedelta(weeks=4)
-
+from .base import ExportQueryType, ExportStatus, DEFAULT_EXPIRATION
 
-class ExportStatus(six.text_type, Enum):
-    Early = "EARLY"  # The download is being prepared
-    Valid = "VALID"  # The download is ready for the user
-    Expired = "EXPIRED"  # The download has been deleted
+logger = logging.getLogger(__name__)
 
 
 class ExportedData(Model):
     """
-    Stores references to asynchronous data export jobs being stored
-    in the Google Cloud Platform temporary storage solution.
+    Stores references to asynchronous data export jobs
     """
 
     __core__ = False
@@ -68,6 +55,13 @@ class ExportedData(Model):
         payload["export_type"] = ExportQueryType.as_str(self.query_type)
         return payload
 
+    @property
+    def file_name(self):
+        date = self.date_added.strftime("%Y-%B-%d")
+        export_type = ExportQueryType.as_str(self.query_type)
+        # Example: Discover_2020-July-21_27.csv
+        return "{}_{}_{}.csv".format(export_type, date, self.id)
+
     @staticmethod
     def format_date(date):
         # Example: 12:21 PM on July 21, 2020 (UTC)
diff --git a/src/sentry/data_export/processors/__init__.py b/src/sentry/data_export/processors/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/src/sentry/data_export/processors/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/src/sentry/data_export/processors/issues_by_tag.py b/src/sentry/data_export/processors/issues_by_tag.py
new file mode 100644
index 0000000000..d5edd39dce
--- /dev/null
+++ b/src/sentry/data_export/processors/issues_by_tag.py
@@ -0,0 +1,115 @@
+from __future__ import absolute_import
+
+import six
+
+from sentry import tagstore
+from sentry.models import EventUser, Group, get_group_with_redirect, Project
+
+from ..base import ExportError
+
+
+class IssuesByTagProcessor(object):
+    """
+    Export processor for data exports of issues data based on a provided tag
+    """
+
+    def __init__(self, project_id, group_id, key, environment_id):
+        self.project = self.get_project(project_id)
+        self.group = self.get_group(group_id, self.project)
+        self.key = key
+        self.environment_id = environment_id
+        self.header_fields = self.get_header_fields(self.key)
+        self.lookup_key = self.get_lookup_key(self.key)
+        # Ensure the tag key exists, as it may have been deleted
+        try:
+            tagstore.get_tag_key(self.project.id, environment_id, self.lookup_key)
+        except tagstore.TagKeyNotFound:
+            raise ExportError("Requested key does not exist")
+        self.callbacks = self.get_callbacks(self.key, self.group.project_id)
+
+    @staticmethod
+    def get_project(project_id):
+        try:
+            project = Project.objects.get_from_cache(id=project_id)
+            return project
+        except Project.DoesNotExist:
+            raise ExportError("Requested project does not exist")
+
+    @staticmethod
+    def get_group(group_id, project):
+        try:
+            group, _ = get_group_with_redirect(
+                group_id, queryset=Group.objects.filter(project=project)
+            )
+            return group
+        except Group.DoesNotExist:
+            raise ExportError("Requested issue does not exist")
+
+    @staticmethod
+    def get_header_fields(key):
+        if key == "user":
+            return [
+                "value",
+                "id",
+                "email",
+                "username",
+                "ip_address",
+                "times_seen",
+                "last_seen",
+                "first_seen",
+            ]
+        else:
+            return ["value", "times_seen", "last_seen", "first_seen"]
+
+    @staticmethod
+    def get_lookup_key(key):
+        return six.text_type("sentry:{}".format(key)) if tagstore.is_reserved_key(key) else key
+
+    @staticmethod
+    def get_eventuser_callback(project_id):
+        def attach_eventuser(items):
+            users = EventUser.for_tags(project_id, [i.value for i in items])
+            for item in items:
+                item._eventuser = users.get(item.value)
+
+        return attach_eventuser
+
+    @staticmethod
+    def get_callbacks(key, project_id):
+        return [IssuesByTagProcessor.get_eventuser_callback(project_id)] if key == "user" else []
+
+    @staticmethod
+    def serialize_row(item, key):
+        result = {
+            "value": item.value,
+            "times_seen": item.times_seen,
+            "last_seen": item.last_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
+            "first_seen": item.first_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
+        }
+        if key == "user":
+            euser = item._eventuser
+            result["id"] = euser.ident if euser else ""
+            result["email"] = euser.email if euser else ""
+            result["username"] = euser.username if euser else ""
+            result["ip_address"] = euser.ip_address if euser else ""
+        return result
+
+    def get_raw_data(self, offset=0):
+        """
+        Returns list of GroupTagValues
+        """
+        return tagstore.get_group_tag_value_iter(
+            project_id=self.group.project_id,
+            group_id=self.group.id,
+            environment_id=self.environment_id,
+            key=self.lookup_key,
+            callbacks=self.callbacks,
+            offset=offset,
+        )
+
+    def get_serialized_data(self, offset=0):
+        """
+        Returns list of serialized GroupTagValue dictionaries
+        """
+        raw_data = self.get_raw_data(offset)
+        return [self.serialize_row(item, self.key) for item in raw_data]
diff --git a/src/sentry/data_export/tasks.py b/src/sentry/data_export/tasks.py
new file mode 100644
index 0000000000..9325299717
--- /dev/null
+++ b/src/sentry/data_export/tasks.py
@@ -0,0 +1,121 @@
+from __future__ import absolute_import
+
+import csv
+import logging
+import six
+import tempfile
+from django.db import transaction, IntegrityError
+
+from sentry.models import File
+from sentry.tasks.base import instrumented_task
+from sentry.utils import metrics
+from sentry.utils.sdk import capture_exception
+
+from .base import ExportError, ExportQueryType, SNUBA_MAX_RESULTS
+from .models import ExportedData
+from .utils import snuba_error_handler
+from .processors.issues_by_tag import IssuesByTagProcessor
+
+
+logger = logging.getLogger(__name__)
+
+
+@instrumented_task(name="sentry.data_export.tasks.assemble_download", queue="data_export")
+def assemble_download(data_export_id, limit=None, environment_id=None):
+    # Get the ExportedData object
+    try:
+        logger.info("dataexport.start", extra={"data_export_id": data_export_id})
+        data_export = ExportedData.objects.get(id=data_export_id)
+    except ExportedData.DoesNotExist as error:
+        capture_exception(error)
+        return
+
+    # Create a temporary file
+    try:
+        with tempfile.TemporaryFile() as tf:
+            # Process the query based on its type
+            if data_export.query_type == ExportQueryType.ISSUES_BY_TAG:
+                process_issues_by_tag(
+                    data_export=data_export, file=tf, limit=limit, environment_id=environment_id
+                )
+            elif data_export.query_type == ExportQueryType.DISCOVER:
+                process_discover(
+                    data_export=data_export, file=tf, limit=limit, environment_id=environment_id
+                )
+            # Create a new File object and attach it to the ExportedData
+            tf.seek(0)
+            try:
+                with transaction.atomic():
+                    file = File.objects.create(
+                        name=data_export.file_name,
+                        type="export.csv",
+                        headers={"Content-Type": "text/csv"},
+                    )
+                    file.putfile(tf, logger=logger)
+                    data_export.finalize_upload(file=file)
+                    logger.info("dataexport.end", extra={"data_export_id": data_export_id})
+            except IntegrityError as error:
+                metrics.incr("dataexport.error", instance=six.text_type(error))
+                logger.error(
+                    "dataexport.error: {}".format(six.text_type(error)),
+                    extra={"query": data_export.payload, "org": data_export.organization_id},
+                )
+                raise ExportError("Failed to save the assembled file")
+    except ExportError as error:
+        return data_export.email_failure(message=six.text_type(error))
+    except NotImplementedError as error:
+        return data_export.email_failure(message=six.text_type(error))
+    except BaseException as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error(
+            "dataexport.error: {}".format(six.text_type(error)),
+            extra={"query": data_export.payload, "org": data_export.organization_id},
+        )
+        return data_export.email_failure(message="Internal processing failure")
+
+
+def process_issues_by_tag(data_export, file, limit, environment_id):
+    """
+    Convert the tag query to a CSV, writing it to the provided file.
+    """
+    payload = data_export.query_info
+    try:
+        processor = IssuesByTagProcessor(
+            project_id=payload["project_id"],
+            group_id=payload["group_id"],
+            key=payload["key"],
+            environment_id=environment_id,
+        )
+    except ExportError as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
+        raise error
+
+    # Iterate through all the GroupTagValues
+    writer = create_writer(file, processor.header_fields)
+    iteration = 0
+    with snuba_error_handler(logger=logger):
+        while True:
+            offset = SNUBA_MAX_RESULTS * iteration
+            next_offset = SNUBA_MAX_RESULTS * (iteration + 1)
+            gtv_list = processor.get_serialized_data(offset=offset)
+            if len(gtv_list) == 0:
+                break
+            if limit and limit < next_offset:
+                # Since the next offset will pass the limit, write the remainder and quit
+                writer.writerows(gtv_list[: limit % SNUBA_MAX_RESULTS])
+                break
+            else:
+                writer.writerows(gtv_list)
+                iteration += 1
+
+
+def process_discover(data_export, file):
+    # TODO(Leander): Implement processing for Discover
+    raise NotImplementedError("Discover processing has not been implemented yet")
+
+
+def create_writer(file, fields):
+    writer = csv.DictWriter(file, fields)
+    writer.writeheader()
+    return writer
diff --git a/src/sentry/data_export/utils.py b/src/sentry/data_export/utils.py
new file mode 100644
index 0000000000..ddd00d9475
--- /dev/null
+++ b/src/sentry/data_export/utils.py
@@ -0,0 +1,42 @@
+from __future__ import absolute_import
+
+import six
+from contextlib import contextmanager
+
+from sentry.utils import metrics, snuba
+
+from .base import ExportError
+
+
+# Adapted into contextmanager from 'src/sentry/api/endpoints/organization_events.py'
+@contextmanager
+def snuba_error_handler(logger):
+    try:
+        yield
+    except snuba.QueryOutsideRetentionError as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
+        raise ExportError("Invalid date range. Please try a more recent date range.")
+    except snuba.QueryIllegalTypeOfArgument as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
+        raise ExportError("Invalid query. Argument to function is wrong type.")
+    except snuba.SnubaError as error:
+        metrics.incr("dataexport.error", instance=six.text_type(error))
+        logger.error("dataexport.error: {}".format(six.text_type(error)))
+        message = "Internal error. Please try again."
+        if isinstance(
+            error,
+            (
+                snuba.RateLimitExceeded,
+                snuba.QueryMemoryLimitExceeded,
+                snuba.QueryTooManySimultaneous,
+            ),
+        ):
+            message = "Query timeout. Please try again. If the problem persists try a smaller date range or fewer projects."
+        elif isinstance(
+            error,
+            (snuba.UnqualifiedQueryError, snuba.QueryExecutionError, snuba.SchemaValidationError),
+        ):
+            message = "Internal error. Your query failed to run."
+        raise ExportError(message)
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index 6d90ec2022..421ab64777 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -154,6 +154,7 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
     from sentry.app import nodestore
     from sentry.db.deletion import BulkDeleteQuery
     from sentry import models
+    from sentry.data_export.models import ExportedData
 
     if timed:
         import time
@@ -229,11 +230,11 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
     if not silent:
         click.echo("Removing expired files associated with ExportedData")
 
-    if is_filtered(models.ExportedData):
+    if is_filtered(ExportedData):
         if not silent:
             click.echo(">> Skipping ExportedData files")
     else:
-        queryset = models.ExportedData.objects.filter(date_expired__lt=(timezone.now()))
+        queryset = ExportedData.objects.filter(date_expired__lt=(timezone.now()))
         for item in queryset:
             item.delete_file()
 
diff --git a/src/sentry/tasks/data_export.py b/src/sentry/tasks/data_export.py
deleted file mode 100644
index 75fa5f3fac..0000000000
--- a/src/sentry/tasks/data_export.py
+++ /dev/null
@@ -1,226 +0,0 @@
-from __future__ import absolute_import
-
-import csv
-import logging
-import six
-import tempfile
-from contextlib import contextmanager
-from django.db import transaction, IntegrityError
-
-from sentry import tagstore
-from sentry.constants import ExportQueryType
-from sentry.models import EventUser, ExportedData, File, Group, Project, get_group_with_redirect
-from sentry.tasks.base import instrumented_task
-from sentry.utils import metrics, snuba
-from sentry.utils.sdk import capture_exception
-
-SNUBA_MAX_RESULTS = 1000
-
-logger = logging.getLogger(__name__)
-
-
-class DataExportError(Exception):
-    pass
-
-
-@instrumented_task(name="sentry.tasks.data_export.assemble_download", queue="data_export")
-def assemble_download(data_export_id):
-    # Extract the ExportedData object
-    try:
-        logger.info("dataexport.start", extra={"data_export_id": data_export_id})
-        data_export = ExportedData.objects.get(id=data_export_id)
-    except ExportedData.DoesNotExist as error:
-        capture_exception(error)
-        return
-
-    # Create a temporary file
-    try:
-        with tempfile.TemporaryFile() as tf:
-            # Process the query based on its type
-            if data_export.query_type == ExportQueryType.ISSUES_BY_TAG:
-                file_name = process_issue_by_tag(data_export, tf)
-            elif data_export.query_type == ExportQueryType.DISCOVER:
-                file_name = process_discover(data_export, tf)
-            # Create a new File object and attach it to the ExportedData
-            tf.seek(0)
-            try:
-                with transaction.atomic():
-                    file = File.objects.create(
-                        name=file_name, type="export.csv", headers={"Content-Type": "text/csv"}
-                    )
-                    file.putfile(tf, logger=logger)
-                    data_export.finalize_upload(file=file)
-                    logger.info("dataexport.end", extra={"data_export_id": data_export_id})
-            except IntegrityError as error:
-                metrics.incr("dataexport.error", instance=six.text_type(error))
-                logger.error(
-                    "dataexport.error: {}".format(six.text_type(error)),
-                    extra={"query": data_export.payload, "org": data_export.organization_id},
-                )
-                raise DataExportError("Failed to save the assembled file")
-    except DataExportError as error:
-        return data_export.email_failure(message=error)
-    except NotImplementedError as error:
-        return data_export.email_failure(message=error)
-    except BaseException as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error(
-            "dataexport.error: {}".format(six.text_type(error)),
-            extra={"query": data_export.payload, "org": data_export.organization_id},
-        )
-        return data_export.email_failure(message="Internal processing failure")
-
-
-def process_issue_by_tag(data_export, file, limit=None):
-    """
-    Convert the tag query to a CSV, writing it to the provided file.
-    Returns the suggested file name.
-    (Adapted from 'src/sentry/web/frontend/group_tag_export.py')
-    """
-    # Get the pertaining project
-    try:
-        payload = data_export.query_info
-        project = Project.objects.get(id=payload["project_id"])
-    except Project.DoesNotExist as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error("dataexport.error: {}".format(six.text_type(error)))
-        raise DataExportError("Requested project does not exist")
-
-    # Get the pertaining issue
-    try:
-        group, _ = get_group_with_redirect(
-            payload["group_id"], queryset=Group.objects.filter(project=project)
-        )
-    except Group.DoesNotExist as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error("dataexport.error: {}".format(six.text_type(error)))
-        raise DataExportError("Requested issue does not exist")
-
-    # Get the pertaining key
-    key = payload["key"]
-    lookup_key = six.text_type("sentry:{}").format(key) if tagstore.is_reserved_key(key) else key
-
-    # If the key is the 'user' tag, attach the event user
-    def attach_eventuser(items):
-        users = EventUser.for_tags(group.project_id, [i.value for i in items])
-        for item in items:
-            item._eventuser = users.get(item.value)
-
-    # Create the fields/callback lists
-    if key == "user":
-        callbacks = [attach_eventuser]
-        fields = [
-            "value",
-            "id",
-            "email",
-            "username",
-            "ip_address",
-            "times_seen",
-            "last_seen",
-            "first_seen",
-        ]
-    else:
-        callbacks = []
-        fields = ["value", "times_seen", "last_seen", "first_seen"]
-
-    # Example file name: Issues-by-Tag-project10-user__721.csv
-    file_details = six.text_type("{}-{}__{}").format(project.slug, key, data_export.id)
-    file_name = get_file_name(ExportQueryType.ISSUES_BY_TAG_STR, file_details)
-
-    # Iterate through all the GroupTagValues
-    writer = create_writer(file, fields)
-    iteration = 0
-    with snuba_error_handler():
-        while True:
-            offset = SNUBA_MAX_RESULTS * iteration
-            next_offset = SNUBA_MAX_RESULTS * (iteration + 1)
-            gtv_list = tagstore.get_group_tag_value_iter(
-                project_id=group.project_id,
-                group_id=group.id,
-                environment_id=None,
-                key=lookup_key,
-                callbacks=callbacks,
-                offset=offset,
-            )
-            if len(gtv_list) == 0:
-                break
-            gtv_list_raw = [serialize_issue_by_tag(key, item) for item in gtv_list]
-            if limit and limit < next_offset:
-                # Since the next offset will pass the limit, write the remainder and quit
-                writer.writerows(gtv_list_raw[: limit % SNUBA_MAX_RESULTS])
-                break
-            else:
-                writer.writerows(gtv_list_raw)
-                iteration += 1
-    return file_name
-
-
-def process_discover(data_export, file):
-    # TODO(Leander): Implement processing for Discover
-    raise NotImplementedError("Discover processing has not been implemented yet")
-
-
-def create_writer(file, fields):
-    writer = csv.DictWriter(file, fields)
-    writer.writeheader()
-    return writer
-
-
-def get_file_name(export_type, custom_string, extension="csv"):
-    file_name = six.text_type("{}-{}.{}").format(export_type, custom_string, extension)
-    return file_name
-
-
-# Adapted into contextmanager from 'src/sentry/api/endpoints/organization_events.py'
-@contextmanager
-def snuba_error_handler():
-    try:
-        yield
-    except snuba.QueryOutsideRetentionError as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error("dataexport.error: {}".format(six.text_type(error)))
-        raise DataExportError("Invalid date range. Please try a more recent date range.")
-    except snuba.QueryIllegalTypeOfArgument as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error("dataexport.error: {}".format(six.text_type(error)))
-        raise DataExportError("Invalid query. Argument to function is wrong type.")
-    except snuba.SnubaError as error:
-        metrics.incr("dataexport.error", instance=six.text_type(error))
-        logger.error("dataexport.error: {}".format(six.text_type(error)))
-        message = "Internal error. Please try again."
-        if isinstance(
-            error,
-            (
-                snuba.RateLimitExceeded,
-                snuba.QueryMemoryLimitExceeded,
-                snuba.QueryTooManySimultaneous,
-            ),
-        ):
-            message = "Query timeout. Please try again. If the problem persists try a smaller date range or fewer projects."
-        elif isinstance(
-            error,
-            (snuba.UnqualifiedQueryError, snuba.QueryExecutionError, snuba.SchemaValidationError),
-        ):
-            message = "Internal error. Your query failed to run."
-        raise DataExportError(message)
-
-
-################################
-#  Process-specific functions  #
-################################
-
-
-def serialize_issue_by_tag(key, item):
-    result = {
-        "value": item.value,
-        "times_seen": item.times_seen,
-        "last_seen": item.last_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-        "first_seen": item.first_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-    }
-    if key == "user":
-        euser = item._eventuser
-        result["id"] = euser.ident if euser else ""
-        result["email"] = euser.email if euser else ""
-        result["username"] = euser.username if euser else ""
-        result["ip_address"] = euser.ip_address if euser else ""
-    return result
diff --git a/src/sentry/web/frontend/group_tag_export.py b/src/sentry/web/frontend/group_tag_export.py
index c50d213286..67a9a6aff0 100644
--- a/src/sentry/web/frontend/group_tag_export.py
+++ b/src/sentry/web/frontend/group_tag_export.py
@@ -2,107 +2,41 @@ from __future__ import absolute_import
 
 from django.http import Http404
 
-from sentry import tagstore
 from sentry.api.base import EnvironmentMixin
-from sentry.models import Environment, EventUser, Group, get_group_with_redirect
+from sentry.models import Environment
+
+from sentry.data_export.base import ExportError
+from sentry.data_export.processors.issues_by_tag import IssuesByTagProcessor
 from sentry.web.frontend.base import ProjectView
 from sentry.web.frontend.mixins.csv import CsvMixin
 
 
-def attach_eventuser(project_id):
-    def wrapped(items):
-        users = EventUser.for_tags(project_id, [i.value for i in items])
-        for item in items:
-            item._eventuser = users.get(item.value)
-
-    return wrapped
-
-
 class GroupTagExportView(ProjectView, CsvMixin, EnvironmentMixin):
     required_scope = "event:read"
 
     def get_header(self, key):
-        if key == "user":
-            return self.get_user_header()
-        return self.get_generic_header()
+        return tuple(IssuesByTagProcessor.get_header_fields(key))
 
     def get_row(self, item, key):
-        if key == "user":
-            return self.get_user_row(item)
-        return self.get_generic_row(item)
-
-    def get_generic_header(self):
-        return ("value", "times_seen", "last_seen", "first_seen")
-
-    def get_generic_row(self, item):
-        return (
-            item.value,
-            item.times_seen,
-            item.last_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-            item.first_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-        )
-
-    def get_user_header(self):
-        return (
-            "value",
-            "id",
-            "email",
-            "username",
-            "ip_address",
-            "times_seen",
-            "last_seen",
-            "first_seen",
-        )
-
-    def get_user_row(self, item):
-        euser = item._eventuser
-        return (
-            item.value,
-            euser.ident if euser else "",
-            euser.email if euser else "",
-            euser.username if euser else "",
-            euser.ip_address if euser else "",
-            item.times_seen,
-            item.last_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-            item.first_seen.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
-        )
+        fields = IssuesByTagProcessor.get_header_fields(key)
+        item_dict = IssuesByTagProcessor.serialize_row(item, key)
+        return (item_dict[field] for field in fields)
 
     def get(self, request, organization, project, group_id, key):
-        try:
-            # TODO(tkaemming): This should *actually* redirect, see similar
-            # comment in ``GroupEndpoint.convert_args``.
-            group, _ = get_group_with_redirect(
-                group_id, queryset=Group.objects.filter(project=project)
-            )
-        except Group.DoesNotExist:
-            raise Http404
-
-        if tagstore.is_reserved_key(key):
-            lookup_key = u"sentry:{0}".format(key)
-        else:
-            lookup_key = key
 
+        # If the environment doesn't exist then the tag can't possibly exist
         try:
             environment_id = self._get_environment_id_from_request(request, project.organization_id)
         except Environment.DoesNotExist:
-            # if the environment doesn't exist then the tag can't possibly exist
             raise Http404
 
-        # validate existence as it may be deleted
         try:
-            tagstore.get_tag_key(project.id, environment_id, lookup_key)
-        except tagstore.TagKeyNotFound:
+            processor = IssuesByTagProcessor(
+                project_id=project.id, group_id=group_id, key=key, environment_id=environment_id
+            )
+        except ExportError:
             raise Http404
 
-        if key == "user":
-            callbacks = [attach_eventuser(project.id)]
-        else:
-            callbacks = []
-
-        gtv_iter = tagstore.get_group_tag_value_iter(
-            group.project_id, group.id, environment_id, lookup_key, callbacks=callbacks
-        )
-
-        filename = u"{}-{}".format(group.qualified_short_id or group.id, key)
+        filename = u"{}-{}".format(processor.group.qualified_short_id or processor.group.id, key)
 
-        return self.to_csv_response(gtv_iter, filename, key=key)
+        return self.to_csv_response(processor.get_raw_data(), filename, key=key)
diff --git a/tests/sentry/api/endpoints/test_data_export.py b/tests/sentry/api/endpoints/test_data_export.py
index 94043ec25b..92184b107e 100644
--- a/tests/sentry/api/endpoints/test_data_export.py
+++ b/tests/sentry/api/endpoints/test_data_export.py
@@ -2,9 +2,8 @@ from __future__ import absolute_import
 
 import six
 
-from sentry.constants import ExportQueryType
-from sentry.models import ExportedData
-from sentry.models.exporteddata import ExportStatus
+from sentry.data_export.base import ExportStatus, ExportQueryType
+from sentry.data_export.models import ExportedData
 from sentry.testutils import APITestCase
 
 
diff --git a/tests/sentry/api/endpoints/test_data_export_details.py b/tests/sentry/api/endpoints/test_data_export_details.py
index 4fd958c7ad..1f4052dae4 100644
--- a/tests/sentry/api/endpoints/test_data_export_details.py
+++ b/tests/sentry/api/endpoints/test_data_export_details.py
@@ -4,9 +4,8 @@ import six
 from datetime import timedelta
 from django.utils import timezone
 
-from sentry.constants import ExportQueryType
-from sentry.models import ExportedData
-from sentry.models.exporteddata import ExportStatus
+from sentry.data_export.base import ExportStatus, ExportQueryType
+from sentry.data_export.models import ExportedData
 from sentry.testutils import APITestCase
 
 
diff --git a/tests/sentry/models/test_exporteddata.py b/tests/sentry/models/test_exporteddata.py
index 8fc9874ef9..f6f2ca8618 100644
--- a/tests/sentry/models/test_exporteddata.py
+++ b/tests/sentry/models/test_exporteddata.py
@@ -8,8 +8,9 @@ from django.core import mail
 from django.core.urlresolvers import reverse
 from django.utils import timezone
 
-from sentry.models import ExportedData, File
-from sentry.models.exporteddata import DEFAULT_EXPIRATION, ExportStatus
+from sentry.data_export.base import ExportQueryType, ExportStatus, DEFAULT_EXPIRATION
+from sentry.data_export.models import ExportedData
+from sentry.models import File
 from sentry.testutils import TestCase
 from sentry.utils.http import absolute_uri
 from sentry.utils.compat.mock import patch
@@ -47,6 +48,12 @@ class ExportedDataTest(TestCase):
         keys = self.data_export.query_info.keys() + ["export_type"]
         assert sorted(self.data_export.payload.keys()) == sorted(keys)
 
+    def test_file_name_property(self):
+        assert isinstance(self.data_export.file_name, six.string_types)
+        file_name = self.data_export.file_name
+        assert file_name.startswith(ExportQueryType.as_str(self.data_export.query_type))
+        assert file_name.endswith(six.text_type(self.data_export.id) + ".csv")
+
     def test_format_date(self):
         assert ExportedData.format_date(self.data_export.date_finished) is None
         assert isinstance(ExportedData.format_date(self.data_export.date_added), six.binary_type)
diff --git a/tests/sentry/tasks/test_data_export.py b/tests/sentry/tasks/test_data_export.py
index 439686f907..1b69f21314 100644
--- a/tests/sentry/tasks/test_data_export.py
+++ b/tests/sentry/tasks/test_data_export.py
@@ -1,9 +1,8 @@
 from __future__ import absolute_import
 
-import six
-
-from sentry.models import ExportedData, File
-from sentry.tasks.data_export import assemble_download, get_file_name, DataExportError
+from sentry.data_export.models import ExportedData
+from sentry.data_export.tasks import assemble_download
+from sentry.models import File
 from sentry.testutils import TestCase, SnubaTestCase
 from sentry.utils.compat.mock import patch
 
@@ -25,37 +24,10 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         )
 
     def test_task_persistent_name(self):
-        assert assemble_download.name == "sentry.tasks.data_export.assemble_download"
-
-    def test_get_file_name(self):
-        file_name = get_file_name("TESTING", "proj1_user1_test", "ext")
-        assert file_name == "TESTING-proj1_user1_test.ext"
-        file_name = get_file_name("TESTING", "proj1_user1_test")
-        assert file_name == "TESTING-proj1_user1_test.csv"
+        assert assemble_download.name == "sentry.data_export.tasks.assemble_download"
 
     def test_issue_by_tag(self):
-        de1 = ExportedData.objects.create(
-            user=self.user,
-            organization=self.org,
-            query_type=0,
-            query_info={
-                "project_id": self.project.id,
-                "group_id": self.event.group_id,
-                "key": "user",
-            },
-        )
-        with self.tasks():
-            assemble_download(de1.id)
-        de1 = ExportedData.objects.get(id=de1.id)
-        assert de1.date_finished is not None
-        assert de1.date_expired is not None
-        assert de1.file is not None
-        f1 = de1.file
-        assert isinstance(f1, File)
-        assert f1.headers == {"Content-Type": "text/csv"}
-        raw1 = f1.getfile().read()
-        assert raw1 == "value,id,email,username,ip_address,times_seen,last_seen,first_seen\r\n"
-        de2 = ExportedData.objects.create(
+        de = ExportedData.objects.create(
             user=self.user,
             organization=self.org,
             query_type=0,
@@ -66,17 +38,22 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
             },
         )
         with self.tasks():
-            assemble_download(de2.id)
-        de2 = ExportedData.objects.get(id=de2.id)
+            assemble_download(de.id)
+        de = ExportedData.objects.get(id=de.id)
+        assert de.date_finished is not None
+        assert de.date_expired is not None
+        assert de.file is not None
+        assert isinstance(de.file, File)
+        assert de.file.headers == {"Content-Type": "text/csv"}
         # Convert raw csv to list of line-strings
-        header, raw1, raw2 = de2.file.getfile().read().strip().split("\r\n")
+        header, raw1, raw2 = de.file.getfile().read().strip().split("\r\n")
         assert header == "value,times_seen,last_seen,first_seen"
 
         raw1, raw2 = sorted([raw1, raw2])
         assert raw1.startswith("bar,1,")
         assert raw2.startswith("bar2,2,")
 
-    @patch("sentry.models.ExportedData.email_failure")
+    @patch("sentry.data_export.models.ExportedData.email_failure")
     def test_issue_by_tag_errors(self, emailer):
         de1 = ExportedData.objects.create(
             user=self.user,
@@ -87,8 +64,7 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         with self.tasks():
             assemble_download(de1.id)
         error = emailer.call_args[1]["message"]
-        assert isinstance(error, DataExportError)
-        assert six.text_type(error) == u"Requested project does not exist"
+        assert error == "Requested project does not exist"
         de2 = ExportedData.objects.create(
             user=self.user,
             organization=self.org,
@@ -98,5 +74,4 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         with self.tasks():
             assemble_download(de2.id)
         error = emailer.call_args[1]["message"]
-        assert isinstance(error, DataExportError)
-        assert six.text_type(error) == u"Requested issue does not exist"
+        assert error == "Requested issue does not exist"
