commit b99e4a145096857f0f6c136af2e863ebddfa3539
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Fri Feb 21 15:10:31 2020 +0100

    feat(ingest): Ability to disable automatic GC (#17188)
    
    * feat: Disable auto gc for ingest consumers
    
    * fix: Fix test
    
    * feat: Move it to CLI flag

diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index 3f724d005f..d5101aec2c 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -404,6 +404,13 @@ def batching_kafka_options(group):
             help="Position in the commit log topic to begin reading from when no prior offset has been recorded.",
         )(f)
 
+        f = click.option(
+            "--pause-gc/--no-pause-gc",
+            "pause_gc",
+            default=False,
+            help="Disable GC, and run it manually after each batch flush.",
+        )(f)
+
         return f
 
     return inner
diff --git a/src/sentry/utils/batching_kafka_consumer.py b/src/sentry/utils/batching_kafka_consumer.py
index 78253e770e..81720273ae 100644
--- a/src/sentry/utils/batching_kafka_consumer.py
+++ b/src/sentry/utils/batching_kafka_consumer.py
@@ -1,6 +1,7 @@
 from __future__ import absolute_import
 
 import abc
+import gc
 import logging
 import six
 import time
@@ -115,6 +116,7 @@ class BatchingKafkaConsumer(object):
         queued_min_messages=DEFAULT_QUEUED_MIN_MESSAGES,
         metrics_sample_rates=None,
         metrics_default_tags=None,
+        pause_gc=False,
     ):
         assert isinstance(worker, AbstractBatchWorker)
         self.worker = worker
@@ -156,6 +158,9 @@ class BatchingKafkaConsumer(object):
         self.producer = producer
         self.commit_log_topic = commit_log_topic
         self.dead_letter_topic = dead_letter_topic
+        self.pause_gc = pause_gc
+        if pause_gc:
+            gc.disable()
 
     def __record_timing(self, metric, value, tags=None):
         if self.__metrics is None:
@@ -299,6 +304,15 @@ class BatchingKafkaConsumer(object):
         self.__batch_messages_processed_count = 0
         self.__batch_processing_time_ms = 0.0
 
+        if self.pause_gc:
+            logger.debug("Running manual full GC after batch")
+            gc_start = time.time()
+            gc.collect()
+            gc_end = time.time()
+            gc_duration = (gc_end - gc_start) * 1000
+            logger.debug("Finished manual GC, took %dms", gc_duration)
+            self.__record_timing("gc.collect", gc_duration)
+
     def _flush(self, force=False):
         """Decides whether the `BatchingKafkaConsumer` should flush because of either
         batch size or time. If so, delegate to the worker, clear the current batch,
diff --git a/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py b/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
index 297c22fe9d..99b52c31d8 100644
--- a/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
+++ b/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
@@ -12,7 +12,6 @@ from sentry import eventstore
 from sentry.event_manager import EventManager
 from sentry.ingest.ingest_consumer import ConsumerType, get_ingest_consumer
 from sentry.utils import json
-from sentry.testutils.factories import Factories
 
 logger = logging.getLogger(__name__)
 
@@ -51,9 +50,9 @@ def _get_test_message(project):
     return val, event_id
 
 
-@pytest.mark.django_db
+@pytest.mark.django_db(transaction=True)
 def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
-    task_runner, kafka_producer, kafka_admin, requires_kafka
+    task_runner, kafka_producer, kafka_admin, requires_kafka, default_project
 ):
     group_id = "test-consumer"
     topic_event_name = ConsumerType.get_topic_name(ConsumerType.Events)
@@ -62,12 +61,9 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
     admin.delete_topic(topic_event_name)
     producer = kafka_producer(settings)
 
-    organization = Factories.create_organization()
-    project = Factories.create_project(organization=organization)
-
     event_ids = set()
     for _ in range(3):
-        message, event_id = _get_test_message(project)
+        message, event_id = _get_test_message(default_project)
         event_ids.add(event_id)
         producer.produce(topic_event_name, message)
 
@@ -82,7 +78,7 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
     with task_runner():
         i = 0
         while i < MAX_POLL_ITERATIONS:
-            if eventstore.get_event_by_id(project.id, event_id):
+            if eventstore.get_event_by_id(default_project.id, event_id):
                 break
 
             consumer._run_once()
@@ -90,7 +86,7 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
 
     # check that we got the messages
     for event_id in event_ids:
-        message = eventstore.get_event_by_id(project.id, event_id)
+        message = eventstore.get_event_by_id(default_project.id, event_id)
         assert message is not None
         # check that the data has not been scrambled
         assert message.data["extra"]["the_id"] == event_id
