commit a8cddadd1ce00957911c92efbb6500ca53c045ac
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Fri Feb 7 13:34:45 2020 +0100

    ref: Measure parts of EventManager.save for APM (#16839)
    
    * ref: Measure parts of EventManager.save for APM
    
    * fix: Fix typos
    
    * ref: Skip over attachments for tx events
    
    * ref: Add more metrics as per review feedback

diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index 8d1f5e91c9..60c649a844 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -415,6 +415,7 @@ class EventManager(object):
     def get_data(self):
         return self._data
 
+    @metrics.wraps("event_manager.get_event_instance")
     def _get_event_instance(self, project_id=None):
         data = self._data
         event_id = data.get("event_id")
@@ -545,6 +546,7 @@ class EventManager(object):
                 file=file,
             )
 
+    @metrics.wraps("event_manager.save")
     def save(self, project_id, raw=False, assume_normalized=False, cache_key=None):
         """
         We re-insert events with duplicate IDs into Snuba, which is responsible
@@ -567,10 +569,13 @@ class EventManager(object):
 
         data = self._data
 
-        project = Project.objects.get_from_cache(id=project_id)
-        project._organization_cache = Organization.objects.get_from_cache(
-            id=project.organization_id
-        )
+        with metrics.timer("event_manager.save.project.get_from_cache"):
+            project = Project.objects.get_from_cache(id=project_id)
+
+        with metrics.timer("event_manager.save.organization.get_from_cache"):
+            project._organization_cache = Organization.objects.get_from_cache(
+                id=project.organization_id
+            )
 
         # Pull out the culprit
         culprit = self.get_culprit()
@@ -733,13 +738,13 @@ class EventManager(object):
                     tags={"organization_id": project.organization_id, "platform": platform},
                 )
                 raise
-            else:
-                event_saved.send_robust(project=project, event_size=event.size, sender=EventManager)
             event.group = group
         else:
             group = None
             is_new = False
             is_regression = False
+
+        with metrics.timer("event_manager.event_saved_signal"):
             event_saved.send_robust(project=project, event_size=event.size, sender=EventManager)
 
         # store a reference to the group id to guarantee validation of isolation
@@ -778,7 +783,9 @@ class EventManager(object):
         if release:
             counters.append((tsdb.models.release, release.id))
 
-        tsdb.incr_multi(counters, timestamp=event.datetime, environment_id=environment.id)
+        with metrics.timer("event_manager.tsdb_incr_group_and_release_counters") as metrics_tags:
+            metrics_tags["has_group"] = "true" if group else "false"
+            tsdb.incr_multi(counters, timestamp=event.datetime, environment_id=environment.id)
 
         frequencies = []
 
@@ -807,13 +814,14 @@ class EventManager(object):
         # Capture the actual size that goes into node store.
         event_metrics["bytes.stored.event"] = len(json.dumps(dict(event.data.items())))
 
-        # Load attachments first, but persist them at the very last after
-        # posting to eventstream to make sure all counters and eventstream are
-        # incremented for sure.
-        attachments = self.get_attachments(cache_key, event)
-        for attachment in attachments:
-            key = "bytes.stored.%s" % (attachment.type,)
-            event_metrics[key] = (event_metrics.get(key) or 0) + len(attachment.data)
+        if not issueless_event:
+            # Load attachments first, but persist them at the very last after
+            # posting to eventstream to make sure all counters and eventstream are
+            # incremented for sure.
+            attachments = self.get_attachments(cache_key, event)
+            for attachment in attachments:
+                key = "bytes.stored.%s" % (attachment.type,)
+                event_metrics[key] = (event_metrics.get(key) or 0) + len(attachment.data)
 
         # Write the event to Nodestore
         event.data.save()
@@ -828,7 +836,9 @@ class EventManager(object):
                     (tsdb.models.users_affected_by_group, group.id, (event_user.tag_value,))
                 )
 
-            tsdb.record_multi(counters, timestamp=event.datetime, environment_id=environment.id)
+            with metrics.timer("event_manager.tsdb_record_users_affected") as metrics_tags:
+                metrics_tags["has_group"] = "true" if group else "false"
+                tsdb.record_multi(counters, timestamp=event.datetime, environment_id=environment.id)
 
         if release:
             if is_new:
@@ -853,24 +863,26 @@ class EventManager(object):
                 project.update(first_event=date)
                 first_event_received.send_robust(project=project, event=event, sender=Project)
 
-        eventstream.insert(
-            group=group,
-            event=event,
-            is_new=is_new,
-            is_regression=is_regression,
-            is_new_group_environment=is_new_group_environment,
-            primary_hash=hashes[0],
-            # We are choosing to skip consuming the event back
-            # in the eventstream if it's flagged as raw.
-            # This means that we want to publish the event
-            # through the event stream, but we don't care
-            # about post processing and handling the commit.
-            skip_consume=raw,
-        )
+        with metrics.timer("event_manager.eventstream.insert"):
+            eventstream.insert(
+                group=group,
+                event=event,
+                is_new=is_new,
+                is_regression=is_regression,
+                is_new_group_environment=is_new_group_environment,
+                primary_hash=hashes[0],
+                # We are choosing to skip consuming the event back
+                # in the eventstream if it's flagged as raw.
+                # This means that we want to publish the event
+                # through the event stream, but we don't care
+                # about post processing and handling the commit.
+                skip_consume=raw,
+            )
 
-        # Do this last to ensure signals get emitted even if connection to the
-        # file store breaks temporarily.
-        self.save_attachments(attachments, event)
+        if not issueless_event:
+            # Do this last to ensure signals get emitted even if connection to the
+            # file store breaks temporarily.
+            self.save_attachments(attachments, event)
 
         metric_tags = {"from_relay": "_relay_processed" in self._data}
 
@@ -885,10 +897,17 @@ class EventManager(object):
         return event
 
     def _get_event_user(self, project, data):
+        with metrics.timer("event_manager.get_event_user") as metrics_tags:
+            return self._get_event_user_impl(project, data, metrics_tags)
+
+    def _get_event_user_impl(self, project, data, metrics_tags):
         user_data = data.get("user")
         if not user_data:
+            metrics_tags["event_has_user"] = "false"
             return
 
+        metrics_tags["event_has_user"] = "true"
+
         ip_address = user_data.get("ip_address")
 
         if ip_address:
@@ -912,13 +931,17 @@ class EventManager(object):
         cache_key = u"euserid:1:{}:{}".format(project.id, euser.hash)
         euser_id = cache.get(cache_key)
         if euser_id is None:
+            metrics_tags["cache_hit"] = "false"
             try:
                 with transaction.atomic(using=router.db_for_write(EventUser)):
                     euser.save()
+                metrics_tags["created"] = "true"
             except IntegrityError:
+                metrics_tags["created"] = "false"
                 try:
                     euser = EventUser.objects.get(project_id=project.id, hash=euser.hash)
                 except EventUser.DoesNotExist:
+                    metrics_tags["created"] = "lol"
                     # why???
                     e_userid = -1
                 else:
@@ -926,6 +949,9 @@ class EventManager(object):
                         euser.update(name=user_data["name"])
                     e_userid = euser.id
                 cache.set(cache_key, e_userid, 3600)
+        else:
+            metrics_tags["cache_hit"] = "true"
+
         return euser
 
     def _find_hashes(self, project, hash_list):
diff --git a/src/sentry/models/environment.py b/src/sentry/models/environment.py
index 118af5dff0..debcedeec0 100644
--- a/src/sentry/models/environment.py
+++ b/src/sentry/models/environment.py
@@ -5,6 +5,7 @@ from django.utils import timezone
 
 from sentry.constants import ENVIRONMENT_NAME_PATTERN, ENVIRONMENT_NAME_MAX_LENGTH
 from sentry.db.models import BoundedPositiveIntegerField, FlexibleForeignKey, Model, sane_repr
+from sentry.utils import metrics
 from sentry.utils.cache import cache
 from sentry.utils.hashlib import md5_text
 import re
@@ -76,18 +77,24 @@ class Environment(Model):
 
     @classmethod
     def get_or_create(cls, project, name):
-        name = cls.get_name_or_default(name)
+        with metrics.timer("models.environment.get_or_create") as metrics_tags:
+            name = cls.get_name_or_default(name)
 
-        cache_key = cls.get_cache_key(project.organization_id, name)
+            cache_key = cls.get_cache_key(project.organization_id, name)
 
-        env = cache.get(cache_key)
-        if env is None:
-            env = cls.objects.get_or_create(name=name, organization_id=project.organization_id)[0]
-            cache.set(cache_key, env, 3600)
+            env = cache.get(cache_key)
+            if env is None:
+                metrics_tags["cache_hit"] = "false"
+                env = cls.objects.get_or_create(name=name, organization_id=project.organization_id)[
+                    0
+                ]
+                cache.set(cache_key, env, 3600)
+            else:
+                metrics_tags["cache_hit"] = "true"
 
-        env.add_project(project)
+            env.add_project(project)
 
-        return env
+            return env
 
     def add_project(self, project, is_hidden=None):
         cache_key = "envproj:c:%s:%s" % (self.id, project.id)
diff --git a/src/sentry/models/release.py b/src/sentry/models/release.py
index 37c580d597..cff6cf3845 100644
--- a/src/sentry/models/release.py
+++ b/src/sentry/models/release.py
@@ -129,6 +129,11 @@ class Release(Model):
 
     @classmethod
     def get_or_create(cls, project, version, date_added=None):
+        with metrics.timer("models.release.get_or_create") as metric_tags:
+            return cls._get_or_create_impl(project, version, date_added, metric_tags)
+
+    @classmethod
+    def _get_or_create_impl(cls, project, version, date_added, metric_tags):
         from sentry.models import Project
 
         if date_added is None:
@@ -137,6 +142,7 @@ class Release(Model):
         cache_key = cls.get_cache_key(project.organization_id, version)
 
         release = cache.get(cache_key)
+
         if release in (None, -1):
             # TODO(dcramer): if the cache result is -1 we could attempt a
             # default create here instead of default get
@@ -148,11 +154,13 @@ class Release(Model):
                     projects=project,
                 )
             )
+
             if releases:
                 try:
                     release = [r for r in releases if r.version == project_version][0]
                 except IndexError:
                     release = releases[0]
+                metric_tags["created"] = "false"
             else:
                 try:
                     with transaction.atomic():
@@ -162,10 +170,14 @@ class Release(Model):
                             date_added=date_added,
                             total_deploys=0,
                         )
+
+                    metric_tags["created"] = "true"
                 except IntegrityError:
+                    metric_tags["created"] = "false"
                     release = cls.objects.get(
                         organization_id=project.organization_id, version=version
                     )
+
                 release.add_project(project)
                 if not project.flags.has_releases:
                     project.flags.has_releases = True
@@ -174,6 +186,9 @@ class Release(Model):
             # TODO(dcramer): upon creating a new release, check if it should be
             # the new "latest release" for this project
             cache.set(cache_key, release, 3600)
+            metric_tags["cache_hit"] = "false"
+        else:
+            metric_tags["cache_hit"] = "true"
 
         return release
 
diff --git a/src/sentry/models/releaseenvironment.py b/src/sentry/models/releaseenvironment.py
index 7d3e137469..0f570a11df 100644
--- a/src/sentry/models/releaseenvironment.py
+++ b/src/sentry/models/releaseenvironment.py
@@ -5,6 +5,7 @@ from django.db import models
 from django.utils import timezone
 
 from sentry.utils.cache import cache
+from sentry.utils import metrics
 from sentry.db.models import BoundedPositiveIntegerField, FlexibleForeignKey, Model, sane_repr
 
 
@@ -32,10 +33,16 @@ class ReleaseEnvironment(Model):
 
     @classmethod
     def get_or_create(cls, project, release, environment, datetime, **kwargs):
+        with metrics.timer("models.releaseenvironment.get_or_create") as metric_tags:
+            return cls._get_or_create_impl(project, release, environment, datetime, metric_tags)
+
+    @classmethod
+    def _get_or_create_impl(cls, project, release, environment, datetime, metric_tags):
         cache_key = cls.get_cache_key(project.id, release.id, environment.id)
 
         instance = cache.get(cache_key)
         if instance is None:
+            metric_tags["cache_hit"] = "false"
             instance, created = cls.objects.get_or_create(
                 release_id=release.id,
                 organization_id=project.organization_id,
@@ -44,15 +51,22 @@ class ReleaseEnvironment(Model):
             )
             cache.set(cache_key, instance, 3600)
         else:
+            metric_tags["cache_hit"] = "true"
             created = False
 
+        metric_tags["created"] = "true" if created else "false"
+
         # TODO(dcramer): this would be good to buffer, but until then we minimize
         # updates to once a minute, and allow Postgres to optimistically skip
         # it even if we can't
         if not created and instance.last_seen < datetime - timedelta(seconds=60):
+            metric_tags["bumped"] = "true"
             cls.objects.filter(
                 id=instance.id, last_seen__lt=datetime - timedelta(seconds=60)
             ).update(last_seen=datetime)
             instance.last_seen = datetime
             cache.set(cache_key, instance, 3600)
+        else:
+            metric_tags["bumped"] = "false"
+
         return instance
diff --git a/src/sentry/models/releaseprojectenvironment.py b/src/sentry/models/releaseprojectenvironment.py
index 4620a84bc7..9dbfed2fd0 100644
--- a/src/sentry/models/releaseprojectenvironment.py
+++ b/src/sentry/models/releaseprojectenvironment.py
@@ -4,6 +4,7 @@ from datetime import timedelta
 from django.db import models
 from django.utils import timezone
 
+from sentry.utils import metrics
 from sentry.utils.cache import cache
 from sentry.db.models import BoundedPositiveIntegerField, FlexibleForeignKey, Model, sane_repr
 
@@ -32,10 +33,18 @@ class ReleaseProjectEnvironment(Model):
 
     @classmethod
     def get_or_create(cls, release, project, environment, datetime, **kwargs):
+        with metrics.timer("models.releaseprojectenvironment.get_or_create") as metrics_tags:
+            return cls._get_or_create_impl(
+                release, project, environment, datetime, metrics_tags, **kwargs
+            )
+
+    @classmethod
+    def _get_or_create_impl(cls, release, project, environment, datetime, metrics_tags, **kwargs):
         cache_key = cls.get_cache_key(project.id, release.id, environment.id)
 
         instance = cache.get(cache_key)
         if instance is None:
+            metrics_tags["cache_hit"] = "false"
             instance, created = cls.objects.get_or_create(
                 release=release,
                 project=project,
@@ -43,9 +52,12 @@ class ReleaseProjectEnvironment(Model):
                 defaults={"first_seen": datetime, "last_seen": datetime},
             )
             cache.set(cache_key, instance, 3600)
+            metrics_tags["cache_hit"] = "true"
         else:
             created = False
 
+        metrics_tags["created"] = "true" if created else "false"
+
         # Same as releaseenvironment model. Minimizes last_seen updates to once a minute
         if not created and instance.last_seen < datetime - timedelta(seconds=60):
             cls.objects.filter(
@@ -53,4 +65,8 @@ class ReleaseProjectEnvironment(Model):
             ).update(last_seen=datetime)
             instance.last_seen = datetime
             cache.set(cache_key, instance, 3600)
+            metrics_tags["bumped"] = "true"
+        else:
+            metrics_tags["bumped"] = "false"
+
         return instance
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 8db262edb5..43260375dd 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -477,16 +477,17 @@ def _do_save_event(
         # event.project.organization is populated after this statement.
         event = manager.save(project_id, assume_normalized=True, cache_key=cache_key)
 
-        # This is where we can finally say that we have accepted the event.
-        track_outcome(
-            event.project.organization_id,
-            event.project.id,
-            key_id,
-            Outcome.ACCEPTED,
-            None,
-            timestamp,
-            event_id,
-        )
+        with metrics.timer("tasks.store.track_outcome"):
+            # This is where we can finally say that we have accepted the event.
+            track_outcome(
+                event.project.organization_id,
+                event.project.id,
+                key_id,
+                Outcome.ACCEPTED,
+                None,
+                timestamp,
+                event_id,
+            )
 
     except HashDiscarded:
         project = Project.objects.get_from_cache(id=project_id)
