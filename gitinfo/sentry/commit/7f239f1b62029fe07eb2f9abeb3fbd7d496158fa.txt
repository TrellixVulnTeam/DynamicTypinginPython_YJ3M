commit 7f239f1b62029fe07eb2f9abeb3fbd7d496158fa
Author: Evan Purkhiser <evanpurkhiser@gmail.com>
Date:   Tue Sep 5 10:41:29 2017 -0700

    Migrate similarity feature to use redis clusters

diff --git a/src/sentry/scripts/similarity/index.lua b/src/sentry/scripts/similarity/index.lua
index 206e7273e8..2161a61b2f 100644
--- a/src/sentry/scripts/similarity/index.lua
+++ b/src/sentry/scripts/similarity/index.lua
@@ -220,8 +220,10 @@ end
 -- Key Generation
 
 local function get_key_prefix(configuration, index)
+    -- NB: The brackets around the scope allow redis cluster to shard keys
+    -- using the value within the brackets, this is known as a redis hash tag.
     return string.format(
-        '%s:%s:%s',
+        '%s:{%s}:%s',
         configuration.namespace,
         configuration.scope,
         index
diff --git a/src/sentry/similarity/__init__.py b/src/sentry/similarity/__init__.py
index b7c1a38cc8..8ffce9f689 100644
--- a/src/sentry/similarity/__init__.py
+++ b/src/sentry/similarity/__init__.py
@@ -1,12 +1,13 @@
 from __future__ import absolute_import
 
 import itertools
+import logging
 
 from django.conf import settings
 
 from sentry.interfaces.stacktrace import Frame
 from sentry.similarity.encoder import Encoder
-from sentry.similarity.index import MinHashIndex
+from sentry.similarity.index import MinHashIndex, DummyIndex
 from sentry.similarity.features import (
     ExceptionFeature,
     FeatureSet,
@@ -19,6 +20,8 @@ from sentry.utils import redis
 from sentry.utils.datastructures import BidirectionalMapping
 from sentry.utils.iterators import shingle
 
+logger = logging.getLogger(__name__)
+
 
 def text_shingle(n, value):
     return itertools.imap(
@@ -57,21 +60,33 @@ def get_frame_attributes(frame):
     return attributes
 
 
-features = FeatureSet(
-    MinHashIndex(
-        redis.clusters.get(
-            getattr(
-                settings,
-                'SENTRY_SIMILARITY_INDEX_REDIS_CLUSTER',
-                'default',
-            ),
-        ),
+def _make_index(cluster=None):
+    if not cluster:
+        cluster_id = getattr(
+            settings,
+            'SENTRY_SIMILARITY_INDEX_REDIS_CLUSTER',
+            'similarity',
+        )
+
+        try:
+            cluster = redis.redis_clusters.get(cluster_id)
+        except KeyError:
+            index = DummyIndex()
+            logger.info('No redis cluster provided for similarity, using {!r}.'.format(index))
+            return index
+
+    return MinHashIndex(
+        cluster,
         'sim:1',
         MinHashSignatureBuilder(16, 0xFFFF),
         8,
         60 * 60 * 24 * 30,
         3,
-    ),
+    )
+
+
+features = FeatureSet(
+    _make_index(),
     Encoder({
         Frame: get_frame_attributes,
     }),
diff --git a/src/sentry/similarity/features.py b/src/sentry/similarity/features.py
index a15159f245..cc93dad1aa 100644
--- a/src/sentry/similarity/features.py
+++ b/src/sentry/similarity/features.py
@@ -265,8 +265,8 @@ class FeatureSet(object):
             [(self.aliases[label], key) for label in self.features.keys()],
         )
 
-    def flush(self, project=None):
+    def flush(self, project):
         return self.index.flush(
-            '*' if project is None else self.__get_scope(project),
+            self.__get_scope(project),
             self.aliases.values(),
         )
diff --git a/src/sentry/similarity/index.py b/src/sentry/similarity/index.py
index 6811c8d8d1..fe4b05de8c 100644
--- a/src/sentry/similarity/index.py
+++ b/src/sentry/similarity/index.py
@@ -60,8 +60,12 @@ class MinHashIndex(object):
 
         return arguments
 
-    def _get_connection(self, scope):
-        return self.cluster.get_local_client_for_key(scope)
+    def __index(self, scope, args):
+        # scope must be passed into the script call as a key to allow the
+        # cluster client to determine what cluster the script should be
+        # executed on. The script itself will use the scope as the hashtag for
+        # all redis operations.
+        return index(self.cluster, [scope], args)
 
     def classify(self, scope, items, timestamp=None):
         if timestamp is None:
@@ -81,11 +85,7 @@ class MinHashIndex(object):
 
         return [
             [(item, float(score)) for item, score in result]
-            for result in index(
-                self._get_connection(scope),
-                [],
-                arguments,
-            )
+            for result in self.__index(scope, arguments)
         ]
 
     def compare(self, scope, key, indices, timestamp=None):
@@ -107,11 +107,7 @@ class MinHashIndex(object):
 
         return [
             [(item, float(score)) for item, score in result]
-            for result in index(
-                self._get_connection(scope),
-                [],
-                arguments,
-            )
+            for result in self.__index(scope, arguments)
         ]
 
     def record(self, scope, key, items, timestamp=None):
@@ -134,11 +130,7 @@ class MinHashIndex(object):
 
         arguments.extend(self.__build_signatures(items))
 
-        return index(
-            self._get_connection(scope),
-            [],
-            arguments,
-        )
+        return self.__index(scope, arguments)
 
     def merge(self, scope, destination, items, timestamp=None):
         if timestamp is None:
@@ -158,11 +150,7 @@ class MinHashIndex(object):
         for idx, source in items:
             arguments.extend([idx, source])
 
-        return index(
-            self._get_connection(scope),
-            [],
-            arguments,
-        )
+        return self.__index(scope, arguments)
 
     def delete(self, scope, items, timestamp=None):
         if timestamp is None:
@@ -181,11 +169,7 @@ class MinHashIndex(object):
         for idx, key in items:
             arguments.extend([idx, key])
 
-        return index(
-            self._get_connection(scope),
-            [],
-            arguments,
-        )
+        return self.__index(scope, arguments)
 
     def scan(self, scope, indices, batch=1000, timestamp=None):
         if timestamp is None:
@@ -201,37 +185,27 @@ class MinHashIndex(object):
             scope,
         ]
 
-        clients = map(
-            self.cluster.get_local_client,
-            self.cluster.hosts,
-        )
+        cursors = {idx: 0 for idx in indices}
+        while cursors:
+            requests = []
+            for idx, cursor in cursors.items():
+                requests.append([idx, cursor, batch])
 
-        for client in clients:
-            cursors = {idx: 0 for idx in indices}
-            while cursors:
-                requests = []
-                for idx, cursor in cursors.items():
-                    requests.append([idx, cursor, batch])
-
-                responses = index(
-                    client,
-                    [],
-                    arguments + flatten(requests),
-                )
+            responses = self.__index(scope, arguments + flatten(requests))
 
-                for (idx, _, _), (cursor, chunk) in zip(requests, responses):
-                    cursor = int(cursor)
-                    if cursor == 0:
-                        del cursors[idx]
-                    else:
-                        cursors[idx] = cursor
+            for (idx, _, _), (cursor, chunk) in zip(requests, responses):
+                cursor = int(cursor)
+                if cursor == 0:
+                    del cursors[idx]
+                else:
+                    cursors[idx] = cursor
 
-                    yield client, idx, chunk
+                yield idx, chunk
 
     def flush(self, scope, indices, batch=1000, timestamp=None):
-        for client, index, chunk in self.scan(scope, indices, batch, timestamp):
+        for index, chunk in self.scan(scope, indices, batch, timestamp):
             if chunk:
-                client.delete(*chunk)
+                self.cluster.delete(*chunk)
 
     def export(self, scope, items, timestamp=None):
         if timestamp is None:
@@ -250,11 +224,7 @@ class MinHashIndex(object):
         for idx, key in items:
             arguments.extend([idx, key])
 
-        return index(
-            self._get_connection(scope),
-            [],
-            arguments,
-        )
+        return self.__index(scope, arguments)
 
     def import_(self, scope, items, timestamp=None):
         if timestamp is None:
@@ -273,8 +243,35 @@ class MinHashIndex(object):
         for idx, key, data in items:
             arguments.extend([idx, key, data])
 
-        return index(
-            self._get_connection(scope),
-            [],
-            arguments,
-        )
+        return self.__index(scope, arguments)
+
+
+class DummyIndex(object):
+    def classify(self, scope, items, timestamp=None):
+        return []
+
+    def compare(self, scope, key, indices, timestamp=None):
+        return []
+
+    def record(self, scope, key, items, timestamp=None):
+        return {}
+
+    def merge(self, scope, destination, items, timestamp=None):
+        return False
+
+    def delete(self, scope, items, timestamp=None):
+        return False
+
+    def scan(self, scope, indices, batch=1000, timestamp=None):
+        # empty generator
+        return
+        yield
+
+    def flush(self, scope, indices, batch=1000, timestamp=None):
+        pass
+
+    def export(self, scope, items, timestamp=None):
+        return {}
+
+    def import_(self, scope, items, timestamp=None):
+        return {}
diff --git a/tests/sentry/similarity/test_index.py b/tests/sentry/similarity/test_index.py
index eac73f3220..007c60cacf 100644
--- a/tests/sentry/similarity/test_index.py
+++ b/tests/sentry/similarity/test_index.py
@@ -17,7 +17,7 @@ class MinHashIndexTestCase(TestCase):
     @fixture
     def index(self):
         return MinHashIndex(
-            redis.clusters.get('default'),
+            redis.clusters.get('default').get_local_client(0),
             'sim',
             signature_builder,
             16,
diff --git a/tests/sentry/tasks/test_merge.py b/tests/sentry/tasks/test_merge.py
index 140563e003..dc5a9c6fb9 100644
--- a/tests/sentry/tasks/test_merge.py
+++ b/tests/sentry/tasks/test_merge.py
@@ -1,12 +1,19 @@
 from __future__ import absolute_import
 
 from collections import defaultdict
+from mock import patch
 
 from sentry.tasks.merge import merge_group, rehash_group_events
 from sentry.models import Event, Group, GroupMeta, GroupRedirect, GroupTagKey, GroupTagValue
+from sentry.similarity import _make_index
 from sentry.testutils import TestCase
+from sentry.utils import redis
 
+# Use the default redis client as a cluster client in the similarity index
+index = _make_index(redis.clusters.get('default').get_local_client(0))
 
+
+@patch('sentry.similarity.features.index', new=index)
 class MergeGroupTest(TestCase):
     def test_merge_with_event_integrity(self):
         project1 = self.create_project()
diff --git a/tests/sentry/tasks/test_unmerge.py b/tests/sentry/tasks/test_unmerge.py
index 71b205bd72..477df4884d 100644
--- a/tests/sentry/tasks/test_unmerge.py
+++ b/tests/sentry/tasks/test_unmerge.py
@@ -9,6 +9,7 @@ from datetime import datetime, timedelta
 
 import pytz
 from django.utils import timezone
+from mock import patch
 
 from sentry.app import tsdb
 from sentry.event_manager import ScoreClause
@@ -16,15 +17,20 @@ from sentry.models import (
     Activity, Environment, EnvironmentProject, Event, EventMapping, Group, GroupHash, GroupRelease,
     GroupTagKey, GroupTagValue, Release, UserReport
 )
-from sentry.similarity import features
+from sentry.similarity import features, _make_index
 from sentry.tasks.unmerge import (
     get_caches, get_event_user_from_interface, get_fingerprint, get_group_backfill_attributes,
     get_group_creation_attributes, unmerge
 )
 from sentry.testutils import TestCase
 from sentry.utils.dates import to_timestamp
+from sentry.utils import redis
 
+# Use the default redis client as a cluster client in the similarity index
+index = _make_index(redis.clusters.get('default').get_local_client(0))
 
+
+@patch('sentry.similarity.features.index', new=index)
 class UnmergeTestCase(TestCase):
     def test_get_group_creation_attributes(self):
         now = datetime(2017, 5, 3, 6, 6, 6, tzinfo=pytz.utc)
