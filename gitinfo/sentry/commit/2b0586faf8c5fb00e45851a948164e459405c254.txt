commit 2b0586faf8c5fb00e45851a948164e459405c254
Author: ted kaemming <t.kaemming+github@gmail.com>
Date:   Mon Jun 6 11:12:06 2016 -0700

    Remove timelines that have passed TTL from schedules. (#3388)
    
    This removes timelines from digest schedules after all of the contents have presumably expired.

diff --git a/src/sentry/digests/backends/redis.py b/src/sentry/digests/backends/redis.py
index 0d79a7105c..1de6da4c4d 100644
--- a/src/sentry/digests/backends/redis.py
+++ b/src/sentry/digests/backends/redis.py
@@ -60,6 +60,18 @@ ensure_timeline_scheduled = load_script('digests/ensure_timeline_scheduled.lua')
 truncate_timeline = load_script('digests/truncate_timeline.lua')
 
 
+def clear_timeline_contents(pipeline, timeline_key):
+    """
+    Removes all keys associated with a timeline key. This does not remove the
+    timeline from schedules.
+
+    This assumes the timeline lock has already been acquired.
+    """
+    truncate_timeline(pipeline, (timeline_key,), (0, timeline_key))
+    truncate_timeline(pipeline, (make_digest_key(timeline_key),), (0, timeline_key))
+    pipeline.delete(make_last_processed_timestamp_key(timeline_key))
+
+
 class RedisBackend(Backend):
     """
     Implements the digest backend API, backed by Redis.
@@ -273,23 +285,25 @@ class RedisBackend(Backend):
         maximum_iterations = 1000
         for i in xrange(maximum_iterations):
             fetch_size = chunk + extra
-            items = connection.zrangebyscore(
-                make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                min=start,
-                max=deadline,
-                withscores=True,
-                start=0,
-                num=fetch_size,
+            entries = map(
+                lambda (key, timestamp): ScheduleEntry(key, timestamp),
+                connection.zrangebyscore(
+                    make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
+                    min=start,
+                    max=deadline,
+                    withscores=True,
+                    start=0,
+                    num=fetch_size,
+                )
             )
 
-            def try_lock(item):
+            def try_lock(entry):
                 """
                 Attempt to immedately acquire a lock on the timeline at
                 key, returning the lock if it can be acquired, otherwise
                 returning ``None``.
                 """
-                key, timestamp = item
-                timeline_key = make_timeline_key(self.namespace, key),
+                timeline_key = make_timeline_key(self.namespace, entry.key),
                 lock = self.locks.get(
                     timeline_key,
                     duration=5,
@@ -299,20 +313,16 @@ class RedisBackend(Backend):
                     lock.acquire()
                 except Exception:
                     lock = None
-                return lock, item
+                return lock, entry
 
-            # Try to take out a lock on each item. If we can't acquire the
-            # lock, that means this is currently being digested and cannot
-            # be rescheduled.
-            can_reschedule = {
-                True: [],
-                False: [],
-            }
-
-            for result in map(try_lock, items):
+            # Try to take out a lock on each entry. If we can't acquire the
+            # lock, that means this is currently being digested and cannot be
+            # rescheduled.
+            can_reschedule = ([], [])  # indexed by True and False
+            for result in map(try_lock, entries):
                 can_reschedule[result[0] is not None].append(result)
 
-            logger.debug('Fetched %s items, able to reschedule %s.', len(items), len(can_reschedule[True]))
+            logger.debug('Fetched %s items, able to reschedule %s.', len(entries), len(can_reschedule[True]))
 
             # Set the start position for the next query. (If there are no
             # items, we don't need to worry about this, since there won't
@@ -326,12 +336,12 @@ class RedisBackend(Backend):
             # couldn't acquire the lock this iteration) and add that count
             # to the next query limit. (This unfortunately could also
             # lead to unbounded growth too, so we have to limit it as well.)
-            if items:
-                start = items[-1][0]  # (This value is (key, timestamp).)
+            if entries:
+                start = entries[-1].key
                 extra = min(
                     ilen(
                         itertools.takewhile(
-                            lambda (lock, (key, timestamp)): timestamp == start,
+                            lambda (lock, entry): entry.timestamp == start,
                             can_reschedule[False][::-1],
                         ),
                     ),
@@ -343,7 +353,7 @@ class RedisBackend(Backend):
             # need to perform a similar check after the completion of the
             # transaction as well.)
             if not can_reschedule[True]:
-                if len(items) == fetch_size:
+                if len(entries) == fetch_size:
                     # There is nothing to reschedule in this chunk, but we
                     # need check if there are others after this chunk.
                     continue
@@ -357,37 +367,66 @@ class RedisBackend(Backend):
 
                     pipeline.zrem(
                         make_schedule_key(self.namespace, SCHEDULE_STATE_READY),
-                        *[key for (lock, (key, timestamp)) in can_reschedule[True]]
+                        *[entry.key for (lock, entry) in can_reschedule[True]]
                     )
 
-                    pipeline.zadd(
-                        make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
-                        *itertools.chain.from_iterable([(timestamp, key) for (lock, (key, timestamp)) in can_reschedule[True]])
+                    should_reschedule = ([], [])  # indexed by True and False
+                    timeout = time.time() - self.ttl
+                    for lock, entry in can_reschedule[True]:
+                        should_reschedule[entry.timestamp > timeout].append(entry)
+
+                    logger.debug(
+                        'Identified %s items that should be rescheduled, and %s that will be removed.',
+                        len(should_reschedule[True]),
+                        len(should_reschedule[False]),
                     )
 
+                    # Move items that should be rescheduled to the waiting state.
+                    if should_reschedule[True]:
+                        pipeline.zadd(
+                            make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING),
+                            *itertools.chain.from_iterable([(entry.timestamp, entry.key) for entry in should_reschedule[True]])
+                        )
+
+                    # Clear out timelines that should not be rescheduled.
+                    # Ideally this path is never actually hit, but this can
+                    # happen if the queue is **extremely** backlogged, or if a
+                    # cluster size change caused partition ownership to change
+                    # and timelines are now stuck within partitions that they
+                    # no longer should be. (For more details, see GH-2479.)
+                    for entry in should_reschedule[False]:
+                        logger.warning(
+                            'Clearing expired timeline %r from host %s, schedule timestamp was %s.',
+                            entry.key,
+                            host,
+                            entry.timestamp,
+                        )
+                        clear_timeline_contents(
+                            pipeline,
+                            make_timeline_key(self.namespace, entry.key),
+                        )
+
                     pipeline.execute()
             finally:
                 # Regardless of the outcome of the transaction, we should
                 # try to unlock the items for processing.
-                for lock, item in can_reschedule[True]:
+                for lock, entry in can_reschedule[True]:
                     try:
                         lock.release()
                     except Exception as error:
                         # XXX: This shouldn't be hit (the ``Lock`` code
                         # should swallow the exception) but this is here
                         # for safety anyway.
-                        logger.warning('Could not unlock %r: %s', item, error)
+                        logger.warning('Could not unlock %r: %s', entry, error)
 
             # If we retrieved less than the chunk size of items, we don't
             # need try to retrieve more items.
-            if len(items) < fetch_size:
+            if len(entries) < fetch_size:
                 break
         else:
             raise RuntimeError('loop exceeded maximum iterations (%s)' % (maximum_iterations,))
 
     def maintenance(self, deadline, chunk=1000):
-        # TODO: This needs tests!
-
         # TODO: Ideally, this would also return the number of items that were
         # rescheduled (and possibly even how late they were at the point of
         # rescheduling) but that causes a bit of an API issue since in the case
@@ -516,9 +555,7 @@ class RedisBackend(Backend):
 
         lock = self.locks.get(timeline_key, duration=30, routing_key=timeline_key)
         with lock.acquire(), connection.pipeline() as pipeline:
-            truncate_timeline(pipeline, (timeline_key,), (0, timeline_key))
-            truncate_timeline(pipeline, (make_digest_key(timeline_key),), (0, timeline_key))
-            pipeline.delete(make_last_processed_timestamp_key(timeline_key))
+            clear_timeline_contents(pipeline, timeline_key)
             pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_READY), key)
             pipeline.zrem(make_schedule_key(self.namespace, SCHEDULE_STATE_WAITING), key)
             pipeline.execute()
diff --git a/tests/sentry/digests/backends/test_redis.py b/tests/sentry/digests/backends/test_redis.py
index e8fbd8e492..6c22e38813 100644
--- a/tests/sentry/digests/backends/test_redis.py
+++ b/tests/sentry/digests/backends/test_redis.py
@@ -194,6 +194,48 @@ class RedisBackendTestCase(BaseRedisBackendTestCase):
                 assert entry.key == 'timelines:{0}'.format(i)
                 assert entry.timestamp == float(i)
 
+    def test_maintenance(self):
+        timeline = 'timeline'
+        backend = RedisBackend(ttl=3600)
+
+        timeline_key = make_timeline_key(backend.namespace, timeline)
+        digest_key = make_digest_key(timeline_key)
+        waiting_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_WAITING)
+        ready_set_key = make_schedule_key(backend.namespace, SCHEDULE_STATE_READY)
+
+        now = time.time()
+
+        connection = backend.cluster.get_local_client_for_key(timeline_key)
+        schedule_time = now - 60
+        connection.zadd(ready_set_key, schedule_time, timeline)
+        connection.zadd(timeline_key, 0, '1')
+        connection.set(make_record_key(timeline_key, '1'), 'data')
+        connection.zadd(digest_key, 0, '2')
+        connection.set(make_record_key(timeline_key, '2'), 'data')
+
+        # Move the digest from the ready set to the waiting set.
+        backend.maintenance(now)
+        assert connection.zcard(ready_set_key) == 0
+        assert connection.zrange(waiting_set_key, 0, -1, withscores=True) == [(timeline, schedule_time)]
+
+        connection.zrem(waiting_set_key, timeline)
+        connection.zadd(ready_set_key, schedule_time, timeline)
+
+        # Delete the digest from the ready set.
+        with mock.patch('time.time', return_value=now + (backend.ttl + 1)):
+            backend.maintenance(now)
+
+        keys = (
+            ready_set_key,
+            waiting_set_key,
+            timeline_key,
+            digest_key,
+            make_record_key(timeline_key, '1'),
+            make_record_key(timeline_key, '2'),
+        )
+        for key in keys:
+            assert connection.exists(key) is False
+
     def test_delete(self):
         timeline = 'timeline'
         backend = RedisBackend()
