commit 585e6176e3de10a8088ccb60becca7f30bcd71e3
Author: Mark Story <mark@sentry.io>
Date:   Thu Jan 9 16:07:54 2020 -0500

    ref: Remove now unused discover query code (#16306)
    
    Remove the automatic dataset detection logic from sentry. We have
    switched over to the snuba based implementation of this. Not having
    dataset selection in sentry also means we can remove some (but not all)
    of the supporting logic as well.

diff --git a/src/sentry/api/bases/organization_events.py b/src/sentry/api/bases/organization_events.py
index 86f8294ead..4d5d960d21 100644
--- a/src/sentry/api/bases/organization_events.py
+++ b/src/sentry/api/bases/organization_events.py
@@ -5,9 +5,7 @@ from rest_framework.exceptions import PermissionDenied
 from sentry.api.bases import OrganizationEndpoint, OrganizationEventsError
 from sentry.api.event_search import get_filter, InvalidSearchQuery
 from sentry.models.project import Project
-from sentry.snuba.dataset import Dataset
 from sentry.snuba.discover import ReferenceEvent
-from sentry.utils import snuba
 
 
 class OrganizationEventsEndpointBase(OrganizationEndpoint):
@@ -70,12 +68,4 @@ class OrganizationEventsEndpointBase(OrganizationEndpoint):
             "filter_keys": _filter.filter_keys,
         }
 
-        # 'legacy' endpoints cannot access transactions dataset.
-        # as they often have assumptions about which columns are returned.
-        dataset = snuba.detect_dataset(snuba_args)
-        if dataset != Dataset.Events:
-            raise OrganizationEventsError(
-                "Invalid query. You cannot reference non-events data in this endpoint."
-            )
-
         return snuba_args
diff --git a/src/sentry/eventstore/snuba/backend.py b/src/sentry/eventstore/snuba/backend.py
index 521160ba32..2d786fdcdb 100644
--- a/src/sentry/eventstore/snuba/backend.py
+++ b/src/sentry/eventstore/snuba/backend.py
@@ -68,6 +68,7 @@ class SnubaEventStorage(EventStorage):
             limit=limit,
             offset=offset,
             referrer=referrer,
+            dataset=snuba.Dataset.Events,
         )
 
         if "error" not in result:
@@ -188,6 +189,9 @@ class SnubaEventStorage(EventStorage):
         columns = [Columns.EVENT_ID.value.alias, Columns.PROJECT_ID.value.alias]
 
         try:
+            # This query uses the discover dataset to enable
+            # getting events across both errors and transactions, which is
+            # required when doing pagination in discover
             result = snuba.dataset_query(
                 selected_columns=columns,
                 conditions=filter.conditions,
@@ -197,7 +201,7 @@ class SnubaEventStorage(EventStorage):
                 limit=1,
                 referrer="eventstore.get_next_or_prev_event_id",
                 orderby=orderby,
-                dataset=snuba.detect_dataset({"conditions": filter.conditions}),
+                dataset=snuba.Dataset.Discover,
             )
         except (snuba.QueryOutsideRetentionError, snuba.QueryOutsideGroupActivityError):
             # This can happen when the date conditions for paging
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 09fa97477a..8149fb35f2 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -292,88 +292,6 @@ def get_snuba_column_name(name, dataset=Dataset.Events):
     return DATASETS[dataset].get(name, u"tags[{}]".format(name))
 
 
-def detect_dataset(query_args, aliased_conditions=False):
-    """
-    Determine the dataset to use based on the conditions, selected_columns,
-    groupby clauses.
-
-    This function operates on the end user field aliases and not the internal column
-    names that have been converted using the field mappings.
-
-    The aliased_conditions parameter switches column detection between
-    the public aliases and the internal names. When query conditions
-    have been pre-parsed by api.event_search set aliased_conditions=True
-    as we need to look for internal names.
-
-    :deprecated: This method and the automatic dataset resolution is deprecated.
-    You should use sentry.snuba.discover instead.
-    """
-    if query_args.get("dataset", None):
-        return query_args["dataset"]
-
-    dataset = Dataset.Events
-    transaction_fields = set(DATASETS[Dataset.Transactions].keys()) - set(
-        DATASETS[Dataset.Events].keys()
-    )
-    condition_fieldset = transaction_fields
-
-    if aliased_conditions:
-        # Release and user are also excluded as they are present on both
-        # datasets and don't trigger usage of transactions.
-        condition_fieldset = (
-            set(DATASET_FIELDS[Dataset.Transactions])
-            - set(DATASET_FIELDS[Dataset.Events])
-            - set(["release", "user"])
-        )
-
-    for condition in query_args.get("conditions") or []:
-        if isinstance(condition[0], six.string_types) and condition[0] in condition_fieldset:
-            return Dataset.Transactions
-        if condition == ["event.type", "=", "transaction"] or condition == [
-            "type",
-            "=",
-            "transaction",
-        ]:
-            return Dataset.Transactions
-
-        if condition == ["event.type", "!=", "transaction"] or condition == [
-            "type",
-            "!=",
-            "transaction",
-        ]:
-            return Dataset.Events
-
-    for field in query_args.get("selected_columns") or []:
-        if isinstance(field, six.string_types) and field in transaction_fields:
-            return Dataset.Transactions
-
-    for field in query_args.get("aggregations") or []:
-        if len(field) != 3:
-            continue
-        # Check field or fields
-        if isinstance(field[1], six.string_types) and field[1] in transaction_fields:
-            return Dataset.Transactions
-        if isinstance(field[1], (list, tuple)):
-            is_transaction = [column for column in field[1] if column in transaction_fields]
-            if is_transaction:
-                return Dataset.Transactions
-        # Check for transaction only field aliases
-        if isinstance(field[2], six.string_types) and field[2] in (
-            "apdex",
-            "impact",
-            "p75",
-            "p95",
-            "p99",
-        ):
-            return Dataset.Transactions
-
-    for field in query_args.get("groupby") or []:
-        if field in transaction_fields:
-            return Dataset.Transactions
-
-    return dataset
-
-
 def get_function_index(column_expr, depth=0):
     """
     If column_expr list contains a function, returns the index of its function name
@@ -411,7 +329,7 @@ def get_function_index(column_expr, depth=0):
         return None
 
 
-def parse_columns_in_functions(col, context=None, index=None, dataset=Dataset.Events):
+def parse_columns_in_functions(col, context=None, index=None):
     """
     Checks expressions for arguments that should be considered a column while
     ignoring strings that represent clickhouse function names
@@ -434,23 +352,23 @@ def parse_columns_in_functions(col, context=None, index=None, dataset=Dataset.Ev
         if function_name_index > 0:
             for i in six.moves.xrange(0, function_name_index):
                 if context is not None:
-                    context[i] = get_snuba_column_name(col[i], dataset)
+                    context[i] = get_snuba_column_name(col[i])
 
         args = col[function_name_index + 1]
 
         # check for nested functions in args
         if get_function_index(args):
             # look for columns
-            return parse_columns_in_functions(args, args, dataset=dataset)
+            return parse_columns_in_functions(args, args)
 
         # check each argument for column names
         else:
             for (i, arg) in enumerate(args):
-                parse_columns_in_functions(arg, args, i, dataset=dataset)
+                parse_columns_in_functions(arg, args, i)
     else:
         # probably a column name
         if context is not None and index is not None:
-            context[index] = get_snuba_column_name(col, dataset)
+            context[index] = get_snuba_column_name(col)
 
 
 def get_arrayjoin(column):
@@ -459,23 +377,6 @@ def get_arrayjoin(column):
         return match.groups()[0]
 
 
-def valid_orderby(orderby, custom_fields=None, dataset=Dataset.Events):
-    """
-    Check if a field can be used in sorting. We don't allow
-    sorting on fields that would be aliased as tag[foo] because those
-    fields are unlikely to be selected.
-    """
-    if custom_fields is None:
-        custom_fields = []
-    fields = orderby if isinstance(orderby, (list, tuple)) else [orderby]
-    mapping = DATASETS[dataset]
-    for field in fields:
-        field = field.lstrip("-")
-        if field not in mapping and field not in custom_fields:
-            return False
-    return True
-
-
 def transform_aliases_and_query(**kwargs):
     """
     Convert aliases in selected_columns, groupby, aggregation, conditions,
@@ -499,7 +400,7 @@ def transform_aliases_and_query(**kwargs):
     arrayjoin = kwargs.get("arrayjoin")
     orderby = kwargs.get("orderby")
     having = kwargs.get("having", [])
-    dataset = detect_dataset(kwargs)
+    dataset = Dataset.Events
 
     if selected_columns:
         for (idx, col) in enumerate(selected_columns):
@@ -511,14 +412,14 @@ def transform_aliases_and_query(**kwargs):
                 translated_columns[col[2]] = col[2]
                 derived_columns.add(col[2])
             else:
-                name = get_snuba_column_name(col, dataset)
+                name = get_snuba_column_name(col)
                 selected_columns[idx] = name
                 translated_columns[name] = col
 
     if groupby:
         for (idx, col) in enumerate(groupby):
             if col not in derived_columns:
-                name = get_snuba_column_name(col, dataset)
+                name = get_snuba_column_name(col)
             else:
                 name = col
 
@@ -528,12 +429,12 @@ def transform_aliases_and_query(**kwargs):
     for aggregation in aggregations or []:
         derived_columns.add(aggregation[2])
         if isinstance(aggregation[1], six.string_types):
-            aggregation[1] = get_snuba_column_name(aggregation[1], dataset)
+            aggregation[1] = get_snuba_column_name(aggregation[1])
         elif isinstance(aggregation[1], (set, tuple, list)):
-            aggregation[1] = [get_snuba_column_name(col, dataset) for col in aggregation[1]]
+            aggregation[1] = [get_snuba_column_name(col) for col in aggregation[1]]
 
     for col in filter_keys.keys():
-        name = get_snuba_column_name(col, dataset)
+        name = get_snuba_column_name(col)
         filter_keys[name] = filter_keys.pop(col)
 
     if conditions:
@@ -558,7 +459,7 @@ def transform_aliases_and_query(**kwargs):
             translated_orderby.append(
                 u"{}{}".format(
                     "-" if field_with_order.startswith("-") else "",
-                    field if field in derived_columns else get_snuba_column_name(field, dataset),
+                    field if field in derived_columns else get_snuba_column_name(field),
                 )
             )
 
@@ -1052,15 +953,7 @@ def dataset_query(
     You should use sentry.snuba.discover instead.
     """
     if dataset is None:
-        dataset = detect_dataset(
-            dict(
-                dataset=dataset,
-                aggregations=aggregations,
-                conditions=conditions,
-                selected_columns=selected_columns,
-                groupby=groupby,
-            )
-        )
+        raise ValueError("A dataset is required, and is no longer automatically detected.")
 
     derived_columns = []
     if selected_columns:
diff --git a/tests/sentry/eventstore/test_base.py b/tests/sentry/eventstore/test_base.py
index 16ca650b9e..9dd2baa870 100644
--- a/tests/sentry/eventstore/test_base.py
+++ b/tests/sentry/eventstore/test_base.py
@@ -2,6 +2,7 @@ from __future__ import absolute_import
 
 import logging
 import mock
+import pytest
 import six
 
 from sentry import eventstore
@@ -71,8 +72,8 @@ class ServiceDelegationTest(TestCase, SnubaTestCase):
 
         self.transaction_event = self.store_event(data=event_data, project_id=self.project.id)
 
+    @pytest.mark.skip(reason="There is no longer a difference in underlying dataset.")
     def test_logs_differences(self):
-
         logger = logging.getLogger("sentry.eventstore")
 
         with mock.patch.object(logger, "info") as mock_logger:
diff --git a/tests/sentry/utils/test_snuba.py b/tests/sentry/utils/test_snuba.py
index fd71d109dd..74bec91a27 100644
--- a/tests/sentry/utils/test_snuba.py
+++ b/tests/sentry/utils/test_snuba.py
@@ -1,7 +1,6 @@
 from __future__ import absolute_import
 
 from datetime import datetime
-from mock import patch
 import pytest
 import pytz
 
@@ -14,7 +13,6 @@ from sentry.utils.snuba import (
     zerofill,
     get_json_type,
     get_snuba_column_name,
-    detect_dataset,
     transform_aliases_and_query,
     Dataset,
     SnubaQueryParams,
@@ -283,315 +281,6 @@ class TransformAliasesAndQueryTest(SnubaTestCase, TestCase):
         assert len(result["data"]) == 1
 
 
-class TransformAliasesAndQueryTransactionsTest(TestCase):
-    """
-    This test mocks snuba.raw_query because there is currently no
-    way to insert data into the transactions dataset during tests.
-    """
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_selected_columns_aliasing_in_function(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction"}, {"name": "duration"}],
-            "data": [{"transaction": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            aggregations=[
-                ["argMax", ["id", "transaction.duration"], "longest"],
-                ["uniq", "transaction", "uniq_transaction"],
-            ],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            aggregations=[
-                ["argMax", ["event_id", "duration"], "longest"],
-                ["uniq", "transaction_name", "uniq_transaction"],
-            ],
-            filter_keys={"project_id": [self.project.id]},
-            dataset=Dataset.Transactions,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            conditions=None,
-            groupby=None,
-            having=None,
-            orderby=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_selected_columns_opaque_string(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction"}, {"name": "p95"}],
-            "data": [{"transaction": "api.do_things", "p95": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction"],
-            aggregations=[
-                ["quantile(0.95)(duration)", "", "p95"],
-                ["uniq", "transaction", "uniq_transaction"],
-            ],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name"],
-            aggregations=[
-                ["quantile(0.95)(duration)", "", "p95"],
-                ["uniq", "transaction_name", "uniq_transaction"],
-            ],
-            filter_keys={"project_id": [self.project.id]},
-            dataset=Dataset.Transactions,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            conditions=None,
-            groupby=None,
-            having=None,
-            orderby=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_orderby_aliasing(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
-            "data": [{"transaction_name": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            filter_keys={"project_id": [self.project.id]},
-            orderby=["timestamp"],
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            filter_keys={"project_id": [self.project.id]},
-            dataset=Dataset.Transactions,
-            orderby=["finish_ts"],
-            aggregations=None,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            conditions=None,
-            groupby=None,
-            having=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_conditions_order_and_groupby_aliasing(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
-            "data": [{"transaction_name": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            conditions=[
-                ["transaction.duration", "=", 200],
-                ["time", ">", "2019-09-23"],
-                ["http.method", "=", "GET"],
-            ],
-            aggregations=[["count", "", "count"]],
-            groupby=["transaction.op"],
-            orderby=["-timestamp", "-count"],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            conditions=[
-                ["duration", "=", 200],
-                ["bucketed_end", ">", "2019-09-23"],
-                ["tags[http.method]", "=", "GET"],
-            ],
-            aggregations=[["count", "", "count"]],
-            filter_keys={"project_id": [self.project.id]},
-            groupby=["transaction_op"],
-            orderby=["-finish_ts", "-count"],
-            dataset=Dataset.Transactions,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            having=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_conditions_nested_function_aliasing(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}],
-            "data": [{"transaction_name": "api.do_things"}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction"],
-            conditions=[
-                ["event.type", "=", "transaction"],
-                ["match", [["ifNull", ["tags[user_email]", ""]], "'(?i)^.*\@sentry\.io$'"]],
-                [["positionCaseInsensitive", ["message", "'recent-searches'"]], "!=", 0],
-            ],
-            aggregations=[["count", "", "count"]],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name"],
-            conditions=[
-                ["match", [["ifNull", ["tags[user_email]", ""]], "'(?i)^.*\@sentry\.io$'"]],
-                [["positionCaseInsensitive", ["transaction_name", "'recent-searches'"]], "!=", 0],
-            ],
-            aggregations=[["count", "", "count"]],
-            filter_keys={"project_id": [self.project.id]},
-            dataset=Dataset.Transactions,
-            groupby=None,
-            orderby=None,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            having=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_condition_removal(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
-            "data": [{"transaction_name": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            conditions=[["event.type", "=", "transaction"], ["duration", ">", 200]],
-            groupby=["transaction.op"],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            conditions=[["duration", ">", 200]],
-            filter_keys={"project_id": [self.project.id]},
-            groupby=["transaction_op"],
-            dataset=Dataset.Transactions,
-            aggregations=None,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            having=None,
-            orderby=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_condition_not_remove_type_csp(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
-            "data": [{"transaction_name": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            conditions=[
-                ["event.type", "=", "transaction"],
-                ["type", "=", "csp"],
-                ["duration", ">", 200],
-            ],
-            groupby=["transaction.op"],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            conditions=[["tags[type]", "=", "csp"], ["duration", ">", 200]],
-            filter_keys={"project_id": [self.project.id]},
-            groupby=["transaction_op"],
-            dataset=Dataset.Transactions,
-            aggregations=None,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            having=None,
-            orderby=None,
-        )
-
-    @patch("sentry.utils.snuba.raw_query")
-    def test_condition_transform(self, mock_query):
-        mock_query.return_value = {
-            "meta": [{"name": "transaction_name"}, {"name": "duration"}],
-            "data": [{"transaction_name": "api.do_things", "duration": 200}],
-        }
-        transform_aliases_and_query(
-            selected_columns=["transaction", "transaction.duration"],
-            conditions=[["http_method", "=", "GET"], ["geo.country_code", "=", "CA"]],
-            groupby=["transaction.op"],
-            filter_keys={"project_id": [self.project.id]},
-        )
-        mock_query.assert_called_with(
-            selected_columns=["transaction_name", "duration"],
-            conditions=[
-                ["tags[http_method]", "=", "GET"],
-                ["contexts[geo.country_code]", "=", "CA"],
-            ],
-            filter_keys={"project_id": [self.project.id]},
-            groupby=["transaction_op"],
-            dataset=Dataset.Transactions,
-            aggregations=None,
-            arrayjoin=None,
-            end=None,
-            start=None,
-            having=None,
-            orderby=None,
-        )
-
-
-class DetectDatasetTest(TestCase):
-    def test_dataset_key(self):
-        query = {"dataset": Dataset.Events, "conditions": [["event.type", "=", "transaction"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-    def test_event_type_condition(self):
-        query = {"conditions": [["event.type", "=", "transaction"]]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-        query = {"conditions": [["event.type", "=", "error"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"conditions": [["event.type", "=", "transaction"]]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-        query = {"conditions": [["event.type", "=", "error"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"conditions": [["type", "!=", "transactions"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-    def test_conditions(self):
-        query = {"conditions": [["transaction", "=", "api.do_thing"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"conditions": [["transaction.duration", ">", "3"]]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-        # Internal aliases are treated as tags
-        query = {"conditions": [["duration", ">", "3"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-    def test_conditions_aliased(self):
-        query = {"conditions": [["duration", ">", "3"]]}
-        assert detect_dataset(query, aliased_conditions=True) == Dataset.Transactions
-
-        # Not an internal alias
-        query = {"conditions": [["transaction.duration", ">", "3"]]}
-        assert detect_dataset(query, aliased_conditions=True) == Dataset.Events
-
-    def test_selected_columns(self):
-        query = {"selected_columns": ["id", "message"]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"selected_columns": ["id", "transaction", "transaction.duration"]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-    def test_aggregations(self):
-        query = {"aggregations": [["argMax", ["id", "timestamp"], "latest_event"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"aggregations": [["argMax", ["id", "duration"], "longest"]]}
-        assert detect_dataset(query) == Dataset.Events
-
-        query = {"aggregations": [["quantile(0.95)", "transaction.duration", "p95_duration"]]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-        query = {"aggregations": [["uniq", "transaction.op", "uniq_transaction_op"]]}
-        assert detect_dataset(query) == Dataset.Transactions
-
-
 class PrepareQueryParamsTest(TestCase):
     def test_events_dataset_with_project_id(self):
         query_params = SnubaQueryParams(
diff --git a/tests/snuba/api/endpoints/test_organization_events.py b/tests/snuba/api/endpoints/test_organization_events.py
index 336c8f3d23..e0dde1119b 100644
--- a/tests/snuba/api/endpoints/test_organization_events.py
+++ b/tests/snuba/api/endpoints/test_organization_events.py
@@ -151,18 +151,6 @@ class OrganizationEventsEndpointTest(APITestCase, SnubaTestCase):
             == "Parse error: 'search' (column 4). This is commonly caused by unmatched-parentheses. Enclose any text in double quotes."
         )
 
-    def test_invalid_search_referencing_transactions(self):
-        self.login_as(user=self.user)
-        project = self.create_project()
-        url = reverse(
-            "sentry-api-0-organization-events",
-            kwargs={"organization_slug": project.organization.slug},
-        )
-        response = self.client.get(url, {"query": "transaction.duration:>200"}, format="json")
-
-        assert response.status_code == 400, response.content
-        assert "cannot reference non-events data" in response.data["detail"]
-
     def test_project_filtering(self):
         user = self.create_user(is_staff=False, is_superuser=False)
         org = self.create_organization()
diff --git a/tests/snuba/test_util.py b/tests/snuba/test_util.py
index 21984d2139..f5047ff5e8 100644
--- a/tests/snuba/test_util.py
+++ b/tests/snuba/test_util.py
@@ -63,16 +63,3 @@ class SnubaUtilTest(TestCase, SnubaTestCase):
                 assert snuba.OVERRIDE_OPTIONS == {"foo": 2, "consistent": False}
             assert snuba.OVERRIDE_OPTIONS == {"foo": 1, "consistent": False}
         assert snuba.OVERRIDE_OPTIONS == {"consistent": False}
-
-    def test_valid_orderby(self):
-        assert snuba.valid_orderby("event.type")
-        assert snuba.valid_orderby("project.id")
-        assert snuba.valid_orderby(["event.type", "-id"])
-        assert not snuba.valid_orderby("project.name")
-        assert not snuba.valid_orderby("issue_count")
-
-        extra_fields = ["issue_count", "event_count"]
-        assert snuba.valid_orderby(["issue_count", "-timestamp"], extra_fields)
-        assert snuba.valid_orderby("issue_count", extra_fields)
-        assert not snuba.valid_orderby(["invalid", "issue_count"], extra_fields)
-        assert not snuba.valid_orderby(["issue_count", "invalid"], extra_fields)
