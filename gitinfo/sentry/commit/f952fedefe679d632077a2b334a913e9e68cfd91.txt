commit f952fedefe679d632077a2b334a913e9e68cfd91
Author: josh <josh@jrl.ninja>
Date:   Mon Apr 13 20:21:51 2020 +0000

    ci: upgrade, apply, and require pre-commit (#18176)

diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
index 31b418dfb9..3e89b780ca 100644
--- a/.pre-commit-config.yaml
+++ b/.pre-commit-config.yaml
@@ -1,6 +1,7 @@
 exclude: >
     (?x)(
         LICENSE$|
+        ^bin/yarn$|
         \.snap$|
         \.map$|
         \.map\.js$|
@@ -21,13 +22,13 @@ repos:
       files: \.py$
       log_file: '.artifacts/flake8.pycodestyle.log'
 -   repo: https://github.com/ambv/black
-    rev: 19.3b0
+    rev: 19.10b0
     hooks:
     - id: black
       types: [python]
-      exclude: (south_migrations/|migrations/)
+      exclude: (migrations/)
 -   repo: git://github.com/pre-commit/pre-commit-hooks
-    rev: v1.3.0
+    rev: v2.5.0
     hooks:
     - id: check-case-conflict
     - id: check-executables-have-shebangs
diff --git a/.travis.yml b/.travis.yml
index f7f893ddbc..e98f0d6634 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -155,29 +155,29 @@ matrix:
       name: 'Linter (Javascript)'
       env: TEST_SUITE=lint-js
       install:
-        # XXX: Under a "generic" language environment, travis pyenv is still present.
-        # .python-version will make it error because there is no pyenv python installed.
-        - rm .python-version
         - find "$NODE_DIR" -type d -empty -delete
+        # Under a "generic" language environment, this will make travis pyenv error because there
+        # is no pyenv python installed.
+        - rm .python-version
         - nvm install
         - ./bin/yarn install --frozen-lockfile
 
-    # Proactive linting on 3.7 during the porting process
     - python: 3.7
-      name: 'Linter (Python) and dependency scanning'
+      name: 'pre-commit hooks (includes python linting + format check) and dependency scanning'
       install:
         # XXX: this must be synced with requirements-dev.txt
         - pip install 'sentry-flake8==0.3.0'
-      # configuration for flake8 can be found in setup.cfg
+        - SENTRY_NO_VIRTUALENV_CREATION=1 make setup-git
       script:
-        - flake8
-        # If linting was good, then we scan dependencies.
+        # black is going to rewrite the files in-place.
+        # i need to either define a second hook that only runs --check --diff and conditionally run that,
+        # or i need to figure out how to pass additional args to a hook from the pre-commit invocation.
+        - pre-commit run --all-files
+        # If pre-commit was good, then we scan dependencies.
         # Note that this isn't moved to an after_script since that would override what we already have.
-        # XXX: force six to what sentry needs from what is installed by travis 3.7's virtualenv to avoid version conflict
-        # This is fixed with make setup-git in https://github.com/getsentry/sentry/pull/18176
-        - pip install "six>=1.10.0,<1.11.0"
         # XXX: ideally we don't have to install sentry in its entirety just to scan dependencies...
         # see the note in bin/scan on why this needs to be done for now.
+        # Note that this will mostly noop because we pull in a cached venv from Travis.
         - SENTRY_LIGHT_BUILD=1 SENTRY_PYTHON3=1 pip install -e .
         - pip install safety
         # XXX: fun fact, as of April 2020 travis preinstalls numpy 1.15.4 in their 3.7 environments
diff --git a/bin/load-mocks b/bin/load-mocks
index 5b9267a4c3..47c5d5ec75 100755
--- a/bin/load-mocks
+++ b/bin/load-mocks
@@ -112,7 +112,7 @@ def create_sample_event(*args, **kwargs):
     try:
         event = _create_sample_event(*args, **kwargs)
     except HashDiscarded as e:
-        print ("> Skipping Event: {}".format(e.message))  # NOQA
+        print("> Skipping Event: {}".format(e.message))  # NOQA
     else:
         if event is not None:
             features.record([event])
@@ -346,9 +346,9 @@ def main(num_events=1, extra_events=False):
 
     if settings.SENTRY_SINGLE_ORGANIZATION:
         org = Organization.get_default()
-        print ("Mocking org {}".format(org.name))  # NOQA
+        print("Mocking org {}".format(org.name))  # NOQA
     else:
-        print ("Mocking org {}".format("Default"))  # NOQA
+        print("Mocking org {}".format("Default"))  # NOQA
         org, _ = Organization.objects.get_or_create(slug="default")
 
     OrganizationMember.objects.get_or_create(
@@ -360,11 +360,11 @@ def main(num_events=1, extra_events=False):
     )
 
     for team_name, project_names in mocks:
-        print ("> Mocking team {}".format(team_name))  # NOQA
+        print("> Mocking team {}".format(team_name))  # NOQA
         team, _ = Team.objects.get_or_create(name=team_name, defaults={"organization": org})
 
         for project_name in project_names:
-            print ("  > Mocking project {}".format(project_name))  # NOQA
+            print("  > Mocking project {}".format(project_name))  # NOQA
             project, _ = Project.objects.get_or_create(
                 name=project_name,
                 defaults={
@@ -679,7 +679,7 @@ def main(num_events=1, extra_events=False):
             except AlertRuleNameAlreadyUsedError:
                 pass
 
-            print ("    > Loading time series data".format(project_name))  # NOQA
+            print("    > Loading time series data".format(project_name))  # NOQA
             create_sample_time_series(event1, release=release)
             create_sample_time_series(event2, release=release)
             create_sample_time_series(event3)
@@ -687,7 +687,7 @@ def main(num_events=1, extra_events=False):
             create_sample_time_series(event5, release=release)
 
             if hasattr(buffer, "process_pending"):
-                print ("    > Processing pending buffers")  # NOQA
+                print("    > Processing pending buffers")  # NOQA
                 buffer.process_pending()
 
             mocks_loaded.send(project=project, sender=__name__)
diff --git a/bin/mock-event b/bin/mock-event
index 83b6715ca9..01c43841ea 100755
--- a/bin/mock-event
+++ b/bin/mock-event
@@ -28,7 +28,7 @@ def main(project, sample_type):
     if not project.first_event:
         project.update(first_event=timezone.now())
 
-    print ("> Created event {}".format(event.event_id))
+    print("> Created event {}".format(event.event_id))
 
 
 if __name__ == "__main__":
diff --git a/docker/sentry.conf.py b/docker/sentry.conf.py
index 1146f40858..2ad521012f 100644
--- a/docker/sentry.conf.py
+++ b/docker/sentry.conf.py
@@ -41,32 +41,17 @@ import os.path
 CONF_ROOT = os.path.dirname(__file__)
 env = os.environ.get
 
-postgres = env('SENTRY_POSTGRES_HOST') or (env('POSTGRES_PORT_5432_TCP_ADDR') and 'postgres')
+postgres = env("SENTRY_POSTGRES_HOST") or (env("POSTGRES_PORT_5432_TCP_ADDR") and "postgres")
 if postgres:
     DATABASES = {
-        'default': {
-            'ENGINE': 'sentry.db.postgres',
-            'NAME': (
-                env('SENTRY_DB_NAME')
-                or env('POSTGRES_ENV_POSTGRES_USER')
-                or 'postgres'
-            ),
-            'USER': (
-                env('SENTRY_DB_USER')
-                or env('POSTGRES_ENV_POSTGRES_USER')
-                or 'postgres'
-            ),
-            'PASSWORD': (
-                env('SENTRY_DB_PASSWORD')
-                or env('POSTGRES_ENV_POSTGRES_PASSWORD')
-                or ''
-            ),
-            'HOST': postgres,
-            'PORT': (
-                env('SENTRY_POSTGRES_PORT')
-                or ''
-            ),
-        },
+        "default": {
+            "ENGINE": "sentry.db.postgres",
+            "NAME": (env("SENTRY_DB_NAME") or env("POSTGRES_ENV_POSTGRES_USER") or "postgres"),
+            "USER": (env("SENTRY_DB_USER") or env("POSTGRES_ENV_POSTGRES_USER") or "postgres"),
+            "PASSWORD": (env("SENTRY_DB_PASSWORD") or env("POSTGRES_ENV_POSTGRES_PASSWORD") or ""),
+            "HOST": postgres,
+            "PORT": (env("SENTRY_POSTGRES_PORT") or ""),
+        }
     }
 
 # You should not change this setting after your database has been created
@@ -82,7 +67,7 @@ SENTRY_USE_BIG_INTS = True
 
 # Instruct Sentry that this install intends to be run by a single organization
 # and thus various UI optimizations should be enabled.
-SENTRY_SINGLE_ORGANIZATION = Bool(env('SENTRY_SINGLE_ORGANIZATION', True))
+SENTRY_SINGLE_ORGANIZATION = Bool(env("SENTRY_SINGLE_ORGANIZATION", True))
 
 #########
 # Redis #
@@ -91,28 +76,32 @@ SENTRY_SINGLE_ORGANIZATION = Bool(env('SENTRY_SINGLE_ORGANIZATION', True))
 # Generic Redis configuration used as defaults for various things including:
 # Buffers, Quotas, TSDB
 
-redis = env('SENTRY_REDIS_HOST') or (env('REDIS_PORT_6379_TCP_ADDR') and 'redis')
+redis = env("SENTRY_REDIS_HOST") or (env("REDIS_PORT_6379_TCP_ADDR") and "redis")
 if not redis:
-    raise Exception('Error: REDIS_PORT_6379_TCP_ADDR (or SENTRY_REDIS_HOST) is undefined, did you forget to `--link` a redis container?')
-
-redis_password = env('SENTRY_REDIS_PASSWORD') or ''
-redis_port = env('SENTRY_REDIS_PORT') or '6379'
-redis_db = env('SENTRY_REDIS_DB') or '0'
-
-SENTRY_OPTIONS.update({
-    'redis.clusters': {
-        'default': {
-            'hosts': {
-                0: {
-                    'host': redis,
-                    'password': redis_password,
-                    'port': redis_port,
-                    'db': redis_db,
-                },
-            },
-        },
-    },
-})
+    raise Exception(
+        "Error: REDIS_PORT_6379_TCP_ADDR (or SENTRY_REDIS_HOST) is undefined, did you forget to `--link` a redis container?"
+    )
+
+redis_password = env("SENTRY_REDIS_PASSWORD") or ""
+redis_port = env("SENTRY_REDIS_PORT") or "6379"
+redis_db = env("SENTRY_REDIS_DB") or "0"
+
+SENTRY_OPTIONS.update(
+    {
+        "redis.clusters": {
+            "default": {
+                "hosts": {
+                    0: {
+                        "host": redis,
+                        "password": redis_password,
+                        "port": redis_port,
+                        "db": redis_db,
+                    }
+                }
+            }
+        }
+    }
+)
 
 #########
 # Cache #
@@ -121,22 +110,19 @@ SENTRY_OPTIONS.update({
 # Sentry currently utilizes two separate mechanisms. While CACHES is not a
 # requirement, it will optimize several high throughput patterns.
 
-memcached = env('SENTRY_MEMCACHED_HOST') or (env('MEMCACHED_PORT_11211_TCP_ADDR') and 'memcached')
+memcached = env("SENTRY_MEMCACHED_HOST") or (env("MEMCACHED_PORT_11211_TCP_ADDR") and "memcached")
 if memcached:
-    memcached_port = (
-        env('SENTRY_MEMCACHED_PORT')
-        or '11211'
-    )
+    memcached_port = env("SENTRY_MEMCACHED_PORT") or "11211"
     CACHES = {
-        'default': {
-            'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
-            'LOCATION': [memcached + ':' + memcached_port],
-            'TIMEOUT': 3600,
+        "default": {
+            "BACKEND": "django.core.cache.backends.memcached.MemcachedCache",
+            "LOCATION": [memcached + ":" + memcached_port],
+            "TIMEOUT": 3600,
         }
     }
 
 # A primary cache is required for things such as processing events
-SENTRY_CACHE = 'sentry.cache.redis.RedisCache'
+SENTRY_CACHE = "sentry.cache.redis.RedisCache"
 
 #########
 # Queue #
@@ -146,26 +132,21 @@ SENTRY_CACHE = 'sentry.cache.redis.RedisCache'
 # information on configuring your queue broker and workers. Sentry relies
 # on a Python framework called Celery to manage queues.
 
-rabbitmq = env('SENTRY_RABBITMQ_HOST') or (env('RABBITMQ_PORT_5672_TCP_ADDR') and 'rabbitmq')
+rabbitmq = env("SENTRY_RABBITMQ_HOST") or (env("RABBITMQ_PORT_5672_TCP_ADDR") and "rabbitmq")
 
 if rabbitmq:
     BROKER_URL = (
-        'amqp://' + (
-            env('SENTRY_RABBITMQ_USERNAME')
-            or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_USER')
-            or 'guest'
-        ) + ':' + (
-            env('SENTRY_RABBITMQ_PASSWORD')
-            or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_PASS')
-            or 'guest'
-        ) + '@' + rabbitmq + '/' + (
-            env('SENTRY_RABBITMQ_VHOST')
-            or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_VHOST')
-            or '/'
-        )
+        "amqp://"
+        + (env("SENTRY_RABBITMQ_USERNAME") or env("RABBITMQ_ENV_RABBITMQ_DEFAULT_USER") or "guest")
+        + ":"
+        + (env("SENTRY_RABBITMQ_PASSWORD") or env("RABBITMQ_ENV_RABBITMQ_DEFAULT_PASS") or "guest")
+        + "@"
+        + rabbitmq
+        + "/"
+        + (env("SENTRY_RABBITMQ_VHOST") or env("RABBITMQ_ENV_RABBITMQ_DEFAULT_VHOST") or "/")
     )
 else:
-    BROKER_URL = 'redis://:' + redis_password + '@' + redis + ':' + redis_port + '/' + redis_db
+    BROKER_URL = "redis://:" + redis_password + "@" + redis + ":" + redis_port + "/" + redis_db
 
 
 ###############
@@ -175,7 +156,7 @@ else:
 # Rate limits apply to notification handlers and are enforced per-project
 # automatically.
 
-SENTRY_RATELIMITER = 'sentry.ratelimits.redis.RedisRateLimiter'
+SENTRY_RATELIMITER = "sentry.ratelimits.redis.RedisRateLimiter"
 
 ##################
 # Update Buffers #
@@ -186,7 +167,7 @@ SENTRY_RATELIMITER = 'sentry.ratelimits.redis.RedisRateLimiter'
 # numbers of the same events being sent to the API in a short amount of time.
 # (read: if you send any kind of real data to Sentry, you should enable buffers)
 
-SENTRY_BUFFER = 'sentry.buffer.redis.RedisBuffer'
+SENTRY_BUFFER = "sentry.buffer.redis.RedisBuffer"
 
 ##########
 # Quotas #
@@ -195,7 +176,7 @@ SENTRY_BUFFER = 'sentry.buffer.redis.RedisBuffer'
 # Quotas allow you to rate limit individual projects or the Sentry install as
 # a whole.
 
-SENTRY_QUOTAS = 'sentry.quotas.redis.RedisQuota'
+SENTRY_QUOTAS = "sentry.quotas.redis.RedisQuota"
 
 ########
 # TSDB #
@@ -204,7 +185,7 @@ SENTRY_QUOTAS = 'sentry.quotas.redis.RedisQuota'
 # The TSDB is used for building charts as well as making things like per-rate
 # alerts possible.
 
-SENTRY_TSDB = 'sentry.tsdb.redis.RedisTSDB'
+SENTRY_TSDB = "sentry.tsdb.redis.RedisTSDB"
 
 ###########
 # Digests #
@@ -212,7 +193,7 @@ SENTRY_TSDB = 'sentry.tsdb.redis.RedisTSDB'
 
 # The digest backend powers notification summaries.
 
-SENTRY_DIGESTS = 'sentry.digests.backends.redis.RedisBackend'
+SENTRY_DIGESTS = "sentry.digests.backends.redis.RedisBackend"
 
 ##############
 # Web Server #
@@ -221,12 +202,12 @@ SENTRY_DIGESTS = 'sentry.digests.backends.redis.RedisBackend'
 # If you're using a reverse SSL proxy, you should enable the X-Forwarded-Proto
 # header and set `SENTRY_USE_SSL=1`
 
-if Bool(env('SENTRY_USE_SSL', False)):
-    SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
+if Bool(env("SENTRY_USE_SSL", False)):
+    SECURE_PROXY_SSL_HEADER = ("HTTP_X_FORWARDED_PROTO", "https")
     SESSION_COOKIE_SECURE = True
     CSRF_COOKIE_SECURE = True
 
-SENTRY_WEB_HOST = '0.0.0.0'
+SENTRY_WEB_HOST = "0.0.0.0"
 SENTRY_WEB_PORT = 9000
 SENTRY_WEB_OPTIONS = {
     # 'workers': 1,  # the number of web workers
@@ -237,46 +218,48 @@ SENTRY_WEB_OPTIONS = {
 ###############
 
 
-email = env('SENTRY_EMAIL_HOST') or (env('SMTP_PORT_25_TCP_ADDR') and 'smtp')
+email = env("SENTRY_EMAIL_HOST") or (env("SMTP_PORT_25_TCP_ADDR") and "smtp")
 if email:
-    SENTRY_OPTIONS['mail.backend'] = 'smtp'
-    SENTRY_OPTIONS['mail.host'] = email
-    SENTRY_OPTIONS['mail.password'] = env('SENTRY_EMAIL_PASSWORD') or ''
-    SENTRY_OPTIONS['mail.username'] = env('SENTRY_EMAIL_USER') or ''
-    SENTRY_OPTIONS['mail.port'] = int(env('SENTRY_EMAIL_PORT') or 25)
-    SENTRY_OPTIONS['mail.use-tls'] = Bool(env('SENTRY_EMAIL_USE_TLS', False))
+    SENTRY_OPTIONS["mail.backend"] = "smtp"
+    SENTRY_OPTIONS["mail.host"] = email
+    SENTRY_OPTIONS["mail.password"] = env("SENTRY_EMAIL_PASSWORD") or ""
+    SENTRY_OPTIONS["mail.username"] = env("SENTRY_EMAIL_USER") or ""
+    SENTRY_OPTIONS["mail.port"] = int(env("SENTRY_EMAIL_PORT") or 25)
+    SENTRY_OPTIONS["mail.use-tls"] = Bool(env("SENTRY_EMAIL_USE_TLS", False))
 else:
-    SENTRY_OPTIONS['mail.backend'] = 'dummy'
+    SENTRY_OPTIONS["mail.backend"] = "dummy"
 
 # The email address to send on behalf of
-SENTRY_OPTIONS['mail.from'] = env('SENTRY_SERVER_EMAIL') or 'root@localhost'
+SENTRY_OPTIONS["mail.from"] = env("SENTRY_SERVER_EMAIL") or "root@localhost"
 
 # If you're using mailgun for inbound mail, set your API key and configure a
 # route to forward to /api/hooks/mailgun/inbound/
-SENTRY_OPTIONS['mail.mailgun-api-key'] = env('SENTRY_MAILGUN_API_KEY') or ''
+SENTRY_OPTIONS["mail.mailgun-api-key"] = env("SENTRY_MAILGUN_API_KEY") or ""
 
 # If you specify a MAILGUN_API_KEY, you definitely want EMAIL_REPLIES
-if SENTRY_OPTIONS['mail.mailgun-api-key']:
-    SENTRY_OPTIONS['mail.enable-replies'] = True
+if SENTRY_OPTIONS["mail.mailgun-api-key"]:
+    SENTRY_OPTIONS["mail.enable-replies"] = True
 else:
-    SENTRY_OPTIONS['mail.enable-replies'] = Bool(env('SENTRY_ENABLE_EMAIL_REPLIES', False))
+    SENTRY_OPTIONS["mail.enable-replies"] = Bool(env("SENTRY_ENABLE_EMAIL_REPLIES", False))
 
-if SENTRY_OPTIONS['mail.enable-replies']:
-    SENTRY_OPTIONS['mail.reply-hostname'] = env('SENTRY_SMTP_HOSTNAME') or ''
+if SENTRY_OPTIONS["mail.enable-replies"]:
+    SENTRY_OPTIONS["mail.reply-hostname"] = env("SENTRY_SMTP_HOSTNAME") or ""
 
 # If this value ever becomes compromised, it's important to regenerate your
 # SENTRY_SECRET_KEY. Changing this value will result in all current sessions
 # being invalidated.
-secret_key = env('SENTRY_SECRET_KEY')
+secret_key = env("SENTRY_SECRET_KEY")
 if not secret_key:
-    raise Exception('Error: SENTRY_SECRET_KEY is undefined, run `generate-secret-key` and set to -e SENTRY_SECRET_KEY')
+    raise Exception(
+        "Error: SENTRY_SECRET_KEY is undefined, run `generate-secret-key` and set to -e SENTRY_SECRET_KEY"
+    )
 
-if 'SENTRY_RUNNING_UWSGI' not in os.environ and len(secret_key) < 32:
-    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
-    print('!!                    CAUTION                       !!')
-    print('!! Your SENTRY_SECRET_KEY is potentially insecure.  !!')
-    print('!!    We recommend at least 32 characters long.     !!')
-    print('!!     Regenerate with `generate-secret-key`.       !!')
-    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')
+if "SENTRY_RUNNING_UWSGI" not in os.environ and len(secret_key) < 32:
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
+    print("!!                    CAUTION                       !!")
+    print("!! Your SENTRY_SECRET_KEY is potentially insecure.  !!")
+    print("!!    We recommend at least 32 characters long.     !!")
+    print("!!     Regenerate with `generate-secret-key`.       !!")
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
 
-SENTRY_OPTIONS['system.secret-key'] = secret_key
+SENTRY_OPTIONS["system.secret-key"] = secret_key
diff --git a/src/sentry/api/serializers/models/release.py b/src/sentry/api/serializers/models/release.py
index 8ad69aa805..e75a6065e0 100644
--- a/src/sentry/api/serializers/models/release.py
+++ b/src/sentry/api/serializers/models/release.py
@@ -241,9 +241,11 @@ class ReleaseSerializer(Serializer):
                 project, item_list
             )
         else:
-            first_seen, last_seen, issue_counts_by_release = self.__get_release_data_with_environment(
-                project, item_list, environment
-            )
+            (
+                first_seen,
+                last_seen,
+                issue_counts_by_release,
+            ) = self.__get_release_data_with_environment(project, item_list, environment)
 
         owners = {
             d["id"]: d for d in serialize(set(i.owner for i in item_list if i.owner_id), user)
diff --git a/src/sentry/auth/helper.py b/src/sentry/auth/helper.py
index 42ce6bce9b..1409026415 100644
--- a/src/sentry/auth/helper.py
+++ b/src/sentry/auth/helper.py
@@ -150,7 +150,9 @@ def handle_existing_identity(
         return HttpResponseRedirect(auth.get_login_redirect(request))
 
     state.clear()
-    metrics.incr("sso.login-success", tags={"provider": provider.key}, skip_internal=False, sample_rate=1.0)
+    metrics.incr(
+        "sso.login-success", tags={"provider": provider.key}, skip_internal=False, sample_rate=1.0
+    )
 
     return HttpResponseRedirect(auth.get_login_redirect(request))
 
diff --git a/src/sentry/eventstream/kafka/consumer.py b/src/sentry/eventstream/kafka/consumer.py
index 7da098d2c9..f49aab2381 100644
--- a/src/sentry/eventstream/kafka/consumer.py
+++ b/src/sentry/eventstream/kafka/consumer.py
@@ -174,9 +174,10 @@ class SynchronizedConsumer(object):
         self.__partition_state_manager = SynchronizedPartitionStateManager(
             self.__on_partition_state_change
         )
-        self.__commit_log_consumer, self.__commit_log_consumer_stop_request = (
-            self.__start_commit_log_consumer()
-        )
+        (
+            self.__commit_log_consumer,
+            self.__commit_log_consumer_stop_request,
+        ) = self.__start_commit_log_consumer()
 
         self.__positions = {}
 
diff --git a/src/sentry/incidents/subscription_processor.py b/src/sentry/incidents/subscription_processor.py
index aa00138006..711f50942f 100644
--- a/src/sentry/incidents/subscription_processor.py
+++ b/src/sentry/incidents/subscription_processor.py
@@ -61,9 +61,11 @@ class SubscriptionProcessor(object):
         self.triggers = AlertRuleTrigger.objects.get_for_alert_rule(self.alert_rule)
         self.triggers.sort(key=lambda trigger: trigger.alert_threshold)
 
-        self.last_update, self.trigger_alert_counts, self.trigger_resolve_counts = get_alert_rule_stats(
-            self.alert_rule, self.subscription, self.triggers
-        )
+        (
+            self.last_update,
+            self.trigger_alert_counts,
+            self.trigger_resolve_counts,
+        ) = get_alert_rule_stats(self.alert_rule, self.subscription, self.triggers)
         self.orig_trigger_alert_counts = deepcopy(self.trigger_alert_counts)
         self.orig_trigger_resolve_counts = deepcopy(self.trigger_resolve_counts)
 
diff --git a/src/sentry/middleware/stats.py b/src/sentry/middleware/stats.py
index 722be495b5..12be395a99 100644
--- a/src/sentry/middleware/stats.py
+++ b/src/sentry/middleware/stats.py
@@ -29,7 +29,9 @@ class ResponseCodeMiddleware(object):
 
 class RequestTimingMiddleware(object):
     allowed_methods = ("POST", "GET")
-    allowed_paths = getattr(settings, "SENTRY_REQUEST_METRIC_ALLOWED_PATHS", ("sentry.web.api", "sentry.api.endpoints"))  # Store endpoints
+    allowed_paths = getattr(
+        settings, "SENTRY_REQUEST_METRIC_ALLOWED_PATHS", ("sentry.web.api", "sentry.api.endpoints")
+    )  # Store endpoints
 
     def process_view(self, request, view_func, view_args, view_kwargs):
         if not hasattr(request, "_metric_tags"):
diff --git a/src/sentry/ownership/grammar.py b/src/sentry/ownership/grammar.py
index 43b62feaf7..14b14d83ec 100644
--- a/src/sentry/ownership/grammar.py
+++ b/src/sentry/ownership/grammar.py
@@ -147,7 +147,7 @@ class OwnershipVisitor(NodeVisitor):
     def visit_matcher_tag(self, node, children):
         if not children:
             return "path"
-        tag, = children
+        (tag,) = children
         type, _ = tag
         return type[0].text
 
diff --git a/src/sentry/projectoptions/defaults.py b/src/sentry/projectoptions/defaults.py
index 097e6fd6a5..018ada4e51 100644
--- a/src/sentry/projectoptions/defaults.py
+++ b/src/sentry/projectoptions/defaults.py
@@ -44,7 +44,10 @@ register(key="sentry:default_loader_version", epoch_defaults={1: "4.x", 2: "5.x"
 # Default symbol sources.  The ios source does not exist by default and
 # will be skipped later.  The microsoft source exists by default and is
 # unlikely to be disabled.
-register(key="sentry:builtin_symbol_sources", epoch_defaults={1: ["ios"], 2: ["ios", "microsoft"], 5: ["ios", "microsoft", "android"]})
+register(
+    key="sentry:builtin_symbol_sources",
+    epoch_defaults={1: ["ios"], 2: ["ios", "microsoft"], 5: ["ios", "microsoft", "android"]},
+)
 
 # Default legacy-browsers filter
 register(key="filters:legacy-browsers", epoch_defaults={1: "0"})
diff --git a/src/sentry/tsdb/redis.py b/src/sentry/tsdb/redis.py
index 0e8ea2f20e..8d9794269c 100644
--- a/src/sentry/tsdb/redis.py
+++ b/src/sentry/tsdb/redis.py
@@ -340,7 +340,10 @@ class RedisTSDB(BaseTSDB):
                         for environment_id, promises in results.items():
                             total = sum([int(p.value) for p in promises if p.value])
                             if total:
-                                destination_hash_key, destination_hash_field = self.make_counter_key(
+                                (
+                                    destination_hash_key,
+                                    destination_hash_field,
+                                ) = self.make_counter_key(
                                     model, rollup, timestamp, destination, environment_id
                                 )
                                 client.hincrby(destination_hash_key, destination_hash_field, total)
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
index 220508df31..833642b60e 100644
--- a/src/sentry/utils/kafka.py
+++ b/src/sentry/utils/kafka.py
@@ -62,7 +62,7 @@ def create_batching_kafka_consumer(topic_names, worker, **options):
             )
         )
 
-    cluster_name, = cluster_names
+    (cluster_name,) = cluster_names
 
     bootstrap_servers = settings.KAFKA_CLUSTERS[cluster_name]["bootstrap.servers"]
     if not isinstance(bootstrap_servers, (list, tuple)):
diff --git a/src/sentry/utils/pytest/fixtures.py b/src/sentry/utils/pytest/fixtures.py
index b8d0721141..39d19dd30a 100644
--- a/src/sentry/utils/pytest/fixtures.py
+++ b/src/sentry/utils/pytest/fixtures.py
@@ -303,7 +303,7 @@ def insta_snapshot(request, log):
                 reference_file += ".new"
             with open(reference_file, "w") as f:
                 f.write(
-                    u"---\n%s\n---\n%s\n"
+                    "---\n%s\n---\n%s\n"
                     % (
                         yaml.safe_dump(
                             {
diff --git a/src/sentry/utils/retries.py b/src/sentry/utils/retries.py
index 50192864ca..015184efa5 100644
--- a/src/sentry/utils/retries.py
+++ b/src/sentry/utils/retries.py
@@ -55,7 +55,13 @@ class TimedRetryPolicy(RetryPolicy):
     """
 
     def __init__(
-        self, timeout, delay=None, exceptions=(Exception,), metric_instance=None, metric_tags=None, log_original_error=False
+        self,
+        timeout,
+        delay=None,
+        exceptions=(Exception,),
+        metric_instance=None,
+        metric_tags=None,
+        log_original_error=False,
     ):
         if delay is None:
             # 100ms +/- 50ms of randomized jitter
diff --git a/src/sentry/web/frontend/auth_login.py b/src/sentry/web/frontend/auth_login.py
index f7fb436bcd..b8e11a902c 100644
--- a/src/sentry/web/frontend/auth_login.py
+++ b/src/sentry/web/frontend/auth_login.py
@@ -77,7 +77,7 @@ class AuthLoginView(BaseView):
             request.POST if op == "register" else None,
             initial=initial,
             # Custom auto_id to avoid ID collision with AuthenticationForm.
-            auto_id="id_registration_%s"
+            auto_id="id_registration_%s",
         )
 
     def can_register(self, request):
diff --git a/src/sentry_plugins/bitbucket/README.rst b/src/sentry_plugins/bitbucket/README.rst
index de50bb3dc1..a1837377f2 100644
--- a/src/sentry_plugins/bitbucket/README.rst
+++ b/src/sentry_plugins/bitbucket/README.rst
@@ -11,4 +11,3 @@ Create OAuth consumer key and secret and then put
 
 
 into ``sentry.conf.py``
-
diff --git a/src/sentry_plugins/github/README.rst b/src/sentry_plugins/github/README.rst
index 2ab3b55b69..040b1e2f36 100644
--- a/src/sentry_plugins/github/README.rst
+++ b/src/sentry_plugins/github/README.rst
@@ -33,4 +33,3 @@ Caveats
 
 If you have multiple GitHub identities associated in Sentry, the plugin will just select
 one to use.
-
diff --git a/src/sentry_plugins/gitlab/README.rst b/src/sentry_plugins/gitlab/README.rst
index 8fd0385ef7..534e272d8a 100644
--- a/src/sentry_plugins/gitlab/README.rst
+++ b/src/sentry_plugins/gitlab/README.rst
@@ -6,4 +6,3 @@ token<https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html>`_)
 and click save changes.
 
 It's recommended to create a specific user for Sentry with only `Reporter` privileges on your projects.
-
diff --git a/tests/acceptance/test_organization_security_privacy.py b/tests/acceptance/test_organization_security_privacy.py
index 95ff78550e..f96fbe6e02 100644
--- a/tests/acceptance/test_organization_security_privacy.py
+++ b/tests/acceptance/test_organization_security_privacy.py
@@ -8,7 +8,9 @@ class OrganizationSecurityAndPrivacyTest(AcceptanceTestCase):
         self.browser.wait_until_not(".loading-indicator")
         if snapshot_name is not None:
             self.browser.snapshot("organization settings security and privacy -- " + snapshot_name)
-        assert self.browser.element_exists('[data-test-id="organization-settings-security-and-privacy"]')
+        assert self.browser.element_exists(
+            '[data-test-id="organization-settings-security-and-privacy"]'
+        )
 
     def renders_2fa_setting(self):
         return self.browser.element_exists("#require2FA")
diff --git a/tests/sentry/api/bases/test_integration.py b/tests/sentry/api/bases/test_integration.py
index 20b3b61bcb..9eeaa31792 100644
--- a/tests/sentry/api/bases/test_integration.py
+++ b/tests/sentry/api/bases/test_integration.py
@@ -15,9 +15,7 @@ class IntegrationEndpointTest(APITestCase):
     def test_handle_exception(self):
         exc = APIException("There was a problem!")
         exc.status_code = 400  # set the status code to 400 not possible to set in init
-        exc.code = (
-            400
-        )  # rest framework APIError is not compatible with integration APIError exception type
+        exc.code = 400  # rest framework APIError is not compatible with integration APIError exception type
         resp = self.endpoint.handle_exception(HttpRequest(), exc)
         assert resp.status_code == 400
         assert resp.exception is True
diff --git a/tests/sentry/api/endpoints/test_debug_files.py b/tests/sentry/api/endpoints/test_debug_files.py
index 4ae09a03e4..3cfd4ebfe9 100644
--- a/tests/sentry/api/endpoints/test_debug_files.py
+++ b/tests/sentry/api/endpoints/test_debug_files.py
@@ -190,7 +190,7 @@ class DebugFilesUploadTest(APITestCase):
 
         assert response.status_code == 200, response.content
 
-        dsym, = response.data
+        (dsym,) = response.data
         assert dsym["cpuName"] == "any"
         assert dsym["headers"] == {"Content-Type": "text/x-proguard+plain"}
         assert dsym["objectName"] == "proguard-mapping"
diff --git a/tests/sentry/api/endpoints/test_organization_details.py b/tests/sentry/api/endpoints/test_organization_details.py
index 9eb47c7be7..b28ba99046 100644
--- a/tests/sentry/api/endpoints/test_organization_details.py
+++ b/tests/sentry/api/endpoints/test_organization_details.py
@@ -278,7 +278,7 @@ class OrganizationUpdateTest(APITestCase):
             response = self.client.put(url, data=data)
             assert response.status_code == 200
 
-        option, = OrganizationOption.objects.filter(organization=org, key="sentry:trusted-relays")
+        (option,) = OrganizationOption.objects.filter(organization=org, key="sentry:trusted-relays")
 
         assert option.value == data["trustedRelays"]
         log = AuditLogEntry.objects.get(organization=org)
diff --git a/tests/sentry/api/endpoints/test_relay_projectconfigs.py b/tests/sentry/api/endpoints/test_relay_projectconfigs.py
index 6fa6a7a6c2..3157edb14a 100644
--- a/tests/sentry/api/endpoints/test_relay_projectconfigs.py
+++ b/tests/sentry/api/endpoints/test_relay_projectconfigs.py
@@ -126,7 +126,7 @@ def test_internal_relays_should_receive_full_configs(
     cfg = safe.get_path(result, "configs", six.text_type(default_project.id))
     assert safe.get_path(cfg, "disabled") is False
 
-    public_key, = cfg["publicKeys"]
+    (public_key,) = cfg["publicKeys"]
     assert public_key["publicKey"] == default_projectkey.public_key
     assert public_key["isEnabled"]
     assert "quotas" in public_key
@@ -214,7 +214,7 @@ def test_trusted_external_relays_should_receive_minimal_configs(
 
     cfg = safe.get_path(result, "configs", six.text_type(default_project.id))
     assert safe.get_path(cfg, "disabled") is False
-    public_key, = cfg["publicKeys"]
+    (public_key,) = cfg["publicKeys"]
     assert public_key["publicKey"] == default_projectkey.public_key
     assert public_key["isEnabled"]
     assert "quotas" not in public_key
@@ -289,8 +289,8 @@ def test_relay_projectconfig_cache_full_config(
         result, status_code = call_endpoint(full_config=True)
         assert status_code < 400
 
-    http_cfg, = six.itervalues(result["configs"])
-    call, = projectconfig_cache_set
+    (http_cfg,) = six.itervalues(result["configs"])
+    (call,) = projectconfig_cache_set
     assert len(call) == 1
     redis_cfg = call[six.text_type(default_project.id)]
 
@@ -310,7 +310,7 @@ def test_relay_nonexistent_project(call_endpoint, projectconfig_cache_set, task_
         result, status_code = call_endpoint(full_config=True, projects=[wrong_id])
         assert status_code < 400
 
-    http_cfg, = six.itervalues(result["configs"])
+    (http_cfg,) = six.itervalues(result["configs"])
     assert http_cfg == {"disabled": True}
 
     assert projectconfig_cache_set == [{six.text_type(wrong_id): http_cfg}]
@@ -328,7 +328,7 @@ def test_relay_disabled_project(
         result, status_code = call_endpoint(full_config=True, projects=[wrong_id])
         assert status_code < 400
 
-    http_cfg, = six.itervalues(result["configs"])
+    (http_cfg,) = six.itervalues(result["configs"])
     assert http_cfg == {"disabled": True}
 
     assert projectconfig_cache_set == [{six.text_type(wrong_id): http_cfg}]
diff --git a/tests/sentry/attachments/test_base.py b/tests/sentry/attachments/test_base.py
index c3f5146abe..6007e00346 100644
--- a/tests/sentry/attachments/test_base.py
+++ b/tests/sentry/attachments/test_base.py
@@ -42,7 +42,7 @@ def test_basic_chunked():
     att = CachedAttachment(key="c:foo", id=123, name="lol.txt", content_type="text/plain", chunks=3)
     cache.set("c:foo", [att])
 
-    att2, = cache.get("c:foo")
+    (att2,) = cache.get("c:foo")
     assert att2.key == att.key == "c:foo"
     assert att2.id == att.id == 123
     assert att2.data == att.data == b"Hello World! Bye."
@@ -58,7 +58,7 @@ def test_basic_unchunked():
     att = CachedAttachment(name="lol.txt", content_type="text/plain", data=b"Hello World! Bye.")
     cache.set("c:foo", [att])
 
-    att2, = cache.get("c:foo")
+    (att2,) = cache.get("c:foo")
     assert att2.key == att.key == "c:foo"
     assert att2.id == att.id == 0
     assert att2.data == att.data == b"Hello World! Bye."
diff --git a/tests/sentry/attachments/test_redis.py b/tests/sentry/attachments/test_redis.py
index afb4c04944..555e794419 100644
--- a/tests/sentry/attachments/test_redis.py
+++ b/tests/sentry/attachments/test_redis.py
@@ -58,7 +58,7 @@ def test_process_pending_one_batch(mocked_attachment_cache, mock_client):
     mock_client.data["c:1:foo:a"] = '[{"name":"foo.txt","content_type":"text/plain"}]'
     mock_client.data["c:1:foo:a:0"] = zlib.compress(b"Hello World!")
 
-    attachment, = mocked_attachment_cache.get("foo")
+    (attachment,) = mocked_attachment_cache.get("foo")
     assert attachment.meta() == {
         "id": 0,
         "type": "event.attachment",
@@ -74,7 +74,7 @@ def test_chunked(mocked_attachment_cache, mock_client):
     mock_client.data["c:1:foo:a:0:1"] = zlib.compress(b" This attachment is ")
     mock_client.data["c:1:foo:a:0:2"] = zlib.compress(b"chunked up.")
 
-    attachment, = mocked_attachment_cache.get("foo")
+    (attachment,) = mocked_attachment_cache.get("foo")
     assert attachment.meta() == {
         "id": 0,
         "chunks": 3,
diff --git a/tests/sentry/event_manager/test_validate_data.py b/tests/sentry/event_manager/test_validate_data.py
index 6e60ccf919..4fa57cf78f 100644
--- a/tests/sentry/event_manager/test_validate_data.py
+++ b/tests/sentry/event_manager/test_validate_data.py
@@ -250,7 +250,7 @@ def test_environment_too_long():
 def test_environment_invalid():
     data = validate_and_normalize({"environment": "a/b"})
     assert not data.get("environment")
-    error, = data["errors"]
+    (error,) = data["errors"]
     error["type"] == "invalid_data"
 
     assert error["name"] == "environment"
diff --git a/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_processing.py b/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_processing.py
index 38d83f738d..55b37aa6d8 100644
--- a/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_processing.py
+++ b/tests/sentry/ingest/ingest_consumer/test_ingest_consumer_processing.py
@@ -51,7 +51,7 @@ def test_deduplication_works(default_project, task_runner, preprocess_event):
             projects={default_project.id: default_project},
         )
 
-    kwargs, = preprocess_event
+    (kwargs,) = preprocess_event
     assert kwargs == {
         "cache_key": u"e:{event_id}:{project_id}".format(event_id=event_id, project_id=project_id),
         "data": payload,
@@ -123,7 +123,7 @@ def test_with_attachments(default_project, task_runner, missing_chunks, monkeypa
     )
 
     if not missing_chunks:
-        attachment, = persisted_attachments
+        (attachment,) = persisted_attachments
         assert attachment.file.type == "custom.attachment"
         assert attachment.file.headers == {"Content-Type": "text/plain"}
         file = attachment.file.getfile()
@@ -196,7 +196,7 @@ def test_individual_attachments(
     if not event_attachments:
         assert not attachments
     else:
-        attachment, = attachments
+        (attachment,) = attachments
         assert attachment.file.type == "event.attachment"
         assert attachment.file.headers == {"Content-Type": "application/octet-stream"}
         assert attachment.group_id == group_id
@@ -220,7 +220,7 @@ def test_userreport(default_project, monkeypatch):
     mgr.normalize()
     mgr.save(default_project.id)
 
-    evtuser, = EventUser.objects.all()
+    (evtuser,) = EventUser.objects.all()
     assert not evtuser.name
 
     assert not UserReport.objects.all()
@@ -242,10 +242,10 @@ def test_userreport(default_project, monkeypatch):
         projects={default_project.id: default_project},
     )
 
-    report, = UserReport.objects.all()
+    (report,) = UserReport.objects.all()
     assert report.comments == "hello world"
 
-    evtuser, = EventUser.objects.all()
+    (evtuser,) = EventUser.objects.all()
     assert evtuser.name == "Hans Gans"
 
 
@@ -281,10 +281,10 @@ def test_userreport_reverse_order(default_project, monkeypatch):
     mgr.normalize()
     mgr.save(default_project.id)
 
-    report, = UserReport.objects.all()
+    (report,) = UserReport.objects.all()
     assert report.comments == "hello world"
 
-    evtuser, = EventUser.objects.all()
+    (evtuser,) = EventUser.objects.all()
     # Event got saved after user report, and the sync only works in the
     # opposite direction. That's fine, we just accept it.
     assert evtuser.name is None
diff --git a/tests/sentry/integrations/bitbucket_server/testutils.py b/tests/sentry/integrations/bitbucket_server/testutils.py
index 8412226f34..f76492712c 100644
--- a/tests/sentry/integrations/bitbucket_server/testutils.py
+++ b/tests/sentry/integrations/bitbucket_server/testutils.py
@@ -42,16 +42,14 @@ COMMIT_CHANGELIST_EXAMPLE = {
                 "parent": "",
                 "name": "a.txt",
                 "extension": "txt",
-                "toString": "a.txt"
+                "toString": "a.txt",
             },
             "executable": False,
             "percentUnchanged": -1,
             "type": "MODIFY",
             "nodeType": "FILE",
             "srcExecutable": False,
-            "properties": {
-                "gitChangeType": "MODIFY"
-            }
+            "properties": {"gitChangeType": "MODIFY"},
         },
         {
             "path": {
@@ -59,16 +57,14 @@ COMMIT_CHANGELIST_EXAMPLE = {
                 "parent": "",
                 "name": "b.txt",
                 "extension": "txt",
-                "toString": "b.txt"
+                "toString": "b.txt",
             },
             "executable": False,
             "percentUnchanged": -1,
             "type": "ADD",
             "nodeType": "FILE",
             "srcExecutable": False,
-            "properties": {
-                "gitChangeType": "ADD"
-            }
+            "properties": {"gitChangeType": "ADD"},
         },
         {
             "path": {
@@ -76,16 +72,14 @@ COMMIT_CHANGELIST_EXAMPLE = {
                 "parent": "",
                 "name": "c.txt",
                 "extension": "txt",
-                "toString": "c.txt"
+                "toString": "c.txt",
             },
             "executable": False,
             "percentUnchanged": -1,
             "type": "DELETE",
             "nodeType": "FILE",
             "srcExecutable": False,
-            "properties": {
-                "gitChangeType": "DELETE"
-            }
+            "properties": {"gitChangeType": "DELETE"},
         },
         {
             "path": {
@@ -93,24 +87,22 @@ COMMIT_CHANGELIST_EXAMPLE = {
                 "parent": "",
                 "name": "d.txt",
                 "extension": "txt",
-                "toString": "d.txt"
+                "toString": "d.txt",
             },
             "srcPath": {
                 "components": ["d.txt"],
                 "parent": "",
                 "name": "e.txt",
                 "extension": "txt",
-                "toString": "e.txt"
+                "toString": "e.txt",
             },
             "executable": False,
             "percentUnchanged": -1,
             "type": "MOVE",
             "nodeType": "FILE",
             "srcExecutable": False,
-            "properties": {
-                "gitChangeType": "MOVE"
-            }
-        }
+            "properties": {"gitChangeType": "MOVE"},
+        },
     ]
 }
 
diff --git a/tests/sentry/integrations/gitlab/test_integration.py b/tests/sentry/integrations/gitlab/test_integration.py
index a0566219e4..bc0f2ccaa4 100644
--- a/tests/sentry/integrations/gitlab/test_integration.py
+++ b/tests/sentry/integrations/gitlab/test_integration.py
@@ -149,7 +149,7 @@ class GitlabIntegrationTest(IntegrationTestCase):
         self.assertContains(resp, "Step 2")
 
         # Go to back to instructions
-        resp = self.client.get(self.init_path + '?goback=1')
+        resp = self.client.get(self.init_path + "?goback=1")
         assert resp.status_code == 200
         self.assertContains(resp, "Step 1")
 
diff --git a/tests/sentry/lang/native/test_processing.py b/tests/sentry/lang/native/test_processing.py
index 3d0630a05a..21655bd93a 100644
--- a/tests/sentry/lang/native/test_processing.py
+++ b/tests/sentry/lang/native/test_processing.py
@@ -104,7 +104,7 @@ def test_merge_symbolicator_image_errors(code_file, error):
 
     _merge_image(raw_image, complete_image, sdk_info, errors.append)
 
-    e, = errors
+    (e,) = errors
 
     assert e.image_name == "foo"
     assert e.type == error
diff --git a/tests/sentry/search/test_utils.py b/tests/sentry/search/test_utils.py
index a512a90e6f..2b75858614 100644
--- a/tests/sentry/search/test_utils.py
+++ b/tests/sentry/search/test_utils.py
@@ -475,7 +475,7 @@ class ParseQueryTest(TestCase):
         assert result["tags"]["event.type"] == "error"
 
     def test_leading_colon(self):
-        result = self.parse_query('country:canada :unresolved')
+        result = self.parse_query("country:canada :unresolved")
         assert result["query"] == ":unresolved"
         assert result["tags"]["country"] == "canada"
 
diff --git a/tests/sentry/tasks/test_store.py b/tests/sentry/tasks/test_store.py
index 7e8841ab1e..a40b56b408 100644
--- a/tests/sentry/tasks/test_store.py
+++ b/tests/sentry/tasks/test_store.py
@@ -177,7 +177,7 @@ def test_symbolicate_event_call_process_inline(
         symbolicate_event(cache_key="e:1", start_time=1)
 
     # The event mutated, so make sure we save it back
-    (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
 
     assert key == "e:1"
     assert event == symbolicated_data
@@ -236,7 +236,7 @@ def test_process_event_mutate_and_save(
     process_event(cache_key="e:1", start_time=1)
 
     # The event mutated, so make sure we save it back
-    (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
 
     assert key == "e:1"
     assert "extra" not in event
@@ -291,7 +291,7 @@ def test_process_event_unprocessed(
 
     process_event(cache_key="e:1", start_time=1)
 
-    (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
     assert key == "e:1"
     assert event["unprocessed"] is True
     assert duration == 3600
@@ -386,7 +386,7 @@ def test_scrubbing_after_processing(
     with Feature({"organizations:datascrubbers-v2": True}):
         process_event(cache_key="e:1", start_time=1)
 
-    (_, (key, event, duration), _), = mock_default_cache.set.mock_calls
+    ((_, (key, event, duration), _),) = mock_default_cache.set.mock_calls
     assert key == "e:1"
     assert event["extra"] == {u"aaa": u"[Filtered]", u"aaa2": u"event preprocessor"}
     assert duration == 3600
diff --git a/tests/snuba/api/endpoints/test_group_events.py b/tests/snuba/api/endpoints/test_group_events.py
index 849e8046e2..080bde39b9 100644
--- a/tests/snuba/api/endpoints/test_group_events.py
+++ b/tests/snuba/api/endpoints/test_group_events.py
@@ -198,7 +198,7 @@ class GroupEventsTest(APITestCase, SnubaTestCase):
             )
 
         # Asserts that all are in the same group
-        group_id, = set(e.group.id for e in events.values())
+        (group_id,) = set(e.group.id for e in events.values())
 
         url = u"/api/0/issues/{}/events/".format(group_id)
         response = self.client.get(url + "?environment=production", format="json")
diff --git a/tests/snuba/api/serializers/test_group.py b/tests/snuba/api/serializers/test_group.py
index 8fc31e486f..ee8e7cda6e 100644
--- a/tests/snuba/api/serializers/test_group.py
+++ b/tests/snuba/api/serializers/test_group.py
@@ -301,7 +301,7 @@ class GroupSerializerSnubaTest(APITestCase, SnubaTestCase):
             )
 
         # Assert all events are in the same group
-        group_id, = set(e.group.id for e in events)
+        (group_id,) = set(e.group.id for e in events)
 
         group = Group.objects.get(id=group_id)
         group.times_seen = 3
diff --git a/tests/snuba/eventstream/test_eventstream.py b/tests/snuba/eventstream/test_eventstream.py
index 22be0037c0..62785d99c2 100644
--- a/tests/snuba/eventstream/test_eventstream.py
+++ b/tests/snuba/eventstream/test_eventstream.py
@@ -72,12 +72,15 @@ class SnubaEventStreamTest(TestCase, SnubaTestCase):
         }
 
         self.__produce_event(*insert_args, **insert_kwargs)
-        assert snuba.query(
-            start=now - timedelta(days=1),
-            end=now + timedelta(days=1),
-            groupby=["project_id"],
-            filter_keys={"project_id": [self.project.id]},
-        ).get(self.project.id, 0) == 1
+        assert (
+            snuba.query(
+                start=now - timedelta(days=1),
+                end=now + timedelta(days=1),
+                groupby=["project_id"],
+                filter_keys={"project_id": [self.project.id]},
+            ).get(self.project.id, 0)
+            == 1
+        )
 
     @patch("sentry.eventstream.insert")
     def test_issueless(self, mock_eventstream_insert):
