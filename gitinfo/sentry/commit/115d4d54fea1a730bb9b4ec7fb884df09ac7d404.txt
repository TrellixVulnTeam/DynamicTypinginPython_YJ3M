commit 115d4d54fea1a730bb9b4ec7fb884df09ac7d404
Author: Chris Fuller <cfuller@sentry.io>
Date:   Mon Mar 30 11:15:32 2020 -0400

    feat(workflow): Display "windowed stats" for alert details graphs (#17826)
    
    * Changing TS start/end based on windowed_stats
    * Making frontend end get start + end times from eventStats for discover URL.

diff --git a/src/sentry/incidents/endpoints/organization_incident_stats.py b/src/sentry/incidents/endpoints/organization_incident_stats.py
index cd06d24bf3..97861c046a 100644
--- a/src/sentry/incidents/endpoints/organization_incident_stats.py
+++ b/src/sentry/incidents/endpoints/organization_incident_stats.py
@@ -16,7 +16,7 @@ class OrganizationIncidentStatsEndpoint(IncidentEndpoint):
         ``````````````````
         :auth: required
         """
-        stats = bulk_get_incident_stats([incident], prewindow=True)[0]
+        stats = bulk_get_incident_stats([incident], windowed_stats=True)[0]
         event_stats_serializer = SnubaTSResultSerializer(organization, None, request.user)
         results = {
             "eventStats": event_stats_serializer.serialize(stats["event_stats"]),
diff --git a/src/sentry/incidents/logic.py b/src/sentry/incidents/logic.py
index 30acdf7b76..4131141cce 100644
--- a/src/sentry/incidents/logic.py
+++ b/src/sentry/incidents/logic.py
@@ -43,6 +43,11 @@ from sentry.snuba.subscriptions import (
 from sentry.utils.snuba import bulk_raw_query, SnubaQueryParams, SnubaTSResult
 from sentry.utils.compat import zip
 
+# We can return an incident as "windowed" which returns a range of points around the start of the incident
+# It attempts to center the start of the incident, only showing earlier data if there isn't enough time
+# after the incident started to display the correct start date.
+WINDOWED_STATS_DATA_POINTS = 200
+
 
 class AlreadyDeletedError(Exception):
     pass
@@ -245,16 +250,15 @@ def delete_comment(activity):
     return activity.delete()
 
 
-def create_incident_snapshot(incident, prewindow=True):
+def create_incident_snapshot(incident, windowed_stats=False):
     """
     Creates a snapshot of an incident. This includes the count of unique users
     and total events, plus a time series snapshot of the entire incident.
     """
+
     assert incident.status == IncidentStatus.CLOSED.value
 
-    event_stats_snapshot = create_event_stat_snapshot(
-        incident, incident.date_started, incident.date_closed, prewindow
-    )
+    event_stats_snapshot = create_event_stat_snapshot(incident, windowed_stats=windowed_stats)
     aggregates = get_incident_aggregates(incident)
     return IncidentSnapshot.objects.create(
         incident=incident,
@@ -264,14 +268,13 @@ def create_incident_snapshot(incident, prewindow=True):
     )
 
 
-def create_event_stat_snapshot(incident, start, end, prewindow):
+def create_event_stat_snapshot(incident, windowed_stats=False):
     """
     Creates an event stats snapshot for an incident in a given period of time.
     """
-    if prewindow:
-        start = start - calculate_incident_prewindow(start, end, incident)
 
-    event_stats = get_incident_event_stats(incident, start, end)
+    event_stats = get_incident_event_stats(incident, windowed_stats=windowed_stats)
+    start, end = calculate_incident_time_range(incident, windowed_stats=windowed_stats)
     return TimeSeriesSnapshot.objects.create(
         start=start,
         end=end,
@@ -280,13 +283,13 @@ def create_event_stat_snapshot(incident, start, end, prewindow):
     )
 
 
-def build_incident_query_params(incident, start=None, end=None, prewindow=False):
-    return bulk_build_incident_query_params([incident], start=start, end=end, prewindow=prewindow)[
-        0
-    ]
+def build_incident_query_params(incident, start=None, end=None, windowed_stats=False):
+    return bulk_build_incident_query_params(
+        [incident], start=start, end=end, windowed_stats=windowed_stats
+    )[0]
 
 
-def bulk_build_incident_query_params(incidents, start=None, end=None, prewindow=False):
+def bulk_build_incident_query_params(incidents, start=None, end=None, windowed_stats=False):
     incident_groups = defaultdict(list)
     for incident_id, group_id in IncidentGroup.objects.filter(incident__in=incidents).values_list(
         "incident_id", "group_id"
@@ -300,13 +303,12 @@ def bulk_build_incident_query_params(incidents, start=None, end=None, prewindow=
 
     query_args_list = []
     for incident in incidents:
-        params = {
-            "start": incident.date_started if start is None else start,
-            "end": incident.current_end_date if end is None else end,
-        }
-        if prewindow:
-            prewindow_time_range = calculate_incident_prewindow(params["start"], params["end"])
-            params["start"] = params["start"] - prewindow_time_range
+        params = {}
+
+        params["start"], params["end"] = calculate_incident_time_range(
+            incident, start, end, windowed_stats=windowed_stats
+        )
+
         group_ids = incident_groups[incident.id]
         if group_ids:
             params["group_ids"] = group_ids
@@ -328,6 +330,23 @@ def bulk_build_incident_query_params(incidents, start=None, end=None, prewindow=
     return query_args_list
 
 
+def calculate_incident_time_range(incident, start=None, end=None, windowed_stats=False):
+    # TODO: When time_window is persisted, switch to using that instead of alert_rule.time_window.
+    time_window = incident.alert_rule.time_window if incident.alert_rule is not None else 1
+    time_window_delta = timedelta(minutes=time_window)
+    start = incident.date_started - time_window_delta if start is None else start
+    end = incident.current_end_date if end is None else end
+    if windowed_stats:
+        now = timezone.now()
+        end = start + timedelta(minutes=time_window * (WINDOWED_STATS_DATA_POINTS / 2))
+        start = start - timedelta(minutes=time_window * (WINDOWED_STATS_DATA_POINTS / 2))
+        if end > now:
+            end = now
+            start = now - timedelta(minutes=time_window * WINDOWED_STATS_DATA_POINTS)
+
+    return start, end
+
+
 def calculate_incident_prewindow(start, end, incident=None):
     # Make the a bit earlier to show more relevant data from before the incident started:
     prewindow = (end - start) / 5
@@ -337,18 +356,18 @@ def calculate_incident_prewindow(start, end, incident=None):
     return prewindow
 
 
-def get_incident_event_stats(incident, start=None, end=None, data_points=50, prewindow=False):
+def get_incident_event_stats(incident, start=None, end=None, windowed_stats=False):
     """
     Gets event stats for an incident. If start/end are provided, uses that time
     period, otherwise uses the incident start/current_end.
     """
     query_params = bulk_build_incident_query_params(
-        [incident], start=start, end=end, prewindow=prewindow
+        [incident], start=start, end=end, windowed_stats=windowed_stats
     )
-    return bulk_get_incident_event_stats([incident], query_params, data_points=data_points)[0]
+    return bulk_get_incident_event_stats([incident], query_params)[0]
 
 
-def bulk_get_incident_event_stats(incidents, query_params_list, data_points=50):
+def bulk_get_incident_event_stats(incidents, query_params_list):
     snuba_params_list = [
         SnubaQueryParams(
             aggregations=[
@@ -360,7 +379,10 @@ def bulk_get_incident_event_stats(incidents, query_params_list, data_points=50):
             ],
             orderby="time",
             groupby=["time"],
-            rollup=max(int(incident.duration.total_seconds() / data_points), 1),
+            rollup=incident.alert_rule.time_window * 60
+            if incident.alert_rule is not None
+            else 1
+            * 60,  # TODO: When time_window is persisted, switch to using that instead of alert_rule.time_window.
             limit=10000,
             **query_param
         )
@@ -373,13 +395,13 @@ def bulk_get_incident_event_stats(incidents, query_params_list, data_points=50):
     ]
 
 
-def get_incident_aggregates(incident, start=None, end=None, prewindow=False):
+def get_incident_aggregates(incident, start=None, end=None, windowed_stats=False):
     """
     Calculates aggregate stats across the life of an incident, or the provided range.
     - count: Total count of events
     - unique_users: Total number of unique users
     """
-    query_params = build_incident_query_params(incident, start, end, prewindow)
+    query_params = build_incident_query_params(incident, start, end, windowed_stats)
     return bulk_get_incident_aggregates([query_params])[0]
 
 
@@ -396,30 +418,39 @@ def bulk_get_incident_aggregates(query_params_list):
     return [result["data"][0] for result in results]
 
 
-def bulk_get_incident_stats(incidents, prewindow=False):
+def bulk_get_incident_stats(incidents, windowed_stats=False):
     """
     Returns bulk stats for a list of incidents. This includes unique user count,
     total event count and event stats.
-    Note that even though this function accepts a prewindow parameter, it does not
-    affect the snapshots if they were created using a prewindow. Only the live fetched stats.
+    Note that even though this function accepts a windowed_stats parameter, it does not
+    affect the snapshots. Only the live fetched stats.
     """
-    closed = [i for i in incidents if i.status == IncidentStatus.CLOSED.value]
     incident_stats = {}
-    snapshots = IncidentSnapshot.objects.filter(incident__in=closed)
-    for snapshot in snapshots:
-        event_stats = snapshot.event_stats_snapshot
-        incident_stats[snapshot.incident_id] = {
-            "event_stats": SnubaTSResult(
-                event_stats.snuba_values, event_stats.start, event_stats.end, event_stats.period
-            ),
-            "total_events": snapshot.total_events,
-            "unique_users": snapshot.unique_users,
-        }
+    if windowed_stats is True:
+        # At the moment, snapshots are only ever created with windowed_stats as True
+        # so if they send False, we need to do a live calculation below.
+        closed = [i for i in incidents if i.status == IncidentStatus.CLOSED.value]
+        snapshots = IncidentSnapshot.objects.filter(incident__in=closed)
+        for snapshot in snapshots:
+            event_stats = snapshot.event_stats_snapshot
+            incident_stats[snapshot.incident_id] = {
+                "event_stats": SnubaTSResult(
+                    event_stats.snuba_values, event_stats.start, event_stats.end, event_stats.period
+                ),
+                "total_events": snapshot.total_events,
+                "unique_users": snapshot.unique_users,
+            }
 
     to_fetch = [i for i in incidents if i.id not in incident_stats]
     if to_fetch:
-        query_params_list = bulk_build_incident_query_params(to_fetch, prewindow=prewindow)
-        all_event_stats = bulk_get_incident_event_stats(to_fetch, query_params_list)
+        query_params_list = bulk_build_incident_query_params(to_fetch, windowed_stats=False)
+        if windowed_stats:
+            windowed_query_params_list = bulk_build_incident_query_params(
+                to_fetch, windowed_stats=True
+            )
+            all_event_stats = bulk_get_incident_event_stats(to_fetch, windowed_query_params_list)
+        else:
+            all_event_stats = bulk_get_incident_event_stats(to_fetch, query_params_list)
         all_aggregates = bulk_get_incident_aggregates(query_params_list)
         for incident, event_stats, aggregates in zip(to_fetch, all_event_stats, all_aggregates):
             incident_stats[incident.id] = {
diff --git a/src/sentry/static/sentry/app/views/alerts/details/body.tsx b/src/sentry/static/sentry/app/views/alerts/details/body.tsx
index c9dcc15c94..a7861c851d 100644
--- a/src/sentry/static/sentry/app/views/alerts/details/body.tsx
+++ b/src/sentry/static/sentry/app/views/alerts/details/body.tsx
@@ -1,6 +1,5 @@
 import {RouteComponentProps} from 'react-router/lib/Router';
 import React from 'react';
-import moment from 'moment-timezone';
 import styled from '@emotion/styled';
 
 import {
@@ -12,7 +11,7 @@ import {NewQuery, Project} from 'app/types';
 import {PageContent} from 'app/styles/organization';
 import {defined} from 'app/utils';
 import {getDisplayForAlertRuleAggregation} from 'app/views/alerts/utils';
-import {getUtcDateString, intervalToMilliseconds} from 'app/utils/dates';
+import {getUtcDateString} from 'app/utils/dates';
 import {t} from 'app/locale';
 import Duration from 'app/components/duration';
 import EventView from 'app/utils/discover/eventView';
@@ -38,25 +37,18 @@ type Props = {
 
 export default class DetailsBody extends React.Component<Props> {
   getDiscoverUrl(projects: Project[]) {
-    const {incident, params} = this.props;
+    const {incident, params, stats} = this.props;
     const {orgId} = params;
 
-    if (!projects || !projects.length || !incident) {
+    if (!projects || !projects.length || !incident || !stats) {
       return '';
     }
 
     const timeWindowString = `${incident.alertRule.timeWindow}m`;
-    const timeWindowInMs = intervalToMilliseconds(timeWindowString);
-    const startBeforeTimeWindow = moment(incident.dateStarted).subtract(
-      timeWindowInMs,
-      'ms'
+    const start = getUtcDateString(stats.eventStats.data[0][0] * 1000);
+    const end = getUtcDateString(
+      stats.eventStats.data[stats.eventStats.data.length - 1][0] * 1000
     );
-    const end = incident.dateClosed ?? getUtcDateString(new Date());
-
-    // We want the discover chart to start at "dateStarted" - "timeWindow" - "20%"
-    const additionalWindowBeforeStart =
-      moment(end).diff(startBeforeTimeWindow, 'ms') * 0.2;
-    const start = startBeforeTimeWindow.subtract(additionalWindowBeforeStart, 'ms');
 
     const discoverQuery: NewQuery = {
       id: undefined,
@@ -72,7 +64,7 @@ export default class DetailsBody extends React.Component<Props> {
         .filter(({slug}) => incident.projects.includes(slug))
         .map(({id}) => Number(id)),
       version: 2 as const,
-      start: getUtcDateString(start),
+      start,
       end,
     };
 
diff --git a/tests/sentry/incidents/endpoints/test_organization_incident_stats.py b/tests/sentry/incidents/endpoints/test_organization_incident_stats.py
index b1bac91a50..6caa59dab0 100644
--- a/tests/sentry/incidents/endpoints/test_organization_incident_stats.py
+++ b/tests/sentry/incidents/endpoints/test_organization_incident_stats.py
@@ -48,4 +48,4 @@ class OrganizationIncidentDetailsTest(APITestCase):
 
         assert resp.data["totalEvents"] == 0
         assert resp.data["uniqueUsers"] == 0
-        assert [data[1] for data in resp.data["eventStats"]["data"]] == [[]] * 61
+        assert [data[1] for data in resp.data["eventStats"]["data"]] == [[]] * 201
diff --git a/tests/sentry/incidents/test_logic.py b/tests/sentry/incidents/test_logic.py
index a980f7442a..5796aba227 100644
--- a/tests/sentry/incidents/test_logic.py
+++ b/tests/sentry/incidents/test_logic.py
@@ -28,6 +28,7 @@ from sentry.incidents.logic import (
     create_alert_rule_trigger,
     create_alert_rule_trigger_action,
     create_event_stat_snapshot,
+    calculate_incident_time_range,
     create_incident,
     create_incident_activity,
     create_incident_snapshot,
@@ -35,6 +36,7 @@ from sentry.incidents.logic import (
     delete_alert_rule_trigger,
     delete_alert_rule_trigger_action,
     DEFAULT_ALERT_RULE_RESOLUTION,
+    WINDOWED_STATS_DATA_POINTS,
     get_actions_for_trigger,
     get_available_action_integrations_for_org,
     get_excluded_projects_for_alert_rule,
@@ -48,7 +50,6 @@ from sentry.incidents.logic import (
     update_alert_rule_trigger_action,
     update_alert_rule_trigger,
     update_incident_status,
-    calculate_incident_prewindow,
 )
 from sentry.incidents.models import (
     AlertRule,
@@ -250,77 +251,77 @@ class BaseIncidentEventStatsTest(BaseIncidentsTest):
             groups=[event.group],
         )
 
+    def validate_result(self, incident, result, expected_results, start, end, windowed_stats):
+        # Duration of 300s, but no alert rule
+        time_window = 1
+        assert result.rollup == time_window * 60
+        expected_start = start if start else incident.date_started - timedelta(minutes=1)
+        expected_end = end if end else incident.current_end_date
+
+        if windowed_stats:
+            now = timezone.now()
+            expected_end = expected_start + timedelta(
+                minutes=time_window * (WINDOWED_STATS_DATA_POINTS / 2)
+            )
+            expected_start = expected_start - timedelta(
+                minutes=time_window * (WINDOWED_STATS_DATA_POINTS / 2)
+            )
+            if expected_end > now:
+                expected_end = now
+                expected_start = now - timedelta(minutes=time_window * WINDOWED_STATS_DATA_POINTS)
 
+        assert result.start == expected_start
+        assert result.end == expected_end
+        assert [r["count"] for r in result.data["data"]] == expected_results
+
+
+@freeze_time()
 class GetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
-    def run_test(self, incident, expected_results, start=None, end=None):
+    def run_test(self, incident, expected_results, start=None, end=None, windowed_stats=False):
         kwargs = {}
         if start is not None:
             kwargs["start"] = start
         if end is not None:
             kwargs["end"] = end
 
-        result = get_incident_event_stats(incident, data_points=20, **kwargs)
-        # Duration of 300s / 20 data points
-        assert result.rollup == 15
-        expected_start = start if start else incident.date_started
-        expected_end = end if end else incident.current_end_date
-        assert result.start == expected_start
-        assert result.end == expected_end
-        assert [r["count"] for r in result.data["data"]] == expected_results
-
-        # A prewindow version of the same test:
-        result = get_incident_event_stats(incident, data_points=20, prewindow=True, **kwargs)
-        # Duration of 300s / 20 data points
-        assert result.rollup == 15
-        expected_start = start if start else incident.date_started
-        expected_end = end if end else incident.current_end_date
-        expected_start = expected_start - calculate_incident_prewindow(
-            expected_start, expected_end, incident
-        )
-        assert result.start == expected_start
-        assert result.end == expected_end
-        assert [r["count"] for r in result.data["data"]] == expected_results
+        result = get_incident_event_stats(incident, windowed_stats=windowed_stats, **kwargs)
+        self.validate_result(incident, result, expected_results, start, end, windowed_stats)
 
     def test_project(self):
         self.run_test(self.project_incident, [2, 1])
         self.run_test(self.project_incident, [1], start=self.now - timedelta(minutes=1))
         self.run_test(self.project_incident, [2], end=self.now - timedelta(minutes=1, seconds=59))
 
+        self.run_test(self.project_incident, [2, 1], windowed_stats=True)
+        self.run_test(
+            self.project_incident,
+            [2, 1],
+            start=self.now - timedelta(minutes=1),
+            windowed_stats=True,
+        )
+        self.run_test(
+            self.project_incident,
+            [2, 1],
+            end=self.now - timedelta(minutes=1, seconds=59),
+            windowed_stats=True,
+        )
+
     def test_groups(self):
         self.run_test(self.group_incident, [1, 1])
+        self.run_test(self.group_incident, [1, 1], windowed_stats=True)
 
 
+@freeze_time()
 class BulkGetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
-    def run_test(self, incidents, expected_results_list, start=None, end=None):
+    def run_test(
+        self, incidents, expected_results_list, start=None, end=None, windowed_stats=False
+    ):
         query_params_list = bulk_build_incident_query_params(
-            incidents, start=start, end=end, prewindow=False
+            incidents, start=start, end=end, windowed_stats=windowed_stats
         )
-        results = bulk_get_incident_event_stats(incidents, query_params_list, data_points=20)
+        results = bulk_get_incident_event_stats(incidents, query_params_list)
         for incident, result, expected_results in zip(incidents, results, expected_results_list):
-            # Duration of 300s / 20 data points
-            assert result.rollup == 15
-            expected_start = start if start else incident.date_started
-            expected_end = end if end else incident.current_end_date
-            assert result.start == expected_start
-            assert result.end == expected_end
-            assert [r["count"] for r in result.data["data"]] == expected_results
-
-        # A prewindow version of the same test:
-        query_params_list = bulk_build_incident_query_params(
-            incidents, start=start, end=end, prewindow=True
-        )
-        results = bulk_get_incident_event_stats(incidents, query_params_list, data_points=20)
-        for incident, result, expected_results in zip(incidents, results, expected_results_list):
-            # Duration of 300s / 20 data points
-            assert result.rollup == 15
-            expected_start = start if start else incident.date_started
-            expected_end = end if end else incident.current_end_date
-            expected_start = expected_start - calculate_incident_prewindow(
-                expected_start, expected_end, incident
-            )
-            assert result.start == expected_start
-            assert result.end == expected_end
-            assert [r["count"] for r in result.data["data"]] == expected_results
+            self.validate_result(incident, result, expected_results, start, end, windowed_stats)
 
     def test_project(self):
         other_project = self.create_project(fire_project_created=True)
@@ -333,6 +334,10 @@ class BulkGetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
         incidents = [self.project_incident, other_incident]
         self.run_test(incidents, [[2, 1], []])
         self.run_test(incidents, [[1], []], start=self.now - timedelta(minutes=1))
+        self.run_test(
+            incidents, [[2, 1], []], start=self.now - timedelta(minutes=1), windowed_stats=True
+        )
+        self.run_test(incidents, [[2, 1], []], windowed_stats=True)
         self.run_test(incidents, [[2], []], end=self.now - timedelta(minutes=1, seconds=59))
 
     def test_groups(self):
@@ -345,6 +350,7 @@ class BulkGetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
         )
 
         self.run_test([self.group_incident, other_incident], [[1, 1], []])
+        self.run_test([self.group_incident, other_incident], [[1, 1], []], windowed_stats=True)
 
 
 class BaseIncidentAggregatesTest(BaseIncidentsTest):
@@ -422,20 +428,15 @@ class CreateEventStatTest(TestCase, BaseIncidentsTest):
         incident = self.create_incident(
             date_started=self.now - timedelta(minutes=5), query="", projects=[self.project]
         )
-        snapshot = create_event_stat_snapshot(
-            incident, incident.date_started, incident.current_end_date, False
-        )
-        assert snapshot.start == incident.date_started
+        snapshot = create_event_stat_snapshot(incident, windowed_stats=False)
+        assert snapshot.start == incident.date_started - timedelta(minutes=1)
         assert snapshot.end == incident.current_end_date
         assert [row[1] for row in snapshot.values] == [2, 1]
 
-        snapshot = create_event_stat_snapshot(
-            incident, incident.date_started, incident.current_end_date, True
-        )
-        assert snapshot.start == incident.date_started - calculate_incident_prewindow(
-            incident.date_started, incident.current_end_date, incident
-        )
-        assert snapshot.end == incident.current_end_date
+        snapshot = create_event_stat_snapshot(incident, windowed_stats=True)
+        expected_start, expected_end = calculate_incident_time_range(incident, windowed_stats=True)
+        assert snapshot.start == expected_start
+        assert snapshot.end == expected_end
         assert [row[1] for row in snapshot.values] == [2, 1]
 
 
@@ -553,10 +554,23 @@ class CreateIncidentSnapshotTest(TestCase, BaseIncidentsTest):
     def test(self):
         incident = self.create_incident(self.organization)
         incident.update(status=IncidentStatus.CLOSED.value)
-        snapshot = create_incident_snapshot(incident)
-        expected_snapshot = create_event_stat_snapshot(
-            incident, incident.date_started, incident.date_closed, prewindow=True
-        )
+        snapshot = create_incident_snapshot(incident, windowed_stats=False)
+        expected_snapshot = create_event_stat_snapshot(incident, windowed_stats=False)
+
+        assert snapshot.event_stats_snapshot.start == expected_snapshot.start
+        assert snapshot.event_stats_snapshot.end == expected_snapshot.end
+        assert snapshot.event_stats_snapshot.values == expected_snapshot.values
+        assert snapshot.event_stats_snapshot.period == expected_snapshot.period
+        assert snapshot.event_stats_snapshot.date_added == expected_snapshot.date_added
+        aggregates = get_incident_aggregates(incident)
+        assert snapshot.unique_users == aggregates["unique_users"]
+        assert snapshot.total_events == aggregates["count"]
+
+    def test_windowed(self):
+        incident = self.create_incident(self.organization)
+        incident.update(status=IncidentStatus.CLOSED.value)
+        snapshot = create_incident_snapshot(incident, windowed_stats=True)
+        expected_snapshot = create_event_stat_snapshot(incident, windowed_stats=True)
 
         assert snapshot.event_stats_snapshot.start == expected_snapshot.start
         assert snapshot.event_stats_snapshot.end == expected_snapshot.end
@@ -569,7 +583,7 @@ class CreateIncidentSnapshotTest(TestCase, BaseIncidentsTest):
 
 
 @freeze_time()
-class BulkGetIncidentStatusTest(TestCase, BaseIncidentsTest):
+class BulkGetIncidentStatsTest(TestCase, BaseIncidentsTest):
     def test(self):
         closed_incident = create_incident(
             self.organization,
@@ -591,14 +605,15 @@ class BulkGetIncidentStatusTest(TestCase, BaseIncidentsTest):
             date_started=timezone.now() - timedelta(days=30),
         )
         incidents = [closed_incident, open_incident]
-        # Note: Closing an incident above uses a prewindow in the snapshot by default, so without prewindows this test fails.
+
         for incident, incident_stats in zip(
-            incidents, bulk_get_incident_stats(incidents, prewindow=True)
+            incidents, bulk_get_incident_stats(incidents, windowed_stats=True)
         ):
-            event_stats = get_incident_event_stats(incident, prewindow=True)
+            event_stats = get_incident_event_stats(incident, windowed_stats=True)
             assert incident_stats["event_stats"].data["data"] == event_stats.data["data"]
-            expected_start = incident_stats["event_stats"].start
-            expected_end = incident_stats["event_stats"].end
+            expected_start, expected_end = calculate_incident_time_range(
+                incident, windowed_stats=True
+            )
             assert event_stats.start == expected_start
             assert event_stats.end == expected_end
             assert incident_stats["event_stats"].rollup == event_stats.rollup
