commit f8f100254863cb96643b5f4356fdf883c3626ceb
Author: evanh <evanh@users.noreply.github.com>
Date:   Mon Mar 16 16:13:31 2020 -0400

    feat(discover) Add histogram function (#17464)
    
    Add a histogram function that will return the buckets for a column. This will be
    used by the performance detail page.

diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index b1a26b8158..5187193d19 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -897,6 +897,11 @@ class InvalidFunctionArgument(Exception):
     pass
 
 
+class ArgValue(object):
+    def __init__(self, arg):
+        self.arg = arg
+
+
 class FunctionArg(object):
     def __init__(self, name):
         self.name = name
@@ -1045,6 +1050,20 @@ FUNCTIONS = {
         "transform": "divide(countIf(notEquals(transaction_status, 0)), count())",
         "result_type": "number",
     },
+    "histogram": {
+        "name": "histogram",
+        "args": [
+            DurationColumnNoLookup("column"),
+            NumberRange("num_buckets", 1, 500),
+            NumberRange("bucket", 0, None),
+        ],
+        "column": [
+            "multiply",
+            [["floor", [["divide", [u"{column}", ArgValue("bucket")]]]], ArgValue("bucket")],
+            None,
+        ],
+        "result_type": "number",
+    },
     "count_unique": {
         "name": "count_unique",
         "args": [CountColumn("column")],
@@ -1109,6 +1128,17 @@ def get_function_alias_with_columns(function_name, columns):
     return u"{}_{}".format(function_name, columns).rstrip("_")
 
 
+def format_column_arguments(column, arguments):
+    args = column[1]
+    for i in range(len(args)):
+        if isinstance(args[i], (list, tuple)):
+            format_column_arguments(args[i], arguments)
+        elif isinstance(args[i], six.string_types):
+            args[i] = args[i].format(**arguments)
+        elif isinstance(args[i], ArgValue):
+            args[i] = arguments[args[i].arg]
+
+
 def resolve_function(field, match=None, params=None):
     if not match:
         match = FUNCTION_PATTERN.search(field)
@@ -1177,6 +1207,21 @@ def resolve_function(field, match=None, params=None):
             )
 
         return ([], [aggregate])
+    elif "column" in function:
+        # These can be very nested functions, so we need to iterate through all the layers
+        addition = deepcopy(function["column"])
+        format_column_arguments(addition, arguments)
+        if len(addition) < 3:
+            addition.append(
+                get_function_alias_with_columns(
+                    function["name"], columns if not used_default else []
+                )
+            )
+        elif len(addition) == 3 and addition[2] is None:
+            addition[2] = get_function_alias_with_columns(
+                function["name"], columns if not used_default else []
+            )
+        return ([addition], [])
 
 
 def resolve_orderby(orderby, fields, aggregations):
@@ -1211,6 +1256,11 @@ def resolve_orderby(orderby, fields, aggregations):
             validated.append(prefix + FIELD_ALIASES[bare_column]["column_alias"])
             continue
 
+        found = [col[2] for col in fields if isinstance(col, (list, tuple))]
+        if found:
+            prefix = "-" if column.startswith("-") else ""
+            validated.append(prefix + bare_column)
+
     if len(validated) == len(orderby):
         return validated
 
@@ -1322,7 +1372,11 @@ def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
     # If aggregations are present all columns
     # need to be added to the group by so that the query is valid.
     if aggregations:
-        groupby.extend(columns)
+        for column in columns:
+            if isinstance(column, (list, tuple)):
+                groupby.append(column[2])
+            else:
+                groupby.append(column)
 
     return {
         "selected_columns": columns,
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 92254c7ad8..7e35e42902 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -136,6 +136,121 @@ def create_reference_event_conditions(reference_event):
     return conditions
 
 
+# TODO (evanh) This whole function is here because we are using the max value to
+# calculate the entire bucket width. If we could do that in a smarter way,
+# we could avoid this whole calculation.
+def find_histogram_buckets(field, params, conditions):
+    match = is_function(field)
+    if not match:
+        raise InvalidSearchQuery(u"received {}, expected histogram function".format(field))
+
+    columns = [c.strip() for c in match.group("columns").split(",") if len(c.strip()) > 0]
+
+    if len(columns) != 2:
+        raise InvalidSearchQuery(
+            u"histogram(...) expects 2 column arguments, received {:g} arguments".format(
+                len(columns)
+            )
+        )
+
+    column = columns[0]
+    # TODO evanh: This can be expanded to more fields at a later date, for now keep this limited.
+    if column != "transaction.duration":
+        raise InvalidSearchQuery(
+            "histogram(...) can only be used with the transaction.duration column"
+        )
+
+    try:
+        num_buckets = int(columns[1])
+        if num_buckets < 1 or num_buckets > 500:
+            raise Exception()
+    except Exception:
+        raise InvalidSearchQuery(
+            u"histogram(...) requires a bucket value between 1 and 500, not {}".format(columns[1])
+        )
+
+    alias = u"max_{}".format(column)
+
+    conditions = deepcopy(conditions) if conditions else []
+    found = False
+    for cond in conditions:
+        if (cond[0], cond[1], cond[2]) == ("event.type", "=", "transaction"):
+            found = True
+    if not found:
+        conditions.append(["event.type", "=", "transaction"])
+    translated_args, _ = resolve_discover_aliases({"conditions": conditions})
+
+    results = raw_query(
+        filter_keys={"project_id": params.get("project_id")},
+        start=params.get("start"),
+        end=params.get("end"),
+        dataset=Dataset.Discover,
+        conditions=translated_args["conditions"],
+        aggregations=[["max", "duration", alias]],
+    )
+    if len(results["data"]) != 1:
+        # If there are no transactions, so no max duration, return one empty bucket
+        return "histogram({}, 1, 1)".format(column)
+
+    bucket_max = results["data"][0][alias]
+    if bucket_max == 0:
+        raise InvalidSearchQuery(u"Cannot calculate histogram for {}".format(field))
+
+    bucket_number = bucket_max / float(num_buckets)
+
+    return "histogram({}, {:g}, {:g})".format(column, num_buckets, bucket_number)
+
+
+def zerofill_histogram(results, column_meta, orderby, sentry_function_alias, snuba_function_alias):
+    parts = snuba_function_alias.split("_")
+    if len(parts) < 2:
+        raise Exception(u"{} is not a valid histogram alias".format(snuba_function_alias))
+
+    bucket_size, num_buckets = int(parts[-1]), int(parts[-2])
+    if len(results) == num_buckets:
+        return results
+
+    dummy_data = {}
+    for column in column_meta:
+        dummy_data[column["name"]] = "" if column.get("type") == "String" else 0
+
+    def build_new_bucket_row(bucket):
+        row = {key: value for key, value in six.iteritems(dummy_data)}
+        row[sentry_function_alias] = bucket
+        return row
+
+    bucket_map = {r[sentry_function_alias]: r for r in results}
+    new_results = []
+    is_sorted, is_reversed = False, False
+    if orderby:
+        for o in orderby:
+            if o.lstrip("-") == snuba_function_alias:
+                is_sorted = True
+                is_reversed = o.startswith("-")
+                break
+
+    for i in range(num_buckets):
+        bucket = bucket_size * (i + 1)
+        if bucket not in bucket_map:
+            bucket_map[bucket] = build_new_bucket_row(bucket)
+
+    # If the list was sorted, pull results out in sorted order, else concat the results
+    if is_sorted:
+        i, diff, end = (0, 1, num_buckets - 1) if not is_reversed else (num_buckets - 1, -1, 0)
+        while i <= end:
+            bucket = bucket_size * (i + 1)
+            new_results.append(bucket_map[bucket])
+            i += diff
+    else:
+        new_results = results
+        exists = set(r[sentry_function_alias] for r in results)
+        for bucket in bucket_map:
+            if bucket not in exists:
+                new_results.append(bucket_map[bucket])
+
+    return new_results
+
+
 def resolve_column(col):
     """
     Used as a column resolver in discover queries.
@@ -145,16 +260,32 @@ def resolve_column(col):
     """
     if col is None:
         return col
+    elif isinstance(col, (list, tuple)):
+        return col
     # Whilst project_id is not part of the public schema we convert
     # the project.name field into project_id way before we get here.
     if col == "project_id":
         return col
     if col.startswith("tags[") or QUOTED_LITERAL_RE.match(col):
         return col
+
     return DISCOVER_COLUMN_MAP.get(col, u"tags[{}]".format(col))
 
 
-def resolve_discover_aliases(snuba_args):
+# TODO (evanh) Since we are assuming that all string values are columns,
+# this will get tricky if we ever have complex columns where there are
+# string arguments to the functions that aren't columns
+def resolve_complex_column(col):
+    args = col[1]
+
+    for i in range(len(args)):
+        if isinstance(args[i], (list, tuple)):
+            resolve_complex_column(args[i])
+        elif isinstance(args[i], six.string_types):
+            args[i] = resolve_column(args[i])
+
+
+def resolve_discover_aliases(snuba_args, function_translations=None):
     """
     Resolve the public schema aliases to the discover dataset.
 
@@ -165,23 +296,33 @@ def resolve_discover_aliases(snuba_args):
     resolved = deepcopy(snuba_args)
     translated_columns = {}
     derived_columns = set()
+    if function_translations:
+        for snuba_name, sentry_name in six.iteritems(function_translations):
+            derived_columns.add(snuba_name)
+            translated_columns[snuba_name] = sentry_name
 
     selected_columns = resolved.get("selected_columns")
     if selected_columns:
         for (idx, col) in enumerate(selected_columns):
             if isinstance(col, (list, tuple)):
-                raise ValueError("discover selected_columns should only be str. got %s" % col)
-            name = resolve_column(col)
-            selected_columns[idx] = name
-            translated_columns[name] = col
+                resolve_complex_column(col)
+            else:
+                name = resolve_column(col)
+                selected_columns[idx] = name
+                translated_columns[name] = col
+
         resolved["selected_columns"] = selected_columns
 
     groupby = resolved.get("groupby")
     if groupby:
         for (idx, col) in enumerate(groupby):
             name = col
-            if col not in derived_columns:
+            if isinstance(col, (list, tuple)):
+                if len(col) == 3:
+                    name = col[2]
+            elif col not in derived_columns:
                 name = resolve_column(col)
+
             groupby[idx] = name
         resolved["groupby"] = groupby
 
@@ -201,8 +342,6 @@ def resolve_discover_aliases(snuba_args):
             conditions[i] = replacement
         resolved["conditions"] = [c for c in conditions if c]
 
-    # TODO add support for extracting having conditions.
-
     orderby = resolved.get("orderby")
     if orderby:
         orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
@@ -268,6 +407,20 @@ def transform_results(result, translated_columns, snuba_args):
             result["data"], snuba_args["start"], snuba_args["end"], rollup, snuba_args["orderby"]
         )
 
+    for col in result["meta"]:
+        if col["name"].startswith("histogram"):
+            # The column name here has been translated, we need the original name
+            for snuba_name, sentry_name in six.iteritems(translated_columns):
+                if sentry_name == col["name"]:
+                    result["data"] = zerofill_histogram(
+                        result["data"],
+                        result["meta"],
+                        snuba_args["orderby"],
+                        sentry_name,
+                        snuba_name,
+                    )
+            break
+
     return result
 
 
@@ -302,33 +455,6 @@ def transform_deprecated_functions_in_columns(columns):
     return new_list, translations
 
 
-def transform_deprecated_functions_in_orderby(orderby):
-    if not orderby:
-        return
-
-    orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
-    new_orderby = []
-    for order in orderby:
-        has_negative = False
-        column = order
-        if order.startswith("-"):
-            has_negative = True
-            column = order.strip("-")
-
-        new_column = column
-        if column in OLD_FUNCTIONS_TO_NEW:
-            new_column = get_function_alias(OLD_FUNCTIONS_TO_NEW[column])
-        elif column.replace("()", "") in OLD_FUNCTIONS_TO_NEW:
-            new_column = get_function_alias(OLD_FUNCTIONS_TO_NEW[column.replace("()", "")])
-
-        if has_negative:
-            new_column = "-" + new_column
-
-        new_orderby.append(new_column)
-
-    return new_orderby
-
-
 def transform_deprecated_functions_in_query(query):
     if query is None:
         return query
@@ -388,7 +514,6 @@ def query(
     selected_columns, function_translations = transform_deprecated_functions_in_columns(
         selected_columns
     )
-    orderby = transform_deprecated_functions_in_orderby(orderby)
     query = transform_deprecated_functions_in_query(query)
 
     snuba_filter = get_filter(query, params)
@@ -408,6 +533,36 @@ def query(
     if use_aggregate_conditions:
         snuba_args["having"] = snuba_filter.having
 
+    # We need to run a separate query to be able to properly bucket the values for the histogram
+    # Do that here, and format the bucket number in to the columns before passing it through
+    # to event search.
+    idx = 0
+    for col in selected_columns:
+        if col.startswith("histogram("):
+            histogram_column = find_histogram_buckets(col, params, snuba_filter.conditions)
+            selected_columns[idx] = histogram_column
+            function_translations[get_function_alias(histogram_column)] = get_function_alias(col)
+            break
+
+        idx += 1
+
+    # Check to see if we are ordering by any functions and convert the orderby to be the correct alias.
+    if orderby:
+        orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+        new_orderby = []
+        for ordering in orderby:
+            is_reversed = ordering.startswith("-")
+            ordering = ordering.lstrip("-")
+            for snuba_name, sentry_name in six.iteritems(function_translations):
+                if sentry_name == ordering:
+                    ordering = snuba_name
+                    break
+
+            ordering = "{}{}".format("-" if is_reversed else "", ordering)
+            new_orderby.append(ordering)
+
+        snuba_args["orderby"] = new_orderby
+
     snuba_args.update(
         resolve_field_list(selected_columns, snuba_args, params=params, auto_fields=auto_fields)
     )
@@ -418,9 +573,7 @@ def query(
             snuba_args["conditions"].extend(ref_conditions)
 
     # Resolve the public aliases into the discover dataset names.
-    snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
-    for snuba_name, sentry_name in six.iteritems(function_translations):
-        translated_columns[snuba_name] = sentry_name
+    snuba_args, translated_columns = resolve_discover_aliases(snuba_args, function_translations)
 
     # Make sure that any aggregate conditions are also in the selected columns
     for having_clause in snuba_args.get("having"):
diff --git a/tests/sentry/api/test_event_search.py b/tests/sentry/api/test_event_search.py
index 6c0ed32a06..6be357547f 100644
--- a/tests/sentry/api/test_event_search.py
+++ b/tests/sentry/api/test_event_search.py
@@ -1453,6 +1453,45 @@ class ResolveFieldListTest(unittest.TestCase):
             in six.text_type(err)
         )
 
+    def test_histogram_function(self):
+        fields = ["histogram(transaction.duration, 10, 1000)", "count()"]
+        result = resolve_field_list(fields, {})
+        assert result["selected_columns"] == [
+            [
+                "multiply",
+                [["floor", [["divide", ["transaction.duration", 1000]]]], 1000],
+                "histogram_transaction_duration_10_1000",
+            ]
+        ]
+        assert result["aggregations"] == [
+            ["count", None, "count"],
+            ["argMax", ["id", "timestamp"], "latest_event"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
+            ["transform(projectid, array(), array(), '')", None, "project.name"],
+        ]
+        assert result["groupby"] == ["histogram_transaction_duration_10_1000"]
+
+        with pytest.raises(InvalidSearchQuery) as err:
+            fields = ["histogram(stack.colno, 10, 1000)"]
+            resolve_field_list(fields, {})
+        assert (
+            "histogram(stack.colno, 10, 1000): column argument invalid: stack.colno is not a duration column"
+            in six.text_type(err)
+        )
+
+        with pytest.raises(InvalidSearchQuery) as err:
+            fields = ["histogram(transaction.duration, 10)"]
+            resolve_field_list(fields, {})
+        assert "histogram(transaction.duration, 10): expected 3 arguments" in six.text_type(err)
+
+        with pytest.raises(InvalidSearchQuery) as err:
+            fields = ["histogram(transaction.duration, 1000, 1000)"]
+            resolve_field_list(fields, {})
+        assert (
+            "histogram(transaction.duration, 1000, 1000): num_buckets argument invalid: 1000 must be less than 500"
+            in six.text_type(err)
+        )
+
     def test_rollup_with_unaggregated_fields(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["message"]
diff --git a/tests/sentry/snuba/test_discover.py b/tests/sentry/snuba/test_discover.py
index f0927093b1..c4c78c0deb 100644
--- a/tests/sentry/snuba/test_discover.py
+++ b/tests/sentry/snuba/test_discover.py
@@ -11,6 +11,7 @@ from sentry.api.event_search import InvalidSearchQuery
 from sentry.snuba import discover
 from sentry.testutils import TestCase, SnubaTestCase
 from sentry.testutils.helpers.datetime import iso_format, before_now
+from sentry.utils.compat import zip
 from sentry.utils.samples import load_data
 from sentry.utils.snuba import Dataset
 
@@ -1027,6 +1028,246 @@ class QueryTransformTest(TestCase):
             referrer=None,
         )
 
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_translations(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [{"histogram_transaction_duration_10_1000": 1000, "count": 1123}],
+            },
+        ]
+        discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+        mock_query.assert_called_with(
+            selected_columns=[
+                [
+                    "multiply",
+                    [["floor", [["divide", ["duration", 1000]]]], 1000],
+                    "histogram_transaction_duration_10_1000",
+                ]
+            ],
+            aggregations=[
+                ["count", None, "count"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+                ["argMax", ["project_id", "timestamp"], "projectid"],
+                [
+                    "transform(projectid, array({}), array('{}'), '')".format(
+                        six.text_type(self.project.id), six.text_type(self.project.slug)
+                    ),
+                    None,
+                    "project.name",
+                ],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            groupby=["histogram_transaction_duration_10_1000"],
+            conditions=[],
+            end=None,
+            start=None,
+            orderby=None,
+            having=[],
+            limit=50,
+            offset=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_bad_histogram_translations(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [{"histogram_transaction_duration_10_1000": 1000, "count": 1123}],
+            },
+        ]
+        with pytest.raises(InvalidSearchQuery) as err:
+            discover.query(
+                selected_columns=["histogram(transaction.duration)", "count()"],
+                query="",
+                params={"project_id": [self.project.id]},
+                auto_fields=True,
+                use_aggregate_conditions=False,
+            )
+        assert "histogram(...) expects 2 column arguments, received 1 arguments" in six.text_type(
+            err
+        )
+
+        with pytest.raises(InvalidSearchQuery) as err:
+            discover.query(
+                selected_columns=["histogram(stack.colno, 10)", "count()"],
+                query="",
+                params={"project_id": [self.project.id]},
+                auto_fields=True,
+                use_aggregate_conditions=False,
+            )
+        assert (
+            "histogram(...) can only be used with the transaction.duration column"
+            in six.text_type(err)
+        )
+
+        with pytest.raises(InvalidSearchQuery) as err:
+            discover.query(
+                selected_columns=["histogram(transaction.duration, 1000)", "count()"],
+                query="",
+                params={"project_id": [self.project.id]},
+                auto_fields=True,
+                use_aggregate_conditions=False,
+            )
+        assert (
+            "histogram(...) requires a bucket value between 1 and 500, not 1000"
+            in six.text_type(err)
+        )
+
+    # empty results
+    # full results
+    # missing results
+    # missing results sorted asc
+    # missing results sorted desc
+    # missing results sorted otherwise
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_zerofill_empty_results(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [],
+            },
+        ]
+
+        results = discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby="histogram_transaction_duration_10",
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+
+        expected = [i * 1000 for i in range(1, 10)]
+        for result, exp in zip(results["data"], expected):
+            assert result["histogram_transaction_duration_10"] == exp
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_zerofill_full_results(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [
+                    {"histogram_transaction_duration_10_1000": i * 1000, "count": i}
+                    for i in range(1, 10)
+                ],
+            },
+        ]
+
+        results = discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby="histogram_transaction_duration_10",
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+
+        expected = [i * 1000 for i in range(1, 10)]
+        for result, exp in zip(results["data"], expected):
+            assert result["histogram_transaction_duration_10"] == exp
+            assert result["count"] == exp / 1000
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_zerofill_missing_results_asc_sort(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [
+                    {"histogram_transaction_duration_10_1000": i * 1000, "count": i}
+                    for i in range(1, 10, 2)
+                ],
+            },
+        ]
+
+        results = discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby="histogram_transaction_duration_10",
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+
+        expected = [i * 1000 for i in range(1, 10)]
+        for result, exp in zip(results["data"], expected):
+            assert result["histogram_transaction_duration_10"] == exp
+            assert result["count"] == (exp / 1000 if (exp / 1000) % 2 == 1 else 0)
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_zerofill_missing_results_desc_sort(self, mock_query):
+        seed = range(1, 10, 2)
+        seed.reverse()
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [
+                    {"histogram_transaction_duration_10_1000": i * 1000, "count": i} for i in seed
+                ],
+            },
+        ]
+
+        results = discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby="-histogram_transaction_duration_10",
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+
+        expected = [i * 1000 for i in range(1, 10)]
+        expected.reverse()
+        for result, exp in zip(results["data"], expected):
+            assert result["histogram_transaction_duration_10"] == exp
+            assert result["count"] == (exp / 1000 if (exp / 1000) % 2 == 1 else 0)
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_histogram_zerofill_missing_results_no_sort(self, mock_query):
+        mock_query.side_effect = [
+            {"data": [{"max_transaction.duration": 10000}]},
+            {
+                "meta": [{"name": "histogram_transaction_duration_10_1000"}, {"name": "count"}],
+                "data": [
+                    {"histogram_transaction_duration_10_1000": i * 1000, "count": i}
+                    for i in range(1, 10, 2)
+                ],
+            },
+        ]
+
+        results = discover.query(
+            selected_columns=["histogram(transaction.duration, 10)", "count()"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby="count",
+            auto_fields=True,
+            use_aggregate_conditions=False,
+        )
+
+        expected = [1000, 3000, 5000, 7000, 9000]
+        for result, exp in zip(results["data"], expected):
+            assert result["histogram_transaction_duration_10"] == exp
+            assert result["count"] == exp / 1000
+
+        expected_extra_buckets = set([2000, 4000, 6000, 8000, 10000])
+        extra_buckets = set(r["histogram_transaction_duration_10"] for r in results["data"][5:])
+        assert expected_extra_buckets == extra_buckets
+
 
 class TimeseriesQueryTest(SnubaTestCase, TestCase):
     def setUp(self):
diff --git a/tests/snuba/api/endpoints/test_organization_events_v2.py b/tests/snuba/api/endpoints/test_organization_events_v2.py
index a4b36dc9a9..dcc92fe0dd 100644
--- a/tests/snuba/api/endpoints/test_organization_events_v2.py
+++ b/tests/snuba/api/endpoints/test_organization_events_v2.py
@@ -2,6 +2,8 @@ from __future__ import absolute_import
 
 import six
 import pytest
+import random
+from datetime import timedelta
 
 from django.core.urlresolvers import reverse
 
@@ -1884,7 +1886,7 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 format="json",
                 data={
                     "field": ["event.type", "percentile(transaction.duration, 0.99)"],
-                    "sort": "-percentile(transaction.duration, 0.99)",
+                    "sort": "-percentile_transaction_duration_0_99",
                     "query": "event.type:transaction",
                 },
             )
@@ -1991,10 +1993,10 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 },
             )
 
-            assert response.status_code == 200, response.content
-            data = response.data["data"]
-            assert len(data) == 1
-            assert data[0]["count_unique_issue"] == 2
+        assert response.status_code == 200, response.content
+        data = response.data["data"]
+        assert len(data) == 1
+        assert data[0]["count_unique_issue"] == 2
 
     def test_deleted_issue_in_results(self):
         self.login_as(user=self.user)
@@ -2017,11 +2019,11 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 self.url, format="json", data={"field": ["issue", "count()"], "sort": "issue"}
             )
 
-            assert response.status_code == 200, response.content
-            data = response.data["data"]
-            assert len(data) == 2
-            assert data[0]["issue"] == event1.group.qualified_short_id
-            assert data[1]["issue"] == "unknown"
+        assert response.status_code == 200, response.content
+        data = response.data["data"]
+        assert len(data) == 2
+        assert data[0]["issue"] == event1.group.qualified_short_id
+        assert data[1]["issue"] == "unknown"
 
     def test_context_fields(self):
         self.login_as(user=self.user)
@@ -2150,4 +2152,117 @@ class OrganizationEventsV2EndpointTest(APITestCase, SnubaTestCase):
                 expected = "{:.1f}".format(expected)
 
             assert results[0][field] == expected
+
         assert results[0]["count"] == 1
+
+    def test_histogram_function(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        start = before_now(minutes=2).replace(microsecond=0)
+        latencies = [
+            (1, 999, 5),
+            (1000, 1999, 4),
+            (3000, 3999, 3),
+            (6000, 6999, 2),
+            (10000, 10000, 1),  # just to make the math easy
+        ]
+        for bucket in latencies:
+            for i in range(bucket[2]):
+                milliseconds = random.randint(bucket[0], bucket[1])
+                data = load_data("transaction")
+                data["transaction"] = "/error_rate/{}".format(milliseconds)
+                data["timestamp"] = iso_format(start)
+                data["start_timestamp"] = iso_format(start - timedelta(milliseconds=milliseconds))
+                self.store_event(data, project_id=project.id)
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["histogram(transaction.duration, 10)", "count()"],
+                    "query": "event.type:transaction",
+                    "sort": "histogram_transaction_duration_10",
+                },
+            )
+
+        assert response.status_code == 200, response.content
+        data = response.data["data"]
+        assert len(data) == 10
+        expected = [
+            (1000, 5),
+            (2000, 4),
+            (3000, 0),
+            (4000, 3),
+            (5000, 0),
+            (6000, 0),
+            (7000, 2),
+            (8000, 0),
+            (9000, 0),
+            (10000, 1),
+        ]
+        for idx, datum in enumerate(data):
+            assert datum["histogram_transaction_duration_10"] == expected[idx][0]
+            assert datum["count"] == expected[idx][1]
+
+    def test_histogram_function_with_filters(self):
+        self.login_as(user=self.user)
+        project = self.create_project()
+        start = before_now(minutes=2).replace(microsecond=0)
+        latencies = [
+            (1, 999, 5),
+            (1000, 1999, 4),
+            (3000, 3999, 3),
+            (6000, 6999, 2),
+            (10000, 10000, 1),  # just to make the math easy
+        ]
+        for bucket in latencies:
+            for i in range(bucket[2]):
+                milliseconds = random.randint(bucket[0], bucket[1])
+                data = load_data("transaction")
+                data["transaction"] = "/error_rate/sleepy_gary/{}".format(milliseconds)
+                data["timestamp"] = iso_format(start)
+                data["start_timestamp"] = iso_format(start - timedelta(milliseconds=milliseconds))
+                self.store_event(data, project_id=project.id)
+
+        # Add a transaction that totally throws off the buckets
+        milliseconds = random.randint(bucket[0], bucket[1])
+        data = load_data("transaction")
+        data["transaction"] = "/error_rate/hamurai"
+        data["timestamp"] = iso_format(start)
+        data["start_timestamp"] = iso_format(start - timedelta(milliseconds=1000000))
+        self.store_event(data, project_id=project.id)
+
+        with self.feature(
+            {"organizations:discover-basic": True, "organizations:global-views": True}
+        ):
+            response = self.client.get(
+                self.url,
+                format="json",
+                data={
+                    "field": ["histogram(transaction.duration, 10)", "count()"],
+                    "query": "event.type:transaction transaction:/error_rate/sleepy_gary*",
+                    "sort": "histogram_transaction_duration_10",
+                },
+            )
+
+        assert response.status_code == 200, response.content
+        data = response.data["data"]
+        assert len(data) == 10
+        expected = [
+            (1000, 5),
+            (2000, 4),
+            (3000, 0),
+            (4000, 3),
+            (5000, 0),
+            (6000, 0),
+            (7000, 2),
+            (8000, 0),
+            (9000, 0),
+            (10000, 1),
+        ]
+        for idx, datum in enumerate(data):
+            assert datum["histogram_transaction_duration_10"] == expected[idx][0]
+            assert datum["count"] == expected[idx][1]
