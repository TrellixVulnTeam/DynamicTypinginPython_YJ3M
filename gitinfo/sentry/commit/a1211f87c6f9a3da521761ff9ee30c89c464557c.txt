commit a1211f87c6f9a3da521761ff9ee30c89c464557c
Author: Lyn Nagara <lyn.nagara@gmail.com>
Date:   Wed Jan 29 15:24:03 2020 -0800

    feat(eventstore): Use nodestore for get_events (#16644)
    
    Now that we are caching nodestore data, try using nodestore for
    get_events again. This functionality is toggled by the "eventstore.use-nodestore" option.

diff --git a/src/sentry/api/endpoints/project_events.py b/src/sentry/api/endpoints/project_events.py
index 6d388c465b..8e4fb336b5 100644
--- a/src/sentry/api/endpoints/project_events.py
+++ b/src/sentry/api/endpoints/project_events.py
@@ -49,12 +49,13 @@ class ProjectEventsEndpoint(ProjectEndpoint):
             )
 
         full = request.GET.get("full", False)
+
         cols = None if full else eventstore.full_columns
 
         data_fn = partial(
             eventstore.get_events,
-            filter=eventstore.Filter(conditions=conditions, project_ids=[project.id]),
             additional_columns=cols,
+            filter=eventstore.Filter(conditions=conditions, project_ids=[project.id]),
             referrer="api.project-events",
         )
 
diff --git a/src/sentry/api/serializers/models/event.py b/src/sentry/api/serializers/models/event.py
index 3fe49fe105..c07da36213 100644
--- a/src/sentry/api/serializers/models/event.py
+++ b/src/sentry/api/serializers/models/event.py
@@ -6,7 +6,7 @@ from datetime import datetime
 from django.utils import timezone
 from sentry_relay import meta_with_chunks
 
-from sentry import eventstore
+from sentry import eventstore, options
 from sentry.api.serializers import Serializer, register, serialize
 from sentry.models import EventAttachment, EventError, Release, UserReport
 from sentry.search.utils import convert_user_tag_to_query
@@ -167,7 +167,8 @@ class EventSerializer(Serializer):
         return serialize(user_report, user)
 
     def get_attrs(self, item_list, user, is_public=False):
-        eventstore.bind_nodes(item_list, "data")
+        if not options.get("eventstore.use-nodestore"):
+            eventstore.bind_nodes(item_list, "data")
 
         crash_files = get_crash_files(item_list)
         results = {}
diff --git a/src/sentry/deletions/defaults/group.py b/src/sentry/deletions/defaults/group.py
index cad5fb988b..d8f63dab8e 100644
--- a/src/sentry/deletions/defaults/group.py
+++ b/src/sentry/deletions/defaults/group.py
@@ -34,7 +34,7 @@ class EventDataDeletionTask(BaseDeletionTask):
                 ]
             )
 
-        events = eventstore.get_events(
+        events = eventstore.get_unfetched_events(
             filter=eventstore.Filter(
                 conditions=conditions, project_ids=[self.project_id], group_ids=[self.group_id]
             ),
diff --git a/src/sentry/eventstore/base.py b/src/sentry/eventstore/base.py
index ba8a31779a..9b9093b828 100644
--- a/src/sentry/eventstore/base.py
+++ b/src/sentry/eventstore/base.py
@@ -66,6 +66,7 @@ class EventStorage(Service):
         "create_event",
         "get_event_by_id",
         "get_events",
+        "get_unfetched_events",
         "get_prev_event_id",
         "get_next_event_id",
         "get_earliest_event_id",
@@ -112,7 +113,6 @@ class EventStorage(Service):
 
         Arguments:
         filter (Filter): Filter
-        additional_columns (Sequence[Column]): List of additional columns to fetch - default None
         orderby (Sequence[str]): List of fields to order by - default ['-time', '-event_id']
         limit (int): Query limit - default 100
         offset (int): Query offset - default 0
@@ -120,6 +120,27 @@ class EventStorage(Service):
         """
         raise NotImplementedError
 
+    def get_unfetched_events(
+        self, filter, orderby=None, limit=100, offset=0, referrer="eventstore.get_unfetched_events"
+    ):
+        """
+        Same as get_events but returns events without their node datas loaded.
+        Only the event ID, projectID, groupID and timestamp field will be present without
+        an additional fetch to nodestore.
+
+        Used for fetching large volumes of events that do not need data loaded
+        from nodestore. Currently this is just used for event data deletions where
+        we just need the event IDs in order to process the deletions.
+
+        Arguments:
+        filter (Filter): Filter
+        orderby (Sequence[str]): List of fields to order by - default ['-time', '-event_id']
+        limit (int): Query limit - default 100
+        offset (int): Query offset - default 0
+        referrer (string): Referrer - default "eventstore.get_unfetched_events"
+        """
+        raise NotImplementedError
+
     def get_event_by_id(self, project_id, event_id):
         """
         Gets a single event given a project_id and event_id.
diff --git a/src/sentry/eventstore/snuba/backend.py b/src/sentry/eventstore/snuba/backend.py
index 93bed42c62..b8ba14e9cc 100644
--- a/src/sentry/eventstore/snuba/backend.py
+++ b/src/sentry/eventstore/snuba/backend.py
@@ -6,6 +6,7 @@ from copy import deepcopy
 from datetime import datetime, timedelta
 import logging
 
+from sentry import options
 from sentry.eventstore.base import EventStorage
 from sentry.snuba.events import Columns
 from sentry.utils import snuba
@@ -54,8 +55,58 @@ class SnubaEventStorage(EventStorage):
         referrer="eventstore.get_events",
     ):
         """
-        Get events from Snuba.
+        Get events from Snuba, with node data loaded.
         """
+        if not options.get("eventstore.use-nodestore"):
+            return self.__get_events(
+                filter,
+                additional_columns=additional_columns,
+                orderby=orderby,
+                limit=limit,
+                offset=offset,
+                referrer=referrer,
+                should_bind_nodes=False,
+            )
+
+        return self.__get_events(
+            filter,
+            orderby=orderby,
+            limit=limit,
+            offset=offset,
+            referrer=referrer,
+            should_bind_nodes=True,
+        )
+
+    def get_unfetched_events(
+        self,
+        filter,
+        orderby=None,
+        limit=DEFAULT_LIMIT,
+        offset=DEFAULT_OFFSET,
+        referrer="eventstore.get_unfetched_events",
+    ):
+        """
+        Get events from Snuba, without node data loaded.
+        """
+        return self.__get_events(
+            filter,
+            orderby=orderby,
+            limit=limit,
+            offset=offset,
+            referrer=referrer,
+            should_bind_nodes=False,
+        )
+
+    def __get_events(
+        self,
+        filter,
+        additional_columns=None,
+        orderby=None,
+        limit=DEFAULT_LIMIT,
+        offset=DEFAULT_OFFSET,
+        referrer=None,
+        should_bind_nodes=False,
+    ):
         assert filter, "You must provide a filter"
         cols = self.__get_columns(additional_columns)
         orderby = orderby or DESC_ORDERING
@@ -74,7 +125,10 @@ class SnubaEventStorage(EventStorage):
         )
 
         if "error" not in result:
-            return [self.__make_event(evt) for evt in result["data"]]
+            events = [self.__make_event(evt) for evt in result["data"]]
+            if should_bind_nodes:
+                self.bind_nodes(events)
+            return events
 
         return []
 
@@ -169,7 +223,7 @@ class SnubaEventStorage(EventStorage):
 
         return self.__get_event_id_from_filter(filter=filter, orderby=DESC_ORDERING)
 
-    def __get_columns(self, additional_columns):
+    def __get_columns(self, additional_columns=None):
         columns = EventStorage.minimal_columns
 
         if additional_columns:
diff --git a/src/sentry/tasks/unmerge.py b/src/sentry/tasks/unmerge.py
index 4b77d46ac8..f69b9c4718 100644
--- a/src/sentry/tasks/unmerge.py
+++ b/src/sentry/tasks/unmerge.py
@@ -5,7 +5,7 @@ from collections import defaultdict, OrderedDict
 
 from django.db import transaction
 
-from sentry import eventstore, eventstream
+from sentry import eventstore, eventstream, options
 from sentry.app import tsdb
 from sentry.constants import DEFAULT_LOGGER_NAME, LOG_LEVELS_MAP
 from sentry.event_manager import generate_culprit
@@ -518,7 +518,8 @@ def unmerge(
 
         return destination_id
 
-    eventstore.bind_nodes(events, "data")
+    if not options.get("eventstore.use-nodestore"):
+        eventstore.bind_nodes(events, "data")
 
     source_events = []
     destination_events = []
diff --git a/tests/sentry/eventstore/test_base.py b/tests/sentry/eventstore/test_base.py
index 1aa85f4698..6444f49a53 100644
--- a/tests/sentry/eventstore/test_base.py
+++ b/tests/sentry/eventstore/test_base.py
@@ -21,9 +21,6 @@ class EventStorageTest(TestCase):
     def test_minimal_columns(self):
         assert len(self.eventstorage.minimal_columns) == 4
 
-    def test_full_columns(self):
-        assert len(self.eventstorage.full_columns) == 17
-
     def test_bind_nodes(self):
         """
         Test that bind_nodes populates _node_data
diff --git a/tests/sentry/eventstore/test_models.py b/tests/sentry/eventstore/test_models.py
index 474b719808..595a15aead 100644
--- a/tests/sentry/eventstore/test_models.py
+++ b/tests/sentry/eventstore/test_models.py
@@ -3,7 +3,6 @@ from __future__ import absolute_import
 import pickle
 import pytest
 
-from sentry import eventstore
 from sentry.db.models.fields.node import NodeData
 from sentry.eventstore.models import Event
 from sentry.models import Environment
@@ -200,7 +199,24 @@ class EventTest(TestCase):
             project_id=self.project.id,
             event_id="a" * 32,
             snuba_data=snuba.raw_query(
-                selected_columns=[col.value.event_name for col in eventstore.full_columns],
+                selected_columns=[
+                    "event_id",
+                    "project_id",
+                    "group_id",
+                    "timestamp",
+                    "culprit",
+                    "location",
+                    "message",
+                    "title",
+                    "type",
+                    "transaction",
+                    "tags.key",
+                    "tags.value",
+                    "email",
+                    "ip_address",
+                    "user_id",
+                    "username",
+                ],
                 filter_keys={"project_id": [self.project.id], "event_id": ["a" * 32]},
             )["data"][0],
         )
