commit 59186078940ad7a5d308ea1c7c779882389c75a4
Author: Dan Fuller <dfuller@sentry.io>
Date:   Tue Feb 4 14:38:17 2020 -0800

    feat(subscriptions): Update subscription results schema to match snuba.
    
    We've changed the format of the schema in snuba, so updating code here to handle that.

diff --git a/src/sentry/incidents/subscription_processor.py b/src/sentry/incidents/subscription_processor.py
index 36139706cf..f5dfe51b62 100644
--- a/src/sentry/incidents/subscription_processor.py
+++ b/src/sentry/incidents/subscription_processor.py
@@ -24,7 +24,7 @@ from sentry.incidents.models import (
 from sentry.incidents.tasks import handle_trigger_action
 from sentry.snuba.models import QueryAggregations
 from sentry.utils import metrics, redis
-from sentry.utils.dates import to_datetime
+from sentry.utils.dates import to_datetime, to_timestamp
 
 
 logger = logging.getLogger(__name__)
@@ -129,7 +129,7 @@ class SubscriptionProcessor(object):
 
         aggregation = QueryAggregations(self.alert_rule.aggregation)
         aggregation_name = query_aggregation_to_snuba[aggregation][2]
-        aggregation_value = subscription_update["values"][aggregation_name]
+        aggregation_value = subscription_update["values"]["data"][aggregation_name]
 
         for trigger in self.triggers:
             alert_operator, resolve_operator = self.THRESHOLD_TYPE_OPERATORS[
@@ -172,7 +172,7 @@ class SubscriptionProcessor(object):
         if self.trigger_alert_counts[trigger.id] >= self.alert_rule.threshold_period:
             # Only create a new incident if we don't already have an active one
             if not self.active_incident:
-                detected_at = to_datetime(self.last_update)
+                detected_at = self.last_update
                 self.active_incident = create_incident(
                     self.alert_rule.organization,
                     IncidentType.ALERT_TRIGGERED,
@@ -356,7 +356,7 @@ def get_alert_rule_stats(alert_rule, subscription, triggers):
     trigger_keys = build_trigger_stat_keys(alert_rule, subscription, triggers)
     results = get_redis_client().mget(alert_rule_keys + trigger_keys)
     results = tuple(0 if result is None else int(result) for result in results)
-    last_update = results[0]
+    last_update = to_datetime(results[0])
     trigger_results = results[1:]
     trigger_alert_counts = {}
     trigger_resolve_counts = {}
@@ -387,7 +387,7 @@ def update_alert_rule_stats(alert_rule, subscription, last_update, alert_counts,
             )
 
     last_update_key = build_alert_rule_stat_keys(alert_rule, subscription)[0]
-    pipeline.set(last_update_key, last_update, ex=REDIS_TTL)
+    pipeline.set(last_update_key, int(to_timestamp(last_update)), ex=REDIS_TTL)
     pipeline.execute()
 
 
diff --git a/src/sentry/snuba/json_schemas.py b/src/sentry/snuba/json_schemas.py
index dd41f7ec88..d547d998d6 100644
--- a/src/sentry/snuba/json_schemas.py
+++ b/src/sentry/snuba/json_schemas.py
@@ -16,15 +16,14 @@ SUBSCRIPTION_PAYLOAD_VERSIONS = {
             "subscription_id": {"type": "string", "minLength": 1},
             "values": {
                 "type": "object",
-                "minProperties": 1,
-                "additionalProperties": {"type": "number"},
+                "properties": {
+                    "data": {"minProperties": 1, "additionalProperties": {"type": "number"}}
+                },
+                "required": ["data"],
             },
-            "timestamp": {"type": "number", "minimum": 0},
-            "interval": {"type": "number", "minimum": 0},
-            "partition": {"type": "number"},
-            "offset": {"type": "number"},
+            "timestamp": {"type": "string", "format": "date-time"},
         },
-        "required": ["subscription_id", "values", "timestamp", "interval", "partition", "offset"],
+        "required": ["subscription_id", "values", "timestamp"],
         "additionalProperties": False,
     }
 }
diff --git a/src/sentry/snuba/query_subscription_consumer.py b/src/sentry/snuba/query_subscription_consumer.py
index 7390941379..a506febd21 100644
--- a/src/sentry/snuba/query_subscription_consumer.py
+++ b/src/sentry/snuba/query_subscription_consumer.py
@@ -3,7 +3,9 @@ import logging
 from json import loads
 
 import jsonschema
+import pytz
 from confluent_kafka import Consumer, KafkaException, TopicPartition
+from dateutil.parser import parse as parse_date
 from django.conf import settings
 
 from sentry.snuba.json_schemas import SUBSCRIPTION_PAYLOAD_VERSIONS, SUBSCRIPTION_WRAPPER_SCHEMA
@@ -197,4 +199,5 @@ class QuerySubscriptionConsumer(object):
             metrics.incr("snuba_query_subscriber.message_payload_invalid")
             raise InvalidSchemaError("Message payload does not match schema")
 
+        payload["timestamp"] = parse_date(payload["timestamp"]).replace(tzinfo=pytz.utc)
         return payload
diff --git a/tests/sentry/incidents/test_subscription_processor.py b/tests/sentry/incidents/test_subscription_processor.py
index 8e8d5f7ef0..d8833d3054 100644
--- a/tests/sentry/incidents/test_subscription_processor.py
+++ b/tests/sentry/incidents/test_subscription_processor.py
@@ -1,11 +1,11 @@
 from __future__ import absolute_import
 
 import unittest
-from datetime import timedelta
-from time import time
+from datetime import datetime, timedelta
 from random import randint
 from uuid import uuid4
 
+import pytz
 import six
 from django.utils import timezone
 from exam import fixture, patcher
@@ -109,18 +109,20 @@ class ProcessUpdateTest(TestCase):
 
     def build_subscription_update(self, subscription, time_delta=None, value=None):
         if time_delta is not None:
-            timestamp = int(to_timestamp(timezone.now() + time_delta))
+            timestamp = timezone.now() + time_delta
         else:
-            timestamp = int(time())
+            timestamp = timezone.now()
+        timestamp = timestamp.replace(tzinfo=pytz.utc, microsecond=0)
 
-        values = {}
+        data = {}
 
         if subscription:
             aggregation_type = query_aggregation_to_snuba[
                 QueryAggregations(subscription.aggregation)
             ]
             value = randint(0, 100) if value is None else value
-            values = {aggregation_type[2]: value}
+            data = {aggregation_type[2]: value}
+        values = {"data": data}
         return {
             "subscription_id": subscription.subscription_id if subscription else uuid4().hex,
             "values": values,
@@ -722,7 +724,8 @@ class TestGetAlertRuleStats(TestCase):
         triggers = [AlertRuleTrigger(id=3), AlertRuleTrigger(id=4)]
         client = get_redis_client()
         pipeline = client.pipeline()
-        pipeline.set("{alert_rule:1:project:2}:last_update", 1234)
+        timestamp = datetime.now().replace(tzinfo=pytz.utc, microsecond=0)
+        pipeline.set("{alert_rule:1:project:2}:last_update", int(to_timestamp(timestamp)))
         for key, value in [
             ("{alert_rule:1:project:2}:trigger:3:alert_triggered", 1),
             ("{alert_rule:1:project:2}:trigger:3:resolve_triggered", 2),
@@ -733,7 +736,7 @@ class TestGetAlertRuleStats(TestCase):
         pipeline.execute()
 
         last_update, alert_counts, resolve_counts = get_alert_rule_stats(alert_rule, sub, triggers)
-        assert last_update == 1234
+        assert last_update == timestamp
         assert alert_counts == {3: 1, 4: 3}
         assert resolve_counts == {3: 2, 4: 4}
 
@@ -742,7 +745,8 @@ class TestUpdateAlertRuleStats(TestCase):
     def test(self):
         alert_rule = AlertRule(id=1)
         sub = QuerySubscription(project_id=2)
-        update_alert_rule_stats(alert_rule, sub, 1234, {3: 20, 4: 3}, {3: 10, 4: 15})
+        date = datetime.utcnow().replace(tzinfo=pytz.utc)
+        update_alert_rule_stats(alert_rule, sub, date, {3: 20, 4: 3}, {3: 10, 4: 15})
         client = get_redis_client()
         results = map(
             int,
@@ -757,4 +761,4 @@ class TestUpdateAlertRuleStats(TestCase):
             ),
         )
 
-        assert results == [1234, 20, 10, 3, 15]
+        assert results == [int(to_timestamp(date)), 20, 10, 3, 15]
diff --git a/tests/sentry/snuba/test_query_subscription_consumer.py b/tests/sentry/snuba/test_query_subscription_consumer.py
index 4996a434bb..96dc128e7d 100644
--- a/tests/sentry/snuba/test_query_subscription_consumer.py
+++ b/tests/sentry/snuba/test_query_subscription_consumer.py
@@ -5,6 +5,8 @@ import unittest
 from copy import deepcopy
 
 import six
+import pytz
+from dateutil.parser import parse as parse_date
 from exam import fixture, patcher
 
 from sentry.utils.compat.mock import Mock
@@ -32,11 +34,8 @@ class BaseQuerySubscriptionTest(object):
     def valid_payload(self):
         return {
             "subscription_id": "1234",
-            "values": {"hello": 50},
-            "timestamp": 1235,
-            "interval": 5,
-            "partition": 50,
-            "offset": 10,
+            "values": {"data": {"hello": 50}},
+            "timestamp": "2020-01-01T01:23:45.1234",
         }
 
     def build_mock_message(self, data):
@@ -89,6 +88,9 @@ class HandleMessageTest(BaseQuerySubscriptionTest, TestCase):
         data = self.valid_wrapper
         data["payload"]["subscription_id"] = sub.subscription_id
         self.consumer.handle_message(self.build_mock_message(data))
+        data["payload"]["timestamp"] = parse_date(data["payload"]["timestamp"]).replace(
+            tzinfo=pytz.utc
+        )
         mock_callback.assert_called_once_with(data["payload"], sub)
 
 
@@ -113,18 +115,10 @@ class ParseMessageValueTest(BaseQuerySubscriptionTest, unittest.TestCase):
         self.run_invalid_payload_test(remove_fields=["subscription_id"])
         self.run_invalid_payload_test(remove_fields=["values"])
         self.run_invalid_payload_test(remove_fields=["timestamp"])
-        self.run_invalid_payload_test(remove_fields=["interval"])
-        self.run_invalid_payload_test(remove_fields=["partition"])
-        self.run_invalid_payload_test(remove_fields=["offset"])
         self.run_invalid_payload_test(update_fields={"subscription_id": ""})
         self.run_invalid_payload_test(update_fields={"values": {}})
         self.run_invalid_payload_test(update_fields={"values": {"hello": "hi"}})
         self.run_invalid_payload_test(update_fields={"timestamp": -1})
-        self.run_invalid_payload_test(update_fields={"timestamp": "1"})
-        self.run_invalid_payload_test(update_fields={"interval": -1})
-        self.run_invalid_payload_test(update_fields={"interval": "1"})
-        self.run_invalid_payload_test(update_fields={"partition": "1"})
-        self.run_invalid_payload_test(update_fields={"offset": "1"})
 
     def test_invalid_version(self):
         with self.assertRaises(InvalidMessageError) as cm:
diff --git a/tests/snuba/incidents/test_tasks.py b/tests/snuba/incidents/test_tasks.py
index 6e4abbe526..e9dd58c68b 100644
--- a/tests/snuba/incidents/test_tasks.py
+++ b/tests/snuba/incidents/test_tasks.py
@@ -118,11 +118,8 @@ class HandleSnubaQueryUpdateTest(TestCase):
             "version": 1,
             "payload": {
                 "subscription_id": self.subscription.subscription_id,
-                "values": {value_name: self.trigger.alert_threshold + 1},
-                "timestamp": 1235,
-                "interval": 5,
-                "partition": 50,
-                "offset": 10,
+                "values": {"data": {value_name: self.trigger.alert_threshold + 1}},
+                "timestamp": "2020-01-01T01:23:45.1234",
             },
         }
         self.producer.produce(self.topic, json.dumps(message))
diff --git a/tests/snuba/snuba/test_query_subscription_consumer.py b/tests/snuba/snuba/test_query_subscription_consumer.py
index c2a07f83ab..019153d0d2 100644
--- a/tests/snuba/snuba/test_query_subscription_consumer.py
+++ b/tests/snuba/snuba/test_query_subscription_consumer.py
@@ -4,7 +4,9 @@ import json
 from copy import deepcopy
 from uuid import uuid4
 
+import pytz
 from confluent_kafka import Producer
+from dateutil.parser import parse as parse_date
 from django.conf import settings
 from django.test.utils import override_settings
 from exam import fixture
@@ -32,11 +34,8 @@ class QuerySubscriptionConsumerTest(TestCase, SnubaTestCase):
     def valid_payload(self):
         return {
             "subscription_id": self.subscription_id,
-            "values": {"hello": 50},
-            "timestamp": 1235,
-            "interval": 5,
-            "partition": 50,
-            "offset": 10,
+            "values": {"data": {"hello": 50}},
+            "timestamp": "2020-01-01T01:23:45.1234",
         }
 
     @fixture
@@ -96,12 +95,17 @@ class QuerySubscriptionConsumerTest(TestCase, SnubaTestCase):
         )
         consumer = QuerySubscriptionConsumer("hi", topic=self.topic, commit_batch_size=1)
         consumer.run()
-        mock_callback.assert_called_once_with(self.valid_payload, sub)
+
+        payload = self.valid_payload
+        payload["timestamp"] = parse_date(payload["timestamp"]).replace(tzinfo=pytz.utc)
+        mock_callback.assert_called_once_with(payload, sub)
 
     def test_shutdown(self):
         self.producer.produce(self.topic, json.dumps(self.valid_wrapper))
         valid_wrapper_2 = deepcopy(self.valid_wrapper)
         valid_wrapper_2["payload"]["values"]["hello"] = 25
+        valid_wrapper_3 = deepcopy(valid_wrapper_2)
+        valid_wrapper_3["payload"]["values"]["hello"] = 5000
         self.producer.produce(self.topic, json.dumps(valid_wrapper_2))
         self.producer.flush()
 
@@ -128,18 +132,23 @@ class QuerySubscriptionConsumerTest(TestCase, SnubaTestCase):
         )
         consumer = QuerySubscriptionConsumer("hi", topic=self.topic, commit_batch_size=100)
         consumer.run()
-        mock.assert_has_calls(
-            [call(self.valid_payload, sub), call(valid_wrapper_2["payload"], sub)]
-        )
+        valid_payload = self.valid_payload
+        valid_payload["timestamp"] = parse_date(valid_payload["timestamp"]).replace(tzinfo=pytz.utc)
+        valid_wrapper_2["payload"]["timestamp"] = parse_date(
+            valid_wrapper_2["payload"]["timestamp"]
+        ).replace(tzinfo=pytz.utc)
+        mock.assert_has_calls([call(valid_payload, sub), call(valid_wrapper_2["payload"], sub)])
         # Offset should be committed for the first message, so second run should process
         # the second message again
-        valid_wrapper_3 = deepcopy(valid_wrapper_2)
-        valid_wrapper_3["payload"]["values"]["hello"] = 5000
         self.producer.produce(self.topic, json.dumps(valid_wrapper_3))
         self.producer.flush()
         mock.reset_mock()
         counts[0] = 0
         consumer.run()
+        valid_wrapper_3["payload"]["timestamp"] = parse_date(
+            valid_wrapper_3["payload"]["timestamp"]
+        ).replace(tzinfo=pytz.utc)
+
         mock.assert_has_calls(
             [call(valid_wrapper_2["payload"], sub), call(valid_wrapper_3["payload"], sub)]
         )
