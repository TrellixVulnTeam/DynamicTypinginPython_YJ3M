commit d11035c1bb2107944df62cc1a77562ee1e685c15
Author: ted kaemming <ted@kaemming.com>
Date:   Thu Jul 6 13:59:16 2017 -0700

    Split up similarity indexing and feature extraction into different modules. (#5684)

diff --git a/src/sentry/similarity/__init__.py b/src/sentry/similarity/__init__.py
new file mode 100644
index 0000000000..49c51faaa0
--- /dev/null
+++ b/src/sentry/similarity/__init__.py
@@ -0,0 +1,86 @@
+from __future__ import absolute_import
+
+from django.conf import settings
+
+from sentry.utils import redis
+from sentry.utils.datastructures import BidirectionalMapping
+from sentry.utils.iterators import shingle
+
+from sentry.similarity.index import MinHashIndex
+from sentry.similarity.features import (
+    FRAME_SEPARATOR,
+    FeatureSet,
+    ExceptionFeature,
+    MessageFeature,
+    serialize_frame,
+    serialize_text_shingle,
+    get_application_chunks,
+    get_exception_frames,
+)
+
+
+features = FeatureSet(
+    MinHashIndex(
+        redis.clusters.get(
+            getattr(
+                settings,
+                'SENTRY_SIMILARITY_INDEX_REDIS_CLUSTER',
+                'default',
+            ),
+        ),
+        0xFFFF,
+        8,
+        2,
+        60 * 60 * 24 * 30,
+        3,
+    ),
+    BidirectionalMapping({
+        'exception:message:character-shingles': 'a',
+        'exception:stacktrace:application-chunks': 'b',
+        'exception:stacktrace:pairs': 'c',
+        'message:message:character-shingles': 'd',
+    }),
+    {
+        'exception:message:character-shingles': ExceptionFeature(
+            lambda exception: map(
+                serialize_text_shingle,
+                shingle(
+                    13,
+                    exception.get('value') or '',
+                ),
+            )
+        ),
+        'exception:stacktrace:application-chunks': ExceptionFeature(
+            lambda exception: map(
+                lambda frames: FRAME_SEPARATOR.join(
+                    map(
+                        serialize_frame,
+                        frames,
+                    ),
+                ),
+                get_application_chunks(exception),
+            ),
+        ),
+        'exception:stacktrace:pairs': ExceptionFeature(
+            lambda exception: map(
+                FRAME_SEPARATOR.join,
+                shingle(
+                    2,
+                    map(
+                        serialize_frame,
+                        get_exception_frames(exception),
+                    ),
+                ),
+            ),
+        ),
+        'message:message:character-shingles': MessageFeature(
+            lambda message: map(
+                serialize_text_shingle,
+                shingle(
+                    13,
+                    message['message'],
+                ),
+            ),
+        ),
+    }
+)
diff --git a/src/sentry/similarity.py b/src/sentry/similarity/features.py
similarity index 59%
rename from src/sentry/similarity.py
rename to src/sentry/similarity/features.py
index e888d9f285..ab67410a78 100644
--- a/src/sentry/similarity.py
+++ b/src/sentry/similarity/features.py
@@ -4,192 +4,16 @@ import itertools
 import logging
 import operator
 import struct
-import time
 from collections import Sequence
 
 import mmh3
 import six
-from django.conf import settings
 
-from sentry.utils import redis
-from sentry.utils.datastructures import BidirectionalMapping
 from sentry.utils.dates import to_timestamp
-from sentry.utils.iterators import shingle
-from sentry.utils.redis import load_script
-
-index = load_script('similarity/index.lua')
-
 
 logger = logging.getLogger('sentry.similarity')
 
 
-class MinHashIndex(object):
-    def __init__(self, cluster, rows, bands, buckets, interval, retention):
-        self.cluster = cluster
-        self.rows = rows
-
-        sequence = itertools.count()
-        self.bands = [[next(sequence) for j in xrange(buckets)] for i in xrange(bands)]
-        self.buckets = buckets
-        self.interval = interval
-        self.retention = retention
-
-    def get_signature(self, value):
-        """Generate a signature for a value."""
-        return map(
-            lambda band: map(
-                lambda bucket: min(
-                    map(
-                        lambda item: mmh3.hash(item, bucket) % self.rows,
-                        value,
-                    ),
-                ),
-                band,
-            ),
-            self.bands,
-        )
-
-    def query(self, scope, key, indices, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'QUERY',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-            key,
-        ]
-
-        arguments.extend(indices)
-
-        return [
-            [(item, float(score)) for item, score in result]
-            for result in
-            index(
-                self.cluster.get_local_client_for_key(scope),
-                [],
-                arguments,
-            )
-        ]
-
-    def record(self, scope, key, items, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'RECORD',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-            key,
-        ]
-
-        for idx, features in items:
-            arguments.append(idx)
-            arguments.extend([','.join(map('{}'.format, band))
-                              for band in self.get_signature(features)])
-
-        return index(
-            self.cluster.get_local_client_for_key(scope),
-            [],
-            arguments,
-        )
-
-    def merge(self, scope, destination, items, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'MERGE',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-            destination,
-        ]
-
-        for idx, source in items:
-            arguments.extend([idx, source])
-
-        return index(
-            self.cluster.get_local_client_for_key(scope),
-            [],
-            arguments,
-        )
-
-    def delete(self, scope, items, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'DELETE',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-        ]
-
-        for idx, key in items:
-            arguments.extend([idx, key])
-
-        return index(
-            self.cluster.get_local_client_for_key(scope),
-            [],
-            arguments,
-        )
-
-    def export(self, scope, items, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'EXPORT',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-        ]
-
-        for idx, key in items:
-            arguments.extend([idx, key])
-
-        return index(
-            self.cluster.get_local_client_for_key(scope),
-            [],
-            arguments,
-        )
-
-    def import_(self, scope, items, timestamp=None):
-        if timestamp is None:
-            timestamp = int(time.time())
-
-        arguments = [
-            'IMPORT',
-            timestamp,
-            len(self.bands),
-            self.interval,
-            self.retention,
-            scope,
-        ]
-
-        for idx, key, data in items:
-            arguments.extend([idx, key, data])
-
-        return index(
-            self.cluster.get_local_client_for_key(scope),
-            [],
-            arguments,
-        )
-
-
 FRAME_ITEM_SEPARATOR = b'\x00'
 FRAME_PAIR_SEPARATOR = b'\x01'
 FRAME_SEPARATOR = b'\x02'
@@ -481,70 +305,3 @@ def serialize_text_shingle(value, separator=b''):
             value,
         ),
     )
-
-
-features = FeatureSet(
-    MinHashIndex(
-        redis.clusters.get(
-            getattr(
-                settings,
-                'SENTRY_SIMILARITY_INDEX_REDIS_CLUSTER',
-                'default',
-            ),
-        ),
-        0xFFFF,
-        8,
-        2,
-        60 * 60 * 24 * 30,
-        3,
-    ),
-    BidirectionalMapping({
-        'exception:message:character-shingles': 'a',
-        'exception:stacktrace:application-chunks': 'b',
-        'exception:stacktrace:pairs': 'c',
-        'message:message:character-shingles': 'd',
-    }),
-    {
-        'exception:message:character-shingles': ExceptionFeature(
-            lambda exception: map(
-                serialize_text_shingle,
-                shingle(
-                    13,
-                    exception.get('value') or '',
-                ),
-            )
-        ),
-        'exception:stacktrace:application-chunks': ExceptionFeature(
-            lambda exception: map(
-                lambda frames: FRAME_SEPARATOR.join(
-                    map(
-                        serialize_frame,
-                        frames,
-                    ),
-                ),
-                get_application_chunks(exception),
-            ),
-        ),
-        'exception:stacktrace:pairs': ExceptionFeature(
-            lambda exception: map(
-                FRAME_SEPARATOR.join,
-                shingle(
-                    2,
-                    map(
-                        serialize_frame,
-                        get_exception_frames(exception),
-                    ),
-                ),
-            ),
-        ),
-        'message:message:character-shingles': MessageFeature(
-            lambda message: map(
-                serialize_text_shingle,
-                shingle(
-                    13,
-                    message['message'],
-                ),
-            ),
-        ),
-    }
-)
diff --git a/src/sentry/similarity/index.py b/src/sentry/similarity/index.py
new file mode 100644
index 0000000000..b23780e37e
--- /dev/null
+++ b/src/sentry/similarity/index.py
@@ -0,0 +1,177 @@
+from __future__ import absolute_import
+
+import itertools
+import time
+
+import mmh3
+
+from sentry.utils.redis import load_script
+
+index = load_script('similarity/index.lua')
+
+
+class MinHashIndex(object):
+    def __init__(self, cluster, rows, bands, buckets, interval, retention):
+        self.cluster = cluster
+        self.rows = rows
+
+        sequence = itertools.count()
+        self.bands = [[next(sequence) for j in xrange(buckets)] for i in xrange(bands)]
+        self.buckets = buckets
+        self.interval = interval
+        self.retention = retention
+
+    def get_signature(self, value):
+        """Generate a signature for a value."""
+        return map(
+            lambda band: map(
+                lambda bucket: min(
+                    map(
+                        lambda item: mmh3.hash(item, bucket) % self.rows,
+                        value,
+                    ),
+                ),
+                band,
+            ),
+            self.bands,
+        )
+
+    def query(self, scope, key, indices, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'QUERY',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+            key,
+        ]
+
+        arguments.extend(indices)
+
+        return [
+            [(item, float(score)) for item, score in result]
+            for result in
+            index(
+                self.cluster.get_local_client_for_key(scope),
+                [],
+                arguments,
+            )
+        ]
+
+    def record(self, scope, key, items, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'RECORD',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+            key,
+        ]
+
+        for idx, features in items:
+            arguments.append(idx)
+            arguments.extend([','.join(map('{}'.format, band))
+                              for band in self.get_signature(features)])
+
+        return index(
+            self.cluster.get_local_client_for_key(scope),
+            [],
+            arguments,
+        )
+
+    def merge(self, scope, destination, items, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'MERGE',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+            destination,
+        ]
+
+        for idx, source in items:
+            arguments.extend([idx, source])
+
+        return index(
+            self.cluster.get_local_client_for_key(scope),
+            [],
+            arguments,
+        )
+
+    def delete(self, scope, items, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'DELETE',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+        ]
+
+        for idx, key in items:
+            arguments.extend([idx, key])
+
+        return index(
+            self.cluster.get_local_client_for_key(scope),
+            [],
+            arguments,
+        )
+
+    def export(self, scope, items, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'EXPORT',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+        ]
+
+        for idx, key in items:
+            arguments.extend([idx, key])
+
+        return index(
+            self.cluster.get_local_client_for_key(scope),
+            [],
+            arguments,
+        )
+
+    def import_(self, scope, items, timestamp=None):
+        if timestamp is None:
+            timestamp = int(time.time())
+
+        arguments = [
+            'IMPORT',
+            timestamp,
+            len(self.bands),
+            self.interval,
+            self.retention,
+            scope,
+        ]
+
+        for idx, key, data in items:
+            arguments.extend([idx, key, data])
+
+        return index(
+            self.cluster.get_local_client_for_key(scope),
+            [],
+            arguments,
+        )
diff --git a/tests/sentry/similarity/__init__.py b/tests/sentry/similarity/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/sentry/similarity/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/sentry/test_similarity.py b/tests/sentry/similarity/test_features.py
similarity index 55%
rename from tests/sentry/test_similarity.py
rename to tests/sentry/similarity/test_features.py
index 3e2478ce22..ab1a64c629 100644
--- a/tests/sentry/test_similarity.py
+++ b/tests/sentry/similarity/test_features.py
@@ -1,17 +1,12 @@
 from __future__ import absolute_import
 
-import time
-
-import msgpack
 import pytest
 
 from sentry.models import Event
-from sentry.similarity import (
-    ExceptionFeature, InsufficientContext, MinHashIndex, get_exception_frames,
+from sentry.similarity.features import (
+    ExceptionFeature, InsufficientContext, get_exception_frames,
     get_frame_signature, serialize_frame
 )
-from sentry.testutils import TestCase
-from sentry.utils import redis
 
 
 def test_get_exception_frames():
@@ -170,87 +165,3 @@ def test_exception_feature():
             ]),
         )
     ) == []
-
-
-class MinHashIndexTestCase(TestCase):
-    def test_index(self):
-        index = MinHashIndex(
-            redis.clusters.get('default'),
-            0xFFFF,
-            8,
-            2,
-            60 * 60,
-            12,
-        )
-
-        index.record('example', '1', [('index', 'hello world')])
-        index.record('example', '2', [('index', 'hello world')])
-        index.record('example', '3', [('index', 'jello world')])
-        index.record('example', '4', [('index', 'yellow world')])
-        index.record('example', '4', [('index', 'mellow world')])
-        index.record('example', '5', [('index', 'pizza world')])
-
-        results = index.query('example', '1', ['index'])[0]
-        assert results[0] == ('1', 1.0)
-        assert results[1] == ('2', 1.0)  # identical contents
-        assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
-        assert results[3][0] in ('3', '4')
-        assert results[4][0] == '5'
-
-        index.delete('example', [('index', '3')])
-        assert [key for key, _ in index.query('example', '1', ['index'])[0]] == [
-            '1', '2', '4', '5'
-        ]
-
-    def test_export_import(self):
-        bands = 2
-        retention = 12
-        index = MinHashIndex(
-            redis.clusters.get('default'),
-            0xFFFF,
-            bands,
-            2,
-            60 * 60,
-            retention,
-        )
-
-        index.record('example', '1', [('index', 'hello world')])
-
-        timestamp = int(time.time())
-        result = index.export('example', [('index', 1)], timestamp=timestamp)
-        assert len(result) == 1
-
-        data = msgpack.unpackb(result[0])
-        assert len(data) == bands
-
-        for band in data:
-            assert len(band) == (retention + 1)
-            assert sum(sum(dict(bucket_frequencies).values())
-                       for index, bucket_frequencies in band) == 1
-
-        # Copy the data from key 1 to key 2.
-        index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
-
-        assert index.export(
-            'example',
-            [('index', 1)],
-            timestamp=timestamp
-        ) == index.export(
-            'example',
-            [('index', 2)],
-            timestamp=timestamp
-        )
-
-        # Copy the data again to key 2 (duplicating all of the data.)
-        index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
-
-        result = index.export('example', [('index', 2)], timestamp=timestamp)
-        assert len(result) == 1
-
-        data = msgpack.unpackb(result[0])
-        assert len(data) == bands
-
-        for band in data:
-            assert len(band) == (retention + 1)
-            assert sum(sum(dict(bucket_frequencies).values())
-                       for index, bucket_frequencies in band) == 2
diff --git a/tests/sentry/similarity/test_index.py b/tests/sentry/similarity/test_index.py
new file mode 100644
index 0000000000..0c3fd6baf1
--- /dev/null
+++ b/tests/sentry/similarity/test_index.py
@@ -0,0 +1,93 @@
+from __future__ import absolute_import
+
+import time
+
+import msgpack
+
+from sentry.similarity.index import MinHashIndex
+from sentry.testutils import TestCase
+from sentry.utils import redis
+
+
+class MinHashIndexTestCase(TestCase):
+    def test_index(self):
+        index = MinHashIndex(
+            redis.clusters.get('default'),
+            0xFFFF,
+            8,
+            2,
+            60 * 60,
+            12,
+        )
+
+        index.record('example', '1', [('index', 'hello world')])
+        index.record('example', '2', [('index', 'hello world')])
+        index.record('example', '3', [('index', 'jello world')])
+        index.record('example', '4', [('index', 'yellow world')])
+        index.record('example', '4', [('index', 'mellow world')])
+        index.record('example', '5', [('index', 'pizza world')])
+
+        results = index.query('example', '1', ['index'])[0]
+        assert results[0] == ('1', 1.0)
+        assert results[1] == ('2', 1.0)  # identical contents
+        assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
+        assert results[3][0] in ('3', '4')
+        assert results[4][0] == '5'
+
+        index.delete('example', [('index', '3')])
+        assert [key for key, _ in index.query('example', '1', ['index'])[0]] == [
+            '1', '2', '4', '5'
+        ]
+
+    def test_export_import(self):
+        bands = 2
+        retention = 12
+        index = MinHashIndex(
+            redis.clusters.get('default'),
+            0xFFFF,
+            bands,
+            2,
+            60 * 60,
+            retention,
+        )
+
+        index.record('example', '1', [('index', 'hello world')])
+
+        timestamp = int(time.time())
+        result = index.export('example', [('index', 1)], timestamp=timestamp)
+        assert len(result) == 1
+
+        data = msgpack.unpackb(result[0])
+        assert len(data) == bands
+
+        for band in data:
+            assert len(band) == (retention + 1)
+            assert sum(sum(dict(bucket_frequencies).values())
+                       for index, bucket_frequencies in band) == 1
+
+        # Copy the data from key 1 to key 2.
+        index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
+
+        assert index.export(
+            'example',
+            [('index', 1)],
+            timestamp=timestamp
+        ) == index.export(
+            'example',
+            [('index', 2)],
+            timestamp=timestamp
+        )
+
+        # Copy the data again to key 2 (duplicating all of the data.)
+        index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
+
+        result = index.export('example', [('index', 2)], timestamp=timestamp)
+        assert len(result) == 1
+
+        data = msgpack.unpackb(result[0])
+        assert len(data) == bands
+
+        for band in data:
+            assert len(band) == (retention + 1)
+            assert sum(sum(dict(bucket_frequencies).values())
+                       for index, bucket_frequencies in band) == 2
