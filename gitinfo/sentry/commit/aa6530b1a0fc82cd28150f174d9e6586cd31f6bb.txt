commit aa6530b1a0fc82cd28150f174d9e6586cd31f6bb
Author: Tony <Zylphrex@users.noreply.github.com>
Date:   Fri May 29 14:51:48 2020 -0400

    fix(async-csv): Handle unicode in log messages (#19071)

diff --git a/src/sentry/data_export/tasks.py b/src/sentry/data_export/tasks.py
index 4e9851fc76..9100f7f29b 100644
--- a/src/sentry/data_export/tasks.py
+++ b/src/sentry/data_export/tasks.py
@@ -13,7 +13,7 @@ from sentry.utils.sdk import capture_exception
 
 from .base import ExportError, ExportQueryType, SNUBA_MAX_RESULTS
 from .models import ExportedData
-from .utils import convert_to_utf8, snuba_error_handler
+from .utils import convert_to_utf8, handle_snuba_errors
 from .processors.discover import DiscoverProcessor
 from .processors.issues_by_tag import IssuesByTagProcessor
 
@@ -94,6 +94,7 @@ def assemble_download(
         return data_export.email_failure(message="Internal processing failure")
 
 
+@handle_snuba_errors(logger)
 def process_issues_by_tag(data_export, file, export_limit, batch_size, environment_id):
     """
     Convert the tag query to a CSV, writing it to the provided file.
@@ -114,26 +115,26 @@ def process_issues_by_tag(data_export, file, export_limit, batch_size, environme
 
     writer = create_writer(file, processor.header_fields)
     iteration = 0
-    with snuba_error_handler(logger=logger):
-        is_completed = False
-        while not is_completed:
-            offset = batch_size * iteration
-            next_offset = batch_size * (iteration + 1)
-            is_exceeding_limit = export_limit and export_limit < next_offset
-            gtv_list_unicode = processor.get_serialized_data(limit=batch_size, offset=offset)
-            # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
-            # See associated comment in './utils.py'
-            gtv_list = convert_to_utf8(gtv_list_unicode)
-            if is_exceeding_limit:
-                # Since the next offset will pass the export_limit, just write the remainder
-                writer.writerows(gtv_list[: export_limit % batch_size])
-            else:
-                writer.writerows(gtv_list)
-                iteration += 1
-            # If there are no returned results, or we've passed the export_limit, stop iterating
-            is_completed = len(gtv_list) == 0 or is_exceeding_limit
-
-
+    is_completed = False
+    while not is_completed:
+        offset = batch_size * iteration
+        next_offset = batch_size * (iteration + 1)
+        is_exceeding_limit = export_limit and export_limit < next_offset
+        gtv_list_unicode = processor.get_serialized_data(limit=batch_size, offset=offset)
+        # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
+        # See associated comment in './utils.py'
+        gtv_list = convert_to_utf8(gtv_list_unicode)
+        if is_exceeding_limit:
+            # Since the next offset will pass the export_limit, just write the remainder
+            writer.writerows(gtv_list[: export_limit % batch_size])
+        else:
+            writer.writerows(gtv_list)
+            iteration += 1
+        # If there are no returned results, or we've passed the export_limit, stop iterating
+        is_completed = len(gtv_list) == 0 or is_exceeding_limit
+
+
+@handle_snuba_errors(logger)
 def process_discover(data_export, file, export_limit, batch_size, environment_id):
     """
     Convert the discovery query to a CSV, writing it to the provided file.
@@ -150,25 +151,24 @@ def process_discover(data_export, file, export_limit, batch_size, environment_id
 
     writer = create_writer(file, processor.header_fields)
     iteration = 0
-    with snuba_error_handler(logger=logger):
-        is_completed = False
-        while not is_completed:
-            offset = batch_size * iteration
-            next_offset = batch_size * (iteration + 1)
-            is_exceeding_limit = export_limit and export_limit < next_offset
-            raw_data_unicode = processor.data_fn(offset=offset, limit=batch_size)["data"]
-            # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
-            # See associated comment in './utils.py'
-            raw_data = convert_to_utf8(raw_data_unicode)
-            raw_data = processor.handle_fields(raw_data)
-            if is_exceeding_limit:
-                # Since the next offset will pass the export_limit, just write the remainder
-                writer.writerows(raw_data[: export_limit % batch_size])
-            else:
-                writer.writerows(raw_data)
-                iteration += 1
-            # If there are no returned results, or we've passed the export_limit, stop iterating
-            is_completed = len(raw_data) == 0 or is_exceeding_limit
+    is_completed = False
+    while not is_completed:
+        offset = batch_size * iteration
+        next_offset = batch_size * (iteration + 1)
+        is_exceeding_limit = export_limit and export_limit < next_offset
+        raw_data_unicode = processor.data_fn(offset=offset, limit=batch_size)["data"]
+        # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
+        # See associated comment in './utils.py'
+        raw_data = convert_to_utf8(raw_data_unicode)
+        raw_data = processor.handle_fields(raw_data)
+        if is_exceeding_limit:
+            # Since the next offset will pass the export_limit, just write the remainder
+            writer.writerows(raw_data[: export_limit % batch_size])
+        else:
+            writer.writerows(raw_data)
+            iteration += 1
+        # If there are no returned results, or we've passed the export_limit, stop iterating
+        is_completed = len(raw_data) == 0 or is_exceeding_limit
 
 
 def create_writer(file, fields):
diff --git a/src/sentry/data_export/utils.py b/src/sentry/data_export/utils.py
index 6b1264c2fc..d41933a149 100644
--- a/src/sentry/data_export/utils.py
+++ b/src/sentry/data_export/utils.py
@@ -1,7 +1,7 @@
 from __future__ import absolute_import
 
 import six
-from contextlib import contextmanager
+from functools import wraps
 
 from sentry.snuba import discover
 from sentry.utils import metrics, snuba
@@ -10,46 +10,64 @@ from sentry.utils.sdk import capture_exception
 from .base import ExportError
 
 
-# Adapted into contextmanager from 'src/sentry/api/endpoints/organization_events.py'
-@contextmanager
-def snuba_error_handler(logger):
-    try:
-        yield
-    except discover.InvalidSearchQuery as error:
-        metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info("dataexport.error: {}".format(six.text_type(error)))
-        capture_exception(error)
-        raise ExportError("Invalid query. Please fix the query and try again.")
-    except snuba.QueryOutsideRetentionError as error:
-        metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info("dataexport.error: {}".format(six.text_type(error)))
-        capture_exception(error)
-        raise ExportError("Invalid date range. Please try a more recent date range.")
-    except snuba.QueryIllegalTypeOfArgument as error:
-        metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info("dataexport.error: {}".format(six.text_type(error)))
-        capture_exception(error)
-        raise ExportError("Invalid query. Argument to function is wrong type.")
-    except snuba.SnubaError as error:
-        metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info("dataexport.error: {}".format(six.text_type(error)))
-        capture_exception(error)
-        message = "Internal error. Please try again."
-        if isinstance(
-            error,
-            (
-                snuba.RateLimitExceeded,
-                snuba.QueryMemoryLimitExceeded,
-                snuba.QueryTooManySimultaneous,
-            ),
-        ):
-            message = "Query timeout. Please try again. If the problem persists try a smaller date range or fewer projects."
-        elif isinstance(
-            error,
-            (snuba.UnqualifiedQueryError, snuba.QueryExecutionError, snuba.SchemaValidationError),
-        ):
-            message = "Internal error. Your query failed to run."
-        raise ExportError(message)
+# Adapted into decorator from 'src/sentry/api/endpoints/organization_events.py'
+def handle_snuba_errors(logger):
+    def wrapper(func):
+        @wraps(func)
+        def wrapped(*args, **kwargs):
+            try:
+                return func(*args, **kwargs)
+            except discover.InvalidSearchQuery as error:
+                metrics.incr(
+                    "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
+                )
+                logger.info("dataexport.error: %s", six.text_type(error))
+                capture_exception(error)
+                raise ExportError("Invalid query. Please fix the query and try again.")
+            except snuba.QueryOutsideRetentionError as error:
+                metrics.incr(
+                    "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
+                )
+                logger.info("dataexport.error: %s", six.text_type(error))
+                capture_exception(error)
+                raise ExportError("Invalid date range. Please try a more recent date range.")
+            except snuba.QueryIllegalTypeOfArgument as error:
+                metrics.incr(
+                    "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
+                )
+                logger.info("dataexport.error: %s", six.text_type(error))
+                capture_exception(error)
+                raise ExportError("Invalid query. Argument to function is wrong type.")
+            except snuba.SnubaError as error:
+                metrics.incr(
+                    "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
+                )
+                logger.info("dataexport.error: %s", six.text_type(error))
+                capture_exception(error)
+                message = "Internal error. Please try again."
+                if isinstance(
+                    error,
+                    (
+                        snuba.RateLimitExceeded,
+                        snuba.QueryMemoryLimitExceeded,
+                        snuba.QueryTooManySimultaneous,
+                    ),
+                ):
+                    message = "Query timeout. Please try again. If the problem persists try a smaller date range or fewer projects."
+                elif isinstance(
+                    error,
+                    (
+                        snuba.UnqualifiedQueryError,
+                        snuba.QueryExecutionError,
+                        snuba.SchemaValidationError,
+                    ),
+                ):
+                    message = "Internal error. Your query failed to run."
+                raise ExportError(message)
+
+        return wrapped
+
+    return wrapper
 
 
 # TODO(python3): For now, this function must be run to ensure only utf-8 is passed into the 'csv' module
diff --git a/tests/sentry/data_export/test_tasks.py b/tests/sentry/data_export/test_tasks.py
index 58ce961473..88f1038d17 100644
--- a/tests/sentry/data_export/test_tasks.py
+++ b/tests/sentry/data_export/test_tasks.py
@@ -176,6 +176,13 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         error = emailer.call_args[1]["message"]
         assert error == "Invalid date range. Please try a more recent date range."
 
+        # unicode
+        mock_query.side_effect = QueryOutsideRetentionError(u"\xfc")
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Invalid date range. Please try a more recent date range."
+
     @patch("sentry.snuba.discover.query")
     @patch("sentry.data_export.models.ExportedData.email_failure")
     def test_discover_invalid_search_query(self, emailer, mock_query):
@@ -192,6 +199,13 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         error = emailer.call_args[1]["message"]
         assert error == "Invalid query. Please fix the query and try again."
 
+        # unicode
+        mock_query.side_effect = InvalidSearchQuery(u"\xfc")
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Invalid query. Please fix the query and try again."
+
     @patch("sentry.snuba.discover.raw_query")
     @patch("sentry.data_export.models.ExportedData.email_failure")
     def test_discover_snuba_error(self, emailer, mock_query):
@@ -208,12 +222,26 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         error = emailer.call_args[1]["message"]
         assert error == "Invalid query. Argument to function is wrong type."
 
+        # unicode
+        mock_query.side_effect = QueryIllegalTypeOfArgument(u"\xfc")
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Invalid query. Argument to function is wrong type."
+
         mock_query.side_effect = SnubaError("test")
         with self.tasks():
             assemble_download(de.id)
         error = emailer.call_args[1]["message"]
         assert error == "Internal error. Please try again."
 
+        # unicode
+        mock_query.side_effect = SnubaError(u"\xfc")
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Internal error. Please try again."
+
         mock_query.side_effect = RateLimitExceeded("test")
         with self.tasks():
             assemble_download(de.id)
