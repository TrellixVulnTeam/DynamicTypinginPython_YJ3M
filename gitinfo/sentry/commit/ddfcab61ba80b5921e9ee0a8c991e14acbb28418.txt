commit ddfcab61ba80b5921e9ee0a8c991e14acbb28418
Author: Dan Fuller <dfuller@sentry.io>
Date:   Wed Jun 10 14:25:05 2020 -0700

    feat(metric_alerts): Include alert start/end bucket in the incident stats endpoint. (#19282)
    
    We want to be able to accurately show the alert start/end dates in the graph. Currently, if these
    dates aren't aligned with the buckets we fetch from snuba the frontend just interpolates to
    determine the value, which frequently ends up with incorrect results.
    
    This pr makes extra queries when necessary to fetch these extra buckets. We make sure to check
    whether the buckets are already inherently included in the query to avoid making unnecessary extra
    queries to snuba.

diff --git a/src/sentry/incidents/logic.py b/src/sentry/incidents/logic.py
index 91b6e6794f..a5d2f90c58 100644
--- a/src/sentry/incidents/logic.py
+++ b/src/sentry/incidents/logic.py
@@ -2,6 +2,7 @@ from __future__ import absolute_import
 
 from copy import deepcopy
 from datetime import timedelta
+from itertools import chain
 
 import six
 from django.db import transaction
@@ -40,6 +41,8 @@ from sentry.snuba.subscriptions import (
     update_snuba_query,
 )
 from sentry.snuba.tasks import build_snuba_filter
+from sentry.utils.compat import zip
+from sentry.utils.dates import to_timestamp
 from sentry.utils.snuba import bulk_raw_query, SnubaQueryParams, SnubaTSResult
 from sentry.shared_integrations.exceptions import DuplicateDisplayNameError
 
@@ -332,7 +335,6 @@ def build_incident_query_params(incident, start=None, end=None, windowed_stats=F
 
 
 def calculate_incident_time_range(incident, start=None, end=None, windowed_stats=False):
-    # TODO: When time_window is persisted, switch to using that instead of alert_rule.time_window.
     time_window = (
         incident.alert_rule.snuba_query.time_window if incident.alert_rule is not None else 60
     )
@@ -378,18 +380,55 @@ def get_incident_event_stats(incident, start=None, end=None, windowed_stats=Fals
     query_params = build_incident_query_params(
         incident, start=start, end=end, windowed_stats=windowed_stats
     )
+    time_window = incident.alert_rule.snuba_query.time_window
     aggregations = query_params.pop("aggregations")[0]
-    snuba_params = SnubaQueryParams(
-        aggregations=[(aggregations[0], aggregations[1], "count")],
-        orderby="time",
-        groupby=["time"],
-        rollup=incident.alert_rule.snuba_query.time_window,
-        limit=10000,
-        **query_params
-    )
+    snuba_params = [
+        SnubaQueryParams(
+            aggregations=[(aggregations[0], aggregations[1], "count")],
+            orderby="time",
+            groupby=["time"],
+            rollup=time_window,
+            limit=10000,
+            **query_params
+        )
+    ]
+
+    # We want to include the specific buckets for the incident start and closed times,
+    # so that there's no need to interpolate to show them on the frontend. If they're
+    # cleanly divisible by the `time_window` then there's no need to fetch, since
+    # they'll be included in the standard results anyway.
+    extra_buckets = []
+    if int(to_timestamp(incident.date_started)) % time_window:
+        extra_buckets.append(incident.date_started)
+    if incident.date_closed and int(to_timestamp(incident.date_closed)) % time_window:
+        extra_buckets.append(incident.date_closed.replace(second=0, microsecond=0))
+
+    # We make extra queries to fetch these buckets
+    for bucket_start in extra_buckets:
+        extra_bucket_query_params = build_incident_query_params(
+            incident, start=bucket_start, end=bucket_start + timedelta(seconds=time_window)
+        )
+        aggregations = extra_bucket_query_params.pop("aggregations")[0]
+        snuba_params.append(
+            SnubaQueryParams(
+                aggregations=[(aggregations[0], aggregations[1], "count")],
+                limit=1,
+                **extra_bucket_query_params
+            )
+        )
 
-    results = bulk_raw_query([snuba_params], referrer="incidents.get_incident_event_stats")
-    return SnubaTSResult(results[0], snuba_params.start, snuba_params.end, snuba_params.rollup)
+    results = bulk_raw_query(snuba_params, referrer="incidents.get_incident_event_stats")
+    # Once we receive the results, if we requested extra buckets we now need to label
+    # them with timestamp data, since the query we ran only returns the count.
+    for extra_start, result in zip(extra_buckets, results[1:]):
+        result["data"][0]["time"] = int(to_timestamp(extra_start))
+    merged_data = list(chain(*[r["data"] for r in results]))
+    merged_data.sort(key=lambda row: row["time"])
+    results[0]["data"] = merged_data
+
+    return SnubaTSResult(
+        results[0], snuba_params[0].start, snuba_params[0].end, snuba_params[0].rollup
+    )
 
 
 def get_incident_aggregates(
@@ -1000,7 +1039,7 @@ def get_alert_rule_trigger_action_slack_channel_id(organization, integration_id,
     from sentry.integrations.slack.utils import get_channel_id
 
     try:
-        _prefix, channel_id, timed_out = get_channel_id(organization, integration_id, name,)
+        _prefix, channel_id, timed_out = get_channel_id(organization, integration_id, name)
     except DuplicateDisplayNameError as e:
         integration = Integration.objects.get(id=integration_id)
         domain = integration.metadata["domain_name"]
diff --git a/src/sentry/testutils/factories.py b/src/sentry/testutils/factories.py
index 82f182a0b0..ac6ba46102 100644
--- a/src/sentry/testutils/factories.py
+++ b/src/sentry/testutils/factories.py
@@ -828,7 +828,7 @@ class Factories(object):
             alert_rule=alert_rule,
             date_started=date_started or timezone.now(),
             date_detected=date_detected or timezone.now(),
-            date_closed=date_closed or timezone.now(),
+            date_closed=timezone.now() if date_closed is not None else date_closed,
             type=IncidentType.ALERT_TRIGGERED.value,
         )
         for project in projects:
diff --git a/tests/sentry/incidents/test_logic.py b/tests/sentry/incidents/test_logic.py
index fc6658e5b7..f69098416e 100644
--- a/tests/sentry/incidents/test_logic.py
+++ b/tests/sentry/incidents/test_logic.py
@@ -248,8 +248,8 @@ class BaseIncidentEventStatsTest(BaseIncidentsTest):
         # Duration of 300s, but no alert rule
         time_window = incident.alert_rule.snuba_query.time_window if incident.alert_rule else 60
         assert result.rollup == time_window
-        expected_start = start if start else incident.date_started - timedelta(minutes=1)
-        expected_end = end if end else incident.current_end_date + timedelta(minutes=1)
+        expected_start = start if start else incident.date_started - timedelta(seconds=time_window)
+        expected_end = end if end else incident.current_end_date + timedelta(seconds=time_window)
 
         if windowed_stats:
             now = timezone.now()
@@ -270,6 +270,18 @@ class BaseIncidentEventStatsTest(BaseIncidentsTest):
 
 @freeze_time()
 class GetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
+    @fixture
+    def bucket_incident(self):
+        incident_start = self.now.replace(minute=0, second=0, microsecond=0) - timedelta(minutes=23)
+        self.create_event(incident_start + timedelta(seconds=1))
+        self.create_event(incident_start + timedelta(minutes=2))
+        self.create_event(incident_start + timedelta(minutes=6))
+        self.create_event(incident_start + timedelta(minutes=9, seconds=59))
+        self.create_event(incident_start + timedelta(minutes=14))
+        self.create_event(incident_start + timedelta(minutes=16))
+        alert_rule = self.create_alert_rule(time_window=10)
+        return self.create_incident(date_started=incident_start, query="", alert_rule=alert_rule)
+
     def run_test(self, incident, expected_results, start=None, end=None, windowed_stats=False):
         kwargs = {}
         if start is not None:
@@ -281,24 +293,37 @@ class GetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
         self.validate_result(incident, result, expected_results, start, end, windowed_stats)
 
     def test_project(self):
-        self.run_test(self.project_incident, [2, 1])
-        self.run_test(self.project_incident, [1], start=self.now - timedelta(minutes=1))
-        self.run_test(self.project_incident, [2], end=self.now - timedelta(minutes=1, seconds=59))
+        self.run_test(self.project_incident, [0, 2, 1])
+        self.run_test(self.project_incident, [0, 1], start=self.now - timedelta(minutes=1))
+        self.run_test(
+            self.project_incident, [0, 2], end=self.now - timedelta(minutes=1, seconds=59)
+        )
 
-        self.run_test(self.project_incident, [2, 1], windowed_stats=True)
+        self.run_test(self.project_incident, [0, 2, 1], windowed_stats=True)
         self.run_test(
             self.project_incident,
-            [2, 1],
+            [0, 2, 1],
             start=self.now - timedelta(minutes=1),
             windowed_stats=True,
         )
         self.run_test(
             self.project_incident,
-            [2, 1],
+            [0, 2, 1],
             end=self.now - timedelta(minutes=1, seconds=59),
             windowed_stats=True,
         )
 
+    def test_start_bucket(self):
+        self.run_test(self.bucket_incident, [2, 4, 2, 2])
+
+    def test_start_and_end_bucket(self):
+        self.create_event(self.bucket_incident.date_started + timedelta(minutes=19))
+
+        self.bucket_incident.update(
+            date_closed=self.bucket_incident.date_started + timedelta(minutes=18)
+        )
+        self.run_test(self.bucket_incident, [2, 4, 2, 3, 1])
+
     def test_with_transactions(self):
         incident = self.project_incident
         alert_rule = self.create_alert_rule(
@@ -316,7 +341,7 @@ class GetIncidentEventStatsTest(TestCase, BaseIncidentEventStatsTest):
         event_data["transaction"] = "/foo_transaction/"
         self.store_event(data=event_data, project_id=self.project.id)
 
-        self.run_test(incident, [2, 1])
+        self.run_test(incident, [0, 2, 1])
 
 
 class BaseIncidentAggregatesTest(BaseIncidentsTest):
@@ -349,13 +374,13 @@ class CreateEventStatTest(TestCase, BaseIncidentsTest):
         snapshot = create_event_stat_snapshot(incident, windowed_stats=False)
         assert snapshot.start == incident.date_started - timedelta(minutes=1)
         assert snapshot.end == incident.current_end_date + timedelta(minutes=1)
-        assert [row[1] for row in snapshot.values] == [2, 1]
+        assert [row[1] for row in snapshot.values] == [0, 2, 1]
 
         snapshot = create_event_stat_snapshot(incident, windowed_stats=True)
         expected_start, expected_end = calculate_incident_time_range(incident, windowed_stats=True)
         assert snapshot.start == expected_start
         assert snapshot.end == expected_end
-        assert [row[1] for row in snapshot.values] == [2, 1]
+        assert [row[1] for row in snapshot.values] == [0, 2, 1]
 
 
 @freeze_time()
