commit d400cc3c1b3e9f3f7a9578fd9682493ebb228ea8
Author: David Cramer <dcramer@gmail.com>
Date:   Sat Jun 7 10:19:48 2014 -0400

    Various fixes/cleanup for http usage

diff --git a/src/sentry/http.py b/src/sentry/http.py
index e32e75410f..68806f317b 100644
--- a/src/sentry/http.py
+++ b/src/sentry/http.py
@@ -74,19 +74,16 @@ def safe_urlopen(url, data=None, headers=DEFAULT_HEADERS,
 
 
 def safe_urlread(request):
-    headers = dict(request.headers)
-
     body = request.read()
 
-    if headers.get('content-encoding') == 'gzip':
+    if request.headers.get('content-encoding') == 'gzip':
         # Content doesn't *have* to respect the Accept-Encoding header
         # and may send gzipped data regardless.
         # See: http://stackoverflow.com/questions/2423866/python-decompressing-gzip-chunk-by-chunk/2424549#2424549
         body = zlib.decompress(body, 16 + zlib.MAX_WBITS)
 
-    try:
-        content_type = headers['content-type']
-    except KeyError:
+    content_type = request.headers.get('content-type')
+    if content_type is None:
         # If there is no content_type header at all, quickly assume default utf-8 encoding
         encoding = DEFAULT_ENCODING
     else:
diff --git a/src/sentry/tasks/fetch_source.py b/src/sentry/tasks/fetch_source.py
index bed90e6097..4a278cda86 100644
--- a/src/sentry/tasks/fetch_source.py
+++ b/src/sentry/tasks/fetch_source.py
@@ -121,7 +121,7 @@ def fetch_url(url):
     Attempts to fetch from the cache.
     """
 
-    cache_key = 'fetch_url:v2:%s' % (
+    cache_key = 'source:%s' % (
         hashlib.md5(url.encode('utf-8')).hexdigest(),)
     result = cache.get(cache_key)
     if result is None:
@@ -133,8 +133,8 @@ def fetch_url(url):
             return BAD_SOURCE
 
         try:
-            req = safe_urlopen(url, allow_redirects=True,
-                               timeout=settings.SENTRY_SOURCE_FETCH_TIMEOUT)
+            request = safe_urlopen(url, allow_redirects=True,
+                                   timeout=settings.SENTRY_SOURCE_FETCH_TIMEOUT)
         except Exception:
             # it's likely we've failed due to a timeout, dns, etc so let's
             # ensure we can't cascade the failure by pinning this for 5 minutes
@@ -144,9 +144,11 @@ def fetch_url(url):
             return BAD_SOURCE
 
         try:
-            result = safe_urlread(req)
+            body = safe_urlread(request)
         except Exception:
             result = BAD_SOURCE
+        else:
+            result = (dict(request.headers), body)
 
         if result == BAD_SOURCE:
             timeout = 300
@@ -157,7 +159,7 @@ def fetch_url(url):
     if result == BAD_SOURCE:
         return result
 
-    return UrlResult(*result)
+    return UrlResult(url, *result)
 
 
 def fetch_sourcemap(url):
