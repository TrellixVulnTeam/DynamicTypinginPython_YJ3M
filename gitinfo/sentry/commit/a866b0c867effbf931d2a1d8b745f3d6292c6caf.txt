commit a866b0c867effbf931d2a1d8b745f3d6292c6caf
Author: Dan Fuller <dfuller@sentry.io>
Date:   Fri Apr 5 13:34:34 2019 -0700

    refs: Remove mysql related code
    
    Removes all code related to running mysql. Also removes uses of `.replace(microsecond=0)` where
    appropriate, since most of the time that was used it was for mysql compatibility.

diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 2d478465d8..c22f987aa5 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -135,8 +135,6 @@ if 'DATABASE_URL' in os.environ:
     if url.scheme == 'postgres':
         DATABASES['default']['ENGINE'] = 'sentry.db.postgres'
 
-    if url.scheme == 'mysql':
-        DATABASES['default']['ENGINE'] = 'django.db.backends.mysql'
 
 # This should always be UTC.
 TIME_ZONE = 'UTC'
diff --git a/src/sentry/db/models/fields/bounded.py b/src/sentry/db/models/fields/bounded.py
index a6630ec97f..78da8e8fc2 100644
--- a/src/sentry/db/models/fields/bounded.py
+++ b/src/sentry/db/models/fields/bounded.py
@@ -71,11 +71,7 @@ if settings.SENTRY_USE_BIG_INTS:
 
         def db_type(self, connection):
             engine = connection.settings_dict['ENGINE']
-            if 'mysql' in engine:
-                return "bigint AUTO_INCREMENT"
-            elif 'oracle' in engine:
-                return "NUMBER(19)"
-            elif 'postgres' in engine:
+            if 'postgres' in engine:
                 return "bigserial"
             # SQLite doesnt actually support bigints with auto incr
             elif 'sqlite' in engine:
diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index cfa598b62a..45af3ad316 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -52,7 +52,7 @@ from sentry.utils.data_filters import (
     FilterStatKeys,
 )
 from sentry.utils.dates import to_timestamp
-from sentry.utils.db import is_postgres, is_mysql
+from sentry.utils.db import is_postgres
 from sentry.utils.safe import safe_execute, trim, get_path, setdefault_path
 from sentry.utils.geo import rust_geoip
 from sentry.utils.validators import is_float
@@ -239,11 +239,6 @@ def scoreclause_sql(sc, connection):
             sql = 'log(times_seen + %d) * 600 + %d' % (sc.times_seen, to_timestamp(sc.last_seen))
         else:
             sql = 'log(times_seen) * 600 + last_seen::abstime::int'
-    elif is_mysql(db):
-        if has_values:
-            sql = 'log(times_seen + %d) * 600 + %d' % (sc.times_seen, to_timestamp(sc.last_seen))
-        else:
-            sql = 'log(times_seen) * 600 + unix_timestamp(last_seen)'
     else:
         # XXX: if we cant do it atomically let's do it the best we can
         sql = int(sc)
diff --git a/src/sentry/middleware/maintenance.py b/src/sentry/middleware/maintenance.py
index 98f7433a85..64b66eac75 100644
--- a/src/sentry/middleware/maintenance.py
+++ b/src/sentry/middleware/maintenance.py
@@ -16,13 +16,6 @@ logger = logging.getLogger('sentry.errors')
 
 DB_ERRORS = []
 
-try:
-    import MySQLdb
-except ImportError:
-    pass
-else:
-    DB_ERRORS.append(MySQLdb.OperationalError)
-
 try:
     import psycopg2
 except ImportError:
diff --git a/src/sentry/models/counter.py b/src/sentry/models/counter.py
index cfad86079f..df2b25af2e 100644
--- a/src/sentry/models/counter.py
+++ b/src/sentry/models/counter.py
@@ -12,7 +12,7 @@ from django.db import connection, connections
 from django.db.models.signals import post_syncdb
 
 from sentry.db.models import (FlexibleForeignKey, Model, sane_repr, BoundedBigIntegerField)
-from sentry.utils.db import is_mysql, is_postgres, is_sqlite
+from sentry.utils.db import is_postgres, is_sqlite
 
 
 class Counter(Model):
@@ -75,18 +75,6 @@ def increment_project_counter(project, delta=1):
                 ).fetchone()[0]
                 if changes != 0:
                     return value + delta
-        elif is_mysql():
-            cur.execute(
-                '''
-                insert into sentry_projectcounter
-                            (project_id, value)
-                     values (%s, @new_val := %s)
-           on duplicate key
-                     update value = @new_val := value + %s
-            ''', [project.id, delta, delta]
-            )
-            cur.execute('select @new_val')
-            return cur.fetchone()[0]
         else:
             raise AssertionError("Not implemented database engine path")
     finally:
diff --git a/src/sentry/models/debugfile.py b/src/sentry/models/debugfile.py
index dc8a70b332..4ab61a7312 100644
--- a/src/sentry/models/debugfile.py
+++ b/src/sentry/models/debugfile.py
@@ -37,7 +37,6 @@ from sentry.models.file import File
 from sentry.reprocessing import resolve_processing_issue, \
     bump_reprocessing_revision
 from sentry.utils import metrics
-from sentry.utils.db import mysql_disabled_integrity
 from sentry.utils.zip import safe_extract_zip
 from sentry.utils.decorators import classproperty
 
@@ -205,27 +204,26 @@ class ProjectDebugFile(Model):
     def delete(self, *args, **kwargs):
         dif_id = self.id
 
-        with mysql_disabled_integrity(db=ProjectDebugFile.objects.db):
-            with transaction.atomic():
-                # First, delete the debug file entity. This ensures no other
-                # worker can attach caches to it. Integrity checks are deferred
-                # within this transaction, so existing caches stay intact.
-                super(ProjectDebugFile, self).delete(*args, **kwargs)
-
-                # Explicitly select referencing caches and delete them. Using
-                # the backref does not work, since `dif.id` is None after the
-                # delete.
-                symcaches = ProjectSymCacheFile.objects \
-                    .filter(debug_file_id=dif_id) \
-                    .select_related('cache_file')
-                for symcache in symcaches:
-                    symcache.delete()
-
-                cficaches = ProjectCfiCacheFile.objects \
-                    .filter(debug_file_id=dif_id) \
-                    .select_related('cache_file')
-                for cficache in cficaches:
-                    cficache.delete()
+        with transaction.atomic():
+            # First, delete the debug file entity. This ensures no other
+            # worker can attach caches to it. Integrity checks are deferred
+            # within this transaction, so existing caches stay intact.
+            super(ProjectDebugFile, self).delete(*args, **kwargs)
+
+            # Explicitly select referencing caches and delete them. Using
+            # the backref does not work, since `dif.id` is None after the
+            # delete.
+            symcaches = ProjectSymCacheFile.objects \
+                .filter(debug_file_id=dif_id) \
+                .select_related('cache_file')
+            for symcache in symcaches:
+                symcache.delete()
+
+            cficaches = ProjectCfiCacheFile.objects \
+                .filter(debug_file_id=dif_id) \
+                .select_related('cache_file')
+            for cficache in cficaches:
+                cficache.delete()
 
         self.file.delete()
 
diff --git a/src/sentry/models/recentsearch.py b/src/sentry/models/recentsearch.py
index dba7c67fac..20c4ac152a 100644
--- a/src/sentry/models/recentsearch.py
+++ b/src/sentry/models/recentsearch.py
@@ -6,7 +6,6 @@ from django.dispatch import receiver
 from django.utils import timezone
 
 from sentry.db.models import FlexibleForeignKey, Model, sane_repr
-from sentry.utils.db import is_mysql
 from sentry.utils.hashlib import md5_text
 
 
@@ -46,9 +45,6 @@ def remove_excess_recent_searches(organization, user, search_type):
         user=user,
         type=search_type,
     ).order_by('-last_seen')[MAX_RECENT_SEARCHES:]
-    if is_mysql():
-        # Mysql doesn't support limits in these types of subqueries
-        recent_searches_to_remove = list(recent_searches_to_remove.values_list("id", flat=True))
     RecentSearch.objects.filter(id__in=recent_searches_to_remove).delete()
 
 
diff --git a/src/sentry/search/django/backend.py b/src/sentry/search/django/backend.py
index 12980ebfeb..fe98e72dc0 100644
--- a/src/sentry/search/django/backend.py
+++ b/src/sentry/search/django/backend.py
@@ -20,8 +20,7 @@ from sentry.api.paginator import DateTimePaginator, Paginator, SequencePaginator
 from sentry.api.event_search import InvalidSearchQuery
 from sentry.search.base import SearchBackend
 from sentry.search.django.constants import (
-    MSSQL_ENGINES, MSSQL_SORT_CLAUSES, MYSQL_SORT_CLAUSES, ORACLE_SORT_CLAUSES, SORT_CLAUSES,
-    SQLITE_SORT_CLAUSES
+    SORT_CLAUSES, SQLITE_SORT_CLAUSES,
 )
 from sentry.utils.dates import to_timestamp
 from sentry.utils.db import get_db_engine, is_postgres
@@ -252,12 +251,6 @@ def get_sort_clause(sort_by):
     engine = get_db_engine('default')
     if engine.startswith('sqlite'):
         return SQLITE_SORT_CLAUSES[sort_by]
-    elif engine.startswith('mysql'):
-        return MYSQL_SORT_CLAUSES[sort_by]
-    elif engine.startswith('oracle'):
-        return ORACLE_SORT_CLAUSES[sort_by]
-    elif engine in MSSQL_ENGINES:
-        return MSSQL_SORT_CLAUSES[sort_by]
     else:
         return SORT_CLAUSES[sort_by]
 
diff --git a/src/sentry/search/django/constants.py b/src/sentry/search/django/constants.py
index de60313a76..78df14f4a2 100644
--- a/src/sentry/search/django/constants.py
+++ b/src/sentry/search/django/constants.py
@@ -24,30 +24,3 @@ SQLITE_SORT_CLAUSES.update(
         "cast((julianday(sentry_groupedmessage.first_seen) - 2440587.5) * 86400.0 as INTEGER)",
     }
 )
-
-MYSQL_SORT_CLAUSES = SORT_CLAUSES.copy()
-MYSQL_SORT_CLAUSES.update(
-    {
-        'date': 'UNIX_TIMESTAMP(sentry_groupedmessage.last_seen)',
-        'new': 'UNIX_TIMESTAMP(sentry_groupedmessage.first_seen)',
-    }
-)
-
-ORACLE_SORT_CLAUSES = SORT_CLAUSES.copy()
-ORACLE_SORT_CLAUSES.update(
-    {
-        'date':
-        "(cast(sentry_groupedmessage.last_seen as date)-TO_DATE('01/01/1970 00:00:00', 'MM-DD-YYYY HH24:MI:SS')) * 24 * 60 * 60",
-        'new':
-        "(cast(sentry_groupedmessage.first_seen as date)-TO_DATE('01/01/1970 00:00:00', 'MM-DD-YYYY HH24:MI:SS')) * 24 * 60 * 60",
-    }
-)
-
-MSSQL_SORT_CLAUSES = SORT_CLAUSES.copy()
-MSSQL_SORT_CLAUSES.update(
-    {
-        'date': "DATEDIFF(s, '1970-01-01T00:00:00', sentry_groupedmessage.last_seen)",
-        'new': "DATEDIFF(s, '1970-01-01T00:00:00', sentry_groupedmessage.first_seen)",
-    }
-)
-MSSQL_ENGINES = set(['django_pytds', 'sqlserver_ado', 'sql_server.pyodbc'])
diff --git a/src/sentry/south_migrations/0246_auto__add_dsymsymbol__add_unique_dsymsymbol_object_address__add_dsymsd.py b/src/sentry/south_migrations/0246_auto__add_dsymsymbol__add_unique_dsymsymbol_object_address__add_dsymsd.py
index 2e23802ab0..f542992372 100644
--- a/src/sentry/south_migrations/0246_auto__add_dsymsymbol__add_unique_dsymsymbol_object_address__add_dsymsd.py
+++ b/src/sentry/south_migrations/0246_auto__add_dsymsymbol__add_unique_dsymsymbol_object_address__add_dsymsd.py
@@ -3,7 +3,6 @@ from south.utils import datetime_utils as datetime
 from south.db import db
 from south.v2 import SchemaMigration
 from django.db import models
-from sentry.utils.db import is_mysql
 
 
 class Migration(SchemaMigration):
@@ -50,7 +49,8 @@ class Migration(SchemaMigration):
         )
         db.send_create_signal('sentry', ['DSymSDK'])
 
-        # Adding index on 'DSymSDK', fields ['version_major', 'version_minor', 'version_patchlevel', 'version_build']
+        # Adding index on 'DSymSDK', fields ['version_major', 'version_minor',
+        # 'version_patchlevel', 'version_build']
         db.create_index(
             'sentry_dsymsdk',
             ['version_major', 'version_minor', 'version_patchlevel', 'version_build']
@@ -65,7 +65,7 @@ class Migration(SchemaMigration):
                     )
                 ), ('cpu_name', self.gf('django.db.models.fields.CharField')(max_length=40)), (
                     'object_path',
-                    self.gf('django.db.models.fields.TextField')(db_index=not is_mysql())
+                    self.gf('django.db.models.fields.TextField')(db_index=True)
                 ), (
                     'uuid',
                     self.gf('django.db.models.fields.CharField')(max_length=36, db_index=True)
@@ -79,18 +79,6 @@ class Migration(SchemaMigration):
             )
         )
 
-        # On MySQL we need to create the index here differently because
-        # the index must have a limit.  As we have the type already
-        # defined to text in the model earlier we just restrict the index
-        # to 255.  The hash matches what south would have created.
-        if is_mysql():
-            db.execute(
-                '''
-                create index sentry_dsymobject_39c06cbd
-                          on sentry_dsymobject (object_path(255))
-            '''
-            )
-
         db.send_create_signal('sentry', ['DSymObject'])
 
         # Adding model 'DSymBundle'
@@ -114,7 +102,8 @@ class Migration(SchemaMigration):
         db.send_create_signal('sentry', ['DSymBundle'])
 
     def backwards(self, orm):
-        # Removing index on 'DSymSDK', fields ['version_major', 'version_minor', 'version_patchlevel', 'version_build']
+        # Removing index on 'DSymSDK', fields ['version_major', 'version_minor',
+        # 'version_patchlevel', 'version_build']
         db.delete_index(
             'sentry_dsymsdk',
             ['version_major', 'version_minor', 'version_patchlevel', 'version_build']
diff --git a/src/sentry/tagstore/v2/backend.py b/src/sentry/tagstore/v2/backend.py
index 7a9e46719d..f53e4f294b 100644
--- a/src/sentry/tagstore/v2/backend.py
+++ b/src/sentry/tagstore/v2/backend.py
@@ -157,7 +157,7 @@ class V2TagStorage(TagStorage):
 
                 # required to deal with custom SQL queries and the ORM
                 # in `bulk_delete_objects`
-                key_id_field_name = 'key_id' if (db.is_postgres() or db.is_mysql()) else '_key_id'
+                key_id_field_name = 'key_id' if (db.is_postgres()) else '_key_id'
 
                 relations = [
                     ModelRelation(m, query={
diff --git a/src/sentry/tasks/deletion.py b/src/sentry/tasks/deletion.py
index c188cf6832..98af07b4bf 100644
--- a/src/sentry/tasks/deletion.py
+++ b/src/sentry/tasks/deletion.py
@@ -108,7 +108,7 @@ def revoke_api_tokens(object_id, transaction_id=None, timestamp=None, **kwargs):
         queryset = queryset.filter(date_added__lte=timestamp)
 
     # we're using a slow deletion strategy to avoid a lot of custom code for
-    # mysql/postgres
+    # postgres
     has_more = False
     for obj in queryset[:1000]:
         obj.delete()
diff --git a/src/sentry/testutils/asserts.py b/src/sentry/testutils/asserts.py
index cb22e63a9c..68003d9952 100644
--- a/src/sentry/testutils/asserts.py
+++ b/src/sentry/testutils/asserts.py
@@ -9,11 +9,6 @@ from __future__ import absolute_import
 from sentry.models import CommitFileChange
 
 
-def assert_date_resembles(one, two):
-    # this is mostly intended to handle discrepancies between mysql/postgres
-    assert one.replace(microsecond=0) == two.replace(microsecond=0)
-
-
 def assert_mock_called_once_with_partial(mock, *args, **kwargs):
     """
     Similar to ``mock.assert_called_once_with()``, but we dont require all
diff --git a/src/sentry/testutils/cases.py b/src/sentry/testutils/cases.py
index 26416dca6f..94f46339d8 100644
--- a/src/sentry/testutils/cases.py
+++ b/src/sentry/testutils/cases.py
@@ -351,9 +351,7 @@ class BaseTestCase(Fixtures, Exam):
             assert deleted_log.organization_name == original_object.organization.name
             assert deleted_log.organization_slug == original_object.organization.slug
 
-        # Truncating datetime for mysql compatibility
-        assert deleted_log.date_created.replace(
-            microsecond=0) == original_object.date_added.replace(microsecond=0)
+        assert deleted_log.date_created == original_object.date_added
         assert deleted_log.date_deleted >= deleted_log.date_created
 
     def assertWriteQueries(self, queries, debug=False, *args, **kwargs):
diff --git a/src/sentry/testutils/factories.py b/src/sentry/testutils/factories.py
index 662b319a2f..22452262e9 100644
--- a/src/sentry/testutils/factories.py
+++ b/src/sentry/testutils/factories.py
@@ -265,7 +265,7 @@ class Factories(object):
             version = os.urandom(20).encode('hex')
 
         if date_added is None:
-            date_added = timezone.now().replace(microsecond=0)
+            date_added = timezone.now()
 
         release = Release.objects.create(
             version=version,
diff --git a/src/sentry/testutils/helpers/query.py b/src/sentry/testutils/helpers/query.py
index 9974787b97..482fdb78d3 100644
--- a/src/sentry/testutils/helpers/query.py
+++ b/src/sentry/testutils/helpers/query.py
@@ -27,8 +27,6 @@ def parse_queries(captured_queries):
                     table_name = parsed[0].get_real_name()
                     if parsed[0].get_real_name() == "*":  # DELETE * FROM ...
                         table_name = parsed[0].get_name()
-                    # there is ` in mysql queries
-                    table_name = table_name.replace('`', '')
                     if real_queries.get(table_name) is None:
                         real_queries[table_name] = 0
                     real_queries[table_name] += 1
diff --git a/src/sentry/utils/dates.py b/src/sentry/utils/dates.py
index c8b4f2d766..f59a778e41 100644
--- a/src/sentry/utils/dates.py
+++ b/src/sentry/utils/dates.py
@@ -19,17 +19,10 @@ import pytz
 from dateutil.parser import parse
 from django.db import connections
 
-from sentry.utils.db import get_db_engine
-
 DATE_TRUNC_GROUPERS = {
-    'oracle': {
-        'hour': 'hh24',
-    },
-    'default': {
-        'date': 'day',
-        'hour': 'hour',
-        'minute': 'minute',
-    },
+    'date': 'day',
+    'hour': 'hour',
+    'minute': 'minute',
 }
 
 epoch = datetime(1970, 1, 1, tzinfo=pytz.utc)
@@ -67,16 +60,7 @@ def floor_to_utc_day(value):
 
 def get_sql_date_trunc(col, db='default', grouper='hour'):
     conn = connections[db]
-
-    engine = get_db_engine(db)
-    # TODO: does extract work for sqlite?
-    if engine.startswith('oracle'):
-        method = DATE_TRUNC_GROUPERS['oracle'].get(
-            grouper, DATE_TRUNC_GROUPERS['default'][grouper])
-        if '"' not in col:
-            col = '"%s"' % col.upper()
-    else:
-        method = DATE_TRUNC_GROUPERS['default'][grouper]
+    method = DATE_TRUNC_GROUPERS[grouper]
     return conn.ops.date_trunc_sql(method, col)
 
 
diff --git a/src/sentry/utils/db.py b/src/sentry/utils/db.py
index 0500532405..dfdbce12b4 100644
--- a/src/sentry/utils/db.py
+++ b/src/sentry/utils/db.py
@@ -8,7 +8,6 @@ sentry.utils.db
 from __future__ import absolute_import
 
 import six
-from contextlib import contextmanager, closing
 
 from django.conf import settings
 from django.db import connections, DEFAULT_DB_ALIAS
@@ -17,8 +16,6 @@ from django.db.models.fields.related import SingleRelatedObjectDescriptor
 
 def get_db_engine(alias='default'):
     value = settings.DATABASES[alias]['ENGINE']
-    if value == 'mysql.connector.django':
-        return 'mysql'
     return value.rsplit('.', 1)[-1]
 
 
@@ -27,11 +24,6 @@ def is_postgres(alias='default'):
     return 'postgres' in engine
 
 
-def is_mysql(alias='default'):
-    engine = get_db_engine(alias)
-    return 'mysql' in engine
-
-
 def is_sqlite(alias='default'):
     engine = get_db_engine(alias)
     return 'sqlite' in engine
@@ -105,19 +97,3 @@ def attach_foreignkey(objects, field, related=[], database=None):
 
 def table_exists(name, using=DEFAULT_DB_ALIAS):
     return name in connections[using].introspection.table_names()
-
-
-def _set_mysql_foreign_key_checks(flag, using=DEFAULT_DB_ALIAS):
-    if is_mysql():
-        query = 'SET FOREIGN_KEY_CHECKS=%s' % (1 if flag else 0)
-        with closing(connections[using].cursor()) as cursor:
-            cursor.execute(query)
-
-
-@contextmanager
-def mysql_disabled_integrity(db=DEFAULT_DB_ALIAS):
-    try:
-        _set_mysql_foreign_key_checks(False, using=db)
-        yield
-    finally:
-        _set_mysql_foreign_key_checks(True, using=db)
diff --git a/src/sentry/utils/pytest/sentry.py b/src/sentry/utils/pytest/sentry.py
index 7a5b7467de..eccb242d1d 100644
--- a/src/sentry/utils/pytest/sentry.py
+++ b/src/sentry/utils/pytest/sentry.py
@@ -38,17 +38,7 @@ def pytest_configure(config):
     if not settings.configured:
         # only configure the db if its not already done
         test_db = os.environ.get('DB', 'postgres')
-        if test_db == 'mysql':
-            settings.DATABASES['default'].update(
-                {
-                    'ENGINE': 'django.db.backends.mysql',
-                    'NAME': 'sentry',
-                    'USER': 'root',
-                    'HOST': '127.0.0.1',
-                }
-            )
-            # mysql requires running full migration all the time
-        elif test_db == 'postgres':
+        if test_db == 'postgres':
             settings.DATABASES['default'].update(
                 {
                     'ENGINE': 'sentry.db.postgres',
diff --git a/src/sentry/utils/query.py b/src/sentry/utils/query.py
index 7ab7ceb043..06dc7aaa65 100644
--- a/src/sentry/utils/query.py
+++ b/src/sentry/utils/query.py
@@ -324,17 +324,6 @@ def bulk_delete_objects(model, limit=10000, transaction_id=None,
             table=model._meta.db_table,
             limit=limit,
         )
-    elif db.is_mysql():
-        query = """
-            delete from %(table)s
-            where %(partition_query)s (%(query)s)
-            limit %(limit)d
-        """ % dict(
-            partition_query=(' AND '.join(partition_query)) + (' AND ' if partition_query else ''),
-            query=' AND '.join(query),
-            table=model._meta.db_table,
-            limit=limit,
-        )
     else:
         if logger is not None:
             logger.warning('Using slow deletion strategy due to unknown database')
diff --git a/src/sentry/utils/settings.py b/src/sentry/utils/settings.py
index 673d92360c..d067c74f8c 100644
--- a/src/sentry/utils/settings.py
+++ b/src/sentry/utils/settings.py
@@ -16,8 +16,6 @@ from sentry.utils.imports import import_string
 PACKAGES = {
     'django.db.backends.postgresql_psycopg2': 'psycopg2.extensions',
     'sentry.db.postgres': 'psycopg2.extensions',
-    'django.db.backends.mysql': 'MySQLdb',
-    'django.db.backends.oracle': 'cx_Oracle',
     'django.core.cache.backends.memcached.MemcachedCache': 'memcache',
     'django.core.cache.backends.memcached.PyLibMCCache': 'pylibmc'
 }
diff --git a/src/south/db/__init__.py b/src/south/db/__init__.py
index a74ec23b47..06633cce62 100644
--- a/src/south/db/__init__.py
+++ b/src/south/db/__init__.py
@@ -9,8 +9,6 @@ import sys
 engine_modules = {
     'django.db.backends.postgresql_psycopg2': 'postgresql_psycopg2',
     'django.db.backends.sqlite3': 'sqlite3',
-    'django.db.backends.mysql': 'mysql',
-    'mysql.connector.django': 'mysql',  # MySQL Connector/Python
 }
 
 # First, work out if we're multi-db or not, and which databases we have
diff --git a/src/south/db/generic.py b/src/south/db/generic.py
index 72c53e78ec..afdd4f79fd 100644
--- a/src/south/db/generic.py
+++ b/src/south/db/generic.py
@@ -119,13 +119,6 @@ class DatabaseOperations(object):
         connection = self._get_connection()
         if hasattr(connection.features, "confirm") and not connection.features._confirmed:
             connection.features.confirm()
-        # Django 1.3's MySQLdb backend doesn't raise DatabaseError
-        exceptions = (DatabaseError, )
-        try:
-            from MySQLdb import OperationalError
-            exceptions += (OperationalError, )
-        except ImportError:
-            pass
         # Now do the test
         if getattr(connection.features, 'supports_transactions', True):
             cursor = connection.cursor()
@@ -135,7 +128,7 @@ class DatabaseOperations(object):
             try:
                 try:
                     cursor.execute('CREATE TABLE DDL_TRANSACTION_TEST (X INT)')
-                except exceptions:
+                except DatabaseError:
                     return False
                 else:
                     return True
@@ -253,7 +246,7 @@ class DatabaseOperations(object):
     def connection_init(self):
         """
         Run before any SQL to let database-specific config be sent as a command,
-        e.g. which storage engine (MySQL) or transaction serialisability level.
+        e.g. which storage engine or transaction serialisability level.
         """
         pass
 
@@ -353,7 +346,7 @@ class DatabaseOperations(object):
         """
 
         if len(table_name) > 63:
-            print("   ! WARNING: You have a table name longer than 63 characters; this will not fully work on PostgreSQL or MySQL.")
+            print("   ! WARNING: You have a table name longer than 63 characters; this will not fully work on PostgreSQL.")
 
         # avoid default values in CREATE TABLE statements (#925)
         for field_name, field in fields:
@@ -550,7 +543,7 @@ class DatabaseOperations(object):
                 flatten(values),
             )
         else:
-            # Databases like e.g. MySQL don't like more than one alter at once.
+            # Some databases don't like more than one alter at once.
             for sql, values in sqls:
                 self.execute("ALTER TABLE %s %s;" % (self.quote_name(table_name), sql), values)
 
@@ -684,7 +677,7 @@ class DatabaseOperations(object):
         if hasattr(field, 'south_init'):
             field.south_init()
 
-        # Possible hook to fiddle with the fields (e.g. defaults & TEXT on MySQL)
+        # Possible hook to fiddle with the fields
         field = self._field_sanity(field)
 
         try:
@@ -780,8 +773,7 @@ class DatabaseOperations(object):
 
     def _field_sanity(self, field):
         """
-        Placeholder for DBMS-specific field alterations (some combos aren't valid,
-        e.g. DEFAULT and TEXT on MySQL)
+        Placeholder for DBMS-specific field alterations (some combos aren't valid)
         """
         return field
 
diff --git a/src/south/db/mysql.py b/src/south/db/mysql.py
deleted file mode 100644
index d726243124..0000000000
--- a/src/south/db/mysql.py
+++ /dev/null
@@ -1,297 +0,0 @@
-# MySQL-specific implementations for south
-# Original author: Andrew Godwin
-# Patches by: F. Gabriel Gosselin <gabrielNOSPAM@evidens.ca>
-
-from south.db import generic
-from south.db.generic import DryRunError, INVALID
-from south.logger import get_logger
-
-
-def delete_column_constraints(func):
-    """
-    Decorates column operation functions for MySQL.
-    Deletes the constraints from the database and clears local cache.
-    """
-
-    def _column_rm(self, table_name, column_name, *args, **opts):
-        # Delete foreign key constraints
-        try:
-            self.delete_foreign_key(table_name, column_name)
-        except ValueError:
-            pass  # If no foreign key on column, OK because it checks first
-        # Delete constraints referring to this column
-        try:
-            reverse = self._lookup_reverse_constraint(table_name, column_name)
-            for cname, rtable, rcolumn in reverse:
-                self.delete_foreign_key(rtable, rcolumn)
-        except DryRunError:
-            pass
-        return func(self, table_name, column_name, *args, **opts)
-    return _column_rm
-
-
-def copy_column_constraints(func):
-    """
-    Decorates column operation functions for MySQL.
-    Determines existing constraints and copies them to a new column
-    """
-
-    def _column_cp(self, table_name, column_old, column_new, *args, **opts):
-        # Copy foreign key constraint
-        try:
-            constraint = self._find_foreign_constraints(
-                table_name, column_old)[0]
-            refs = self._lookup_constraint_references(table_name, constraint)
-            if refs is not None:
-                (ftable, fcolumn) = refs
-                if ftable and fcolumn:
-                    fk_sql = self.foreign_key_sql(
-                        table_name, column_new, ftable, fcolumn)
-                    get_logger().debug("Foreign key SQL: " + fk_sql)
-                    self.add_deferred_sql(fk_sql)
-        except IndexError:
-            pass  # No constraint exists so ignore
-        except DryRunError:
-            pass
-        # Copy constraints referring to this column
-        try:
-            reverse = self._lookup_reverse_constraint(table_name, column_old)
-            for cname, rtable, rcolumn in reverse:
-                fk_sql = self.foreign_key_sql(
-                    rtable, rcolumn, table_name, column_new)
-                self.add_deferred_sql(fk_sql)
-        except DryRunError:
-            pass
-        return func(self, table_name, column_old, column_new, *args, **opts)
-    return _column_cp
-
-
-def invalidate_table_constraints(func):
-    """
-    For MySQL we grab all table constraints simultaneously, so this is
-    effective.
-    It further solves the issues of invalidating referred table constraints.
-    """
-
-    def _cache_clear(self, table, *args, **opts):
-        db_name = self._get_setting('NAME')
-        if db_name in self._constraint_cache:
-            del self._constraint_cache[db_name]
-        if db_name in self._reverse_cache:
-            del self._reverse_cache[db_name]
-        if db_name in self._constraint_references:
-            del self._constraint_references[db_name]
-        return func(self, table, *args, **opts)
-    return _cache_clear
-
-
-class DatabaseOperations(generic.DatabaseOperations):
-    """
-    MySQL implementation of database operations.
-
-    MySQL has no DDL transaction support This can confuse people when they ask
-    how to roll back - hence the dry runs, etc., found in the migration code.
-    """
-
-    backend_name = "mysql"
-    alter_string_set_type = ''
-    alter_string_set_null = 'MODIFY %(column)s %(type)s NULL;'
-    alter_string_drop_null = 'MODIFY %(column)s %(type)s NOT NULL;'
-    drop_index_string = 'DROP INDEX %(index_name)s ON %(table_name)s'
-    delete_primary_key_sql = "ALTER TABLE %(table)s DROP PRIMARY KEY"
-    delete_foreign_key_sql = "ALTER TABLE %(table)s DROP FOREIGN KEY %(constraint)s"
-    delete_unique_sql = "ALTER TABLE %s DROP INDEX %s"
-    rename_table_sql = "RENAME TABLE %s TO %s;"
-
-    allows_combined_alters = False
-    has_check_constraints = False
-    raises_default_errors = False
-
-    geom_types = ['geometry', 'point', 'linestring', 'polygon']
-    text_types = ['text', 'blob']
-
-    def __init__(self, db_alias):
-        self._constraint_references = {}
-        self._reverse_cache = {}
-        super(DatabaseOperations, self).__init__(db_alias)
-        if self._has_setting('STORAGE_ENGINE') and self._get_setting('STORAGE_ENGINE'):
-            self.create_table_sql = self.create_table_sql + \
-                ' ENGINE=%s' % self._get_setting('STORAGE_ENGINE')
-
-    def _is_valid_cache(self, db_name, table_name):
-        cache = self._constraint_cache
-        # we cache the whole db so if there are any tables table_name is valid
-        return db_name in cache and cache[db_name].get(table_name, None) is not INVALID
-
-    def _fill_constraint_cache(self, db_name, table_name):
-        # for MySQL grab all constraints for this database.  It's just as cheap as a single column.
-        self._constraint_cache[db_name] = {}
-        self._constraint_cache[db_name][table_name] = {}
-        self._reverse_cache[db_name] = {}
-        self._constraint_references[db_name] = {}
-
-        name_query = """
-            SELECT kc.`constraint_name`, kc.`column_name`, kc.`table_name`,
-                kc.`referenced_table_name`, kc.`referenced_column_name`
-            FROM information_schema.key_column_usage AS kc
-            WHERE
-                kc.table_schema = %s
-        """
-        rows = self.execute(name_query, [db_name])
-        if not rows:
-            return
-        cnames = {}
-        for constraint, column, table, ref_table, ref_column in rows:
-            key = (table, constraint)
-            cnames.setdefault(key, set())
-            cnames[key].add((column, ref_table, ref_column))
-
-        type_query = """
-            SELECT c.constraint_name, c.table_name, c.constraint_type
-            FROM information_schema.table_constraints AS c
-            WHERE
-                c.table_schema = %s
-        """
-        rows = self.execute(type_query, [db_name])
-        for constraint, table, kind in rows:
-            key = (table, constraint)
-            self._constraint_cache[db_name].setdefault(table, {})
-            try:
-                cols = cnames[key]
-            except KeyError:
-                cols = set()
-            for column_set in cols:
-                (column, ref_table, ref_column) = column_set
-                self._constraint_cache[db_name][table].setdefault(column, set())
-                if kind == 'FOREIGN KEY':
-                    self._constraint_cache[db_name][table][column].add((kind,
-                                                                        constraint))
-                    # Create constraint lookup, see constraint_references
-                    self._constraint_references[db_name][(table,
-                                                          constraint)] = (ref_table, ref_column)
-                    # Create reverse table lookup, reverse_lookup
-                    self._reverse_cache[db_name].setdefault(ref_table, {})
-                    self._reverse_cache[db_name][ref_table].setdefault(ref_column,
-                                                                       set())
-                    self._reverse_cache[db_name][ref_table][ref_column].add(
-                        (constraint, table, column))
-                else:
-                    self._constraint_cache[db_name][table][column].add((kind,
-                                                                        constraint))
-
-    def connection_init(self):
-        """
-        Run before any SQL to let database-specific config be sent as a command,
-        e.g. which storage engine (MySQL) or transaction serialisability level.
-        """
-        cursor = self._get_connection().cursor()
-        if cursor.execute("SHOW variables WHERE Variable_Name='default_storage_engine';"):
-            engine_var = 'default_storage_engine'
-        else:
-            engine_var = 'storage_engine'
-        if self._has_setting('STORAGE_ENGINE') and self._get_setting('STORAGE_ENGINE'):
-            cursor.execute("SET %s=%s;" % (engine_var, self._get_setting('STORAGE_ENGINE')))
-
-    def start_transaction(self):
-        super(DatabaseOperations, self).start_transaction()
-        self.execute("SET FOREIGN_KEY_CHECKS=0;")
-
-    @copy_column_constraints
-    @delete_column_constraints
-    @invalidate_table_constraints
-    def rename_column(self, table_name, old, new):
-        if old == new or self.dry_run:
-            return []
-
-        rows = [
-            x for x in self.execute(
-                'DESCRIBE %s' %
-                (self.quote_name(table_name),)) if x[0] == old]
-
-        if not rows:
-            raise ValueError("No column '%s' in '%s'." % (old, table_name))
-
-        params = (
-            self.quote_name(table_name),
-            self.quote_name(old),
-            self.quote_name(new),
-            rows[0][1],
-            rows[0][2] == "YES" and "NULL" or "NOT NULL",
-            rows[0][4] and "DEFAULT " or "",
-            rows[0][4] and "%s" or "",
-            rows[0][5] or "",
-        )
-
-        sql = 'ALTER TABLE %s CHANGE COLUMN %s %s %s %s %s %s %s;' % params
-
-        if rows[0][4]:
-            self.execute(sql, (rows[0][4],))
-        else:
-            self.execute(sql)
-
-    @delete_column_constraints
-    def delete_column(self, table_name, name):
-        super(DatabaseOperations, self).delete_column(table_name, name)
-
-    @invalidate_table_constraints
-    def rename_table(self, old_table_name, table_name):
-        super(DatabaseOperations, self).rename_table(old_table_name,
-                                                     table_name)
-
-    @invalidate_table_constraints
-    def delete_table(self, table_name):
-        super(DatabaseOperations, self).delete_table(table_name)
-
-    def _lookup_constraint_references(self, table_name, cname):
-        """
-        Provided an existing table and constraint, returns tuple of (foreign
-        table, column)
-        """
-        db_name = self._get_setting('NAME')
-        try:
-            return self._constraint_references[db_name][(table_name, cname)]
-        except KeyError:
-            return None
-
-    def _lookup_reverse_constraint(self, table_name, column_name=None):
-        """Look for the column referenced by a foreign constraint"""
-        db_name = self._get_setting('NAME')
-        if self.dry_run:
-            raise DryRunError("Cannot get constraints for columns.")
-
-        if not self._is_valid_cache(db_name, table_name):
-            # Piggy-back on lookup_constraint, ensures cache exists
-            self.lookup_constraint(db_name, table_name)
-
-        try:
-            table = self._reverse_cache[db_name][table_name]
-            if column_name == None:
-                return [(y, tuple(y)) for x, y in table.items()]
-            else:
-                return tuple(table[column_name])
-        except KeyError:
-            return []
-
-    def _field_sanity(self, field):
-        """
-        This particular override stops us sending DEFAULTs for BLOB/TEXT columns.
-        """
-        #  MySQL does not support defaults for geometry columns also
-        type = self._db_type_for_alter_column(field).lower()
-        is_geom = True in [type.find(t) > -1 for t in self.geom_types]
-        is_text = True in [type.find(t) > -1 for t in self.text_types]
-
-        if is_geom or is_text:
-            field._suppress_default = True
-        return field
-
-    def _alter_set_defaults(self, field, name, params, sqls):
-        """
-        MySQL does not support defaults on text or blob columns.
-        """
-        type = params['type']
-        #  MySQL does not support defaults for geometry columns also
-        is_geom = True in [type.find(t) > -1 for t in self.geom_types]
-        is_text = True in [type.find(t) > -1 for t in self.text_types]
-        if not is_geom and not is_text:
-            super(DatabaseOperations, self)._alter_set_defaults(field, name, params, sqls)
diff --git a/src/south/management/commands/syncdb.py b/src/south/management/commands/syncdb.py
index 26a5e7ca05..3e25753c48 100644
--- a/src/south/management/commands/syncdb.py
+++ b/src/south/management/commands/syncdb.py
@@ -103,7 +103,7 @@ class Command(NoArgsCommand):
         if verbosity:
             self.stdout.write("Syncing...\n")
 
-        # This will allow the setting of the MySQL storage engine, for example.
+        # This will allow the setting of the storage engine, for example.
         for db in dbs.values():
             db.connection_init()
 
diff --git a/src/south/migration/migrators.py b/src/south/migration/migrators.py
index 8d1acc3473..367ef2d887 100644
--- a/src/south/migration/migrators.py
+++ b/src/south/migration/migrators.py
@@ -105,8 +105,7 @@ class Migrator(object):
         # Get the correct ORM.
         south.db.db.current_orm = self.orm(migration)
         # If we're not already in a dry run, and the database doesn't support
-        # running DDL inside a transaction, *cough*MySQL*cough* then do a dry
-        # run first.
+        # running DDL inside a transaction, then do a dry run first.
         if not isinstance(getattr(self, '_wrapper', self), DryRunMigrator):
             if not south.db.db.has_ddl_transactions:
                 dry_run = DryRunMigrator(migrator=self, ignore_fail=False)
diff --git a/tests/integration/tests.py b/tests/integration/tests.py
index 8bc86484b8..6ac6e1892e 100644
--- a/tests/integration/tests.py
+++ b/tests/integration/tests.py
@@ -39,30 +39,6 @@ DEPENDENCY_TEST_DATA = {
             }
         }
     ),
-    "mysql": (
-        'DATABASES', 'MySQLdb', "database engine", "django.db.backends.mysql", {
-            'default': {
-                'ENGINE': "django.db.backends.mysql",
-                'NAME': 'test',
-                'USER': 'root',
-                'PASSWORD': '',
-                'HOST': 'localhost',
-                'PORT': ''
-            }
-        }
-    ),
-    "oracle": (
-        'DATABASES', 'cx_Oracle', "database engine", "django.db.backends.oracle", {
-            'default': {
-                'ENGINE': "django.db.backends.oracle",
-                'NAME': 'test',
-                'USER': 'root',
-                'PASSWORD': '',
-                'HOST': 'localhost',
-                'PORT': ''
-            }
-        }
-    ),
     "memcache": (
         'CACHES', 'memcache', "caching backend",
         "django.core.cache.backends.memcached.MemcachedCache", {
@@ -257,7 +233,7 @@ class SentryRemoteTest(TestCase):
 
     def test_timestamp(self):
         timestamp = timezone.now().replace(
-            microsecond=0, tzinfo=timezone.utc
+            microsecond=0, tzinfo=timezone.utc,
         ) - datetime.timedelta(hours=1)
         kwargs = {u'message': 'hello', 'timestamp': float(timestamp.strftime('%s.%f'))}
         resp = self._postWithSignature(kwargs)
@@ -514,12 +490,6 @@ class DependencyTest(TestCase):
     def test_validate_fails_on_postgres(self):
         self.validate_dependency(*DEPENDENCY_TEST_DATA['postgresql'])
 
-    def test_validate_fails_on_mysql(self):
-        self.validate_dependency(*DEPENDENCY_TEST_DATA['mysql'])
-
-    def test_validate_fails_on_oracle(self):
-        self.validate_dependency(*DEPENDENCY_TEST_DATA['oracle'])
-
     def test_validate_fails_on_memcache(self):
         self.validate_dependency(*DEPENDENCY_TEST_DATA['memcache'])
 
diff --git a/tests/sentry/api/endpoints/test_organization_recent_searches.py b/tests/sentry/api/endpoints/test_organization_recent_searches.py
index 614a5589ea..b79147e254 100644
--- a/tests/sentry/api/endpoints/test_organization_recent_searches.py
+++ b/tests/sentry/api/endpoints/test_organization_recent_searches.py
@@ -49,8 +49,8 @@ class RecentSearchesListTest(APITestCase):
             user=self.user,
             type=SearchType.EVENT.value,
             query='some test',
-            last_seen=timezone.now().replace(microsecond=0),
-            date_added=timezone.now().replace(microsecond=0),
+            last_seen=timezone.now(),
+            date_added=timezone.now(),
         )
         issue_recent_searches = [
             RecentSearch.objects.create(
@@ -58,24 +58,24 @@ class RecentSearchesListTest(APITestCase):
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='some test',
-                last_seen=timezone.now().replace(microsecond=0),
-                date_added=timezone.now().replace(microsecond=0),
+                last_seen=timezone.now(),
+                date_added=timezone.now(),
             ),
             RecentSearch.objects.create(
                 organization=self.organization,
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='older query',
-                last_seen=timezone.now().replace(microsecond=0) - timedelta(minutes=30),
-                date_added=timezone.now().replace(microsecond=0) - timedelta(minutes=30),
+                last_seen=timezone.now() - timedelta(minutes=30),
+                date_added=timezone.now() - timedelta(minutes=30),
             ),
             RecentSearch.objects.create(
                 organization=self.organization,
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='oldest query',
-                last_seen=timezone.now().replace(microsecond=0) - timedelta(hours=1),
-                date_added=timezone.now().replace(microsecond=0) - timedelta(hours=1),
+                last_seen=timezone.now() - timedelta(hours=1),
+                date_added=timezone.now() - timedelta(hours=1),
             ),
         ]
         self.check_results(issue_recent_searches, search_type=SearchType.ISSUE)
@@ -103,24 +103,24 @@ class RecentSearchesListTest(APITestCase):
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='some test',
-                last_seen=timezone.now().replace(microsecond=0),
-                date_added=timezone.now().replace(microsecond=0),
+                last_seen=timezone.now(),
+                date_added=timezone.now(),
             ),
             RecentSearch.objects.create(
                 organization=self.organization,
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='older query',
-                last_seen=timezone.now().replace(microsecond=0) - timedelta(minutes=30),
-                date_added=timezone.now().replace(microsecond=0) - timedelta(minutes=30),
+                last_seen=timezone.now() - timedelta(minutes=30),
+                date_added=timezone.now() - timedelta(minutes=30),
             ),
             RecentSearch.objects.create(
                 organization=self.organization,
                 user=self.user,
                 type=SearchType.ISSUE.value,
                 query='oldest query',
-                last_seen=timezone.now().replace(microsecond=0) - timedelta(hours=1),
-                date_added=timezone.now().replace(microsecond=0) - timedelta(hours=1),
+                last_seen=timezone.now() - timedelta(hours=1),
+                date_added=timezone.now() - timedelta(hours=1),
             ),
         ]
         self.check_results(
diff --git a/tests/sentry/api/endpoints/test_organization_searches.py b/tests/sentry/api/endpoints/test_organization_searches.py
index cabb1a3036..e8d0dc60bb 100644
--- a/tests/sentry/api/endpoints/test_organization_searches.py
+++ b/tests/sentry/api/endpoints/test_organization_searches.py
@@ -39,25 +39,25 @@ class OrganizationSearchesListTest(APITestCase):
                 name='Global Query',
                 query=DEFAULT_SAVED_SEARCHES[0]['query'],
                 is_global=True,
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
             SavedSearch.objects.create(
                 project=project1,
                 name='foo',
                 query='some test',
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
             SavedSearch.objects.create(
                 project=project1,
                 name='wat',
                 query='is:unassigned is:unresolved',
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
             SavedSearch.objects.create(
                 project=project2,
                 name='foo',
                 query='some test',
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
         ]
 
@@ -92,33 +92,33 @@ class OrgLevelOrganizationSearchesListTest(APITestCase):
             project=self.create_project(teams=[team], name='foo'),
             name='foo',
             query='some test',
-            date_added=timezone.now().replace(microsecond=0)
+            date_added=timezone.now(),
         )
         SavedSearch.objects.create(
             organization=self.organization,
             owner=self.create_user(),
             name='foo',
             query='some other user\'s query',
-            date_added=timezone.now().replace(microsecond=0)
+            date_added=timezone.now(),
         )
         included = [
             SavedSearch.objects.create(
                 name='Global Query',
                 query=DEFAULT_SAVED_SEARCHES[0]['query'],
                 is_global=True,
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
             SavedSearch.objects.create(
                 organization=self.organization,
                 name='foo',
                 query='some test',
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
             SavedSearch.objects.create(
                 organization=self.organization,
                 name='wat',
                 query='is:unassigned is:unresolved',
-                date_added=timezone.now().replace(microsecond=0)
+                date_added=timezone.now(),
             ),
         ]
         return included
@@ -140,7 +140,7 @@ class OrgLevelOrganizationSearchesListTest(APITestCase):
             owner=self.user,
             name='My Pinned Query',
             query='pinned junk',
-            date_added=timezone.now().replace(microsecond=0)
+            date_added=timezone.now(),
         )
         included.append(pinned_query)
         self.check_results(included)
diff --git a/tests/sentry/api/endpoints/test_project_group_index.py b/tests/sentry/api/endpoints/test_project_group_index.py
index fce4d66577..f4b74876b3 100644
--- a/tests/sentry/api/endpoints/test_project_group_index.py
+++ b/tests/sentry/api/endpoints/test_project_group_index.py
@@ -70,7 +70,7 @@ class GroupListTest(APITestCase):
         assert 'could not' in response.data['detail']
 
     def test_simple_pagination(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         group1 = self.create_group(
             checksum='a' * 32,
             last_seen=now - timedelta(seconds=1),
@@ -1267,10 +1267,8 @@ class GroupUpdateTest(APITestCase):
         assert response.status_code == 200
 
         snooze = GroupSnooze.objects.get(group=group)
-        snooze.until = snooze.until.replace(microsecond=0)
 
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         assert snooze.count is None
         assert snooze.until > now + timedelta(minutes=29)
@@ -1279,12 +1277,6 @@ class GroupUpdateTest(APITestCase):
         assert snooze.user_window is None
         assert snooze.window is None
 
-        # Drop microsecond value for MySQL
-        response.data['statusDetails']['ignoreUntil'] = response.data['statusDetails'
-                                                                      ]['ignoreUntil'].replace(
-                                                                          microsecond=0
-                                                                      )  # noqa
-
         assert response.data['status'] == 'ignored'
         assert response.data['statusDetails']['ignoreCount'] == snooze.count
         assert response.data['statusDetails']['ignoreWindow'] == snooze.window
diff --git a/tests/sentry/api/endpoints/test_sentry_app_authorizations.py b/tests/sentry/api/endpoints/test_sentry_app_authorizations.py
index 6ac0e35146..e3272a6a1e 100644
--- a/tests/sentry/api/endpoints/test_sentry_app_authorizations.py
+++ b/tests/sentry/api/endpoints/test_sentry_app_authorizations.py
@@ -74,7 +74,6 @@ class TestSentryAppAuthorizations(APITestCase):
             second=0,
             microsecond=0,
         )
-
         assert expires_at == expected_expires_at
 
     def test_incorrect_grant_type(self):
diff --git a/tests/sentry/api/serializers/test_group.py b/tests/sentry/api/serializers/test_group.py
index 4149be50cb..5be23c0ae7 100644
--- a/tests/sentry/api/serializers/test_group.py
+++ b/tests/sentry/api/serializers/test_group.py
@@ -32,7 +32,7 @@ class GroupSerializerTest(TestCase):
         assert 'platform' in result['project']
 
     def test_is_ignored_with_expired_snooze(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
@@ -48,7 +48,7 @@ class GroupSerializerTest(TestCase):
         assert result['statusDetails'] == {}
 
     def test_is_ignored_with_valid_snooze(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
@@ -69,7 +69,7 @@ class GroupSerializerTest(TestCase):
         assert result['statusDetails']['actor'] is None
 
     def test_is_ignored_with_valid_snooze_and_actor(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
diff --git a/tests/sentry/api/serializers/test_team.py b/tests/sentry/api/serializers/test_team.py
index 28c01f6e72..a0f43ab293 100644
--- a/tests/sentry/api/serializers/test_team.py
+++ b/tests/sentry/api/serializers/test_team.py
@@ -154,15 +154,7 @@ class TeamWithProjectsSerializerTest(TestCase):
         project2 = self.create_project(teams=[team], organization=organization, name='bar')
 
         result = serialize(team, user, TeamWithProjectsSerializer())
-        result.pop('dateCreated')
-
-        # don't compare dateCreated because of mysql
         serialized_projects = serialize([project2, project], user)
-        for p in serialized_projects:
-            p.pop('dateCreated')
-
-        for p in result['projects']:
-            p.pop('dateCreated')
 
         assert result == {
             'slug': team.slug,
@@ -176,5 +168,6 @@ class TeamWithProjectsSerializerTest(TestCase):
                 'avatarType': 'letter_avatar',
                 'avatarUuid': None,
             },
-            'memberCount': 0
+            'memberCount': 0,
+            'dateCreated': team.date_added,
         }
diff --git a/tests/sentry/api/test_paginator.py b/tests/sentry/api/test_paginator.py
index 05cc3b59e9..700ef04e2a 100644
--- a/tests/sentry/api/test_paginator.py
+++ b/tests/sentry/api/test_paginator.py
@@ -1,6 +1,5 @@
 from __future__ import absolute_import
 
-import pytest
 from datetime import timedelta
 from django.utils import timezone
 from unittest import TestCase as SimpleTestCase
@@ -15,7 +14,6 @@ from sentry.api.paginator import (
 from sentry.models import User
 from sentry.testutils import TestCase
 from sentry.utils.cursors import Cursor
-from sentry.utils.db import is_mysql
 
 
 class PaginatorTest(TestCase):
@@ -244,7 +242,6 @@ class DateTimePaginatorTest(TestCase):
         result4 = paginator.get_result(limit=10, cursor=result1.next)
         assert len(result4) == 0, result4
 
-    @pytest.mark.skipif(is_mysql(), reason='MySQL does not support above second accuracy')
     def test_rounding_offset(self):
         joined = timezone.now()
 
diff --git a/tests/sentry/buffer/base/tests.py b/tests/sentry/buffer/base/tests.py
index bbda5674f8..7fae33742b 100644
--- a/tests/sentry/buffer/base/tests.py
+++ b/tests/sentry/buffer/base/tests.py
@@ -45,12 +45,11 @@ class BufferTest(TestCase):
         group = Group.objects.create(project=Project(id=1))
         columns = {'times_seen': 1}
         filters = {'id': group.id, 'project_id': 1}
-        # strip micrseconds because MySQL doesn't seem to handle them correctly
-        the_date = (timezone.now() + timedelta(days=5)).replace(microsecond=0)
+        the_date = (timezone.now() + timedelta(days=5))
         self.buf.process(Group, columns, filters, {'last_seen': the_date})
         group_ = Group.objects.get(id=group.id)
         assert group_.times_seen == group.times_seen + 1
-        assert group_.last_seen.replace(microsecond=0) == the_date
+        assert group_.last_seen == the_date
 
     def test_increments_when_null(self):
         org = Organization.objects.create(slug='test-org')
diff --git a/tests/sentry/db/test_parse_query.py b/tests/sentry/db/test_parse_query.py
index 4093bf069a..68a1077a72 100644
--- a/tests/sentry/db/test_parse_query.py
+++ b/tests/sentry/db/test_parse_query.py
@@ -50,120 +50,6 @@ class ParseQuery(TestCase):
             'sentry_userreport': 1
         }
 
-    def test_parse_mysql_queries(self):
-        result = parse_queries(
-            [{u'sql': u'SAVEPOINT `s47055674149248_x49`', u'time': u'0.000'},
-             {u'sql': u'RELEASE SAVEPOINT `s47055674149248_x49`', u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_rawevent`.`id`, `sentry_rawevent`.`project_id`, `sentry_rawevent`.`event_id`, `sentry_rawevent`.`datetime`, `sentry_rawevent`.`data` FROM `sentry_rawevent` WHERE (`sentry_rawevent`.`event_id` = '1fa6e7d1c2674273be07852952e1bafc'  AND `sentry_rawevent`.`project_id` = 815 )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_reprocessingreport`.`id`, `sentry_reprocessingreport`.`project_id`, `sentry_reprocessingreport`.`event_id`, `sentry_reprocessingreport`.`datetime` FROM `sentry_reprocessingreport` WHERE (`sentry_reprocessingreport`.`event_id` = '1fa6e7d1c2674273be07852952e1bafc'  AND `sentry_reprocessingreport`.`project_id` = 815 )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_message`.`id`, `sentry_message`.`group_id`, `sentry_message`.`message_id`, `sentry_message`.`project_id`, `sentry_message`.`message`, `sentry_message`.`platform`, `sentry_message`.`datetime`, `sentry_message`.`time_spent`, `sentry_message`.`data` FROM `sentry_message` WHERE (`sentry_message`.`message_id` = '1fa6e7d1c2674273be07852952e1bafc'  AND `sentry_message`.`project_id` = 815 )",
-              u'time': u'0.000'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x50`', u'time': u'0.000'},
-             {u'sql': u"INSERT INTO `sentry_eventuser` (`project_id`, `hash`, `ident`, `email`, `username`, `name`, `ip_address`, `date_added`) VALUES (815, 'f528764d624db129b32c21fbca0cb8d6', NULL, NULL, NULL, NULL, '127.0.0.1', '2018-05-22 10:54:14')",
-              u'time': u'0.000'},
-             {u'sql': u'ROLLBACK TO SAVEPOINT `s47055674149248_x50`', u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_eventuser`.`id`, `sentry_eventuser`.`project_id`, `sentry_eventuser`.`hash`, `sentry_eventuser`.`ident`, `sentry_eventuser`.`email`, `sentry_eventuser`.`username`, `sentry_eventuser`.`name`, `sentry_eventuser`.`ip_address`, `sentry_eventuser`.`date_added` FROM `sentry_eventuser` WHERE (`sentry_eventuser`.`project_id` = 815  AND `sentry_eventuser`.`hash` = 'f528764d624db129b32c21fbca0cb8d6' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_grouphash`.`id`, `sentry_grouphash`.`project_id`, `sentry_grouphash`.`hash`, `sentry_grouphash`.`group_id`, `sentry_grouphash`.`group_tombstone_id`, `sentry_grouphash`.`state` FROM `sentry_grouphash` WHERE (`sentry_grouphash`.`project_id` = 815  AND `sentry_grouphash`.`hash` = '5d41402abc4b2a76b9719d911017c592' )",
-              u'time': u'0.000'},
-             {u'sql': u'SELECT `sentry_groupedmessage`.`id`, `sentry_groupedmessage`.`project_id`, `sentry_groupedmessage`.`logger`, `sentry_groupedmessage`.`level`, `sentry_groupedmessage`.`message`, `sentry_groupedmessage`.`view`, `sentry_groupedmessage`.`num_comments`, `sentry_groupedmessage`.`platform`, `sentry_groupedmessage`.`status`, `sentry_groupedmessage`.`times_seen`, `sentry_groupedmessage`.`last_seen`, `sentry_groupedmessage`.`first_seen`, `sentry_groupedmessage`.`first_release_id`, `sentry_groupedmessage`.`resolved_at`, `sentry_groupedmessage`.`active_at`, `sentry_groupedmessage`.`time_spent_total`, `sentry_groupedmessage`.`time_spent_count`, `sentry_groupedmessage`.`score`, `sentry_groupedmessage`.`is_public`, `sentry_groupedmessage`.`data`, `sentry_groupedmessage`.`short_id` FROM `sentry_groupedmessage` WHERE `sentry_groupedmessage`.`id` = 592 ',
-              u'time': u'0.001'},
-             {u'sql': u'SELECT `sentry_project`.`id`, `sentry_project`.`slug`, `sentry_project`.`name`, `sentry_project`.`forced_color`, `sentry_project`.`organization_id`, `sentry_project`.`public`, `sentry_project`.`date_added`, `sentry_project`.`status`, `sentry_project`.`first_event`, `sentry_project`.`flags`, `sentry_project`.`platform` FROM `sentry_project` WHERE `sentry_project`.`id` = 815 ',
-              u'time': u'0.000'},
-             {u'sql': u"UPDATE `sentry_groupedmessage` SET `times_seen` = `sentry_groupedmessage`.`times_seen` + 1, `score` = log(times_seen) * 600 + unix_timestamp(last_seen), `data` = 'eJwdyk0Kg0AMhuF9LjKuBH9mHC/gBQS3JZgUB1IanCj09k27fL/vaUg7WINgtcfJO5ebKYD2sHSxT3NOYxyhrsE+yr4PbomfeIl5/Z8XGxIaekdoSJMTKyY/PsF2sMgbNEOt7RfkkiAY', `last_seen` = '2018-05-22 10:54:14' WHERE `sentry_groupedmessage`.`id` = 592 ",
-              u'time': u'0.000'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x51`', u'time': u'0.000'},
-             {u'sql': u'INSERT INTO `sentry_environmentproject` (`project_id`, `environment_id`, `is_hidden`) VALUES (815, 96, NULL)',
-              u'time': u'0.000'},
-             {u'sql': u'ROLLBACK TO SAVEPOINT `s47055674149248_x51`', u'time': u'0.000'},
-             {u'sql': u"UPDATE `sentry_userreport` SET `environment_id` = 96, `group_id` = 592 WHERE (`sentry_userreport`.`project_id` = 815  AND `sentry_userreport`.`event_id` = '1fa6e7d1c2674273be07852952e1bafc' )",
-              u'time': u'0.000'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x52`', u'time': u'0.000'},
-             {u'sql': u"UPDATE `nodestore_node` SET `timestamp` = '2018-05-22 10:54:14', `data` = 'eJxtU01v2zAMvetX6BYX2BxLtmSnOxUDtgz92CFdcgxUm3G0OLEgK127ov99pOKmOxQBjJB8pKj3npLGCbaYON//hjpMmJPsphLqhg2LyQCH4J9TewjgN6aGIf01gEdMzpLGFdhm3do0jYdhwKzChJBlmuFPYKzZ8OGQW4SbFhBR0pwK2/bn1Iwtt9B1PXMii/3wFLzBghAEFpIWW3vYrB/BD7Y/UClnP+THC89DcIQoYjNtePQdJTSeg7XL6RSezN51kNb9HvMlzdmCaXA64SqWdE7M6CsztiQCPl+1eA4yJZhZ3vZ/bdeZqUoznqzsoen/DPzunutUfuGrnytdXPArh/NX8HBtw1TlZZprnlzP729vPvHO7oB/h3rXX/CvW9/vYSor4k9lWVpKvjAb4+3YhUdKZgyxUvd4x6dAK8qohiQ5+hirGGuM3ymSJbvDtoPZE8mSOH9btqLELHL94DFxUjijIblgLXYuJuedqIRUt3HCaWFK5diO/Rt7aME7jwJQtiDWciL95YU3sDHHLvDXVyppZkYdKSr/81yzo0wVj59ha91ZZHttScYiY60m1PvFCsGWZLgM/0YPjFcscrZcu34IKxu286gnJot4TQ812EdoCKbYN6GknlW6UAX1g/d9lL7QtH4RDRFMG1PRDQW6YTHp4BHISSpjYxMFggWnUKTkzWgq/9hoeBgiVUSebHt5PFGv0JrWXZ5fEuHKiOuH9HS75Vk7KlaxOGo3IpzCh3TSBx9ihjCNbk1a1HVM87OmVMSVh5a4jPTu4HltiR6dozJSRmXCs6PBmow2ikmhotoegmlMoHeqNUmnyTXBhi62VKyNrzn9By1kSX8=' WHERE `nodestore_node`.`id` = '9cwO83agTCqM5QNjewZF+g==' ",
-              u'time': u'0.000'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x53`', u'time': u'0.000'},
-             {u'sql': u"INSERT INTO `nodestore_node` (`id`, `data`, `timestamp`) VALUES ('9cwO83agTCqM5QNjewZF+g==', 'eJxtU01v2zAMvetX6BYX2BxLtmSnOxUDtgz92CFdcgxUm3G0OLEgK127ov99pOKmOxQBjJB8pKj3npLGCbaYON//hjpMmJPsphLqhg2LyQCH4J9TewjgN6aGIf01gEdMzpLGFdhm3do0jYdhwKzChJBlmuFPYKzZ8OGQW4SbFhBR0pwK2/bn1Iwtt9B1PXMii/3wFLzBghAEFpIWW3vYrB/BD7Y/UClnP+THC89DcIQoYjNtePQdJTSeg7XL6RSezN51kNb9HvMlzdmCaXA64SqWdE7M6CsztiQCPl+1eA4yJZhZ3vZ/bdeZqUoznqzsoen/DPzunutUfuGrnytdXPArh/NX8HBtw1TlZZprnlzP729vPvHO7oB/h3rXX/CvW9/vYSor4k9lWVpKvjAb4+3YhUdKZgyxUvd4x6dAK8qohiQ5+hirGGuM3ymSJbvDtoPZE8mSOH9btqLELHL94DFxUjijIblgLXYuJuedqIRUt3HCaWFK5diO/Rt7aME7jwJQtiDWciL95YU3sDHHLvDXVyppZkYdKSr/81yzo0wVj59ha91ZZHttScYiY60m1PvFCsGWZLgM/0YPjFcscrZcu34IKxu286gnJot4TQ812EdoCKbYN6GknlW6UAX1g/d9lL7QtH4RDRFMG1PRDQW6YTHp4BHISSpjYxMFggWnUKTkzWgq/9hoeBgiVUSebHt5PFGv0JrWXZ5fEuHKiOuH9HS75Vk7KlaxOGo3IpzCh3TSBx9ihjCNbk1a1HVM87OmVMSVh5a4jPTu4HltiR6dozJSRmXCs6PBmow2ikmhotoegmlMoHeqNUmnyTXBhi62VKyNrzn9By1kSX8=', '2018-05-22 10:54:14')",
-              u'time': u'0.000'},
-             {u'sql': u'RELEASE SAVEPOINT `s47055674149248_x53`', u'time': u'0.000'},
-             {u'sql': u"INSERT INTO `sentry_message` (`group_id`, `message_id`, `project_id`, `message`, `platform`, `datetime`, `time_spent`, `data`) VALUES (592, '1fa6e7d1c2674273be07852952e1bafc', 815, 'hello http://example.com', 'javascript', '2018-05-22 10:54:14', NULL, 'eJzTSCkw5ApWz8tPSY3PTFHnKjAC8iyTy/0tjBPTQ5wLfU0D/bJSy6PctNNtbYHSxlzFegCVlg8K')",
-              u'time': u'0.000'},
-             {u'sql': u'RELEASE SAVEPOINT `s47055674149248_x52`', u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'level' )",
-              u'time': u'0.001'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'error'  AND `sentry_filtervalue`.`key` = 'level' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'url' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'http://example.com'  AND `sentry_filtervalue`.`key` = 'url' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'sentry:user' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'ip:127.0.0.1'  AND `sentry_filtervalue`.`key` = 'sentry:user' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'os.name' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Windows 8'  AND `sentry_filtervalue`.`key` = 'os.name' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'browser.name' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Chrome'  AND `sentry_filtervalue`.`key` = 'browser.name' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filterkey`.`id`, `sentry_filterkey`.`project_id`, `sentry_filterkey`.`key`, `sentry_filterkey`.`values_seen`, `sentry_filterkey`.`label`, `sentry_filterkey`.`status` FROM `sentry_filterkey` WHERE (`sentry_filterkey`.`project_id` = 815  AND `sentry_filterkey`.`key` = 'browser' )",
-              u'time': u'0.000'},
-             {u'sql': u"SELECT `sentry_filtervalue`.`id`, `sentry_filtervalue`.`project_id`, `sentry_filtervalue`.`key`, `sentry_filtervalue`.`value`, `sentry_filtervalue`.`data`, `sentry_filtervalue`.`times_seen`, `sentry_filtervalue`.`last_seen`, `sentry_filtervalue`.`first_seen` FROM `sentry_filtervalue` WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Chrome 28.0.1500'  AND `sentry_filtervalue`.`key` = 'browser' )",
-              u'time': u'0.000'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x54`', u'time': u'0.000'},
-             {u'sql': u"INSERT INTO `sentry_eventtag` (`project_id`, `group_id`, `event_id`, `key_id`, `value_id`, `date_added`) VALUES (815, 592, 373, 43, 42, '2018-05-22 10:54:14'), (815, 592, 373, 44, 43, '2018-05-22 10:54:14'), (815, 592, 373, 45, 44, '2018-05-22 10:54:14'), (815, 592, 373, 46, 45, '2018-05-22 10:54:14'), (815, 592, 373, 47, 46, '2018-05-22 10:54:14'), (815, 592, 373, 48, 47, '2018-05-22 10:54:14')",
-              u'time': u'0.000'},
-             {u'sql': u'RELEASE SAVEPOINT `s47055674149248_x54`', u'time': u'0.000'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'error'  AND `sentry_filtervalue`.`key` = 'level' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'error'  AND `sentry_messagefiltervalue`.`key` = 'level' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'http://example.com'  AND `sentry_filtervalue`.`key` = 'url' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'http://example.com'  AND `sentry_messagefiltervalue`.`key` = 'url' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'ip:127.0.0.1'  AND `sentry_filtervalue`.`key` = 'sentry:user' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'ip:127.0.0.1'  AND `sentry_messagefiltervalue`.`key` = 'sentry:user' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Windows 8'  AND `sentry_filtervalue`.`key` = 'os.name' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'Windows 8'  AND `sentry_messagefiltervalue`.`key` = 'os.name' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Chrome'  AND `sentry_filtervalue`.`key` = 'browser.name' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'Chrome'  AND `sentry_messagefiltervalue`.`key` = 'browser.name' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_filtervalue` SET `times_seen` = `sentry_filtervalue`.`times_seen` + 1, `data` = NULL, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_filtervalue`.`project_id` = 815  AND `sentry_filtervalue`.`value` = 'Chrome 28.0.1500'  AND `sentry_filtervalue`.`key` = 'browser' )",
-              u'time': u'0.001'},
-             {u'sql': u"UPDATE `sentry_messagefiltervalue` SET `times_seen` = `sentry_messagefiltervalue`.`times_seen` + 1, `project_id` = 815, `last_seen` = '2018-05-22 10:54:14' WHERE (`sentry_messagefiltervalue`.`group_id` = 592  AND `sentry_messagefiltervalue`.`value` = 'Chrome 28.0.1500'  AND `sentry_messagefiltervalue`.`key` = 'browser' )",
-              u'time': u'0.001'},
-             {u'sql': u'SELECT `sentry_groupedmessage`.`id`, `sentry_groupedmessage`.`project_id`, `sentry_groupedmessage`.`logger`, `sentry_groupedmessage`.`level`, `sentry_groupedmessage`.`message`, `sentry_groupedmessage`.`view`, `sentry_groupedmessage`.`num_comments`, `sentry_groupedmessage`.`platform`, `sentry_groupedmessage`.`status`, `sentry_groupedmessage`.`times_seen`, `sentry_groupedmessage`.`last_seen`, `sentry_groupedmessage`.`first_seen`, `sentry_groupedmessage`.`first_release_id`, `sentry_groupedmessage`.`resolved_at`, `sentry_groupedmessage`.`active_at`, `sentry_groupedmessage`.`time_spent_total`, `sentry_groupedmessage`.`time_spent_count`, `sentry_groupedmessage`.`score`, `sentry_groupedmessage`.`is_public`, `sentry_groupedmessage`.`data`, `sentry_groupedmessage`.`short_id` FROM `sentry_groupedmessage` WHERE `sentry_groupedmessage`.`id` = 592 ',
-              u'time': u'0.001'},
-             {u'sql': u'SELECT `sentry_groupsnooze`.`id`, `sentry_groupsnooze`.`group_id`, `sentry_groupsnooze`.`until`, `sentry_groupsnooze`.`count`, `sentry_groupsnooze`.`window`, `sentry_groupsnooze`.`user_count`, `sentry_groupsnooze`.`user_window`, `sentry_groupsnooze`.`state`, `sentry_groupsnooze`.`actor_id` FROM `sentry_groupsnooze` WHERE `sentry_groupsnooze`.`group_id` = 592 ',
-              u'time': u'0.000'},
-             {u'sql': u'SELECT `sentry_grouprulestatus`.`id`, `sentry_grouprulestatus`.`project_id`, `sentry_grouprulestatus`.`rule_id`, `sentry_grouprulestatus`.`group_id`, `sentry_grouprulestatus`.`status`, `sentry_grouprulestatus`.`date_added`, `sentry_grouprulestatus`.`last_active` FROM `sentry_grouprulestatus` WHERE (`sentry_grouprulestatus`.`group_id` = 592  AND `sentry_grouprulestatus`.`rule_id` = 827 )',
-              u'time': u'0.001'},
-             {u'sql': u'SAVEPOINT `s47055674149248_x55`', u'time': u'0.000'},
-             {u'sql': u'RELEASE SAVEPOINT `s47055674149248_x55`', u'time': u'0.000'}]
-        )
-
-        assert result == {
-            'nodestore_node': 2,
-            'sentry_environmentproject': 1,
-            'sentry_eventtag': 1,
-            'sentry_eventuser': 1,
-            'sentry_filtervalue': 6,
-            'sentry_groupedmessage': 1,
-            'sentry_message': 1,
-            'sentry_messagefiltervalue': 6,
-            'sentry_userreport': 1
-        }
-
     def test_parse_postgres_queries(self):
         result = parse_queries([
             {u'sql': u'SAVEPOINT "s47890194282880_x49"', u'time': u'0.000'},
diff --git a/tests/sentry/event_manager/test_event_manager.py b/tests/sentry/event_manager/test_event_manager.py
index a3970a80a5..66a9287de4 100644
--- a/tests/sentry/event_manager/test_event_manager.py
+++ b/tests/sentry/event_manager/test_event_manager.py
@@ -151,7 +151,7 @@ class EventManagerTest(TestCase):
         group = Group.objects.get(id=event.group_id)
 
         assert group.times_seen == 2
-        assert group.last_seen.replace(microsecond=0) == event2.datetime.replace(microsecond=0)
+        assert group.last_seen == event2.datetime
         assert group.message == event2.message
         assert group.data.get('type') == 'default'
         assert group.data.get('metadata') == {
@@ -185,7 +185,7 @@ class EventManagerTest(TestCase):
         group = Group.objects.get(id=event.group_id)
 
         assert group.times_seen == 2
-        assert group.last_seen.replace(microsecond=0) == event.datetime.replace(microsecond=0)
+        assert group.last_seen == event.datetime
         assert group.message == event2.message
 
     def test_differentiates_with_fingerprint(self):
@@ -217,7 +217,7 @@ class EventManagerTest(TestCase):
         ts = time() - 300
 
         # N.B. EventManager won't unresolve the group unless the event2 has a
-        # later timestamp than event1. MySQL doesn't support microseconds.
+        # later timestamp than event1.
         manager = EventManager(
             make_event(
                 event_id='a' * 32,
@@ -249,7 +249,7 @@ class EventManagerTest(TestCase):
     @mock.patch('sentry.event_manager.plugin_is_regression')
     def test_does_not_unresolve_group(self, plugin_is_regression):
         # N.B. EventManager won't unresolve the group unless the event2 has a
-        # later timestamp than event1. MySQL doesn't support microseconds.
+        # later timestamp than event1.
         plugin_is_regression.return_value = False
 
         manager = EventManager(
@@ -611,13 +611,8 @@ class EventManagerTest(TestCase):
         assert event.group_id == event2.group_id
 
         group = Group.objects.get(id=event.group.id)
-        # MySQL removes sub-second portion
-        assert group.active_at.replace(
-            second=0, microsecond=0) == event2.datetime.replace(
-            second=0, microsecond=0)
-        assert group.active_at.replace(
-            second=0, microsecond=0) != event.datetime.replace(
-            second=0, microsecond=0)
+        assert group.active_at.replace(second=0) == event2.datetime.replace(second=0)
+        assert group.active_at.replace(second=0) != event.datetime.replace(second=0)
 
     def test_invalid_transaction(self):
         dict_input = {'messages': 'foo'}
diff --git a/tests/sentry/receivers/test_featureadoption.py b/tests/sentry/receivers/test_featureadoption.py
index 8b8bd7ad2b..60588345cb 100644
--- a/tests/sentry/receivers/test_featureadoption.py
+++ b/tests/sentry/receivers/test_featureadoption.py
@@ -28,7 +28,7 @@ from sentry.testutils import TestCase
 class FeatureAdoptionTest(TestCase):
     def setUp(self):
         super(FeatureAdoptionTest, self).setUp()
-        self.now = timezone.now().replace(microsecond=0)
+        self.now = timezone.now()
         self.owner = self.create_user()
         self.organization = self.create_organization(owner=self.owner)
         self.team = self.create_team(organization=self.organization)
diff --git a/tests/sentry/receivers/test_onboarding.py b/tests/sentry/receivers/test_onboarding.py
index 205233135c..2c20ad03d1 100644
--- a/tests/sentry/receivers/test_onboarding.py
+++ b/tests/sentry/receivers/test_onboarding.py
@@ -21,8 +21,7 @@ from sentry.testutils import TestCase
 
 class OrganizationOnboardingTaskTest(TestCase):
     def test_no_existing_task(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         first_event_received.send(project=project, group=self.group, sender=type(project))
 
@@ -34,8 +33,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task.date_completed == project.first_event
 
     def test_existing_pending_task(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
 
         first_event_pending.send(project=project, user=self.user, sender=type(project))
@@ -60,8 +58,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task.date_completed == project.first_event
 
     def test_existing_complete_task(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         task = OrganizationOnboardingTask.objects.create(
             organization=project.organization,
@@ -77,8 +74,7 @@ class OrganizationOnboardingTaskTest(TestCase):
 
     # Tests on the receivers
     def test_event_processed(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         event = self.create_full_event()
         event_processed.send(project=project, group=self.group, event=event, sender=type(project))
@@ -105,8 +101,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task is not None
 
     def test_project_created(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         project_created.send(project=project, user=self.user, sender=type(project))
 
@@ -118,8 +113,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task is not None
 
     def test_first_event_pending(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         first_event_pending.send(project=project, user=self.user, sender=type(project))
 
@@ -131,8 +125,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task is not None
 
     def test_first_event_received(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         project = self.create_project(first_event=now)
         project_created.send(project=project, user=self.user, sender=type(project))
         group = self.create_group(
@@ -251,8 +244,7 @@ class OrganizationOnboardingTaskTest(TestCase):
         assert task is not None
 
     def test_onboarding_complete(self):
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         user = self.create_user(email='test@example.org')
         project = self.create_project(first_event=now)
         second_project = self.create_project(first_event=now)
diff --git a/tests/sentry/tagstore/v2/test_backend.py b/tests/sentry/tagstore/v2/test_backend.py
index 1c62536ffc..f70da178bc 100644
--- a/tests/sentry/tagstore/v2/test_backend.py
+++ b/tests/sentry/tagstore/v2/test_backend.py
@@ -1,6 +1,5 @@
 from __future__ import absolute_import
 
-import os
 import pytest
 
 from collections import OrderedDict
@@ -18,13 +17,6 @@ from sentry.tagstore.v2.backend import V2TagStorage, transformers
 from sentry.tagstore.exceptions import TagKeyNotFound, TagValueNotFound, GroupTagKeyNotFound, GroupTagValueNotFound
 
 
-def xfail_if_mysql(function):
-    return pytest.mark.xfail(
-        os.environ.get('TEST_SUITE') == 'mysql',
-        reason='mysql microsecond truncation breaks comparison',
-    )(function)
-
-
 class TagStorage(TestCase):
     def setUp(self):
         self.ts = V2TagStorage()
@@ -98,7 +90,6 @@ class TagStorage(TestCase):
         ).count() == 1
         assert models.TagKey.objects.all().count() == 1
 
-    @xfail_if_mysql
     def test_create_tag_value(self):
         with pytest.raises(TagValueNotFound):
             self.ts.get_tag_value(
@@ -246,7 +237,6 @@ class TagStorage(TestCase):
         ).count() == 1
         assert models.GroupTagKey.objects.all().count() == 1
 
-    @xfail_if_mysql
     def test_create_group_tag_value(self):
         with pytest.raises(GroupTagValueNotFound):
             self.ts.get_group_tag_value(
@@ -631,7 +621,6 @@ class TagStorage(TestCase):
             self.proj1group1.id,
         ) == '2.0'
 
-    @xfail_if_mysql
     def test_get_release_tags(self):
         tv, _ = self.ts.get_or_create_tag_value(
             self.proj1.id,
@@ -662,7 +651,6 @@ class TagStorage(TestCase):
             [self.proj1.id],
             [eu]) == set([self.proj1group1.id])
 
-    @xfail_if_mysql
     def test_get_group_tag_values_for_users(self):
         from sentry.models import EventUser
 
diff --git a/tests/sentry/tasks/test_check_auth.py b/tests/sentry/tasks/test_check_auth.py
index 6bca3bd042..3115e8113b 100644
--- a/tests/sentry/tasks/test_check_auth.py
+++ b/tests/sentry/tasks/test_check_auth.py
@@ -36,10 +36,7 @@ class CheckAuthTest(TestCase):
 
         updated_ai = AuthIdentity.objects.get(id=ai.id)
         assert updated_ai.last_synced != ai.last_synced
-        # mysql doesnt store ms
-        assert updated_ai.last_verified.replace(microsecond=0) == ai.last_verified.replace(
-            microsecond=0
-        )
+        assert updated_ai.last_verified == ai.last_verified
 
         mock_check_auth_identity.apply_async.assert_called_once_with(
             kwargs={'auth_identity_id': ai.id},
diff --git a/tests/sentry/tasks/test_unmerge.py b/tests/sentry/tasks/test_unmerge.py
index 788809d876..98f8fe5c06 100644
--- a/tests/sentry/tasks/test_unmerge.py
+++ b/tests/sentry/tasks/test_unmerge.py
@@ -194,7 +194,7 @@ class UnmergeTestCase(TestCase):
         def shift(i):
             return timedelta(seconds=1 << i)
 
-        now = timezone.now().replace(microsecond=0) - shift(16)
+        now = timezone.now() - shift(16)
 
         project = self.create_project()
         source = self.create_group(project)
diff --git a/tests/sentry/utils/db/tests.py b/tests/sentry/utils/db/tests.py
index e06a680b5a..2d69690df8 100644
--- a/tests/sentry/utils/db/tests.py
+++ b/tests/sentry/utils/db/tests.py
@@ -12,5 +12,5 @@ class GetDbEngineTest(TestCase):
             self.assertEquals(get_db_engine(), 'sqlite3')
 
     def test_no_path(self):
-        with self.settings(DATABASES={'default': {'ENGINE': 'mysql'}}):
-            self.assertEquals(get_db_engine(), 'mysql')
+        with self.settings(DATABASES={'default': {'ENGINE': 'postgres'}}):
+            self.assertEquals(get_db_engine(), 'postgres')
diff --git a/tests/sentry/web/frontend/test_group_tag_export.py b/tests/sentry/web/frontend/test_group_tag_export.py
index 18208955e1..5a7809ded5 100644
--- a/tests/sentry/web/frontend/test_group_tag_export.py
+++ b/tests/sentry/web/frontend/test_group_tag_export.py
@@ -11,8 +11,7 @@ class GroupTagExportTest(TestCase):
     def test_simple(self):
         key, value = 'foo', u'b\xe4r'
 
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         project = self.create_project()
         group = self.create_group(project=project)
diff --git a/tests/snuba/api/endpoints/test_organization_group_index.py b/tests/snuba/api/endpoints/test_organization_group_index.py
index 8816b4a1a9..7b37a3c3fd 100644
--- a/tests/snuba/api/endpoints/test_organization_group_index.py
+++ b/tests/snuba/api/endpoints/test_organization_group_index.py
@@ -84,7 +84,7 @@ class GroupListTest(APITestCase, SnubaTestCase):
         assert 'could not' in response.data['detail']
 
     def test_simple_pagination(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
         group1 = self.create_group(
             project=self.project,
             last_seen=now - timedelta(seconds=2),
@@ -1191,10 +1191,9 @@ class GroupUpdateTest(APITestCase, SnubaTestCase):
             ignoreDuration=30,
         )
         snooze = GroupSnooze.objects.get(group=group)
-        snooze.until = snooze.until.replace(microsecond=0)
+        snooze.until = snooze.until
 
-        # Drop microsecond value for MySQL
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         assert snooze.count is None
         assert snooze.until > now + timedelta(minutes=29)
@@ -1203,11 +1202,7 @@ class GroupUpdateTest(APITestCase, SnubaTestCase):
         assert snooze.user_window is None
         assert snooze.window is None
 
-        # Drop microsecond value for MySQL
-        response.data['statusDetails']['ignoreUntil'] = response.data['statusDetails'
-                                                                      ]['ignoreUntil'].replace(
-                                                                          microsecond=0
-                                                                      )  # noqa
+        response.data['statusDetails']['ignoreUntil'] = response.data['statusDetails']['ignoreUntil']
 
         assert response.data['status'] == 'ignored'
         assert response.data['statusDetails']['ignoreCount'] == snooze.count
diff --git a/tests/snuba/api/serializers/test_group.py b/tests/snuba/api/serializers/test_group.py
index 5c8281082b..b1577e2e62 100644
--- a/tests/snuba/api/serializers/test_group.py
+++ b/tests/snuba/api/serializers/test_group.py
@@ -27,7 +27,7 @@ class GroupSerializerSnubaTest(APITestCase, SnubaTestCase):
         self.week_ago = timezone.now() - timedelta(days=7)
 
     def test_is_ignored_with_expired_snooze(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
@@ -43,7 +43,7 @@ class GroupSerializerSnubaTest(APITestCase, SnubaTestCase):
         assert result['statusDetails'] == {}
 
     def test_is_ignored_with_valid_snooze(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
@@ -64,7 +64,7 @@ class GroupSerializerSnubaTest(APITestCase, SnubaTestCase):
         assert result['statusDetails']['actor'] is None
 
     def test_is_ignored_with_valid_snooze_and_actor(self):
-        now = timezone.now().replace(microsecond=0)
+        now = timezone.now()
 
         user = self.create_user()
         group = self.create_group(
