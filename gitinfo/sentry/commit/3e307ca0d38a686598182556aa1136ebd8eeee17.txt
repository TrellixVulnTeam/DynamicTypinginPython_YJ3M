commit 3e307ca0d38a686598182556aa1136ebd8eeee17
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Mon Apr 27 10:58:57 2020 +0200

    ref: Remove outcomes consumer (#18440)
    
    
    
    This consumer was only relevant for billing which is proprietary anyway,
    so let's move it out of onprem/oss. This consumer literally just burned
    CPU when not running on SaaS/sentry.io.
    
    supersedes #18249

diff --git a/src/sentry/ingest/outcomes_consumer.py b/src/sentry/ingest/outcomes_consumer.py
deleted file mode 100644
index f7168a1e66..0000000000
--- a/src/sentry/ingest/outcomes_consumer.py
+++ /dev/null
@@ -1,150 +0,0 @@
-"""
-The OutcomeConsumer is a task that runs a loop in which it reads outcome messages coming on a kafka queue and
-processes them.
-
-Long Story: Event outcomes are placed on the same Kafka event queue by both Sentry and Relay.
-When Sentry generates an outcome for a message it also sends a signal ( a Django signal) that
-is used by getSentry for internal accounting.
-
-Relay (running as a Rust process) cannot send django signals so in order to get outcome signals sent from
-Relay into getSentry we have this outcome consumers which listens to all outcomes in the kafka queue and
-for outcomes that were sent from Relay sends the signals to getSentry.
-
-In conclusion the OutcomeConsumer listens on the the outcomes kafka topic, filters the outcomes by dropping
-the outcomes that originate from sentry and keeping the outcomes originating in relay and sends
-signals to getSentry for these outcomes.
-
-"""
-from __future__ import absolute_import
-
-import time
-import atexit
-import logging
-import multiprocessing.dummy
-import multiprocessing as _multiprocessing
-
-from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
-
-from django.conf import settings
-
-from sentry.constants import DataCategory
-from sentry.models.project import Project
-from sentry.db.models.manager import BaseManager
-from sentry.signals import event_filtered, event_discarded, event_dropped, event_saved
-from sentry.utils.kafka import create_batching_kafka_consumer
-from sentry.utils import json, metrics
-from sentry.utils.data_filters import FilterStatKeys
-from sentry.utils.dates import to_datetime, parse_timestamp
-from sentry.utils.outcomes import Outcome
-from sentry.buffer.redis import batch_buffers_incr
-
-logger = logging.getLogger(__name__)
-
-
-def _get_signal_cache_key(project_id, event_id):
-    return "signal:{}:{}".format(project_id, event_id)
-
-
-def _process_signal(msg):
-    project_id = int(msg.get("project_id") or 0)
-    if project_id == 0:
-        metrics.incr("outcomes_consumer.skip_outcome", tags={"reason": "project_zero"})
-        return  # no project. this is valid, so ignore silently.
-
-    outcome = int(msg.get("outcome", -1))
-    if outcome not in (Outcome.ACCEPTED, Outcome.FILTERED, Outcome.RATE_LIMITED):
-        metrics.incr("outcomes_consumer.skip_outcome", tags={"reason": "wrong_outcome_type"})
-        return  # nothing to do here
-
-    event_id = msg.get("event_id")
-    if not event_id:
-        metrics.incr("outcomes_consumer.skip_outcome", tags={"reason": "missing_event_id"})
-        return
-
-    try:
-        project = Project.objects.get_from_cache(id=project_id)
-    except Project.DoesNotExist:
-        metrics.incr("outcomes_consumer.skip_outcome", tags={"reason": "unknown_project"})
-        logger.error("OutcomesConsumer could not find project with id: %s", project_id)
-        return
-
-    reason = msg.get("reason")
-    remote_addr = msg.get("remote_addr")
-    quantity = msg.get("quantity")
-
-    category = msg.get("category")
-    if category is not None:
-        category = DataCategory(category)
-
-    if outcome == Outcome.ACCEPTED:
-        event_saved.send_robust(
-            project=project, category=category, quantity=quantity, sender=OutcomesConsumerWorker
-        )
-    elif outcome == Outcome.FILTERED and reason == FilterStatKeys.DISCARDED_HASH:
-        event_discarded.send_robust(
-            project=project, category=category, quantity=quantity, sender=OutcomesConsumerWorker
-        )
-    elif outcome == Outcome.FILTERED:
-        event_filtered.send_robust(
-            ip=remote_addr,
-            project=project,
-            category=category,
-            quantity=quantity,
-            sender=OutcomesConsumerWorker,
-        )
-    elif outcome == Outcome.RATE_LIMITED:
-        event_dropped.send_robust(
-            ip=remote_addr,
-            project=project,
-            reason_code=reason,
-            category=category,
-            quantity=quantity,
-            sender=OutcomesConsumerWorker,
-        )
-
-    timestamp = msg.get("timestamp")
-    if timestamp is not None:
-        delta = to_datetime(time.time()) - parse_timestamp(timestamp)
-        metrics.timing("outcomes_consumer.timestamp_lag", delta.total_seconds())
-
-    metrics.incr("outcomes_consumer.signal_sent", tags={"reason": reason, "outcome": outcome})
-
-
-def _process_signal_with_timer(message):
-    with metrics.timer("outcomes_consumer.process_signal"):
-        return _process_signal(message)
-
-
-class OutcomesConsumerWorker(AbstractBatchWorker):
-    def __init__(self, concurrency):
-        self.pool = _multiprocessing.dummy.Pool(concurrency)
-        atexit.register(self.pool.close)
-
-    def process_message(self, message):
-        return json.loads(message.value())
-
-    def flush_batch(self, batch):
-        batch.sort(key=lambda msg: msg.get("project_id", 0) or 0)
-
-        with metrics.timer("outcomes_consumer.process_signal_batch"):
-            with batch_buffers_incr():
-                with BaseManager.local_cache():
-                    for _ in self.pool.imap_unordered(
-                        _process_signal_with_timer, batch, chunksize=100
-                    ):
-                        pass
-
-    def shutdown(self):
-        pass
-
-
-def get_outcomes_consumer(concurrency=None, **options):
-    """
-    Handles outcome requests coming via a kafka queue from Relay.
-    """
-
-    return create_batching_kafka_consumer(
-        topic_names=[settings.KAFKA_OUTCOMES],
-        worker=OutcomesConsumerWorker(concurrency=concurrency),
-        **options
-    )
diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index 339a2a3d35..32a5cc968a 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -459,25 +459,3 @@ def ingest_consumer(consumer_types, all_consumer_types, **options):
         ingest_consumer_types=",".join(sorted(consumer_types)), _all_threads=True
     ):
         get_ingest_consumer(consumer_types=consumer_types, **options).run()
-
-
-@run.command("outcomes-consumer")
-@log_options()
-@batching_kafka_options("outcomes-consumer")
-@click.option(
-    "--concurrency",
-    type=int,
-    default=1,
-    help="Spawn this many threads to process outcomes. Defaults to 1.",
-)
-@configuration
-def outcome_consumer(**options):
-    """
-    Runs an "outcomes consumer" task.
-
-    The "outcomes consumer" tasks read outcomes from a kafka topic and sends
-    signals for some of them.
-    """
-    from sentry.ingest.outcomes_consumer import get_outcomes_consumer
-
-    get_outcomes_consumer(**options).run()
diff --git a/src/sentry/signals.py b/src/sentry/signals.py
index f55494c353..41e899c994 100644
--- a/src/sentry/signals.py
+++ b/src/sentry/signals.py
@@ -52,14 +52,8 @@ class BetterSignal(Signal):
 
 
 buffer_incr_complete = BetterSignal(providing_args=["model", "columns", "extra", "result"])
-event_discarded = BetterSignal(providing_args=["project", "category", "quantity"])
-event_dropped = BetterSignal(
-    providing_args=["ip", "data", "project", "reason_code", "category", "quantity"]
-)
-event_filtered = BetterSignal(providing_args=["ip", "project", "category", "quantity"])
 pending_delete = BetterSignal(providing_args=["instance", "actor"])
 event_processed = BetterSignal(providing_args=["project", "event"])
-event_saved = BetterSignal(providing_args=["project", "category", "quantity"])
 
 # DEPRECATED
 event_received = BetterSignal(providing_args=["ip", "project"])
diff --git a/src/sentry/utils/sdk.py b/src/sentry/utils/sdk.py
index e062ef291e..4e3d1e5fd0 100644
--- a/src/sentry/utils/sdk.py
+++ b/src/sentry/utils/sdk.py
@@ -20,7 +20,8 @@ UNSAFE_FILES = (
     "sentry/event_manager.py",
     "sentry/tasks/process_buffer.py",
     "sentry/ingest/ingest_consumer.py",
-    "sentry/ingest/outcomes_consumer.py",
+    # This consumer lives outside of sentry but is just as unsafe.
+    "outcomes_consumer.py",
 )
 
 # Reexport sentry_sdk just in case we ever have to write another shim like we
diff --git a/tests/sentry/ingest/test_outcomes_consumer.py b/tests/sentry/ingest/test_outcomes_consumer.py
deleted file mode 100644
index cc90bd1435..0000000000
--- a/tests/sentry/ingest/test_outcomes_consumer.py
+++ /dev/null
@@ -1,238 +0,0 @@
-from __future__ import absolute_import
-
-import logging
-import pytest
-import six
-
-from sentry.ingest.outcomes_consumer import get_outcomes_consumer
-from sentry.signals import event_filtered, event_discarded, event_dropped, event_saved
-from sentry.testutils.factories import Factories
-from sentry.utils.outcomes import Outcome
-from django.conf import settings
-from sentry.utils import json
-from sentry.utils.json import prune_empty_keys
-from sentry.utils.data_filters import FilterStatKeys
-
-
-logger = logging.getLogger(__name__)
-
-# Poll this amount of times (for 0.1 sec each) at most to wait for messages
-MAX_POLL_ITERATIONS = 50
-
-group_counter = 0
-
-
-def _get_next_group_id():
-    """
-    Returns a unique kafka consumer group identifier, which is required to get
-    tests passing.
-    """
-    global group_counter
-    group_counter += 1
-    return "test-outcome-consumer-%s" % group_counter
-
-
-def _get_event_id(base_event_id):
-    return "{:032}".format(int(base_event_id))
-
-
-class OutcomeTester(object):
-    def __init__(self, kafka_producer, kafka_admin, task_runner):
-        self.events_filtered = []
-        self.events_discarded = []
-        self.events_dropped = []
-        self.events_saved = []
-
-        event_filtered.connect(self._event_filtered_receiver)
-        event_discarded.connect(self._event_discarded_receiver)
-        event_dropped.connect(self._event_dropped_receiver)
-        event_saved.connect(self._event_saved_receiver)
-
-        self.task_runner = task_runner
-        self.topic_name = settings.KAFKA_OUTCOMES
-        self.organization = Factories.create_organization()
-        self.project = Factories.create_project(organization=self.organization)
-
-        self.producer = self._create_producer(kafka_producer, kafka_admin)
-
-    def track_outcome(
-        self,
-        event_id=None,
-        key_id=None,
-        outcome=None,
-        reason=None,
-        remote_addr=None,
-        timestamp=None,
-    ):
-        message = {
-            "project_id": self.project.id,
-            "org_id": self.organization.id,
-            "event_id": event_id,
-            "key_id": key_id,
-            "outcome": outcome,
-            "reason": reason,
-            "remote_addr": remote_addr,
-            "timestamp": timestamp,
-        }
-
-        message = json.dumps(prune_empty_keys(message))
-        self.producer.produce(self.topic_name, message)
-
-    def run(self, predicate=None):
-        if predicate is None:
-            predicate = lambda: True
-
-        consumer = get_outcomes_consumer(
-            max_batch_size=1,
-            max_batch_time=100,
-            group_id=_get_next_group_id(),
-            auto_offset_reset="earliest",
-        )
-
-        with self.task_runner():
-            i = 0
-            while predicate() and i < MAX_POLL_ITERATIONS:
-                consumer._run_once()
-                i += 1
-
-        # Verify that we consumed everything and didn't time out
-        # assert not predicate()
-        assert i < MAX_POLL_ITERATIONS
-
-    def _event_filtered_receiver(self, **kwargs):
-        self.events_filtered.append(kwargs)
-
-    def _event_discarded_receiver(self, **kwargs):
-        self.events_discarded.append(kwargs)
-
-    def _event_dropped_receiver(self, **kwargs):
-        self.events_dropped.append(kwargs)
-
-    def _event_saved_receiver(self, **kwargs):
-        self.events_saved.append(kwargs)
-
-    def _create_producer(self, kafka_producer, kafka_admin):
-        # Clear the topic to ensure we run in a pristine environment
-        admin = kafka_admin(settings)
-        admin.delete_topic(self.topic_name)
-
-        producer = kafka_producer(settings)
-        return producer
-
-
-@pytest.fixture
-def outcome_tester(requires_kafka, kafka_producer, kafka_admin, task_runner):
-    return OutcomeTester(kafka_producer, kafka_admin, task_runner)
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_ignores_invalid_outcomes(outcome_tester):
-    # Add two FILTERED items so we know when the producer has reached the end
-    for i in range(4):
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.INVALID if i < 2 else Outcome.FILTERED,
-            reason="some_reason",
-            remote_addr="127.33.44.{}".format(i),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_filtered) < 2)
-
-    # verify that the appropriate filters were called
-    ips = [outcome["ip"] for outcome in outcome_tester.events_filtered]
-    assert ips == ["127.33.44.2", "127.33.44.3"]
-    assert not outcome_tester.events_dropped
-    assert not outcome_tester.events_saved
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_remembers_handled_outcomes(outcome_tester):
-    for i in six.moves.range(1, 3):
-        # emit the same outcome twice (simulate the case when the producer goes
-        # down without committing the kafka offsets and is restarted)
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.FILTERED,
-            reason="some_reason",
-            remote_addr="127.33.44.{}".format(1),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_filtered) < 1)
-
-    ips = [outcome["ip"] for outcome in outcome_tester.events_filtered]
-    assert ips == ["127.33.44.1"]  # only once!
-    assert not outcome_tester.events_dropped
-    assert not outcome_tester.events_saved
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_handles_filtered_outcomes(outcome_tester):
-    for i in six.moves.range(1, 3):
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.FILTERED,
-            reason="some_reason",
-            remote_addr="127.33.44.{}".format(i),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_filtered) < 2)
-
-    # verify that the appropriate filters were called
-    ips = [outcome["ip"] for outcome in outcome_tester.events_filtered]
-    assert len(ips) == 2
-    assert set(ips) == set(["127.33.44.1", "127.33.44.2"])
-    assert not outcome_tester.events_dropped
-    assert not outcome_tester.events_saved
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_handles_rate_limited_outcomes(outcome_tester):
-    for i in six.moves.range(1, 3):
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.RATE_LIMITED,
-            reason="reason_{}".format(i),
-            remote_addr="127.33.44.{}".format(i),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_dropped) < 2)
-
-    assert not outcome_tester.events_filtered
-    tuples = [(o["ip"], o["reason_code"]) for o in outcome_tester.events_dropped]
-    assert set(tuples) == set([("127.33.44.1", "reason_1"), ("127.33.44.2", "reason_2")])
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_handles_accepted_outcomes(outcome_tester):
-    for i in six.moves.range(1, 3):
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.ACCEPTED,
-            remote_addr="127.33.44.{}".format(i),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_saved) < 2)
-
-    assert not outcome_tester.events_filtered
-    assert len(outcome_tester.events_saved) == 2
-
-
-@pytest.mark.django_db
-def test_outcome_consumer_handles_discarded_outcomes(outcome_tester):
-    for i in six.moves.range(4):
-        if i < 2:
-            reason = "something_else"
-        else:
-            reason = FilterStatKeys.DISCARDED_HASH
-
-        outcome_tester.track_outcome(
-            event_id=_get_event_id(i),
-            outcome=Outcome.FILTERED,
-            reason=reason,
-            remote_addr="127.33.44.{}".format(i),
-        )
-
-    outcome_tester.run(lambda: len(outcome_tester.events_discarded) < 2)
-
-    assert len(outcome_tester.events_filtered) == 2
-    assert len(outcome_tester.events_discarded) == 2
