commit 234449f7232331d5584856553669aaf113aa6e8b
Author: Dan Fuller <dfuller@sentry.io>
Date:   Wed Apr 3 16:54:29 2019 -0700

    refs: Remove sqlite references from code
    
    We no longer support sqlite, so remove references from the codebase

diff --git a/.gitignore b/.gitignore
index 71bad123dd..0deb3f4631 100644
--- a/.gitignore
+++ b/.gitignore
@@ -23,7 +23,6 @@ sentry-package.json
 /tmp
 /node_modules/
 /docs-ui/node_modules/
-example/db.sqlite
 /src/sentry/assets.json
 /src/sentry/static/version
 /src/sentry/static/sentry/dist/
diff --git a/Makefile b/Makefile
index fed1c89171..8a23cf7efa 100644
--- a/Makefile
+++ b/Makefile
@@ -216,9 +216,8 @@ travis-noop:
 .PHONY: travis-test-lint
 travis-test-lint: lint-python lint-js
 
-.PHONY: travis-test-postgres travis-test-mysql travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js travis-test-cli travis-test-dist travis-test-riak
+.PHONY: travis-test-postgres travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js travis-test-cli travis-test-dist travis-test-riak
 travis-test-postgres: test-python
-travis-test-mysql: test-python
 travis-test-acceptance: test-acceptance
 travis-test-snuba: test-snuba
 travis-test-symbolicator: test-symbolicator
@@ -233,7 +232,7 @@ travis-test-dist:
 	@ls -lh dist/
 travis-test-riak: test-riak
 
-.PHONY: scan-python travis-scan-postgres travis-scan-mysql travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator travis-scan-js travis-scan-cli travis-scan-dist travis-scan-lint travis-scan-riak
+.PHONY: scan-python travis-scan-postgres travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator travis-scan-js travis-scan-cli travis-scan-dist travis-scan-lint travis-scan-riak
 scan-python:
 	@echo "--> Running Python vulnerability scanner"
 	$(PIP) install safety
@@ -241,7 +240,6 @@ scan-python:
 	@echo ""
 
 travis-scan-postgres: scan-python
-travis-scan-mysql: scan-python
 travis-scan-acceptance: travis-noop
 travis-scan-snuba: scan-python
 travis-scan-symbolicator: travis-noop
diff --git a/src/bitfield/types.py b/src/bitfield/types.py
index 0e17d896fe..9fef8cbde8 100644
--- a/src/bitfield/types.py
+++ b/src/bitfield/types.py
@@ -265,13 +265,6 @@ if django.VERSION[:2] >= (1, 8):
 
     # We need to register adapters in Django 1.8 in order to prevent
     # "ProgrammingError: can't adapt type"
-    try:
-        from django.db.backends.sqlite3.base import Database
-        Database.register_adapter(Bit, lambda x: int(x))
-        Database.register_adapter(BitHandler, lambda x: int(x))
-    except ImproperlyConfigured:
-        pass
-
     try:
         from django.db.backends.postgresql_psycopg2.base import Database
         Database.extensions.register_adapter(Bit, lambda x: Database.extensions.AsIs(int(x)))
diff --git a/src/debug_toolbar/panels/sql/views.py b/src/debug_toolbar/panels/sql/views.py
index 0694c4ed64..85e43d3f51 100644
--- a/src/debug_toolbar/panels/sql/views.py
+++ b/src/debug_toolbar/panels/sql/views.py
@@ -43,12 +43,7 @@ def sql_explain(request):
         vendor = form.connection.vendor
         cursor = form.cursor
 
-        if vendor == 'sqlite':
-            # SQLite's EXPLAIN dumps the low-level opcodes generated for a query;
-            # EXPLAIN QUERY PLAN dumps a more human-readable summary
-            # See http://www.sqlite.org/lang_explain.html for details
-            cursor.execute("EXPLAIN QUERY PLAN %s" % (sql, ), params)
-        elif vendor == 'postgresql':
+        if vendor == 'postgresql':
             cursor.execute("EXPLAIN ANALYZE %s" % (sql, ), params)
         else:
             cursor.execute("EXPLAIN %s" % (sql, ), params)
diff --git a/src/sentry/api/endpoints/team_groups_new.py b/src/sentry/api/endpoints/team_groups_new.py
index 3281ab792a..cb417f6b3e 100644
--- a/src/sentry/api/endpoints/team_groups_new.py
+++ b/src/sentry/api/endpoints/team_groups_new.py
@@ -8,7 +8,6 @@ from sentry.api.base import EnvironmentMixin
 from sentry.api.bases.team import TeamEndpoint
 from sentry.api.serializers import serialize, GroupSerializer
 from sentry.models import Group, GroupStatus, Project
-from sentry.utils.db import get_db_engine
 
 
 class TeamGroupsNewEndpoint(TeamEndpoint, EnvironmentMixin):
@@ -30,11 +29,7 @@ class TeamGroupsNewEndpoint(TeamEndpoint, EnvironmentMixin):
         cutoff = timedelta(minutes=minutes)
         cutoff_dt = timezone.now() - cutoff
 
-        if get_db_engine('default') == 'sqlite':
-            sort_value = 'times_seen'
-        else:
-            sort_value = 'score'
-
+        sort_value = 'score'
         group_list = list(
             Group.objects.filter(
                 project__in=project_dict.keys(),
diff --git a/src/sentry/api/endpoints/team_groups_trending.py b/src/sentry/api/endpoints/team_groups_trending.py
index 8b31dd28ea..2b0ab6a95b 100644
--- a/src/sentry/api/endpoints/team_groups_trending.py
+++ b/src/sentry/api/endpoints/team_groups_trending.py
@@ -8,7 +8,6 @@ from sentry.api.base import EnvironmentMixin
 from sentry.api.bases.team import TeamEndpoint
 from sentry.api.serializers import serialize, GroupSerializer
 from sentry.models import Group, GroupStatus, Project
-from sentry.utils.db import get_db_engine
 
 
 class TeamGroupsTrendingEndpoint(TeamEndpoint, EnvironmentMixin):
@@ -30,11 +29,7 @@ class TeamGroupsTrendingEndpoint(TeamEndpoint, EnvironmentMixin):
         cutoff = timedelta(minutes=minutes)
         cutoff_dt = timezone.now() - cutoff
 
-        if get_db_engine('default') == 'sqlite':
-            sort_value = 'times_seen'
-        else:
-            sort_value = 'score'
-
+        sort_value = 'score'
         group_list = list(
             Group.objects.filter(
                 project__in=project_dict.keys(),
diff --git a/src/sentry/db/models/fields/bounded.py b/src/sentry/db/models/fields/bounded.py
index 78da8e8fc2..2cac3ecb60 100644
--- a/src/sentry/db/models/fields/bounded.py
+++ b/src/sentry/db/models/fields/bounded.py
@@ -73,9 +73,6 @@ if settings.SENTRY_USE_BIG_INTS:
             engine = connection.settings_dict['ENGINE']
             if 'postgres' in engine:
                 return "bigserial"
-            # SQLite doesnt actually support bigints with auto incr
-            elif 'sqlite' in engine:
-                return 'integer'
             else:
                 raise NotImplemented
 
diff --git a/src/sentry/models/counter.py b/src/sentry/models/counter.py
index df2b25af2e..93a6c4a0ef 100644
--- a/src/sentry/models/counter.py
+++ b/src/sentry/models/counter.py
@@ -12,7 +12,7 @@ from django.db import connection, connections
 from django.db.models.signals import post_syncdb
 
 from sentry.db.models import (FlexibleForeignKey, Model, sane_repr, BoundedBigIntegerField)
-from sentry.utils.db import is_postgres, is_sqlite
+from sentry.utils.db import is_postgres
 
 
 class Counter(Model):
@@ -47,34 +47,6 @@ def increment_project_counter(project, delta=1):
             ''', [project.id, delta]
             )
             return cur.fetchone()[0]
-        elif is_sqlite():
-            value = cur.execute(
-                '''
-                insert or ignore into sentry_projectcounter
-                  (project_id, value) values (%s, 0);
-            ''', [project.id]
-            )
-            value = cur.execute(
-                '''
-                select value from sentry_projectcounter
-                 where project_id = %s
-            ''', [project.id]
-            ).fetchone()[0]
-            while True:
-                cur.execute(
-                    '''
-                    update sentry_projectcounter
-                       set value = value + %s
-                     where project_id = %s;
-                ''', [delta, project.id]
-                )
-                changes = cur.execute(
-                    '''
-                    select changes();
-                '''
-                ).fetchone()[0]
-                if changes != 0:
-                    return value + delta
         else:
             raise AssertionError("Not implemented database engine path")
     finally:
diff --git a/src/sentry/models/group.py b/src/sentry/models/group.py
index 700ee07404..ff4e15b509 100644
--- a/src/sentry/models/group.py
+++ b/src/sentry/models/group.py
@@ -273,7 +273,6 @@ class Group(Model):
     active_at = models.DateTimeField(null=True, db_index=True)
     time_spent_total = BoundedIntegerField(default=0)
     time_spent_count = BoundedIntegerField(default=0)
-    # score will be incorrect in sqlite as it doesnt support the required functions
     score = BoundedIntegerField(default=0)
     # deprecated, do not use. GroupShare has superseded
     is_public = models.NullBooleanField(default=False, null=True)
diff --git a/src/sentry/south_migrations/0135_auto__chg_field_project_team.py b/src/sentry/south_migrations/0135_auto__chg_field_project_team.py
index 9c5732cb85..12325f2b70 100644
--- a/src/sentry/south_migrations/0135_auto__chg_field_project_team.py
+++ b/src/sentry/south_migrations/0135_auto__chg_field_project_team.py
@@ -44,10 +44,6 @@ class Migration(SchemaMigration):
         Project.objects.filter(team__isnull=True).update(team=team)
 
     def forwards(self, orm):
-        # this is shitty, but we dont care
-        if connection.vendor == 'sqlite':
-            transaction.set_autocommit(True)
-
         # ideally we would have done this data migration before this change, but
         # it was an oversight
         if not db.dry_run:
diff --git a/src/sentry/south_migrations/0143_fill_project_orgs.py b/src/sentry/south_migrations/0143_fill_project_orgs.py
index bcfc77e65b..0587ad23c0 100644
--- a/src/sentry/south_migrations/0143_fill_project_orgs.py
+++ b/src/sentry/south_migrations/0143_fill_project_orgs.py
@@ -6,11 +6,7 @@ from django.db import IntegrityError, models, transaction
 
 
 def atomic_save(model):
-    try:
-        with transaction.atomic():
-            model.save()
-    except transaction.TransactionManagementError:
-        # sqlite isn't happy
+    with transaction.atomic():
         model.save()
 
 
diff --git a/src/sentry/south_migrations/0238_fill_org_onboarding_tasks.py b/src/sentry/south_migrations/0238_fill_org_onboarding_tasks.py
index 4adb867e9c..171d62dd64 100644
--- a/src/sentry/south_migrations/0238_fill_org_onboarding_tasks.py
+++ b/src/sentry/south_migrations/0238_fill_org_onboarding_tasks.py
@@ -10,9 +10,6 @@ from sentry.plugins import IssueTrackingPlugin, NotificationPlugin
 
 class Migration(DataMigration):
     def forwards(self, orm):
-        if connection.vendor == 'sqlite':
-            transaction.set_autocommit(True)
-
         # These are constants, not models
         from sentry.models import OnboardingTask, OnboardingTaskStatus
         from sentry.utils.query import RangeQuerySetWrapperWithProgressBar
@@ -104,7 +101,8 @@ class Migration(DataMigration):
                                 pass
 
                             break
-                        # This occurs if we've iterated through all the projects and only one platform is found
+                        # This occurs if we've iterated through all the projects and only one
+                        # platform is found
                         else:
                             try:
                                 with transaction.atomic():
diff --git a/src/sentry/south_migrations/0263_remove_default_regression_rule.py b/src/sentry/south_migrations/0263_remove_default_regression_rule.py
index 8708fe5fb5..96daf9e246 100644
--- a/src/sentry/south_migrations/0263_remove_default_regression_rule.py
+++ b/src/sentry/south_migrations/0263_remove_default_regression_rule.py
@@ -11,9 +11,6 @@ class Migration(DataMigration):
     def forwards(self, orm):
         from sentry.utils.query import RangeQuerySetWrapperWithProgressBar
 
-        if connection.vendor == 'sqlite':
-            transaction.set_autocommit(True)
-
         label = 'Send a notification for regressions'
         rule_data = {
             'match':
diff --git a/src/sentry/south_migrations/0279_populate_release_orgs_and_projects.py b/src/sentry/south_migrations/0279_populate_release_orgs_and_projects.py
index cf3438997e..64207ee9df 100644
--- a/src/sentry/south_migrations/0279_populate_release_orgs_and_projects.py
+++ b/src/sentry/south_migrations/0279_populate_release_orgs_and_projects.py
@@ -9,9 +9,6 @@ from sentry.utils.query import RangeQuerySetWrapperWithProgressBar
 class Migration(DataMigration):
     def forwards(self, orm):
         "Write your forwards methods here."
-        if connection.vendor == 'sqlite':
-            transaction.set_autocommit(True)
-
         qs = orm.Release.objects.all().select_related('project')
         for r in RangeQuerySetWrapperWithProgressBar(qs):
             if not r.organization_id:
diff --git a/src/sentry/south_migrations/0416_auto__del_field_identityprovider_organization__add_field_identityprovi.py b/src/sentry/south_migrations/0416_auto__del_field_identityprovider_organization__add_field_identityprovi.py
index aa254c3d2d..bb3e9b8dca 100644
--- a/src/sentry/south_migrations/0416_auto__del_field_identityprovider_organization__add_field_identityprovi.py
+++ b/src/sentry/south_migrations/0416_auto__del_field_identityprovider_organization__add_field_identityprovi.py
@@ -3,7 +3,6 @@ from south.utils import datetime_utils as datetime
 from south.db import db
 from south.v2 import SchemaMigration
 from django.db import models
-from sentry.utils.db import is_sqlite
 
 
 class Migration(SchemaMigration):
@@ -13,8 +12,6 @@ class Migration(SchemaMigration):
     is_dangerous = False
 
     def forwards(self, orm):
-        if is_sqlite():
-            return
         db.delete_foreign_key(u'sentry_identityprovider', 'organization_id')
         db.alter_column(
             u'sentry_identityprovider',
diff --git a/src/sentry/testutils/helpers/query.py b/src/sentry/testutils/helpers/query.py
index 482fdb78d3..d32fa6c0c9 100644
--- a/src/sentry/testutils/helpers/query.py
+++ b/src/sentry/testutils/helpers/query.py
@@ -1,8 +1,6 @@
 from __future__ import absolute_import
 
 import sqlparse
-import re
-import ast
 
 from sqlparse.tokens import DML
 
@@ -17,9 +15,6 @@ def parse_queries(captured_queries):
 
     for query in captured_queries:
         raw_sql = query['sql']
-        match = re.search(r"QUERY = (.+) - PARAMS", query['sql'])
-        if match:  # this is a sqlite query
-            raw_sql = ast.literal_eval(match.group(1))
         parsed = sqlparse.parse(raw_sql)
         for token in parsed[0].tokens:
             if token.ttype is DML:
diff --git a/src/sentry/utils/db.py b/src/sentry/utils/db.py
index dfdbce12b4..e77bf8716c 100644
--- a/src/sentry/utils/db.py
+++ b/src/sentry/utils/db.py
@@ -24,17 +24,6 @@ def is_postgres(alias='default'):
     return 'postgres' in engine
 
 
-def is_sqlite(alias='default'):
-    engine = get_db_engine(alias)
-    return 'sqlite' in engine
-
-
-def has_charts(db):
-    if is_sqlite(db):
-        return False
-    return True
-
-
 def attach_foreignkey(objects, field, related=[], database=None):
     """
     Shortcut method which handles a pythonic LEFT OUTER JOIN.
diff --git a/src/sentry/utils/pytest/sentry.py b/src/sentry/utils/pytest/sentry.py
index eccb242d1d..a9783c9a65 100644
--- a/src/sentry/utils/pytest/sentry.py
+++ b/src/sentry/utils/pytest/sentry.py
@@ -50,13 +50,6 @@ def pytest_configure(config):
             # postgres requires running full migration all the time
             # since it has to install stored functions which come from
             # an actual migration.
-        elif test_db == 'sqlite':
-            settings.DATABASES['default'].update(
-                {
-                    'ENGINE': 'django.db.backends.sqlite3',
-                    'NAME': ':memory:',
-                }
-            )
         else:
             raise RuntimeError('oops, wrong database: %r' % test_db)
 
diff --git a/src/sentry/web/frontend/organization_auth_settings.py b/src/sentry/web/frontend/organization_auth_settings.py
index cec54126fe..11d83a5c9e 100644
--- a/src/sentry/web/frontend/organization_auth_settings.py
+++ b/src/sentry/web/frontend/organization_auth_settings.py
@@ -15,7 +15,6 @@ from sentry.auth.superuser import is_active_superuser
 from sentry.models import AuditLogEntryEvent, AuthProvider, OrganizationMember, User
 from sentry.plugins import Response
 from sentry.tasks.auth import email_missing_links, email_unlink_notifications
-from sentry.utils import db
 from sentry.utils.http import absolute_uri
 from sentry.web.frontend.base import OrganizationView
 
@@ -58,21 +57,15 @@ class OrganizationAuthSettingsView(OrganizationView):
             data=auth_provider.get_audit_log_data(),
         )
 
-        if db.is_sqlite():
-            for om in OrganizationMember.objects.filter(organization=organization):
-                om.flags['sso:linked'] = False
-                om.flags['sso:invalid'] = False
-                om.save()
-        else:
-            OrganizationMember.objects.filter(
-                organization=organization,
-            ).update(
-                flags=F('flags').bitand(
-                    ~OrganizationMember.flags['sso:linked'],
-                ).bitand(
-                    ~OrganizationMember.flags['sso:invalid'],
-                ),
-            )
+        OrganizationMember.objects.filter(
+            organization=organization,
+        ).update(
+            flags=F('flags').bitand(
+                ~OrganizationMember.flags['sso:linked'],
+            ).bitand(
+                ~OrganizationMember.flags['sso:invalid'],
+            ),
+        )
 
         user_ids = OrganizationMember.objects.filter(organization=organization).values('user')
         User.objects.filter(id__in=user_ids).update(is_managed=False)
diff --git a/src/south/db/__init__.py b/src/south/db/__init__.py
index 06633cce62..ab3f7fda5d 100644
--- a/src/south/db/__init__.py
+++ b/src/south/db/__init__.py
@@ -8,7 +8,6 @@ import sys
 # A few aliases, because there's FQMNs now
 engine_modules = {
     'django.db.backends.postgresql_psycopg2': 'postgresql_psycopg2',
-    'django.db.backends.sqlite3': 'sqlite3',
 }
 
 # First, work out if we're multi-db or not, and which databases we have
diff --git a/src/south/db/generic.py b/src/south/db/generic.py
index afdd4f79fd..2492899731 100644
--- a/src/south/db/generic.py
+++ b/src/south/db/generic.py
@@ -687,7 +687,7 @@ class DatabaseOperations(object):
 
         if sql:
 
-            # Some callers, like the sqlite stuff, just want the extended type.
+            # Some callers just want the extended type.
             if with_name:
                 field_output = [self.quote_name(field.column), sql]
             else:
diff --git a/src/south/db/sqlite3.py b/src/south/db/sqlite3.py
deleted file mode 100644
index e3d1d31d07..0000000000
--- a/src/south/db/sqlite3.py
+++ /dev/null
@@ -1,288 +0,0 @@
-from south.db import generic
-
-
-class DatabaseOperations(generic.DatabaseOperations):
-
-    """
-    SQLite3 implementation of database operations.
-    """
-
-    backend_name = "sqlite3"
-
-    # SQLite ignores several constraints. I wish I could.
-    supports_foreign_keys = False
-    has_check_constraints = False
-    has_booleans = False
-
-    def add_column(self, table_name, name, field, *args, **kwds):
-        """
-        Adds a column.
-        """
-        # XXX(dcramer): disabling this functionality as it causes problems with some invalid
-        # migrations
-        # If it's not nullable, and has no default, raise an error (SQLite is picky)
-        # if (not field.null and
-        #     (not field.has_default() or field.get_default() is None) and
-        #         not field.empty_strings_allowed):
-        #     raise ValueError("You cannot add a null=False column without a default value.")
-
-        # Initialise the field.
-        field.set_attributes_from_name(name)
-        # We add columns by remaking the table; even though SQLite supports
-        # adding columns, it doesn't support adding PRIMARY KEY or UNIQUE cols.
-        # We define fields with no default; a default will be used, though, to
-        # fill up the remade table
-        field_default = None
-        if not getattr(field, '_suppress_default', False):
-            default = field.get_default()
-            if default is not None:
-                field_default = "'%s'" % field.get_db_prep_save(
-                    default, connection=self._get_connection())
-        field._suppress_default = True
-        self._remake_table(table_name, added={
-            field.column: (self._column_sql_for_create(table_name, name, field, False), field_default)
-        })
-
-    def _get_full_table_description(self, connection, cursor, table_name):
-        cursor.execute('PRAGMA table_info(%s)' % connection.ops.quote_name(table_name))
-        # cid, name, type, notnull, dflt_value, pk
-        return [{'name': field[1],
-                 'type': field[2],
-                 'null_ok': not field[3],
-                 'dflt_value': field[4],
-                 'pk': field[5]     # undocumented
-                 } for field in cursor.fetchall()]
-
-    @generic.invalidate_table_constraints
-    def _remake_table(self, table_name, added={}, renames={}, deleted=[],
-                      altered={}, primary_key_override=None, uniques_deleted=[]):
-        """
-        Given a table and three sets of changes (renames, deletes, alters),
-        recreates it with the modified schema.
-        """
-        # Dry runs get skipped completely
-        if self.dry_run:
-            return
-        # Temporary table's name
-        temp_name = "_south_new_" + table_name
-        # Work out the (possibly new) definitions of each column
-        definitions = {}
-        cursor = self._get_connection().cursor()
-        # Get the index descriptions
-        indexes = self._get_connection().introspection.get_indexes(cursor, table_name)
-        standalone_indexes = self._get_standalone_indexes(table_name)
-        # Work out new column defs.
-        for column_info in self._get_full_table_description(
-                self._get_connection(), cursor, table_name):
-            name = column_info['name']
-            if name in deleted:
-                continue
-            # Get the type, ignoring PRIMARY KEY (we need to be consistent)
-            type = column_info['type'].replace("PRIMARY KEY", "")
-            # Add on primary key, not null or unique if needed.
-            if (primary_key_override and primary_key_override == name) or \
-               (not primary_key_override and name in indexes and
-                    indexes[name]['primary_key']):
-                type += " PRIMARY KEY"
-            elif not column_info['null_ok']:
-                type += " NOT NULL"
-            if (name in indexes and indexes[name]['unique'] and
-                    name not in uniques_deleted):
-                type += " UNIQUE"
-            if column_info['dflt_value'] is not None:
-                type += " DEFAULT " + column_info['dflt_value']
-            # Deal with a rename
-            if name in renames:
-                name = renames[name]
-            # Add to the defs
-            definitions[name] = type
-        # Add on altered columns
-        for name, type in altered.items():
-            if (primary_key_override and primary_key_override == name) or \
-               (not primary_key_override and name in indexes and
-                    indexes[name]['primary_key']):
-                type += " PRIMARY KEY"
-            if (name in indexes and indexes[name]['unique'] and
-                    name not in uniques_deleted):
-                type += " UNIQUE"
-            definitions[name] = type
-        # Add on the new columns
-        for name, (type, _) in added.items():
-            if (primary_key_override and primary_key_override == name):
-                type += " PRIMARY KEY"
-            definitions[name] = type
-        # Alright, Make the table
-        self.execute("CREATE TABLE %s (%s)" % (
-            self.quote_name(temp_name),
-            ", ".join(["%s %s" % (self.quote_name(cname), ctype)
-                       for cname, ctype in definitions.items()]),
-        ))
-        # Copy over the data
-        self._copy_data(table_name, temp_name, renames, added)
-        # Delete the old table, move our new one over it
-        self.delete_table(table_name)
-        self.rename_table(temp_name, table_name)
-        # Recreate multi-valued indexes
-        # We can't do that before since it's impossible to rename indexes
-        # and index name scope is global
-        self._make_standalone_indexes(
-            table_name,
-            standalone_indexes,
-            renames=renames,
-            deleted=deleted,
-            uniques_deleted=uniques_deleted)
-        self.deferred_sql = []  # prevent double indexing
-
-    def _copy_data(self, src, dst, field_renames={}, added={}):
-        "Used to copy data into a new table"
-        # Make a list of all the fields to select
-        cursor = self._get_connection().cursor()
-        src_fields = [column_info[0] for column_info in self._get_connection(
-        ).introspection.get_table_description(cursor, src)]
-        dst_fields = [column_info[0] for column_info in self._get_connection(
-        ).introspection.get_table_description(cursor, dst)]
-        src_fields_new = []
-        dst_fields_new = []
-        for field in src_fields:
-            if field in field_renames:
-                dst_fields_new.append(self.quote_name(field_renames[field]))
-            elif field in dst_fields:
-                dst_fields_new.append(self.quote_name(field))
-            else:
-                continue
-            src_fields_new.append(self.quote_name(field))
-        for field, (_, default) in added.items():
-            if default is not None:
-                field = self.quote_name(field)
-                src_fields_new.append("%s as %s" % (default, field))
-                dst_fields_new.append(field)
-        # Copy over the data
-        self.execute("INSERT INTO %s (%s) SELECT %s FROM %s;" % (
-            self.quote_name(dst),
-            ', '.join(dst_fields_new),
-            ', '.join(src_fields_new),
-            self.quote_name(src),
-        ))
-
-    def _create_unique(self, table_name, columns):
-        self._create_index(table_name, columns, True)
-
-    def _create_index(self, table_name, columns, unique=False, index_name=None):
-        if index_name is None:
-            index_name = '%s_%s' % (table_name, '__'.join(columns))
-        self.execute("CREATE %sINDEX %s ON %s(%s);" % (
-            unique and "UNIQUE " or "",
-            self.quote_name(index_name),
-            self.quote_name(table_name),
-            ', '.join(self.quote_name(c) for c in columns),
-        ))
-
-    def _get_standalone_indexes(self, table_name):
-        indexes = []
-        cursor = self._get_connection().cursor()
-        cursor.execute('PRAGMA index_list(%s)' % self.quote_name(table_name))
-        # seq, name, unique
-        for index, unique in [(field[1], field[2]) for field in cursor.fetchall()]:
-            cursor.execute('PRAGMA index_info(%s)' % self.quote_name(index))
-            info = cursor.fetchall()
-            if len(info) == 1 and unique:
-                # This index is already specified in the CREATE TABLE columns
-                # specification
-                continue
-            columns = []
-            for field in info:
-                columns.append(field[2])
-            indexes.append((index, columns, unique))
-        return indexes
-
-    def _make_standalone_indexes(self, table_name, indexes, deleted=[],
-                                 renames={}, uniques_deleted=[]):
-        for index_name, index, unique in indexes:
-            columns = []
-
-            for name in index:
-                # Handle deletion
-                if name in deleted:
-                    columns = []
-                    break
-
-                # Handle renames
-                if name in renames:
-                    name = renames[name]
-                columns.append(name)
-
-            if columns and (set(columns) != set(uniques_deleted) or not unique):
-                self._create_index(table_name, columns, unique, index_name)
-
-    def _column_sql_for_create(self, table_name, name, field, explicit_name=True):
-        "Given a field and its name, returns the full type for the CREATE TABLE (without unique/pk)"
-        field.set_attributes_from_name(name)
-        if not explicit_name:
-            name = field.db_column
-        else:
-            field.column = name
-        sql = self.column_sql(table_name, name, field, with_name=False, field_prepared=True)
-        # Remove keywords we don't want (this should be type only, not constraint)
-        if sql:
-            sql = sql.replace("PRIMARY KEY", "")
-        return sql
-
-    def alter_column(self, table_name, name, field, explicit_name=True, ignore_constraints=False):
-        """
-        Changes a column's SQL definition.
-
-        Note that this sqlite3 implementation ignores the ignore_constraints argument.
-        The argument is accepted for API compatibility with the generic
-        DatabaseOperations.alter_column() method.
-        """
-        # Change nulls to default if needed
-        if not field.null and field.has_default():
-            params = {
-                "column": self.quote_name(name),
-                "table_name": self.quote_name(table_name)
-            }
-            self._update_nulls_to_default(params, field)
-        # Remake the table correctly
-        field._suppress_default = True
-        self._remake_table(table_name, altered={
-            name: self._column_sql_for_create(table_name, name, field, explicit_name),
-        })
-
-    def delete_column(self, table_name, column_name):
-        """
-        Deletes a column.
-        """
-        self._remake_table(table_name, deleted=[column_name])
-
-    def rename_column(self, table_name, old, new):
-        """
-        Renames a column from one name to another.
-        """
-        self._remake_table(table_name, renames={old: new})
-
-    def create_unique(self, table_name, columns):
-        """
-        Create an unique index on columns
-        """
-        self._create_unique(table_name, columns)
-
-    def delete_unique(self, table_name, columns):
-        """
-        Delete an unique index
-        """
-        self._remake_table(table_name, uniques_deleted=columns)
-
-    def create_primary_key(self, table_name, columns):
-        if not isinstance(columns, (list, tuple)):
-            columns = [columns]
-        assert len(columns) == 1, "SQLite backend does not support multi-column primary keys"
-        self._remake_table(table_name, primary_key_override=columns[0])
-
-    # Not implemented this yet.
-    def delete_primary_key(self, table_name):
-        # By passing True in, we make sure we wipe all existing PKs.
-        self._remake_table(table_name, primary_key_override=True)
-
-    # No cascades on deletes
-    def delete_table(self, table_name, cascade=True):
-        generic.DatabaseOperations.delete_table(self, table_name, False)
diff --git a/tests/sentry/db/test_parse_query.py b/tests/sentry/db/test_parse_query.py
index 68a1077a72..ec1d82bb9f 100644
--- a/tests/sentry/db/test_parse_query.py
+++ b/tests/sentry/db/test_parse_query.py
@@ -5,51 +5,6 @@ from sentry.testutils.helpers import parse_queries
 
 
 class ParseQuery(TestCase):
-    def test_parse_query(self):
-        result = parse_queries(
-            [
-                {u'sql': u'QUERY = u\'INSERT INTO "sentry_useremail" ("user_id", "email", "validation_hash", "date_hash_added", "is_verified") VALUES (%s, %s, %s, %s, %s)\' - PARAMS = (1, u\'admin@localhost\', u\'i0NlOcwzPKoObK8uNfg7mowTlOnvvlSI\', u\'2018-05-16 08:02:39.022342\', False)',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'INSERT INTO "sentry_email" ("email", "date_added") VALUES (%s, %s)\' - PARAMS = (u\'admin@localhost\', u\'2018-05-16 08:02:39.023101\')',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'UPDATE "sentry_useremail" SET "is_verified" = %s WHERE ("sentry_useremail"."user_id" = %s  AND "sentry_useremail"."email" = %s )\' - PARAMS = (True, 1, u\'admin@localhost\')',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'DELETE * FROM "sentry_organization"\' - PARAMS = (u\'baz\', u\'baz\', 0, u\'2018-05-16 08:02:39.025899\', u\'member\', 1)',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'INSERT INTO "sentry_organizationmember" ("organization_id", "user_id", "email", "role", "flags", "token", "date_added", "has_global_access", "type") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\' - PARAMS = (2, 1, None, u\'owner\', 0, None, u\'2018-05-16 08:02:39.026919\', True, 50)',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'UPDATE "sentry_projectoptions" SET "value" = %s WHERE ("sentry_projectoptions"."project_id" = %s  AND "sentry_projectoptions"."key" = %s )\' - PARAMS = (u\'gAJYIAAAADgwNmQxZjQ1NThkZjExZTg5ZWExOGM4NTkwMGNhNWI3cQEu\', 2, u\'sentry:relay-rev\')',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'UPDATE "sentry_projectoptions" SET "value" = %s WHERE ("sentry_projectoptions"."project_id" = %s  AND "sentry_projectoptions"."key" = %s )\' - PARAMS = (u\'gAJjZGF0ZXRpbWUKZGF0ZXRpbWUKcQFVCgfiBRAIAicApBhjcHl0egpfVVRDCnECKVJxA4ZScQQu\', 2, u\'sentry:relay-rev-lastchange\')',
-                 u'time': u'0.000'},
-                {u'sql': u"QUERY = '\\n                insert or ignore into sentry_projectcounter\\n                  (project_id, value) values (%s, 0);\\n            ' - PARAMS = (2,)",
-                 u'time': u'0.000'},
-                {u'sql': u"QUERY = '\\n                select value from sentry_projectcounter\\n                 where project_id = %s\\n            ' - PARAMS = (2,)",
-                 u'time': u'0.000'},
-                {u'sql': u"QUERY = '\\n                    update sentry_projectcounter\\n                       set value = value + %s\\n                     where project_id = %s;\\n                ' - PARAMS = (1, 2)",
-                 u'time': u'0.000'},
-                {u'sql': u"QUERY = '\\n                    select changes();\\n                ' - PARAMS = ()",
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'INSERT INTO "sentry_groupedmessage" ("project_id", "logger", "level", "message", "view", "num_comments", "platform", "status", "times_seen", "last_seen", "first_seen", "first_release_id", "resolved_at", "active_at", "time_spent_total", "time_spent_count", "score", "is_public", "data", "short_id") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\' - PARAMS = (2, u\'\', 40, u\'hello http://example.com\', u\'http://example.com\', 0, u\'javascript\', 0, 1, u\'2018-05-16 08:02:39\', u\'2018-05-16 08:02:39\', None, None, u\'2018-05-16 08:02:39\', 0, 0, 1526457759, False, u\'eJwVykEKg0AMheF9LjKuCk4dx16gFxDcSmgiHYgYOrHg7c0s//e+jrSHOQhWW3/84fJnCqAR3n2K45ByTi+oc7BL2fenW+INTzGvoT07GxIaeifoSEcnVkwaz7B8WeQAnaDWxw3kwCAZ\', 1)',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'UPDATE "sentry_grouphash" SET "group_id" = %s WHERE ("sentry_grouphash"."id" IN (%s) AND NOT ("sentry_grouphash"."state" = %s  AND "sentry_grouphash"."state" IS NOT NULL))\' - PARAMS = (1, 1, 1)',
-                 u'time': u'0.000'},
-                {u'sql': u'QUERY = u\'UPDATE "sentry_userreport" SET "environment_id" = %s, "group_id" = %s WHERE ("sentry_userreport"."project_id" = %s  AND "sentry_userreport"."event_id" = %s )\' - PARAMS = (1, 1, 2, u\'45b41f6d313c442393aaa0293853d70f\')',
-                 u'time': u'0.000'}]
-        )
-
-        assert result == {
-            'sentry_email': 1,
-            'sentry_groupedmessage': 1,
-            'sentry_grouphash': 1,
-            'sentry_organization': 1,
-            'sentry_organizationmember': 1,
-            'sentry_projectcounter': 2,
-            'sentry_projectoptions': 2,
-            'sentry_useremail': 2,
-            'sentry_userreport': 1
-        }
-
     def test_parse_postgres_queries(self):
         result = parse_queries([
             {u'sql': u'SAVEPOINT "s47890194282880_x49"', u'time': u'0.000'},
diff --git a/tests/sentry/utils/db/tests.py b/tests/sentry/utils/db/tests.py
index 2d69690df8..eb6d2a799c 100644
--- a/tests/sentry/utils/db/tests.py
+++ b/tests/sentry/utils/db/tests.py
@@ -8,8 +8,8 @@ from sentry.testutils import TestCase
 
 class GetDbEngineTest(TestCase):
     def test_with_dotted_path(self):
-        with self.settings(DATABASES={'default': {'ENGINE': 'blah.sqlite3'}}):
-            self.assertEquals(get_db_engine(), 'sqlite3')
+        with self.settings(DATABASES={'default': {'ENGINE': 'blah.postgres'}}):
+            self.assertEquals(get_db_engine(), 'postgres')
 
     def test_no_path(self):
         with self.settings(DATABASES={'default': {'ENGINE': 'postgres'}}):
