commit 1d706290996571e4a8e3e908adf8acaa8f4caa24
Author: Ted Kaemming <ted@kaemming.com>
Date:   Tue Sep 22 15:00:26 2015 -0700

    Document `Backend`, move things over to `digests` for clarity.
    
    (The timeline backend is too coupled to the scheduler to call it
    `timelines`, so `digests` makes more sense here although it is more
    limiting.)

diff --git a/timelines.py b/digests.py
similarity index 75%
rename from timelines.py
rename to digests.py
index b01ef9d036..80ca67b21b 100644
--- a/timelines.py
+++ b/digests.py
@@ -10,8 +10,8 @@ import sys
 import time
 import uuid
 
-from sentry.app import timelines
-from sentry.timelines.redis import Record
+from sentry.app import digests
+from sentry.digests.redis import Record
 
 
 logging.basicConfig(level=logging.DEBUG)
@@ -36,7 +36,7 @@ with timer('Generated {0} records to be loaded into {1} timelines'.format(n_reco
     for i in xrange(0, n_records):
         p = random.randint(1, n_timelines)
         record = Record(uuid.uuid1().hex, payload, time.time())
-        calls.append(functools.partial(timelines.add, 'projects/{0}'.format(p), record))
+        calls.append(functools.partial(digests.add, 'projects/{0}'.format(p), record))
 
 
 with timer('Loaded {0} records'.format(len(calls))):
@@ -49,16 +49,15 @@ with timer('Loaded {0} records'.format(len(calls))):
 ready = set()
 
 with timer('Scheduled timelines for digestion'):
-    for chunk in timelines.schedule(time.time()):
-        for timeline, timestamp in chunk:
-            ready.add(timeline)
+    for entry in digests.schedule(time.time()):
+        ready.add(entry.key)
 
 
 # Run them through the digestion process.
 
 with timer('Digested {0} timelines'.format(len(ready))):
     for timeline in ready:
-        with timelines.digest(timeline) as records:
+        with digests.digest(timeline) as records:
             i = 0
 
             # Iterate through the records to ensure that all data is deserialized.
@@ -71,9 +70,8 @@ with timer('Digested {0} timelines'.format(len(ready))):
 ready.clear()
 
 with timer('Scheduled timelines for digestion'):
-    for chunk in timelines.schedule(time.time() + timelines.backoff(1)):
-        for timeline, timestamp in chunk:
-            ready.add(timeline)
+    for entry in digests.schedule(time.time() + digests.backoff(1)):
+        ready.add(entry.key)
 
 
 # Run them through the digestion process again (this should result in all of
@@ -81,7 +79,7 @@ with timer('Scheduled timelines for digestion'):
 
 with timer('Digested {0} timelines'.format(len(ready))):
     for timeline in ready:
-        with timelines.digest(timeline) as records:
+        with digests.digest(timeline) as records:
             i = 0
             for i, record in enumerate(records, 1):
                 pass
@@ -89,7 +87,7 @@ with timer('Digested {0} timelines'.format(len(ready))):
 
 # Check to make sure we're not leaking any data.
 
-with timelines.cluster.all() as client:
+with digests.cluster.all() as client:
     result = client.keys('*')
 
 for host, value in result.value.iteritems():
diff --git a/src/sentry/app.py b/src/sentry/app.py
index a5d70a6792..037e11f36b 100644
--- a/src/sentry/app.py
+++ b/src/sentry/app.py
@@ -28,7 +28,7 @@ def get_instance(path, options):
 # TODO(dcramer): this is getting heavy, we should find a better way to structure
 # this
 buffer = get_instance(settings.SENTRY_BUFFER, settings.SENTRY_BUFFER_OPTIONS)
-timelines = get_instance(settings.SENTRY_TIMELINES, settings.SENTRY_TIMELINES_OPTIONS)
+digests = get_instance(settings.SENTRY_DIGESTS, settings.SENTRY_DIGESTS_OPTIONS)
 quotas = get_instance(settings.SENTRY_QUOTAS, settings.SENTRY_QUOTA_OPTIONS)
 nodestore = get_instance(
     settings.SENTRY_NODESTORE, settings.SENTRY_NODESTORE_OPTIONS)
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 1b10fcb6d5..74ba1f5a50 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -702,9 +702,9 @@ CACHES = {
 # CACHES backend.
 CACHE_VERSION = 1
 
-# Timelines backend
-SENTRY_TIMELINES = 'sentry.timelines.redis.RedisBackend'
-SENTRY_TIMELINES_OPTIONS = {}
+# Digests backend
+SENTRY_DIGESTS = 'sentry.digests.redis.RedisBackend'
+SENTRY_DIGESTS_OPTIONS = {}
 
 # Quota backend
 SENTRY_QUOTAS = 'sentry.quotas.Quota'
diff --git a/src/sentry/timelines/__init__.py b/src/sentry/digests/__init__.py
similarity index 100%
rename from src/sentry/timelines/__init__.py
rename to src/sentry/digests/__init__.py
diff --git a/src/sentry/timelines/backoff.py b/src/sentry/digests/backoff.py
similarity index 100%
rename from src/sentry/timelines/backoff.py
rename to src/sentry/digests/backoff.py
diff --git a/src/sentry/digests/base.py b/src/sentry/digests/base.py
new file mode 100644
index 0000000000..b4a4da36d8
--- /dev/null
+++ b/src/sentry/digests/base.py
@@ -0,0 +1,159 @@
+import logging
+from collections import namedtuple
+
+from sentry.utils.imports import import_string
+
+
+logger = logging.getLogger('sentry.digests')
+
+
+Record = namedtuple('Record', 'key value timestamp')
+
+ScheduleEntry = namedtuple('ScheduleEntry', 'key timestamp')
+
+
+def load(options):
+    return import_string(options['path'])(**options.get('options', {}))
+
+
+DEFAULT_BACKOFF = {
+    'path': 'sentry.digests.backoff.IntervalBackoffStrategy',
+}
+
+
+DEFAULT_CODEC = {
+    'path': 'sentry.digests.codecs.CompressedPickleCodec',
+}
+
+
+class Backend(object):
+    """
+    A digest backend coordinates the addition of records to timelines, as well
+    as scheduling their digestion (processing.) This allows for summarizations
+    of activity that was recorded as having occurrred during a window of time.
+
+    A timeline is the central abstraction for digests. A timeline is a
+    reverse-chronological set of records. Timelines are identified by a unique
+    key. Records within a timeline are also identified by a key that is unique
+    with respect to the timeline they are a part of.
+
+    A timeline can be in one of two states: "waiting" or "ready". When the
+    first record is added to a timeline, the timeline transitions to the
+    "waiting" state. This transition also causes the timeline to be scheduled
+    for digestion after the waiting period. After the waiting period expires,
+    the timeline transitions to the "ready" state, which causes the timeline to
+    be digested. After the timeline is digested, it transitions back to the
+    "waiting" state (causing it to be rescheduled) if it contained records. If
+    the timeline did not contain any records when it was digested, it can be
+    deleted (although deletion may be preempted by a new record being added to
+    the timeline, requiring it to be transitioned to "waiting" instead.)
+    """
+    def __init__(self, **options):
+        # The ``codec`` option provides the strategy for encoding and decoding
+        # records in the timeline.
+        self.codec = load(options.pop('codec', DEFAULT_CODEC))
+
+        # The ``backoff`` option provides the strategy for calculating
+        # scheduling intervals.
+        self.backoff = load(options.pop('backoff', DEFAULT_BACKOFF))
+
+        # The ``capacity`` option defines the maximum number of items that
+        # should be contained within a timeline. (Whether this is a hard or
+        # soft limit is backend dependent -- see the ``trim_chance`` option.)
+        self.capacity = options.pop('capacity', None)
+        if self.capacity is not None and self.capacity < 1:
+            raise ValueError('Timeline capacity must be at least 1 if used.')
+
+        # The ``truncation_chance`` option defines the probability that an
+        # ``add`` operation will trigger a truncation of the timeline to keep
+        # it's size close to the defined capacity. A value of 1 will cause the
+        # timeline to be truncated on every ``add`` operation (effectively
+        # making it a hard limit), while a lower probability will increase the
+        # chance of the timeline growing past it's intended capacity, but
+        # increases the performance of ``add`` operations (by avoiding
+        # truncation, which is a potentially expensive operation, especially on
+        # large data sets.)
+        if self.capacity:
+            self.truncation_chance = options.pop('truncation_chance', 1.0 / self.capacity)
+        else:
+            self.truncation_chance = None
+            if 'truncation_chance' in options:
+                logger.warning('No timeline capacity has been set, ignoring "truncation_chance" option.')
+                del options['truncation_chance']
+
+    def add(self, key, record):
+        """
+        Add a record to a timeline.
+
+        Adding a record to a timeline also causes it to be added to the
+        schedule, if it is not already present.
+
+        If another record exists in the timeline with the same record key, it
+        will be overwritten.
+        """
+        raise NotImplementedError
+
+    def digest(self, key):
+        """
+        Extract records from a timeline for processing.
+
+        This method acts as a context manager. The target of the ``as`` clause
+        is an iterator contains all of the records contained within the digest.
+
+        If the context manager successfully exits, all records that were part
+        of the digest are removed from the timeline and the timeline is placed
+        back in the "waiting" state. If an exception is raised during the
+        execution of the context manager, all records are preserved and no
+        state change occurs so that the next invocation will contain all
+        records that the were included previously, as well as any records that
+        were added in between invocations. (This means that the caller must
+        either retry the digest operation until it succeeds, or wait for the
+        operation to be rescheduled as part of the maintenance process for the
+        items to be processed.)
+
+        Typically, the block that is surrounded by context manager includes all
+        of the processing logic necessary to summarize the timeline contents
+        (since this process is generally has no side effects), while any
+        irrevocable action -- such as sending an email -- should occur after
+        the context manager has exited, to ensure that action is performed at
+        most once.
+
+        For example::
+
+            with timelines.digest('project:1') as records:
+                message = build_digest_email(records)
+
+            message.send()
+
+        """
+        raise NotImplementedError
+
+    def schedule(self, deadline):
+        """
+        Identify timelines that are ready for processing.
+
+        This method moves all timelines that are ready to be digested from the
+        waiting state to the ready state if their schedule time is prior to the
+        deadline. This method returns an iterator of schedule entries that were
+        moved.
+        """
+        raise NotImplementedError
+
+    def maintenance(self, deadline):
+        """
+        Identify timelines that appear to be stuck in the ready state.
+
+        This method moves all timelines that are in the ready state back to the
+        waiting state if their schedule time is prior to the deadline. (This
+        does not reschdule any tasks directly, and should generally be
+        performed as part of the scheduler task, before the ``schedule``
+        call.)
+
+        This is designed to handle the situation where task execution is
+        managed by a separate system such as RabbitMQ & Celery from scheduling.
+        A digest task may not be able to be succesfully retried after a failure
+        (e.g. if the process executing the task can no longer communicate with
+        the messaging broker) which can result in a task remaining in the ready
+        state without an execution plan.
+        """
+        raise NotImplementedError
diff --git a/src/sentry/timelines/codecs.py b/src/sentry/digests/codecs.py
similarity index 100%
rename from src/sentry/timelines/codecs.py
rename to src/sentry/digests/codecs.py
diff --git a/src/sentry/timelines/redis.py b/src/sentry/digests/redis.py
similarity index 88%
rename from src/sentry/timelines/redis.py
rename to src/sentry/digests/redis.py
index 2b20c86cfd..80571385cc 100644
--- a/src/sentry/timelines/redis.py
+++ b/src/sentry/digests/redis.py
@@ -17,14 +17,14 @@ from redis.exceptions import (
     WatchError,
 )
 
-
 from .base import (
     Backend,
     Record,
+    ScheduleEntry,
 )
 
 
-logger = logging.getLogger('sentry.timelines')
+logger = logging.getLogger('sentry.digests')
 
 
 ADD_TO_SCHEDULE_SCRIPT = """\
@@ -88,8 +88,8 @@ def make_schedule_key(namespace, state):
     return '{0}:s:{1}'.format(namespace, state)
 
 
-def make_timeline_key(namespace, target):
-    return '{0}:t:{1}'.format(namespace, target)
+def make_timeline_key(namespace, key):
+    return '{0}:t:{1}'.format(namespace, key)
 
 
 def make_iteration_key(timeline_key):
@@ -117,8 +117,8 @@ class RedisBackend(Backend):
         if options:
             logger.warning('Discarding invalid options: %r', options)
 
-    def add(self, target, record):
-        timeline_key = make_timeline_key(self.namespace, target)
+    def add(self, key, record):
+        timeline_key = make_timeline_key(self.namespace, key)
         record_key = make_record_key(timeline_key, record.key)
 
         connection = self.cluster.get_local_client_for_key(timeline_key)
@@ -143,23 +143,19 @@ class RedisBackend(Backend):
                     functools.partial(make_schedule_key, self.namespace),
                     (WAITING_STATE, READY_STATE),
                 ),
-                (target, record.timestamp),
+                (key, record.timestamp),
                 pipeline,
             )
 
-            should_truncate = random.random() < self.trim_chance
+            should_truncate = random.random() < self.truncation_chance
             if should_truncate:
                 truncate_timeline((timeline_key,), (self.capacity,), pipeline)
 
             results = pipeline.execute()
             if should_truncate:
-                logger.info('Removed %s extra records from %s.', results[-1], target)
+                logger.info('Removed %s extra records from %s.', results[-1], key)
 
-    def schedule(self, cutoff, chunk=1000):
-        """
-        Moves timelines that are ready to be digested from the ``WAITING`` to
-        the ``READY`` state.
-        """
+    def schedule(self, deadline, chunk=1000):
         # TODO: This doesn't lead to a fair balancing of workers, ideally each
         # scheduling task would be executed by a different process for each
         # host.
@@ -169,7 +165,7 @@ class RedisBackend(Backend):
                 items = connection.zrangebyscore(
                     make_schedule_key(self.namespace, WAITING_STATE),
                     min=0,
-                    max=cutoff,
+                    max=deadline,
                     withscores=True,
                     start=0,
                     num=chunk,
@@ -197,7 +193,8 @@ class RedisBackend(Backend):
                         *itertools.chain.from_iterable([(timestamp, key) for (key, timestamp) in items])
                     )
 
-                    yield items
+                    for key, timestamp in items:
+                        yield ScheduleEntry(key, timestamp)
 
                     pipeline.execute()
 
@@ -206,18 +203,18 @@ class RedisBackend(Backend):
                 if len(items) < chunk:
                     break
 
-    def maintenance(self, timeout):
+    def maintenance(self, deadline):
         raise NotImplementedError
 
     @contextmanager
-    def digest(self, target):
-        timeline_key = make_timeline_key(self.namespace, target)
+    def digest(self, key):
+        timeline_key = make_timeline_key(self.namespace, key)
         digest_key = make_digest_key(timeline_key)
 
         # TODO: Need to wrap this whole section in a lease to try and avoid data races.
         connection = self.cluster.get_local_client_for_key(timeline_key)
 
-        if connection.zscore(make_schedule_key(self.namespace, READY_STATE), target) is None:
+        if connection.zscore(make_schedule_key(self.namespace, READY_STATE), key) is None:
             raise Exception('Cannot digest timeline, timeline is not in the ready state.')
 
         with connection.pipeline() as pipeline:
@@ -247,7 +244,7 @@ class RedisBackend(Backend):
         def get_iteration_count(default=0):
             value = connection.get(make_iteration_key(timeline_key))
             if not value:
-                logger.warning('Could not retrieve iteration counter for %s, defaulting to %s.', target, default)
+                logger.warning('Could not retrieve iteration counter for %s, defaulting to %s.', key, default)
                 return default
             return int(value)
 
@@ -255,10 +252,10 @@ class RedisBackend(Backend):
 
         def get_records_for_digest():
             with connection.pipeline(transaction=False) as pipeline:
-                for key, timestamp in records:
-                    pipeline.get(make_record_key(timeline_key, key))
+                for record_key, timestamp in records:
+                    pipeline.get(make_record_key(timeline_key, record_key))
 
-                for (key, timestamp), value in zip(records, pipeline.execute()):
+                for (record_key, timestamp), value in zip(records, pipeline.execute()):
                     # We have to handle failures if the key does not exist --
                     # this could happen due to evictions or race conditions
                     # where the record was added to a timeline while it was
@@ -266,7 +263,7 @@ class RedisBackend(Backend):
                     if value is None:
                         logger.warning('Could not retrieve event for timeline.')
                     else:
-                        yield Record(key, self.codec.decode(value), timestamp)
+                        yield Record(record_key, self.codec.decode(value), timestamp)
 
         yield itertools.islice(get_records_for_digest(), self.capacity)
 
@@ -275,10 +272,10 @@ class RedisBackend(Backend):
                 pipeline.watch(digest_key)  # This shouldn't be necessary, but better safe than sorry?
                 pipeline.multi()
 
-                record_keys = [make_record_key(timeline_key, key) for key, score in records]
+                record_keys = [make_record_key(timeline_key, record_key) for record_key, score in records]
                 pipeline.delete(digest_key, *record_keys)
-                pipeline.zrem(make_schedule_key(self.namespace, READY_STATE), target)
-                pipeline.zadd(make_schedule_key(self.namespace, WAITING_STATE), time.time() + self.backoff(iteration + 1), target)
+                pipeline.zrem(make_schedule_key(self.namespace, READY_STATE), key)
+                pipeline.zadd(make_schedule_key(self.namespace, WAITING_STATE), time.time() + self.backoff(iteration + 1), key)
                 pipeline.set(make_iteration_key(timeline_key), iteration + 1)
                 pipeline.execute()
 
@@ -290,8 +287,8 @@ class RedisBackend(Backend):
                 pipeline.multi()
                 if connection.zcard(timeline_key) is 0:
                     pipeline.delete(make_iteration_key(timeline_key))
-                    pipeline.zrem(make_schedule_key(self.namespace, READY_STATE), target)
-                    pipeline.zrem(make_schedule_key(self.namespace, WAITING_STATE), target)
+                    pipeline.zrem(make_schedule_key(self.namespace, READY_STATE), key)
+                    pipeline.zrem(make_schedule_key(self.namespace, WAITING_STATE), key)
                     pipeline.execute()
 
         # If there were records in the digest, we need to schedule it so that
diff --git a/src/sentry/timelines/base.py b/src/sentry/timelines/base.py
deleted file mode 100644
index 7ea5deb0cc..0000000000
--- a/src/sentry/timelines/base.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import logging
-from collections import namedtuple
-
-from sentry.utils.imports import import_string
-
-
-logger = logging.getLogger('sentry.timelines')
-
-
-Record = namedtuple('Record', 'key value timestamp')
-
-
-def load(options):
-    return import_string(options['path'])(**options.get('options', {}))
-
-
-class Backend(object):
-    def __init__(self, **options):
-        self.codec = load(options.pop('codec', {'path': 'sentry.timelines.codecs.CompressedPickleCodec'}))
-        self.backoff = load(options.pop('backoff', {'path': 'sentry.timelines.backoff.IntervalBackoffStrategy'}))
-
-        self.capacity = options.pop('capacity', None)
-        if self.capacity is not None and self.capacity < 1:
-            raise ValueError('Timeline capacity must be at least 1 if used.')
-
-        if self.capacity:
-            self.trim_chance = options.pop('trim_chance', 1.0 / self.capacity)
-        else:
-            self.trim_chance = None
-            if 'trim_chance' in options:
-                logger.warning('No timeline capacity has been set, ignoring "trim_chance" option.')
-                del options['trim_chance']
diff --git a/tests/sentry/timelines/__init__.py b/tests/sentry/timelines/__init__.py
deleted file mode 100644
index e69de29bb2..0000000000
diff --git a/tests/sentry/timelines/test_redis.py b/tests/sentry/timelines/test_redis.py
deleted file mode 100644
index 0ef1227d48..0000000000
--- a/tests/sentry/timelines/test_redis.py
+++ /dev/null
@@ -1,40 +0,0 @@
-import pytest
-
-from sentry.timelines.redis import (
-    Record,
-    RedisBackend
-)
-from sentry.testutils import TestCase
-
-
-class ExpectedError(Exception):
-    pass
-
-
-class RedisBackendTestCase(TestCase):
-    def setUp(self):
-        self.backend = RedisBackend(hosts={
-            0: {'db': 9},
-        })
-
-    def test_simple(self):
-        record = Record('key', 'value', 1)
-        self.backend.add('timeline', record)
-
-        with self.backend.digest('timeline') as records:
-            assert list(records) == [record]
-
-    def test_merge_on_failure(self):
-        first = Record('first', 'value', 1)
-        self.backend.add('timeline', first)
-
-        with pytest.raises(ExpectedError):
-            with self.backend.digest('timeline') as records:
-                assert list(records) == [first]
-                raise ExpectedError()
-
-        second = Record('second', 'value', 2)
-        self.backend.add('timeline', second)
-
-        with self.backend.digest('timeline') as records:
-            assert list(records) == [second, first]
