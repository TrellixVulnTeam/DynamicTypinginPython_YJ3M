commit 5c22673994a62f54a782d1c595852986ccc51ae9
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Fri Oct 18 14:10:28 2019 +0200

    fix(outcomes_consumer): Batch calls to buffer.incr (#15171)

diff --git a/src/sentry/buffer/redis.py b/src/sentry/buffer/redis.py
index 0b3ce28a49..4a6917cc06 100644
--- a/src/sentry/buffer/redis.py
+++ b/src/sentry/buffer/redis.py
@@ -2,6 +2,8 @@ from __future__ import absolute_import
 
 import six
 
+import contextlib
+import threading
 from time import time
 from binascii import crc32
 
@@ -19,6 +21,30 @@ from sentry.utils.hashlib import md5_text
 from sentry.utils.imports import import_string
 from sentry.utils.redis import get_cluster_from_options
 
+_local_buffers = None
+_local_buffers_lock = threading.Lock()
+
+
+@contextlib.contextmanager
+def batch_buffers_incr():
+    global _local_buffers
+
+    with _local_buffers_lock:
+        assert _local_buffers is None
+        _local_buffers = {}
+
+    try:
+        yield
+    finally:
+        from sentry.app import buffer
+
+        with _local_buffers_lock:
+            buffers_to_flush = _local_buffers
+            _local_buffers = None
+
+            for (filters, model), (columns, extra) in buffers_to_flush.items():
+                buffer.incr(model=model, columns=columns, filters=dict(filters), extra=extra)
+
 
 class PendingBuffer(object):
     def __init__(self, size):
@@ -155,6 +181,24 @@ class RedisBuffer(Buffer):
             - Perform a set (last write wins) on extra
         - Add hashmap key to pending flushes
         """
+
+        if _local_buffers is not None:
+            with _local_buffers_lock:
+                if _local_buffers is not None:
+                    frozen_filters = tuple(sorted(filters.items()))
+                    key = (frozen_filters, model)
+
+                    stored_columns, stored_extra = _local_buffers.get(key, ({}, None))
+
+                    for k, v in columns.items():
+                        stored_columns[k] = stored_columns.get(k, 0) + v
+
+                    if extra is not None:
+                        stored_extra = extra
+
+                    _local_buffers[key] = stored_columns, stored_extra
+                    return
+
         # TODO(dcramer): longer term we'd rather not have to serialize values
         # here (unless it's to JSON)
         key = self._make_key(model, filters)
diff --git a/src/sentry/ingest/outcomes_consumer.py b/src/sentry/ingest/outcomes_consumer.py
index 6035c85a99..5c81de73b0 100644
--- a/src/sentry/ingest/outcomes_consumer.py
+++ b/src/sentry/ingest/outcomes_consumer.py
@@ -36,6 +36,7 @@ from sentry.utils.kafka import create_batching_kafka_consumer
 from sentry.utils import json, metrics
 from sentry.utils.outcomes import Outcome
 from sentry.utils.dates import to_datetime
+from sentry.buffer.redis import batch_buffers_incr
 
 logger = logging.getLogger(__name__)
 
@@ -124,9 +125,12 @@ class OutcomesConsumerWorker(AbstractBatchWorker):
     def flush_batch(self, batch):
         batch.sort(key=lambda msg: msg.get("project_id", 0) or 0)
 
-        with BaseManager.local_cache():
-            for _ in self.pool.imap_unordered(_process_message_with_timer, batch, chunksize=100):
-                pass
+        with batch_buffers_incr():
+            with BaseManager.local_cache():
+                for _ in self.pool.imap_unordered(
+                    _process_message_with_timer, batch, chunksize=100
+                ):
+                    pass
 
     def shutdown(self):
         pass
