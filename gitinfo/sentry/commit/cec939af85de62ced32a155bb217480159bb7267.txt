commit cec939af85de62ced32a155bb217480159bb7267
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Mon Dec 9 11:25:57 2019 +0100

    ref: Implement attachments consumer with chunked attachments (#15929)
    
    This implements the attachments consumer with attachment chunking. It should fully work for events that are submitted with attachments from Relay.
    
    Individual/standalone attachments are unimplemented and will cause the consumer to crash.
    
    PRs in semaphore:
    
    * https://github.com/getsentry/semaphore/pull/347
    * https://github.com/getsentry/semaphore/pull/317

diff --git a/src/sentry/attachments/base.py b/src/sentry/attachments/base.py
index cee6b6b5be..152aaf50c0 100644
--- a/src/sentry/attachments/base.py
+++ b/src/sentry/attachments/base.py
@@ -1,21 +1,40 @@
 from __future__ import absolute_import
 
+from six import string_types
 import zlib
 
 from sentry.utils import metrics
+from sentry.utils.json import prune_empty_keys
+
+
+ATTACHMENT_META_KEY = u"{key}:a"
+ATTACHMENT_UNCHUNKED_DATA_KEY = u"{key}:a:{id}"
+ATTACHMENT_DATA_CHUNK_KEY = u"{key}:a:{id}:{chunk_index}"
 
 
 class CachedAttachment(object):
-    def __init__(self, name=None, content_type=None, type=None, data=None, load=None):
-        if data is None and load is None:
-            raise AttributeError("Missing attachment data")
+    def __init__(
+        self,
+        key=None,
+        id=None,
+        name=None,
+        content_type=None,
+        type=None,
+        data=None,
+        chunks=None,
+        cache=None,
+    ):
+        self.key = key
+        self.id = id
 
         self.name = name
         self.content_type = content_type
         self.type = type or "event.attachment"
+        assert isinstance(self.type, string_types), self.type
 
         self._data = data
-        self._load = load
+        self.chunks = chunks
+        self._cache = cache
 
     @classmethod
     def from_upload(cls, file, **kwargs):
@@ -25,60 +44,102 @@ class CachedAttachment(object):
 
     @property
     def data(self):
-        if self._data is None and self._load is not None:
-            self._data = self._load()
+        if self._data is None and self._cache is not None:
+            self._data = self._cache.get_data(self)
+            self._cache = None
 
         return self._data
 
+    @property
+    def chunk_keys(self):
+        assert self.key is not None
+        assert self.id is not None
+
+        if self.chunks is None:
+            yield ATTACHMENT_UNCHUNKED_DATA_KEY.format(key=self.key, id=self.id)
+            return
+
+        for chunk_index in range(self.chunks):
+            yield ATTACHMENT_DATA_CHUNK_KEY.format(
+                key=self.key, id=self.id, chunk_index=chunk_index
+            )
+
     def meta(self):
-        return {"name": self.name, "content_type": self.content_type, "type": self.type}
+        return prune_empty_keys(
+            {
+                "id": self.id,
+                "name": self.name,
+                "content_type": self.content_type,
+                "type": self.type,
+                "chunks": self.chunks,
+            }
+        )
 
 
 class BaseAttachmentCache(object):
-    def __init__(self, inner, appendix=None):
-        if appendix is None:
-            appendix = "a"
-        self.appendix = appendix
+    def __init__(self, inner):
         self.inner = inner
 
-    def make_key(self, key):
-        return u"{}:{}".format(key, self.appendix)
-
     def set(self, key, attachments, timeout=None):
-        key = self.make_key(key)
-        for index, attachment in enumerate(attachments):
-            compressed = zlib.compress(attachment.data)
-            self.inner.set(u"{}:{}".format(key, index), compressed, timeout, raw=True)
+        for id, attachment in enumerate(attachments):
+            if attachment.chunks is not None:
+                continue
+            # TODO(markus): We need to get away from sequential IDs, they
+            # are risking collision when using Relay.
+            if attachment.id is None:
+                attachment.id = id
 
-            metrics_tags = {"type": attachment.type}
-            metrics.incr("attachments.received", tags=metrics_tags, skip_internal=False)
-            metrics.timing("attachments.blob-size.raw", len(attachment.data), tags=metrics_tags)
-            metrics.timing("attachments.blob-size.compressed", len(compressed), tags=metrics_tags)
+            if attachment.key is None:
+                attachment.key = key
 
-        meta = [attachment.meta() for attachment in attachments]
-        self.inner.set(key, meta, timeout, raw=False)
+            metrics_tags = {"type": attachment.type}
+            self.set_unchunked_data(
+                key=key,
+                id=attachment.id,
+                data=attachment.data,
+                timeout=timeout,
+                metrics_tags=metrics_tags,
+            )
+
+        meta = []
+
+        for attachment in attachments:
+            attachment._cache = self
+            meta.append(attachment.meta())
+
+        self.inner.set(ATTACHMENT_META_KEY.format(key=key), meta, timeout, raw=False)
+
+    def set_chunk(self, key, id, chunk_index, chunk_data, timeout=None):
+        key = ATTACHMENT_DATA_CHUNK_KEY.format(key=key, id=id, chunk_index=chunk_index)
+        self.inner.set(key, zlib.compress(chunk_data), timeout, raw=True)
+
+    def set_unchunked_data(self, key, id, data, timeout=None, metrics_tags=None):
+        key = ATTACHMENT_UNCHUNKED_DATA_KEY.format(key=key, id=id)
+        compressed = zlib.compress(data)
+        metrics.timing("attachments.blob-size.raw", len(data), tags=metrics_tags)
+        metrics.timing("attachments.blob-size.compressed", len(compressed), tags=metrics_tags)
+        metrics.incr("attachments.received", tags=metrics_tags, skip_internal=False)
+        self.inner.set(key, compressed, timeout, raw=True)
 
     def get(self, key):
-        key = self.make_key(key)
-        result = self.inner.get(key, raw=False)
-        if result is not None:
-            result = [
-                CachedAttachment(
-                    load=lambda index=index: zlib.decompress(
-                        self.inner.get(u"{}:{}".format(key, index), raw=True)
-                    ),
-                    **attachment
-                )
-                for index, attachment in enumerate(result)
-            ]
-        return result
+        result = self.inner.get(ATTACHMENT_META_KEY.format(key=key), raw=False)
+
+        for id, attachment in enumerate(result or ()):
+            attachment.setdefault("id", id)
+            attachment.setdefault("key", key)
+            yield CachedAttachment(cache=self, **attachment)
+
+    def get_data(self, attachment):
+        data = []
+
+        for key in attachment.chunk_keys:
+            data.append(zlib.decompress(self.inner.get(key, raw=True)))
+
+        return b"".join(data)
 
     def delete(self, key):
-        key = self.make_key(key)
-        attachments = self.inner.get(key, raw=False)
-        if attachments is None:
-            return
+        for attachment in self.get(key):
+            for k in attachment.chunk_keys:
+                self.inner.delete(k)
 
-        for index in range(0, len(attachments)):
-            self.inner.delete(u"{}:{}".format(key, index))
-        self.inner.delete(key)
+        self.inner.delete(ATTACHMENT_META_KEY.format(key=key))
diff --git a/src/sentry/attachments/redis.py b/src/sentry/attachments/redis.py
index 4982a6de02..80172e254e 100644
--- a/src/sentry/attachments/redis.py
+++ b/src/sentry/attachments/redis.py
@@ -12,19 +12,15 @@ logger = logging.getLogger(__name__)
 
 class RedisClusterAttachmentCache(BaseAttachmentCache):
     def __init__(self, **options):
-        appendix = options.pop("appendix", None)
         cluster_id = options.pop("cluster_id", None)
         if cluster_id is None:
             cluster_id = getattr(settings, "SENTRY_ATTACHMENTS_REDIS_CLUSTER", "rc-short")
-        BaseAttachmentCache.__init__(
-            self, inner=RedisClusterCache(cluster_id, **options), appendix=appendix
-        )
+        BaseAttachmentCache.__init__(self, inner=RedisClusterCache(cluster_id, **options))
 
 
 class RbAttachmentCache(BaseAttachmentCache):
     def __init__(self, **options):
-        appendix = options.pop("appendix", None)
-        BaseAttachmentCache.__init__(self, inner=RbCache(**options), appendix=appendix)
+        BaseAttachmentCache.__init__(self, inner=RbCache(**options))
 
 
 # Confusing legacy name for RediscClusterCache
diff --git a/src/sentry/ingest/ingest_consumer.py b/src/sentry/ingest/ingest_consumer.py
index 737ca34ed2..f20cda9949 100644
--- a/src/sentry/ingest/ingest_consumer.py
+++ b/src/sentry/ingest/ingest_consumer.py
@@ -3,7 +3,6 @@ from __future__ import absolute_import
 import logging
 import msgpack
 
-from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
 
 from django.conf import settings
 from django.core.cache import cache
@@ -15,10 +14,15 @@ from sentry.tasks.store import preprocess_event
 from sentry.utils import json
 from sentry.utils.cache import cache_key_for_event
 from sentry.utils.kafka import create_batching_kafka_consumer
+from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
+from sentry.attachments import CachedAttachment, attachment_cache
 
 logger = logging.getLogger(__name__)
 
 
+CACHE_TIMEOUT = 3600
+
+
 class ConsumerType(object):
     """
     Defines the types of ingestion consumers
@@ -42,58 +46,16 @@ class ConsumerType(object):
 class IngestConsumerWorker(AbstractBatchWorker):
     def process_message(self, message):
         message = msgpack.unpackb(message.value(), use_list=False)
-        payload = message["payload"]
-        start_time = float(message["start_time"])
-        event_id = message["event_id"]
-        project_id = message["project_id"]
-        remote_addr = message.get("remote_addr")
-
-        # check that we haven't already processed this event (a previous instance of the forwarder
-        # died before it could commit the event queue offset)
-        deduplication_key = "ev:{}:{}".format(project_id, event_id)
-        if cache.get(deduplication_key) is not None:
-            logger.warning(
-                "pre-process-forwarder detected a duplicated event" " with id:%s for project:%s.",
-                event_id,
-                project_id,
-            )
-            return True  # message already processed do not reprocess
-
-        try:
-            project = Project.objects.get_from_cache(id=project_id)
-        except Project.DoesNotExist:
-            logger.error("Project for ingested event does not exist: %s", project_id)
-            return True
-
-        # Parse the JSON payload. This is required to compute the cache key and
-        # call process_event. The payload will be put into Kafka raw, to avoid
-        # serializing it again.
-        # XXX: Do not use CanonicalKeyDict here. This may break preprocess_event
-        # which assumes that data passed in is a raw dictionary.
-        data = json.loads(payload)
-
-        cache_timeout = 3600
-        cache_key = cache_key_for_event(data)
-        default_cache.set(cache_key, data, cache_timeout)
-
-        # Preprocess this event, which spawns either process_event or
-        # save_event. Pass data explicitly to avoid fetching it again from the
-        # cache.
-        preprocess_event(
-            cache_key=cache_key,
-            data=data,
-            start_time=start_time,
-            event_id=event_id,
-            project=project,
-        )
+        message_type = message["type"]
 
-        # remember for an 1 hour that we saved this event (deduplication protection)
-        cache.set(deduplication_key, "", 3600)
-
-        # emit event_accepted once everything is done
-        event_accepted.send_robust(
-            ip=remote_addr, data=data, project=project, sender=self.process_message
-        )
+        if message_type in ("event", "transaction"):
+            process_event(message)
+        elif message_type == "attachment_chunk":
+            process_attachment_chunk(message)
+        elif message_type == "attachment":
+            process_individual_attachment(message)
+        else:
+            raise ValueError("Unknown message type: {}".format(message_type))
 
         # Return *something* so that it counts against batch size
         return True
@@ -105,6 +67,79 @@ class IngestConsumerWorker(AbstractBatchWorker):
         pass
 
 
+def process_event(message):
+    payload = message["payload"]
+    start_time = float(message["start_time"])
+    event_id = message["event_id"]
+    project_id = message["project_id"]
+    remote_addr = message.get("remote_addr")
+    attachments = message.get("attachments") or ()
+
+    # check that we haven't already processed this event (a previous instance of the forwarder
+    # died before it could commit the event queue offset)
+    deduplication_key = "ev:{}:{}".format(project_id, event_id)
+    if cache.get(deduplication_key) is not None:
+        logger.warning(
+            "pre-process-forwarder detected a duplicated event" " with id:%s for project:%s.",
+            event_id,
+            project_id,
+        )
+        return  # message already processed do not reprocess
+
+    try:
+        project = Project.objects.get_from_cache(id=project_id)
+    except Project.DoesNotExist:
+        logger.error("Project for ingested event does not exist: %s", project_id)
+        return
+
+    # Parse the JSON payload. This is required to compute the cache key and
+    # call process_event. The payload will be put into Kafka raw, to avoid
+    # serializing it again.
+    # XXX: Do not use CanonicalKeyDict here. This may break preprocess_event
+    # which assumes that data passed in is a raw dictionary.
+    data = json.loads(payload)
+
+    cache_key = cache_key_for_event(data)
+    default_cache.set(cache_key, data, CACHE_TIMEOUT)
+
+    if attachments:
+        attachment_objects = [
+            CachedAttachment(type=attachment.pop("attachment_type"), **attachment)
+            for attachment in attachments
+        ]
+
+        attachment_cache.set(cache_key, attachments=attachment_objects, timeout=CACHE_TIMEOUT)
+
+    # Preprocess this event, which spawns either process_event or
+    # save_event. Pass data explicitly to avoid fetching it again from the
+    # cache.
+    preprocess_event(
+        cache_key=cache_key, data=data, start_time=start_time, event_id=event_id, project=project
+    )
+
+    # remember for an 1 hour that we saved this event (deduplication protection)
+    cache.set(deduplication_key, "", CACHE_TIMEOUT)
+
+    # emit event_accepted once everything is done
+    event_accepted.send_robust(ip=remote_addr, data=data, project=project, sender=process_event)
+
+
+def process_attachment_chunk(message):
+    payload = message["payload"]
+    event_id = message["event_id"]
+    project_id = message["project_id"]
+    id = message["id"]
+    chunk_index = message["chunk_index"]
+    cache_key = cache_key_for_event({"event_id": event_id, "project": project_id})
+    attachment_cache.set_chunk(
+        key=cache_key, id=id, chunk_index=chunk_index, chunk_data=payload, timeout=CACHE_TIMEOUT
+    )
+
+
+def process_individual_attachment(message):
+    raise RuntimeError("Not implemented yet")
+
+
 def get_ingest_consumer(consumer_type, once=False, **options):
     """
     Handles events coming via a kafka queue.
diff --git a/src/sentry/lang/native/utils.py b/src/sentry/lang/native/utils.py
index 1b57321914..7447054591 100644
--- a/src/sentry/lang/native/utils.py
+++ b/src/sentry/lang/native/utils.py
@@ -111,7 +111,7 @@ def signal_from_data(data):
 
 def get_event_attachment(data, attachment_type):
     cache_key = cache_key_for_event(data)
-    attachments = attachment_cache.get(cache_key) or []
+    attachments = attachment_cache.get(cache_key)
     return next((a for a in attachments if a.type == attachment_type), None)
 
 
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index d5da5ec431..68fdc59ffd 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -474,7 +474,7 @@ def save_attachments(cache_key, event):
     if not features.has("organizations:event-attachments", event.project.organization, actor=None):
         return
 
-    attachments = attachment_cache.get(cache_key) or []
+    attachments = list(attachment_cache.get(cache_key))
     if not attachments:
         return
 
diff --git a/src/sentry/utils/pytest/fixtures.py b/src/sentry/utils/pytest/fixtures.py
index 6e63df9bcc..de84138373 100644
--- a/src/sentry/utils/pytest/fixtures.py
+++ b/src/sentry/utils/pytest/fixtures.py
@@ -145,12 +145,6 @@ def factories():
     return Factories
 
 
-@pytest.mark.django_db
-@pytest.fixture
-def project(team, factories):
-    return factories.create_project(name="bar", slug="bar", teams=[team])
-
-
 @pytest.fixture
 def task_runner():
     from sentry.testutils.helpers.task_runner import TaskRunner
diff --git a/tests/sentry/attachments/test_base.py b/tests/sentry/attachments/test_base.py
new file mode 100644
index 0000000000..c3f5146abe
--- /dev/null
+++ b/tests/sentry/attachments/test_base.py
@@ -0,0 +1,67 @@
+from __future__ import absolute_import
+
+import copy
+
+from sentry.attachments.base import CachedAttachment, BaseAttachmentCache
+
+
+class InMemoryCache(object):
+    """
+    In-memory mock cache that roughly works like Django cache. Extended with
+    internal assertions to ensure correct use of `raw`.
+    """
+
+    def __init__(self):
+        self.data = {}
+        #: Used to check for consistent usage of `raw` param
+        self.raw_map = {}
+
+    def get(self, key, raw=False):
+        assert key not in self.raw_map or raw == self.raw_map[key]
+        return copy.deepcopy(self.data.get(key))
+
+    def set(self, key, value, timeout=None, raw=False):
+        # Attachment chunks MUST be bytestrings. Josh please don't change this
+        # to unicode.
+        assert isinstance(value, bytes) or not raw
+        assert key not in self.raw_map or raw == self.raw_map[key]
+        self.data[key] = value
+
+    def delete(self, key):
+        del self.data[key]
+
+
+def test_basic_chunked():
+    data = InMemoryCache()
+    cache = BaseAttachmentCache(data)
+
+    cache.set_chunk("c:foo", 123, 0, b"Hello World! ")
+    cache.set_chunk("c:foo", 123, 1, b"")
+    cache.set_chunk("c:foo", 123, 2, b"Bye.")
+
+    att = CachedAttachment(key="c:foo", id=123, name="lol.txt", content_type="text/plain", chunks=3)
+    cache.set("c:foo", [att])
+
+    att2, = cache.get("c:foo")
+    assert att2.key == att.key == "c:foo"
+    assert att2.id == att.id == 123
+    assert att2.data == att.data == b"Hello World! Bye."
+
+    cache.delete("c:foo")
+    assert not list(cache.get("c:foo"))
+
+
+def test_basic_unchunked():
+    data = InMemoryCache()
+    cache = BaseAttachmentCache(data)
+
+    att = CachedAttachment(name="lol.txt", content_type="text/plain", data=b"Hello World! Bye.")
+    cache.set("c:foo", [att])
+
+    att2, = cache.get("c:foo")
+    assert att2.key == att.key == "c:foo"
+    assert att2.id == att.id == 0
+    assert att2.data == att.data == b"Hello World! Bye."
+
+    cache.delete("c:foo")
+    assert not list(cache.get("c:foo"))
diff --git a/tests/sentry/attachments/test_redis.py b/tests/sentry/attachments/test_redis.py
index ef0764f146..a6891258d3 100644
--- a/tests/sentry/attachments/test_redis.py
+++ b/tests/sentry/attachments/test_redis.py
@@ -4,62 +4,82 @@ from __future__ import absolute_import
 
 import mock
 import zlib
+import pytest
 
 from sentry.cache.redis import RedisClusterCache, RbCache
-from sentry.testutils import TestCase
 from sentry.utils.imports import import_string
 
 
 class FakeClient(object):
+    def __init__(self):
+        self.data = {}
+
     def get(self, key):
-        if key == "c:1:foo:a":
-            return '[{"name":"foo.txt","content_type":"text/plain"}]'
-        elif key == "c:1:foo:a:0":
-            return zlib.compress(b"Hello World!")
-
-
-class RbCluster(object):
-    def get_routing_client(self):
-        return CLIENT
-
-
-CLIENT = FakeClient()
-RB_CLUSTER = RbCluster()
-
-
-class RedisClusterAttachmentTest(TestCase):
-    @mock.patch("sentry.utils.redis.redis_clusters.get", return_value=CLIENT)
-    def test_process_pending_one_batch(self, cluster_get):
-        attachment_cache = import_string("sentry.attachments.redis.RedisClusterAttachmentCache")()
-        cluster_get.assert_any_call("rc-short")
-        assert isinstance(attachment_cache.inner, RedisClusterCache)
-        assert attachment_cache.inner.client is CLIENT
-
-        rv = attachment_cache.get("foo")
-        assert len(rv) == 1
-        attachment = rv[0]
-        assert attachment.meta() == {
-            "type": "event.attachment",
-            "name": "foo.txt",
-            "content_type": "text/plain",
-        }
-        assert attachment.data == b"Hello World!"
-
-
-class RbAttachmentTest(TestCase):
-    @mock.patch("sentry.cache.redis.get_cluster_from_options", return_value=(RB_CLUSTER, {}))
-    def test_process_pending_one_batch(self, cluster_get):
-        attachment_cache = import_string("sentry.attachments.redis.RbAttachmentCache")(hosts=[])
-        cluster_get.assert_any_call("SENTRY_CACHE_OPTIONS", {"hosts": []})
-        assert isinstance(attachment_cache.inner, RbCache)
-        assert attachment_cache.inner.client is CLIENT
-
-        rv = attachment_cache.get("foo")
-        assert len(rv) == 1
-        attachment = rv[0]
-        assert attachment.meta() == {
-            "type": "event.attachment",
-            "name": "foo.txt",
-            "content_type": "text/plain",
-        }
-        assert attachment.data == b"Hello World!"
+        return self.data[key]
+
+
+@pytest.fixture
+def mock_client():
+    return FakeClient()
+
+
+@pytest.fixture(params=["rb", "rediscluster"])
+def mocked_attachment_cache(request, mock_client):
+    class RbCluster(object):
+        def get_routing_client(self):
+            return mock_client
+
+    if request.param == "rb":
+        with mock.patch(
+            "sentry.cache.redis.get_cluster_from_options", return_value=(RbCluster(), {})
+        ) as cluster_get:
+            attachment_cache = import_string("sentry.attachments.redis.RbAttachmentCache")(hosts=[])
+            cluster_get.assert_any_call("SENTRY_CACHE_OPTIONS", {"hosts": []})
+            assert isinstance(attachment_cache.inner, RbCache)
+
+    elif request.param == "rediscluster":
+        with mock.patch(
+            "sentry.utils.redis.redis_clusters.get", return_value=mock_client
+        ) as cluster_get:
+            attachment_cache = import_string(
+                "sentry.attachments.redis.RedisClusterAttachmentCache"
+            )()
+            cluster_get.assert_any_call("rc-short")
+            assert isinstance(attachment_cache.inner, RedisClusterCache)
+
+    else:
+        assert False
+
+    assert attachment_cache.inner.client is mock_client
+    yield attachment_cache
+
+
+def test_process_pending_one_batch(mocked_attachment_cache, mock_client):
+    mock_client.data["c:1:foo:a"] = '[{"name":"foo.txt","content_type":"text/plain"}]'
+    mock_client.data["c:1:foo:a:0"] = zlib.compress(b"Hello World!")
+
+    attachment, = mocked_attachment_cache.get("foo")
+    assert attachment.meta() == {
+        "id": 0,
+        "type": "event.attachment",
+        "name": "foo.txt",
+        "content_type": "text/plain",
+    }
+    assert attachment.data == b"Hello World!"
+
+
+def test_chunked(mocked_attachment_cache, mock_client):
+    mock_client.data["c:1:foo:a"] = '[{"name":"foo.txt","content_type":"text/plain","chunks":3}]'
+    mock_client.data["c:1:foo:a:0:0"] = zlib.compress(b"Hello World!")
+    mock_client.data["c:1:foo:a:0:1"] = zlib.compress(b" This attachment is ")
+    mock_client.data["c:1:foo:a:0:2"] = zlib.compress(b"chunked up.")
+
+    attachment, = mocked_attachment_cache.get("foo")
+    assert attachment.meta() == {
+        "id": 0,
+        "chunks": 3,
+        "type": "event.attachment",
+        "name": "foo.txt",
+        "content_type": "text/plain",
+    }
+    assert attachment.data == b"Hello World! This attachment is chunked up."
diff --git a/tests/sentry/ingest/test_ingest_consumer.py b/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
similarity index 99%
rename from tests/sentry/ingest/test_ingest_consumer.py
rename to tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
index fad53bb758..06ffb9cd72 100644
--- a/tests/sentry/ingest/test_ingest_consumer.py
+++ b/tests/sentry/ingest/ingest_consumer/test_ingest_kafka.py
@@ -40,7 +40,7 @@ def _get_test_message(project):
     em.normalize()
     normalized_event = dict(em.get_data())
     message = {
-        "ty": (0, ()),
+        "type": "event",
         "start_time": time.time(),
         "event_id": event_id,
         "project_id": int(project_id),
diff --git a/tests/sentry/ingest/ingest_consumer/test_ingest_processing.py b/tests/sentry/ingest/ingest_consumer/test_ingest_processing.py
new file mode 100644
index 0000000000..3d84980ebc
--- /dev/null
+++ b/tests/sentry/ingest/ingest_consumer/test_ingest_processing.py
@@ -0,0 +1,117 @@
+from __future__ import absolute_import
+
+import pytest
+import time
+
+from sentry.utils import json
+from sentry.ingest.ingest_consumer import process_event, process_attachment_chunk
+from sentry.attachments import attachment_cache
+from sentry.event_manager import EventManager
+
+
+def get_normalized_event(data, project):
+    mgr = EventManager(data, project=project)
+    mgr.normalize()
+    return dict(mgr.get_data())
+
+
+@pytest.fixture
+def preprocess_event(monkeypatch):
+    calls = []
+
+    def inner(**kwargs):
+        calls.append(kwargs)
+
+    monkeypatch.setattr("sentry.ingest.ingest_consumer.preprocess_event", inner)
+    return calls
+
+
+@pytest.mark.django_db
+def test_deduplication_works(default_project, task_runner, monkeypatch, preprocess_event):
+    payload = get_normalized_event({"message": "hello world"}, default_project)
+    event_id = payload["event_id"]
+    project_id = default_project.id
+    start_time = time.time() - 3600
+
+    for _ in range(2):
+        process_event(
+            {
+                "payload": json.dumps(payload),
+                "start_time": start_time,
+                "event_id": event_id,
+                "project_id": project_id,
+                "remote_addr": "127.0.0.1",
+            }
+        )
+
+    kwargs, = preprocess_event
+    assert kwargs == {
+        "cache_key": u"e:{event_id}:{project_id}".format(event_id=event_id, project_id=project_id),
+        "data": payload,
+        "event_id": event_id,
+        "project": default_project,
+        "start_time": start_time,
+    }
+
+
+@pytest.mark.django_db
+def test_with_attachments(default_project, task_runner, monkeypatch, preprocess_event):
+    payload = get_normalized_event({"message": "hello world"}, default_project)
+    event_id = payload["event_id"]
+    project_id = default_project.id
+    start_time = time.time() - 3600
+
+    process_attachment_chunk(
+        {
+            "payload": b"Hello ",
+            "event_id": event_id,
+            "project_id": project_id,
+            "id": 0,
+            "chunk_index": 0,
+        }
+    )
+
+    process_attachment_chunk(
+        {
+            "payload": b"World!",
+            "event_id": event_id,
+            "project_id": project_id,
+            "id": 0,
+            "chunk_index": 1,
+        }
+    )
+
+    process_event(
+        {
+            "payload": json.dumps(payload),
+            "start_time": start_time,
+            "event_id": event_id,
+            "project_id": project_id,
+            "remote_addr": "127.0.0.1",
+            "attachments": [
+                {
+                    "id": 0,
+                    "name": "lol.txt",
+                    "content_type": "text/plain",
+                    "attachment_type": "custom.attachment",
+                    "chunks": 2,
+                }
+            ],
+        }
+    )
+
+    kwargs, = preprocess_event
+    cache_key = u"e:{event_id}:{project_id}".format(event_id=event_id, project_id=project_id)
+    assert kwargs == {
+        "cache_key": cache_key,
+        "data": payload,
+        "event_id": event_id,
+        "project": default_project,
+        "start_time": start_time,
+    }
+
+    att, = attachment_cache.get(cache_key)
+    assert att.data == b"Hello World!"
+    assert att.name == "lol.txt"
+    assert att.content_type == "text/plain"
+    assert att.type == "custom.attachment"
diff --git a/tests/sentry/ingest/test_outcome_consumer.py b/tests/sentry/ingest/outcome_consumer/test_outcomes_kafka.py
similarity index 100%
rename from tests/sentry/ingest/test_outcome_consumer.py
rename to tests/sentry/ingest/outcome_consumer/test_outcomes_kafka.py
