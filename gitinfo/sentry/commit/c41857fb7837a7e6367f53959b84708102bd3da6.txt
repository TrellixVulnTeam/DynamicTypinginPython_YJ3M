commit c41857fb7837a7e6367f53959b84708102bd3da6
Author: David Cramer <dcramer@gmail.com>
Date:   Sun Jan 29 00:10:16 2012 -0800

    Additions to tokenization (dont tokenize short words, lowercase everything, and change empty string key to text key.

diff --git a/sentry/interfaces.py b/sentry/interfaces.py
index 8b2ebce75a..fe81eee04c 100644
--- a/sentry/interfaces.py
+++ b/sentry/interfaces.py
@@ -85,9 +85,18 @@ class Interface(object):
         return ''
 
     def get_search_context(self, event):
+        """
+        Returns a dictionary describing the data that should be indexed
+        by the search engine. Several fields are accepted:
+
+        - text: a list of text items to index as part of the generic query
+        - filters: a map of fields which are used for precise matching
+        """
         return {
-            # '': ['...'],
-            # 'field_name': ['...'],
+            # 'text': ['...'],
+            # 'filters': {
+            #     'field": ['...'],
+            # },
         }
 
 
@@ -104,7 +113,7 @@ class Message(Interface):
 
     def get_search_context(self, event):
         return {
-            '': [self.message] + self.params,
+            'text': [self.message] + self.params,
         }
 
 
@@ -121,7 +130,7 @@ class Query(Interface):
 
     def get_search_context(self, event):
         return {
-            '': [self.query],
+            'text': [self.query],
         }
 
 
@@ -189,7 +198,7 @@ class Stacktrace(Interface):
 
     def get_search_context(self, event):
         return {
-            '': list(itertools.chain(*[[f['filename'], f['function'], f['context_line']] for f in self.frames])),
+            'text': list(itertools.chain(*[[f['filename'], f['function'], f['context_line']] for f in self.frames])),
         }
 
 
@@ -219,7 +228,7 @@ class Exception(Interface):
 
     def get_search_context(self, event):
         return {
-            '': [self.value, self.type, self.module]
+            'text': [self.value, self.type, self.module]
         }
 
 
@@ -314,7 +323,9 @@ class Http(Interface):
 
     def get_search_context(self, event):
         return {
-            'url': [self.url],
+            'filters': {
+                'url': [self.url],
+            }
         }
 
 
@@ -362,7 +373,7 @@ class Template(Interface):
 
     def get_search_context(self, event):
         return {
-            '': [self.abs_path, self.filename, self.context_line, self.pre_context, self.post_context],
+            'text': [self.abs_path, self.filename, self.context_line, self.pre_context, self.post_context],
         }
 
 
diff --git a/sentry/manager.py b/sentry/manager.py
index dd7fcf5791..4d193362cf 100644
--- a/sentry/manager.py
+++ b/sentry/manager.py
@@ -595,7 +595,7 @@ class InstanceMetaManager(models.Manager):
 
 class SearchDocumentManager(models.Manager):
     def _tokenize(self, text):
-        return text.split(' ')
+        return [t for t in text.split(' ') if len(t) < 3]
 
     def search(self, query):
         tokens = self._tokenize(query)
@@ -633,24 +633,25 @@ class SearchDocumentManager(models.Manager):
             for k, v in interface.get_search_context(event).iteritems():
                 context[k].extend(v)
 
-        context[''].extend([event.message, event.logger, event.server_name])
+        context['text'].extend([event.message, event.logger, event.server_name])
 
         token_counts = defaultdict(lambda: defaultdict(int))
         for field, values in context.iteritems():
-            if field == '':
+            field = field.lower()
+            if field == 'text':
                 # we only tokenize the base text field
                 values = itertools.chain(*[self._tokenize(v) for v in values])
             for value in values:
                 if not value:
                     continue
-                token_counts[field][value] += 1
+                token_counts[field][value.lower()] += 1
 
         # TODO: might be worthwhile to make this update then create
         for field, tokens in token_counts.iteritems():
             for token, count in tokens.iteritems():
                 token, created = document.token_set.get_or_create(
                     field=field,
-                    token=token.lower(),
+                    token=token,
                     defaults={
                         'times_seen': count,
                     }
