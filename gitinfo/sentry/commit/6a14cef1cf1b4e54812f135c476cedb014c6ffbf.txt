commit 6a14cef1cf1b4e54812f135c476cedb014c6ffbf
Author: ted kaemming <ted@kaemming.com>
Date:   Fri Sep 8 13:00:42 2017 -0700

    ref(similarity): improve indexing performance and result quality

diff --git a/src/sentry/api/endpoints/group_similar_issues.py b/src/sentry/api/endpoints/group_similar_issues.py
index fd65021ac3..b6ba2de80a 100644
--- a/src/sentry/api/endpoints/group_similar_issues.py
+++ b/src/sentry/api/endpoints/group_similar_issues.py
@@ -13,10 +13,13 @@ from sentry.utils.functional import apply_values
 
 class GroupSimilarIssuesEndpoint(GroupEndpoint):
     def get(self, request, group):
-        # TODO(tkaemming): This should have a limit somewhere.
+        limit = request.GET.get('limit', None)
+        if limit is not None:
+            limit = int(limit) + 1  # the target group will always be included
+
         results = filter(
             lambda (group_id, scores): group_id != group.id,
-            features.compare(group)
+            features.compare(group, limit=limit)
         )
 
         serialized_groups = apply_values(
diff --git a/src/sentry/scripts/similarity/index.lua b/src/sentry/scripts/similarity/index.lua
index 2161a61b2f..0ec1330373 100644
--- a/src/sentry/scripts/similarity/index.lua
+++ b/src/sentry/scripts/similarity/index.lua
@@ -30,28 +30,56 @@ if not pcall(redis.replicate_commands) then
     redis.log(redis.LOG_DEBUG, 'Could not enable script effects replication.')
 end
 
-local function identity(value)
-    return value
+
+-- Utilities
+
+local function identity(...)
+    return ...
 end
 
-local function range(start, stop)
-    local result = {}
-    for i = start, stop do
-        table.insert(result, i)
+local function sum(t)
+    return table.ireduce(
+        t,
+        function (total, value)
+            return total + value
+        end,
+        0
+    )
+end
+
+local function avg(t)
+    return sum(t) / #t
+end
+
+function table.count(t)
+    -- Shitty O(N) table size implementation
+    local n = 0
+    for k in pairs(t) do
+        n = n + 1
     end
-    return result
+    return n
 end
 
-function table.ifilter(t, f)
+function table.slice(t, start, stop)
+    -- NOTE: ``stop`` is inclusive!
     local result = {}
-    for i, value in ipairs(t) do
-        if f(value) then
-            table.insert(result, value)
-        end
+    for i = start or 1, stop or #t do
+        result[i - start + 1] = t[i]
     end
     return result
 end
 
+function table.get_or_set_default(t, k, f)
+    local v = t[k]
+    if v ~= nil then
+        return v
+    else
+        v = f(k)
+        t[k] = v
+        return v
+    end
+end
+
 function table.imap(t, f)
     local result = {}
     for i, value in ipairs(t) do
@@ -59,7 +87,6 @@ function table.imap(t, f)
     end
     return result
 end
-
 function table.ireduce(t, f, i)
     if i == nil then
         i = {}
@@ -92,127 +119,214 @@ function table.izip(...)
     return result
 end
 
-function table.slice(t, start, stop)
-    -- NOTE: ``stop`` is inclusive!
-    local result = {}
-    for i = start or 1, stop or #t do
-        table.insert(result, t[i])
-    end
-    return result
-end
-
 
 -- Argument Parsing and Validation
 
-local function parse_number(value)
+local function validate_value(value)
+    assert(value ~= nil, 'got nil, expected value')
+    return value
+end
+
+local function validate_number(value)
     local result = tonumber(value)
-    assert(result ~= nil, 'got nil, expected number')
+    assert(result ~= nil, string.format('got nil (%q), expected number', value))
     return result
 end
 
-local function parse_integer(value)
-    local result = parse_number(value)
-    assert(result % 1 == 0, 'got float, expected integer')
+local function validate_integer(value)
+    local result = validate_number(value)
+    assert(result % 1 == 0, string.format('got float (%q), expected integer', value))
     return result
 end
 
-local function build_argument_parser(fields)
-    return function (arguments, offset)
-        if offset == nil then
-            offset = 0
+local function argument_parser(callback)
+    if callback == nil then
+        callback = identity
+    end
+
+    return function (cursor, arguments)
+        return cursor + 1, callback(arguments[cursor])
+    end
+end
+
+local function flag_argument_parser(flags)
+    return function (cursor, arguments)
+        local result = {}
+        while flags[arguments[cursor]] do
+            result[arguments[cursor]] = true
+            cursor = cursor + 1
+        end
+        return cursor, result
+    end
+end
+
+local function repeated_argument_parser(argument_parser, quantity_parser, callback)
+    if quantity_parser == nil then
+        quantity_parser = function (cursor, arguments)
+            return cursor + 1, validate_integer(arguments[cursor])
         end
+    end
+
+    if callback == nil then
+        callback = identity
+    end
+
+    return function (cursor, arguments)
         local results = {}
-        for i = 1, #fields do
-            local name, parser = unpack(fields[i])
-            local value = arguments[i]
-            local ok, result = pcall(parser, value)
-            if not ok then
-                error(string.format('received invalid argument for %q in position %s with value %q; %s', name, offset + i, value, result))
-            else
-                results[name] = result
-            end
+        local cursor, count = quantity_parser(cursor, arguments)
+        for i = 1, count do
+            cursor, results[i] = argument_parser(cursor, arguments)
         end
-        return results, table.slice(arguments, #fields + 1)
+        return cursor, callback(results)
     end
 end
 
-local function build_variadic_argument_parser(fields, validator)
-    if validator == nil then
-        validator = identity
+local function object_argument_parser(schema, callback)
+    if callback == nil then
+        callback = identity
     end
-    local parser = build_argument_parser(fields)
-    return function (arguments, offset)
-        if offset == nil then
-            offset = 0
+
+    return function (cursor, arguments)
+        local result = {}
+        for i, specification in ipairs(schema) do
+            local key, parser = unpack(specification)
+            cursor, result[key] = parser(cursor, arguments)
         end
-        if #arguments % #fields ~= 0 then
-            -- TODO: make this error less crummy
-            error('invalid number of arguments')
+        return cursor, callback(result)
+    end
+end
+
+local function variadic_argument_parser(argument_parser)
+    return function (cursor, arguments)
+        local results = {}
+        local i = 1
+        while arguments[cursor] ~= nil do
+            cursor, results[i] = argument_parser(cursor, arguments)
+            i = i + 1
         end
+        return cursor, results
+    end
+end
+
+local function multiple_argument_parser(...)
+    local parsers = {...}
+    return function (cursor, arguments)
         local results = {}
-        for i = 1, #arguments, #fields do
-            local value, _ = parser(table.slice(arguments, i, i + #fields - 1), i)
-            table.insert(
-                results,
-                validator(value)
-            )
+        for i, parser in ipairs(parsers) do
+            cursor, results[i] = parser(cursor, arguments)
         end
-        return results
+        return cursor, unpack(results)
     end
 end
 
+local function frequencies_argument_parser(configuration)
+    return repeated_argument_parser(
+        function (cursor, arguments)
+            local buckets = {}
+            return repeated_argument_parser(
+                function (cursor, arguments)
+                    buckets[validate_value(arguments[cursor])] = validate_integer(arguments[cursor + 1])
+                    return cursor + 2
+                end
+            )(cursor, arguments), buckets
+        end,
+        function (cursor, arguments)
+            return cursor, configuration.bands
+        end
+    )
+end
+
 
--- Time Series
+-- Time Series Set
 
-local function get_active_indices(interval, retention, timestamp)
-    local result = {}
-    local upper = math.floor(timestamp / interval)
-    for i = upper - retention, upper do
-        table.insert(result, i)
-    end
-    return result
+local TimeSeriesSet = {}
+
+function TimeSeriesSet:new(interval, retention, timestamp, key_function)
+    return setmetatable({
+        interval = interval,
+        retention = retention,
+        timestamp = timestamp,
+        key_function = key_function,
+    }, {__index = self})
 end
 
-local function get_index_expiration_time(interval, retention, index)
-    return (
-        (index + 1)  -- upper bound of this interval
-        + retention
-    ) * interval
+function TimeSeriesSet:members()
+    local results = {}
+    local current = math.floor(self.timestamp / self.interval)
+    for index = current - self.retention, current do
+        local members = redis.call('SMEMBERS', self.key_function(index))
+        for i = 1, #members do
+            local k = members[i]
+            results[k] = (results[k] or 0) + 1
+        end
+    end
+    return results
 end
 
+function TimeSeriesSet:add(...)
+    local index = math.floor(self.timestamp / self.interval)
+    local key = self.key_function(index)
+    local result = redis.call('SADD', key, ...)
+    if result > 0 then
+        redis.call('EXPIREAT', key, (index + 1 + self.retention) * self.interval)
+    end
+    return result
+end
 
--- Redis Helpers
+function TimeSeriesSet:remove(...)
+    local current = math.floor(self.timestamp / self.interval)
+    for index = current - self.retention, current do
+        redis.call('SREM', self.key_function(index), ...)
+    end
+end
 
-local function redis_hgetall_response_to_table(response, value_type)
-    if value_type == nil then
-        value_type = identity
+function TimeSeriesSet:swap(old, new)
+    --[[
+    Replace the "old" member wtih the "new" member in all sets where it is
+    present.
+    ]]--
+    local current = math.floor(self.timestamp / self.interval)
+    for index = current - self.retention, current do
+        local key = self.key_function(index)
+        if redis.call('SREM', key, old) > 0 and redis.call('SADD', key, new) > 0 then
+            -- It's possible that the `SREM` operation implicitly caused this
+            -- set to be deleted if it reached 0 elements, so we need to be
+            -- sure to reset the TTL if we successfully added the element to
+            -- the potentially empty set.
+            redis.call('EXPIREAT', key, (index + 1 + self.retention) * self.interval)
+        end
     end
-    local result = {}
-    for i = 1, #response, 2 do
-        result[response[i]] = value_type(response[i + 1])
+end
+
+function TimeSeriesSet:export(member)
+    local current = math.floor(self.timestamp / self.interval)
+    local results = {}
+    for index = current - self.retention, current do
+        if redis.call('SISMEMBER', self.key_function(index), member) == 1 then
+            results[#results + 1] = index
+        end
     end
-    return result
+    return results
 end
 
+function TimeSeriesSet:import(member, data)
+    for _, index in ipairs(data) do
+        local key = self.key_function(index)
+        if redis.call('SADD', key, member) > 1 then
+            redis.call('EXPIREAT', key, (index + 1 + self.retention) * self.interval)
+        end
+    end
+end
 
--- Generic Configuration
 
-local configuration_parser = build_argument_parser({
-    {"timestamp", parse_integer},
-    {"namespace", identity},
-    {"bands", parse_integer},
-    {"interval", parse_integer},
-    {"retention", parse_integer},  -- how many previous intervals to store (does not include current interval)
-    {"scope", function (value)
-        assert(value ~= nil)
-        return value
-    end}
-})
+-- Redis Helpers
 
-local function takes_configuration(command)
-    return function(arguments)
-        local configuration, arguments = configuration_parser(arguments)
-        return command(configuration, arguments)
+local function redis_hash_response_iterator(response)
+    local i = 1
+    return function ()
+        local key, value = response[i], response[i + 1]
+        i = i + 2
+        return key, value
     end
 end
 
@@ -230,26 +344,14 @@ local function get_key_prefix(configuration, index)
     )
 end
 
-local function get_bucket_frequency_key(configuration, index, time, band, item)
+local function get_frequency_key(configuration, index, item)
     return string.format(
-        '%s:f:%s:%s:%s',
+        '%s:f:%s',
         get_key_prefix(configuration, index),
-        time,
-        band,
         item
     )
 end
 
-local function get_bucket_membership_key(configuration, index, time, band, bucket)
-    return string.format(
-        '%s:m:%s:%s:%s',
-        get_key_prefix(configuration, index),
-        time,
-        band,
-        bucket
-    )
-end
-
 local function get_manhattan_distance(target, other)
     local keys = {}
     for k, _ in pairs(target) do
@@ -280,507 +382,324 @@ local function scale_to_total(values)
     return result
 end
 
-local function collect_index_key_pairs(arguments, validator)
-    return build_variadic_argument_parser({
-        {"index", identity},
-        {"key", identity},
-    }, validator)(arguments)
-end
-
 
 -- Signature Matching
 
-local function parse_band(configuration, arguments, cursor)
-    local result = {}
+local function pack_frequency_coordinate(band, bucket)
+    return struct.pack('>B', band) .. bucket
+end
 
-    local count = tonumber(arguments[cursor])
-    cursor = cursor + 1
+local function unpack_frequency_coordinate(field)
+    local band, index = struct.unpack('>B', field)
+    return band, string.sub(field, index)
+end
 
-    for i = 1, count do
-        result[arguments[cursor]] = tonumber(arguments[cursor + 1])
-        cursor = cursor + 2
+local function get_bucket_membership_set(configuration, index, band, bucket)
+    return TimeSeriesSet:new(
+        configuration.interval,
+        configuration.retention,
+        configuration.timestamp,
+        function (i)
+            return string.format(
+                '%s:m:%s:',
+                get_key_prefix(configuration, index),
+                i
+            ) .. pack_frequency_coordinate(band, bucket)
+        end
+    )
+end
+
+local function get_frequencies(configuration, index, item)
+    local frequencies = {}
+    for i = 1, configuration.bands do
+        frequencies[i] = {}
+    end
+
+    local key = get_frequency_key(configuration, index, item)
+    -- TODO: This needs to handle this returning no data due to TTL gracefully.
+    local response = redis.call('HGETALL', key)
+    for field, value in redis_hash_response_iterator(response) do
+        local band, bucket = unpack_frequency_coordinate(field)
+        frequencies[band][bucket] = tonumber(value)
     end
 
-    return result, cursor
+    return frequencies
 end
 
-local function parse_signature(configuration, arguments, cursor)
-    local result = {}
+local function set_frequencies(configuration, index, item, frequencies, expiration)
+    if expiration == nil then
+        expiration = configuration.timestamp + configuration.interval * configuration.retention
+    end
 
-    result.index = arguments[cursor]
-    cursor = cursor + 1
+    local key = get_frequency_key(configuration, index, item)
 
-    for i = 1, configuration.bands do
-        result[i], cursor = parse_band(
-            configuration,
-            arguments,
-            cursor
-        )
+    for band, buckets in ipairs(frequencies) do
+        for bucket, count in pairs(buckets) do
+            local field = pack_frequency_coordinate(band, bucket)
+            redis.call('HINCRBY', key, field, count)
+        end
     end
 
-    return result, cursor
+    redis.call('EXPIREAT', key, expiration)
 end
 
-local function parse_signatures(configuration, arguments, cursor)
-    local result = {}
+local function merge_frequencies(configuration, index, source, destination)
+    local source_key = get_frequency_key(configuration, index, source)
+    local destination_key = get_frequency_key(configuration, index, destination)
 
-    local count = tonumber(arguments[cursor])
-    cursor = cursor + 1
+    local response = redis.call('HGETALL', source_key)
+    if #response == 0 then
+        return  -- nothing to do
+    end
 
-    for i = 1, count  do
-        result[i], cursor = parse_signature(
-            configuration,
-            arguments,
-            cursor
-        )
+    for field, value in redis_hash_response_iterator(response) do
+        redis.call('HINCRBY', destination_key, field, value)
     end
 
-    return result, cursor
-end
+    local source_ttl = redis.call('TTL', source_key)
+    assert(source_ttl >= 0)  -- this ttl should not be 0 unless we messed up
+    redis.call('EXPIRE', destination_key, math.max(source_ttl, redis.call('TTL', destination_key)))
 
-local function fetch_candidates(configuration, time_series, index, frequencies)
-    --[[
-    Fetch all possible keys that share some characteristics with the provided
-    frequencies. The frequencies should be structured as an array-like table,
-    with one table for each band that represents the number of times that
-    bucket has been associated with the target object. (This is also the output
-    structure of `fetch_bucket_frequencies`.) For example, a four-band
-    request with two recorded observations may be strucured like this:
-
-    {
-        {a=1, b=1},
-        {a=2},
-        {b=2},
-        {a=1, d=1},
-    }
-
-    Results are returned as table where the keys represent candidate keys.
-    ]]--
-    local candidates = {}  -- acts as a set
-    for band, buckets in ipairs(frequencies) do
-        for bucket, count in pairs(buckets) do
-            for _, time in ipairs(time_series) do
-                -- Fetch all other items that have been added to
-                -- the same bucket in this band during this time
-                -- period.
-                local members = redis.call(
-                    'SMEMBERS',
-                    get_bucket_membership_key(
-                        configuration,
-                        index,
-                        time,
-                        band,
-                        bucket
-                    )
-                )
-                for _, member in ipairs(members) do
-                    -- TODO: Count the number of bands that we've collided in
-                    -- to allow setting thresholds here.
-                    candidates[member] = true
-                end
-            end
-        end
-    end
-    return candidates
+    redis.call('DEL', source_key)
 end
 
-local function fetch_bucket_frequencies(configuration, time_series, index, key)
-    --[[
-    Fetches all of the bucket frequencies for a key from a specific index from
-    all active time series chunks. This returns an array-like table that
-    contains one table for each band that maps bucket identifiers to counts
-    across the entire time series.
-    ]]--
-    return table.imap(
-        range(1, configuration.bands),
-        function (band)
-            return table.ireduce(
-                table.imap(
-                    time_series,
-                    function (time)
-                        return redis_hgetall_response_to_table(
-                            redis.call(
-                                'HGETALL',
-                                get_bucket_frequency_key(
-                                    configuration,
-                                    index,
-                                    time,
-                                    band,
-                                    key
-                                )
-                            ),
-                            tonumber
-                        )
-                    end
-                ),
-                function (result, response)
-                    for bucket, count in pairs(response) do
-                        result[bucket] = (result[bucket] or 0) + count
-                    end
-                    return result
-                end,
-                {}
-            )
-        end
-    )
+local function clear_frequencies(configuration, index, item)
+    local key = get_frequency_key(configuration, index, item)
+    redis.call('DEL', key)
 end
 
-local function calculate_similarity(configuration, item_frequencies, candidate_frequencies)
-    --[[
-    Calculate the similarity between an item's frequency and an array-like
-    table of candidate frequencies. This returns a table of candidate keys to
-    and [0, 1] scores where 0 is totally dissimilar and 1 is exactly similar.
-    ]]--
-    local results = {}
-    for key, value in pairs(candidate_frequencies) do
-        table.insert(
-            results,
-            {
-                key,
-                table.ireduce(  -- sum, then avg
-                    table.imap(  -- calculate similarity
-                        table.izip(
-                            item_frequencies,
-                            value
-                        ),
-                        function (v)
-                            -- We calculate the "similarity" between two items
-                            -- by comparing how often their contents exist in
-                            -- the same buckets for a band.
-                            local dist = get_manhattan_distance(
-                                scale_to_total(v[1]),
-                                scale_to_total(v[2])
-                            )
-                            -- Since this is a measure of similarity (and not
-                            -- distance) we normalize the result to [0, 1]
-                            -- scale.
-                            return 1 - (dist / 2)
-                        end
-                    ),
-                    function (total, item)
-                        return total + item
-                    end,
-                    0
-                ) / configuration.bands
-            }
-        )
+local function is_empty(frequencies)
+    for _ in pairs(frequencies[1]) do
+        return false
     end
-    return results
+    return true
 end
 
-local function fetch_similar(configuration, time_series, index, item_frequencies)
-    --[[
-    Fetch the items that are similar to an item's frequencies (as returned by
-    `fetch_bucket_frequencies`), returning a table of similar items keyed by
-    the candidate key where the value is on a [0, 1] similarity scale.
-    ]]--
-    local candidates = fetch_candidates(configuration, time_series, index, item_frequencies)
-    local candidate_frequencies = {}
-    for candidate_key, _ in pairs(candidates) do
-        candidate_frequencies[candidate_key] = fetch_bucket_frequencies(
-            configuration,
-            time_series,
-            index,
-            candidate_key
-        )
+local function calculate_similarity(configuration, item_frequencies, candidate_frequencies)
+    if is_empty(item_frequencies) and is_empty(candidate_frequencies) then
+        return -1
+    elseif is_empty(item_frequencies) or is_empty(candidate_frequencies) then
+        return -2
     end
 
-    return calculate_similarity(
-        configuration,
-        item_frequencies,
-        candidate_frequencies
+    return avg(
+        table.imap(
+            table.izip(
+                item_frequencies,
+                candidate_frequencies
+            ),
+            function (v)
+                -- We calculate the "similarity" between two items
+                -- by comparing how often their contents exist in
+                -- the same buckets for a band.
+                local dist = get_manhattan_distance(
+                    scale_to_total(v[1]),
+                    scale_to_total(v[2])
+                )
+                -- Since this is a measure of similarity (and not
+                -- distance) we normalize the result to [0, 1]
+                -- scale.
+                return 1 - (dist / 2)
+            end
+        )
     )
 end
 
--- Command Parsing
+local function fetch_candidates(configuration, index, frequencies)
+    local create_table = function ()
+        return {}
+    end
 
-local commands = {
-    RECORD = takes_configuration(
-        function (configuration, arguments)
-            local key = arguments[1]
-            local signatures = parse_signatures(
-                configuration,
-                arguments,
-                2
-            )
+    local candidates = {}
+    for band, buckets in ipairs(frequencies) do
+        for bucket in pairs(buckets) do
+            -- Fetch all other items that have been added to
+            -- the same bucket in this band during this time
+            -- period.
+            local members = get_bucket_membership_set(configuration, index, band, bucket):members()
+            for member in pairs(members) do
+                table.get_or_set_default(candidates, member, create_table)[band] = true
+            end
+        end
+    end
 
-            local time = math.floor(configuration.timestamp / configuration.interval)
-            local expiration = get_index_expiration_time(
-                configuration.interval,
-                configuration.retention,
-                time
-            )
+    local results = {}
+    for candidate, bands in pairs(candidates) do
+        results[candidate] = table.count(bands)
+    end
+    return results
+end
 
-            return table.imap(
-                signatures,
-                function (signature)
-                    local results = {}
-
-                    for band, buckets in ipairs(signature) do
-                        for bucket, count in pairs(buckets) do
-                            local bucket_membership_key = get_bucket_membership_key(
-                                configuration,
-                                signature.index,
-                                time,
-                                band,
-                                bucket
-                            )
-                            redis.call('SADD', bucket_membership_key, key)
-                            redis.call('EXPIREAT', bucket_membership_key, expiration)
-
-                            local bucket_frequency_key = get_bucket_frequency_key(
-                                configuration,
-                                signature.index,
-                                time,
-                                band,
-                                key
-                            )
-                            table.insert(
-                                results,
-                                tonumber(redis.call('HINCRBY', bucket_frequency_key, bucket, count))
-                            )
-                            redis.call('EXPIREAT', bucket_frequency_key, expiration)
-                        end
-                    end
+local function search(configuration, parameters, limit)
+    local possible_candidates = {}
+    local create_table = function ()
+        return {}
+    end
 
-                    return results
-                end
-            )
+    for i, p in ipairs(parameters) do
+        for candidate, hits in pairs(fetch_candidates(configuration, p.index, p.frequencies)) do
+            if hits >= p.threshold then
+                table.get_or_set_default(possible_candidates, candidate, create_table)[i] = hits
+            end
         end
-    ),
-    CLASSIFY = takes_configuration(
-        function (configuration, arguments)
-            local signatures = parse_signatures(
-                configuration,
-                arguments,
-                1
-            )
-            local time_series = get_active_indices(
-                configuration.interval,
-                configuration.retention,
-                configuration.timestamp
-            )
+    end
 
-            return table.imap(
-                signatures,
-                function (signature)
-                    local results = fetch_similar(
-                        configuration,
-                        time_series,
-                        signature.index,
-                        signature
-                    )
-
-                    -- Sort the results in descending order (most similar first.)
-                    table.sort(
-                        results,
-                        function (left, right)
-                            return left[2] > right[2]
-                        end
-                    )
-
-                    return table.imap(
-                        results,
-                        function (item)
-                            return {
-                                item[1],
-                                string.format(
-                                    '%f',  -- converting floats to strings avoids truncation
-                                    item[2]
-                                ),
-                            }
-                        end
-                    )
-                end
-            )
-        end
-    ),
-    COMPARE = takes_configuration(
-        function (configuration, arguments)
-            local item_key = arguments[1]
-            local indices = table.slice(arguments, 2)
-
-            local time_series = get_active_indices(
-                configuration.interval,
-                configuration.retention,
-                configuration.timestamp
-            )
+    local candidates = {}
+    local i = 1
+    for candidate, index_hits in pairs(possible_candidates) do
+        candidates[i] = {
+            key = candidate,
+            score = avg(index_hits),
+        }
+        i = i + 1
+    end
 
-            return table.imap(
-                indices,
-                function (index)
-                    local results = fetch_similar(
-                        configuration,
-                        time_series,
-                        index,
-                        fetch_bucket_frequencies(
-                            configuration,
-                            time_series,
-                            index,
-                            item_key
-                        )
-                    )
-
-                    -- Sort the results in descending order (most similar first.)
-                    table.sort(
-                        results,
-                        function (left, right)
-                            return left[2] > right[2]
-                        end
-                    )
-
-                    return table.imap(
-                        results,
-                        function (item)
-                            return {
-                                item[1],
-                                string.format(
-                                    '%f',  -- converting floats to strings avoids truncation
-                                    item[2]
-                                ),
-                            }
-                        end
-                    )
+    if limit >= 0 and #candidates > limit then
+        table.sort(
+            candidates,
+            function (this, that)
+                if this.score > that.score then
+                    return true
+                elseif this.score < that.score then
+                    return false
+                else
+                    return this.key < that.key -- NOTE: reverse lex
                 end
-            )
-        end
-    ),
-    MERGE = takes_configuration(
-        function (configuration, arguments)
-            local destination_key = arguments[1]
-            local sources = collect_index_key_pairs(
-                table.slice(arguments, 2),
-                function (entry)
-                    assert(entry.key ~= destination_key, 'cannot merge destination into itself')
-                    return entry
-                end
-            )
-
-            local time_series = get_active_indices(
-                configuration.interval,
-                configuration.retention,
-                configuration.timestamp
-            )
+            end
+        )
+        candidates = table.slice(candidates, 1, limit)
+    end
 
-            for _, source in ipairs(sources) do
-                for band = 1, configuration.bands do
-                    for _, time in ipairs(time_series) do
-                        local source_bucket_frequency_key = get_bucket_frequency_key(
-                            configuration,
-                            source.index,
-                            time,
-                            band,
-                            source.key
-                        )
-                        local destination_bucket_frequency_key = get_bucket_frequency_key(
-                            configuration,
-                            source.index,
-                            time,
-                            band,
-                            destination_key
-                        )
-                        local expiration_time = get_index_expiration_time(
-                            configuration.interval,
-                            configuration.retention,
-                            time
-                        )
+    local results = {}
+    for i, candidate in ipairs(candidates) do
+        local result = {}
+        for j, p in ipairs(parameters) do
+            local candidate_frequencies = get_frequencies(configuration, p.index, candidate.key)
+            result[j] = string.format('%f', calculate_similarity(
+                configuration,
+                p.frequencies,
+                candidate_frequencies
+            ))
+        end
+        results[i] = {candidate.key, result}
+    end
+    return results
+end
 
-                        local response = redis_hgetall_response_to_table(
-                            redis.call(
-                                'HGETALL',
-                                source_bucket_frequency_key
-                            ),
-                            tonumber
-                        )
 
-                        for bucket, count in pairs(response) do
-                            -- Remove the source from the bucket membership
-                            -- set, and add the destination to the membership
-                            -- set.
-                            local bucket_membership_key = get_bucket_membership_key(
-                                configuration,
-                                source.index,
-                                time,
-                                band,
-                                bucket
-                            )
-                            redis.call('SREM', bucket_membership_key, source.key)
-                            redis.call('SADD', bucket_membership_key, destination_key)
-                            redis.call('EXPIREAT', bucket_membership_key, expiration_time)
-
-                            -- Merge the counter values into the destination frequencies.
-                            redis.call(
-                                'HINCRBY',
-                                destination_bucket_frequency_key,
-                                bucket,
-                                count
-                            )
-                        end
-
-                        -- TODO: We only need to do this if the bucket has contents.
-                        -- The destination bucket frequency key may have not
-                        -- existed previously, so we need to make sure we set
-                        -- the expiration on it in case it is new.
-                        redis.call(
-                            'EXPIREAT',
-                            destination_bucket_frequency_key,
-                            expiration_time
-                        )
+-- Command Parsing
 
-                        -- We no longer need the source frequencies.
-                        redis.call('DEL', source_bucket_frequency_key)
+local commands = {
+    RECORD = function (configuration, cursor, arguments)
+        local cursor, key, signatures = multiple_argument_parser(
+            argument_parser(validate_value),
+            variadic_argument_parser(
+                object_argument_parser({
+                    {"index", argument_parser(validate_value)},
+                    {"frequencies", frequencies_argument_parser(configuration)},
+                })
+            )
+        )(cursor, arguments)
+
+        return table.imap(
+            signatures,
+            function (signature)
+                set_frequencies(configuration, signature.index, key, signature.frequencies)
+                for band, buckets in ipairs(signature.frequencies) do
+                    for bucket in pairs(buckets) do
+                        get_bucket_membership_set(configuration, signature.index, band, bucket):add(key)
                     end
                 end
             end
-        end
-    ),
-    DELETE = takes_configuration(
-        function (configuration, arguments)
-            local sources = collect_index_key_pairs(arguments)
-            local time_series = get_active_indices(
-                configuration.interval,
-                configuration.retention,
-                configuration.timestamp
+        )
+    end,
+    CLASSIFY = function (configuration, cursor, arguments)
+        local cursor, limit, parameters = multiple_argument_parser(
+            argument_parser(validate_integer),
+            variadic_argument_parser(
+                object_argument_parser({
+                    {"index", argument_parser(validate_value)},
+                    {"threshold", argument_parser(validate_integer)},
+                    {"frequencies", frequencies_argument_parser(configuration)},
+                })
             )
+        )(cursor, arguments)
 
-            for _, source in ipairs(sources) do
-                for band = 1, configuration.bands do
-                    for _, time in ipairs(time_series) do
-                        local source_bucket_frequency_key = get_bucket_frequency_key(
-                            configuration,
-                            source.index,
-                            time,
-                            band,
-                            source.key
-                        )
-
-                        local buckets = redis.call(
-                            'HKEYS',
-                            source_bucket_frequency_key
-                        )
+        return search(
+            configuration,
+            parameters,
+            limit
+        )
+    end,
+    COMPARE = function (configuration, cursor, arguments)
+        local cursor, limit, item_key = multiple_argument_parser(
+            argument_parser(validate_integer),
+            argument_parser(validate_value)
+        )(cursor, arguments)
+
+        local cursor, parameters = variadic_argument_parser(
+            object_argument_parser({
+                {"index", argument_parser(validate_value)},
+                {"threshold", argument_parser(validate_integer)},
+            }, function (parameter)
+                parameter.frequencies = get_frequencies(
+                    configuration,
+                    parameter.index,
+                    item_key
+                )
+                return parameter
+            end)
+        )(cursor, arguments)
 
-                        for _, bucket in ipairs(buckets) do
-                            redis.call(
-                                'SREM',
-                                get_bucket_membership_key(
-                                    configuration,
-                                    source.index,
-                                    time,
-                                    band,
-                                    bucket
-                                ),
-                                source.key
-                            )
-                        end
-
-                        -- We no longer need the source frequencies.
-                        redis.call('DEL', source_bucket_frequency_key)
-                    end
+        return search(
+            configuration,
+            parameters,
+            limit
+        )
+    end,
+    MERGE = function (configuration, cursor, arguments)
+        local cursor, destination_key = argument_parser(validate_value)(cursor, arguments)
+        local cursor, sources = variadic_argument_parser(
+            object_argument_parser({
+                {"index", argument_parser(validate_value)},
+                {"key", argument_parser(validate_value)},
+            }, function (entry)
+                assert(entry.key ~= destination_key, 'cannot merge destination into itself')
+                return entry
+            end)
+        )(cursor, arguments)
+
+        for _, source in ipairs(sources) do
+            local source_frequencies = get_frequencies(configuration, source.index, source.key)
+            merge_frequencies(configuration, source.index, source.key, destination_key)
+
+            for band, buckets in ipairs(source_frequencies) do
+                for bucket in pairs(buckets) do
+                    get_bucket_membership_set(configuration, source.index, band, bucket):swap(source.key, destination_key)
                 end
             end
         end
-    ),
-    IMPORT = takes_configuration(
+    end,
+    DELETE = function (configuration, cursor, arguments)
+        local cursor, sources = variadic_argument_parser(
+            object_argument_parser({
+                {"index", argument_parser(validate_value)},
+                {"key", argument_parser(validate_value)},
+            })
+        )(cursor, arguments)
+
+        for _, source in ipairs(sources) do
+            local frequencies = get_frequencies(configuration, source.index, source.key)
+            clear_frequencies(configuration, source.index, source.key)
+
+            for band, buckets in ipairs(frequencies) do
+                for bucket in pairs(buckets) do
+                    get_bucket_membership_set(configuration, source.index, band, bucket):remove(source.key)
+                end
+            end
+        end
+    end,
+    IMPORT = function (configuration, cursor, arguments)
         --[[
         Loads data returned by the ``EXPORT`` command into the location
         specified by the ``index`` and ``key`` arguments. Data can be loaded
@@ -791,161 +710,126 @@ local commands = {
         data already exists at the new destination, the imported data will be
         appended to the existing data.
         ]]--
-        function (configuration, arguments)
-            local entries = build_variadic_argument_parser({
-                {'index', identity},
-                {'key', identity},
-                {'data', cmsgpack.unpack},
-            })(arguments)
-
-            for _, entry in ipairs(entries) do
-                for band, data in ipairs(entry.data) do
-                    for _, item in ipairs(data) do
-                        local time, buckets = item[1], item[2]
-                        local expiration_time = get_index_expiration_time(
-                            configuration.interval,
-                            configuration.retention,
-                            time
-                        )
-                        local destination_bucket_frequency_key = get_bucket_frequency_key(
-                            configuration,
-                            entry.index,
-                            time,
-                            band,
-                            entry.key
-                        )
+        local cursor, entries = variadic_argument_parser(
+            object_argument_parser({
+                {'index', argument_parser(validate_value)},
+                {'key', argument_parser(validate_value)},
+                {'data', argument_parser(cmsgpack.unpack)}
+            })
+        )(cursor, arguments)
+
+        return table.imap(
+            entries,
+            function (source)
+                if #source.data == 0 then
+                    return
+                end
 
-                        for bucket, count in pairs(buckets) do
-                            local bucket_membership_key = get_bucket_membership_key(
-                                configuration,
-                                entry.index,
-                                time,
-                                band,
-                                bucket
-                            )
-                            redis.call('SADD', bucket_membership_key, entry.key)
-                            redis.call('EXPIREAT', bucket_membership_key, expiration_time)
-
-                            redis.call(
-                                'HINCRBY',
-                                destination_bucket_frequency_key,
-                                bucket,
-                                count
-                            )
-                        end
-
-                        -- The destination bucket frequency key may have not
-                        -- existed previously, so we need to make sure we set
-                        -- the expiration on it in case it is new. (We only
-                        -- have to do this if there we changed any bucket counts.)
-                        if next(buckets) ~= nil then
-                            redis.call(
-                                'EXPIREAT',
-                                destination_bucket_frequency_key,
-                                expiration_time
-                            )
-                        end
+                local data, ttl = unpack(source.data)
+                local frequencies = {}
+                for band = 1, #data do
+                    frequencies[band] = {}
+                    for bucket, value in pairs(data[band]) do
+                        frequencies[band][bucket] = value[1]
+                        get_bucket_membership_set(configuration, source.index, band, bucket):import(source.key, value[2])
                     end
                 end
+                set_frequencies(configuration, source.index, source.key, frequencies, ttl)
             end
-        end
-    ),
-    EXPORT = takes_configuration(
+        )
+    end,
+    EXPORT = function (configuration, cursor, arguments)
         --[[
         Exports data that is located at the provided ``index`` and ``key`` pairs.
 
         Generally, this data should be treated as opaque method for extracting
         data to be provided to the ``IMPORT`` command. Exported data is
-        returned in the same order as the arguments are provided. Each item is
-        a messagepacked blob that is at the top level list, where each member
-        represents the data contained within one band.  Each item in the band
-        list is another list, where each member represents one time series
-        interval. Each item in the time series list is a tuple containing the
-        time series index and a mapping containing the counts for each bucket
-        within the interval. (Due to the Lua data model, an empty mapping will
-        be represented as an empty list. The consumer of this data must convert
-        it back to the correct type.)
+        returned in the same order as the arguments are provided.
         ]]--
-        function (configuration, arguments)
-            local bands = range(1, configuration.bands)
-            local time_series = get_active_indices(
-                configuration.interval,
-                configuration.retention,
-                configuration.timestamp
-            )
-            return table.imap(
-                collect_index_key_pairs(arguments),
-                function (source)
-                    return cmsgpack.pack(
-                        table.imap(
-                            bands,
-                            function (band)
-                                return table.imap(
-                                    time_series,
-                                    function (time)
-                                        return {
-                                            time,
-                                            redis_hgetall_response_to_table(
-                                                redis.call(
-                                                    'HGETALL',
-                                                    get_bucket_frequency_key(
-                                                        configuration,
-                                                        source.index,
-                                                        time,
-                                                        band,
-                                                        source.key
-                                                    )
-                                                ),
-                                                tonumber
-                                            ),
-                                        }
-                                    end
-                                )
-                            end
-                        )
-                    )
-                end
-            )
-        end
-    ),
-    SCAN = takes_configuration(
-        function (configuration, arguments)
-            local arguments = build_variadic_argument_parser({
-                {"index", identity},
-                {"cursor", identity},
-                {"count", identity},
-            })(arguments)
-            return table.imap(
-                arguments,
-                function (argument)
-                    return redis.call(
-                        'SCAN',
-                        argument.cursor,
-                        'MATCH',
-                        string.format(
-                            '%s:*',
-                            get_key_prefix(
-                                configuration,
-                                argument.index
-                            )
-                        ),
-                        'COUNT',
-                        argument.count
-                    )
+        local cursor, entries = variadic_argument_parser(
+            object_argument_parser({
+                {'index', argument_parser(validate_value)},
+                {'key', argument_parser(validate_value)},
+            })
+        )(cursor, arguments)
+
+        return table.imap(
+            entries,
+            function (source)
+                local frequency_key = get_frequency_key(configuration, source.index, source.key)
+                if redis.call('EXISTS', frequency_key) < 1 then
+                    return cmsgpack.pack({})
                 end
-            )
-        end
-    )
-}
 
+                local data = {}
+                local frequencies = get_frequencies(configuration, source.index, source.key)
+                for band = 1, #frequencies do
+                    local result = {}
+                    for bucket, count in pairs(frequencies[band]) do
+                        result[bucket] = {
+                            count,
+                            get_bucket_membership_set(configuration, source.index, band, bucket):export(source.key)
+                        }
+                    end
+                    data[band] = result
+                end
 
-local command_parser = build_argument_parser({
-    {"command", function (value)
-        local command = commands[value]
-        assert(command ~= nil)
-        return command
-    end},
-})
+                return cmsgpack.pack({
+                    data,
+                    configuration.timestamp + math.max(
+                        redis.call('TTL', frequency_key),
+                        0
+                    )  -- the TTL should always exist, but this is just to be safe
+                })
+            end
+        )
+    end,
+    SCAN = function (configuration, cursor, arguments)
+        local cursor, entries = variadic_argument_parser(
+            object_argument_parser({
+                {'index', argument_parser(validate_value)},
+                {'cursor', argument_parser(validate_value)},
+                {'count', argument_parser(validate_integer)}
+            })
+        )(cursor, arguments)
+        return table.imap(
+            entries,
+            function (argument)
+                return redis.call(
+                    'SCAN',
+                    argument.cursor,
+                    'MATCH',
+                    string.format(
+                        '%s:*',
+                        get_key_prefix(
+                            configuration,
+                            argument.index
+                        )
+                    ),
+                    'COUNT',
+                    argument.count
+                )
+            end
+        )
+    end,
+}
 
-local parsed, arguments = command_parser(ARGV)
-return parsed.command(arguments)
+local cursor, command, configuration = multiple_argument_parser(
+    argument_parser(
+        function (value)
+            local command = commands[value]
+            assert(command ~= nil)
+            return command
+        end
+    ),
+    object_argument_parser({
+        {"timestamp", argument_parser(validate_number)},
+        {"namespace", argument_parser()},
+        {"bands", argument_parser(validate_integer)},
+        {"interval", argument_parser(validate_integer)},
+        {"retention", argument_parser(validate_integer)},  -- how many previous intervals to store (does not include current interval)
+        {"scope", argument_parser(validate_value)},
+    })
+)(1, ARGV)
+
+return command(configuration, cursor, ARGV)
diff --git a/src/sentry/similarity/features.py b/src/sentry/similarity/features.py
index cc93dad1aa..5e8e740bff 100644
--- a/src/sentry/similarity/features.py
+++ b/src/sentry/similarity/features.py
@@ -141,12 +141,16 @@ class FeatureSet(object):
             timestamp=int(to_timestamp(event.datetime)),
         )
 
-    def classify(self, events):
+    def classify(self, events, limit=None, thresholds=None):
         if not events:
             return []
 
+        if thresholds is None:
+            thresholds = {}
+
         scope = None
 
+        labels = []
         items = []
         for event in events:
             for label, features in self.extract(event).items():
@@ -172,40 +176,41 @@ class FeatureSet(object):
                     )
                 else:
                     if features:
-                        items.append((self.aliases[label], features, ))
-        return zip(
-            map(
-                lambda (alias, characteristics): self.aliases.get_key(alias),
-                items,
+                        items.append((self.aliases[label], thresholds.get(label, 0), features))
+                        labels.append(label)
+
+        return map(
+            lambda (key, scores): (
+                int(key),
+                dict(zip(labels, scores)),
             ),
             self.index.classify(
                 scope,
                 items,
+                limit=limit,
                 timestamp=int(to_timestamp(event.datetime)),
             ),
         )
 
-    def compare(self, group):
+    def compare(self, group, limit=None, thresholds=None):
+        if thresholds is None:
+            thresholds = {}
+
         features = list(self.features.keys())
 
-        results = self.index.compare(
-            self.__get_scope(group.project),
-            self.__get_key(group),
-            [self.aliases[label] for label in features],
-        )
+        items = [(self.aliases[label], thresholds.get(label, 0), ) for label in features]
 
-        items = {}
-        for feature, result in zip(features, results):
-            for item, score in result:
-                items.setdefault(
-                    int(item),
-                    {},
-                )[feature] = score
-
-        return sorted(
-            items.items(),
-            key=lambda (id, features): sum(features.values()),
-            reverse=True,
+        return map(
+            lambda (key, scores): (
+                int(key),
+                dict(zip(features, scores)),
+            ),
+            self.index.compare(
+                self.__get_scope(group.project),
+                self.__get_key(group),
+                items,
+                limit=limit,
+            ),
         )
 
     def merge(self, destination, sources, allow_unsafe=False):
diff --git a/src/sentry/similarity/index.py b/src/sentry/similarity/index.py
index fe4b05de8c..30d053451d 100644
--- a/src/sentry/similarity/index.py
+++ b/src/sentry/similarity/index.py
@@ -2,7 +2,6 @@ from __future__ import absolute_import
 
 import itertools
 import time
-from collections import Counter, defaultdict
 
 from sentry.utils.iterators import chunked
 from sentry.utils.redis import load_script
@@ -28,36 +27,13 @@ class MinHashIndex(object):
         self.interval = interval
         self.retention = retention
 
-    def __build_signatures(self, items):
-        data = defaultdict(
-            lambda: [Counter() for _ in xrange(self.bands)],
-        )
-
-        for idx, features in items:
-            bands = map(
-                ','.join, band(
-                    self.bands,
-                    map(
-                        '{}'.format,
-                        self.signature_builder(features),
-                    ),
-                )
-            )
-
-            for i, bucket in enumerate(bands):
-                data[idx][i][bucket] += 1
-
-        arguments = [len(data)]
-        for idx, bands in data.items():
-            arguments.append(idx)
-            for buckets in bands:
-                arguments.append(len(buckets))
-                for bucket, count in buckets.items():
-                    arguments.extend([
-                        bucket,
-                        count,
-                    ])
+    def _build_signature_arguments(self, features):
+        if not features:
+            return [0] * self.bands
 
+        arguments = []
+        for bucket in band(self.bands, self.signature_builder(features)):
+            arguments.extend([1, ','.join(map('{}'.format, bucket)), 1])
         return arguments
 
     def __index(self, scope, args):
@@ -67,7 +43,42 @@ class MinHashIndex(object):
         # all redis operations.
         return index(self.cluster, [scope], args)
 
-    def classify(self, scope, items, timestamp=None):
+    def _as_search_result(self, results):
+        score_replacements = {
+            -1.0: None,  # both items don't have the feature (no comparison)
+            -2.0: 0,     # one item doesn't have the feature (totally dissimilar)
+        }
+
+        def decode_search_result(result):
+            key, scores = result
+            return (
+                key,
+                map(
+                    lambda score: score_replacements.get(score, score),
+                    map(float, scores),
+                )
+            )
+
+        def get_comparison_key(result):
+            key, scores = result
+
+            scores = filter(
+                lambda score: score is not None,
+                scores,
+            )
+
+            return (
+                sum(scores) / len(scores) * -1,  # average score, descending
+                len(scores) * -1,  # number of indexes with scores, descending
+                key,  # lexicographical sort on key, ascending
+            )
+
+        return sorted(
+            map(decode_search_result, results),
+            key=get_comparison_key,
+        )
+
+    def classify(self, scope, items, limit=None, timestamp=None):
         if timestamp is None:
             timestamp = int(time.time())
 
@@ -79,16 +90,16 @@ class MinHashIndex(object):
             self.interval,
             self.retention,
             scope,
+            limit if limit is not None else -1,
         ]
 
-        arguments.extend(self.__build_signatures(items))
+        for idx, threshold, features in items:
+            arguments.extend([idx, threshold])
+            arguments.extend(self._build_signature_arguments(features))
 
-        return [
-            [(item, float(score)) for item, score in result]
-            for result in self.__index(scope, arguments)
-        ]
+        return self._as_search_result(self.__index(scope, arguments))
 
-    def compare(self, scope, key, indices, timestamp=None):
+    def compare(self, scope, key, items, limit=None, timestamp=None):
         if timestamp is None:
             timestamp = int(time.time())
 
@@ -100,15 +111,14 @@ class MinHashIndex(object):
             self.interval,
             self.retention,
             scope,
+            limit if limit is not None else -1,
             key,
         ]
 
-        arguments.extend(indices)
+        for idx, threshold in items:
+            arguments.extend([idx, threshold])
 
-        return [
-            [(item, float(score)) for item, score in result]
-            for result in self.__index(scope, arguments)
-        ]
+        return self._as_search_result(self.__index(scope, arguments))
 
     def record(self, scope, key, items, timestamp=None):
         if not items:
@@ -128,7 +138,9 @@ class MinHashIndex(object):
             key,
         ]
 
-        arguments.extend(self.__build_signatures(items))
+        for idx, features in items:
+            arguments.append(idx)
+            arguments.extend(self._build_signature_arguments(features))
 
         return self.__index(scope, arguments)
 
@@ -247,10 +259,10 @@ class MinHashIndex(object):
 
 
 class DummyIndex(object):
-    def classify(self, scope, items, timestamp=None):
+    def classify(self, scope, items, limit=None, timestamp=None):
         return []
 
-    def compare(self, scope, key, indices, timestamp=None):
+    def compare(self, scope, key, items, limit=None, timestamp=None):
         return []
 
     def record(self, scope, key, items, timestamp=None):
diff --git a/src/sentry/tasks/merge.py b/src/sentry/tasks/merge.py
index 22f12a30d9..9f0621b72e 100644
--- a/src/sentry/tasks/merge.py
+++ b/src/sentry/tasks/merge.py
@@ -28,7 +28,6 @@ delete_logger = logging.getLogger('sentry.deletions.async')
     default_retry_delay=60 * 5,
     max_retries=None
 )
-@retry
 def merge_group(
     from_object_id=None, to_object_id=None, transaction_id=None, recursed=False, **kwargs
 ):
diff --git a/tests/sentry/similarity/test_index.py b/tests/sentry/similarity/test_index.py
index 007c60cacf..15a4cbc434 100644
--- a/tests/sentry/similarity/test_index.py
+++ b/tests/sentry/similarity/test_index.py
@@ -35,23 +35,65 @@ class MinHashIndexTestCase(TestCase):
         ])
         self.index.record('example', '5', [('index', 'pizza world')])
 
-        results = self.index.compare('example', '1', ['index'])[0]
-        assert results[0] == ('1', 1.0)
-        assert results[1] == ('2', 1.0)  # identical contents
+        # comparison, without thresholding
+        results = self.index.compare('example', '1', [('index', 0)])
+        assert results[0] == ('1', [1.0])
+        assert results[1] == ('2', [1.0])  # identical contents
         assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
         assert results[3][0] in ('3', '4')
         assert results[4][0] == '5'
 
-        results = self.index.classify('example', [('index', 'hello world')])[0]
-        assert results[0:2] == [('1', 1.0), ('2', 1.0)]
+        # comparison, low threshold
+        results = self.index.compare('example', '1', [('index', 6)])
+        assert len(results) == 4
+        assert results[0] == ('1', [1.0])
+        assert results[1] == ('2', [1.0])  # identical contents
+        assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
+        assert results[3][0] in ('3', '4')
+
+        # comparison, high threshold (exact match)
+        results = self.index.compare('example', '1', [('index', self.index.bands)])
+        assert len(results) == 2
+        assert results[0] == ('1', [1.0])
+        assert results[1] == ('2', [1.0])  # identical contents
+
+        # comparison, candidate limit (with lexicographical collision sort)
+        results = self.index.compare('example', '1', [('index', 0)], limit=1)
+        assert len(results) == 1
+        assert results[0] == ('1', [1.0])
+
+        # classification, without thresholding
+        results = self.index.classify('example', [('index', 0, 'hello world')])
+        assert results[0:2] == [('1', [1.0]), ('2', [1.0])]
         assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
         assert results[3][0] in ('3', '4')
         assert results[4][0] == '5'
 
+        # classification, low threshold
+        results = self.index.classify('example', [('index', 6, 'hello world')])
+        assert len(results) == 4
+        assert results[0] == ('1', [1.0])
+        assert results[1] == ('2', [1.0])  # identical contents
+        assert results[2][0] in ('3', '4')  # equidistant pairs, order doesn't really matter
+        assert results[3][0] in ('3', '4')
+
+        # classification, high threshold (exact match)
+        results = self.index.classify('example', [('index', self.index.bands, 'hello world')])
+        assert len(results) == 2
+        assert results[0] == ('1', [1.0])
+        assert results[1] == ('2', [1.0])  # identical contents
+
+        # classification, candidate limit (with lexicographical collision sort)
+        results = self.index.classify(
+            'example', [
+                ('index', 0, 'hello world')], limit=1)
+        assert len(results) == 1
+        assert results[0] == ('1', [1.0])
+
         self.index.delete('example', [('index', '3')])
-        assert [key for key, _ in self.index.compare('example', '1', ['index'])[0]] == [
-            '1', '2', '4', '5'
-        ]
+        assert [key
+                for key, _ in self.index.compare('example', '1', [('index',
+                                                                   0)])] == ['1', '2', '4', '5']
 
         assert MinHashIndex(
             self.index.cluster,
@@ -60,24 +102,142 @@ class MinHashIndexTestCase(TestCase):
             self.index.bands,
             self.index.interval,
             self.index.retention,
-        ).compare('example', '1', ['index']) == [[]]
+        ).compare('example', '1', [('index', 0)]) == []
+
+    def test_multiple_index(self):
+        self.index.record('example', '1', [
+            ('index:a', 'hello world'),
+            ('index:b', 'hello world'),
+        ])
+        self.index.record('example', '2', [
+            ('index:a', 'hello world'),
+            ('index:b', 'hello world'),
+        ])
+        self.index.record('example', '3', [
+            ('index:a', 'hello world'),
+            ('index:b', 'pizza world'),
+        ])
+        self.index.record('example', '4', [
+            ('index:a', 'hello world'),
+        ])
+        self.index.record('example', '5', [
+            ('index:b', 'hello world'),
+        ])
+
+        # comparison, without thresholding
+        results = self.index.compare('example', '1', [
+            ('index:a', 0),
+            ('index:b', 0),
+        ])
+        assert len(results) == 5
+        assert results[:2] == [
+            ('1', [1.0, 1.0]),
+            ('2', [1.0, 1.0]),
+        ]
+        assert results[2][0] == '3'
+        assert results[2][1][0] == 1.0
+        assert results[3] == ('4', [1.0, 0.0])
+        assert results[4] == ('5', [0.0, 1.0])
+
+        # comparison, candidate limit (with lexicographical collision sort)
+        results = self.index.compare('example', '1', [
+            ('index:a', 0),
+            ('index:b', 0),
+        ], limit=4)
+        assert len(results) == 4
+        assert results[:2] == [
+            ('1', [1.0, 1.0]),
+            ('2', [1.0, 1.0]),
+        ]
+        assert results[2][0] == '3'
+        assert results[2][1][0] == 1.0
+        assert results[3] == ('4', [1.0, 0.0])
+
+        # classification, without thresholding
+        results = self.index.classify('example', [
+            ('index:a', 0, 'hello world'),
+            ('index:b', 0, 'hello world'),
+        ])
+        assert len(results) == 5
+        assert results[:2] == [
+            ('1', [1.0, 1.0]),
+            ('2', [1.0, 1.0]),
+        ]
+        assert results[2][0] == '3'
+        assert results[2][1][0] == 1.0
+        assert results[3] == ('4', [1.0, 0.0])
+        assert results[4] == ('5', [0.0, 1.0])
+
+        # classification, with thresholding (low)
+        results = self.index.classify('example', [
+            ('index:a', self.index.bands, 'pizza world'),   # no direct hits
+            ('index:b', 8, 'pizza world'),   # one direct hit
+        ])
+        assert len(results) == 1
+        assert results[0][0] == '3'
+        # this should have a value since it's similar even thought it was not
+        # considered as a candidate for this index
+        assert results[0][1][0] > 0
+        assert results[0][1][1] == 1.0
+
+        # classification, with thresholding (high)
+        results = self.index.classify('example', [
+            ('index:a', self.index.bands, 'pizza world'),   # no direct hits
+            ('index:b', self.index.bands, 'hello world'),   # 3 direct hits
+        ])
+        assert len(results) == 3
+        assert results[0][0] == '1'  # tie btw first 2 items is broken by lex sort
+        assert results[0][1][0] > 0
+        assert results[0][1][1] == 1.0
+        assert results[1][0] == '2'
+        assert results[1][1][0] > 0
+        assert results[1][1][1] == 1.0
+        assert results[2] == ('5', [0.0, 1.0])
+
+        # classification, candidate limit (with lexicographical collision sort)
+        results = self.index.classify('example', [
+            ('index:a', 0, 'hello world'),
+            ('index:b', 0, 'hello world'),
+        ], limit=4)
+        assert len(results) == 4
+        assert results[:2] == [
+            ('1', [1.0, 1.0]),
+            ('2', [1.0, 1.0]),
+        ]
+        assert results[2][0] == '3'
+        assert results[2][1][0] == 1.0
+        assert results[3] == ('4', [1.0, 0.0])
+
+        # empty query
+        assert self.index.classify('example', [
+            ('index:a', 0, 'hello world'),
+            ('index:b', 0, ''),
+        ]) == self.index.compare('example', '4', [
+            ('index:a', 0),
+            ('index:b', 0),
+        ]) == [
+            ('4', [1.0, None]),
+            ('1', [1.0, 0.0]),
+            ('2', [1.0, 0.0]),
+            ('3', [1.0, 0.0]),
+        ]
 
     def test_merge(self):
         self.index.record('example', '1', [('index', ['foo', 'bar'])])
         self.index.record('example', '2', [('index', ['baz'])])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == [
-            ('1', 1.0),
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == [
+            ('1', [1.0]),
         ]
 
         self.index.merge('example', '1', [('index', '2')])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == [
-            ('1', 0.5),
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == [
+            ('1', [0.5]),
         ]
 
         # merge into an empty key should act as a move
         self.index.merge('example', '2', [('index', '1')])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == [
-            ('2', 0.5),
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == [
+            ('2', [0.5]),
         ]
 
     def test_export_import(self):
@@ -87,23 +247,13 @@ class MinHashIndexTestCase(TestCase):
         result = self.index.export('example', [('index', 1)], timestamp=timestamp)
         assert len(result) == 1
 
-        data = msgpack.unpackb(result[0])
-        assert len(data) == self.index.bands
-
-        for band in data:
-            assert len(band) == (self.index.retention + 1)
-            assert sum(
-                sum(dict(bucket_frequencies).values()) for index, bucket_frequencies in band
-            ) == 1
-
         # Copy the data from key 1 to key 2.
         self.index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
 
-        assert self.index.export(
-            'example', [('index', 1)], timestamp=timestamp
-        ) == self.index.export(
-            'example', [('index', 2)], timestamp=timestamp
-        )
+        r1 = msgpack.unpackb(self.index.export('example', [('index', 1)], timestamp=timestamp)[0])
+        r2 = msgpack.unpackb(self.index.export('example', [('index', 2)], timestamp=timestamp)[0])
+        assert r1[0] == r2[0]
+        self.assertAlmostEqual(r1[1], r2[1], delta=10)  # cannot ensure exact TTL match
 
         # Copy the data again to key 2 (duplicating all of the data.)
         self.index.import_('example', [('index', 2, result[0])], timestamp=timestamp)
@@ -111,29 +261,20 @@ class MinHashIndexTestCase(TestCase):
         result = self.index.export('example', [('index', 2)], timestamp=timestamp)
         assert len(result) == 1
 
-        data = msgpack.unpackb(result[0])
-        assert len(data) == self.index.bands
-
-        for band in data:
-            assert len(band) == (self.index.retention + 1)
-            assert sum(
-                sum(dict(bucket_frequencies).values()) for index, bucket_frequencies in band
-            ) == 2
-
     def test_flush_scoped(self):
         self.index.record('example', '1', [('index', ['foo', 'bar'])])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == [
-            ('1', 1.0),
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == [
+            ('1', [1.0]),
         ]
 
         self.index.flush('example', ['index'])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == []
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == []
 
     def test_flush_unscoped(self):
         self.index.record('example', '1', [('index', ['foo', 'bar'])])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == [
-            ('1', 1.0),
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == [
+            ('1', [1.0]),
         ]
 
         self.index.flush('*', ['index'])
-        assert self.index.classify('example', [('index', ['foo', 'bar'])])[0] == []
+        assert self.index.classify('example', [('index', 0, ['foo', 'bar'])]) == []
diff --git a/tests/sentry/tasks/test_unmerge.py b/tests/sentry/tasks/test_unmerge.py
index 477df4884d..a430803576 100644
--- a/tests/sentry/tasks/test_unmerge.py
+++ b/tests/sentry/tasks/test_unmerge.py
@@ -291,6 +291,9 @@ class UnmergeTestCase(TestCase):
 
         assert features.compare(source) == [
             (source.id, {
+                'exception:message:character-shingles': None,
+                'exception:stacktrace:application-chunks': None,
+                'exception:stacktrace:pairs': None,
                 'message:message:character-shingles': 1.0
             }),
         ]
@@ -622,17 +625,23 @@ class UnmergeTestCase(TestCase):
         )
 
         source_similar_items = features.compare(source)
-        assert source_similar_items[0] == (source.id, {'message:message:character-shingles': 1.0})
+        assert source_similar_items[0] == (source.id, {
+            'exception:message:character-shingles': None,
+            'exception:stacktrace:application-chunks': None,
+            'exception:stacktrace:pairs': None,
+            'message:message:character-shingles': 1.0,
+        })
         assert source_similar_items[1][0] == destination.id
-        assert source_similar_items[1][1].keys() == ['message:message:character-shingles']
         assert source_similar_items[1][1]['message:message:character-shingles'] < 1.0
 
         destination_similar_items = features.compare(destination)
         assert destination_similar_items[0] == (
             destination.id, {
+                'exception:message:character-shingles': None,
+                'exception:stacktrace:application-chunks': None,
+                'exception:stacktrace:pairs': None,
                 'message:message:character-shingles': 1.0
             }
         )
         assert destination_similar_items[1][0] == source.id
-        assert destination_similar_items[1][1].keys() == ['message:message:character-shingles']
         assert destination_similar_items[1][1]['message:message:character-shingles'] < 1.0
