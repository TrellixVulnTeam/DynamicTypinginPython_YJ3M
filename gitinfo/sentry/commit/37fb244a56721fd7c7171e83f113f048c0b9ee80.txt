commit 37fb244a56721fd7c7171e83f113f048c0b9ee80
Author: ted kaemming <ted@kaemming.com>
Date:   Thu Feb 20 14:17:40 2020 -0800

    feat(eventstream): Send received timestamp in insertion message headers (#17021)

diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index df3c709599..64e14b5c9f 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -872,6 +872,7 @@ class EventManager(object):
                 is_regression=is_regression,
                 is_new_group_environment=is_new_group_environment,
                 primary_hash=hashes[0],
+                received_timestamp=received_timestamp,
                 # We are choosing to skip consuming the event back
                 # in the eventstream if it's flagged as raw.
                 # This means that we want to publish the event
diff --git a/src/sentry/eventstream/base.py b/src/sentry/eventstream/base.py
index d89fdb6a3a..b1f4eb7ca6 100644
--- a/src/sentry/eventstream/base.py
+++ b/src/sentry/eventstream/base.py
@@ -59,6 +59,7 @@ class EventStream(Service):
         is_regression,
         is_new_group_environment,
         primary_hash,
+        recieved_timestamp,  # type: float
         skip_consume=False,
     ):
         self._dispatch_post_process_group_task(
diff --git a/src/sentry/eventstream/kafka/backend.py b/src/sentry/eventstream/kafka/backend.py
index 28619344bc..eec44db7ad 100644
--- a/src/sentry/eventstream/kafka/backend.py
+++ b/src/sentry/eventstream/kafka/backend.py
@@ -28,7 +28,17 @@ class KafkaEventStream(SnubaProtocolEventStream):
         if error is not None:
             logger.warning("Could not publish message (error: %s): %r", error, message)
 
-    def _send(self, project_id, _type, extra_data=(), asynchronous=True):
+    def _send(
+        self,
+        project_id,
+        _type,
+        extra_data=(),
+        asynchronous=True,
+        headers=None,  # Optional[Mapping[str, str]]
+    ):
+        if headers is None:
+            headers = {}
+
         # Polling the producer is required to ensure callbacks are fired. This
         # means that the latency between a message being delivered (or failing
         # to be delivered) and the corresponding callback being fired is
@@ -50,6 +60,7 @@ class KafkaEventStream(SnubaProtocolEventStream):
                 key=key.encode("utf-8"),
                 value=json.dumps((self.EVENT_PROTOCOL_VERSION, _type) + extra_data),
                 on_delivery=self.delivery_callback,
+                headers=[(k, v.encode('utf-8')) for k, v in headers.items()],
             )
         except Exception as error:
             logger.error("Could not publish message: %s", error, exc_info=True)
diff --git a/src/sentry/eventstream/snuba.py b/src/sentry/eventstream/snuba.py
index a2d464ba21..4cd402eaad 100644
--- a/src/sentry/eventstream/snuba.py
+++ b/src/sentry/eventstream/snuba.py
@@ -5,6 +5,7 @@ from datetime import datetime
 from uuid import uuid4
 
 import pytz
+import six
 import urllib3
 
 from sentry import quotas
@@ -83,6 +84,7 @@ class SnubaProtocolEventStream(EventStream):
         is_regression,
         is_new_group_environment,
         primary_hash,
+        received_timestamp,  # type: float
         skip_consume=False,
     ):
         project = event.project
@@ -125,6 +127,7 @@ class SnubaProtocolEventStream(EventStream):
                     "skip_consume": skip_consume,
                 },
             ),
+            headers={'Received-Timestamp': six.text_type(received_timestamp)}
         )
 
     def start_delete_groups(self, project_id, group_ids):
@@ -212,12 +215,29 @@ class SnubaProtocolEventStream(EventStream):
         state["datetime"] = datetime.now(tz=pytz.utc)
         self._send(state["project_id"], "end_delete_tag", extra_data=(state,), asynchronous=False)
 
-    def _send(self, project_id, _type, extra_data=(), asynchronous=True):
+    def _send(
+        self,
+        project_id,
+        _type,
+        extra_data=(),
+        asynchronous=True,
+        headers=None,  # Optional[Mapping[str, str]]
+    ):
         raise NotImplementedError
 
 
 class SnubaEventStream(SnubaProtocolEventStream):
-    def _send(self, project_id, _type, extra_data=(), asynchronous=True):
+    def _send(
+        self,
+        project_id,
+        _type,
+        extra_data=(),
+        asynchronous=True,
+        headers=None,  # Optional[Mapping[str, str]]
+    ):
+        if headers is None:
+            headers = {}
+
         data = (self.EVENT_PROTOCOL_VERSION, _type) + extra_data
 
         # TODO remove this once the unified dataset is available.
@@ -230,7 +250,10 @@ class SnubaEventStream(SnubaProtocolEventStream):
         try:
             for dataset in datasets:
                 resp = snuba._snuba_pool.urlopen(
-                    "POST", "/tests/{}/eventstream".format(dataset), body=json.dumps(data)
+                    "POST",
+                    "/tests/{}/eventstream".format(dataset),
+                    body=json.dumps(data),
+                    headers={'X-Sentry-{}'.format(k): v for k, v in headers.items()},
                 )
                 if resp.status != 200:
                     raise snuba.SnubaError("HTTP %s response from Snuba!" % resp.status)
@@ -249,6 +272,7 @@ class SnubaEventStream(SnubaProtocolEventStream):
         is_regression,
         is_new_group_environment,
         primary_hash,
+        received_timestamp,  # type: float
         skip_consume=False,
     ):
         super(SnubaEventStream, self).insert(
@@ -258,6 +282,7 @@ class SnubaEventStream(SnubaProtocolEventStream):
             is_regression,
             is_new_group_environment,
             primary_hash,
+            received_timestamp,
             skip_consume,
         )
         self._dispatch_post_process_group_task(
diff --git a/tests/sentry/event_manager/test_event_manager.py b/tests/sentry/event_manager/test_event_manager.py
index d680bd6b25..2eaabbe1a8 100644
--- a/tests/sentry/event_manager/test_event_manager.py
+++ b/tests/sentry/event_manager/test_event_manager.py
@@ -776,6 +776,7 @@ class EventManagerTest(TestCase):
             is_new_group_environment=True,
             primary_hash="acbd18db4cc2f85cedef654fccc4a4d8",
             skip_consume=False,
+            received_timestamp=event.data['received'],
         )
 
         event = save_event()
@@ -790,6 +791,7 @@ class EventManagerTest(TestCase):
             is_new_group_environment=False,
             primary_hash="acbd18db4cc2f85cedef654fccc4a4d8",
             skip_consume=False,
+            received_timestamp=event.data['received'],
         )
 
     def test_default_fingerprint(self):
diff --git a/tests/snuba/eventstream/test_eventstream.py b/tests/snuba/eventstream/test_eventstream.py
index a79d20a1bc..23755906fd 100644
--- a/tests/snuba/eventstream/test_eventstream.py
+++ b/tests/snuba/eventstream/test_eventstream.py
@@ -78,6 +78,7 @@ class SnubaEventStreamTest(TestCase, SnubaTestCase):
             "is_regression": False,
             "primary_hash": "acbd18db4cc2f85cedef654fccc4a4d8",
             "skip_consume": False,
+            "received_timestamp": event.data["received"],
         }
 
         self.__produce_event(*insert_args, **insert_kwargs)
@@ -98,6 +99,7 @@ class SnubaEventStreamTest(TestCase, SnubaTestCase):
             "is_regression": False,
             "primary_hash": "acbd18db4cc2f85cedef654fccc4a4d8",
             "skip_consume": False,
+            "received_timestamp": event.data["received"],
         }
 
         self.__produce_event(*insert_args, **insert_kwargs)
