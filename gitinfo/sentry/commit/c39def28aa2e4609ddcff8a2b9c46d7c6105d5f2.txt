commit c39def28aa2e4609ddcff8a2b9c46d7c6105d5f2
Author: Anton Ovchinnikov <anton@tonyo.info>
Date:   Mon Sep 30 17:25:07 2019 +0200

    feat: Add some metrics to outcome/ingest consumers (#14862)

diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
index 11c858a87c..8fbc45d05d 100644
--- a/src/sentry/utils/kafka.py
+++ b/src/sentry/utils/kafka.py
@@ -1,14 +1,15 @@
 from __future__ import absolute_import
 
-import logging
 import abc
-from contextlib import contextmanager
+import logging
 import signal as os_signal
+from contextlib import contextmanager
 
 import six
 import confluent_kafka as kafka
 from django.conf import settings
 
+from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
 
 logger = logging.getLogger(__name__)
@@ -113,6 +114,12 @@ class SimpleKafkaConsumer(object):
         consumer = kafka.Consumer(self.consumer_configuration)
         consumer.subscribe([self.topic_name])
 
+        metrics_tags = {
+            "topic": self.topic_name,
+            "consumer_group": self.consumer_group,
+            "type": self.__class__.__name__,
+        }
+
         # setup a flag to mark termination signals received, see below why we use an array
         termination_signal_received = [False]
 
@@ -145,12 +152,17 @@ class SimpleKafkaConsumer(object):
                             "Bad message received from consumer", self.topic_name, message_error
                         )
 
-                    safe_execute(self.process_message, message, _with_transaction=False)
+                    with metrics.timer("simple_consumer.processing_time", tags=metrics_tags):
+                        safe_execute(self.process_message, message, _with_transaction=False)
 
                 if len(messages) > 0:
                     # we have read some messages in the previous consume, commit the offset
                     consumer.commit(asynchronous=False)
 
+                metrics.timing(
+                    "simple_consumer.committed_batch.size", len(messages), tags=metrics_tags
+                )
+
         consumer.close()
         logger.debug(
             "Closing kafka consumer for topic:{} with consumer group:{}",
diff --git a/tests/relay/test_outcome_consumer.py b/tests/relay/test_outcome_consumer.py
index c3351c963c..c7830ff03a 100644
--- a/tests/relay/test_outcome_consumer.py
+++ b/tests/relay/test_outcome_consumer.py
@@ -2,6 +2,7 @@ from __future__ import absolute_import
 
 import logging
 import time
+import os
 import pytest
 import six.moves
 
@@ -14,6 +15,8 @@ from sentry.utils import json
 
 logger = logging.getLogger(__name__)
 
+SKIP_CONSUMER_TESTS = os.environ.get("SENTRY_RUN_CONSUMER_TESTS") != "1"
+
 
 def _get_event_id(base_event_id):
     return "{:032}".format(int(base_event_id))
@@ -92,7 +95,9 @@ def _setup_outcome_test(kafka_producer, kafka_admin):
     return producer, project_id, topic_name
 
 
-@pytest.mark.skip(reason="extreamly slow test, reading the first kafka message takes many seconds")
+@pytest.mark.skipif(
+    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
+)
 @pytest.mark.django_db
 def test_outcome_consumer_ignores_outcomes_already_handled(
     kafka_producer, task_runner, kafka_admin
@@ -145,7 +150,9 @@ def test_outcome_consumer_ignores_outcomes_already_handled(
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skip(reason="extreamly slow test, reading the first kafka message takes many seconds")
+@pytest.mark.skipif(
+    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
+)
 @pytest.mark.django_db
 def test_outcome_consumer_ignores_invalid_outcomes(kafka_producer, task_runner, kafka_admin):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
@@ -194,7 +201,9 @@ def test_outcome_consumer_ignores_invalid_outcomes(kafka_producer, task_runner,
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skip(reason="extreamly slow test, reading the first kafka message takes many seconds")
+@pytest.mark.skipif(
+    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
+)
 @pytest.mark.django_db
 def test_outcome_consumer_remembers_handled_outcomes(kafka_producer, task_runner, kafka_admin):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
@@ -246,7 +255,9 @@ def test_outcome_consumer_remembers_handled_outcomes(kafka_producer, task_runner
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skip(reason="extreamly slow test, reading the first kafka message takes many seconds")
+@pytest.mark.skipif(
+    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
+)
 @pytest.mark.django_db
 def test_outcome_consumer_handles_filtered_outcomes(kafka_producer, task_runner, kafka_admin):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
@@ -296,7 +307,9 @@ def test_outcome_consumer_handles_filtered_outcomes(kafka_producer, task_runner,
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skip(reason="extreamly slow test, reading the first kafka message takes many seconds")
+@pytest.mark.skipif(
+    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
+)
 @pytest.mark.django_db
 def test_outcome_consumer_handles_rate_limited_outcomes(kafka_producer, task_runner, kafka_admin):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
