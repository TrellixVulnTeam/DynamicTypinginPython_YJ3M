commit 367c4b7646b7688b39169017ae1fd65b0361594d
Author: Mark Story <mark@sentry.io>
Date:   Tue Mar 31 14:16:51 2020 -0400

    ref(discover) Remove some hacky shims in discover (#17967)
    
    Expand the responsibilities of eventstore.Filter so that we can further
    reduce the amount of dictionary munging we do. I think that using
    eventstore.Filter gives us a way to move forward with encapsulating
    snuba queries in a non-dictionary form and incrementally improve
    the internal interfaces.
    
    Co-authored-by: William Mak <william@wmak.io>

diff --git a/src/sentry/api/endpoints/organization_events_stats.py b/src/sentry/api/endpoints/organization_events_stats.py
index e75804c562..83f8026480 100644
--- a/src/sentry/api/endpoints/organization_events_stats.py
+++ b/src/sentry/api/endpoints/organization_events_stats.py
@@ -6,7 +6,7 @@ from datetime import timedelta
 from rest_framework.response import Response
 from rest_framework.exceptions import ParseError
 
-from sentry import features
+from sentry import features, eventstore
 from sentry.api.bases import OrganizationEventsEndpointBase, OrganizationEventsError, NoProjects
 from sentry.api.event_search import resolve_field_list, InvalidSearchQuery, get_function_alias
 from sentry.api.serializers.snuba import SnubaTSResultSerializer
@@ -119,8 +119,15 @@ class OrganizationEventsStatsEndpoint(OrganizationEventsEndpointBase):
         elif y_axis == "user_count":
             y_axis = "count_unique(user)"
 
+        snuba_filter = eventstore.Filter(
+            {
+                "start": snuba_args.get("start"),
+                "end": snuba_args.get("end"),
+                "rollup": snuba_args.get("rollup"),
+            }
+        )
         try:
-            resolved = resolve_field_list([y_axis], {})
+            resolved = resolve_field_list([y_axis], snuba_filter)
         except InvalidSearchQuery as err:
             raise ParseError(detail=six.text_type(err))
         try:
diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index 785b97df3b..1f39b98183 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -662,7 +662,7 @@ def convert_search_boolean_to_snuba_query(search_boolean):
     return [operator, [left, right]]
 
 
-def convert_aggregate_filter_to_snuba_query(aggregate_filter, is_alias, params=None):
+def convert_aggregate_filter_to_snuba_query(aggregate_filter, is_alias, params):
     name = aggregate_filter.key.name
     value = aggregate_filter.value.value
 
@@ -1016,7 +1016,9 @@ class IntervalDefault(NumberRange):
     def has_default(self, params):
         if not params or not params.get("start") or not params.get("end"):
             raise InvalidFunctionArgument("function called without default")
-        elif not isinstance(params["start"], datetime) or not isinstance(params["end"], datetime):
+        elif not isinstance(params.get("start"), datetime) or not isinstance(
+            params.get("end"), datetime
+        ):
             raise InvalidFunctionArgument("function called with invalid default")
 
         interval = (params["end"] - params["start"]).total_seconds()
@@ -1334,7 +1336,7 @@ def resolve_field(field, params=None):
     return ([field], None)
 
 
-def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
+def resolve_field_list(fields, snuba_filter, auto_fields=True):
     """
     Expand a list of fields based on aliases and aggregate functions.
 
@@ -1362,14 +1364,14 @@ def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
             fields.append("project.id")
 
     for field in fields:
-        column_additions, agg_additions = resolve_field(field, params)
+        column_additions, agg_additions = resolve_field(field, snuba_filter.date_params)
         if column_additions:
             columns.extend(column_additions)
 
         if agg_additions:
             aggregations.extend(agg_additions)
 
-    rollup = snuba_args.get("rollup")
+    rollup = snuba_filter.rollup
     if not rollup and auto_fields:
         # Ensure fields we require to build a functioning interface
         # are present. We don't add fields when using a rollup as the additional fields
@@ -1392,7 +1394,7 @@ def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
             project_key = PROJECT_NAME_ALIAS
 
     if project_key:
-        project_ids = snuba_args.get("filter_keys", {}).get("project_id", [])
+        project_ids = snuba_filter.filter_keys.get("project_id", [])
         projects = Project.objects.filter(id__in=project_ids).values("slug", "id")
         aggregations.append(
             [
@@ -1411,7 +1413,7 @@ def resolve_field_list(fields, snuba_args, params=None, auto_fields=True):
     if rollup and columns and not aggregations:
         raise InvalidSearchQuery("You cannot use rollup without an aggregate field.")
 
-    orderby = snuba_args.get("orderby")
+    orderby = snuba_filter.orderby
     if orderby:
         orderby = resolve_orderby(orderby, columns, aggregations)
 
diff --git a/src/sentry/discover/utils.py b/src/sentry/discover/utils.py
index f3ce069851..1635af3bcc 100644
--- a/src/sentry/discover/utils.py
+++ b/src/sentry/discover/utils.py
@@ -6,6 +6,7 @@ from sentry.utils.snuba import Dataset, aliased_query, get_snuba_column_name, ge
 
 # TODO(mark) Once this import is removed, transform_results should not
 # be exported.
+from sentry import eventstore
 from sentry.snuba.discover import transform_results
 
 
@@ -144,4 +145,10 @@ def transform_aliases_and_query(**kwargs):
 
     result = aliased_query(**kwargs)
 
-    return transform_results(result, translated_columns, kwargs)
+    snuba_filter = eventstore.Filter(
+        rollup=kwargs.get("rollup"),
+        start=kwargs.get("start"),
+        end=kwargs.get("end"),
+        orderby=kwargs.get("orderby"),
+    )
+    return transform_results(result, translated_columns, snuba_filter)
diff --git a/src/sentry/eventstore/base.py b/src/sentry/eventstore/base.py
index 9b1ad9ac66..3942f5372e 100644
--- a/src/sentry/eventstore/base.py
+++ b/src/sentry/eventstore/base.py
@@ -1,5 +1,6 @@
 from __future__ import absolute_import
 
+from copy import deepcopy
 
 from sentry import nodestore
 from sentry.snuba.events import Columns
@@ -20,6 +21,10 @@ class Filter(object):
     project_ids (Sequence[int]): List of project IDs to fetch - default None
     group_ids (Sequence[int]): List of group IDs to fetch - defualt None
     event_ids (Sequence[int]): List of event IDs to fetch - default None
+
+    selected_columns (Sequence[str]): List of columns to select
+    aggregations (Sequence[Any, str|None, str]): Aggregate functions to fetch.
+    groupby (Sequence[str]): List of columns to group results by
     """
 
     def __init__(
@@ -31,6 +36,11 @@ class Filter(object):
         project_ids=None,
         group_ids=None,
         event_ids=None,
+        selected_columns=None,
+        aggregations=None,
+        rollup=None,
+        groupby=None,
+        orderby=None,
     ):
         self.start = start
         self.end = end
@@ -40,6 +50,12 @@ class Filter(object):
         self.group_ids = group_ids
         self.event_ids = event_ids
 
+        self.rollup = rollup
+        self.selected_columns = selected_columns if selected_columns is not None else []
+        self.aggregations = aggregations if aggregations is not None else []
+        self.groupby = groupby
+        self.orderby = orderby
+
     @property
     def filter_keys(self):
         """
@@ -58,6 +74,22 @@ class Filter(object):
 
         return filter_keys
 
+    @property
+    def date_params(self):
+        """
+        Get the datetime parameters as a dictionary
+        """
+        return {"start": self.start, "end": self.end}
+
+    def update_with(self, updates):
+        keys = ("selected_columns", "aggregations", "conditions", "orderby", "groupby")
+        for key in keys:
+            if key in updates:
+                setattr(self, key, updates[key])
+
+    def clone(self):
+        return deepcopy(self)
+
 
 class EventStorage(Service):
     __all__ = (
@@ -80,13 +112,18 @@ class EventStorage(Service):
     minimal_columns = [Columns.EVENT_ID, Columns.GROUP_ID, Columns.PROJECT_ID, Columns.TIMESTAMP]
 
     def get_events(
-        self, filter, orderby=None, limit=100, offset=0, referrer="eventstore.get_events"  # NOQA
+        self,
+        snuba_filter,
+        orderby=None,
+        limit=100,
+        offset=0,
+        referrer="eventstore.get_events",  # NOQA
     ):
         """
         Fetches a list of events given a set of criteria.
 
         Arguments:
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         orderby (Sequence[str]): List of fields to order by - default ['-time', '-event_id']
         limit (int): Query limit - default 100
         offset (int): Query offset - default 0
@@ -95,7 +132,12 @@ class EventStorage(Service):
         raise NotImplementedError
 
     def get_unfetched_events(
-        self, filter, orderby=None, limit=100, offset=0, referrer="eventstore.get_unfetched_events"  # NOQA
+        self,
+        snuba_filter,
+        orderby=None,
+        limit=100,
+        offset=0,
+        referrer="eventstore.get_unfetched_events",  # NOQA
     ):
         """
         Same as get_events but returns events without their node datas loaded.
@@ -107,7 +149,7 @@ class EventStorage(Service):
         we just need the event IDs in order to process the deletions.
 
         Arguments:
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         orderby (Sequence[str]): List of fields to order by - default ['-time', '-event_id']
         limit (int): Query limit - default 100
         offset (int): Query offset - default 0
@@ -125,47 +167,47 @@ class EventStorage(Service):
         """
         raise NotImplementedError
 
-    def get_next_event_id(self, event, filter):  # NOQA
+    def get_next_event_id(self, event, snuba_filter):  # NOQA
         """
         Gets the next event given a current event and some conditions/filters.
         Returns a tuple of (project_id, event_id)
 
         Arguments:
         event (Event): Event object
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         """
         raise NotImplementedError
 
-    def get_prev_event_id(self, event, filter):  # NOQA
+    def get_prev_event_id(self, event, snuba_filter):  # NOQA
         """
         Gets the previous event given a current event and some conditions/filters.
         Returns a tuple of (project_id, event_id)
 
         Arguments:
         event (Event): Event object
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         """
         raise NotImplementedError
 
-    def get_earliest_event_id(self, event, filter):  # NOQA
+    def get_earliest_event_id(self, event, snuba_filter):  # NOQA
         """
         Gets the earliest event given a current event and some conditions/filters.
         Returns a tuple of (project_id, event_id)
 
         Arguments:
         event (Event): Event object
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         """
         raise NotImplementedError
 
-    def get_latest_event_id(self, event, filter):  # NOQA
+    def get_latest_event_id(self, event, snuba_filter):  # NOQA
         """
         Gets the latest event given a current event and some conditions/filters.
         Returns a tuple of (project_id, event_id)
 
         Arguments:
         event (Event): Event object
-        filter (Filter): Filter
+        snuba_filter (Filter): Filter
         """
         raise NotImplementedError
 
diff --git a/src/sentry/incidents/logic.py b/src/sentry/incidents/logic.py
index 4131141cce..edf4616919 100644
--- a/src/sentry/incidents/logic.py
+++ b/src/sentry/incidents/logic.py
@@ -320,11 +320,10 @@ def bulk_build_incident_query_params(incidents, start=None, end=None, windowed_s
         snuba_args = {
             "start": snuba_filter.start,
             "end": snuba_filter.end,
-            "conditions": snuba_filter.conditions,
+            "conditions": resolve_discover_aliases(snuba_filter)[0].conditions,
             "filter_keys": snuba_filter.filter_keys,
             "having": [],
         }
-        snuba_args["conditions"] = resolve_discover_aliases(snuba_args)[0]["conditions"]
         query_args_list.append(snuba_args)
 
     return query_args_list
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 4de60b22a3..2c4ee0d441 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -179,14 +179,15 @@ def find_histogram_buckets(field, params, conditions):
             found = True
     if not found:
         conditions.append(["event.type", "=", "transaction"])
-    translated_args, _ = resolve_discover_aliases({"conditions": conditions})
+    snuba_filter = eventstore.Filter(conditions=conditions)
+    translated_args, _ = resolve_discover_aliases(snuba_filter)
 
     results = raw_query(
         filter_keys={"project_id": params.get("project_id")},
         start=params.get("start"),
         end=params.get("end"),
         dataset=Dataset.Discover,
-        conditions=translated_args["conditions"],
+        conditions=translated_args.conditions,
         aggregations=[["max", "duration", alias]],
     )
     if len(results["data"]) != 1:
@@ -287,7 +288,7 @@ def resolve_complex_column(col):
             args[i] = resolve_column(args[i])
 
 
-def resolve_discover_aliases(snuba_args, function_translations=None):
+def resolve_discover_aliases(snuba_filter, function_translations=None):
     """
     Resolve the public schema aliases to the discover dataset.
 
@@ -295,7 +296,7 @@ def resolve_discover_aliases(snuba_args, function_translations=None):
     `translated_columns` key containing the selected fields that need to
     be renamed in the result set.
     """
-    resolved = deepcopy(snuba_args)
+    resolved = snuba_filter.clone()
     translated_columns = {}
     derived_columns = set()
     if function_translations:
@@ -303,7 +304,7 @@ def resolve_discover_aliases(snuba_args, function_translations=None):
             derived_columns.add(snuba_name)
             translated_columns[snuba_name] = sentry_name
 
-    selected_columns = resolved.get("selected_columns")
+    selected_columns = resolved.selected_columns
     if selected_columns:
         for (idx, col) in enumerate(selected_columns):
             if isinstance(col, (list, tuple)):
@@ -313,9 +314,9 @@ def resolve_discover_aliases(snuba_args, function_translations=None):
                 selected_columns[idx] = name
                 translated_columns[name] = col
 
-        resolved["selected_columns"] = selected_columns
+        resolved.selected_columns = selected_columns
 
-    groupby = resolved.get("groupby")
+    groupby = resolved.groupby
     if groupby:
         for (idx, col) in enumerate(groupby):
             name = col
@@ -326,25 +327,25 @@ def resolve_discover_aliases(snuba_args, function_translations=None):
                 name = resolve_column(col)
 
             groupby[idx] = name
-        resolved["groupby"] = groupby
+        resolved.groupby = groupby
 
-    aggregations = resolved.get("aggregations")
+    aggregations = resolved.aggregations
     for aggregation in aggregations or []:
         derived_columns.add(aggregation[2])
         if isinstance(aggregation[1], six.string_types):
             aggregation[1] = resolve_column(aggregation[1])
         elif isinstance(aggregation[1], (set, tuple, list)):
             aggregation[1] = [resolve_column(col) for col in aggregation[1]]
-    resolved["aggregations"] = aggregations
+    resolved.aggregations = aggregations
 
-    conditions = resolved.get("conditions")
+    conditions = resolved.conditions
     if conditions:
         for (i, condition) in enumerate(conditions):
             replacement = resolve_condition(condition, resolve_column)
             conditions[i] = replacement
-        resolved["conditions"] = [c for c in conditions if c]
+        resolved.conditions = [c for c in conditions if c]
 
-    orderby = resolved.get("orderby")
+    orderby = resolved.orderby
     if orderby:
         orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
         resolved_orderby = []
@@ -357,7 +358,7 @@ def resolve_discover_aliases(snuba_args, function_translations=None):
                     field if field in derived_columns else resolve_column(field),
                 )
             )
-        resolved["orderby"] = resolved_orderby
+        resolved.orderby = resolved_orderby
     return resolved, translated_columns
 
 
@@ -386,7 +387,7 @@ def zerofill(data, start, end, rollup, orderby):
     return rv
 
 
-def transform_results(result, translated_columns, snuba_args):
+def transform_results(result, translated_columns, snuba_filter):
     """
     Transform internal names back to the public schema ones.
 
@@ -403,10 +404,10 @@ def transform_results(result, translated_columns, snuba_args):
     if len(translated_columns):
         result["data"] = [get_row(row) for row in result["data"]]
 
-    rollup = snuba_args.get("rollup")
+    rollup = snuba_filter.rollup
     if rollup and rollup > 0:
         result["data"] = zerofill(
-            result["data"], snuba_args["start"], snuba_args["end"], rollup, snuba_args["orderby"]
+            result["data"], snuba_filter.start, snuba_filter.end, rollup, snuba_filter.orderby
         )
 
     for col in result["meta"]:
@@ -417,7 +418,7 @@ def transform_results(result, translated_columns, snuba_args):
                     result["data"] = zerofill_histogram(
                         result["data"],
                         result["meta"],
-                        snuba_args["orderby"],
+                        snuba_filter.orderby,
                         sentry_name,
                         snuba_name,
                     )
@@ -519,21 +520,8 @@ def query(
     query = transform_deprecated_functions_in_query(query)
 
     snuba_filter = get_filter(query, params)
-
-    # TODO(mark) Refactor the need for this translation shim once all of
-    # discover is using this module. Remember to update all the functions
-    # in this module.
-    snuba_args = {
-        "start": snuba_filter.start,
-        "end": snuba_filter.end,
-        "conditions": snuba_filter.conditions,
-        "filter_keys": snuba_filter.filter_keys,
-        "orderby": orderby,
-        "having": [],
-    }
-
-    if use_aggregate_conditions:
-        snuba_args["having"] = snuba_filter.having
+    if not use_aggregate_conditions:
+        snuba_filter.having = []
 
     # We need to run a separate query to be able to properly bucket the values for the histogram
     # Do that here, and format the bucket number in to the columns before passing it through
@@ -563,25 +551,23 @@ def query(
             ordering = "{}{}".format("-" if is_reversed else "", ordering)
             new_orderby.append(ordering)
 
-        snuba_args["orderby"] = new_orderby
+        snuba_filter.orderby = new_orderby
 
-    snuba_args.update(
-        resolve_field_list(selected_columns, snuba_args, params=params, auto_fields=auto_fields)
+    snuba_filter.update_with(
+        resolve_field_list(selected_columns, snuba_filter, auto_fields=auto_fields)
     )
 
     if reference_event:
         ref_conditions = create_reference_event_conditions(reference_event)
         if ref_conditions:
-            snuba_args["conditions"].extend(ref_conditions)
+            snuba_filter.conditions.extend(ref_conditions)
 
     # Resolve the public aliases into the discover dataset names.
-    snuba_args, translated_columns = resolve_discover_aliases(snuba_args, function_translations)
+    snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter, function_translations)
 
     # Make sure that any aggregate conditions are also in the selected columns
-    for having_clause in snuba_args.get("having"):
-        found = any(
-            having_clause[0] == agg_clause[-1] for agg_clause in snuba_args.get("aggregations")
-        )
+    for having_clause in snuba_filter.having:
+        found = any(having_clause[0] == agg_clause[-1] for agg_clause in snuba_filter.aggregations)
         if not found:
             raise InvalidSearchQuery(
                 u"Aggregate {} used in a condition but is not a selected column.".format(
@@ -590,25 +576,25 @@ def query(
             )
 
     if conditions is not None:
-        snuba_args["conditions"].extend(conditions)
+        snuba_filter.conditions.extend(conditions)
 
     result = raw_query(
-        start=snuba_args.get("start"),
-        end=snuba_args.get("end"),
-        groupby=snuba_args.get("groupby"),
-        conditions=snuba_args.get("conditions"),
-        aggregations=snuba_args.get("aggregations"),
-        selected_columns=snuba_args.get("selected_columns"),
-        filter_keys=snuba_args.get("filter_keys"),
-        having=snuba_args.get("having"),
-        orderby=snuba_args.get("orderby"),
+        start=snuba_filter.start,
+        end=snuba_filter.end,
+        groupby=snuba_filter.groupby,
+        conditions=snuba_filter.conditions,
+        aggregations=snuba_filter.aggregations,
+        selected_columns=snuba_filter.selected_columns,
+        filter_keys=snuba_filter.filter_keys,
+        having=snuba_filter.having,
+        orderby=snuba_filter.orderby,
         dataset=Dataset.Discover,
         limit=limit,
         offset=offset,
         referrer=referrer,
     )
 
-    return transform_results(result, translated_columns, snuba_args)
+    return transform_results(result, translated_columns, snuba_filter)
 
 
 def timeseries_query(selected_columns, query, params, rollup, reference_event=None, referrer=None):
@@ -639,38 +625,31 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
     query = transform_deprecated_functions_in_query(query)
 
     snuba_filter = get_filter(query, params)
-    snuba_args = {
-        "start": snuba_filter.start,
-        "end": snuba_filter.end,
-        "conditions": snuba_filter.conditions,
-        "filter_keys": snuba_filter.filter_keys,
-        "having": snuba_filter.having,
-    }
-    if not snuba_args["start"] and not snuba_args["end"]:
+    if not snuba_filter.start and not snuba_filter.end:
         raise InvalidSearchQuery("Cannot get timeseries result without a start and end.")
 
-    snuba_args.update(resolve_field_list(selected_columns, snuba_args, auto_fields=False))
+    snuba_filter.update_with(resolve_field_list(selected_columns, snuba_filter, auto_fields=False))
     if reference_event:
         ref_conditions = create_reference_event_conditions(reference_event)
         if ref_conditions:
-            snuba_args["conditions"].extend(ref_conditions)
+            snuba_filter.conditions.extend(ref_conditions)
 
     # Resolve the public aliases into the discover dataset names.
-    snuba_args, _ = resolve_discover_aliases(snuba_args)
-    if not snuba_args["aggregations"]:
+    snuba_filter, _ = resolve_discover_aliases(snuba_filter)
+    if not snuba_filter.aggregations:
         raise InvalidSearchQuery("Cannot get timeseries result with no aggregation.")
 
     # Change the alias of the first aggregation to count. This ensures compatibility
     # with other parts of the timeseries endpoint expectations
-    if len(snuba_args["aggregations"]) == 1:
-        snuba_args["aggregations"][0][2] = "count"
+    if len(snuba_filter.aggregations) == 1:
+        snuba_filter.aggregations[0][2] = "count"
 
     result = raw_query(
-        aggregations=snuba_args.get("aggregations"),
-        conditions=snuba_args.get("conditions"),
-        filter_keys=snuba_args.get("filter_keys"),
-        start=snuba_args.get("start"),
-        end=snuba_args.get("end"),
+        aggregations=snuba_filter.aggregations,
+        conditions=snuba_filter.conditions,
+        filter_keys=snuba_filter.filter_keys,
+        start=snuba_filter.start,
+        end=snuba_filter.end,
         rollup=rollup,
         orderby="time",
         groupby=["time"],
@@ -678,7 +657,7 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
         limit=10000,
         referrer=referrer,
     )
-    result = zerofill(result["data"], snuba_args["start"], snuba_args["end"], rollup, "time")
+    result = zerofill(result["data"], snuba_filter.start, snuba_filter.end, rollup, "time")
 
     return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
 
@@ -768,15 +747,8 @@ def get_facets(query, params, limit=10, referrer=None):
 
     snuba_filter = get_filter(query, params)
 
-    # TODO(mark) Refactor the need for this translation shim.
-    snuba_args = {
-        "start": snuba_filter.start,
-        "end": snuba_filter.end,
-        "conditions": snuba_filter.conditions,
-        "filter_keys": snuba_filter.filter_keys,
-    }
     # Resolve the public aliases into the discover dataset names.
-    snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
+    snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter)
 
     # Exclude tracing tags as they are noisy and generally not helpful.
     excluded_tags = ["tags_key", "NOT IN", ["trace", "trace.ctx", "trace.span", "project"]]
@@ -788,10 +760,10 @@ def get_facets(query, params, limit=10, referrer=None):
     # Get the most frequent tag keys
     key_names = raw_query(
         aggregations=[["count", None, "count"]],
-        start=snuba_args.get("start"),
-        end=snuba_args.get("end"),
-        conditions=snuba_args.get("conditions"),
-        filter_keys=snuba_args.get("filter_keys"),
+        start=snuba_filter.start,
+        end=snuba_filter.end,
+        conditions=snuba_filter.conditions,
+        filter_keys=snuba_filter.filter_keys,
         orderby=["-count", "tags_key"],
         groupby="tags_key",
         having=[excluded_tags],
@@ -822,10 +794,10 @@ def get_facets(query, params, limit=10, referrer=None):
     if fetch_projects:
         project_values = raw_query(
             aggregations=[["count", None, "count"]],
-            start=snuba_args.get("start"),
-            end=snuba_args.get("end"),
-            conditions=snuba_args.get("conditions"),
-            filter_keys=snuba_args.get("filter_keys"),
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            conditions=snuba_filter.conditions,
+            filter_keys=snuba_filter.filter_keys,
             groupby="project_id",
             orderby="-count",
             dataset=Dataset.Discover,
@@ -860,10 +832,10 @@ def get_facets(query, params, limit=10, referrer=None):
         tag = u"tags[{}]".format(tag_name)
         tag_values = raw_query(
             aggregations=[["count", None, "count"]],
-            conditions=snuba_args.get("conditions"),
-            start=snuba_args.get("start"),
-            end=snuba_args.get("end"),
-            filter_keys=snuba_args.get("filter_keys"),
+            conditions=snuba_filter.conditions,
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            filter_keys=snuba_filter.filter_keys,
             orderby=["-count"],
             groupby=[tag],
             limit=TOP_VALUES_DEFAULT_LIMIT,
@@ -881,14 +853,14 @@ def get_facets(query, params, limit=10, referrer=None):
         )
 
     if aggregate_tags:
-        conditions = snuba_args.get("conditions", [])
+        conditions = snuba_filter.conditions
         conditions.append(["tags_key", "IN", aggregate_tags])
         tag_values = raw_query(
             aggregations=[["count", None, "count"]],
             conditions=conditions,
-            start=snuba_args.get("start"),
-            end=snuba_args.get("end"),
-            filter_keys=snuba_args.get("filter_keys"),
+            start=snuba_filter.start,
+            end=snuba_filter.end,
+            filter_keys=snuba_filter.filter_keys,
             orderby=["tags_key", "-count"],
             groupby=["tags_key", "tags_value"],
             dataset=Dataset.Discover,
diff --git a/src/sentry/snuba/tasks.py b/src/sentry/snuba/tasks.py
index f62f96641c..7b295d1d78 100644
--- a/src/sentry/snuba/tasks.py
+++ b/src/sentry/snuba/tasks.py
@@ -103,9 +103,7 @@ def delete_subscription_from_snuba(query_subscription_id):
 
 
 def _create_in_snuba(subscription):
-    conditions = resolve_discover_aliases(
-        {"conditions": get_filter(subscription.query).conditions}
-    )[0]["conditions"]
+    conditions = resolve_discover_aliases(get_filter(subscription.query))[0].conditions
     environments = list(subscription.environments.all())
     if environments:
         conditions.append(["environment", "IN", [env.name for env in environments]])
diff --git a/tests/sentry/api/test_event_search.py b/tests/sentry/api/test_event_search.py
index 82eb5db585..1ec4784bd6 100644
--- a/tests/sentry/api/test_event_search.py
+++ b/tests/sentry/api/test_event_search.py
@@ -10,6 +10,7 @@ from sentry_relay.consts import SPAN_STATUS_CODE_TO_NAME
 from django.utils import timezone
 from freezegun import freeze_time
 
+from sentry import eventstore
 from sentry.api.event_search import (
     AggregateKey,
     event_search_grammar,
@@ -1242,12 +1243,12 @@ class ResolveFieldListTest(unittest.TestCase):
     def test_non_string_field_error(self):
         fields = [["any", "thing", "lol"]]
         with pytest.raises(InvalidSearchQuery) as err:
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "Field names" in six.text_type(err)
 
     def test_automatic_fields_no_aggregates(self):
         fields = ["event.type", "message"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         assert result["selected_columns"] == ["event.type", "message", "id", "project.id"]
         assert result["aggregations"] == [
             ["transform(project_id, array(), array(), '')", None, "project.name"]
@@ -1265,7 +1266,7 @@ class ResolveFieldListTest(unittest.TestCase):
             "percentile(transaction.duration, 0.95)",
             "percentile(transaction.duration, 0.99)",
         ]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
 
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1288,7 +1289,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_field_alias_expansion(self):
         fields = ["title", "last_seen()", "latest_event()", "project", "issue", "user", "message"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         assert result["selected_columns"] == [
             "title",
             "issue.id",
@@ -1317,7 +1318,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_aggregate_function_expansion(self):
         fields = ["count_unique(user)", "count(id)", "min(timestamp)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         # Automatic fields should be inserted, count() should have its column dropped.
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1332,7 +1333,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_count_function_expansion(self):
         fields = ["count(id)", "count(user)", "count(transaction.duration)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         # Automatic fields should be inserted, count() should have its column dropped.
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1347,7 +1348,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_aggregate_function_dotted_argument(self):
         fields = ["count_unique(user.id)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         assert result["aggregations"] == [
             ["uniq", "user.id", "count_unique_user_id"],
             ["argMax", ["id", "timestamp"], "latest_event"],
@@ -1358,19 +1359,19 @@ class ResolveFieldListTest(unittest.TestCase):
     def test_aggregate_function_invalid_name(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["derp(user)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "derp(user) is not a valid function" in six.text_type(err)
 
     def test_aggregate_function_case_sensitive(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["MAX(user)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "MAX(user) is not a valid function" in six.text_type(err)
 
     def test_aggregate_function_invalid_column(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["min(message)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "InvalidSearchQuery: min(message): column argument invalid: message is not a numeric column"
             in six.text_type(err)
@@ -1378,7 +1379,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_percentile_function(self):
         fields = ["percentile(transaction.duration, 0.75)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
 
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1391,17 +1392,17 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["percentile(0.75)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "percentile(0.75): expected 2 arguments" in six.text_type(err)
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["percentile(0.75,)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "percentile(0.75,): expected 2 arguments" in six.text_type(err)
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["percentile(sanchez, 0.75)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "percentile(sanchez, 0.75): column argument invalid: sanchez is not a valid column"
             in six.text_type(err)
@@ -1409,7 +1410,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["percentile(id, 0.75)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "percentile(id, 0.75): column argument invalid: id is not a duration column"
             in six.text_type(err)
@@ -1417,7 +1418,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["percentile(transaction.duration, 75)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "percentile(transaction.duration, 75): percentile argument invalid: 75 must be less than 1"
             in six.text_type(err)
@@ -1425,7 +1426,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_rpm_function(self):
         fields = ["rpm(3600)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
             ["divide(count(), divide(3600, 60))", None, "rpm_3600"],
@@ -1437,7 +1438,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["rpm(30)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "rpm(30): interval argument invalid: 30 must be greater than or equal to 60"
             in six.text_type(err)
@@ -1445,19 +1446,19 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["rpm()"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "rpm(): invalid arguments: function called without default" in six.text_type(err)
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["rpm()"]
-            resolve_field_list(fields, {}, params={"start": "abc", "end": "def"})
+            resolve_field_list(fields, eventstore.Filter(start="abc", end="def"))
         assert "rpm(): invalid arguments: function called with invalid default" in six.text_type(
             err
         )
 
         fields = ["rpm()"]
         result = resolve_field_list(
-            fields, {}, params={"start": before_now(hours=2), "end": before_now(hours=1)}
+            fields, eventstore.Filter(start=before_now(hours=2), end=before_now(hours=1))
         )
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1470,7 +1471,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_rps_function(self):
         fields = ["rps(3600)"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
 
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
@@ -1483,7 +1484,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["rps(0)"]
-            result = resolve_field_list(fields, {})
+            result = resolve_field_list(fields, eventstore.Filter())
         assert (
             "rps(0): interval argument invalid: 0 must be greater than or equal to 1"
             in six.text_type(err)
@@ -1491,7 +1492,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_histogram_function(self):
         fields = ["histogram(transaction.duration, 10, 1000)", "count()"]
-        result = resolve_field_list(fields, {})
+        result = resolve_field_list(fields, eventstore.Filter())
         assert result["selected_columns"] == [
             [
                 "multiply",
@@ -1509,7 +1510,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["histogram(stack.colno, 10, 1000)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "histogram(stack.colno, 10, 1000): column argument invalid: stack.colno is not a duration column"
             in six.text_type(err)
@@ -1517,12 +1518,12 @@ class ResolveFieldListTest(unittest.TestCase):
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["histogram(transaction.duration, 10)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert "histogram(transaction.duration, 10): expected 3 arguments" in six.text_type(err)
 
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["histogram(transaction.duration, 1000, 1000)"]
-            resolve_field_list(fields, {})
+            resolve_field_list(fields, eventstore.Filter())
         assert (
             "histogram(transaction.duration, 1000, 1000): num_buckets argument invalid: 1000 must be less than 500"
             in six.text_type(err)
@@ -1531,14 +1532,12 @@ class ResolveFieldListTest(unittest.TestCase):
     def test_rollup_with_unaggregated_fields(self):
         with pytest.raises(InvalidSearchQuery) as err:
             fields = ["message"]
-            snuba_args = {"rollup": 15}
-            resolve_field_list(fields, snuba_args)
+            resolve_field_list(fields, eventstore.Filter(rollup=15))
         assert "rollup without an aggregate" in six.text_type(err)
 
     def test_rollup_with_basic_and_aggregated_fields(self):
         fields = ["message", "count()"]
-        snuba_args = {"rollup": 15}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(rollup=15))
 
         assert result["aggregations"] == [["count", None, "count"]]
         assert result["selected_columns"] == ["message"]
@@ -1546,23 +1545,20 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_rollup_with_aggregated_fields(self):
         fields = ["count_unique(user)"]
-        snuba_args = {"rollup": 15}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(rollup=15))
         assert result["aggregations"] == [["uniq", "user", "count_unique_user"]]
         assert result["selected_columns"] == []
         assert result["groupby"] == []
 
     def test_orderby_unselected_field(self):
         fields = ["message"]
-        snuba_args = {"orderby": "timestamp"}
         with pytest.raises(InvalidSearchQuery) as err:
-            resolve_field_list(fields, snuba_args)
+            resolve_field_list(fields, eventstore.Filter(orderby="timestamp"))
         assert "Cannot order" in six.text_type(err)
 
     def test_orderby_basic_field(self):
         fields = ["message"]
-        snuba_args = {"orderby": "-message"}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(orderby="-message"))
         assert result["selected_columns"] == ["message", "id", "project.id"]
         assert result["aggregations"] == [
             ["transform(project_id, array(), array(), '')", None, "project.name"]
@@ -1571,8 +1567,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_orderby_field_aggregate(self):
         fields = ["count(id)", "count_unique(user)"]
-        snuba_args = {"orderby": "-count(id)"}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(orderby="-count(id)"))
         assert result["orderby"] == ["-count_id"]
         assert result["aggregations"] == [
             ["count", None, "count_id"],
@@ -1585,8 +1580,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_orderby_issue_alias(self):
         fields = ["issue"]
-        snuba_args = {"orderby": "-issue"}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(orderby="-issue"))
         assert result["orderby"] == ["-issue.id"]
         assert result["selected_columns"] == ["issue.id", "id", "project.id"]
         assert result["aggregations"] == [
@@ -1596,8 +1590,7 @@ class ResolveFieldListTest(unittest.TestCase):
 
     def test_orderby_project_alias(self):
         fields = ["project"]
-        snuba_args = {"orderby": "-project"}
-        result = resolve_field_list(fields, snuba_args)
+        result = resolve_field_list(fields, eventstore.Filter(orderby="-project"))
         assert result["orderby"] == ["-project"]
         assert result["aggregations"] == [
             ["transform(project_id, array(), array(), '')", None, "project"]
