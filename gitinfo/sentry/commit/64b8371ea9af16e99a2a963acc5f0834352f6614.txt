commit 64b8371ea9af16e99a2a963acc5f0834352f6614
Author: Matt Robenolt <matt@ydekproductions.com>
Date:   Mon Jan 2 12:26:50 2017 -0800

    cleanup: actually implement concurrency
    
    We've had a flag for `--concurrency` that did absoltuely nothing. This
    implements deletions based on sharding across primary keys and will spin
    up a thread per shard to work in parallel.
    
    The normal bulk deletions are slow, and the bottleneck isn't solely on
    the database. Bulk deletions also need to hit the nodestore, so
    parallelizing here is an easy win, as well as spreads load across
    multiple cores across the database.

diff --git a/CHANGES b/CHANGES
index 0a9e6c147a..bfeb9ffb7b 100644
--- a/CHANGES
+++ b/CHANGES
@@ -1,8 +1,9 @@
 Version 8.15 (Unreleased)
 -------------------------
-- Added overview for a release to view a breakdown of files changes, commit authors, new issues, and issues resolved
 
+- Added overview for a release to view a breakdown of files changes, commit authors, new issues, and issues resolved
 - Refactor usage of ``sentry.app`` to use individual modules.
+- Implemented ``--concurrency` on `sentry cleanup`
 
 API Changes
 ~~~~~~~~~~~
diff --git a/src/sentry/db/deletion.py b/src/sentry/db/deletion.py
index 6c5a40fb0f..721e602ba5 100644
--- a/src/sentry/db/deletion.py
+++ b/src/sentry/db/deletion.py
@@ -69,12 +69,38 @@ class BulkDeleteQuery(object):
             else:
                 qs = qs.filter(project_id=self.project_id)
 
+        return self._continuous_generic_query(qs, chunk_size)
+
+    def execute_sharded(self, total_shards, shard_id, chunk_size=100):
+        assert total_shards > 1
+        assert shard_id < total_shards
+        qs = self.model.objects.all().extra(where=[
+            'id %% {total_shards} = {shard_id}'.format(
+                total_shards=total_shards,
+                shard_id=shard_id,
+            )
+        ])
+
+        if self.days:
+            cutoff = timezone.now() - timedelta(days=self.days)
+            qs = qs.filter(
+                **{'{}__lte'.format(self.dtfield): cutoff}
+            )
+        if self.project_id:
+            if 'project' in self.model._meta.get_all_field_names():
+                qs = qs.filter(project=self.project_id)
+            else:
+                qs = qs.filter(project_id=self.project_id)
+
+        return self._continuous_generic_query(qs, chunk_size)
+
+    def _continuous_generic_query(self, query, chunk_size):
         # XXX: we step through because the deletion collector will pull all
         # relations into memory
         exists = True
         while exists:
             exists = False
-            for item in qs[:chunk_size].iterator():
+            for item in query[:chunk_size].iterator():
                 item.delete()
                 exists = True
 
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index 731bf8221f..00006b8ccc 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -48,6 +48,11 @@ def cleanup(days, project, concurrency, silent, model):
     done with the `--project` flag which accepts a project ID or a string
     with the form `org/project` where both are slugs.
     """
+    if concurrency < 1:
+        click.echo('Error: Minimum concurrency is 1', err=True)
+        raise click.Abort()
+
+    from threading import Thread
     from sentry.app import nodestore
     from sentry.db.deletion import BulkDeleteQuery
     from sentry.models import (
@@ -160,12 +165,23 @@ def cleanup(days, project, concurrency, silent, model):
             if not silent:
                 click.echo('>> Skipping %s' % model.__name__)
         else:
-            BulkDeleteQuery(
+            query = BulkDeleteQuery(
                 model=model,
                 dtfield=dtfield,
                 days=days,
                 project_id=project_id,
-            ).execute_generic()
+            )
+            if concurrency > 1:
+                threads = []
+                for shard_id in range(concurrency):
+                    t = Thread(target=lambda shard_id=shard_id: query.execute_sharded(concurrency, shard_id))
+                    t.start()
+                    threads.append(t)
+
+                for t in threads:
+                    t.join()
+            else:
+                query.execute_generic()
 
 
 def cleanup_unused_files(quiet=False):
