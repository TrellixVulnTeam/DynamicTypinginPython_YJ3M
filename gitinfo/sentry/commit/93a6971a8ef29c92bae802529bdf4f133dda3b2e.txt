commit 93a6971a8ef29c92bae802529bdf4f133dda3b2e
Author: Matt Robenolt <matt@ydekproductions.com>
Date:   Mon Jul 17 15:05:27 2017 -0700

    nodestore: support bulk cleanup through Riak TTLs
    
    Optimize cleanup to explicitly skip deleting node objects when managed
    by a larger/bulk cleanup. This makes the assumption that if
    `Nodestore.cleanup()` completes successfully, we can safely skip
    deleting on a per-object level.

diff --git a/src/sentry/db/deletion.py b/src/sentry/db/deletion.py
index e281d98402..9d96feeb22 100644
--- a/src/sentry/db/deletion.py
+++ b/src/sentry/db/deletion.py
@@ -8,16 +8,19 @@ from django.db import connections, router
 from django.utils import timezone
 
 from sentry.utils import db
+from sentry.db.models.fields.node import NodeField
 
 
 class BulkDeleteQuery(object):
-    def __init__(self, model, project_id=None, dtfield=None, days=None, order_by=None):
+    def __init__(self, model, project_id=None, dtfield=None, days=None,
+                 order_by=None, skip_nodestore=False):
         self.model = model
         self.project_id = int(project_id) if project_id else None
         self.dtfield = dtfield
         self.days = int(days) if days is not None else None
         self.order_by = order_by
         self.using = router.db_for_write(model)
+        self.skip_nodestore = skip_nodestore
 
     def execute_postgres(self, chunk_size=10000):
         quote_name = connections[self.using].ops.quote_name
@@ -97,12 +100,24 @@ class BulkDeleteQuery(object):
         return qs
 
     def _continuous_generic_query(self, query, chunk_size):
+        # Detect which fields, if any, should be ignored
+        # from nodestore deletion.
+        node_fields = []
+        if self.skip_nodestore:
+            for f in self.model._meta.fields:
+                if isinstance(f, NodeField):
+                    node_fields.append(f.name)
         # XXX: we step through because the deletion collector will pull all
         # relations into memory
         exists = True
         while exists:
             exists = False
             for item in query[:chunk_size].iterator():
+                # Setting nodestore ids to None will prevent
+                # deletion. This is used in the case when nodestore
+                # has it's own cleanup.
+                for f in node_fields:
+                    getattr(item, f).id = None
                 item.delete()
                 exists = True
 
diff --git a/src/sentry/nodestore/riak/backend.py b/src/sentry/nodestore/riak/backend.py
index 6330a7f261..ed9008c81e 100644
--- a/src/sentry/nodestore/riak/backend.py
+++ b/src/sentry/nodestore/riak/backend.py
@@ -46,7 +46,8 @@ class RiakNodeStorage(NodeStorage):
         max_retries=3,
         multiget_pool_size=5,
         tcp_keepalive=True,
-        protocol=None
+        protocol=None,
+        automatic_expiry=False
     ):
         # protocol being defined is useless, but is needed for backwards
         # compatability and leveraged as an opportunity to yell at the user
@@ -63,6 +64,7 @@ class RiakNodeStorage(NodeStorage):
             cooldown=cooldown,
             tcp_keepalive=tcp_keepalive,
         )
+        self.automatic_expiry = automatic_expiry
 
     def set(self, id, data):
         self.conn.put(self.bucket, id, json_dumps(data), returnbody='false')
@@ -95,6 +97,5 @@ class RiakNodeStorage(NodeStorage):
         return results
 
     def cleanup(self, cutoff_timestamp):
-        # TODO(dcramer): we should either index timestamps or have this run
-        # a map/reduce (probably the latter)
-        raise NotImplementedError
+        if not self.automatic_expiry:
+            raise NotImplementedError
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index ff7e532ac5..42dc5a0095 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -192,6 +192,8 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
         (models.Group, 'last_seen', 'last_seen'),
     )
 
+    skip_nodestore = False
+
     if not silent:
         click.echo('Removing expired values for LostPasswordHash')
 
@@ -235,6 +237,7 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
         cutoff = timezone.now() - timedelta(days=days)
         try:
             nodestore.cleanup(cutoff)
+            skip_nodestore = True
         except NotImplementedError:
             click.echo(
                 "NodeStore backend does not support cleanup operation", err=True)
@@ -264,6 +267,7 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
                 days=days,
                 project_id=project_id,
                 order_by=order_by,
+                skip_nodestore=skip_nodestore,
             ).execute(chunk_size=chunk_size)
 
     for model, dtfield, order_by in DELETES:
