commit a7f8f17e1c54582e61304dfb6f897d49066ebd81
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Mon Apr 23 14:30:24 2018 -0500

    feat(snuba): Add SnubaSearchBackend (#7925)

diff --git a/.gitignore b/.gitignore
index 68f4865f4e..87fdab11a3 100644
--- a/.gitignore
+++ b/.gitignore
@@ -40,3 +40,4 @@ Gemfile.lock
 coverage.xml
 junit.xml
 *.codestyle.xml
+.pytest_cache/
diff --git a/.travis.yml b/.travis.yml
index f64b749c55..2226e82a29 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -128,7 +128,7 @@ matrix:
         - redis-server
         - postgresql
     - python: 2.7
-      env: TEST_SUITE=snuba SNUBA=http://localhost:8000
+      env: TEST_SUITE=snuba SENTRY_TAGSTORE=sentry.tagstore.snuba.SnubaTagStorage SNUBA=http://localhost:8000
       services:
         - docker
         - memcached
@@ -140,7 +140,7 @@ matrix:
         - docker ps -a
   allow_failures:
     - python: 2.7
-      env: TEST_SUITE=snuba SNUBA=http://localhost:8000
+      env: TEST_SUITE=snuba SENTRY_TAGSTORE=sentry.tagstore.snuba.SnubaTagStorage SNUBA=http://localhost:8000
 notifications:
   webhooks:
     urls:
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 160d508c0a..c95ec69a25 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -957,7 +957,7 @@ SENTRY_TAGSTORE_OPTIONS = (
 )
 
 # Search backend
-SENTRY_SEARCH = 'sentry.search.django.DjangoSearchBackend'
+SENTRY_SEARCH = os.environ.get('SENTRY_SEARCH', 'sentry.search.django.DjangoSearchBackend')
 SENTRY_SEARCH_OPTIONS = {}
 # SENTRY_SEARCH_OPTIONS = {
 #     'urls': ['http://localhost:9200/'],
diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index d81f69340d..1be3b1ede0 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -47,6 +47,7 @@ from sentry.stacktraces import normalize_in_app
 
 HASH_RE = re.compile(r'^[0-9a-f]{32}$')
 DEFAULT_FINGERPRINT_VALUES = frozenset(['{{ default }}', '{{default}}'])
+ALLOWED_FUTURE_DELTA = timedelta(minutes=1)
 
 
 def count_limit(count):
@@ -221,7 +222,7 @@ def process_timestamp(value, current_datetime=None):
     if current_datetime is None:
         current_datetime = datetime.now()
 
-    if value > current_datetime + timedelta(minutes=1):
+    if value > current_datetime + ALLOWED_FUTURE_DELTA:
         raise InvalidTimestamp(EventError.FUTURE_TIMESTAMP)
 
     if value < current_datetime - timedelta(days=30):
diff --git a/src/sentry/search/django/backend.py b/src/sentry/search/django/backend.py
index 1ef8817f8b..6b2da95ebd 100644
--- a/src/sentry/search/django/backend.py
+++ b/src/sentry/search/django/backend.py
@@ -206,8 +206,8 @@ def get_latest_release(project, environment):
 class DjangoSearchBackend(SearchBackend):
     def query(self, project, tags=None, environment=None, sort_by='date', limit=100,
               cursor=None, count_hits=False, paginator_options=None, **parameters):
-        from sentry.models import (Environment, Event, Group, GroupEnvironment,
-                                   GroupStatus, GroupSubscription, Release)
+
+        from sentry.models import Group, GroupStatus, GroupSubscription, Release
 
         if paginator_options is None:
             paginator_options = {}
@@ -272,13 +272,26 @@ class DjangoSearchBackend(SearchBackend):
         retention = quotas.get_event_retention(organization=project.organization)
         if retention:
             retention_window_start = timezone.now() - timedelta(days=retention)
-            # TODO: This could be optimized when building querysets to identify
-            # criteria that are logically impossible (e.g. if the upper bound
-            # for last seen is before the retention window starts, no results
-            # exist.)
-            group_queryset = group_queryset.filter(last_seen__gte=retention_window_start)
         else:
             retention_window_start = None
+        # TODO: This could be optimized when building querysets to identify
+        # criteria that are logically impossible (e.g. if the upper bound
+        # for last seen is before the retention window starts, no results
+        # exist.)
+        if retention_window_start:
+            group_queryset = group_queryset.filter(last_seen__gte=retention_window_start)
+
+        # This is a punt because the SnubaSearchBackend (a subclass) shares so much that it
+        # seemed better to handle all the shared initialization and then handoff to the
+        # actual backend.
+        return self._query(project, retention_window_start, group_queryset, tags,
+                           environment, sort_by, limit, cursor, count_hits,
+                           paginator_options, **parameters)
+
+    def _query(self, project, retention_window_start, group_queryset, tags, environment,
+               sort_by, limit, cursor, count_hits, paginator_options, **parameters):
+
+        from sentry.models import (Group, Environment, Event, GroupEnvironment, Release)
 
         if environment is not None:
             if 'environment' in tags:
@@ -293,13 +306,14 @@ class DjangoSearchBackend(SearchBackend):
                 'date_from': ScalarCondition('date_added', 'gt'),
                 'date_to': ScalarCondition('date_added', 'lt'),
             })
+
             if any(key in parameters for key in event_queryset_builder.conditions.keys()):
                 event_queryset = event_queryset_builder.build(
                     tagstore.get_event_tag_qs(
-                        project.id,
-                        environment.id,
-                        'environment',
-                        environment.name,
+                        project_id=project.id,
+                        environment_id=environment.id,
+                        key='environment',
+                        value=environment.name,
                     ),
                     parameters,
                 )
@@ -414,16 +428,17 @@ class DjangoSearchBackend(SearchBackend):
             get_sort_expression, sort_value_to_cursor_value = environment_sort_strategies[sort_by]
 
             group_tag_value_queryset = tagstore.get_group_tag_value_qs(
-                project.id,
-                set(group_queryset.values_list('id', flat=True)),  # TODO: Limit?,
-                environment.id,
-                'environment',
-                environment.name,
+                project_id=project.id,
+                group_id=set(group_queryset.values_list('id', flat=True)),  # TODO: Limit?,
+                environment_id=environment.id,
+                key='environment',
+                value=environment.name,
             )
 
             if retention_window_start is not None:
                 group_tag_value_queryset = group_tag_value_queryset.filter(
-                    last_seen__gte=retention_window_start)
+                    last_seen__gte=retention_window_start
+                )
 
             candidates = dict(
                 QuerySetBuilder({
@@ -451,10 +466,10 @@ class DjangoSearchBackend(SearchBackend):
                 # utilize the retention window start parameter for additional
                 # optimizations.
                 matches = tagstore.get_group_ids_for_search_filter(
-                    project.id,
-                    environment.id,
-                    tags,
-                    candidates.keys(),
+                    project_id=project.id,
+                    environment_id=environment.id,
+                    tags=tags,
+                    candidates=candidates.keys(),
                     limit=len(candidates),
                 )
                 for key in set(candidates) - set(matches or []):
@@ -475,6 +490,7 @@ class DjangoSearchBackend(SearchBackend):
                 'date_from': ScalarCondition('datetime', 'gt'),
                 'date_to': ScalarCondition('datetime', 'lt'),
             })
+
             if any(key in parameters for key in event_queryset_builder.conditions.keys()):
                 group_queryset = group_queryset.filter(
                     id__in=list(
@@ -511,9 +527,15 @@ class DjangoSearchBackend(SearchBackend):
             )
 
             if tags:
-                matches = tagstore.get_group_ids_for_search_filter(project.id, None, tags)
-                if matches:
-                    group_queryset = group_queryset.filter(id__in=matches)
+                group_ids = tagstore.get_group_ids_for_search_filter(
+                    project_id=project.id,
+                    environment_id=None,
+                    tags=tags,
+                    candidates=None,
+                )
+
+                if group_ids:
+                    group_queryset = group_queryset.filter(id__in=group_ids)
                 else:
                     group_queryset = group_queryset.none()
 
diff --git a/src/sentry/search/snuba/__init__.py b/src/sentry/search/snuba/__init__.py
new file mode 100644
index 0000000000..1bd374084e
--- /dev/null
+++ b/src/sentry/search/snuba/__init__.py
@@ -0,0 +1,3 @@
+from __future__ import absolute_import, print_function
+
+from .backend import *  # NOQA
diff --git a/src/sentry/search/snuba/backend.py b/src/sentry/search/snuba/backend.py
new file mode 100644
index 0000000000..42e11bf1b0
--- /dev/null
+++ b/src/sentry/search/snuba/backend.py
@@ -0,0 +1,367 @@
+from __future__ import absolute_import
+
+import six
+
+import logging
+import math
+import pytz
+from collections import defaultdict
+from datetime import timedelta, datetime
+
+from django.utils import timezone
+
+from sentry.api.paginator import SequencePaginator
+from sentry.event_manager import ALLOWED_FUTURE_DELTA
+from sentry.models import Release, Group, GroupEnvironment, GroupHash
+from sentry.search.django import backend as ds
+from sentry.utils import snuba
+from sentry.utils.dates import to_timestamp
+
+
+logger = logging.getLogger('sentry.search.snuba')
+
+
+# https://github.com/getsentry/sentry/blob/804c85100d0003cfdda91701911f21ed5f66f67c/src/sentry/event_manager.py#L241-L271
+priority_expr = 'toUInt32(log(times_seen) * 600) + toUInt32(last_seen)'
+
+
+datetime_format = '%Y-%m-%dT%H:%M:%S+00:00'
+
+
+# TODO: Would be nice if this was handled in the Snuba abstraction, but that
+# would require knowledge of which fields are datetimes
+def snuba_str_to_datetime(d):
+    if not isinstance(d, datetime):
+        d = datetime.strptime(d, datetime_format)
+
+    if not d.tzinfo:
+        d = d.replace(tzinfo=pytz.utc)
+
+    return d
+
+
+def calculate_priority_cursor(data):
+    times_seen = sum(data['times_seen'])
+    last_seen = max(int(to_timestamp(snuba_str_to_datetime(d)) * 1000) for d in data['last_seen'])
+    return ((math.log(times_seen) * 600) + last_seen)
+
+
+def _datetime_cursor_calculator(field, fn):
+    def calculate(data):
+        datetime = fn(snuba_str_to_datetime(d) for d in data[field])
+        return int(to_timestamp(datetime) * 1000)
+
+    return calculate
+
+
+sort_strategies = {
+    # sort_by -> Tuple[
+    #   String: expression to generate sort value (of type T, used below),
+    #   Function[T] -> int: function for converting a group's data to a cursor value),
+    # ]
+    'priority': (
+        '-priority', calculate_priority_cursor,
+    ),
+    'date': (
+        '-last_seen', _datetime_cursor_calculator('last_seen', max),
+    ),
+    'new': (
+        '-first_seen', _datetime_cursor_calculator('first_seen', min),
+    ),
+    'freq': (
+        '-times_seen', lambda data: sum(data['times_seen']),
+    ),
+}
+
+
+class SnubaConditionBuilder(object):
+    """\
+    Constructions a Snuba conditions list from a ``parameters`` mapping.
+
+    ``Condition`` objects are registered by their parameter name and used to
+    construct the Snuba condition list if they are present in the ``parameters``
+    mapping.
+    """
+
+    def __init__(self, conditions):
+        self.conditions = conditions
+
+    def build(self, parameters):
+        result = []
+        for name, condition in self.conditions.items():
+            if name in parameters:
+                result.append(condition.apply(name, parameters))
+        return result
+
+
+class Condition(object):
+    """\
+    Adds a single condition to a Snuba conditions list. Used with
+    ``SnubaConditionBuilder``.
+    """
+
+    def apply(self, name, parameters):
+        raise NotImplementedError
+
+
+class CallbackCondition(Condition):
+    def __init__(self, callback):
+        self.callback = callback
+
+    def apply(self, name, parameters):
+        return self.callback(parameters[name])
+
+
+class ScalarCondition(Condition):
+    """\
+    Adds a scalar filter (less than or greater than are supported) to a Snuba
+    condition list. Whether or not the filter is inclusive is defined by the
+    '{parameter_name}_inclusive' parameter.
+    """
+
+    def __init__(self, field, operator, default_inclusivity=True):
+        assert operator in ['<', '>']
+        self.field = field
+        self.operator = operator
+        self.default_inclusivity = default_inclusivity
+
+    def apply(self, name, parameters):
+        inclusive = parameters.get(
+            '{}_inclusive'.format(name),
+            self.default_inclusivity,
+        )
+
+        arg = parameters[name]
+        if isinstance(arg, datetime):
+            arg = int(to_timestamp(arg))
+
+        return (
+            self.field,
+            self.operator + ('=' if inclusive else ''),
+            arg
+        )
+
+
+class SnubaSearchBackend(ds.DjangoSearchBackend):
+    def _query(self, project, retention_window_start, group_queryset, tags, environment,
+               sort_by, limit, cursor, count_hits, paginator_options, **parameters):
+
+        # TODO: Product decision: we currently search Group.message to handle
+        # the `query` parameter, because that's what we've always done. We could
+        # do that search against every event in Snuba instead, but results may
+        # differ.
+
+        now = timezone.now()
+        end = parameters.get('date_to') or (now + ALLOWED_FUTURE_DELTA)
+        # TODO: Presumably we want to search back to the project's full retention,
+        #       which may be higher than 90 days in the future, but apparently
+        #       `retention_window_start` can be None?
+        start = max(
+            filter(None, [
+                retention_window_start,
+                parameters.get('date_from'),
+                now - timedelta(days=90)
+            ])
+        )
+        assert start < end
+
+        # TODO: It's possible `first_release` could be handled by Snuba.
+        if environment is not None:
+            group_queryset = ds.QuerySetBuilder({
+                'first_release': ds.CallbackCondition(
+                    lambda queryset, version: queryset.extra(
+                        where=[
+                            '{} = {}'.format(
+                                ds.get_sql_column(GroupEnvironment, 'first_release_id'),
+                                ds.get_sql_column(Release, 'id'),
+                            ),
+                            '{} = %s'.format(
+                                ds.get_sql_column(Release, 'organization'),
+                            ),
+                            '{} = %s'.format(
+                                ds.get_sql_column(Release, 'version'),
+                            ),
+                        ],
+                        params=[project.organization_id, version],
+                        tables=[Release._meta.db_table],
+                    ),
+                ),
+            }).build(
+                group_queryset.extra(
+                    where=[
+                        '{} = {}'.format(
+                            ds.get_sql_column(Group, 'id'),
+                            ds.get_sql_column(GroupEnvironment, 'group_id'),
+                        ),
+                        '{} = %s'.format(
+                            ds.get_sql_column(GroupEnvironment, 'environment_id'),
+                        ),
+                    ],
+                    params=[environment.id],
+                    tables=[GroupEnvironment._meta.db_table],
+                ),
+                parameters,
+            )
+        else:
+            group_queryset = ds.QuerySetBuilder({
+                'first_release': ds.CallbackCondition(
+                    lambda queryset, version: queryset.filter(
+                        first_release__organization_id=project.organization_id,
+                        first_release__version=version,
+                    ),
+                ),
+            }).build(
+                group_queryset,
+                parameters,
+            )
+
+        # TODO: If the query didn't include anything to significantly filter
+        # down the number of groups at this point ('first_release', 'query',
+        # 'status', 'bookmarked_by', 'assigned_to', 'unassigned',
+        # 'subscribed_by', 'active_at_from', or 'active_at_to') then this
+        # queryset might return a *huge* number of groups. In this case, we
+        # probably *don't* want to pass candidates down to Snuba, and rather we
+        # want Snuba to do all the filtering/sorting it can and *then* apply
+        # this queryset to the results from Snuba.
+        #
+        # However, if this did filter down the number of groups significantly,
+        # then passing in candidates is, of course, valuable.
+        #
+        # Should we decide which way to handle it based on the number of
+        # group_ids, the number of hashes? Or should we just always start the
+        # query with Snuba? Something else?
+        candidate_group_ids = list(group_queryset.values_list('id', flat=True))
+
+        sort_expression, calculate_cursor_for_group = sort_strategies[sort_by]
+
+        group_data = do_search(
+            project_id=project.id,
+            environment_id=environment and environment.id,
+            tags=tags,
+            start=start,
+            end=end,
+            sort=sort_expression,
+            candidates=candidate_group_ids,
+            **parameters
+        )
+
+        group_to_score = {}
+        for group_id, data in group_data.items():
+            group_to_score[group_id] = calculate_cursor_for_group(data)
+
+        paginator_results = SequencePaginator(
+            [(score, id) for (id, score) in group_to_score.items()],
+            reverse=True,
+            **paginator_options
+        ).get_result(limit, cursor, count_hits=count_hits)
+
+        groups = Group.objects.in_bulk(paginator_results.results)
+        paginator_results.results = [groups[k] for k in paginator_results.results if k in groups]
+
+        return paginator_results
+
+
+def do_search(project_id, environment_id, tags, start, end,
+              sort, candidates=None, limit=1000, **parameters):
+    from sentry.search.base import ANY
+
+    filters = {
+        'project_id': [project_id],
+    }
+
+    if environment_id is not None:
+        filters['environment'] = [environment_id]
+
+    if candidates is not None:
+        hashes = list(
+            GroupHash.objects.filter(
+                group_id__in=candidates
+            ).values_list(
+                'hash', flat=True
+            ).distinct()
+        )
+
+        if not hashes:
+            return {}
+
+        filters['primary_hash'] = hashes
+
+    having = SnubaConditionBuilder({
+        'age_from': ScalarCondition('first_seen', '>'),
+        'age_to': ScalarCondition('first_seen', '<'),
+        'last_seen_from': ScalarCondition('last_seen', '>'),
+        'last_seen_to': ScalarCondition('last_seen', '<'),
+        'times_seen': CallbackCondition(
+            lambda times_seen: ('times_seen', '=', times_seen),
+        ),
+        'times_seen_lower': ScalarCondition('times_seen', '>'),
+        'times_seen_upper': ScalarCondition('times_seen', '<'),
+    }).build(parameters)
+
+    conditions = []
+    for tag, val in six.iteritems(tags):
+        col = 'tags[{}]'.format(tag)
+        if val == ANY:
+            conditions.append((col, '!=', ''))
+        else:
+            conditions.append((col, '=', val))
+
+    aggregations = [
+        ['count()', '', 'times_seen'],
+        ['min', 'timestamp', 'first_seen'],
+        ['max', 'timestamp', 'last_seen'],
+        [priority_expr, '', 'priority']
+    ]
+
+    # {hash -> {times_seen -> int
+    #           first_seen -> date_str,
+    #           last_seen -> date_str,
+    #           priority -> int},
+    #  ...}
+    snuba_results = snuba.query(
+        start=start,
+        end=end,
+        groupby=['primary_hash'],
+        conditions=conditions,
+        having=having,
+        filter_keys=filters,
+        aggregations=aggregations,
+        orderby=sort,
+        limit=limit,
+    )
+
+    # {hash -> group_id, ...}
+    hash_to_group = dict(
+        GroupHash.objects.filter(
+            project_id=project_id,
+            hash__in=snuba_results.keys()
+        ).values_list(
+            'hash', 'group_id'
+        )
+    )
+
+    # {group_id -> {field1: [...all values from field1 for all hashes...],
+    #               field2: [...all values from field2 for all hashes...]
+    #               ...}
+    #  ...}
+    group_data = {}
+    for hash, obj in snuba_results.items():
+        if hash in hash_to_group:
+            group_id = hash_to_group[hash]
+
+            if group_id not in group_data:
+                group_data[group_id] = defaultdict(list)
+
+            dest = group_data[group_id]
+            for k, v in obj.items():
+                dest[k].append(v)
+        else:
+            logger.warning(
+                'search.hash_not_found',
+                extra={
+                    'project_id': project_id,
+                    'hash': hash,
+                },
+            )
+
+    return group_data
diff --git a/src/sentry/tagstore/legacy/backend.py b/src/sentry/tagstore/legacy/backend.py
index 9156de76a5..9a48ec3caa 100644
--- a/src/sentry/tagstore/legacy/backend.py
+++ b/src/sentry/tagstore/legacy/backend.py
@@ -573,6 +573,7 @@ class LegacyTagStorage(TagStorage):
 
     def get_group_ids_for_search_filter(
             self, project_id, environment_id, tags, candidates=None, limit=1000):
+
         from sentry.search.base import ANY
         # Django doesnt support union, so we limit results and try to find
         # reasonable matches
@@ -582,7 +583,7 @@ class LegacyTagStorage(TagStorage):
         tag_lookups = sorted(six.iteritems(tags), key=lambda (k, v): v == ANY)
 
         # get initial matches to start the filter
-        matches = candidates
+        matches = candidates or []
 
         # for each remaining tag, find matches contained in our
         # existing set, pruning it down each iteration
@@ -609,7 +610,7 @@ class LegacyTagStorage(TagStorage):
             matches = list(base_qs.values_list('group_id', flat=True)[:limit])
 
             if not matches:
-                return None
+                return []
 
         return matches
 
diff --git a/src/sentry/tagstore/snuba/backend.py b/src/sentry/tagstore/snuba/backend.py
index f5b46b4af4..c1f5389138 100644
--- a/src/sentry/tagstore/snuba/backend.py
+++ b/src/sentry/tagstore/snuba/backend.py
@@ -24,6 +24,7 @@ from sentry.tagstore.exceptions import (
 )
 from sentry.utils import snuba
 
+
 SEEN_COLUMN = 'timestamp'
 
 
@@ -390,27 +391,9 @@ class SnubaTagStorage(TagStorage):
         result = snuba.query(start, end, ['issue'], None, filters, aggregations)
         return defaultdict(int, result.items())
 
-    # Search
-    def get_group_ids_for_search_filter(self, project_id, environment_id, tags):
-        from sentry.search.base import ANY
-
-        start, end = self.get_time_range()
-
-        filters = {
-            'environment': [environment_id],
-            'project_id': [project_id],
-        }
-
-        conditions = []
-        for tag, val in six.iteritems(tags):
-            col = 'tags[{}]'.format(tag)
-            if val == ANY:
-                conditions.append((col, 'IS NOT NULL', None))
-            else:
-                conditions.append((col, '=', val))
-
-        issues = snuba.query(start, end, ['issue'], conditions, filters)
-        return issues.keys()
+    def get_group_ids_for_search_filter(
+            self, project_id, environment_id, tags, candidates=None, limit=1000):
+        raise NotImplementedError
 
     # Everything from here down is basically no-ops
     def create_tag_key(self, project_id, environment_id, key, **kwargs):
@@ -501,10 +484,10 @@ class SnubaTagStorage(TagStorage):
         pass
 
     def get_tag_value_qs(self, project_id, environment_id, key, query=None):
-        return None
+        raise NotImplementedError
 
     def get_group_tag_value_qs(self, project_id, group_id, environment_id, key, value=None):
-        return None
+        raise NotImplementedError
 
     def get_event_tag_qs(self, project_id, environment_id, key, value):
-        return None
+        raise NotImplementedError
diff --git a/src/sentry/tagstore/v2/backend.py b/src/sentry/tagstore/v2/backend.py
index 20b0fb7bc4..21cf527db2 100644
--- a/src/sentry/tagstore/v2/backend.py
+++ b/src/sentry/tagstore/v2/backend.py
@@ -824,6 +824,7 @@ class V2TagStorage(TagStorage):
 
     def get_group_ids_for_search_filter(
             self, project_id, environment_id, tags, candidates=None, limit=1000):
+
         from sentry.search.base import ANY
         # Django doesnt support union, so we limit results and try to find
         # reasonable matches
@@ -833,7 +834,7 @@ class V2TagStorage(TagStorage):
         tag_lookups = sorted(six.iteritems(tags), key=lambda (k, v): v == ANY)
 
         # get initial matches to start the filter
-        matches = candidates
+        matches = candidates or []
 
         # for each remaining tag, find matches contained in our
         # existing set, pruning it down each iteration
@@ -865,7 +866,7 @@ class V2TagStorage(TagStorage):
             matches = list(base_qs.values_list('group_id', flat=True)[:limit])
 
             if not matches:
-                return None
+                return []
 
         return matches
 
diff --git a/src/sentry/testutils/cases.py b/src/sentry/testutils/cases.py
index 2af17c881c..1118a095a4 100644
--- a/src/sentry/testutils/cases.py
+++ b/src/sentry/testutils/cases.py
@@ -11,13 +11,15 @@ from __future__ import absolute_import
 __all__ = (
     'TestCase', 'TransactionTestCase', 'APITestCase', 'TwoFactorAPITestCase', 'AuthProviderTestCase', 'RuleTestCase',
     'PermissionTestCase', 'PluginTestCase', 'CliTestCase', 'AcceptanceTestCase',
-    'IntegrationTestCase', 'UserReportEnvironmentTestCase',
+    'IntegrationTestCase', 'UserReportEnvironmentTestCase', 'SnubaTestCase',
 )
 
 import base64
+import calendar
 import os
 import os.path
 import pytest
+import requests
 import six
 import types
 import logging
@@ -48,11 +50,12 @@ from sentry.auth.superuser import (
 )
 from sentry.constants import MODULE_ROOT
 from sentry.models import (
-    GroupMeta, ProjectOption, DeletedOrganization, Environment, GroupStatus, Organization, TotpInterface, UserReport
+    GroupEnvironment, GroupHash, GroupMeta, ProjectOption, DeletedOrganization,
+    Environment, GroupStatus, Organization, TotpInterface, UserReport,
 )
 from sentry.plugins import plugins
 from sentry.rules import EventState
-from sentry.utils import json
+from sentry.utils import json, snuba
 from sentry.utils.auth import SSO_SESSION_KEY
 
 from .fixtures import Fixtures
@@ -685,3 +688,83 @@ class IntegrationTestCase(TestCase):
 
     def assertDialogSuccess(self, resp):
         assert 'window.opener.postMessage(' in resp.content
+
+
+class SnubaTestCase(TestCase):
+    def setUp(self):
+        super(SnubaTestCase, self).setUp()
+
+        assert requests.post(snuba.SNUBA + '/tests/drop').status_code == 200
+
+    def __wrap_event(self, event, data, primary_hash):
+        # TODO: Abstract and combine this with the stream code in
+        #       getsentry once it is merged, so that we don't alter one
+        #       without updating the other.
+        return {
+            'group_id': event.group_id,
+            'event_id': event.event_id,
+            'project_id': event.project_id,
+            'message': event.message,
+            'platform': event.platform,
+            'datetime': event.datetime,
+            'data': data,
+            'primary_hash': primary_hash,
+        }
+
+    def create_event(self, *args, **kwargs):
+        """\
+        Takes the results from the existing `create_event` method and
+        inserts into the local test Snuba cluster so that tests can be
+        run against the same event data.
+
+        Note that we create a GroupHash as necessary because `create_event`
+        doesn't run them through the 'real' event pipeline. In a perfect
+        world all test events would go through the full regular pipeline.
+        """
+
+        from sentry.event_manager import get_hashes_from_fingerprint, md5_from_hash
+
+        event = super(SnubaTestCase, self).create_event(*args, **kwargs)
+
+        data = event.data.data
+        tags = dict(data.get('tags', []))
+
+        if not data.get('received'):
+            data['received'] = calendar.timegm(event.datetime.timetuple())
+
+        environment = Environment.get_or_create(
+            event.project,
+            tags['environment'],
+        )
+
+        GroupEnvironment.objects.get_or_create(
+            environment_id=environment.id,
+            group_id=event.group_id,
+        )
+
+        hashes = get_hashes_from_fingerprint(
+            event,
+            data.get('fingerprint', ['{{ default }}']),
+        )
+        primary_hash = md5_from_hash(hashes[0])
+
+        grouphash, _ = GroupHash.objects.get_or_create(
+            project=event.project,
+            group=event.group,
+            hash=primary_hash,
+        )
+
+        self.snuba_insert(self.__wrap_event(event, data, grouphash.hash))
+
+        return event
+
+    def snuba_insert(self, events):
+        "Write a (wrapped) event (or events) to Snuba."
+
+        if not isinstance(events, list):
+            events = [events]
+
+        assert requests.post(
+            snuba.SNUBA + '/tests/insert',
+            data=json.dumps(events)
+        ).status_code == 200
diff --git a/src/sentry/testutils/fixtures.py b/src/sentry/testutils/fixtures.py
index c700ad559e..6a75fb5e3a 100644
--- a/src/sentry/testutils/fixtures.py
+++ b/src/sentry/testutils/fixtures.py
@@ -162,6 +162,7 @@ DEFAULT_EVENT_DATA = {
         ],
     },
     'tags': [],
+    'platform': 'python',
 }
 
 
@@ -457,11 +458,16 @@ class Fixtures(object):
             kwargs['group'] = self.group
         kwargs.setdefault('project', kwargs['group'].project)
         kwargs.setdefault('data', copy.deepcopy(DEFAULT_EVENT_DATA))
+        kwargs.setdefault('platform', kwargs['data'].get('platform', 'python'))
+        kwargs.setdefault('message', kwargs['data'].get('message', 'message'))
         if kwargs.get('tags'):
             tags = kwargs.pop('tags')
             if isinstance(tags, dict):
                 tags = list(tags.items())
             kwargs['data']['tags'] = tags
+        if kwargs.get('stacktrace'):
+            stacktrace = kwargs.pop('stacktrace')
+            kwargs['data']['sentry.interfaces.Stacktrace'] = stacktrace
 
         kwargs['data'].setdefault(
             'errors', [{
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 6f65574127..ecdc69fab5 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -18,7 +18,8 @@ class SnubaError(Exception):
 
 
 def query(start, end, groupby, conditions=None, filter_keys=None,
-          aggregations=None, rollup=None, arrayjoin=None, limit=None, orderby=None):
+          aggregations=None, rollup=None, arrayjoin=None, limit=None, orderby=None,
+          having=None):
     """
     Sends a query to snuba.
 
@@ -39,6 +40,7 @@ def query(start, end, groupby, conditions=None, filter_keys=None,
     """
     groupby = groupby or []
     conditions = conditions or []
+    having = having or []
     aggregations = aggregations or [['count()', '', 'aggregate']]
     filter_keys = filter_keys or {}
 
@@ -81,6 +83,7 @@ def query(start, end, groupby, conditions=None, filter_keys=None,
         'from_date': start.isoformat(),
         'to_date': end.isoformat(),
         'conditions': conditions,
+        'having': having,
         'groupby': groupby,
         'project': project_ids,
         'aggregations': aggregations,
@@ -106,6 +109,7 @@ def query(start, end, groupby, conditions=None, filter_keys=None,
     aggregate_cols = [a[2] for a in aggregations]
     expected_cols = set(groupby + aggregate_cols)
     got_cols = set(c['name'] for c in response['meta'])
+
     assert expected_cols == got_cols
 
     for d in response['data']:
diff --git a/tests/snuba/search/__init__.py b/tests/snuba/search/__init__.py
new file mode 100644
index 0000000000..c3961685ab
--- /dev/null
+++ b/tests/snuba/search/__init__.py
@@ -0,0 +1 @@
+from __future__ import absolute_import
diff --git a/tests/snuba/search/test_backend.py b/tests/snuba/search/test_backend.py
new file mode 100644
index 0000000000..a99b214d72
--- /dev/null
+++ b/tests/snuba/search/test_backend.py
@@ -0,0 +1,769 @@
+from __future__ import absolute_import
+
+import pytz
+import pytest
+from datetime import datetime, timedelta
+from django.conf import settings
+
+from sentry.event_manager import ScoreClause
+from sentry.models import (
+    Environment, GroupAssignee, GroupBookmark, GroupStatus, GroupSubscription,
+    Release, ReleaseEnvironment, ReleaseProjectEnvironment
+)
+from sentry.search.base import ANY
+from sentry.search.django.backend import get_latest_release
+from sentry.search.snuba.backend import SnubaSearchBackend
+from sentry.testutils import SnubaTestCase
+
+
+class SnubaSearchTest(SnubaTestCase):
+    def setUp(self):
+        super(SnubaSearchTest, self).setUp()
+
+        self.backend = SnubaSearchBackend()
+
+        self.environments = {}
+
+        base_datetime = (datetime.utcnow() - timedelta(days=7)).replace(tzinfo=pytz.utc)
+        self.group1 = self.create_group(
+            project=self.project,
+            checksum='a' * 32,
+            message='foo',
+            times_seen=5,
+            status=GroupStatus.UNRESOLVED,
+            last_seen=base_datetime,
+            first_seen=base_datetime - timedelta(days=31),
+            score=ScoreClause.calculate(
+                times_seen=5,
+                last_seen=base_datetime,
+            ),
+        )
+
+        self.event1 = self.create_event(
+            event_id='a' * 32,
+            group=self.group1,
+            datetime=base_datetime - timedelta(days=31),
+            message='group1',
+            stacktrace={
+                'frames': [{
+                    'module': 'group1'
+                }]},
+            tags={
+                'server': 'example.com',
+                'environment': 'production',
+            }
+        )
+        self.event3 = self.create_event(
+            event_id='c' * 32,
+            group=self.group1,
+            datetime=base_datetime,
+            message='group1',
+            stacktrace={
+                'frames': [{
+                    'module': 'group1'
+                }]},
+            tags={
+                'server': 'example.com',
+                'environment': 'production',
+            }
+        )
+
+        self.group2 = self.create_group(
+            project=self.project,
+            checksum='b' * 32,
+            message='bar',
+            times_seen=10,
+            status=GroupStatus.RESOLVED,
+            last_seen=base_datetime - timedelta(days=30),
+            first_seen=base_datetime - timedelta(days=30),
+            score=ScoreClause.calculate(
+                times_seen=10,
+                last_seen=base_datetime - timedelta(days=30),
+            ),
+        )
+
+        self.event2 = self.create_event(
+            event_id='b' * 32,
+            group=self.group2,
+            datetime=base_datetime - timedelta(days=30),
+            message='group2',
+            stacktrace={
+                'frames': [{
+                    'module': 'group2'
+                }]},
+            tags={
+                'server': 'example.com',
+                'environment': 'staging',
+                'url': 'http://example.com',
+            }
+        )
+
+        GroupBookmark.objects.create(
+            user=self.user,
+            group=self.group2,
+            project=self.group2.project,
+        )
+
+        GroupAssignee.objects.create(
+            user=self.user,
+            group=self.group2,
+            project=self.group2.project,
+        )
+
+        GroupSubscription.objects.create(
+            user=self.user,
+            group=self.group1,
+            project=self.group1.project,
+            is_active=True,
+        )
+
+        GroupSubscription.objects.create(
+            user=self.user,
+            group=self.group2,
+            project=self.group2.project,
+            is_active=False,
+        )
+
+    def create_event(self, *args, **kwargs):
+        event = super(SnubaSearchTest, self).create_event(*args, **kwargs)
+
+        data = event.data.data
+        tags = dict(data.get('tags', []))
+
+        if tags['environment'] not in self.environments:
+            self.environments[tags['environment']] = Environment.get_or_create(
+                event.project,
+                tags['environment'],
+            )
+
+        return event
+
+    def test_query(self):
+        results = self.backend.query(self.project, query='foo')
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(self.project, query='bar')
+        assert set(results) == set([self.group2])
+
+    def test_query_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            query='foo')
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            query='bar')
+        assert set(results) == set([])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            query='bar')
+        assert set(results) == set([self.group2])
+
+    def test_sort(self):
+        results = self.backend.query(self.project, sort_by='date')
+        assert list(results) == [self.group1, self.group2]
+
+        results = self.backend.query(self.project, sort_by='new')
+        assert list(results) == [self.group2, self.group1]
+
+        results = self.backend.query(self.project, sort_by='freq')
+        assert list(results) == [self.group1, self.group2]
+
+        results = self.backend.query(self.project, sort_by='priority')
+        assert list(results) == [self.group1, self.group2]
+
+    def test_sort_with_environment(self):
+        for dt in [
+                self.group1.first_seen + timedelta(days=1),
+                self.group1.first_seen + timedelta(days=2),
+                self.group1.last_seen + timedelta(days=1)]:
+            self.create_event(
+                group=self.group2,
+                datetime=dt,
+                message='group2',
+                stacktrace={
+                    'frames': [{
+                        'module': 'group2'
+                    }]},
+                tags={'environment': 'production'}
+            )
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='date',
+        )
+        assert list(results) == [self.group2, self.group1]
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='new',
+        )
+        assert list(results) == [self.group2, self.group1]
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='freq',
+        )
+        assert list(results) == [self.group2, self.group1]
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='priority',
+        )
+        assert list(results) == [self.group2, self.group1]
+
+    def test_status(self):
+        results = self.backend.query(self.project, status=GroupStatus.UNRESOLVED)
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(self.project, status=GroupStatus.RESOLVED)
+        assert set(results) == set([self.group2])
+
+    def test_status_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            status=GroupStatus.UNRESOLVED)
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            status=GroupStatus.RESOLVED)
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            status=GroupStatus.RESOLVED)
+        assert set(results) == set([])
+
+    def test_tags(self):
+        results = self.backend.query(
+            self.project,
+            tags={'environment': 'staging'})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            tags={'environment': 'example.com'})
+        assert set(results) == set([])
+
+        results = self.backend.query(
+            self.project,
+            tags={'environment': ANY})
+        assert set(results) == set([self.group2, self.group1])
+
+        results = self.backend.query(
+            self.project,
+            tags={'environment': 'staging',
+                  'server': 'example.com'})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            tags={'environment': 'staging',
+                  'server': ANY})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            tags={'environment': 'staging',
+                  'server': 'bar.example.com'})
+        assert set(results) == set([])
+
+    def test_tags_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            tags={'server': 'example.com'})
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            tags={'server': 'example.com'})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            tags={'server': ANY})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            tags={'url': 'http://example.com'})
+        assert set(results) == set([])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            tags={'url': 'http://example.com'})
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            tags={'server': 'bar.example.com'})
+        assert set(results) == set([])
+
+    def test_bookmarked_by(self):
+        results = self.backend.query(self.project, bookmarked_by=self.user)
+        assert set(results) == set([self.group2])
+
+    def test_bookmarked_by_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            bookmarked_by=self.user)
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            bookmarked_by=self.user)
+        assert set(results) == set([])
+
+    def test_project(self):
+        results = self.backend.query(self.create_project(name='other'))
+        assert set(results) == set([])
+
+    def test_pagination(self):
+        results = self.backend.query(self.project, limit=1, sort_by='date')
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(self.project, cursor=results.next, limit=1, sort_by='date')
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(self.project, cursor=results.next, limit=1, sort_by='date')
+        assert set(results) == set([])
+
+    def test_pagination_with_environment(self):
+        for dt in [
+                self.group1.first_seen + timedelta(days=1),
+                self.group1.first_seen + timedelta(days=2),
+                self.group1.last_seen + timedelta(days=1)]:
+            self.create_event(
+                group=self.group2,
+                datetime=dt,
+                message='group2',
+                stacktrace={
+                    'frames': [{
+                        'module': 'group2'
+                    }]},
+                tags={'environment': 'production'}
+            )
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='date',
+            limit=1,
+            count_hits=True,
+        )
+        assert list(results) == [self.group2]
+        assert results.hits == 2
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='date',
+            limit=1,
+            cursor=results.next,
+            count_hits=True,
+        )
+        assert list(results) == [self.group1]
+        assert results.hits == 2
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            sort_by='date',
+            limit=1,
+            cursor=results.next,
+            count_hits=True,
+        )
+        assert list(results) == []
+        assert results.hits == 2
+
+    def test_age_filter(self):
+        results = self.backend.query(
+            self.project,
+            age_from=self.group2.first_seen,
+            age_from_inclusive=True,
+        )
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            age_to=self.group1.first_seen + timedelta(minutes=1),
+            age_to_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            age_from=self.group1.first_seen,
+            age_from_inclusive=True,
+            age_to=self.group1.first_seen + timedelta(minutes=1),
+            age_to_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+    def test_age_filter_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            age_from=self.group1.first_seen,
+            age_from_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            age_to=self.group1.first_seen,
+            age_to_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            age_from=self.group1.first_seen,
+            age_from_inclusive=False,
+        )
+        assert set(results) == set([])
+
+        self.create_event(
+            group=self.group1,
+            datetime=self.group1.first_seen + timedelta(days=1),
+            message='group1',
+            stacktrace={
+                'frames': [{
+                    'module': 'group1'
+                }]},
+            tags={
+                'environment': 'development',
+            }
+        )
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            age_from=self.group1.first_seen,
+            age_from_inclusive=False,
+        )
+        assert set(results) == set([])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['development'],
+            age_from=self.group1.first_seen,
+            age_from_inclusive=False,
+        )
+        assert set(results) == set([self.group1])
+
+    def test_times_seen_filter(self):
+        results = self.backend.query(
+            self.project,
+            times_seen=2,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            times_seen_lower=2,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            times_seen_upper=1,
+        )
+        assert set(results) == set([self.group2])
+
+    def test_last_seen_filter(self):
+        results = self.backend.query(
+            self.project,
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            last_seen_to=self.group2.last_seen + timedelta(minutes=1),
+            last_seen_to_inclusive=True,
+        )
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=True,
+            last_seen_to=self.group1.last_seen + timedelta(minutes=1),
+            last_seen_to_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+    def test_last_seen_filter_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            last_seen_to=self.group1.last_seen,
+            last_seen_to_inclusive=True,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=False,
+        )
+        assert set(results) == set([])
+
+        self.create_event(
+            group=self.group1,
+            datetime=self.group1.last_seen + timedelta(days=1),
+            message='group1',
+            stacktrace={
+                'frames': [{
+                    'module': 'group1'
+                }]},
+            tags={
+                'environment': 'development',
+            }
+        )
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=False,
+        )
+        assert set(results) == set([])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['development'],
+            last_seen_from=self.group1.last_seen,
+            last_seen_from_inclusive=False,
+        )
+        assert set(results) == set([self.group1])
+
+    def test_date_filter(self):
+        results = self.backend.query(
+            self.project,
+            date_from=self.event2.datetime,
+        )
+        assert set(results) == set([self.group1, self.group2])
+
+        results = self.backend.query(
+            self.project,
+            date_to=self.event1.datetime + timedelta(minutes=1),
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            date_from=self.event1.datetime,
+            date_to=self.event2.datetime + timedelta(minutes=1),
+        )
+        assert set(results) == set([self.group1, self.group2])
+
+    @pytest.mark.xfail(
+        not settings.SENTRY_TAGSTORE.startswith('sentry.tagstore.v2'),
+        reason='unsupported on legacy backend due to insufficient index',
+    )
+    def test_date_filter_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            date_from=self.event2.datetime,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            date_to=self.event1.datetime + timedelta(minutes=1),
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            date_from=self.event1.datetime,
+            date_to=self.event2.datetime + timedelta(minutes=1),
+        )
+        assert set(results) == set([self.group2])
+
+    def test_unassigned(self):
+        results = self.backend.query(self.project, unassigned=True)
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(self.project, unassigned=False)
+        assert set(results) == set([self.group2])
+
+    def test_unassigned_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            unassigned=True)
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            unassigned=False)
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            unassigned=False)
+        assert set(results) == set([])
+
+    def test_assigned_to(self):
+        results = self.backend.query(self.project, assigned_to=self.user)
+        assert set(results) == set([self.group2])
+
+        # test team assignee
+        ga = GroupAssignee.objects.get(
+            user=self.user,
+            group=self.group2,
+            project=self.group2.project,
+        )
+        ga.update(team=self.team, user=None)
+        assert GroupAssignee.objects.get(id=ga.id).user is None
+
+        results = self.backend.query(self.project, assigned_to=self.user)
+        assert set(results) == set([self.group2])
+
+        # test when there should be no results
+        other_user = self.create_user()
+        results = self.backend.query(self.project, assigned_to=other_user)
+        assert set(results) == set([])
+
+        owner = self.create_user()
+        self.create_member(
+            organization=self.project.organization,
+            user=owner,
+            role='owner',
+            teams=[],
+        )
+
+        # test that owners don't see results for all teams
+        results = self.backend.query(self.project, assigned_to=owner)
+        assert set(results) == set([])
+
+    def test_assigned_to_with_environment(self):
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['staging'],
+            assigned_to=self.user)
+        assert set(results) == set([self.group2])
+
+        results = self.backend.query(
+            self.project,
+            environment=self.environments['production'],
+            assigned_to=self.user)
+        assert set(results) == set([])
+
+    def test_subscribed_by(self):
+        results = self.backend.query(
+            self.group1.project,
+            subscribed_by=self.user,
+        )
+        assert set(results) == set([self.group1])
+
+    def test_subscribed_by_with_environment(self):
+        results = self.backend.query(
+            self.group1.project,
+            environment=self.environments['production'],
+            subscribed_by=self.user,
+        )
+        assert set(results) == set([self.group1])
+
+        results = self.backend.query(
+            self.group1.project,
+            environment=self.environments['staging'],
+            subscribed_by=self.user,
+        )
+        assert set(results) == set([])
+
+    def test_parse_release_latest(self):
+        with pytest.raises(Release.DoesNotExist):
+            # no releases exist period
+            environment = None
+            result = get_latest_release(self.project, environment)
+
+        old = Release.objects.create(
+            organization_id=self.project.organization_id,
+            version='old'
+        )
+        old.add_project(self.project)
+
+        new_date = old.date_added + timedelta(minutes=1)
+        new = Release.objects.create(
+            version='new-but-in-environment',
+            organization_id=self.project.organization_id,
+            date_released=new_date,
+        )
+        new.add_project(self.project)
+        ReleaseEnvironment.get_or_create(
+            project=self.project,
+            release=new,
+            environment=self.environment,
+            datetime=new_date,
+        )
+        ReleaseProjectEnvironment.get_or_create(
+            project=self.project,
+            release=new,
+            environment=self.environment,
+            datetime=new_date,
+        )
+
+        newest = Release.objects.create(
+            version='newest-overall',
+            organization_id=self.project.organization_id,
+            date_released=old.date_added + timedelta(minutes=5),
+        )
+        newest.add_project(self.project)
+
+        # latest overall (no environment filter)
+        environment = None
+        result = get_latest_release(self.project, environment)
+        assert result == newest.version
+
+        # latest in environment
+        environment = self.environment
+        result = get_latest_release(self.project, environment)
+        assert result == new.version
+
+        with pytest.raises(Release.DoesNotExist):
+            # environment with no releases
+            environment = self.create_environment()
+            result = get_latest_release(self.project, environment)
+            assert result == new.version
diff --git a/tests/snuba/tagstore/test_tagstore_backend.py b/tests/snuba/tagstore/test_tagstore_backend.py
index d960b71991..1e22ed2c91 100644
--- a/tests/snuba/tagstore/test_tagstore_backend.py
+++ b/tests/snuba/tagstore/test_tagstore_backend.py
@@ -1,12 +1,10 @@
 from __future__ import absolute_import
 
-
 import calendar
 from datetime import datetime, timedelta
 import json
 import pytest
 import requests
-import responses
 import six
 
 from sentry.models import GroupHash, EventUser
@@ -80,51 +78,6 @@ class TagStorage(TestCase):
 
         assert requests.post(snuba.SNUBA + '/tests/insert', data=data).status_code == 200
 
-    @responses.activate
-    def test_get_group_ids_for_search_filter(self):
-        from sentry.search.base import ANY
-        tags = {
-            'foo': 'bar',
-            'baz': 'quux',
-        }
-
-        with responses.RequestsMock() as rsps:
-            def snuba_response(request):
-                body = json.loads(request.body)
-                assert body['project'] == [self.proj1.id]
-                assert body['groupby'] == ['issue']
-                assert body['issues']
-                assert ['tags[foo]', '=', 'bar'] in body['conditions']
-                assert ['tags[baz]', '=', 'quux'] in body['conditions']
-                return (200, {}, json.dumps({
-                    'meta': [{'name': 'issue'}, {'name': 'aggregate'}],
-                    'data': [{'issue': self.proj1group1.id, 'aggregate': 1}],
-                }))
-
-            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response)
-            result = self.ts.get_group_ids_for_search_filter(self.proj1.id, self.proj1env1.id, tags)
-            assert result == [self.proj1group1.id]
-
-        tags = {
-            'foo': ANY,
-        }
-
-        with responses.RequestsMock() as rsps:
-            def snuba_response_2(request):
-                body = json.loads(request.body)
-                assert body['project'] == [self.proj1.id]
-                assert body['groupby'] == ['issue']
-                assert body['issues']
-                assert ['tags[foo]', 'IS NOT NULL', None] in body['conditions']
-                return (200, {}, json.dumps({
-                    'meta': [{'name': 'issue'}, {'name': 'aggregate'}],
-                    'data': [{'issue': self.proj1group2.id, 'aggregate': 1}],
-                }))
-
-            rsps.add_callback(responses.POST, snuba.SNUBA + '/query', callback=snuba_response_2)
-            result = self.ts.get_group_ids_for_search_filter(self.proj1.id, self.proj1env1.id, tags)
-            assert result == [self.proj1group2.id]
-
     def test_get_group_tag_keys_and_top_values(self):
         result = self.ts.get_group_tag_keys_and_top_values(
             self.proj1.id,
diff --git a/tests/snuba/test_snuba.py b/tests/snuba/test_snuba.py
index 4b889eb969..dfb98bfdad 100644
--- a/tests/snuba/test_snuba.py
+++ b/tests/snuba/test_snuba.py
@@ -1,29 +1,22 @@
 from __future__ import absolute_import
 
 from datetime import datetime, timedelta
-import requests
-import json
 import time
 
-from exam import before
+from sentry.testutils import SnubaTestCase
+from sentry.utils import snuba
 
-from sentry.testutils import TestCase
-from sentry.utils.snuba import query, SNUBA
-
-
-class SnubaTest(TestCase):
-    @before
-    def setup(self):
-        r = requests.post(SNUBA + '/tests/drop')
-        assert r.status_code == 200
 
+class SnubaTest(SnubaTestCase):
     def test(self):
+        "This is just a simple 'hello, world' example test."
+
         now = datetime.now()
 
         events = [{
             'event_id': 'x' * 32,
             'primary_hash': '1' * 32,
-            'project_id': 1,
+            'project_id': 100,
             'message': 'message',
             'platform': 'python',
             'datetime': now.strftime('%Y-%m-%dT%H:%M:%S.%fZ'),
@@ -32,13 +25,11 @@ class SnubaTest(TestCase):
             }
         }]
 
-        r = requests.post(SNUBA + '/tests/insert', data=json.dumps(events))
-        assert r.status_code == 200
+        self.snuba_insert(events)
 
-        r = query(
+        assert snuba.query(
             start=now - timedelta(days=1),
             end=now + timedelta(days=1),
             groupby=['project_id'],
-            filter_keys={'project_id': [1]},
-        )
-        assert r == {1: 1}
+            filter_keys={'project_id': [100]},
+        ) == {100: 1}
