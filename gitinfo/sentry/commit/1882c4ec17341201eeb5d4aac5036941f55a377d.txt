commit 1882c4ec17341201eeb5d4aac5036941f55a377d
Author: ted kaemming <ted@kaemming.com>
Date:   Thu Aug 17 15:15:52 2017 -0700

    Truncate timelines during digestion to prevent excessive resource consumption. (#5909)
    
    Without this, the digest can grow by the timeline capacity during each invocation of `DIGEST_OPEN` if the digest is unable to be successfully closed.

diff --git a/src/sentry/digests/backends/redis.py b/src/sentry/digests/backends/redis.py
index 70902ae49d..47cc9d8fa2 100644
--- a/src/sentry/digests/backends/redis.py
+++ b/src/sentry/digests/backends/redis.py
@@ -209,7 +209,14 @@ class RedisBackend(Backend):
         with self._get_timeline_lock(key, duration=30).acquire():
             try:
                 response = script(
-                    connection, [key], ['DIGEST_OPEN', self.namespace, self.ttl, timestamp, key]
+                    connection, [key], [
+                        'DIGEST_OPEN',
+                        self.namespace,
+                        self.ttl,
+                        timestamp,
+                        key,
+                        self.capacity if self.capacity else -1,
+                    ]
                 )
             except ResponseError as e:
                 if 'err(invalid_state):' in e.message:
diff --git a/src/sentry/scripts/digests/digests.lua b/src/sentry/scripts/digests/digests.lua
index 01010cfd86..90011673d9 100644
--- a/src/sentry/scripts/digests/digests.lua
+++ b/src/sentry/scripts/digests/digests.lua
@@ -266,7 +266,7 @@ local function add_record_to_timeline(configuration, timeline_id, record_id, val
     return ready
 end
 
-local function digest_timeline(configuration, timeline_id)
+local function digest_timeline(configuration, timeline_id, timeline_capacity)
     -- Check to ensure that the timeline is in the correct state.
     if redis.call('ZSCORE', configuration:get_schedule_ready_key(), timeline_id) == false then
         error('err(invalid_state): timeline is not in the ready state, cannot be digested')
@@ -278,10 +278,15 @@ local function digest_timeline(configuration, timeline_id)
         if redis.call('EXISTS', digest_key) == 1 then
             -- If the digest set already exists (possibly because we already tried
             -- to send it and failed for some reason), merge any new data into it.
-            -- TODO: It might make sense to trim here to avoid returning capacity *
-            -- 2 if timeline was full when it was previously digested.
             redis.call('ZUNIONSTORE', digest_key, 2, timeline_key, digest_key, 'AGGREGATE', 'MAX')
             redis.call('DEL', timeline_key)
+
+            -- After merging, we have to do a capacity check (if we didn't,
+            -- it's possible that this digest could grow to an unbounded size
+            -- if it is never actually closed.)
+            if timeline_capacity > 0 then
+                truncate_digest(configuration, timeline_id, timeline_capacity)
+            end
         else
             -- Otherwise, we can just move the timeline contents to the digest key.
             redis.call('RENAME', timeline_key, digest_key)
@@ -427,11 +432,12 @@ local commands = {
         return delete_timeline(configuration, timeline_id)
     end,
     DIGEST_OPEN = function (cursor, arguments)
-        local cursor, configuration, timeline_id = multiple_argument_parser(
+        local cursor, configuration, timeline_id, timeline_capacity = multiple_argument_parser(
             configuration_argument_parser,
-            argument_parser()
+            argument_parser(),
+            argument_parser(tonumber)
         )(cursor, arguments)
-        return digest_timeline(configuration, timeline_id)
+        return digest_timeline(configuration, timeline_id, timeline_capacity)
     end,
     DIGEST_CLOSE = function (cursor, arguments)
         local cursor, configuration, timeline_id, delay_minimum, record_ids = multiple_argument_parser(
diff --git a/tests/sentry/digests/backends/test_redis.py b/tests/sentry/digests/backends/test_redis.py
index 2476dfb492..0783b4d09e 100644
--- a/tests/sentry/digests/backends/test_redis.py
+++ b/tests/sentry/digests/backends/test_redis.py
@@ -81,6 +81,40 @@ class RedisBackendTestCase(TestCase):
         with backend.digest('timeline', 0) as records:
             assert set(records) == set([record_1, record_2])
 
+    def test_maintenance_failure_recovery_with_capacity(self):
+        backend = RedisBackend(capacity=10, truncation_chance=0.0)
+
+        t = time.time()
+
+        # Add 10 items to the timeline.
+        for i in xrange(10):
+            backend.add('timeline', Record('record:{}'.format(i), '{}'.format(i), t + i))
+
+        try:
+            with backend.digest('timeline', 0) as records:
+                raise Exception('This causes the digest to not be closed.')
+        except Exception:
+            pass
+
+        # The 10 existing items should now be in the digest set (the exception
+        # prevented the close operation from occurring, so they were never
+        # deleted from Redis or removed from the digest set.) If we add 10 more
+        # items, they should be added to the timeline set (not the digest set.)
+        for i in xrange(10, 20):
+            backend.add('timeline', Record('record:{}'.format(i), '{}'.format(i), t + i))
+
+        # Maintenance should move the timeline back to the waiting state, ...
+        backend.maintenance(time.time())
+
+        # The schedule should now contain the timeline.
+        assert set(entry.key for entry in backend.schedule(time.time())) == set(['timeline'])
+
+        # Only the new records should exist -- the older one should have been
+        # trimmed to avoid the digest growing beyond the timeline capacity.
+        with backend.digest('timeline', 0) as records:
+            expected_keys = set('record:{}'.format(i) for i in xrange(10, 20))
+            assert set(record.key for record in records) == expected_keys
+
     def test_delete(self):
         backend = RedisBackend()
         backend.add('timeline', Record('record:1', 'value', time.time()))
