commit b7e65bde19773a0ba1ebf04f2473ce6eecd63995
Author: Ted Kaemming <ted@kaemming.com>
Date:   Mon Sep 21 14:11:42 2015 -0700

    Add timeline identifier, properties, drop prefix key (for now at least.)
    
    (I've realized that this is almost certainly a bad idea, but I'd like to
    commit it just in case I need to come back to it later.)

diff --git a/src/sentry/timelines/redis.py b/src/sentry/timelines/redis.py
index bf07944ed0..e4125d3e12 100644
--- a/src/sentry/timelines/redis.py
+++ b/src/sentry/timelines/redis.py
@@ -1,5 +1,7 @@
 from __future__ import absolute_import
 
+import base64
+import hashlib
 import itertools
 import logging
 import random
@@ -18,6 +20,7 @@ from redis.exceptions import (
     WatchError,
 )
 
+from sentry.utils import json
 from sentry.utils.compat import pickle
 
 from .base import Backend
@@ -96,22 +99,35 @@ def make_schedule_key(state):
     return 's:{0}'.format(state)
 
 
-def make_timeline_key(timeline):
-    return 't:{0}'.format(timeline)
+def make_timeline_identifier(target, properties):
+    # XXX: This doesn't use pickle to ensure keys are sorted recursively.
+    # (Ideally we'd just use something like Avro here in the future instead.)
+    # XXX: This doesn't really **need** base64, but it makes the keys more
+    # readable. We can disable this for performance reasons, if necessary.
+    signature = base64.b64encode(hashlib.sha1(json.dumps(properties, sort_keys=True)).digest())
+    return '{0}:{1}'.format(target, signature)
 
 
-def make_iteration_key(timeline):
-    return '{0}:i'.format(make_timeline_key(timeline))
+def make_timeline_key(identifier):
+    return 't:{0}'.format(identifier)
 
 
-def make_digest_key(timeline):
-    return '{0}:d'.format(make_timeline_key(timeline))
+def make_properties_key(timeline_key):
+    return '{0}:p'.format(timeline_key)
 
 
-def make_record_key(timeline, record):
+def make_iteration_key(timeline_key):
+    return '{0}:i'.format(timeline_key)
+
+
+def make_digest_key(timeline_key):
+    return '{0}:d'.format(timeline_key)
+
+
+def make_record_key(timeline_key, record):
     # XXX: Don't update this without updating the Lua script!
     # TODO: Just interpolate the hierarchical part here into the script.
-    return '{0}:r:{1}'.format(make_timeline_key(timeline), record)
+    return '{0}:r:{1}'.format(timeline_key, record)
 
 
 def backoff(duration, maximum=2):
@@ -125,9 +141,6 @@ class RedisBackend(Backend):
 
         self.cluster = Cluster(options['hosts'])
 
-        # TODO: Make this configurable.
-        self.prefix = 'digests'
-
         # TODO: Allow this to be configured (probably via a import path.)
         self.codec = CompressedPickleCodec()
 
@@ -138,12 +151,10 @@ class RedisBackend(Backend):
         self.capacity = 1000
         self.trim_chance = 1.0 / self.capacity
 
-    def prefix_key(self, key):
-        return '{0}:{1}'.format(self.prefix, key)
-
-    def add(self, timeline, record):
-        timeline_key = self.prefix_key(make_timeline_key(timeline))
-        record_key = self.prefix_key(make_record_key(timeline, record.key))
+    def add(self, target, properties, record):
+        timeline_identifier = make_timeline_identifier(target, properties)
+        timeline_key = make_timeline_key(timeline_identifier)
+        record_key = make_record_key(timeline_key, record.key)
 
         connection = self.cluster.get_local_client_for_key(timeline_key)
         with connection.pipeline() as pipeline:
@@ -155,11 +166,12 @@ class RedisBackend(Backend):
                 ex=self.backoff(self.maximum_backoff_steps) + self.delivery_grace_seconds,
             )
 
-            # TODO: This probably should just be rolled into some sort of
-            # metadata key instead of something specifically for backoff.
             # TODO: Does this need a timeout? This assumes that the iteration
             # counter will be deleted when the timeline has no more records.
-            pipeline.set(self.prefix_key(make_iteration_key(timeline)), 0, nx=True)
+            pipeline.set(make_iteration_key(timeline_key), 0, nx=True)
+
+            # XXX: What happens if this is evicted? Do we lose the timeline? :(
+            pipeline.set(make_properties_key(timeline_key), self.codec.encode(properties), nx=True)
 
             # TODO: Prefix the entry with the timestamp (lexicographically
             # sortable) to ensure that we can maintain abitrary precision:
@@ -167,8 +179,8 @@ class RedisBackend(Backend):
             pipeline.zadd(timeline_key, record.timestamp, record.key)
 
             add_to_schedule(
-                map(self.prefix_key, map(make_schedule_key, (WAITING_STATE, READY_STATE))),
-                (timeline, record.timestamp),
+                map(make_schedule_key, (WAITING_STATE, READY_STATE)),
+                (timeline_identifier, record.timestamp),
                 pipeline,
             )
 
@@ -178,7 +190,7 @@ class RedisBackend(Backend):
 
             results = pipeline.execute()
             if should_truncate:
-                logger.info('Removed %s extra records from %s.', results[-1], timeline)
+                logger.info('Removed %s extra records from %s.', results[-1], timeline_identifier)
 
     def schedule(self, cutoff, chunk=1000):
         """
@@ -192,7 +204,7 @@ class RedisBackend(Backend):
             # TODO: Scheduling for each host should be protected by a lease.
             while True:
                 items = connection.zrangebyscore(
-                    self.prefix_key(make_schedule_key(WAITING_STATE)),
+                    make_schedule_key(WAITING_STATE),
                     min=0,
                     max=cutoff,
                     withscores=True,
@@ -213,12 +225,12 @@ class RedisBackend(Backend):
                     pipeline.multi()
 
                     pipeline.zrem(
-                        self.prefix_key(make_schedule_key(WAITING_STATE)),
+                        make_schedule_key(WAITING_STATE),
                         *[key for key, timestamp in items]
                     )
 
                     pipeline.zadd(
-                        self.prefix_key(make_schedule_key(READY_STATE)),
+                        make_schedule_key(READY_STATE),
                         *itertools.chain.from_iterable([(timestamp, key) for (key, timestamp) in items])
                     )
 
@@ -235,14 +247,14 @@ class RedisBackend(Backend):
         raise NotImplementedError
 
     @contextmanager
-    def digest(self, timeline):
-        timeline_key = self.prefix_key(make_timeline_key(timeline))
-        digest_key = self.prefix_key(make_digest_key(timeline))
+    def digest(self, timeline_identifier):
+        timeline_key = make_timeline_key(timeline_identifier)
+        digest_key = make_digest_key(timeline_key)
 
         # TODO: Need to wrap this whole section in a lease to try and avoid data races.
         connection = self.cluster.get_local_client_for_key(timeline_key)
 
-        if connection.zscore(self.prefix_key(make_schedule_key(READY_STATE)), timeline) is None:
+        if connection.zscore(make_schedule_key(READY_STATE), timeline_identifier) is None:
             raise Exception('Cannot digest timeline, timeline is not in the ready state.')
 
         with connection.pipeline() as pipeline:
@@ -268,12 +280,12 @@ class RedisBackend(Backend):
         records = connection.zrevrange(digest_key, 0, -1, withscores=True)
 
         # TODO: This should gracefully handle the iteration key being evicted.
-        iteration = int(connection.get(self.prefix_key(make_iteration_key(timeline))))
+        iteration = int(connection.get(make_iteration_key(timeline_key)))
 
         def get_records_for_digest():
             with connection.pipeline(transaction=False) as pipeline:
                 for key, timestamp in records:
-                    pipeline.get(self.prefix_key(make_record_key(timeline, key)))
+                    pipeline.get(make_record_key(timeline_key, key))
 
                 for (key, timestamp), value in zip(records, pipeline.execute()):
                     # We have to handle failures if the key does not exist --
@@ -286,18 +298,20 @@ class RedisBackend(Backend):
                         # TODO: Could make this lazy, but that might be unnecessary complexity.
                         yield Record(key, self.codec.decode(value), timestamp)
 
-        yield itertools.islice(get_records_for_digest(), self.capacity)
+        properties = self.codec.decode(connection.get(make_properties_key(timeline_key)))
+
+        yield properties, itertools.islice(get_records_for_digest(), self.capacity)
 
         def reschedule():
             with connection.pipeline() as pipeline:
                 pipeline.watch(digest_key)  # This shouldn't be necessary, but better safe than sorry?
                 pipeline.multi()
 
-                record_keys = [self.prefix_key(make_record_key(timeline, key)) for key, score in records]
+                record_keys = [make_record_key(timeline_key, key) for key, score in records]
                 pipeline.delete(digest_key, *record_keys)
-                pipeline.zrem(self.prefix_key(make_schedule_key(READY_STATE)), timeline)
-                pipeline.zadd(self.prefix_key(make_schedule_key(WAITING_STATE)), time.time() + self.backoff(iteration + 1), timeline)
-                pipeline.set(self.prefix_key(make_iteration_key(timeline)), iteration + 1)
+                pipeline.zrem(make_schedule_key(READY_STATE), timeline_identifier)
+                pipeline.zadd(make_schedule_key(WAITING_STATE), time.time() + self.backoff(iteration + 1), timeline_identifier)
+                pipeline.set(make_iteration_key(timeline_key), iteration + 1)
                 pipeline.execute()
 
         def unschedule():
@@ -307,9 +321,10 @@ class RedisBackend(Backend):
                 pipeline.watch(timeline_key)
                 pipeline.multi()
                 if connection.zcard(timeline_key) is 0:
-                    pipeline.delete(self.prefix_key(make_iteration_key(timeline)))
-                    pipeline.zrem(self.prefix_key(make_schedule_key(READY_STATE)), timeline)
-                    pipeline.zrem(self.prefix_key(make_schedule_key(WAITING_STATE)), timeline)
+                    pipeline.delete(make_iteration_key(timeline_key))
+                    pipeline.delete(make_properties_key(timeline_key))
+                    pipeline.zrem(make_schedule_key(READY_STATE), timeline_identifier)
+                    pipeline.zrem(make_schedule_key(WAITING_STATE), timeline_identifier)
                     pipeline.execute()
 
         # If there were records in the digest, we need to schedule it so that
diff --git a/timelines.py b/timelines.py
index b01ef9d036..f0b334c1db 100644
--- a/timelines.py
+++ b/timelines.py
@@ -36,7 +36,8 @@ with timer('Generated {0} records to be loaded into {1} timelines'.format(n_reco
     for i in xrange(0, n_records):
         p = random.randint(1, n_timelines)
         record = Record(uuid.uuid1().hex, payload, time.time())
-        calls.append(functools.partial(timelines.add, 'projects/{0}'.format(p), record))
+        properties = {'plugin': random.choice(('foo', 'bar', 'baz'))}
+        calls.append(functools.partial(timelines.add, 'projects/{0}'.format(p), properties, record))
 
 
 with timer('Loaded {0} records'.format(len(calls))):
@@ -81,7 +82,7 @@ with timer('Scheduled timelines for digestion'):
 
 with timer('Digested {0} timelines'.format(len(ready))):
     for timeline in ready:
-        with timelines.digest(timeline) as records:
+        with timelines.digest(timeline) as (properties, records):
             i = 0
             for i, record in enumerate(records, 1):
                 pass
