commit 742153e33fdd67f370c7bdc1d9f4759d38239a7d
Author: David Cramer <dcramer@gmail.com>
Date:   Thu Apr 9 09:09:55 2015 -0400

    Switch off of task loggers

diff --git a/src/sentry/management/commands/cleanup.py b/src/sentry/management/commands/cleanup.py
index 29aec9826e..3217f6886b 100644
--- a/src/sentry/management/commands/cleanup.py
+++ b/src/sentry/management/commands/cleanup.py
@@ -21,10 +21,9 @@ class Command(BaseCommand):
 
     def handle(self, **options):
         import logging
-        from sentry.tasks.cleanup import cleanup
+        from sentry.tasks.cleanup import cleanup, logger
 
         if options['verbosity'] > 1:
-            logger = cleanup.get_logger()
             logger.setLevel(logging.DEBUG)
             logger.addHandler(logging.StreamHandler())
 
diff --git a/src/sentry/tasks/cleanup.py b/src/sentry/tasks/cleanup.py
index f6992b45dd..3dba2c6fed 100644
--- a/src/sentry/tasks/cleanup.py
+++ b/src/sentry/tasks/cleanup.py
@@ -7,10 +7,13 @@ sentry.tasks.cleanup
 """
 from __future__ import absolute_import
 
+from celery.utils.log import get_task_logger
 from nydus.utils import ThreadPool
 
 from sentry.tasks.base import instrumented_task
 
+logger = get_task_logger(__name__)
+
 
 def delete_object(item):
     item.delete()
@@ -51,26 +54,24 @@ def cleanup(days=30, project=None, chunk_size=1000, concurrency=1, **kwargs):
         (Group, 'last_seen'),
     )
 
-    log = cleanup.get_logger()
-
     ts = timezone.now() - datetime.timedelta(days=days)
 
-    log.info("Removing expired values for LostPasswordHash")
+    logger.info("Removing expired values for LostPasswordHash")
     LostPasswordHash.objects.filter(
         date_added__lte=timezone.now() - datetime.timedelta(hours=48)
     ).delete()
 
     # TODO: we should move this into individual backends
     if not project:
-        log.info("Removing old Node values")
+        logger.info("Removing old Node values")
         try:
             app.nodestore.cleanup(ts)
         except NotImplementedError:
-            log.warning("Node backend does not support cleanup operation")
+            logger.warning("Node backend does not support cleanup operation")
 
     # Remove types which can easily be bound to project + date
     for model, date_col in GENERIC_DELETES:
-        log.info("Removing %s for days=%s project=%s", model.__name__, days, project or '*')
+        logger.info("Removing %s for days=%s project=%s", model.__name__, days, project or '*')
         qs = model.objects.filter(**{'%s__lte' % (date_col,): ts})
         if project:
             qs = qs.filter(project=project)
@@ -78,7 +79,7 @@ def cleanup(days=30, project=None, chunk_size=1000, concurrency=1, **kwargs):
 
         count = 0
         while qs.exists():
-            log.info("Removing %s chunk %d", model.__name__, count)
+            logger.info("Removing %s chunk %d", model.__name__, count)
             if concurrency > 1:
                 worker_pool = ThreadPool(workers=concurrency)
                 for obj in qs[:chunk_size].iterator():
@@ -93,7 +94,7 @@ def cleanup(days=30, project=None, chunk_size=1000, concurrency=1, **kwargs):
     # EventMapping is fairly expensive and is special cased as it's likely you
     # won't need a reference to an event for nearly as long
     if days > 7:
-        log.info("Removing expired values for EventMapping")
+        logger.info("Removing expired values for EventMapping")
         EventMapping.objects.filter(
             date_added__lte=timezone.now() - datetime.timedelta(days=7)
         ).delete()
diff --git a/src/sentry/tasks/deletion.py b/src/sentry/tasks/deletion.py
index 936e531aaa..d1e597c37e 100644
--- a/src/sentry/tasks/deletion.py
+++ b/src/sentry/tasks/deletion.py
@@ -8,11 +8,15 @@ sentry.tasks.deletion
 
 from __future__ import absolute_import
 
-from sentry.tasks.base import instrumented_task, retry
 
+from celery.utils.log import get_task_logger
 from django.db import connections
+
+from sentry.tasks.base import instrumented_task, retry
 from sentry.utils import db
 
+logger = get_task_logger(__name__)
+
 
 @instrumented_task(name='sentry.tasks.deletion.delete_organization', queue='cleanup',
                    default_retry_delay=60 * 5, max_retries=None)
@@ -30,7 +34,6 @@ def delete_organization(object_id, continuous=True, **kwargs):
     if o.status != OrganizationStatus.DELETION_IN_PROGRESS:
         o.update(status=OrganizationStatus.DELETION_IN_PROGRESS)
 
-    logger = delete_organization.get_logger()
     for team in Team.objects.filter(organization=o).order_by('id')[:1]:
         logger.info('Removing Team id=%s where organization=%s', team.id, o.id)
         delete_team(team.id, continuous=False)
@@ -64,8 +67,6 @@ def delete_team(object_id, continuous=True, **kwargs):
     if t.status != TeamStatus.DELETION_IN_PROGRESS:
         t.update(status=TeamStatus.DELETION_IN_PROGRESS)
 
-    logger = delete_team.get_logger()
-
     # Delete 1 project at a time since this is expensive by itself
     for project in Project.objects.filter(team=t).order_by('id')[:1]:
         logger.info('Removing Project id=%s where team=%s', project.id, t.id)
@@ -102,8 +103,6 @@ def delete_project(object_id, continuous=True, **kwargs):
     if p.status != ProjectStatus.DELETION_IN_PROGRESS:
         p.update(status=ProjectStatus.DELETION_IN_PROGRESS)
 
-    logger = delete_project.get_logger()
-
     # XXX: remove keys first to prevent additional data from flowing in
     model_list = (
         ProjectKey, TagKey, TagValue, GroupTagKey, GroupTagValue, EventMapping,
@@ -146,8 +145,6 @@ def delete_group(object_id, continuous=True, **kwargs):
     except Group.DoesNotExist:
         return
 
-    logger = delete_group.get_logger()
-
     bulk_model_list = (
         GroupHash, GroupRuleStatus, GroupTagValue, GroupTagKey, EventMapping
     )
@@ -179,8 +176,6 @@ def delete_tag_key(object_id, continuous=True, **kwargs):
     except TagKey.DoesNotExist:
         return
 
-    logger = delete_tag_key.get_logger()
-
     if tagkey.status != TagKeyStatus.DELETION_IN_PROGRESS:
         tagkey.update(status=TagKeyStatus.DELETION_IN_PROGRESS)
 
diff --git a/src/sentry/tasks/merge.py b/src/sentry/tasks/merge.py
index a6ad7c005c..a9cab52907 100644
--- a/src/sentry/tasks/merge.py
+++ b/src/sentry/tasks/merge.py
@@ -8,11 +8,14 @@ sentry.tasks.merge
 
 from __future__ import absolute_import
 
+from celery.utils.log import get_task_logger
 from django.db import IntegrityError, transaction
 from django.db.models import F
 
 from sentry.tasks.base import instrumented_task, retry
 
+logger = get_task_logger(__name__)
+
 
 @instrumented_task(name='sentry.tasks.merge.merge_group', queue='cleanup',
                    default_retry_delay=60 * 5, max_retries=None)
@@ -23,8 +26,6 @@ def merge_group(from_object_id, to_object_id, **kwargs):
         EventMapping, Event
     )
 
-    logger = merge_group.get_logger()
-
     try:
         group = Group.objects.get(id=from_object_id)
     except Group.DoesNotExist:
diff --git a/src/sentry/tasks/post_process.py b/src/sentry/tasks/post_process.py
index 09c878811d..7f1b3d23b4 100644
--- a/src/sentry/tasks/post_process.py
+++ b/src/sentry/tasks/post_process.py
@@ -8,8 +8,7 @@ sentry.tasks.post_process
 
 from __future__ import absolute_import, print_function
 
-import logging
-
+from celery.utils.log import get_task_logger
 from django.conf import settings
 from hashlib import md5
 
@@ -19,8 +18,7 @@ from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
 
-
-rules_logger = logging.getLogger('sentry')
+logger = get_task_logger(__name__)
 
 
 def _capture_stats(event, is_new):
@@ -95,8 +93,6 @@ def plugin_post_process_group(plugin_slug, event, **kwargs):
 def record_affected_user(event, **kwargs):
     from sentry.models import Group
 
-    logger = record_affected_user.get_logger()
-
     if not settings.SENTRY_ENABLE_EXPLORE_USERS:
         logger.info('Skipping sentry:user tag due to SENTRY_ENABLE_EXPLORE_USERS')
         return
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 2cdecfc807..888cbd0af7 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -8,10 +8,13 @@ sentry.tasks.store
 
 from __future__ import absolute_import
 
+from celery.utils.log import get_task_logger
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
 from sentry.utils.safe import safe_execute
 
+logger = get_task_logger(__name__)
+
 
 @instrumented_task(
     name='sentry.tasks.store.preprocess_event',
@@ -22,8 +25,6 @@ def preprocess_event(cache_key=None, data=None, **kwargs):
     if cache_key:
         data = default_cache.get(cache_key)
 
-    logger = preprocess_event.get_logger()
-
     if data is None:
         logger.error('Data not available in preprocess_event (cache_key=%s)', cache_key)
         return
