commit a5000fbc55422104e07de9378ec12f1413105f40
Author: ted kaemming <ted@getsentry.com>
Date:   Tue Jul 30 10:41:57 2019 -0700

    ref: Remove Kafka pipeline consumer, batching-kafka-consumer (#14143)

diff --git a/requirements-base.txt b/requirements-base.txt
index d61063cc76..2c31bbe211 100644
--- a/requirements-base.txt
+++ b/requirements-base.txt
@@ -1,4 +1,3 @@
-batching-kafka-consumer==0.0.3
 BeautifulSoup>=3.2.1
 boto3>=1.4.1,<1.4.6
 botocore<1.5.71
diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 162e12f6f4..77c53c5903 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -1718,25 +1718,10 @@ KAFKA_CLUSTERS = {
     }
 }
 
-KAFKA_PREPROCESS = 'events-preprocess'
-KAFKA_PROCESS = 'events-process'
-KAFKA_SAVE = 'events-save'
 KAFKA_EVENTS = 'events'
 KAFKA_OUTCOMES = 'outcomes'
 
 KAFKA_TOPICS = {
-    KAFKA_PREPROCESS: {
-        'cluster': 'default',
-        'topic': KAFKA_PREPROCESS,
-    },
-    KAFKA_PROCESS: {
-        'cluster': 'default',
-        'topic': KAFKA_PROCESS,
-    },
-    KAFKA_SAVE: {
-        'cluster': 'default',
-        'topic': KAFKA_SAVE,
-    },
     KAFKA_EVENTS: {
         'cluster': 'default',
         'topic': KAFKA_EVENTS,
diff --git a/src/sentry/consumer.py b/src/sentry/consumer.py
deleted file mode 100644
index 44243242e4..0000000000
--- a/src/sentry/consumer.py
+++ /dev/null
@@ -1,69 +0,0 @@
-from __future__ import absolute_import, print_function
-
-from batching_kafka_consumer import AbstractBatchWorker
-
-from django.conf import settings
-
-import sentry.tasks.store as store_tasks
-from sentry.utils import json
-
-
-class ConsumerWorker(AbstractBatchWorker):
-    def __init__(self):
-        self.dispatch = {}
-        for key, handler in (
-            (settings.KAFKA_PREPROCESS, self.handle_preprocess),
-            (settings.KAFKA_PROCESS, self.handle_process),
-            (settings.KAFKA_SAVE, self.handle_save)
-        ):
-            topic = settings.KAFKA_TOPICS[key]['topic']
-            self.dispatch[topic] = handler
-
-    def handle_preprocess(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-        process_task = (
-            store_tasks.process_event_from_reprocessing
-            if message['from_reprocessing']
-            else store_tasks.process_event
-        )
-
-        store_tasks._do_preprocess_event(cache_key, data, start_time, event_id, process_task)
-
-    def handle_process(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-
-        if message['from_reprocessing']:
-            task = store_tasks.process_event_from_reprocessing
-        else:
-            task = store_tasks.process_event
-
-        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
-
-    def handle_save(self, message):
-        data = message['data']
-        event_id = data['event_id']
-        cache_key = message['cache_key']
-        start_time = message['start_time']
-        project_id = data['project']
-
-        store_tasks._do_save_event(cache_key, data, start_time, event_id, project_id)
-
-    def process_message(self, message):
-        topic = message.topic()
-        return self._handle(topic, json.loads(message.value()))
-
-    def _handle(self, topic, message):
-        handler = self.dispatch[topic]
-        return handler(message)
-
-    def flush_batch(self, batch):
-        pass
-
-    def shutdown(self):
-        pass
diff --git a/src/sentry/coreapi.py b/src/sentry/coreapi.py
index 5bb2e57b79..2d73febe15 100644
--- a/src/sentry/coreapi.py
+++ b/src/sentry/coreapi.py
@@ -10,20 +10,18 @@ import re
 import six
 import zlib
 
-from django.conf import settings
 from django.core.exceptions import SuspiciousOperation
 from django.utils.crypto import constant_time_compare
 from gzip import GzipFile
 from six import BytesIO
 from time import time
 
-from sentry import features
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.models import ProjectKey
 from sentry.tasks.store import preprocess_event, \
     preprocess_event_from_reprocessing
-from sentry.utils import kafka, json
+from sentry.utils import json
 from sentry.utils.auth import parse_auth_header
 from sentry.utils.http import origin_from_request
 from sentry.utils.strings import decompress
@@ -176,25 +174,10 @@ class ClientApiHelper(object):
         if attachments is not None:
             attachment_cache.set(cache_key, attachments, cache_timeout)
 
-        # NOTE: Project is bound to the context in most cases in production, which
-        # is enough for us to do `projects:kafka-ingest` testing.
-        project = self.context and self.context.project
-
-        if project and features.has('projects:kafka-ingest', project=project):
-            kafka.produce_sync(
-                settings.KAFKA_PREPROCESS,
-                value=json.dumps({
-                    'cache_key': cache_key,
-                    'start_time': start_time,
-                    'from_reprocessing': from_reprocessing,
-                    'data': data,
-                }),
-            )
-        else:
-            task = from_reprocessing and \
-                preprocess_event_from_reprocessing or preprocess_event
-            task.delay(cache_key=cache_key, start_time=start_time,
-                       event_id=data['event_id'])
+        task = from_reprocessing and \
+            preprocess_event_from_reprocessing or preprocess_event
+        task.delay(cache_key=cache_key, start_time=start_time,
+                   event_id=data['event_id'])
 
 
 @six.add_metaclass(abc.ABCMeta)
diff --git a/src/sentry/features/__init__.py b/src/sentry/features/__init__.py
index 36e887274a..127e47c669 100644
--- a/src/sentry/features/__init__.py
+++ b/src/sentry/features/__init__.py
@@ -95,7 +95,6 @@ default_manager.add('projects:sample-events', ProjectFeature)  # NOQA
 default_manager.add('projects:servicehooks', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-view', ProjectFeature)  # NOQA
 default_manager.add('projects:similarity-indexing', ProjectFeature)  # NOQA
-default_manager.add('projects:kafka-ingest', ProjectFeature)  # NOQA
 
 # Project plugin features
 default_manager.add('projects:plugins', ProjectPluginFeature)  # NOQA
diff --git a/src/sentry/runner/__init__.py b/src/sentry/runner/__init__.py
index 872fedf117..8d9620c370 100644
--- a/src/sentry/runner/__init__.py
+++ b/src/sentry/runner/__init__.py
@@ -49,7 +49,7 @@ list(
         lambda cmd: cli.add_command(import_string(cmd)), (
             'sentry.runner.commands.backup.export', 'sentry.runner.commands.backup.import_',
             'sentry.runner.commands.cleanup.cleanup', 'sentry.runner.commands.config.config',
-            'sentry.runner.commands.consumer.consumer', 'sentry.runner.commands.createuser.createuser',
+            'sentry.runner.commands.createuser.createuser',
             'sentry.runner.commands.devserver.devserver', 'sentry.runner.commands.django.django',
             'sentry.runner.commands.exec.exec_', 'sentry.runner.commands.files.files',
             'sentry.runner.commands.help.help', 'sentry.runner.commands.init.init',
diff --git a/src/sentry/runner/commands/consumer.py b/src/sentry/runner/commands/consumer.py
deleted file mode 100644
index 29e450044f..0000000000
--- a/src/sentry/runner/commands/consumer.py
+++ /dev/null
@@ -1,49 +0,0 @@
-from __future__ import absolute_import, print_function
-
-import click
-import signal
-from django.conf import settings
-
-from sentry.runner.decorators import configuration
-
-
-@click.command()
-@click.option('--topic', multiple=True, required=True,
-              help='Topic(s) to consume from.')
-@click.option('--consumer-group', default='sentry-consumers',
-              help='Consumer group name.')
-@click.option('--bootstrap-server', default=['localhost:9092'], multiple=True,
-              help='Kafka bootstrap server(s) to use.')
-@click.option('--max-batch-size', default=10000,
-              help='Max number of messages to batch in memory before committing offsets to Kafka.')
-@click.option('--max-batch-time-ms', default=60000,
-              help='Max length of time to buffer messages in memory before committing offsets to Kafka.')
-@click.option('--auto-offset-reset', default='error', type=click.Choice(['error', 'earliest', 'latest']),
-              help='Kafka consumer auto offset reset.')
-@configuration
-def consumer(**options):
-    from batching_kafka_consumer import BatchingKafkaConsumer
-    from sentry.consumer import ConsumerWorker
-
-    known_topics = {x['topic'] for x in settings.KAFKA_TOPICS.values()}
-    topics = options['topic']
-    for topic in topics:
-        if topic not in known_topics:
-            raise RuntimeError("topic '%s' is not one of: %s" % (topic, known_topics))
-
-    consumer = BatchingKafkaConsumer(
-        topics=options['topic'],
-        worker=ConsumerWorker(),
-        max_batch_size=options['max_batch_size'],
-        max_batch_time=options['max_batch_time_ms'],
-        bootstrap_servers=options['bootstrap_server'],
-        group_id=options['consumer_group'],
-        auto_offset_reset=options['auto_offset_reset'],
-    )
-
-    def handler(signum, frame):
-        consumer.signal_shutdown()
-
-    signal.signal(signal.SIGINT, handler)
-
-    consumer.run()
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 84a0bd763f..f809bad6bd 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -5,7 +5,6 @@ from datetime import datetime
 import six
 
 from time import time
-from django.conf import settings
 from django.utils import timezone
 
 from semaphore.processing import StoreNormalizer
@@ -15,7 +14,7 @@ from sentry.constants import DEFAULT_STORE_NORMALIZER_ARGS
 from sentry.attachments import attachment_cache
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
-from sentry.utils import json, kafka, metrics
+from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
 from sentry.stacktraces.processing import process_stacktraces, \
     should_process_for_stacktraces
@@ -68,39 +67,18 @@ def should_process(data):
 
 
 def submit_process(project, from_reprocessing, cache_key, event_id, start_time, data):
-    if features.has('projects:kafka-ingest', project=project):
-        kafka.produce_sync(
-            settings.KAFKA_PROCESS,
-            value=json.dumps({
-                'cache_key': cache_key,
-                'start_time': start_time,
-                'from_reprocessing': from_reprocessing,
-                'data': data,
-            }),
-        )
-    else:
-        task = process_event_from_reprocessing if from_reprocessing else process_event
-        task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
+    task = process_event_from_reprocessing if from_reprocessing else process_event
+    task.delay(cache_key=cache_key, start_time=start_time, event_id=event_id)
 
 
 def submit_save_event(project, cache_key, event_id, start_time, data):
-    if features.has('projects:kafka-ingest', project=project):
-        kafka.produce_sync(
-            settings.KAFKA_SAVE,
-            value=json.dumps({
-                'cache_key': cache_key,
-                'start_time': start_time,
-                'data': data,
-            }),
-        )
-    else:
-        if cache_key:
-            data = None
+    if cache_key:
+        data = None
 
-        save_event.delay(
-            cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
-            project_id=project.id
-        )
+    save_event.delay(
+        cache_key=cache_key, data=data, start_time=start_time, event_id=event_id,
+        project_id=project.id
+    )
 
 
 def _do_preprocess_event(cache_key, data, start_time, event_id, process_task):
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
index 2b78369986..fa6bf1023c 100644
--- a/src/sentry/utils/kafka.py
+++ b/src/sentry/utils/kafka.py
@@ -33,24 +33,3 @@ class ProducerManager(object):
 
 
 producers = ProducerManager()
-
-
-def delivery_callback(error, message):
-    if error is not None:
-        logger.error('Could not publish message (error: %s): %r', error, message)
-
-
-def produce_sync(topic_key, **kwargs):
-    producer = producers.get(topic_key)
-
-    try:
-        producer.produce(
-            topic=settings.KAFKA_TOPICS[topic_key]['topic'],
-            on_delivery=delivery_callback,
-            **kwargs
-        )
-    except Exception as error:
-        logger.error('Could not publish message: %s', error, exc_info=True)
-        return
-
-    producer.flush()
diff --git a/tests/sentry/consumer/test_consumer.py b/tests/sentry/consumer/test_consumer.py
deleted file mode 100644
index da77b9b4a0..0000000000
--- a/tests/sentry/consumer/test_consumer.py
+++ /dev/null
@@ -1,102 +0,0 @@
-from __future__ import absolute_import, print_function
-
-from mock import patch
-
-from sentry.consumer import ConsumerWorker
-from sentry.coreapi import ClientApiHelper
-from sentry.models import Event
-from sentry.plugins import Plugin2
-from sentry.testutils import PluginTestCase
-from sentry.utils import json
-
-
-class BasicPreprocessorPlugin(Plugin2):
-    def add_foo(self, data):
-        data['foo'] = 'bar'
-        return data
-
-    def get_event_preprocessors(self, data):
-        if data.get('platform') == 'needs_process':
-            return [self.add_foo]
-
-        return []
-
-    def is_enabled(self, project=None):
-        return True
-
-
-class TestConsumer(PluginTestCase):
-    plugin = BasicPreprocessorPlugin
-
-    def _call_consumer(self, mock_produce):
-        args, kwargs = list(mock_produce.call_args)
-        mock_produce.call_args = None
-        topic = args[0]
-        value = json.loads(kwargs['value'])
-
-        consumer = ConsumerWorker()
-        consumer._handle(topic, value)
-
-    def _create_event_with_platform(self, project, platform):
-        from sentry.event_manager import EventManager
-        em = EventManager({}, project=project)
-        em.normalize()
-        data = em.get_data()
-        data['platform'] = platform
-        return data
-
-    @patch('sentry.tasks.store.save_event')
-    @patch('sentry.tasks.store.preprocess_event')
-    @patch('sentry.utils.kafka.produce_sync')
-    def test_process_path(self, mock_produce, mock_preprocess_event, mock_save_event):
-        with self.feature('projects:kafka-ingest'):
-            project = self.create_project()
-            data = self._create_event_with_platform(project, 'needs_process')
-
-            helper = ClientApiHelper(project_id=self.project.id)
-            helper.context.bind_project(project)
-            helper.insert_data_to_database(data)
-
-            # preprocess
-            self._call_consumer(mock_produce)
-            # process
-            with self.tasks():
-                self._call_consumer(mock_produce)
-            # save
-            self._call_consumer(mock_produce)
-
-            assert mock_preprocess_event.delay.call_count == 0
-            assert mock_save_event.delay.call_count == 0
-
-            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
-            saved_data = event.get_raw_data()
-            assert saved_data['foo'] == 'bar'
-            assert saved_data['platform'] == 'needs_process'
-
-    @patch('sentry.tasks.store.save_event')
-    @patch('sentry.tasks.store.process_event')
-    @patch('sentry.tasks.store.preprocess_event')
-    @patch('sentry.utils.kafka.produce_sync')
-    def test_save_path(self, mock_produce, mock_preprocess_event,
-                       mock_process_event, mock_save_event):
-        with self.feature('projects:kafka-ingest'):
-            project = self.create_project()
-            data = self._create_event_with_platform(project, 'doesnt_need_process')
-
-            helper = ClientApiHelper(project_id=self.project.id)
-            helper.context.bind_project(project)
-            helper.insert_data_to_database(data)
-
-            # preprocess
-            self._call_consumer(mock_produce)
-            # save
-            self._call_consumer(mock_produce)
-
-            assert mock_preprocess_event.delay.call_count == 0
-            assert mock_process_event.delay.call_count == 0
-            assert mock_save_event.delay.call_count == 0
-
-            event = Event.objects.get(project_id=project.id, event_id=data['event_id'])
-            saved_data = event.get_raw_data()
-            assert 'foo' not in saved_data
-            assert saved_data['platform'] == 'doesnt_need_process'
