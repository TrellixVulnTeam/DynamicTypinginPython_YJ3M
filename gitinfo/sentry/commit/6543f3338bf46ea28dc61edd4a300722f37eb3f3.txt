commit 6543f3338bf46ea28dc61edd4a300722f37eb3f3
Author: Matt Robenolt <matt@ydekproductions.com>
Date:   Thu Jul 5 14:24:15 2018 -0700

    feat: Add EventStream service abstraction (#8879)

diff --git a/src/sentry/conf/server.py b/src/sentry/conf/server.py
index 5da348ac5b..51ea39703b 100644
--- a/src/sentry/conf/server.py
+++ b/src/sentry/conf/server.py
@@ -255,6 +255,7 @@ INSTALLED_APPS = (
     'sentry.lang.javascript', 'sentry.lang.native', 'sentry.plugins.sentry_interface_types',
     'sentry.plugins.sentry_mail', 'sentry.plugins.sentry_urls', 'sentry.plugins.sentry_useragents',
     'sentry.plugins.sentry_webhooks', 'social_auth', 'sudo', 'sentry.tagstore',
+    'sentry.eventstream',
 )
 
 import django
@@ -985,6 +986,9 @@ SENTRY_TSDB_OPTIONS = {}
 SENTRY_NEWSLETTER = 'sentry.newsletter.base.Newsletter'
 SENTRY_NEWSLETTER_OPTIONS = {}
 
+SENTRY_EVENTSTREAM = 'sentry.eventstream.base.EventStream'
+SENTRY_EVENTSTREAM_OPTIONS = {}
+
 # rollups must be ordered from highest granularity to lowest
 SENTRY_TSDB_ROLLUPS = (
     # (time in seconds, samples to keep)
diff --git a/src/sentry/event_manager.py b/src/sentry/event_manager.py
index 4f5dbe81c3..73a096b26d 100644
--- a/src/sentry/event_manager.py
+++ b/src/sentry/event_manager.py
@@ -20,7 +20,7 @@ from django.utils.encoding import force_bytes, force_text
 from hashlib import md5
 from uuid import uuid4
 
-from sentry import buffer, eventtypes, features, tsdb
+from sentry import buffer, eventtypes, eventstream, features, tsdb
 from sentry.constants import (
     CLIENT_RESERVED_ATTRS, LOG_LEVELS, LOG_LEVELS_MAP, DEFAULT_LOG_LEVEL,
     DEFAULT_LOGGER_NAME, MAX_CULPRIT_LENGTH, VALID_PLATFORMS
@@ -42,7 +42,6 @@ from sentry.tasks.merge import merge_group
 from sentry.utils import metrics
 from sentry.utils.cache import default_cache
 from sentry.utils.db import get_db_engine
-from sentry.utils.imports import import_string
 from sentry.utils.safe import safe_execute, trim, trim_dict, get_path
 from sentry.utils.strings import truncatechars
 from sentry.utils.validators import is_float
@@ -54,14 +53,6 @@ DEFAULT_FINGERPRINT_VALUES = frozenset(['{{ default }}', '{{default}}'])
 ALLOWED_FUTURE_DELTA = timedelta(minutes=1)
 
 
-post_process_callback = getattr(settings, 'SENTRY_POST_PROCESS_CALLBACK', None)
-if post_process_callback is None:
-    from sentry.tasks.post_process import post_process_group
-    post_process_callback = post_process_group.delay
-else:
-    post_process_callback = import_string(post_process_callback)
-
-
 def count_limit(count):
     # TODO: could we do something like num_to_store = max(math.sqrt(100*count)+59, 200) ?
     # ~ 150 * ((log(n) - 1.5) ^ 2 - 0.25)
@@ -940,17 +931,21 @@ class EventManager(object):
                 project.update(first_event=date)
                 first_event_received.send(project=project, group=group, sender=Project)
 
-            post_process_callback(
-                group=group,
-                event=event,
-                is_new=is_new,
-                is_sample=is_sample,
-                is_regression=is_regression,
-                is_new_group_environment=is_new_group_environment,
-                primary_hash=hashes[0],
-            )
-        else:
-            self.logger.info('post_process.skip.raw_event', extra={'event_id': event.id})
+        eventstream.publish(
+            group=group,
+            event=event,
+            is_new=is_new,
+            is_sample=is_sample,
+            is_regression=is_regression,
+            is_new_group_environment=is_new_group_environment,
+            primary_hash=hashes[0],
+            # We are choosing to skip consuming the event back
+            # in the eventstream if it's flagged as raw.
+            # This means that we want to publish the event
+            # through the event stream, but we don't care
+            # about post processing and handling the commit.
+            skip_consume=raw,
+        )
 
         metrics.timing(
             'events.latency',
diff --git a/src/sentry/eventstream/__init__.py b/src/sentry/eventstream/__init__.py
new file mode 100644
index 0000000000..71e53ec5ba
--- /dev/null
+++ b/src/sentry/eventstream/__init__.py
@@ -0,0 +1,14 @@
+from __future__ import absolute_import
+
+from django.conf import settings
+
+from sentry.utils.services import LazyServiceWrapper
+
+from .base import EventStream
+
+backend = LazyServiceWrapper(
+    EventStream,
+    settings.SENTRY_EVENTSTREAM,
+    settings.SENTRY_EVENTSTREAM_OPTIONS,
+)
+backend.expose(locals())
diff --git a/src/sentry/eventstream/base.py b/src/sentry/eventstream/base.py
new file mode 100644
index 0000000000..9ebbac9e0b
--- /dev/null
+++ b/src/sentry/eventstream/base.py
@@ -0,0 +1,29 @@
+from __future__ import absolute_import
+
+import logging
+
+from sentry.utils.services import Service
+from sentry.tasks.post_process import post_process_group
+
+
+logger = logging.getLogger(__name__)
+
+
+class EventStream(Service):
+    __all__ = (
+        'publish',
+    )
+
+    def publish(self, group, event, is_new, is_sample, is_regression, is_new_group_environment, primary_hash, skip_consume=False):
+        if skip_consume:
+            logger.info('post_process.skip.raw_event', extra={'event_id': event.id})
+        else:
+            post_process_group.delay(
+                group=group,
+                event=event,
+                is_new=is_new,
+                is_sample=is_sample,
+                is_regression=is_regression,
+                is_new_group_environment=is_new_group_environment,
+                primary_hash=primary_hash,
+            )
diff --git a/src/sentry/eventstream/kafka.py b/src/sentry/eventstream/kafka.py
new file mode 100644
index 0000000000..115569cece
--- /dev/null
+++ b/src/sentry/eventstream/kafka.py
@@ -0,0 +1,66 @@
+from __future__ import absolute_import
+
+import logging
+
+from kafka import KafkaProducer
+from django.utils.functional import cached_property
+
+from sentry import quotas
+from sentry.models import Organization
+from sentry.eventstream.base import EventStream
+from sentry.utils import json
+from sentry.utils.pubsub import QueuedPublisher
+
+logger = logging.getLogger(__name__)
+
+
+# Beware! Changing this, or the message format/fields themselves requires
+# consideration of all downstream consumers.
+# Version 0 format: (0, '(insert|delete)', {..event json...})
+EVENT_PROTOCOL_VERSION = 0
+
+
+class KafkaPublisher(object):
+    def __init__(self, connection):
+        self.connection = connection or {}
+
+    @cached_property
+    def client(self):
+        return KafkaProducer(**self.connection)
+
+    def publish(self, topic, value, key=None):
+        return self.client.send(topic, key=key, value=value)
+
+
+class KafkaEventStream(EventStream):
+    def __init__(self, publish_topic='events', sync=False, connection=None, **options):
+        self.publish_topic = publish_topic
+        self.pubsub = KafkaPublisher(connection)
+        if not sync:
+            self.pubsub = QueuedPublisher(self.pubsub)
+
+    def publish(self, group, event, is_new, is_sample, is_regression, is_new_group_environment, primary_hash, skip_consume=False):
+        project = event.project
+        retention_days = quotas.get_event_retention(
+            organization=Organization(project.organization_id)
+        )
+
+        try:
+            key = '%s:%s' % (event.project_id, event.event_id)
+            value = (EVENT_PROTOCOL_VERSION, 'insert', {
+                'group_id': event.group_id,
+                'event_id': event.event_id,
+                'organization_id': project.organization_id,
+                'project_id': event.project_id,
+                'message': event.message,
+                'platform': event.platform,
+                'datetime': event.datetime,
+                'data': event.data.data,
+                'primary_hash': primary_hash,
+                'retention_days': retention_days,
+            })
+
+            self.pubsub.publish(self.publish_topic, key=key.encode('utf-8'), value=json.dumps(value))
+        except Exception as error:
+            logger.warning('Could not publish event: %s', error, exc_info=True)
+            raise
diff --git a/tests/sentry/test_event_manager.py b/tests/sentry/test_event_manager.py
index 387065484a..fbc23c7143 100644
--- a/tests/sentry/test_event_manager.py
+++ b/tests/sentry/test_event_manager.py
@@ -924,8 +924,8 @@ class EventManagerTest(TransactionTestCase):
 
         assert dict(event.tags).get('environment') == 'beta'
 
-    @mock.patch('sentry.event_manager.post_process_callback')
-    def test_group_environment(self, mock_post_process_callback):
+    @mock.patch('sentry.event_manager.eventstream.publish')
+    def test_group_environment(self, eventstream_publish):
         release_version = '1.0'
 
         def save_event():
@@ -952,7 +952,7 @@ class EventManagerTest(TransactionTestCase):
 
         # Ensure that the first event in the (group, environment) pair is
         # marked as being part of a new environment.
-        mock_post_process_callback.assert_called_with(
+        eventstream_publish.assert_called_with(
             group=event.group,
             event=event,
             is_new=True,
@@ -960,13 +960,14 @@ class EventManagerTest(TransactionTestCase):
             is_regression=False,
             is_new_group_environment=True,
             primary_hash='acbd18db4cc2f85cedef654fccc4a4d8',
+            skip_consume=False,
         )
 
         event = save_event()
 
         # Ensure that the next event in the (group, environment) pair is *not*
         # marked as being part of a new environment.
-        mock_post_process_callback.assert_called_with(
+        eventstream_publish.assert_called_with(
             group=event.group,
             event=event,
             is_new=False,
@@ -974,6 +975,7 @@ class EventManagerTest(TransactionTestCase):
             is_regression=None,  # XXX: wut
             is_new_group_environment=False,
             primary_hash='acbd18db4cc2f85cedef654fccc4a4d8',
+            skip_consume=False,
         )
 
     def test_default_fingerprint(self):
