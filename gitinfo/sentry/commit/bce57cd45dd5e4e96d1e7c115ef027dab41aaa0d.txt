commit bce57cd45dd5e4e96d1e7c115ef027dab41aaa0d
Author: Mark Story <mark@sentry.io>
Date:   Thu Nov 28 13:05:26 2019 -0500

    feat(discover) Start high level discover dataset API (#15788)
    
    With the discover dataset ready in snuba we can start using it. However,
    before we do that it would be good to have a reasonable API to contain
    the discover logic and keep it out of the endpoints.
    
    This new module aims to provide that API. It currently pulls in a few
    functions from api.event_search, but longer term I'd like to move those
    functions once they have been disentangled from the current discover
    code.
    
    Update internally generated function expressions to use None instead
    of '' as it is easier to special case, as we don't get it from users.

diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index b2b8579552..09036fced9 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -681,7 +681,7 @@ FIELD_ALIASES = {
     "last_seen": {"aggregations": [["max", "timestamp", "last_seen"]]},
     "latest_event": {"aggregations": [["argMax", ["id", "timestamp"], "latest_event"]]},
     "project": {"fields": ["project.id"]},
-    "user": {"fields": ["user.id", "user.name", "user.username", "user.email", "user.ip"]},
+    "user": {"fields": ["user.id", "user.username", "user.email", "user.ip"]},
     # Long term these will become more complex functions but these are
     # field aliases.
     "apdex": {"result_type": "number", "aggregations": [["apdex(duration, 300)", "", "apdex"]]},
@@ -695,9 +695,9 @@ FIELD_ALIASES = {
             ]
         ],
     },
-    "p75": {"result_type": "duration", "aggregations": [["quantile(0.75)(duration)", "", "p75"]]},
-    "p95": {"result_type": "duration", "aggregations": [["quantile(0.95)(duration)", "", "p95"]]},
-    "p99": {"result_type": "duration", "aggregations": [["quantile(0.99)(duration)", "", "p99"]]},
+    "p75": {"result_type": "duration", "aggregations": [["quantile(0.75)(duration)", None, "p75"]]},
+    "p95": {"result_type": "duration", "aggregations": [["quantile(0.95)(duration)", None, "p95"]]},
+    "p99": {"result_type": "duration", "aggregations": [["quantile(0.99)(duration)", None, "p99"]]},
 }
 
 VALID_AGGREGATES = {
@@ -737,6 +737,10 @@ def resolve_orderby(orderby, fields, aggregations):
     """
     We accept column names, aggregate functions, and aliases as order by
     values. Aggregates and field aliases need to be resolve/validated.
+
+    TODO(mark) Once we're no longer using the dataset selection function
+    should allow all non-tag fields to be used as sort clauses, instead of only
+    those that are currently selected.
     """
     orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
     validated = []
@@ -804,7 +808,7 @@ def resolve_field_list(fields, snuba_args):
 
         if match.group("function") == "count":
             # count() is a special function that ignores its column arguments.
-            aggregations.append(["count", "", get_aggregate_alias(match)])
+            aggregations.append(["count", None, get_aggregate_alias(match)])
         else:
             aggregations.append(
                 [
@@ -824,11 +828,12 @@ def resolve_field_list(fields, snuba_args):
         # generates invalid queries.
         if not aggregations and "id" not in columns:
             columns.append("id")
+        if not aggregations and "project.id" not in columns:
             columns.append("project.id")
         if aggregations and "latest_event" not in fields:
             aggregations.extend(deepcopy(FIELD_ALIASES["latest_event"]["aggregations"]))
         if aggregations and "project.id" not in columns:
-            aggregations.append(["argMax", ["project_id", "timestamp"], "projectid"])
+            aggregations.append(["argMax", ["project.id", "timestamp"], "projectid"])
 
     if rollup and columns and not aggregations:
         raise InvalidSearchQuery("You cannot use rollup without an aggregate field.")
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
new file mode 100644
index 0000000000..91fe1e112b
--- /dev/null
+++ b/src/sentry/snuba/discover.py
@@ -0,0 +1,207 @@
+from __future__ import absolute_import
+
+import six
+
+from copy import deepcopy
+from sentry.api.event_search import get_filter, resolve_field_list
+from sentry.utils.snuba import (
+    Dataset,
+    DISCOVER_COLUMN_MAP,
+    QUOTED_LITERAL_RE,
+    get_function_index,
+    raw_query,
+    transform_results,
+)
+
+
+def resolve_column(col):
+    """
+    Resolve a public schema name to the discover dataset.
+    unknown columns are converted into tags expressions.
+    """
+    if col is None:
+        return col
+    if col.startswith("tags[") or QUOTED_LITERAL_RE.match(col):
+        return col
+    return DISCOVER_COLUMN_MAP.get(col, u"tags[{}]".format(col))
+
+
+def resolve_condition(cond):
+    """
+    When conditions have been parsed by the api.event_search module
+    we can end up with conditions that are not valid on the current dataset
+    due to how ap.event_search checks for valid field names without
+    being aware of the dataset.
+
+    We have the dataset context here, so we need to re-scope conditions to the
+    current dataset.
+    """
+    index = get_function_index(cond)
+    if index is not None:
+        # IN conditions are detected as a function but aren't really.
+        if cond[index] == "IN":
+            cond[0] = resolve_column(cond[0])
+            return cond
+
+        func_args = cond[index + 1]
+        for (i, arg) in enumerate(func_args):
+            # Nested function
+            if isinstance(arg, (list, tuple)):
+                func_args[i] = resolve_condition(arg)
+            else:
+                func_args[i] = resolve_column(arg)
+        cond[index + 1] = func_args
+        return cond
+
+    # No function name found
+    if isinstance(cond, (list, tuple)) and len(cond):
+        # Condition is [col, operator, value]
+        if isinstance(cond[0], six.string_types) and len(cond) == 3:
+            cond[0] = resolve_column(cond[0])
+            return cond
+        if isinstance(cond[0], (list, tuple)):
+            if get_function_index(cond[0]) is not None:
+                cond[0] = resolve_condition(cond[0])
+                return cond
+            else:
+                # Nested conditions
+                return [resolve_condition(item) for item in cond]
+    raise ValueError("Unexpected condition format %s" % cond)
+
+
+def resolve_discover_aliases(snuba_args):
+    """
+    Resolve the public schema aliases to the discover dataset.
+
+    Returns a copy of the input structure, and includes a
+    `translated_columns` key containing the selected fields that need to
+    be renamed in the result set.
+    """
+    resolved = deepcopy(snuba_args)
+    translated_columns = {}
+    derived_columns = set()
+
+    selected_columns = resolved.get("selected_columns")
+    if selected_columns:
+        for (idx, col) in enumerate(selected_columns):
+            if isinstance(col, (list, tuple)):
+                raise ValueError("discover selected_columns should only be str. got %s" % col)
+            name = resolve_column(col)
+            selected_columns[idx] = name
+            translated_columns[name] = col
+        resolved["selected_columns"] = selected_columns
+
+    groupby = resolved.get("groupby")
+    if groupby:
+        for (idx, col) in enumerate(groupby):
+            name = col
+            if col not in derived_columns:
+                name = resolve_column(col)
+            groupby[idx] = name
+        resolved["groupby"] = groupby
+
+    aggregations = resolved.get("aggregations")
+    for aggregation in aggregations or []:
+        derived_columns.add(aggregation[2])
+        if isinstance(aggregation[1], six.string_types):
+            aggregation[1] = resolve_column(aggregation[1])
+        elif isinstance(aggregation[1], (set, tuple, list)):
+            aggregation[1] = [resolve_column(col) for col in aggregation[1]]
+    resolved["aggregations"] = aggregations
+
+    conditions = resolved.get("conditions")
+    if conditions:
+        for (i, condition) in enumerate(conditions):
+            replacement = resolve_condition(condition)
+            conditions[i] = replacement
+        resolved["conditions"] = list(filter(None, conditions))
+
+    # TODO add support for extracting having conditions.
+
+    orderby = resolved.get("orderby")
+    if orderby:
+        orderby = orderby if isinstance(orderby, (list, tuple)) else [orderby]
+        resolved_orderby = []
+
+        for field_with_order in orderby:
+            field = field_with_order.lstrip("-")
+            resolved_orderby.append(
+                u"{}{}".format(
+                    "-" if field_with_order.startswith("-") else "",
+                    field if field in derived_columns else resolve_column(field),
+                )
+            )
+        resolved["orderby"] = resolved_orderby
+    return resolved, translated_columns
+
+
+def query(selected_columns, query, params, orderby=None, referrer=None):
+    """
+    High-level API for doing arbitrary user queries against events.
+
+    This function operates on the Discover public event schema and
+    virtual fields/aggregate functions for selected columns and
+    conditions are supported through this function.
+
+    The resulting list will have all internal field names mapped
+    back into their public schema names.
+
+    selected_columns (Sequence[str]) List of public aliases to fetch.
+    query (str) Filter query string to create conditions from.
+    params (Dict[str, str]) Filtering parameters with start, end, project_id, environment
+    orderby (str|Sequence[str]) The field to order results by.
+    """
+    snuba_filter = get_filter(query, params)
+
+    # TODO(mark) Refactor the need for this translation shim once all of
+    # discover is using this module
+    snuba_args = {
+        "start": snuba_filter.start,
+        "end": snuba_filter.end,
+        "conditions": snuba_filter.conditions,
+        "filter_keys": snuba_filter.filter_keys,
+        "orderby": orderby,
+    }
+    snuba_args.update(resolve_field_list(selected_columns, snuba_args))
+
+    # Resolve the public aliases into the discover dataset names.
+    snuba_args, translated_columns = resolve_discover_aliases(snuba_args)
+
+    result = raw_query(
+        start=snuba_args.get("start"),
+        end=snuba_args.get("end"),
+        groupby=snuba_args.get("groupby"),
+        conditions=snuba_args.get("conditions"),
+        aggregations=snuba_args.get("aggregations"),
+        selected_columns=snuba_args.get("selected_columns"),
+        filter_keys=snuba_args.get("filter_keys"),
+        orderby=snuba_args.get("orderby"),
+        dataset=Dataset.Discover,
+        referrer=referrer,
+    )
+
+    return transform_results(result, translated_columns, snuba_args)
+
+
+def timeseries_query(selected_columns, query, params, rollup):
+    """
+    High-level API for doing arbitrary user timeseries queries against events.
+
+    This function operates on the public event schema and
+    virtual fields/aggregate functions for selected columns and
+    conditions are supported through this function.
+
+    This function is intended to only get timeseries based
+    results and thus requires the `rollup` parameter.
+    """
+    raise NotImplementedError
+
+
+def get_pagination_ids(event, query, params):
+    """
+    High-level API for getting pagination data for an event + filter
+
+    The provided event is used as a reference event to find events
+    that are older and newer than the current one.
+    """
+    raise NotImplementedError
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 45377cb8ff..9f76a0376a 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -276,6 +276,9 @@ def detect_dataset(query_args, aliased_conditions=False):
     the public aliases and the internal names. When query conditions
     have been pre-parsed by api.event_search set aliased_conditions=True
     as we need to look for internal names.
+
+    :deprecated: This method and the automatic dataset resolution is deprecated.
+    You should use sentry.snuba.discover instead.
     """
     if query_args.get("dataset", None):
         return query_args["dataset"]
@@ -451,6 +454,8 @@ def transform_aliases_and_query(**kwargs):
     orderby and arrayjoin fields to their internal Snuba format and post the
     query to Snuba. Convert back translated aliases before returning snuba
     results.
+
+    :deprecated: This method is deprecated. You should use sentry.snuba.discover instead.
     """
 
     arrayjoin_map = {"error": "exception_stacks", "stack": "exception_frames"}
@@ -464,7 +469,6 @@ def transform_aliases_and_query(**kwargs):
     conditions = kwargs.get("conditions")
     filter_keys = kwargs["filter_keys"]
     arrayjoin = kwargs.get("arrayjoin")
-    rollup = kwargs.get("rollup")
     orderby = kwargs.get("orderby")
     having = kwargs.get("having", [])
     dataset = detect_dataset(kwargs)
@@ -537,6 +541,16 @@ def transform_aliases_and_query(**kwargs):
 
     result = dataset_query(**kwargs)
 
+    return transform_results(result, translated_columns, kwargs)
+
+
+def transform_results(result, translated_columns, snuba_args):
+    """
+    Transform internal names back to the public schema ones.
+
+    When getting timeseries results via rollup, this function will
+    zerofill the output results.
+    """
     # Translate back columns that were converted to snuba format
     for col in result["meta"]:
         col["name"] = translated_columns.get(col["name"], col["name"])
@@ -546,10 +560,12 @@ def transform_aliases_and_query(**kwargs):
 
     if len(translated_columns):
         result["data"] = [get_row(row) for row in result["data"]]
-        if rollup and rollup > 0:
-            result["data"] = zerofill(
-                result["data"], kwargs["start"], kwargs["end"], kwargs["rollup"], kwargs["orderby"]
-            )
+
+    rollup = snuba_args.get("rollup")
+    if rollup and rollup > 0:
+        result["data"] = zerofill(
+            result["data"], snuba_args["start"], snuba_args["end"], rollup, snuba_args["orderby"]
+        )
 
     return result
 
@@ -910,6 +926,9 @@ def constrain_column_to_dataset(col, dataset, value=None):
     Ensure conditions only reference valid columns on the provided
     dataset. Return none for conditions to be removed, and convert
     unknown columns into tags expressions.
+
+    :deprecated: This method and the automatic dataset resolution is deprecated.
+    You should use sentry.snuba.discover instead.
     """
     if col.startswith("tags["):
         return col
@@ -935,6 +954,9 @@ def constrain_condition_to_dataset(cond, dataset):
 
     We have the dataset context here, so we need to re-scope conditions to the
     current dataset.
+
+    :deprecated: This method and the automatic dataset resolution is deprecated.
+    You should use sentry.snuba.discover instead.
     """
     index = get_function_index(cond)
     if index is not None:
@@ -995,6 +1017,9 @@ def dataset_query(
     either error or transaction events.
 
     This function will also re-alias columns to match the selected dataset
+
+    :deprecated: This method and the automatic dataset resolution is deprecated.
+    You should use sentry.snuba.discover instead.
     """
     if dataset is None:
         dataset = detect_dataset(
diff --git a/tests/sentry/api/test_event_search.py b/tests/sentry/api/test_event_search.py
index dae434359a..c4e5f790a8 100644
--- a/tests/sentry/api/test_event_search.py
+++ b/tests/sentry/api/test_event_search.py
@@ -1031,7 +1031,7 @@ class ResolveFieldListTest(unittest.TestCase):
         assert result["aggregations"] == [
             ["max", "timestamp", "last_seen"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == ["title"]
 
@@ -1047,11 +1047,11 @@ class ResolveFieldListTest(unittest.TestCase):
                 "",
                 "impact",
             ],
-            ["quantile(0.75)(duration)", "", "p75"],
-            ["quantile(0.95)(duration)", "", "p95"],
-            ["quantile(0.99)(duration)", "", "p99"],
+            ["quantile(0.75)(duration)", None, "p75"],
+            ["quantile(0.95)(duration)", None, "p95"],
+            ["quantile(0.99)(duration)", None, "p99"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == []
 
@@ -1062,7 +1062,6 @@ class ResolveFieldListTest(unittest.TestCase):
             "title",
             "project.id",
             "user.id",
-            "user.name",
             "user.username",
             "user.email",
             "user.ip",
@@ -1076,7 +1075,6 @@ class ResolveFieldListTest(unittest.TestCase):
             "title",
             "project.id",
             "user.id",
-            "user.name",
             "user.username",
             "user.email",
             "user.ip",
@@ -1090,10 +1088,10 @@ class ResolveFieldListTest(unittest.TestCase):
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
             ["uniq", "user", "count_unique_user"],
-            ["count", "", "count_id"],
+            ["count", None, "count_id"],
             ["min", "timestamp", "min_timestamp"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == []
 
@@ -1103,11 +1101,11 @@ class ResolveFieldListTest(unittest.TestCase):
         # Automatic fields should be inserted, count() should have its column dropped.
         assert result["selected_columns"] == []
         assert result["aggregations"] == [
-            ["count", "", "count_id"],
-            ["count", "", "count_user"],
-            ["count", "", "count_transaction_duration"],
+            ["count", None, "count_id"],
+            ["count", None, "count_user"],
+            ["count", None, "count_transaction_duration"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == []
 
@@ -1117,7 +1115,7 @@ class ResolveFieldListTest(unittest.TestCase):
         assert result["aggregations"] == [
             ["uniq", "user.id", "count_unique_user_id"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
 
     def test_aggregate_function_invalid_name(self):
@@ -1150,7 +1148,7 @@ class ResolveFieldListTest(unittest.TestCase):
         snuba_args = {"rollup": 15}
         result = resolve_field_list(fields, snuba_args)
 
-        assert result["aggregations"] == [["count", "", "count"]]
+        assert result["aggregations"] == [["count", None, "count"]]
         assert result["selected_columns"] == ["message"]
         assert result["groupby"] == ["message"]
 
@@ -1185,7 +1183,7 @@ class ResolveFieldListTest(unittest.TestCase):
         assert result["aggregations"] == [
             ["max", "timestamp", "last_seen"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == []
 
@@ -1195,10 +1193,10 @@ class ResolveFieldListTest(unittest.TestCase):
         result = resolve_field_list(fields, snuba_args)
         assert result["orderby"] == ["-count_id"]
         assert result["aggregations"] == [
-            ["count", "", "count_id"],
+            ["count", None, "count_id"],
             ["uniq", "user", "count_unique_user"],
             ["argMax", ["id", "timestamp"], "latest_event"],
-            ["argMax", ["project_id", "timestamp"], "projectid"],
+            ["argMax", ["project.id", "timestamp"], "projectid"],
         ]
         assert result["groupby"] == []
 
diff --git a/tests/sentry/snuba/test_discover.py b/tests/sentry/snuba/test_discover.py
new file mode 100644
index 0000000000..2cfd02a202
--- /dev/null
+++ b/tests/sentry/snuba/test_discover.py
@@ -0,0 +1,406 @@
+from __future__ import absolute_import
+
+from sentry.api.event_search import InvalidSearchQuery
+from sentry.snuba import discover
+from sentry.testutils import TestCase, SnubaTestCase
+from sentry.testutils.helpers.datetime import iso_format, before_now
+from sentry.utils.snuba import Dataset
+
+from mock import patch
+import pytest
+
+
+class QueryIntegrationTest(SnubaTestCase, TestCase):
+    def setUp(self):
+        super(QueryIntegrationTest, self).setUp()
+        self.environment = self.create_environment(self.project, name="prod")
+        self.release = self.create_release(self.project, version="first-release")
+
+        self.event = self.store_event(
+            data={
+                "message": "oh no",
+                "release": "first-release",
+                "environment": "prod",
+                "platform": "python",
+                "user": {"id": "99", "email": "bruce@example.com", "username": "brucew"},
+                "timestamp": iso_format(before_now(minutes=1)),
+            },
+            project_id=self.project.id,
+        )
+
+    def test_field_aliasing_in_selected_columns(self):
+        result = discover.query(
+            selected_columns=["project.id", "user.email", "release"],
+            query="",
+            params={"project_id": [self.project.id]},
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["id"] == self.event.event_id
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["user.email"] == "bruce@example.com"
+        assert data[0]["release"] == "first-release"
+
+        assert len(result["meta"]) == 4
+        assert result["meta"][0] == {"name": "project.id", "type": "UInt64"}
+        assert result["meta"][1] == {"name": "user.email", "type": "Nullable(String)"}
+        assert result["meta"][2] == {"name": "release", "type": "Nullable(String)"}
+        assert result["meta"][3] == {"name": "id", "type": "FixedString(32)"}
+
+    def test_field_aliasing_in_aggregate_functions_and_groupby(self):
+        result = discover.query(
+            selected_columns=["project.id", "count_unique(user.email)"],
+            query="",
+            params={"project_id": [self.project.id]},
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["count_unique_user_email"] == 1
+
+    def test_field_aliasing_in_conditions(self):
+        result = discover.query(
+            selected_columns=["project.id", "user.email"],
+            query="user.email:bruce@example.com",
+            params={"project_id": [self.project.id]},
+        )
+        data = result["data"]
+        assert len(data) == 1
+        assert data[0]["project.id"] == self.project.id
+        assert data[0]["user.email"] == "bruce@example.com"
+
+    def test_release_condition(self):
+        result = discover.query(
+            selected_columns=["id", "message"],
+            query="release:{}".format(self.create_release(self.project).version),
+            params={"project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 0
+
+        result = discover.query(
+            selected_columns=["id", "message"],
+            query="release:{}".format(self.release.version),
+            params={"project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 1
+        data = result["data"]
+        assert data[0]["id"] == self.event.event_id
+        assert data[0]["message"] == self.event.message
+        assert data[0]["project.id"] == self.project.id, "project.id should be inserted"
+        assert "event_id" not in data[0]
+
+    def test_environment_condition(self):
+        result = discover.query(
+            selected_columns=["id", "message"],
+            query="environment:{}".format(self.create_environment(self.project).name),
+            params={"project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 0
+
+        result = discover.query(
+            selected_columns=["id", "message"],
+            query="environment:{}".format(self.environment.name),
+            params={"project_id": [self.project.id]},
+        )
+        assert len(result["data"]) == 1
+        data = result["data"]
+        assert data[0]["id"] == self.event.event_id
+        assert data[0]["message"] == self.event.message
+        assert data[0]["project.id"] == self.project.id, "project.id should be inserted"
+
+
+class QueryTransformTest(TestCase):
+    """
+    This test mocks snuba.raw_query to let us isolate column transformations.
+    """
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_query_parse_error(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        with pytest.raises(InvalidSearchQuery):
+            discover.query(
+                selected_columns=[],
+                query="foo(id):<1dino",
+                params={"project_id": [self.project.id]},
+            )
+        assert mock_query.call_count == 0
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_selected_columns_field_alias_macro(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "user_id"}, {"name": "email"}],
+            "data": [{"user_id": "1", "email": "a@example.org"}],
+        }
+        discover.query(
+            selected_columns=["user", "project"], query="", params={"project_id": [self.project.id]}
+        )
+        mock_query.assert_called_with(
+            selected_columns=[
+                "user_id",
+                "username",
+                "email",
+                "ip_address",
+                "project_id",
+                "event_id",
+            ],
+            aggregations=[],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            end=None,
+            start=None,
+            conditions=[],
+            groupby=[],
+            orderby=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_selected_columns_aliasing_in_function(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        discover.query(
+            selected_columns=[
+                "transaction",
+                "transaction.duration",
+                "count_unique(transaction.duration)",
+            ],
+            query="",
+            params={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction", "duration"],
+            aggregations=[
+                ["uniq", "duration", "count_unique_transaction_duration"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+                ["argMax", ["project_id", "timestamp"], "projectid"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            end=None,
+            start=None,
+            conditions=[],
+            groupby=["transaction", "duration"],
+            orderby=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_selected_columns_aggregate_alias(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "p95"}],
+            "data": [{"transaction": "api.do_things", "p95": 200}],
+        }
+        discover.query(
+            selected_columns=["transaction", "p95", "count_unique(transaction)"],
+            query="",
+            params={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction"],
+            aggregations=[
+                ["quantile(0.95)(duration)", None, "p95"],
+                ["uniq", "transaction", "count_unique_transaction"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+                ["argMax", ["project_id", "timestamp"], "projectid"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            groupby=["transaction"],
+            conditions=[],
+            end=None,
+            start=None,
+            orderby=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_orderby(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "title"}, {"name": "project.id"}],
+            "data": [{"project.id": "tester", "title": "test title"}],
+        }
+        discover.query(
+            selected_columns=["project.id", "title"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby=["project.id"],
+        )
+        mock_query.assert_called_with(
+            selected_columns=["project_id", "title", "event_id"],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            orderby=["project_id"],
+            aggregations=[],
+            end=None,
+            start=None,
+            conditions=[],
+            groupby=[],
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_orderby_must_be_selected_if_aggregate(self, mock_query):
+        with pytest.raises(InvalidSearchQuery):
+            discover.query(
+                selected_columns=["transaction", "transaction.duration"],
+                query="",
+                params={"project_id": [self.project.id]},
+                orderby=["count()"],
+            )
+        assert mock_query.call_count == 0
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_orderby_aggregate_alias(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "count_id"}, {"name": "project.id"}],
+            "data": [{"project.id": "tester", "count_id": 10}],
+        }
+        discover.query(
+            selected_columns=["count(id)", "project.id", "id"],
+            query="",
+            params={"project_id": [self.project.id]},
+            orderby=["count_id"],
+        )
+        mock_query.assert_called_with(
+            selected_columns=["project_id", "event_id"],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            orderby=["count_id"],
+            aggregations=[
+                ["count", None, "count_id"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+            ],
+            end=None,
+            start=None,
+            conditions=[],
+            groupby=["project_id", "event_id"],
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_conditions_order_and_groupby_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        discover.query(
+            selected_columns=["timestamp", "transaction", "transaction.duration", "count()"],
+            query="transaction.duration:200 sdk.name:python tags[projectid]:123",
+            params={"project_id": [self.project.id]},
+            orderby=["-timestamp", "-count"],
+        )
+        mock_query.assert_called_with(
+            selected_columns=["timestamp", "transaction", "duration"],
+            aggregations=[
+                ["count", None, "count"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+                ["argMax", ["project_id", "timestamp"], "projectid"],
+            ],
+            conditions=[
+                ["duration", "=", 200],
+                ["sdk_name", "=", "python"],
+                [["ifNull", ["tags[projectid]", "''"]], "=", "123"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=["timestamp", "transaction", "duration"],
+            orderby=["-timestamp", "-count"],
+            dataset=Dataset.Discover,
+            end=None,
+            start=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_conditions_nested_function_aliasing(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}],
+            "data": [{"transaction": "api.do_things"}],
+        }
+        discover.query(
+            selected_columns=["transaction", "count()"],
+            query="event.type:transaction user.email:*@sentry.io message:recent-searches",
+            params={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction"],
+            conditions=[
+                ["type", "=", "transaction"],
+                [["match", ["email", "'(?i)^.*\@sentry\.io$'"]], "=", 1],
+                [["positionCaseInsensitive", ["message", "'recent-searches'"]], "!=", 0],
+            ],
+            aggregations=[
+                ["count", None, "count"],
+                ["argMax", ["event_id", "timestamp"], "latest_event"],
+                ["argMax", ["project_id", "timestamp"], "projectid"],
+            ],
+            filter_keys={"project_id": [self.project.id]},
+            dataset=Dataset.Discover,
+            groupby=["transaction"],
+            orderby=None,
+            end=None,
+            start=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_condition_transform(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        discover.query(
+            selected_columns=["transaction", "transaction.duration"],
+            query="http.method:GET",
+            params={"project_id": [self.project.id]},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction", "duration", "event_id", "project_id"],
+            conditions=[["http_method", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=[],
+            dataset=Dataset.Discover,
+            aggregations=[],
+            orderby=None,
+            end=None,
+            start=None,
+            referrer=None,
+        )
+
+    @patch("sentry.snuba.discover.raw_query")
+    def test_params_forward(self, mock_query):
+        mock_query.return_value = {
+            "meta": [{"name": "transaction"}, {"name": "duration"}],
+            "data": [{"transaction": "api.do_things", "duration": 200}],
+        }
+        start_time = before_now(minutes=10)
+        end_time = before_now(seconds=1)
+        discover.query(
+            selected_columns=["transaction", "transaction.duration"],
+            query="http.method:GET",
+            params={"project_id": [self.project.id], "start": start_time, "end": end_time},
+        )
+        mock_query.assert_called_with(
+            selected_columns=["transaction", "duration", "event_id", "project_id"],
+            conditions=[["http_method", "=", "GET"]],
+            filter_keys={"project_id": [self.project.id]},
+            groupby=[],
+            dataset=Dataset.Discover,
+            aggregations=[],
+            end=end_time,
+            start=start_time,
+            orderby=None,
+            referrer=None,
+        )
+
+
+class TimeseriesQueryTest(TestCase):
+    pass
+
+
+class GetPaginationIdsTest(TestCase):
+    pass
