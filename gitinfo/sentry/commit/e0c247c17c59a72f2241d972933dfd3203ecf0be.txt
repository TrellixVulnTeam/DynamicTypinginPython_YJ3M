commit e0c247c17c59a72f2241d972933dfd3203ecf0be
Author: Chris Fuller <cfuller@sentry.io>
Date:   Sun May 24 08:54:32 2020 -0400

    feat(workflow): Additional metrics for our SLO (#18912)

diff --git a/src/sentry/api/endpoints/group_details.py b/src/sentry/api/endpoints/group_details.py
index 1c0aa057f7..c2fd16f895 100644
--- a/src/sentry/api/endpoints/group_details.py
+++ b/src/sentry/api/endpoints/group_details.py
@@ -32,6 +32,7 @@ from sentry.models import (
 from sentry.plugins.base import plugins
 from sentry.plugins.bases import IssueTrackingPlugin2
 from sentry.signals import issue_deleted
+from sentry.utils import metrics
 from sentry.utils.safe import safe_execute
 from sentry.utils.apidocs import scenario, attach_scenarios
 from sentry.utils.compat import zip
@@ -195,111 +196,124 @@ class GroupDetailsEndpoint(GroupEndpoint, EnvironmentMixin):
         :pparam string issue_id: the ID of the issue to retrieve.
         :auth: required
         """
-        # TODO(dcramer): handle unauthenticated/public response
+        try:
+            # TODO(dcramer): handle unauthenticated/public response
 
-        organization = group.project.organization
-        environments = get_environments(request, organization)
-        environment_ids = [e.id for e in environments]
+            organization = group.project.organization
+            environments = get_environments(request, organization)
+            environment_ids = [e.id for e in environments]
 
-        # WARNING: the rest of this endpoint relies on this serializer
-        # populating the cache SO don't move this :)
-        data = serialize(group, request.user, GroupSerializerSnuba(environment_ids=environment_ids))
+            # WARNING: the rest of this endpoint relies on this serializer
+            # populating the cache SO don't move this :)
+            data = serialize(
+                group, request.user, GroupSerializerSnuba(environment_ids=environment_ids)
+            )
 
-        # TODO: these probably should be another endpoint
-        activity = self._get_activity(request, group, num=100)
-        seen_by = self._get_seen_by(request, group)
+            # TODO: these probably should be another endpoint
+            activity = self._get_activity(request, group, num=100)
+            seen_by = self._get_seen_by(request, group)
 
-        first_release = group.get_first_release()
+            first_release = group.get_first_release()
 
-        if first_release is not None:
-            last_release = group.get_last_release()
-        else:
-            last_release = None
+            if first_release is not None:
+                last_release = group.get_last_release()
+            else:
+                last_release = None
 
-        action_list = self._get_actions(request, group)
+            action_list = self._get_actions(request, group)
 
-        if first_release is not None and last_release is not None:
-            first_release, last_release = self._get_first_last_release_info(
-                request, group, [first_release, last_release]
-            )
-        elif first_release is not None:
-            first_release = self._get_release_info(request, group, first_release)
-        elif last_release is not None:
-            last_release = self._get_release_info(request, group, last_release)
-
-        get_range = functools.partial(tsdb.get_range, environment_ids=environment_ids)
-
-        tags = tagstore.get_group_tag_keys(group.project_id, group.id, environment_ids, limit=100)
-        if not environment_ids:
-            user_reports = UserReport.objects.filter(group=group)
-        else:
-            user_reports = UserReport.objects.filter(
-                group=group, environment_id__in=environment_ids
-            )
+            if first_release is not None and last_release is not None:
+                first_release, last_release = self._get_first_last_release_info(
+                    request, group, [first_release, last_release]
+                )
+            elif first_release is not None:
+                first_release = self._get_release_info(request, group, first_release)
+            elif last_release is not None:
+                last_release = self._get_release_info(request, group, last_release)
 
-        now = timezone.now()
-        hourly_stats = tsdb.rollup(
-            get_range(
-                model=tsdb.models.group, keys=[group.id], end=now, start=now - timedelta(days=1)
-            ),
-            3600,
-        )[group.id]
-        daily_stats = tsdb.rollup(
-            get_range(
-                model=tsdb.models.group, keys=[group.id], end=now, start=now - timedelta(days=30)
-            ),
-            3600 * 24,
-        )[group.id]
-
-        participants = list(
-            User.objects.filter(groupsubscription__is_active=True, groupsubscription__group=group)
-        )
+            get_range = functools.partial(tsdb.get_range, environment_ids=environment_ids)
 
-        data.update(
-            {
-                "firstRelease": first_release,
-                "lastRelease": last_release,
-                "activity": serialize(activity, request.user),
-                "seenBy": seen_by,
-                "participants": serialize(participants, request.user),
-                "pluginActions": action_list,
-                "pluginIssues": self._get_available_issue_plugins(request, group),
-                "pluginContexts": self._get_context_plugins(request, group),
-                "userReportCount": user_reports.count(),
-                "tags": sorted(serialize(tags, request.user), key=lambda x: x["name"]),
-                "stats": {"24h": hourly_stats, "30d": daily_stats},
-            }
-        )
+            tags = tagstore.get_group_tag_keys(
+                group.project_id, group.id, environment_ids, limit=100
+            )
+            if not environment_ids:
+                user_reports = UserReport.objects.filter(group=group)
+            else:
+                user_reports = UserReport.objects.filter(
+                    group=group, environment_id__in=environment_ids
+                )
 
-        # the current release is the 'latest seen' release within the
-        # environment even if it hasnt affected this issue
-        if environments:
-            try:
-                current_release = GroupRelease.objects.filter(
-                    group_id=group.id,
-                    environment__in=[env.name for env in environments],
-                    release_id=ReleaseEnvironment.objects.filter(
-                        release_id__in=ReleaseProject.objects.filter(
-                            project_id=group.project_id
-                        ).values_list("release_id", flat=True),
-                        organization_id=group.project.organization_id,
-                        environment_id__in=environment_ids,
-                    )
-                    .order_by("-first_seen")
-                    .values_list("release_id", flat=True)[:1],
-                )[0]
-            except IndexError:
-                current_release = None
+            now = timezone.now()
+            hourly_stats = tsdb.rollup(
+                get_range(
+                    model=tsdb.models.group, keys=[group.id], end=now, start=now - timedelta(days=1)
+                ),
+                3600,
+            )[group.id]
+            daily_stats = tsdb.rollup(
+                get_range(
+                    model=tsdb.models.group,
+                    keys=[group.id],
+                    end=now,
+                    start=now - timedelta(days=30),
+                ),
+                3600 * 24,
+            )[group.id]
+
+            participants = list(
+                User.objects.filter(
+                    groupsubscription__is_active=True, groupsubscription__group=group
+                )
+            )
 
             data.update(
                 {
-                    "currentRelease": serialize(
-                        current_release, request.user, GroupReleaseWithStatsSerializer()
-                    )
+                    "firstRelease": first_release,
+                    "lastRelease": last_release,
+                    "activity": serialize(activity, request.user),
+                    "seenBy": seen_by,
+                    "participants": serialize(participants, request.user),
+                    "pluginActions": action_list,
+                    "pluginIssues": self._get_available_issue_plugins(request, group),
+                    "pluginContexts": self._get_context_plugins(request, group),
+                    "userReportCount": user_reports.count(),
+                    "tags": sorted(serialize(tags, request.user), key=lambda x: x["name"]),
+                    "stats": {"24h": hourly_stats, "30d": daily_stats},
                 }
             )
 
-        return Response(data)
+            # the current release is the 'latest seen' release within the
+            # environment even if it hasnt affected this issue
+            if environments:
+                try:
+                    current_release = GroupRelease.objects.filter(
+                        group_id=group.id,
+                        environment__in=[env.name for env in environments],
+                        release_id=ReleaseEnvironment.objects.filter(
+                            release_id__in=ReleaseProject.objects.filter(
+                                project_id=group.project_id
+                            ).values_list("release_id", flat=True),
+                            organization_id=group.project.organization_id,
+                            environment_id__in=environment_ids,
+                        )
+                        .order_by("-first_seen")
+                        .values_list("release_id", flat=True)[:1],
+                    )[0]
+                except IndexError:
+                    current_release = None
+
+                data.update(
+                    {
+                        "currentRelease": serialize(
+                            current_release, request.user, GroupReleaseWithStatsSerializer()
+                        )
+                    }
+                )
+            metrics.incr("group.update.http_response", sample_rate=1.0, tags={"status": 200})
+            return Response(data)
+        except Exception:
+            metrics.incr("group.update.http_response", sample_rate=1.0, tags={"status": 500})
+            raise
 
     @attach_scenarios([update_aggregate_scenario])
     def put(self, request, group):
@@ -329,11 +343,11 @@ class GroupDetailsEndpoint(GroupEndpoint, EnvironmentMixin):
         :param boolean isPublic: sets the issue to public or private.
         :auth: required
         """
-        discard = request.data.get("discard")
-
-        # TODO(dcramer): we need to implement assignedTo in the bulk mutation
-        # endpoint
         try:
+            discard = request.data.get("discard")
+
+            # TODO(dcramer): we need to implement assignedTo in the bulk mutation
+            # endpoint
             response = client.put(
                 path=u"/projects/{}/{}/issues/".format(
                     group.project.organization.slug, group.project.slug
@@ -342,30 +356,40 @@ class GroupDetailsEndpoint(GroupEndpoint, EnvironmentMixin):
                 data=request.data,
                 request=request,
             )
+
+            # if action was discard, there isn't a group to serialize anymore
+            if discard:
+                return response
+
+            # we need to fetch the object against as the bulk mutation endpoint
+            # only returns a delta, and object mutation returns a complete updated
+            # entity.
+            # TODO(dcramer): we should update the API and have this be an explicit
+            # flag (or remove it entirely) so that delta's are the primary response
+            # for mutation.
+            group = Group.objects.get(id=group.id)
+
+            serialized = serialize(
+                group,
+                request.user,
+                GroupSerializer(
+                    environment_func=self._get_environment_func(
+                        request, group.project.organization_id
+                    )
+                ),
+            )
+            metrics.incr(
+                "group.update.http_response", sample_rate=1.0, tags={"status": response.status_code}
+            )
+            return Response(serialized, status=response.status_code)
         except client.ApiError as e:
+            metrics.incr(
+                "group.update.http_response", sample_rate=1.0, tags={"status": e.status_code}
+            )
             return Response(e.body, status=e.status_code)
-
-        # if action was discard, there isn't a group to serialize anymore
-        if discard:
-            return response
-
-        # we need to fetch the object against as the bulk mutation endpoint
-        # only returns a delta, and object mutation returns a complete updated
-        # entity.
-        # TODO(dcramer): we should update the API and have this be an explicit
-        # flag (or remove it entirely) so that delta's are the primary response
-        # for mutation.
-        group = Group.objects.get(id=group.id)
-
-        serialized = serialize(
-            group,
-            request.user,
-            GroupSerializer(
-                environment_func=self._get_environment_func(request, group.project.organization_id)
-            ),
-        )
-
-        return Response(serialized, status=response.status_code)
+        except Exception:
+            metrics.incr("group.update.http_response", sample_rate=1.0, tags={"status": 500})
+            raise
 
     @attach_scenarios([delete_aggregate_scenario])
     def delete(self, request, group):
@@ -378,48 +402,54 @@ class GroupDetailsEndpoint(GroupEndpoint, EnvironmentMixin):
         :pparam string issue_id: the ID of the issue to delete.
         :auth: required
         """
-        from sentry.tasks.deletion import delete_groups
-
-        updated = (
-            Group.objects.filter(id=group.id)
-            .exclude(status__in=[GroupStatus.PENDING_DELETION, GroupStatus.DELETION_IN_PROGRESS])
-            .update(status=GroupStatus.PENDING_DELETION)
-        )
-        if updated:
-            project = group.project
+        try:
+            from sentry.tasks.deletion import delete_groups
 
-            eventstream_state = eventstream.start_delete_groups(group.project_id, [group.id])
-            transaction_id = uuid4().hex
+            updated = (
+                Group.objects.filter(id=group.id)
+                .exclude(
+                    status__in=[GroupStatus.PENDING_DELETION, GroupStatus.DELETION_IN_PROGRESS]
+                )
+                .update(status=GroupStatus.PENDING_DELETION)
+            )
+            if updated:
+                project = group.project
 
-            GroupHash.objects.filter(project_id=group.project_id, group__id=group.id).delete()
+                eventstream_state = eventstream.start_delete_groups(group.project_id, [group.id])
+                transaction_id = uuid4().hex
 
-            delete_groups.apply_async(
-                kwargs={
-                    "object_ids": [group.id],
-                    "transaction_id": transaction_id,
-                    "eventstream_state": eventstream_state,
-                },
-                countdown=3600,
-            )
+                GroupHash.objects.filter(project_id=group.project_id, group__id=group.id).delete()
 
-            self.create_audit_entry(
-                request=request,
-                organization_id=project.organization_id if project else None,
-                target_object=group.id,
-                transaction_id=transaction_id,
-            )
+                delete_groups.apply_async(
+                    kwargs={
+                        "object_ids": [group.id],
+                        "transaction_id": transaction_id,
+                        "eventstream_state": eventstream_state,
+                    },
+                    countdown=3600,
+                )
 
-            delete_logger.info(
-                "object.delete.queued",
-                extra={
-                    "object_id": group.id,
-                    "transaction_id": transaction_id,
-                    "model": type(group).__name__,
-                },
-            )
+                self.create_audit_entry(
+                    request=request,
+                    organization_id=project.organization_id if project else None,
+                    target_object=group.id,
+                    transaction_id=transaction_id,
+                )
 
-            issue_deleted.send_robust(
-                group=group, user=request.user, delete_type="delete", sender=self.__class__
-            )
+                delete_logger.info(
+                    "object.delete.queued",
+                    extra={
+                        "object_id": group.id,
+                        "transaction_id": transaction_id,
+                        "model": type(group).__name__,
+                    },
+                )
 
-        return Response(status=202)
+                issue_deleted.send_robust(
+                    group=group, user=request.user, delete_type="delete", sender=self.__class__
+                )
+            metrics.incr("group.update.http_response", sample_rate=1.0, tags={"status": 200})
+            return Response(status=202)
+        except Exception:
+            metrics.incr("group.update.http_response", sample_rate=1.0, tags={"status": 500})
+            raise
diff --git a/src/sentry/tasks/base.py b/src/sentry/tasks/base.py
index 6a4e780403..aae1b2ef66 100644
--- a/src/sentry/tasks/base.py
+++ b/src/sentry/tasks/base.py
@@ -78,3 +78,21 @@ def retry(func=None, on=(Exception,), exclude=()):
         return wrapped
 
     return inner
+
+
+def track_group_async_operation(function):
+    def wrapper(*args, **kwargs):
+        try:
+            response = function(*args, **kwargs)
+            metrics.incr(
+                "group.update.async_response",
+                sample_rate=1.0,
+                tags={"status": 500 if response is False else 200},
+            )
+            return response
+        except Exception:
+            metrics.incr("group.update.async_response", sample_rate=1.0, tags={"status": 500})
+            # Continue raising the error now that we've incr the metric
+            raise
+
+    return wrapper
diff --git a/src/sentry/tasks/deletion.py b/src/sentry/tasks/deletion.py
index ac205804a4..47b44cdd4e 100644
--- a/src/sentry/tasks/deletion.py
+++ b/src/sentry/tasks/deletion.py
@@ -10,7 +10,7 @@ from django.utils import timezone
 from sentry.constants import ObjectStatus
 from sentry.exceptions import DeleteAborted
 from sentry.signals import pending_delete
-from sentry.tasks.base import instrumented_task, retry
+from sentry.tasks.base import instrumented_task, retry, track_group_async_operation
 
 # in prod we run with infinite retries to recover from errors
 # in debug/development, we assume these tasks generally shouldn't fail
@@ -209,6 +209,7 @@ def delete_project(object_id, transaction_id=None, **kwargs):
     max_retries=MAX_RETRIES,
 )
 @retry(exclude=(DeleteAborted,))
+@track_group_async_operation
 def delete_groups(object_ids, transaction_id=None, eventstream_state=None, **kwargs):
     from sentry import deletions, eventstream
     from sentry.models import Group
diff --git a/src/sentry/tasks/integrations.py b/src/sentry/tasks/integrations.py
index 5de201c230..f8e79c1f4e 100644
--- a/src/sentry/tasks/integrations.py
+++ b/src/sentry/tasks/integrations.py
@@ -23,7 +23,7 @@ from sentry.models import (
 
 from sentry.shared_integrations.exceptions import ApiError, ApiUnauthorized, IntegrationError
 from sentry.models.apitoken import generate_token
-from sentry.tasks.base import instrumented_task, retry
+from sentry.tasks.base import instrumented_task, retry, track_group_async_operation
 
 logger = logging.getLogger("sentry.tasks.integrations")
 
@@ -192,17 +192,18 @@ def sync_assignee_outbound(external_issue_id, user_id, assign, **kwargs):
     max_retries=5,
 )
 @retry(exclude=(Integration.DoesNotExist))
+@track_group_async_operation
 def sync_status_outbound(group_id, external_issue_id, **kwargs):
     try:
         group = Group.objects.filter(
             id=group_id, status__in=[GroupStatus.UNRESOLVED, GroupStatus.RESOLVED]
         )[0]
     except IndexError:
-        return
+        return False
 
     has_issue_sync = features.has("organizations:integrations-issue-sync", group.organization)
     if not has_issue_sync:
-        return
+        return False
 
     try:
         external_issue = ExternalIssue.objects.get(id=external_issue_id)
@@ -230,7 +231,9 @@ def sync_status_outbound(group_id, external_issue_id, **kwargs):
     max_retries=5,
 )
 @retry()
+@track_group_async_operation
 def kick_off_status_syncs(project_id, group_id, **kwargs):
+
     # doing this in a task since this has to go in the event manager
     # and didn't want to introduce additional queries there
     external_issue_ids = GroupLink.objects.filter(
diff --git a/src/sentry/tasks/merge.py b/src/sentry/tasks/merge.py
index 45023878a4..e8254e3c5b 100644
--- a/src/sentry/tasks/merge.py
+++ b/src/sentry/tasks/merge.py
@@ -8,7 +8,7 @@ from django.db.models import F
 from sentry import eventstream
 from sentry.app import tsdb
 from sentry.similarity import features
-from sentry.tasks.base import instrumented_task
+from sentry.tasks.base import instrumented_task, track_group_async_operation
 
 logger = logging.getLogger("sentry.merge")
 delete_logger = logging.getLogger("sentry.deletions.async")
@@ -23,6 +23,7 @@ EXTRA_MERGE_MODELS = []
     default_retry_delay=60 * 5,
     max_retries=None,
 )
+@track_group_async_operation
 def merge_groups(
     from_object_ids=None,
     to_object_id=None,
@@ -50,7 +51,7 @@ def merge_groups(
 
     if not (from_object_ids and to_object_id):
         logger.error("group.malformed.missing_params", extra={"transaction_id": transaction_id})
-        return
+        return False
 
     # Operate on one "from" group per task iteration. The task is recursed
     # until each group has been merged.
@@ -63,7 +64,7 @@ def merge_groups(
             "group.malformed.invalid_id",
             extra={"transaction_id": transaction_id, "old_object_ids": from_object_ids},
         )
-        return
+        return False
 
     if not recursed:
         logger.info(
@@ -157,7 +158,6 @@ def merge_groups(
             with transaction.atomic():
                 GroupRedirect.create_for_group(group, new_group)
                 group.delete()
-
             delete_logger.info(
                 "object.delete.executed",
                 extra={
@@ -191,10 +191,8 @@ def merge_groups(
             recursed=True,
             eventstream_state=eventstream_state,
         )
-        return
-
-    # All `from_object_ids` have been merged!
-    if eventstream_state:
+    elif eventstream_state:
+        # All `from_object_ids` have been merged!
         eventstream.end_merge(eventstream_state)
 
 
