commit 32ea9cbf1f1c3da4722f34885010ea4238bcdd7c
Author: josh <josh@jrl.ninja>
Date:   Mon Oct 14 13:46:04 2019 -0700

    chore: goodbye riak (#15072)

diff --git a/.travis.yml b/.travis.yml
index ae35212a1c..6d12f89e88 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -136,24 +136,6 @@ matrix:
       name: 'Backend [Postgres] (2/2)'
       env: TEST_SUITE=postgres DB=postgres TOTAL_TEST_GROUPS=2 TEST_GROUP=1
 
-    - python: 2.7
-      name: 'Backend [Riak]'
-      env: TEST_SUITE=riak DB=postgres
-      services:
-        - memcached
-        - redis-server
-        - postgresql
-        - riak
-      before_install:
-        - docker run -d --network host --name clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server:19.4
-        - docker run -d --network host --name snuba --env SNUBA_SETTINGS=test --env CLICKHOUSE_SERVER=localhost:9000 getsentry/snuba
-        - docker ps -a
-      install:
-        - python setup.py install_egg_info
-        - pip install -U -e ".[dev,tests,optional]"
-      before_script:
-        - psql -c 'create database sentry;' -U postgres
-
     - <<: *acceptance_default
       name: 'Acceptance'
       env: TEST_SUITE=acceptance USE_SNUBA=1
diff --git a/Makefile b/Makefile
index 922655b7fc..8a8d66f604 100644
--- a/Makefile
+++ b/Makefile
@@ -161,12 +161,6 @@ else
 endif
 	@echo ""
 
-test-riak:
-	sentry init
-	@echo "--> Running Riak tests"
-	py.test tests/sentry/nodestore/riak/backend --cov . --cov-report="xml:.artifacts/riak.coverage.xml" --junit-xml=".artifacts/riak.junit.xml" || exit 1
-	@echo ""
-
 test-snuba:
 	@echo "--> Running snuba tests"
 	py.test tests/snuba tests/sentry/eventstream/kafka -vv --cov . --cov-report="xml:.artifacts/snuba.coverage.xml" --junit-xml=".artifacts/snuba.junit.xml"
@@ -232,7 +226,7 @@ travis-noop:
 .PHONY: travis-test-lint
 travis-test-lint: lint-python lint-js
 
-.PHONY: travis-test-postgres travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js travis-test-cli travis-test-dist travis-test-riak
+.PHONY: travis-test-postgres travis-test-acceptance travis-test-snuba travis-test-symbolicator travis-test-js travis-test-cli travis-test-dist
 travis-test-postgres: test-python
 travis-test-acceptance: test-acceptance
 travis-test-snuba: test-snuba
@@ -246,9 +240,8 @@ travis-test-dist:
 	# See: https://github.com/travis-ci/travis-ci/issues/4704
 	SENTRY_BUILD=$(TRAVIS_COMMIT) SENTRY_LIGHT_BUILD=0 python setup.py -q sdist bdist_wheel
 	@ls -lh dist/
-travis-test-riak: test-riak
 
-.PHONY: scan-python travis-scan-postgres travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator travis-scan-js travis-scan-cli travis-scan-dist travis-scan-lint travis-scan-riak
+.PHONY: scan-python travis-scan-postgres travis-scan-acceptance travis-scan-snuba travis-scan-symbolicator travis-scan-js travis-scan-cli travis-scan-dist travis-scan-lint
 scan-python:
 	@echo "--> Running Python vulnerability scanner"
 	$(PIP) install safety $(PIP_OPTS)
@@ -263,4 +256,3 @@ travis-scan-js: travis-noop
 travis-scan-cli: travis-noop
 travis-scan-dist: travis-noop
 travis-scan-lint: scan-python
-travis-scan-riak: travis-noop
diff --git a/conftest.py b/conftest.py
index 8bbdd13a09..7f4a51aa4c 100644
--- a/conftest.py
+++ b/conftest.py
@@ -14,11 +14,9 @@ sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))
 def pytest_configure(config):
     import warnings
 
-    # XXX(dcramer): Riak throws a UserWarning re:OpenSSL which isnt important
-    # to tests
     # XXX(dramer): Kombu throws a warning due to transaction.commit_manually
     # being used
-    warnings.filterwarnings("error", "", Warning, r"^(?!(|kombu|raven|riak|sentry))")
+    warnings.filterwarnings("error", "", Warning, r"^(?!(|kombu|raven|sentry))")
 
 
 def pytest_collection_modifyitems(items):
diff --git a/src/sentry/nodestore/riak/__init__.py b/src/sentry/nodestore/riak/__init__.py
deleted file mode 100644
index b1fcd80ebd..0000000000
--- a/src/sentry/nodestore/riak/__init__.py
+++ /dev/null
@@ -1,3 +0,0 @@
-from __future__ import absolute_import
-
-from .backend import *  # NOQA
diff --git a/src/sentry/nodestore/riak/backend.py b/src/sentry/nodestore/riak/backend.py
deleted file mode 100644
index 9776519b52..0000000000
--- a/src/sentry/nodestore/riak/backend.py
+++ /dev/null
@@ -1,99 +0,0 @@
-from __future__ import absolute_import
-
-import os
-import six
-
-from simplejson import JSONEncoder, _default_decoder
-
-from sentry.nodestore.base import NodeStorage
-from .client import RiakClient
-
-# Cache an instance of the encoder we want to use
-json_dumps = JSONEncoder(
-    separators=(",", ":"),
-    skipkeys=False,
-    ensure_ascii=True,
-    check_circular=True,
-    allow_nan=True,
-    indent=None,
-    encoding="utf-8",
-    default=None,
-).encode
-
-json_loads = _default_decoder.decode
-
-
-class RiakNodeStorage(NodeStorage):
-    """
-    A Riak-based backend for storing node data.
-
-    >>> RiakNodeStorage(nodes=[{'host':'127.0.0.1','port':8098}])
-    """
-
-    def __init__(
-        self,
-        nodes,
-        bucket="nodes",
-        timeout=1,
-        cooldown=5,
-        max_retries=3,
-        multiget_pool_size=5,
-        tcp_keepalive=True,
-        protocol=None,
-        automatic_expiry=False,
-    ):
-        # protocol being defined is useless, but is needed for backwards
-        # compatability and leveraged as an opportunity to yell at the user
-        if protocol == "pbc":
-            raise ValueError("'pbc' protocol is no longer supported")
-        if protocol is not None:
-            import warnings
-
-            warnings.warn("'protocol' has been deprecated", DeprecationWarning)
-        self.bucket = bucket
-        self.conn = RiakClient(
-            hosts=nodes,
-            max_retries=max_retries,
-            multiget_pool_size=multiget_pool_size,
-            cooldown=cooldown,
-            tcp_keepalive=tcp_keepalive,
-        )
-        self.automatic_expiry = automatic_expiry
-        self.skip_deletes = automatic_expiry and "_SENTRY_CLEANUP" in os.environ
-
-    def set(self, id, data, ttl=None):
-        self.conn.put(self.bucket, id, json_dumps(data), returnbody="false")
-
-    def delete(self, id):
-        if self.skip_deletes:
-            return
-        self.conn.delete(self.bucket, id)
-
-    def get(self, id):
-        rv = self.conn.get(self.bucket, id, r=1)
-        if rv.status != 200:
-            return None
-        return json_loads(rv.data)
-
-    def get_multi(self, id_list):
-        # shortcut for just one id since this is a common
-        # case for us from EventManager.bind_nodes
-        if len(id_list) == 1:
-            id = id_list[0]
-            return {id: self.get(id)}
-
-        rv = self.conn.multiget(self.bucket, id_list, r=1)
-        results = {}
-        for key, value in six.iteritems(rv):
-            if isinstance(value, Exception):
-                six.reraise(type(value), value)
-            if value.status != 200:
-                results[key] = None
-            else:
-                results[key] = json_loads(value.data)
-        return results
-
-    def cleanup(self, cutoff_timestamp):
-        # TODO(dcramer): we should either index timestamps or have this run
-        # a map/reduce (probably the latter)
-        raise NotImplementedError
diff --git a/src/sentry/nodestore/riak/client.py b/src/sentry/nodestore/riak/client.py
deleted file mode 100644
index 2d07a7b1b8..0000000000
--- a/src/sentry/nodestore/riak/client.py
+++ /dev/null
@@ -1,359 +0,0 @@
-from __future__ import absolute_import
-
-import functools
-import six
-import sys
-import socket
-from base64 import b64encode
-from random import shuffle
-from six.moves.queue import Queue
-from time import time
-from threading import Lock, Thread, Event
-
-# utilize the ca_certs path from requests since we already depend on it
-# and they bundle a ca cert.
-from requests.certs import where as ca_certs
-from six.moves.urllib.parse import urlencode, quote_plus
-from urllib3 import HTTPConnectionPool, HTTPSConnectionPool
-from urllib3.exceptions import HTTPError
-
-from sentry.net.http import UnixHTTPConnectionPool
-
-
-DEFAULT_NODES = ({"host": "127.0.0.1", "port": 8098},)
-
-
-def encode_basic_auth(auth):
-    auth = ":".join(auth)
-    return "Basic " + b64encode(auth).decode("utf-8")
-
-
-class SimpleThreadedWorkerPool(object):
-    """\
-    Manages a simple threaded worker pool. The pool will be started when the
-    first job is submitted, and will run to process completion.
-    """
-
-    def __init__(self, size):
-        assert size > 0, "pool must have at laest one worker thread"
-
-        self.__started = False
-        self.__size = size
-
-    def __start(self):
-        self.__tasks = tasks = Queue()
-
-        def consumer():
-            while True:
-                func, args, kwargs, cb = tasks.get()
-                try:
-                    rv = func(*args, **kwargs)
-                except Exception as e:
-                    rv = e
-                finally:
-                    cb(rv)
-                    tasks.task_done()
-
-        for _ in range(self.__size):
-            t = Thread(target=consumer)
-            t.setDaemon(True)
-            t.start()
-
-        self.__started = True
-
-    def submit(self, func_arg_kwargs_cb):
-        """\
-        Submit a task to the worker pool.
-        """
-        if not self.__started:
-            self.__start()
-
-        self.__tasks.put(func_arg_kwargs_cb)
-
-
-class RiakClient(object):
-    """
-    A thread-safe simple light-weight riak client that does only
-    the bare minimum.
-    """
-
-    def __init__(self, multiget_pool_size=5, **kwargs):
-        self.manager = ConnectionManager(**kwargs)
-        self.pool = SimpleThreadedWorkerPool(multiget_pool_size)
-
-    def build_url(self, bucket, key, qs):
-        url = "/buckets/%s/keys/%s" % tuple(map(quote_plus, (bucket, key)))
-        if qs:
-            url += "?" + urlencode(qs)
-        return url
-
-    def put(self, bucket, key, data, headers=None, **kwargs):
-        if headers is None:
-            headers = {}
-        headers["content-type"] = "application/json"
-
-        return self.manager.urlopen(
-            "PUT", self.build_url(bucket, key, kwargs), headers=headers, body=data
-        )
-
-    def delete(self, bucket, key, headers=None, **kwargs):
-        return self.manager.urlopen("DELETE", self.build_url(bucket, key, kwargs), headers=headers)
-
-    def get(self, bucket, key, headers=None, **kwargs):
-        if headers is None:
-            headers = {}
-        headers["accept-encoding"] = "gzip"  # urllib3 will automatically decompress
-
-        return self.manager.urlopen("GET", self.build_url(bucket, key, kwargs), headers=headers)
-
-    def multiget(self, bucket, keys, headers=None, **kwargs):
-        """
-        Thread-safe multiget implementation that shares the same thread pool
-        for all requests.
-        """
-        # Each request is paired with a thread.Event to signal when it is finished
-        requests = [(key, self.build_url(bucket, key, kwargs), Event()) for key in keys]
-
-        results = {}
-
-        def callback(key, event, rv):
-            results[key] = rv
-            # Signal that this request is finished
-            event.set()
-
-        for key, url, event in requests:
-            self.pool.submit(
-                (
-                    self.manager.urlopen,  # func
-                    ("GET", url),  # args
-                    {"headers": headers},  # kwargs
-                    functools.partial(callback, key, event),  # callback
-                )
-            )
-
-        # Now we wait for all of the callbacks to be finished
-        for _, _, event in requests:
-            event.wait()
-
-        return results
-
-    def close(self):
-        self.manager.close()
-
-
-class RoundRobinStrategy(object):
-    def __init__(self):
-        self.i = -1
-
-    def next(self, connections):
-        self.i += 1
-        return connections[self.i % len(connections)]
-
-
-class FirstStrategy(object):
-    def next(self, connections):
-        return connections[0]
-
-
-class ConnectionManager(object):
-    """
-    A thread-safe multi-host http connection manager.
-    """
-
-    def __init__(
-        self,
-        hosts=DEFAULT_NODES,
-        strategy=RoundRobinStrategy,
-        randomize=True,
-        timeout=3,
-        cooldown=5,
-        max_retries=None,
-        tcp_keepalive=True,
-    ):
-        assert hosts
-        self.dead_connections = []
-        self.timeout = timeout
-        self.cooldown = cooldown
-        self.tcp_keepalive = tcp_keepalive
-
-        # Default max_retries to number of hosts
-        if max_retries is None:
-            self.max_retries = len(hosts)
-        else:
-            self.max_retries = max_retries
-
-        self.connections = map(self.create_pool, hosts)
-        # Shuffle up the order to prevent stampeding the same hosts
-        if randomize:
-            shuffle(self.connections)
-
-        # If we have a single connection, we can short-circuit some logic
-        self.single_connection = len(hosts) == 1
-
-        # If we only have one connection, let's override and use a more optimized
-        # strategy
-        if self.single_connection:
-            strategy = FirstStrategy
-
-        self.strategy = strategy()
-
-        # Lock needed when mutating the alive/dead list of connections
-        self._lock = Lock()
-
-    def create_pool(self, host):
-        """
-        Create a new HTTP(S)ConnectionPool for a (host, port) tuple
-        """
-        options = {
-            "timeout": self.timeout,
-            "strict": True,
-            "retries": host.get("retries", 2),
-            # Max of 5 connections open per host
-            # this is arbitrary. The # of connections can burst
-            # above 5 if needed becuase we're also setting
-            # block=False
-            "maxsize": host.get("maxsize", 5),
-            "block": False,
-            "headers": host.get("headers", {}),
-        }
-
-        if "basic_auth" in host:
-            options["headers"]["authorization"] = encode_basic_auth(host["basic_auth"])
-
-        # Support backwards compatibility with `http_port`
-        if "http_port" in host:
-            import warnings
-
-            warnings.warn("'http_port' has been deprecated. Use 'port'.", DeprecationWarning)
-            host["port"] = host.pop("http_port")
-
-        addr = host.get("host", "127.0.0.1")
-        port = int(host.get("port", 8098))
-        secure = host.get("secure", False)
-        if addr[:1] == "/":
-            pool_cls = UnixHTTPConnectionPool
-        elif not secure:
-            pool_cls = HTTPConnectionPool
-        else:
-            pool_cls = HTTPSConnectionPool
-            verify_ssl = host.get("verify_ssl", False)
-            if verify_ssl:
-                options.extend(
-                    {
-                        "cert_reqs": host.get("cert_reqs", "CERT_REQUIRED"),
-                        "ca_certs": host.get("ca_certs", ca_certs()),
-                    }
-                )
-
-        if self.tcp_keepalive:
-            options["socket_options"] = pool_cls.ConnectionCls.default_socket_options + [
-                (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
-            ]
-
-        return pool_cls(addr, port, **options)
-
-    def urlopen(self, method, path, headers=None, **kwargs):
-        """
-        Make a request using the next server according to the connection
-        strategy, and retries up to max_retries attempts. Ultimately,
-        if the request still failed, we reraise the HTTPError from
-        urllib3. If at the start of the request, there are no known
-        available hosts, we revive all dead connections and forcefully
-        attempt to reconnect.
-        """
-
-        # We don't need strict host checking since our client is enforcing
-        # the correct behavior anyways
-        kwargs.setdefault("assert_same_host", False)
-
-        # Keep track of the last exception, so we can raise it if needed
-        last_error = None
-
-        try:
-            for _ in range(self.max_retries + 1):
-                # If we're trying to initiate a new connection, and
-                # all connections are already dead, then we should flail
-                # and attempt to connect to one of them
-                if len(self.connections) == 0:
-                    self.force_revive()
-
-                conn = self.strategy.next(self.connections)  # NOQA
-                if headers is not None:
-                    headers = dict(conn.headers, **headers)
-                try:
-                    return conn.urlopen(method, path, headers=headers, **kwargs)
-                except HTTPError:
-                    self.mark_dead(conn)
-                    last_error = sys.exc_info()
-
-            # We've exhausted the retries, and we still have
-            # all errors, so re-raise the last known error
-            if last_error is not None:
-                six.reraise(*last_error)
-        finally:
-            self.cleanup_dead()
-
-    def mark_dead(self, conn):
-        """
-        Mark a connection as dead.
-        """
-
-        # If we are operating with only a single connection,
-        # it's futile to mark the connection as dead since it'll
-        # just flap between active and dead with no value. In the
-        # event of one connection, we just want to keep retrying
-        # in hopes that it'll eventually work.
-        if self.single_connection:
-            return
-
-        timeout = time() + self.cooldown
-        with self._lock:
-            self.dead_connections.append((conn, timeout))
-            self.connections.remove(conn)
-
-    def force_revive(self):
-        """
-        Forcefully revive all dead connections
-        """
-        with self._lock:
-            for conn, _ in self.dead_connections:
-                self.connections.append(conn)
-            self.dead_connections = []
-
-    def cleanup_dead(self):
-        """
-        Check dead connections and see if any timeouts have expired
-        """
-        if not self.dead_connections:
-            return
-
-        now = time()
-        for conn, timeout in self.dead_connections[:]:
-            if timeout > now:
-                # Can exit fast here on the first non-expired
-                # since dead_connections is ordered
-                return
-
-            # timeout has expired, so move from dead to alive pool
-            with self._lock:
-                try:
-                    # Attempt to remove the connection from dead_connections
-                    # pool, but it's possible that it was already removed in
-                    # another thread.
-                    self.dead_connections.remove((conn, timeout))
-                except ValueError:
-                    # In which case, we don't care and we just carry on.
-                    pass
-                else:
-                    # Only add the connection back into the live pool
-                    # if we've successfully removed from dead pool.
-                    self.connections.append(conn)
-
-    def close(self):
-        """
-        Close all connections to all servers
-        """
-        self.force_revive()
-
-        for conn in self.connections:
-            conn.close()
diff --git a/src/sentry/testutils/skips.py b/src/sentry/testutils/skips.py
index 851e2e578d..f729e4e182 100644
--- a/src/sentry/testutils/skips.py
+++ b/src/sentry/testutils/skips.py
@@ -10,21 +10,6 @@ import pytest
 _service_status = {}
 
 
-def riak_is_available():
-    if "riak" in _service_status:
-        return _service_status["riak"]
-    try:
-        socket.create_connection(("127.0.0.1", 8098), 1.0)
-    except socket.error:
-        _service_status["riak"] = False
-    else:
-        _service_status["riak"] = True
-    return _service_status["riak"]
-
-
-requires_riak = pytest.mark.skipif(not riak_is_available(), reason="requires riak server running")
-
-
 def cassandra_is_available():
     if "cassandra" in _service_status:
         return _service_status["cassandra"]
diff --git a/tests/sentry/nodestore/riak/__init__.py b/tests/sentry/nodestore/riak/__init__.py
deleted file mode 100644
index c3961685ab..0000000000
--- a/tests/sentry/nodestore/riak/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from __future__ import absolute_import
diff --git a/tests/sentry/nodestore/riak/backend/__init__.py b/tests/sentry/nodestore/riak/backend/__init__.py
deleted file mode 100644
index c3961685ab..0000000000
--- a/tests/sentry/nodestore/riak/backend/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from __future__ import absolute_import
diff --git a/tests/sentry/nodestore/riak/backend/tests.py b/tests/sentry/nodestore/riak/backend/tests.py
deleted file mode 100644
index 8a25346e4a..0000000000
--- a/tests/sentry/nodestore/riak/backend/tests.py
+++ /dev/null
@@ -1,33 +0,0 @@
-# -*- coding: utf-8 -*-
-
-from __future__ import absolute_import
-
-from sentry.nodestore.riak.backend import RiakNodeStorage
-from sentry.testutils import TestCase, requires_riak
-
-
-@requires_riak
-class RiakNodeStorageTest(TestCase):
-    def setUp(self):
-        self.ns = RiakNodeStorage(nodes=[{"host": "127.0.0.1", "http_port": 8098}])
-
-    def test_integration(self):
-        node_id = self.ns.create({"foo": "bar"})
-        assert node_id is not None
-
-        self.ns.set(node_id, {"foo": "baz"})
-
-        result = self.ns.get(node_id)
-        assert result == {"foo": "baz"}
-
-        node_id2 = self.ns.create({"foo": "bar"})
-
-        result = self.ns.get_multi([node_id, node_id2])
-        assert result[node_id] == {"foo": "baz"}
-        assert result[node_id2] == {"foo": "bar"}
-
-        self.ns.delete(node_id)
-        assert not self.ns.get(node_id)
-
-        self.ns.delete_multi([node_id2])
-        assert not self.ns.get(node_id2)
