commit 3d6d21da987045f611a4ff2516ed001bf67d48b0
Author: Manu <manu@sentry.io>
Date:   Tue Oct 8 13:48:12 2019 -0700

    ref: Provide a dataset when querying (#14909)
    
    We are moving outcomes to use Snuba instead of Redis.
    
    To make this work, we need to be able to call `snuba.query` with a dataset. So, this change sends a dataset along with the query.
    
    Why not just pass it in `kwargs`? Because in the next step, I'll make `_prepare_query_params` specific to datasets that need to send a `project_id` as outcomes will not always have `project_id`s.

diff --git a/src/sentry/api/bases/organization_events.py b/src/sentry/api/bases/organization_events.py
index ccf816eaa4..04a8fefd2c 100644
--- a/src/sentry/api/bases/organization_events.py
+++ b/src/sentry/api/bases/organization_events.py
@@ -106,7 +106,7 @@ class OrganizationEventsEndpointBase(OrganizationEndpoint):
         # 'legacy' endpoints cannot access transactions dataset.
         # as they often have assumptions about which columns are returned.
         dataset = snuba.detect_dataset(snuba_args, aliased_conditions=True)
-        if dataset != "events":
+        if dataset != snuba.Dataset.Events:
             raise OrganizationEventsError(
                 "Invalid query. You cannot reference non-events data in this endpoint."
             )
diff --git a/src/sentry/api/event_search.py b/src/sentry/api/event_search.py
index 319f98112a..ab88a80212 100644
--- a/src/sentry/api/event_search.py
+++ b/src/sentry/api/event_search.py
@@ -21,7 +21,7 @@ from sentry.search.utils import (
     InvalidQuery,
 )
 from sentry.utils.dates import to_timestamp
-from sentry.utils.snuba import DATASETS, get_snuba_column_name
+from sentry.utils.snuba import Dataset, DATASETS, get_snuba_column_name
 
 WILDCARD_CHARS = re.compile(r"[\*]")
 
@@ -150,8 +150,8 @@ SEARCH_MAP = {
     # TODO(mark) figure out how to safelist aggregate functions/field aliases
     # so they can be used in conditions
 }
-SEARCH_MAP.update(**DATASETS["transactions"])
-SEARCH_MAP.update(**DATASETS["events"])
+SEARCH_MAP.update(**DATASETS[Dataset.Transactions])
+SEARCH_MAP.update(**DATASETS[Dataset.Events])
 
 no_conversion = set(["project_id", "start", "end"])
 
diff --git a/src/sentry/tagstore/snuba/backend.py b/src/sentry/tagstore/snuba/backend.py
index 4b003d51d9..05d03d1cfb 100644
--- a/src/sentry/tagstore/snuba/backend.py
+++ b/src/sentry/tagstore/snuba/backend.py
@@ -93,12 +93,12 @@ class SnubaTagStorage(TagStorage):
         ]
 
         result, totals = snuba.query(
-            kwargs.get("start"),
-            kwargs.get("end"),
-            [tag],
-            conditions,
-            filters,
-            aggregations,
+            start=kwargs.get("start"),
+            end=kwargs.get("end"),
+            groupby=[tag],
+            conditions=conditions,
+            filter_keys=filters,
+            aggregations=aggregations,
             orderby="-count",
             limit=limit,
             totals=True,
@@ -180,12 +180,12 @@ class SnubaTagStorage(TagStorage):
         conditions = [["tags_key", "NOT IN", self.EXCLUDE_TAG_KEYS]]
 
         result = snuba.query(
-            start,
-            end,
-            ["tags_key"],
-            conditions,
-            filters,
-            aggregations,
+            start=start,
+            end=end,
+            groupby=["tags_key"],
+            conditions=conditions,
+            filter_keys=filters,
+            aggregations=aggregations,
             limit=limit,
             orderby="-count",
             referrer="tagstore.__get_tag_keys",
@@ -345,12 +345,12 @@ class SnubaTagStorage(TagStorage):
         ]
 
         result = snuba.query(
-            start,
-            end,
-            ["issue"],
-            conditions,
-            filters,
-            aggregations,
+            start=start,
+            end=end,
+            groupby=["issue"],
+            conditions=conditions,
+            filter_keys=filters,
+            aggregations=aggregations,
             referrer="tagstore.get_group_seen_values_for_environments",
         )
 
@@ -414,12 +414,12 @@ class SnubaTagStorage(TagStorage):
             conditions.append(["tags_key", "NOT IN", self.EXCLUDE_TAG_KEYS])
 
         values_by_key = snuba.query(
-            kwargs.get("start"),
-            kwargs.get("end"),
-            ["tags_key", "tags_value"],
-            conditions,
-            filters,
-            aggregations,
+            start=kwargs.get("start"),
+            end=kwargs.get("end"),
+            groupby=["tags_key", "tags_value"],
+            conditions=conditions,
+            filter_keys=filters,
+            aggregations=aggregations,
             orderby="-count",
             limitby=[value_limit, "tags_key"],
             referrer="tagstore.__get_tag_keys_and_top_values",
@@ -562,12 +562,12 @@ class SnubaTagStorage(TagStorage):
         aggregations = [["uniq", "tags[sentry:user]", "count"]]
 
         result = snuba.query(
-            start,
-            end,
-            ["issue"],
-            None,
-            filters,
-            aggregations,
+            start=start,
+            end=end,
+            groupby=["issue"],
+            conditions=None,
+            filter_keys=filters,
+            aggregations=aggregations,
             referrer="tagstore.get_groups_user_counts",
         )
         return defaultdict(int, {k: v for k, v in result.items() if v})
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 8ce58b6650..701a15a992 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -5,6 +5,7 @@ from copy import deepcopy
 from contextlib import contextmanager
 from datetime import datetime, timedelta
 from dateutil.parser import parse as parse_datetime
+from enum import Enum, unique
 import os
 import pytz
 import re
@@ -37,9 +38,6 @@ MAX_HASHES = 5000
 SAFE_FUNCTION_RE = re.compile(r"-?[a-zA-Z_][a-zA-Z0-9_]*$")
 QUOTED_LITERAL_RE = re.compile(r"^'.*'$")
 
-TRANSACTIONS = "transactions"
-EVENTS = "events"
-
 # Global Snuba request option override dictionary. Only intended
 # to be used with the `options_override` contextmanager below.
 # NOT THREAD SAFE!
@@ -161,12 +159,20 @@ TRANSACTIONS_SENTRY_SNUBA_MAP = {
     "time": "bucketed_end",
 }
 
-DATASETS = {EVENTS: SENTRY_SNUBA_MAP, TRANSACTIONS: TRANSACTIONS_SENTRY_SNUBA_MAP}
+
+@unique
+class Dataset(Enum):
+    Events = "events"
+    Transactions = "transactions"
+    Outcomes = "outcomes"
+
+
+DATASETS = {Dataset.Events: SENTRY_SNUBA_MAP, Dataset.Transactions: TRANSACTIONS_SENTRY_SNUBA_MAP}
 
 # Store the internal field names to save work later on.
 DATASET_FIELDS = {
-    EVENTS: SENTRY_SNUBA_MAP.values(),
-    TRANSACTIONS: TRANSACTIONS_SENTRY_SNUBA_MAP.values(),
+    Dataset.Events: SENTRY_SNUBA_MAP.values(),
+    Dataset.Transactions: TRANSACTIONS_SENTRY_SNUBA_MAP.values(),
 }
 
 
@@ -333,7 +339,7 @@ def zerofill(data, start, end, rollup, orderby):
     return rv
 
 
-def get_snuba_column_name(name, dataset="events"):
+def get_snuba_column_name(name, dataset=Dataset.Events):
     """
     Get corresponding Snuba column name from Sentry snuba map, if not found
     the column is assumed to be a tag. If name is falsy or name is a quoted literal
@@ -366,46 +372,48 @@ def detect_dataset(query_args, aliased_conditions=False):
     if query_args.get("dataset", None):
         return query_args["dataset"]
 
-    dataset = EVENTS
-    transaction_fields = set(DATASETS[TRANSACTIONS].keys()) - set(DATASETS[EVENTS].keys())
+    dataset = Dataset.Events
+    transaction_fields = set(DATASETS[Dataset.Transactions].keys()) - set(
+        DATASETS[Dataset.Events].keys()
+    )
     condition_fieldset = transaction_fields
 
     if aliased_conditions:
         # Release and user are also excluded as they are present on both
         # datasets and don't trigger usage of transactions.
         condition_fieldset = (
-            set(DATASET_FIELDS[TRANSACTIONS])
-            - set(DATASET_FIELDS[EVENTS])
+            set(DATASET_FIELDS[Dataset.Transactions])
+            - set(DATASET_FIELDS[Dataset.Events])
             - set(["release", "user"])
         )
 
     for condition in query_args.get("conditions") or []:
         if isinstance(condition[0], six.string_types) and condition[0] in condition_fieldset:
-            return TRANSACTIONS
+            return Dataset.Transactions
         if condition == ["event.type", "=", "transaction"] or condition == [
             "type",
             "=",
             "transaction",
         ]:
-            return TRANSACTIONS
+            return Dataset.Transactions
 
     for field in query_args.get("selected_columns") or []:
         if isinstance(field, six.string_types) and field in transaction_fields:
-            return TRANSACTIONS
+            return Dataset.Transactions
 
     for field in query_args.get("aggregations") or []:
         if len(field) != 3:
             continue
         if isinstance(field[1], six.string_types) and field[1] in transaction_fields:
-            return TRANSACTIONS
+            return Dataset.Transactions
         if isinstance(field[1], (list, tuple)):
             is_transaction = [column for column in field[1] if column in transaction_fields]
             if is_transaction:
-                return TRANSACTIONS
+                return Dataset.Transactions
 
     for field in query_args.get("groupby") or []:
         if field in transaction_fields:
-            return TRANSACTIONS
+            return Dataset.Transactions
 
     return dataset
 
@@ -447,7 +455,7 @@ def get_function_index(column_expr, depth=0):
         return None
 
 
-def parse_columns_in_functions(col, context=None, index=None, dataset="events"):
+def parse_columns_in_functions(col, context=None, index=None, dataset=Dataset.Events):
     """
     Checks expressions for arguments that should be considered a column while
     ignoring strings that represent clickhouse function names
@@ -495,7 +503,7 @@ def get_arrayjoin(column):
         return match.groups()[0]
 
 
-def valid_orderby(orderby, custom_fields=None, dataset="events"):
+def valid_orderby(orderby, custom_fields=None, dataset=Dataset.Events):
     """
     Check if a field can be used in sorting. We don't allow
     sorting on fields that would be aliased as tag[foo] because those
@@ -699,6 +707,7 @@ def _prepare_query_params(query_params):
 
     query_params.kwargs.update(
         {
+            "dataset": query_params.dataset.value,
             "from_date": start.isoformat(),
             "to_date": end.isoformat(),
             "groupby": query_params.groupby,
@@ -743,6 +752,7 @@ class SnubaQueryParams(object):
 
     def __init__(
         self,
+        dataset=None,
         start=None,
         end=None,
         groupby=None,
@@ -754,6 +764,8 @@ class SnubaQueryParams(object):
         is_grouprelease=False,
         **kwargs
     ):
+        # TODO: instead of having events be the default, make dataset required.
+        self.dataset = dataset or Dataset.Events
         self.start = start or datetime.utcfromtimestamp(0)  # will be clamped to project retention
         self.end = end or datetime.utcnow()
         self.groupby = groupby or []
@@ -767,6 +779,7 @@ class SnubaQueryParams(object):
 
 
 def raw_query(
+    dataset=None,
     start=None,
     end=None,
     groupby=None,
@@ -783,6 +796,7 @@ def raw_query(
     descriptions.
     """
     snuba_params = SnubaQueryParams(
+        dataset=dataset,
         start=start,
         end=end,
         groupby=groupby,
@@ -857,6 +871,7 @@ def bulk_raw_query(snuba_param_list, referrer=None):
 
 
 def query(
+    dataset=None,
     start=None,
     end=None,
     groupby=None,
@@ -875,6 +890,7 @@ def query(
 
     try:
         body = raw_query(
+            dataset=dataset,
             start=start,
             end=end,
             groupby=groupby,
@@ -941,7 +957,7 @@ def constrain_column_to_dataset(col, dataset, value=None):
         return col
     # Special case for the type condition as we only want
     # to drop it when we are querying transactions.
-    if dataset == TRANSACTIONS and col == "type" and value == "transaction":
+    if dataset == Dataset.Transactions and col == "type" and value == "transaction":
         return None
     if not col or QUOTED_LITERAL_RE.match(col):
         return col
@@ -975,7 +991,7 @@ def constrain_condition_to_dataset(cond, dataset):
             # Reformat 32 byte uuids to 36 byte variants.
             # The transactions dataset requires properly formatted uuid values.
             # But the rest of sentry isn't aware of that requirement.
-            if dataset == TRANSACTIONS and name == "event_id" and len(cond[2]) == 32:
+            if dataset == Dataset.Transactions and name == "event_id" and len(cond[2]) == 32:
                 cond[2] = six.text_type(uuid.UUID(cond[2]))
         elif len(cond) == 2 and cond[0] == "has":
             # first function argument is the column if function is "has"
diff --git a/tests/sentry/utils/test_snuba.py b/tests/sentry/utils/test_snuba.py
index ba07c894ec..3b13053a7d 100644
--- a/tests/sentry/utils/test_snuba.py
+++ b/tests/sentry/utils/test_snuba.py
@@ -14,6 +14,7 @@ from sentry.utils.snuba import (
     get_snuba_column_name,
     detect_dataset,
     transform_aliases_and_query,
+    Dataset,
 )
 
 
@@ -305,7 +306,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
                 ["uniq", "transaction_name", "uniq_transaction"],
             ],
             filter_keys={"project_id": [self.project.id]},
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             arrayjoin=None,
             end=None,
             start=None,
@@ -329,7 +330,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
         mock_query.assert_called_with(
             selected_columns=["transaction_name", "duration"],
             filter_keys={"project_id": [self.project.id]},
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             orderby=["finish_ts"],
             aggregations=None,
             arrayjoin=None,
@@ -369,7 +370,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction_op"],
             orderby=["-finish_ts", "-count"],
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             arrayjoin=None,
             end=None,
             start=None,
@@ -399,7 +400,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             ],
             aggregations=[["count", "", "count"]],
             filter_keys={"project_id": [self.project.id]},
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             groupby=None,
             orderby=None,
             arrayjoin=None,
@@ -426,7 +427,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             conditions=[["duration", ">", 200]],
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction_op"],
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             aggregations=None,
             arrayjoin=None,
             end=None,
@@ -453,7 +454,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             conditions=[["tags[type]", "=", "csp"], ["duration", ">", 200]],
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction_op"],
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             aggregations=None,
             arrayjoin=None,
             end=None,
@@ -480,7 +481,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             conditions=[["tags[http_method]", "=", "GET"]],
             filter_keys={"project_id": [self.project.id]},
             groupby=["transaction_op"],
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             aggregations=None,
             arrayjoin=None,
             end=None,
@@ -505,7 +506,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
             selected_columns=["event_id", "duration"],
             conditions=[["event_id", "=", "aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa"]],
             filter_keys={"project_id": [self.project.id]},
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             aggregations=None,
             arrayjoin=None,
             end=None,
@@ -536,7 +537,7 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
                 ]
             ],
             filter_keys={"project_id": [self.project.id]},
-            dataset="transactions",
+            dataset=Dataset.Transactions,
             aggregations=None,
             arrayjoin=None,
             end=None,
@@ -549,60 +550,60 @@ class TransformAliasesAndQueryTransactionsTest(TestCase):
 
 class DetectDatasetTest(TestCase):
     def test_dataset_key(self):
-        query = {"dataset": "events", "conditions": [["event.type", "=", "transaction"]]}
-        assert detect_dataset(query) == "events"
+        query = {"dataset": Dataset.Events, "conditions": [["event.type", "=", "transaction"]]}
+        assert detect_dataset(query) == Dataset.Events
 
     def test_event_type_condition(self):
         query = {"conditions": [["type", "=", "transaction"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
         query = {"conditions": [["type", "=", "error"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
         query = {"conditions": [["type", "=", "transaction"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
         query = {"conditions": [["type", "=", "error"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
     def test_conditions(self):
         query = {"conditions": [["transaction", "=", "api.do_thing"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
         query = {"conditions": [["transaction.name", "=", "api.do_thing"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
         query = {"conditions": [["transaction.duration", ">", "3"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
         # Internal aliases are treated as tags
         query = {"conditions": [["duration", ">", "3"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
     def test_conditions_aliased(self):
         query = {"conditions": [["duration", ">", "3"]]}
-        assert detect_dataset(query, aliased_conditions=True) == "transactions"
+        assert detect_dataset(query, aliased_conditions=True) == Dataset.Transactions
 
         # Not an internal alias
         query = {"conditions": [["transaction.duration", ">", "3"]]}
-        assert detect_dataset(query, aliased_conditions=True) == "events"
+        assert detect_dataset(query, aliased_conditions=True) == Dataset.Events
 
     def test_selected_columns(self):
         query = {"selected_columns": ["id", "message"]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
         query = {"selected_columns": ["id", "transaction", "transaction.duration"]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
     def test_aggregations(self):
         query = {"aggregations": [["argMax", ["id", "timestamp"], "latest_event"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
         query = {"aggregations": [["argMax", ["id", "duration"], "longest"]]}
-        assert detect_dataset(query) == "events"
+        assert detect_dataset(query) == Dataset.Events
 
         query = {"aggregations": [["quantileTiming(0.95)", "transaction.duration", "p95_duration"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
 
         query = {"aggregations": [["uniq", "transaction.name", "uniq_transaction"]]}
-        assert detect_dataset(query) == "transactions"
+        assert detect_dataset(query) == Dataset.Transactions
