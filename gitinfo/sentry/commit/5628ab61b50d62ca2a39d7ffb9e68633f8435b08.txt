commit 5628ab61b50d62ca2a39d7ffb9e68633f8435b08
Author: ted kaemming <ted@kaemming.com>
Date:   Thu Feb 28 15:22:42 2019 -0800

    ref(eventstream): Refactor out `SnubaProtocolEventStream` from HTTP and Kafka transports (#12144)

diff --git a/src/sentry/eventstream/kafka/backend.py b/src/sentry/eventstream/kafka/backend.py
index 6e21a9f6f3..df55892963 100644
--- a/src/sentry/eventstream/kafka/backend.py
+++ b/src/sentry/eventstream/kafka/backend.py
@@ -1,88 +1,22 @@
 from __future__ import absolute_import
 
-from datetime import datetime
 import logging
-import pytz
 import six
-from uuid import uuid4
 
 from confluent_kafka import OFFSET_INVALID, Producer, TopicPartition
 from django.utils.functional import cached_property
 
-from sentry import quotas
-from sentry.eventstream.base import EventStream
 from sentry.eventstream.kafka.consumer import SynchronizedConsumer
 from sentry.eventstream.kafka.protocol import get_task_kwargs_for_message
+from sentry.eventstream.snuba import SnubaProtocolEventStream
 from sentry.tasks.post_process import post_process_group
 from sentry.utils import json
-from sentry.utils.safe import get_path
 
-logger = logging.getLogger(__name__)
 
+logger = logging.getLogger(__name__)
 
-# Beware! Changing this protocol (introducing a new version, or the message
-# format/fields themselves) requires consideration of all downstream consumers.
-# This includes the post-process forwarder code!
-EVENT_PROTOCOL_VERSION = 2
-
-# Version 1 format: (1, TYPE, [...REST...])
-#   Insert: (1, 'insert', {
-#       ...event json...
-#   }, {
-#       ...state for post-processing...
-#   })
-#
-#   Mutations that *should be ignored*: (1, ('delete_groups'|'unmerge'|'merge'), {...})
-#
-#   In short, for protocol version 1 only messages starting with (1, 'insert', ...)
-#   should be processed.
-
-# Version 2 format: (2, TYPE, [...REST...])
-#   Insert: (2, 'insert', {
-#       ...event json...
-#   }, {
-#       ...state for post-processing...
-#   })
-#   Delete Groups: (2, '(start_delete_groups|end_delete_groups)', {
-#       'transaction_id': uuid,
-#       'project_id': id,
-#       'group_ids': [id2, id2, id3],
-#       'datetime': timestamp,
-#   })
-#   Merge: (2, '(start_merge|end_merge)', {
-#       'transaction_id': uuid,
-#       'project_id': id,
-#       'previous_group_ids': [id2, id2],
-#       'new_group_id': id,
-#       'datetime': timestamp,
-#   })
-#   Unmerge: (2, '(start_unmerge|end_unmerge)', {
-#       'transaction_id': uuid,
-#       'project_id': id,
-#       'previous_group_id': id,
-#       'new_group_id': id,
-#       'hashes': [hash2, hash2]
-#       'datetime': timestamp,
-#   })
-#   Delete Tag: (2, '(start_delete_tag|end_delete_tag)', {
-#       'transaction_id': uuid,
-#       'project_id': id,
-#       'tag': 'foo',
-#       'datetime': timestamp,
-#   })
-
-
-class KafkaEventStream(EventStream):
-
-    # These keys correspond to tags that are typically prefixed with `sentry:`
-    # and will wreak havok in the UI if both the `sentry:`-prefixed and
-    # non-prefixed variations occur in a response.
-    UNEXPECTED_TAG_KEYS = frozenset([
-        'dist',
-        'release',
-        'user',
-    ])
 
+class KafkaEventStream(SnubaProtocolEventStream):
     def __init__(self, publish_topic='events', producer_configuration=None, **options):
         if producer_configuration is None:
             producer_configuration = {}
@@ -119,7 +53,7 @@ class KafkaEventStream(EventStream):
                 topic=self.publish_topic,
                 key=key.encode('utf-8'),
                 value=json.dumps(
-                    (EVENT_PROTOCOL_VERSION, _type) + extra_data
+                    (self.EVENT_PROTOCOL_VERSION, _type) + extra_data
                 ),
                 on_delivery=self.delivery_callback,
             )
@@ -131,166 +65,6 @@ class KafkaEventStream(EventStream):
             # flush() is a convenience method that calls poll() until len() is zero
             self.producer.flush()
 
-    def insert(self, group, event, is_new, is_sample, is_regression,
-               is_new_group_environment, primary_hash, skip_consume=False):
-        project = event.project
-        retention_days = quotas.get_event_retention(
-            organization=project.organization,
-        )
-
-        event_data = event.get_raw_data()
-
-        unexpected_tags = set([
-            k for (k, v) in (get_path(event_data, 'tags', filter=True) or [])
-            if k in self.UNEXPECTED_TAG_KEYS
-        ])
-        if unexpected_tags:
-            logger.error('%r received unexpected tags: %r', self, unexpected_tags)
-
-        self._send(project.id, 'insert', extra_data=({
-            'group_id': event.group_id,
-            'event_id': event.event_id,
-            'organization_id': project.organization_id,
-            'project_id': event.project_id,
-            # TODO(mitsuhiko): We do not want to send this incorrect
-            # message but this is what snuba needs at the moment.
-            'message': event.message,
-            'platform': event.platform,
-            'datetime': event.datetime,
-            'data': event_data,
-            'primary_hash': primary_hash,
-            'retention_days': retention_days,
-        }, {
-            'is_new': is_new,
-            'is_sample': is_sample,
-            'is_regression': is_regression,
-            'is_new_group_environment': is_new_group_environment,
-            'skip_consume': skip_consume,
-        },))
-
-    def start_delete_groups(self, project_id, group_ids):
-        if not group_ids:
-            return
-
-        state = {
-            'transaction_id': uuid4().hex,
-            'project_id': project_id,
-            'group_ids': list(group_ids),
-            'datetime': datetime.now(tz=pytz.utc),
-        }
-
-        self._send(
-            project_id,
-            'start_delete_groups',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-        return state
-
-    def end_delete_groups(self, state):
-        state = state.copy()
-        state['datetime'] = datetime.now(tz=pytz.utc)
-        self._send(
-            state['project_id'],
-            'end_delete_groups',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-    def start_merge(self, project_id, previous_group_ids, new_group_id):
-        if not previous_group_ids:
-            return
-
-        state = {
-            'transaction_id': uuid4().hex,
-            'project_id': project_id,
-            'previous_group_ids': list(previous_group_ids),
-            'new_group_id': new_group_id,
-            'datetime': datetime.now(tz=pytz.utc),
-        }
-
-        self._send(
-            project_id,
-            'start_merge',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-        return state
-
-    def end_merge(self, state):
-        state = state.copy()
-        state['datetime'] = datetime.now(tz=pytz.utc)
-        self._send(
-            state['project_id'],
-            'end_merge',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-    def start_unmerge(self, project_id, hashes, previous_group_id, new_group_id):
-        if not hashes:
-            return
-
-        state = {
-            'transaction_id': uuid4().hex,
-            'project_id': project_id,
-            'previous_group_id': previous_group_id,
-            'new_group_id': new_group_id,
-            'hashes': list(hashes),
-            'datetime': datetime.now(tz=pytz.utc),
-        }
-
-        self._send(
-            project_id,
-            'start_unmerge',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-        return state
-
-    def end_unmerge(self, state):
-        state = state.copy()
-        state['datetime'] = datetime.now(tz=pytz.utc)
-        self._send(
-            state['project_id'],
-            'end_unmerge',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-    def start_delete_tag(self, project_id, tag):
-        if not tag:
-            return
-
-        state = {
-            'transaction_id': uuid4().hex,
-            'project_id': project_id,
-            'tag': tag,
-            'datetime': datetime.now(tz=pytz.utc),
-        }
-
-        self._send(
-            project_id,
-            'start_delete_tag',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
-        return state
-
-    def end_delete_tag(self, state):
-        state = state.copy()
-        state['datetime'] = datetime.now(tz=pytz.utc)
-        self._send(
-            state['project_id'],
-            'end_delete_tag',
-            extra_data=(state,),
-            asynchronous=False
-        )
-
     def requires_post_process_forwarder(self):
         return True
 
diff --git a/src/sentry/eventstream/snuba.py b/src/sentry/eventstream/snuba.py
index 259c5be3c1..120e7d55c1 100644
--- a/src/sentry/eventstream/snuba.py
+++ b/src/sentry/eventstream/snuba.py
@@ -1,15 +1,251 @@
 from __future__ import absolute_import
 
+import logging
+from datetime import datetime
+from uuid import uuid4
+
+import pytz
 import urllib3
 
-from sentry.eventstream.kafka import KafkaEventStream
-from sentry.eventstream.kafka.backend import EVENT_PROTOCOL_VERSION
+from sentry import quotas
+from sentry.eventstream.base import EventStream
 from sentry.utils import snuba, json
+from sentry.utils.safe import get_path
+
+
+logger = logging.getLogger(__name__)
+
+
+# Version 1 format: (1, TYPE, [...REST...])
+#   Insert: (1, 'insert', {
+#       ...event json...
+#   }, {
+#       ...state for post-processing...
+#   })
+#
+#   Mutations that *should be ignored*: (1, ('delete_groups'|'unmerge'|'merge'), {...})
+#
+#   In short, for protocol version 1 only messages starting with (1, 'insert', ...)
+#   should be processed.
+
+# Version 2 format: (2, TYPE, [...REST...])
+#   Insert: (2, 'insert', {
+#       ...event json...
+#   }, {
+#       ...state for post-processing...
+#   })
+#   Delete Groups: (2, '(start_delete_groups|end_delete_groups)', {
+#       'transaction_id': uuid,
+#       'project_id': id,
+#       'group_ids': [id2, id2, id3],
+#       'datetime': timestamp,
+#   })
+#   Merge: (2, '(start_merge|end_merge)', {
+#       'transaction_id': uuid,
+#       'project_id': id,
+#       'previous_group_ids': [id2, id2],
+#       'new_group_id': id,
+#       'datetime': timestamp,
+#   })
+#   Unmerge: (2, '(start_unmerge|end_unmerge)', {
+#       'transaction_id': uuid,
+#       'project_id': id,
+#       'previous_group_id': id,
+#       'new_group_id': id,
+#       'hashes': [hash2, hash2]
+#       'datetime': timestamp,
+#   })
+#   Delete Tag: (2, '(start_delete_tag|end_delete_tag)', {
+#       'transaction_id': uuid,
+#       'project_id': id,
+#       'tag': 'foo',
+#       'datetime': timestamp,
+#   })
+
+
+class SnubaProtocolEventStream(EventStream):
+
+    # Beware! Changing this protocol (introducing a new version, or the message
+    # format/fields themselves) requires consideration of all downstream
+    # consumers. This includes the post-processing forwarder code!
+    EVENT_PROTOCOL_VERSION = 2
+
+    # These keys correspond to tags that are typically prefixed with `sentry:`
+    # and will wreak havok in the UI if both the `sentry:`-prefixed and
+    # non-prefixed variations occur in a response.
+    UNEXPECTED_TAG_KEYS = frozenset([
+        'dist',
+        'release',
+        'user',
+    ])
+
+    def insert(self, group, event, is_new, is_sample, is_regression,
+               is_new_group_environment, primary_hash, skip_consume=False):
+        project = event.project
+        retention_days = quotas.get_event_retention(
+            organization=project.organization,
+        )
+
+        event_data = event.get_raw_data()
+
+        unexpected_tags = set([
+            k for (k, v) in (get_path(event_data, 'tags', filter=True) or [])
+            if k in self.UNEXPECTED_TAG_KEYS
+        ])
+        if unexpected_tags:
+            logger.error('%r received unexpected tags: %r', self, unexpected_tags)
+
+        self._send(project.id, 'insert', extra_data=({
+            'group_id': event.group_id,
+            'event_id': event.event_id,
+            'organization_id': project.organization_id,
+            'project_id': event.project_id,
+            # TODO(mitsuhiko): We do not want to send this incorrect
+            # message but this is what snuba needs at the moment.
+            'message': event.message,
+            'platform': event.platform,
+            'datetime': event.datetime,
+            'data': event_data,
+            'primary_hash': primary_hash,
+            'retention_days': retention_days,
+        }, {
+            'is_new': is_new,
+            'is_sample': is_sample,
+            'is_regression': is_regression,
+            'is_new_group_environment': is_new_group_environment,
+            'skip_consume': skip_consume,
+        },))
+
+    def start_delete_groups(self, project_id, group_ids):
+        if not group_ids:
+            return
+
+        state = {
+            'transaction_id': uuid4().hex,
+            'project_id': project_id,
+            'group_ids': list(group_ids),
+            'datetime': datetime.now(tz=pytz.utc),
+        }
+
+        self._send(
+            project_id,
+            'start_delete_groups',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+        return state
+
+    def end_delete_groups(self, state):
+        state = state.copy()
+        state['datetime'] = datetime.now(tz=pytz.utc)
+        self._send(
+            state['project_id'],
+            'end_delete_groups',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+    def start_merge(self, project_id, previous_group_ids, new_group_id):
+        if not previous_group_ids:
+            return
+
+        state = {
+            'transaction_id': uuid4().hex,
+            'project_id': project_id,
+            'previous_group_ids': list(previous_group_ids),
+            'new_group_id': new_group_id,
+            'datetime': datetime.now(tz=pytz.utc),
+        }
+
+        self._send(
+            project_id,
+            'start_merge',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+        return state
+
+    def end_merge(self, state):
+        state = state.copy()
+        state['datetime'] = datetime.now(tz=pytz.utc)
+        self._send(
+            state['project_id'],
+            'end_merge',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+    def start_unmerge(self, project_id, hashes, previous_group_id, new_group_id):
+        if not hashes:
+            return
+
+        state = {
+            'transaction_id': uuid4().hex,
+            'project_id': project_id,
+            'previous_group_id': previous_group_id,
+            'new_group_id': new_group_id,
+            'hashes': list(hashes),
+            'datetime': datetime.now(tz=pytz.utc),
+        }
+
+        self._send(
+            project_id,
+            'start_unmerge',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+        return state
+
+    def end_unmerge(self, state):
+        state = state.copy()
+        state['datetime'] = datetime.now(tz=pytz.utc)
+        self._send(
+            state['project_id'],
+            'end_unmerge',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+    def start_delete_tag(self, project_id, tag):
+        if not tag:
+            return
+
+        state = {
+            'transaction_id': uuid4().hex,
+            'project_id': project_id,
+            'tag': tag,
+            'datetime': datetime.now(tz=pytz.utc),
+        }
+
+        self._send(
+            project_id,
+            'start_delete_tag',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+        return state
+
+    def end_delete_tag(self, state):
+        state = state.copy()
+        state['datetime'] = datetime.now(tz=pytz.utc)
+        self._send(
+            state['project_id'],
+            'end_delete_tag',
+            extra_data=(state,),
+            asynchronous=False
+        )
+
+    def _send(self, project_id, _type, extra_data=(), asynchronous=True):
+        raise NotImplementedError
 
 
-class SnubaEventStream(KafkaEventStream):
+class SnubaEventStream(SnubaProtocolEventStream):
     def _send(self, project_id, _type, extra_data=(), asynchronous=True):
-        data = (EVENT_PROTOCOL_VERSION, _type) + extra_data
+        data = (self.EVENT_PROTOCOL_VERSION, _type) + extra_data
 
         try:
             resp = snuba._snuba_pool.urlopen(
