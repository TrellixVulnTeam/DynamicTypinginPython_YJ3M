commit 3fef5fe2481c0cdd95368415543ec5b17f07b134
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Thu Sep 20 13:10:27 2018 -0500

    fix(snuba): Add option for max Snuba chunk search time (#9830)

diff --git a/src/sentry/options/defaults.py b/src/sentry/options/defaults.py
index f387aac2a7..737d4def98 100644
--- a/src/sentry/options/defaults.py
+++ b/src/sentry/options/defaults.py
@@ -138,3 +138,4 @@ register('vsts.client-secret', flags=FLAG_PRIORITIZE_DISK)
 register('snuba.search.max-pre-snuba-candidates', default=500)
 register('snuba.search.chunk-growth-rate', default=1.5)
 register('snuba.search.max-chunk-size', default=2000)
+register('snuba.search.max-total-chunk-time-seconds', default=30.0)
diff --git a/src/sentry/search/snuba/backend.py b/src/sentry/search/snuba/backend.py
index 0c848145e2..3224fa5e27 100644
--- a/src/sentry/search/snuba/backend.py
+++ b/src/sentry/search/snuba/backend.py
@@ -5,6 +5,7 @@ import six
 import logging
 import math
 import pytz
+import time
 from collections import defaultdict
 from datetime import timedelta, datetime
 
@@ -159,8 +160,8 @@ class SnubaSearchBackend(ds.DjangoSearchBackend):
         now = timezone.now()
         end = parameters.get('date_to') or (now + ALLOWED_FUTURE_DELTA)
         # TODO: Presumably we want to search back to the project's full retention,
-        #       which may be higher than 90 days in the future, but apparently
-        #       `retention_window_start` can be None?
+        #       which may be higher than 90 days in the past, but apparently
+        #       `retention_window_start` can be None(?), so we need a fallback.
         start = max(
             filter(None, [
                 retention_window_start,
@@ -269,13 +270,16 @@ class SnubaSearchBackend(ds.DjangoSearchBackend):
         min_score = float('inf')
         max_score = -1
 
+        max_time = options.get('snuba.search.max-total-chunk-time-seconds')
+        time_start = time.time()
+
         # Do smaller searches in chunks until we have enough results
         # to answer the query (or hit the end of possible results). We do
         # this because a common case for search is to return 100 groups
         # sorted by `last_seen`, and we want to avoid returning all of
         # a project's hashes and then post-sorting them all in Postgres
         # when typically the first N results will do.
-        while True:
+        while (time.time() - time_start) < max_time:
             num_chunks += 1
 
             # grow the chunk size on each iteration to account for huge projects
