commit 20d89abff19a06bf142e7bfa5c0b91125f2f5d8f
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Thu Apr 26 13:39:45 2018 -0500

    perf(snuba): Only add aggregation columns that are necessary for the â€¦ (#8150)

diff --git a/src/sentry/search/snuba/backend.py b/src/sentry/search/snuba/backend.py
index 42e11bf1b0..11b529bcc7 100644
--- a/src/sentry/search/snuba/backend.py
+++ b/src/sentry/search/snuba/backend.py
@@ -19,12 +19,6 @@ from sentry.utils.dates import to_timestamp
 
 
 logger = logging.getLogger('sentry.search.snuba')
-
-
-# https://github.com/getsentry/sentry/blob/804c85100d0003cfdda91701911f21ed5f66f67c/src/sentry/event_manager.py#L241-L271
-priority_expr = 'toUInt32(log(times_seen) * 600) + toUInt32(last_seen)'
-
-
 datetime_format = '%Y-%m-%dT%H:%M:%S+00:00'
 
 
@@ -56,24 +50,34 @@ def _datetime_cursor_calculator(field, fn):
 
 sort_strategies = {
     # sort_by -> Tuple[
-    #   String: expression to generate sort value (of type T, used below),
+    #   String: column or alias to sort by (of type T, used below),
+    #   List[String]: extra aggregate columns required for this sorting strategy,
     #   Function[T] -> int: function for converting a group's data to a cursor value),
     # ]
     'priority': (
-        '-priority', calculate_priority_cursor,
+        'priority', ['last_seen', 'times_seen'], calculate_priority_cursor,
     ),
     'date': (
-        '-last_seen', _datetime_cursor_calculator('last_seen', max),
+        'last_seen', [], _datetime_cursor_calculator('last_seen', max),
     ),
     'new': (
-        '-first_seen', _datetime_cursor_calculator('first_seen', min),
+        'first_seen', [], _datetime_cursor_calculator('first_seen', min),
     ),
     'freq': (
-        '-times_seen', lambda data: sum(data['times_seen']),
+        'times_seen', [], lambda data: sum(data['times_seen']),
     ),
 }
 
 
+aggregation_defs = {
+    'times_seen': ['count()', ''],
+    'first_seen': ['min', 'timestamp'],
+    'last_seen': ['max', 'timestamp'],
+    # https://github.com/getsentry/sentry/blob/804c85100d0003cfdda91701911f21ed5f66f67c/src/sentry/event_manager.py#L241-L271
+    'priority': ['toUInt32(log(times_seen) * 600) + toUInt32(last_seen)', ''],
+}
+
+
 class SnubaConditionBuilder(object):
     """\
     Constructions a Snuba conditions list from a ``parameters`` mapping.
@@ -232,7 +236,7 @@ class SnubaSearchBackend(ds.DjangoSearchBackend):
         # query with Snuba? Something else?
         candidate_group_ids = list(group_queryset.values_list('id', flat=True))
 
-        sort_expression, calculate_cursor_for_group = sort_strategies[sort_by]
+        sort, extra_aggregations, calculate_cursor_for_group = sort_strategies[sort_by]
 
         group_data = do_search(
             project_id=project.id,
@@ -240,7 +244,8 @@ class SnubaSearchBackend(ds.DjangoSearchBackend):
             tags=tags,
             start=start,
             end=end,
-            sort=sort_expression,
+            sort=sort,
+            extra_aggregations=extra_aggregations,
             candidates=candidate_group_ids,
             **parameters
         )
@@ -262,7 +267,8 @@ class SnubaSearchBackend(ds.DjangoSearchBackend):
 
 
 def do_search(project_id, environment_id, tags, start, end,
-              sort, candidates=None, limit=1000, **parameters):
+              sort, extra_aggregations, candidates=None, limit=1000, **parameters):
+
     from sentry.search.base import ANY
 
     filters = {
@@ -286,6 +292,10 @@ def do_search(project_id, environment_id, tags, start, end,
 
         filters['primary_hash'] = hashes
 
+        # TODO: See TODO above about sending too many candidates to Snuba.
+        # https://github.com/getsentry/sentry/blob/63c50ec68ed4fb2314e20323198dd384487feba7/src/sentry/search/snuba/backend.py#L218
+        limit = len(hashes)
+
     having = SnubaConditionBuilder({
         'age_from': ScalarCondition('first_seen', '>'),
         'age_to': ScalarCondition('first_seen', '<'),
@@ -306,17 +316,21 @@ def do_search(project_id, environment_id, tags, start, end,
         else:
             conditions.append((col, '=', val))
 
-    aggregations = [
-        ['count()', '', 'times_seen'],
-        ['min', 'timestamp', 'first_seen'],
-        ['max', 'timestamp', 'last_seen'],
-        [priority_expr, '', 'priority']
-    ]
-
-    # {hash -> {times_seen -> int
-    #           first_seen -> date_str,
-    #           last_seen -> date_str,
-    #           priority -> int},
+    required_aggregations = set([sort] + extra_aggregations)
+    for h in having:
+        alias = h[0]
+        required_aggregations.add(alias)
+
+    aggregations = []
+    for alias in required_aggregations:
+        aggregations.append(aggregation_defs[alias] + [alias])
+
+    # {hash -> {<agg_alias> -> <agg_value>,
+    #           <agg_alias> -> <agg_value>,
+    #           ...},
+    #  ...}
+    # _OR_ if there's only one <agg_alias> in use
+    # {hash -> <agg_value>,
     #  ...}
     snuba_results = snuba.query(
         start=start,
@@ -326,7 +340,7 @@ def do_search(project_id, environment_id, tags, start, end,
         having=having,
         filter_keys=filters,
         aggregations=aggregations,
-        orderby=sort,
+        orderby='-' + sort,
         limit=limit,
     )
 
@@ -353,8 +367,19 @@ def do_search(project_id, environment_id, tags, start, end,
                 group_data[group_id] = defaultdict(list)
 
             dest = group_data[group_id]
-            for k, v in obj.items():
-                dest[k].append(v)
+
+            # NOTE: The Snuba utility code is trying to be helpful by collapsing
+            # results with only one aggregate down to the single value. It's a
+            # bit of a hack that we then immediately undo that work here, but
+            # many other callers get value out of that functionality. If we see
+            # this pattern again we should either add an option to opt-out of
+            # the 'help' here or remove it from the Snuba code altogether.
+            if len(required_aggregations) == 1:
+                alias = list(required_aggregations)[0]
+                dest[alias].append(obj)
+            else:
+                for k, v in obj.items():
+                    dest[k].append(v)
         else:
             logger.warning(
                 'search.hash_not_found',
diff --git a/tests/snuba/search/test_backend.py b/tests/snuba/search/test_backend.py
index a99b214d72..d2cc9a5a4b 100644
--- a/tests/snuba/search/test_backend.py
+++ b/tests/snuba/search/test_backend.py
@@ -1,9 +1,11 @@
 from __future__ import absolute_import
 
+import mock
 import pytz
 import pytest
 from datetime import datetime, timedelta
 from django.conf import settings
+from django.utils import timezone
 
 from sentry.event_manager import ScoreClause
 from sentry.models import (
@@ -767,3 +769,69 @@ class SnubaSearchTest(SnubaTestCase):
             environment = self.create_environment()
             result = get_latest_release(self.project, environment)
             assert result == new.version
+
+    @mock.patch('sentry.utils.snuba.query')
+    def test_optimized_aggregates(self, query_mock):
+        query_mock.return_value = {}
+
+        def Any(cls):
+            class Any(object):
+                def __eq__(self, other):
+                    return isinstance(other, cls)
+            return Any()
+
+        common_args = {
+            'start': Any(datetime),
+            'end': Any(datetime),
+            'filter_keys': {
+                'project_id': [2],
+                'primary_hash': [u'513772ee53011ad9f4dc374b2d34d0e9']
+            },
+            'groupby': ['primary_hash'],
+            'conditions': [],
+            'limit': Any(int),
+        }
+
+        self.backend.query(self.project, query='foo')
+        assert query_mock.call_args == mock.call(
+            orderby='-last_seen',
+            aggregations=[['max', 'timestamp', 'last_seen']],
+            having=[],
+            **common_args
+        )
+
+        self.backend.query(self.project, query='foo', sort_by='date', last_seen_from=timezone.now())
+        assert query_mock.call_args == mock.call(
+            orderby='-last_seen',
+            aggregations=[['max', 'timestamp', 'last_seen']],
+            having=[('last_seen', '>=', Any(int))],
+            **common_args
+        )
+
+        self.backend.query(self.project, query='foo', sort_by='priority')
+        assert query_mock.call_args == mock.call(
+            orderby='-priority',
+            aggregations=[
+                ['toUInt32(log(times_seen) * 600) + toUInt32(last_seen)', '', 'priority'],
+                ['count()', '', 'times_seen'],
+                ['max', 'timestamp', 'last_seen']
+            ],
+            having=[],
+            **common_args
+        )
+
+        self.backend.query(self.project, query='foo', sort_by='freq', times_seen=5)
+        assert query_mock.call_args == mock.call(
+            orderby='-times_seen',
+            aggregations=[['count()', '', 'times_seen']],
+            having=[('times_seen', '=', 5)],
+            **common_args
+        )
+
+        self.backend.query(self.project, query='foo', sort_by='new', age_from=timezone.now())
+        assert query_mock.call_args == mock.call(
+            orderby='-first_seen',
+            aggregations=[['min', 'timestamp', 'first_seen']],
+            having=[('first_seen', '>=', Any(int))],
+            **common_args
+        )
