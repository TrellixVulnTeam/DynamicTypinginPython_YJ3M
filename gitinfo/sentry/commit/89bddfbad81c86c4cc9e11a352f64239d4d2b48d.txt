commit 89bddfbad81c86c4cc9e11a352f64239d4d2b48d
Author: Mark Story <mark@sentry.io>
Date:   Mon Jan 20 16:16:47 2020 -0500

    ref: Consolidate code for resolving aliases (#16521)
    
    Consolidate the two implementations of alias resolution so that we only
    have one copy of it. I've removed the special casing around the
    transactions dataset, as that dataset is no longer in active use and
    will be removed from sentry's client code very soon.

diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index b69ba6d9f7..2fdee8dfc0 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -23,10 +23,10 @@ from sentry.utils.snuba import (
     SnubaTSResult,
     DISCOVER_COLUMN_MAP,
     QUOTED_LITERAL_RE,
-    get_function_index,
     raw_query,
     to_naive_timestamp,
     naiveify_datetime,
+    resolve_condition,
 )
 
 __all__ = (
@@ -137,6 +137,8 @@ def create_reference_event_conditions(reference_event):
 
 def resolve_column(col):
     """
+    Used as a column resolver in discover queries.
+
     Resolve a public schema name to the discover dataset.
     unknown columns are converted into tags expressions.
     """
@@ -151,49 +153,6 @@ def resolve_column(col):
     return DISCOVER_COLUMN_MAP.get(col, u"tags[{}]".format(col))
 
 
-def resolve_condition(cond):
-    """
-    When conditions have been parsed by the api.event_search module
-    we can end up with conditions that are not valid on the current dataset
-    due to how ap.event_search checks for valid field names without
-    being aware of the dataset.
-
-    We have the dataset context here, so we need to re-scope conditions to the
-    current dataset.
-    """
-    index = get_function_index(cond)
-    if index is not None:
-        # IN conditions are detected as a function but aren't really.
-        if cond[index] == "IN":
-            cond[0] = resolve_column(cond[0])
-            return cond
-
-        func_args = cond[index + 1]
-        for (i, arg) in enumerate(func_args):
-            # Nested function
-            if isinstance(arg, (list, tuple)):
-                func_args[i] = resolve_condition(arg)
-            else:
-                func_args[i] = resolve_column(arg)
-        cond[index + 1] = func_args
-        return cond
-
-    # No function name found
-    if isinstance(cond, (list, tuple)) and len(cond):
-        # Condition is [col, operator, value]
-        if isinstance(cond[0], six.string_types) and len(cond) == 3:
-            cond[0] = resolve_column(cond[0])
-            return cond
-        if isinstance(cond[0], (list, tuple)):
-            if get_function_index(cond[0]) is not None:
-                cond[0] = resolve_condition(cond[0])
-                return cond
-            else:
-                # Nested conditions
-                return [resolve_condition(item) for item in cond]
-    raise ValueError("Unexpected condition format %s" % cond)
-
-
 def resolve_discover_aliases(snuba_args):
     """
     Resolve the public schema aliases to the discover dataset.
@@ -237,7 +196,7 @@ def resolve_discover_aliases(snuba_args):
     conditions = resolved.get("conditions")
     if conditions:
         for (i, condition) in enumerate(conditions):
-            replacement = resolve_condition(condition)
+            replacement = resolve_condition(condition, resolve_column)
             conditions[i] = replacement
         resolved["conditions"] = list(filter(None, conditions))
 
diff --git a/src/sentry/utils/snuba.py b/src/sentry/utils/snuba.py
index 1f09a5ab60..d641e351f4 100644
--- a/src/sentry/utils/snuba.py
+++ b/src/sentry/utils/snuba.py
@@ -5,6 +5,7 @@ from copy import deepcopy
 from contextlib import contextmanager
 from datetime import datetime, timedelta
 from dateutil.parser import parse as parse_datetime
+import functools
 import os
 import pytz
 import re
@@ -665,22 +666,10 @@ def nest_groups(data, groups, aggregate_cols):
         )
 
 
-def constrain_column_to_dataset(col, dataset, value=None):
-    """
-    Ensure conditions only reference valid columns on the provided
-    dataset. Return none for conditions to be removed, and convert
-    unknown columns into tags expressions.
-
-    :deprecated: This method and the automatic dataset resolution is deprecated.
-    You should use sentry.snuba.discover instead.
-    """
+def resolve_column(col, dataset):
     if col.startswith("tags["):
         return col
 
-    # Special case for the type condition as we only want
-    # to drop it when we are querying transactions.
-    if dataset == Dataset.Transactions and col == "event.type" and value == "transaction":
-        return None
     if not col or QUOTED_LITERAL_RE.match(col):
         return col
     if col in DATASETS[dataset]:
@@ -691,7 +680,7 @@ def constrain_column_to_dataset(col, dataset, value=None):
     return u"tags[{}]".format(col)
 
 
-def constrain_condition_to_dataset(cond, dataset):
+def resolve_condition(cond, column_resolver):
     """
     When conditions have been parsed by the api.event_search module
     we can end up with conditions that are not valid on the current dataset
@@ -701,23 +690,24 @@ def constrain_condition_to_dataset(cond, dataset):
     We have the dataset context here, so we need to re-scope conditions to the
     current dataset.
 
-    :deprecated: This method and the automatic dataset resolution is deprecated.
-    You should use sentry.snuba.discover instead.
+    cond (tuple) Condition to resolve aliases in.
+    column_resolver (Function[string]) Function to resolve column names for the
+                                       current dataset.
     """
     index = get_function_index(cond)
     if index is not None:
         # IN conditions are detected as a function but aren't really.
         if cond[index] == "IN":
-            cond[0] = constrain_column_to_dataset(cond[0], dataset)
+            cond[0] = column_resolver(cond[0])
             return cond
 
         func_args = cond[index + 1]
         for (i, arg) in enumerate(func_args):
             # Nested function
             if isinstance(arg, (list, tuple)):
-                func_args[i] = constrain_condition_to_dataset(arg, dataset)
+                func_args[i] = resolve_condition(arg, column_resolver)
             else:
-                func_args[i] = constrain_column_to_dataset(arg, dataset)
+                func_args[i] = column_resolver(arg)
         cond[index + 1] = func_args
         return cond
 
@@ -725,20 +715,15 @@ def constrain_condition_to_dataset(cond, dataset):
     if isinstance(cond, (list, tuple)) and len(cond):
         # Condition is [col, operator, value]
         if isinstance(cond[0], six.string_types) and len(cond) == 3:
-            # Map column name to current dataset removing
-            # invalid conditions based on the dataset.
-            name = constrain_column_to_dataset(cond[0], dataset, cond[2])
-            if name is None:
-                return None
-            cond[0] = name
+            cond[0] = column_resolver(cond[0])
             return cond
         if isinstance(cond[0], (list, tuple)):
             if get_function_index(cond[0]) is not None:
-                cond[0] = constrain_condition_to_dataset(cond[0], dataset)
+                cond[0] = resolve_condition(cond[0], column_resolver)
                 return cond
             else:
                 # Nested conditions
-                return [constrain_condition_to_dataset(item, dataset) for item in cond]
+                return [resolve_condition(item, column_resolver) for item in cond]
     raise ValueError("Unexpected condition format %s" % cond)
 
 
@@ -776,7 +761,7 @@ def aliased_query(
             if isinstance(col, (list, tuple)):
                 derived_columns.append(col[2])
             else:
-                selected_columns[i] = constrain_column_to_dataset(col, dataset)
+                selected_columns[i] = resolve_column(col, dataset)
         selected_columns = list(filter(None, selected_columns))
 
     if aggregations:
@@ -784,8 +769,9 @@ def aliased_query(
             derived_columns.append(aggregation[2])
 
     if conditions:
+        column_resolver = functools.partial(resolve_column, dataset=dataset)
         for (i, condition) in enumerate(conditions):
-            replacement = constrain_condition_to_dataset(condition, dataset)
+            replacement = resolve_condition(condition, column_resolver)
             conditions[i] = replacement
         conditions = list(filter(None, conditions))
 
@@ -795,7 +781,7 @@ def aliased_query(
         for (i, order) in enumerate(orderby):
             order_field = order.lstrip("-")
             if order_field not in derived_columns:
-                order_field = constrain_column_to_dataset(order_field, dataset)
+                order_field = resolve_column(order_field, dataset)
             updated_order.append(u"{}{}".format("-" if order.startswith("-") else "", order_field))
         orderby = updated_order
 
