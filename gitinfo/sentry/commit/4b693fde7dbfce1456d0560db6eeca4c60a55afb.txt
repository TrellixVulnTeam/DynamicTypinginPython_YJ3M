commit 4b693fde7dbfce1456d0560db6eeca4c60a55afb
Author: Jan Michael Auer <account@jauer.org>
Date:   Tue Nov 27 16:30:41 2018 +0100

    feat(chunk_upload): Indicate maximum file sizes (#10762)

diff --git a/src/sentry/api/endpoints/chunk.py b/src/sentry/api/endpoints/chunk.py
index 9985b71cd0..a380d23ed9 100644
--- a/src/sentry/api/endpoints/chunk.py
+++ b/src/sentry/api/endpoints/chunk.py
@@ -14,6 +14,7 @@ from sentry import options
 from sentry.models import FileBlob
 from sentry.api.bases.organization import (OrganizationEndpoint,
                                            OrganizationReleasePermission)
+from sentry.utils.files import get_max_file_size
 
 
 # The blob size must be a power of two
@@ -54,6 +55,7 @@ class ChunkUploadEndpoint(OrganizationEndpoint):
                 'url': endpoint,
                 'chunkSize': CHUNK_UPLOAD_BLOB_SIZE,
                 'chunksPerRequest': MAX_CHUNKS_PER_REQUEST,
+                'maxFileSize': get_max_file_size(organization),
                 'maxRequestSize': MAX_REQUEST_SIZE,
                 'concurrency': MAX_CONCURRENCY,
                 'hashAlgorithm': HASH_ALGORITHM,
diff --git a/src/sentry/features/__init__.py b/src/sentry/features/__init__.py
index d888b868cd..84dc69f1cf 100644
--- a/src/sentry/features/__init__.py
+++ b/src/sentry/features/__init__.py
@@ -75,6 +75,7 @@ default_manager.add('organizations:suggested-commits', OrganizationFeature)  # N
 default_manager.add('organizations:unreleased-changes', OrganizationFeature)  # NOQA
 default_manager.add('organizations:gitlab-integration', OrganizationFeature)  # NOQA
 default_manager.add('organizations:jira-server-integration', OrganizationFeature)  # NOQA
+default_manager.add('organizations:large-debug-files', OrganizationFeature)  # NOQA
 
 # Project scoped features
 default_manager.add('projects:similarity-view', ProjectFeature)  # NOQA
diff --git a/src/sentry/models/file.py b/src/sentry/models/file.py
index e67c57210b..ca8fe96181 100644
--- a/src/sentry/models/file.py
+++ b/src/sentry/models/file.py
@@ -40,6 +40,7 @@ UPLOAD_RETRY_TIME = 60  # 1min
 DEFAULT_BLOB_SIZE = 1024 * 1024  # one mb
 CHUNK_STATE_HEADER = '__state'
 MULTI_BLOB_UPLOAD_CONCURRENCY = 8
+MAX_FILE_SIZE = 2 ** 31  # 2GB is the maximum offset supported by fileblob
 
 
 def enum(**named_values):
diff --git a/src/sentry/options/defaults.py b/src/sentry/options/defaults.py
index 7931b8ffea..7be799bfed 100644
--- a/src/sentry/options/defaults.py
+++ b/src/sentry/options/defaults.py
@@ -37,6 +37,7 @@ register('system.root-api-key', flags=FLAG_PRIORITIZE_DISK)
 register('system.logging-format', default=LoggingFormat.HUMAN, flags=FLAG_NOSTORE)
 # This is used for the chunk upload endpoint
 register('system.upload-url-prefix', flags=FLAG_PRIORITIZE_DISK)
+register('system.maximum-file-size', default=2 ** 31, flags=FLAG_PRIORITIZE_DISK)
 
 # Redis
 register(
diff --git a/src/sentry/tasks/assemble.py b/src/sentry/tasks/assemble.py
index 48680574d9..0da92cede7 100644
--- a/src/sentry/tasks/assemble.py
+++ b/src/sentry/tasks/assemble.py
@@ -4,6 +4,7 @@ import os
 import logging
 
 from sentry.tasks.base import instrumented_task
+from sentry.utils.files import get_max_file_size
 from sentry.utils.sdk import configure_scope
 
 logger = logging.getLogger(__name__)
@@ -91,7 +92,15 @@ def assemble_file(project, name, checksum, chunks, file_type):
     # chunks need to build the file
     file_blobs = FileBlob.objects.filter(
         checksum__in=chunks
-    ).values_list('id', 'checksum')
+    ).values_list('id', 'checksum', 'size')
+
+    # Reject all files that exceed the maximum allowed size for this
+    # organization. This value cannot be
+    file_size = sum(x[2] for x in file_blobs)
+    if file_size > get_max_file_size(project.organization):
+        set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                            detail='File exceeds maximum size')
+        return
 
     # We need to make sure the blobs are in the order in which
     # we received them from the request.
diff --git a/src/sentry/utils/files.py b/src/sentry/utils/files.py
index 1252e0002b..aea894309e 100644
--- a/src/sentry/utils/files.py
+++ b/src/sentry/utils/files.py
@@ -9,6 +9,9 @@ from __future__ import absolute_import
 
 import zlib
 
+from sentry import features, options
+from sentry.models import MAX_FILE_SIZE
+
 
 def compress_file(fp, level=6):
     compressor = zlib.compressobj(level)
@@ -18,3 +21,11 @@ def compress_file(fp, level=6):
         chunks.append(chunk)
         z_chunks.append(compressor.compress(chunk))
     return (b''.join(z_chunks) + compressor.flush(), b''.join(chunks))
+
+
+def get_max_file_size(organization):
+    """Returns the maximum allowed debug file size for this organization."""
+    if features.has('organizations:large-debug-files', organization):
+        return MAX_FILE_SIZE
+    else:
+        return options.get('system.maximum-file-size')
diff --git a/tests/sentry/api/endpoints/test_chunk_upload.py b/tests/sentry/api/endpoints/test_chunk_upload.py
index 77b64ff23a..aaa1d40ce2 100644
--- a/tests/sentry/api/endpoints/test_chunk_upload.py
+++ b/tests/sentry/api/endpoints/test_chunk_upload.py
@@ -6,7 +6,7 @@ from django.core.urlresolvers import reverse
 from django.core.files.uploadedfile import SimpleUploadedFile
 
 from sentry import options
-from sentry.models import ApiToken, FileBlob
+from sentry.models import ApiToken, FileBlob, MAX_FILE_SIZE
 from sentry.testutils import APITestCase
 from sentry.api.endpoints.chunk import (MAX_CHUNKS_PER_REQUEST, MAX_CONCURRENCY,
                                         HASH_ALGORITHM, MAX_REQUEST_SIZE,
@@ -38,6 +38,7 @@ class ChunkUploadTest(APITestCase):
         assert response.data['chunkSize'] == CHUNK_UPLOAD_BLOB_SIZE
         assert response.data['chunksPerRequest'] == MAX_CHUNKS_PER_REQUEST
         assert response.data['maxRequestSize'] == MAX_REQUEST_SIZE
+        assert response.data['maxFileSize'] == options.get('system.maximum-file-size')
         assert response.data['concurrency'] == MAX_CONCURRENCY
         assert response.data['hashAlgorithm'] == HASH_ALGORITHM
         assert response.data['url'] == options.get('system.url-prefix') + self.url
@@ -51,6 +52,16 @@ class ChunkUploadTest(APITestCase):
 
         assert response.data['url'] == options.get('system.upload-url-prefix') + self.url
 
+    def test_large_uploads(self):
+        with self.feature('organizations:large-debug-files'):
+            response = self.client.get(
+                self.url,
+                HTTP_AUTHORIZATION=u'Bearer {}'.format(self.token.token),
+                format='json'
+            )
+
+        assert response.data['maxFileSize'] == MAX_FILE_SIZE
+
     def test_wrong_api_token(self):
         token = ApiToken.objects.create(
             user=self.user,
