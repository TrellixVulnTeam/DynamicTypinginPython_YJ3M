commit e5c6ed9f0637eadbe8fb1354375c6568be4f1a9c
Author: Tony <Zylphrex@users.noreply.github.com>
Date:   Mon Jun 8 18:14:26 2020 -0400

    feat(async-csv): Batched exports in smaller tasks (#19034)
    
    Batch the exports in to smaller tasks that exports 10,000 rows at a time. Currently, it
    continuously exports 10,000 rows in a single task until the export is complete. This is
    problematic due to the fact that deploys happen throughout the day and will kill the export task
    if they are in progress, forcing the export to start over from the beginning. This breaks down the
    export into separate tasks that will export 10,000 rows at a time then sequentially start another
    task to export the next batch. This will minimize the impact that deploys will have on exports as
    the task will only have to restart a single 10,000 row export.

diff --git a/migrations_lockfile.txt b/migrations_lockfile.txt
index 2a24d8f4df..c8c8bda898 100644
--- a/migrations_lockfile.txt
+++ b/migrations_lockfile.txt
@@ -10,7 +10,7 @@ auth: 0008_alter_user_username_max_length
 contenttypes: 0002_remove_content_type_name
 jira_ac: 0001_initial
 nodestore: 0001_initial
-sentry: 0083_add_max_length_webhook_url
+sentry: 0084_exported_data_blobs
 sessions: 0001_initial
 sites: 0002_alter_domain_unique
 social_auth: 0001_initial
diff --git a/src/sentry/data_export/base.py b/src/sentry/data_export/base.py
index 9664243181..61383d7cd3 100644
--- a/src/sentry/data_export/base.py
+++ b/src/sentry/data_export/base.py
@@ -4,6 +4,7 @@ import six
 from datetime import timedelta
 from enum import Enum
 
+EXPORTED_ROWS_LIMIT = 10000000
 SNUBA_MAX_RESULTS = 10000
 DEFAULT_EXPIRATION = timedelta(weeks=4)
 
diff --git a/src/sentry/data_export/models.py b/src/sentry/data_export/models.py
index 4c08818282..c416643cba 100644
--- a/src/sentry/data_export/models.py
+++ b/src/sentry/data_export/models.py
@@ -8,6 +8,7 @@ from django.db import models
 from django.utils import timezone
 
 from sentry.db.models import (
+    BoundedBigIntegerField,
     BoundedPositiveIntegerField,
     FlexibleForeignKey,
     JSONField,
@@ -128,3 +129,16 @@ class ExportedData(Model):
         db_table = "sentry_exporteddata"
 
     __repr__ = sane_repr("query_type", "query_info")
+
+
+class ExportedDataBlob(Model):
+    __core__ = False
+
+    data_export = FlexibleForeignKey("sentry.ExportedData")
+    blob = FlexibleForeignKey("sentry.FileBlob", db_constraint=False)
+    offset = BoundedBigIntegerField()
+
+    class Meta:
+        app_label = "sentry"
+        db_table = "sentry_exporteddatablob"
+        unique_together = (("data_export", "blob", "offset"),)
diff --git a/src/sentry/data_export/tasks.py b/src/sentry/data_export/tasks.py
index 9100f7f29b..33601ea9b6 100644
--- a/src/sentry/data_export/tasks.py
+++ b/src/sentry/data_export/tasks.py
@@ -4,15 +4,28 @@ import csv
 import logging
 import six
 import tempfile
+
+from hashlib import sha1
+
+from celery.task import current
+from celery.exceptions import MaxRetriesExceededError
+from django.core.files.base import ContentFile
 from django.db import transaction, IntegrityError
 
-from sentry.models import File
+from sentry.models import (
+    AssembleChecksumMismatch,
+    DEFAULT_BLOB_SIZE,
+    File,
+    FileBlob,
+    FileBlobIndex,
+    MAX_FILE_SIZE,
+)
 from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
 from sentry.utils.sdk import capture_exception
 
-from .base import ExportError, ExportQueryType, SNUBA_MAX_RESULTS
-from .models import ExportedData
+from .base import ExportError, ExportQueryType, EXPORTED_ROWS_LIMIT, SNUBA_MAX_RESULTS
+from .models import ExportedData, ExportedDataBlob
 from .utils import convert_to_utf8, handle_snuba_errors
 from .processors.discover import DiscoverProcessor
 from .processors.issues_by_tag import IssuesByTagProcessor
@@ -21,157 +34,214 @@ from .processors.issues_by_tag import IssuesByTagProcessor
 logger = logging.getLogger(__name__)
 
 
-@instrumented_task(name="sentry.data_export.tasks.assemble_download", queue="data_export")
+@instrumented_task(
+    name="sentry.data_export.tasks.assemble_download",
+    queue="data_export",
+    default_retry_delay=30,
+    max_retries=3,
+)
 def assemble_download(
     data_export_id,
-    export_limit=1000000,
+    export_limit=EXPORTED_ROWS_LIMIT,
     batch_size=SNUBA_MAX_RESULTS,
+    offset=0,
+    bytes_written=0,
     environment_id=None,
     **kwargs
 ):
-    # Get the ExportedData object
+    first_page = offset == 0
+
     try:
-        logger.info("dataexport.start", extra={"data_export_id": data_export_id})
-        metrics.incr("dataexport.start", tags={"success": True}, sample_rate=1.0)
+        if first_page:
+            logger.info("dataexport.start", extra={"data_export_id": data_export_id})
         data_export = ExportedData.objects.get(id=data_export_id)
+        if first_page:
+            metrics.incr("dataexport.start", tags={"success": True}, sample_rate=1.0)
+        logger.info("dataexport.run", extra={"data_export_id": data_export_id, "offset": offset})
     except ExportedData.DoesNotExist as error:
-        metrics.incr("dataexport.start", tags={"success": False}, sample_rate=1.0)
-        capture_exception(error)
+        if first_page:
+            metrics.incr("dataexport.start", tags={"success": False}, sample_rate=1.0)
+        logger.exception(error)
         return
 
-    # Create a temporary file
     try:
+        if export_limit is None:
+            export_limit = EXPORTED_ROWS_LIMIT
+        else:
+            export_limit = min(export_limit, EXPORTED_ROWS_LIMIT)
+
+        # if there is an export limit, the last batch should only return up to the export limit
+        if export_limit is not None:
+            batch_size = min(batch_size, max(export_limit - offset, 0))
+
+        # NOTE: the processors don't have an unified interface at the moment
+        # so this function handles it for us
+        headers, rows = get_processed(data_export, environment_id, batch_size, offset)
+
+        # starting position for the next batch
+        next_offset = offset + len(rows)
+
         with tempfile.TemporaryFile() as tf:
-            # Process the query based on its type
-            if data_export.query_type == ExportQueryType.ISSUES_BY_TAG:
-                process_issues_by_tag(
-                    data_export=data_export,
-                    file=tf,
-                    export_limit=export_limit,
-                    batch_size=batch_size,
-                    environment_id=environment_id,
-                )
-            elif data_export.query_type == ExportQueryType.DISCOVER:
-                process_discover(
-                    data_export=data_export,
-                    file=tf,
-                    export_limit=export_limit,
-                    batch_size=batch_size,
-                    environment_id=environment_id,
-                )
-            # Create a new File object and attach it to the ExportedData
+            writer = csv.DictWriter(tf, headers, extrasaction="ignore")
+            if first_page:
+                writer.writeheader()
+            writer.writerows(rows)
             tf.seek(0)
-            try:
-                with transaction.atomic():
-                    file = File.objects.create(
-                        name=data_export.file_name,
-                        type="export.csv",
-                        headers={"Content-Type": "text/csv"},
-                    )
-                    file.putfile(tf, logger=logger)
-                    data_export.finalize_upload(file=file)
-                    logger.info("dataexport.end", extra={"data_export_id": data_export_id})
-                    metrics.incr("dataexport.end", sample_rate=1.0)
-            except IntegrityError as error:
-                metrics.incr(
-                    "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
-                )
-                logger.info(
-                    "dataexport.error: {}".format(six.text_type(error)),
-                    extra={"query": data_export.payload, "org": data_export.organization_id},
-                )
-                capture_exception(error)
-                raise ExportError("Failed to save the assembled file")
+
+            new_bytes_written = store_export_chunk_as_blob(data_export, bytes_written, tf)
+            bytes_written += new_bytes_written
     except ExportError as error:
         return data_export.email_failure(message=six.text_type(error))
-    except BaseException as error:
+    except Exception as error:
         metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info(
-            "dataexport.error: {}".format(six.text_type(error)),
+        logger.error(
+            "dataexport.error: %s",
+            six.text_type(error),
             extra={"query": data_export.payload, "org": data_export.organization_id},
         )
         capture_exception(error)
-        return data_export.email_failure(message="Internal processing failure")
 
+        try:
+            current.retry()
+        except MaxRetriesExceededError:
+            return data_export.email_failure(message="Internal processing failure")
+    else:
+        if (
+            rows
+            and new_bytes_written
+            and len(rows) >= batch_size
+            and (export_limit is None or next_offset < export_limit)
+        ):
+            assemble_download.delay(
+                data_export_id,
+                export_limit=export_limit,
+                batch_size=batch_size,
+                offset=next_offset,
+                bytes_written=bytes_written,
+                environment_id=environment_id,
+            )
+        else:
+            merge_export_blobs.delay(data_export_id)
 
-@handle_snuba_errors(logger)
-def process_issues_by_tag(data_export, file, export_limit, batch_size, environment_id):
-    """
-    Convert the tag query to a CSV, writing it to the provided file.
-    """
-    payload = data_export.query_info
+
+def get_processed(data_export, environment_id, batch_size, offset):
     try:
-        processor = IssuesByTagProcessor(
-            project_id=payload["project"][0],
-            group_id=payload["group"],
-            key=payload["key"],
-            environment_id=environment_id,
-        )
+        if data_export.query_type == ExportQueryType.ISSUES_BY_TAG:
+            processor, processed = process_issues_by_tag(
+                data_export, environment_id, batch_size, offset
+            )
+
+        elif data_export.query_type == ExportQueryType.DISCOVER:
+            processor, processed = process_discover(data_export, environment_id, batch_size, offset)
+
+        return processor.header_fields, processed
     except ExportError as error:
         metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
         logger.info("dataexport.error: {}".format(six.text_type(error)))
         capture_exception(error)
-        raise error
-
-    writer = create_writer(file, processor.header_fields)
-    iteration = 0
-    is_completed = False
-    while not is_completed:
-        offset = batch_size * iteration
-        next_offset = batch_size * (iteration + 1)
-        is_exceeding_limit = export_limit and export_limit < next_offset
-        gtv_list_unicode = processor.get_serialized_data(limit=batch_size, offset=offset)
-        # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
-        # See associated comment in './utils.py'
-        gtv_list = convert_to_utf8(gtv_list_unicode)
-        if is_exceeding_limit:
-            # Since the next offset will pass the export_limit, just write the remainder
-            writer.writerows(gtv_list[: export_limit % batch_size])
-        else:
-            writer.writerows(gtv_list)
-            iteration += 1
-        # If there are no returned results, or we've passed the export_limit, stop iterating
-        is_completed = len(gtv_list) == 0 or is_exceeding_limit
+        raise
 
 
 @handle_snuba_errors(logger)
-def process_discover(data_export, file, export_limit, batch_size, environment_id):
-    """
-    Convert the discovery query to a CSV, writing it to the provided file.
-    """
-    try:
-        processor = DiscoverProcessor(
-            discover_query=data_export.query_info, organization_id=data_export.organization_id
+def process_issues_by_tag(data_export, environment_id, limit, offset):
+    payload = data_export.query_info
+    processor = IssuesByTagProcessor(
+        project_id=payload["project"][0],
+        group_id=payload["group"],
+        key=payload["key"],
+        environment_id=environment_id,
+    )
+    gtv_list_unicode = processor.get_serialized_data(limit=limit, offset=offset)
+    # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
+    # See associated comment in './utils.py'
+    gtv_list = convert_to_utf8(gtv_list_unicode)
+    return processor, gtv_list
+
+
+@handle_snuba_errors(logger)
+def process_discover(data_export, environment_id, limit, offset):
+    processor = DiscoverProcessor(
+        discover_query=data_export.query_info, organization_id=data_export.organization_id,
+    )
+    raw_data_unicode = processor.data_fn(limit=limit, offset=offset)["data"]
+    # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
+    # See associated comment in './utils.py'
+    raw_data = convert_to_utf8(raw_data_unicode)
+    raw_data = processor.handle_fields(raw_data)
+    return processor, raw_data
+
+
+@transaction.atomic()
+def store_export_chunk_as_blob(data_export, bytes_written, fileobj, blob_size=DEFAULT_BLOB_SIZE):
+    # adapted from `putfile` in  `src/sentry/models/file.py`
+    bytes_offset = 0
+    while True:
+        contents = fileobj.read(blob_size)
+        if not contents:
+            return bytes_offset
+
+        blob_fileobj = ContentFile(contents)
+        blob = FileBlob.from_file(blob_fileobj, logger=logger)
+        ExportedDataBlob.objects.create(
+            data_export=data_export, blob=blob, offset=bytes_written + bytes_offset
         )
-    except ExportError as error:
+
+        bytes_offset += blob.size
+
+        # there is a maximum file size allowed, so we need to make sure we don't exceed it
+        if bytes_written + bytes_offset >= MAX_FILE_SIZE:
+            transaction.set_rollback(True)
+            return 0
+
+
+@instrumented_task(name="sentry.data_export.tasks.merge_blobs", queue="data_export")
+def merge_export_blobs(data_export_id, **kwargs):
+    try:
+        data_export = ExportedData.objects.get(id=data_export_id)
+    except ExportedData.DoesNotExist as error:
+        logger.exception(error)
+        return
+
+    # adapted from `putfile` in  `src/sentry/models/file.py`
+    try:
+        with transaction.atomic():
+            file = File.objects.create(
+                name=data_export.file_name, type="export.csv", headers={"Content-Type": "text/csv"},
+            )
+            size = 0
+            file_checksum = sha1(b"")
+
+            for export_blob in ExportedDataBlob.objects.filter(data_export=data_export).order_by(
+                "offset"
+            ):
+                blob = export_blob.blob
+                FileBlobIndex.objects.create(file=file, blob=blob, offset=size)
+                size += blob.size
+                blob_checksum = sha1(b"")
+
+                for chunk in blob.getfile().chunks():
+                    blob_checksum.update(chunk)
+                    file_checksum.update(chunk)
+
+                if blob.checksum != blob_checksum.hexdigest():
+                    raise AssembleChecksumMismatch("Checksum mismatch")
+
+            file.size = size
+            file.checksum = file_checksum.hexdigest()
+            data_export.finalize_upload(file=file)
+
+            logger.info("dataexport.end", extra={"data_export_id": data_export_id})
+            metrics.incr("dataexport.end", sample_rate=1.0)
+    except Exception as error:
         metrics.incr("dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0)
-        logger.info("dataexport.error: {}".format(six.text_type(error)))
+        logger.error(
+            "dataexport.error: %s",
+            six.text_type(error),
+            extra={"query": data_export.payload, "org": data_export.organization_id},
+        )
         capture_exception(error)
-        raise error
-
-    writer = create_writer(file, processor.header_fields)
-    iteration = 0
-    is_completed = False
-    while not is_completed:
-        offset = batch_size * iteration
-        next_offset = batch_size * (iteration + 1)
-        is_exceeding_limit = export_limit and export_limit < next_offset
-        raw_data_unicode = processor.data_fn(offset=offset, limit=batch_size)["data"]
-        # TODO(python3): Remove next line once the 'csv' module has been updated to Python 3
-        # See associated comment in './utils.py'
-        raw_data = convert_to_utf8(raw_data_unicode)
-        raw_data = processor.handle_fields(raw_data)
-        if is_exceeding_limit:
-            # Since the next offset will pass the export_limit, just write the remainder
-            writer.writerows(raw_data[: export_limit % batch_size])
+        if isinstance(error, IntegrityError):
+            message = "Failed to save the assembled file."
         else:
-            writer.writerows(raw_data)
-            iteration += 1
-        # If there are no returned results, or we've passed the export_limit, stop iterating
-        is_completed = len(raw_data) == 0 or is_exceeding_limit
-
-
-def create_writer(file, fields):
-    writer = csv.DictWriter(file, fields, extrasaction="ignore")
-    writer.writeheader()
-    return writer
+            message = "Internal processing failure."
+        return data_export.email_failure(message=message)
diff --git a/src/sentry/data_export/utils.py b/src/sentry/data_export/utils.py
index d41933a149..e9ce1bdc66 100644
--- a/src/sentry/data_export/utils.py
+++ b/src/sentry/data_export/utils.py
@@ -21,28 +21,28 @@ def handle_snuba_errors(logger):
                 metrics.incr(
                     "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
                 )
-                logger.info("dataexport.error: %s", six.text_type(error))
+                logger.warn("dataexport.error: %s", six.text_type(error))
                 capture_exception(error)
                 raise ExportError("Invalid query. Please fix the query and try again.")
             except snuba.QueryOutsideRetentionError as error:
                 metrics.incr(
                     "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
                 )
-                logger.info("dataexport.error: %s", six.text_type(error))
+                logger.warn("dataexport.error: %s", six.text_type(error))
                 capture_exception(error)
                 raise ExportError("Invalid date range. Please try a more recent date range.")
             except snuba.QueryIllegalTypeOfArgument as error:
                 metrics.incr(
                     "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
                 )
-                logger.info("dataexport.error: %s", six.text_type(error))
+                logger.warn("dataexport.error: %s", six.text_type(error))
                 capture_exception(error)
                 raise ExportError("Invalid query. Argument to function is wrong type.")
             except snuba.SnubaError as error:
                 metrics.incr(
                     "dataexport.error", tags={"error": six.text_type(error)}, sample_rate=1.0
                 )
-                logger.info("dataexport.error: %s", six.text_type(error))
+                logger.warn("dataexport.error: %s", six.text_type(error))
                 capture_exception(error)
                 message = "Internal error. Please try again."
                 if isinstance(
diff --git a/src/sentry/migrations/0084_exported_data_blobs.py b/src/sentry/migrations/0084_exported_data_blobs.py
new file mode 100644
index 0000000000..d6b55574a0
--- /dev/null
+++ b/src/sentry/migrations/0084_exported_data_blobs.py
@@ -0,0 +1,51 @@
+# -*- coding: utf-8 -*-
+# Generated by Django 1.11.29 on 2020-06-03 01:15
+from __future__ import unicode_literals
+
+from django.db import migrations
+import django.db.models.deletion
+import sentry.db.models.fields.bounded
+import sentry.db.models.fields.foreignkey
+
+
+class Migration(migrations.Migration):
+    # This flag is used to mark that a migration shouldn't be automatically run in
+    # production. We set this to True for operations that we think are risky and want
+    # someone from ops to run manually and monitor.
+    # General advice is that if in doubt, mark your migration as `is_dangerous`.
+    # Some things you should always mark as dangerous:
+    # - Large data migrations. Typically we want these to be run manually by ops so that
+    #   they can be monitored. Since data migrations will now hold a transaction open
+    #   this is even more important.
+    # - Adding columns to highly active tables, even ones that are NULL.
+    is_dangerous = False
+
+    # This flag is used to decide whether to run this migration in a transaction or not.
+    # By default we prefer to run in a transaction, but for migrations where you want
+    # to `CREATE INDEX CONCURRENTLY` this needs to be set to False. Typically you'll
+    # want to create an index concurrently when adding one to an existing table.
+    atomic = True
+
+
+    dependencies = [
+        ('sentry', '0083_add_max_length_webhook_url'),
+    ]
+
+    operations = [
+        migrations.CreateModel(
+            name='ExportedDataBlob',
+            fields=[
+                ('id', sentry.db.models.fields.bounded.BoundedBigAutoField(primary_key=True, serialize=False)),
+                ('offset', sentry.db.models.fields.bounded.BoundedBigIntegerField()),
+                ('blob', sentry.db.models.fields.foreignkey.FlexibleForeignKey(db_constraint=False, on_delete=django.db.models.deletion.CASCADE, to='sentry.FileBlob')),
+                ('data_export', sentry.db.models.fields.foreignkey.FlexibleForeignKey(on_delete=django.db.models.deletion.CASCADE, to='sentry.ExportedData')),
+            ],
+            options={
+                'db_table': 'sentry_exporteddatablob',
+            },
+        ),
+        migrations.AlterUniqueTogether(
+            name='exporteddatablob',
+            unique_together=set([('data_export', 'blob', 'offset')]),
+        ),
+    ]
diff --git a/tests/sentry/data_export/test_tasks.py b/tests/sentry/data_export/test_tasks.py
index 88f1038d17..7213154e93 100644
--- a/tests/sentry/data_export/test_tasks.py
+++ b/tests/sentry/data_export/test_tasks.py
@@ -1,8 +1,9 @@
 from __future__ import absolute_import
 
+from django.db import IntegrityError
 from sentry.data_export.base import ExportQueryType
 from sentry.data_export.models import ExportedData
-from sentry.data_export.tasks import assemble_download
+from sentry.data_export.tasks import assemble_download, merge_export_blobs
 from sentry.models import File
 from sentry.snuba.discover import InvalidSearchQuery
 from sentry.testutils import TestCase, SnubaTestCase
@@ -55,7 +56,8 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
     def test_task_persistent_name(self):
         assert assemble_download.name == "sentry.data_export.tasks.assemble_download"
 
-    def test_issue_by_tag(self):
+    @patch("sentry.data_export.models.ExportedData.email_success")
+    def test_issue_by_tag_batched(self, emailer):
         de = ExportedData.objects.create(
             user=self.user,
             organization=self.org,
@@ -78,6 +80,21 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         assert raw1.startswith("bar,1,")
         assert raw2.startswith("bar2,2,")
 
+        assert emailer.called
+
+    @patch("sentry.data_export.models.ExportedData.email_failure")
+    def test_issue_by_tag_missing_key(self, emailer):
+        de = ExportedData.objects.create(
+            user=self.user,
+            organization=self.org,
+            query_type=ExportQueryType.ISSUES_BY_TAG,
+            query_info={"project": [self.project.id], "group": self.event.group_id, "key": "bar"},
+        )
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Requested key does not exist"
+
     @patch("sentry.data_export.models.ExportedData.email_failure")
     def test_issue_by_tag_missing_project(self, emailer):
         de = ExportedData.objects.create(
@@ -132,8 +149,8 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         header = de.file.getfile().read().strip()
         assert header == "value,times_seen,last_seen,first_seen"
 
-    @patch("sentry.data_export.models.ExportedData.email_failure")
-    def test_discover(self, emailer):
+    @patch("sentry.data_export.models.ExportedData.email_success")
+    def test_discover_batched(self, emailer):
         de = ExportedData.objects.create(
             user=self.user,
             organization=self.org,
@@ -141,7 +158,7 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
             query_info={"project": [self.project.id], "field": ["title"], "query": ""},
         )
         with self.tasks():
-            assemble_download(de.id)
+            assemble_download(de.id, batch_size=1)
         de = ExportedData.objects.get(id=de.id)
         assert de.date_finished is not None
         assert de.date_expired is not None
@@ -156,6 +173,74 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
         assert raw2.startswith("<unlabeled event>")
         assert raw3.startswith("<unlabeled event>")
 
+        assert emailer.called
+
+    @patch("sentry.data_export.models.ExportedData.email_failure")
+    def test_discover_missing_project(self, emailer):
+        de = ExportedData.objects.create(
+            user=self.user,
+            organization=self.org,
+            query_type=ExportQueryType.DISCOVER,
+            query_info={"project": [-1], "group": self.event.group_id, "key": "user"},
+        )
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Requested project does not exist"
+
+    @patch("sentry.data_export.tasks.MAX_FILE_SIZE", 55)
+    @patch("sentry.data_export.models.ExportedData.email_success")
+    def test_discover_export_file_too_large(self, emailer):
+        de = ExportedData.objects.create(
+            user=self.user,
+            organization=self.org,
+            query_type=ExportQueryType.DISCOVER,
+            query_info={"project": [self.project.id], "field": ["title"], "query": ""},
+        )
+        with self.tasks():
+            assemble_download(de.id, batch_size=1)
+        de = ExportedData.objects.get(id=de.id)
+        assert de.date_finished is not None
+        assert de.date_expired is not None
+        assert de.file is not None
+        assert isinstance(de.file, File)
+        assert de.file.headers == {"Content-Type": "text/csv"}
+        # Convert raw csv to list of line-strings
+        # capping MAX_FILE_SIZE forces the last batch to be dropped, leaving 2 rows
+        header, raw1, raw2 = de.file.getfile().read().strip().split("\r\n")
+        assert header == "title"
+
+        assert raw1.startswith("<unlabeled event>")
+        assert raw2.startswith("<unlabeled event>")
+
+        assert emailer.called
+
+    @patch("sentry.data_export.models.ExportedData.email_success")
+    def test_discover_export_too_many_rows(self, emailer):
+        de = ExportedData.objects.create(
+            user=self.user,
+            organization=self.org,
+            query_type=ExportQueryType.DISCOVER,
+            query_info={"project": [self.project.id], "field": ["title"], "query": ""},
+        )
+        with self.tasks():
+            assemble_download(de.id, export_limit=2)
+        de = ExportedData.objects.get(id=de.id)
+        assert de.date_finished is not None
+        assert de.date_expired is not None
+        assert de.file is not None
+        assert isinstance(de.file, File)
+        assert de.file.headers == {"Content-Type": "text/csv"}
+        # Convert raw csv to list of line-strings
+        # capping MAX_FILE_SIZE forces the last batch to be dropped, leaving 2 rows
+        header, raw1, raw2 = de.file.getfile().read().strip().split("\r\n")
+        assert header == "title"
+
+        assert raw1.startswith("<unlabeled event>")
+        assert raw2.startswith("<unlabeled event>")
+
+        assert emailer.called
+
     @patch("sentry.snuba.discover.raw_query")
     @patch("sentry.data_export.models.ExportedData.email_failure")
     def test_discover_outside_retention(self, emailer, mock_query):
@@ -286,3 +371,23 @@ class AssembleDownloadTest(TestCase, SnubaTestCase):
             assemble_download(de.id)
         error = emailer.call_args[1]["message"]
         assert error == "Internal error. Your query failed to run."
+
+    @patch("sentry.data_export.models.ExportedData.finalize_upload")
+    @patch("sentry.data_export.models.ExportedData.email_failure")
+    def test_discover_integrity_error(self, emailer, finalize_upload):
+        de = ExportedData.objects.create(
+            user=self.user,
+            organization=self.org,
+            query_type=ExportQueryType.DISCOVER,
+            query_info={"project": [self.project.id], "field": ["title"], "query": ""},
+        )
+        finalize_upload.side_effect = IntegrityError("test")
+        with self.tasks():
+            assemble_download(de.id)
+        error = emailer.call_args[1]["message"]
+        assert error == "Failed to save the assembled file."
+
+
+class MergeExportBlobsTest(TestCase, SnubaTestCase):
+    def test_task_persistent_name(self):
+        assert merge_export_blobs.name == "sentry.data_export.tasks.merge_blobs"
