commit 1a70156375c8f8ed4e039023553f4437654df7d7
Author: Brett Hoerner <brett@bretthoerner.com>
Date:   Thu Oct 19 11:38:56 2017 -0500

    perf: Start new processes for cleanup concurrency (#6362)

diff --git a/src/sentry/runner/__init__.py b/src/sentry/runner/__init__.py
index 17491ff425..a3c7499992 100755
--- a/src/sentry/runner/__init__.py
+++ b/src/sentry/runner/__init__.py
@@ -56,6 +56,7 @@ list(
         lambda cmd: cli.add_command(import_string(cmd)), (
             'sentry.runner.commands.backup.export', 'sentry.runner.commands.backup.import_',
             'sentry.runner.commands.cleanup.cleanup', 'sentry.runner.commands.config.config',
+            'sentry.runner.commands.cleanup.cleanup_chunk', 'sentry.runner.commands.config.config',
             'sentry.runner.commands.createuser.createuser',
             'sentry.runner.commands.devserver.devserver', 'sentry.runner.commands.django.django',
             'sentry.runner.commands.exec.exec_', 'sentry.runner.commands.files.files',
diff --git a/src/sentry/runner/commands/cleanup.py b/src/sentry/runner/commands/cleanup.py
index 947c96eb70..4be13df6ce 100644
--- a/src/sentry/runner/commands/cleanup.py
+++ b/src/sentry/runner/commands/cleanup.py
@@ -7,6 +7,7 @@ sentry.runner.commands.cleanup
 """
 from __future__ import absolute_import, print_function
 
+import six
 from datetime import timedelta
 from uuid import uuid4
 
@@ -33,6 +34,90 @@ def get_project(value):
         return None
 
 
+def chunker(seq, size):
+    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))
+
+
+@click.command()
+@click.option('--days', type=click.INT, required=True)
+@click.option('--project_id', type=click.INT, required=False)
+@click.option('--model', required=True)
+@click.option('--dtfield', required=True)
+@click.option('--order_by', required=True)
+@click.option('--num_shards', required=True)
+@click.option('--shard_ids', required=True)
+@configuration
+def cleanup_chunk(days, project_id, model, dtfield, order_by, num_shards, shard_ids):
+    import pickle
+    from threading import Thread
+
+    model = pickle.loads(model)
+    shard_ids = [int(s) for s in shard_ids.split(",")]
+
+    task = create_deletion_task(
+        days, project_id, model, dtfield, order_by)
+
+    click.echo("days: %s, project_id: %s, model: %s, dtfield: %s, order_by: %s, shard_ids:%s" %
+               (days, project_id, model, dtfield, order_by, shard_ids))
+
+    threads = []
+    for shard_id in shard_ids:
+        t = Thread(
+            target=(
+                lambda shard_id=shard_id: _chunk_until_complete(
+                    task, num_shards=num_shards, shard_id=shard_id)
+            )
+        )
+        t.start()
+        threads.append(t)
+
+    for t in threads:
+        t.join()
+
+
+def create_deletion_task(days, project_id, model, dtfield, order_by):
+    from sentry import models
+    from sentry import deletions
+    from sentry import similarity
+
+    query = {
+        '{}__lte'.format(dtfield): (timezone.now() - timedelta(days=days)),
+    }
+
+    if project_id:
+        if 'project' in model._meta.get_all_field_names():
+            query['project'] = project_id
+        else:
+            query['project_id'] = project_id
+
+    task = deletions.get(
+        model=model,
+        query=query,
+        order_by=order_by,
+        skip_models=[
+            # Handled by other parts of cleanup
+            models.Event,
+            models.EventMapping,
+            models.EventTag,
+            models.GroupEmailThread,
+            models.GroupRuleStatus,
+            models.GroupTagValue,
+            # Handled by TTL
+            similarity.features,
+        ],
+        transaction_id=uuid4().hex,
+    )
+
+    return task
+
+
+def _chunk_until_complete(task, num_shards=None, shard_id=None):
+    has_more = True
+    while has_more:
+        has_more = task.chunk(
+            num_shards=num_shards, shard_id=shard_id)
+
+
 @click.command()
 @click.option('--days', default=30, show_default=True, help='Numbers of days to truncate on.')
 @click.option('--project', help='Limit truncation to only entries from project.')
@@ -41,7 +126,14 @@ def get_project(value):
     type=int,
     default=1,
     show_default=True,
-    help='The number of concurrent workers to run.'
+    help='The total number of concurrent threads to run across processes.'
+)
+@click.option(
+    '--max_procs',
+    type=int,
+    default=8,
+    show_default=True,
+    help='The maximum number of processes to fork off for concurrency.'
 )
 @click.option(
     '--silent', '-q', default=False, is_flag=True, help='Run quietly. No output on success.'
@@ -57,7 +149,7 @@ def get_project(value):
 )
 @log_options()
 @configuration
-def cleanup(days, project, concurrency, silent, model, router, timed):
+def cleanup(days, project, concurrency, max_procs, silent, model, router, timed):
     """Delete a portion of trailing data based on creation date.
 
     All data that is older than `--days` will be deleted.  The default for
@@ -70,13 +162,15 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
         click.echo('Error: Minimum concurrency is 1', err=True)
         raise click.Abort()
 
-    from threading import Thread
+    import math
+    import multiprocessing
+    import pickle
+    import subprocess
+    import sys
     from django.db import router as db_router
     from sentry.app import nodestore
     from sentry.db.deletion import BulkDeleteQuery
-    from sentry import deletions
     from sentry import models
-    from sentry import similarity
 
     if timed:
         import time
@@ -185,56 +279,43 @@ def cleanup(days, project, concurrency, silent, model, router, timed):
             if not silent:
                 click.echo('>> Skipping %s' % model.__name__)
         else:
-            query = {
-                '{}__lte'.format(dtfield): (timezone.now() - timedelta(days=days)),
-            }
-
-            if project_id:
-                if 'project' in model._meta.get_all_field_names():
-                    query['project'] = project_id
-                else:
-                    query['project_id'] = project_id
-
-            task = deletions.get(
-                model=model,
-                query=query,
-                order_by=order_by,
-                skip_models=[
-                    # Handled by other parts of cleanup
-                    models.Event,
-                    models.EventMapping,
-                    models.EventTag,
-                    models.GroupEmailThread,
-                    models.GroupRuleStatus,
-                    models.GroupTagValue,
-                    # Handled by TTL
-                    similarity.features,
-                ],
-                transaction_id=uuid4().hex,
-            )
+            if concurrency > 1:
+                shard_ids = range(concurrency)
+                num_procs = min(concurrency, max_procs)
+                threads_per_proc = int(math.ceil(
+                    num_procs / float(multiprocessing.cpu_count())))
+
+                pids = []
+                for shard_id_chunk in chunker(shard_ids, threads_per_proc):
+                    pid = subprocess.Popen([
+                        sys.argv[0],
+                        'cleanup_chunk',
+                        '--days', six.binary_type(days),
+                    ] + (['--project_id', six.binary_type(project_id)] if project_id else []) + [
+                        '--model', pickle.dumps(model),
+                        '--dtfield', dtfield,
+                        '--order_by', order_by,
+                        '--num_shards', six.binary_type(concurrency),
+                        '--shard_ids', ",".join([six.binary_type(s)
+                                                 for s in shard_id_chunk]),
+                    ])
+                    pids.append(pid)
+
+                total_pid_count = len(pids)
+                click.echo(
+                    "%s concurrent processes forked, waiting on them to complete." % total_pid_count)
 
-            def _chunk_until_complete(num_shards=None, shard_id=None):
-                has_more = True
-                while has_more:
-                    has_more = task.chunk(
-                        num_shards=num_shards, shard_id=shard_id)
+                complete = 0
+                for pid in pids:
+                    pid.wait()
+                    complete += 1
+                    click.echo(
+                        "%s/%s concurrent processes are finished." % (complete, total_pid_count))
 
-            if concurrency > 1:
-                threads = []
-                for shard_id in range(concurrency):
-                    t = Thread(
-                        target=(
-                            lambda shard_id=shard_id: _chunk_until_complete(
-                                num_shards=concurrency, shard_id=shard_id)
-                        )
-                    )
-                    t.start()
-                    threads.append(t)
-
-                for t in threads:
-                    t.join()
             else:
-                _chunk_until_complete()
+                task = create_deletion_task(
+                    days, project_id, model, dtfield, order_by)
+                _chunk_until_complete(task)
 
     # EventMapping is fairly expensive and is special cased as it's likely you
     # won't need a reference to an event for nearly as long
