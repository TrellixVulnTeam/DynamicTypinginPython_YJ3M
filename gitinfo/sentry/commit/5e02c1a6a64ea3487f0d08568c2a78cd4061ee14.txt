commit 5e02c1a6a64ea3487f0d08568c2a78cd4061ee14
Author: Ted Kaemming <ted@kaemming.com>
Date:   Wed Oct 7 15:25:15 2015 -0700

    Allow querying for series data.

diff --git a/example.py b/example.py
index 9157b637fd..1ce1da9fe1 100644
--- a/example.py
+++ b/example.py
@@ -4,28 +4,37 @@ from sentry.utils.runner import configure
 
 configure()
 
+import random
+import pprint
 from datetime import timedelta
 
 from django.utils import timezone
 
 from sentry.app import tsdb
 
+keys = range(10)
 
-items = (1, 2, 3)
-tsdb.record_multi((
-    (tsdb.models.users_affected_by_event, 1, items),
-))
+tsdb.record_multi([
+    (tsdb.models.users_affected_by_event, k, [random.randint(0, 1e6) for _ in xrange(random.randint(1, 50))]) for k in keys
+])
 
 end = timezone.now()
-start = end - timedelta(minutes=5)
+start = end - timedelta(seconds=60)
 
-interval, results = tsdb.get_distinct_counts(
+totals = tsdb.get_distinct_counts_totals(
     tsdb.models.users_affected_by_event,
-    (0, 1,),
+    keys,
     start,
     end,
 )
 
-print interval, results
-print 'Extra time included prior to start position:', timedelta(seconds=int(start.strftime('%s')) - interval[0])
-print 'Extra time included after end position:', timedelta(seconds=interval[1] - int(end.strftime('%s')))
+pprint.pprint(totals)
+
+series = tsdb.get_distinct_counts_series(
+    tsdb.models.users_affected_by_event,
+    keys,
+    start,
+    end,
+)
+
+pprint.pprint(series)
diff --git a/src/sentry/tsdb/redis.py b/src/sentry/tsdb/redis.py
index 8cefa83f1c..7bd6ea4016 100644
--- a/src/sentry/tsdb/redis.py
+++ b/src/sentry/tsdb/redis.py
@@ -196,19 +196,44 @@ class RedisTSDB(BaseTSDB):
         # TODO: Check to make sure these operations didn't fail, so we can
         # raise an error if there were issues.
 
-    def get_distinct_counts(self, model, keys, start, end):
-        """
-        Count distinct items during a time range.
-        """
+    def _get_intervals(self, start, end, rollup=None):
         # NOTE: "optimal" here means "able to most closely reflect the upper
         # and lower bounds", not "able to construct the most efficient query"
-        rollup = self.get_optimal_rollup(start, end)
+        if rollup is None:
+            rollup = self.get_optimal_rollup(start, end)
 
         intervals = [self.normalize_to_epoch(start, rollup)]
         end_ts = int(end.strftime('%s'))  # XXX: HACK
         while intervals[-1] + rollup < end_ts:
             intervals.append(intervals[-1] + rollup)
 
+        return rollup, intervals
+
+    def get_distinct_counts_series(self, model, keys, start, end, rollup=None):
+        rollup, intervals = self._get_intervals(start, end, rollup)
+
+        def get_key(key, timestamp):
+            return self.make_key(
+                model,
+                self.normalize_ts_to_rollup(timestamp, rollup),
+                self.get_model_key(key),
+            )
+
+        responses = {}
+        with self.cluster.fanout() as client:
+            for key in keys:
+                make_key = functools.partial(get_key, key)
+                c = client.target_key(key)
+                responses[key] = [(timestamp, c.pfcount(make_key(timestamp))) for timestamp in intervals]
+
+        return {k: [(t, p.value) for t, p in v] for k, v in responses.iteritems()}
+
+    def get_distinct_counts_totals(self, model, keys, start, end, rollup=None):
+        """
+        Count distinct items during a time range.
+        """
+        rollup, intervals = self._get_intervals(start, end, rollup)
+
         def get_key(key, timestamp):
             return self.make_key(
                 model,
@@ -225,6 +250,4 @@ class RedisTSDB(BaseTSDB):
                     *map(make_key, intervals)
                 )
 
-        upper = intervals[0]
-        lower = intervals[-1] + rollup
-        return (upper, lower), {k: v.value for k, v in responses.iteritems()}
+        return {k: v.value for k, v in responses.iteritems()}
