commit 16f5685411ff59119f95e87c2d242a5b115e5132
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Fri Feb 16 12:41:33 2018 +0100

    feat: Added split assemble for difs (#7270)
    
    * feat: Added split assemble for difs
    
    * ref(dif): Move cache based state management around
    
    * fix: Stuff
    
    * fix: Added owner checks and set content type correctly
    
    * fix: More fixes
    
    * ref: Use default_cache instead of cache
    
    * feat: Stuff
    
    * fix: Invalid import
    
    * fix: Generate correct cache key
    
    * ref(dif): Assemble symcaches in assemble directly
    
    * fix(dif): Invoke symcache generation correclty
    
    * fix(dif): Fix checks for DIF assembly
    
    * fix(dif): Fix file name and chunk validation in assembly
    
    * test(dif): Fix assemble tests
    
    * test(dif): Fix assemble tests
    
    * fix(dif): Fix legacy symbol upload
    
    * fix(dif): Fix chunk ownership check

diff --git a/src/sentry/api/bases/chunk.py b/src/sentry/api/bases/chunk.py
deleted file mode 100644
index 42967266d4..0000000000
--- a/src/sentry/api/bases/chunk.py
+++ /dev/null
@@ -1,113 +0,0 @@
-from __future__ import absolute_import
-
-from sentry.models import File, FileBlob, FileBlobOwner
-from sentry.models.file import ChunkFileState, CHUNK_STATE_HEADER
-
-
-class ChunkAssembleMixin(object):
-    def _create_file_response(self, state, missing_chunks=[]):
-        """
-        Helper function to create response for assemble endpoint
-        """
-        return {
-            'state': state,
-            'missingChunks': missing_chunks
-        }
-
-    def _check_chunk_ownership(self, organization, file_blobs, chunks, file=None):
-        # Check the ownership of these blobs with the org
-        all_owned_blobs = FileBlobOwner.objects.filter(
-            blob__in=file_blobs,
-            organization=organization
-        ).prefetch_related('blob').all()
-
-        owned_blobs = []
-        for owned_blob in all_owned_blobs:
-            owned_blobs.append((owned_blob.blob.id, owned_blob.blob.checksum))
-
-        # If the request does not contain any chunks for a file
-        # we return nothing since this should never happen only
-        # if the client sends an invalid request
-        if len(chunks) == 0:
-            return self._create_file_response(
-                ChunkFileState.NOT_FOUND
-            )
-        # Only if this org already has the ownership of all blobs
-        # and the count of chunks is the same as in the request
-        # and the file already exists, we say this file is OK
-        elif len(file_blobs) == len(owned_blobs) == len(chunks) and file is not None:
-            return self._create_file_response(
-                file.headers.get(CHUNK_STATE_HEADER, ChunkFileState.OK)
-            )
-        # If the length of owned and sent chunks is not the same
-        # we return all missing blobs
-        elif len(owned_blobs) != len(chunks):
-            # Create a missing chunks array which we return as response
-            # so the client knows which chunks to reupload
-            missing_chunks = set(chunks)
-            for blob in owned_blobs:
-                if blob[1] in missing_chunks:
-                    missing_chunks.discard(blob[1])
-            # If we have any missing chunks at all, return it to the client
-            # that we need them to assemble the file
-            if len(missing_chunks) > 0:
-                return self._create_file_response(
-                    ChunkFileState.NOT_FOUND,
-                    missing_chunks
-                )
-
-    def _check_file_blobs(self, organization, checksum, chunks):
-        files = File.objects.filter(
-            checksum=checksum
-        ).prefetch_related('blobs').all()
-        # If there is no file at all, we try to find chunks in the db and check
-        # their ownership
-        if len(files) == 0:
-            file_blobs = FileBlob.objects.filter(
-                checksum__in=chunks
-            ).all()
-            return (
-                None,
-                self._check_chunk_ownership(organization, file_blobs, chunks)
-            )
-        # It is possible to have multiple identical files in the DB for every
-        # architecture inside a FatObject. We can safely assume here that if
-        # there are multiple matching files, their blobs will be the same and
-        # we only have to check ownership for one representative.
-        files = list(files)
-        file = files[0]
-
-        # We need to fetch all blobs
-        file_blobs = file.blobs.all()
-        rv = self._check_chunk_ownership(organization, file_blobs, chunks, file)
-        if rv is not None:
-            return (files, rv)
-
-        return (None, None)
-
-    def _create_file_for_assembling(self, name, checksum, chunks):
-        # If we have all chunks and the file wasn't found before
-        # we create a new file here with the state CREATED
-        # Note that this file only exsists while the assemble tasks run
-        file = File.objects.create(
-            name=name,
-            checksum=checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
-
-        # Load all FileBlobs from db since we can be sure here we already own all
-        # chunks need to build the file
-        file_blobs = FileBlob.objects.filter(
-            checksum__in=chunks
-        ).values_list('id', 'checksum')
-
-        # We need to make sure the blobs are in the order in which
-        # we received them from the request.
-        # Otherwise it could happen that we assemble the file in the wrong order
-        # and get an garbage file.
-        file_blob_ids = [x[0] for x in sorted(
-            file_blobs, key=lambda blob: chunks.index(blob[1])
-        )]
-
-        return (file, file_blob_ids)
diff --git a/src/sentry/api/endpoints/dif_files.py b/src/sentry/api/endpoints/dif_files.py
index b7e43def02..940198f848 100644
--- a/src/sentry/api/endpoints/dif_files.py
+++ b/src/sentry/api/endpoints/dif_files.py
@@ -7,29 +7,22 @@ from rest_framework.response import Response
 
 from sentry.utils import json
 from sentry.api.serializers import serialize
-from sentry.api.bases.chunk import ChunkAssembleMixin
 from sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission
-from sentry.models import File, ChunkFileState, ProjectDSymFile
+from sentry.models import ChunkFileState, ProjectDSymFile, FileBlobOwner, \
+    get_assemble_status, set_assemble_status
 
 
-class DifAssembleEndpoint(ChunkAssembleMixin, ProjectEndpoint):
-    permission_classes = (ProjectReleasePermission, )
-
-    def _add_project_dsym_to_reponse(self, found_files, response):
-        for found_file in found_files or []:
-            error = found_file.headers.get('error', None)
-            if error is not None:
-                response.setdefault('errors', []).append(error)
-                response['state'] = ChunkFileState.ERROR
-
-            dsym = ProjectDSymFile.objects.filter(
-                file=found_file
-            ).first()
+def find_missing_chunks(organization, chunks):
+    """Returns a list of chunks which are missing for an org."""
+    owned = set(FileBlobOwner.objects.filter(
+        blob__checksum__in=chunks,
+        organization=organization,
+    ).values_list('blob__checksum', flat=True))
+    return list(set(chunks) - owned)
 
-            if dsym is not None:
-                response.setdefault('difs', []).append(serialize(dsym))
 
-        return response
+class DifAssembleEndpoint(ProjectEndpoint):
+    permission_classes = (ProjectReleasePermission, )
 
     def post(self, request, project):
         """
@@ -74,34 +67,65 @@ class DifAssembleEndpoint(ChunkAssembleMixin, ProjectEndpoint):
             name = file_to_assemble.get('name', None)
             chunks = file_to_assemble.get('chunks', [])
 
+            # First, check if this project already owns the DSymFile
             try:
-                found_files, response = self._check_file_blobs(
-                    project.organization, checksum, chunks)
-                # This either returns a file OK because we already own all chunks
-                # OR we return not_found with the missing chunks (or not owned)
-                if response is not None:
-                    # We also found a file, we try to fetch project dsym to return more
-                    # information in the request
-                    file_response[checksum] = self._add_project_dsym_to_reponse(
-                        found_files, response)
+                dif = ProjectDSymFile.objects.filter(
+                    project=project,
+                    file__checksum=checksum
+                ).get()
+            except ProjectDSymFile.DoesNotExist:
+                # It does not exist yet.  Check the state we have in cache
+                # in case this is a retry poll.
+                state, detail = get_assemble_status(project, checksum)
+                if state is not None:
+                    file_response[checksum] = {
+                        'state': state,
+                        'detail': detail,
+                        'missingChunks': [],
+                    }
                     continue
-            except File.DoesNotExist:
-                pass
-
-            file, file_blob_ids = self._create_file_for_assembling(name, checksum, chunks)
-
-            # Start the actual worker which does the assembling.
-            assemble_dif.apply_async(
-                kwargs={
-                    'project_id': project.id,
-                    'file_id': file.id,
-                    'file_blob_ids': file_blob_ids,
-                    'checksum': checksum,
-                }
-            )
 
-            file_response[checksum] = self._create_file_response(
-                ChunkFileState.CREATED
-            )
+                # There is neither a known file nor a cached state, so we will
+                # have to create a new file.  Assure that there are checksums.
+                # If not, we assume this is a poll and report NOT_FOUND
+                if not chunks:
+                    file_response[checksum] = {
+                        'state': ChunkFileState.NOT_FOUND,
+                        'missingChunks': [],
+                    }
+                    continue
+
+                # Check if all requested chunks have been uploaded.
+                missing_chunks = find_missing_chunks(project.organization, chunks)
+                if missing_chunks:
+                    file_response[checksum] = {
+                        'state': ChunkFileState.NOT_FOUND,
+                        'missingChunks': missing_chunks,
+                    }
+                    continue
+
+                # We don't have a state yet, this means we can now start
+                # an assemble job in the background.
+                set_assemble_status(project, checksum, state)
+                assemble_dif.apply_async(
+                    kwargs={
+                        'project_id': project.id,
+                        'name': name,
+                        'checksum': checksum,
+                        'chunks': chunks,
+                    }
+                )
+
+                file_response[checksum] = {
+                    'state': ChunkFileState.CREATED,
+                    'missingChunks': [],
+                }
+            else:
+                file_response[checksum] = {
+                    'state': ChunkFileState.OK,
+                    'detail': None,
+                    'missingChunks': [],
+                    'dif': serialize(dif),
+                }
 
         return Response(file_response, status=200)
diff --git a/src/sentry/models/dsymfile.py b/src/sentry/models/dsymfile.py
index c84ecc4d33..18a2c4f45c 100644
--- a/src/sentry/models/dsymfile.py
+++ b/src/sentry/models/dsymfile.py
@@ -29,7 +29,7 @@ from symbolic import FatObject, SymbolicError, UnsupportedObjectFile, \
     SymCache, SYMCACHE_LATEST_VERSION
 
 from sentry import options
-from sentry.utils.cache import cache
+from sentry.cache import default_cache
 from sentry.db.models import FlexibleForeignKey, Model, \
     sane_repr, BaseManager, BoundedPositiveIntegerField
 from sentry.models.file import File
@@ -54,6 +54,37 @@ DSYM_MIMETYPES = dict((v, k) for k, v in KNOWN_DSYM_TYPES.items())
 _proguard_file_re = re.compile(r'/proguard/(?:mapping-)?(.*?)\.txt$')
 
 
+def _get_idempotency_id(project, checksum):
+    """For some operations an idempotency ID is needed."""
+    return hashlib.sha1(b'%s|%s|project.dsym' % (
+        str(project.id).encode('ascii'),
+        checksum.encode('ascii'),
+    )).hexdigest()
+
+
+def get_assemble_status(project, checksum):
+    """For a given file it checks what the current status of the assembling is.
+    Returns a tuple in the form ``(status, details)`` where details is either
+    `None` or a string identifying an error condition or notice.
+    """
+    cache_key = 'assemble-status:%s' % _get_idempotency_id(
+        project, checksum)
+    rv = default_cache.get(cache_key)
+    if rv is None:
+        return None, None
+    return tuple(rv)
+
+
+def set_assemble_status(project, checksum, state, detail=None):
+    cache_key = 'assemble-status:%s' % _get_idempotency_id(
+        project, checksum)
+    default_cache.set(cache_key, (state, detail), 300)
+
+
+class BadDif(Exception):
+    pass
+
+
 class VersionDSymFile(Model):
     __core__ = False
 
@@ -195,7 +226,7 @@ class ProjectDSymFile(Model):
 
     @property
     def dsym_type(self):
-        ct = self.file.headers.get('Content-Type').lower()
+        ct = self.file.headers.get('Content-Type', 'unknown').lower()
         return KNOWN_DSYM_TYPES.get(ct, 'unknown')
 
     @property
@@ -228,7 +259,8 @@ class ProjectSymCacheFile(Model):
         self.cache_file.delete()
 
 
-def _create_dsym_from_uuid(project, dsym_type, cpu_name, uuid, fileobj, basename):
+def create_dsym_from_uuid(project, dsym_type, cpu_name, uuid,
+                          basename, fileobj=None, file=None):
     """This creates a mach dsym file or proguard mapping from the given
     uuid and open file object to a dsym file.  This will not verify the
     uuid (intentionally so).  Use `create_files_from_dsym_zip` for doing
@@ -243,44 +275,70 @@ def _create_dsym_from_uuid(project, dsym_type, cpu_name, uuid, fileobj, basename
     else:
         raise TypeError('unknown dsym type %r' % (dsym_type, ))
 
-    h = hashlib.sha1()
-    while 1:
-        chunk = fileobj.read(16384)
-        if not chunk:
-            break
-        h.update(chunk)
-    checksum = h.hexdigest()
-    fileobj.seek(0, 0)
+    if file is None:
+        assert fileobj is not None, 'missing file object'
+        h = hashlib.sha1()
+        while 1:
+            chunk = fileobj.read(16384)
+            if not chunk:
+                break
+            h.update(chunk)
+        checksum = h.hexdigest()
+        fileobj.seek(0, 0)
 
-    try:
-        rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
-        if rv.file.checksum == checksum:
-            return rv, False
-    except ProjectDSymFile.DoesNotExist:
-        pass
+        try:
+            rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
+            if rv.file.checksum == checksum:
+                return rv, False
+        except ProjectDSymFile.DoesNotExist:
+            pass
+        else:
+            # The checksum mismatches.  In this case we delete the old object
+            # and perform a re-upload.
+            rv.delete()
+
+        file = File.objects.create(
+            name=uuid,
+            type='project.dsym',
+            headers={'Content-Type': DSYM_MIMETYPES[dsym_type]},
+        )
+        file.putfile(fileobj)
+        try:
+            with transaction.atomic():
+                rv = ProjectDSymFile.objects.create(
+                    file=file,
+                    uuid=uuid,
+                    cpu_name=cpu_name,
+                    object_name=object_name,
+                    project=project,
+                )
+        except IntegrityError:
+            file.delete()
+            rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
     else:
-        # The checksum mismatches.  In this case we delete the old object
-        # and perform a re-upload.
-        rv.delete()
-
-    file = File.objects.create(
-        name=uuid,
-        type='project.dsym',
-        headers={'Content-Type': DSYM_MIMETYPES[dsym_type]},
-    )
-    file.putfile(fileobj)
-    try:
-        with transaction.atomic():
-            rv = ProjectDSymFile.objects.create(
-                file=file,
-                uuid=uuid,
-                cpu_name=cpu_name,
-                object_name=object_name,
-                project=project,
-            )
-    except IntegrityError:
-        file.delete()
-        rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
+        try:
+            rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
+        except ProjectDSymFile.DoesNotExist:
+            try:
+                with transaction.atomic():
+                    rv = ProjectDSymFile.objects.create(
+                        file=file,
+                        uuid=uuid,
+                        cpu_name=cpu_name,
+                        object_name=object_name,
+                        project=project,
+                    )
+            except IntegrityError:
+                rv = ProjectDSymFile.objects.get(uuid=uuid, project=project)
+                rv.file.delete()
+                rv.file = file
+                rv.save()
+        else:
+            rv.file.delete()
+            rv.file = file
+            rv.save()
+        rv.file.headers['Content-Type'] = DSYM_MIMETYPES[dsym_type]
+        rv.file.save()
 
     resolve_processing_issue(
         project=project,
@@ -304,31 +362,29 @@ def _analyze_progard_filename(filename):
         pass
 
 
-def detect_dif_from_filename(filename):
-    """This detects which kind of dif (Debug Information File) the filename
+def detect_dif_from_path(path):
+    """This detects which kind of dif (Debug Information File) the path
     provided is. It returns an array since a FatObject can contain more than
     on dif.
     """
     # proguard files (proguard/UUID.txt) or
     # (proguard/mapping-UUID.txt).
-    proguard_uuid = _analyze_progard_filename(filename)
+    proguard_uuid = _analyze_progard_filename(path)
     if proguard_uuid is not None:
-        return [('proguard', 'any', six.text_type(proguard_uuid), filename)]
+        return [('proguard', 'any', six.text_type(proguard_uuid), path)]
 
     # macho style debug symbols
     try:
-        fo = FatObject.from_path(filename)
-    except UnsupportedObjectFile:
-        pass
-    except SymbolicError:
-        # Whatever was contained there, was probably not a
-        # macho file.
-        # XXX: log?
+        fo = FatObject.from_path(path)
+    except UnsupportedObjectFile as e:
+        raise BadDif("Unsupported debug information file: %s" % e)
+    except SymbolicError as e:
         logger.warning('dsymfile.bad-fat-object', exc_info=True)
+        raise BadDif("Invalid debug information file: %s" % e)
     else:
         objs = []
         for obj in fo.iter_objects():
-            objs.append((obj.kind, obj.arch, six.text_type(obj.uuid), filename))
+            objs.append((obj.kind, obj.arch, six.text_type(obj.uuid), path))
         return objs
 
 
@@ -342,8 +398,9 @@ def create_dsym_from_dif(to_create, project, overwrite_filename=None):
             result_filename = os.path.basename(filename)
             if overwrite_filename is not None:
                 result_filename = overwrite_filename
-            dsym, created = _create_dsym_from_uuid(
-                project, dsym_type, cpu, file_uuid, f, result_filename
+            dsym, created = create_dsym_from_uuid(
+                project, dsym_type, cpu, file_uuid, result_filename,
+                fileobj=f
             )
             if created:
                 rv.append(dsym)
@@ -363,7 +420,11 @@ def create_files_from_dsym_zip(fileobj, project,
         for dirpath, dirnames, filenames in os.walk(scratchpad):
             for fn in filenames:
                 fn = os.path.join(dirpath, fn)
-                difs = detect_dif_from_filename(fn)
+                try:
+                    difs = detect_dif_from_path(fn)
+                except BadDif:
+                    difs = None
+
                 if difs is None:
                     difs = []
                 to_create = to_create + difs
@@ -424,6 +485,14 @@ class DSymCache(object):
                                    for k, v in conversion_errors.items())
         return symcaches
 
+    def get_symcache(self, project, uuid, with_conversion_errors=False):
+        """Return a single symcache."""
+        symcaches, errors = self.get_symcaches(
+            project, [uuid], with_conversion_errors=True)
+        if with_conversion_errors:
+            return symcaches.get(uuid), errors.get(uuid)
+        return symcaches.get(uuid)
+
     def fetch_dsyms(self, project, uuids):
         """Given some uuids returns a uuid to path mapping for where the
         debug symbol files are on the FS.
@@ -506,7 +575,7 @@ class DSymCache(object):
         conversion_errors = {}
         for dsym_file in dsym_files:
             cache_key = 'scbe:%s:%s' % (dsym_file.uuid, dsym_file.file.checksum)
-            err = cache.get(cache_key)
+            err = default_cache.get(cache_key)
             if err is not None:
                 conversion_errors[dsym_file.uuid] = err
 
@@ -523,7 +592,7 @@ class DSymCache(object):
                         continue
                     symcache = o.make_symcache()
             except SymbolicError as e:
-                cache.set('scbe:%s:%s' % (
+                default_cache.set('scbe:%s:%s' % (
                     dsym_uuid, dsym_file.file.checksum), e.message,
                     CONVERSION_ERROR_TTL)
                 conversion_errors[dsym_uuid] = e.message
diff --git a/src/sentry/models/file.py b/src/sentry/models/file.py
index 818f57d5ff..e235039b9a 100644
--- a/src/sentry/models/file.py
+++ b/src/sentry/models/file.py
@@ -21,7 +21,7 @@ from django.conf import settings
 from django.core.files.base import File as FileObj
 from django.core.files.base import ContentFile
 from django.core.files.storage import get_storage_class
-from django.db import models
+from django.db import models, transaction
 from django.utils import timezone
 from jsonfield import JSONField
 
@@ -49,6 +49,10 @@ ChunkFileState = enum(
 )
 
 
+class AssembleChecksumMismatch(Exception):
+    pass
+
+
 def get_storage():
     from sentry import options
     backend = options.get('filestore.backend')
@@ -272,28 +276,28 @@ class File(Model):
         """
         This creates a file, from file blobs
         """
-        file_blobs = FileBlob.objects.filter(id__in=file_blob_ids).all()
-        # Make sure the blobs are sorted with the order provided
-        file_blobs = sorted(file_blobs, key=lambda blob: file_blob_ids.index(blob.id))
-
-        new_checksum = sha1(b'')
-        offset = 0
-        for blob in file_blobs:
-            FileBlobIndex.objects.create(
-                file=self,
-                blob=blob,
-                offset=offset,
-            )
-            for chunk in blob.getfile().chunks():
-                new_checksum.update(chunk)
-            offset += blob.size
-
-        self.size = offset
-        self.checksum = new_checksum.hexdigest()
-
-        if checksum != self.checksum:
-            self.headers['__state'] = ChunkFileState.ERROR
-            self.headers['error'] = 'Checksum missmatch between chunks and file'
+        with transaction.atomic():
+            file_blobs = FileBlob.objects.filter(id__in=file_blob_ids).all()
+            # Make sure the blobs are sorted with the order provided
+            file_blobs = sorted(file_blobs, key=lambda blob: file_blob_ids.index(blob.id))
+
+            new_checksum = sha1(b'')
+            offset = 0
+            for blob in file_blobs:
+                FileBlobIndex.objects.create(
+                    file=self,
+                    blob=blob,
+                    offset=offset,
+                )
+                for chunk in blob.getfile().chunks():
+                    new_checksum.update(chunk)
+                offset += blob.size
+
+            self.size = offset
+            self.checksum = new_checksum.hexdigest()
+
+            if checksum != self.checksum:
+                raise AssembleChecksumMismatch('Checksum mismatch')
 
         metrics.timing('filestore.file-size', offset)
         if commit:
diff --git a/src/sentry/tasks/assemble.py b/src/sentry/tasks/assemble.py
index 1cfd321d13..45e5789c13 100644
--- a/src/sentry/tasks/assemble.py
+++ b/src/sentry/tasks/assemble.py
@@ -1,6 +1,6 @@
 from __future__ import absolute_import, print_function
 
-import six
+import os
 import logging
 
 from django.db import transaction
@@ -10,76 +10,101 @@ logger = logging.getLogger(__name__)
 
 
 @instrumented_task(name='sentry.tasks.assemble.assemble_dif', queue='assemble')
-def assemble_dif(project_id, file_id, file_blob_ids, checksum, **kwargs):
-    from sentry.models import ChunkFileState, dsymfile, Project, CHUNK_STATE_HEADER
+def assemble_dif(project_id, name, checksum, chunks, **kwargs):
+    from sentry.models import ChunkFileState, dsymfile, Project, \
+        ProjectDSymFile, set_assemble_status, BadDif
+    from sentry.reprocessing import bump_reprocessing_revision
+
     with transaction.atomic():
+        project = Project.objects.filter(id=project_id).get()
+        set_assemble_status(project, checksum, ChunkFileState.ASSEMBLING)
 
         # Assemble the chunks into files
-        file = assemble_chunks(file_id, file_blob_ids, checksum)
+        file = assemble_file(project, name, checksum, chunks,
+                             file_type='project.dsym')
 
-        # If an error happend during assembling, we early return here
-        if file.headers.get(CHUNK_STATE_HEADER) == ChunkFileState.ERROR:
+        # If not file has been created this means that the file failed to
+        # assemble because of bad input data.  Return.
+        if file is None:
             return
 
-        project = Project.objects.filter(
-            id=project_id
-        ).get()
-
-        with file.getfile(as_tempfile=True) as tf:
-            result = dsymfile.detect_dif_from_filename(tf.name)
-            if result:
-                dsyms = dsymfile.create_dsym_from_dif(result, project, file.name)
-
-                from sentry.tasks.symcache_update import symcache_update
-                uuids_to_update = [six.text_type(x.uuid) for x in dsyms
-                                   if x.supports_symcache]
-                if uuids_to_update:
-                    symcache_update.delay(project_id=project.id,
-                                          uuids=uuids_to_update)
-
-                # Uploading new dsysm changes the reprocessing revision
-                dsymfile.bump_reprocessing_revision(project)
-                # We can delete the original chunk file since we created new dsym files
+        delete_file = True
+        try:
+            with file.getfile(as_tempfile=True) as tf:
+                # We only permit split difs to hit this endpoint.  The
+                # client is required to split them up first or we error.
+                try:
+                    result = dsymfile.detect_dif_from_path(tf.name)
+                except BadDif as e:
+                    set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                                        detail=e.args[0])
+                    return
+
+                if len(result) != 1:
+                    set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                                        detail='Contained wrong number of '
+                                        'architectures (expected one, got %s)'
+                                        % len(result))
+                    return
+
+                dsym_type, cpu, file_uuid, filename = result[0]
+                dsym, created = dsymfile.create_dsym_from_uuid(
+                    project, dsym_type, cpu, file_uuid,
+                    os.path.basename(name),
+                    file=file)
+                delete_file = False
+                bump_reprocessing_revision(project)
+
+                # XXX: this should only be done for files that
+                symcache, error = ProjectDSymFile.dsymcache.get_symcache(
+                    project, file_uuid, with_conversion_errors=True)
+                if error is not None:
+                    set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                                        detail=error)
+                else:
+                    set_assemble_status(project, checksum, ChunkFileState.OK)
+        finally:
+            if delete_file:
                 file.delete()
-            else:
-                file.headers[CHUNK_STATE_HEADER] = ChunkFileState.ERROR
-                file.headers['error'] = 'Invalid object file'
-                file.save()
-                logger.error(
-                    'assemble_chunks.invalid_object_file',
-                    extra={
-                        'error': file.headers.get('error', ''),
-                        'file_id': file.id
-                    }
-                )
-
-
-def assemble_chunks(file_id, file_blob_ids, checksum, **kwargs):
+
+
+def assemble_file(project, name, checksum, chunks, file_type):
     '''This assembles multiple chunks into on File.'''
-    if len(file_blob_ids) == 0:
-        logger.warning('assemble_chunks.empty_file_blobs', extra={
-            'error': 'Empty file blobs'
-        })
-
-    from sentry.models import File, ChunkFileState, CHUNK_STATE_HEADER
-
-    file = File.objects.filter(
-        id=file_id,
-    ).get()
-
-    file.headers[CHUNK_STATE_HEADER] = ChunkFileState.ASSEMBLING
-    # Do the actual assembling here
-
-    file.assemble_from_file_blob_ids(file_blob_ids, checksum)
-    if file.headers.get(CHUNK_STATE_HEADER, '') == ChunkFileState.ERROR:
-        logger.error(
-            'assemble_chunks.assemble_error',
-            extra={
-                'error': file.headers.get('error', ''),
-                'file_id': file.id
-            }
-        )
+    from sentry.models import File, ChunkFileState, AssembleChecksumMismatch, \
+        FileBlob, set_assemble_status
+
+    # Load all FileBlobs from db since we can be sure here we already own all
+    # chunks need to build the file
+    file_blobs = FileBlob.objects.filter(
+        checksum__in=chunks
+    ).values_list('id', 'checksum')
+
+    # We need to make sure the blobs are in the order in which
+    # we received them from the request.
+    # Otherwise it could happen that we assemble the file in the wrong order
+    # and get an garbage file.
+    file_blob_ids = [x[0] for x in sorted(
+        file_blobs, key=lambda blob: chunks.index(blob[1])
+    )]
+
+    # Sanity check.  In case not all blobs exist at this point we have a
+    # race condition.
+    if set(x[1] for x in file_blobs) != set(chunks):
+        set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                            detail='Not all chunks available for assembling')
+        return
+
+    file = File.objects.create(
+        name=name,
+        checksum=checksum,
+        type=file_type,
+    )
+    try:
+        file.assemble_from_file_blob_ids(file_blob_ids, checksum)
+    except AssembleChecksumMismatch:
+        file.delete()
+        set_assemble_status(project, checksum, ChunkFileState.ERROR,
+                            detail='Reported checksum mismatch')
+    else:
+        file.save()
         return file
-    file.headers[CHUNK_STATE_HEADER] = ChunkFileState.OK
-    file.save()
-    return file
diff --git a/tests/sentry/api/endpoints/test_dif_assemble.py b/tests/sentry/api/endpoints/test_dif_assemble.py
index fc45020aff..969908602f 100644
--- a/tests/sentry/api/endpoints/test_dif_assemble.py
+++ b/tests/sentry/api/endpoints/test_dif_assemble.py
@@ -7,9 +7,10 @@ from django.core.urlresolvers import reverse
 from django.core.files.base import ContentFile
 
 from sentry.models import ApiToken, FileBlob, File, FileBlobIndex, FileBlobOwner
-from sentry.models.file import ChunkFileState, CHUNK_STATE_HEADER
+from sentry.models.file import ChunkFileState
+from sentry.models.dsymfile import get_assemble_status, ProjectDSymFile
 from sentry.testutils import APITestCase
-from sentry.tasks.assemble import assemble_dif
+from sentry.tasks.assemble import assemble_dif, assemble_file
 
 
 class DifAssembleEndpoint(APITestCase):
@@ -31,7 +32,7 @@ class DifAssembleEndpoint(APITestCase):
                 self.organization.slug,
                 self.project.slug])
 
-    def test_assemble_json_scheme(self):
+    def test_assemble_json_schema(self):
         response = self.client.post(
             self.url,
             data={
@@ -78,12 +79,11 @@ class DifAssembleEndpoint(APITestCase):
         content = 'foo bar'.encode('utf-8')
         fileobj = ContentFile(content)
         file1 = File.objects.create(
-            name='baz.js',
+            name='baz.dSYM',
             type='default',
             size=7,
         )
         file1.putfile(fileobj, 3)
-
         checksum = sha1(content).hexdigest()
 
         blobs = FileBlob.objects.all()
@@ -106,7 +106,7 @@ class DifAssembleEndpoint(APITestCase):
 
         assert response.status_code == 200, response.content
         assert response.data[checksum]['state'] == ChunkFileState.NOT_FOUND
-        assert response.data[checksum]['missingChunks'] == set(checksums)
+        assert set(response.data[checksum]['missingChunks']) == set(checksums)
 
         # Now we add ownership to the blob
         blobs = FileBlob.objects.all()
@@ -116,6 +116,31 @@ class DifAssembleEndpoint(APITestCase):
                 organization=self.organization
             )
 
+        # The request will start the job to assemble the file
+        response = self.client.post(
+            self.url,
+            data={
+                checksum: {
+                    'name': 'dif',
+                    'chunks': checksums,
+                }
+            },
+            HTTP_AUTHORIZATION='Bearer {}'.format(self.token.token)
+        )
+
+        assert response.status_code == 200, response.content
+        assert response.data[checksum]['state'] == ChunkFileState.CREATED
+        assert response.data[checksum]['missingChunks'] == []
+
+        # Finally, we simulate a successful job
+        ProjectDSymFile.objects.create(
+            file=file1,
+            object_name='baz.dSYM',
+            cpu_name='x86_64',
+            project=self.project,
+            uuid='df449af8-0dcd-4320-9943-ec192134d593',
+        )
+
         # Request now tells us that everything is alright
         response = self.client.post(
             self.url,
@@ -147,7 +172,7 @@ class DifAssembleEndpoint(APITestCase):
 
         assert response.status_code == 200, response.content
         assert response.data[not_found_checksum]['state'] == ChunkFileState.NOT_FOUND
-        assert response.data[not_found_checksum]['missingChunks'] == set([not_found_checksum])
+        assert set(response.data[not_found_checksum]['missingChunks']) == set([not_found_checksum])
 
     @patch('sentry.tasks.assemble.assemble_dif')
     def test_assemble(self, mock_assemble_dif):
@@ -171,12 +196,12 @@ class DifAssembleEndpoint(APITestCase):
             organization=self.organization,
             blob=blob1
         )
-        bolb3 = FileBlob.from_file(fileobj3)
+        blob3 = FileBlob.from_file(fileobj3)
         FileBlobOwner.objects.get_or_create(
             organization=self.organization,
-            blob=bolb3
+            blob=blob3
         )
-        bolb2 = FileBlob.from_file(fileobj2)
+        blob2 = FileBlob.from_file(fileobj2)
 
         # we make a request now but we are missing ownership for chunk 2
         response = self.client.post(
@@ -193,12 +218,12 @@ class DifAssembleEndpoint(APITestCase):
         )
         assert response.status_code == 200, response.content
         assert response.data[total_checksum]['state'] == ChunkFileState.NOT_FOUND
-        assert response.data[total_checksum]['missingChunks'] == set([checksum2])
+        assert response.data[total_checksum]['missingChunks'] == [checksum2]
 
         # we add ownership to chunk 2
         FileBlobOwner.objects.get_or_create(
             organization=self.organization,
-            blob=bolb2
+            blob=blob2
         )
 
         # new request, ownership for all chunks is there but file does not exist yet
@@ -218,21 +243,18 @@ class DifAssembleEndpoint(APITestCase):
         assert response.data[total_checksum]['state'] == ChunkFileState.CREATED
         assert response.data[total_checksum]['missingChunks'] == []
 
-        file_blob_id_order = [bolb2.id, blob1.id, bolb3.id]
-
+        chunks = [checksum2, checksum1, checksum3]
         mock_assemble_dif.apply_async.assert_called_once_with(
             kwargs={
                 'project_id': self.project.id,
-                'file_id': 1,
-                'file_blob_ids': file_blob_id_order,
+                'name': 'test',
+                'chunks': chunks,
                 'checksum': total_checksum,
             }
         )
 
-        file = File.objects.filter(
-            id=1,
-        ).get()
-        file.assemble_from_file_blob_ids(file_blob_id_order, total_checksum)
+        file = assemble_file(self.project, 'test', total_checksum, chunks, 'project.dsym')
+        assert get_assemble_status(self.project, total_checksum)[0] != ChunkFileState.ERROR
         assert file.checksum == total_checksum
 
         file_blob_index = FileBlobIndex.objects.all()
@@ -241,23 +263,14 @@ class DifAssembleEndpoint(APITestCase):
     def test_dif_reponse(self):
         sym_file = self.load_fixture('crash.sym')
         blob1 = FileBlob.from_file(ContentFile(sym_file))
-
         total_checksum = sha1(sym_file).hexdigest()
-
-        file = File.objects.create(
-            name='test.sym',
-            checksum=total_checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
-
-        file_blob_id_order = [blob1.id]
+        chunks = [blob1.checksum]
 
         assemble_dif(
             project_id=self.project.id,
-            file_id=file.id,
-            file_blob_ids=file_blob_id_order,
+            name='crash.sym',
             checksum=total_checksum,
+            chunks=chunks,
         )
 
         response = self.client.post(
@@ -265,37 +278,28 @@ class DifAssembleEndpoint(APITestCase):
             data={
                 total_checksum: {
                     'name': 'test.sym',
-                    'chunks': [
-                        blob1.checksum
-                    ]
+                    'chunks': chunks,
                 }
             },
             HTTP_AUTHORIZATION='Bearer {}'.format(self.token.token)
         )
 
         assert response.status_code == 200, response.content
-        assert response.data[total_checksum]['difs'][0]['cpuName'] == 'x86_64'
+        assert response.data[total_checksum]['state'] == ChunkFileState.OK
+        assert response.data[total_checksum]['dif']['cpuName'] == 'x86_64'
+        assert response.data[total_checksum]['dif']['uuid'] == '67e9247c-814e-392b-a027-dbde6748fcbf'
 
     def test_dif_error_reponse(self):
         sym_file = 'fail'
         blob1 = FileBlob.from_file(ContentFile(sym_file))
-
         total_checksum = sha1(sym_file).hexdigest()
-
-        file = File.objects.create(
-            name='test.sym',
-            checksum=total_checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
-
-        file_blob_id_order = [blob1.id]
+        chunks = [blob1.checksum]
 
         assemble_dif(
             project_id=self.project.id,
-            file_id=file.id,
-            file_blob_ids=file_blob_id_order,
+            name='test.sym',
             checksum=total_checksum,
+            chunks=chunks,
         )
 
         response = self.client.post(
@@ -303,13 +307,12 @@ class DifAssembleEndpoint(APITestCase):
             data={
                 total_checksum: {
                     'name': 'test.sym',
-                    'chunks': [
-                        blob1.checksum
-                    ]
+                    'chunks': [],
                 }
             },
             HTTP_AUTHORIZATION='Bearer {}'.format(self.token.token)
         )
 
         assert response.status_code == 200, response.content
-        assert response.data[total_checksum]['errors'][0] == 'Invalid object file'
+        assert response.data[total_checksum]['state'] == ChunkFileState.ERROR
+        assert response.data[total_checksum]['detail'].startswith('Invalid debug information file')
diff --git a/tests/sentry/tasks/test_assemble.py b/tests/sentry/tasks/test_assemble.py
index 5808ba05ee..a01ed9f121 100644
--- a/tests/sentry/tasks/test_assemble.py
+++ b/tests/sentry/tasks/test_assemble.py
@@ -6,8 +6,9 @@ from django.core.files.base import ContentFile
 
 from sentry.testutils import TestCase
 from sentry.tasks.assemble import assemble_dif
-from sentry.models import FileBlob, File
-from sentry.models.file import ChunkFileState, CHUNK_STATE_HEADER
+from sentry.models import FileBlob
+from sentry.models.file import ChunkFileState
+from sentry.models.dsymfile import get_assemble_status, ProjectDSymFile
 
 
 class AssembleTest(TestCase):
@@ -34,62 +35,35 @@ class AssembleTest(TestCase):
 
         # The order here is on purpose because we check for the order of checksums
         blob1 = FileBlob.from_file(fileobj1)
-        bolb3 = FileBlob.from_file(fileobj3)
-        bolb2 = FileBlob.from_file(fileobj2)
+        blob3 = FileBlob.from_file(fileobj3)
+        blob2 = FileBlob.from_file(fileobj2)
 
-        file = File.objects.create(
-            name='test',
-            checksum=total_checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
-
-        file_blob_id_order = [bolb2.id, blob1.id, bolb3.id]
-
-        file = File.objects.create(
-            name='test',
-            checksum=total_checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
+        chunks = [blob2.checksum, blob1.checksum, blob3.checksum]
 
         assemble_dif(
             project_id=self.project.id,
-            file_id=file.id,
-            file_blob_ids=file_blob_id_order,
+            name='foo.sym',
             checksum=total_checksum,
+            chunks=chunks,
         )
 
-        file = File.objects.filter(
-            id=file.id,
-        ).get()
-
-        assert file.headers.get(CHUNK_STATE_HEADER) == ChunkFileState.ERROR
+        assert get_assemble_status(self.project, total_checksum)[0] == ChunkFileState.ERROR
 
     def test_dif(self):
         sym_file = self.load_fixture('crash.sym')
         blob1 = FileBlob.from_file(ContentFile(sym_file))
-
         total_checksum = sha1(sym_file).hexdigest()
 
-        file = File.objects.create(
-            name='test.sym',
-            checksum=total_checksum,
-            type='chunked',
-            headers={CHUNK_STATE_HEADER: ChunkFileState.CREATED}
-        )
-
-        file_blob_id_order = [blob1.id]
-
         assemble_dif(
             project_id=self.project.id,
-            file_id=file.id,
-            file_blob_ids=file_blob_id_order,
+            name='crash.sym',
             checksum=total_checksum,
+            chunks=[blob1.checksum],
         )
 
-        file = File.objects.filter(
-            checksum=total_checksum,
+        dif = ProjectDSymFile.objects.filter(
+            project=self.project,
+            file__checksum=total_checksum,
         ).get()
 
-        assert file.headers == {'Content-Type': 'text/x-breakpad'}
+        assert dif.file.headers == {'Content-Type': 'text/x-breakpad'}
