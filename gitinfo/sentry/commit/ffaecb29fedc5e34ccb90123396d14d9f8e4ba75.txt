commit ffaecb29fedc5e34ccb90123396d14d9f8e4ba75
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Sat Feb 4 16:33:44 2017 +0300

    Added basic reprocessing back in

diff --git a/src/sentry/models/dsymfile.py b/src/sentry/models/dsymfile.py
index ab17490c94..f028d25039 100644
--- a/src/sentry/models/dsymfile.py
+++ b/src/sentry/models/dsymfile.py
@@ -26,6 +26,7 @@ from sentry.utils.zip import safe_extract_zip
 from sentry.utils.db import is_sqlite
 from sentry.utils.native import parse_addr
 from sentry.constants import KNOWN_DSYM_TYPES
+from sentry.reprocessing import resolve_processing_issue
 
 
 class DSymSDKManager(BaseManager):
@@ -386,7 +387,7 @@ def _create_macho_dsym_from_uuid(project, cpu_name, uuid, fileobj,
     file.putfile(fileobj)
     try:
         with transaction.atomic():
-            return cls.objects.create(
+            rv = cls.objects.create(
                 file=file,
                 uuid=uuid,
                 cpu_name=cpu_name,
@@ -395,7 +396,15 @@ def _create_macho_dsym_from_uuid(project, cpu_name, uuid, fileobj,
             )
     except IntegrityError:
         file.delete()
-        return cls.objects.get(uuid=uuid, **extra)
+        rv = cls.objects.get(uuid=uuid, **extra)
+
+    resolve_processing_issue(
+        project=project,
+        scope='native',
+        object='dsym:%s' % uuid,
+    )
+
+    return rv
 
 
 def create_files_from_macho_zip(fileobj, project=None):
diff --git a/src/sentry/models/processingissue.py b/src/sentry/models/processingissue.py
index 73f2699fbe..328490df99 100644
--- a/src/sentry/models/processingissue.py
+++ b/src/sentry/models/processingissue.py
@@ -15,9 +15,8 @@ from sentry.db.models import (
 )
 
 
-def get_processing_issue_checksum(scope, object, type):
+def get_processing_issue_checksum(scope, object):
     h = sha1()
-    h.update(type.encode('utf-8') + '\x00')
     h.update(scope.encode('utf-8') + '\x00')
     h.update(object.encode('utf-8') + '\x00')
     return h.hexdigest()
@@ -29,23 +28,26 @@ class ProcessingIssueManager(BaseManager):
         """Given scope, object and type this marks all issues as resolved
         and returns a list of events that now require reprocessing.
         """
-        checksum = get_processing_issue_checksum(scope, object, type)
+        checksum = get_processing_issue_checksum(scope, object)
 
         # Find all raw events that suffer from this issue.
-        raw_events = set(EventProcessingIssue.objects.filter(
+        q = EventProcessingIssue.objects.filter(
             processing_issue__project=project,
             processing_issue__checksum=checksum,
-        ).values_list('raw_event_id', flat=True).distinct())
+        )
+        if type is not None:
+            q = q.filter(processing_issue__type=type)
+        raw_events = set(q.values_list('raw_event_id', flat=True).distinct())
 
         # Delete all affected processing issue mappings
-        EventProcessingIssue.objects.filter(
-            raw_event__project=project,
-            raw_event__checksum=checksum,
-        ).delete()
-        ProcessingIssue.objects.filter(
+        q.delete()
+        q = ProcessingIssue.objects.filter(
             project=project,
             checksum=checksum,
-        ).delete()
+        )
+        if type is not None:
+            q = q.filter(type=type)
+        q.delete()
 
         # If we did not find any raw events, we can bail here now safely.
         if not raw_events:
@@ -63,13 +65,13 @@ class ProcessingIssueManager(BaseManager):
     def record_processing_issue(self, project, raw_event, scope, object,
                                 type, data=None):
         data = dict(data or {})
-        checksum = get_processing_issue_checksum(scope, object, type)
+        checksum = get_processing_issue_checksum(scope, object)
         data['_scope'] = scope
         data['_object'] = object
-        data['_type'] = type
         issue = ProcessingIssue.objects.get_or_create(
             project=project,
             checksum=checksum,
+            type=type,
             data=data,
         )
         EventProcessingIssue.objects.get_or_create(
@@ -81,8 +83,9 @@ class ProcessingIssueManager(BaseManager):
 class ProcessingIssue(Model):
     __core__ = False
 
-    project = FlexibleForeignKey('sentry.Project')
-    checksum = models.CharField(max_length=40)
+    project = FlexibleForeignKey('sentry.Project', db_index=True)
+    checksum = models.CharField(max_length=40, db_index=True)
+    type = models.IntegerField()
     data = GzippedDictField()
 
     objects = BaseManager()
@@ -90,7 +93,7 @@ class ProcessingIssue(Model):
     class Meta:
         app_label = 'sentry'
         db_table = 'sentry_processingissue'
-        unique_together = (('project', 'checksum'),)
+        unique_together = (('project', 'checksum', 'type'),)
 
     __repr__ = sane_repr('project_id')
 
@@ -102,10 +105,6 @@ class ProcessingIssue(Model):
     def object(self):
         return self.data['_object']
 
-    @property
-    def type(self):
-        return self.data['_type']
-
 
 class EventProcessingIssue(Model):
     __core__ = False
diff --git a/src/sentry/reprocessing.py b/src/sentry/reprocessing.py
index df4a8dda3f..24ad20f1e5 100644
--- a/src/sentry/reprocessing.py
+++ b/src/sentry/reprocessing.py
@@ -19,3 +19,17 @@ def report_processing_issue(event_data, scope, object=None, type=None, data=None
         'type': type,
         'data': data,
     }
+
+
+def resolve_processing_issue(project, scope, object=None, type=None):
+    if object is None:
+        object = '*'
+    from sentry.models import ProcessingIssue
+    from sentry.tasks.store import reprocess_events
+    raw_event_ids = ProcessingIssue.objects.resolve_processing_issue(
+        project=project,
+        scope=scope,
+        object=object,
+        type=type
+    )
+    reprocess_events.delay(raw_event_ids=raw_event_ids)
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 8e73a8d829..0ba29a981d 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -189,3 +189,19 @@ def save_event(cache_key=None, data=None, start_time=None, **kwargs):
         if start_time:
             metrics.timing('events.time-to-process', time() - start_time,
                            instance=data['platform'])
+
+
+@instrumented_task(
+    name='sentry.tasks.store.reprocess_events',
+    queue='events.reprocess_events')
+def reprocess_events(raw_event_ids, **kwargs):
+    from sentry.models import RawEvent
+    from sentry.coreapi import ClientApiHelper
+    helper = ClientApiHelper()
+
+    for id in raw_event_ids:
+        raw_event = RawEvent.objects.get(id)
+        if raw_event is None:
+            continue
+        helper.insert_data_to_database(raw_event.data)
+        raw_event.delete()
