commit ac5dfebec12fb6aacde8edd5d88f6fbf9b28f181
Author: Ted Kaemming <ted@kaemming.com>
Date:   Thu May 18 15:24:16 2017 -0700

    Move all module-scoped imports in `unmerge` task to local imports.
    
    We're suspicious that this will prevent a deadlock when running this
    task in a Celery worker process while also using the Riak NodeStore
    backend.

diff --git a/src/sentry/tasks/unmerge.py b/src/sentry/tasks/unmerge.py
index 75d7d597f7..106b2375e7 100644
--- a/src/sentry/tasks/unmerge.py
+++ b/src/sentry/tasks/unmerge.py
@@ -7,14 +7,6 @@ from django.db.models import F
 
 from sentry.app import tsdb
 from sentry.constants import DEFAULT_LOGGER_NAME, LOG_LEVELS_MAP
-from sentry.event_manager import (
-    ScoreClause, generate_culprit, get_hashes_for_event, md5_from_hash
-)
-from sentry.models import (
-    Activity, Environment, Event, EventMapping, EventTag, EventUser, Group,
-    GroupHash, GroupRelease, GroupTagKey, GroupTagValue, Project, Release,
-    UserReport
-)
 from sentry.tasks.base import instrumented_task
 
 
@@ -39,24 +31,26 @@ def cache(function):
 
 
 def get_caches():
+    from sentry.models import Environment, GroupRelease, Project, Release
+
     return {
-        Environment: cache(
+        'Environment': cache(
             lambda organization_id, name: Environment.objects.get(
                 organization_id=organization_id,
                 name=name,
             ),
         ),
-        GroupRelease: cache(
+        'GroupRelease': cache(
             lambda group_id, environment, release_id: GroupRelease.objects.get(
                 group_id=group_id,
                 environment=environment,
                 release_id=release_id,
             ),
         ),
-        Project: cache(
+        'Project': cache(
             lambda id: Project.objects.get(id=id),
         ),
-        Release: cache(
+        'Release': cache(
             lambda organization_id, version: Release.objects.get(
                 organization_id=organization_id,
                 version=version,
@@ -72,11 +66,14 @@ def merge_mappings(values):
     return result
 
 
+def get_culprit(event):
+    from sentry.event_manager import generate_culprit
+
+    return generate_culprit(event.data, event.platform)
+
+
 initial_fields = {
-    'culprit': lambda event: generate_culprit(
-        event.data,
-        event.platform,
-    ),
+    'culprit': get_culprit,
     'data': lambda event: {
         'last_received': event.data.get('received') or float(event.datetime.strftime('%s')),
         'type': event.data['type'],
@@ -92,20 +89,26 @@ initial_fields = {
 }
 
 
+def get_event_score(caches, data, event):
+    from sentry.event_manager import ScoreClause
+
+    return ScoreClause.calculate(
+        data['times_seen'] + 1,
+        data['last_seen'],
+    )
+
+
 backfill_fields = {
     'platform': lambda caches, data, event: event.platform,
     'logger': lambda caches, data, event: event.get_tag('logger') or DEFAULT_LOGGER_NAME,
     'first_seen': lambda caches, data, event: event.datetime,
     'active_at': lambda caches, data, event: event.datetime,
-    'first_release': lambda caches, data, event: caches[Release](
-        caches[Project](event.project_id).organization_id,
+    'first_release': lambda caches, data, event: caches['Release'](
+        caches['Project'](event.project_id).organization_id,
         event.get_tag('sentry:release'),
     ) if event.get_tag('sentry:release') else data.get('first_release', None),
     'times_seen': lambda caches, data, event: data['times_seen'] + 1,
-    'score': lambda caches, data, event: ScoreClause.calculate(
-        data['times_seen'] + 1,
-        data['last_seen'],
-    ),
+    'score': get_event_score,
 }
 
 
@@ -137,6 +140,8 @@ def get_group_backfill_attributes(caches, group, events):
 
 
 def get_fingerprint(event):
+    from sentry.event_manager import get_hashes_for_event, md5_from_hash
+
     # TODO: This *might* need to be protected from an IndexError?
     primary_hash = get_hashes_for_event(event)[0]
     return md5_from_hash(primary_hash)
@@ -151,7 +156,11 @@ def migrate_events(caches, project, source_id, destination_id, fingerprints, eve
     if not events:
         return destination_id
 
+    from sentry.models import Group
+
     if destination_id is None:
+        from sentry.models import Activity, GroupHash
+
         # XXX: There is a race condition here between the (wall clock) time
         # that the migration is started by the user and when we actually
         # get to this block where the new destination is created and we've
@@ -209,6 +218,8 @@ def migrate_events(caches, project, source_id, destination_id, fingerprints, eve
 
     event_id_set = set(event.id for event in events)
 
+    from sentry.models import Event, EventTag, EventMapping, UserReport
+
     Event.objects.filter(
         project_id=project.id,
         id__in=event_id_set,
@@ -238,6 +249,8 @@ def migrate_events(caches, project, source_id, destination_id, fingerprints, eve
 
 
 def truncate_denormalizations(group_id):
+    from sentry.models import GroupTagKey, GroupTagValue, GroupRelease
+
     GroupTagKey.objects.filter(
         group_id=group_id,
     ).delete()
@@ -283,6 +296,8 @@ def collect_tag_data(events):
 
 
 def repair_tag_data(caches, project, events):
+    from sentry.models import GroupTagKey, GroupTagValue
+
     for group_id, keys in collect_tag_data(events).items():
         for key, values in keys.items():
             GroupTagKey.objects.get_or_create(
@@ -335,7 +350,7 @@ def collect_release_data(caches, project, events):
         key = (
             event.group_id,
             environment,
-            caches[Release](
+            caches['Release'](
                 project.organization_id,
                 release,
             ).id,
@@ -351,6 +366,8 @@ def collect_release_data(caches, project, events):
 
 
 def repair_group_release_data(caches, project, events):
+    from sentry.models import GroupRelease
+
     attributes = collect_release_data(caches, project, events).items()
     for (group_id, environment, release_id), (first_seen, last_seen) in attributes:
         instance, created = GroupRelease.objects.get_or_create(
@@ -369,6 +386,8 @@ def repair_group_release_data(caches, project, events):
 
 
 def get_event_user_from_interface(value):
+    from sentry.models import EventUser
+
     return EventUser(
         ident=value.get('id'),
         email=value.get('email'),
@@ -407,7 +426,7 @@ def collect_tsdb_data(caches, project, events):
                 get_event_user_from_interface(user).tag_value,
             )
 
-        environment = caches[Environment](
+        environment = caches['Environment'](
             project.organization_id,
             event.data.get('environment', ''),
         )
@@ -418,10 +437,10 @@ def collect_tsdb_data(caches, project, events):
         if release:
             # TODO: I'm also not sure if "environment" here is correct, see
             # similar comment above during creation.
-            grouprelease = caches[GroupRelease](
+            grouprelease = caches['GroupRelease'](
                 event.group_id,
                 event.data.get('environment', ''),
-                caches[Release](
+                caches['Release'](
                     project.organization_id,
                     release,
                 ).id,
@@ -457,6 +476,8 @@ def repair_denormalizations(caches, project, events):
 
 
 def update_tag_value_counts(id_list):
+    from sentry.models import GroupTagKey, GroupTagValue
+
     instances = GroupTagKey.objects.filter(group_id__in=id_list)
     for instance in instances:
         instance.update(
@@ -470,6 +491,8 @@ def update_tag_value_counts(id_list):
 
 @instrumented_task(name='sentry.tasks.unmerge', queue='unmerge')
 def unmerge(project_id, source_id, destination_id, fingerprints, actor_id, cursor=None, batch_size=500):
+    from sentry.models import Event
+
     # XXX: If a ``GroupHash`` is unmerged *again* while this operation is
     # already in progress, some events from the fingerprint associated with the
     # hash may not be migrated to the new destination! We could solve this with
@@ -488,7 +511,9 @@ def unmerge(project_id, source_id, destination_id, fingerprints, actor_id, curso
     if cursor is None:
         truncate_denormalizations(source_id)
 
-    project = Project.objects.get(id=project_id)
+    caches = get_caches()
+
+    project = caches['Project'](project_id)
 
     # TODO: It might make sense to fetch the source group to assert that it is
     # contained within the project, even though we don't actually directy use
@@ -513,8 +538,6 @@ def unmerge(project_id, source_id, destination_id, fingerprints, actor_id, curso
 
     Event.objects.bind_nodes(events, 'data')
 
-    caches = get_caches()
-
     destination_id = migrate_events(
         caches,
         project,
