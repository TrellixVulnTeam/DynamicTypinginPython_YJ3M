commit ae2077c7ec76d8cbe27279522b7c83045f36e9f1
Author: William Mak <william@wmak.io>
Date:   Thu Apr 16 20:25:09 2020 -0400

    feature(performance): Top5 Events (#17987)
    
    * feature(performance): Adding the topEvents param to event-stats
    
    - Using gropuby to determine topEvents
    - Two snuba queries are made, one with limit=5 to determine what the top events are, and another for the timeseries
    - Should work with multi yaxis
    - Created a new discover function for top events

diff --git a/src/sentry/api/bases/organization_events.py b/src/sentry/api/bases/organization_events.py
index ce43daa408..d86e2b20cb 100644
--- a/src/sentry/api/bases/organization_events.py
+++ b/src/sentry/api/bases/organization_events.py
@@ -114,15 +114,9 @@ class OrganizationEventsV2EndpointBase(OrganizationEventsEndpointBase):
                 row["transaction.status"] = SPAN_STATUS_CODE_TO_NAME.get(row["transaction.status"])
 
         fields = request.GET.getlist("field")
-        issues = {}
         if "issue" in fields:  # Look up the short ID and return that in the results
-            issue_ids = set(row["issue.id"] for row in results)
-            issues = {
-                i.id: i.qualified_short_id
-                for i in Group.objects.filter(
-                    id__in=issue_ids, project_id__in=project_ids, project__organization=organization
-                )
-            }
+            issue_ids = set(row.get("issue.id") for row in results)
+            issues = Group.issues_mapping(issue_ids, project_ids, organization)
             for result in results:
                 if "issue.id" in result:
                     result["issue"] = issues.get(result["issue.id"], "unknown")
@@ -138,7 +132,7 @@ class OrganizationEventsV2EndpointBase(OrganizationEventsEndpointBase):
 
         return results
 
-    def get_event_stats_data(self, request, organization, get_event_stats):
+    def get_event_stats_data(self, request, organization, get_event_stats, top_events=False):
         try:
             columns = request.GET.getlist("yAxis", ["count()"])
             query = request.GET.get("query")
@@ -171,15 +165,32 @@ class OrganizationEventsV2EndpointBase(OrganizationEventsEndpointBase):
         except InvalidSearchQuery as err:
             raise ParseError(detail=six.text_type(err))
         serializer = SnubaTSResultSerializer(organization, None, request.user)
-        if len(columns) > 1:
-            # Return with requested yAxis as the key
-            return {
-                column: serializer.serialize(result, get_function_alias(query_column))
-                for column, query_column in zip(columns, query_columns)
-            }
+
+        if top_events:
+            results = {}
+            for key, event_result in six.iteritems(result):
+                if len(query_columns) > 1:
+                    results[key] = self.serialize_multiple_axis(
+                        serializer, event_result, columns, query_columns
+                    )
+                else:
+                    # Need to get function alias if count is a field, but not the axis
+                    results[key] = serializer.serialize(
+                        event_result, get_function_alias(query_columns[0])
+                    )
+            return results
+        elif len(query_columns) > 1:
+            return self.serialize_multiple_axis(serializer, result, columns, query_columns)
         else:
             return serializer.serialize(result)
 
+    def serialize_multiple_axis(self, serializer, event_result, columns, query_columns):
+        # Return with requested yAxis as the key
+        return {
+            column: serializer.serialize(event_result, get_function_alias(query_column))
+            for column, query_column in zip(columns, query_columns)
+        }
+
 
 class KeyTransactionBase(OrganizationEventsV2EndpointBase):
     def has_feature(self, request, organization):
diff --git a/src/sentry/api/endpoints/organization_events_stats.py b/src/sentry/api/endpoints/organization_events_stats.py
index 29bd750901..2c55e28f14 100644
--- a/src/sentry/api/endpoints/organization_events_stats.py
+++ b/src/sentry/api/endpoints/organization_events_stats.py
@@ -6,6 +6,7 @@ from rest_framework.response import Response
 from rest_framework.exceptions import ParseError
 
 from sentry import features, eventstore
+from sentry.constants import MAX_TOP_EVENTS
 from sentry.api.bases import OrganizationEventsV2EndpointBase, OrganizationEventsError, NoProjects
 from sentry.api.event_search import resolve_field_list, InvalidSearchQuery
 from sentry.api.serializers.snuba import SnubaTSResultSerializer
@@ -20,7 +21,35 @@ class OrganizationEventsStatsEndpoint(OrganizationEventsV2EndpointBase):
         if not features.has("organizations:discover-basic", organization, actor=request.user):
             return self.get_v1_results(request, organization)
 
+        top_events = "topEvents" in request.GET
+        limit = None
+
+        if top_events:
+            try:
+                limit = int(request.GET.get("topEvents", 0))
+            except ValueError:
+                return Response({"detail": "topEvents must be an integer"}, status=400)
+            if limit > MAX_TOP_EVENTS:
+                return Response(
+                    {"detail": "Can only get up to {} top events".format(MAX_TOP_EVENTS)},
+                    status=400,
+                )
+            elif limit <= 0:
+                return Response({"detail": "If topEvents needs to be at least 1"}, status=400)
+
         def get_event_stats(query_columns, query, params, rollup, reference_event):
+            if top_events:
+                return discover.top_events_timeseries(
+                    timeseries_columns=query_columns,
+                    selected_columns=request.GET.getlist("field")[:],
+                    user_query=query,
+                    params=params,
+                    orderby=self.get_orderby(request),
+                    rollup=rollup,
+                    limit=limit,
+                    organization=organization,
+                    referrer="api.organization-event-stats.find-topn",
+                )
             return discover.timeseries_query(
                 selected_columns=query_columns,
                 query=query,
@@ -31,7 +60,8 @@ class OrganizationEventsStatsEndpoint(OrganizationEventsV2EndpointBase):
             )
 
         return Response(
-            self.get_event_stats_data(request, organization, get_event_stats), status=200
+            self.get_event_stats_data(request, organization, get_event_stats, top_events),
+            status=200,
         )
 
     def get_v1_results(self, request, organization):
diff --git a/src/sentry/constants.py b/src/sentry/constants.py
index 73a272c7d7..0aa4adad68 100644
--- a/src/sentry/constants.py
+++ b/src/sentry/constants.py
@@ -496,6 +496,9 @@ INTERNAL_INTEGRATION_TOKEN_COUNT_MAX = 20
 
 ALL_ACCESS_PROJECTS = {-1}
 
+# Most number of events for the top-n graph
+MAX_TOP_EVENTS = 5
+
 
 @unique
 class DataCategory(IntEnum):
diff --git a/src/sentry/models/group.py b/src/sentry/models/group.py
index 20b2ad8385..9c934aee63 100644
--- a/src/sentry/models/group.py
+++ b/src/sentry/models/group.py
@@ -473,3 +473,13 @@ class Group(Model):
     @classmethod
     def calculate_score(cls, times_seen, last_seen):
         return math.log(float(times_seen or 1)) * 600 + float(last_seen.strftime("%s"))
+
+    @staticmethod
+    def issues_mapping(group_ids, project_ids, organization):
+        """ Create a dictionary of group_ids to their qualified_short_ids """
+        return {
+            i.id: i.qualified_short_id
+            for i in Group.objects.filter(
+                id__in=group_ids, project_id__in=project_ids, project__organization=organization
+            )
+        }
diff --git a/src/sentry/snuba/discover.py b/src/sentry/snuba/discover.py
index 787958dc56..472db602c2 100644
--- a/src/sentry/snuba/discover.py
+++ b/src/sentry/snuba/discover.py
@@ -19,7 +19,7 @@ from sentry.api.event_search import (
 
 from sentry import eventstore
 
-from sentry.models import Project, ProjectStatus
+from sentry.models import Project, ProjectStatus, Group
 from sentry.tagstore.base import TOP_VALUES_DEFAULT_LIMIT
 from sentry.utils.snuba import (
     Dataset,
@@ -40,6 +40,7 @@ __all__ = (
     "query",
     "key_transaction_query",
     "timeseries_query",
+    "top_events_timeseries",
     "get_pagination_ids",
     "get_facets",
     "transform_results",
@@ -692,7 +693,7 @@ def get_timeseries_snuba_filter(selected_columns, query, params, rollup, referen
             snuba_filter.conditions.extend(ref_conditions)
 
     # Resolve the public aliases into the discover dataset names.
-    snuba_filter, _ = resolve_discover_aliases(snuba_filter)
+    snuba_filter, translated_columns = resolve_discover_aliases(snuba_filter)
     if not snuba_filter.aggregations:
         raise InvalidSearchQuery("Cannot get timeseries result with no aggregation.")
 
@@ -701,7 +702,7 @@ def get_timeseries_snuba_filter(selected_columns, query, params, rollup, referen
     if len(snuba_filter.aggregations) == 1:
         snuba_filter.aggregations[0][2] = "count"
 
-    return snuba_filter
+    return snuba_filter, translated_columns
 
 
 def key_transaction_timeseries_query(selected_columns, query, params, rollup, referrer, queryset):
@@ -717,7 +718,7 @@ def key_transaction_timeseries_query(selected_columns, query, params, rollup, re
         referrer (str|None) A referrer string to help locate the origin of this query.
         queryset (QuerySet) Filtered QuerySet of KeyTransactions
     """
-    snuba_filter = get_timeseries_snuba_filter(selected_columns, query, params, rollup)
+    snuba_filter, _ = get_timeseries_snuba_filter(selected_columns, query, params, rollup)
 
     if queryset.exists():
         snuba_filter.conditions.extend(key_transaction_conditions(queryset))
@@ -765,8 +766,7 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
                     conditions based on the provided reference.
     referrer (str|None) A referrer string to help locate the origin of this query.
     """
-
-    snuba_filter = get_timeseries_snuba_filter(
+    snuba_filter, _ = get_timeseries_snuba_filter(
         selected_columns, query, params, rollup, reference_event
     )
 
@@ -788,6 +788,110 @@ def timeseries_query(selected_columns, query, params, rollup, reference_event=No
     return SnubaTSResult({"data": result}, snuba_filter.start, snuba_filter.end, rollup)
 
 
+def top_events_timeseries(
+    timeseries_columns,
+    selected_columns,
+    user_query,
+    params,
+    orderby,
+    rollup,
+    limit,
+    organization,
+    referrer=None,
+):
+    """
+    High-level API for doing arbitrary user timeseries queries for a limited number of top events
+
+    Returns a dictionary of SnubaTSResult objects that have been zerofilled in
+    case of gaps. Each value of the dictionary should match the result of a timeseries query
+
+    timeseries_columns (Sequence[str]) List of public aliases to fetch for the timeseries query,
+                        usually matches the y-axis of the graph
+    selected_columns (Sequence[str]) List of public aliases to fetch for the events query,
+                        this is to determine what the top events are
+    user_query (str) Filter query string to create conditions from. needs to be user_query
+                        to not conflict with the function query
+    params (Dict[str, str]) Filtering parameters with start, end, project_id, environment,
+    orderby (Sequence[str]) The fields to order results by.
+    rollup (int) The bucket width in seconds
+    limit (int) The number of events to get timeseries for
+    organization (Organization) Used to map group ids to short ids
+    referrer (str|None) A referrer string to help locate the origin of this query.
+    """
+    top_events = query(
+        selected_columns,
+        query=user_query,
+        params=params,
+        orderby=orderby,
+        limit=limit,
+        referrer=referrer,
+    )
+
+    snuba_filter, translated_columns = get_timeseries_snuba_filter(
+        timeseries_columns + selected_columns, user_query, params, rollup
+    )
+
+    for field in selected_columns:
+        # project is handled by filter_keys already
+        if field in ["project", "project.id"]:
+            continue
+        values = list({event.get(field) for event in top_events["data"] if field in event})
+        if values and all(value is not None for value in values):
+            # timestamp needs special handling, creating a big OR instead
+            if field == "timestamp":
+                snuba_filter.conditions.append([["timestamp", "=", value] for value in values])
+            else:
+                snuba_filter.conditions.append([resolve_column(field), "IN", values])
+
+    result = raw_query(
+        aggregations=snuba_filter.aggregations,
+        conditions=snuba_filter.conditions,
+        filter_keys=snuba_filter.filter_keys,
+        start=snuba_filter.start,
+        end=snuba_filter.end,
+        rollup=rollup,
+        orderby="time",
+        groupby=["time"] + snuba_filter.groupby,
+        dataset=Dataset.Discover,
+        limit=10000,
+        referrer=referrer,
+    )
+
+    result = transform_results(result, translated_columns, snuba_filter)
+    issues = {}
+    if "issue" in selected_columns:
+        issues = Group.issues_mapping(
+            set([event["issue.id"] for event in top_events["data"]]),
+            params["project_id"],
+            organization,
+        )
+
+    translated_columns["project_id"] = "project"
+    translated_groupby = [translated_columns.get(field, field) for field in snuba_filter.groupby]
+    # so the result key is consistent
+    translated_groupby.sort()
+
+    results = {}
+    for row in result["data"]:
+        values = []
+        for field in translated_groupby:
+            if field == "issue.id":
+                values.append(issues.get(row["issue.id"], "unknown"))
+            else:
+                values.append(six.text_type(row.get(field)))
+        result_key = ",".join(values)
+        results.setdefault(result_key, []).append(row)
+    for key, item in six.iteritems(results):
+        results[key] = SnubaTSResult(
+            {"data": zerofill(item, snuba_filter.start, snuba_filter.end, rollup, "time")},
+            snuba_filter.start,
+            snuba_filter.end,
+            rollup,
+        )
+
+    return results
+
+
 def get_id(result):
     if result:
         return result[1]
diff --git a/tests/snuba/api/endpoints/test_organization_events_stats.py b/tests/snuba/api/endpoints/test_organization_events_stats.py
index aff445b3fe..3dea1db19c 100644
--- a/tests/snuba/api/endpoints/test_organization_events_stats.py
+++ b/tests/snuba/api/endpoints/test_organization_events_stats.py
@@ -11,6 +11,7 @@ from django.core.urlresolvers import reverse
 from sentry.testutils import APITestCase, SnubaTestCase
 from sentry.testutils.helpers.datetime import iso_format, before_now
 from sentry.utils.compat import zip
+from sentry.utils.samples import load_data
 
 
 class OrganizationEventsStatsEndpointTest(APITestCase, SnubaTestCase):
@@ -572,3 +573,454 @@ class OrganizationEventsStatsEndpointTest(APITestCase, SnubaTestCase):
                 },
             )
         assert response.status_code == 400
+
+
+class OrganizationEventsStatsTopNEvents(APITestCase, SnubaTestCase):
+    def setUp(self):
+        super(OrganizationEventsStatsTopNEvents, self).setUp()
+        self.login_as(user=self.user)
+
+        self.day_ago = before_now(days=1).replace(hour=10, minute=0, second=0, microsecond=0)
+
+        self.project = self.create_project()
+        self.project2 = self.create_project()
+        self.user2 = self.create_user()
+        transaction_data = load_data("transaction")
+        transaction_data["start_timestamp"] = iso_format(self.day_ago + timedelta(minutes=2))
+        transaction_data["timestamp"] = iso_format(self.day_ago + timedelta(minutes=4))
+        self.event_data = [
+            {
+                "data": {
+                    "message": "poof",
+                    "timestamp": iso_format(self.day_ago + timedelta(minutes=2)),
+                    "user": {"email": self.user.email},
+                    "fingerprint": ["group1"],
+                },
+                "project": self.project2,
+                "count": 3,
+            },
+            {
+                "data": {
+                    "message": "voof",
+                    "timestamp": iso_format(self.day_ago + timedelta(hours=1, minutes=2)),
+                    "fingerprint": ["group2"],
+                    "user": {"email": self.user2.email},
+                },
+                "project": self.project2,
+                "count": 3,
+            },
+            {
+                "data": {
+                    "message": "very bad",
+                    "timestamp": iso_format(self.day_ago + timedelta(minutes=2)),
+                    "fingerprint": ["group3"],
+                    "user": {"email": "foo@example.com"},
+                },
+                "project": self.project,
+                "count": 3,
+            },
+            {
+                "data": {
+                    "message": "oh no",
+                    "timestamp": iso_format(self.day_ago + timedelta(minutes=2)),
+                    "fingerprint": ["group4"],
+                    "user": {"email": "bar@example.com"},
+                },
+                "project": self.project,
+                "count": 3,
+            },
+            {"data": transaction_data, "project": self.project, "count": 3},
+            # Not in the top 5
+            {
+                "data": {
+                    "message": "sorta bad",
+                    "timestamp": iso_format(self.day_ago + timedelta(minutes=2)),
+                    "fingerprint": ["group5"],
+                    "user": {"email": "bar@example.com"},
+                },
+                "project": self.project,
+                "count": 2,
+            },
+            {
+                "data": {
+                    "message": "not so bad",
+                    "timestamp": iso_format(self.day_ago + timedelta(minutes=2)),
+                    "fingerprint": ["group5"],
+                    "user": {"email": "bar@example.com"},
+                },
+                "project": self.project,
+                "count": 1,
+            },
+        ]
+
+        self.events = []
+        for index, event_data in enumerate(self.event_data):
+            data = event_data["data"].copy()
+            for i in range(event_data["count"]):
+                data["event_id"] = "{}{}".format(index, i) * 16
+                event = self.store_event(data, project_id=event_data["project"].id)
+            self.events.append(event)
+        self.transaction = self.events[4]
+
+        self.url = reverse(
+            "sentry-api-0-organization-events-stats",
+            kwargs={"organization_slug": self.project.organization.slug},
+        )
+
+    def test_simple_top_events(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "field": ["count()", "message", "user.email"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            results = data[
+                ",".join([message, self.event_data[index]["data"]["user"].get("email", "None")])
+            ]
+            assert [{"count": self.event_data[index]["count"]}] in [
+                attrs for time, attrs in results["data"]
+            ]
+
+    def test_top_events_limits(self):
+        data = {
+            "start": iso_format(self.day_ago),
+            "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+            "interval": "1h",
+            "yAxis": "count()",
+            "orderby": ["-count()"],
+            "field": ["count()", "message", "user.email"],
+        }
+        with self.feature("organizations:discover-basic"):
+            data["topEvents"] = 50
+            response = self.client.get(self.url, data, format="json",)
+            assert response.status_code == 400
+
+            data["topEvents"] = 0
+            response = self.client.get(self.url, data, format="json",)
+            assert response.status_code == 400
+
+            data["topEvents"] = "a"
+            response = self.client.get(self.url, data, format="json",)
+            assert response.status_code == 400
+
+    def test_top_events_with_projects(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "field": ["count()", "message", "project"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            results = data[",".join([message, event.project.slug])]
+            assert [{"count": self.event_data[index]["count"]}] in [
+                attrs for time, attrs in results["data"]
+            ]
+
+    def test_top_events_with_issue(self):
+        # delete a group to make sure if this happens the value becomes unknown
+        event_group = self.events[0].group
+        event_group.delete()
+
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "field": ["count()", "message", "issue"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            # Because we deleted the group for event 0
+            if index == 0 or event.group is None:
+                issue = "unknown"
+            else:
+                issue = event.group.qualified_short_id
+
+            results = data[",".join([issue, message])]
+            assert [{"count": self.event_data[index]["count"]}] in [
+                attrs for time, attrs in results["data"]
+            ]
+
+    def test_top_events_with_functions(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-p99()"],
+                    "field": ["transaction", "avg(transaction.duration)", "p99()"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+
+        assert response.status_code == 200, response.content
+        assert len(data) == 1
+
+        results = data[self.transaction.transaction]
+        assert [attrs for time, attrs in results["data"]] == [
+            [{"count": 3}],
+            [{"count": 0}],
+        ]
+
+    def test_top_events_with_functions_on_different_transactions(self):
+        transaction_data = load_data("transaction")
+        transaction_data["start_timestamp"] = iso_format(self.day_ago + timedelta(minutes=2))
+        transaction_data["timestamp"] = iso_format(self.day_ago + timedelta(minutes=6))
+        transaction_data["transaction"] = "/foo_bar/"
+        transaction2 = self.store_event(transaction_data, project_id=self.project.id)
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-p99()"],
+                    "field": ["transaction", "avg(transaction.duration)", "p99()"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+
+        assert response.status_code == 200, response.content
+        assert len(data) == 2
+
+        results = data[self.transaction.transaction]
+        assert [attrs for time, attrs in results["data"]] == [
+            [{"count": 3}],
+            [{"count": 0}],
+        ]
+
+        results = data[transaction2.transaction]
+        assert [attrs for time, attrs in results["data"]] == [
+            [{"count": 1}],
+            [{"count": 0}],
+        ]
+
+    def test_top_events_with_query(self):
+        transaction_data = load_data("transaction")
+        transaction_data["start_timestamp"] = iso_format(self.day_ago + timedelta(minutes=2))
+        transaction_data["timestamp"] = iso_format(self.day_ago + timedelta(minutes=6))
+        transaction_data["transaction"] = "/foo_bar/"
+        self.store_event(transaction_data, project_id=self.project.id)
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-p99()"],
+                    "query": "transaction:/foo_bar/",
+                    "field": ["transaction", "avg(transaction.duration)", "p99()"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+
+        assert response.status_code == 200, response.content
+        assert len(data) == 1
+
+        transaction2_data = data["/foo_bar/"]
+        assert [attrs for time, attrs in transaction2_data["data"]] == [
+            [{"count": 1}],
+            [{"count": 0}],
+        ]
+
+    def test_top_events_with_rpm(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "rpm()",
+                    "orderby": ["-count()"],
+                    "field": ["message", "user.email", "count()"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            results = data[
+                ",".join([message, self.event_data[index]["data"]["user"].get("email", "None")])
+            ]
+            assert [{"count": self.event_data[index]["count"] / (3600.0 / 60.0)}] in [
+                attrs for time, attrs in results["data"]
+            ]
+
+    def test_top_events_with_multiple_yaxis(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": ["rpm()", "count()"],
+                    "orderby": ["-count()"],
+                    "field": ["message", "user.email", "count()"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            results = data[
+                ",".join([message, self.event_data[index]["data"]["user"].get("email", "None")])
+            ]
+            assert [{"count": self.event_data[index]["count"] / (3600.0 / 60.0)}] in [
+                attrs for time, attrs in results["rpm()"]["data"]
+            ]
+
+            assert [{"count": self.event_data[index]["count"]}] in [
+                attrs for time, attrs in results["count()"]["data"]
+            ]
+
+    def test_top_events_with_boolean(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "field": ["count()", "message", "device.charging"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:5]):
+            message = event.message or event.transaction
+            results = data[",".join(["False", message])]
+            assert [{"count": self.event_data[index]["count"]}] in [
+                attrs for time, attrs in results["data"]
+            ]
+
+    def test_top_events_with_timestamp(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "query": "event.type:default",
+                    "field": ["count()", "message", "timestamp"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 5
+
+        for index, event in enumerate(self.events[:6]):
+            if event.message:
+                results = data[",".join([event.message, event.timestamp])]
+                assert [{"count": self.event_data[index]["count"]}] in [
+                    attrs for time, attrs in results["data"]
+                ]
+
+    def test_top_events_with_int(self):
+        with self.feature("organizations:discover-basic"):
+            response = self.client.get(
+                self.url,
+                data={
+                    "start": iso_format(self.day_ago),
+                    "end": iso_format(self.day_ago + timedelta(hours=1, minutes=59)),
+                    "interval": "1h",
+                    "yAxis": "count()",
+                    "orderby": ["-count()"],
+                    "field": ["count()", "message", "transaction.duration"],
+                    "topEvents": 5,
+                },
+                format="json",
+            )
+
+        data = response.data
+        assert response.status_code == 200, response.content
+        assert len(data) == 1
+
+        results = data[",".join([self.transaction.transaction, "120000"])]
+        assert [attrs for time, attrs in results["data"]] == [
+            [{"count": 3}],
+            [{"count": 0}],
+        ]
