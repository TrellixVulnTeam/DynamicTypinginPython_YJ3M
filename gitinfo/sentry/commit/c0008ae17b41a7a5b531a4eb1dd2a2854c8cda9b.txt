commit c0008ae17b41a7a5b531a4eb1dd2a2854c8cda9b
Author: Armin Ronacher <armin.ronacher@active-4.com>
Date:   Wed Dec 20 19:41:35 2017 +0100

    feat: Add reprocessing revisions (#6794)
    
    We update a hash in the project option set every time we upload new
    debug symbols so that we do not get into concurrency issues when
    reprocessing issues are created.
    
    Before if a task was failing with a processing issue while the issue was
    resolve a processing issue could be persisted forever.  How instead of
    persisting a processing issue we immediately retry the task in case the
    reprocessing revision changed.

diff --git a/src/sentry/models/dsymfile.py b/src/sentry/models/dsymfile.py
index d7c38a07de..e8184f4819 100644
--- a/src/sentry/models/dsymfile.py
+++ b/src/sentry/models/dsymfile.py
@@ -34,7 +34,8 @@ from sentry.db.models import FlexibleForeignKey, Model, \
 from sentry.models.file import File
 from sentry.utils.zip import safe_extract_zip
 from sentry.constants import KNOWN_DSYM_TYPES
-from sentry.reprocessing import resolve_processing_issue
+from sentry.reprocessing import resolve_processing_issue, \
+    bump_reprocessing_revision
 
 
 logger = logging.getLogger(__name__)
@@ -352,6 +353,9 @@ def create_files_from_dsym_zip(fileobj, project,
                 symcache_update.delay(project_id=project.id,
                                       uuids=uuids_to_update)
 
+        # Uploading new dsysm changes the reprocessing revision
+        bump_reprocessing_revision(project)
+
         return rv
     finally:
         shutil.rmtree(scratchpad)
diff --git a/src/sentry/reprocessing.py b/src/sentry/reprocessing.py
index 22ad1fe925..e226226946 100644
--- a/src/sentry/reprocessing.py
+++ b/src/sentry/reprocessing.py
@@ -1,5 +1,34 @@
 from __future__ import absolute_import
 
+import uuid
+
+
+REPROCESSING_OPTION = 'sentry:processing-rev'
+
+
+def get_reprocessing_revision(project, cached=True):
+    """Returns the current revision of the projects reprocessing config set."""
+    from sentry.models import ProjectOption, Project
+    if cached:
+        return ProjectOption.objects.get_value(project, REPROCESSING_OPTION)
+    try:
+        if isinstance(project, Project):
+            project = project.id
+        return ProjectOption.objects.get(
+            project=project,
+            key=REPROCESSING_OPTION
+        )
+    except ProjectOption.DoesNotExist:
+        pass
+
+
+def bump_reprocessing_revision(project):
+    """Bumps the reprocessing revision."""
+    from sentry.models import ProjectOption
+    rev = uuid.uuid4().hex
+    ProjectOption.objects.set_value(project, REPROCESSING_OPTION, rev)
+    return rev
+
 
 def report_processing_issue(event_data, scope, object=None, type=None, data=None):
     """Reports a processing issue for a given scope and object.  Per
diff --git a/src/sentry/tasks/store.py b/src/sentry/tasks/store.py
index 2f4cc3ba9b..57dca96b9e 100644
--- a/src/sentry/tasks/store.py
+++ b/src/sentry/tasks/store.py
@@ -15,6 +15,7 @@ from raven.contrib.django.models import client as Raven
 from time import time
 from django.utils import timezone
 
+from sentry import reprocessing
 from sentry.cache import default_cache
 from sentry.tasks.base import instrumented_task
 from sentry.utils import metrics
@@ -31,6 +32,10 @@ info_logger = logging.getLogger('sentry.store')
 REPROCESSING_DEFAULT = False
 
 
+class RetryProcessing(Exception):
+    pass
+
+
 def should_process(data):
     """Quick check if processing is needed at all."""
     from sentry.plugins import plugins
@@ -99,7 +104,7 @@ def preprocess_event_from_reprocessing(
     )
 
 
-def _do_process_event(cache_key, start_time, event_id):
+def _do_process_event(cache_key, start_time, event_id, process_task):
     from sentry.plugins import plugins
 
     data = default_cache.get(cache_key)
@@ -115,6 +120,9 @@ def _do_process_event(cache_key, start_time, event_id):
     })
     has_changed = False
 
+    # Fetch the reprocessing revision
+    reprocessing_rev = reprocessing.get_reprocessing_revision(project)
+
     # Stacktrace based event processors.  These run before anything else.
     new_data = process_stacktraces(data)
     if new_data is not None:
@@ -137,9 +145,19 @@ def _do_process_event(cache_key, start_time, event_id):
 
     if has_changed:
         issues = data.get('processing_issues')
-        if issues and create_failed_event(
-            cache_key, project, list(issues.values()), event_id=event_id, start_time=start_time
-        ):
+        try:
+            if issues and create_failed_event(
+                cache_key, project, list(issues.values()),
+                event_id=event_id, start_time=start_time,
+                reprocessing_rev=reprocessing_rev
+            ):
+                return
+        except RetryProcessing:
+            # If `create_failed_event` indicates that we need to retry we
+            # invoke outselves again.  This happens when the reprocessing
+            # revision changed while we were processing.
+            process_task.delay(cache_key, start_time=start_time,
+                               event_id=event_id)
             return
 
         default_cache.set(cache_key, data, 3600)
@@ -156,7 +174,7 @@ def _do_process_event(cache_key, start_time, event_id):
     soft_time_limit=60,
 )
 def process_event(cache_key, start_time=None, event_id=None, **kwargs):
-    return _do_process_event(cache_key, start_time, event_id)
+    return _do_process_event(cache_key, start_time, event_id, process_event)
 
 
 @instrumented_task(
@@ -166,7 +184,8 @@ def process_event(cache_key, start_time=None, event_id=None, **kwargs):
     soft_time_limit=60,
 )
 def process_event_from_reprocessing(cache_key, start_time=None, event_id=None, **kwargs):
-    return _do_process_event(cache_key, start_time, event_id)
+    return _do_process_event(cache_key, start_time, event_id,
+                             process_event_from_reprocessing)
 
 
 def delete_raw_event(project_id, event_id, allow_hint_clear=False):
@@ -193,7 +212,8 @@ def delete_raw_event(project_id, event_id, allow_hint_clear=False):
                 ProjectOption.objects.set_value(project, 'sentry:sent_failed_event_hint', False)
 
 
-def create_failed_event(cache_key, project_id, issues, event_id, start_time=None):
+def create_failed_event(cache_key, project_id, issues, event_id, start_time=None,
+                        reprocessing_rev=None):
     """If processing failed we put the original data from the cache into a
     raw event.  Returns `True` if a failed event was inserted
     """
@@ -201,6 +221,16 @@ def create_failed_event(cache_key, project_id, issues, event_id, start_time=None
         project_id, 'sentry:reprocessing_active', REPROCESSING_DEFAULT
     )
 
+    # In case there is reprocessing active but the current reprocessing
+    # revision is already different than when we started, we want to
+    # immediately retry the event.  This resolves the problem when
+    # otherwise a concurrent change of debug symbols might leave a
+    # reprocessing issue stuck in the project forever.
+    if reprocessing_active and \
+       reprocessing.get_reprocessing_revision(project_id, cached=False) != \
+       reprocessing_rev:
+        raise RetryProcessing()
+
     # The first time we encounter a failed event and the hint was cleared
     # we send a notification.
     sent_notification = ProjectOption.objects.get_value(
