commit ed934dac5390ee6c3f4eedeb3ad559ce7458a3cb
Author: Markus Unterwaditzer <markus@unterwaditzer.net>
Date:   Fri Oct 11 09:40:08 2019 +0200

    ref(ingest): Use batching-kafka-consumer (#15010)
    
    * ref: Use batching ingest consumer
    
    * fix: Skip tests if confluent_kafka is not installed
    
    * fix: Remove useless code
    
    * ref: Vendor batching_kafka_consumer
    
    * fix: Reduce timeout for tests
    
    * fix: Return something in consumers to honor batch size
    
    * ref: Move magic int literal into magic constant
    
    * fix: Emit basic timing metrics
    
    * fix: Add topic_name as tag to metrics
    
    * fix: Fix None-deref
    
    * feat(kafka-consumer): Add consumer group as tag, if available

diff --git a/src/sentry/ingest/ingest_consumer.py b/src/sentry/ingest/ingest_consumer.py
index 8f5b3c60de..0b8288f8cf 100644
--- a/src/sentry/ingest/ingest_consumer.py
+++ b/src/sentry/ingest/ingest_consumer.py
@@ -3,6 +3,8 @@ from __future__ import absolute_import
 import logging
 import msgpack
 
+from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
+
 from django.conf import settings
 from django.core.cache import cache
 
@@ -12,7 +14,7 @@ from sentry.models import Project
 from sentry.signals import event_accepted
 from sentry.tasks.store import preprocess_event
 from sentry.utils import json
-from sentry.utils.kafka import SimpleKafkaConsumer
+from sentry.utils.kafka import create_batching_kafka_consumer
 
 logger = logging.getLogger(__name__)
 
@@ -37,7 +39,7 @@ class ConsumerType(object):
         raise ValueError("Invalid consumer type", consumer_type)
 
 
-class IngestConsumer(SimpleKafkaConsumer):
+class IngestConsumerWorker(AbstractBatchWorker):
     def process_message(self, message):
         message = msgpack.unpackb(message.value(), use_list=False)
         payload = message["payload"]
@@ -87,43 +89,23 @@ class IngestConsumer(SimpleKafkaConsumer):
             ip=remote_addr, data=data, project=project, sender=self.process_message
         )
 
+        # Return *something* so that it counts against batch size
+        return True
+
+    def flush_batch(self, batch):
+        pass
+
+    def shutdown(self):
+        pass
+
 
-def run_ingest_consumer(
-    commit_batch_size,
-    consumer_group,
-    consumer_type,
-    max_fetch_time_seconds,
-    initial_offset_reset="latest",
-    is_shutdown_requested=lambda: False,
-):
+def get_ingest_consumer(consumer_type, once=False, **options):
     """
     Handles events coming via a kafka queue.
 
     The events should have already been processed (normalized... ) upstream (by Relay).
-
-    :param commit_batch_size: the number of message the consumer will try to process/commit in one loop
-    :param consumer_group: kafka consumer group name
-    :param consumer_type: an enumeration defining the types of ingest messages see `ConsumerType`
-    :param max_fetch_time_seconds: the maximum number of seconds a consume operation will be blocked waiting
-        for the specified commit_batch_size number of messages to appear in the queue before it returns. At the
-        end of the specified time the consume operation will return however many messages it has ( including
-        an empty array if no new messages are available).
-    :param initial_offset_reset: offset reset policy when there's no available offset for the consumer
-    :param is_shutdown_requested: Callable[[],bool] predicate checked after each loop, if it returns
-        True the forwarder stops (by default is lambda: False). In normal operation this should be left to default.
-        For unit testing it offers a way to cleanly stop the forwarder after some particular condition is achieved.
     """
-    logger.debug("Starting ingest-consumer...")
     topic_name = ConsumerType.get_topic_name(consumer_type)
-
-    ingest_consumer = IngestConsumer(
-        commit_batch_size=commit_batch_size,
-        consumer_group=consumer_group,
-        topic_name=topic_name,
-        max_fetch_time_seconds=max_fetch_time_seconds,
-        initial_offset_reset=initial_offset_reset,
+    return create_batching_kafka_consumer(
+        topic_name=topic_name, worker=IngestConsumerWorker(), **options
     )
-
-    ingest_consumer.run(is_shutdown_requested)
-
-    logger.debug("ingest-consumer terminated.")
diff --git a/src/sentry/ingest/outcomes_consumer.py b/src/sentry/ingest/outcomes_consumer.py
index 5e47ec7dc6..c6ac6b9edf 100644
--- a/src/sentry/ingest/outcomes_consumer.py
+++ b/src/sentry/ingest/outcomes_consumer.py
@@ -19,12 +19,14 @@ from __future__ import absolute_import
 
 import logging
 
+from sentry.utils.batching_kafka_consumer import AbstractBatchWorker
+
 from django.conf import settings
 from django.core.cache import cache
 
 from sentry.models.project import Project
 from sentry.signals import event_filtered, event_dropped
-from sentry.utils.kafka import SimpleKafkaConsumer
+from sentry.utils.kafka import create_batching_kafka_consumer
 from sentry.utils import json
 from sentry.utils.outcomes import Outcome
 
@@ -57,7 +59,7 @@ def is_signal_sent(project_id, event_id):
     return cache.get(key, None) is not None
 
 
-class OutcomesConsumer(SimpleKafkaConsumer):
+class OutcomesConsumerWorker(AbstractBatchWorker):
     def process_message(self, message):
         msg = json.loads(message.value())
 
@@ -92,40 +94,21 @@ class OutcomesConsumer(SimpleKafkaConsumer):
         # remember that we sent the signal just in case the processor dies before
         mark_signal_sent(project_id=project_id, event_id=event_id)
 
+        # Return *something* so that it counts against batch size
+        return True
+
+    def flush_batch(self, batch):
+        pass
+
+    def shutdown(self):
+        pass
+
 
-def run_outcomes_consumer(
-    commit_batch_size,
-    consumer_group,
-    max_fetch_time_seconds,
-    initial_offset_reset,
-    is_shutdown_requested=lambda: False,
-):
+def get_outcomes_consumer(**options):
     """
     Handles outcome requests coming via a kafka queue from Relay.
-
-    :param commit_batch_size: the number of message the consumer will try to process/commit in one loop
-    :param consumer_group: kafka consumer group name
-    :param max_fetch_time_seconds: the maximum number of seconds a consume operation will be blocked waiting
-        for the specified commit_batch_size number of messages to appear in the queue before it returns. At the
-        end of the specified time the consume operation will return however many messages it has ( including
-        an empty array if no new messages are available).
-    :param initial_offset_reset: offset reset policy when there's no available offset for the consumer
-    :param is_shutdown_requested: Callable[[],bool] predicate checked after each loop, if it returns
-        True the forwarder stops (by default is lambda: False). In normal operation this should be left to default.
-        For unit testing it offers a way to cleanly stop the forwarder after some particular condition is achieved.
     """
 
-    logger.debug("Starting outcomes-consumer...")
-    topic_name = settings.KAFKA_OUTCOMES
-
-    outcomes_consumer = OutcomesConsumer(
-        commit_batch_size=commit_batch_size,
-        consumer_group=consumer_group,
-        topic_name=topic_name,
-        max_fetch_time_seconds=max_fetch_time_seconds,
-        initial_offset_reset=initial_offset_reset,
+    return create_batching_kafka_consumer(
+        topic_name=settings.KAFKA_OUTCOMES, worker=OutcomesConsumerWorker(), **options
     )
-
-    outcomes_consumer.run(is_shutdown_requested)
-
-    logger.debug("outcomes-consumer terminated.")
diff --git a/src/sentry/runner/commands/run.py b/src/sentry/runner/commands/run.py
index 552df73c1e..12536a8a7a 100644
--- a/src/sentry/runner/commands/run.py
+++ b/src/sentry/runner/commands/run.py
@@ -356,47 +356,71 @@ def query_subscription_consumer(**options):
     subscriber.run()
 
 
+def batching_kafka_options(group):
+    """
+    Expose batching_kafka_consumer options as CLI args.
+
+    TODO(markus): Probably want to have this as part of batching_kafka_consumer
+    as this is duplicated effort between Snuba and Sentry.
+    """
+
+    def inner(f):
+        f = click.option(
+            "--consumer-group",
+            "group_id",
+            default=group,
+            help="Kafka consumer group for the outcomes consumer. ",
+        )(f)
+
+        f = click.option(
+            "--max-batch-size",
+            "max_batch_size",
+            default=100,
+            type=int,
+            help="How many messages to process before committing offsets.",
+        )(f)
+
+        f = click.option(
+            "--max-batch-time-ms",
+            "max_batch_time",
+            default=1000,
+            type=int,
+            help="How long to batch for before committing offsets.",
+        )(f)
+
+        f = click.option(
+            "--auto-offset-reset",
+            "auto_offset_reset",
+            default="latest",
+            type=click.Choice(["earliest", "latest", "error"]),
+            help="Position in the commit log topic to begin reading from when no prior offset has been recorded.",
+        )(f)
+
+        return f
+
+    return inner
+
+
 @run.command("ingest-consumer")
 @log_options()
 @click.option(
     "--consumer-type",
     default=None,
+    required=True,
     help="Specify which type of consumer to create, i.e. from which topic to consume messages.",
     type=click.Choice(["events", "transactions", "attachments"]),
 )
-@click.option(
-    "--group", default="ingest-consumer", help="Kafka consumer group for the ingest consumer. "
-)
-@click.option(
-    "--commit-batch-size",
-    default=100,
-    type=int,
-    help="How many messages to process before committing offsets.",
-)
-@click.option(
-    "--max-fetch-time-ms",
-    default=100,
-    type=int,
-    help="Timeout (in milliseconds) for a consume operation. Max time the kafka consumer will wait "
-    "before returning the available messages in the topic.",
-)
-@click.option(
-    "--initial-offset-reset",
-    default="latest",
-    type=click.Choice(["earliest", "latest", "error"]),
-    help="Position in the commit log topic to begin reading from when no prior offset has been recorded.",
-)
+@batching_kafka_options("ingest-consumer")
 @configuration
-def ingest_consumer(**options):
+def ingest_consumer(consumer_type, **options):
     """
     Runs an "ingest consumer" task.
 
     The "ingest consumer" tasks read events from a kafka topic (coming from Relay) and schedules
     process event celery tasks for them
     """
-    from sentry.ingest.ingest_consumer import ConsumerType, run_ingest_consumer
+    from sentry.ingest.ingest_consumer import ConsumerType, get_ingest_consumer
 
-    consumer_type = options["consumer_type"]
     if consumer_type == "events":
         consumer_type = ConsumerType.Events
     elif consumer_type == "transactions":
@@ -404,41 +428,12 @@ def ingest_consumer(**options):
     elif consumer_type == "attachments":
         consumer_type = ConsumerType.Attachments
 
-    max_fetch_time_seconds = options["max_fetch_time_ms"] / 1000.0
-
-    run_ingest_consumer(
-        commit_batch_size=options["commit_batch_size"],
-        consumer_group=options["group"],
-        consumer_type=consumer_type,
-        max_fetch_time_seconds=max_fetch_time_seconds,
-        initial_offset_reset=options["initial_offset_reset"],
-    )
+    get_ingest_consumer(consumer_type=consumer_type, **options).run()
 
 
 @run.command("outcomes-consumer")
 @log_options()
-@click.option(
-    "--group", default="outcomes-consumer", help="Kafka consumer group for the outcomes consumer. "
-)
-@click.option(
-    "--commit-batch-size",
-    default=100,
-    type=int,
-    help="How many messages to process before committing offsets.",
-)
-@click.option(
-    "--max-fetch-time-ms",
-    default=100,
-    type=int,
-    help="Timeout (in milliseconds) for a consume operation. Max time the kafka consumer will wait "
-    "before returning the available messages in the topic.",
-)
-@click.option(
-    "--initial-offset-reset",
-    default="latest",
-    type=click.Choice(["earliest", "latest", "error"]),
-    help="Position in the commit log topic to begin reading from when no prior offset has been recorded.",
-)
+@batching_kafka_options("outcomes-consumer")
 @configuration
 def outcome_consumer(**options):
     """
@@ -447,13 +442,6 @@ def outcome_consumer(**options):
     The "outcomes consumer" tasks read outcomes from a kafka topic and sends
     signals for some of them.
     """
-    from sentry.ingest.outcomes_consumer import run_outcomes_consumer
-
-    max_fetch_time_seconds = options["max_fetch_time_ms"] / 1000.0
+    from sentry.ingest.outcomes_consumer import get_outcomes_consumer
 
-    run_outcomes_consumer(
-        commit_batch_size=options["commit_batch_size"],
-        consumer_group=options["group"],
-        max_fetch_time_seconds=max_fetch_time_seconds,
-        initial_offset_reset=options["initial_offset_reset"],
-    )
+    get_outcomes_consumer(**options).run()
diff --git a/src/sentry/utils/batching_kafka_consumer.py b/src/sentry/utils/batching_kafka_consumer.py
new file mode 100644
index 0000000000..78253e770e
--- /dev/null
+++ b/src/sentry/utils/batching_kafka_consumer.py
@@ -0,0 +1,397 @@
+from __future__ import absolute_import
+
+import abc
+import logging
+import six
+import time
+
+from confluent_kafka import (
+    Consumer,
+    KafkaError,
+    KafkaException,
+    OFFSET_BEGINNING,
+    OFFSET_END,
+    OFFSET_STORED,
+    OFFSET_INVALID,
+)
+
+
+logger = logging.getLogger("batching-kafka-consumer")
+
+
+DEFAULT_QUEUED_MAX_MESSAGE_KBYTES = 50000
+DEFAULT_QUEUED_MIN_MESSAGES = 10000
+
+
+@six.add_metaclass(abc.ABCMeta)
+class AbstractBatchWorker(object):
+    """The `BatchingKafkaConsumer` requires an instance of this class to
+    handle user provided work such as processing raw messages and flushing
+    processed batches to a custom backend."""
+
+    @abc.abstractmethod
+    def process_message(self, message):
+        """Called with each (raw) Kafka message, allowing the worker to do
+        incremental (preferablly local!) work on events. The object returned
+        is put into the batch maintained by the `BatchingKafkaConsumer`.
+
+        If this method returns `None` it is not added to the batch.
+
+        A simple example would be decoding the JSON value and extracting a few
+        fields.
+        """
+        pass
+
+    @abc.abstractmethod
+    def flush_batch(self, batch):
+        """Called with a list of pre-processed (by `process_message`) objects.
+        The worker should write the batch of processed messages into whatever
+        store(s) it is maintaining. Afterwards the Kafka offsets are committed.
+
+        A simple example would be writing the batch to another Kafka topic.
+        """
+        pass
+
+    @abc.abstractmethod
+    def shutdown(self):
+        """Called when the `BatchingKafkaConsumer` is shutting down (because it
+        was signalled to do so). Provides the worker a chance to do any final
+        cleanup.
+
+        A simple example would be closing any remaining backend connections."""
+        pass
+
+
+class BatchingKafkaConsumer(object):
+    """The `BatchingKafkaConsumer` is an abstraction over most Kafka consumer's main event
+    loops. For this reason it uses inversion of control: the user provides an implementation
+    for the `AbstractBatchWorker` and then the `BatchingKafkaConsumer` handles the rest.
+
+    Main differences from the default KafkaConsumer are as follows:
+    * Messages are processed locally (e.g. not written to an external datastore!) as they are
+      read from Kafka, then added to an in-memory batch
+    * Batches are flushed based on the batch size or time sent since the first message
+      in the batch was recieved (e.g. "500 items or 1000ms")
+    * Kafka offsets are not automatically committed! If they were, offsets might be committed
+      for messages that are still sitting in an in-memory batch, or they might *not* be committed
+      when messages are sent to an external datastore right before the consumer process dies
+    * Instead, when a batch of items is flushed they are written to the external datastore and
+      then Kafka offsets are immediately committed (in the same thread/loop)
+    * Users need only provide an implementation of what it means to process a raw message
+      and flush a batch of events
+    * Supports an optional "dead letter topic" where messages that raise an exception during
+      `process_message` are sent so as not to block the pipeline.
+
+    NOTE: This does not eliminate the possibility of duplicates if the consumer process
+    crashes between writing to its backend and commiting Kafka offsets. This should eliminate
+    the possibility of *losing* data though. An "exactly once" consumer would need to store
+    offsets in the external datastore and reconcile them on any partition rebalance.
+    """
+
+    # Set of logical (not literal) offsets to not publish to the commit log.
+    # https://github.com/confluentinc/confluent-kafka-python/blob/443177e1c83d9b66ce30f5eb8775e062453a738b/tests/test_enums.py#L22-L25
+    LOGICAL_OFFSETS = frozenset([OFFSET_BEGINNING, OFFSET_END, OFFSET_STORED, OFFSET_INVALID])
+
+    # Set of error codes that can be returned by ``consumer.poll`` calls which
+    # are generally able to be recovered from after a series of retries.
+    RECOVERABLE_ERRORS = frozenset(
+        [KafkaError._PARTITION_EOF, KafkaError._TRANSPORT]  # Local: Broker transport failure
+    )
+
+    def __init__(
+        self,
+        topics,
+        worker,
+        max_batch_size,
+        max_batch_time,
+        bootstrap_servers,
+        group_id,
+        metrics=None,
+        producer=None,
+        dead_letter_topic=None,
+        commit_log_topic=None,
+        auto_offset_reset="error",
+        queued_max_messages_kbytes=DEFAULT_QUEUED_MAX_MESSAGE_KBYTES,
+        queued_min_messages=DEFAULT_QUEUED_MIN_MESSAGES,
+        metrics_sample_rates=None,
+        metrics_default_tags=None,
+    ):
+        assert isinstance(worker, AbstractBatchWorker)
+        self.worker = worker
+
+        self.max_batch_size = max_batch_size
+        self.max_batch_time = max_batch_time  # in milliseconds
+        self.__metrics = metrics
+        self.__metrics_sample_rates = (
+            metrics_sample_rates if metrics_sample_rates is not None else {}
+        )
+        self.__metrics_default_tags = metrics_default_tags or {}
+        self.group_id = group_id
+
+        self.shutdown = False
+
+        self.__batch_results = []
+        self.__batch_offsets = {}  # (topic, partition) = [low, high]
+        self.__batch_deadline = None
+        self.__batch_messages_processed_count = 0
+        # the total amount of time, in milliseconds, that it took to process
+        # the messages in this batch (does not included time spent waiting for
+        # new messages)
+        self.__batch_processing_time_ms = 0.0
+
+        if not isinstance(topics, (list, tuple)):
+            topics = [topics]
+        elif isinstance(topics, tuple):
+            topics = list(topics)
+
+        self.consumer = self.create_consumer(
+            topics,
+            bootstrap_servers,
+            group_id,
+            auto_offset_reset,
+            queued_max_messages_kbytes,
+            queued_min_messages,
+        )
+
+        self.producer = producer
+        self.commit_log_topic = commit_log_topic
+        self.dead_letter_topic = dead_letter_topic
+
+    def __record_timing(self, metric, value, tags=None):
+        if self.__metrics is None:
+            return
+
+        tags = dict(tags or ())
+        tags.update(self.__metrics_default_tags)
+
+        return self.__metrics.timing(
+            metric, value, tags=tags, sample_rate=self.__metrics_sample_rates.get(metric, 1)
+        )
+
+    def create_consumer(
+        self,
+        topics,
+        bootstrap_servers,
+        group_id,
+        auto_offset_reset,
+        queued_max_messages_kbytes,
+        queued_min_messages,
+    ):
+
+        consumer_config = {
+            "enable.auto.commit": False,
+            "bootstrap.servers": ",".join(bootstrap_servers),
+            "group.id": group_id,
+            "default.topic.config": {"auto.offset.reset": auto_offset_reset},
+            # overridden to reduce memory usage when there's a large backlog
+            "queued.max.messages.kbytes": queued_max_messages_kbytes,
+            "queued.min.messages": queued_min_messages,
+        }
+
+        consumer = Consumer(consumer_config)
+
+        def on_partitions_assigned(consumer, partitions):
+            logger.info("New partitions assigned: %r", partitions)
+
+        def on_partitions_revoked(consumer, partitions):
+            "Reset the current in-memory batch, letting the next consumer take over where we left off."
+            logger.info("Partitions revoked: %r", partitions)
+            self._flush(force=True)
+
+        consumer.subscribe(
+            topics, on_assign=on_partitions_assigned, on_revoke=on_partitions_revoked
+        )
+
+        return consumer
+
+    def run(self):
+        "The main run loop, see class docstring for more information."
+
+        logger.debug("Starting")
+        while not self.shutdown:
+            self._run_once()
+
+        self._shutdown()
+
+    def _run_once(self):
+        self._flush()
+
+        if self.producer:
+            self.producer.poll(0.0)
+
+        msg = self.consumer.poll(timeout=1.0)
+
+        if msg is None:
+            return
+        if msg.error():
+            if msg.error().code() in self.RECOVERABLE_ERRORS:
+                return
+            else:
+                raise Exception(msg.error())
+
+        self._handle_message(msg)
+
+    def signal_shutdown(self):
+        """Tells the `BatchingKafkaConsumer` to shutdown on the next run loop iteration.
+        Typically called from a signal handler."""
+        logger.debug("Shutdown signalled")
+
+        self.shutdown = True
+
+    def _handle_message(self, msg):
+        start = time.time()
+
+        # set the deadline only after the first message for this batch is seen
+        if not self.__batch_deadline:
+            self.__batch_deadline = self.max_batch_time / 1000.0 + start
+
+        try:
+            result = self.worker.process_message(msg)
+        except Exception:
+            if self.dead_letter_topic:
+                logger.exception("Error handling message, sending to dead letter topic.")
+                self.producer.produce(
+                    self.dead_letter_topic,
+                    key=msg.key(),
+                    value=msg.value(),
+                    headers={
+                        "partition": six.text_type(msg.partition()) if msg.partition() else None,
+                        "offset": six.text_type(msg.offset()) if msg.offset() else None,
+                        "topic": msg.topic(),
+                    },
+                    on_delivery=self._commit_message_delivery_callback,
+                )
+            else:
+                raise
+        else:
+            if result is not None:
+                self.__batch_results.append(result)
+        finally:
+            duration = (time.time() - start) * 1000
+            self.__batch_messages_processed_count += 1
+            self.__batch_processing_time_ms += duration
+            self.__record_timing("process_message", duration)
+
+            topic_partition_key = (msg.topic(), msg.partition())
+            if topic_partition_key in self.__batch_offsets:
+                self.__batch_offsets[topic_partition_key][1] = msg.offset()
+            else:
+                self.__batch_offsets[topic_partition_key] = [msg.offset(), msg.offset()]
+
+    def _shutdown(self):
+        logger.debug("Stopping")
+
+        # drop in-memory events, letting the next consumer take over where we left off
+        self._reset_batch()
+
+        # tell the consumer to shutdown, and close the consumer
+        logger.debug("Stopping worker")
+        self.worker.shutdown()
+        logger.debug("Stopping consumer")
+        self.consumer.close()
+        logger.debug("Stopped")
+
+    def _reset_batch(self):
+        logger.debug("Resetting in-memory batch")
+        self.__batch_results = []
+        self.__batch_offsets = {}
+        self.__batch_deadline = None
+        self.__batch_messages_processed_count = 0
+        self.__batch_processing_time_ms = 0.0
+
+    def _flush(self, force=False):
+        """Decides whether the `BatchingKafkaConsumer` should flush because of either
+        batch size or time. If so, delegate to the worker, clear the current batch,
+        and commit offsets to Kafka."""
+        if not self.__batch_messages_processed_count > 0:
+            return  # No messages were processed, so there's nothing to do.
+
+        batch_by_size = len(self.__batch_results) >= self.max_batch_size
+        batch_by_time = self.__batch_deadline and time.time() > self.__batch_deadline
+        if not (force or batch_by_size or batch_by_time):
+            return
+
+        logger.info(
+            "Flushing %s items (from %r): forced:%s size:%s time:%s",
+            len(self.__batch_results),
+            self.__batch_offsets,
+            force,
+            batch_by_size,
+            batch_by_time,
+        )
+
+        self.__record_timing(
+            "process_message.normalized",
+            self.__batch_processing_time_ms / self.__batch_messages_processed_count,
+        )
+
+        batch_results_length = len(self.__batch_results)
+        if batch_results_length > 0:
+            logger.debug("Flushing batch via worker")
+            flush_start = time.time()
+            self.worker.flush_batch(self.__batch_results)
+            flush_duration = (time.time() - flush_start) * 1000
+            logger.info("Worker flush took %dms", flush_duration)
+            self.__record_timing("batch.flush", flush_duration)
+            self.__record_timing("batch.flush.normalized", flush_duration / batch_results_length)
+
+        logger.debug("Committing Kafka offsets")
+        commit_start = time.time()
+        self._commit()
+        commit_duration = (time.time() - commit_start) * 1000
+        logger.debug("Kafka offset commit took %dms", commit_duration)
+
+        self._reset_batch()
+
+    def _commit_message_delivery_callback(self, error, message):
+        if error is not None:
+            raise Exception(error.str())
+
+    def _commit(self):
+        retries = 3
+        while True:
+            try:
+                offsets = self.consumer.commit(asynchronous=False)
+                logger.debug("Committed offsets: %s", offsets)
+                break  # success
+            except KafkaException as e:
+                if e.args[0].code() in (
+                    KafkaError.REQUEST_TIMED_OUT,
+                    KafkaError.NOT_COORDINATOR_FOR_GROUP,
+                    KafkaError._WAIT_COORD,
+                ):
+                    logger.warning("Commit failed: %s (%d retries)", e, retries)
+                    if retries <= 0:
+                        raise
+                    retries -= 1
+                    time.sleep(1)
+                    continue
+                else:
+                    raise
+
+        if self.commit_log_topic:
+            for item in offsets:
+                if item.offset in self.LOGICAL_OFFSETS:
+                    logger.debug(
+                        "Skipped publishing logical offset (%r) to commit log for %s/%s",
+                        item.offset,
+                        item.topic,
+                        item.partition,
+                    )
+                    continue
+                elif item.offset < 0:
+                    logger.warning(
+                        "Found unexpected negative offset (%r) after commit for %s/%s",
+                        item.offset,
+                        item.topic,
+                        item.partition,
+                    )
+
+                self.producer.produce(
+                    self.commit_log_topic,
+                    key="{}:{}:{}".format(item.topic, item.partition, self.group_id).encode(
+                        "utf-8"
+                    ),
+                    value="{}".format(item.offset).encode("utf-8"),
+                    on_delivery=self._commit_message_delivery_callback,
+                )
diff --git a/src/sentry/utils/kafka.py b/src/sentry/utils/kafka.py
index 572bddefd8..230ccdac8c 100644
--- a/src/sentry/utils/kafka.py
+++ b/src/sentry/utils/kafka.py
@@ -1,16 +1,13 @@
 from __future__ import absolute_import
 
-import abc
 import logging
-import signal as os_signal
-from contextlib import contextmanager
-
-import six
-import confluent_kafka as kafka
-from django.conf import settings
+import signal
 
+from sentry.utils.batching_kafka_consumer import BatchingKafkaConsumer
 from sentry.utils import metrics
 
+from django.conf import settings
+
 logger = logging.getLogger(__name__)
 
 
@@ -41,139 +38,25 @@ class ProducerManager(object):
 producers = ProducerManager()
 
 
-@contextmanager
-def set_termination_request_handlers(handler):
-    # hook the new handlers
-    old_sigint = os_signal.signal(os_signal.SIGINT, handler)
-    old_sigterm = os_signal.signal(os_signal.SIGTERM, handler)
-    try:
-        # run the code inside the with context ( with the hooked handler)
-        yield
-    finally:
-        # restore the old handlers when exiting the with context
-        os_signal.signal(os_signal.SIGINT, old_sigint)
-        os_signal.signal(os_signal.SIGTERM, old_sigterm)
-
-
-@six.add_metaclass(abc.ABCMeta)
-class SimpleKafkaConsumer(object):
-    def __init__(
-        self,
-        commit_batch_size,
-        consumer_group,
-        topic_name,
-        max_fetch_time_seconds,
-        initial_offset_reset="latest",
-        consumer_configuration=None,
-    ):
-        """
-        Base class for implementing kafka consumers.
-        """
-
-        self.commit_batch_size = commit_batch_size
-        self.topic_name = topic_name
-        self.max_fetch_time_seconds = max_fetch_time_seconds
-        self.initial_offset_reset = initial_offset_reset
-        self.consumer_group = consumer_group
-
-        if self.commit_batch_size <= 0:
-            raise ValueError("Commit batch size must be a positive integer")
-
-        cluster_name = settings.KAFKA_TOPICS[topic_name]["cluster"]
-        bootstrap_servers = settings.KAFKA_CLUSTERS[cluster_name]["bootstrap.servers"]
-
-        self.consumer_configuration = {
-            "bootstrap.servers": bootstrap_servers,
-            "group.id": consumer_group,
-            "enable.auto.commit": "false",  # we commit manually
-            "enable.auto.offset.store": "true",  # we let the broker keep count of the current offset (when committing)
-            "enable.partition.eof": "false",  # stop EOF errors when we read all messages in the topic
-            "default.topic.config": {"auto.offset.reset": initial_offset_reset},
-        }
-
-        if consumer_configuration is not None:
-            for key in six.iterkeys(consumer_configuration):
-                self.consumer_configuration[key] = consumer_configuration[key]
-
-    @abc.abstractmethod
-    def process_message(self, message):
-        """
-        This function is called for each message
-        :param message: the kafka message:
-        """
-        pass
-
-    def run(self, is_shutdown_requested=lambda: False):
-        """
-        Runs the message processing loop
-        """
-        logger.debug(
-            "Staring kafka consumer for topic:%s with consumer group:%s",
-            self.topic_name,
-            self.consumer_group,
-        )
-
-        consumer = kafka.Consumer(self.consumer_configuration)
-        consumer.subscribe([self.topic_name])
-
-        metrics_tags = {
-            "topic": self.topic_name,
-            "consumer_group": self.consumer_group,
-            "type": self.__class__.__name__,
-        }
-
-        # setup a flag to mark termination signals received, see below why we use an array
-        termination_signal_received = [False]
-
-        def termination_signal_handler(_sig_id, _frame):
-            """
-            Function to use a hook for SIGINT and SIGTERM
-
-            This signal handler only remembers that the signal was emitted.
-            The batch processing loop detects that the signal was emitted
-            and stops once the whole batch is processed.
-            """
-            # We need to use an array so that terminal_signal_received is not a
-            # local variable assignment, but a lookup in the clojure's outer scope.
-            termination_signal_received[0] = True
-
-        with set_termination_request_handlers(termination_signal_handler):
-            while not (is_shutdown_requested() or termination_signal_received[0]):
-                # get up to commit_batch_size messages
-                messages = consumer.consume(
-                    num_messages=self.commit_batch_size, timeout=self.max_fetch_time_seconds
-                )
-
-                for message in messages:
-                    message_error = message.error()
-                    if message_error is not None:
-                        logger.error(
-                            "Received message with error on %s: %s", self.topic_name, message_error
-                        )
-                        raise ValueError(
-                            "Bad message received from consumer", self.topic_name, message_error
-                        )
-
-                    with metrics.timer("simple_consumer.processing_time", tags=metrics_tags):
-                        self.process_message(message)
-
-                if len(messages) > 0:
-                    # we have read some messages in the previous consume, commit the offset
-                    consumer.commit(asynchronous=False)
-
-                metrics.timing(
-                    "simple_consumer.committed_batch.size", len(messages), tags=metrics_tags
-                )
-                # Value between 0.0 and 1.0 that can help to estimate the consumer bandwidth/usage
-                metrics.timing(
-                    "simple_consumer.batch_capacity.usage",
-                    1.0 * len(messages) / self.commit_batch_size,
-                    tags=metrics_tags,
-                )
-
-        consumer.close()
-        logger.debug(
-            "Closing kafka consumer for topic:%s with consumer group:%s",
-            self.topic_name,
-            self.consumer_group,
-        )
+def create_batching_kafka_consumer(topic_name, worker, **options):
+    cluster_name = settings.KAFKA_TOPICS[topic_name]["cluster"]
+    bootstrap_servers = settings.KAFKA_CLUSTERS[cluster_name]["bootstrap.servers"]
+    if not isinstance(bootstrap_servers, (list, tuple)):
+        bootstrap_servers = bootstrap_servers.split(",")
+
+    consumer = BatchingKafkaConsumer(
+        topics=[topic_name],
+        bootstrap_servers=bootstrap_servers,
+        worker=worker,
+        metrics=metrics,
+        metrics_default_tags={"topic": topic_name, "group_id": options.get("group_id")},
+        **options
+    )
+
+    def handler(signum, frame):
+        consumer.signal_shutdown()
+
+    signal.signal(signal.SIGINT, handler)
+    signal.signal(signal.SIGTERM, handler)
+
+    return consumer
diff --git a/src/sentry/utils/pytest/kafka.py b/src/sentry/utils/pytest/kafka.py
index b6b8adac87..f348f16069 100644
--- a/src/sentry/utils/pytest/kafka.py
+++ b/src/sentry/utils/pytest/kafka.py
@@ -1,5 +1,7 @@
 from __future__ import absolute_import
 
+import os
+
 import pytest
 import six
 from confluent_kafka.admin import AdminClient
@@ -56,3 +58,11 @@ def kafka_admin(request):
         return _KafkaAdminWrapper(request, settings)
 
     return inner
+
+
+@pytest.fixture
+def requires_kafka():
+    pytest.importorskip("confluent_kafka")
+
+    if "SENTRY_KAFKA_HOSTS" not in os.environ:
+        pytest.xfail("test requires SENTRY_KAFKA_HOSTS environment variable which is not set")
diff --git a/tests/sentry/eventstream/kafka/test_consumer.py b/tests/sentry/eventstream/kafka/test_consumer.py
index 915cea7ef7..e785b460d2 100644
--- a/tests/sentry/eventstream/kafka/test_consumer.py
+++ b/tests/sentry/eventstream/kafka/test_consumer.py
@@ -1,6 +1,5 @@
 from __future__ import absolute_import
 
-import functools
 import os
 import subprocess
 import uuid
@@ -14,10 +13,8 @@ from six.moves import xrange
 try:
     from confluent_kafka import Consumer, KafkaError, Producer, TopicPartition
     from sentry.eventstream.kafka.consumer import SynchronizedConsumer
-
-    has_kafka_client = True
 except ImportError:
-    has_kafka_client = False
+    pass
 
 
 @contextmanager
@@ -45,22 +42,7 @@ def create_topic(partitions=1, replication_factor=1):
         subprocess.check_call(command + ["--delete", "--topic", topic])
 
 
-def requires_kafka(function):
-    @functools.wraps(function)
-    def wrapper(*args, **kwargs):
-        if not has_kafka_client:
-            return pytest.xfail("test requires confluent_kafka which is not installed")
-        if "SENTRY_KAFKA_HOSTS" not in os.environ:
-            return pytest.xfail(
-                "test requires SENTRY_KAFKA_HOSTS environment variable which is not set"
-            )
-        return function(*args, **kwargs)
-
-    return wrapper
-
-
-@requires_kafka
-def test_consumer_start_from_partition_start():
+def test_consumer_start_from_partition_start(requires_kafka):
     synchronize_commit_group = "consumer-{}".format(uuid.uuid1().hex)
 
     messages_delivered = defaultdict(list)
@@ -146,8 +128,7 @@ def test_consumer_start_from_partition_start():
         assert consumer.poll(1) is None
 
 
-@requires_kafka
-def test_consumer_start_from_committed_offset():
+def test_consumer_start_from_committed_offset(requires_kafka):
     consumer_group = "consumer-{}".format(uuid.uuid1().hex)
     synchronize_commit_group = "consumer-{}".format(uuid.uuid1().hex)
 
@@ -248,8 +229,7 @@ def test_consumer_start_from_committed_offset():
         assert consumer.poll(1) is None
 
 
-@requires_kafka
-def test_consumer_rebalance_from_partition_start():
+def test_consumer_rebalance_from_partition_start(requires_kafka):
     consumer_group = "consumer-{}".format(uuid.uuid1().hex)
     synchronize_commit_group = "consumer-{}".format(uuid.uuid1().hex)
 
@@ -370,8 +350,7 @@ def test_consumer_rebalance_from_partition_start():
             assert consumer.poll(1) is None
 
 
-@requires_kafka
-def test_consumer_rebalance_from_committed_offset():
+def test_consumer_rebalance_from_committed_offset(requires_kafka):
     consumer_group = "consumer-{}".format(uuid.uuid1().hex)
     synchronize_commit_group = "consumer-{}".format(uuid.uuid1().hex)
 
@@ -534,12 +513,11 @@ def collect_messages_recieved(count):
     return messages_recieved_constraint
 
 
-@requires_kafka
 @pytest.mark.xfail(
     reason="assignment during rebalance requires partition rollback to last committed offset",
     run=False,
 )
-def test_consumer_rebalance_from_uncommitted_offset():
+def test_consumer_rebalance_from_uncommitted_offset(requires_kafka):
     consumer_group = "consumer-{}".format(uuid.uuid1().hex)
     synchronize_commit_group = "consumer-{}".format(uuid.uuid1().hex)
 
diff --git a/tests/sentry/ingest/ingest_utils.py b/tests/sentry/ingest/ingest_utils.py
deleted file mode 100644
index a1f02cf664..0000000000
--- a/tests/sentry/ingest/ingest_utils.py
+++ /dev/null
@@ -1,18 +0,0 @@
-from __future__ import absolute_import
-
-import os
-
-import functools
-import pytest
-
-
-def requires_kafka(function):
-    @functools.wraps(function)
-    def wrapper(*args, **kwargs):
-        if "SENTRY_KAFKA_HOSTS" not in os.environ:
-            return pytest.xfail(
-                "test requires SENTRY_KAFKA_HOSTS environment variable which is not set"
-            )
-        return function(*args, **kwargs)
-
-    return wrapper
diff --git a/tests/sentry/ingest/test_ingest_consumer.py b/tests/sentry/ingest/test_ingest_consumer.py
index dc4b635ea7..28588111ac 100644
--- a/tests/sentry/ingest/test_ingest_consumer.py
+++ b/tests/sentry/ingest/test_ingest_consumer.py
@@ -9,15 +9,16 @@ import pytest
 from django.conf import settings
 
 from sentry.event_manager import EventManager
-from sentry.ingest.ingest_consumer import ConsumerType, run_ingest_consumer
+from sentry.ingest.ingest_consumer import ConsumerType, get_ingest_consumer
 from sentry.models.event import Event
 from sentry.utils import json
 from sentry.testutils.factories import Factories
 
-from .ingest_utils import requires_kafka
-
 logger = logging.getLogger(__name__)
 
+# Poll this amount of times (for 0.1 sec each) at most to wait for messages
+MAX_POLL_ITERATIONS = 100
+
 
 def _get_test_message(project):
     """
@@ -75,11 +76,10 @@ def _shutdown_requested(max_secs, num_events):
 
 
 @pytest.mark.django_db
-@requires_kafka
 def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
-    task_runner, kafka_producer, kafka_admin
+    task_runner, kafka_producer, kafka_admin, requires_kafka
 ):
-    consumer_group = "test-consumer"
+    group_id = "test-consumer"
     topic_event_name = ConsumerType.get_topic_name(ConsumerType.Events)
 
     admin = kafka_admin(settings)
@@ -95,15 +95,19 @@ def test_ingest_consumer_reads_from_topic_and_calls_celery_task(
         event_ids.add(event_id)
         producer.produce(topic_event_name, message)
 
+    consumer = get_ingest_consumer(
+        max_batch_size=2,
+        max_batch_time=5000,
+        group_id=group_id,
+        consumer_type=ConsumerType.Events,
+        auto_offset_reset="earliest",
+    )
+
     with task_runner():
-        run_ingest_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            consumer_type=ConsumerType.Events,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(max_secs=10, num_events=3),
-        )
+        i = 0
+        while Event.objects.count() < 3 and i < MAX_POLL_ITERATIONS:
+            consumer._run_once()
+            i += 1
 
     # check that we got the messages
     assert Event.objects.count() == 3
diff --git a/tests/sentry/ingest/test_outcome_consumer.py b/tests/sentry/ingest/test_outcome_consumer.py
index 82576e10ce..90747fdeed 100644
--- a/tests/sentry/ingest/test_outcome_consumer.py
+++ b/tests/sentry/ingest/test_outcome_consumer.py
@@ -1,23 +1,20 @@
 from __future__ import absolute_import
 
 import logging
-import time
-import os
 import pytest
 import six.moves
 
-from sentry.ingest.outcomes_consumer import run_outcomes_consumer, mark_signal_sent
+from sentry.ingest.outcomes_consumer import get_outcomes_consumer, mark_signal_sent, is_signal_sent
 from sentry.signals import event_filtered, event_dropped
 from sentry.testutils.factories import Factories
 from sentry.utils.outcomes import Outcome
 from django.conf import settings
 from sentry.utils import json
 
-from .ingest_utils import requires_kafka
-
 logger = logging.getLogger(__name__)
 
-SKIP_CONSUMER_TESTS = os.environ.get("SENTRY_RUN_CONSUMER_TESTS") != "1"
+# Poll this amount of times (for 0.1 sec each) at most to wait for messages
+MAX_POLL_ITERATIONS = 100
 
 
 def _get_event_id(base_event_id):
@@ -60,32 +57,6 @@ def _get_outcome_topic_name():
     return settings.KAFKA_OUTCOMES
 
 
-def _shutdown_requested(max_secs, num_outcomes, signal_sink):
-    """
-    Requests a shutdown after the specified interval has passed or the specified number
-    of outcomes are detected
-
-    :param max_secs: number of seconds after which to request a shutdown
-    :param num_outcomes: number of events after which to request a shutdown
-    :param signal_sink: a list where the signal handler accumulates the outcomes
-    :return: True if a shutdown is requested False otherwise
-    """
-
-    def inner():
-        end_time = time.time()
-        if end_time - start_time > max_secs:
-            logger.debug("Shutdown requested because max secs exceeded")
-            return True
-        elif len(signal_sink) >= num_outcomes:
-            logger.debug("Shutdown requested because num outcomes reached")
-            return True
-        else:
-            return False
-
-    start_time = time.time()
-    return inner
-
-
 def _setup_outcome_test(kafka_producer, kafka_admin):
     topic_name = _get_outcome_topic_name()
     organization = Factories.create_organization()
@@ -97,20 +68,17 @@ def _setup_outcome_test(kafka_producer, kafka_admin):
     return producer, project_id, topic_name
 
 
-@pytest.mark.skipif(
-    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
-)
 @pytest.mark.django_db
-@requires_kafka
 def test_outcome_consumer_ignores_outcomes_already_handled(
-    kafka_producer, task_runner, kafka_admin
+    kafka_producer, task_runner, kafka_admin, requires_kafka
 ):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
 
-    consumer_group = "test-outcome-consumer-1"
+    group_id = "test-outcome-consumer-1"
+    last_event_id = None
 
     # put a few outcome messages on the kafka topic and also mark them in the cache
-    for i in six.moves.range(1, 3):
+    for i in range(4):
         msg = _get_outcome(
             event_id=i,
             project_id=project_id,
@@ -118,8 +86,12 @@ def test_outcome_consumer_ignores_outcomes_already_handled(
             reason="some_reason",
             remote_addr="127.33.44.{}".format(i),
         )
-        # pretend that we have already processed this outcome before
-        mark_signal_sent(project_id=project_id, event_id=_get_event_id(i))
+        if i in (0, 1):
+            # pretend that we have already processed this outcome before
+            mark_signal_sent(project_id=project_id, event_id=_get_event_id(i))
+        else:
+            # Last event is used to check when the outcome producer is done
+            last_event_id = _get_event_id(i)
         # put the outcome on the kafka topic
         producer.produce(topic_name, msg)
 
@@ -136,39 +108,42 @@ def test_outcome_consumer_ignores_outcomes_already_handled(
     event_filtered.connect(event_filtered_receiver)
     event_dropped.connect(event_dropped_receiver)
 
+    consumer = get_outcomes_consumer(
+        max_batch_size=2, max_batch_time=100, group_id=group_id, auto_offset_reset="earliest"
+    )
+
     # run the outcome consumer
     with task_runner():
-        run_outcomes_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(
-                max_secs=10, num_outcomes=1, signal_sink=event_filtered_sink
-            ),
-        )
+        i = 0
+        while (
+            not is_signal_sent(project_id=project_id, event_id=last_event_id)
+            and i < MAX_POLL_ITERATIONS
+        ):
+            consumer._run_once()
+            i += 1
+
+    assert is_signal_sent(project_id=project_id, event_id=last_event_id)
 
     # verify that no signal was called (since the events have been previously processed)
-    assert len(event_filtered_sink) == 0
+    assert event_filtered_sink == ["127.33.44.2", "127.33.44.3"]
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skipif(
-    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
-)
 @pytest.mark.django_db
-@requires_kafka
-def test_outcome_consumer_ignores_invalid_outcomes(kafka_producer, task_runner, kafka_admin):
+def test_outcome_consumer_ignores_invalid_outcomes(
+    kafka_producer, task_runner, kafka_admin, requires_kafka
+):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
 
-    consumer_group = "test-outcome-consumer-2"
+    group_id = "test-outcome-consumer-2"
 
-    # put a few outcome messages on the kafka topic
-    for i in six.moves.range(1, 3):
+    # put a few outcome messages on the kafka topic. Add two FILTERED items so
+    # we know when the producer has reached the end
+    for i in range(4):
         msg = _get_outcome(
             event_id=i,
             project_id=project_id,
-            outcome=Outcome.INVALID,
+            outcome=Outcome.INVALID if i < 2 else Outcome.FILTERED,
             reason="some_reason",
             remote_addr="127.33.44.{}".format(i),
         )
@@ -188,32 +163,29 @@ def test_outcome_consumer_ignores_invalid_outcomes(kafka_producer, task_runner,
     event_filtered.connect(event_filtered_receiver)
     event_dropped.connect(event_dropped_receiver)
 
+    consumer = get_outcomes_consumer(
+        max_batch_size=4, max_batch_time=100, group_id=group_id, auto_offset_reset="earliest"
+    )
+
     # run the outcome consumer
     with task_runner():
-        run_outcomes_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(
-                max_secs=10, num_outcomes=1, signal_sink=event_filtered_sink
-            ),
-        )
+        i = 0
+        while len(event_filtered_sink) < 2 and i < MAX_POLL_ITERATIONS:
+            consumer._run_once()
+            i += 1
 
     # verify that the appropriate filters were called
-    assert len(event_filtered_sink) == 0
+    assert event_filtered_sink == ["127.33.44.2", "127.33.44.3"]
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skipif(
-    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
-)
 @pytest.mark.django_db
-@requires_kafka
-def test_outcome_consumer_remembers_handled_outcomes(kafka_producer, task_runner, kafka_admin):
+def test_outcome_consumer_remembers_handled_outcomes(
+    kafka_producer, task_runner, kafka_admin, requires_kafka
+):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
 
-    consumer_group = "test-outcome-consumer-3"
+    group_id = "test-outcome-consumer-3"
 
     # put a few outcome messages on the kafka topic
     for i in six.moves.range(1, 3):
@@ -242,17 +214,16 @@ def test_outcome_consumer_remembers_handled_outcomes(kafka_producer, task_runner
     event_filtered.connect(event_filtered_receiver)
     event_dropped.connect(event_dropped_receiver)
 
+    consumer = get_outcomes_consumer(
+        max_batch_size=2, max_batch_time=100, group_id=group_id, auto_offset_reset="earliest"
+    )
+
     # run the outcome consumer
     with task_runner():
-        run_outcomes_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(
-                max_secs=10, num_outcomes=1, signal_sink=event_filtered_sink
-            ),
-        )
+        i = 0
+        while not event_filtered_sink and i < MAX_POLL_ITERATIONS:
+            consumer._run_once()
+            i += 1
 
     # verify that the appropriate filters were called
     assert len(event_filtered_sink) == 1
@@ -260,15 +231,13 @@ def test_outcome_consumer_remembers_handled_outcomes(kafka_producer, task_runner
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skipif(
-    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
-)
 @pytest.mark.django_db
-@requires_kafka
-def test_outcome_consumer_handles_filtered_outcomes(kafka_producer, task_runner, kafka_admin):
+def test_outcome_consumer_handles_filtered_outcomes(
+    kafka_producer, task_runner, kafka_admin, requires_kafka
+):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
 
-    consumer_group = "test-outcome-consumer-4"
+    group_id = "test-outcome-consumer-4"
 
     # put a few outcome messages on the kafka topic
     for i in six.moves.range(1, 3):
@@ -295,17 +264,16 @@ def test_outcome_consumer_handles_filtered_outcomes(kafka_producer, task_runner,
     event_filtered.connect(event_filtered_receiver)
     event_dropped.connect(event_dropped_receiver)
 
+    consumer = get_outcomes_consumer(
+        max_batch_size=2, max_batch_time=100, group_id=group_id, auto_offset_reset="earliest"
+    )
+
     # run the outcome consumer
     with task_runner():
-        run_outcomes_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(
-                max_secs=10, num_outcomes=1, signal_sink=event_filtered_sink
-            ),
-        )
+        i = 0
+        while len(event_filtered_sink) < 2 and i < MAX_POLL_ITERATIONS:
+            consumer._run_once()
+            i += 1
 
     # verify that the appropriate filters were called
     assert len(event_filtered_sink) == 2
@@ -313,15 +281,13 @@ def test_outcome_consumer_handles_filtered_outcomes(kafka_producer, task_runner,
     assert len(event_dropped_sink) == 0
 
 
-@pytest.mark.skipif(
-    SKIP_CONSUMER_TESTS, reason="slow test, reading the first kafka message takes many seconds"
-)
 @pytest.mark.django_db
-@requires_kafka
-def test_outcome_consumer_handles_rate_limited_outcomes(kafka_producer, task_runner, kafka_admin):
+def test_outcome_consumer_handles_rate_limited_outcomes(
+    kafka_producer, task_runner, kafka_admin, requires_kafka
+):
     producer, project_id, topic_name = _setup_outcome_test(kafka_producer, kafka_admin)
 
-    consumer_group = "test-outcome-consumer-5"
+    group_id = "test-outcome-consumer-5"
 
     # put a few outcome messages on the kafka topic
     for i in six.moves.range(1, 3):
@@ -348,17 +314,16 @@ def test_outcome_consumer_handles_rate_limited_outcomes(kafka_producer, task_run
     event_filtered.connect(event_filtered_receiver)
     event_dropped.connect(event_dropped_receiver)
 
+    consumer = get_outcomes_consumer(
+        max_batch_size=2, max_batch_time=100, group_id=group_id, auto_offset_reset="earliest"
+    )
+
     # run the outcome consumer
     with task_runner():
-        run_outcomes_consumer(
-            commit_batch_size=2,
-            consumer_group=consumer_group,
-            max_fetch_time_seconds=0.1,
-            initial_offset_reset="earliest",
-            is_shutdown_requested=_shutdown_requested(
-                max_secs=10, num_outcomes=1, signal_sink=event_filtered_sink
-            ),
-        )
+        i = 0
+        while len(event_dropped_sink) < 2 and i < MAX_POLL_ITERATIONS:
+            consumer._run_once()
+            i += 1
 
     # verify that the appropriate filters were called
     assert len(event_filtered_sink) == 0
