commit 2a3b05a3a7167c7b384375e9442c350f740e9629
Author: Dominik Stanczak <stanczakdominik@gmail.com>
Date:   Wed Mar 22 07:55:29 2017 -0400

    CLN/INT: Rename _possibly to _maybe (GH15764)
    
    Also rename "private" functions in pandas.type.cast
    closes #15764
    
    Author: Dominik Stanczak <stanczakdominik@gmail.com>
    
    Closes #15771 from StanczakDominik/rename-possibly and squashes the following commits:
    
    486b932 [Dominik Stanczak] Cleanup missed linting errors
    188c48b [Dominik Stanczak] CLN/INT: Rename _possibly to _maybe

diff --git a/pandas/computation/expr.py b/pandas/computation/expr.py
index a78228717..e78806b38 100644
--- a/pandas/computation/expr.py
+++ b/pandas/computation/expr.py
@@ -348,7 +348,7 @@ class BaseExprVisitor(ast.NodeVisitor):
         op = self.visit(op_instance)
         return op, op_instance, left, right
 
-    def _possibly_transform_eq_ne(self, node, left=None, right=None):
+    def _maybe_transform_eq_ne(self, node, left=None, right=None):
         if left is None:
             left = self.visit(node.left, side='left')
         if right is None:
@@ -357,7 +357,7 @@ class BaseExprVisitor(ast.NodeVisitor):
                                                                 right)
         return op, op_class, left, right
 
-    def _possibly_downcast_constants(self, left, right):
+    def _maybe_downcast_constants(self, left, right):
         f32 = np.dtype(np.float32)
         if left.isscalar and not right.isscalar and right.return_type == f32:
             # right is a float32 array, left is a scalar
@@ -370,7 +370,7 @@ class BaseExprVisitor(ast.NodeVisitor):
 
         return left, right
 
-    def _possibly_eval(self, binop, eval_in_python):
+    def _maybe_eval(self, binop, eval_in_python):
         # eval `in` and `not in` (for now) in "partial" python space
         # things that can be evaluated in "eval" space will be turned into
         # temporary variables. for example,
@@ -380,10 +380,10 @@ class BaseExprVisitor(ast.NodeVisitor):
         return binop.evaluate(self.env, self.engine, self.parser,
                               self.term_type, eval_in_python)
 
-    def _possibly_evaluate_binop(self, op, op_class, lhs, rhs,
-                                 eval_in_python=('in', 'not in'),
-                                 maybe_eval_in_python=('==', '!=', '<', '>',
-                                                       '<=', '>=')):
+    def _maybe_evaluate_binop(self, op, op_class, lhs, rhs,
+                              eval_in_python=('in', 'not in'),
+                              maybe_eval_in_python=('==', '!=', '<', '>',
+                                                    '<=', '>=')):
         res = op(lhs, rhs)
 
         if res.has_invalid_return_type:
@@ -397,24 +397,24 @@ class BaseExprVisitor(ast.NodeVisitor):
                     getattr(rhs, 'is_datetime', False)):
                 # all date ops must be done in python bc numexpr doesn't work
                 # well with NaT
-                return self._possibly_eval(res, self.binary_ops)
+                return self._maybe_eval(res, self.binary_ops)
 
         if res.op in eval_in_python:
             # "in"/"not in" ops are always evaluated in python
-            return self._possibly_eval(res, eval_in_python)
+            return self._maybe_eval(res, eval_in_python)
         elif self.engine != 'pytables':
             if (getattr(lhs, 'return_type', None) == object or
                     getattr(rhs, 'return_type', None) == object):
                 # evaluate "==" and "!=" in python if either of our operands
                 # has an object return type
-                return self._possibly_eval(res, eval_in_python +
-                                           maybe_eval_in_python)
+                return self._maybe_eval(res, eval_in_python +
+                                        maybe_eval_in_python)
         return res
 
     def visit_BinOp(self, node, **kwargs):
-        op, op_class, left, right = self._possibly_transform_eq_ne(node)
-        left, right = self._possibly_downcast_constants(left, right)
-        return self._possibly_evaluate_binop(op, op_class, left, right)
+        op, op_class, left, right = self._maybe_transform_eq_ne(node)
+        left, right = self._maybe_downcast_constants(left, right)
+        return self._maybe_evaluate_binop(op, op_class, left, right)
 
     def visit_Div(self, node, **kwargs):
         truediv = self.env.scope['truediv']
@@ -662,9 +662,9 @@ class BaseExprVisitor(ast.NodeVisitor):
             lhs = self._try_visit_binop(x)
             rhs = self._try_visit_binop(y)
 
-            op, op_class, lhs, rhs = self._possibly_transform_eq_ne(node, lhs,
-                                                                    rhs)
-            return self._possibly_evaluate_binop(op, node.op, lhs, rhs)
+            op, op_class, lhs, rhs = self._maybe_transform_eq_ne(
+                node, lhs, rhs)
+            return self._maybe_evaluate_binop(op, node.op, lhs, rhs)
 
         operands = node.values
         return reduce(visitor, operands)
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 9a8d0a779..3b77bda6f 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -7,7 +7,7 @@ from warnings import warn
 import numpy as np
 
 from pandas import compat, _np_version_under1p8
-from pandas.types.cast import _maybe_promote
+from pandas.types.cast import maybe_promote
 from pandas.types.generic import ABCSeries, ABCIndex
 from pandas.types.common import (is_unsigned_integer_dtype,
                                  is_signed_integer_dtype,
@@ -1297,7 +1297,7 @@ def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan, mask_info=None,
         else:
             # check for promotion based on types only (do this first because
             # it's faster than computing a mask)
-            dtype, fill_value = _maybe_promote(arr.dtype, fill_value)
+            dtype, fill_value = maybe_promote(arr.dtype, fill_value)
             if dtype != arr.dtype and (out is None or out.dtype != dtype):
                 # check if promotion is actually required based on indexer
                 if mask_info is not None:
@@ -1380,7 +1380,7 @@ def take_2d_multi(arr, indexer, out=None, fill_value=np.nan, mask_info=None,
         else:
             # check for promotion based on types only (do this first because
             # it's faster than computing a mask)
-            dtype, fill_value = _maybe_promote(arr.dtype, fill_value)
+            dtype, fill_value = maybe_promote(arr.dtype, fill_value)
             if dtype != arr.dtype and (out is None or out.dtype != dtype):
                 # check if promotion is actually required based on indexer
                 if mask_info is not None:
diff --git a/pandas/core/categorical.py b/pandas/core/categorical.py
index af51c7f2e..0e58c1863 100644
--- a/pandas/core/categorical.py
+++ b/pandas/core/categorical.py
@@ -10,8 +10,8 @@ from pandas._libs import lib, algos as libalgos
 
 from pandas.types.generic import ABCSeries, ABCIndexClass, ABCCategoricalIndex
 from pandas.types.missing import isnull, notnull
-from pandas.types.cast import (_possibly_infer_to_datetimelike,
-                               _coerce_indexer_dtype)
+from pandas.types.cast import (maybe_infer_to_datetimelike,
+                               coerce_indexer_dtype)
 from pandas.types.dtypes import CategoricalDtype
 from pandas.types.common import (_ensure_int64,
                                  _ensure_object,
@@ -237,7 +237,7 @@ class Categorical(PandasObject):
 
         if fastpath:
             # fast path
-            self._codes = _coerce_indexer_dtype(values, categories)
+            self._codes = coerce_indexer_dtype(values, categories)
             self._categories = self._validate_categories(
                 categories, fastpath=isinstance(categories, ABCIndexClass))
             self._ordered = ordered
@@ -266,8 +266,7 @@ class Categorical(PandasObject):
             # correctly no need here this is an issue because _sanitize_array
             # also coerces np.nan to a string under certain versions of numpy
             # as well
-            values = _possibly_infer_to_datetimelike(values,
-                                                     convert_dates=True)
+            values = maybe_infer_to_datetimelike(values, convert_dates=True)
             if not isinstance(values, np.ndarray):
                 values = _convert_to_list_like(values)
                 from pandas.core.series import _sanitize_array
@@ -324,7 +323,7 @@ class Categorical(PandasObject):
 
         self.set_ordered(ordered or False, inplace=True)
         self._categories = categories
-        self._codes = _coerce_indexer_dtype(codes, categories)
+        self._codes = coerce_indexer_dtype(codes, categories)
 
     @property
     def _constructor(self):
@@ -877,7 +876,7 @@ class Categorical(PandasObject):
         new_categories = list(self._categories) + list(new_categories)
         cat = self if inplace else self.copy()
         cat._categories = self._validate_categories(new_categories)
-        cat._codes = _coerce_indexer_dtype(cat._codes, new_categories)
+        cat._codes = coerce_indexer_dtype(cat._codes, new_categories)
         if not inplace:
             return cat
 
@@ -961,7 +960,7 @@ class Categorical(PandasObject):
             idx, inv = idx[1:], inv - 1
 
         cat._categories = cat.categories.take(idx)
-        cat._codes = _coerce_indexer_dtype(inv, self._categories)
+        cat._codes = coerce_indexer_dtype(inv, self._categories)
 
         if not inplace:
             return cat
@@ -1065,8 +1064,8 @@ class Categorical(PandasObject):
             state['_categories'] = self._validate_categories(state.pop(
                 '_levels'))
         if '_codes' not in state and 'labels' in state:
-            state['_codes'] = _coerce_indexer_dtype(state.pop('labels'),
-                                                    state['_categories'])
+            state['_codes'] = coerce_indexer_dtype(
+                state.pop('labels'), state['_categories'])
 
         # 0.16.0 ordered change
         if '_ordered' not in state:
@@ -2062,7 +2061,7 @@ def _get_codes_for_values(values, categories):
     (_, _), cats = _get_data_algo(categories, _hashtables)
     t = hash_klass(len(cats))
     t.map_locations(cats)
-    return _coerce_indexer_dtype(t.lookup(vals), cats)
+    return coerce_indexer_dtype(t.lookup(vals), cats)
 
 
 def _convert_to_list_like(list_like):
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index b49aa926d..6b5e8e079 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -23,15 +23,15 @@ from numpy import nan as NA
 import numpy as np
 import numpy.ma as ma
 
-from pandas.types.cast import (_maybe_upcast, _infer_dtype_from_scalar,
-                               _possibly_cast_to_datetime,
-                               _possibly_infer_to_datetimelike,
-                               _possibly_convert_platform,
-                               _possibly_downcast_to_dtype,
-                               _invalidate_string_dtypes,
-                               _coerce_to_dtypes,
-                               _maybe_upcast_putmask,
-                               _find_common_type)
+from pandas.types.cast import (maybe_upcast, infer_dtype_from_scalar,
+                               maybe_cast_to_datetime,
+                               maybe_infer_to_datetimelike,
+                               maybe_convert_platform,
+                               maybe_downcast_to_dtype,
+                               invalidate_string_dtypes,
+                               coerce_to_dtypes,
+                               maybe_upcast_putmask,
+                               find_common_type)
 from pandas.types.common import (is_categorical_dtype,
                                  is_object_dtype,
                                  is_extension_type,
@@ -275,7 +275,7 @@ class DataFrame(NDFrame):
             else:
                 mask = ma.getmaskarray(data)
                 if mask.any():
-                    data, fill_value = _maybe_upcast(data, copy=True)
+                    data, fill_value = maybe_upcast(data, copy=True)
                     data[mask] = fill_value
                 else:
                     data = data.copy()
@@ -335,7 +335,7 @@ class DataFrame(NDFrame):
                 if isinstance(data, compat.string_types) and dtype is None:
                     dtype = np.object_
                 if dtype is None:
-                    dtype, data = _infer_dtype_from_scalar(data)
+                    dtype, data = infer_dtype_from_scalar(data)
 
                 values = np.empty((len(index), len(columns)), dtype=dtype)
                 values.fill(data)
@@ -469,7 +469,7 @@ class DataFrame(NDFrame):
         # on the entire block; this is to convert if we have datetimelike's
         # embedded in an object type
         if dtype is None and is_object_dtype(values):
-            values = _possibly_infer_to_datetimelike(values)
+            values = maybe_infer_to_datetimelike(values)
 
         return create_block_manager_from_blocks([values], [columns, index])
 
@@ -2359,7 +2359,7 @@ it is assumed to be aliases for the column names.')
         include, exclude = map(
             lambda x: frozenset(map(_get_dtype_from_object, x)), selection)
         for dtypes in (include, exclude):
-            _invalidate_string_dtypes(dtypes)
+            invalidate_string_dtypes(dtypes)
 
         # can't both include AND exclude!
         if not include.isdisjoint(exclude):
@@ -2659,7 +2659,7 @@ it is assumed to be aliases for the column names.')
             value = _sanitize_index(value, self.index, copy=False)
             if not isinstance(value, (np.ndarray, Index)):
                 if isinstance(value, list) and len(value) > 0:
-                    value = _possibly_convert_platform(value)
+                    value = maybe_convert_platform(value)
                 else:
                     value = com._asarray_tuplesafe(value)
             elif value.ndim == 2:
@@ -2671,13 +2671,13 @@ it is assumed to be aliases for the column names.')
 
             # possibly infer to datetimelike
             if is_object_dtype(value.dtype):
-                value = _possibly_infer_to_datetimelike(value)
+                value = maybe_infer_to_datetimelike(value)
 
         else:
             # upcast the scalar
-            dtype, value = _infer_dtype_from_scalar(value)
+            dtype, value = infer_dtype_from_scalar(value)
             value = np.repeat(value, len(self.index)).astype(dtype)
-            value = _possibly_cast_to_datetime(value, dtype)
+            value = maybe_cast_to_datetime(value, dtype)
 
         # return internal types directly
         if is_extension_type(value):
@@ -3000,8 +3000,8 @@ it is assumed to be aliases for the column names.')
                 else:
                     values = values.take(labels)
                     if mask.any():
-                        values, changed = _maybe_upcast_putmask(values, mask,
-                                                                np.nan)
+                        values, changed = maybe_upcast_putmask(
+                            values, mask, np.nan)
             return values
 
         new_index = _default_index(len(new_obj))
@@ -3722,7 +3722,7 @@ it is assumed to be aliases for the column names.')
             # if we have different dtypes, possibily promote
             new_dtype = this_dtype
             if not is_dtype_equal(this_dtype, other_dtype):
-                new_dtype = _find_common_type([this_dtype, other_dtype])
+                new_dtype = find_common_type([this_dtype, other_dtype])
                 if not is_dtype_equal(this_dtype, new_dtype):
                     series = series.astype(new_dtype)
                 if not is_dtype_equal(other_dtype, new_dtype):
@@ -3743,13 +3743,13 @@ it is assumed to be aliases for the column names.')
             # try to downcast back to the original dtype
             if needs_i8_conversion_i:
                 # ToDo: This conversion should be handled in
-                # _possibly_cast_to_datetime but the change affects lot...
+                # _maybe_cast_to_datetime but the change affects lot...
                 if is_datetime64tz_dtype(new_dtype):
                     arr = DatetimeIndex._simple_new(arr, tz=new_dtype.tz)
                 else:
-                    arr = _possibly_cast_to_datetime(arr, new_dtype)
+                    arr = maybe_cast_to_datetime(arr, new_dtype)
             else:
-                arr = _possibly_downcast_to_dtype(arr, this_dtype)
+                arr = maybe_downcast_to_dtype(arr, this_dtype)
 
             result[col] = arr
 
@@ -5003,7 +5003,7 @@ it is assumed to be aliases for the column names.')
 
                 # try to coerce to the original dtypes item by item if we can
                 if axis == 0:
-                    result = _coerce_to_dtypes(result, self.dtypes)
+                    result = coerce_to_dtypes(result, self.dtypes)
 
         return Series(result, index=labels)
 
@@ -5505,7 +5505,7 @@ def _prep_ndarray(values, copy=True):
             return np.empty((0, 0), dtype=object)
 
         def convert(v):
-            return _possibly_convert_platform(v)
+            return maybe_convert_platform(v)
 
         # we could have a 1-dim or 2-dim list here
         # this is equiv of np.asarray, but does object conversion
@@ -5601,7 +5601,7 @@ def _masked_rec_array_to_mgr(data, index, columns, dtype, copy):
     for fv, arr, col in zip(fill_value, arrays, arr_columns):
         mask = ma.getmaskarray(data[col])
         if mask.any():
-            arr, fv = _maybe_upcast(arr, fill_value=fv, copy=True)
+            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)
             arr[mask] = fv
         new_arrays.append(arr)
 
@@ -5699,7 +5699,7 @@ def _convert_object_array(content, columns, coerce_float=False, dtype=None):
     def convert(arr):
         if dtype != object and dtype != np.object:
             arr = lib.maybe_convert_objects(arr, try_float=coerce_float)
-            arr = _possibly_cast_to_datetime(arr, dtype)
+            arr = maybe_cast_to_datetime(arr, dtype)
         return arr
 
     arrays = [convert(arr) for arr in content]
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 1db967765..87052800b 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -23,7 +23,7 @@ from pandas.types.common import (_coerce_to_dtype,
                                  is_list_like,
                                  is_dict_like,
                                  is_re_compilable)
-from pandas.types.cast import _maybe_promote, _maybe_upcast_putmask
+from pandas.types.cast import maybe_promote, maybe_upcast_putmask
 from pandas.types.missing import isnull, notnull
 from pandas.types.generic import ABCSeries, ABCPanel
 
@@ -4956,10 +4956,10 @@ class NDFrame(PandasObject):
                         # or not try_quick
                         if not try_quick:
 
-                            dtype, fill_value = _maybe_promote(other.dtype)
+                            dtype, fill_value = maybe_promote(other.dtype)
                             new_other = np.empty(len(icond), dtype=dtype)
                             new_other.fill(fill_value)
-                            _maybe_upcast_putmask(new_other, icond, other)
+                            maybe_upcast_putmask(new_other, icond, other)
                             other = new_other
 
                     else:
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 4095a14aa..0a6398129 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -32,7 +32,7 @@ from pandas.types.common import (is_numeric_dtype,
                                  _ensure_object,
                                  _ensure_categorical,
                                  _ensure_float)
-from pandas.types.cast import _possibly_downcast_to_dtype
+from pandas.types.cast import maybe_downcast_to_dtype
 from pandas.types.missing import isnull, notnull, _maybe_fill
 
 from pandas.core.common import (_values_from_object, AbstractMethodError,
@@ -783,7 +783,7 @@ class _GroupBy(PandasObject, SelectionMixin):
 
         if not is_scalar(result):
             if numeric_only and is_numeric_dtype(dtype) or not numeric_only:
-                result = _possibly_downcast_to_dtype(result, dtype)
+                result = maybe_downcast_to_dtype(result, dtype)
 
         return result
 
@@ -2914,7 +2914,7 @@ class SeriesGroupBy(GroupBy):
         # the cython take a different path (and casting)
         dtype = self._selected_obj.dtype
         if is_numeric_dtype(dtype):
-            result = _possibly_downcast_to_dtype(result, dtype)
+            result = maybe_downcast_to_dtype(result, dtype)
 
         result.name = self._selected_obj.name
         result.index = self._selected_obj.index
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 6487c2108..8db801f8e 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -29,15 +29,15 @@ from pandas.types.common import (_TD_DTYPE, _NS_DTYPE,
                                  is_re_compilable,
                                  is_scalar,
                                  _get_dtype)
-from pandas.types.cast import (_possibly_downcast_to_dtype,
-                               _maybe_convert_string_to_object,
-                               _maybe_upcast,
-                               _maybe_convert_scalar, _maybe_promote,
-                               _infer_dtype_from_scalar,
-                               _soft_convert_objects,
-                               _possibly_convert_objects,
-                               _astype_nansafe,
-                               _find_common_type)
+from pandas.types.cast import (maybe_downcast_to_dtype,
+                               maybe_convert_string_to_object,
+                               maybe_upcast,
+                               maybe_convert_scalar, maybe_promote,
+                               infer_dtype_from_scalar,
+                               soft_convert_objects,
+                               maybe_convert_objects,
+                               astype_nansafe,
+                               find_common_type)
 from pandas.types.missing import (isnull, array_equivalent,
                                   _is_na_compat,
                                   is_null_datelike_scalar)
@@ -429,7 +429,7 @@ class Block(PandasObject):
             if dtypes is None:
                 dtypes = 'infer'
 
-            nv = _possibly_downcast_to_dtype(values, dtypes)
+            nv = maybe_downcast_to_dtype(values, dtypes)
             return self.make_block(nv, fastpath=True)
 
         # ndim > 1
@@ -455,7 +455,7 @@ class Block(PandasObject):
             if dtype is None:
                 nv = _block_shape(values[i], ndim=self.ndim)
             else:
-                nv = _possibly_downcast_to_dtype(values[i], dtype)
+                nv = maybe_downcast_to_dtype(values[i], dtype)
                 nv = _block_shape(nv, ndim=self.ndim)
 
             blocks.append(self.make_block(nv, fastpath=True, placement=[rl]))
@@ -514,7 +514,7 @@ class Block(PandasObject):
                     values = self.get_values(dtype=dtype)
 
                 # _astype_nansafe works fine with 1-d only
-                values = _astype_nansafe(values.ravel(), dtype, copy=True)
+                values = astype_nansafe(values.ravel(), dtype, copy=True)
                 values = values.reshape(self.shape)
 
             newb = make_block(values, placement=self.mgr_locs, dtype=dtype,
@@ -578,7 +578,7 @@ class Block(PandasObject):
             return result
 
         # may need to change the dtype here
-        return _possibly_downcast_to_dtype(result, dtype)
+        return maybe_downcast_to_dtype(result, dtype)
 
     def _try_operate(self, values):
         """ return a version to operate on as the input """
@@ -684,7 +684,7 @@ class Block(PandasObject):
 
         # cast the values to a type that can hold nan (if necessary)
         if not self._can_hold_element(value):
-            dtype, _ = _maybe_promote(arr_value.dtype)
+            dtype, _ = maybe_promote(arr_value.dtype)
             values = values.astype(dtype)
 
         transf = (lambda x: x.T) if self.ndim == 2 else (lambda x: x)
@@ -758,7 +758,7 @@ class Block(PandasObject):
                                                           value.dtype):
                 dtype = value.dtype
             elif is_scalar(value):
-                dtype, _ = _infer_dtype_from_scalar(value)
+                dtype, _ = infer_dtype_from_scalar(value)
             else:
                 dtype = 'infer'
             values = self._try_coerce_and_cast_result(values, dtype)
@@ -871,7 +871,7 @@ class Block(PandasObject):
                             n = np.array(new)
 
                         # type of the new block
-                        dtype, _ = _maybe_promote(n.dtype)
+                        dtype, _ = maybe_promote(n.dtype)
 
                         # we need to explicitly astype here to make a copy
                         n = n.astype(dtype)
@@ -1066,7 +1066,7 @@ class Block(PandasObject):
 
         # convert integer to float if necessary. need to do a lot more than
         # that, handle boolean etc also
-        new_values, fill_value = _maybe_upcast(self.values)
+        new_values, fill_value = maybe_upcast(self.values)
 
         # make sure array sent to np.roll is c_contiguous
         f_ordered = new_values.flags.f_contiguous
@@ -1250,8 +1250,8 @@ class Block(PandasObject):
             raise ValueError("where must have a condition that is ndarray "
                              "like")
 
-        other = _maybe_convert_string_to_object(other)
-        other = _maybe_convert_scalar(other)
+        other = maybe_convert_string_to_object(other)
+        other = maybe_convert_scalar(other)
 
         # our where function
         def func(cond, values, other):
@@ -1864,10 +1864,10 @@ class ObjectBlock(Block):
             new_style |= kw in kwargs
 
         if new_style:
-            fn = _soft_convert_objects
+            fn = soft_convert_objects
             fn_inputs = new_inputs
         else:
-            fn = _possibly_convert_objects
+            fn = maybe_convert_objects
             fn_inputs = ['convert_dates', 'convert_numeric',
                          'convert_timedeltas']
         fn_inputs += ['copy']
@@ -2643,7 +2643,7 @@ class SparseBlock(NonConsolidatableMixIn, Block):
         new_values = self.values.to_dense().take(indexer)
         # convert integer to float if necessary. need to do a lot more than
         # that, handle boolean etc also
-        new_values, fill_value = _maybe_upcast(new_values)
+        new_values, fill_value = maybe_upcast(new_values)
         if periods > 0:
             new_values[:periods] = fill_value
         else:
@@ -3239,13 +3239,12 @@ class BlockManager(PandasObject):
         def comp(s):
             if isnull(s):
                 return isnull(values)
-            return _possibly_compare(values, getattr(s, 'asm8', s),
-                                     operator.eq)
+            return _maybe_compare(values, getattr(s, 'asm8', s), operator.eq)
 
         def _cast_scalar(block, scalar):
-            dtype, val = _infer_dtype_from_scalar(scalar, pandas_dtype=True)
+            dtype, val = infer_dtype_from_scalar(scalar, pandas_dtype=True)
             if not is_dtype_equal(block.dtype, dtype):
-                dtype = _find_common_type([block.dtype, dtype])
+                dtype = find_common_type([block.dtype, dtype])
                 block = block.astype(dtype)
                 # use original value
                 val = scalar
@@ -3920,7 +3919,7 @@ class BlockManager(PandasObject):
                 return [blk.getitem_block(slobj, new_mgr_locs=slice(0, sllen))]
             elif not allow_fill or self.ndim == 1:
                 if allow_fill and fill_tuple[0] is None:
-                    _, fill_value = _maybe_promote(blk.dtype)
+                    _, fill_value = maybe_promote(blk.dtype)
                     fill_tuple = (fill_value, )
 
                 return [blk.take_nd(slobj, axis=0,
@@ -3978,7 +3977,7 @@ class BlockManager(PandasObject):
         block_shape = list(self.shape)
         block_shape[0] = len(placement)
 
-        dtype, fill_value = _infer_dtype_from_scalar(fill_value)
+        dtype, fill_value = infer_dtype_from_scalar(fill_value)
         block_values = np.empty(block_shape, dtype=dtype)
         block_values.fill(fill_value)
         return make_block(block_values, placement=placement)
@@ -4497,7 +4496,7 @@ def _interleaved_dtype(blocks):
     if not len(blocks):
         return None
 
-    dtype = _find_common_type([b.dtype for b in blocks])
+    dtype = find_common_type([b.dtype for b in blocks])
 
     # only numpy compat
     if isinstance(dtype, ExtensionDtype):
@@ -4587,7 +4586,7 @@ def _vstack(to_stack, dtype):
         return np.vstack(to_stack)
 
 
-def _possibly_compare(a, b, op):
+def _maybe_compare(a, b, op):
 
     is_a_array = isinstance(a, np.ndarray)
     is_b_array = isinstance(b, np.ndarray)
@@ -4637,7 +4636,7 @@ def _block2d_to_blocknd(values, placement, shape, labels, ref_items):
     if mask.all():
         pvalues = np.empty(panel_shape, dtype=values.dtype)
     else:
-        dtype, fill_value = _maybe_promote(values.dtype)
+        dtype, fill_value = maybe_promote(values.dtype)
         pvalues = np.empty(panel_shape, dtype=dtype)
         pvalues.fill(fill_value)
 
@@ -4786,7 +4785,7 @@ def _putmask_smart(v, m, n):
         pass
 
     # change the dtype
-    dtype, _ = _maybe_promote(n.dtype)
+    dtype, _ = maybe_promote(n.dtype)
 
     if is_extension_type(v.dtype) and is_object_dtype(dtype):
         nv = v.get_values(dtype)
@@ -5142,8 +5141,8 @@ class JoinUnit(object):
         if not self.needs_filling:
             return self.block.dtype
         else:
-            return _get_dtype(_maybe_promote(self.block.dtype,
-                                             self.block.fill_value)[0])
+            return _get_dtype(maybe_promote(self.block.dtype,
+                                            self.block.fill_value)[0])
 
         return self._dtype
 
diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index bb6c9b454..6ec94e697 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -20,7 +20,7 @@ from pandas.types.common import (_get_dtype,
                                  is_datetime64_dtype, is_timedelta64_dtype,
                                  is_datetime_or_timedelta_dtype,
                                  is_int_or_datetime_dtype, is_any_int_dtype)
-from pandas.types.cast import _int64_max, _maybe_upcast_putmask
+from pandas.types.cast import _int64_max, maybe_upcast_putmask
 from pandas.types.missing import isnull, notnull
 
 from pandas.core.common import _values_from_object
@@ -200,7 +200,7 @@ def _get_values(values, skipna, fill_value=None, fill_value_typ=None,
 
         # promote if needed
         else:
-            values, changed = _maybe_upcast_putmask(values, mask, fill_value)
+            values, changed = maybe_upcast_putmask(values, mask, fill_value)
 
     elif copy:
         values = values.copy()
diff --git a/pandas/core/ops.py b/pandas/core/ops.py
index fe83f8a35..5dac8a7e4 100644
--- a/pandas/core/ops.py
+++ b/pandas/core/ops.py
@@ -33,7 +33,7 @@ from pandas.types.common import (needs_i8_conversion,
                                  is_list_like,
                                  is_scalar,
                                  _ensure_object)
-from pandas.types.cast import _maybe_upcast_putmask, _find_common_type
+from pandas.types.cast import maybe_upcast_putmask, find_common_type
 from pandas.types.generic import ABCSeries, ABCIndex, ABCPeriodIndex
 
 # -----------------------------------------------------------------------------
@@ -657,7 +657,7 @@ def _arith_method_SERIES(op, name, str_rep, fill_zeros=None, default_axis=None,
                                           raise_on_error=True, **eval_kwargs)
         except TypeError:
             if isinstance(y, (np.ndarray, ABCSeries, pd.Index)):
-                dtype = _find_common_type([x.dtype, y.dtype])
+                dtype = find_common_type([x.dtype, y.dtype])
                 result = np.empty(x.size, dtype=dtype)
                 mask = notnull(x) & notnull(y)
                 result[mask] = op(x[mask], _values_from_object(y[mask]))
@@ -670,7 +670,7 @@ def _arith_method_SERIES(op, name, str_rep, fill_zeros=None, default_axis=None,
                                 "{op}".format(typ=type(x).__name__,
                                               op=str_rep))
 
-            result, changed = _maybe_upcast_putmask(result, ~mask, np.nan)
+            result, changed = maybe_upcast_putmask(result, ~mask, np.nan)
 
         result = missing.fill_zeros(result, x, y, name, fill_zeros)
         return result
@@ -1204,7 +1204,7 @@ def _arith_method_FRAME(op, name, str_rep=None, default_axis='columns',
                                 "objects of type {x} and {y}".format(
                                     op=name, x=type(x), y=type(y)))
 
-            result, changed = _maybe_upcast_putmask(result, ~mask, np.nan)
+            result, changed = maybe_upcast_putmask(result, ~mask, np.nan)
             result = result.reshape(x.shape)
 
         result = missing.fill_zeros(result, x, y, name, fill_zeros)
@@ -1329,7 +1329,7 @@ def _arith_method_PANEL(op, name, str_rep=None, fill_zeros=None,
             result = np.empty(len(x), dtype=x.dtype)
             mask = notnull(x)
             result[mask] = op(x[mask], y)
-            result, changed = _maybe_upcast_putmask(result, ~mask, np.nan)
+            result, changed = maybe_upcast_putmask(result, ~mask, np.nan)
 
         result = missing.fill_zeros(result, x, y, name, fill_zeros)
         return result
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index 5c7b66a2d..50ddc24ac 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -6,8 +6,8 @@ from __future__ import division
 
 import numpy as np
 
-from pandas.types.cast import (_infer_dtype_from_scalar,
-                               _possibly_cast_item)
+from pandas.types.cast import (infer_dtype_from_scalar,
+                               maybe_cast_item)
 from pandas.types.common import (is_integer, is_list_like,
                                  is_string_like, is_scalar)
 from pandas.types.missing import notnull
@@ -165,7 +165,7 @@ class Panel(NDFrame):
             dtype = None
         elif is_scalar(data) and all(x is not None for x in passed_axes):
             if dtype is None:
-                dtype, data = _infer_dtype_from_scalar(data)
+                dtype, data = infer_dtype_from_scalar(data)
             values = np.empty([len(x) for x in passed_axes], dtype=dtype)
             values.fill(data)
             mgr = self._init_matrix(values, passed_axes, dtype=dtype,
@@ -533,11 +533,11 @@ class Panel(NDFrame):
             d = self._construct_axes_dict_from(self, axes, copy=False)
             result = self.reindex(**d)
             args = list(args)
-            likely_dtype, args[-1] = _infer_dtype_from_scalar(args[-1])
+            likely_dtype, args[-1] = infer_dtype_from_scalar(args[-1])
             made_bigger = not np.array_equal(axes[0], self._info_axis)
             # how to make this logic simpler?
             if made_bigger:
-                _possibly_cast_item(result, args[0], likely_dtype)
+                maybe_cast_item(result, args[0], likely_dtype)
 
             return result.set_value(*args)
 
@@ -568,7 +568,7 @@ class Panel(NDFrame):
                                      shape[1:], tuple(map(int, value.shape))))
             mat = np.asarray(value)
         elif is_scalar(value):
-            dtype, value = _infer_dtype_from_scalar(value)
+            dtype, value = infer_dtype_from_scalar(value)
             mat = np.empty(shape[1:], dtype=dtype)
             mat.fill(value)
         else:
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 1e685ae68..2822d98b7 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -10,7 +10,7 @@ import numpy as np
 from pandas.types.common import (_ensure_platform_int,
                                  is_list_like, is_bool_dtype,
                                  needs_i8_conversion)
-from pandas.types.cast import _maybe_promote
+from pandas.types.cast import maybe_promote
 from pandas.types.missing import notnull
 import pandas.types.concat as _concat
 
@@ -202,7 +202,7 @@ class _Unstacker(object):
             dtype = values.dtype
             new_values = np.empty(result_shape, dtype=dtype)
         else:
-            dtype, fill_value = _maybe_promote(values.dtype, self.fill_value)
+            dtype, fill_value = maybe_promote(values.dtype, self.fill_value)
             new_values = np.empty(result_shape, dtype=dtype)
             new_values.fill(fill_value)
 
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 4c51ced18..0913592e0 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -29,9 +29,9 @@ from pandas.types.common import (_coerce_to_dtype, is_categorical_dtype,
                                  _is_unorderable_exception,
                                  _ensure_platform_int)
 from pandas.types.generic import ABCSparseArray, ABCDataFrame
-from pandas.types.cast import (_maybe_upcast, _infer_dtype_from_scalar,
-                               _possibly_convert_platform,
-                               _possibly_cast_to_datetime, _possibly_castable)
+from pandas.types.cast import (maybe_upcast, infer_dtype_from_scalar,
+                               maybe_convert_platform,
+                               maybe_cast_to_datetime, maybe_castable)
 from pandas.types.missing import isnull, notnull
 
 from pandas.core.common import (is_bool_indexer,
@@ -2794,7 +2794,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
     if isinstance(data, ma.MaskedArray):
         mask = ma.getmaskarray(data)
         if mask.any():
-            data, fill_value = _maybe_upcast(data, copy=True)
+            data, fill_value = maybe_upcast(data, copy=True)
             data[mask] = fill_value
         else:
             data = data.copy()
@@ -2803,11 +2803,11 @@ def _sanitize_array(data, index, dtype=None, copy=False,
 
         # perf shortcut as this is the most common case
         if take_fast_path:
-            if _possibly_castable(arr) and not copy and dtype is None:
+            if maybe_castable(arr) and not copy and dtype is None:
                 return arr
 
         try:
-            subarr = _possibly_cast_to_datetime(arr, dtype)
+            subarr = maybe_cast_to_datetime(arr, dtype)
             if not is_extension_type(subarr):
                 subarr = np.array(subarr, dtype=dtype, copy=copy)
         except (ValueError, TypeError):
@@ -2863,9 +2863,9 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                 subarr = lib.maybe_convert_objects(subarr)
 
         else:
-            subarr = _possibly_convert_platform(data)
+            subarr = maybe_convert_platform(data)
 
-        subarr = _possibly_cast_to_datetime(subarr, dtype)
+        subarr = maybe_cast_to_datetime(subarr, dtype)
 
     else:
         subarr = _try_cast(data, False)
@@ -2894,10 +2894,10 @@ def _sanitize_array(data, index, dtype=None, copy=False,
 
             # figure out the dtype from the value (upcast if necessary)
             if dtype is None:
-                dtype, value = _infer_dtype_from_scalar(value)
+                dtype, value = infer_dtype_from_scalar(value)
             else:
                 # need to possibly convert the value here
-                value = _possibly_cast_to_datetime(value, dtype)
+                value = maybe_cast_to_datetime(value, dtype)
 
             subarr = create_from_value(value, index, dtype)
 
diff --git a/pandas/indexes/base.py b/pandas/indexes/base.py
index d262ecd81..54f73a246 100644
--- a/pandas/indexes/base.py
+++ b/pandas/indexes/base.py
@@ -2445,7 +2445,7 @@ class Index(IndexOpsMixin, StringAccessorMixin, PandasObject):
         if tolerance is not None:
             tolerance = self._convert_tolerance(tolerance)
 
-        pself, ptarget = self._possibly_promote(target)
+        pself, ptarget = self._maybe_promote(target)
         if pself is not self or ptarget is not target:
             return pself.get_indexer(ptarget, method=method, limit=limit,
                                      tolerance=tolerance)
@@ -2572,7 +2572,7 @@ class Index(IndexOpsMixin, StringAccessorMixin, PandasObject):
     @Appender(_index_shared_docs['get_indexer_non_unique'] % _index_doc_kwargs)
     def get_indexer_non_unique(self, target):
         target = _ensure_index(target)
-        pself, ptarget = self._possibly_promote(target)
+        pself, ptarget = self._maybe_promote(target)
         if pself is not self or ptarget is not target:
             return pself.get_indexer_non_unique(ptarget)
 
@@ -2595,7 +2595,7 @@ class Index(IndexOpsMixin, StringAccessorMixin, PandasObject):
         indexer, _ = self.get_indexer_non_unique(target, **kwargs)
         return indexer
 
-    def _possibly_promote(self, other):
+    def _maybe_promote(self, other):
         # A hack, but it works
         from pandas.tseries.index import DatetimeIndex
         if self.inferred_type == 'date' and isinstance(other, DatetimeIndex):
diff --git a/pandas/indexes/frozen.py b/pandas/indexes/frozen.py
index e043ba64b..97a1a3ea9 100644
--- a/pandas/indexes/frozen.py
+++ b/pandas/indexes/frozen.py
@@ -10,7 +10,7 @@ These are used for:
 
 import numpy as np
 from pandas.core.base import PandasObject
-from pandas.types.cast import _coerce_indexer_dtype
+from pandas.types.cast import coerce_indexer_dtype
 from pandas.formats.printing import pprint_thing
 
 
@@ -119,7 +119,7 @@ class FrozenNDArray(PandasObject, np.ndarray):
 
 
 def _ensure_frozen(array_like, categories, copy=False):
-    array_like = _coerce_indexer_dtype(array_like, categories)
+    array_like = coerce_indexer_dtype(array_like, categories)
     array_like = array_like.view(FrozenNDArray)
     if copy:
         array_like = array_like.copy()
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index af57cc3ce..f7b2d75c1 100755
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -21,7 +21,7 @@ from pandas.types.common import (is_integer, _ensure_object,
                                  is_object_dtype, is_string_dtype,
                                  is_scalar, is_categorical_dtype)
 from pandas.types.missing import isnull
-from pandas.types.cast import _astype_nansafe
+from pandas.types.cast import astype_nansafe
 from pandas.core.index import Index, MultiIndex, RangeIndex
 from pandas.core.series import Series
 from pandas.core.frame import DataFrame
@@ -1498,11 +1498,11 @@ class ParserBase(object):
             # c-parser which parses all categories
             # as strings
             if not is_object_dtype(values):
-                values = _astype_nansafe(values, str)
+                values = astype_nansafe(values, str)
             values = Categorical(values)
         else:
             try:
-                values = _astype_nansafe(values, cast_type, copy=True)
+                values = astype_nansafe(values, cast_type, copy=True)
             except ValueError:
                 raise ValueError("Unable to convert column %s to "
                                  "type %s" % (column, cast_type))
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index 5f4c07971..f149e724c 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -22,8 +22,8 @@ from pandas.types.common import (_ensure_platform_int,
                                  is_list_like,
                                  is_string_dtype,
                                  is_scalar, is_dtype_equal)
-from pandas.types.cast import (_possibly_convert_platform, _maybe_promote,
-                               _astype_nansafe, _find_common_type)
+from pandas.types.cast import (maybe_convert_platform, maybe_promote,
+                               astype_nansafe, find_common_type)
 from pandas.types.missing import isnull, notnull, na_value_for_dtype
 
 from pandas.sparse import libsparse as splib
@@ -93,7 +93,7 @@ def _sparse_array_op(left, right, op, name, series=False):
 
     # dtype used to find corresponding sparse method
     if not is_dtype_equal(left.dtype, right.dtype):
-        dtype = _find_common_type([left.dtype, right.dtype])
+        dtype = find_common_type([left.dtype, right.dtype])
         left = left.astype(dtype)
         right = right.astype(dtype)
     else:
@@ -370,7 +370,7 @@ class SparseArray(PandasObject, np.ndarray):
         if not is_scalar(value):
             raise ValueError('fill_value must be a scalar')
         # if the specified value triggers type promotion, raise ValueError
-        new_dtype, fill_value = _maybe_promote(self.dtype, value)
+        new_dtype, fill_value = maybe_promote(self.dtype, value)
         if is_dtype_equal(self.dtype, new_dtype):
             self._fill_value = fill_value
         else:
@@ -532,7 +532,7 @@ class SparseArray(PandasObject, np.ndarray):
 
     def astype(self, dtype=None, copy=True):
         dtype = np.dtype(dtype)
-        sp_values = _astype_nansafe(self.sp_values, dtype, copy=copy)
+        sp_values = astype_nansafe(self.sp_values, dtype, copy=copy)
         try:
             if is_bool_dtype(dtype):
                 # to avoid np.bool_ dtype
@@ -736,7 +736,7 @@ def _sanitize_values(arr):
             pass
 
         elif is_list_like(arr) and len(arr) > 0:
-            arr = _possibly_convert_platform(arr)
+            arr = maybe_convert_platform(arr)
 
         else:
             arr = np.asarray(arr)
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index a21f64f52..41f301f26 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -11,7 +11,7 @@ from pandas import compat
 import numpy as np
 
 from pandas.types.missing import isnull, notnull
-from pandas.types.cast import _maybe_upcast, _find_common_type
+from pandas.types.cast import maybe_upcast, find_common_type
 from pandas.types.common import _ensure_platform_int, is_scipy_sparse
 
 from pandas.core.common import _try_sort
@@ -250,7 +250,7 @@ class SparseDataFrame(DataFrame):
         except ImportError:
             raise ImportError('Scipy is not installed')
 
-        dtype = _find_common_type(self.dtypes)
+        dtype = find_common_type(self.dtypes)
         cols, rows, datas = [], [], []
         for col, name in enumerate(self):
             s = self[name]
@@ -635,7 +635,7 @@ class SparseDataFrame(DataFrame):
                 new = new.values
                 # convert integer to float if necessary. need to do a lot
                 # more than that, handle boolean etc also
-                new, fill_value = _maybe_upcast(new, fill_value=fill_value)
+                new, fill_value = maybe_upcast(new, fill_value=fill_value)
                 np.putmask(new, mask, fill_value)
 
             new_series[col] = new
diff --git a/pandas/tests/types/test_cast.py b/pandas/tests/types/test_cast.py
index d7b086dae..dd4ea3bb0 100644
--- a/pandas/tests/types/test_cast.py
+++ b/pandas/tests/types/test_cast.py
@@ -9,33 +9,33 @@ from datetime import datetime
 import numpy as np
 
 from pandas import Timedelta, Timestamp, DatetimeIndex
-from pandas.types.cast import (_possibly_downcast_to_dtype,
-                               _possibly_convert_objects,
-                               _infer_dtype_from_scalar,
-                               _maybe_convert_string_to_object,
-                               _maybe_convert_scalar,
-                               _find_common_type)
+from pandas.types.cast import (maybe_downcast_to_dtype,
+                               maybe_convert_objects,
+                               infer_dtype_from_scalar,
+                               maybe_convert_string_to_object,
+                               maybe_convert_scalar,
+                               find_common_type)
 from pandas.types.dtypes import (CategoricalDtype,
                                  DatetimeTZDtype, PeriodDtype)
 from pandas.util import testing as tm
 
 
-class TestPossiblyDowncast(tm.TestCase):
+class TestMaybeDowncast(tm.TestCase):
 
     def test_downcast_conv(self):
         # test downcasting
 
         arr = np.array([8.5, 8.6, 8.7, 8.8, 8.9999999999995])
-        result = _possibly_downcast_to_dtype(arr, 'infer')
+        result = maybe_downcast_to_dtype(arr, 'infer')
         assert (np.array_equal(result, arr))
 
         arr = np.array([8., 8., 8., 8., 8.9999999999995])
-        result = _possibly_downcast_to_dtype(arr, 'infer')
+        result = maybe_downcast_to_dtype(arr, 'infer')
         expected = np.array([8, 8, 8, 8, 9])
         assert (np.array_equal(result, expected))
 
         arr = np.array([8., 8., 8., 8., 9.0000000000005])
-        result = _possibly_downcast_to_dtype(arr, 'infer')
+        result = maybe_downcast_to_dtype(arr, 'infer')
         expected = np.array([8, 8, 8, 8, 9])
         assert (np.array_equal(result, expected))
 
@@ -44,41 +44,41 @@ class TestPossiblyDowncast(tm.TestCase):
         expected = np.array([1, 2])
         for dtype in [np.float64, object, np.int64]:
             arr = np.array([1.0, 2.0], dtype=dtype)
-            result = _possibly_downcast_to_dtype(arr, 'infer')
+            result = maybe_downcast_to_dtype(arr, 'infer')
             tm.assert_almost_equal(result, expected, check_dtype=False)
 
         for dtype in [np.float64, object]:
             expected = np.array([1.0, 2.0, np.nan], dtype=dtype)
             arr = np.array([1.0, 2.0, np.nan], dtype=dtype)
-            result = _possibly_downcast_to_dtype(arr, 'infer')
+            result = maybe_downcast_to_dtype(arr, 'infer')
             tm.assert_almost_equal(result, expected)
 
         # empties
         for dtype in [np.int32, np.float64, np.float32, np.bool_,
                       np.int64, object]:
             arr = np.array([], dtype=dtype)
-            result = _possibly_downcast_to_dtype(arr, 'int64')
+            result = maybe_downcast_to_dtype(arr, 'int64')
             tm.assert_almost_equal(result, np.array([], dtype=np.int64))
             assert result.dtype == np.int64
 
     def test_datetimelikes_nan(self):
         arr = np.array([1, 2, np.nan])
         exp = np.array([1, 2, np.datetime64('NaT')], dtype='datetime64[ns]')
-        res = _possibly_downcast_to_dtype(arr, 'datetime64[ns]')
+        res = maybe_downcast_to_dtype(arr, 'datetime64[ns]')
         tm.assert_numpy_array_equal(res, exp)
 
         exp = np.array([1, 2, np.timedelta64('NaT')], dtype='timedelta64[ns]')
-        res = _possibly_downcast_to_dtype(arr, 'timedelta64[ns]')
+        res = maybe_downcast_to_dtype(arr, 'timedelta64[ns]')
         tm.assert_numpy_array_equal(res, exp)
 
     def test_datetime_with_timezone(self):
         # GH 15426
         ts = Timestamp("2016-01-01 12:00:00", tz='US/Pacific')
         exp = DatetimeIndex([ts, ts])
-        res = _possibly_downcast_to_dtype(exp, exp.dtype)
+        res = maybe_downcast_to_dtype(exp, exp.dtype)
         tm.assert_index_equal(res, exp)
 
-        res = _possibly_downcast_to_dtype(exp.asi8, exp.dtype)
+        res = maybe_downcast_to_dtype(exp.asi8, exp.dtype)
         tm.assert_index_equal(res, exp)
 
 
@@ -91,121 +91,121 @@ class TestInferDtype(tm.TestCase):
         for dtypec in [np.uint8, np.int8, np.uint16, np.int16, np.uint32,
                        np.int32, np.uint64, np.int64]:
             data = dtypec(12)
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, type(data))
 
         data = 12
-        dtype, val = _infer_dtype_from_scalar(data)
+        dtype, val = infer_dtype_from_scalar(data)
         self.assertEqual(dtype, np.int64)
 
         for dtypec in [np.float16, np.float32, np.float64]:
             data = dtypec(12)
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, dtypec)
 
         data = np.float(12)
-        dtype, val = _infer_dtype_from_scalar(data)
+        dtype, val = infer_dtype_from_scalar(data)
         self.assertEqual(dtype, np.float64)
 
         for data in [True, False]:
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, np.bool_)
 
         for data in [np.complex64(1), np.complex128(1)]:
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, np.complex_)
 
         import datetime
         for data in [np.datetime64(1, 'ns'), Timestamp(1),
                      datetime.datetime(2000, 1, 1, 0, 0)]:
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, 'M8[ns]')
 
         for data in [np.timedelta64(1, 'ns'), Timedelta(1),
                      datetime.timedelta(1)]:
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, 'm8[ns]')
 
         for data in [datetime.date(2000, 1, 1),
                      Timestamp(1, tz='US/Eastern'), 'foo']:
-            dtype, val = _infer_dtype_from_scalar(data)
+            dtype, val = infer_dtype_from_scalar(data)
             self.assertEqual(dtype, np.object_)
 
 
 class TestMaybe(tm.TestCase):
 
     def test_maybe_convert_string_to_array(self):
-        result = _maybe_convert_string_to_object('x')
+        result = maybe_convert_string_to_object('x')
         tm.assert_numpy_array_equal(result, np.array(['x'], dtype=object))
         self.assertTrue(result.dtype == object)
 
-        result = _maybe_convert_string_to_object(1)
+        result = maybe_convert_string_to_object(1)
         self.assertEqual(result, 1)
 
         arr = np.array(['x', 'y'], dtype=str)
-        result = _maybe_convert_string_to_object(arr)
+        result = maybe_convert_string_to_object(arr)
         tm.assert_numpy_array_equal(result, np.array(['x', 'y'], dtype=object))
         self.assertTrue(result.dtype == object)
 
         # unicode
         arr = np.array(['x', 'y']).astype('U')
-        result = _maybe_convert_string_to_object(arr)
+        result = maybe_convert_string_to_object(arr)
         tm.assert_numpy_array_equal(result, np.array(['x', 'y'], dtype=object))
         self.assertTrue(result.dtype == object)
 
         # object
         arr = np.array(['x', 2], dtype=object)
-        result = _maybe_convert_string_to_object(arr)
+        result = maybe_convert_string_to_object(arr)
         tm.assert_numpy_array_equal(result, np.array(['x', 2], dtype=object))
         self.assertTrue(result.dtype == object)
 
     def test_maybe_convert_scalar(self):
 
         # pass thru
-        result = _maybe_convert_scalar('x')
+        result = maybe_convert_scalar('x')
         self.assertEqual(result, 'x')
-        result = _maybe_convert_scalar(np.array([1]))
+        result = maybe_convert_scalar(np.array([1]))
         self.assertEqual(result, np.array([1]))
 
         # leave scalar dtype
-        result = _maybe_convert_scalar(np.int64(1))
+        result = maybe_convert_scalar(np.int64(1))
         self.assertEqual(result, np.int64(1))
-        result = _maybe_convert_scalar(np.int32(1))
+        result = maybe_convert_scalar(np.int32(1))
         self.assertEqual(result, np.int32(1))
-        result = _maybe_convert_scalar(np.float32(1))
+        result = maybe_convert_scalar(np.float32(1))
         self.assertEqual(result, np.float32(1))
-        result = _maybe_convert_scalar(np.int64(1))
+        result = maybe_convert_scalar(np.int64(1))
         self.assertEqual(result, np.float64(1))
 
         # coerce
-        result = _maybe_convert_scalar(1)
+        result = maybe_convert_scalar(1)
         self.assertEqual(result, np.int64(1))
-        result = _maybe_convert_scalar(1.0)
+        result = maybe_convert_scalar(1.0)
         self.assertEqual(result, np.float64(1))
-        result = _maybe_convert_scalar(Timestamp('20130101'))
+        result = maybe_convert_scalar(Timestamp('20130101'))
         self.assertEqual(result, Timestamp('20130101').value)
-        result = _maybe_convert_scalar(datetime(2013, 1, 1))
+        result = maybe_convert_scalar(datetime(2013, 1, 1))
         self.assertEqual(result, Timestamp('20130101').value)
-        result = _maybe_convert_scalar(Timedelta('1 day 1 min'))
+        result = maybe_convert_scalar(Timedelta('1 day 1 min'))
         self.assertEqual(result, Timedelta('1 day 1 min').value)
 
 
 class TestConvert(tm.TestCase):
 
-    def test_possibly_convert_objects_copy(self):
+    def test_maybe_convert_objects_copy(self):
         values = np.array([1, 2])
 
-        out = _possibly_convert_objects(values, copy=False)
+        out = maybe_convert_objects(values, copy=False)
         self.assertTrue(values is out)
 
-        out = _possibly_convert_objects(values, copy=True)
+        out = maybe_convert_objects(values, copy=True)
         self.assertTrue(values is not out)
 
         values = np.array(['apply', 'banana'])
-        out = _possibly_convert_objects(values, copy=False)
+        out = maybe_convert_objects(values, copy=False)
         self.assertTrue(values is out)
 
-        out = _possibly_convert_objects(values, copy=True)
+        out = maybe_convert_objects(values, copy=True)
         self.assertTrue(values is not out)
 
 
@@ -267,34 +267,34 @@ class TestCommonTypes(tm.TestCase):
             ((np.dtype('datetime64[ns]'), np.int64), np.object)
         )
         for src, common in testcases:
-            self.assertEqual(_find_common_type(src), common)
+            self.assertEqual(find_common_type(src), common)
 
         with tm.assertRaises(ValueError):
             # empty
-            _find_common_type([])
+            find_common_type([])
 
     def test_categorical_dtype(self):
         dtype = CategoricalDtype()
-        self.assertEqual(_find_common_type([dtype]), 'category')
-        self.assertEqual(_find_common_type([dtype, dtype]), 'category')
-        self.assertEqual(_find_common_type([np.object, dtype]), np.object)
+        self.assertEqual(find_common_type([dtype]), 'category')
+        self.assertEqual(find_common_type([dtype, dtype]), 'category')
+        self.assertEqual(find_common_type([np.object, dtype]), np.object)
 
     def test_datetimetz_dtype(self):
         dtype = DatetimeTZDtype(unit='ns', tz='US/Eastern')
-        self.assertEqual(_find_common_type([dtype, dtype]),
+        self.assertEqual(find_common_type([dtype, dtype]),
                          'datetime64[ns, US/Eastern]')
 
         for dtype2 in [DatetimeTZDtype(unit='ns', tz='Asia/Tokyo'),
                        np.dtype('datetime64[ns]'), np.object, np.int64]:
-            self.assertEqual(_find_common_type([dtype, dtype2]), np.object)
-            self.assertEqual(_find_common_type([dtype2, dtype]), np.object)
+            self.assertEqual(find_common_type([dtype, dtype2]), np.object)
+            self.assertEqual(find_common_type([dtype2, dtype]), np.object)
 
     def test_period_dtype(self):
         dtype = PeriodDtype(freq='D')
-        self.assertEqual(_find_common_type([dtype, dtype]), 'period[D]')
+        self.assertEqual(find_common_type([dtype, dtype]), 'period[D]')
 
         for dtype2 in [DatetimeTZDtype(unit='ns', tz='Asia/Tokyo'),
                        PeriodDtype(freq='2D'), PeriodDtype(freq='H'),
                        np.dtype('datetime64[ns]'), np.object, np.int64]:
-            self.assertEqual(_find_common_type([dtype, dtype2]), np.object)
-            self.assertEqual(_find_common_type([dtype2, dtype]), np.object)
+            self.assertEqual(find_common_type([dtype, dtype2]), np.object)
+            self.assertEqual(find_common_type([dtype2, dtype]), np.object)
diff --git a/pandas/tools/util.py b/pandas/tools/util.py
index bf78a9dfb..263d2f16a 100644
--- a/pandas/tools/util.py
+++ b/pandas/tools/util.py
@@ -9,7 +9,7 @@ from pandas.types.common import (is_number,
                                  is_decimal,
                                  is_scalar as isscalar)
 
-from pandas.types.cast import _possibly_downcast_to_dtype
+from pandas.types.cast import maybe_downcast_to_dtype
 
 import pandas as pd
 from pandas.compat import reduce
@@ -226,8 +226,7 @@ def to_numeric(arg, errors='raise', downcast=None):
             # from smallest to largest
             for dtype in typecodes:
                 if np.dtype(dtype).itemsize <= values.dtype.itemsize:
-                    values = _possibly_downcast_to_dtype(
-                        values, dtype)
+                    values = maybe_downcast_to_dtype(values, dtype)
 
                     # successful conversion
                     if values.dtype == dtype:
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index f80618ef3..983c1a4cd 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -1329,7 +1329,7 @@ class DatetimeIndex(DatelikeOps, TimelikeOps, DatetimeIndexOpsMixin,
         # try to find a the dates
         return (lhs_mask & rhs_mask).nonzero()[0]
 
-    def _possibly_promote(self, other):
+    def _maybe_promote(self, other):
         if other.inferred_type == 'date':
             other = DatetimeIndex(other)
         return self, other
diff --git a/pandas/tseries/tdi.py b/pandas/tseries/tdi.py
index f47d80a31..13d844bb6 100644
--- a/pandas/tseries/tdi.py
+++ b/pandas/tseries/tdi.py
@@ -623,7 +623,7 @@ class TimedeltaIndex(DatetimeIndexOpsMixin, TimelikeOps, Int64Index):
             left_chunk = left.values[lslice]
             return self._shallow_copy(left_chunk)
 
-    def _possibly_promote(self, other):
+    def _maybe_promote(self, other):
         if other.inferred_type == 'timedelta':
             other = TimedeltaIndex(other)
         return self, other
diff --git a/pandas/types/cast.py b/pandas/types/cast.py
index 0e26cd085..91c7d287d 100644
--- a/pandas/types/cast.py
+++ b/pandas/types/cast.py
@@ -32,7 +32,7 @@ _int32_max = np.iinfo(np.int32).max
 _int64_max = np.iinfo(np.int64).max
 
 
-def _possibly_convert_platform(values):
+def maybe_convert_platform(values):
     """ try to do platform conversion, allow ndarray or list here """
 
     if isinstance(values, (list, tuple)):
@@ -45,7 +45,7 @@ def _possibly_convert_platform(values):
     return values
 
 
-def _possibly_downcast_to_dtype(result, dtype):
+def maybe_downcast_to_dtype(result, dtype):
     """ try to cast to the specified dtype (e.g. convert back to bool/int
     or could be an astype of float64->float32
     """
@@ -142,7 +142,7 @@ def _possibly_downcast_to_dtype(result, dtype):
     return result
 
 
-def _maybe_upcast_putmask(result, mask, other):
+def maybe_upcast_putmask(result, mask, other):
     """
     A safe version of putmask that potentially upcasts the result
 
@@ -193,7 +193,7 @@ def _maybe_upcast_putmask(result, mask, other):
 
             # we are forced to change the dtype of the result as the input
             # isn't compatible
-            r, _ = _maybe_upcast(result, fill_value=other, copy=True)
+            r, _ = maybe_upcast(result, fill_value=other, copy=True)
             np.place(r, mask, other)
 
             return r, True
@@ -203,7 +203,7 @@ def _maybe_upcast_putmask(result, mask, other):
         # upcast (possibly), otherwise we DON't want to upcast (e.g. if we
         # have values, say integers, in the success portion then it's ok to not
         # upcast)
-        new_dtype, _ = _maybe_promote(result.dtype, other)
+        new_dtype, _ = maybe_promote(result.dtype, other)
         if new_dtype != result.dtype:
 
             # we have a scalar or len 0 ndarray
@@ -227,7 +227,7 @@ def _maybe_upcast_putmask(result, mask, other):
     return result, False
 
 
-def _maybe_promote(dtype, fill_value=np.nan):
+def maybe_promote(dtype, fill_value=np.nan):
 
     # if we passed an array here, determine the fill value by dtype
     if isinstance(fill_value, np.ndarray):
@@ -312,7 +312,7 @@ def _maybe_promote(dtype, fill_value=np.nan):
     return dtype, fill_value
 
 
-def _infer_dtype_from_scalar(val, pandas_dtype=False):
+def infer_dtype_from_scalar(val, pandas_dtype=False):
     """
     interpret the dtype from a scalar
 
@@ -387,7 +387,7 @@ def _infer_dtype_from_scalar(val, pandas_dtype=False):
     return dtype, val
 
 
-def _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
+def maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
     """ provide explict type promotion and coercion
 
     Parameters
@@ -404,7 +404,7 @@ def _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
     else:
         if dtype is None:
             dtype = values.dtype
-        new_dtype, fill_value = _maybe_promote(dtype, fill_value)
+        new_dtype, fill_value = maybe_promote(dtype, fill_value)
         if new_dtype != values.dtype:
             values = values.astype(new_dtype)
         elif copy:
@@ -413,7 +413,7 @@ def _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
     return values, fill_value
 
 
-def _possibly_cast_item(obj, item, dtype):
+def maybe_cast_item(obj, item, dtype):
     chunk = obj[item]
 
     if chunk.values.dtype != dtype:
@@ -423,7 +423,7 @@ def _possibly_cast_item(obj, item, dtype):
             raise ValueError("Unexpected dtype encountered: %s" % dtype)
 
 
-def _invalidate_string_dtypes(dtype_set):
+def invalidate_string_dtypes(dtype_set):
     """Change string like dtypes to object for
     ``DataFrame.select_dtypes()``.
     """
@@ -432,7 +432,7 @@ def _invalidate_string_dtypes(dtype_set):
         raise TypeError("string dtypes are not allowed, use 'object' instead")
 
 
-def _maybe_convert_string_to_object(values):
+def maybe_convert_string_to_object(values):
     """
 
     Convert string-like and string-like array to convert object dtype.
@@ -446,13 +446,13 @@ def _maybe_convert_string_to_object(values):
     return values
 
 
-def _maybe_convert_scalar(values):
+def maybe_convert_scalar(values):
     """
     Convert a python scalar to the appropriate numpy dtype if possible
     This avoids numpy directly converting according to platform preferences
     """
     if is_scalar(values):
-        dtype, values = _infer_dtype_from_scalar(values)
+        dtype, values = infer_dtype_from_scalar(values)
         try:
             values = dtype(values)
         except TypeError:
@@ -460,7 +460,7 @@ def _maybe_convert_scalar(values):
     return values
 
 
-def _coerce_indexer_dtype(indexer, categories):
+def coerce_indexer_dtype(indexer, categories):
     """ coerce the indexer input array to the smallest dtype possible """
     l = len(categories)
     if l < _int8_max:
@@ -472,7 +472,7 @@ def _coerce_indexer_dtype(indexer, categories):
     return _ensure_int64(indexer)
 
 
-def _coerce_to_dtypes(result, dtypes):
+def coerce_to_dtypes(result, dtypes):
     """
     given a dtypes and a result set, coerce the result elements to the
     dtypes
@@ -507,7 +507,7 @@ def _coerce_to_dtypes(result, dtypes):
     return [conv(r, dtype) for r, dtype in zip(result, dtypes)]
 
 
-def _astype_nansafe(arr, dtype, copy=True):
+def astype_nansafe(arr, dtype, copy=True):
     """ return a view if copy is False, but
         need to be very careful as the result shape could change! """
     if not isinstance(dtype, np.dtype):
@@ -564,8 +564,8 @@ def _astype_nansafe(arr, dtype, copy=True):
     return arr.view(dtype)
 
 
-def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True,
-                              convert_timedeltas=True, copy=True):
+def maybe_convert_objects(values, convert_dates=True, convert_numeric=True,
+                          convert_timedeltas=True, copy=True):
     """ if we have an object dtype, try to coerce dates and/or numbers """
 
     # if we have passed in a list or scalar
@@ -579,8 +579,8 @@ def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True,
 
         # we take an aggressive stance and convert to datetime64[ns]
         if convert_dates == 'coerce':
-            new_values = _possibly_cast_to_datetime(values, 'M8[ns]',
-                                                    errors='coerce')
+            new_values = maybe_cast_to_datetime(
+                values, 'M8[ns]', errors='coerce')
 
             # if we are all nans then leave me alone
             if not isnull(new_values).all():
@@ -627,8 +627,8 @@ def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True,
     return values
 
 
-def _soft_convert_objects(values, datetime=True, numeric=True, timedelta=True,
-                          coerce=False, copy=True):
+def soft_convert_objects(values, datetime=True, numeric=True, timedelta=True,
+                         coerce=False, copy=True):
     """ if we have an object dtype, try to coerce dates and/or numbers """
 
     conversion_count = sum((datetime, numeric, timedelta))
@@ -683,7 +683,7 @@ def _soft_convert_objects(values, datetime=True, numeric=True, timedelta=True,
     return values
 
 
-def _possibly_castable(arr):
+def maybe_castable(arr):
     # return False to force a non-fastpath
 
     # check datetime64[ns]/timedelta64[ns] are valid
@@ -695,7 +695,7 @@ def _possibly_castable(arr):
     return arr.dtype.name not in _POSSIBLY_CAST_DTYPES
 
 
-def _possibly_infer_to_datetimelike(value, convert_dates=False):
+def maybe_infer_to_datetimelike(value, convert_dates=False):
     """
     we might have a array (or single object) that is datetime like,
     and no dtype is passed don't change the value unless we find a
@@ -788,7 +788,7 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
     return value
 
 
-def _possibly_cast_to_datetime(value, dtype, errors='raise'):
+def maybe_cast_to_datetime(value, dtype, errors='raise'):
     """ try to cast the array/value to a datetimelike dtype, converting float
     nan to iNaT
     """
@@ -886,12 +886,12 @@ def _possibly_cast_to_datetime(value, dtype, errors='raise'):
         # conversion
         elif not (is_array and not (issubclass(value.dtype.type, np.integer) or
                                     value.dtype == np.object_)):
-            value = _possibly_infer_to_datetimelike(value)
+            value = maybe_infer_to_datetimelike(value)
 
     return value
 
 
-def _find_common_type(types):
+def find_common_type(types):
     """
     Find a common data type among the given dtypes.
 
