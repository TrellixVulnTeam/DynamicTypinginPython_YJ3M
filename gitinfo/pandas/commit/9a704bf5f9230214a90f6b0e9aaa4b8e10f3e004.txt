commit 9a704bf5f9230214a90f6b0e9aaa4b8e10f3e004
Author: Adam Klein <adamklein@gmail.com>
Date:   Wed Jan 11 18:29:56 2012 -0500

    ENH: basic datetime64 integration and tests

diff --git a/pandas/core/api.py b/pandas/core/api.py
index 1201bde94..f42b59660 100644
--- a/pandas/core/api.py
+++ b/pandas/core/api.py
@@ -6,7 +6,8 @@ from pandas.core.datetools import DateOffset
 import pandas.core.datetools as datetools
 
 from pandas.core.common import isnull, notnull, set_printoptions, save, load
-from pandas.core.index import Index, Int64Index, Factor, MultiIndex
+from pandas.core.index import (Index, Int64Index, Factor, MultiIndex,
+                               DatetimeIndex)
 from pandas.core.daterange import DateRange
 from pandas.core.series import Series, TimeSeries
 from pandas.core.frame import DataFrame
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 3b2e72d50..7f0734323 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -53,11 +53,15 @@ def isnull(obj):
             # Working around NumPy ticket 1542
             shape = obj.shape
             result = np.empty(shape, dtype=bool)
-            vec = lib.isnullobj(obj.ravel())
+            raveled = obj.ravel()
+            vec = lib.isnullobj(raveled)
             result[:] = vec.reshape(shape)
 
             if isinstance(obj, Series):
                 result = Series(result, index=obj.index, copy=False)
+        elif obj.dtype == np.datetime64:
+            # this is the NaT pattern
+            result = obj.ravel().view('i8') == 0x8000000000000000
         else:
             result = -np.isfinite(obj)
         return result
@@ -85,6 +89,28 @@ def notnull(obj):
         return not res
     return -res
 
+_unbox_cache = dict()
+def _dt_unbox(key):
+    '''
+    Unbox datetime to datetime64
+    '''
+    try:
+        return _unbox_cache[key]
+    except KeyError:
+        _unbox_cache[key] = np.datetime64(key)
+        return _unbox_cache[key]
+
+_box_cache = dict()
+def _dt_box(key):
+    '''
+    Box datetime64 to datetime
+    '''
+    try:
+        return _box_cache[key]
+    except KeyError:
+        _box_cache[key] = key.astype('O')
+        return _box_cache[key]
+
 def _pickle_array(arr):
     arr = arr.view(np.ndarray)
 
@@ -765,7 +791,8 @@ def is_integer_dtype(arr_or_dtype):
         tipo = arr_or_dtype.type
     else:
         tipo = arr_or_dtype.dtype.type
-    return issubclass(tipo, np.integer)
+    return (issubclass(tipo, np.integer) and not
+            issubclass(tipo, np.datetime64))
 
 def is_float_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
diff --git a/pandas/core/daterange.py b/pandas/core/daterange.py
index 408ec43c1..34e98119b 100644
--- a/pandas/core/daterange.py
+++ b/pandas/core/daterange.py
@@ -5,8 +5,9 @@ import operator
 
 import numpy as np
 
-from pandas.core.index import Index
+from pandas.core.index import DatetimeIndex
 import pandas.core.datetools as datetools
+from pandas.core.common import _dt_box
 
 __all__ = ['DateRange']
 
@@ -15,6 +16,8 @@ __all__ = ['DateRange']
 
 def _bin_op(op):
     def f(self, other):
+        if isinstance(other, datetime):
+            other = np.datetime64(other)
         return op(self.view(np.ndarray), other)
 
     return f
@@ -24,7 +27,7 @@ _CACHE_END   = datetime(2030, 1, 1)
 
 _daterange_cache = {}
 
-class DateRange(Index):
+class DateRange(DatetimeIndex):
     """
     Fixed frequency date range according to input parameters.
 
@@ -66,10 +69,12 @@ class DateRange(Index):
         start = datetools.to_datetime(start)
         end = datetools.to_datetime(end)
 
-        if start is not None and not isinstance(start, datetime):
+        if (start is not None
+            and not isinstance(start, (datetime, np.datetime64))):
             raise ValueError('Failed to convert %s to datetime' % start)
 
-        if end is not None and not isinstance(end, datetime):
+        if (end is not None
+            and not isinstance(end, (datetime, np.datetime64))):
             raise ValueError('Failed to convert %s to datetime' % end)
 
         # inside cache range. Handle UTC case
@@ -92,7 +97,7 @@ class DateRange(Index):
         if tzinfo is not None:
             index = [d.replace(tzinfo=tzinfo) for d in index]
 
-        index = np.array(index, dtype=object, copy=False)
+        index = np.array(index, dtype=np.datetime64, copy=False)
         index = index.view(cls)
         index.name = name
         index.offset = offset
@@ -101,7 +106,7 @@ class DateRange(Index):
 
     def __reduce__(self):
         """Necessary for making this object picklable"""
-        a, b, state = Index.__reduce__(self)
+        a, b, state = DatetimeIndex.__reduce__(self)
         aug_state = state, self.offset, self.tzinfo
 
         return a, b, aug_state
@@ -119,16 +124,16 @@ class DateRange(Index):
 
         self.offset = offset
         self.tzinfo = tzinfo
-        Index.__setstate__(self, *index_state)
+        DatetimeIndex.__setstate__(self, *index_state)
 
     def equals(self, other):
         if self is other:
             return True
 
-        if not isinstance(other, Index):
+        if not isinstance(other, DatetimeIndex):
             return False
 
-        return Index.equals(self.view(Index), other)
+        return DatetimeIndex.equals(self.view(DatetimeIndex), other)
 
     @property
     def is_all_dates(self):
@@ -147,7 +152,7 @@ class DateRange(Index):
 
         if offset not in _daterange_cache:
             xdr = generate_range(_CACHE_START, _CACHE_END, offset=offset)
-            arr = np.array(list(xdr), dtype=object, copy=False)
+            arr = np.array(list(xdr), dtype=np.datetime64, copy=False)
 
             cachedRange = arr.view(DateRange)
             cachedRange.offset = offset
@@ -208,6 +213,9 @@ class DateRange(Index):
         """Override numpy.ndarray's __getitem__ method to work as desired"""
         result = self.view(np.ndarray)[key]
 
+        if isinstance(result, np.datetime64):
+            result = _dt_box(result).replace(tzinfo=self.tzinfo)
+
         if isinstance(key, (int, np.integer)):
             return result
         elif isinstance(key, slice):
@@ -221,10 +229,7 @@ class DateRange(Index):
             new_index.name = self.name
             return new_index
         else:
-            if result.ndim > 1:
-                return result
-
-            return Index(result, name=self.name)
+            return DatetimeIndex(result, name=self.name)
 
     def summary(self):
         if len(self) > 0:
@@ -263,7 +268,7 @@ class DateRange(Index):
         shifted : DateRange
         """
         if offset is not None and offset != self.offset:
-            return Index.shift(self, n, offset)
+            return DatetimeIndex.shift(self, n, offset)
 
         if n == 0:
             # immutable so OK
@@ -288,18 +293,18 @@ class DateRange(Index):
         y : Index or DateRange
         """
         if not isinstance(other, DateRange) or other.offset != self.offset:
-            return Index.union(self.view(Index), other)
+            return DatetimeIndex.union(self.view(DatetimeIndex), other)
 
         if self._can_fast_union(other):
             return self._fast_union(other)
         else:
-            return Index.union(self, other)
+            return DatetimeIndex.union(self, other)
 
     def _wrap_union_result(self, other, result):
         # If we are here, _can_fast_union is false or other is not a
         # DateRange, so their union has to be an Index.
         name = self.name if self.name == other.name else None
-        return Index(result, name=name)
+        return DatetimeIndex(result, name=name)
 
     def _wrap_joined_index(self, joined, other):
         name = self.name if self.name == other.name else None
@@ -310,7 +315,7 @@ class DateRange(Index):
             joined.name = name
             return joined
         else:
-            return Index(joined, name=name)
+            return DatetimeIndex(joined, name=name)
 
     def _can_fast_union(self, other):
         offset = self.offset
@@ -364,7 +369,7 @@ class DateRange(Index):
         y : Index or DateRange
         """
         if not isinstance(other, DateRange) or other.offset != self.offset:
-            return Index.intersection(self.view(Index), other)
+            return DatetimeIndex.intersection(self.view(DatetimeIndex), other)
 
         # to make our life easier, "sort" the two ranges
         if self[0] <= other[0]:
@@ -376,7 +381,7 @@ class DateRange(Index):
         right_start = right[0]
 
         if left_end < right_start:
-            return Index([])
+            return DatetimeIndex([])
         else:
             lslice = slice(*left.slice_locs(right_start, None))
             left_chunk = left.values[lslice]
@@ -397,7 +402,8 @@ class DateRange(Index):
         -------
         normalized : DateRange
         """
-        new_dates = np.array([tz.normalize(x) for x in self])
+        new_dates = np.array([tz.normalize(x.replace(tzinfo=self.tzinfo)) 
+                              for x in self])
         new_dates = new_dates.view(DateRange)
         new_dates.offset = self.offset
         new_dates.tzinfo = tz
@@ -412,7 +418,9 @@ class DateRange(Index):
         -------
         localized : DateRange
         """
-        new_dates = np.array([tz.localize(x) for x in self])
+        new_dates = np.array(
+                [np.datetime64(tz.localize(x.replace(tzinfo=self.tzinfo))) 
+                 for x in self])
         new_dates = new_dates.view(DateRange)
         new_dates.offset = self.offset
         new_dates.tzinfo = tz
@@ -542,6 +550,15 @@ def generate_range(start=None, end=None, periods=None,
 #         return False
 
 def _in_range(start, end, rng_start, rng_end):
+    if isinstance(rng_start, datetime):
+        rng_start = np.datetime64(rng_start)
+    if isinstance(rng_end, datetime):
+        rng_end = np.datetime64(rng_end)
+    if isinstance(start, datetime):
+        start = np.datetime64(start)
+    if isinstance(end, datetime):
+        end = np.datetime64(end)
+
     return start > rng_start and end < rng_end
 
 def _naive_in_cache_range(start, end):
diff --git a/pandas/core/datetools.py b/pandas/core/datetools.py
index 8dd44f66f..fe3f38225 100644
--- a/pandas/core/datetools.py
+++ b/pandas/core/datetools.py
@@ -2,6 +2,8 @@
 
 from datetime import datetime, timedelta
 import sys
+import numpy as np
+from pandas.core.common import _dt_box
 
 try:
     import dateutil
@@ -49,6 +51,8 @@ def to_datetime(arg):
         return arg
 
 def normalize_date(dt):
+    if isinstance(dt, np.datetime64):
+        dt = _dt_box(dt)
     return datetime(dt.year, dt.month, dt.day)
 
 #-------------------------------------------------------------------------------
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 4baa385d1..e8a2bc779 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -1100,6 +1100,7 @@ class DataFrame(NDFrame):
         """
         return self._constructor(data=self.values.T, index=self.columns,
                                  columns=self.index, copy=False)
+
     T = property(transpose)
 
     #----------------------------------------------------------------------
@@ -1861,11 +1862,15 @@ class DataFrame(NDFrame):
                 level_values = lev.values
                 if level_values.dtype == np.object_:
                     level_values = lib.maybe_convert_objects(level_values)
+                if level_values.dtype == np.datetime64:
+                    # converts to datetime
+                    # TODO: need new block type to handle datetime64
+                    level_values = level_values.astype('O')
 
                 new_obj.insert(0, col_name, level_values.take(lab))
         else:
             name = self.index.name
-            if name is None:
+            if name is None or name == 'index':
                 name = 'index' if 'index' not in self else 'level_0'
             new_obj.insert(0, name, self.index.values)
         new_obj.index = np.arange(len(new_obj))
@@ -3930,8 +3935,6 @@ def _convert_object_array(content, columns):
     return sdict, columns
 
 def _homogenize(data, index, columns, dtype=None):
-    from pandas.core.series import _sanitize_array
-
     homogenized = {}
 
     if dtype is not None:
@@ -3958,13 +3961,9 @@ def _homogenize(data, index, columns, dtype=None):
                 # are putting it into an ndarray later
                 v = v.reindex(index, copy=False)
         else:
-            if isinstance(v, dict):
-                if oindex is None:
-                    oindex = index.astype('O')
-                v = lib.fast_multiget(v, oindex, default=np.nan)
-
-            v = _sanitize_array(v, index, dtype=dtype, copy=False,
-                                raise_cast_failure=False)
+            v = Series(v, index=index, dtype=dtype)
+            if oindex is None:
+                oindex = index.astype('O')
 
         homogenized[k] = v
 
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 8ec6d9bf8..5ae2e3dc4 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -310,6 +310,8 @@ class NDFrame(PandasObject):
         new_axes = []
         for k, ax in zip(key, self.axes):
             if k not in ax:
+                if type(k) != ax.dtype.type:
+                    ax = ax.astype('O')
                 new_axes.append(np.concatenate([ax, [k]]))
             else:
                 new_axes.append(ax)
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index e83f22342..1d1debfc9 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -432,6 +432,8 @@ class GroupBy(object):
         result = None
 
         for label, group in self._generate_groups(obj, group_index, ngroups):
+            if group is None:
+                continue
             res = func(group)
             if result is None:
                 try:
diff --git a/pandas/core/index.py b/pandas/core/index.py
index a9978d9a0..b7501893a 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -1,17 +1,19 @@
 # pylint: disable=E1101,E1103,W0232
 
-from datetime import time
+from datetime import time, timedelta
 from itertools import izip
 
 import numpy as np
 
 from pandas.core.common import (adjoin as _adjoin, _stringify, _try_sort,
-                                _is_bool_indexer, _asarray_tuplesafe)
+                                _is_bool_indexer, _asarray_tuplesafe,
+                                _dt_box, _dt_unbox, is_iterator)
 from pandas.util.decorators import cache_readonly
-import pandas.core.common as com
 import pandas._tseries as lib
 import pandas._engines as _gin
 
+from datetime import datetime
+
 __all__ = ['Index']
 
 def _indexOp(opname):
@@ -59,6 +61,9 @@ class Index(np.ndarray):
 
     def __new__(cls, data, dtype=None, copy=False, name=None):
         if isinstance(data, np.ndarray):
+            if dtype is None and issubclass(data.dtype.type, np.datetime64):
+                return DatetimeIndex(data, copy=copy, name=name)
+
             if dtype is None and issubclass(data.dtype.type, np.integer):
                 return Int64Index(data, copy=copy, name=name)
 
@@ -70,6 +75,11 @@ class Index(np.ndarray):
             # other iterable of some kind
             subarr = _asarray_tuplesafe(data, dtype=object)
 
+        if (dtype is None
+            and (lib.is_datetime_array(subarr)
+                 or lib.is_datetime64_array(subarr))):
+            return DatetimeIndex(subarr.astype('M8'), name=name)
+
         if lib.is_integer_array(subarr) and dtype is None:
             return Int64Index(subarr.astype('i8'), name=name)
 
@@ -170,6 +180,9 @@ class Index(np.ndarray):
     def inferred_type(self):
         return lib.infer_dtype(self)
 
+    def is_type_compatible(self, typ):
+        return typ == self.inferred_type
+
     @cache_readonly
     def is_all_dates(self):
         return self.inferred_type == 'datetime'
@@ -338,7 +351,10 @@ class Index(np.ndarray):
     __ge__ = _indexOp('__ge__')
 
     def __sub__(self, other):
-        return self.diff(other)
+        if isinstance(other, (Index, list)):
+            return self.diff(other)
+        else:
+            return Index(self.view(np.ndarray) - other)
 
     def __and__(self, other):
         return self.intersection(other)
@@ -494,7 +510,7 @@ class Index(np.ndarray):
                 raise
             except TypeError:
                 # generator/iterator-like
-                if com.is_iterator(key):
+                if is_iterator(key):
                     raise InvalidIndexError(key)
                 else:
                     raise e1
@@ -825,7 +841,7 @@ class Index(np.ndarray):
         """
         index = np.asarray(self)
         # because numpy is fussy with tuples
-        item_idx = Index([item])
+        item_idx = Index([item], dtype=index.dtype)
         new_index = np.concatenate((index[:loc], item_idx, index[loc:]))
         return Index(new_index, name=self.name)
 
@@ -884,7 +900,8 @@ class Int64Index(Index):
             # other iterable of some kind
             if not isinstance(data, (list, tuple)):
                 data = list(data)
-            data= np.asarray(data)
+
+            data = np.asarray(data)
 
         if issubclass(data.dtype.type, basestring):
             raise TypeError('String dtype not supported, you may need '
@@ -937,9 +954,240 @@ class Int64Index(Index):
         name = self.name if self.name == other.name else None
         return Int64Index(joined, name=name)
 
-class DateIndex(Index):
-    pass
+# -------- some conversion functions for datetime <--> datetime64
+
+def _as_i8(arg):
+    if isinstance(arg, np.ndarray):
+        return arg.view('i8', type=np.ndarray)
+    else:
+        return arg
+
+def _wrap_i8_function(f):
+    @staticmethod
+    def wrapper(*args, **kwargs):
+        view_args = [_as_i8(arg) for arg in args]
+        return f(*view_args, **kwargs)
+    return wrapper
+
+def _dt_index_box(arg):
+    if isinstance(arg, np.ndarray):
+        return arg.astype('O')
+    else:
+        return arg
+
+def _wrap_dt_function(f):
+    @staticmethod
+    def wrapper(*args, **kwargs):
+        view_args = [_dt_index_box(arg) for arg in args]
+        return f(*view_args, **kwargs)
+    return wrapper
+
+def _join_i8_wrapper(joinf, with_indexers=True):
+    @staticmethod
+    def wrapper(left, right):
+        if isinstance(left, np.ndarray):
+            left = left.view('i8', type=np.ndarray)
+        if isinstance(right, np.ndarray):
+            right = right.view('i8', type=np.ndarray)
+        results = joinf(left, right)
+        if with_indexers:
+            join_index, left_indexer, right_indexer = results
+            join_index = join_index.view('M8')
+            return join_index, left_indexer, right_indexer
+        return results
+    return wrapper
+
+def _dt_index_cmp(opname):
+    """
+    Wrap comparison operations to unbox a datetime to a datetime64.
+    """
+    def wrapper(self, other):
+        if isinstance(other, datetime):
+            func = getattr(self, opname)
+            return func(_dt_unbox(other))
+        else:
+            func = getattr(super(DatetimeIndex, self), opname)
+            return func(other)
+    return wrapper
+
+def _dt_index_op(opname):
+    """
+    Wrap arithmetic operations to unbox a datetime to a datetime64.
+    """
+    def wrapper(self, other):
+        if isinstance(other, timedelta):
+            func = getattr(self, opname)
+            return func(np.timedelta64(other))
+        else:
+            func = getattr(super(DatetimeIndex, self), opname)
+            return func(other)
+    return wrapper
 
+class DatetimeIndex(Int64Index):
+
+    _is_monotonic  = _wrap_i8_function(lib.is_monotonic_int64)
+    _inner_indexer = _join_i8_wrapper(lib.inner_join_indexer_int64)
+    _outer_indexer = _join_i8_wrapper(lib.outer_join_indexer_int64)
+    _left_indexer  = _join_i8_wrapper(lib.left_join_indexer_int64,
+                                      with_indexers=False)
+    _merge_indexer = _join_i8_wrapper(lib.merge_indexer_int64,
+                                      with_indexers=False)
+    _map_indices   = _wrap_i8_function(lib.map_indices_int64)
+    _pad           = _wrap_i8_function(lib.pad_int64)
+    _backfill      = _wrap_i8_function(lib.backfill_int64)
+
+    _arrmap        = _wrap_dt_function(lib.arrmap_object)
+    _groupby       = _wrap_dt_function(lib.groupby_object)
+
+    __eq__ = _dt_index_cmp('__eq__')
+    __ne__ = _dt_index_cmp('__ne__')
+    __lt__ = _dt_index_cmp('__lt__')
+    __gt__ = _dt_index_cmp('__gt__')
+    __le__ = _dt_index_cmp('__le__')
+    __ge__ = _dt_index_cmp('__ge__')
+
+    __add__ = _dt_index_op('__add__')
+    __sub__ = _dt_index_op('__sub__')
+
+    def __new__(cls, data, dtype=None, copy=False, name=None):
+        if not isinstance(data, np.ndarray):
+            if np.isscalar(data):
+                raise ValueError('DatetimeIndex() must be called with a '
+                                 'collection of some kind, %s was passed'
+                                 % repr(data))
+
+            # other iterable of some kind
+            if not isinstance(data, (list, tuple)):
+                data = list(data)
+
+            # try to make it datetime64
+            data = np.asarray(data, dtype=np.datetime64)
+
+        if issubclass(data.dtype.type, basestring):
+            raise TypeError('String dtype not supported, you may need '
+                            'to explicitly cast to datetime64')
+        elif issubclass(data.dtype.type, np.integer):
+            subarr = np.array(data, dtype=np.datetime64, copy=copy)
+        elif issubclass(data.dtype.type, np.datetime64):
+            subarr = np.array(data, dtype=np.datetime64, copy=copy)
+        else:
+            subarr = np.array(data, dtype=np.datetime64, copy=copy)
+            if len(data) > 0:
+                test = (subarr != data)
+                if (type(test) == bool and test == True) or test.any():
+                    raise TypeError('Unsafe NumPy casting, you must '
+                                    'explicitly cast')
+
+        subarr = subarr.view(cls)
+        subarr.name = name
+        return subarr
+
+    def __getitem__(self, key):
+        """Override numpy.ndarray's __getitem__ method to work as desired"""
+        arr_idx = self.view(np.ndarray)
+        if np.isscalar(key):
+            if type(key) == datetime:
+                key = _dt_unbox(key)
+            return _dt_box(arr_idx[key])
+        else:
+            if _is_bool_indexer(key):
+                key = np.asarray(key)
+
+            result = arr_idx[key]
+            if result.ndim > 1:
+                return result
+
+            return DatetimeIndex(result, name=self.name)
+
+    def __iter__(self):
+        # TODO: again, figure out how to expose elements as nice datetime
+        # objects so you can do obj.year etc
+        return iter(self.values.astype('O'))
+
+    def searchsorted(self, key, side='left'):
+        """
+        Workaround numpy coredump in searchsorted
+        """
+        if isinstance(key, datetime):
+            key = _dt_unbox(key)
+        elif isinstance(key, np.ndarray):
+            key = np.array(key, dtype=np.datetime64, copy=False)
+        elif not isinstance(key, np.datetime64):
+            raise TypeError("Key %s is unrecognized type" % key)
+        return self.values.searchsorted(key, side=side)
+
+    def is_type_compatible(self, typ):
+        return typ == self.inferred_type or typ == 'datetime'
+
+    # hack to workaround argmin failure
+    def argmin(self):
+        return (-self).argmax()
+
+    @property
+    def inferred_type(self):
+        # b/c datetime is represented as microseconds since the epoch, make
+        # sure we can't have ambiguous indexing
+        return 'datetime64'
+
+    @property
+    def _constructor(self):
+        return DatetimeIndex
+
+    @property
+    def dtype(self):
+        return np.dtype('M8')
+
+    @property
+    def is_all_dates(self):
+        return True
+
+    @cache_readonly
+    def _engine(self):
+        view = self.view('i8', type=np.ndarray)
+        mapping = lib.map_indices_int64
+        return _gin.DictIndexEngineDatetime(view, mapping)
+
+    def equals(self, other):
+        """
+        Determines if two Index objects contain the same elements.
+        """
+        if self is other:
+            return True
+
+        if other.inferred_type == 'datetime64':
+            other = other.view('i8', type=np.ndarray)
+        elif other.inferred_type == 'datetime':
+            # TODO: faster conversion from datetime object to datetime64?
+            other = np.array(other, dtype='M8', copy=False)
+            other = other.view('i8', type=np.ndarray)
+        elif len(other) == 0 and len(self) == 0 and other.dtype == object:
+            # fun corner case
+            return True
+
+        selfi8 = self.view('i8', type=np.ndarray)
+        return np.array_equal(selfi8, other)
+
+    def insert(self, loc, item):
+        """
+        Make new Index inserting new item at location
+
+        Parameters
+        ----------
+        loc : int
+        item : object
+
+        Returns
+        -------
+        new_index : Index
+        """
+        if type(item) == datetime:
+            item = _dt_unbox(item)
+
+        return super(DatetimeIndex, self).insert(loc, item)
+
+    def _wrap_joined_index(self, joined, other):
+        name = self.name if self.name == other.name else None
+        return DatetimeIndex(joined, name=name)
 
 class Factor(np.ndarray):
     """
@@ -990,9 +1238,11 @@ class Factor(np.ndarray):
             return np.ndarray.__getitem__(self, key)
 
 def unique_with_labels(values):
-    uniques = Index(lib.fast_unique(values))
-    labels = lib.get_unique_labels(values, uniques.indexMap)
-    uniques._cleanup()
+    rizer = lib.Factorizer(len(values))
+    labels, counts = rizer.factorize(values, sort=True)
+    # TODO: fix if this necessary, is factorize supposed to sort it?
+    rizer.uniques.sort()
+    uniques = Index(rizer.uniques)
     return uniques, labels
 
 class MultiIndex(Index):
@@ -1143,7 +1393,7 @@ class MultiIndex(Index):
                 raise
             except TypeError:
                 # generator/iterator-like
-                if com.is_iterator(key):
+                if is_iterator(key):
                     raise InvalidIndexError(key)
                 else:
                     raise e1
@@ -1651,7 +1901,7 @@ class MultiIndex(Index):
             section = labs[start:end]
 
             if lab not in lev:
-                if lib.infer_dtype([lab]) != lev.inferred_type:
+                if not lev.is_type_compatible(lib.infer_dtype([lab])):
                     raise Exception('Level type mismatch: %s' % lab)
 
                 # short circuit
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index e3cb28073..41f15bbeb 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -368,7 +368,8 @@ def _maybe_convert_ix(*args):
         return args
 
 def _is_integer_dtype(arr):
-    return issubclass(arr.dtype.type, np.integer)
+    return (issubclass(arr.dtype.type, np.integer) and 
+            not arr.dtype.type == np.datetime64)
 
 def _is_integer_index(index):
     return index.inferred_type == 'integer'
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 5a99d456a..7db65dfe1 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -32,6 +32,7 @@ import pandas.core.nanops as nanops
 import pandas._tseries as lib
 from pandas.util.decorators import Appender, Substitution
 
+
 __all__ = ['Series', 'TimeSeries']
 
 _np_version = np.version.short_version
@@ -148,6 +149,19 @@ _doc_exclude_na = "NA/null values are excluded"
 _doc_ndarray_interface = ("Extra parameters are to preserve ndarray"
                           "interface.\n")
 
+def _series_dict_handler(data, index, dtype, copy):
+    if index is None:
+        index = Index(sorted(data))
+    else:
+        index = _ensure_index(index)
+
+    data_index = Index(data)
+    indexer = data_index.get_indexer(index)
+    values = _sanitize_array(data.values(), len(index), 
+                             dtype, copy, raise_cast_failure=True)
+    data = com.take_1d(values, indexer)
+    return data, index
+
 #-------------------------------------------------------------------------------
 # Series class
 
@@ -169,16 +183,11 @@ class Series(np.ndarray, generic.PandasObject):
             if index is None:
                 index = data.index
         elif isinstance(data, dict):
-            if index is None:
-                index = Index(sorted(data))
-            else:
-                index = _ensure_index(index)
-            try:
-                data = lib.fast_multiget(data, index, default=np.nan)
-            except TypeError:
-                data = [data.get(i, nan) for i in index]
+            data, index = _series_dict_handler(data, index, dtype, copy)
+
+        idxlen = len(index) if index is not None else None
 
-        subarr = _sanitize_array(data, index, dtype, copy,
+        subarr = _sanitize_array(data, idxlen, dtype, copy,
                                  raise_cast_failure=True)
 
         if not isinstance(subarr, np.ndarray):
@@ -2129,7 +2138,7 @@ def remove_na(arr):
     return arr[notnull(arr)]
 
 
-def _sanitize_array(data, index, dtype=None, copy=False,
+def _sanitize_array(data, idxlen, dtype=None, copy=False,
                     raise_cast_failure=False):
     if isinstance(data, ma.MaskedArray):
         mask = ma.getmaskarray(data)
@@ -2147,7 +2156,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
     if subarr.ndim == 0:
         if isinstance(data, list):  # pragma: no cover
             subarr = np.array(data, dtype=object)
-        elif index is not None:
+        elif idxlen is not None:
             value = data
 
             # If we create an empty array using a string to infer
@@ -2159,9 +2168,9 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                 dtype = np.object_
 
             if dtype is None:
-                subarr = np.empty(len(index), dtype=type(value))
+                subarr = np.empty(idxlen, dtype=type(value))
             else:
-                subarr = np.empty(len(index), dtype=dtype)
+                subarr = np.empty(idxlen, dtype=dtype)
             subarr.fill(value)
         else:
             return subarr.item()
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index a4e129b3a..1954591cc 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -482,7 +482,11 @@ class HDFStore(object):
         if 'name' in node._v_attrs:
             name = node._v_attrs.name
 
-        index = Index(_unconvert_index(data, kind))
+        if kind in ('date', 'datetime'):
+            index = Index(_unconvert_index(data, kind), dtype=object)
+        else:
+            index = Index(_unconvert_index(data, kind))
+
         index.name = name
 
         return name, index
@@ -511,6 +515,8 @@ class HDFStore(object):
             vlarr = self.handle.createVLArray(group, key,
                                               _tables().ObjectAtom())
             vlarr.append(value)
+        elif value.dtype == np.datetime64:
+            self.handle.createArray(group, key, value.view('i8'))
         else:
             self.handle.createArray(group, key, value)
 
@@ -627,14 +633,15 @@ class HDFStore(object):
         from pandas.core.common import _asarray_tuplesafe
 
         table = getattr(group, 'table')
+        fields = table._v_attrs.fields
 
         # create the selection
-        sel = Selection(table, where)
+        sel = Selection(table, where, table._v_attrs.index_kind)
         sel.select()
-        fields = table._v_attrs.fields
 
         columns = _maybe_convert(sel.values['column'],
                                  table._v_attrs.columns_kind)
+
         index = _maybe_convert(sel.values['index'],
                                table._v_attrs.index_kind)
         # reconstruct
@@ -674,7 +681,7 @@ class HDFStore(object):
         table = getattr(group, 'table')
 
         # create the selection
-        s = Selection(table,where)
+        s = Selection(table, where, table._v_attrs.index_kind)
         s.select_coords()
 
         # delete the rows in reverse order
@@ -686,10 +693,16 @@ class HDFStore(object):
         return len(s.values)
 
 def _convert_index(index):
+    inferred_type = lib.infer_dtype(index)
+
     # Let's assume the index is homogeneous
     values = np.asarray(index)
 
-    if isinstance(values[0], (datetime, date)):
+    if inferred_type == 'datetime64':
+        converted = values.view('i8')
+        return converted, 'datetime64', _tables().Int64Col()
+    elif inferred_type == 'datetime':
+        # backward compatibility handling with < 0.7.0
         if isinstance(values[0], datetime):
             kind = 'datetime'
         else:
@@ -697,20 +710,25 @@ def _convert_index(index):
         converted = np.array([time.mktime(v.timetuple()) for v in values],
                              dtype=np.int64)
         return converted, kind, _tables().Time64Col()
-    elif isinstance(values[0], basestring):
-        converted = np.array(list(values), dtype=np.str_)
-        itemsize = converted.dtype.itemsize
-        return converted, 'string', _tables().StringCol(itemsize)
-    elif com.is_integer(values[0]):
-        # take a guess for now, hope the values fit
+    elif inferred_type == 'integer':
         atom = _tables().Int64Col()
         return np.asarray(values, dtype=np.int64), 'integer', atom
-    elif com.is_float(values[0]):
+    elif inferred_type == 'floating':
         atom = _tables().Float64Col()
         return np.asarray(values, dtype=np.float64), 'float', atom
-    else: # pragma: no cover
+        pass
+    elif inferred_type == 'boolean':
+        atom = _tables().Int64Col()
+        return np.asarray(values, dtype=np.int64), 'integer', atom
+    elif inferred_type == 'string':
+        converted = np.array(list(values), dtype=np.str_)
+        itemsize = converted.dtype.itemsize
+        return converted, 'string', _tables().StringCol(itemsize)
+    elif inferred_type == 'mixed':
         atom = _tables().ObjectAtom()
         return np.asarray(values, dtype='O'), 'object', atom
+    else:
+        raise TypeError("Unrecognized inferred type '%s'" % inferred_type)
 
 def _read_array(group, key):
     import tables
@@ -723,13 +741,13 @@ def _read_array(group, key):
         return data
 
 def _unconvert_index(data, kind):
-    if kind == 'datetime':
+    if kind == 'datetime64':
+        index = np.array(data, dtype=np.datetime64)
+    elif kind == 'datetime':
         index = np.array([datetime.fromtimestamp(v) for v in data],
                          dtype=object)
     elif kind == 'date':
-        index = np.array([date.fromtimestamp(v) for v in data],
-                         dtype=object)
-
+        index = np.array([date.fromtimestamp(v) for v in data], dtype=object)
     elif kind in ('string', 'integer', 'float'):
         index = np.array(data)
     elif kind == 'object':
@@ -755,13 +773,15 @@ def _maybe_convert(values, val_kind):
     return values
 
 def _get_converter(kind):
+    if kind == 'datetime64':
+        return lambda x: np.datetime64(x)
     if kind == 'datetime':
         return datetime.fromtimestamp
     else: # pragma: no cover
         raise ValueError('invalid kind %s' % kind)
 
 def _need_convert(kind):
-    if kind == 'datetime':
+    if kind in ('datetime', 'datetime64'):
         return True
     return False
 
@@ -794,9 +814,10 @@ class Selection(object):
            {'field' : 'index',
             'value' : [v1, v2, v3]}
     """
-    def __init__(self, table, where=None):
+    def __init__(self, table, where=None, index_kind=None):
         self.table = table
         self.where = where
+        self.index_kind = index_kind
         self.column_filter = None
         self.the_condition = None
         self.conditions = []
@@ -811,7 +832,10 @@ class Selection(object):
             value = c['value']
             field = c['field']
 
-            if field == 'index' and isinstance(value, datetime):
+            if field == 'index' and self.index_kind == 'datetime64':
+                val = np.datetime64(value).view('i8')
+                self.conditions.append('(%s %s %s)' % (field,op,val))
+            elif field == 'index' and isinstance(value, datetime):
                 value = time.mktime(value.timetuple())
                 self.conditions.append('(%s %s %s)' % (field,op,value))
             else:
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index 7698bcf39..58ade4383 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -156,7 +156,7 @@ c,4,5
 """
         df = read_csv(StringIO(data), parse_dates=True)
         expected = read_csv(StringIO(data), index_col=0, parse_dates=True)
-        self.assert_(isinstance(df.index[0], datetime))
+        self.assert_(isinstance(df.index[0], (datetime, np.datetime64)))
         assert_frame_equal(df, expected)
 
     def test_no_header(self):
@@ -196,7 +196,7 @@ baz,7,8,9
         df2 = read_table(self.csv1, sep=',', index_col=0, parse_dates=True)
         self.assert_(np.array_equal(df.columns, ['A', 'B', 'C', 'D']))
         self.assert_(df.index.name == 'index')
-        self.assert_(isinstance(df.index[0], datetime))
+        self.assert_(isinstance(df.index[0], (datetime, np.datetime64)))
         self.assert_(df.values.dtype == np.float64)
         assert_frame_equal(df, df2)
 
@@ -204,7 +204,7 @@ baz,7,8,9
         df = read_csv(self.csv2, index_col=0, parse_dates=True)
         df2 = read_table(self.csv2, sep=',', index_col=0, parse_dates=True)
         self.assert_(np.array_equal(df.columns, ['A', 'B', 'C', 'D', 'E']))
-        self.assert_(isinstance(df.index[0], datetime))
+        self.assert_(isinstance(df.index[0], (datetime, np.datetime64)))
         self.assert_(df.ix[:, ['A', 'B', 'C', 'D']].values.dtype == np.float64)
         assert_frame_equal(df, df2)
 
@@ -424,11 +424,13 @@ bar,two,12,13,14,15
 20090103,three,c,4,5
 """
         df = read_csv(StringIO(data), index_col=[0, 1], parse_dates=True)
-        self.assert_(isinstance(df.index.levels[0][0], datetime))
+        self.assert_(isinstance(df.index.levels[0][0],
+                     (datetime, np.datetime64)))
 
         # specify columns out of order!
         df2 = read_csv(StringIO(data), index_col=[1, 0], parse_dates=True)
-        self.assert_(isinstance(df2.index.levels[1][0], datetime))
+        self.assert_(isinstance(df2.index.levels[1][0],
+                     (datetime, np.datetime64)))
 
     def test_skip_footer(self):
         data = """A,B,C
diff --git a/pandas/src/engines.pyx b/pandas/src/engines.pyx
index e08585939..da813a90d 100644
--- a/pandas/src/engines.pyx
+++ b/pandas/src/engines.pyx
@@ -1,11 +1,20 @@
 from numpy cimport ndarray
+
 cimport numpy as cnp
-cimport cpython
 
 cnp.import_array()
 
 cimport util
 
+import numpy as np
+
+
+cdef extern from "datetime.h":
+    bint PyDateTime_Check(object o)
+    void PyDateTime_IMPORT()
+
+PyDateTime_IMPORT
+
 cdef extern from "Python.h":
     int PySlice_Check(object)
 
@@ -106,6 +115,44 @@ cdef class DictIndexEngine(IndexEngine):
             raise Exception('Index values are not unique')
         return self.mapping[val]
 
+cdef class DictIndexEngineDatetime(DictIndexEngine):
+
+    def __contains__(self, object val):
+        self._ensure_initialized()
+
+        if util.is_datetime64_object(val):
+            return val.view('i8') in self.mapping
+
+        if PyDateTime_Check(val):
+            key = np.datetime64(val)
+            return key.view('i8') in self.mapping
+
+        return val in self.mapping
+
+    cpdef get_loc(self, object val):
+        if is_definitely_invalid_key(val):
+            raise TypeError
+
+        self._ensure_initialized()
+        if not self.integrity:
+            raise Exception('Index values are not unique')
+
+        if util.is_datetime64_object(val):
+            val = val.view('i8')
+
+        if PyDateTime_Check(val):
+            val = np.datetime64(val)
+            val = val.view('i8')
+
+        return self.mapping[val]
+
+    cdef initialize(self):
+        # already passed a view on ndarray
+        values = self.index_weakref
+        self.mapping = self.mapfun(values)
+        if len(self.mapping) == len(values):
+            self.integrity = 1
+        self.initialized = 1
 
 cdef inline is_definitely_invalid_key(object val):
     return PySlice_Check(val) or cnp.PyArray_Check(val)
diff --git a/pandas/src/generate_code.py b/pandas/src/generate_code.py
index a0c8057ef..5bf57acfa 100644
--- a/pandas/src/generate_code.py
+++ b/pandas/src/generate_code.py
@@ -82,8 +82,12 @@ def take_2d_axis1_%(name)s(ndarray[%(c_type)s, ndim=2] values,
 
 """
 
-set_na = "outbuf[i] = NaN"
-set_na_2d = "outbuf[i, j] = NaN"
+def set_na(na ="NaN"):
+    return "outbuf[i] = %s" % na
+
+def set_na_2d(na = "NaN"):
+    return "outbuf[i, j] = %s" % na
+
 raise_on_na = "raise ValueError('No NA values allowed')"
 
 merge_indexer_template = """@cython.wraparound(False)
@@ -611,25 +615,27 @@ def generate_put_functions():
         output.write(func)
     return output.getvalue()
 
-# name, ctype, capable of holding NA
+
+# name, ctype, capable of holding NA, NA representation
 function_list = [
-    ('float64', 'float64_t', 'np.float64', True),
-    ('object', 'object', 'object', True),
-    ('int32', 'int32_t', 'np.int32', False),
-    ('int64', 'int64_t', 'np.int64', False),
-    ('bool', 'uint8_t', 'np.bool', False)
+    ('float64', 'float64_t', 'np.float64', True, 'NaN'),
+    ('object', 'object', 'object', True, 'NaN'),
+    ('int32', 'int32_t', 'np.int32', False, ''),
+    ('int64', 'int64_t', 'np.int64', False, ''),
+    ('bool', 'uint8_t', 'np.bool', False, ''),
 ]
 
 def generate_from_template(template, ndim=1, exclude=None):
     output = StringIO()
-    for name, c_type, dtype, can_hold_na in function_list:
+    for name, c_type, dtype, can_hold_na, na_val in function_list:
         if exclude is not None and name in exclude:
             continue
 
         if ndim == 1:
-            na_action = set_na if can_hold_na else raise_on_na
+            na_action = set_na(na=na_val) if can_hold_na else raise_on_na
         elif ndim == 2:
-            na_action = set_na_2d if can_hold_na else raise_on_na
+            na_action = set_na_2d(na=na_val) if can_hold_na else raise_on_na
+
         func = template % {'name' : name, 'c_type' : c_type,
                            'dtype' : dtype, 'na_action' : na_action}
         output.write(func)
diff --git a/pandas/src/inference.pyx b/pandas/src/inference.pyx
index a87b9feaf..044862b27 100644
--- a/pandas/src/inference.pyx
+++ b/pandas/src/inference.pyx
@@ -15,14 +15,15 @@ _TYPE_MAP = {
     np.complex128: 'complex',
     np.string_: 'string',
     np.unicode_: 'unicode',
-    np.bool_: 'boolean'
+    np.bool_: 'boolean',
+    np.datetime64 : 'datetime64'
 }
 
 try:
     _TYPE_MAP[np.float128] = 'floating'
     _TYPE_MAP[np.complex256] = 'complex'
     _TYPE_MAP[np.float16] = 'floating'
-    _TYPE_MAP[np.datetime64] = 'datetime64'
+
 except AttributeError:
     pass
 
@@ -50,7 +51,11 @@ def infer_dtype(object _values):
 
     test_val = util.get_value_1d(values, 0)
 
-    if util.is_integer_object(test_val):
+    if util.is_datetime64_object(test_val):
+        if is_datetime64_array(values):
+            return 'datetime64'
+
+    elif util.is_integer_object(test_val):
         if is_integer_array(values):
             return 'integer'
 
@@ -60,7 +65,6 @@ def infer_dtype(object _values):
 
     elif util.is_float_object(test_val):
         if is_float_array(values):
-
             return 'floating'
 
     elif util.is_bool_object(test_val):
@@ -84,7 +88,6 @@ cdef inline bint is_datetime(object o):
 cpdef is_array(object o):
     return np.PyArray_Check(o)
 
-
 def is_bool_array(ndarray values):
     cdef:
         Py_ssize_t i, n = len(values)
@@ -181,6 +184,14 @@ def is_datetime_array(ndarray[object] values):
             return False
     return True
 
+def is_datetime64_array(ndarray values):
+    cdef int i, n = len(values)
+    if n == 0:
+        return False
+    for i in range(n):
+        if not util.is_datetime64_object(values[i]):
+            return False
+    return True
 
 def maybe_convert_numeric(ndarray[object] values, set na_values):
     '''
@@ -265,6 +276,10 @@ def maybe_convert_objects(ndarray[object] objects, bint try_float=0):
         elif util.is_bool_object(val):
             seen_bool = 1
             bools[i] = val
+        elif util.is_datetime64_object(val):
+            # convert to datetime.datetime for now
+            seen_object = 1
+            objects[i] = val.astype('O')
         elif util.is_integer_object(val):
             seen_int = 1
             floats[i] = <float64_t> val
diff --git a/pandas/src/numpy_helper.h b/pandas/src/numpy_helper.h
index 56106803b..46debcfe1 100644
--- a/pandas/src/numpy_helper.h
+++ b/pandas/src/numpy_helper.h
@@ -18,6 +18,7 @@
 #define PANDAS_BOOL 2
 #define PANDAS_STRING 3
 #define PANDAS_OBJECT 4
+#define PANDAS_DATETIME 5
 
 PANDAS_INLINE int
 infer_type(PyObject* obj) {
@@ -27,6 +28,9 @@ infer_type(PyObject* obj) {
   else if (PyArray_IsIntegerScalar(obj)) {
     return PANDAS_INT;
   }
+  else if (PyArray_IsScalar(obj, Datetime)) {
+    return PANDAS_DATETIME;
+  }
   else if (PyFloat_Check(obj) || PyArray_IsScalar(obj, Floating)) {
     return PANDAS_FLOAT;
   }
@@ -59,6 +63,11 @@ is_string_object(PyObject* obj) {
   return (PyString_Check(obj) || PyUnicode_Check(obj));
 }
 
+PANDAS_INLINE int
+is_datetime64_object(PyObject *obj) {
+  return PyArray_IsScalar(obj, Datetime);
+}
+
 PANDAS_INLINE int
 assign_value_1d(PyArrayObject* ap, Py_ssize_t _i, PyObject* v) {
   npy_intp i = (npy_intp) _i;
@@ -125,3 +134,4 @@ PANDAS_INLINE PyObject* floatify(PyObject* str) {
 //   }
 //   return ap;
 // }
+
diff --git a/pandas/src/util.pxd b/pandas/src/util.pxd
index 12fe0d6dc..30697b959 100644
--- a/pandas/src/util.pxd
+++ b/pandas/src/util.pxd
@@ -2,10 +2,11 @@ from numpy cimport ndarray
 cimport numpy as cnp
 
 cdef extern from "numpy_helper.h":
-    inline bint is_integer_object(object)
-    inline bint is_float_object(object)
-    inline bint is_bool_object(object)
-    inline bint is_string_object(object)
+    inline int is_integer_object(object)
+    inline int is_float_object(object)
+    inline int is_bool_object(object)
+    inline int is_string_object(object)
+    inline int is_datetime64_object(object)
     inline int assign_value_1d(ndarray, Py_ssize_t, object) except -1
     inline object get_value_1d(ndarray, Py_ssize_t)
     inline char *get_c_string(object)
diff --git a/pandas/tests/test_daterange.py b/pandas/tests/test_daterange.py
index 9b94b6fc3..3d5a62d9a 100644
--- a/pandas/tests/test_daterange.py
+++ b/pandas/tests/test_daterange.py
@@ -5,9 +5,10 @@ import unittest
 import numpy as np
 
 import pandas.core.datetools as datetools
-from pandas.core.index import Index
+from pandas.core.index import Index, DatetimeIndex
 from pandas.core.daterange import DateRange, generate_range
 import pandas.core.daterange as daterange
+import pandas.util.testing as tm
 
 try:
     import pytz
@@ -220,12 +221,12 @@ class TestDateRange(unittest.TestCase):
         self.assert_(isinstance(the_int, DateRange))
         self.assert_(the_int.offset == rng.offset)
 
-        the_int = rng1.intersection(rng2.view(Index))
+        the_int = rng1.intersection(rng2.view(DatetimeIndex))
         self.assert_(the_int.equals(expected))
 
         # non-overlapping
         the_int = rng[:10].intersection(rng[10:])
-        expected = Index([])
+        expected = DatetimeIndex([])
         self.assert_(the_int.equals(expected))
 
     def test_with_tzinfo(self):
@@ -350,6 +351,17 @@ class TestDateRange(unittest.TestCase):
         result = rng1.union(rng2)
         self.assert_(type(result) == DateRange)
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestDatetime64Range(TestDateRange):
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestDatetime64Range, self).setUp()
+
+        def tearDown(self):
+            super(TestDatetime64Range, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 def _skip_if_no_pytz():
     try:
         import pytz
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index 4ddc88c16..c5b301a53 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -4378,6 +4378,20 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         exp = Y['g'].sum()
         self.assert_(isnull(Y['g']['c']))
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestDataFrameDatetime64(TestDataFrame):
+        '''
+        Same tests as for TestSeries, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestDataFrameDatetime64, self).setUp()
+
+        def tearDown(self):
+            super(TestDataFrameDatetime64, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 if __name__ == '__main__':
     # unittest.main()
     import nose
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 5ff89db4c..391b64a8f 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -430,7 +430,7 @@ class TestGroupBy(unittest.TestCase):
         indices = grouped.primary.indices
 
         for k, v in groups.iteritems():
-            samething = self.tsframe.index.take(indices[k])
+            samething = self.tsframe.index.take(indices[k]).astype('O')
             self.assert_(np.array_equal(v, samething))
 
     def test_frame_groupby_columns(self):
diff --git a/pandas/tests/test_index.py b/pandas/tests/test_index.py
index 2d39dbadb..09f3ff3f0 100644
--- a/pandas/tests/test_index.py
+++ b/pandas/tests/test_index.py
@@ -14,6 +14,7 @@ from pandas.util import py3compat
 
 import pandas.util.testing as tm
 import pandas._tseries as tseries
+from pandas.core.common import _dt_unbox
 
 class TestIndex(unittest.TestCase):
 
@@ -112,6 +113,10 @@ class TestIndex(unittest.TestCase):
     def test_comparators(self):
         index = self.dateIndex
         element = index[len(index) // 2]
+
+        if tm._test_with_datetime64:
+            element = _dt_unbox(element)
+
         arr = np.array(index)
 
         def _check(op):
@@ -148,7 +153,12 @@ class TestIndex(unittest.TestCase):
 
     def test_getitem(self):
         arr = np.array(self.dateIndex)
-        self.assertEquals(self.dateIndex[5], arr[5])
+        exp = self.dateIndex[5]
+
+        if tm._test_with_datetime64:
+            exp = _dt_unbox(exp)
+
+        self.assertEquals(exp, arr[5])
 
     def test_shift(self):
         shifted = self.dateIndex.shift(0, timedelta(1))
@@ -202,8 +212,12 @@ class TestIndex(unittest.TestCase):
         firstCat = self.strIndex + self.dateIndex
         secondCat = self.strIndex + self.strIndex
 
-        self.assert_(tm.equalContents(np.append(self.strIndex,
-                                                self.dateIndex), firstCat))
+        if self.dateIndex.dtype == np.object_:
+            appended = np.append(self.strIndex, self.dateIndex)
+        else:
+            appended = np.append(self.strIndex, self.dateIndex.astype('O'))
+
+        self.assert_(tm.equalContents(firstCat, appended))
         self.assert_(tm.equalContents(secondCat, self.strIndex))
         tm.assert_contains_all(self.strIndex, firstCat.indexMap)
         tm.assert_contains_all(self.strIndex, secondCat.indexMap)
@@ -624,7 +638,7 @@ class TestInt64Index(unittest.TestCase):
         from datetime import datetime, timedelta
         # corner case, non-Int64Index
         now = datetime.now()
-        other = Index([now + timedelta(i) for i in xrange(4)])
+        other = Index([now + timedelta(i) for i in xrange(4)], dtype=object)
         result = self.index.union(other)
         expected = np.concatenate((self.index, other))
         self.assert_(np.array_equal(result, expected))
@@ -662,6 +676,20 @@ class TestInt64Index(unittest.TestCase):
         repr(s)
         repr(df)
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestDatetime64Index(TestIndex):
+        '''
+        Same tests as for TestIndex, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestDatetime64Index, self).setUp()
+
+        def tearDown(self):
+            super(TestDatetime64Index, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 class TestMultiIndex(unittest.TestCase):
 
     def setUp(self):
@@ -1324,6 +1352,20 @@ class TestMultiIndex(unittest.TestCase):
                                    [0, 1, 2, 0, 0, 1, 2]])
         self.assert_(index.has_duplicates)
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestDatetime64MultiIndex(TestMultiIndex):
+        '''
+        Same tests as for TestIndex, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestDatetime64MultiIndex, self).setUp()
+
+        def tearDown(self):
+            super(TestDatetime64MultiIndex, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 class TestFactor(unittest.TestCase):
 
     def setUp(self):
diff --git a/pandas/tests/test_panel.py b/pandas/tests/test_panel.py
index b6720e5d2..c1e4ee020 100644
--- a/pandas/tests/test_panel.py
+++ b/pandas/tests/test_panel.py
@@ -1181,6 +1181,34 @@ class TestLongPanel(unittest.TestCase):
         # corner case, empty
         df = pivot(np.array([]), np.array([]), np.array([]))
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestPanelDatetime64(TestPanel):
+        '''
+        Same tests as for TestPanel, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestPanelDatetime64, self).setUp()
+
+        def tearDown(self):
+            super(TestPanelDatetime64, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestLongPanelDatetime64(TestLongPanel):
+        '''
+        Same tests as for TestLongPanel, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestLongPanelDatetime64, self).setUp()
+
+        def tearDown(self):
+            super(TestLongPanelDatetime64, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 def test_monotonic():
     pos = np.array([1, 2, 3, 5])
 
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index 6ae8fd792..2540369b8 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -962,12 +962,12 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(result, expected)
 
     def test_append(self):
-        appendedSeries = self.series.append(self.ts)
+        appendedSeries = self.series.append(self.objSeries)
         for idx, value in appendedSeries.iteritems():
             if idx in self.series.index:
                 self.assertEqual(value, self.series[idx])
-            elif idx in self.ts.index:
-                self.assertEqual(value, self.ts[idx])
+            elif idx in self.objSeries.index:
+                self.assertEqual(value, self.objSeries[idx])
             else:
                 self.fail("orphaned index!")
 
@@ -1980,6 +1980,20 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         result = self.ts.dropna()
         self.assertEquals(result.name, self.ts.name)
 
+if tm.PERFORM_DATETIME64_TESTS:
+    class TestSeriesDatetime64(TestSeries):
+        '''
+        Same tests as for TestSeries, but force datetime64 usage"
+        '''
+        def setUp(self):
+            self.dt64_setting = tm._test_with_datetime64
+            tm._test_with_datetime64 = True
+            super(TestSeriesDatetime64, self).setUp()
+
+        def tearDown(self):
+            super(TestSeriesDatetime64, self).tearDown()
+            tm._test_with_datetime64 = self.dt64_setting
+
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
                    exit=False)
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 6d15814fc..87f5c08c4 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -5,6 +5,7 @@ from pandas import Index
 from pandas.util.testing import assert_almost_equal
 import pandas.util.testing as common
 import pandas._tseries as lib
+from datetime import datetime
 
 class TestTseriesUtil(unittest.TestCase):
 
@@ -320,7 +321,19 @@ class TestTypeInference(unittest.TestCase):
         pass
 
     def test_datetime(self):
-        pass
+        arr1 = np.array([1,2,3], dtype='M8')
+        result = lib.infer_dtype(arr1)
+        self.assertEqual(result, 'datetime64')
+
+        result = lib.infer_dtype(np.array(list(arr1), dtype='O'))
+        self.assertEqual(result, 'datetime64')
+
+        arr2 = np.array([datetime(2010,10,5)]*5)
+        result = lib.infer_dtype(arr2)
+        self.assertEqual(result, 'datetime')
+
+        result = lib.infer_dtype(np.array(list(arr2), dtype='O'))
+        self.assertEqual(result, 'datetime')
 
     def test_to_object_array_tuples(self):
         r = (5,6)
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
index 232a18f78..d92a0003b 100644
--- a/pandas/tools/tests/test_merge.py
+++ b/pandas/tools/tests/test_merge.py
@@ -1072,6 +1072,9 @@ class TestConcatenate(unittest.TestCase):
         result = concat(pieces, keys=[0, 1, 2])
         expected = ts.copy()
 
+        ts.index = DatetimeIndex(np.array(ts.index.values, 
+                                          dtype=np.datetime64))
+
         exp_labels = [np.repeat([0, 1, 2], [len(x) for x in pieces]),
                       np.arange(len(ts))]
         exp_index = MultiIndex(levels=[[0, 1, 2], ts.index],
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 65b726a9f..57e3af506 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -19,6 +19,9 @@ import pandas.core.series as series
 import pandas.core.frame as frame
 import pandas.core.panel as panel
 
+from pandas.core.index import DatetimeIndex
+from pandas.core.datetools import BDay
+
 # to_reload = ['index', 'daterange', 'series', 'frame', 'matrix', 'panel']
 # for mod in to_reload:
 #     reload(locals()[mod])
@@ -32,6 +35,12 @@ Panel = panel.Panel
 N = 30
 K = 4
 
+# PERFORM_DATETIME64_TESTS additionally runs each of:
+#   test_(frame|panel|series|index|multiindex|daterange)
+# where date ranges and indices are composed of datetime64[us] dtype
+
+PERFORM_DATETIME64_TESTS = True
+
 def rands(n):
     choices = string.ascii_letters + string.digits
     return ''.join([random.choice(choices) for _ in xrange(n)])
@@ -159,9 +168,8 @@ def makeStringIndex(k):
 def makeIntIndex(k):
     return Index(range(k))
 
-def makeDateIndex(k):
-    dates = list(DateRange(datetime(2000, 1, 1), periods=k))
-    return Index(dates)
+def makeDateObjIndex(k):
+    return Index(makeDateIndex(k), dtype=object)
 
 def makeFloatSeries():
     index = makeStringIndex(N)
@@ -173,22 +181,20 @@ def makeStringSeries():
 
 def makeObjectSeries():
     dateIndex = makeDateIndex(N)
+    dateIndex = Index(dateIndex, dtype=object)
     index = makeStringIndex(N)
     return Series(dateIndex, index=index)
 
-def makeTimeSeries():
-    return Series(randn(N), index=makeDateIndex(N))
-
-def getArangeMat():
-    return np.arange(N * K).reshape((N, K))
-
 def getSeriesData():
     index = makeStringIndex(N)
-
     return dict((c, Series(randn(N), index=index)) for c in getCols(K))
 
-def getTimeSeriesData():
-    return dict((c, makeTimeSeries()) for c in getCols(K))
+def makeDataFrame():
+    data = getSeriesData()
+    return DataFrame(data)
+
+def getArangeMat():
+    return np.arange(N * K).reshape((N, K))
 
 def getMixedTypeDict():
     index = Index(['a', 'b', 'c', 'd', 'e'])
@@ -202,9 +208,22 @@ def getMixedTypeDict():
 
     return index, data
 
-def makeDataFrame():
-    data = getSeriesData()
-    return DataFrame(data)
+_test_with_datetime64 = False
+
+def makeDateIndex(k):
+    dt = datetime(2000,1,1)
+    dr = DateRange(dt, periods=k)
+    if _test_with_datetime64:
+        return DatetimeIndex(dr)
+    else:
+        dates = list(dr)
+        return Index(dates, dtype=object)
+
+def makeTimeSeries():
+    return Series(randn(N), index=makeDateIndex(N))
+
+def getTimeSeriesData():
+    return dict((c, makeTimeSeries()) for c in getCols(K))
 
 def makeTimeDataFrame():
     data = getTimeSeriesData()
diff --git a/setup.py b/setup.py
index 65cf6528d..b91e8f244 100755
--- a/setup.py
+++ b/setup.py
@@ -7,8 +7,6 @@ Lesser GNU General Public License.
 Parts are from lxml (https://github.com/lxml/lxml)
 """
 
-from datetime import datetime
-from glob import glob
 import os
 import sys
 import shutil
@@ -266,7 +264,7 @@ class CheckSDist(sdist):
         sdist.run(self)
 
 class CheckingBuildExt(build_ext):
-    """Subclass build_ext to get clearer report if Cython is neccessary."""
+    """Subclass build_ext to get clearer report if Cython is necessary."""
 
     def check_cython_extensions(self, extensions):
         for ext in extensions:
@@ -289,6 +287,7 @@ cmdclass = {'clean': CleanCommand,
 
 try:
     from Cython.Distutils import build_ext
+    from Cython.Distutils import Extension # to get pyrex debugging symbols
     cython=True
 except ImportError:
     cython=False
@@ -337,6 +336,7 @@ tseries_ext = Extension('pandas._tseries',
                         depends=tseries_depends + ['pandas/src/numpy_helper.h'],
                         sources=[srcpath('tseries', suffix=suffix)],
                         include_dirs=[np.get_include()],
+                        pyrex_gdb=True,
                         # extra_compile_args=['-Wconversion']
                         )
 
@@ -347,6 +347,7 @@ sparse_ext = Extension('pandas._sparse',
 engines_ext = Extension('pandas._engines',
                         depends=['pandas/src/numpy_helper.h'],
                         sources=[srcpath('engines', suffix=suffix)],
+                        pyrex_gdb=True,
                         include_dirs=[np.get_include()])
 
 sandbox_ext = Extension('pandas._sandbox',
diff --git a/test.sh b/test.sh
index b2d34eedd..ecb8a7987 100755
--- a/test.sh
+++ b/test.sh
@@ -1,8 +1,9 @@
 #!/bin/sh
 coverage erase
 # nosetests pandas/tests/test_index.py --with-coverage --cover-package=pandas.core --pdb-failure --pdb
-nosetests -w pandas --with-coverage --cover-package=pandas --pdb-failure --pdb #--cover-inclusive
+#nosetests -w pandas --with-coverage --cover-package=pandas --pdb-failure --pdb #--cover-inclusive
+nosetests -w pandas --with-coverage --cover-package=pandas #--cover-inclusive
 # nosetests -w pandas/io --with-coverage --cover-package=pandas.io --pdb-failure --pdb
 # nosetests -w pandas/core --with-coverage --cover-package=pandas.core --pdb-failure --pdb
 # nosetests -w pandas/stats --with-coverage --cover-package=pandas.stats
-# coverage run runtests.py
\ No newline at end of file
+# coverage run runtests.py
