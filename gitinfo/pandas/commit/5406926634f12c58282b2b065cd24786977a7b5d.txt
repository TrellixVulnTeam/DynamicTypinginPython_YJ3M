commit 5406926634f12c58282b2b065cd24786977a7b5d
Author: lexual <lex@jbadigital.com>
Date:   Sun Jul 13 21:22:12 2014 +1000

    Make consistent use of acronyms, and US spellings for serializ*

diff --git a/doc/README.rst b/doc/README.rst
index c0c821edb..660a3b723 100644
--- a/doc/README.rst
+++ b/doc/README.rst
@@ -140,7 +140,7 @@ last committed version can always be restored from git.
 
 ::
 
-    #omit autosummary and api section
+    #omit autosummary and API section
     python make.py clean
     python make.py --no-api
 
diff --git a/doc/source/gotchas.rst b/doc/source/gotchas.rst
index 1625e6fb7..438e2f79c 100644
--- a/doc/source/gotchas.rst
+++ b/doc/source/gotchas.rst
@@ -500,7 +500,7 @@ parse HTML tables in the top-level pandas io function ``read_html``.
        molasses.  However consider the fact that many tables on the web are not
        big enough for the parsing algorithm runtime to matter. It is more
        likely that the bottleneck will be in the process of reading the raw
-       text from the url over the web, i.e., IO (input-output). For very large
+       text from the URL over the web, i.e., IO (input-output). For very large
        tables, this might not be true.
 
 **Issues with using** |Anaconda|_
diff --git a/doc/source/io.rst b/doc/source/io.rst
index 6f763a97a..fa6ab646a 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -29,7 +29,7 @@
 IO Tools (Text, CSV, HDF5, ...)
 *******************************
 
-The pandas I/O api is a set of top level ``reader`` functions accessed like ``pd.read_csv()`` that generally return a ``pandas``
+The pandas I/O API is a set of top level ``reader`` functions accessed like ``pd.read_csv()`` that generally return a ``pandas``
 object.
 
     * :ref:`read_csv<io.read_csv_table>`
@@ -78,8 +78,8 @@ for some advanced strategies
 
 They can take a number of arguments:
 
-  - ``filepath_or_buffer``: Either a string path to a file, url
-    (including http, ftp, and s3 locations), or any object with a ``read``
+  - ``filepath_or_buffer``: Either a string path to a file, URL
+    (including http, ftp, and S3 locations), or any object with a ``read``
     method (such as an open file or ``StringIO``).
   - ``sep`` or ``delimiter``: A delimiter / separator to split fields
     on. `read_csv` is capable of inferring the delimiter automatically in some
@@ -1100,7 +1100,7 @@ function takes a number of arguments. Only the first is required.
     used. (A sequence should be given if the DataFrame uses MultiIndex).
   - ``mode`` : Python write mode, default 'w'
   - ``encoding``: a string representing the encoding to use if the contents are
-    non-ascii, for python versions prior to 3
+    non-ASCII, for python versions prior to 3
   - ``line_terminator``: Character sequence denoting line end (default '\\n')
   - ``quoting``: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL)
   - ``quotechar``: Character used to quote fields (default '"')
@@ -1184,7 +1184,7 @@ with optional parameters:
 - ``double_precision`` : The number of decimal places to use when encoding floating point values, default 10.
 - ``force_ascii`` : force encoded string to be ASCII, default True.
 - ``date_unit`` : The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us' or 'ns' for seconds, milliseconds, microseconds and nanoseconds respectively. Default 'ms'.
-- ``default_handler`` : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serialisable object.
+- ``default_handler`` : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.
 
 Note ``NaN``'s, ``NaT``'s and ``None`` will be converted to ``null`` and ``datetime`` objects will be converted based on the ``date_format`` and ``date_unit`` parameters.
 
@@ -1208,7 +1208,7 @@ file / string. Consider the following DataFrame and Series:
   sjo = Series(dict(x=15, y=16, z=17), name='D')
   sjo
 
-**Column oriented** (the default for ``DataFrame``) serialises the data as
+**Column oriented** (the default for ``DataFrame``) serializes the data as
 nested JSON objects with column labels acting as the primary index:
 
 .. ipython:: python
@@ -1224,7 +1224,7 @@ but the index labels are now primary:
   dfjo.to_json(orient="index")
   sjo.to_json(orient="index")
 
-**Record oriented** serialises the data to a JSON array of column -> value records,
+**Record oriented** serializes the data to a JSON array of column -> value records,
 index labels are not included. This is useful for passing DataFrame data to plotting
 libraries, for example the JavaScript library d3.js:
 
@@ -1233,7 +1233,7 @@ libraries, for example the JavaScript library d3.js:
   dfjo.to_json(orient="records")
   sjo.to_json(orient="records")
 
-**Value oriented** is a bare-bones option which serialises to nested JSON arrays of
+**Value oriented** is a bare-bones option which serializes to nested JSON arrays of
 values only, column and index labels are not included:
 
 .. ipython:: python
@@ -1241,7 +1241,7 @@ values only, column and index labels are not included:
   dfjo.to_json(orient="values")
   # Not available for Series
 
-**Split oriented** serialises to a JSON object containing separate entries for
+**Split oriented** serializes to a JSON object containing separate entries for
 values, index and columns. Name is also included for ``Series``:
 
 .. ipython:: python
@@ -1252,13 +1252,13 @@ values, index and columns. Name is also included for ``Series``:
 .. note::
 
   Any orient option that encodes to a JSON object will not preserve the ordering of
-  index and column labels during round-trip serialisation. If you wish to preserve
+  index and column labels during round-trip serialization. If you wish to preserve
   label ordering use the `split` option as it uses ordered containers.
 
 Date Handling
 +++++++++++++
 
-Writing in iso date format
+Writing in ISO date format
 
 .. ipython:: python
 
@@ -1268,7 +1268,7 @@ Writing in iso date format
    json = dfd.to_json(date_format='iso')
    json
 
-Writing in iso date format, with microseconds
+Writing in ISO date format, with microseconds
 
 .. ipython:: python
 
@@ -1297,17 +1297,17 @@ Writing to a file, with a date index and a date column
 Fallback Behavior
 +++++++++++++++++
 
-If the JSON serialiser cannot handle the container contents directly it will fallback in the following manner:
+If the JSON serializer cannot handle the container contents directly it will fallback in the following manner:
 
 - if a ``toDict`` method is defined by the unrecognised object then that
-  will be called and its returned ``dict`` will be JSON serialised.
+  will be called and its returned ``dict`` will be JSON serialized.
 - if a ``default_handler`` has been passed to ``to_json`` that will
   be called to convert the object.
 - otherwise an attempt is made to convert the object to a ``dict`` by
   parsing its contents. However if the object is complex this will often fail
   with an ``OverflowError``.
 
-Your best bet when encountering ``OverflowError`` during serialisation
+Your best bet when encountering ``OverflowError`` during serialization
 is to specify a ``default_handler``. For example ``timedelta`` can cause
 problems:
 
@@ -1349,7 +1349,7 @@ The parser will try to parse a ``DataFrame`` if ``typ`` is not supplied or
 is ``None``. To explicitly force ``Series`` parsing, pass ``typ=series``
 
 - ``filepath_or_buffer`` : a **VALID** JSON string or file handle / StringIO. The string could be
-  a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host
+  a URL. Valid URL schemes include http, ftp, S3, and file. For file URLs, a host
   is expected. For instance, a local file could be
   file ://localhost/path/to/table.json
 - ``typ``    : type of object to recover (series or frame), default 'frame'
@@ -1480,7 +1480,7 @@ The Numpy Parameter
   This supports numeric data only. Index and columns labels may be non-numeric, e.g. strings, dates etc.
 
 If ``numpy=True`` is passed to ``read_json`` an attempt will be made to sniff
-an appropriate dtype during deserialisation and to subsequently decode directly
+an appropriate dtype during deserialization and to subsequently decode directly
 to numpy arrays, bypassing the need for intermediate Python objects.
 
 This can provide speedups if you are deserialising a large amount of numeric
@@ -1586,7 +1586,7 @@ Reading HTML Content
 .. versionadded:: 0.12.0
 
 The top-level :func:`~pandas.io.html.read_html` function can accept an HTML
-string/file/url and will parse HTML tables into list of pandas DataFrames.
+string/file/URL and will parse HTML tables into list of pandas DataFrames.
 Let's look at a few examples.
 
 .. note::
diff --git a/doc/source/release.rst b/doc/source/release.rst
index b29c4bc5d..9dc96219f 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -329,7 +329,7 @@ Bug Fixes
 - Bug in groupby dtype conversion with datetimelike (:issue:`5869`)
 - Regression in handling of empty Series as indexers to Series  (:issue:`5877`)
 - Bug in internal caching, related to (:issue:`5727`)
-- Testing bug in reading json/msgpack from a non-filepath on windows under py3 (:issue:`5874`)
+- Testing bug in reading JSON/msgpack from a non-filepath on windows under py3 (:issue:`5874`)
 - Bug when assigning to .ix[tuple(...)] (:issue:`5896`)
 - Bug in fully reindexing a Panel (:issue:`5905`)
 - Bug in idxmin/max with object dtypes (:issue:`5914`)
@@ -649,7 +649,7 @@ API Changes
     Options are seconds, milliseconds, microseconds and nanoseconds.
     (:issue:`4362`, :issue:`4498`).
   - added ``default_handler`` parameter to allow a callable to be passed
-    which will be responsible for handling otherwise unserialisable objects.
+    which will be responsible for handling otherwise unserialiable objects.
     (:issue:`5138`)
 
 - ``Index`` and ``MultiIndex`` changes (:issue:`4039`):
@@ -802,7 +802,7 @@ See :ref:`Internal Refactoring<whatsnew_0130.refactoring>`
 
  - ``swapaxes`` on a ``Panel`` with the same axes specified now return a copy
  - support attribute access for setting
- - ``filter`` supports same api as original ``DataFrame`` filter
+ - ``filter`` supports same API as original ``DataFrame`` filter
  - ``fillna`` refactored to ``core/generic.py``, while > 3ndim is
    ``NotImplemented``
 
@@ -927,7 +927,7 @@ Bug Fixes
   as the docstring says (:issue:`4362`).
 - ``as_index`` is no longer ignored when doing groupby apply (:issue:`4648`,
   :issue:`3417`)
-- JSON NaT handling fixed, NaTs are now serialised to `null` (:issue:`4498`)
+- JSON NaT handling fixed, NaTs are now serialized to `null` (:issue:`4498`)
 - Fixed JSON handling of escapable characters in JSON object keys
   (:issue:`4593`)
 - Fixed passing ``keep_default_na=False`` when ``na_values=None``
@@ -1760,7 +1760,7 @@ Bug Fixes
 - Fixed a bug in the legend of plotting.andrews_curves() (:issue:`3278`)
 - Produce a series on apply if we only generate a singular series and have
   a simple index (:issue:`2893`)
-- Fix Python ascii file parsing when integer falls outside of floating point
+- Fix Python ASCII file parsing when integer falls outside of floating point
   spacing (:issue:`3258`)
 - fixed pretty priniting of sets (:issue:`3294`)
 - Panel() and Panel.from_dict() now respects ordering when give OrderedDict (:issue:`3303`)
@@ -2783,7 +2783,7 @@ Bug Fixes
   (:issue:`1013`)
 - DataFrame.plot(logy=True) has no effect (:issue:`1011`).
 - Broken arithmetic operations between SparsePanel-Panel (:issue:`1015`)
-- Unicode repr issues in MultiIndex with non-ascii characters (:issue:`1010`)
+- Unicode repr issues in MultiIndex with non-ASCII characters (:issue:`1010`)
 - DataFrame.lookup() returns inconsistent results if exact match not present
   (:issue:`1001`)
 - DataFrame arithmetic operations not treating None as NA (:issue:`992`)
