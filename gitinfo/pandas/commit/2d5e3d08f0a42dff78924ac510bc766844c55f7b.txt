commit 2d5e3d08f0a42dff78924ac510bc766844c55f7b
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon Jun 13 23:54:08 2011 -0400

    CRLF -> LF

diff --git a/bench/io_roundtrip.py b/bench/io_roundtrip.py
index 84a3016b3..bb71d3818 100644
--- a/bench/io_roundtrip.py
+++ b/bench/io_roundtrip.py
@@ -1,188 +1,188 @@
-import time, os
-import numpy as np
-
-import la
-import pandas
-
-def timeit(f, iterations):
-    start = time.clock()
-
-    for i in xrange(iterations):
-        f()
-
-    return time.clock() - start
-
-def rountrip_archive(N, iterations=10):
-
-    # Create data
-    arr = np.random.randn(N, N)
-    lar = la.larry(arr)
-    dma = pandas.DataFrame(arr, range(N), range(N))
-
-    # filenames
-    filename_numpy = 'c:/temp/numpy.npz'
-    filename_larry = 'c:/temp/archive.hdf5'
-    filename_pandas = 'c:/temp/pandas_tmp'
-
-    # Delete old files
-    try:
-        os.unlink(filename_numpy)
-    except:
-        pass
-    try:
-        os.unlink(filename_larry)
-    except:
-        pass
-
-    try:
-        os.unlink(filename_pandas)
-    except:
-        pass
-
-    # Time a round trip save and load
-    numpy_f = lambda: numpy_roundtrip(filename_numpy, arr, arr)
-    numpy_time = timeit(numpy_f, iterations) / iterations
-
-    larry_f = lambda: larry_roundtrip(filename_larry, lar, lar)
-    larry_time = timeit(larry_f, iterations) / iterations
-
-    pandas_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
-    pandas_time = timeit(pandas_f, iterations) / iterations
-
-    print 'Numpy (npz)   %7.4f seconds' % numpy_time
-    print 'larry (HDF5)  %7.4f seconds' % larry_time
-    print 'pandas (HDF5) %7.4f seconds' % pandas_time
-
-def numpy_roundtrip(filename, arr1, arr2):
-    np.savez(filename, arr1=arr1, arr2=arr2)
-    npz = np.load(filename)
-    arr1 = npz['arr1']
-    arr2 = npz['arr2']
-
-def larry_roundtrip(filename, lar1, lar2):
-    io = la.IO(filename)
-    io['lar1'] = lar1
-    io['lar2'] = lar2
-    lar1 = io['lar1']
-    lar2 = io['lar2']
-
-def pandas_roundtrip(filename, dma1, dma2):
-    # What's the best way to code this?
-    from pandas.io.pytables import HDFStore
-    store = HDFStore(filename)
-    store['dma1'] = dma1
-    store['dma2'] = dma2
-    dma1 = store['dma1']
-    dma2 = store['dma2']
-
-def pandas_roundtrip_pickle(filename, dma1, dma2):
-    dma1.save(filename)
-    dma1 = pandas.DataFrame.load(filename)
-    dma2.save(filename)
-    dma2 = pandas.DataFrame.load(filename)
-
-
-    In [65]: df1
-    Out[65]:
-                           A              B
-    2000-01-03 00:00:00    -0.1174        -0.941
-    2000-01-04 00:00:00    -0.6034        -0.008094
-    2000-01-05 00:00:00    -0.3816        -0.9338
-    2000-01-06 00:00:00    -0.3298        -0.9548
-    2000-01-07 00:00:00    0.9576         0.4652
-    2000-01-10 00:00:00    -0.7208        -1.131
-    2000-01-11 00:00:00    1.568          0.8498
-    2000-01-12 00:00:00    0.3717         -0.2323
-    2000-01-13 00:00:00    -1.428         -1.997
-    2000-01-14 00:00:00    -1.084         -0.271
-
-
-    In [66]: df1.join?
-    Type:           instancemethod
-    Base Class:     <type 'instancemethod'>
-    <bound method DataFrame.join of                        A              B
-                2000-01-03  <...> 0:00:00    -1.428         -1.997
-                2000-01-14 00:00:00    -1.084         -0.271
-                >
-    Namespace:      Interactive
-    File:           h:\workspace\pandas\pandas\core\frame.py
-    Definition:     df1.join(self, other, on=None, how=None)
-    Docstring:
-        Join columns with other DataFrame either on index or on a key
-    column
-
-    Parameters
-    ----------
-    other : DataFrame
-        Index should be similar to one of the columns in this one
-    on : string, default None
-        Column name to use, otherwise join on index
-    how : {'left', 'right', 'outer', 'inner'}
-        default: 'left' for joining on index, None otherwise
-        How to handle indexes of the two objects.
-          * left: use calling frame's index
-          * right: use input frame's index
-          * outer: form union of indexes
-          * inner: use intersection of indexes
-
-
-    In [67]: df2
-    Out[67]:
-                           C              D
-    2000-01-03 00:00:00    0.2833         -0.1937
-    2000-01-05 00:00:00    1.868          1.207
-    2000-01-07 00:00:00    -0.8586        -0.7367
-    2000-01-11 00:00:00    2.121          0.9104
-    2000-01-13 00:00:00    0.7856         0.9063
-
-
-    In [68]: df1.join(df2)
-    Out[68]:
-                           A              B              C              D
-    2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
-    2000-01-04 00:00:00    -0.6034        -0.008094      NaN            NaN
-    2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
-    2000-01-06 00:00:00    -0.3298        -0.9548        NaN            NaN
-    2000-01-07 00:00:00    0.9576         0.4652         -0.8586        -0.7367
-    2000-01-10 00:00:00    -0.7208        -1.131         NaN            NaN
-    2000-01-11 00:00:00    1.568          0.8498         2.121          0.9104
-    2000-01-12 00:00:00    0.3717         -0.2323        NaN            NaN
-    2000-01-13 00:00:00    -1.428         -1.997         0.7856         0.9063
-    2000-01-14 00:00:00    -1.084         -0.271         NaN            NaN
-
-    In [70]: df1.join(df2, how='inner')
-    Out[70]:
-                           A              B              C              D
-    2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
-    2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
-    2000-01-07 00:00:00    0.9576         0.4652         -0.8586        -0.7367
-    2000-01-11 00:00:00    1.568          0.8498         2.121          0.9104
-    2000-01-13 00:00:00    -1.428         -1.997         0.7856         0.9063
-
-    In [73]: df2
-    Out[73]:
-                           C              D              key
-    2000-01-03 00:00:00    0.2833         -0.1937        0
-    2000-01-05 00:00:00    1.868          1.207          1
-    2000-01-07 00:00:00    -0.8586        -0.7367        0
-    2000-01-11 00:00:00    2.121          0.9104         1
-    2000-01-13 00:00:00    0.7856         0.9063         0
-
-
-    In [74]: df3 = DataFrame({'code' : {0 : 'foo', 1 : 'bar'}})
-
-    In [75]: df3
-    Out[75]:
-         code
-    0    foo
-    1    bar
-
-
-    In [76]: df2.join(df3, on='key')
-    Out[76]:
-                           C              D              code           key
-    2000-01-03 00:00:00    0.2833         -0.1937        foo            0
-    2000-01-05 00:00:00    1.868          1.207          bar            1
-    2000-01-07 00:00:00    -0.8586        -0.7367        foo            0
-    2000-01-11 00:00:00    2.121          0.9104         bar            1
-    2000-01-13 00:00:00    0.7856         0.9063         foo            0
+import time, os
+import numpy as np
+
+import la
+import pandas
+
+def timeit(f, iterations):
+    start = time.clock()
+
+    for i in xrange(iterations):
+        f()
+
+    return time.clock() - start
+
+def rountrip_archive(N, iterations=10):
+
+    # Create data
+    arr = np.random.randn(N, N)
+    lar = la.larry(arr)
+    dma = pandas.DataFrame(arr, range(N), range(N))
+
+    # filenames
+    filename_numpy = 'c:/temp/numpy.npz'
+    filename_larry = 'c:/temp/archive.hdf5'
+    filename_pandas = 'c:/temp/pandas_tmp'
+
+    # Delete old files
+    try:
+        os.unlink(filename_numpy)
+    except:
+        pass
+    try:
+        os.unlink(filename_larry)
+    except:
+        pass
+
+    try:
+        os.unlink(filename_pandas)
+    except:
+        pass
+
+    # Time a round trip save and load
+    numpy_f = lambda: numpy_roundtrip(filename_numpy, arr, arr)
+    numpy_time = timeit(numpy_f, iterations) / iterations
+
+    larry_f = lambda: larry_roundtrip(filename_larry, lar, lar)
+    larry_time = timeit(larry_f, iterations) / iterations
+
+    pandas_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
+    pandas_time = timeit(pandas_f, iterations) / iterations
+
+    print 'Numpy (npz)   %7.4f seconds' % numpy_time
+    print 'larry (HDF5)  %7.4f seconds' % larry_time
+    print 'pandas (HDF5) %7.4f seconds' % pandas_time
+
+def numpy_roundtrip(filename, arr1, arr2):
+    np.savez(filename, arr1=arr1, arr2=arr2)
+    npz = np.load(filename)
+    arr1 = npz['arr1']
+    arr2 = npz['arr2']
+
+def larry_roundtrip(filename, lar1, lar2):
+    io = la.IO(filename)
+    io['lar1'] = lar1
+    io['lar2'] = lar2
+    lar1 = io['lar1']
+    lar2 = io['lar2']
+
+def pandas_roundtrip(filename, dma1, dma2):
+    # What's the best way to code this?
+    from pandas.io.pytables import HDFStore
+    store = HDFStore(filename)
+    store['dma1'] = dma1
+    store['dma2'] = dma2
+    dma1 = store['dma1']
+    dma2 = store['dma2']
+
+def pandas_roundtrip_pickle(filename, dma1, dma2):
+    dma1.save(filename)
+    dma1 = pandas.DataFrame.load(filename)
+    dma2.save(filename)
+    dma2 = pandas.DataFrame.load(filename)
+
+
+    In [65]: df1
+    Out[65]:
+                           A              B
+    2000-01-03 00:00:00    -0.1174        -0.941
+    2000-01-04 00:00:00    -0.6034        -0.008094
+    2000-01-05 00:00:00    -0.3816        -0.9338
+    2000-01-06 00:00:00    -0.3298        -0.9548
+    2000-01-07 00:00:00    0.9576         0.4652
+    2000-01-10 00:00:00    -0.7208        -1.131
+    2000-01-11 00:00:00    1.568          0.8498
+    2000-01-12 00:00:00    0.3717         -0.2323
+    2000-01-13 00:00:00    -1.428         -1.997
+    2000-01-14 00:00:00    -1.084         -0.271
+
+
+    In [66]: df1.join?
+    Type:           instancemethod
+    Base Class:     <type 'instancemethod'>
+    <bound method DataFrame.join of                        A              B
+                2000-01-03  <...> 0:00:00    -1.428         -1.997
+                2000-01-14 00:00:00    -1.084         -0.271
+                >
+    Namespace:      Interactive
+    File:           h:\workspace\pandas\pandas\core\frame.py
+    Definition:     df1.join(self, other, on=None, how=None)
+    Docstring:
+        Join columns with other DataFrame either on index or on a key
+    column
+
+    Parameters
+    ----------
+    other : DataFrame
+        Index should be similar to one of the columns in this one
+    on : string, default None
+        Column name to use, otherwise join on index
+    how : {'left', 'right', 'outer', 'inner'}
+        default: 'left' for joining on index, None otherwise
+        How to handle indexes of the two objects.
+          * left: use calling frame's index
+          * right: use input frame's index
+          * outer: form union of indexes
+          * inner: use intersection of indexes
+
+
+    In [67]: df2
+    Out[67]:
+                           C              D
+    2000-01-03 00:00:00    0.2833         -0.1937
+    2000-01-05 00:00:00    1.868          1.207
+    2000-01-07 00:00:00    -0.8586        -0.7367
+    2000-01-11 00:00:00    2.121          0.9104
+    2000-01-13 00:00:00    0.7856         0.9063
+
+
+    In [68]: df1.join(df2)
+    Out[68]:
+                           A              B              C              D
+    2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
+    2000-01-04 00:00:00    -0.6034        -0.008094      NaN            NaN
+    2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
+    2000-01-06 00:00:00    -0.3298        -0.9548        NaN            NaN
+    2000-01-07 00:00:00    0.9576         0.4652         -0.8586        -0.7367
+    2000-01-10 00:00:00    -0.7208        -1.131         NaN            NaN
+    2000-01-11 00:00:00    1.568          0.8498         2.121          0.9104
+    2000-01-12 00:00:00    0.3717         -0.2323        NaN            NaN
+    2000-01-13 00:00:00    -1.428         -1.997         0.7856         0.9063
+    2000-01-14 00:00:00    -1.084         -0.271         NaN            NaN
+
+    In [70]: df1.join(df2, how='inner')
+    Out[70]:
+                           A              B              C              D
+    2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
+    2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
+    2000-01-07 00:00:00    0.9576         0.4652         -0.8586        -0.7367
+    2000-01-11 00:00:00    1.568          0.8498         2.121          0.9104
+    2000-01-13 00:00:00    -1.428         -1.997         0.7856         0.9063
+
+    In [73]: df2
+    Out[73]:
+                           C              D              key
+    2000-01-03 00:00:00    0.2833         -0.1937        0
+    2000-01-05 00:00:00    1.868          1.207          1
+    2000-01-07 00:00:00    -0.8586        -0.7367        0
+    2000-01-11 00:00:00    2.121          0.9104         1
+    2000-01-13 00:00:00    0.7856         0.9063         0
+
+
+    In [74]: df3 = DataFrame({'code' : {0 : 'foo', 1 : 'bar'}})
+
+    In [75]: df3
+    Out[75]:
+         code
+    0    foo
+    1    bar
+
+
+    In [76]: df2.join(df3, on='key')
+    Out[76]:
+                           C              D              code           key
+    2000-01-03 00:00:00    0.2833         -0.1937        foo            0
+    2000-01-05 00:00:00    1.868          1.207          bar            1
+    2000-01-07 00:00:00    -0.8586        -0.7367        foo            0
+    2000-01-11 00:00:00    2.121          0.9104         bar            1
+    2000-01-13 00:00:00    0.7856         0.9063         foo            0
diff --git a/examples/finance.py b/examples/finance.py
index b801edf59..cc493c76f 100644
--- a/examples/finance.py
+++ b/examples/finance.py
@@ -1,83 +1,83 @@
-"""
-Some examples playing around with yahoo finance data
-"""
-
-from datetime import datetime
-
-import matplotlib.finance as fin
-import numpy as np
-from pylab import show
-
-
-from pandas import Index, DataFrame
-from pandas.core.datetools import BMonthEnd
-from pandas import ols
-
-startDate = datetime(2008, 1, 1)
-endDate = datetime(2009, 9, 1)
-
-def getQuotes(symbol, start, end):
-    quotes = fin.quotes_historical_yahoo(symbol, start, end)
-    dates, open, close, high, low, volume = zip(*quotes)
-
-    data = {
-        'open' : open,
-        'close' : close,
-        'high' : high,
-        'low' : low,
-        'volume' : volume
-    }
-
-    dates = Index([datetime.fromordinal(int(d)) for d in dates])
-    return DataFrame(data, index=dates)
-
-msft = getQuotes('MSFT', startDate, endDate)
-aapl = getQuotes('AAPL', startDate, endDate)
-goog = getQuotes('GOOG', startDate, endDate)
-ibm = getQuotes('IBM', startDate, endDate)
-
-px = DataFrame({'MSFT' : msft['close'],
-                'IBM' : ibm['close'],
-                'GOOG' : goog['close'],
-                'AAPL' : aapl['close']})
-returns = px / px.shift(1) - 1
-
-# Select dates
-
-subIndex = ibm.index[(ibm['close'] > 95) & (ibm['close'] < 100)]
-msftOnSameDates = msft.reindex(subIndex)
-
-# Insert columns
-
-msft['hi-lo spread'] = msft['high'] - msft['low']
-ibm['hi-lo spread'] = ibm['high'] - ibm['low']
-
-# Aggregate monthly
-
-def toMonthly(frame, how):
-    offset = BMonthEnd()
-
-    return frame.groupby(offset.rollforward).aggregate(how)
-
-msftMonthly = toMonthly(msft, np.mean)
-ibmMonthly = toMonthly(ibm, np.mean)
-
-# Statistics
-
-stdev = DataFrame({
-    'MSFT' : msft.std(),
-    'IBM'  : ibm.std()
-})
-
-# Arithmetic
-
-ratios = ibm / msft
-
-# Works with different indices
-
-ratio = ibm / ibmMonthly
-monthlyRatio = ratio.reindex(ibmMonthly.index)
-
-# Ratio relative to past month average
-
-filledRatio = ibm / ibmMonthly.reindex(ibm.index, fillMethod='pad')
+"""
+Some examples playing around with yahoo finance data
+"""
+
+from datetime import datetime
+
+import matplotlib.finance as fin
+import numpy as np
+from pylab import show
+
+
+from pandas import Index, DataFrame
+from pandas.core.datetools import BMonthEnd
+from pandas import ols
+
+startDate = datetime(2008, 1, 1)
+endDate = datetime(2009, 9, 1)
+
+def getQuotes(symbol, start, end):
+    quotes = fin.quotes_historical_yahoo(symbol, start, end)
+    dates, open, close, high, low, volume = zip(*quotes)
+
+    data = {
+        'open' : open,
+        'close' : close,
+        'high' : high,
+        'low' : low,
+        'volume' : volume
+    }
+
+    dates = Index([datetime.fromordinal(int(d)) for d in dates])
+    return DataFrame(data, index=dates)
+
+msft = getQuotes('MSFT', startDate, endDate)
+aapl = getQuotes('AAPL', startDate, endDate)
+goog = getQuotes('GOOG', startDate, endDate)
+ibm = getQuotes('IBM', startDate, endDate)
+
+px = DataFrame({'MSFT' : msft['close'],
+                'IBM' : ibm['close'],
+                'GOOG' : goog['close'],
+                'AAPL' : aapl['close']})
+returns = px / px.shift(1) - 1
+
+# Select dates
+
+subIndex = ibm.index[(ibm['close'] > 95) & (ibm['close'] < 100)]
+msftOnSameDates = msft.reindex(subIndex)
+
+# Insert columns
+
+msft['hi-lo spread'] = msft['high'] - msft['low']
+ibm['hi-lo spread'] = ibm['high'] - ibm['low']
+
+# Aggregate monthly
+
+def toMonthly(frame, how):
+    offset = BMonthEnd()
+
+    return frame.groupby(offset.rollforward).aggregate(how)
+
+msftMonthly = toMonthly(msft, np.mean)
+ibmMonthly = toMonthly(ibm, np.mean)
+
+# Statistics
+
+stdev = DataFrame({
+    'MSFT' : msft.std(),
+    'IBM'  : ibm.std()
+})
+
+# Arithmetic
+
+ratios = ibm / msft
+
+# Works with different indices
+
+ratio = ibm / ibmMonthly
+monthlyRatio = ratio.reindex(ibmMonthly.index)
+
+# Ratio relative to past month average
+
+filledRatio = ibm / ibmMonthly.reindex(ibm.index, fillMethod='pad')
diff --git a/examples/regressions.py b/examples/regressions.py
index 1da65c692..e78ff90a2 100644
--- a/examples/regressions.py
+++ b/examples/regressions.py
@@ -1,49 +1,49 @@
-from datetime import datetime
-import string
-
-import numpy as np
-
-from pandas.core.api import Series, DataFrame, DateRange
-from pandas.stats.api import ols
-
-N = 100
-
-start = datetime(2009, 9, 2)
-dateRange = DateRange(start, periods=N)
-
-def makeDataFrame():
-    data = DataFrame(np.random.randn(N, 7),
-                      columns=list(string.ascii_uppercase[:7]),
-                      index=dateRange)
-
-    return data
-
-def makeSeries():
-    return Series(np.random.randn(N), index=dateRange)
-
-#-------------------------------------------------------------------------------
-# Standard rolling linear regression
-
-X = makeDataFrame()
-Y =  makeSeries()
-
-model = ols(y=Y, x=X)
-
-print model
-
-#-------------------------------------------------------------------------------
-# Panel regression
-
-data = {
-    'A' : makeDataFrame(),
-    'B' : makeDataFrame(),
-    'C' : makeDataFrame()
-}
-
-Y = makeDataFrame()
-
-panelModel = ols(y=Y, x=data, window=50)
-
-model = ols(y=Y, x=data)
-
-print panelModel
+from datetime import datetime
+import string
+
+import numpy as np
+
+from pandas.core.api import Series, DataFrame, DateRange
+from pandas.stats.api import ols
+
+N = 100
+
+start = datetime(2009, 9, 2)
+dateRange = DateRange(start, periods=N)
+
+def makeDataFrame():
+    data = DataFrame(np.random.randn(N, 7),
+                      columns=list(string.ascii_uppercase[:7]),
+                      index=dateRange)
+
+    return data
+
+def makeSeries():
+    return Series(np.random.randn(N), index=dateRange)
+
+#-------------------------------------------------------------------------------
+# Standard rolling linear regression
+
+X = makeDataFrame()
+Y =  makeSeries()
+
+model = ols(y=Y, x=X)
+
+print model
+
+#-------------------------------------------------------------------------------
+# Panel regression
+
+data = {
+    'A' : makeDataFrame(),
+    'B' : makeDataFrame(),
+    'C' : makeDataFrame()
+}
+
+Y = makeDataFrame()
+
+panelModel = ols(y=Y, x=data, window=50)
+
+model = ols(y=Y, x=data)
+
+print panelModel
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 86d7d1d7a..0204be6f8 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -1,175 +1,175 @@
-"""
-Misc tools for implementing data structures
-"""
-
-from cStringIO import StringIO
-
-from numpy.lib.format import read_array, write_array
-import numpy as np
-
-import pandas.lib.tseries as tseries
-
-# XXX: HACK for NumPy 1.5.1 to suppress warnings
-try:
-    np.seterr(all='ignore')
-except Exception: # pragma: no cover
-    pass
-
-def isnull(input):
-    '''
-    Replacement for numpy.isnan / -numpy.isfinite which is suitable
-    for use on object arrays.
-
-    Parameters
-    ----------
-    arr: ndarray or object value
-
-    Returns
-    -------
-    boolean ndarray or boolean
-    '''
-    if isinstance(input, np.ndarray):
-        if input.dtype.kind in ('O', 'S'):
-            # Working around NumPy ticket 1542
-            result = input.copy().astype(bool)
-            result[:] = tseries.isnullobj(input)
-        else:
-            result = -np.isfinite(input)
-    else:
-        result = tseries.checknull(input)
-
-    return result
-
-def notnull(input):
-    '''
-    Replacement for numpy.isfinite / -numpy.isnan which is suitable
-    for use on object arrays.
-
-    Parameters
-    ----------
-    arr: ndarray or object value
-
-    Returns
-    -------
-    boolean ndarray or boolean
-    '''
-    if isinstance(input, np.ndarray):
-        return -isnull(input)
-    else:
-        return not tseries.checknull(input)
-
-def _pickle_array(arr):
-    arr = arr.view(np.ndarray)
-
-    buf = StringIO()
-    write_array(buf, arr)
-
-    return buf.getvalue()
-
-def _unpickle_array(bytes):
-    arr = read_array(StringIO(bytes))
-    return arr
-
-def get_indexer(source, target, fill_method):
-    if fill_method:
-        fill_method = fill_method.upper()
-
-    indexer, mask = tseries.getFillVec(source, target, source.indexMap,
-                                       target.indexMap, fill_method)
-
-    return indexer, mask
-
-def null_out_axis(arr, mask, axis):
-    if axis == 0:
-        arr[mask] = np.NaN
-    else:
-        indexer = [slice(None)] * arr.ndim
-        indexer[axis] = mask
-
-        arr[tuple(indexer)] = np.NaN
-
-#-------------------------------------------------------------------------------
-# Lots of little utilities
-
-def ensure_float(arr):
-    if issubclass(arr.dtype.type, np.integer):
-        arr = arr.astype(float)
-
-    return arr
-
-def _mut_exclusive(arg1, arg2):
-    if arg1 is not None and arg2 is not None:
-        raise Exception('mutually exclusive arguments')
-    elif arg1 is not None:
-        return arg1
-    else:
-        return arg2
-
-
-def _is_list_like(obj):
-    return isinstance(obj, (list, np.ndarray))
-
-def _is_label_slice(labels, obj):
-    def crit(x):
-        if x in labels:
-            return False
-        else:
-            return isinstance(x, int) or x is None
-    return not crit(obj.start) or not crit(obj.stop)
-
-def _need_slice(obj):
-    return obj.start is not None or obj.stop is not None
-
-def _check_step(obj):
-    if obj.step is not None:
-        raise Exception('steps other than 1 are not supported')
-
-def _ensure_index(index_like):
-    from pandas.core.index import Index
-    if not isinstance(index_like, Index):
-        index_like = Index(index_like)
-
-    return index_like
-
-def _any_none(*args):
-    for arg in args:
-        if arg is None:
-            return True
-    return False
-
-def _all_not_none(*args):
-    for arg in args:
-        if arg is None:
-            return False
-    return True
-
-def _try_sort(iterable):
-    listed = list(iterable)
-    try:
-        return sorted(listed)
-    except Exception:
-        return listed
-
-
-def _pfixed(s, space, nanRep=None, float_format=None):
-    if isinstance(s, float):
-        if nanRep is not None and isnull(s):
-            if np.isnan(s):
-                s = nanRep
-            return (' %s' % s).ljust(space)
-
-        if float_format:
-            formatted = float_format(s)
-        else:
-            is_neg = s < 0
-            formatted = '%.4g' % np.abs(s)
-
-            if is_neg:
-                formatted = '-' + formatted
-            else:
-                formatted = ' ' + formatted
-
-        return formatted.ljust(space)
-    else:
-        return (' %s' % s)[:space].ljust(space)
-
+"""
+Misc tools for implementing data structures
+"""
+
+from cStringIO import StringIO
+
+from numpy.lib.format import read_array, write_array
+import numpy as np
+
+import pandas.lib.tseries as tseries
+
+# XXX: HACK for NumPy 1.5.1 to suppress warnings
+try:
+    np.seterr(all='ignore')
+except Exception: # pragma: no cover
+    pass
+
+def isnull(input):
+    '''
+    Replacement for numpy.isnan / -numpy.isfinite which is suitable
+    for use on object arrays.
+
+    Parameters
+    ----------
+    arr: ndarray or object value
+
+    Returns
+    -------
+    boolean ndarray or boolean
+    '''
+    if isinstance(input, np.ndarray):
+        if input.dtype.kind in ('O', 'S'):
+            # Working around NumPy ticket 1542
+            result = input.copy().astype(bool)
+            result[:] = tseries.isnullobj(input)
+        else:
+            result = -np.isfinite(input)
+    else:
+        result = tseries.checknull(input)
+
+    return result
+
+def notnull(input):
+    '''
+    Replacement for numpy.isfinite / -numpy.isnan which is suitable
+    for use on object arrays.
+
+    Parameters
+    ----------
+    arr: ndarray or object value
+
+    Returns
+    -------
+    boolean ndarray or boolean
+    '''
+    if isinstance(input, np.ndarray):
+        return -isnull(input)
+    else:
+        return not tseries.checknull(input)
+
+def _pickle_array(arr):
+    arr = arr.view(np.ndarray)
+
+    buf = StringIO()
+    write_array(buf, arr)
+
+    return buf.getvalue()
+
+def _unpickle_array(bytes):
+    arr = read_array(StringIO(bytes))
+    return arr
+
+def get_indexer(source, target, fill_method):
+    if fill_method:
+        fill_method = fill_method.upper()
+
+    indexer, mask = tseries.getFillVec(source, target, source.indexMap,
+                                       target.indexMap, fill_method)
+
+    return indexer, mask
+
+def null_out_axis(arr, mask, axis):
+    if axis == 0:
+        arr[mask] = np.NaN
+    else:
+        indexer = [slice(None)] * arr.ndim
+        indexer[axis] = mask
+
+        arr[tuple(indexer)] = np.NaN
+
+#-------------------------------------------------------------------------------
+# Lots of little utilities
+
+def ensure_float(arr):
+    if issubclass(arr.dtype.type, np.integer):
+        arr = arr.astype(float)
+
+    return arr
+
+def _mut_exclusive(arg1, arg2):
+    if arg1 is not None and arg2 is not None:
+        raise Exception('mutually exclusive arguments')
+    elif arg1 is not None:
+        return arg1
+    else:
+        return arg2
+
+
+def _is_list_like(obj):
+    return isinstance(obj, (list, np.ndarray))
+
+def _is_label_slice(labels, obj):
+    def crit(x):
+        if x in labels:
+            return False
+        else:
+            return isinstance(x, int) or x is None
+    return not crit(obj.start) or not crit(obj.stop)
+
+def _need_slice(obj):
+    return obj.start is not None or obj.stop is not None
+
+def _check_step(obj):
+    if obj.step is not None:
+        raise Exception('steps other than 1 are not supported')
+
+def _ensure_index(index_like):
+    from pandas.core.index import Index
+    if not isinstance(index_like, Index):
+        index_like = Index(index_like)
+
+    return index_like
+
+def _any_none(*args):
+    for arg in args:
+        if arg is None:
+            return True
+    return False
+
+def _all_not_none(*args):
+    for arg in args:
+        if arg is None:
+            return False
+    return True
+
+def _try_sort(iterable):
+    listed = list(iterable)
+    try:
+        return sorted(listed)
+    except Exception:
+        return listed
+
+
+def _pfixed(s, space, nanRep=None, float_format=None):
+    if isinstance(s, float):
+        if nanRep is not None and isnull(s):
+            if np.isnan(s):
+                s = nanRep
+            return (' %s' % s).ljust(space)
+
+        if float_format:
+            formatted = float_format(s)
+        else:
+            is_neg = s < 0
+            formatted = '%.4g' % np.abs(s)
+
+            if is_neg:
+                formatted = '-' + formatted
+            else:
+                formatted = ' ' + formatted
+
+        return formatted.ljust(space)
+    else:
+        return (' %s' % s)[:space].ljust(space)
+
diff --git a/pandas/core/tests/test_panel.py b/pandas/core/tests/test_panel.py
index 4095fa0f7..713fb7592 100644
--- a/pandas/core/tests/test_panel.py
+++ b/pandas/core/tests/test_panel.py
@@ -1,1002 +1,1002 @@
-# pylint: disable=W0612,E1101
-
-
-from datetime import datetime
-import os
-import operator
-import unittest
-
-import numpy as np
-
-from pandas.core.api import DataFrame, Index, notnull
-from pandas.core.datetools import bday
-from pandas.core.panel import (WidePanel, LongPanelIndex, LongPanel,
-                               group_agg, pivot)
-import pandas.core.panel as panelmod
-
-from pandas.util.testing import (assert_panel_equal,
-                                 assert_frame_equal,
-                                 assert_series_equal,
-                                 assert_almost_equal)
-import pandas.core.panel as panelm
-import pandas.util.testing as common
-
-class PanelTests(object):
-    panel = None
-
-    def test_pickle(self):
-        import cPickle
-        pickled = cPickle.dumps(self.panel)
-        unpickled = cPickle.loads(pickled)
-        assert_frame_equal(unpickled['ItemA'], self.panel['ItemA'])
-
-    def test_set_values(self):
-        self.panel.values = np.array(self.panel.values, order='F')
-        assert(self.panel.values.flags.contiguous)
-
-    def test_cumsum(self):
-        cumsum = self.panel.cumsum()
-        assert_frame_equal(cumsum['ItemA'], self.panel['ItemA'].cumsum())
-
-class SafeForLongAndSparse(object):
-
-    def test_repr(self):
-        foo = repr(self.panel)
-
-    def test_iter(self):
-        common.equalContents(list(self.panel), self.panel.items)
-
-    def _check_statistic(self, frame, name, alternative):
-        f = getattr(frame, name)
-
-        for i, ax in enumerate(['items', 'major', 'minor']):
-            result = f(axis=i)
-            assert_frame_equal(result, frame.apply(alternative, axis=ax))
-
-    def test_count(self):
-        f = lambda s: notnull(s).sum()
-
-        self._check_statistic(self.panel, 'count', f)
-
-    def test_sum(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) == 0:
-                return np.NaN
-            else:
-                return nona.sum()
-
-        self._check_statistic(self.panel, 'sum', f)
-
-    def test_prod(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) == 0:
-                return np.NaN
-            else:
-                return np.prod(nona)
-
-        self._check_statistic(self.panel, 'prod', f)
-
-    def test_mean(self):
-        def f(x):
-            x = np.asarray(x)
-            return x[notnull(x)].mean()
-
-        self._check_statistic(self.panel, 'mean', f)
-
-    def test_median(self):
-        def f(x):
-            x = np.asarray(x)
-            return np.median(x[notnull(x)])
-
-        self._check_statistic(self.panel, 'median', f)
-
-    def test_min(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) == 0:
-                return np.NaN
-            else:
-                return nona.min()
-
-        self._check_statistic(self.panel, 'min', f)
-
-    def test_max(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) == 0:
-                return np.NaN
-            else:
-                return nona.max()
-
-        self._check_statistic(self.panel, 'max', f)
-
-    def test_var(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) < 2:
-                return np.NaN
-            else:
-                return nona.var(ddof=1)
-
-        self._check_statistic(self.panel, 'var', f)
-
-    def test_std(self):
-        def f(x):
-            x = np.asarray(x)
-            nona = x[notnull(x)]
-
-            if len(nona) < 2:
-                return np.NaN
-            else:
-                return nona.std(ddof=1)
-
-        self._check_statistic(self.panel, 'std', f)
-
-    def test_skew(self):
-        return
-        try:
-            from scipy.stats import skew
-        except ImportError:
-            return
-
-        def f(x):
-            x = np.asarray(x)
-            return skew(x[notnull(x)], bias=False)
-
-        self._check_statistic(self.panel, 'skew', f)
-
-
-class SafeForSparse(object):
-
-    @staticmethod
-    def assert_panel_equal(x, y):
-        assert_panel_equal(x, y)
-
-    def test_get_axis(self):
-        assert(self.panel._get_axis(0) is self.panel.items)
-        assert(self.panel._get_axis(1) is self.panel.major_axis)
-        assert(self.panel._get_axis(2) is self.panel.minor_axis)
-
-    def test_get_axis_number(self):
-        self.assertEqual(self.panel._get_axis_number('items'), 0)
-        self.assertEqual(self.panel._get_axis_number('major'), 1)
-        self.assertEqual(self.panel._get_axis_number('minor'), 2)
-
-    def test_get_axis_name(self):
-        self.assertEqual(self.panel._get_axis_name(0), 'items')
-        self.assertEqual(self.panel._get_axis_name(1), 'major_axis')
-        self.assertEqual(self.panel._get_axis_name(2), 'minor_axis')
-
-    def test_get_plane_axes(self):
-        # what to do here?
-
-        index, columns = self.panel._get_plane_axes('items')
-        index, columns = self.panel._get_plane_axes('major_axis')
-        index, columns = self.panel._get_plane_axes('minor_axis')
-        index, columns = self.panel._get_plane_axes(0)
-
-    def test_truncate(self):
-        dates = self.panel.major_axis
-        start, end = dates[1], dates[5]
-
-        trunced = self.panel.truncate(start, end, axis='major')
-        expected = self.panel['ItemA'].truncate(start, end)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        trunced = self.panel.truncate(before=start, axis='major')
-        expected = self.panel['ItemA'].truncate(before=start)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        trunced = self.panel.truncate(after=end, axis='major')
-        expected = self.panel['ItemA'].truncate(after=end)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        # XXX test other axes
-
-    def test_arith(self):
-        self._test_op(self.panel, operator.add)
-        self._test_op(self.panel, operator.sub)
-        self._test_op(self.panel, operator.mul)
-        self._test_op(self.panel, operator.div)
-        self._test_op(self.panel, operator.pow)
-
-        self._test_op(self.panel, lambda x, y: y + x)
-        self._test_op(self.panel, lambda x, y: y - x)
-        self._test_op(self.panel, lambda x, y: y * x)
-        self._test_op(self.panel, lambda x, y: y / x)
-        self._test_op(self.panel, lambda x, y: y ** x)
-
-        self.assertRaises(Exception, self.panel.__add__, self.panel['ItemA'])
-
-    @staticmethod
-    def _test_op(panel, op):
-        result = op(panel, 1)
-        assert_frame_equal(result['ItemA'], op(panel['ItemA'], 1))
-
-    def test_keys(self):
-        common.equalContents(self.panel.keys(), self.panel.items)
-
-    def test_iteritems(self):
-        # just test that it works
-        for k, v in self.panel.iteritems():
-            pass
-
-        self.assertEqual(len(list(self.panel.iteritems())),
-                         len(self.panel.items))
-
-    def test_combineFrame(self):
-        def check_op(op, name):
-            # items
-            df = self.panel['ItemA']
-
-            func = getattr(self.panel, name)
-
-            result = func(df, axis='items')
-
-            assert_frame_equal(result['ItemB'], op(self.panel['ItemB'], df))
-
-            # major
-            xs = self.panel.major_xs(self.panel.major_axis[0])
-            result = func(xs, axis='major')
-
-            idx = self.panel.major_axis[1]
-
-            assert_frame_equal(result.major_xs(idx),
-                               op(self.panel.major_xs(idx), xs))
-
-            # minor
-            xs = self.panel.minor_xs(self.panel.minor_axis[0])
-            result = func(xs, axis='minor')
-
-            idx = self.panel.minor_axis[1]
-
-            assert_frame_equal(result.minor_xs(idx),
-                               op(self.panel.minor_xs(idx), xs))
-
-        check_op(operator.add, 'add')
-        check_op(operator.sub, 'subtract')
-        check_op(operator.mul, 'multiply')
-        check_op(operator.div, 'divide')
-
-    def test_combinePanel(self):
-        result = self.panel.add(self.panel)
-        self.assert_panel_equal(result, self.panel * 2)
-
-    def test_neg(self):
-        self.assert_panel_equal(-self.panel, self.panel * -1)
-
-    def test_select(self):
-        p = self.panel
-
-        # select items
-        result = p.select(lambda x: x in ('ItemA', 'ItemC'), axis='items')
-        expected = p.reindex(items=['ItemA', 'ItemC'])
-        self.assert_panel_equal(result, expected)
-
-        # select major_axis
-        result = p.select(lambda x: x >= datetime(2000, 1, 15), axis='major')
-        new_major = p.major_axis[p.major_axis >= datetime(2000, 1, 15)]
-        expected = p.reindex(major=new_major)
-        self.assert_panel_equal(result, expected)
-
-        # select minor_axis
-        result = p.select(lambda x: x in ('D', 'A'), axis=2)
-        expected = p.reindex(minor=['A', 'D'])
-        self.assert_panel_equal(result, expected)
-
-        # corner case, empty thing
-        result = p.select(lambda x: x in ('foo',), axis='items')
-        self.assert_panel_equal(result, p.reindex(items=[]))
-
-class TestWidePanel(unittest.TestCase, PanelTests,
-                    SafeForLongAndSparse,
-                    SafeForSparse):
-
-    @staticmethod
-    def assert_panel_equal(x, y):
-        assert_panel_equal(x, y)
-
-    def setUp(self):
-        self.panel = common.makeWidePanel()
-        common.add_nans(self.panel)
-
-    def test_values(self):
-        # nothing to test for the moment
-        values = self.panel.values
-        self.panel.values = values
-
-    def test_fromDict(self):
-        itema = self.panel['ItemA']
-        itemb = self.panel['ItemB']
-
-        d = {'A' : itema, 'B' : itemb[5:]}
-        d2 = {'A' : itema._series, 'B' : itemb[5:]._series}
-        d3 = {'A' : DataFrame(itema._series),
-              'B' : DataFrame(itemb[5:]._series)}
-
-        wp = WidePanel.fromDict(d)
-        wp2 = WidePanel.fromDict(d2) # nested Dict
-        wp3 = WidePanel.fromDict(d3)
-        self.assert_(wp.major_axis.equals(self.panel.major_axis))
-        assert_panel_equal(wp, wp2)
-
-        # intersect
-        wp = WidePanel.fromDict(d, intersect=True)
-        self.assert_(wp.major_axis.equals(itemb.index[5:]))
-
-    def test_values(self):
-        self.assertRaises(Exception, WidePanel, np.random.randn(5, 5, 5),
-                          range(5), range(5), range(4))
-
-    def test_getitem(self):
-        self.assertRaises(Exception, self.panel.__getitem__, 'ItemQ')
-
-    def test_delitem_and_pop(self):
-        expected = self.panel['ItemA']
-        result = self.panel.pop('ItemA')
-        assert_frame_equal(expected, result)
-        self.assert_('ItemA' not in self.panel.items)
-
-        del self.panel['ItemB']
-        self.assert_('ItemB' not in self.panel.items)
-        self.assertRaises(Exception, self.panel.__delitem__, 'ItemB')
-
-        values = np.empty((3, 3, 3))
-        values[0] = 0
-        values[1] = 1
-        values[2] = 2
-
-        panel = WidePanel(values, range(3), range(3), range(3))
-
-        # did we delete the right row?
-
-        panelc = panel.copy()
-        del panelc[0]
-        assert_frame_equal(panelc[1], panel[1])
-        assert_frame_equal(panelc[2], panel[2])
-
-        panelc = panel.copy()
-        del panelc[1]
-        assert_frame_equal(panelc[0], panel[0])
-        assert_frame_equal(panelc[2], panel[2])
-
-        panelc = panel.copy()
-        del panelc[2]
-        assert_frame_equal(panelc[1], panel[1])
-        assert_frame_equal(panelc[0], panel[0])
-
-    def test_setitem(self):
-
-        # LongPanel with one item
-        lp = self.panel.filter(['ItemA']).to_long()
-        self.panel['ItemE'] = lp
-
-        lp = self.panel.filter(['ItemA', 'ItemB']).to_long()
-        self.assertRaises(Exception, self.panel.__setitem__,
-                          'ItemE', lp)
-
-        # DataFrame
-        df = self.panel['ItemA'][2:].filter(items=['A', 'B'])
-        self.panel['ItemF'] = df
-        self.panel['ItemE'] = df
-
-        df2 = self.panel['ItemF']
-
-        assert_frame_equal(df, df2.reindex(index=df.index,
-                                           columns=df.columns))
-
-        # scalar
-        self.panel['ItemG'] = 1
-        self.panel['ItemE'] = 1
-
-    def test_conform(self):
-        df = self.panel['ItemA'][:-5].filter(items=['A', 'B'])
-        conformed = self.panel.conform(df)
-
-        assert(conformed.index.equals(self.panel.major_axis))
-        assert(conformed.columns.equals(self.panel.minor_axis))
-
-    def test_reindex(self):
-        ref = self.panel['ItemB']
-
-        # items
-        result = self.panel.reindex(items=['ItemA', 'ItemB'])
-        assert_frame_equal(result['ItemB'], ref)
-
-        # major
-        new_major = list(self.panel.major_axis[:10])
-        result = self.panel.reindex(major=new_major)
-        assert_frame_equal(result['ItemB'], ref.reindex(index=new_major))
-
-        # raise exception put both major and major_axis
-        self.assertRaises(Exception, self.panel.reindex,
-                          major_axis=new_major, major=new_major)
-
-        # minor
-        new_minor = list(self.panel.minor_axis[:2])
-        result = self.panel.reindex(minor=new_minor)
-        assert_frame_equal(result['ItemB'], ref.reindex(columns=new_minor))
-
-        result = self.panel.reindex(items=self.panel.items,
-                                    major=self.panel.major_axis,
-                                    minor=self.panel.minor_axis)
-
-        assert(result.items is self.panel.items)
-        assert(result.major_axis is self.panel.major_axis)
-        assert(result.minor_axis is self.panel.minor_axis)
-
-        self.assertRaises(Exception, self.panel.reindex)
-
-        # with filling
-        smaller_major = self.panel.major_axis[::5]
-        smaller = self.panel.reindex(major=smaller_major)
-
-        larger = smaller.reindex(major=self.panel.major_axis,
-                                 method='pad')
-
-        assert_frame_equal(larger.major_xs(self.panel.major_axis[1]),
-                           smaller.major_xs(smaller_major[0]))
-
-        # reindex_like
-
-        smaller = self.panel.reindex(items=self.panel.items[:-1],
-                                     major=self.panel.major_axis[:-1],
-                                     minor=self.panel.minor_axis[:-1])
-        smaller_like = self.panel.reindex_like(smaller)
-        assert_panel_equal(smaller, smaller_like)
-
-    def test_fillna(self):
-        filled = self.panel.fillna(0)
-        self.assert_(np.isfinite(filled.values).all())
-
-        filled = self.panel.fillna(method='backfill')
-        assert_frame_equal(filled['ItemA'],
-                           self.panel['ItemA'].fillna(method='backfill'))
-
-    def test_combinePanel_with_long(self):
-        lng = self.panel.to_long(filter_observations=False)
-        result = self.panel.add(lng)
-        self.assert_panel_equal(result, self.panel * 2)
-
-    def test_major_xs(self):
-        ref = self.panel['ItemA']
-
-        idx = self.panel.major_axis[5]
-        xs = self.panel.major_xs(idx)
-
-        assert_series_equal(xs['ItemA'], ref.xs(idx))
-
-        # not contained
-        idx = self.panel.major_axis[0] - bday
-        self.assertRaises(Exception, self.panel.major_xs, idx)
-
-    def test_minor_xs(self):
-        ref = self.panel['ItemA']
-
-        idx = self.panel.minor_axis[1]
-        xs = self.panel.minor_xs(idx)
-
-        assert_series_equal(xs['ItemA'], ref[idx])
-
-        # not contained
-        self.assertRaises(Exception, self.panel.minor_xs, 'E')
-
-    def test_groupby(self):
-        grouped = self.panel.groupby({'ItemA' : 0, 'ItemB' : 0, 'ItemC' : 1},
-                                     axis='items')
-        agged = grouped.agg(np.mean)
-        self.assert_(np.array_equal(agged.items, [0, 1]))
-
-        grouped = self.panel.groupby(lambda x: x.month, axis='major')
-        agged = grouped.agg(np.mean)
-
-        self.assert_(np.array_equal(agged.major_axis, [1, 2]))
-
-        grouped = self.panel.groupby({'A' : 0, 'B' : 0, 'C' : 1, 'D' : 1},
-                                     axis='minor')
-        agged = grouped.agg(np.mean)
-        self.assert_(np.array_equal(agged.minor_axis, [0, 1]))
-
-    def test_swapaxes(self):
-        result = self.panel.swapaxes('items', 'minor')
-        self.assert_(result.items is self.panel.minor_axis)
-
-        result = self.panel.swapaxes('items', 'major')
-        self.assert_(result.items is self.panel.major_axis)
-
-        result = self.panel.swapaxes('major', 'minor')
-        self.assert_(result.major_axis is self.panel.minor_axis)
-
-        # this should also work
-        result = self.panel.swapaxes(0, 1)
-        self.assert_(result.items is self.panel.major_axis)
-
-        # this should also work
-        self.assertRaises(Exception, self.panel.swapaxes, 'items', 'items')
-
-    def test_to_long(self):
-        # filtered
-        filtered = self.panel.to_long()
-
-        # unfiltered
-        unfiltered = self.panel.to_long(filter_observations=False)
-
-        assert_panel_equal(unfiltered.to_wide(), self.panel)
-
-    def test_filter(self):
-        pass
-
-    def test_apply(self):
-        pass
-
-    def test_compound(self):
-        compounded = self.panel.compound()
-
-        assert_series_equal(compounded['ItemA'],
-                            (1 + self.panel['ItemA']).product(0) - 1)
-
-    def test_shift(self):
-        # major
-        idx = self.panel.major_axis[0]
-        idx_lag = self.panel.major_axis[1]
-
-        shifted = self.panel.shift(1)
-
-        assert_frame_equal(self.panel.major_xs(idx),
-                           shifted.major_xs(idx_lag))
-
-        # minor
-        idx = self.panel.minor_axis[0]
-        idx_lag = self.panel.minor_axis[1]
-
-        shifted = self.panel.shift(1, axis='minor')
-
-        assert_frame_equal(self.panel.minor_xs(idx),
-                           shifted.minor_xs(idx_lag))
-
-        self.assertRaises(Exception, self.panel.shift, 1, axis='items')
-
-class TestLongPanelIndex(unittest.TestCase):
-
-    def setUp(self):
-        major_axis = Index([1, 2, 3, 4])
-        minor_axis = Index([1, 2])
-
-        major_labels = np.array([0, 0, 1, 2, 3, 3])
-        minor_labels = np.array([0, 1, 0, 1, 0, 1])
-
-        self.index = LongPanelIndex(major_axis, minor_axis,
-                                    major_labels, minor_labels)
-
-        major_labels = np.array([0, 0, 1, 1, 1, 2, 2, 3, 3])
-        minor_labels = np.array([0, 1, 0, 1, 1, 0, 1, 0, 1])
-
-        self.incon = LongPanelIndex(major_axis, minor_axis,
-                                    major_labels, minor_labels)
-
-    def test_consistency(self):
-        self.assert_(self.index.consistent)
-        self.assert_(not self.incon.consistent)
-
-        # need to construct an overflow
-        major_axis = range(70000)
-        minor_axis = range(10)
-
-        major_labels = np.arange(70000)
-        minor_labels = np.repeat(range(10), 7000)
-
-        index = LongPanelIndex(major_axis, minor_axis,
-                               major_labels, minor_labels)
-
-        self.assert_(index.consistent)
-
-    def test_truncate(self):
-        result = self.index.truncate(before=1)
-        self.assert_(0 not in result.major_axis)
-        self.assert_(1 in result.major_axis)
-
-        result = self.index.truncate(after=1)
-        self.assert_(2 not in result.major_axis)
-        self.assert_(1 in result.major_axis)
-
-        result = self.index.truncate(before=1, after=2)
-        self.assertEqual(len(result.major_axis), 2)
-
-    def test_getMajorBounds(self):
-        pass
-
-    def test_getAxisBounds(self):
-        pass
-
-    def test_getLabelBounds(self):
-        pass
-
-    def test_bounds(self):
-        pass
-
-    def test_makeMask(self):
-        mask =  self.index.mask
-        expected = np.array([True, True,
-                             True, False,
-                             False, True,
-                             True, True], dtype=bool)
-        self.assert_(np.array_equal(mask, expected))
-
-    def test_dims(self):
-        pass
-
-class TestLongPanel(unittest.TestCase):
-
-    def setUp(self):
-        panel = common.makeWidePanel()
-        common.add_nans(panel)
-
-        self.panel = panel.to_long()
-        self.unfiltered_panel = panel.to_long(filter_observations=False)
-
-    def test_pickle(self):
-        import cPickle
-
-        pickled = cPickle.dumps(self.panel)
-        unpickled = cPickle.loads(pickled)
-
-        assert_almost_equal(unpickled['ItemA'].values,
-                            self.panel['ItemA'].values)
-
-    def test_len(self):
-        len(self.unfiltered_panel)
-
-    def test_constructor(self):
-        pass
-
-    def test_fromRecords_toRecords(self):
-        # structured array
-        K = 10
-
-        recs = np.zeros(K, dtype='O,O,f8,f8')
-        recs['f0'] = range(K / 2) * 2
-        recs['f1'] = np.arange(K) / (K / 2)
-        recs['f2'] = np.arange(K) * 2
-        recs['f3'] = np.arange(K)
-
-        lp = LongPanel.fromRecords(recs, 'f0', 'f1')
-        self.assertEqual(len(lp.items), 2)
-
-        lp = LongPanel.fromRecords(recs, 'f0', 'f1', exclude=['f2'])
-        self.assertEqual(len(lp.items), 1)
-
-        torecs = lp.toRecords()
-        self.assertEqual(len(torecs.dtype.names), len(lp.items) + 2)
-
-        # DataFrame
-        df = DataFrame.fromRecords(recs)
-        lp = LongPanel.fromRecords(df, 'f0', 'f1', exclude=['f2'])
-        self.assertEqual(len(lp.items), 1)
-
-        # dict of arrays
-        series = DataFrame.fromRecords(recs)._series
-        lp = LongPanel.fromRecords(series, 'f0', 'f1', exclude=['f2'])
-        self.assertEqual(len(lp.items), 1)
-        self.assert_('f2' in series)
-
-        self.assertRaises(Exception, LongPanel.fromRecords, np.zeros((3, 3)),
-                          0, 1)
-
-    def test_factors(self):
-        # structured array
-        K = 10
-
-        recs = np.zeros(K, dtype='O,O,f8,f8,O,O')
-        recs['f0'] = ['one'] * 5 + ['two'] * 5
-        recs['f1'] = ['A', 'B', 'C', 'D', 'E'] * 2
-        recs['f2'] = np.arange(K) * 2
-        recs['f3'] = np.arange(K)
-        recs['f4'] = ['A', 'B', 'C', 'D', 'E'] * 2
-        recs['f5'] = ['foo', 'bar'] * 5
-
-        lp = LongPanel.fromRecords(recs, 'f0', 'f1')
-
-    def test_columns(self):
-        self.assert_(np.array_equal(self.panel.items, self.panel.columns))
-
-    def test_copy(self):
-        thecopy = self.panel.copy()
-        self.assert_(np.array_equal(thecopy.values, self.panel.values))
-        self.assert_(thecopy.values is not self.panel.values)
-
-    def test_values(self):
-        valslice = self.panel.values[:-1]
-        self.assertRaises(Exception, self.panel._set_values, valslice)
-
-    def test_getitem(self):
-        col = self.panel['ItemA']
-
-    def test_setitem(self):
-        self.panel['ItemE'] = self.panel['ItemA']
-        self.panel['ItemF'] = 1
-
-        wp = self.panel.to_wide()
-        assert_frame_equal(wp['ItemA'], wp['ItemE'])
-
-        itemf = wp['ItemF'].values.ravel()
-        self.assert_((itemf[np.isfinite(itemf)] == 1).all())
-
-        # check exceptions raised
-        lp = self.panel.filter(['ItemA', 'ItemB'])
-        lp2 = self.panel.filter(['ItemC', 'ItemE'])
-        self.assertRaises(Exception, lp.__setitem__, 'foo', lp2)
-
-    def test_ops_differently_indexed(self):
-        # trying to set non-identically indexed panel
-        wp = self.panel.to_wide()
-        wp2 = wp.reindex(major=wp.major_axis[:-1])
-        lp2 = wp2.to_long()
-
-        self.assertRaises(Exception, self.panel.__setitem__, 'foo',
-                          lp2.filter(['ItemA']))
-
-        self.assertRaises(Exception, self.panel.add, lp2)
-
-    def test_combineFrame(self):
-        wp = self.panel.to_wide()
-        result = self.panel.add(wp['ItemA'])
-        assert_frame_equal(result.to_wide()['ItemA'], wp['ItemA'] * 2)
-
-    def test_combinePanel(self):
-        wp = self.panel.to_wide()
-        result = self.panel.add(self.panel)
-        wide_result = result.to_wide()
-        assert_frame_equal(wp['ItemA'] * 2, wide_result['ItemA'])
-
-        # one item
-        result = self.panel.add(self.panel.filter(['ItemA']))
-
-    def test_operators(self):
-        wp = self.panel.to_wide()
-        result = (self.panel + 1).to_wide()
-        assert_frame_equal(wp['ItemA'] + 1, result['ItemA'])
-
-    def test_sort(self):
-        def is_sorted(arr):
-            return (arr[1:] > arr[:-1]).any()
-
-        sorted_minor = self.panel.sort(axis='minor')
-        self.assert_(is_sorted(sorted_minor.index.minor_labels))
-
-        sorted_major = sorted_minor.sort(axis='major')
-        self.assert_(is_sorted(sorted_major.index.major_labels))
-
-    def test_to_wide(self):
-        pass
-
-    def test_toCSV(self):
-        self.panel.toCSV('__tmp__')
-        os.remove('__tmp__')
-
-    def test_toString(self):
-        from cStringIO import StringIO
-
-        buf = StringIO()
-        self.panel.toString(buf)
-        self.panel.toString(buf, col_space=12)
-
-    def test_swapaxes(self):
-        swapped = self.panel.swapaxes()
-
-        self.assert_(swapped.major_axis is self.panel.minor_axis)
-
-        # what else to test here?
-
-    def test_truncate(self):
-        dates = self.panel.major_axis
-        start, end = dates[1], dates[5]
-
-        trunced = self.panel.truncate(start, end).to_wide()
-        expected = self.panel.to_wide()['ItemA'].truncate(start, end)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        trunced = self.panel.truncate(before=start).to_wide()
-        expected = self.panel.to_wide()['ItemA'].truncate(before=start)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        trunced = self.panel.truncate(after=end).to_wide()
-        expected = self.panel.to_wide()['ItemA'].truncate(after=end)
-
-        assert_frame_equal(trunced['ItemA'], expected)
-
-        # truncate on dates that aren't in there
-        wp = self.panel.to_wide()
-        new_index = wp.major_axis[::5]
-
-        wp2 = wp.reindex(major=new_index)
-
-        lp2 = wp2.to_long()
-        lp_trunc = lp2.truncate(wp.major_axis[2], wp.major_axis[-2])
-
-        wp_trunc = wp2.truncate(wp.major_axis[2], wp.major_axis[-2])
-
-        assert_panel_equal(wp_trunc, lp_trunc.to_wide())
-
-        # throw proper exception
-        self.assertRaises(Exception, lp2.truncate, wp.major_axis[-2],
-                          wp.major_axis[2])
-
-
-    def test_filter(self):
-        pass
-
-    def test_axis_dummies(self):
-        minor_dummies = self.panel.get_axis_dummies('minor')
-        self.assertEqual(len(minor_dummies.items),
-                         len(self.panel.minor_axis))
-
-        major_dummies = self.panel.get_axis_dummies('major')
-        self.assertEqual(len(major_dummies.items),
-                         len(self.panel.major_axis))
-
-        mapping = {'A' : 'one',
-                   'B' : 'one',
-                   'C' : 'two',
-                   'D' : 'two'}
-
-        transformed = self.panel.get_axis_dummies('minor',
-                                                  transform=mapping.get)
-        self.assertEqual(len(transformed.items), 2)
-        self.assert_(np.array_equal(transformed.items, ['one', 'two']))
-
-        # TODO: test correctness
-
-    def test_get_dummies(self):
-        self.panel['Label'] = self.panel.index.minor_labels
-
-        minor_dummies = self.panel.get_axis_dummies('minor')
-        dummies = self.panel.get_dummies('Label')
-
-        self.assert_(np.array_equal(dummies.values, minor_dummies.values))
-
-    def test_apply(self):
-        # ufunc
-        applied = self.panel.apply(np.sqrt)
-        self.assert_(assert_almost_equal(
-                applied.values, np.sqrt(self.panel.values)))
-
-    def test_mean(self):
-        means = self.panel.mean('major')
-
-        # test versus WidePanel version
-        wide_means = self.panel.to_wide().mean('major')
-        assert_frame_equal(means, wide_means)
-
-        means_broadcast = self.panel.mean('major', broadcast=True)
-        self.assert_(isinstance(means_broadcast, LongPanel))
-
-        # how to check correctness?
-
-    def test_sum(self):
-        sums = self.panel.sum('major')
-
-        # test versus WidePanel version
-        wide_sums = self.panel.to_wide().sum('major')
-        assert_frame_equal(sums, wide_sums)
-
-    def test_count(self):
-        index = self.panel.index
-
-        major_count = self.panel.count('major')
-        labels = index.major_labels
-        for i, idx in enumerate(index.major_axis):
-            self.assertEqual(major_count[i], (labels == i).sum())
-
-        minor_count = self.panel.count('minor')
-        labels = index.minor_labels
-        for i, idx in enumerate(index.minor_axis):
-            self.assertEqual(minor_count[i], (labels == i).sum())
-
-    def test_leftJoin(self):
-        lp1 = self.panel.filter(['ItemA', 'ItemB'])
-        lp2 = self.panel.filter(['ItemC'])
-
-        joined = lp1.leftJoin(lp2)
-
-        self.assertEqual(len(joined.items), 3)
-
-        self.assertRaises(Exception, lp1.leftJoin,
-                          self.panel.filter(['ItemB', 'ItemC']))
-
-    def test_merge(self):
-        pass
-
-    def test_addPrefix(self):
-        lp = self.panel.addPrefix('foo#')
-        self.assertEqual(lp.items[0], 'foo#ItemA')
-
-        lp = self.panel.addPrefix()
-        assert_panel_equal(lp.to_wide(), self.panel.to_wide())
-
-    def test_pivot(self):
-        df = pivot(np.array([1, 2, 3, 4, 5]),
-                   np.array(['a', 'b', 'c', 'd', 'e']),
-                   np.array([1, 2, 3, 5, 4.]))
-        self.assertEqual(df['a'][1], 1)
-        self.assertEqual(df['b'][2], 2)
-        self.assertEqual(df['c'][3], 3)
-        self.assertEqual(df['d'][4], 5)
-        self.assertEqual(df['e'][5], 4)
-
-        # weird overlap, TODO: test?
-        df = pivot(np.array([1, 2, 3, 4, 4]),
-                   np.array(['a', 'a', 'a', 'a', 'a']),
-                   np.array([1, 2, 3, 5, 4]))
-
-        # corner case, empty
-        df = pivot(np.array([]), np.array([]), np.array([]))
-
-def test_group_agg():
-    values = np.ones((10, 2)) * np.arange(10).reshape((10, 1))
-    bounds = np.arange(5) * 2
-    f = lambda x: x.mean(axis=0)
-
-    agged = group_agg(values, bounds, f)
-
-    assert(agged[1][0] == 2.5)
-    assert(agged[2][0] == 4.5)
-
-def test_monotonic():
-    pos = np.array([1, 2, 3, 5])
-
-    assert panelm._monotonic(pos)
-
-    neg = np.array([1, 2, 3, 4, 3])
-
-    assert not panelm._monotonic(neg)
-
-    neg2 = np.array([5, 1, 2, 3, 4, 5])
-
-    assert not panelm._monotonic(neg2)
-
-class TestFactor(unittest.TestCase):
-
-    def setUp(self):
-        self.factor = panelmod.Factor.fromarray(['a', 'b', 'b', 'a',
-                                                 'a', 'c', 'c', 'c'])
-
-    def test_getitem(self):
-        self.assertEqual(self.factor[0], 'a')
-        self.assertEqual(self.factor[-1], 'c')
-
-        subf = self.factor[[0, 1, 2]]
-        common.assert_almost_equal(subf.labels, [0, 1, 1])
-
-        subf = self.factor[self.factor.asarray() == 'c']
-        common.assert_almost_equal(subf.labels, [2, 2, 2])
-
-    def test_factor_agg(self):
-        arr = np.arange(len(self.factor))
-
-        f = np.sum
-        agged = panelmod.factor_agg(self.factor, arr, f)
-        labels = self.factor.labels
-        for i, idx in enumerate(self.factor.levels):
-            self.assertEqual(f(arr[labels == i]), agged[i])
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
-                   exit=False)
+# pylint: disable=W0612,E1101
+
+
+from datetime import datetime
+import os
+import operator
+import unittest
+
+import numpy as np
+
+from pandas.core.api import DataFrame, Index, notnull
+from pandas.core.datetools import bday
+from pandas.core.panel import (WidePanel, LongPanelIndex, LongPanel,
+                               group_agg, pivot)
+import pandas.core.panel as panelmod
+
+from pandas.util.testing import (assert_panel_equal,
+                                 assert_frame_equal,
+                                 assert_series_equal,
+                                 assert_almost_equal)
+import pandas.core.panel as panelm
+import pandas.util.testing as common
+
+class PanelTests(object):
+    panel = None
+
+    def test_pickle(self):
+        import cPickle
+        pickled = cPickle.dumps(self.panel)
+        unpickled = cPickle.loads(pickled)
+        assert_frame_equal(unpickled['ItemA'], self.panel['ItemA'])
+
+    def test_set_values(self):
+        self.panel.values = np.array(self.panel.values, order='F')
+        assert(self.panel.values.flags.contiguous)
+
+    def test_cumsum(self):
+        cumsum = self.panel.cumsum()
+        assert_frame_equal(cumsum['ItemA'], self.panel['ItemA'].cumsum())
+
+class SafeForLongAndSparse(object):
+
+    def test_repr(self):
+        foo = repr(self.panel)
+
+    def test_iter(self):
+        common.equalContents(list(self.panel), self.panel.items)
+
+    def _check_statistic(self, frame, name, alternative):
+        f = getattr(frame, name)
+
+        for i, ax in enumerate(['items', 'major', 'minor']):
+            result = f(axis=i)
+            assert_frame_equal(result, frame.apply(alternative, axis=ax))
+
+    def test_count(self):
+        f = lambda s: notnull(s).sum()
+
+        self._check_statistic(self.panel, 'count', f)
+
+    def test_sum(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) == 0:
+                return np.NaN
+            else:
+                return nona.sum()
+
+        self._check_statistic(self.panel, 'sum', f)
+
+    def test_prod(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) == 0:
+                return np.NaN
+            else:
+                return np.prod(nona)
+
+        self._check_statistic(self.panel, 'prod', f)
+
+    def test_mean(self):
+        def f(x):
+            x = np.asarray(x)
+            return x[notnull(x)].mean()
+
+        self._check_statistic(self.panel, 'mean', f)
+
+    def test_median(self):
+        def f(x):
+            x = np.asarray(x)
+            return np.median(x[notnull(x)])
+
+        self._check_statistic(self.panel, 'median', f)
+
+    def test_min(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) == 0:
+                return np.NaN
+            else:
+                return nona.min()
+
+        self._check_statistic(self.panel, 'min', f)
+
+    def test_max(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) == 0:
+                return np.NaN
+            else:
+                return nona.max()
+
+        self._check_statistic(self.panel, 'max', f)
+
+    def test_var(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) < 2:
+                return np.NaN
+            else:
+                return nona.var(ddof=1)
+
+        self._check_statistic(self.panel, 'var', f)
+
+    def test_std(self):
+        def f(x):
+            x = np.asarray(x)
+            nona = x[notnull(x)]
+
+            if len(nona) < 2:
+                return np.NaN
+            else:
+                return nona.std(ddof=1)
+
+        self._check_statistic(self.panel, 'std', f)
+
+    def test_skew(self):
+        return
+        try:
+            from scipy.stats import skew
+        except ImportError:
+            return
+
+        def f(x):
+            x = np.asarray(x)
+            return skew(x[notnull(x)], bias=False)
+
+        self._check_statistic(self.panel, 'skew', f)
+
+
+class SafeForSparse(object):
+
+    @staticmethod
+    def assert_panel_equal(x, y):
+        assert_panel_equal(x, y)
+
+    def test_get_axis(self):
+        assert(self.panel._get_axis(0) is self.panel.items)
+        assert(self.panel._get_axis(1) is self.panel.major_axis)
+        assert(self.panel._get_axis(2) is self.panel.minor_axis)
+
+    def test_get_axis_number(self):
+        self.assertEqual(self.panel._get_axis_number('items'), 0)
+        self.assertEqual(self.panel._get_axis_number('major'), 1)
+        self.assertEqual(self.panel._get_axis_number('minor'), 2)
+
+    def test_get_axis_name(self):
+        self.assertEqual(self.panel._get_axis_name(0), 'items')
+        self.assertEqual(self.panel._get_axis_name(1), 'major_axis')
+        self.assertEqual(self.panel._get_axis_name(2), 'minor_axis')
+
+    def test_get_plane_axes(self):
+        # what to do here?
+
+        index, columns = self.panel._get_plane_axes('items')
+        index, columns = self.panel._get_plane_axes('major_axis')
+        index, columns = self.panel._get_plane_axes('minor_axis')
+        index, columns = self.panel._get_plane_axes(0)
+
+    def test_truncate(self):
+        dates = self.panel.major_axis
+        start, end = dates[1], dates[5]
+
+        trunced = self.panel.truncate(start, end, axis='major')
+        expected = self.panel['ItemA'].truncate(start, end)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        trunced = self.panel.truncate(before=start, axis='major')
+        expected = self.panel['ItemA'].truncate(before=start)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        trunced = self.panel.truncate(after=end, axis='major')
+        expected = self.panel['ItemA'].truncate(after=end)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        # XXX test other axes
+
+    def test_arith(self):
+        self._test_op(self.panel, operator.add)
+        self._test_op(self.panel, operator.sub)
+        self._test_op(self.panel, operator.mul)
+        self._test_op(self.panel, operator.div)
+        self._test_op(self.panel, operator.pow)
+
+        self._test_op(self.panel, lambda x, y: y + x)
+        self._test_op(self.panel, lambda x, y: y - x)
+        self._test_op(self.panel, lambda x, y: y * x)
+        self._test_op(self.panel, lambda x, y: y / x)
+        self._test_op(self.panel, lambda x, y: y ** x)
+
+        self.assertRaises(Exception, self.panel.__add__, self.panel['ItemA'])
+
+    @staticmethod
+    def _test_op(panel, op):
+        result = op(panel, 1)
+        assert_frame_equal(result['ItemA'], op(panel['ItemA'], 1))
+
+    def test_keys(self):
+        common.equalContents(self.panel.keys(), self.panel.items)
+
+    def test_iteritems(self):
+        # just test that it works
+        for k, v in self.panel.iteritems():
+            pass
+
+        self.assertEqual(len(list(self.panel.iteritems())),
+                         len(self.panel.items))
+
+    def test_combineFrame(self):
+        def check_op(op, name):
+            # items
+            df = self.panel['ItemA']
+
+            func = getattr(self.panel, name)
+
+            result = func(df, axis='items')
+
+            assert_frame_equal(result['ItemB'], op(self.panel['ItemB'], df))
+
+            # major
+            xs = self.panel.major_xs(self.panel.major_axis[0])
+            result = func(xs, axis='major')
+
+            idx = self.panel.major_axis[1]
+
+            assert_frame_equal(result.major_xs(idx),
+                               op(self.panel.major_xs(idx), xs))
+
+            # minor
+            xs = self.panel.minor_xs(self.panel.minor_axis[0])
+            result = func(xs, axis='minor')
+
+            idx = self.panel.minor_axis[1]
+
+            assert_frame_equal(result.minor_xs(idx),
+                               op(self.panel.minor_xs(idx), xs))
+
+        check_op(operator.add, 'add')
+        check_op(operator.sub, 'subtract')
+        check_op(operator.mul, 'multiply')
+        check_op(operator.div, 'divide')
+
+    def test_combinePanel(self):
+        result = self.panel.add(self.panel)
+        self.assert_panel_equal(result, self.panel * 2)
+
+    def test_neg(self):
+        self.assert_panel_equal(-self.panel, self.panel * -1)
+
+    def test_select(self):
+        p = self.panel
+
+        # select items
+        result = p.select(lambda x: x in ('ItemA', 'ItemC'), axis='items')
+        expected = p.reindex(items=['ItemA', 'ItemC'])
+        self.assert_panel_equal(result, expected)
+
+        # select major_axis
+        result = p.select(lambda x: x >= datetime(2000, 1, 15), axis='major')
+        new_major = p.major_axis[p.major_axis >= datetime(2000, 1, 15)]
+        expected = p.reindex(major=new_major)
+        self.assert_panel_equal(result, expected)
+
+        # select minor_axis
+        result = p.select(lambda x: x in ('D', 'A'), axis=2)
+        expected = p.reindex(minor=['A', 'D'])
+        self.assert_panel_equal(result, expected)
+
+        # corner case, empty thing
+        result = p.select(lambda x: x in ('foo',), axis='items')
+        self.assert_panel_equal(result, p.reindex(items=[]))
+
+class TestWidePanel(unittest.TestCase, PanelTests,
+                    SafeForLongAndSparse,
+                    SafeForSparse):
+
+    @staticmethod
+    def assert_panel_equal(x, y):
+        assert_panel_equal(x, y)
+
+    def setUp(self):
+        self.panel = common.makeWidePanel()
+        common.add_nans(self.panel)
+
+    def test_values(self):
+        # nothing to test for the moment
+        values = self.panel.values
+        self.panel.values = values
+
+    def test_fromDict(self):
+        itema = self.panel['ItemA']
+        itemb = self.panel['ItemB']
+
+        d = {'A' : itema, 'B' : itemb[5:]}
+        d2 = {'A' : itema._series, 'B' : itemb[5:]._series}
+        d3 = {'A' : DataFrame(itema._series),
+              'B' : DataFrame(itemb[5:]._series)}
+
+        wp = WidePanel.fromDict(d)
+        wp2 = WidePanel.fromDict(d2) # nested Dict
+        wp3 = WidePanel.fromDict(d3)
+        self.assert_(wp.major_axis.equals(self.panel.major_axis))
+        assert_panel_equal(wp, wp2)
+
+        # intersect
+        wp = WidePanel.fromDict(d, intersect=True)
+        self.assert_(wp.major_axis.equals(itemb.index[5:]))
+
+    def test_values(self):
+        self.assertRaises(Exception, WidePanel, np.random.randn(5, 5, 5),
+                          range(5), range(5), range(4))
+
+    def test_getitem(self):
+        self.assertRaises(Exception, self.panel.__getitem__, 'ItemQ')
+
+    def test_delitem_and_pop(self):
+        expected = self.panel['ItemA']
+        result = self.panel.pop('ItemA')
+        assert_frame_equal(expected, result)
+        self.assert_('ItemA' not in self.panel.items)
+
+        del self.panel['ItemB']
+        self.assert_('ItemB' not in self.panel.items)
+        self.assertRaises(Exception, self.panel.__delitem__, 'ItemB')
+
+        values = np.empty((3, 3, 3))
+        values[0] = 0
+        values[1] = 1
+        values[2] = 2
+
+        panel = WidePanel(values, range(3), range(3), range(3))
+
+        # did we delete the right row?
+
+        panelc = panel.copy()
+        del panelc[0]
+        assert_frame_equal(panelc[1], panel[1])
+        assert_frame_equal(panelc[2], panel[2])
+
+        panelc = panel.copy()
+        del panelc[1]
+        assert_frame_equal(panelc[0], panel[0])
+        assert_frame_equal(panelc[2], panel[2])
+
+        panelc = panel.copy()
+        del panelc[2]
+        assert_frame_equal(panelc[1], panel[1])
+        assert_frame_equal(panelc[0], panel[0])
+
+    def test_setitem(self):
+
+        # LongPanel with one item
+        lp = self.panel.filter(['ItemA']).to_long()
+        self.panel['ItemE'] = lp
+
+        lp = self.panel.filter(['ItemA', 'ItemB']).to_long()
+        self.assertRaises(Exception, self.panel.__setitem__,
+                          'ItemE', lp)
+
+        # DataFrame
+        df = self.panel['ItemA'][2:].filter(items=['A', 'B'])
+        self.panel['ItemF'] = df
+        self.panel['ItemE'] = df
+
+        df2 = self.panel['ItemF']
+
+        assert_frame_equal(df, df2.reindex(index=df.index,
+                                           columns=df.columns))
+
+        # scalar
+        self.panel['ItemG'] = 1
+        self.panel['ItemE'] = 1
+
+    def test_conform(self):
+        df = self.panel['ItemA'][:-5].filter(items=['A', 'B'])
+        conformed = self.panel.conform(df)
+
+        assert(conformed.index.equals(self.panel.major_axis))
+        assert(conformed.columns.equals(self.panel.minor_axis))
+
+    def test_reindex(self):
+        ref = self.panel['ItemB']
+
+        # items
+        result = self.panel.reindex(items=['ItemA', 'ItemB'])
+        assert_frame_equal(result['ItemB'], ref)
+
+        # major
+        new_major = list(self.panel.major_axis[:10])
+        result = self.panel.reindex(major=new_major)
+        assert_frame_equal(result['ItemB'], ref.reindex(index=new_major))
+
+        # raise exception put both major and major_axis
+        self.assertRaises(Exception, self.panel.reindex,
+                          major_axis=new_major, major=new_major)
+
+        # minor
+        new_minor = list(self.panel.minor_axis[:2])
+        result = self.panel.reindex(minor=new_minor)
+        assert_frame_equal(result['ItemB'], ref.reindex(columns=new_minor))
+
+        result = self.panel.reindex(items=self.panel.items,
+                                    major=self.panel.major_axis,
+                                    minor=self.panel.minor_axis)
+
+        assert(result.items is self.panel.items)
+        assert(result.major_axis is self.panel.major_axis)
+        assert(result.minor_axis is self.panel.minor_axis)
+
+        self.assertRaises(Exception, self.panel.reindex)
+
+        # with filling
+        smaller_major = self.panel.major_axis[::5]
+        smaller = self.panel.reindex(major=smaller_major)
+
+        larger = smaller.reindex(major=self.panel.major_axis,
+                                 method='pad')
+
+        assert_frame_equal(larger.major_xs(self.panel.major_axis[1]),
+                           smaller.major_xs(smaller_major[0]))
+
+        # reindex_like
+
+        smaller = self.panel.reindex(items=self.panel.items[:-1],
+                                     major=self.panel.major_axis[:-1],
+                                     minor=self.panel.minor_axis[:-1])
+        smaller_like = self.panel.reindex_like(smaller)
+        assert_panel_equal(smaller, smaller_like)
+
+    def test_fillna(self):
+        filled = self.panel.fillna(0)
+        self.assert_(np.isfinite(filled.values).all())
+
+        filled = self.panel.fillna(method='backfill')
+        assert_frame_equal(filled['ItemA'],
+                           self.panel['ItemA'].fillna(method='backfill'))
+
+    def test_combinePanel_with_long(self):
+        lng = self.panel.to_long(filter_observations=False)
+        result = self.panel.add(lng)
+        self.assert_panel_equal(result, self.panel * 2)
+
+    def test_major_xs(self):
+        ref = self.panel['ItemA']
+
+        idx = self.panel.major_axis[5]
+        xs = self.panel.major_xs(idx)
+
+        assert_series_equal(xs['ItemA'], ref.xs(idx))
+
+        # not contained
+        idx = self.panel.major_axis[0] - bday
+        self.assertRaises(Exception, self.panel.major_xs, idx)
+
+    def test_minor_xs(self):
+        ref = self.panel['ItemA']
+
+        idx = self.panel.minor_axis[1]
+        xs = self.panel.minor_xs(idx)
+
+        assert_series_equal(xs['ItemA'], ref[idx])
+
+        # not contained
+        self.assertRaises(Exception, self.panel.minor_xs, 'E')
+
+    def test_groupby(self):
+        grouped = self.panel.groupby({'ItemA' : 0, 'ItemB' : 0, 'ItemC' : 1},
+                                     axis='items')
+        agged = grouped.agg(np.mean)
+        self.assert_(np.array_equal(agged.items, [0, 1]))
+
+        grouped = self.panel.groupby(lambda x: x.month, axis='major')
+        agged = grouped.agg(np.mean)
+
+        self.assert_(np.array_equal(agged.major_axis, [1, 2]))
+
+        grouped = self.panel.groupby({'A' : 0, 'B' : 0, 'C' : 1, 'D' : 1},
+                                     axis='minor')
+        agged = grouped.agg(np.mean)
+        self.assert_(np.array_equal(agged.minor_axis, [0, 1]))
+
+    def test_swapaxes(self):
+        result = self.panel.swapaxes('items', 'minor')
+        self.assert_(result.items is self.panel.minor_axis)
+
+        result = self.panel.swapaxes('items', 'major')
+        self.assert_(result.items is self.panel.major_axis)
+
+        result = self.panel.swapaxes('major', 'minor')
+        self.assert_(result.major_axis is self.panel.minor_axis)
+
+        # this should also work
+        result = self.panel.swapaxes(0, 1)
+        self.assert_(result.items is self.panel.major_axis)
+
+        # this should also work
+        self.assertRaises(Exception, self.panel.swapaxes, 'items', 'items')
+
+    def test_to_long(self):
+        # filtered
+        filtered = self.panel.to_long()
+
+        # unfiltered
+        unfiltered = self.panel.to_long(filter_observations=False)
+
+        assert_panel_equal(unfiltered.to_wide(), self.panel)
+
+    def test_filter(self):
+        pass
+
+    def test_apply(self):
+        pass
+
+    def test_compound(self):
+        compounded = self.panel.compound()
+
+        assert_series_equal(compounded['ItemA'],
+                            (1 + self.panel['ItemA']).product(0) - 1)
+
+    def test_shift(self):
+        # major
+        idx = self.panel.major_axis[0]
+        idx_lag = self.panel.major_axis[1]
+
+        shifted = self.panel.shift(1)
+
+        assert_frame_equal(self.panel.major_xs(idx),
+                           shifted.major_xs(idx_lag))
+
+        # minor
+        idx = self.panel.minor_axis[0]
+        idx_lag = self.panel.minor_axis[1]
+
+        shifted = self.panel.shift(1, axis='minor')
+
+        assert_frame_equal(self.panel.minor_xs(idx),
+                           shifted.minor_xs(idx_lag))
+
+        self.assertRaises(Exception, self.panel.shift, 1, axis='items')
+
+class TestLongPanelIndex(unittest.TestCase):
+
+    def setUp(self):
+        major_axis = Index([1, 2, 3, 4])
+        minor_axis = Index([1, 2])
+
+        major_labels = np.array([0, 0, 1, 2, 3, 3])
+        minor_labels = np.array([0, 1, 0, 1, 0, 1])
+
+        self.index = LongPanelIndex(major_axis, minor_axis,
+                                    major_labels, minor_labels)
+
+        major_labels = np.array([0, 0, 1, 1, 1, 2, 2, 3, 3])
+        minor_labels = np.array([0, 1, 0, 1, 1, 0, 1, 0, 1])
+
+        self.incon = LongPanelIndex(major_axis, minor_axis,
+                                    major_labels, minor_labels)
+
+    def test_consistency(self):
+        self.assert_(self.index.consistent)
+        self.assert_(not self.incon.consistent)
+
+        # need to construct an overflow
+        major_axis = range(70000)
+        minor_axis = range(10)
+
+        major_labels = np.arange(70000)
+        minor_labels = np.repeat(range(10), 7000)
+
+        index = LongPanelIndex(major_axis, minor_axis,
+                               major_labels, minor_labels)
+
+        self.assert_(index.consistent)
+
+    def test_truncate(self):
+        result = self.index.truncate(before=1)
+        self.assert_(0 not in result.major_axis)
+        self.assert_(1 in result.major_axis)
+
+        result = self.index.truncate(after=1)
+        self.assert_(2 not in result.major_axis)
+        self.assert_(1 in result.major_axis)
+
+        result = self.index.truncate(before=1, after=2)
+        self.assertEqual(len(result.major_axis), 2)
+
+    def test_getMajorBounds(self):
+        pass
+
+    def test_getAxisBounds(self):
+        pass
+
+    def test_getLabelBounds(self):
+        pass
+
+    def test_bounds(self):
+        pass
+
+    def test_makeMask(self):
+        mask =  self.index.mask
+        expected = np.array([True, True,
+                             True, False,
+                             False, True,
+                             True, True], dtype=bool)
+        self.assert_(np.array_equal(mask, expected))
+
+    def test_dims(self):
+        pass
+
+class TestLongPanel(unittest.TestCase):
+
+    def setUp(self):
+        panel = common.makeWidePanel()
+        common.add_nans(panel)
+
+        self.panel = panel.to_long()
+        self.unfiltered_panel = panel.to_long(filter_observations=False)
+
+    def test_pickle(self):
+        import cPickle
+
+        pickled = cPickle.dumps(self.panel)
+        unpickled = cPickle.loads(pickled)
+
+        assert_almost_equal(unpickled['ItemA'].values,
+                            self.panel['ItemA'].values)
+
+    def test_len(self):
+        len(self.unfiltered_panel)
+
+    def test_constructor(self):
+        pass
+
+    def test_fromRecords_toRecords(self):
+        # structured array
+        K = 10
+
+        recs = np.zeros(K, dtype='O,O,f8,f8')
+        recs['f0'] = range(K / 2) * 2
+        recs['f1'] = np.arange(K) / (K / 2)
+        recs['f2'] = np.arange(K) * 2
+        recs['f3'] = np.arange(K)
+
+        lp = LongPanel.fromRecords(recs, 'f0', 'f1')
+        self.assertEqual(len(lp.items), 2)
+
+        lp = LongPanel.fromRecords(recs, 'f0', 'f1', exclude=['f2'])
+        self.assertEqual(len(lp.items), 1)
+
+        torecs = lp.toRecords()
+        self.assertEqual(len(torecs.dtype.names), len(lp.items) + 2)
+
+        # DataFrame
+        df = DataFrame.fromRecords(recs)
+        lp = LongPanel.fromRecords(df, 'f0', 'f1', exclude=['f2'])
+        self.assertEqual(len(lp.items), 1)
+
+        # dict of arrays
+        series = DataFrame.fromRecords(recs)._series
+        lp = LongPanel.fromRecords(series, 'f0', 'f1', exclude=['f2'])
+        self.assertEqual(len(lp.items), 1)
+        self.assert_('f2' in series)
+
+        self.assertRaises(Exception, LongPanel.fromRecords, np.zeros((3, 3)),
+                          0, 1)
+
+    def test_factors(self):
+        # structured array
+        K = 10
+
+        recs = np.zeros(K, dtype='O,O,f8,f8,O,O')
+        recs['f0'] = ['one'] * 5 + ['two'] * 5
+        recs['f1'] = ['A', 'B', 'C', 'D', 'E'] * 2
+        recs['f2'] = np.arange(K) * 2
+        recs['f3'] = np.arange(K)
+        recs['f4'] = ['A', 'B', 'C', 'D', 'E'] * 2
+        recs['f5'] = ['foo', 'bar'] * 5
+
+        lp = LongPanel.fromRecords(recs, 'f0', 'f1')
+
+    def test_columns(self):
+        self.assert_(np.array_equal(self.panel.items, self.panel.columns))
+
+    def test_copy(self):
+        thecopy = self.panel.copy()
+        self.assert_(np.array_equal(thecopy.values, self.panel.values))
+        self.assert_(thecopy.values is not self.panel.values)
+
+    def test_values(self):
+        valslice = self.panel.values[:-1]
+        self.assertRaises(Exception, self.panel._set_values, valslice)
+
+    def test_getitem(self):
+        col = self.panel['ItemA']
+
+    def test_setitem(self):
+        self.panel['ItemE'] = self.panel['ItemA']
+        self.panel['ItemF'] = 1
+
+        wp = self.panel.to_wide()
+        assert_frame_equal(wp['ItemA'], wp['ItemE'])
+
+        itemf = wp['ItemF'].values.ravel()
+        self.assert_((itemf[np.isfinite(itemf)] == 1).all())
+
+        # check exceptions raised
+        lp = self.panel.filter(['ItemA', 'ItemB'])
+        lp2 = self.panel.filter(['ItemC', 'ItemE'])
+        self.assertRaises(Exception, lp.__setitem__, 'foo', lp2)
+
+    def test_ops_differently_indexed(self):
+        # trying to set non-identically indexed panel
+        wp = self.panel.to_wide()
+        wp2 = wp.reindex(major=wp.major_axis[:-1])
+        lp2 = wp2.to_long()
+
+        self.assertRaises(Exception, self.panel.__setitem__, 'foo',
+                          lp2.filter(['ItemA']))
+
+        self.assertRaises(Exception, self.panel.add, lp2)
+
+    def test_combineFrame(self):
+        wp = self.panel.to_wide()
+        result = self.panel.add(wp['ItemA'])
+        assert_frame_equal(result.to_wide()['ItemA'], wp['ItemA'] * 2)
+
+    def test_combinePanel(self):
+        wp = self.panel.to_wide()
+        result = self.panel.add(self.panel)
+        wide_result = result.to_wide()
+        assert_frame_equal(wp['ItemA'] * 2, wide_result['ItemA'])
+
+        # one item
+        result = self.panel.add(self.panel.filter(['ItemA']))
+
+    def test_operators(self):
+        wp = self.panel.to_wide()
+        result = (self.panel + 1).to_wide()
+        assert_frame_equal(wp['ItemA'] + 1, result['ItemA'])
+
+    def test_sort(self):
+        def is_sorted(arr):
+            return (arr[1:] > arr[:-1]).any()
+
+        sorted_minor = self.panel.sort(axis='minor')
+        self.assert_(is_sorted(sorted_minor.index.minor_labels))
+
+        sorted_major = sorted_minor.sort(axis='major')
+        self.assert_(is_sorted(sorted_major.index.major_labels))
+
+    def test_to_wide(self):
+        pass
+
+    def test_toCSV(self):
+        self.panel.toCSV('__tmp__')
+        os.remove('__tmp__')
+
+    def test_toString(self):
+        from cStringIO import StringIO
+
+        buf = StringIO()
+        self.panel.toString(buf)
+        self.panel.toString(buf, col_space=12)
+
+    def test_swapaxes(self):
+        swapped = self.panel.swapaxes()
+
+        self.assert_(swapped.major_axis is self.panel.minor_axis)
+
+        # what else to test here?
+
+    def test_truncate(self):
+        dates = self.panel.major_axis
+        start, end = dates[1], dates[5]
+
+        trunced = self.panel.truncate(start, end).to_wide()
+        expected = self.panel.to_wide()['ItemA'].truncate(start, end)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        trunced = self.panel.truncate(before=start).to_wide()
+        expected = self.panel.to_wide()['ItemA'].truncate(before=start)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        trunced = self.panel.truncate(after=end).to_wide()
+        expected = self.panel.to_wide()['ItemA'].truncate(after=end)
+
+        assert_frame_equal(trunced['ItemA'], expected)
+
+        # truncate on dates that aren't in there
+        wp = self.panel.to_wide()
+        new_index = wp.major_axis[::5]
+
+        wp2 = wp.reindex(major=new_index)
+
+        lp2 = wp2.to_long()
+        lp_trunc = lp2.truncate(wp.major_axis[2], wp.major_axis[-2])
+
+        wp_trunc = wp2.truncate(wp.major_axis[2], wp.major_axis[-2])
+
+        assert_panel_equal(wp_trunc, lp_trunc.to_wide())
+
+        # throw proper exception
+        self.assertRaises(Exception, lp2.truncate, wp.major_axis[-2],
+                          wp.major_axis[2])
+
+
+    def test_filter(self):
+        pass
+
+    def test_axis_dummies(self):
+        minor_dummies = self.panel.get_axis_dummies('minor')
+        self.assertEqual(len(minor_dummies.items),
+                         len(self.panel.minor_axis))
+
+        major_dummies = self.panel.get_axis_dummies('major')
+        self.assertEqual(len(major_dummies.items),
+                         len(self.panel.major_axis))
+
+        mapping = {'A' : 'one',
+                   'B' : 'one',
+                   'C' : 'two',
+                   'D' : 'two'}
+
+        transformed = self.panel.get_axis_dummies('minor',
+                                                  transform=mapping.get)
+        self.assertEqual(len(transformed.items), 2)
+        self.assert_(np.array_equal(transformed.items, ['one', 'two']))
+
+        # TODO: test correctness
+
+    def test_get_dummies(self):
+        self.panel['Label'] = self.panel.index.minor_labels
+
+        minor_dummies = self.panel.get_axis_dummies('minor')
+        dummies = self.panel.get_dummies('Label')
+
+        self.assert_(np.array_equal(dummies.values, minor_dummies.values))
+
+    def test_apply(self):
+        # ufunc
+        applied = self.panel.apply(np.sqrt)
+        self.assert_(assert_almost_equal(
+                applied.values, np.sqrt(self.panel.values)))
+
+    def test_mean(self):
+        means = self.panel.mean('major')
+
+        # test versus WidePanel version
+        wide_means = self.panel.to_wide().mean('major')
+        assert_frame_equal(means, wide_means)
+
+        means_broadcast = self.panel.mean('major', broadcast=True)
+        self.assert_(isinstance(means_broadcast, LongPanel))
+
+        # how to check correctness?
+
+    def test_sum(self):
+        sums = self.panel.sum('major')
+
+        # test versus WidePanel version
+        wide_sums = self.panel.to_wide().sum('major')
+        assert_frame_equal(sums, wide_sums)
+
+    def test_count(self):
+        index = self.panel.index
+
+        major_count = self.panel.count('major')
+        labels = index.major_labels
+        for i, idx in enumerate(index.major_axis):
+            self.assertEqual(major_count[i], (labels == i).sum())
+
+        minor_count = self.panel.count('minor')
+        labels = index.minor_labels
+        for i, idx in enumerate(index.minor_axis):
+            self.assertEqual(minor_count[i], (labels == i).sum())
+
+    def test_leftJoin(self):
+        lp1 = self.panel.filter(['ItemA', 'ItemB'])
+        lp2 = self.panel.filter(['ItemC'])
+
+        joined = lp1.leftJoin(lp2)
+
+        self.assertEqual(len(joined.items), 3)
+
+        self.assertRaises(Exception, lp1.leftJoin,
+                          self.panel.filter(['ItemB', 'ItemC']))
+
+    def test_merge(self):
+        pass
+
+    def test_addPrefix(self):
+        lp = self.panel.addPrefix('foo#')
+        self.assertEqual(lp.items[0], 'foo#ItemA')
+
+        lp = self.panel.addPrefix()
+        assert_panel_equal(lp.to_wide(), self.panel.to_wide())
+
+    def test_pivot(self):
+        df = pivot(np.array([1, 2, 3, 4, 5]),
+                   np.array(['a', 'b', 'c', 'd', 'e']),
+                   np.array([1, 2, 3, 5, 4.]))
+        self.assertEqual(df['a'][1], 1)
+        self.assertEqual(df['b'][2], 2)
+        self.assertEqual(df['c'][3], 3)
+        self.assertEqual(df['d'][4], 5)
+        self.assertEqual(df['e'][5], 4)
+
+        # weird overlap, TODO: test?
+        df = pivot(np.array([1, 2, 3, 4, 4]),
+                   np.array(['a', 'a', 'a', 'a', 'a']),
+                   np.array([1, 2, 3, 5, 4]))
+
+        # corner case, empty
+        df = pivot(np.array([]), np.array([]), np.array([]))
+
+def test_group_agg():
+    values = np.ones((10, 2)) * np.arange(10).reshape((10, 1))
+    bounds = np.arange(5) * 2
+    f = lambda x: x.mean(axis=0)
+
+    agged = group_agg(values, bounds, f)
+
+    assert(agged[1][0] == 2.5)
+    assert(agged[2][0] == 4.5)
+
+def test_monotonic():
+    pos = np.array([1, 2, 3, 5])
+
+    assert panelm._monotonic(pos)
+
+    neg = np.array([1, 2, 3, 4, 3])
+
+    assert not panelm._monotonic(neg)
+
+    neg2 = np.array([5, 1, 2, 3, 4, 5])
+
+    assert not panelm._monotonic(neg2)
+
+class TestFactor(unittest.TestCase):
+
+    def setUp(self):
+        self.factor = panelmod.Factor.fromarray(['a', 'b', 'b', 'a',
+                                                 'a', 'c', 'c', 'c'])
+
+    def test_getitem(self):
+        self.assertEqual(self.factor[0], 'a')
+        self.assertEqual(self.factor[-1], 'c')
+
+        subf = self.factor[[0, 1, 2]]
+        common.assert_almost_equal(subf.labels, [0, 1, 1])
+
+        subf = self.factor[self.factor.asarray() == 'c']
+        common.assert_almost_equal(subf.labels, [2, 2, 2])
+
+    def test_factor_agg(self):
+        arr = np.arange(len(self.factor))
+
+        f = np.sum
+        agged = panelmod.factor_agg(self.factor, arr, f)
+        labels = self.factor.labels
+        for i, idx in enumerate(self.factor.levels):
+            self.assertEqual(f(arr[labels == i]), agged[i])
+
+if __name__ == '__main__':
+    import nose
+    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
+                   exit=False)
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 7e77f287e..1a532619a 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -1,220 +1,220 @@
-"""
-Module contains tools for processing files into DataFrames or other objects
-"""
-
-from datetime import datetime, timedelta
-from itertools import izip
-import re
-import string
-
-import numpy as np
-
-from pandas.core.index import Index
-from pandas.core.frame import DataFrame
-
-def parseCSV(filepath, header=0, skiprows=None, indexCol=0,
-             na_values=None):
-    """
-    Parse CSV file into a DataFrame object. Try to parse dates if possible.
-    """
-    import csv
-    f = open(filepath,'U')
-    reader = csv.reader(f, dialect='excel')
-
-    if skiprows is not None:
-        skiprows = set(skiprows)
-        lines = [l for i, l in enumerate(reader) if i not in skiprows]
-    else:
-        lines = [l for l in reader]
-    f.close()
-    return simpleParser(lines, header=header, indexCol=indexCol,
-                        na_values=na_values)
-
-def read_table(path, header=0, index_col=0, delimiter=','):
-    data = np.genfromtxt(path, delimiter=delimiter,
-                         names=header is not None,
-                         dtype=object)
-
-    columns = data.dtype.names
-
-def parseText(filepath, sep='\t', header=0, indexCol=0, colNames=None):
-    """
-    Parse whitespace separated file into a DataFrame object.
-    Try to parse dates if possible.
-    """
-    lines = [re.split(sep, l.rstrip()) for l in open(filepath,'rb').readlines()]
-    return simpleParser(lines, header=header, indexCol=indexCol,
-                        colNames = colNames)
-
-
-
-def simpleParser(lines, colNames=None, header=0, indexCol=0,
-                 na_values=None):
-    """
-    Workhorse function for processing nested list into DataFrame
-
-    Should be replaced by np.genfromtxt eventually?
-    """
-    data = {}
-    if header is not None:
-        columns = []
-        for i, c in enumerate(lines[header]):
-            if c == '':
-                columns.append('Unnamed: %d' % i)
-            else:
-                columns.append(c)
-
-        content = lines[header+1:]
-
-        colCounts = dict(((col, 0) for col in columns))
-        for i, col in enumerate(columns):
-            if columns.count(col) > 1:
-                columns[i] = col + str(colCounts[col])
-                colCounts[col] += 1
-    else:
-        if not colNames:
-            columns = list(string.ascii_uppercase[:len(lines[0])])
-            # columns = ['X.%d' % (i + 1) for i in range(len(lines[0]))]
-        else:
-            columns = colNames
-        content = lines
-
-    data = dict(izip(columns, izip(*content)))
-    if indexCol is not None:
-        index_name = columns[indexCol]
-        # try to parse dates
-        index = _try_parse_dates(data.pop(index_name))
-    else:
-        index = np.arange(len(data.values()[0]))
-
-    data = _floatify(data, na_values=na_values)
-    data = _convert_to_ndarrays(data)
-    return DataFrame(data=data, columns=columns, index=Index(index))
-
-def _floatify(data_dict, na_values=None):
-    # common NA values
-    NA_VALUES = set(['-1.#IND', '1.#QNAN', '1.#IND',
-                     '-1.#QNAN','1.#INF','-1.#INF', '1.#INF000000',
-                     'NA', '#NA', 'NULL', 'NaN', 'nan', ''])
-    if na_values is None:
-        na_values = NA_VALUES
-    else:
-        na_values = set(list(na_values))
-
-    def _convert_float(val):
-        if val in na_values:
-            return np.nan
-        else:
-            try:
-                parsed = np.float64(val)
-                if np.isinf(parsed):
-                    return val
-                return parsed
-            except Exception:
-                return val
-
-    result = {}
-    for col, values in data_dict.iteritems():
-        result[col] = [_convert_float(val) for val in values]
-
-    return result
-
-def _convert_to_ndarrays(dct):
-    result = {}
-    for c, values in dct.iteritems():
-        try:
-            result[c] = np.array(values, dtype=float)
-        except Exception:
-            result[c] = np.array(values, dtype=object)
-
-    return result
-
-def _try_parse_dates(values):
-    try:
-        from dateutil import parser
-        parse_date = parser.parse
-    except ImportError:
-        def parse_date(s):
-            try:
-                return datetime.strptime(s, '%m/%d/%Y')
-            except Exception:
-                return s
-
-    try:
-        # easier to ask forgiveness than permission
-        return [parse_date(val) for val in values]
-    except Exception:
-        # failed
-        return values
-
-#===============================================================================
-# Excel tools
-#===============================================================================
-
-
-class ExcelFile(object):
-    """
-    Class for parsing tabular .xls sheets into DataFrame objects, uses xlrd
-
-    Parameters
-    ----------
-    path : string
-        Path to xls file
-    """
-
-    def __init__(self, path):
-        import xlrd
-        self.path = path
-        self.book = xlrd.open_workbook(path)
-
-    def old_parse(self, sheetname, header=None, index_col=0, date_col=0):
-        from pandas.core.datetools import ole2datetime
-        sheet = self.book.sheet_by_name(sheetname)
-
-        data = [sheet.row_values(i) for i in range(sheet.nrows)]
-        if date_col is not None:
-            for row in data:
-                try:
-                    row[date_col] = ole2datetime(row[date_col])
-                except Exception:
-                    pass
-        return simpleParser(data, header=header, indexCol=index_col)
-
-    def parse(self, sheetname, header=None, skiprows=None, index_col=0,
-              na_values=None):
-        from datetime import MINYEAR, time, datetime
-        from xlrd import xldate_as_tuple, XL_CELL_DATE
-
-        datemode = self.book.datemode
-        sheet = self.book.sheet_by_name(sheetname)
-
-        if skiprows is None:
-            skiprows = set()
-        else:
-            skiprows = set(skiprows)
-
-        data = []
-        for i in range(sheet.nrows):
-            if i in skiprows:
-                continue
-
-            row = []
-            for value, typ in zip(sheet.row_values(i), sheet.row_types(i)):
-                if typ == XL_CELL_DATE:
-                    dt = xldate_as_tuple(value, datemode)
-                    if dt[0] < MINYEAR:
-                        value = time(*dt[3:])
-                    else:
-                        value = datetime(*dt)
-                row.append(value)
-            data.append(row)
-        return simpleParser(data, header=header, indexCol=index_col,
-                            na_values=na_values)
-
-def parseExcel(filepath, header=None, indexCol=0, sheetname=None, **kwds):
-    """
-
-    """
-    excel_file = ExcelFile(filepath)
-    return excel_file.parse(sheetname, header=header, index_col=indexCol)
-
+"""
+Module contains tools for processing files into DataFrames or other objects
+"""
+
+from datetime import datetime, timedelta
+from itertools import izip
+import re
+import string
+
+import numpy as np
+
+from pandas.core.index import Index
+from pandas.core.frame import DataFrame
+
+def parseCSV(filepath, header=0, skiprows=None, indexCol=0,
+             na_values=None):
+    """
+    Parse CSV file into a DataFrame object. Try to parse dates if possible.
+    """
+    import csv
+    f = open(filepath,'U')
+    reader = csv.reader(f, dialect='excel')
+
+    if skiprows is not None:
+        skiprows = set(skiprows)
+        lines = [l for i, l in enumerate(reader) if i not in skiprows]
+    else:
+        lines = [l for l in reader]
+    f.close()
+    return simpleParser(lines, header=header, indexCol=indexCol,
+                        na_values=na_values)
+
+def read_table(path, header=0, index_col=0, delimiter=','):
+    data = np.genfromtxt(path, delimiter=delimiter,
+                         names=header is not None,
+                         dtype=object)
+
+    columns = data.dtype.names
+
+def parseText(filepath, sep='\t', header=0, indexCol=0, colNames=None):
+    """
+    Parse whitespace separated file into a DataFrame object.
+    Try to parse dates if possible.
+    """
+    lines = [re.split(sep, l.rstrip()) for l in open(filepath,'rb').readlines()]
+    return simpleParser(lines, header=header, indexCol=indexCol,
+                        colNames = colNames)
+
+
+
+def simpleParser(lines, colNames=None, header=0, indexCol=0,
+                 na_values=None):
+    """
+    Workhorse function for processing nested list into DataFrame
+
+    Should be replaced by np.genfromtxt eventually?
+    """
+    data = {}
+    if header is not None:
+        columns = []
+        for i, c in enumerate(lines[header]):
+            if c == '':
+                columns.append('Unnamed: %d' % i)
+            else:
+                columns.append(c)
+
+        content = lines[header+1:]
+
+        colCounts = dict(((col, 0) for col in columns))
+        for i, col in enumerate(columns):
+            if columns.count(col) > 1:
+                columns[i] = col + str(colCounts[col])
+                colCounts[col] += 1
+    else:
+        if not colNames:
+            columns = list(string.ascii_uppercase[:len(lines[0])])
+            # columns = ['X.%d' % (i + 1) for i in range(len(lines[0]))]
+        else:
+            columns = colNames
+        content = lines
+
+    data = dict(izip(columns, izip(*content)))
+    if indexCol is not None:
+        index_name = columns[indexCol]
+        # try to parse dates
+        index = _try_parse_dates(data.pop(index_name))
+    else:
+        index = np.arange(len(data.values()[0]))
+
+    data = _floatify(data, na_values=na_values)
+    data = _convert_to_ndarrays(data)
+    return DataFrame(data=data, columns=columns, index=Index(index))
+
+def _floatify(data_dict, na_values=None):
+    # common NA values
+    NA_VALUES = set(['-1.#IND', '1.#QNAN', '1.#IND',
+                     '-1.#QNAN','1.#INF','-1.#INF', '1.#INF000000',
+                     'NA', '#NA', 'NULL', 'NaN', 'nan', ''])
+    if na_values is None:
+        na_values = NA_VALUES
+    else:
+        na_values = set(list(na_values))
+
+    def _convert_float(val):
+        if val in na_values:
+            return np.nan
+        else:
+            try:
+                parsed = np.float64(val)
+                if np.isinf(parsed):
+                    return val
+                return parsed
+            except Exception:
+                return val
+
+    result = {}
+    for col, values in data_dict.iteritems():
+        result[col] = [_convert_float(val) for val in values]
+
+    return result
+
+def _convert_to_ndarrays(dct):
+    result = {}
+    for c, values in dct.iteritems():
+        try:
+            result[c] = np.array(values, dtype=float)
+        except Exception:
+            result[c] = np.array(values, dtype=object)
+
+    return result
+
+def _try_parse_dates(values):
+    try:
+        from dateutil import parser
+        parse_date = parser.parse
+    except ImportError:
+        def parse_date(s):
+            try:
+                return datetime.strptime(s, '%m/%d/%Y')
+            except Exception:
+                return s
+
+    try:
+        # easier to ask forgiveness than permission
+        return [parse_date(val) for val in values]
+    except Exception:
+        # failed
+        return values
+
+#===============================================================================
+# Excel tools
+#===============================================================================
+
+
+class ExcelFile(object):
+    """
+    Class for parsing tabular .xls sheets into DataFrame objects, uses xlrd
+
+    Parameters
+    ----------
+    path : string
+        Path to xls file
+    """
+
+    def __init__(self, path):
+        import xlrd
+        self.path = path
+        self.book = xlrd.open_workbook(path)
+
+    def old_parse(self, sheetname, header=None, index_col=0, date_col=0):
+        from pandas.core.datetools import ole2datetime
+        sheet = self.book.sheet_by_name(sheetname)
+
+        data = [sheet.row_values(i) for i in range(sheet.nrows)]
+        if date_col is not None:
+            for row in data:
+                try:
+                    row[date_col] = ole2datetime(row[date_col])
+                except Exception:
+                    pass
+        return simpleParser(data, header=header, indexCol=index_col)
+
+    def parse(self, sheetname, header=None, skiprows=None, index_col=0,
+              na_values=None):
+        from datetime import MINYEAR, time, datetime
+        from xlrd import xldate_as_tuple, XL_CELL_DATE
+
+        datemode = self.book.datemode
+        sheet = self.book.sheet_by_name(sheetname)
+
+        if skiprows is None:
+            skiprows = set()
+        else:
+            skiprows = set(skiprows)
+
+        data = []
+        for i in range(sheet.nrows):
+            if i in skiprows:
+                continue
+
+            row = []
+            for value, typ in zip(sheet.row_values(i), sheet.row_types(i)):
+                if typ == XL_CELL_DATE:
+                    dt = xldate_as_tuple(value, datemode)
+                    if dt[0] < MINYEAR:
+                        value = time(*dt[3:])
+                    else:
+                        value = datetime(*dt)
+                row.append(value)
+            data.append(row)
+        return simpleParser(data, header=header, indexCol=index_col,
+                            na_values=na_values)
+
+def parseExcel(filepath, header=None, indexCol=0, sheetname=None, **kwds):
+    """
+
+    """
+    excel_file = ExcelFile(filepath)
+    return excel_file.parse(sheetname, header=header, index_col=indexCol)
+
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index 5b6f1808a..8b9fd0c8b 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -1,232 +1,232 @@
-from datetime import datetime
-import numpy as np
-
-from pandas import Series, TimeSeries, DataFrame, WidePanel, LongPanel
-from pandas.core.pytools import adjoin
-import pandas.lib.tseries as tseries
-
-try:
-    import tables
-except ImportError:
-    pass
-
-class HDFStore(object):
-    """
-    dict-like IO interface for storing pandas objects in PyTables
-    format
-
-    Parameters
-    ----------
-    path : string
-        File path to HDF5 file
-
-    Examples
-    --------
-    >>> store = HDFStore('test.h5')
-    >>> store['foo'] = bar   # write to HDF5
-    >>> bar = store['foo']   # retrieve
-    >>> store.close()
-    """
-    def __init__(self, path):
-        self.handle = tables.openFile(path, 'a')
-
-    def close(self):
-        self.handle.close()
-
-    def flush(self):
-        self.handle.flush()
-
-    def __repr__(self):
-        output = str(self.__class__) + '\n'
-
-        keys = []
-        values = []
-        for k, v in sorted(self.handle.root._v_children.iteritems()):
-            kind = v._v_attrs.pandas_type
-
-            keys.append(str(k))
-            values.append(kind)
-
-        output += adjoin(5, keys, values)
-        return output
-
-    def get(self, key):
-        """
-        Retrieve pandas object stored in file
-
-        Parameters
-        ----------
-        key : object
-        """
-
-        return self[key]
-
-    def put(self, key, value):
-        """
-        Store object in file
-
-        Parameters
-        ----------
-        key : object
-        value : {Series, DataFrame, WidePanel, LongPanel}
-            pandas data structure
-        """
-        self[key] = value
-
-    def __getitem__(self, key):
-        group = getattr(self.handle.root, key)
-        return _read_group(group)
-
-    def __setitem__(self, key, value):
-        self._write_group(key, value)
-
-    def _write_group(self, key, value):
-        root = self.handle.root
-
-        if key not in root._v_children:
-            group = self.handle.createGroup(root, key)
-        else:
-            group = getattr(root, key)
-
-        kind = type(value)
-        handler = self._get_write_handler(kind)
-
-        try:
-            handler(group, value)
-        except Exception:
-            raise
-
-        group._v_attrs.pandas_type = kind.__name__
-        return True
-
-    def _write_series(self, group, series):
-        self._write_index(group, 'index', series.index)
-        self._write_array(group, 'values', np.asarray(series))
-
-    def _write_frame(self, group, df):
-        self._write_index(group, 'index', df.index)
-        self._write_index(group, 'columns', df.columns)
-        self._write_array(group, 'values', df.asMatrix(df.columns))
-
-    def _write_matrix(self, group, dm):
-        self._write_index(group, 'index', dm.index)
-        self._write_index(group, 'columns', dm.columns)
-        self._write_array(group, 'values', dm.values)
-
-        if dm.objects is not None:
-            self._write_index(group, 'obj_columns', dm.objects.columns)
-            self._write_array(group, 'obj_values', dm.objects.values)
-
-    def _write_wide(self, group, value):
-        pass
-
-    def _write_long(self, group, value):
-        pass
-
-    def _write_index(self, group, key, value):
-        converted, kind = _convert_index(value)
-        self._write_array(group, key, converted)
-        node = getattr(group, key)
-
-        node._v_attrs.kind = kind
-
-    def _write_array(self, group, key, value):
-        if key in group:
-            self.handle.removeNode(group, key)
-
-        self.handle.createArray(group, key, value)
-
-    def _get_write_handler(self, kind):
-        handlers = {
-            Series : self._write_series,
-            TimeSeries : self._write_series,
-            DataFrame : self._write_frame,
-            DataMatrix : self._write_matrix,
-            WidePanel : self._write_wide,
-            LongPanel : self._write_long
-        }
-        return handlers[kind]
-
-def _read_group(group):
-    kind = group._v_attrs.pandas_type
-
-    if kind in ('Series', 'TimeSeries'):
-        return _read_series(group)
-    elif kind == 'DataFrame':
-        return _read_frame(group)
-    elif kind == 'DataMatrix':
-        return _read_matrix(group)
-    elif kind == 'WidePanel':
-        return _read_wide(group)
-    elif kind == 'LongPanel':
-        return _read_long(group)
-
-
-def _read_series(group):
-    index = _read_index(group, 'index')
-    values = group.values[:]
-
-    return Series(values, index=index)
-
-def _read_frame(group):
-    index = _read_index(group, 'index')
-    columns = _read_index(group, 'columns')
-    values = group.values[:]
-
-    return DataFrame(values, index=index, columns=columns)
-
-def _read_matrix(group):
-    index = _read_index(group, 'index')
-    columns = _read_index(group, 'columns')
-    values = group.values[:]
-    objects = None
-
-    if hasattr(group, 'obj_columns'):
-        obj_columns = _read_index(group, 'columns')
-        obj_values = group.obj_values[:]
-        objects = DataMatrix(obj_values, index=index, columns=obj_columns)
-
-    return DataMatrix(values, index=index, columns=columns,
-                      objects=objects)
-
-def _read_wide(group):
-    index = _read_index(group, 'index')
-    values = group.values[:]
-
-    return Series(values, index=index)
-
-def _read_long(group):
-    index = _read_index(group, 'index')
-    values = group.values[:]
-
-    return Series(values, index=index)
-
-def _read_index(group, key):
-    node = getattr(group, key)
-    data = node[:]
-    kind = node._v_attrs.kind
-
-    return _unconvert_index(data, kind)
-
-def _convert_index(index):
-    # Let's assume the index is homogeneous
-    values = np.asarray(index)
-
-    if isinstance(values[0], datetime):
-        converted = tseries.array_to_timestamp(values)
-        return converted, 'datetime'
-    elif isinstance(values[0], basestring):
-        converted = np.array(list(values), dtype=np.str_)
-        return converted, 'string'
-    else:
-        return np.array(list(values)), 'other'
-
-def _unconvert_index(data, kind):
-    if kind == 'datetime':
-        index = tseries.array_to_datetime(data)
-    elif kind == 'string':
-        index = np.array(data, dtype=object)
-    else:
-        index = data
-
-    return index
+from datetime import datetime
+import numpy as np
+
+from pandas import Series, TimeSeries, DataFrame, WidePanel, LongPanel
+from pandas.core.pytools import adjoin
+import pandas.lib.tseries as tseries
+
+try:
+    import tables
+except ImportError:
+    pass
+
+class HDFStore(object):
+    """
+    dict-like IO interface for storing pandas objects in PyTables
+    format
+
+    Parameters
+    ----------
+    path : string
+        File path to HDF5 file
+
+    Examples
+    --------
+    >>> store = HDFStore('test.h5')
+    >>> store['foo'] = bar   # write to HDF5
+    >>> bar = store['foo']   # retrieve
+    >>> store.close()
+    """
+    def __init__(self, path):
+        self.handle = tables.openFile(path, 'a')
+
+    def close(self):
+        self.handle.close()
+
+    def flush(self):
+        self.handle.flush()
+
+    def __repr__(self):
+        output = str(self.__class__) + '\n'
+
+        keys = []
+        values = []
+        for k, v in sorted(self.handle.root._v_children.iteritems()):
+            kind = v._v_attrs.pandas_type
+
+            keys.append(str(k))
+            values.append(kind)
+
+        output += adjoin(5, keys, values)
+        return output
+
+    def get(self, key):
+        """
+        Retrieve pandas object stored in file
+
+        Parameters
+        ----------
+        key : object
+        """
+
+        return self[key]
+
+    def put(self, key, value):
+        """
+        Store object in file
+
+        Parameters
+        ----------
+        key : object
+        value : {Series, DataFrame, WidePanel, LongPanel}
+            pandas data structure
+        """
+        self[key] = value
+
+    def __getitem__(self, key):
+        group = getattr(self.handle.root, key)
+        return _read_group(group)
+
+    def __setitem__(self, key, value):
+        self._write_group(key, value)
+
+    def _write_group(self, key, value):
+        root = self.handle.root
+
+        if key not in root._v_children:
+            group = self.handle.createGroup(root, key)
+        else:
+            group = getattr(root, key)
+
+        kind = type(value)
+        handler = self._get_write_handler(kind)
+
+        try:
+            handler(group, value)
+        except Exception:
+            raise
+
+        group._v_attrs.pandas_type = kind.__name__
+        return True
+
+    def _write_series(self, group, series):
+        self._write_index(group, 'index', series.index)
+        self._write_array(group, 'values', np.asarray(series))
+
+    def _write_frame(self, group, df):
+        self._write_index(group, 'index', df.index)
+        self._write_index(group, 'columns', df.columns)
+        self._write_array(group, 'values', df.asMatrix(df.columns))
+
+    def _write_matrix(self, group, dm):
+        self._write_index(group, 'index', dm.index)
+        self._write_index(group, 'columns', dm.columns)
+        self._write_array(group, 'values', dm.values)
+
+        if dm.objects is not None:
+            self._write_index(group, 'obj_columns', dm.objects.columns)
+            self._write_array(group, 'obj_values', dm.objects.values)
+
+    def _write_wide(self, group, value):
+        pass
+
+    def _write_long(self, group, value):
+        pass
+
+    def _write_index(self, group, key, value):
+        converted, kind = _convert_index(value)
+        self._write_array(group, key, converted)
+        node = getattr(group, key)
+
+        node._v_attrs.kind = kind
+
+    def _write_array(self, group, key, value):
+        if key in group:
+            self.handle.removeNode(group, key)
+
+        self.handle.createArray(group, key, value)
+
+    def _get_write_handler(self, kind):
+        handlers = {
+            Series : self._write_series,
+            TimeSeries : self._write_series,
+            DataFrame : self._write_frame,
+            DataMatrix : self._write_matrix,
+            WidePanel : self._write_wide,
+            LongPanel : self._write_long
+        }
+        return handlers[kind]
+
+def _read_group(group):
+    kind = group._v_attrs.pandas_type
+
+    if kind in ('Series', 'TimeSeries'):
+        return _read_series(group)
+    elif kind == 'DataFrame':
+        return _read_frame(group)
+    elif kind == 'DataMatrix':
+        return _read_matrix(group)
+    elif kind == 'WidePanel':
+        return _read_wide(group)
+    elif kind == 'LongPanel':
+        return _read_long(group)
+
+
+def _read_series(group):
+    index = _read_index(group, 'index')
+    values = group.values[:]
+
+    return Series(values, index=index)
+
+def _read_frame(group):
+    index = _read_index(group, 'index')
+    columns = _read_index(group, 'columns')
+    values = group.values[:]
+
+    return DataFrame(values, index=index, columns=columns)
+
+def _read_matrix(group):
+    index = _read_index(group, 'index')
+    columns = _read_index(group, 'columns')
+    values = group.values[:]
+    objects = None
+
+    if hasattr(group, 'obj_columns'):
+        obj_columns = _read_index(group, 'columns')
+        obj_values = group.obj_values[:]
+        objects = DataMatrix(obj_values, index=index, columns=obj_columns)
+
+    return DataMatrix(values, index=index, columns=columns,
+                      objects=objects)
+
+def _read_wide(group):
+    index = _read_index(group, 'index')
+    values = group.values[:]
+
+    return Series(values, index=index)
+
+def _read_long(group):
+    index = _read_index(group, 'index')
+    values = group.values[:]
+
+    return Series(values, index=index)
+
+def _read_index(group, key):
+    node = getattr(group, key)
+    data = node[:]
+    kind = node._v_attrs.kind
+
+    return _unconvert_index(data, kind)
+
+def _convert_index(index):
+    # Let's assume the index is homogeneous
+    values = np.asarray(index)
+
+    if isinstance(values[0], datetime):
+        converted = tseries.array_to_timestamp(values)
+        return converted, 'datetime'
+    elif isinstance(values[0], basestring):
+        converted = np.array(list(values), dtype=np.str_)
+        return converted, 'string'
+    else:
+        return np.array(list(values)), 'other'
+
+def _unconvert_index(data, kind):
+    if kind == 'datetime':
+        index = tseries.array_to_datetime(data)
+    elif kind == 'string':
+        index = np.array(data, dtype=object)
+    else:
+        index = data
+
+    return index
diff --git a/pandas/lib/src/groupby.pyx b/pandas/lib/src/groupby.pyx
index 93707611b..1f10ab598 100644
--- a/pandas/lib/src/groupby.pyx
+++ b/pandas/lib/src/groupby.pyx
@@ -1,285 +1,285 @@
-
-#-------------------------------------------------------------------------------
-# Groupby-related functions
-
-cdef inline _isnan(object o):
-    return o != o
-
-@cython.boundscheck(False)
-def arrmap(ndarray[object, ndim=1] index, object func):
-    cdef int length = index.shape[0]
-    cdef int i = 0
-
-    cdef ndarray[object, ndim=1] result = np.empty(length, dtype=np.object_)
-
-    for i from 0 <= i < length:
-        result[i] = func(index[i])
-
-    return result
-
-@cython.boundscheck(False)
-def groupby(object index, object mapper, output=None):
-    cdef dict result
-    cdef ndarray[object, ndim=1] mapped_index
-    cdef ndarray[object, ndim=1] index_buf
-    cdef ndarray[int8_t, ndim=1] mask
-    cdef int i, length
-    cdef list members
-    cdef object idx, key
-
-    length = len(index)
-
-    if output is None:
-        result = {}
-    else:
-        result = <dict> output
-
-    index_buf = np.asarray(index)
-    mapped_index = arrmap(index_buf, mapper)
-    mask = isnullobj(mapped_index)
-    nullkeys = index_buf[mask.astype(bool)]
-    if len(nullkeys) > 0:
-        result[np.NaN] = nullkeys
-
-    for i from 0 <= i < length:
-        if mask[i]:
-            continue
-
-        key = mapped_index[i]
-        idx = index_buf[i]
-        if key in result:
-            members = result[key]
-            members.append(idx)
-        else:
-            result[key] = [idx]
-
-    return result
-
-@cython.boundscheck(False)
-def groupby_indices(object index, object mapper):
-    cdef dict result
-    cdef ndarray[object, ndim=1] mapped_index
-    cdef ndarray[int8_t, ndim=1] mask
-    cdef int i, length
-    cdef list members, null_list
-    cdef object key
-
-    length = len(index)
-
-    result = {}
-    index = np.asarray(index)
-    mapped_index = arrmap(index, mapper)
-
-    mask = isnullobj(mapped_index)
-
-    if mask.astype(bool).any():
-        null_list = []
-        result[np.NaN] = null_list
-
-    for i from 0 <= i < length:
-        if mask[i]:
-            null_list.append(i)
-        key = mapped_index[i]
-        if key in result:
-            (<list> result[key]).append(i)
-        else:
-            result[key] = [i]
-
-    return result
-
-def reduce_mean(ndarray[object, ndim=1] indices,
-                ndarray[object, ndim=1] buckets,
-                ndarray[double_t, ndim=1] values,
-                inclusive=False):
-    cdef:
-        Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t, ndim=1] output
-        double_t the_sum, val, nobs
-
-
-
-    nbuckets = len(buckets)
-    nvalues = len(indices)
-
-    assert(len(values) == len(indices))
-
-    output = np.empty(nbuckets, dtype=float)
-    output.fill(np.NaN)
-
-    j = 0
-    for i from 0 <= i < nbuckets:
-        next_bound = buckets[i]
-        the_sum = 0
-        nobs = 0
-        if inclusive:
-            while j < nvalues and indices[j] <= next_bound:
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-        else:
-            while j < nvalues and indices[j] < next_bound:
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-
-        if nobs > 0:
-            output[i] = the_sum / nobs
-
-        if j >= nvalues:
-            break
-
-    return output
-
-def _bucket_locs(index, buckets, inclusive=False):
-    if inclusive:
-        locs = index.searchsorted(buckets, side='left')
-    else:
-        locs = index.searchsorted(buckets, side='right')
-
-    return locs
-
-
-def ts_upsample_mean(ndarray[object, ndim=1] indices,
-                     ndarray[object, ndim=1] buckets,
-                     ndarray[double_t, ndim=1] values,
-                     inclusive=False):
-    '''
-    put something here
-    '''
-    cdef:
-        Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t, ndim=1] output
-        object next_bound
-        double_t the_sum, val, nobs
-
-    nbuckets = len(buckets)
-    nvalues = len(indices)
-
-    assert(len(values) == len(indices))
-
-    output = np.empty(nbuckets, dtype=float)
-    output.fill(np.NaN)
-
-    j = 0
-    for i from 0 <= i < nbuckets:
-        next_bound = buckets[i]
-        the_sum = 0
-        nobs = 0
-        if inclusive:
-            while j < nvalues and indices[j] <= next_bound:
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-        else:
-            while j < nvalues and indices[j] < next_bound:
-    cdef:
-        Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t, ndim=1] output
-        object next_bound
-        double_t the_sum, val, nobs
-
-    nbuckets = len(buckets)
-    nvalues = len(indices)
-
-    assert(len(values) == len(indices))
-
-    output = np.empty(nbuckets, dtype=float)
-    output.fill(np.NaN)
-
-    j = 0
-    for i from 0 <= i < nbuckets:
-        next_bound = buckets[i]
-        the_sum = 0
-        nobs = 0
-        if inclusive:
-            while j < nvalues and indices[j] <= next_bound:
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-        else:
-            while j < nvalues and indices[j] < next_bound:
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-
-        if nobs > 0:
-            output[i] = the_sum / nobs
-
-        if j >= nvalues:
-            break
-
-    return output
-                val = values[j]
-                # not NaN
-                if val == val:
-                    the_sum += val
-                    nobs += 1
-                j += 1
-
-        if nobs > 0:
-            output[i] = the_sum / nobs
-
-        if j >= nvalues:
-            break
-
-    return output
-
-def ts_upsample_generic(ndarray[object, ndim=1] indices,
-                        ndarray[object, ndim=1] buckets,
-                        ndarray[double_t, ndim=1] values,
-                        object aggfunc,
-                        inclusive=False):
-    '''
-    put something here
-    '''
-    cdef:
-        Py_ssize_t i, j, jstart, nbuckets, nvalues
-        ndarray[double_t, ndim=1] output
-        object next_bound
-        double_t the_sum, val, nobs
-
-    nbuckets = len(buckets)
-    nvalues = len(indices)
-
-    assert(len(values) == len(indices))
-
-    output = np.empty(nbuckets, dtype=float)
-    output.fill(np.NaN)
-
-    j = 0
-    for i from 0 <= i < nbuckets:
-        next_bound = buckets[i]
-        the_sum = 0
-        nobs = 0
-
-        jstart = j
-        if inclusive:
-            while j < nvalues and indices[j] <= next_bound:
-                j += 1
-        else:
-            while j < nvalues and indices[j] < next_bound:
-                j += 1
-
-        if nobs > 0:
-            output[i] = aggfunc(values[jstart:j])
-
-        if j >= nvalues:
-            break
-
-    return output
-
+
+#-------------------------------------------------------------------------------
+# Groupby-related functions
+
+cdef inline _isnan(object o):
+    return o != o
+
+@cython.boundscheck(False)
+def arrmap(ndarray[object, ndim=1] index, object func):
+    cdef int length = index.shape[0]
+    cdef int i = 0
+
+    cdef ndarray[object, ndim=1] result = np.empty(length, dtype=np.object_)
+
+    for i from 0 <= i < length:
+        result[i] = func(index[i])
+
+    return result
+
+@cython.boundscheck(False)
+def groupby(object index, object mapper, output=None):
+    cdef dict result
+    cdef ndarray[object, ndim=1] mapped_index
+    cdef ndarray[object, ndim=1] index_buf
+    cdef ndarray[int8_t, ndim=1] mask
+    cdef int i, length
+    cdef list members
+    cdef object idx, key
+
+    length = len(index)
+
+    if output is None:
+        result = {}
+    else:
+        result = <dict> output
+
+    index_buf = np.asarray(index)
+    mapped_index = arrmap(index_buf, mapper)
+    mask = isnullobj(mapped_index)
+    nullkeys = index_buf[mask.astype(bool)]
+    if len(nullkeys) > 0:
+        result[np.NaN] = nullkeys
+
+    for i from 0 <= i < length:
+        if mask[i]:
+            continue
+
+        key = mapped_index[i]
+        idx = index_buf[i]
+        if key in result:
+            members = result[key]
+            members.append(idx)
+        else:
+            result[key] = [idx]
+
+    return result
+
+@cython.boundscheck(False)
+def groupby_indices(object index, object mapper):
+    cdef dict result
+    cdef ndarray[object, ndim=1] mapped_index
+    cdef ndarray[int8_t, ndim=1] mask
+    cdef int i, length
+    cdef list members, null_list
+    cdef object key
+
+    length = len(index)
+
+    result = {}
+    index = np.asarray(index)
+    mapped_index = arrmap(index, mapper)
+
+    mask = isnullobj(mapped_index)
+
+    if mask.astype(bool).any():
+        null_list = []
+        result[np.NaN] = null_list
+
+    for i from 0 <= i < length:
+        if mask[i]:
+            null_list.append(i)
+        key = mapped_index[i]
+        if key in result:
+            (<list> result[key]).append(i)
+        else:
+            result[key] = [i]
+
+    return result
+
+def reduce_mean(ndarray[object, ndim=1] indices,
+                ndarray[object, ndim=1] buckets,
+                ndarray[double_t, ndim=1] values,
+                inclusive=False):
+    cdef:
+        Py_ssize_t i, j, nbuckets, nvalues
+        ndarray[double_t, ndim=1] output
+        double_t the_sum, val, nobs
+
+
+
+    nbuckets = len(buckets)
+    nvalues = len(indices)
+
+    assert(len(values) == len(indices))
+
+    output = np.empty(nbuckets, dtype=float)
+    output.fill(np.NaN)
+
+    j = 0
+    for i from 0 <= i < nbuckets:
+        next_bound = buckets[i]
+        the_sum = 0
+        nobs = 0
+        if inclusive:
+            while j < nvalues and indices[j] <= next_bound:
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+        else:
+            while j < nvalues and indices[j] < next_bound:
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+
+        if nobs > 0:
+            output[i] = the_sum / nobs
+
+        if j >= nvalues:
+            break
+
+    return output
+
+def _bucket_locs(index, buckets, inclusive=False):
+    if inclusive:
+        locs = index.searchsorted(buckets, side='left')
+    else:
+        locs = index.searchsorted(buckets, side='right')
+
+    return locs
+
+
+def ts_upsample_mean(ndarray[object, ndim=1] indices,
+                     ndarray[object, ndim=1] buckets,
+                     ndarray[double_t, ndim=1] values,
+                     inclusive=False):
+    '''
+    put something here
+    '''
+    cdef:
+        Py_ssize_t i, j, nbuckets, nvalues
+        ndarray[double_t, ndim=1] output
+        object next_bound
+        double_t the_sum, val, nobs
+
+    nbuckets = len(buckets)
+    nvalues = len(indices)
+
+    assert(len(values) == len(indices))
+
+    output = np.empty(nbuckets, dtype=float)
+    output.fill(np.NaN)
+
+    j = 0
+    for i from 0 <= i < nbuckets:
+        next_bound = buckets[i]
+        the_sum = 0
+        nobs = 0
+        if inclusive:
+            while j < nvalues and indices[j] <= next_bound:
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+        else:
+            while j < nvalues and indices[j] < next_bound:
+    cdef:
+        Py_ssize_t i, j, nbuckets, nvalues
+        ndarray[double_t, ndim=1] output
+        object next_bound
+        double_t the_sum, val, nobs
+
+    nbuckets = len(buckets)
+    nvalues = len(indices)
+
+    assert(len(values) == len(indices))
+
+    output = np.empty(nbuckets, dtype=float)
+    output.fill(np.NaN)
+
+    j = 0
+    for i from 0 <= i < nbuckets:
+        next_bound = buckets[i]
+        the_sum = 0
+        nobs = 0
+        if inclusive:
+            while j < nvalues and indices[j] <= next_bound:
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+        else:
+            while j < nvalues and indices[j] < next_bound:
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+
+        if nobs > 0:
+            output[i] = the_sum / nobs
+
+        if j >= nvalues:
+            break
+
+    return output
+                val = values[j]
+                # not NaN
+                if val == val:
+                    the_sum += val
+                    nobs += 1
+                j += 1
+
+        if nobs > 0:
+            output[i] = the_sum / nobs
+
+        if j >= nvalues:
+            break
+
+    return output
+
+def ts_upsample_generic(ndarray[object, ndim=1] indices,
+                        ndarray[object, ndim=1] buckets,
+                        ndarray[double_t, ndim=1] values,
+                        object aggfunc,
+                        inclusive=False):
+    '''
+    put something here
+    '''
+    cdef:
+        Py_ssize_t i, j, jstart, nbuckets, nvalues
+        ndarray[double_t, ndim=1] output
+        object next_bound
+        double_t the_sum, val, nobs
+
+    nbuckets = len(buckets)
+    nvalues = len(indices)
+
+    assert(len(values) == len(indices))
+
+    output = np.empty(nbuckets, dtype=float)
+    output.fill(np.NaN)
+
+    j = 0
+    for i from 0 <= i < nbuckets:
+        next_bound = buckets[i]
+        the_sum = 0
+        nobs = 0
+
+        jstart = j
+        if inclusive:
+            while j < nvalues and indices[j] <= next_bound:
+                j += 1
+        else:
+            while j < nvalues and indices[j] < next_bound:
+                j += 1
+
+        if nobs > 0:
+            output[i] = aggfunc(values[jstart:j])
+
+        if j >= nvalues:
+            break
+
+    return output
+
diff --git a/pandas/stats/fama_macbeth.py b/pandas/stats/fama_macbeth.py
index d4f480ba1..50410fb7c 100644
--- a/pandas/stats/fama_macbeth.py
+++ b/pandas/stats/fama_macbeth.py
@@ -1,226 +1,226 @@
-from cStringIO import StringIO
-
-import numpy as np
-
-from pandas.core.api import Series, DataFrame
-import pandas.stats.common as common
-from pandas.util.decorators import cache_readonly
-import pandas.lib.tseries as tseries
-
-def fama_macbeth(**kwargs):
-    """Runs Fama-MacBeth regression.
-
-    Parameters
-    ----------
-    Takes the same arguments as a panel OLS, in addition to:
-
-    nw_lags_beta: int
-       Newey-West adjusts the betas by the given lags
-    """
-    window_type = kwargs.get('window_type')
-    if window_type is None:
-        klass = FamaMacBeth
-    else:
-        klass = MovingFamaMacBeth
-
-    return klass(**kwargs)
-
-class FamaMacBeth(object):
-    def __init__(self, y, x, weights=None, intercept=True, nw_lags=None,
-                 nw_lags_beta=None,
-                 entity_effects=False, time_effects=False, x_effects=None,
-                 cluster=None, dropped_dummies={}, verbose=False):
-        self._nw_lags_beta = nw_lags_beta
-
-        from pandas.stats.plm import MovingPanelOLS
-        self._ols_result = MovingPanelOLS(
-            y=y, x=x, weights=weights, window_type='rolling', window=1,
-            intercept=intercept,
-            nw_lags=nw_lags, entity_effects=entity_effects,
-            time_effects=time_effects, x_effects=x_effects, cluster=cluster,
-            dropped_dummies=dropped_dummies, verbose=verbose)
-
-        self._cols = self._ols_result._x.items
-
-    @cache_readonly
-    def _beta_raw(self):
-        return self._ols_result._beta_raw
-
-    @cache_readonly
-    def _stats(self):
-        return _calc_t_stat(self._beta_raw, self._nw_lags_beta)
-
-    @cache_readonly
-    def _mean_beta_raw(self):
-        return self._stats[0]
-
-    @cache_readonly
-    def _std_beta_raw(self):
-        return self._stats[1]
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        return self._stats[2]
-
-    def _make_result(self, result):
-        return Series(result, index=self._cols)
-
-    @cache_readonly
-    def mean_beta(self):
-        return self._make_result(self._mean_beta_raw)
-
-    @cache_readonly
-    def std_beta(self):
-        return self._make_result(self._std_beta_raw)
-
-    @cache_readonly
-    def t_stat(self):
-        return self._make_result(self._t_stat_raw)
-
-    @cache_readonly
-    def _result_index(self):
-        mask = self._ols_result._rolling_ols_call[1]
-        return self._index[mask]
-
-    @cache_readonly
-    def _results(self):
-        return {
-            'mean_beta' : self._mean_beta_raw,
-            'std_beta' : self._std_beta_raw,
-            't_stat' : self._t_stat_raw,
-        }
-
-    @cache_readonly
-    def _coef_table(self):
-        buffer = StringIO()
-        buffer.write('%13s %13s %13s %13s %13s %13s\n' %
-            ('Variable','Beta', 'Std Err','t-stat','CI 2.5%','CI 97.5%'))
-        template = '%13s %13.4f %13.4f %13.2f %13.4f %13.4f\n'
-
-        for i, name in enumerate(self._cols):
-            if i and not (i % 5):
-                buffer.write('\n' + common.banner(''))
-
-            mean_beta = self._results['mean_beta'][i]
-            std_beta = self._results['std_beta'][i]
-            t_stat = self._results['t_stat'][i]
-            ci1 = mean_beta - 1.96 * std_beta
-            ci2 = mean_beta + 1.96 * std_beta
-
-            values = '(%s)' % name, mean_beta, std_beta, t_stat, ci1, ci2
-
-            buffer.write(template % values)
-
-        if self._nw_lags_beta is not None:
-            buffer.write('\n')
-            buffer.write('*** The Std Err, t-stat are Newey-West '
-                         'adjusted with Lags %5d\n' % self._nw_lags_beta)
-
-        return buffer.getvalue()
-
-    def __repr__(self):
-        return self.summary
-
-    @cache_readonly
-    def summary(self):
-        template = """
-----------------------Summary of Fama-MacBeth Analysis-------------------------
-
-Formula: Y ~ %(formulaRHS)s
-# betas : %(nu)3d
-
-----------------------Summary of Estimated Coefficients------------------------
-%(coefTable)s
---------------------------------End of Summary---------------------------------
-"""
-        params = {
-            'formulaRHS' : ' + '.join(self._cols),
-            'nu' : len(self._beta_raw),
-            'coefTable' : self._coef_table,
-        }
-
-        return template % params
-
-class MovingFamaMacBeth(FamaMacBeth):
-    def __init__(self, y, x, weights=None, window_type='rolling', window=10,
-                 intercept=True, nw_lags=None, nw_lags_beta=None,
-                 entity_effects=False, time_effects=False, x_effects=None,
-                 cluster=None, dropped_dummies={}, verbose=False):
-        self._window_type = common._get_window_type(window_type)
-        self._window = window
-
-        FamaMacBeth.__init__(
-            self, y=y, x=x, weights=weights, intercept=intercept,
-            nw_lags=nw_lags, nw_lags_beta=nw_lags_beta,
-            entity_effects=entity_effects, time_effects=time_effects,
-            x_effects=x_effects, cluster=cluster,
-            dropped_dummies=dropped_dummies, verbose=verbose)
-
-        self._index = self._ols_result._y.major_axis
-        self._T = len(self._index)
-
-    @property
-    def _is_rolling(self):
-        return self._window_type == common.ROLLING
-
-    def _calc_stats(self):
-        mean_betas = []
-        std_betas = []
-        t_stats = []
-
-        # XXX
-
-        mask = self._ols_result._rolling_ols_call[2]
-        obs_total = mask.astype(int).cumsum()
-
-        start = self._window - 1
-        betas = self._beta_raw
-        for i in xrange(start, self._T):
-            if self._is_rolling:
-                begin = i - start
-            else:
-                begin = 0
-
-            B = betas[max(obs_total[begin] - 1, 0) : obs_total[i]]
-            mean_beta, std_beta, t_stat = _calc_t_stat(B, self._nw_lags_beta)
-            mean_betas.append(mean_beta)
-            std_betas.append(std_beta)
-            t_stats.append(t_stat)
-
-        return np.array([mean_betas, std_betas, t_stats])
-
-    _stats = cache_readonly(_calc_stats)
-
-    def _make_result(self, result):
-        return DataFrame(result, index=self._result_index, columns=self._cols)
-
-    @cache_readonly
-    def _result_index(self):
-        mask = self._ols_result._rolling_ols_call[1]
-        return self._index[mask]
-
-    @cache_readonly
-    def _results(self):
-        return {
-            'mean_beta' : self._mean_beta_raw[-1],
-            'std_beta' : self._std_beta_raw[-1],
-            't_stat' : self._t_stat_raw[-1],
-        }
-
-def _calc_t_stat(beta, nw_lags_beta):
-    N = len(beta)
-    B = beta - beta.mean(0)
-    C = np.dot(B.T, B) / N
-
-    if nw_lags_beta is not None:
-        for i in xrange(nw_lags_beta + 1):
-
-            cov = np.dot(B[i:].T, B[:(N - i)]) / N
-            weight = i / (nw_lags_beta + 1)
-            C += 2 * (1 - weight) * cov
-
-    mean_beta = beta.mean(0)
-    std_beta = np.sqrt(np.diag(C)) / np.sqrt(N)
-    t_stat = mean_beta / std_beta
-
-    return mean_beta, std_beta, t_stat
+from cStringIO import StringIO
+
+import numpy as np
+
+from pandas.core.api import Series, DataFrame
+import pandas.stats.common as common
+from pandas.util.decorators import cache_readonly
+import pandas.lib.tseries as tseries
+
+def fama_macbeth(**kwargs):
+    """Runs Fama-MacBeth regression.
+
+    Parameters
+    ----------
+    Takes the same arguments as a panel OLS, in addition to:
+
+    nw_lags_beta: int
+       Newey-West adjusts the betas by the given lags
+    """
+    window_type = kwargs.get('window_type')
+    if window_type is None:
+        klass = FamaMacBeth
+    else:
+        klass = MovingFamaMacBeth
+
+    return klass(**kwargs)
+
+class FamaMacBeth(object):
+    def __init__(self, y, x, weights=None, intercept=True, nw_lags=None,
+                 nw_lags_beta=None,
+                 entity_effects=False, time_effects=False, x_effects=None,
+                 cluster=None, dropped_dummies={}, verbose=False):
+        self._nw_lags_beta = nw_lags_beta
+
+        from pandas.stats.plm import MovingPanelOLS
+        self._ols_result = MovingPanelOLS(
+            y=y, x=x, weights=weights, window_type='rolling', window=1,
+            intercept=intercept,
+            nw_lags=nw_lags, entity_effects=entity_effects,
+            time_effects=time_effects, x_effects=x_effects, cluster=cluster,
+            dropped_dummies=dropped_dummies, verbose=verbose)
+
+        self._cols = self._ols_result._x.items
+
+    @cache_readonly
+    def _beta_raw(self):
+        return self._ols_result._beta_raw
+
+    @cache_readonly
+    def _stats(self):
+        return _calc_t_stat(self._beta_raw, self._nw_lags_beta)
+
+    @cache_readonly
+    def _mean_beta_raw(self):
+        return self._stats[0]
+
+    @cache_readonly
+    def _std_beta_raw(self):
+        return self._stats[1]
+
+    @cache_readonly
+    def _t_stat_raw(self):
+        return self._stats[2]
+
+    def _make_result(self, result):
+        return Series(result, index=self._cols)
+
+    @cache_readonly
+    def mean_beta(self):
+        return self._make_result(self._mean_beta_raw)
+
+    @cache_readonly
+    def std_beta(self):
+        return self._make_result(self._std_beta_raw)
+
+    @cache_readonly
+    def t_stat(self):
+        return self._make_result(self._t_stat_raw)
+
+    @cache_readonly
+    def _result_index(self):
+        mask = self._ols_result._rolling_ols_call[1]
+        return self._index[mask]
+
+    @cache_readonly
+    def _results(self):
+        return {
+            'mean_beta' : self._mean_beta_raw,
+            'std_beta' : self._std_beta_raw,
+            't_stat' : self._t_stat_raw,
+        }
+
+    @cache_readonly
+    def _coef_table(self):
+        buffer = StringIO()
+        buffer.write('%13s %13s %13s %13s %13s %13s\n' %
+            ('Variable','Beta', 'Std Err','t-stat','CI 2.5%','CI 97.5%'))
+        template = '%13s %13.4f %13.4f %13.2f %13.4f %13.4f\n'
+
+        for i, name in enumerate(self._cols):
+            if i and not (i % 5):
+                buffer.write('\n' + common.banner(''))
+
+            mean_beta = self._results['mean_beta'][i]
+            std_beta = self._results['std_beta'][i]
+            t_stat = self._results['t_stat'][i]
+            ci1 = mean_beta - 1.96 * std_beta
+            ci2 = mean_beta + 1.96 * std_beta
+
+            values = '(%s)' % name, mean_beta, std_beta, t_stat, ci1, ci2
+
+            buffer.write(template % values)
+
+        if self._nw_lags_beta is not None:
+            buffer.write('\n')
+            buffer.write('*** The Std Err, t-stat are Newey-West '
+                         'adjusted with Lags %5d\n' % self._nw_lags_beta)
+
+        return buffer.getvalue()
+
+    def __repr__(self):
+        return self.summary
+
+    @cache_readonly
+    def summary(self):
+        template = """
+----------------------Summary of Fama-MacBeth Analysis-------------------------
+
+Formula: Y ~ %(formulaRHS)s
+# betas : %(nu)3d
+
+----------------------Summary of Estimated Coefficients------------------------
+%(coefTable)s
+--------------------------------End of Summary---------------------------------
+"""
+        params = {
+            'formulaRHS' : ' + '.join(self._cols),
+            'nu' : len(self._beta_raw),
+            'coefTable' : self._coef_table,
+        }
+
+        return template % params
+
+class MovingFamaMacBeth(FamaMacBeth):
+    def __init__(self, y, x, weights=None, window_type='rolling', window=10,
+                 intercept=True, nw_lags=None, nw_lags_beta=None,
+                 entity_effects=False, time_effects=False, x_effects=None,
+                 cluster=None, dropped_dummies={}, verbose=False):
+        self._window_type = common._get_window_type(window_type)
+        self._window = window
+
+        FamaMacBeth.__init__(
+            self, y=y, x=x, weights=weights, intercept=intercept,
+            nw_lags=nw_lags, nw_lags_beta=nw_lags_beta,
+            entity_effects=entity_effects, time_effects=time_effects,
+            x_effects=x_effects, cluster=cluster,
+            dropped_dummies=dropped_dummies, verbose=verbose)
+
+        self._index = self._ols_result._y.major_axis
+        self._T = len(self._index)
+
+    @property
+    def _is_rolling(self):
+        return self._window_type == common.ROLLING
+
+    def _calc_stats(self):
+        mean_betas = []
+        std_betas = []
+        t_stats = []
+
+        # XXX
+
+        mask = self._ols_result._rolling_ols_call[2]
+        obs_total = mask.astype(int).cumsum()
+
+        start = self._window - 1
+        betas = self._beta_raw
+        for i in xrange(start, self._T):
+            if self._is_rolling:
+                begin = i - start
+            else:
+                begin = 0
+
+            B = betas[max(obs_total[begin] - 1, 0) : obs_total[i]]
+            mean_beta, std_beta, t_stat = _calc_t_stat(B, self._nw_lags_beta)
+            mean_betas.append(mean_beta)
+            std_betas.append(std_beta)
+            t_stats.append(t_stat)
+
+        return np.array([mean_betas, std_betas, t_stats])
+
+    _stats = cache_readonly(_calc_stats)
+
+    def _make_result(self, result):
+        return DataFrame(result, index=self._result_index, columns=self._cols)
+
+    @cache_readonly
+    def _result_index(self):
+        mask = self._ols_result._rolling_ols_call[1]
+        return self._index[mask]
+
+    @cache_readonly
+    def _results(self):
+        return {
+            'mean_beta' : self._mean_beta_raw[-1],
+            'std_beta' : self._std_beta_raw[-1],
+            't_stat' : self._t_stat_raw[-1],
+        }
+
+def _calc_t_stat(beta, nw_lags_beta):
+    N = len(beta)
+    B = beta - beta.mean(0)
+    C = np.dot(B.T, B) / N
+
+    if nw_lags_beta is not None:
+        for i in xrange(nw_lags_beta + 1):
+
+            cov = np.dot(B[i:].T, B[:(N - i)]) / N
+            weight = i / (nw_lags_beta + 1)
+            C += 2 * (1 - weight) * cov
+
+    mean_beta = beta.mean(0)
+    std_beta = np.sqrt(np.diag(C)) / np.sqrt(N)
+    t_stat = mean_beta / std_beta
+
+    return mean_beta, std_beta, t_stat
diff --git a/pandas/stats/ols.py b/pandas/stats/ols.py
index 22a34bdbe..10d12d23d 100644
--- a/pandas/stats/ols.py
+++ b/pandas/stats/ols.py
@@ -1,1222 +1,1222 @@
-"""
-Ordinary least squares regression
-"""
-
-# pylint: disable-msg=W0201
-
-from itertools import izip, starmap
-from StringIO import StringIO
-
-import numpy as np
-
-from pandas.core.api import DataFrame, Series
-from pandas.core.panel import WidePanel
-from pandas.util.decorators import cache_readonly
-import pandas.stats.common as common
-import pandas.stats.math as math
-import pandas.stats.moments as moments
-
-_FP_ERR = 1e-13
-
-class OLS(object):
-    """
-    Runs a full sample ordinary least squares regression
-
-    Parameters
-    ----------
-    y: Series
-    x: Series, DataFrame, or dict of Series
-    intercept: bool
-        True if you want an intercept.
-    nw_lags: None or int
-        Number of Newey-West lags.
-    """
-    def __init__(self, y, x, intercept=True, nw_lags=None, nw_overlap=False):
-        try:
-            import scikits.statsmodels.api as sm
-        except ImportError: # pragma: no cover
-            import scikits.statsmodels as sm
-
-        self._x_orig = x
-        self._y_orig = y
-        self._intercept = intercept
-        self._nw_lags = nw_lags
-        self._nw_overlap = nw_overlap
-
-        (self._y, self._x, self._x_filtered,
-         self._index, self._time_has_obs) = self._prepare_data()
-
-        # for compat with PanelOLS
-        self._x_trans = self._x
-        self._y_trans = self._y
-
-        self._x_raw = self._x.values
-        self._y_raw = self._y.view(np.ndarray)
-
-        self.sm_ols = sm.OLS(self._y_raw, self._x.values).fit()
-
-    def _prepare_data(self):
-        """
-        Filters the data and sets up an intercept if necessary.
-
-        Returns
-        -------
-        (DataFrame, Series).
-        """
-        (y, x, x_filtered,
-         union_index, valid) = _filter_data(self._y_orig, self._x_orig)
-
-        if self._intercept:
-            x['intercept'] = x_filtered['intercept'] = 1.
-
-        return y, x, x_filtered, union_index, valid
-
-    @property
-    def nobs(self):
-        return self._nobs
-
-    @property
-    def _nobs(self):
-        return len(self._y_raw)
-
-    @property
-    def nw_lags(self):
-        return self._nw_lags
-
-    @property
-    def x(self):
-        """Returns the filtered x used in the regression."""
-        return self._x
-
-    @property
-    def y(self):
-        """Returns the filtered y used in the regression."""
-        return self._y
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        return self.sm_ols.params
-
-    @cache_readonly
-    def beta(self):
-        """Returns the betas in Series form."""
-        return Series(self._beta_raw, index=self._x.columns)
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        return math.rank(self._x.values)
-
-    @cache_readonly
-    def df(self):
-        """Returns the degrees of freedom.
-
-        This equals the rank of the X matrix.
-        """
-        return self._df_raw
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self.sm_ols.df_model
-
-    @cache_readonly
-    def df_model(self):
-        """Returns the degrees of freedom of the model."""
-        return self._df_model_raw
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self.sm_ols.df_resid
-
-    @cache_readonly
-    def df_resid(self):
-        """Returns the degrees of freedom of the residuals."""
-        return self._df_resid_raw
-
-    @cache_readonly
-    def _f_stat_raw(self):
-        """Returns the raw f-stat value."""
-        from scipy.stats import f
-
-        cols = self._x.columns
-
-        if self._nw_lags is None:
-            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
-
-            q = len(cols)
-            if 'intercept' in cols:
-                q -= 1
-
-            shape = q, self.df_resid
-            p_value = 1 - f.cdf(F, shape[0], shape[1])
-            return F, shape, p_value
-
-        k = len(cols)
-        R = np.eye(k)
-        r = np.zeros((k, 1))
-
-        intercept = cols.indexMap.get('intercept')
-
-        if intercept is not None:
-            R = np.concatenate((R[0 : intercept], R[intercept + 1:]))
-            r = np.concatenate((r[0 : intercept], r[intercept + 1:]))
-
-        return math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
-                           self._nobs, self.df)
-
-    @cache_readonly
-    def f_stat(self):
-        """Returns the f-stat value."""
-        return common.f_stat_to_dict(self._f_stat_raw)
-
-    def f_test(self, hypothesis):
-        """Runs the F test, given a joint hypothesis.  The hypothesis is
-        represented by a collection of equations, in the form
-
-        A*x_1+B*x_2=C
-
-        You must provide the coefficients even if they're 1.  No spaces.
-
-        The equations can be passed as either a single string or a
-        list of strings.
-
-        Examples
-        --------
-        o = ols(...)
-        o.f_test('1*x1+2*x2=0,1*x3=0')
-        o.f_test(['1*x1+2*x2=0','1*x3=0'])
-        """
-
-        x_names = self._x.columns
-
-        R = []
-        r = []
-
-        if isinstance(hypothesis, str):
-            eqs = hypothesis.split(',')
-        elif isinstance(hypothesis, list):
-            eqs = hypothesis
-        else: # pragma: no cover
-            raise Exception('hypothesis must be either string or list')
-        for equation in eqs:
-            row = np.zeros(len(x_names))
-            lhs, rhs = equation.split('=')
-            for s in lhs.split('+'):
-                ss = s.split('*')
-                coeff = float(ss[0])
-                x_name = ss[1]
-
-                if x_name not in x_names:
-                    raise Exception('no coefficient named %s' % x_name)
-                idx = x_names.indexMap[x_name]
-                row[idx] = coeff
-            rhs = float(rhs)
-
-            R.append(row)
-            r.append(rhs)
-
-        R = np.array(R)
-        q = len(r)
-        r = np.array(r).reshape(q, 1)
-
-        result = math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
-                             self._nobs, self.df)
-
-        return common.f_stat_to_dict(result)
-
-    @cache_readonly
-    def _p_value_raw(self):
-        """Returns the raw p values."""
-        from scipy.stats import t
-
-        return 2 * t.sf(np.fabs(self._t_stat_raw),
-                        self._df_resid_raw)
-
-    @cache_readonly
-    def p_value(self):
-        """Returns the p values."""
-        return Series(self._p_value_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _r2_raw(self):
-        """Returns the raw r-squared values."""
-        has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
-
-        if self._intercept:
-            return 1 - self.sm_ols.ssr / self.sm_ols.centered_tss
-        else:
-            return 1 - self.sm_ols.ssr / self.sm_ols.uncentered_tss
-
-    @cache_readonly
-    def r2(self):
-        """Returns the r-squared values."""
-        return self._r2_raw
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        return self.sm_ols.rsquared_adj
-
-    @cache_readonly
-    def r2_adj(self):
-        """Returns the r-squared adjusted values."""
-        return self._r2_adj_raw
-
-    @cache_readonly
-    def _resid_raw(self):
-        """Returns the raw residuals."""
-        return self.sm_ols.resid
-
-    @cache_readonly
-    def resid(self):
-        """Returns the residuals."""
-        return Series(self._resid_raw, index=self._x.index)
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        return np.sqrt(self.sm_ols.mse_resid)
-
-    @cache_readonly
-    def rmse(self):
-        """Returns the rmse value."""
-        return self._rmse_raw
-
-    @cache_readonly
-    def _std_err_raw(self):
-        """Returns the raw standard err values."""
-        return np.sqrt(np.diag(self._var_beta_raw))
-
-    @cache_readonly
-    def std_err(self):
-        """Returns the standard err values of the betas."""
-        return Series(self._std_err_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        """Returns the raw t-stat value."""
-        return self._beta_raw / self._std_err_raw
-
-    @cache_readonly
-    def t_stat(self):
-        """Returns the t-stat values of the betas."""
-        return Series(self._t_stat_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """
-        Returns the raw covariance of beta.
-        """
-        x = self._x.values
-        y = self._y_raw
-
-        xx = np.dot(x.T, x)
-
-        if self._nw_lags is None:
-            return math.inv(xx) * (self._rmse_raw ** 2)
-        else:
-            resid = y - np.dot(x, self._beta_raw)
-            m = (x.T * resid).T
-
-            xeps = math.newey_west(m, self._nw_lags, self._nobs, self._df_raw,
-                                   self._nw_overlap)
-
-            xx_inv = math.inv(xx)
-            return np.dot(xx_inv, np.dot(xeps, xx_inv))
-
-    @cache_readonly
-    def var_beta(self):
-        """Returns the variance-covariance matrix of beta."""
-        return DataFrame(self._var_beta_raw, index=self.beta.index,
-                         columns=self.beta.index)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        return self.sm_ols.fittedvalues
-
-    @cache_readonly
-    def y_fitted(self):
-        """Returns the fitted y values.  This equals BX."""
-        result = Series(self._y_fitted_raw, index=self._y.index)
-        return result.reindex(self._y_orig.index)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        return self._y_fitted_raw
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values.
-
-        For in-sample, this is same as y_fitted."""
-        return self.y_fitted
-
-    RESULT_FIELDS = ['r2', 'r2_adj', 'df', 'df_model', 'df_resid', 'rmse',
-                     'f_stat', 'beta', 'std_err', 't_stat', 'p_value', 'nobs']
-
-    @cache_readonly
-    def _results(self):
-        results = {}
-        for result in self.RESULT_FIELDS:
-            results[result] = getattr(self, result)
-
-        return results
-
-    @cache_readonly
-    def _coef_table(self):
-        buf = StringIO()
-
-        buf.write('%14s %10s %10s %10s %10s %10s %10s\n' %
-                  ('Variable', 'Coef', 'Std Err', 't-stat',
-                   'p-value', 'CI 2.5%', 'CI 97.5%'))
-        buf.write(common.banner(''))
-        coef_template = '\n%14s %10.4f %10.4f %10.2f %10.4f %10.4f %10.4f'
-
-        results = self._results
-
-        beta = results['beta']
-
-        for i, name in enumerate(beta.index):
-            if i and not (i % 5):
-                buf.write('\n' + common.banner(''))
-
-            std_err = results['std_err'][name]
-            CI1 = beta[name] - 1.96 * std_err
-            CI2 = beta[name] + 1.96 * std_err
-
-            t_stat = results['t_stat'][name]
-            p_value = results['p_value'][name]
-
-            line = coef_template % (name,
-                beta[name], std_err, t_stat, p_value, CI1, CI2)
-
-            buf.write(line)
-
-        if self.nw_lags is not None:
-            buf.write('\n')
-            buf.write('*** The calculations are Newey-West '
-                      'adjusted with lags %5d\n' % self.nw_lags)
-
-        return buf.getvalue()
-
-    @cache_readonly
-    def summary_as_matrix(self):
-        """Returns the formatted results of the OLS as a DataFrame."""
-        results = self._results
-        beta = results['beta']
-        data = {'beta' : results['beta'],
-                't-stat' : results['t_stat'],
-                'p-value' : results['p_value'],
-                'std err' : results['std_err']}
-        return DataFrame(data, beta.index).T
-
-    @cache_readonly
-    def summary(self):
-        """
-        This returns the formatted result of the OLS computation
-        """
-        template = """
-%(bannerTop)s
-
-Formula: Y ~ %(formula)s
-
-Number of Observations:         %(nobs)d
-Number of Degrees of Freedom:   %(df)d
-
-R-squared:     %(r2)10.4f
-Adj R-squared: %(r2_adj)10.4f
-
-Rmse:          %(rmse)10.4f
-
-F-stat %(f_stat_shape)s: %(f_stat)10.4f, p-value: %(f_stat_p_value)10.4f
-
-Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
-
-%(bannerCoef)s
-%(coef_table)s
-%(bannerEnd)s
-"""
-        coef_table = self._coef_table
-
-        results = self._results
-
-        f_stat = results['f_stat']
-
-        bracketed = ['<%s>' % c for c in results['beta'].index]
-
-        formula = StringIO()
-        formula.write(bracketed[0])
-        tot = len(bracketed[0])
-        line = 1
-        for coef in bracketed[1:]:
-            tot = tot + len(coef) + 3
-
-            if tot // (68 * line):
-                formula.write('\n' + ' ' * 12)
-                line += 1
-
-            formula.write(' + ' + coef)
-
-        params = {
-            'bannerTop' : common.banner('Summary of Regression Analysis'),
-            'bannerCoef' : common.banner('Summary of Estimated Coefficients'),
-            'bannerEnd' : common.banner('End of Summary'),
-            'formula' : formula.getvalue(),
-            'r2' : results['r2'],
-            'r2_adj' : results['r2_adj'],
-            'nobs' : results['nobs'],
-            'df'  : results['df'],
-            'df_model'  : results['df_model'],
-            'df_resid'  : results['df_resid'],
-            'coef_table' : coef_table,
-            'rmse' : results['rmse'],
-            'f_stat' : f_stat['f-stat'],
-            'f_stat_shape' : '(%d, %d)' % (f_stat['DF X'], f_stat['DF Resid']),
-            'f_stat_p_value' : f_stat['p-value'],
-        }
-
-        return template % params
-
-    def __repr__(self):
-        return self.summary
-
-
-    @cache_readonly
-    def _time_obs_count(self):
-        # XXX
-        return self._time_has_obs.astype(int)
-
-    @property
-    def _total_times(self):
-        return self._time_has_obs.sum()
-
-
-class MovingOLS(OLS):
-    """
-    Runs a rolling/expanding simple OLS.
-
-    Parameters
-    ----------
-    y: Series
-    x: Series, DataFrame, or dict of Series
-    intercept: bool
-        True if you want an intercept.
-    nw_lags: None or int
-        Number of Newey-West lags.
-    window_type: int
-        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
-    window: int
-        size of window (for rolling/expanding OLS)
-    """
-    def __init__(self, y, x, window_type='expanding',
-                 window=None, min_periods=None, intercept=True,
-                 nw_lags=None, nw_overlap=False):
-
-        self._args = dict(intercept=intercept, nw_lags=nw_lags,
-                          nw_overlap=nw_overlap)
-
-        OLS.__init__(self, y=y, x=x, **self._args)
-
-        self._set_window(window_type, window, min_periods)
-
-    def _set_window(self, window_type, window, min_periods):
-        self._window_type = common._get_window_type(window_type)
-
-        if self._is_rolling:
-            if window is None:
-                raise Exception('Must pass window when doing rolling '
-                                'regression')
-
-            if min_periods is None:
-                min_periods = window
-        else:
-            window = len(self._x)
-            if min_periods is None:
-                min_periods = 1
-
-        self._window = int(window)
-        self._min_periods = min_periods
-
-#-------------------------------------------------------------------------------
-# "Public" results
-
-    @cache_readonly
-    def beta(self):
-        """Returns the betas in Series/DataFrame form."""
-        return DataFrame(self._beta_raw,
-                         index=self._result_index,
-                         columns=self._x.columns)
-
-    @cache_readonly
-    def rank(self):
-        return Series(self._rank_raw, index=self._result_index)
-
-    @cache_readonly
-    def df(self):
-        """Returns the degrees of freedom."""
-        return Series(self._df_raw, index=self._result_index)
-
-    @cache_readonly
-    def df_model(self):
-        """Returns the model degrees of freedom."""
-        return Series(self._df_model_raw, index=self._result_index)
-
-    @cache_readonly
-    def df_resid(self):
-        """Returns the residual degrees of freedom."""
-        return Series(self._df_resid_raw, index=self._result_index)
-
-    @cache_readonly
-    def f_stat(self):
-        """Returns the f-stat value."""
-        f_stat_dicts = dict((date, common.f_stat_to_dict(f_stat))
-                            for date, f_stat in zip(self.beta.index,
-                                                    self._f_stat_raw))
-
-        return DataFrame(f_stat_dicts).T
-
-    def f_test(self, hypothesis):
-        raise Exception('f_test not supported for rolling/expanding OLS')
-
-    @cache_readonly
-    def forecast_mean(self):
-        return Series(self._forecast_mean_raw, index=self._result_index)
-
-    @cache_readonly
-    def forecast_vol(self):
-        return Series(self._forecast_vol_raw, index=self._result_index)
-
-    @cache_readonly
-    def p_value(self):
-        """Returns the p values."""
-        cols = self.beta.columns
-        return DataFrame(self._p_value_raw, columns=cols,
-                         index=self._result_index)
-
-    @cache_readonly
-    def r2(self):
-        """Returns the r-squared values."""
-        return Series(self._r2_raw, index=self._result_index)
-
-    @cache_readonly
-    def resid(self):
-        """Returns the residuals."""
-        return Series(self._resid_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-    @cache_readonly
-    def r2_adj(self):
-        """Returns the r-squared adjusted values."""
-        index = self.r2.index
-
-        return Series(self._r2_adj_raw, index=index)
-
-    @cache_readonly
-    def rmse(self):
-        """Returns the rmse values."""
-        return Series(self._rmse_raw, index=self._result_index)
-
-    @cache_readonly
-    def std_err(self):
-        """Returns the standard err values."""
-        return DataFrame(self._std_err_raw, columns=self.beta.columns,
-                         index=self._result_index)
-
-    @cache_readonly
-    def t_stat(self):
-        """Returns the t-stat value."""
-        return DataFrame(self._t_stat_raw, columns=self.beta.columns,
-                         index=self._result_index)
-
-    @cache_readonly
-    def var_beta(self):
-        """Returns the covariance of beta."""
-        result = {}
-        result_index = self._result_index
-        for i in xrange(len(self._var_beta_raw)):
-            dm = DataFrame(self._var_beta_raw[i], columns=self.beta.columns,
-                           index=self.beta.columns)
-            result[result_index[i]] = dm
-
-        return WidePanel.fromDict(result, intersect=False)
-
-    @cache_readonly
-    def y_fitted(self):
-        """Returns the fitted y values."""
-        return Series(self._y_fitted_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values."""
-        return Series(self._y_predict_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-#-------------------------------------------------------------------------------
-# "raw" attributes, calculations
-
-    @property
-    def _is_rolling(self):
-        return self._window_type == common.ROLLING
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        beta, indices, mask = self._rolling_ols_call
-
-        return beta[indices]
-
-    @cache_readonly
-    def _result_index(self):
-        return self._index[self._valid_indices]
-
-    @property
-    def _valid_indices(self):
-        return self._rolling_ols_call[1]
-
-    @cache_readonly
-    def _rolling_ols_call(self):
-        return self._calc_betas(self._x_trans, self._y_trans)
-
-    def _calc_betas(self, x, y):
-        N = len(self._index)
-        K = len(self._x.columns)
-
-        betas = np.empty((N, K), dtype=float)
-        betas[:] = np.NaN
-
-        valid = self._time_has_obs
-        enough = self._enough_obs
-        window = self._window
-
-        # Use transformed (demeaned) Y, X variables
-        cum_xx = self._cum_xx(x)
-        cum_xy = self._cum_xy(x, y)
-
-        for i in xrange(N):
-            if not valid[i] or not enough[i]:
-                continue
-
-            xx = cum_xx[i]
-            xy = cum_xy[i]
-            if self._is_rolling and i >= window:
-                xx = xx - cum_xx[i - window]
-                xy = xy - cum_xy[i - window]
-
-            betas[i] = math.solve(xx, xy)
-
-        mask = -np.isnan(betas).any(axis=1)
-        have_betas = np.arange(N)[mask]
-
-        return betas, have_betas, mask
-
-    def _rolling_rank(self):
-        dates = self._index
-        window = self._window
-
-        ranks = np.empty(len(dates), dtype=float)
-        ranks[:] = np.NaN
-        for i, date in enumerate(dates):
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = self._x.truncate(before=prior_date, after=date).values
-
-            if len(x_slice) == 0:
-                continue
-
-            ranks[i] = math.rank(x_slice)
-
-        return ranks
-
-    def _cum_xx(self, x):
-        dates = self._index
-        K = len(x.columns)
-        valid = self._time_has_obs
-        cum_xx = []
-
-        if isinstance(x, DataFrame):
-            _indexMap = x.index.indexMap
-            def slicer(df, dt):
-                i = _indexMap[dt]
-                return df.values[i:i+1, :]
-        else:
-            slicer = lambda df, dt: df.truncate(dt, dt).values
-
-        last = np.zeros((K, K))
-        for i, date in enumerate(dates):
-            if not valid[i]:
-                cum_xx.append(last)
-                continue
-
-            x_slice = slicer(x, date)
-            xx = last = last + np.dot(x_slice.T, x_slice)
-            cum_xx.append(xx)
-
-        return cum_xx
-
-    def _cum_xy(self, x, y):
-        dates = self._index
-        valid = self._time_has_obs
-        cum_xy = []
-
-        if isinstance(x, DataFrame):
-            _x_indexMap = x.index.indexMap
-            def x_slicer(df, dt):
-                i = _x_indexMap[dt]
-                return df.values[i:i+1, :]
-        else:
-            x_slicer = lambda df, dt: df.truncate(dt, dt).values
-
-
-        if isinstance(y, Series):
-            _y_indexMap = y.index.indexMap
-            _values = y.values
-            def y_slicer(df, dt):
-                i = _y_indexMap[dt]
-                return _values[i:i+1]
-        else:
-            y_slicer = lambda s, dt: _y_converter(s.truncate(dt, dt))
-
-        last = np.zeros(len(x.columns))
-        for i, date in enumerate(dates):
-            if not valid[i]:
-                cum_xy.append(last)
-                continue
-
-            x_slice = x_slicer(x, date)
-            y_slice = y_slicer(y, date)
-
-            xy = last = last + np.dot(x_slice.T, y_slice)
-            cum_xy.append(xy)
-
-        return cum_xy
-
-    @cache_readonly
-    def _rank_raw(self):
-        rank = self._rolling_rank()
-        return rank[self._valid_indices]
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        return self._rank_raw
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self._df_raw - 1
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self._nobs - self._df_raw
-
-    @cache_readonly
-    def _f_stat_raw(self):
-        """Returns the raw f-stat value."""
-        from scipy.stats import f
-
-        items = self.beta.columns
-        nobs = self._nobs
-        df = self._df_raw
-        df_resid = nobs - df
-
-        # var_beta has not been newey-west adjusted
-        if self._nw_lags is None:
-            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
-
-            q = len(items)
-            if 'intercept' in items:
-                q -= 1
-
-            def get_result_simple(Fst, d):
-                return Fst, (q, d), 1 - f.cdf(Fst, q, d)
-
-            # Compute the P-value for each pair
-            result = starmap(get_result_simple, izip(F, df_resid))
-
-            return list(result)
-
-        K = len(items)
-        R = np.eye(K)
-        r = np.zeros((K, 1))
-
-        intercept = items.indexMap.get('intercept')
-
-        if intercept is not None:
-            R = np.concatenate((R[0 : intercept], R[intercept + 1:]))
-            r = np.concatenate((r[0 : intercept], r[intercept + 1:]))
-
-        def get_result(beta, vcov, n, d):
-            return math.calc_F(R, r, beta, vcov, n, d)
-
-        results = starmap(get_result,
-                          izip(self._beta_raw, self._var_beta_raw, nobs, df))
-
-        return list(results)
-
-    @cache_readonly
-    def _p_value_raw(self):
-        """Returns the raw p values."""
-        from scipy.stats import t
-
-        result = [2 * t.sf(a, b)
-                  for a, b in izip(np.fabs(self._t_stat_raw),
-                                   self._df_resid_raw)]
-
-        return np.array(result)
-
-    @cache_readonly
-    def _resid_stats(self):
-        uncentered_sst = []
-        sst = []
-        sse = []
-
-        Y = self._y
-        X = self._x
-
-        dates = self._index
-        window = self._window
-        for n, index in enumerate(self._valid_indices):
-            if self._is_rolling and index >= window:
-                prior_date = dates[index - window + 1]
-            else:
-                prior_date = dates[0]
-
-            date = dates[index]
-            beta = self._beta_raw[n]
-
-            X_slice = X.truncate(before=prior_date, after=date).values
-            Y_slice = _y_converter(Y.truncate(before=prior_date, after=date))
-
-            resid = Y_slice - np.dot(X_slice, beta)
-
-            SS_err = (resid ** 2).sum()
-            SS_total = ((Y_slice - Y_slice.mean()) ** 2).sum()
-            SST_uncentered = (Y_slice ** 2).sum()
-
-            sse.append(SS_err)
-            sst.append(SS_total)
-            uncentered_sst.append(SST_uncentered)
-
-        return {
-            'sse' : np.array(sse),
-            'centered_tss' : np.array(sst),
-            'uncentered_tss' : np.array(uncentered_sst),
-        }
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        return np.sqrt(self._resid_stats['sse'] / self._df_resid_raw)
-
-    @cache_readonly
-    def _r2_raw(self):
-        rs = self._resid_stats
-
-        if self._intercept:
-            return 1 - rs['sse'] / rs['centered_tss']
-        else:
-            return 1 - rs['sse'] / rs['uncentered_tss']
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        nobs = self._nobs
-        factors = (nobs - 1) / (nobs - self._df_raw)
-        return 1 - (1 - self._r2_raw) * factors
-
-    @cache_readonly
-    def _resid_raw(self):
-        """Returns the raw residuals."""
-        return (self._y_raw - self._y_fitted_raw)
-
-    @cache_readonly
-    def _std_err_raw(self):
-        """Returns the raw standard err values."""
-        results = []
-        for i in xrange(len(self._var_beta_raw)):
-            results.append(np.sqrt(np.diag(self._var_beta_raw[i])))
-
-        return np.array(results)
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        """Returns the raw t-stat value."""
-        return self._beta_raw / self._std_err_raw
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """Returns the raw covariance of beta."""
-        x = self._x
-        y = self._y
-        dates = self._index
-        nobs = self._nobs
-        rmse = self._rmse_raw
-        beta = self._beta_raw
-        df = self._df_raw
-        window = self._window
-        cum_xx = self._cum_xx(self._x)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            xx = cum_xx[i]
-            date = dates[i]
-
-            if self._is_rolling and i >= window:
-                xx = xx - cum_xx[i - window]
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = x.truncate(before=prior_date, after=date)
-            y_slice = y.truncate(before=prior_date, after=date)
-            xv = x_slice.values
-            yv = np.asarray(y_slice)
-
-            if self._nw_lags is None:
-                result = math.inv(xx) * (rmse[n] ** 2)
-            else:
-                resid = yv - np.dot(xv, beta[n])
-                m = (xv.T * resid).T
-
-                xeps = math.newey_west(m, self._nw_lags, nobs[n], df[n],
-                                       self._nw_overlap)
-
-                xx_inv = math.inv(xx)
-                result = np.dot(xx_inv, np.dot(xeps, xx_inv))
-
-            results.append(result)
-
-        return np.array(results)
-
-    @cache_readonly
-    def _forecast_mean_raw(self):
-        """Returns the raw covariance of beta."""
-        nobs = self._nobs
-        window = self._window
-
-        # x should be ones
-        dummy = DataFrame(index=self._y.index)
-        dummy['y'] = 1
-
-        cum_xy = self._cum_xy(dummy, self._y)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            sumy = cum_xy[i]
-
-            if self._is_rolling and i >= window:
-                sumy = sumy - cum_xy[i - window]
-
-            results.append(sumy[0] / nobs[n])
-
-        return np.array(results)
-
-    @cache_readonly
-    def _forecast_vol_raw(self):
-        """Returns the raw covariance of beta."""
-        beta = self._beta_raw
-        window = self._window
-        dates = self._index
-        x = self._x
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            date = dates[i]
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = x.truncate(prior_date, date).values
-            x_demeaned = x_slice - x_slice.mean(0)
-            x_cov = np.dot(x_demeaned.T, x_demeaned) / (len(x_slice) - 1)
-
-            B = beta[n]
-            result = np.dot(B, np.dot(x_cov, B))
-            results.append(np.sqrt(result))
-
-        return np.array(results)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        return (self._x.values * self._beta_matrix(lag=0)).sum(1)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        return (self._x.values * self._beta_matrix(lag=1)).sum(1)
-
-    @cache_readonly
-    def _results(self):
-        results = {}
-        for result in self.RESULT_FIELDS:
-            value = getattr(self, result)
-            if isinstance(value, np.ndarray):
-                value = value[-1]
-            elif isinstance(value, Series):
-                value = value[self.beta.index[-1]]
-            elif isinstance(value, DataFrame):
-                value = value.xs(self.beta.index[-1])
-            else:
-                raise Exception('Problem retrieving %s' % result)
-            results[result] = value
-
-        return results
-
-    @cache_readonly
-    def _window_time_obs(self):
-        window_obs = moments.rolling_sum(self._time_obs_count > 0,
-                                         self._window, min_periods=1)
-
-        window_obs[np.isnan(window_obs)] = 0
-        return window_obs.astype(int)
-
-    @cache_readonly
-    def _nobs_raw(self):
-        if self._is_rolling:
-            window = self._window
-        else:
-            # expanding case
-            window = len(self._index)
-
-        result = moments.rolling_sum(self._time_obs_count, window,
-                                     min_periods=1)
-
-        return result.astype(int)
-
-    def _beta_matrix(self, lag=0):
-        assert(lag >= 0)
-
-        labels = np.arange(len(self._y)) - lag
-        indexer = self._valid_obs_labels.searchsorted(labels, side='left')
-
-        beta_matrix = self._beta_raw[indexer]
-        beta_matrix[labels < self._valid_obs_labels[0]] = np.NaN
-
-        return beta_matrix
-
-    @cache_readonly
-    def _valid_obs_labels(self):
-        dates = self._index[self._valid_indices]
-        return self._y.index.searchsorted(dates)
-
-    @cache_readonly
-    def _nobs(self):
-        return self._nobs_raw[self._valid_indices]
-
-    @property
-    def nobs(self):
-        return Series(self._nobs, index=self._result_index)
-
-    @cache_readonly
-    def _enough_obs(self):
-        # XXX: what's the best way to determine where to start?
-        return self._nobs_raw >= max(self._min_periods,
-                                     len(self._x.columns) + 1)
-
-def _safe_update(d, other):
-    """
-    Combine dictionaries with non-overlapping keys
-    """
-    for k, v in other.iteritems():
-        if k in d:
-            raise Exception('Duplicate regressor: %s' % k)
-
-        d[k] = v
-
-def _combine_rhs(rhs):
-    """
-    Glue input X variables together while checking for potential
-    duplicates
-    """
-    series = {}
-
-    if isinstance(rhs, Series):
-        series['x'] = rhs
-    elif isinstance(rhs, DataFrame):
-        series = rhs.copy()
-    elif isinstance(rhs, dict):
-        for name, value in rhs.iteritems():
-            if isinstance(value, Series):
-                _safe_update(series, {name : value})
-            elif isinstance(value, (dict, DataFrame)):
-                _safe_update(series, value)
-            else:
-                raise Exception('Invalid RHS data type: %s' % type(value))
-    else:
-        raise Exception('Invalid RHS type: %s' % type(rhs))
-
-    if not isinstance(series, DataFrame):
-        series = DataFrame(series)
-
-    return series
-
-def _filter_data(lhs, rhs):
-    """
-    Cleans the input for single OLS.
-
-    Parameters
-    ----------
-    lhs: Series
-        Dependent variable in the regression.
-    rhs: dict, whose values are Series, DataFrame, or dict
-        Explanatory variables of the regression.
-
-    Returns
-    -------
-    Series, DataFrame
-        Cleaned lhs and rhs
-    """
-    if not isinstance(lhs, Series):
-        raise Exception('lhs must be a Series')
-
-    rhs = _combine_rhs(rhs)
-
-    rhs_valid = np.isfinite(rhs.values).sum(1) == len(rhs.columns)
-
-    if not rhs_valid.all():
-        pre_filtered_rhs = rhs[rhs_valid]
-    else:
-        pre_filtered_rhs = rhs
-
-    index = lhs.index + rhs.index
-    if not index.equals(rhs.index) or not index.equals(lhs.index):
-        rhs = rhs.reindex(index)
-        lhs = lhs.reindex(index)
-
-        rhs_valid = np.isfinite(rhs.values).sum(1) == len(rhs.columns)
-
-    lhs_valid = np.isfinite(lhs.values)
-    valid = rhs_valid & lhs_valid
-
-    if not valid.all():
-        filt_index = rhs.index[valid]
-        filtered_rhs = rhs.reindex(filt_index)
-        filtered_lhs = lhs.reindex(filt_index)
-    else:
-        filtered_rhs, filtered_lhs = rhs, lhs
-
-    return filtered_lhs, filtered_rhs, pre_filtered_rhs, index, valid
-
-# A little kludge so we can use this method for both
-# MovingOLS and MovingPanelOLS
-def _y_converter(y):
-    if isinstance(y, Series):
-        return np.asarray(y)
-    else:
-        y = y.values.squeeze()
-        if y.ndim == 0:
-            return np.array([y])
-        else:
-            return y
+"""
+Ordinary least squares regression
+"""
+
+# pylint: disable-msg=W0201
+
+from itertools import izip, starmap
+from StringIO import StringIO
+
+import numpy as np
+
+from pandas.core.api import DataFrame, Series
+from pandas.core.panel import WidePanel
+from pandas.util.decorators import cache_readonly
+import pandas.stats.common as common
+import pandas.stats.math as math
+import pandas.stats.moments as moments
+
+_FP_ERR = 1e-13
+
+class OLS(object):
+    """
+    Runs a full sample ordinary least squares regression
+
+    Parameters
+    ----------
+    y: Series
+    x: Series, DataFrame, or dict of Series
+    intercept: bool
+        True if you want an intercept.
+    nw_lags: None or int
+        Number of Newey-West lags.
+    """
+    def __init__(self, y, x, intercept=True, nw_lags=None, nw_overlap=False):
+        try:
+            import scikits.statsmodels.api as sm
+        except ImportError: # pragma: no cover
+            import scikits.statsmodels as sm
+
+        self._x_orig = x
+        self._y_orig = y
+        self._intercept = intercept
+        self._nw_lags = nw_lags
+        self._nw_overlap = nw_overlap
+
+        (self._y, self._x, self._x_filtered,
+         self._index, self._time_has_obs) = self._prepare_data()
+
+        # for compat with PanelOLS
+        self._x_trans = self._x
+        self._y_trans = self._y
+
+        self._x_raw = self._x.values
+        self._y_raw = self._y.view(np.ndarray)
+
+        self.sm_ols = sm.OLS(self._y_raw, self._x.values).fit()
+
+    def _prepare_data(self):
+        """
+        Filters the data and sets up an intercept if necessary.
+
+        Returns
+        -------
+        (DataFrame, Series).
+        """
+        (y, x, x_filtered,
+         union_index, valid) = _filter_data(self._y_orig, self._x_orig)
+
+        if self._intercept:
+            x['intercept'] = x_filtered['intercept'] = 1.
+
+        return y, x, x_filtered, union_index, valid
+
+    @property
+    def nobs(self):
+        return self._nobs
+
+    @property
+    def _nobs(self):
+        return len(self._y_raw)
+
+    @property
+    def nw_lags(self):
+        return self._nw_lags
+
+    @property
+    def x(self):
+        """Returns the filtered x used in the regression."""
+        return self._x
+
+    @property
+    def y(self):
+        """Returns the filtered y used in the regression."""
+        return self._y
+
+    @cache_readonly
+    def _beta_raw(self):
+        """Runs the regression and returns the beta."""
+        return self.sm_ols.params
+
+    @cache_readonly
+    def beta(self):
+        """Returns the betas in Series form."""
+        return Series(self._beta_raw, index=self._x.columns)
+
+    @cache_readonly
+    def _df_raw(self):
+        """Returns the degrees of freedom."""
+        return math.rank(self._x.values)
+
+    @cache_readonly
+    def df(self):
+        """Returns the degrees of freedom.
+
+        This equals the rank of the X matrix.
+        """
+        return self._df_raw
+
+    @cache_readonly
+    def _df_model_raw(self):
+        """Returns the raw model degrees of freedom."""
+        return self.sm_ols.df_model
+
+    @cache_readonly
+    def df_model(self):
+        """Returns the degrees of freedom of the model."""
+        return self._df_model_raw
+
+    @cache_readonly
+    def _df_resid_raw(self):
+        """Returns the raw residual degrees of freedom."""
+        return self.sm_ols.df_resid
+
+    @cache_readonly
+    def df_resid(self):
+        """Returns the degrees of freedom of the residuals."""
+        return self._df_resid_raw
+
+    @cache_readonly
+    def _f_stat_raw(self):
+        """Returns the raw f-stat value."""
+        from scipy.stats import f
+
+        cols = self._x.columns
+
+        if self._nw_lags is None:
+            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
+
+            q = len(cols)
+            if 'intercept' in cols:
+                q -= 1
+
+            shape = q, self.df_resid
+            p_value = 1 - f.cdf(F, shape[0], shape[1])
+            return F, shape, p_value
+
+        k = len(cols)
+        R = np.eye(k)
+        r = np.zeros((k, 1))
+
+        intercept = cols.indexMap.get('intercept')
+
+        if intercept is not None:
+            R = np.concatenate((R[0 : intercept], R[intercept + 1:]))
+            r = np.concatenate((r[0 : intercept], r[intercept + 1:]))
+
+        return math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
+                           self._nobs, self.df)
+
+    @cache_readonly
+    def f_stat(self):
+        """Returns the f-stat value."""
+        return common.f_stat_to_dict(self._f_stat_raw)
+
+    def f_test(self, hypothesis):
+        """Runs the F test, given a joint hypothesis.  The hypothesis is
+        represented by a collection of equations, in the form
+
+        A*x_1+B*x_2=C
+
+        You must provide the coefficients even if they're 1.  No spaces.
+
+        The equations can be passed as either a single string or a
+        list of strings.
+
+        Examples
+        --------
+        o = ols(...)
+        o.f_test('1*x1+2*x2=0,1*x3=0')
+        o.f_test(['1*x1+2*x2=0','1*x3=0'])
+        """
+
+        x_names = self._x.columns
+
+        R = []
+        r = []
+
+        if isinstance(hypothesis, str):
+            eqs = hypothesis.split(',')
+        elif isinstance(hypothesis, list):
+            eqs = hypothesis
+        else: # pragma: no cover
+            raise Exception('hypothesis must be either string or list')
+        for equation in eqs:
+            row = np.zeros(len(x_names))
+            lhs, rhs = equation.split('=')
+            for s in lhs.split('+'):
+                ss = s.split('*')
+                coeff = float(ss[0])
+                x_name = ss[1]
+
+                if x_name not in x_names:
+                    raise Exception('no coefficient named %s' % x_name)
+                idx = x_names.indexMap[x_name]
+                row[idx] = coeff
+            rhs = float(rhs)
+
+            R.append(row)
+            r.append(rhs)
+
+        R = np.array(R)
+        q = len(r)
+        r = np.array(r).reshape(q, 1)
+
+        result = math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
+                             self._nobs, self.df)
+
+        return common.f_stat_to_dict(result)
+
+    @cache_readonly
+    def _p_value_raw(self):
+        """Returns the raw p values."""
+        from scipy.stats import t
+
+        return 2 * t.sf(np.fabs(self._t_stat_raw),
+                        self._df_resid_raw)
+
+    @cache_readonly
+    def p_value(self):
+        """Returns the p values."""
+        return Series(self._p_value_raw, index=self.beta.index)
+
+    @cache_readonly
+    def _r2_raw(self):
+        """Returns the raw r-squared values."""
+        has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
+
+        if self._intercept:
+            return 1 - self.sm_ols.ssr / self.sm_ols.centered_tss
+        else:
+            return 1 - self.sm_ols.ssr / self.sm_ols.uncentered_tss
+
+    @cache_readonly
+    def r2(self):
+        """Returns the r-squared values."""
+        return self._r2_raw
+
+    @cache_readonly
+    def _r2_adj_raw(self):
+        """Returns the raw r-squared adjusted values."""
+        return self.sm_ols.rsquared_adj
+
+    @cache_readonly
+    def r2_adj(self):
+        """Returns the r-squared adjusted values."""
+        return self._r2_adj_raw
+
+    @cache_readonly
+    def _resid_raw(self):
+        """Returns the raw residuals."""
+        return self.sm_ols.resid
+
+    @cache_readonly
+    def resid(self):
+        """Returns the residuals."""
+        return Series(self._resid_raw, index=self._x.index)
+
+    @cache_readonly
+    def _rmse_raw(self):
+        """Returns the raw rmse values."""
+        return np.sqrt(self.sm_ols.mse_resid)
+
+    @cache_readonly
+    def rmse(self):
+        """Returns the rmse value."""
+        return self._rmse_raw
+
+    @cache_readonly
+    def _std_err_raw(self):
+        """Returns the raw standard err values."""
+        return np.sqrt(np.diag(self._var_beta_raw))
+
+    @cache_readonly
+    def std_err(self):
+        """Returns the standard err values of the betas."""
+        return Series(self._std_err_raw, index=self.beta.index)
+
+    @cache_readonly
+    def _t_stat_raw(self):
+        """Returns the raw t-stat value."""
+        return self._beta_raw / self._std_err_raw
+
+    @cache_readonly
+    def t_stat(self):
+        """Returns the t-stat values of the betas."""
+        return Series(self._t_stat_raw, index=self.beta.index)
+
+    @cache_readonly
+    def _var_beta_raw(self):
+        """
+        Returns the raw covariance of beta.
+        """
+        x = self._x.values
+        y = self._y_raw
+
+        xx = np.dot(x.T, x)
+
+        if self._nw_lags is None:
+            return math.inv(xx) * (self._rmse_raw ** 2)
+        else:
+            resid = y - np.dot(x, self._beta_raw)
+            m = (x.T * resid).T
+
+            xeps = math.newey_west(m, self._nw_lags, self._nobs, self._df_raw,
+                                   self._nw_overlap)
+
+            xx_inv = math.inv(xx)
+            return np.dot(xx_inv, np.dot(xeps, xx_inv))
+
+    @cache_readonly
+    def var_beta(self):
+        """Returns the variance-covariance matrix of beta."""
+        return DataFrame(self._var_beta_raw, index=self.beta.index,
+                         columns=self.beta.index)
+
+    @cache_readonly
+    def _y_fitted_raw(self):
+        """Returns the raw fitted y values."""
+        return self.sm_ols.fittedvalues
+
+    @cache_readonly
+    def y_fitted(self):
+        """Returns the fitted y values.  This equals BX."""
+        result = Series(self._y_fitted_raw, index=self._y.index)
+        return result.reindex(self._y_orig.index)
+
+    @cache_readonly
+    def _y_predict_raw(self):
+        """Returns the raw predicted y values."""
+        return self._y_fitted_raw
+
+    @cache_readonly
+    def y_predict(self):
+        """Returns the predicted y values.
+
+        For in-sample, this is same as y_fitted."""
+        return self.y_fitted
+
+    RESULT_FIELDS = ['r2', 'r2_adj', 'df', 'df_model', 'df_resid', 'rmse',
+                     'f_stat', 'beta', 'std_err', 't_stat', 'p_value', 'nobs']
+
+    @cache_readonly
+    def _results(self):
+        results = {}
+        for result in self.RESULT_FIELDS:
+            results[result] = getattr(self, result)
+
+        return results
+
+    @cache_readonly
+    def _coef_table(self):
+        buf = StringIO()
+
+        buf.write('%14s %10s %10s %10s %10s %10s %10s\n' %
+                  ('Variable', 'Coef', 'Std Err', 't-stat',
+                   'p-value', 'CI 2.5%', 'CI 97.5%'))
+        buf.write(common.banner(''))
+        coef_template = '\n%14s %10.4f %10.4f %10.2f %10.4f %10.4f %10.4f'
+
+        results = self._results
+
+        beta = results['beta']
+
+        for i, name in enumerate(beta.index):
+            if i and not (i % 5):
+                buf.write('\n' + common.banner(''))
+
+            std_err = results['std_err'][name]
+            CI1 = beta[name] - 1.96 * std_err
+            CI2 = beta[name] + 1.96 * std_err
+
+            t_stat = results['t_stat'][name]
+            p_value = results['p_value'][name]
+
+            line = coef_template % (name,
+                beta[name], std_err, t_stat, p_value, CI1, CI2)
+
+            buf.write(line)
+
+        if self.nw_lags is not None:
+            buf.write('\n')
+            buf.write('*** The calculations are Newey-West '
+                      'adjusted with lags %5d\n' % self.nw_lags)
+
+        return buf.getvalue()
+
+    @cache_readonly
+    def summary_as_matrix(self):
+        """Returns the formatted results of the OLS as a DataFrame."""
+        results = self._results
+        beta = results['beta']
+        data = {'beta' : results['beta'],
+                't-stat' : results['t_stat'],
+                'p-value' : results['p_value'],
+                'std err' : results['std_err']}
+        return DataFrame(data, beta.index).T
+
+    @cache_readonly
+    def summary(self):
+        """
+        This returns the formatted result of the OLS computation
+        """
+        template = """
+%(bannerTop)s
+
+Formula: Y ~ %(formula)s
+
+Number of Observations:         %(nobs)d
+Number of Degrees of Freedom:   %(df)d
+
+R-squared:     %(r2)10.4f
+Adj R-squared: %(r2_adj)10.4f
+
+Rmse:          %(rmse)10.4f
+
+F-stat %(f_stat_shape)s: %(f_stat)10.4f, p-value: %(f_stat_p_value)10.4f
+
+Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
+
+%(bannerCoef)s
+%(coef_table)s
+%(bannerEnd)s
+"""
+        coef_table = self._coef_table
+
+        results = self._results
+
+        f_stat = results['f_stat']
+
+        bracketed = ['<%s>' % c for c in results['beta'].index]
+
+        formula = StringIO()
+        formula.write(bracketed[0])
+        tot = len(bracketed[0])
+        line = 1
+        for coef in bracketed[1:]:
+            tot = tot + len(coef) + 3
+
+            if tot // (68 * line):
+                formula.write('\n' + ' ' * 12)
+                line += 1
+
+            formula.write(' + ' + coef)
+
+        params = {
+            'bannerTop' : common.banner('Summary of Regression Analysis'),
+            'bannerCoef' : common.banner('Summary of Estimated Coefficients'),
+            'bannerEnd' : common.banner('End of Summary'),
+            'formula' : formula.getvalue(),
+            'r2' : results['r2'],
+            'r2_adj' : results['r2_adj'],
+            'nobs' : results['nobs'],
+            'df'  : results['df'],
+            'df_model'  : results['df_model'],
+            'df_resid'  : results['df_resid'],
+            'coef_table' : coef_table,
+            'rmse' : results['rmse'],
+            'f_stat' : f_stat['f-stat'],
+            'f_stat_shape' : '(%d, %d)' % (f_stat['DF X'], f_stat['DF Resid']),
+            'f_stat_p_value' : f_stat['p-value'],
+        }
+
+        return template % params
+
+    def __repr__(self):
+        return self.summary
+
+
+    @cache_readonly
+    def _time_obs_count(self):
+        # XXX
+        return self._time_has_obs.astype(int)
+
+    @property
+    def _total_times(self):
+        return self._time_has_obs.sum()
+
+
+class MovingOLS(OLS):
+    """
+    Runs a rolling/expanding simple OLS.
+
+    Parameters
+    ----------
+    y: Series
+    x: Series, DataFrame, or dict of Series
+    intercept: bool
+        True if you want an intercept.
+    nw_lags: None or int
+        Number of Newey-West lags.
+    window_type: int
+        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
+    window: int
+        size of window (for rolling/expanding OLS)
+    """
+    def __init__(self, y, x, window_type='expanding',
+                 window=None, min_periods=None, intercept=True,
+                 nw_lags=None, nw_overlap=False):
+
+        self._args = dict(intercept=intercept, nw_lags=nw_lags,
+                          nw_overlap=nw_overlap)
+
+        OLS.__init__(self, y=y, x=x, **self._args)
+
+        self._set_window(window_type, window, min_periods)
+
+    def _set_window(self, window_type, window, min_periods):
+        self._window_type = common._get_window_type(window_type)
+
+        if self._is_rolling:
+            if window is None:
+                raise Exception('Must pass window when doing rolling '
+                                'regression')
+
+            if min_periods is None:
+                min_periods = window
+        else:
+            window = len(self._x)
+            if min_periods is None:
+                min_periods = 1
+
+        self._window = int(window)
+        self._min_periods = min_periods
+
+#-------------------------------------------------------------------------------
+# "Public" results
+
+    @cache_readonly
+    def beta(self):
+        """Returns the betas in Series/DataFrame form."""
+        return DataFrame(self._beta_raw,
+                         index=self._result_index,
+                         columns=self._x.columns)
+
+    @cache_readonly
+    def rank(self):
+        return Series(self._rank_raw, index=self._result_index)
+
+    @cache_readonly
+    def df(self):
+        """Returns the degrees of freedom."""
+        return Series(self._df_raw, index=self._result_index)
+
+    @cache_readonly
+    def df_model(self):
+        """Returns the model degrees of freedom."""
+        return Series(self._df_model_raw, index=self._result_index)
+
+    @cache_readonly
+    def df_resid(self):
+        """Returns the residual degrees of freedom."""
+        return Series(self._df_resid_raw, index=self._result_index)
+
+    @cache_readonly
+    def f_stat(self):
+        """Returns the f-stat value."""
+        f_stat_dicts = dict((date, common.f_stat_to_dict(f_stat))
+                            for date, f_stat in zip(self.beta.index,
+                                                    self._f_stat_raw))
+
+        return DataFrame(f_stat_dicts).T
+
+    def f_test(self, hypothesis):
+        raise Exception('f_test not supported for rolling/expanding OLS')
+
+    @cache_readonly
+    def forecast_mean(self):
+        return Series(self._forecast_mean_raw, index=self._result_index)
+
+    @cache_readonly
+    def forecast_vol(self):
+        return Series(self._forecast_vol_raw, index=self._result_index)
+
+    @cache_readonly
+    def p_value(self):
+        """Returns the p values."""
+        cols = self.beta.columns
+        return DataFrame(self._p_value_raw, columns=cols,
+                         index=self._result_index)
+
+    @cache_readonly
+    def r2(self):
+        """Returns the r-squared values."""
+        return Series(self._r2_raw, index=self._result_index)
+
+    @cache_readonly
+    def resid(self):
+        """Returns the residuals."""
+        return Series(self._resid_raw[self._valid_obs_labels],
+                      index=self._result_index)
+
+    @cache_readonly
+    def r2_adj(self):
+        """Returns the r-squared adjusted values."""
+        index = self.r2.index
+
+        return Series(self._r2_adj_raw, index=index)
+
+    @cache_readonly
+    def rmse(self):
+        """Returns the rmse values."""
+        return Series(self._rmse_raw, index=self._result_index)
+
+    @cache_readonly
+    def std_err(self):
+        """Returns the standard err values."""
+        return DataFrame(self._std_err_raw, columns=self.beta.columns,
+                         index=self._result_index)
+
+    @cache_readonly
+    def t_stat(self):
+        """Returns the t-stat value."""
+        return DataFrame(self._t_stat_raw, columns=self.beta.columns,
+                         index=self._result_index)
+
+    @cache_readonly
+    def var_beta(self):
+        """Returns the covariance of beta."""
+        result = {}
+        result_index = self._result_index
+        for i in xrange(len(self._var_beta_raw)):
+            dm = DataFrame(self._var_beta_raw[i], columns=self.beta.columns,
+                           index=self.beta.columns)
+            result[result_index[i]] = dm
+
+        return WidePanel.fromDict(result, intersect=False)
+
+    @cache_readonly
+    def y_fitted(self):
+        """Returns the fitted y values."""
+        return Series(self._y_fitted_raw[self._valid_obs_labels],
+                      index=self._result_index)
+
+    @cache_readonly
+    def y_predict(self):
+        """Returns the predicted y values."""
+        return Series(self._y_predict_raw[self._valid_obs_labels],
+                      index=self._result_index)
+
+#-------------------------------------------------------------------------------
+# "raw" attributes, calculations
+
+    @property
+    def _is_rolling(self):
+        return self._window_type == common.ROLLING
+
+    @cache_readonly
+    def _beta_raw(self):
+        """Runs the regression and returns the beta."""
+        beta, indices, mask = self._rolling_ols_call
+
+        return beta[indices]
+
+    @cache_readonly
+    def _result_index(self):
+        return self._index[self._valid_indices]
+
+    @property
+    def _valid_indices(self):
+        return self._rolling_ols_call[1]
+
+    @cache_readonly
+    def _rolling_ols_call(self):
+        return self._calc_betas(self._x_trans, self._y_trans)
+
+    def _calc_betas(self, x, y):
+        N = len(self._index)
+        K = len(self._x.columns)
+
+        betas = np.empty((N, K), dtype=float)
+        betas[:] = np.NaN
+
+        valid = self._time_has_obs
+        enough = self._enough_obs
+        window = self._window
+
+        # Use transformed (demeaned) Y, X variables
+        cum_xx = self._cum_xx(x)
+        cum_xy = self._cum_xy(x, y)
+
+        for i in xrange(N):
+            if not valid[i] or not enough[i]:
+                continue
+
+            xx = cum_xx[i]
+            xy = cum_xy[i]
+            if self._is_rolling and i >= window:
+                xx = xx - cum_xx[i - window]
+                xy = xy - cum_xy[i - window]
+
+            betas[i] = math.solve(xx, xy)
+
+        mask = -np.isnan(betas).any(axis=1)
+        have_betas = np.arange(N)[mask]
+
+        return betas, have_betas, mask
+
+    def _rolling_rank(self):
+        dates = self._index
+        window = self._window
+
+        ranks = np.empty(len(dates), dtype=float)
+        ranks[:] = np.NaN
+        for i, date in enumerate(dates):
+            if self._is_rolling and i >= window:
+                prior_date = dates[i - window + 1]
+            else:
+                prior_date = dates[0]
+
+            x_slice = self._x.truncate(before=prior_date, after=date).values
+
+            if len(x_slice) == 0:
+                continue
+
+            ranks[i] = math.rank(x_slice)
+
+        return ranks
+
+    def _cum_xx(self, x):
+        dates = self._index
+        K = len(x.columns)
+        valid = self._time_has_obs
+        cum_xx = []
+
+        if isinstance(x, DataFrame):
+            _indexMap = x.index.indexMap
+            def slicer(df, dt):
+                i = _indexMap[dt]
+                return df.values[i:i+1, :]
+        else:
+            slicer = lambda df, dt: df.truncate(dt, dt).values
+
+        last = np.zeros((K, K))
+        for i, date in enumerate(dates):
+            if not valid[i]:
+                cum_xx.append(last)
+                continue
+
+            x_slice = slicer(x, date)
+            xx = last = last + np.dot(x_slice.T, x_slice)
+            cum_xx.append(xx)
+
+        return cum_xx
+
+    def _cum_xy(self, x, y):
+        dates = self._index
+        valid = self._time_has_obs
+        cum_xy = []
+
+        if isinstance(x, DataFrame):
+            _x_indexMap = x.index.indexMap
+            def x_slicer(df, dt):
+                i = _x_indexMap[dt]
+                return df.values[i:i+1, :]
+        else:
+            x_slicer = lambda df, dt: df.truncate(dt, dt).values
+
+
+        if isinstance(y, Series):
+            _y_indexMap = y.index.indexMap
+            _values = y.values
+            def y_slicer(df, dt):
+                i = _y_indexMap[dt]
+                return _values[i:i+1]
+        else:
+            y_slicer = lambda s, dt: _y_converter(s.truncate(dt, dt))
+
+        last = np.zeros(len(x.columns))
+        for i, date in enumerate(dates):
+            if not valid[i]:
+                cum_xy.append(last)
+                continue
+
+            x_slice = x_slicer(x, date)
+            y_slice = y_slicer(y, date)
+
+            xy = last = last + np.dot(x_slice.T, y_slice)
+            cum_xy.append(xy)
+
+        return cum_xy
+
+    @cache_readonly
+    def _rank_raw(self):
+        rank = self._rolling_rank()
+        return rank[self._valid_indices]
+
+    @cache_readonly
+    def _df_raw(self):
+        """Returns the degrees of freedom."""
+        return self._rank_raw
+
+    @cache_readonly
+    def _df_model_raw(self):
+        """Returns the raw model degrees of freedom."""
+        return self._df_raw - 1
+
+    @cache_readonly
+    def _df_resid_raw(self):
+        """Returns the raw residual degrees of freedom."""
+        return self._nobs - self._df_raw
+
+    @cache_readonly
+    def _f_stat_raw(self):
+        """Returns the raw f-stat value."""
+        from scipy.stats import f
+
+        items = self.beta.columns
+        nobs = self._nobs
+        df = self._df_raw
+        df_resid = nobs - df
+
+        # var_beta has not been newey-west adjusted
+        if self._nw_lags is None:
+            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
+
+            q = len(items)
+            if 'intercept' in items:
+                q -= 1
+
+            def get_result_simple(Fst, d):
+                return Fst, (q, d), 1 - f.cdf(Fst, q, d)
+
+            # Compute the P-value for each pair
+            result = starmap(get_result_simple, izip(F, df_resid))
+
+            return list(result)
+
+        K = len(items)
+        R = np.eye(K)
+        r = np.zeros((K, 1))
+
+        intercept = items.indexMap.get('intercept')
+
+        if intercept is not None:
+            R = np.concatenate((R[0 : intercept], R[intercept + 1:]))
+            r = np.concatenate((r[0 : intercept], r[intercept + 1:]))
+
+        def get_result(beta, vcov, n, d):
+            return math.calc_F(R, r, beta, vcov, n, d)
+
+        results = starmap(get_result,
+                          izip(self._beta_raw, self._var_beta_raw, nobs, df))
+
+        return list(results)
+
+    @cache_readonly
+    def _p_value_raw(self):
+        """Returns the raw p values."""
+        from scipy.stats import t
+
+        result = [2 * t.sf(a, b)
+                  for a, b in izip(np.fabs(self._t_stat_raw),
+                                   self._df_resid_raw)]
+
+        return np.array(result)
+
+    @cache_readonly
+    def _resid_stats(self):
+        uncentered_sst = []
+        sst = []
+        sse = []
+
+        Y = self._y
+        X = self._x
+
+        dates = self._index
+        window = self._window
+        for n, index in enumerate(self._valid_indices):
+            if self._is_rolling and index >= window:
+                prior_date = dates[index - window + 1]
+            else:
+                prior_date = dates[0]
+
+            date = dates[index]
+            beta = self._beta_raw[n]
+
+            X_slice = X.truncate(before=prior_date, after=date).values
+            Y_slice = _y_converter(Y.truncate(before=prior_date, after=date))
+
+            resid = Y_slice - np.dot(X_slice, beta)
+
+            SS_err = (resid ** 2).sum()
+            SS_total = ((Y_slice - Y_slice.mean()) ** 2).sum()
+            SST_uncentered = (Y_slice ** 2).sum()
+
+            sse.append(SS_err)
+            sst.append(SS_total)
+            uncentered_sst.append(SST_uncentered)
+
+        return {
+            'sse' : np.array(sse),
+            'centered_tss' : np.array(sst),
+            'uncentered_tss' : np.array(uncentered_sst),
+        }
+
+    @cache_readonly
+    def _rmse_raw(self):
+        """Returns the raw rmse values."""
+        return np.sqrt(self._resid_stats['sse'] / self._df_resid_raw)
+
+    @cache_readonly
+    def _r2_raw(self):
+        rs = self._resid_stats
+
+        if self._intercept:
+            return 1 - rs['sse'] / rs['centered_tss']
+        else:
+            return 1 - rs['sse'] / rs['uncentered_tss']
+
+    @cache_readonly
+    def _r2_adj_raw(self):
+        """Returns the raw r-squared adjusted values."""
+        nobs = self._nobs
+        factors = (nobs - 1) / (nobs - self._df_raw)
+        return 1 - (1 - self._r2_raw) * factors
+
+    @cache_readonly
+    def _resid_raw(self):
+        """Returns the raw residuals."""
+        return (self._y_raw - self._y_fitted_raw)
+
+    @cache_readonly
+    def _std_err_raw(self):
+        """Returns the raw standard err values."""
+        results = []
+        for i in xrange(len(self._var_beta_raw)):
+            results.append(np.sqrt(np.diag(self._var_beta_raw[i])))
+
+        return np.array(results)
+
+    @cache_readonly
+    def _t_stat_raw(self):
+        """Returns the raw t-stat value."""
+        return self._beta_raw / self._std_err_raw
+
+    @cache_readonly
+    def _var_beta_raw(self):
+        """Returns the raw covariance of beta."""
+        x = self._x
+        y = self._y
+        dates = self._index
+        nobs = self._nobs
+        rmse = self._rmse_raw
+        beta = self._beta_raw
+        df = self._df_raw
+        window = self._window
+        cum_xx = self._cum_xx(self._x)
+
+        results = []
+        for n, i in enumerate(self._valid_indices):
+            xx = cum_xx[i]
+            date = dates[i]
+
+            if self._is_rolling and i >= window:
+                xx = xx - cum_xx[i - window]
+                prior_date = dates[i - window + 1]
+            else:
+                prior_date = dates[0]
+
+            x_slice = x.truncate(before=prior_date, after=date)
+            y_slice = y.truncate(before=prior_date, after=date)
+            xv = x_slice.values
+            yv = np.asarray(y_slice)
+
+            if self._nw_lags is None:
+                result = math.inv(xx) * (rmse[n] ** 2)
+            else:
+                resid = yv - np.dot(xv, beta[n])
+                m = (xv.T * resid).T
+
+                xeps = math.newey_west(m, self._nw_lags, nobs[n], df[n],
+                                       self._nw_overlap)
+
+                xx_inv = math.inv(xx)
+                result = np.dot(xx_inv, np.dot(xeps, xx_inv))
+
+            results.append(result)
+
+        return np.array(results)
+
+    @cache_readonly
+    def _forecast_mean_raw(self):
+        """Returns the raw covariance of beta."""
+        nobs = self._nobs
+        window = self._window
+
+        # x should be ones
+        dummy = DataFrame(index=self._y.index)
+        dummy['y'] = 1
+
+        cum_xy = self._cum_xy(dummy, self._y)
+
+        results = []
+        for n, i in enumerate(self._valid_indices):
+            sumy = cum_xy[i]
+
+            if self._is_rolling and i >= window:
+                sumy = sumy - cum_xy[i - window]
+
+            results.append(sumy[0] / nobs[n])
+
+        return np.array(results)
+
+    @cache_readonly
+    def _forecast_vol_raw(self):
+        """Returns the raw covariance of beta."""
+        beta = self._beta_raw
+        window = self._window
+        dates = self._index
+        x = self._x
+
+        results = []
+        for n, i in enumerate(self._valid_indices):
+            date = dates[i]
+            if self._is_rolling and i >= window:
+                prior_date = dates[i - window + 1]
+            else:
+                prior_date = dates[0]
+
+            x_slice = x.truncate(prior_date, date).values
+            x_demeaned = x_slice - x_slice.mean(0)
+            x_cov = np.dot(x_demeaned.T, x_demeaned) / (len(x_slice) - 1)
+
+            B = beta[n]
+            result = np.dot(B, np.dot(x_cov, B))
+            results.append(np.sqrt(result))
+
+        return np.array(results)
+
+    @cache_readonly
+    def _y_fitted_raw(self):
+        """Returns the raw fitted y values."""
+        return (self._x.values * self._beta_matrix(lag=0)).sum(1)
+
+    @cache_readonly
+    def _y_predict_raw(self):
+        """Returns the raw predicted y values."""
+        return (self._x.values * self._beta_matrix(lag=1)).sum(1)
+
+    @cache_readonly
+    def _results(self):
+        results = {}
+        for result in self.RESULT_FIELDS:
+            value = getattr(self, result)
+            if isinstance(value, np.ndarray):
+                value = value[-1]
+            elif isinstance(value, Series):
+                value = value[self.beta.index[-1]]
+            elif isinstance(value, DataFrame):
+                value = value.xs(self.beta.index[-1])
+            else:
+                raise Exception('Problem retrieving %s' % result)
+            results[result] = value
+
+        return results
+
+    @cache_readonly
+    def _window_time_obs(self):
+        window_obs = moments.rolling_sum(self._time_obs_count > 0,
+                                         self._window, min_periods=1)
+
+        window_obs[np.isnan(window_obs)] = 0
+        return window_obs.astype(int)
+
+    @cache_readonly
+    def _nobs_raw(self):
+        if self._is_rolling:
+            window = self._window
+        else:
+            # expanding case
+            window = len(self._index)
+
+        result = moments.rolling_sum(self._time_obs_count, window,
+                                     min_periods=1)
+
+        return result.astype(int)
+
+    def _beta_matrix(self, lag=0):
+        assert(lag >= 0)
+
+        labels = np.arange(len(self._y)) - lag
+        indexer = self._valid_obs_labels.searchsorted(labels, side='left')
+
+        beta_matrix = self._beta_raw[indexer]
+        beta_matrix[labels < self._valid_obs_labels[0]] = np.NaN
+
+        return beta_matrix
+
+    @cache_readonly
+    def _valid_obs_labels(self):
+        dates = self._index[self._valid_indices]
+        return self._y.index.searchsorted(dates)
+
+    @cache_readonly
+    def _nobs(self):
+        return self._nobs_raw[self._valid_indices]
+
+    @property
+    def nobs(self):
+        return Series(self._nobs, index=self._result_index)
+
+    @cache_readonly
+    def _enough_obs(self):
+        # XXX: what's the best way to determine where to start?
+        return self._nobs_raw >= max(self._min_periods,
+                                     len(self._x.columns) + 1)
+
+def _safe_update(d, other):
+    """
+    Combine dictionaries with non-overlapping keys
+    """
+    for k, v in other.iteritems():
+        if k in d:
+            raise Exception('Duplicate regressor: %s' % k)
+
+        d[k] = v
+
+def _combine_rhs(rhs):
+    """
+    Glue input X variables together while checking for potential
+    duplicates
+    """
+    series = {}
+
+    if isinstance(rhs, Series):
+        series['x'] = rhs
+    elif isinstance(rhs, DataFrame):
+        series = rhs.copy()
+    elif isinstance(rhs, dict):
+        for name, value in rhs.iteritems():
+            if isinstance(value, Series):
+                _safe_update(series, {name : value})
+            elif isinstance(value, (dict, DataFrame)):
+                _safe_update(series, value)
+            else:
+                raise Exception('Invalid RHS data type: %s' % type(value))
+    else:
+        raise Exception('Invalid RHS type: %s' % type(rhs))
+
+    if not isinstance(series, DataFrame):
+        series = DataFrame(series)
+
+    return series
+
+def _filter_data(lhs, rhs):
+    """
+    Cleans the input for single OLS.
+
+    Parameters
+    ----------
+    lhs: Series
+        Dependent variable in the regression.
+    rhs: dict, whose values are Series, DataFrame, or dict
+        Explanatory variables of the regression.
+
+    Returns
+    -------
+    Series, DataFrame
+        Cleaned lhs and rhs
+    """
+    if not isinstance(lhs, Series):
+        raise Exception('lhs must be a Series')
+
+    rhs = _combine_rhs(rhs)
+
+    rhs_valid = np.isfinite(rhs.values).sum(1) == len(rhs.columns)
+
+    if not rhs_valid.all():
+        pre_filtered_rhs = rhs[rhs_valid]
+    else:
+        pre_filtered_rhs = rhs
+
+    index = lhs.index + rhs.index
+    if not index.equals(rhs.index) or not index.equals(lhs.index):
+        rhs = rhs.reindex(index)
+        lhs = lhs.reindex(index)
+
+        rhs_valid = np.isfinite(rhs.values).sum(1) == len(rhs.columns)
+
+    lhs_valid = np.isfinite(lhs.values)
+    valid = rhs_valid & lhs_valid
+
+    if not valid.all():
+        filt_index = rhs.index[valid]
+        filtered_rhs = rhs.reindex(filt_index)
+        filtered_lhs = lhs.reindex(filt_index)
+    else:
+        filtered_rhs, filtered_lhs = rhs, lhs
+
+    return filtered_lhs, filtered_rhs, pre_filtered_rhs, index, valid
+
+# A little kludge so we can use this method for both
+# MovingOLS and MovingPanelOLS
+def _y_converter(y):
+    if isinstance(y, Series):
+        return np.asarray(y)
+    else:
+        y = y.values.squeeze()
+        if y.ndim == 0:
+            return np.array([y])
+        else:
+            return y
diff --git a/pandas/stats/plm.py b/pandas/stats/plm.py
index 6f8ec035c..8392c0940 100644
--- a/pandas/stats/plm.py
+++ b/pandas/stats/plm.py
@@ -1,917 +1,917 @@
-"""
-Linear regression objects for panel data
-"""
-
-# pylint: disable-msg=W0231
-# pylint: disable-msg=E1101,E1103
-
-from __future__ import division
-import warnings
-
-import numpy as np
-
-from pandas.core.panel import WidePanel, LongPanel
-from pandas.core.frame import DataFrame
-from pandas.core.series import Series
-from pandas.core.sparse import SparseWidePanel
-from pandas.stats.ols import OLS, MovingOLS
-import pandas.stats.common as common
-import pandas.stats.math as math
-from pandas.util.decorators import cache_readonly
-
-class PanelOLS(OLS):
-    """Implements panel OLS.
-
-    Parameters
-    ----------
-    y : DataFrame
-    x : Dict of DataFrame or WidePanel
-    intercept : bool
-        True if you want an intercept.  True by default.
-    nw_lags : None or int
-        Number of Newey-West lags.  None by default.
-    nw_overlap : bool
-        Whether there are overlaps in the NW lags.  Defaults to False.
-    window_type : int
-        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
-    window : int
-        size of window (for rolling/expanding OLS)
-    weights : DataFrame
-        Weight for each observation.  The weights are not normalized;
-        they're multiplied directly by each observation.
-    pool : bool, default True
-        Whether to run pooled panel regression
-    entity_effects : bool, deafult False
-        Whether to account for entity fixed effects
-    time_effects : bool, default False
-        Whether to account for time fixed effects
-    x_effects : list, default None
-        List of x's to account for fixed effects
-    dropped_dummies : dict
-        Key is the name of the variable for the fixed effect.
-        Value is the value of that variable for which we drop the dummy.
-
-        For entity fixed effects, key equals 'entity', e.g. {'entity' : 'US'}
-
-        By default, the first item is dropped if one is not specified.
-    cluster : int
-        ENTITY or TIME, indicating entity/time clustering
-        A cluster is a grouping within which observations are correlated.
-
-        For example, if you have a panel data with countries over time and you
-        suspect that:
-
-        1. Countries are correlated - use 'time'
-        2. There is autocorrelation - use 'entity'
-
-    """
-    def __init__(self, y, x, weights=None,
-                 intercept=True, nw_lags=None, entity_effects=False,
-                 time_effects=False, x_effects=None, cluster=None,
-                 dropped_dummies=None, verbose=False, nw_overlap=False):
-        self._x_orig = x
-        self._y_orig = y
-        self._weights = weights
-        self._intercept = intercept
-        self._nw_lags = nw_lags
-        self._nw_overlap = nw_overlap
-        self._entity_effects = entity_effects
-        self._time_effects = time_effects
-        self._x_effects = x_effects
-        self._dropped_dummies = dropped_dummies or {}
-        self._cluster = common._get_cluster_type(cluster)
-        self._verbose = verbose
-
-        (self._x, self._x_trans,
-         self._x_filtered, self._y,
-         self._y_trans) = self._prepare_data()
-
-        self._x_trans_raw = self._x_trans.values
-        self._y_trans_raw = self._y_trans.values.squeeze()
-
-        self._index = self._y.major_axis
-
-        self._T = len(self._index)
-
-    def log(self, msg):
-        if self._verbose:
-            print msg
-
-    def _prepare_data(self):
-        """Cleans and converts input data into LongPanel classes.
-
-        If time effects is True, then we turn off intercepts and omit an item
-        from every (entity and x) fixed effect.
-
-        Otherwise:
-           - If we have an intercept, we omit an item from every fixed effect.
-           - Else, we omit an item from every fixed effect except one of them.
-
-        The categorical variables will get dropped from x.
-        """
-        (x, x_filtered, y, weights,
-         weights_filt, cat_mapping) = self._filter_data()
-
-        self.log('Adding dummies to X variables')
-        x = self._add_dummies(x, cat_mapping)
-
-        self.log('Adding dummies to filtered X variables')
-        x_filtered = self._add_dummies(x_filtered, cat_mapping)
-
-        if self._x_effects:
-            x = x.filter(x.items - self._x_effects)
-            x_filtered = x_filtered.filter(x_filtered.items - self._x_effects)
-
-        if self._time_effects:
-            x_regressor = x.subtract(x.mean('minor', broadcast=True))
-            y_regressor = y.subtract(y.mean('minor', broadcast=True))
-
-        elif self._intercept:
-            # only add intercept when no time effects
-            self.log('Adding intercept')
-            x = x_regressor = add_intercept(x)
-            x_filtered = add_intercept(x_filtered)
-            y_regressor = y
-        else:
-            self.log('No intercept added')
-            x_regressor = x
-            y_regressor = y
-
-        if weights is not None:
-            y_regressor = y_regressor.multiply(weights)
-            x_regressor = x_regressor.multiply(weights)
-
-        return x, x_regressor, x_filtered, y, y_regressor
-
-    def _filter_data(self):
-        """
-
-        """
-        data = self._x_orig
-        cat_mapping = {}
-
-        if isinstance(data, LongPanel):
-            data = data.to_wide()
-        else:
-            if isinstance(data, WidePanel):
-                data = data.copy()
-
-            if not isinstance(data, SparseWidePanel):
-                data, cat_mapping = self._convert_x(data)
-
-            if not isinstance(data, WidePanel):
-                data = WidePanel.from_dict(data, intersect=True)
-
-        x_names = data.items
-
-        if self._weights is not None:
-            data['__weights__'] = self._weights
-
-        # Filter x's without y (so we can make a prediction)
-        filtered = data.to_long()
-
-        # Filter all data together using to_long
-        data['__y__'] = self._y_orig
-        data_long = data.to_long()
-
-        x_filt = filtered.filter(x_names)
-
-        if self._weights:
-            weights_filt = filtered['__weights__']
-        else:
-            weights_filt = None
-
-        x = data_long.filter(x_names)
-        y = data_long['__y__']
-
-        if self._weights:
-            weights = data_long['__weights__']
-        else:
-            weights = None
-
-        return x, x_filt, y, weights, weights_filt, cat_mapping
-
-    def _convert_x(self, x):
-
-        # Converts non-numeric data in x to floats. x_converted is the
-        # DataFrame with converted values, and x_conversion is a dict that
-        # provides the reverse mapping.  For example, if 'A' was converted to 0
-        # for x named 'variety', then x_conversion['variety'][0] is 'A'.
-        x_converted = {}
-        cat_mapping = {}
-        for key, df in x.iteritems():
-            if not isinstance(df, DataFrame):
-                raise TypeError('Input X data set contained an object of '
-                                'type %s' % type(df))
-
-            if _is_numeric(df):
-                x_converted[key] = df
-            else:
-                values = df.values
-                distinct_values = sorted(set(values.flat))
-                cat_mapping[key] = dict(enumerate(distinct_values))
-                new_values = np.searchsorted(distinct_values, values)
-                x_converted[key] = DataFrame(new_values, index=df.index,
-                                              columns=df.columns)
-
-        if len(cat_mapping) == 0:
-            x_converted = x
-
-        return x_converted, cat_mapping
-
-    def _add_dummies(self, panel, mapping):
-        """
-        Add entity and / or categorical dummies to input X LongPanel
-
-        Returns
-        -------
-        LongPanel
-        """
-        panel = self._add_entity_effects(panel)
-        panel = self._add_categorical_dummies(panel, mapping)
-
-        return panel
-
-    def _add_entity_effects(self, panel):
-        """
-        Add entity dummies to panel
-
-        Returns
-        -------
-        LongPanel
-        """
-        if not self._entity_effects:
-            return panel
-
-        self.log('-- Adding entity fixed effect dummies')
-
-        dummies = panel.get_axis_dummies(axis='minor')
-
-        if not self._use_all_dummies:
-            if 'entity' in self._dropped_dummies:
-                to_exclude = str(self._dropped_dummies.get('entity'))
-            else:
-                to_exclude = dummies.items[0]
-
-            if to_exclude not in dummies.items:
-                raise Exception('%s not in %s' % (to_exclude,
-                                                  dummies.items))
-
-            self.log('-- Excluding dummy for entity: %s' % to_exclude)
-
-            dummies = dummies.filter(dummies.items - [to_exclude])
-
-        dummies = dummies.addPrefix('FE_')
-        panel = panel.leftJoin(dummies)
-
-        return panel
-
-    def _add_categorical_dummies(self, panel, cat_mappings):
-        """
-        Add categorical dummies to panel
-
-        Returns
-        -------
-        LongPanel
-        """
-        if not self._x_effects:
-            return panel
-
-        dropped_dummy = (self._entity_effects and not self._use_all_dummies)
-
-        for effect in self._x_effects:
-            self.log('-- Adding fixed effect dummies for %s' % effect)
-
-            dummies = panel.get_dummies(effect)
-
-            val_map = cat_mappings.get(effect)
-            if val_map:
-                val_map = dict((v, k) for k, v in val_map.iteritems())
-
-            if dropped_dummy or not self._use_all_dummies:
-                if effect in self._dropped_dummies:
-                    to_exclude = mapped_name = self._dropped_dummies.get(effect)
-
-                    if val_map:
-                        mapped_name = val_map[to_exclude]
-                else:
-                    to_exclude = mapped_name = dummies.items[0]
-
-                if mapped_name not in dummies.items:
-                    raise Exception('%s not in %s' % (to_exclude,
-                                                      dummies.items))
-
-                self.log('-- Excluding dummy for %s: %s' % (effect, to_exclude))
-
-                dummies = dummies.filter(dummies.items - [mapped_name])
-                dropped_dummy = True
-
-            dummies = _convertDummies(dummies, cat_mappings.get(effect))
-            dummies = dummies.addPrefix('%s_' % effect)
-            panel = panel.leftJoin(dummies)
-
-        return panel
-
-    @property
-    def _use_all_dummies(self):
-        """
-        In the case of using an intercept or including time fixed
-        effects, completely partitioning the sample would make the X
-        not full rank.
-        """
-        return (not self._intercept and not self._time_effects)
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        X = self._x_trans_raw
-        Y = self._y_trans_raw
-
-        beta, _, _, _ = np.linalg.lstsq(X, Y)
-
-        return beta
-
-    @cache_readonly
-    def beta(self):
-        return Series(self._beta_raw, index=self._x.items)
-
-    @cache_readonly
-    def _weighted_x(self):
-        if self._weights:
-            return self._x.multiply(self._weights)
-        return self._x
-
-    @cache_readonly
-    def _weighted_y(self):
-        if self._weights:
-            return self._y.multiply(self._weights)
-
-        return self._y
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self._df_raw - 1
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self._nobs - self._df_raw
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        df = math.rank(self._x_trans_raw)
-        if self._time_effects:
-            df += self._total_times
-
-        return df
-
-    @cache_readonly
-    def _r2_raw(self):
-        Y = self._y.values.squeeze()
-        X = self._x.values
-
-        resid = Y - np.dot(X, self._beta_raw)
-
-        SSE = (resid ** 2).sum()
-        SST = ((Y - np.mean(Y)) ** 2).sum()
-
-        return 1 - SSE / SST
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        nobs = self._nobs
-        factors = (nobs - 1) / (nobs - self._df_raw)
-        return 1 - (1 - self._r2_raw) * factors
-
-    @cache_readonly
-    def _resid_raw(self):
-        Y = self._y.values.squeeze()
-        X = self._x.values
-        return Y - np.dot(X, self._beta_raw)
-
-    @cache_readonly
-    def resid(self):
-        return self._unstack_vector(self._resid_raw)
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        X = self._x.values
-        Y = self._y.values.squeeze()
-
-        resid = Y - np.dot(X, self._beta_raw)
-        ss = (resid ** 2).sum()
-        return np.sqrt(ss / (self._nobs - self._df_raw))
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        cluster_axis = None
-        if self._cluster == common.TIME:
-            cluster_axis = 0
-        elif self._cluster == common.ENTITY:
-            cluster_axis = 1
-
-        x = self._x
-        y = self._y
-
-        if self._time_effects:
-            xx = _xx_time_effects(x, y)
-        else:
-            xx = np.dot(x.values.T, x.values)
-
-        return _var_beta_panel(y, x, self._beta_raw, xx,
-                               self._rmse_raw, cluster_axis, self._nw_lags,
-                               self._nobs, self._df_raw, self._nw_overlap)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        return np.dot(self._x_filtered.values, self._beta_raw)
-
-    @cache_readonly
-    def y_fitted(self):
-        return self._unstack_vector(self._y_fitted_raw,
-                                    index=self._x_filtered.index)
-
-    def f_test(self, hypothesis):
-        """Runs the F test, given a joint hypothesis.  The hypothesis is
-        represented by a collection of equations, in the form
-
-        A*x_1+B*x_2=C
-
-        You must provide the coefficients even if they're 1.  No spaces.
-
-        The equations can be passed as either a single string or a
-        list of strings.
-
-        Examples:
-        o = ols(...)
-        o.f_test('1*x1+2*x2=0,1*x3=0')
-        o.f_test(['1*x1+2*x2=0','1*x3=0'])
-        """
-
-        x_names = self._x.items
-
-        R = []
-        r = []
-
-        if isinstance(hypothesis, str):
-            eqs = hypothesis.split(',')
-        elif isinstance(hypothesis, list):
-            eqs = hypothesis
-        else:
-            raise Exception('hypothesis must be either string or list')
-        for equation in eqs:
-            row = np.zeros(len(x_names))
-            lhs, rhs = equation.split('=')
-            for s in lhs.split('+'):
-                ss = s.split('*')
-                coeff = float(ss[0])
-                x_name = ss[1]
-                idx = x_names.indexMap[x_name]
-                row[idx] = coeff
-            rhs = float(rhs)
-
-            R.append(row)
-            r.append(rhs)
-
-        R = np.array(R)
-        q = len(r)
-        r = np.array(r).reshape(q, 1)
-
-        result = math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
-                             self._nobs, self.df)
-
-        return common.f_stat_to_dict(result)
-
-    def _unstack_vector(self, vec, index=None):
-        if index is None:
-            index = self._y_trans.index
-        panel = LongPanel(vec.reshape((len(vec), 1)), ['dummy'],
-                          index=index)
-
-        return panel.to_wide()['dummy']
-
-    def _unstack_y(self, vec):
-        unstacked = self._unstack_vector(vec)
-        return unstacked.reindex(self.beta.index)
-
-    @cache_readonly
-    def _time_obs_count(self):
-        return self._y_trans.count()
-
-    @cache_readonly
-    def _time_has_obs(self):
-        return self._time_obs_count > 0
-
-    @property
-    def _nobs(self):
-        return len(self._y_trans_raw)
-
-def _convertDummies(dummies, mapping):
-    # cleans up the names of the generated dummies
-    new_items = []
-    for item in dummies.items:
-        if not mapping:
-            if isinstance(item, float):
-                var = '%g' % item
-            else:
-                var = '%s' % item
-
-            new_items.append(var)
-        else:
-            # renames the dummies if a conversion dict is provided
-            new_items.append(mapping[int(item)])
-
-    dummies = LongPanel(dummies.values, new_items, dummies.index)
-
-    return dummies
-
-def _is_numeric(df):
-    for col in df:
-        if df[col].dtype.name == 'object':
-            return False
-
-    return True
-
-def add_intercept(panel, name='intercept'):
-    """
-    Add column of ones to input panel
-
-    Parameters
-    ----------
-    panel: Panel (Long or Wide)
-    name: string, default 'intercept']
-
-    Returns
-    -------
-    New object (same type as input)
-    """
-    panel = panel.copy()
-    panel[name] = 1
-
-    return panel
-
-class MovingPanelOLS(MovingOLS, PanelOLS):
-    """Implements rolling/expanding panel OLS.
-
-    Parameters
-    ----------
-    y : DataFrame
-    x : Dict of DataFrame
-    intercept : bool
-        True if you want an intercept.
-    nw_lags : None or int
-        Number of Newey-West lags.
-    window_type : int
-        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
-    window : int
-        size of window (for rolling/expanding OLS)
-    min_periods : int
-        Minimum number of time periods to include in the window
-    min_obs : int
-        Minimum number of total observations to require. Default is
-        rank(X matrix) + 1. In some cases we might want to be able to
-        relax this number.
-    weights : DataFrame
-        Weight for each observation.  The weights are not normalized;
-        they're multiplied directly by each observation.
-    pool : bool
-        Whether to run pooled panel regression.  Defaults to true.
-    entity_effects : bool
-        Whether to account for entity fixed effects.  Defaults to false.
-    time_effects : bool
-        Whether to account for time fixed effects.  Defaults to false.
-    x_effects : list
-        List of x's to account for fixed effects.  Defaults to none.
-    dropped_dummies : dict
-        Key is the name of the variable for the fixed effect.
-        Value is the value of that variable for which we drop the dummy.
-
-        For entity fixed effects, key equals 'entity'.
-
-        By default, the first dummy is dropped if no dummy is specified.
-    cluster : int
-        ENTITY or TIME, indicating entity/time clustering
-        A cluster is a grouping within which observations are correlated.
-
-        For example, if you have a panel data with countries over time and you suspect that:
-
-        1. Countries are correlated - use 'time'
-        2. There is autocorrelation - use 'entity'
-    """
-    def __init__(self, y, x, weights=None,
-                 window_type='expanding', window=None,
-                 min_periods=None,
-                 min_obs=None,
-                 intercept=True,
-                 nw_lags=None, nw_overlap=False,
-                 entity_effects=False,
-                 time_effects=False,
-                 x_effects=None,
-                 cluster=None,
-                 dropped_dummies=None,
-                 verbose=False):
-
-        self._args = dict(weights=weights,
-                          intercept=intercept,
-                          nw_lags=nw_lags,
-                          nw_overlap=nw_overlap,
-                          entity_effects=entity_effects,
-                          time_effects=time_effects,
-                          x_effects=x_effects,
-                          cluster=cluster,
-                          dropped_dummies=dropped_dummies,
-                          verbose=verbose)
-
-        PanelOLS.__init__(self, y=y, x=x, **self._args)
-
-        self._set_window(window_type, window, min_periods)
-
-        if min_obs is None:
-            min_obs = len(self._x.items) + 1
-
-        self._min_obs = min_obs
-
-    @cache_readonly
-    def resid(self):
-        return self._unstack_y(self._resid_raw)
-
-    @cache_readonly
-    def y_fitted(self):
-        return self._unstack_y(self._y_fitted_raw)
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values."""
-        return self._unstack_y(self._y_predict_raw)
-
-    def lagged_y_predict(self, lag=1):
-        """
-        Compute forecast Y value lagging coefficient by input number
-        of time periods
-
-        Parameters
-        ----------
-        lag : int
-
-        Returns
-        -------
-        DataFrame
-        """
-        x = self._x.values
-        betas = self._beta_matrix(lag=lag)
-        return self._unstack_y((betas * x).sum(1))
-
-    @cache_readonly
-    def _rolling_ols_call(self):
-        return self._calc_betas(self._x_trans, self._y_trans)
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        df = self._rolling_rank()
-
-        if self._time_effects:
-            df += self._window_time_obs
-
-        return df[self._valid_indices]
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """Returns the raw covariance of beta."""
-        x = self._x
-        y = self._y
-
-        dates = x.index.major_axis
-
-        cluster_axis = None
-        if self._cluster == common.TIME:
-            cluster_axis = 0
-        elif self._cluster == common.ENTITY:
-            cluster_axis = 1
-
-        nobs = self._nobs
-        rmse = self._rmse_raw
-        beta = self._beta_raw
-        df = self._df_raw
-        window = self._window
-
-        if not self._time_effects:
-            # Non-transformed X
-            cum_xx = self._cum_xx(x)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            date = dates[i]
-
-            x_slice = x.truncate(prior_date, date)
-            y_slice = y.truncate(prior_date, date)
-
-            if self._time_effects:
-                xx = _xx_time_effects(x_slice, y_slice)
-            else:
-                xx = cum_xx[i]
-                if self._is_rolling and i >= window:
-                    xx = xx - cum_xx[i - window]
-
-            result = _var_beta_panel(y_slice, x_slice, beta[n], xx, rmse[n],
-                                    cluster_axis, self._nw_lags,
-                                    nobs[n], df[n], self._nw_overlap)
-
-            results.append(result)
-
-        return np.array(results)
-
-    @cache_readonly
-    def _resid_raw(self):
-        beta_matrix = self._beta_matrix(lag=0)
-
-        Y = self._y.values.squeeze()
-        X = self._x.values
-        resid = Y - (X * beta_matrix).sum(1)
-
-        return resid
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        x = self._x.values
-        betas = self._beta_matrix(lag=0)
-        return (betas * x).sum(1)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        x = self._x.values
-        betas = self._beta_matrix(lag=1)
-        return (betas * x).sum(1)
-
-    def _beta_matrix(self, lag=0):
-        assert(lag >= 0)
-
-        labels = self._y_trans.index.major_labels - lag
-        indexer = self._valid_indices.searchsorted(labels, side='left')
-
-        beta_matrix = self._beta_raw[indexer]
-        beta_matrix[labels < self._valid_indices[0]] = np.NaN
-
-        return beta_matrix
-
-    @cache_readonly
-    def _enough_obs(self):
-        # XXX: what's the best way to determine where to start?
-        # TODO: write unit tests for this
-
-        rank_threshold = len(self._x.items) + 1
-        if self._min_obs < rank_threshold:
-            warnings.warn('min_obs is smaller than rank of X matrix')
-
-        enough_observations = self._nobs_raw >= self._min_obs
-        enough_time_periods = self._window_time_obs >= self._min_periods
-        return enough_time_periods & enough_observations
-
-def create_ols_dict(attr):
-    def attr_getter(self):
-        d = {}
-        for k, v in self.results.iteritems():
-            result = getattr(v, attr)
-            d[k] = result
-
-        return d
-
-    return attr_getter
-
-def create_ols_attr(attr):
-    return property(create_ols_dict(attr))
-
-class NonPooledPanelOLS(object):
-    """Implements non-pooled panel OLS.
-
-    Parameters
-    ----------
-    y : DataFrame
-    x : Series, DataFrame, or dict of Series
-    intercept : bool
-        True if you want an intercept.
-    nw_lags : None or int
-        Number of Newey-West lags.
-    window_type : int
-        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
-    window : int
-        size of window (for rolling/expanding OLS)
-    """
-
-    ATTRIBUTES = [
-        'beta',
-        'df',
-        'df_model',
-        'df_resid',
-        'f_stat',
-        'p_value',
-        'r2',
-        'r2_adj',
-        'resid',
-        'rmse',
-        'std_err',
-        'summary_as_matrix',
-        't_stat',
-        'var_beta',
-        'x',
-        'y',
-        'y_fitted',
-        'y_predict'
-    ]
-
-    def __init__(self, y, x, window_type=common.FULL_SAMPLE, window=None,
-                 min_periods=None, intercept=True, nw_lags=None,
-                 nw_overlap=False):
-
-        for attr in self.ATTRIBUTES:
-            setattr(self.__class__, attr, create_ols_attr(attr))
-
-        results = {}
-
-        for entity in y:
-            entity_y = y[entity]
-
-            entity_x = {}
-            for x_var in x:
-                entity_x[x_var] = x[x_var][entity]
-
-            from pandas.stats.interface import ols
-            results[entity] = ols(y=entity_y,
-                                  x=entity_x,
-                                  window_type=window_type,
-                                  window=window,
-                                  min_periods=min_periods,
-                                  intercept=intercept,
-                                  nw_lags=nw_lags,
-                                  nw_overlap=nw_overlap)
-
-        self.results = results
-
-
-def _var_beta_panel(y, x, beta, xx, rmse, cluster_axis,
-                   nw_lags, nobs, df, nw_overlap):
-
-    from pandas.core.panel import LongPanel, group_agg
-
-    xx_inv = math.inv(xx)
-
-    if cluster_axis is None:
-        if nw_lags is None:
-            return xx_inv * (rmse ** 2)
-        else:
-            resid = y.values.squeeze() - np.dot(x.values, beta)
-            m = (x.values.T * resid).T
-
-            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)
-
-            return np.dot(xx_inv, np.dot(xeps, xx_inv))
-    else:
-        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))
-        resid = LongPanel(y.values - Xb, ['resid'], y.index)
-
-        if cluster_axis == 1:
-            x = x.swapaxes()
-            resid = resid.swapaxes()
-
-        m = group_agg(x.values * resid.values, x.index._bounds,
-                      lambda x: np.sum(x, axis=0))
-
-        if nw_lags is None:
-            nw_lags = 0
-
-        xox = 0
-        for i in range(len(x.major_axis)):
-            xox += math.newey_west(m[i : i + 1], nw_lags,
-                                   nobs, df, nw_overlap)
-
-        return np.dot(xx_inv, np.dot(xox, xx_inv))
-
-def _xx_time_effects(x, y):
-    """
-    Returns X'X - (X'T) (T'T)^-1 (T'X)
-    """
-    # X'X
-    xx = np.dot(x.values.T, x.values)
-    xt = x.sum('minor').values
-
-    count = y.count()
-    selector = count > 0
-
-    # X'X - (T'T)^-1 (T'X)
-    xt = xt[selector]
-    count = count[selector]
-
-    return xx - np.dot(xt.T / count, xt)
-
-
+"""
+Linear regression objects for panel data
+"""
+
+# pylint: disable-msg=W0231
+# pylint: disable-msg=E1101,E1103
+
+from __future__ import division
+import warnings
+
+import numpy as np
+
+from pandas.core.panel import WidePanel, LongPanel
+from pandas.core.frame import DataFrame
+from pandas.core.series import Series
+from pandas.core.sparse import SparseWidePanel
+from pandas.stats.ols import OLS, MovingOLS
+import pandas.stats.common as common
+import pandas.stats.math as math
+from pandas.util.decorators import cache_readonly
+
+class PanelOLS(OLS):
+    """Implements panel OLS.
+
+    Parameters
+    ----------
+    y : DataFrame
+    x : Dict of DataFrame or WidePanel
+    intercept : bool
+        True if you want an intercept.  True by default.
+    nw_lags : None or int
+        Number of Newey-West lags.  None by default.
+    nw_overlap : bool
+        Whether there are overlaps in the NW lags.  Defaults to False.
+    window_type : int
+        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
+    window : int
+        size of window (for rolling/expanding OLS)
+    weights : DataFrame
+        Weight for each observation.  The weights are not normalized;
+        they're multiplied directly by each observation.
+    pool : bool, default True
+        Whether to run pooled panel regression
+    entity_effects : bool, deafult False
+        Whether to account for entity fixed effects
+    time_effects : bool, default False
+        Whether to account for time fixed effects
+    x_effects : list, default None
+        List of x's to account for fixed effects
+    dropped_dummies : dict
+        Key is the name of the variable for the fixed effect.
+        Value is the value of that variable for which we drop the dummy.
+
+        For entity fixed effects, key equals 'entity', e.g. {'entity' : 'US'}
+
+        By default, the first item is dropped if one is not specified.
+    cluster : int
+        ENTITY or TIME, indicating entity/time clustering
+        A cluster is a grouping within which observations are correlated.
+
+        For example, if you have a panel data with countries over time and you
+        suspect that:
+
+        1. Countries are correlated - use 'time'
+        2. There is autocorrelation - use 'entity'
+
+    """
+    def __init__(self, y, x, weights=None,
+                 intercept=True, nw_lags=None, entity_effects=False,
+                 time_effects=False, x_effects=None, cluster=None,
+                 dropped_dummies=None, verbose=False, nw_overlap=False):
+        self._x_orig = x
+        self._y_orig = y
+        self._weights = weights
+        self._intercept = intercept
+        self._nw_lags = nw_lags
+        self._nw_overlap = nw_overlap
+        self._entity_effects = entity_effects
+        self._time_effects = time_effects
+        self._x_effects = x_effects
+        self._dropped_dummies = dropped_dummies or {}
+        self._cluster = common._get_cluster_type(cluster)
+        self._verbose = verbose
+
+        (self._x, self._x_trans,
+         self._x_filtered, self._y,
+         self._y_trans) = self._prepare_data()
+
+        self._x_trans_raw = self._x_trans.values
+        self._y_trans_raw = self._y_trans.values.squeeze()
+
+        self._index = self._y.major_axis
+
+        self._T = len(self._index)
+
+    def log(self, msg):
+        if self._verbose:
+            print msg
+
+    def _prepare_data(self):
+        """Cleans and converts input data into LongPanel classes.
+
+        If time effects is True, then we turn off intercepts and omit an item
+        from every (entity and x) fixed effect.
+
+        Otherwise:
+           - If we have an intercept, we omit an item from every fixed effect.
+           - Else, we omit an item from every fixed effect except one of them.
+
+        The categorical variables will get dropped from x.
+        """
+        (x, x_filtered, y, weights,
+         weights_filt, cat_mapping) = self._filter_data()
+
+        self.log('Adding dummies to X variables')
+        x = self._add_dummies(x, cat_mapping)
+
+        self.log('Adding dummies to filtered X variables')
+        x_filtered = self._add_dummies(x_filtered, cat_mapping)
+
+        if self._x_effects:
+            x = x.filter(x.items - self._x_effects)
+            x_filtered = x_filtered.filter(x_filtered.items - self._x_effects)
+
+        if self._time_effects:
+            x_regressor = x.subtract(x.mean('minor', broadcast=True))
+            y_regressor = y.subtract(y.mean('minor', broadcast=True))
+
+        elif self._intercept:
+            # only add intercept when no time effects
+            self.log('Adding intercept')
+            x = x_regressor = add_intercept(x)
+            x_filtered = add_intercept(x_filtered)
+            y_regressor = y
+        else:
+            self.log('No intercept added')
+            x_regressor = x
+            y_regressor = y
+
+        if weights is not None:
+            y_regressor = y_regressor.multiply(weights)
+            x_regressor = x_regressor.multiply(weights)
+
+        return x, x_regressor, x_filtered, y, y_regressor
+
+    def _filter_data(self):
+        """
+
+        """
+        data = self._x_orig
+        cat_mapping = {}
+
+        if isinstance(data, LongPanel):
+            data = data.to_wide()
+        else:
+            if isinstance(data, WidePanel):
+                data = data.copy()
+
+            if not isinstance(data, SparseWidePanel):
+                data, cat_mapping = self._convert_x(data)
+
+            if not isinstance(data, WidePanel):
+                data = WidePanel.from_dict(data, intersect=True)
+
+        x_names = data.items
+
+        if self._weights is not None:
+            data['__weights__'] = self._weights
+
+        # Filter x's without y (so we can make a prediction)
+        filtered = data.to_long()
+
+        # Filter all data together using to_long
+        data['__y__'] = self._y_orig
+        data_long = data.to_long()
+
+        x_filt = filtered.filter(x_names)
+
+        if self._weights:
+            weights_filt = filtered['__weights__']
+        else:
+            weights_filt = None
+
+        x = data_long.filter(x_names)
+        y = data_long['__y__']
+
+        if self._weights:
+            weights = data_long['__weights__']
+        else:
+            weights = None
+
+        return x, x_filt, y, weights, weights_filt, cat_mapping
+
+    def _convert_x(self, x):
+
+        # Converts non-numeric data in x to floats. x_converted is the
+        # DataFrame with converted values, and x_conversion is a dict that
+        # provides the reverse mapping.  For example, if 'A' was converted to 0
+        # for x named 'variety', then x_conversion['variety'][0] is 'A'.
+        x_converted = {}
+        cat_mapping = {}
+        for key, df in x.iteritems():
+            if not isinstance(df, DataFrame):
+                raise TypeError('Input X data set contained an object of '
+                                'type %s' % type(df))
+
+            if _is_numeric(df):
+                x_converted[key] = df
+            else:
+                values = df.values
+                distinct_values = sorted(set(values.flat))
+                cat_mapping[key] = dict(enumerate(distinct_values))
+                new_values = np.searchsorted(distinct_values, values)
+                x_converted[key] = DataFrame(new_values, index=df.index,
+                                              columns=df.columns)
+
+        if len(cat_mapping) == 0:
+            x_converted = x
+
+        return x_converted, cat_mapping
+
+    def _add_dummies(self, panel, mapping):
+        """
+        Add entity and / or categorical dummies to input X LongPanel
+
+        Returns
+        -------
+        LongPanel
+        """
+        panel = self._add_entity_effects(panel)
+        panel = self._add_categorical_dummies(panel, mapping)
+
+        return panel
+
+    def _add_entity_effects(self, panel):
+        """
+        Add entity dummies to panel
+
+        Returns
+        -------
+        LongPanel
+        """
+        if not self._entity_effects:
+            return panel
+
+        self.log('-- Adding entity fixed effect dummies')
+
+        dummies = panel.get_axis_dummies(axis='minor')
+
+        if not self._use_all_dummies:
+            if 'entity' in self._dropped_dummies:
+                to_exclude = str(self._dropped_dummies.get('entity'))
+            else:
+                to_exclude = dummies.items[0]
+
+            if to_exclude not in dummies.items:
+                raise Exception('%s not in %s' % (to_exclude,
+                                                  dummies.items))
+
+            self.log('-- Excluding dummy for entity: %s' % to_exclude)
+
+            dummies = dummies.filter(dummies.items - [to_exclude])
+
+        dummies = dummies.addPrefix('FE_')
+        panel = panel.leftJoin(dummies)
+
+        return panel
+
+    def _add_categorical_dummies(self, panel, cat_mappings):
+        """
+        Add categorical dummies to panel
+
+        Returns
+        -------
+        LongPanel
+        """
+        if not self._x_effects:
+            return panel
+
+        dropped_dummy = (self._entity_effects and not self._use_all_dummies)
+
+        for effect in self._x_effects:
+            self.log('-- Adding fixed effect dummies for %s' % effect)
+
+            dummies = panel.get_dummies(effect)
+
+            val_map = cat_mappings.get(effect)
+            if val_map:
+                val_map = dict((v, k) for k, v in val_map.iteritems())
+
+            if dropped_dummy or not self._use_all_dummies:
+                if effect in self._dropped_dummies:
+                    to_exclude = mapped_name = self._dropped_dummies.get(effect)
+
+                    if val_map:
+                        mapped_name = val_map[to_exclude]
+                else:
+                    to_exclude = mapped_name = dummies.items[0]
+
+                if mapped_name not in dummies.items:
+                    raise Exception('%s not in %s' % (to_exclude,
+                                                      dummies.items))
+
+                self.log('-- Excluding dummy for %s: %s' % (effect, to_exclude))
+
+                dummies = dummies.filter(dummies.items - [mapped_name])
+                dropped_dummy = True
+
+            dummies = _convertDummies(dummies, cat_mappings.get(effect))
+            dummies = dummies.addPrefix('%s_' % effect)
+            panel = panel.leftJoin(dummies)
+
+        return panel
+
+    @property
+    def _use_all_dummies(self):
+        """
+        In the case of using an intercept or including time fixed
+        effects, completely partitioning the sample would make the X
+        not full rank.
+        """
+        return (not self._intercept and not self._time_effects)
+
+    @cache_readonly
+    def _beta_raw(self):
+        """Runs the regression and returns the beta."""
+        X = self._x_trans_raw
+        Y = self._y_trans_raw
+
+        beta, _, _, _ = np.linalg.lstsq(X, Y)
+
+        return beta
+
+    @cache_readonly
+    def beta(self):
+        return Series(self._beta_raw, index=self._x.items)
+
+    @cache_readonly
+    def _weighted_x(self):
+        if self._weights:
+            return self._x.multiply(self._weights)
+        return self._x
+
+    @cache_readonly
+    def _weighted_y(self):
+        if self._weights:
+            return self._y.multiply(self._weights)
+
+        return self._y
+
+    @cache_readonly
+    def _df_model_raw(self):
+        """Returns the raw model degrees of freedom."""
+        return self._df_raw - 1
+
+    @cache_readonly
+    def _df_resid_raw(self):
+        """Returns the raw residual degrees of freedom."""
+        return self._nobs - self._df_raw
+
+    @cache_readonly
+    def _df_raw(self):
+        """Returns the degrees of freedom."""
+        df = math.rank(self._x_trans_raw)
+        if self._time_effects:
+            df += self._total_times
+
+        return df
+
+    @cache_readonly
+    def _r2_raw(self):
+        Y = self._y.values.squeeze()
+        X = self._x.values
+
+        resid = Y - np.dot(X, self._beta_raw)
+
+        SSE = (resid ** 2).sum()
+        SST = ((Y - np.mean(Y)) ** 2).sum()
+
+        return 1 - SSE / SST
+
+    @cache_readonly
+    def _r2_adj_raw(self):
+        """Returns the raw r-squared adjusted values."""
+        nobs = self._nobs
+        factors = (nobs - 1) / (nobs - self._df_raw)
+        return 1 - (1 - self._r2_raw) * factors
+
+    @cache_readonly
+    def _resid_raw(self):
+        Y = self._y.values.squeeze()
+        X = self._x.values
+        return Y - np.dot(X, self._beta_raw)
+
+    @cache_readonly
+    def resid(self):
+        return self._unstack_vector(self._resid_raw)
+
+    @cache_readonly
+    def _rmse_raw(self):
+        """Returns the raw rmse values."""
+        X = self._x.values
+        Y = self._y.values.squeeze()
+
+        resid = Y - np.dot(X, self._beta_raw)
+        ss = (resid ** 2).sum()
+        return np.sqrt(ss / (self._nobs - self._df_raw))
+
+    @cache_readonly
+    def _var_beta_raw(self):
+        cluster_axis = None
+        if self._cluster == common.TIME:
+            cluster_axis = 0
+        elif self._cluster == common.ENTITY:
+            cluster_axis = 1
+
+        x = self._x
+        y = self._y
+
+        if self._time_effects:
+            xx = _xx_time_effects(x, y)
+        else:
+            xx = np.dot(x.values.T, x.values)
+
+        return _var_beta_panel(y, x, self._beta_raw, xx,
+                               self._rmse_raw, cluster_axis, self._nw_lags,
+                               self._nobs, self._df_raw, self._nw_overlap)
+
+    @cache_readonly
+    def _y_fitted_raw(self):
+        """Returns the raw fitted y values."""
+        return np.dot(self._x_filtered.values, self._beta_raw)
+
+    @cache_readonly
+    def y_fitted(self):
+        return self._unstack_vector(self._y_fitted_raw,
+                                    index=self._x_filtered.index)
+
+    def f_test(self, hypothesis):
+        """Runs the F test, given a joint hypothesis.  The hypothesis is
+        represented by a collection of equations, in the form
+
+        A*x_1+B*x_2=C
+
+        You must provide the coefficients even if they're 1.  No spaces.
+
+        The equations can be passed as either a single string or a
+        list of strings.
+
+        Examples:
+        o = ols(...)
+        o.f_test('1*x1+2*x2=0,1*x3=0')
+        o.f_test(['1*x1+2*x2=0','1*x3=0'])
+        """
+
+        x_names = self._x.items
+
+        R = []
+        r = []
+
+        if isinstance(hypothesis, str):
+            eqs = hypothesis.split(',')
+        elif isinstance(hypothesis, list):
+            eqs = hypothesis
+        else:
+            raise Exception('hypothesis must be either string or list')
+        for equation in eqs:
+            row = np.zeros(len(x_names))
+            lhs, rhs = equation.split('=')
+            for s in lhs.split('+'):
+                ss = s.split('*')
+                coeff = float(ss[0])
+                x_name = ss[1]
+                idx = x_names.indexMap[x_name]
+                row[idx] = coeff
+            rhs = float(rhs)
+
+            R.append(row)
+            r.append(rhs)
+
+        R = np.array(R)
+        q = len(r)
+        r = np.array(r).reshape(q, 1)
+
+        result = math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
+                             self._nobs, self.df)
+
+        return common.f_stat_to_dict(result)
+
+    def _unstack_vector(self, vec, index=None):
+        if index is None:
+            index = self._y_trans.index
+        panel = LongPanel(vec.reshape((len(vec), 1)), ['dummy'],
+                          index=index)
+
+        return panel.to_wide()['dummy']
+
+    def _unstack_y(self, vec):
+        unstacked = self._unstack_vector(vec)
+        return unstacked.reindex(self.beta.index)
+
+    @cache_readonly
+    def _time_obs_count(self):
+        return self._y_trans.count()
+
+    @cache_readonly
+    def _time_has_obs(self):
+        return self._time_obs_count > 0
+
+    @property
+    def _nobs(self):
+        return len(self._y_trans_raw)
+
+def _convertDummies(dummies, mapping):
+    # cleans up the names of the generated dummies
+    new_items = []
+    for item in dummies.items:
+        if not mapping:
+            if isinstance(item, float):
+                var = '%g' % item
+            else:
+                var = '%s' % item
+
+            new_items.append(var)
+        else:
+            # renames the dummies if a conversion dict is provided
+            new_items.append(mapping[int(item)])
+
+    dummies = LongPanel(dummies.values, new_items, dummies.index)
+
+    return dummies
+
+def _is_numeric(df):
+    for col in df:
+        if df[col].dtype.name == 'object':
+            return False
+
+    return True
+
+def add_intercept(panel, name='intercept'):
+    """
+    Add column of ones to input panel
+
+    Parameters
+    ----------
+    panel: Panel (Long or Wide)
+    name: string, default 'intercept']
+
+    Returns
+    -------
+    New object (same type as input)
+    """
+    panel = panel.copy()
+    panel[name] = 1
+
+    return panel
+
+class MovingPanelOLS(MovingOLS, PanelOLS):
+    """Implements rolling/expanding panel OLS.
+
+    Parameters
+    ----------
+    y : DataFrame
+    x : Dict of DataFrame
+    intercept : bool
+        True if you want an intercept.
+    nw_lags : None or int
+        Number of Newey-West lags.
+    window_type : int
+        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
+    window : int
+        size of window (for rolling/expanding OLS)
+    min_periods : int
+        Minimum number of time periods to include in the window
+    min_obs : int
+        Minimum number of total observations to require. Default is
+        rank(X matrix) + 1. In some cases we might want to be able to
+        relax this number.
+    weights : DataFrame
+        Weight for each observation.  The weights are not normalized;
+        they're multiplied directly by each observation.
+    pool : bool
+        Whether to run pooled panel regression.  Defaults to true.
+    entity_effects : bool
+        Whether to account for entity fixed effects.  Defaults to false.
+    time_effects : bool
+        Whether to account for time fixed effects.  Defaults to false.
+    x_effects : list
+        List of x's to account for fixed effects.  Defaults to none.
+    dropped_dummies : dict
+        Key is the name of the variable for the fixed effect.
+        Value is the value of that variable for which we drop the dummy.
+
+        For entity fixed effects, key equals 'entity'.
+
+        By default, the first dummy is dropped if no dummy is specified.
+    cluster : int
+        ENTITY or TIME, indicating entity/time clustering
+        A cluster is a grouping within which observations are correlated.
+
+        For example, if you have a panel data with countries over time and you suspect that:
+
+        1. Countries are correlated - use 'time'
+        2. There is autocorrelation - use 'entity'
+    """
+    def __init__(self, y, x, weights=None,
+                 window_type='expanding', window=None,
+                 min_periods=None,
+                 min_obs=None,
+                 intercept=True,
+                 nw_lags=None, nw_overlap=False,
+                 entity_effects=False,
+                 time_effects=False,
+                 x_effects=None,
+                 cluster=None,
+                 dropped_dummies=None,
+                 verbose=False):
+
+        self._args = dict(weights=weights,
+                          intercept=intercept,
+                          nw_lags=nw_lags,
+                          nw_overlap=nw_overlap,
+                          entity_effects=entity_effects,
+                          time_effects=time_effects,
+                          x_effects=x_effects,
+                          cluster=cluster,
+                          dropped_dummies=dropped_dummies,
+                          verbose=verbose)
+
+        PanelOLS.__init__(self, y=y, x=x, **self._args)
+
+        self._set_window(window_type, window, min_periods)
+
+        if min_obs is None:
+            min_obs = len(self._x.items) + 1
+
+        self._min_obs = min_obs
+
+    @cache_readonly
+    def resid(self):
+        return self._unstack_y(self._resid_raw)
+
+    @cache_readonly
+    def y_fitted(self):
+        return self._unstack_y(self._y_fitted_raw)
+
+    @cache_readonly
+    def y_predict(self):
+        """Returns the predicted y values."""
+        return self._unstack_y(self._y_predict_raw)
+
+    def lagged_y_predict(self, lag=1):
+        """
+        Compute forecast Y value lagging coefficient by input number
+        of time periods
+
+        Parameters
+        ----------
+        lag : int
+
+        Returns
+        -------
+        DataFrame
+        """
+        x = self._x.values
+        betas = self._beta_matrix(lag=lag)
+        return self._unstack_y((betas * x).sum(1))
+
+    @cache_readonly
+    def _rolling_ols_call(self):
+        return self._calc_betas(self._x_trans, self._y_trans)
+
+    @cache_readonly
+    def _df_raw(self):
+        """Returns the degrees of freedom."""
+        df = self._rolling_rank()
+
+        if self._time_effects:
+            df += self._window_time_obs
+
+        return df[self._valid_indices]
+
+    @cache_readonly
+    def _var_beta_raw(self):
+        """Returns the raw covariance of beta."""
+        x = self._x
+        y = self._y
+
+        dates = x.index.major_axis
+
+        cluster_axis = None
+        if self._cluster == common.TIME:
+            cluster_axis = 0
+        elif self._cluster == common.ENTITY:
+            cluster_axis = 1
+
+        nobs = self._nobs
+        rmse = self._rmse_raw
+        beta = self._beta_raw
+        df = self._df_raw
+        window = self._window
+
+        if not self._time_effects:
+            # Non-transformed X
+            cum_xx = self._cum_xx(x)
+
+        results = []
+        for n, i in enumerate(self._valid_indices):
+            if self._is_rolling and i >= window:
+                prior_date = dates[i - window + 1]
+            else:
+                prior_date = dates[0]
+
+            date = dates[i]
+
+            x_slice = x.truncate(prior_date, date)
+            y_slice = y.truncate(prior_date, date)
+
+            if self._time_effects:
+                xx = _xx_time_effects(x_slice, y_slice)
+            else:
+                xx = cum_xx[i]
+                if self._is_rolling and i >= window:
+                    xx = xx - cum_xx[i - window]
+
+            result = _var_beta_panel(y_slice, x_slice, beta[n], xx, rmse[n],
+                                    cluster_axis, self._nw_lags,
+                                    nobs[n], df[n], self._nw_overlap)
+
+            results.append(result)
+
+        return np.array(results)
+
+    @cache_readonly
+    def _resid_raw(self):
+        beta_matrix = self._beta_matrix(lag=0)
+
+        Y = self._y.values.squeeze()
+        X = self._x.values
+        resid = Y - (X * beta_matrix).sum(1)
+
+        return resid
+
+    @cache_readonly
+    def _y_fitted_raw(self):
+        x = self._x.values
+        betas = self._beta_matrix(lag=0)
+        return (betas * x).sum(1)
+
+    @cache_readonly
+    def _y_predict_raw(self):
+        """Returns the raw predicted y values."""
+        x = self._x.values
+        betas = self._beta_matrix(lag=1)
+        return (betas * x).sum(1)
+
+    def _beta_matrix(self, lag=0):
+        assert(lag >= 0)
+
+        labels = self._y_trans.index.major_labels - lag
+        indexer = self._valid_indices.searchsorted(labels, side='left')
+
+        beta_matrix = self._beta_raw[indexer]
+        beta_matrix[labels < self._valid_indices[0]] = np.NaN
+
+        return beta_matrix
+
+    @cache_readonly
+    def _enough_obs(self):
+        # XXX: what's the best way to determine where to start?
+        # TODO: write unit tests for this
+
+        rank_threshold = len(self._x.items) + 1
+        if self._min_obs < rank_threshold:
+            warnings.warn('min_obs is smaller than rank of X matrix')
+
+        enough_observations = self._nobs_raw >= self._min_obs
+        enough_time_periods = self._window_time_obs >= self._min_periods
+        return enough_time_periods & enough_observations
+
+def create_ols_dict(attr):
+    def attr_getter(self):
+        d = {}
+        for k, v in self.results.iteritems():
+            result = getattr(v, attr)
+            d[k] = result
+
+        return d
+
+    return attr_getter
+
+def create_ols_attr(attr):
+    return property(create_ols_dict(attr))
+
+class NonPooledPanelOLS(object):
+    """Implements non-pooled panel OLS.
+
+    Parameters
+    ----------
+    y : DataFrame
+    x : Series, DataFrame, or dict of Series
+    intercept : bool
+        True if you want an intercept.
+    nw_lags : None or int
+        Number of Newey-West lags.
+    window_type : int
+        FULL_SAMPLE, ROLLING, EXPANDING.  FULL_SAMPLE by default.
+    window : int
+        size of window (for rolling/expanding OLS)
+    """
+
+    ATTRIBUTES = [
+        'beta',
+        'df',
+        'df_model',
+        'df_resid',
+        'f_stat',
+        'p_value',
+        'r2',
+        'r2_adj',
+        'resid',
+        'rmse',
+        'std_err',
+        'summary_as_matrix',
+        't_stat',
+        'var_beta',
+        'x',
+        'y',
+        'y_fitted',
+        'y_predict'
+    ]
+
+    def __init__(self, y, x, window_type=common.FULL_SAMPLE, window=None,
+                 min_periods=None, intercept=True, nw_lags=None,
+                 nw_overlap=False):
+
+        for attr in self.ATTRIBUTES:
+            setattr(self.__class__, attr, create_ols_attr(attr))
+
+        results = {}
+
+        for entity in y:
+            entity_y = y[entity]
+
+            entity_x = {}
+            for x_var in x:
+                entity_x[x_var] = x[x_var][entity]
+
+            from pandas.stats.interface import ols
+            results[entity] = ols(y=entity_y,
+                                  x=entity_x,
+                                  window_type=window_type,
+                                  window=window,
+                                  min_periods=min_periods,
+                                  intercept=intercept,
+                                  nw_lags=nw_lags,
+                                  nw_overlap=nw_overlap)
+
+        self.results = results
+
+
+def _var_beta_panel(y, x, beta, xx, rmse, cluster_axis,
+                   nw_lags, nobs, df, nw_overlap):
+
+    from pandas.core.panel import LongPanel, group_agg
+
+    xx_inv = math.inv(xx)
+
+    if cluster_axis is None:
+        if nw_lags is None:
+            return xx_inv * (rmse ** 2)
+        else:
+            resid = y.values.squeeze() - np.dot(x.values, beta)
+            m = (x.values.T * resid).T
+
+            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)
+
+            return np.dot(xx_inv, np.dot(xeps, xx_inv))
+    else:
+        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))
+        resid = LongPanel(y.values - Xb, ['resid'], y.index)
+
+        if cluster_axis == 1:
+            x = x.swapaxes()
+            resid = resid.swapaxes()
+
+        m = group_agg(x.values * resid.values, x.index._bounds,
+                      lambda x: np.sum(x, axis=0))
+
+        if nw_lags is None:
+            nw_lags = 0
+
+        xox = 0
+        for i in range(len(x.major_axis)):
+            xox += math.newey_west(m[i : i + 1], nw_lags,
+                                   nobs, df, nw_overlap)
+
+        return np.dot(xx_inv, np.dot(xox, xx_inv))
+
+def _xx_time_effects(x, y):
+    """
+    Returns X'X - (X'T) (T'T)^-1 (T'X)
+    """
+    # X'X
+    xx = np.dot(x.values.T, x.values)
+    xt = x.sum('minor').values
+
+    count = y.count()
+    selector = count > 0
+
+    # X'X - (T'T)^-1 (T'X)
+    xt = xt[selector]
+    count = count[selector]
+
+    return xx - np.dot(xt.T / count, xt)
+
+
diff --git a/pandas/stats/tests/common.py b/pandas/stats/tests/common.py
index 593351811..589572cc3 100644
--- a/pandas/stats/tests/common.py
+++ b/pandas/stats/tests/common.py
@@ -1,152 +1,152 @@
-# pylint: disable-msg=W0611,W0402
-
-from datetime import datetime
-import string
-import unittest
-import nose
-
-import numpy as np
-
-from pandas.core.api import DataFrame, DateRange
-from pandas.util.testing import assert_almost_equal # imported in other tests
-N = 100
-K = 4
-
-start = datetime(2007, 1, 1)
-DATE_RANGE = DateRange(start, periods=N)
-
-COLS = ['Col' + c for c in string.ascii_uppercase[:K]]
-
-def makeDataFrame():
-    data = DataFrame(np.random.randn(N, K),
-                      columns=COLS,
-                      index=DATE_RANGE)
-
-    return data
-
-def getBasicDatasets():
-    A = makeDataFrame()
-    B = makeDataFrame()
-    C = makeDataFrame()
-
-    return A, B, C
-
-def check_for_scipy():
-    try:
-        import scipy
-    except ImportError:
-        raise nose.SkipTest('no scipy')
-
-def check_for_statsmodels():
-    try:
-        import scikits.statsmodels as sm
-    except Exception:
-        raise nose.SkipTest('no statsmodels')
-
-
-class BaseTest(unittest.TestCase):
-    def setUp(self):
-        check_for_scipy()
-        check_for_statsmodels()
-
-
-        self.A, self.B, self.C = getBasicDatasets()
-
-        self.createData1()
-        self.createData2()
-        self.createData3()
-
-    def createData1(self):
-        date = datetime(2007, 1, 1)
-        date2 = datetime(2007, 1, 15)
-        date3 = datetime(2007, 1, 22)
-
-        A = self.A.copy()
-        B = self.B.copy()
-        C = self.C.copy()
-
-        A['ColA'][date] = np.NaN
-        B['ColA'][date] = np.NaN
-        C['ColA'][date] = np.NaN
-        C['ColA'][date2] = np.NaN
-
-        # truncate data to save time
-        A = A[:30]
-        B = B[:30]
-        C = C[:30]
-
-        self.panel_y = A
-        self.panel_x = {'B' : B, 'C' : C}
-
-        self.series_panel_y = A.filter(['ColA'])
-        self.series_panel_x = {'B' : B.filter(['ColA']),
-                               'C' : C.filter(['ColA'])}
-        self.series_y = A['ColA']
-        self.series_x = {'B' : B['ColA'],
-                         'C' : C['ColA']}
-
-    def createData2(self):
-        y_data = [[1, np.NaN],
-                  [2, 3],
-                  [4, 5]]
-        y_index = [datetime(2000, 1, 1),
-                   datetime(2000, 1, 2),
-                   datetime(2000, 1, 3)]
-        y_cols = ['A', 'B']
-        self.panel_y2 = DataFrame(np.array(y_data), index=y_index,
-                                   columns=y_cols)
-
-        x1_data = [[6, np.NaN],
-                   [7, 8],
-                   [9, 30],
-                   [11, 12]]
-        x1_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2),
-                    datetime(2000, 1, 3),
-                    datetime(2000, 1, 4)]
-        x1_cols = ['A', 'B']
-        x1 = DataFrame(np.array(x1_data), index=x1_index,
-                        columns=x1_cols)
-
-        x2_data = [[13, 14, np.NaN],
-                   [15, np.NaN, np.NaN],
-                   [16, 17, 48],
-                   [19, 20, 21],
-                   [22, 23, 24]]
-        x2_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2),
-                    datetime(2000, 1, 3),
-                    datetime(2000, 1, 4),
-                    datetime(2000, 1, 5)]
-        x2_cols = ['C', 'A', 'B']
-        x2 = DataFrame(np.array(x2_data), index=x2_index,
-                        columns=x2_cols)
-
-        self.panel_x2 = {'x1' : x1, 'x2' : x2}
-
-    def createData3(self):
-        y_data = [[1, 2],
-                  [3, 4]]
-        y_index = [datetime(2000, 1, 1),
-                   datetime(2000, 1, 2)]
-        y_cols = ['A', 'B']
-        self.panel_y3 = DataFrame(np.array(y_data), index=y_index,
-                                   columns=y_cols)
-
-        x1_data = [['A', 'B'],
-                   ['C', 'A']]
-        x1_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2)]
-        x1_cols = ['A', 'B']
-        x1 = DataFrame(np.array(x1_data), index=x1_index,
-                        columns=x1_cols)
-
-        x2_data = [['3.14', '1.59'],
-                   ['2.65', '3.14']]
-        x2_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2)]
-        x2_cols = ['A', 'B']
-        x2 = DataFrame(np.array(x2_data), index=x2_index,
-                        columns=x2_cols)
-
-        self.panel_x3 = {'x1' : x1, 'x2' : x2}
+# pylint: disable-msg=W0611,W0402
+
+from datetime import datetime
+import string
+import unittest
+import nose
+
+import numpy as np
+
+from pandas.core.api import DataFrame, DateRange
+from pandas.util.testing import assert_almost_equal # imported in other tests
+N = 100
+K = 4
+
+start = datetime(2007, 1, 1)
+DATE_RANGE = DateRange(start, periods=N)
+
+COLS = ['Col' + c for c in string.ascii_uppercase[:K]]
+
+def makeDataFrame():
+    data = DataFrame(np.random.randn(N, K),
+                      columns=COLS,
+                      index=DATE_RANGE)
+
+    return data
+
+def getBasicDatasets():
+    A = makeDataFrame()
+    B = makeDataFrame()
+    C = makeDataFrame()
+
+    return A, B, C
+
+def check_for_scipy():
+    try:
+        import scipy
+    except ImportError:
+        raise nose.SkipTest('no scipy')
+
+def check_for_statsmodels():
+    try:
+        import scikits.statsmodels as sm
+    except Exception:
+        raise nose.SkipTest('no statsmodels')
+
+
+class BaseTest(unittest.TestCase):
+    def setUp(self):
+        check_for_scipy()
+        check_for_statsmodels()
+
+
+        self.A, self.B, self.C = getBasicDatasets()
+
+        self.createData1()
+        self.createData2()
+        self.createData3()
+
+    def createData1(self):
+        date = datetime(2007, 1, 1)
+        date2 = datetime(2007, 1, 15)
+        date3 = datetime(2007, 1, 22)
+
+        A = self.A.copy()
+        B = self.B.copy()
+        C = self.C.copy()
+
+        A['ColA'][date] = np.NaN
+        B['ColA'][date] = np.NaN
+        C['ColA'][date] = np.NaN
+        C['ColA'][date2] = np.NaN
+
+        # truncate data to save time
+        A = A[:30]
+        B = B[:30]
+        C = C[:30]
+
+        self.panel_y = A
+        self.panel_x = {'B' : B, 'C' : C}
+
+        self.series_panel_y = A.filter(['ColA'])
+        self.series_panel_x = {'B' : B.filter(['ColA']),
+                               'C' : C.filter(['ColA'])}
+        self.series_y = A['ColA']
+        self.series_x = {'B' : B['ColA'],
+                         'C' : C['ColA']}
+
+    def createData2(self):
+        y_data = [[1, np.NaN],
+                  [2, 3],
+                  [4, 5]]
+        y_index = [datetime(2000, 1, 1),
+                   datetime(2000, 1, 2),
+                   datetime(2000, 1, 3)]
+        y_cols = ['A', 'B']
+        self.panel_y2 = DataFrame(np.array(y_data), index=y_index,
+                                   columns=y_cols)
+
+        x1_data = [[6, np.NaN],
+                   [7, 8],
+                   [9, 30],
+                   [11, 12]]
+        x1_index = [datetime(2000, 1, 1),
+                    datetime(2000, 1, 2),
+                    datetime(2000, 1, 3),
+                    datetime(2000, 1, 4)]
+        x1_cols = ['A', 'B']
+        x1 = DataFrame(np.array(x1_data), index=x1_index,
+                        columns=x1_cols)
+
+        x2_data = [[13, 14, np.NaN],
+                   [15, np.NaN, np.NaN],
+                   [16, 17, 48],
+                   [19, 20, 21],
+                   [22, 23, 24]]
+        x2_index = [datetime(2000, 1, 1),
+                    datetime(2000, 1, 2),
+                    datetime(2000, 1, 3),
+                    datetime(2000, 1, 4),
+                    datetime(2000, 1, 5)]
+        x2_cols = ['C', 'A', 'B']
+        x2 = DataFrame(np.array(x2_data), index=x2_index,
+                        columns=x2_cols)
+
+        self.panel_x2 = {'x1' : x1, 'x2' : x2}
+
+    def createData3(self):
+        y_data = [[1, 2],
+                  [3, 4]]
+        y_index = [datetime(2000, 1, 1),
+                   datetime(2000, 1, 2)]
+        y_cols = ['A', 'B']
+        self.panel_y3 = DataFrame(np.array(y_data), index=y_index,
+                                   columns=y_cols)
+
+        x1_data = [['A', 'B'],
+                   ['C', 'A']]
+        x1_index = [datetime(2000, 1, 1),
+                    datetime(2000, 1, 2)]
+        x1_cols = ['A', 'B']
+        x1 = DataFrame(np.array(x1_data), index=x1_index,
+                        columns=x1_cols)
+
+        x2_data = [['3.14', '1.59'],
+                   ['2.65', '3.14']]
+        x2_index = [datetime(2000, 1, 1),
+                    datetime(2000, 1, 2)]
+        x2_cols = ['A', 'B']
+        x2 = DataFrame(np.array(x2_data), index=x2_index,
+                        columns=x2_cols)
+
+        self.panel_x3 = {'x1' : x1, 'x2' : x2}
diff --git a/pandas/stats/tests/test_ols.py b/pandas/stats/tests/test_ols.py
index d4291441b..eb68a94ea 100644
--- a/pandas/stats/tests/test_ols.py
+++ b/pandas/stats/tests/test_ols.py
@@ -1,537 +1,537 @@
-"""
-Unit test suite for OLS and PanelOLS classes
-"""
-
-# pylint: disable-msg=W0212
-
-from __future__ import division
-
-from datetime import datetime
-import unittest
-import numpy as np
-
-from pandas.core.panel import LongPanel
-from pandas.core.api import DataFrame, Index, Series
-from pandas.stats.api import ols
-from pandas.stats.plm import NonPooledPanelOLS
-from pandas.util.testing import (assert_almost_equal, assert_series_equal,
-                                 assert_frame_equal)
-import pandas.util.testing as testing
-
-from common import BaseTest
-
-def _check_repr(obj):
-    repr(obj)
-    str(obj)
-
-def _compare_ols_results(model1, model2):
-    assert(type(model1) == type(model2))
-
-    if hasattr(model1, '_window_type'):
-        _compare_moving_ols(model1, model2)
-    else:
-        _compare_fullsample_ols(model1, model2)
-
-def _compare_fullsample_ols(model1, model2):
-    assert_series_equal(model1.beta, model2.beta)
-
-def _compare_moving_ols(model1, model2):
-    assert_frame_equal(model1.beta, model2.beta)
-
-class TestOLS(BaseTest):
-
-    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat', 'p_value',
-              'r2', 'r2_adj', 'rmse', 'std_err', 't_stat',
-              'var_beta']
-
-    # TODO: Add tests for OLS y predict
-    # TODO: Right now we just check for consistency between full-sample and
-    # rolling/expanding results of the panel OLS.  We should also cross-check
-    # with trusted implementations of panel OLS (e.g. R).
-    # TODO: Add tests for non pooled OLS.
-
-    def testOLSWithDatasets(self):
-        import scikits.statsmodels.datasets as datasets
-
-        self.checkDataSet(datasets.ccard.load(), skip_moving=True)
-        self.checkDataSet(datasets.cpunish.load(), skip_moving=True)
-        self.checkDataSet(datasets.longley.load(), skip_moving=True)
-        self.checkDataSet(datasets.stackloss.load(), skip_moving=True)
-        self.checkDataSet(datasets.ccard.load(), 39, 49) # one col in X all 0s
-        self.checkDataSet(datasets.copper.load())
-        self.checkDataSet(datasets.scotland.load())
-
-    def checkDataSet(self, dataset, start=None, end=None, skip_moving=False):
-        exog = dataset.exog[start : end]
-        endog = dataset.endog[start : end]
-        x = DataFrame(exog, index=np.arange(exog.shape[0]),
-                      columns=np.arange(exog.shape[1]))
-        y = Series(endog, index=np.arange(len(endog)))
-
-        self.checkOLS(exog, endog, x, y)
-
-        if not skip_moving:
-            self.checkMovingOLS('rolling', x, y)
-            self.checkMovingOLS('rolling', x, y, nw_lags=0)
-            self.checkMovingOLS('expanding', x, y, nw_lags=0)
-            self.checkMovingOLS('rolling', x, y, nw_lags=1)
-            self.checkMovingOLS('expanding', x, y, nw_lags=1)
-            self.checkMovingOLS('expanding', x, y, nw_lags=1, nw_overlap=True)
-
-    def checkOLS(self, exog, endog, x, y):
-
-        try:
-            import scikits.statsmodels.api as sm
-        except ImportError:
-            import scikits.statsmodels as sm
-
-        reference = sm.OLS(endog, sm.add_constant(exog)).fit()
-        result = ols(y=y, x=x)
-
-        # check that sparse version is the same
-        sparse_result = ols(y=y.to_sparse(), x=x.to_sparse())
-        _compare_ols_results(result, sparse_result)
-
-        assert_almost_equal(reference.params, result._beta_raw)
-        assert_almost_equal(reference.df_model, result._df_model_raw)
-        assert_almost_equal(reference.df_resid, result._df_resid_raw)
-        assert_almost_equal(reference.fvalue, result._f_stat_raw[0])
-        assert_almost_equal(reference.pvalues, result._p_value_raw)
-        assert_almost_equal(reference.rsquared, result._r2_raw)
-        assert_almost_equal(reference.rsquared_adj, result._r2_adj_raw)
-        assert_almost_equal(reference.resid, result._resid_raw)
-        assert_almost_equal(reference.bse, result._std_err_raw)
-        assert_almost_equal(reference.tvalues, result._t_stat_raw)
-        assert_almost_equal(reference.cov_params(), result._var_beta_raw)
-        assert_almost_equal(reference.fittedvalues, result._y_fitted_raw)
-
-        _check_non_raw_results(result)
-
-    def checkMovingOLS(self, window_type, x, y, **kwds):
-        try:
-            from scikits.statsmodels.tools.tools import rank
-        except ImportError:
-            from scikits.statsmodels.tools import rank
-
-        window = rank(x.values) * 2
-
-        moving = ols(y=y, x=x, window_type=window_type,
-                     window=window, **kwds)
-
-        # check that sparse version is the same
-        sparse_moving = ols(y=y.to_sparse(), x=x.to_sparse(),
-                            window_type=window_type,
-                            window=window, **kwds)
-        _compare_ols_results(moving, sparse_moving)
-
-        if isinstance(moving.y, Series):
-            index = moving.y.index
-        elif isinstance(moving.y, LongPanel):
-            index = moving.y.major_axis
-
-        for n, i in enumerate(moving._valid_indices):
-            if window_type == 'rolling' and i >= window:
-                prior_date = index[i - window + 1]
-            else:
-                prior_date = index[0]
-
-            date = index[i]
-
-            x_iter = {}
-            for k, v in x.iteritems():
-                x_iter[k] = v.truncate(before=prior_date, after=date)
-            y_iter = y.truncate(before=prior_date, after=date)
-
-            static = ols(y=y_iter, x=x_iter, **kwds)
-
-            self.compare(static, moving, event_index=i,
-                         result_index=n)
-
-        _check_non_raw_results(moving)
-
-    def compare(self, static, moving, event_index=None,
-                result_index=None):
-
-        # Check resid if we have a time index specified
-        if event_index is not None:
-            ref = static._resid_raw[-1]
-            res = moving._resid_raw[event_index]
-
-            assert_almost_equal(ref, res)
-
-            ref = static._y_fitted_raw[-1]
-            res = moving._y_fitted_raw[event_index]
-
-            assert_almost_equal(ref, res)
-
-        # Check y_fitted
-
-        for field in self.FIELDS:
-            attr = '_%s_raw' % field
-
-            ref = getattr(static, attr)
-            res = getattr(moving, attr)
-
-            if result_index is not None:
-                res = res[result_index]
-
-            assert_almost_equal(ref, res)
-
-    def test_f_test(self):
-        x = testing.makeTimeDataFrame()
-        y = x.pop('A')
-
-        model = ols(y=y, x=x)
-
-        hyp = '1*B+1*C+1*D=0'
-        result = model.f_test(hyp)
-
-        hyp = ['1*B=0',
-               '1*C=0',
-               '1*D=0']
-        result = model.f_test(hyp)
-        assert_almost_equal(result['f-stat'], model.f_stat['f-stat'])
-
-        self.assertRaises(Exception, model.f_test, '1*A=0')
-
-class TestPanelOLS(BaseTest):
-
-
-    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat',
-              'p_value', 'r2', 'r2_adj', 'rmse', 'std_err',
-              't_stat', 'var_beta']
-
-    _other_fields = ['resid', 'y_fitted']
-
-    def testFiltering(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2)
-
-        x = result._x
-        index = [x.major_axis[i] for i in x.index.major_labels]
-        index = Index(sorted(set(index)))
-        exp_index = Index([datetime(2000, 1, 1), datetime(2000, 1, 3)])
-        self.assertTrue(exp_index.equals(index))
-
-        index = [x.minor_axis[i] for i in x.index.minor_labels]
-        index = Index(sorted(set(index)))
-        exp_index = Index(['A', 'B'])
-        self.assertTrue(exp_index.equals(index))
-
-        x = result._x_filtered
-        index = [x.major_axis[i] for i in x.index.major_labels]
-        index = Index(sorted(set(index)))
-        exp_index = Index([datetime(2000, 1, 1),
-                           datetime(2000, 1, 3),
-                           datetime(2000, 1, 4)])
-        self.assertTrue(exp_index.equals(index))
-
-        assert_almost_equal(result._y.values.flat, [1, 4, 5])
-
-        exp_x = [[6, 14, 1],
-                 [9, 17, 1],
-                 [30, 48, 1]]
-        assert_almost_equal(exp_x, result._x.values)
-
-        exp_x_filtered = [[6, 14, 1],
-                          [9, 17, 1],
-                          [30, 48, 1],
-                          [11, 20, 1],
-                          [12, 21, 1]]
-        assert_almost_equal(exp_x_filtered, result._x_filtered.values)
-
-        self.assertTrue(result._x_filtered.major_axis.equals(
-            result.y_fitted.index))
-
-    def testWithWeights(self):
-        data = np.arange(10).reshape((5, 2))
-        index = [datetime(2000, 1, 1),
-                 datetime(2000, 1, 2),
-                 datetime(2000, 1, 3),
-                 datetime(2000, 1, 4),
-                 datetime(2000, 1, 5)]
-        cols = ['A', 'B']
-        weights = DataFrame(data, index=index, columns=cols)
-
-        result = ols(y=self.panel_y2, x=self.panel_x2, weights=weights)
-
-        assert_almost_equal(result._y_trans.values.flat, [0, 16, 25])
-
-        exp_x = [[0, 0, 0],
-                 [36, 68, 4],
-                 [150, 240, 5]]
-        assert_almost_equal(result._x_trans.values, exp_x)
-
-
-        exp_x_filtered = [[6, 14, 1],
-                          [9, 17, 1],
-                          [30, 48, 1],
-                          [11, 20, 1],
-                          [12, 21, 1]]
-#         exp_x_filtered = [[0, 0, 0],
-#                           [36, 68, 4],
-#                           [150, 240, 5],
-#                           [66, 120, 6],
-#                           [84, 147, 7]]
-
-        assert_almost_equal(result._x_filtered.values, exp_x_filtered)
-
-        # _check_non_raw_results(result)
-
-    def testWithTimeEffects(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2, time_effects=True)
-
-        assert_almost_equal(result._y_trans.values.flat, [0, -0.5, 0.5])
-
-        exp_x = [[0, 0], [-10.5, -15.5], [10.5, 15.5]]
-        assert_almost_equal(result._x_trans.values, exp_x)
-
-        # _check_non_raw_results(result)
-
-    def testWithEntityEffects(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True)
-
-        assert_almost_equal(result._y.values.flat, [1, 4, 5])
-        exp_x = [[6, 14, 0, 1], [9, 17, 0, 1], [30, 48, 1, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1', 'x2', 'FE_B', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testWithEntityEffectsAndDroppedDummies(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True,
-                     dropped_dummies={'entity' : 'B'})
-
-        assert_almost_equal(result._y.values.flat, [1, 4, 5])
-        exp_x = [[6, 14, 1, 1], [9, 17, 1, 1], [30, 48, 0, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1', 'x2', 'FE_A', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testWithXEffects(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'])
-
-        assert_almost_equal(result._y.values.flat, [1, 4, 5])
-        exp_x = [[0, 0, 14, 1], [0, 1, 17, 1], [1, 0, 48, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_30', 'x1_9', 'x2', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testWithXEffectsAndDroppedDummies(self):
-        result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'],
-                     dropped_dummies={'x1' : 30})
-
-        assert_almost_equal(result._y.values.flat, [1, 4, 5])
-        exp_x = [[1, 0, 14, 1], [0, 1, 17, 1], [0, 0, 48, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_6', 'x1_9', 'x2', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testWithXEffectsAndConversion(self):
-        result = ols(y=self.panel_y3, x=self.panel_x3, x_effects=['x1', 'x2'])
-
-        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4])
-        exp_x = [[0, 0, 0, 1, 1], [1, 0, 0, 0, 1], [0, 1, 1, 0, 1],
-                 [0, 0, 0, 1, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_B', 'x1_C', 'x2_2.65', 'x2_3.14', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testWithXEffectsAndConversionAndDroppedDummies(self):
-        result = ols(y=self.panel_y3, x=self.panel_x3, x_effects=['x1', 'x2'],
-                     dropped_dummies={'x2' : '3.14'})
-
-        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4])
-        exp_x = [[0, 0, 0, 0, 1], [1, 0, 1, 0, 1], [0, 1, 0, 1, 1],
-                 [0, 0, 0, 0, 1]]
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_B', 'x1_C', 'x2_1.59', 'x2_2.65', 'intercept'])
-        self.assertTrue(exp_index.equals(result._x.items))
-
-        # _check_non_raw_results(result)
-
-    def testForSeries(self):
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y)
-
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y, nw_lags=0)
-
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y, nw_lags=1,
-                            nw_overlap=True)
-
-    def testRollingWithWeights(self):
-        idx = self.panel_y.index
-        cols = self.panel_y.columns
-
-
-        weights = DataFrame(np.random.standard_normal((len(idx), len(cols))),
-                            index=idx, columns=cols)
-        self.checkMovingOLS(self.panel_x,
-                            self.panel_y, weights=weights)
-
-    def testRolling(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y)
-
-    def testRollingWithFixedEffects(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            entity_effects=True)
-
-    def testRollingWithTimeEffects(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            time_effects=True)
-
-    def testRollingWithNeweyWest(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            nw_lags=1)
-
-    def testRollingWithEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            cluster='entity')
-
-    def testRollingWithTimeEffectsAndEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            time_effects=True, cluster='entity')
-
-    def testRollingWithTimeCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            cluster='time')
-
-    def testRollingWithNeweyWestAndEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            nw_lags=1, cluster='entity')
-
-    def testRollingWithNeweyWestAndTimeEffectsAndEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            nw_lags=1, cluster='entity',
-                            time_effects=True)
-
-    def testExpanding(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y, window_type='expanding')
-
-    def testNonPooled(self):
-        self.checkNonPooled(y=self.panel_y, x=self.panel_x)
-        self.checkNonPooled(y=self.panel_y, x=self.panel_x,
-                            window_type='rolling', window=25, min_periods=10)
-
-    def checkNonPooled(self, x, y, **kwds):
-        # For now, just check that it doesn't crash
-        result = ols(y=y, x=x, pool=False, **kwds)
-
-        _check_repr(result)
-        for attr in NonPooledPanelOLS.ATTRIBUTES:
-            _check_repr(getattr(result, attr))
-
-    def checkMovingOLS(self, x, y, window_type='rolling', **kwds):
-        window = 25  # must be larger than rank of x
-
-        moving = ols(y=y, x=x, window_type=window_type,
-                     window=window, **kwds)
-
-        if isinstance(moving.y, Series):
-            index = moving.y.index
-        elif isinstance(moving.y, LongPanel):
-            index = moving.y.major_axis
-
-        for n, i in enumerate(moving._valid_indices):
-            if window_type == 'rolling' and i >= window:
-                prior_date = index[i - window + 1]
-            else:
-                prior_date = index[0]
-
-            date = index[i]
-
-            x_iter = {}
-            for k, v in x.iteritems():
-                x_iter[k] = v.truncate(before=prior_date, after=date)
-            y_iter = y.truncate(before=prior_date, after=date)
-
-            static = ols(y=y_iter, x=x_iter, **kwds)
-
-            self.compare(static, moving, event_index=i,
-                         result_index=n)
-
-        _check_non_raw_results(moving)
-
-    def checkForSeries(self, x, y, series_x, series_y, **kwds):
-        # Consistency check with simple OLS.
-        result = ols(y=y, x=x, **kwds)
-        reference = ols(y=series_y, x=series_x, **kwds)
-
-        self.compare(reference, result)
-
-    def compare(self, static, moving, event_index=None,
-                result_index=None):
-
-        # Check resid if we have a time index specified
-        if event_index is not None:
-            staticSlice = _period_slice(static, -1)
-            movingSlice = _period_slice(moving, event_index)
-
-            ref = static._resid_raw[staticSlice]
-            res = moving._resid_raw[movingSlice]
-
-            assert_almost_equal(ref, res)
-
-            ref = static._y_fitted_raw[staticSlice]
-            res = moving._y_fitted_raw[movingSlice]
-
-            assert_almost_equal(ref, res)
-
-        # Check y_fitted
-
-        for field in self.FIELDS:
-            attr = '_%s_raw' % field
-
-            ref = getattr(static, attr)
-            res = getattr(moving, attr)
-
-            if result_index is not None:
-                res = res[result_index]
-
-            assert_almost_equal(ref, res)
-
-    def test_auto_rolling_window_type(self):
-        data = testing.makeTimeDataFrame()
-        y = data.pop('A')
-
-        window_model = ols(y=y, x=data, window=20, min_periods=10)
-        rolling_model = ols(y=y, x=data, window=20, min_periods=10,
-                            window_type='rolling')
-
-        assert_frame_equal(window_model.beta, rolling_model.beta)
-
-def _check_non_raw_results(model):
-    _check_repr(model)
-    _check_repr(model.resid)
-    _check_repr(model.summary_as_matrix)
-    _check_repr(model.y_fitted)
-    _check_repr(model.y_predict)
-
-def _period_slice(panelModel, i):
-    index = panelModel._x_trans.index
-    period = index.major_axis[i]
-
-    L, R = index.get_major_bounds(period, period)
-
-    return slice(L, R)
-
-if __name__ == '__main__':
-    import nose
-    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
-                   exit=False)
+"""
+Unit test suite for OLS and PanelOLS classes
+"""
+
+# pylint: disable-msg=W0212
+
+from __future__ import division
+
+from datetime import datetime
+import unittest
+import numpy as np
+
+from pandas.core.panel import LongPanel
+from pandas.core.api import DataFrame, Index, Series
+from pandas.stats.api import ols
+from pandas.stats.plm import NonPooledPanelOLS
+from pandas.util.testing import (assert_almost_equal, assert_series_equal,
+                                 assert_frame_equal)
+import pandas.util.testing as testing
+
+from common import BaseTest
+
+def _check_repr(obj):
+    repr(obj)
+    str(obj)
+
+def _compare_ols_results(model1, model2):
+    assert(type(model1) == type(model2))
+
+    if hasattr(model1, '_window_type'):
+        _compare_moving_ols(model1, model2)
+    else:
+        _compare_fullsample_ols(model1, model2)
+
+def _compare_fullsample_ols(model1, model2):
+    assert_series_equal(model1.beta, model2.beta)
+
+def _compare_moving_ols(model1, model2):
+    assert_frame_equal(model1.beta, model2.beta)
+
+class TestOLS(BaseTest):
+
+    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat', 'p_value',
+              'r2', 'r2_adj', 'rmse', 'std_err', 't_stat',
+              'var_beta']
+
+    # TODO: Add tests for OLS y predict
+    # TODO: Right now we just check for consistency between full-sample and
+    # rolling/expanding results of the panel OLS.  We should also cross-check
+    # with trusted implementations of panel OLS (e.g. R).
+    # TODO: Add tests for non pooled OLS.
+
+    def testOLSWithDatasets(self):
+        import scikits.statsmodels.datasets as datasets
+
+        self.checkDataSet(datasets.ccard.load(), skip_moving=True)
+        self.checkDataSet(datasets.cpunish.load(), skip_moving=True)
+        self.checkDataSet(datasets.longley.load(), skip_moving=True)
+        self.checkDataSet(datasets.stackloss.load(), skip_moving=True)
+        self.checkDataSet(datasets.ccard.load(), 39, 49) # one col in X all 0s
+        self.checkDataSet(datasets.copper.load())
+        self.checkDataSet(datasets.scotland.load())
+
+    def checkDataSet(self, dataset, start=None, end=None, skip_moving=False):
+        exog = dataset.exog[start : end]
+        endog = dataset.endog[start : end]
+        x = DataFrame(exog, index=np.arange(exog.shape[0]),
+                      columns=np.arange(exog.shape[1]))
+        y = Series(endog, index=np.arange(len(endog)))
+
+        self.checkOLS(exog, endog, x, y)
+
+        if not skip_moving:
+            self.checkMovingOLS('rolling', x, y)
+            self.checkMovingOLS('rolling', x, y, nw_lags=0)
+            self.checkMovingOLS('expanding', x, y, nw_lags=0)
+            self.checkMovingOLS('rolling', x, y, nw_lags=1)
+            self.checkMovingOLS('expanding', x, y, nw_lags=1)
+            self.checkMovingOLS('expanding', x, y, nw_lags=1, nw_overlap=True)
+
+    def checkOLS(self, exog, endog, x, y):
+
+        try:
+            import scikits.statsmodels.api as sm
+        except ImportError:
+            import scikits.statsmodels as sm
+
+        reference = sm.OLS(endog, sm.add_constant(exog)).fit()
+        result = ols(y=y, x=x)
+
+        # check that sparse version is the same
+        sparse_result = ols(y=y.to_sparse(), x=x.to_sparse())
+        _compare_ols_results(result, sparse_result)
+
+        assert_almost_equal(reference.params, result._beta_raw)
+        assert_almost_equal(reference.df_model, result._df_model_raw)
+        assert_almost_equal(reference.df_resid, result._df_resid_raw)
+        assert_almost_equal(reference.fvalue, result._f_stat_raw[0])
+        assert_almost_equal(reference.pvalues, result._p_value_raw)
+        assert_almost_equal(reference.rsquared, result._r2_raw)
+        assert_almost_equal(reference.rsquared_adj, result._r2_adj_raw)
+        assert_almost_equal(reference.resid, result._resid_raw)
+        assert_almost_equal(reference.bse, result._std_err_raw)
+        assert_almost_equal(reference.tvalues, result._t_stat_raw)
+        assert_almost_equal(reference.cov_params(), result._var_beta_raw)
+        assert_almost_equal(reference.fittedvalues, result._y_fitted_raw)
+
+        _check_non_raw_results(result)
+
+    def checkMovingOLS(self, window_type, x, y, **kwds):
+        try:
+            from scikits.statsmodels.tools.tools import rank
+        except ImportError:
+            from scikits.statsmodels.tools import rank
+
+        window = rank(x.values) * 2
+
+        moving = ols(y=y, x=x, window_type=window_type,
+                     window=window, **kwds)
+
+        # check that sparse version is the same
+        sparse_moving = ols(y=y.to_sparse(), x=x.to_sparse(),
+                            window_type=window_type,
+                            window=window, **kwds)
+        _compare_ols_results(moving, sparse_moving)
+
+        if isinstance(moving.y, Series):
+            index = moving.y.index
+        elif isinstance(moving.y, LongPanel):
+            index = moving.y.major_axis
+
+        for n, i in enumerate(moving._valid_indices):
+            if window_type == 'rolling' and i >= window:
+                prior_date = index[i - window + 1]
+            else:
+                prior_date = index[0]
+
+            date = index[i]
+
+            x_iter = {}
+            for k, v in x.iteritems():
+                x_iter[k] = v.truncate(before=prior_date, after=date)
+            y_iter = y.truncate(before=prior_date, after=date)
+
+            static = ols(y=y_iter, x=x_iter, **kwds)
+
+            self.compare(static, moving, event_index=i,
+                         result_index=n)
+
+        _check_non_raw_results(moving)
+
+    def compare(self, static, moving, event_index=None,
+                result_index=None):
+
+        # Check resid if we have a time index specified
+        if event_index is not None:
+            ref = static._resid_raw[-1]
+            res = moving._resid_raw[event_index]
+
+            assert_almost_equal(ref, res)
+
+            ref = static._y_fitted_raw[-1]
+            res = moving._y_fitted_raw[event_index]
+
+            assert_almost_equal(ref, res)
+
+        # Check y_fitted
+
+        for field in self.FIELDS:
+            attr = '_%s_raw' % field
+
+            ref = getattr(static, attr)
+            res = getattr(moving, attr)
+
+            if result_index is not None:
+                res = res[result_index]
+
+            assert_almost_equal(ref, res)
+
+    def test_f_test(self):
+        x = testing.makeTimeDataFrame()
+        y = x.pop('A')
+
+        model = ols(y=y, x=x)
+
+        hyp = '1*B+1*C+1*D=0'
+        result = model.f_test(hyp)
+
+        hyp = ['1*B=0',
+               '1*C=0',
+               '1*D=0']
+        result = model.f_test(hyp)
+        assert_almost_equal(result['f-stat'], model.f_stat['f-stat'])
+
+        self.assertRaises(Exception, model.f_test, '1*A=0')
+
+class TestPanelOLS(BaseTest):
+
+
+    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat',
+              'p_value', 'r2', 'r2_adj', 'rmse', 'std_err',
+              't_stat', 'var_beta']
+
+    _other_fields = ['resid', 'y_fitted']
+
+    def testFiltering(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2)
+
+        x = result._x
+        index = [x.major_axis[i] for i in x.index.major_labels]
+        index = Index(sorted(set(index)))
+        exp_index = Index([datetime(2000, 1, 1), datetime(2000, 1, 3)])
+        self.assertTrue(exp_index.equals(index))
+
+        index = [x.minor_axis[i] for i in x.index.minor_labels]
+        index = Index(sorted(set(index)))
+        exp_index = Index(['A', 'B'])
+        self.assertTrue(exp_index.equals(index))
+
+        x = result._x_filtered
+        index = [x.major_axis[i] for i in x.index.major_labels]
+        index = Index(sorted(set(index)))
+        exp_index = Index([datetime(2000, 1, 1),
+                           datetime(2000, 1, 3),
+                           datetime(2000, 1, 4)])
+        self.assertTrue(exp_index.equals(index))
+
+        assert_almost_equal(result._y.values.flat, [1, 4, 5])
+
+        exp_x = [[6, 14, 1],
+                 [9, 17, 1],
+                 [30, 48, 1]]
+        assert_almost_equal(exp_x, result._x.values)
+
+        exp_x_filtered = [[6, 14, 1],
+                          [9, 17, 1],
+                          [30, 48, 1],
+                          [11, 20, 1],
+                          [12, 21, 1]]
+        assert_almost_equal(exp_x_filtered, result._x_filtered.values)
+
+        self.assertTrue(result._x_filtered.major_axis.equals(
+            result.y_fitted.index))
+
+    def testWithWeights(self):
+        data = np.arange(10).reshape((5, 2))
+        index = [datetime(2000, 1, 1),
+                 datetime(2000, 1, 2),
+                 datetime(2000, 1, 3),
+                 datetime(2000, 1, 4),
+                 datetime(2000, 1, 5)]
+        cols = ['A', 'B']
+        weights = DataFrame(data, index=index, columns=cols)
+
+        result = ols(y=self.panel_y2, x=self.panel_x2, weights=weights)
+
+        assert_almost_equal(result._y_trans.values.flat, [0, 16, 25])
+
+        exp_x = [[0, 0, 0],
+                 [36, 68, 4],
+                 [150, 240, 5]]
+        assert_almost_equal(result._x_trans.values, exp_x)
+
+
+        exp_x_filtered = [[6, 14, 1],
+                          [9, 17, 1],
+                          [30, 48, 1],
+                          [11, 20, 1],
+                          [12, 21, 1]]
+#         exp_x_filtered = [[0, 0, 0],
+#                           [36, 68, 4],
+#                           [150, 240, 5],
+#                           [66, 120, 6],
+#                           [84, 147, 7]]
+
+        assert_almost_equal(result._x_filtered.values, exp_x_filtered)
+
+        # _check_non_raw_results(result)
+
+    def testWithTimeEffects(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2, time_effects=True)
+
+        assert_almost_equal(result._y_trans.values.flat, [0, -0.5, 0.5])
+
+        exp_x = [[0, 0], [-10.5, -15.5], [10.5, 15.5]]
+        assert_almost_equal(result._x_trans.values, exp_x)
+
+        # _check_non_raw_results(result)
+
+    def testWithEntityEffects(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True)
+
+        assert_almost_equal(result._y.values.flat, [1, 4, 5])
+        exp_x = [[6, 14, 0, 1], [9, 17, 0, 1], [30, 48, 1, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1', 'x2', 'FE_B', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testWithEntityEffectsAndDroppedDummies(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True,
+                     dropped_dummies={'entity' : 'B'})
+
+        assert_almost_equal(result._y.values.flat, [1, 4, 5])
+        exp_x = [[6, 14, 1, 1], [9, 17, 1, 1], [30, 48, 0, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1', 'x2', 'FE_A', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testWithXEffects(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'])
+
+        assert_almost_equal(result._y.values.flat, [1, 4, 5])
+        exp_x = [[0, 0, 14, 1], [0, 1, 17, 1], [1, 0, 48, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1_30', 'x1_9', 'x2', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testWithXEffectsAndDroppedDummies(self):
+        result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'],
+                     dropped_dummies={'x1' : 30})
+
+        assert_almost_equal(result._y.values.flat, [1, 4, 5])
+        exp_x = [[1, 0, 14, 1], [0, 1, 17, 1], [0, 0, 48, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1_6', 'x1_9', 'x2', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testWithXEffectsAndConversion(self):
+        result = ols(y=self.panel_y3, x=self.panel_x3, x_effects=['x1', 'x2'])
+
+        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4])
+        exp_x = [[0, 0, 0, 1, 1], [1, 0, 0, 0, 1], [0, 1, 1, 0, 1],
+                 [0, 0, 0, 1, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1_B', 'x1_C', 'x2_2.65', 'x2_3.14', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testWithXEffectsAndConversionAndDroppedDummies(self):
+        result = ols(y=self.panel_y3, x=self.panel_x3, x_effects=['x1', 'x2'],
+                     dropped_dummies={'x2' : '3.14'})
+
+        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4])
+        exp_x = [[0, 0, 0, 0, 1], [1, 0, 1, 0, 1], [0, 1, 0, 1, 1],
+                 [0, 0, 0, 0, 1]]
+        assert_almost_equal(result._x.values, exp_x)
+
+        exp_index = Index(['x1_B', 'x1_C', 'x2_1.59', 'x2_2.65', 'intercept'])
+        self.assertTrue(exp_index.equals(result._x.items))
+
+        # _check_non_raw_results(result)
+
+    def testForSeries(self):
+        self.checkForSeries(self.series_panel_x, self.series_panel_y,
+                            self.series_x, self.series_y)
+
+        self.checkForSeries(self.series_panel_x, self.series_panel_y,
+                            self.series_x, self.series_y, nw_lags=0)
+
+        self.checkForSeries(self.series_panel_x, self.series_panel_y,
+                            self.series_x, self.series_y, nw_lags=1,
+                            nw_overlap=True)
+
+    def testRollingWithWeights(self):
+        idx = self.panel_y.index
+        cols = self.panel_y.columns
+
+
+        weights = DataFrame(np.random.standard_normal((len(idx), len(cols))),
+                            index=idx, columns=cols)
+        self.checkMovingOLS(self.panel_x,
+                            self.panel_y, weights=weights)
+
+    def testRolling(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y)
+
+    def testRollingWithFixedEffects(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            entity_effects=True)
+
+    def testRollingWithTimeEffects(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            time_effects=True)
+
+    def testRollingWithNeweyWest(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            nw_lags=1)
+
+    def testRollingWithEntityCluster(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            cluster='entity')
+
+    def testRollingWithTimeEffectsAndEntityCluster(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            time_effects=True, cluster='entity')
+
+    def testRollingWithTimeCluster(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            cluster='time')
+
+    def testRollingWithNeweyWestAndEntityCluster(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            nw_lags=1, cluster='entity')
+
+    def testRollingWithNeweyWestAndTimeEffectsAndEntityCluster(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y,
+                            nw_lags=1, cluster='entity',
+                            time_effects=True)
+
+    def testExpanding(self):
+        self.checkMovingOLS(self.panel_x, self.panel_y, window_type='expanding')
+
+    def testNonPooled(self):
+        self.checkNonPooled(y=self.panel_y, x=self.panel_x)
+        self.checkNonPooled(y=self.panel_y, x=self.panel_x,
+                            window_type='rolling', window=25, min_periods=10)
+
+    def checkNonPooled(self, x, y, **kwds):
+        # For now, just check that it doesn't crash
+        result = ols(y=y, x=x, pool=False, **kwds)
+
+        _check_repr(result)
+        for attr in NonPooledPanelOLS.ATTRIBUTES:
+            _check_repr(getattr(result, attr))
+
+    def checkMovingOLS(self, x, y, window_type='rolling', **kwds):
+        window = 25  # must be larger than rank of x
+
+        moving = ols(y=y, x=x, window_type=window_type,
+                     window=window, **kwds)
+
+        if isinstance(moving.y, Series):
+            index = moving.y.index
+        elif isinstance(moving.y, LongPanel):
+            index = moving.y.major_axis
+
+        for n, i in enumerate(moving._valid_indices):
+            if window_type == 'rolling' and i >= window:
+                prior_date = index[i - window + 1]
+            else:
+                prior_date = index[0]
+
+            date = index[i]
+
+            x_iter = {}
+            for k, v in x.iteritems():
+                x_iter[k] = v.truncate(before=prior_date, after=date)
+            y_iter = y.truncate(before=prior_date, after=date)
+
+            static = ols(y=y_iter, x=x_iter, **kwds)
+
+            self.compare(static, moving, event_index=i,
+                         result_index=n)
+
+        _check_non_raw_results(moving)
+
+    def checkForSeries(self, x, y, series_x, series_y, **kwds):
+        # Consistency check with simple OLS.
+        result = ols(y=y, x=x, **kwds)
+        reference = ols(y=series_y, x=series_x, **kwds)
+
+        self.compare(reference, result)
+
+    def compare(self, static, moving, event_index=None,
+                result_index=None):
+
+        # Check resid if we have a time index specified
+        if event_index is not None:
+            staticSlice = _period_slice(static, -1)
+            movingSlice = _period_slice(moving, event_index)
+
+            ref = static._resid_raw[staticSlice]
+            res = moving._resid_raw[movingSlice]
+
+            assert_almost_equal(ref, res)
+
+            ref = static._y_fitted_raw[staticSlice]
+            res = moving._y_fitted_raw[movingSlice]
+
+            assert_almost_equal(ref, res)
+
+        # Check y_fitted
+
+        for field in self.FIELDS:
+            attr = '_%s_raw' % field
+
+            ref = getattr(static, attr)
+            res = getattr(moving, attr)
+
+            if result_index is not None:
+                res = res[result_index]
+
+            assert_almost_equal(ref, res)
+
+    def test_auto_rolling_window_type(self):
+        data = testing.makeTimeDataFrame()
+        y = data.pop('A')
+
+        window_model = ols(y=y, x=data, window=20, min_periods=10)
+        rolling_model = ols(y=y, x=data, window=20, min_periods=10,
+                            window_type='rolling')
+
+        assert_frame_equal(window_model.beta, rolling_model.beta)
+
+def _check_non_raw_results(model):
+    _check_repr(model)
+    _check_repr(model.resid)
+    _check_repr(model.summary_as_matrix)
+    _check_repr(model.y_fitted)
+    _check_repr(model.y_predict)
+
+def _period_slice(panelModel, i):
+    index = panelModel._x_trans.index
+    period = index.major_axis[i]
+
+    L, R = index.get_major_bounds(period, period)
+
+    return slice(L, R)
+
+if __name__ == '__main__':
+    import nose
+    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
+                   exit=False)
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index ab9228364..e72837b33 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -1,205 +1,205 @@
-# pylint: disable-msg=W0402
-
-from datetime import datetime
-import random
-import string
-import sys
-
-from numpy.random import randn
-import numpy as np
-
-from pandas.core.common import isnull
-import pandas.core.index as index
-import pandas.core.daterange as daterange
-import pandas.core.series as series
-import pandas.core.frame as frame
-import pandas.core.panel as panel
-
-# to_reload = ['index', 'daterange', 'series', 'frame', 'matrix', 'panel']
-# for mod in to_reload:
-#     reload(locals()[mod])
-
-DateRange = daterange.DateRange
-Index = index.Index
-Series = series.Series
-DataFrame = frame.DataFrame
-WidePanel = panel.WidePanel
-
-N = 30
-K = 4
-
-def rands(n):
-    choices = string.letters + string.digits
-    return ''.join([random.choice(choices) for _ in xrange(n)])
-
-#-------------------------------------------------------------------------------
-# Console debugging tools
-
-def debug(f, *args, **kwargs):
-    set_trace()
-    return f(*args, **kwargs)
-
-def set_trace():
-    from IPython.core.debugger import Pdb
-    try:
-        Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)
-    except:
-        from pdb import Pdb as OldPdb
-        OldPdb().set_trace(sys._getframe().f_back)
-
-#-------------------------------------------------------------------------------
-# Comparators
-
-def equalContents(arr1, arr2):
-    """Checks if the set of unique elements of arr1 and arr2 are equivalent.
-    """
-    return frozenset(arr1) == frozenset(arr2)
-
-def isiterable(obj):
-    return hasattr(obj, '__iter__')
-
-def assert_almost_equal(a, b):
-    if isinstance(a, dict) or isinstance(b, dict):
-        return assert_dict_equal(a, b)
-
-    if isiterable(a):
-        np.testing.assert_(isiterable(b))
-        np.testing.assert_equal(len(a), len(b))
-        for i in xrange(len(a)):
-            assert_almost_equal(a[i], b[i])
-        return True
-
-    err_msg = lambda a, b: 'expected %.5f but got %.5f' % (a, b)
-
-    if isnull(a):
-        np.testing.assert_(isnull(b))
-        return
-
-    if isinstance(a, (bool, float, int)):
-        # case for zero
-        if abs(a) < 1e-5:
-            np.testing.assert_almost_equal(
-                a, b, decimal=5, err_msg=err_msg(a, b), verbose=False)
-        else:
-            np.testing.assert_almost_equal(
-                1, a/b, decimal=5, err_msg=err_msg(a, b), verbose=False)
-    else:
-        assert(a == b)
-
-def is_sorted(seq):
-    return assert_almost_equal(seq, np.sort(np.array(seq)))
-
-def assert_dict_equal(a, b, compare_keys=True):
-    a_keys = frozenset(a.keys())
-    b_keys = frozenset(b.keys())
-
-    if compare_keys:
-        assert(a_keys == b_keys)
-
-    for k in a_keys:
-        assert_almost_equal(a[k], b[k])
-
-def assert_series_equal(left, right):
-    assert_almost_equal(left, right)
-    assert(left.index.equals(right.index))
-
-def assert_frame_equal(left, right):
-    for col, series in left.iteritems():
-        assert(col in right)
-        assert_series_equal(series, right[col])
-
-    for col in right:
-        assert(col in left)
-
-    assert(left.columns.equals(right.columns))
-
-def assert_panel_equal(left, right):
-    for col, series in left.iteritems():
-        assert(col in right)
-        assert_frame_equal(series, right[col])
-
-    for col in right:
-        assert(col in left)
-
-def assert_contains_all(iterable, dic):
-    for k in iterable:
-        assert(k in dic)
-
-def getCols(k):
-    return string.ascii_uppercase[:k]
-
-def makeStringIndex(k):
-    return Index([rands(10) for _ in xrange(k)])
-
-def makeIntIndex(k):
-    return Index(np.arange(k))
-
-def makeDateIndex(k):
-    dates = list(DateRange(datetime(2000, 1, 1), periods=k))
-    return Index(dates)
-
-def makeFloatSeries():
-    index = makeStringIndex(N)
-    return Series(randn(N), index=index)
-
-def makeStringSeries():
-    index = makeStringIndex(N)
-    return Series(randn(N), index=index)
-
-def makeObjectSeries():
-    dateIndex = makeDateIndex(N)
-    index = makeStringIndex(N)
-    return Series(dateIndex, index=index)
-
-def makeTimeSeries():
-    return Series(randn(N), index=makeDateIndex(N))
-
-def getArangeMat():
-    return np.arange(N * K).reshape((N, K))
-
-def getSeriesData():
-    index = makeStringIndex(N)
-
-    return dict((c, Series(randn(N), index=index)) for c in getCols(K))
-
-def getTimeSeriesData():
-    return dict((c, makeTimeSeries()) for c in getCols(K))
-
-def getMixedTypeDict():
-    index = Index(['a', 'b', 'c', 'd', 'e'])
-
-    data = {
-        'A' : [0., 1., 2., 3., 4.],
-        'B' : [0., 1., 0., 1., 0.],
-        'C' : ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'],
-        'D' : DateRange('1/1/2009', periods=5)
-    }
-
-    return index, data
-
-def makeDataFrame():
-    data = getSeriesData()
-    return DataFrame(data)
-
-def makeTimeDataFrame():
-    data = getTimeSeriesData()
-    return DataFrame(data)
-
-def makeWidePanel():
-    cols = ['Item' + c for c in string.ascii_uppercase[:K - 1]]
-    data = dict((c, makeTimeDataFrame()) for c in cols)
-    return WidePanel.fromDict(data)
-
-def add_nans(panel):
-    I, J, N = panel.shape
-    for i, item in enumerate(panel.items):
-        dm = panel[item]
-        for j, col in enumerate(dm.columns):
-            dm[col][:i + j] = np.NaN
-
-def makeLongPanel():
-    wp = makeWidePanel()
-    add_nans(wp)
-
-    return wp.toLong()
-
+# pylint: disable-msg=W0402
+
+from datetime import datetime
+import random
+import string
+import sys
+
+from numpy.random import randn
+import numpy as np
+
+from pandas.core.common import isnull
+import pandas.core.index as index
+import pandas.core.daterange as daterange
+import pandas.core.series as series
+import pandas.core.frame as frame
+import pandas.core.panel as panel
+
+# to_reload = ['index', 'daterange', 'series', 'frame', 'matrix', 'panel']
+# for mod in to_reload:
+#     reload(locals()[mod])
+
+DateRange = daterange.DateRange
+Index = index.Index
+Series = series.Series
+DataFrame = frame.DataFrame
+WidePanel = panel.WidePanel
+
+N = 30
+K = 4
+
+def rands(n):
+    choices = string.letters + string.digits
+    return ''.join([random.choice(choices) for _ in xrange(n)])
+
+#-------------------------------------------------------------------------------
+# Console debugging tools
+
+def debug(f, *args, **kwargs):
+    set_trace()
+    return f(*args, **kwargs)
+
+def set_trace():
+    from IPython.core.debugger import Pdb
+    try:
+        Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)
+    except:
+        from pdb import Pdb as OldPdb
+        OldPdb().set_trace(sys._getframe().f_back)
+
+#-------------------------------------------------------------------------------
+# Comparators
+
+def equalContents(arr1, arr2):
+    """Checks if the set of unique elements of arr1 and arr2 are equivalent.
+    """
+    return frozenset(arr1) == frozenset(arr2)
+
+def isiterable(obj):
+    return hasattr(obj, '__iter__')
+
+def assert_almost_equal(a, b):
+    if isinstance(a, dict) or isinstance(b, dict):
+        return assert_dict_equal(a, b)
+
+    if isiterable(a):
+        np.testing.assert_(isiterable(b))
+        np.testing.assert_equal(len(a), len(b))
+        for i in xrange(len(a)):
+            assert_almost_equal(a[i], b[i])
+        return True
+
+    err_msg = lambda a, b: 'expected %.5f but got %.5f' % (a, b)
+
+    if isnull(a):
+        np.testing.assert_(isnull(b))
+        return
+
+    if isinstance(a, (bool, float, int)):
+        # case for zero
+        if abs(a) < 1e-5:
+            np.testing.assert_almost_equal(
+                a, b, decimal=5, err_msg=err_msg(a, b), verbose=False)
+        else:
+            np.testing.assert_almost_equal(
+                1, a/b, decimal=5, err_msg=err_msg(a, b), verbose=False)
+    else:
+        assert(a == b)
+
+def is_sorted(seq):
+    return assert_almost_equal(seq, np.sort(np.array(seq)))
+
+def assert_dict_equal(a, b, compare_keys=True):
+    a_keys = frozenset(a.keys())
+    b_keys = frozenset(b.keys())
+
+    if compare_keys:
+        assert(a_keys == b_keys)
+
+    for k in a_keys:
+        assert_almost_equal(a[k], b[k])
+
+def assert_series_equal(left, right):
+    assert_almost_equal(left, right)
+    assert(left.index.equals(right.index))
+
+def assert_frame_equal(left, right):
+    for col, series in left.iteritems():
+        assert(col in right)
+        assert_series_equal(series, right[col])
+
+    for col in right:
+        assert(col in left)
+
+    assert(left.columns.equals(right.columns))
+
+def assert_panel_equal(left, right):
+    for col, series in left.iteritems():
+        assert(col in right)
+        assert_frame_equal(series, right[col])
+
+    for col in right:
+        assert(col in left)
+
+def assert_contains_all(iterable, dic):
+    for k in iterable:
+        assert(k in dic)
+
+def getCols(k):
+    return string.ascii_uppercase[:k]
+
+def makeStringIndex(k):
+    return Index([rands(10) for _ in xrange(k)])
+
+def makeIntIndex(k):
+    return Index(np.arange(k))
+
+def makeDateIndex(k):
+    dates = list(DateRange(datetime(2000, 1, 1), periods=k))
+    return Index(dates)
+
+def makeFloatSeries():
+    index = makeStringIndex(N)
+    return Series(randn(N), index=index)
+
+def makeStringSeries():
+    index = makeStringIndex(N)
+    return Series(randn(N), index=index)
+
+def makeObjectSeries():
+    dateIndex = makeDateIndex(N)
+    index = makeStringIndex(N)
+    return Series(dateIndex, index=index)
+
+def makeTimeSeries():
+    return Series(randn(N), index=makeDateIndex(N))
+
+def getArangeMat():
+    return np.arange(N * K).reshape((N, K))
+
+def getSeriesData():
+    index = makeStringIndex(N)
+
+    return dict((c, Series(randn(N), index=index)) for c in getCols(K))
+
+def getTimeSeriesData():
+    return dict((c, makeTimeSeries()) for c in getCols(K))
+
+def getMixedTypeDict():
+    index = Index(['a', 'b', 'c', 'd', 'e'])
+
+    data = {
+        'A' : [0., 1., 2., 3., 4.],
+        'B' : [0., 1., 0., 1., 0.],
+        'C' : ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'],
+        'D' : DateRange('1/1/2009', periods=5)
+    }
+
+    return index, data
+
+def makeDataFrame():
+    data = getSeriesData()
+    return DataFrame(data)
+
+def makeTimeDataFrame():
+    data = getTimeSeriesData()
+    return DataFrame(data)
+
+def makeWidePanel():
+    cols = ['Item' + c for c in string.ascii_uppercase[:K - 1]]
+    data = dict((c, makeTimeDataFrame()) for c in cols)
+    return WidePanel.fromDict(data)
+
+def add_nans(panel):
+    I, J, N = panel.shape
+    for i, item in enumerate(panel.items):
+        dm = panel[item]
+        for j, col in enumerate(dm.columns):
+            dm[col][:i + j] = np.NaN
+
+def makeLongPanel():
+    wp = makeWidePanel()
+    add_nans(wp)
+
+    return wp.toLong()
+
