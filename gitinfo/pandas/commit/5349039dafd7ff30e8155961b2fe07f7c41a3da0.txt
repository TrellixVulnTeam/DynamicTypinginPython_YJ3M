commit 5349039dafd7ff30e8155961b2fe07f7c41a3da0
Author: Wouter Overmeire <lodagro@gmail.com>
Date:   Tue Jan 10 15:36:00 2012 +0100

    DOC: Create temp files to demo read_csv() and read_table(), when they are needed and remove afterwards.

diff --git a/doc/source/io.rst b/doc/source/io.rst
index 76c606f8e..c83ef06bf 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -201,6 +201,12 @@ Automatically "sniffing" the delimiter
 comma-separated) files. YMMV, as pandas uses the Sniffer_ class of the csv
 module.
 
+.. ipython:: python
+   :suppress:
+
+   df[:7].to_csv('tmp.sv', sep='|')
+   df[:7].to_csv('tmp2.sv', sep=':')
+
 .. ipython:: python
 
     print open('tmp2.sv').read()
@@ -216,11 +222,6 @@ Iterating through files chunk by chunk
 Suppose you wish to iterate through a (potentially very large) file lazily
 rather than reading the entire file into memory, such as the following:
 
-.. ipython:: python
-   :suppress:
-
-   df[:7].to_csv('tmp.sv', sep='|')
-   df[:7].to_csv('tmp2.sv', sep=':')
 
 .. ipython:: python
 
@@ -228,10 +229,6 @@ rather than reading the entire file into memory, such as the following:
    table = read_table('tmp.sv', sep='|')
    table
 
-.. ipython:: python
-   :suppress:
-
-   os.remove('tmp.csv')
 
 By specifiying a ``chunksize`` to ``read_csv`` or ``read_table``, the return
 value will be an iterable object of type ``TextParser``:
@@ -253,6 +250,12 @@ Specifying ``iterator=True`` will also return the ``TextParser`` object:
    reader = read_table('tmp.sv', sep='|', iterator=True)
    reader.get_chunk(5)
 
+.. ipython:: python
+   :suppress:
+
+   os.remove('tmp.sv')
+   os.remove('tmp2.sv')
+
 Writing to CSV format
 ~~~~~~~~~~~~~~~~~~~~~
 
