commit ce8629a365f0b5249ae4ef0ec845aef22011a915
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon Apr 23 00:58:46 2012 -0400

    ENH: refactor / cleanup to do timestamp -> period resampling, re #1064 #1004

diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 8fb91a1c9..c654ca47d 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -137,7 +137,7 @@ class PandasObject(Picklable):
         return groupby(self, by, axis=axis, level=level, as_index=as_index,
                        sort=sort, group_keys=group_keys)
 
-    def convert(self, rule, method='pad', how='mean', axis=0, as_index=True,
+    def resample(self, rule, method='pad', how='mean', axis=0, as_index=True,
                 closed='right', label='right', kind=None):
         """
         Convenience method for frequency conversion and resampling of regular
@@ -159,20 +159,19 @@ class PandasObject(Picklable):
 
         idx = self._get_axis(axis)
         if not isinstance(idx, DatetimeIndex):
-            raise ValueError("Cannot call convert with non-DatetimeIndex")
+            raise ValueError("Cannot call resample with non-DatetimeIndex")
 
         grouper = TimeGrouper(rule, label=label, closed=closed,
                               axis=self.index, kind=kind)
 
         # since binner extends endpoints
-        if len(grouper.bins) <= len(idx):
+        if grouper.downsamples:
             # down- or re-sampling
             grouped  = self.groupby(grouper, axis=axis, as_index=as_index)
             result = grouped.agg(how)
         else:
             # upsampling
-            result = self.reindex(grouper.binner[1:-1].view('M8[us]'),
-                                  method=method)
+            result = self.reindex(grouper.binner[1:], method=method)
 
         result.index.offset = rule
         return result
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index e450c6d4e..b7cb0c9d9 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -643,9 +643,9 @@ class Grouper(object):
         'std' : np.sqrt
     }
 
-    _name_functions = {
-        'ohlc' : lambda *args: ['open', 'high', 'low', 'close']
-    }
+    _name_functions = {}
+
+    _filter_empty_groups = True
 
     def aggregate(self, values, how):
         values = com._ensure_float64(values)
@@ -669,7 +669,8 @@ class Grouper(object):
         agg_func(result, counts, values, comp_ids)
         result = trans_func(result)
 
-        result = lib.row_bool_subset(result, counts > 0)
+        if self._filter_empty_groups:
+            result = lib.row_bool_subset(result, counts > 0)
 
         if squeeze:
             result = result.squeeze()
@@ -732,7 +733,7 @@ class Grouper(object):
         return result, counts
 
 
-def generate_bins_generic(values, binner, closed, label):
+def generate_bins_generic(values, binner, closed):
     """
     Generate bin edge offsets and bin labels for one array using another array
     which has bin edge values. Both arrays must be sorted.
@@ -744,14 +745,12 @@ def generate_bins_generic(values, binner, closed, label):
         the first array. Note, 'values' end-points must fall within 'binner'
         end-points.
     closed : which end of bin is closed; left (default), right
-    label : which end of bin to use as a label: left (default), right
 
     Returns
     -------
     bins : array of offsets (into 'values' argument) of bins.
         Zero and last edge are excluded in result, so for instance the first
         bin is values[0:bin[0]] and the last is values[bin[-1]:]
-    labels : array of labels of bins
     """
     lenidx = len(values)
     lenbin = len(binner)
@@ -766,46 +765,25 @@ def generate_bins_generic(values, binner, closed, label):
     if values[lenidx-1] > binner[lenbin-1]:
         raise ValueError("Values falls after last bin")
 
-    labels = np.empty(lenbin, dtype=np.int64)
-    bins   = np.empty(lenbin, dtype=np.int32)
+    bins   = np.empty(lenbin - 1, dtype=np.int32)
 
     j  = 0 # index into values
     bc = 0 # bin count
-    vc = 0 # value count
 
     # linear scan, presume nothing about values/binner except that it
     # fits ok
     for i in range(0, lenbin-1):
-        l_bin = binner[i]
         r_bin = binner[i+1]
 
-        # set label of bin
-        if label == 'left':
-            labels[bc] = l_bin
-        else:
-            labels[bc] = r_bin
-
         # count values in current bin, advance to next bin
-        while values[j] < r_bin or closed == 'right' and values[j] == r_bin:
+        while j < lenidx and (values[j] < r_bin or
+                              (closed == 'right' and values[j] == r_bin)):
             j += 1
-            vc += 1
-            if j >= lenidx:
-                break
-
-        # check we have data left to scan
-        if j >= lenidx:
-            break
 
-        # if we've seen some values or not ignoring empty bins
-        if vc != 0:
-            bins[bc] = j
-            bc += 1
-            vc = 0
+        bins[bc] = j
+        bc += 1
 
-    labels = np.resize(labels, bc + 1)
-    bins = np.resize(bins, bc)
-
-    return bins, labels
+    return bins
 
 
 
@@ -836,7 +814,8 @@ class BinGrouper(Grouper):
             yield label, data[start:edge]
             start = edge
 
-        yield self.binlabels[-1], data[edge:]
+        if edge < len(data):
+            yield self.binlabels[-1], data[edge:]
 
     @cache_readonly
     def ngroups(self):
@@ -845,8 +824,9 @@ class BinGrouper(Grouper):
     #----------------------------------------------------------------------
     # cython aggregation
 
+    import pandas._sandbox as sbx
     _cython_functions = {
-        'add' : lib.group_add_bin,
+        'add' : sbx.group_add_bin,
         'prod' : lib.group_prod_bin,
         'mean' : lib.group_mean_bin,
         'var' : lib.group_var_bin,
@@ -858,6 +838,12 @@ class BinGrouper(Grouper):
         'ohlc' : 4, # OHLC
     }
 
+    _name_functions = {
+        'ohlc' : lambda *args: ['open', 'high', 'low', 'close']
+    }
+
+    _filter_empty_groups = True
+
     def aggregate(self, values, how):
         values = com._ensure_float64(values)
 
@@ -881,7 +867,8 @@ class BinGrouper(Grouper):
         agg_func(result, counts, values, self.bins)
         result = trans_func(result)
 
-        result = lib.row_bool_subset(result, counts > 0)
+        if self._filter_empty_groups:
+            result = lib.row_bool_subset(result, counts > 0)
 
         if squeeze:
             result = result.squeeze()
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index c10adae3d..d75f5dfc9 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -565,15 +565,15 @@ def group_var(ndarray[float64_t, ndim=2] out,
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def generate_bins_dt64(ndarray[int64_t] values, ndarray[int64_t] binner,
-                       object closed='left', object label='left'):
+                       object closed='left'):
     """
     Int64 (datetime64) version of generic python version in groupby.py
     """
     cdef:
         Py_ssize_t lenidx, lenbin, i, j, bc, vc
-        ndarray[int64_t] labels
         ndarray[int32_t] bins
         int64_t l_bin, r_bin
+        bint right_closed = closed == 'right'
 
     lenidx = len(values)
     lenbin = len(binner)
@@ -588,51 +588,30 @@ def generate_bins_dt64(ndarray[int64_t] values, ndarray[int64_t] binner,
     if values[lenidx-1] > binner[lenbin-1]:
         raise ValueError("Values falls after last bin")
 
-    labels = np.empty(lenbin, dtype=np.int64)
-    bins   = np.empty(lenbin, dtype=np.int32)
+    bins   = np.empty(lenbin - 1, dtype=np.int32)
 
     j  = 0 # index into values
     bc = 0 # bin count
-    vc = 0 # value count
 
-    # linear scan, presume nothing about values/binner except that it
-    # fits ok
-    for i in range(0, lenbin-1):
+    # linear scan
+    for i in range(0, lenbin - 1):
         l_bin = binner[i]
         r_bin = binner[i+1]
 
-        # set label of bin
-        if label == 'left':
-            labels[bc] = l_bin
-        else:
-            labels[bc] = r_bin
-
         # count values in current bin, advance to next bin
-        while values[j] < r_bin or closed == 'right' and values[j] == r_bin:
+        while j < lenidx and (values[j] < r_bin or
+                              (right_closed and values[j] == r_bin)):
             j += 1
-            vc += 1
-            if j >= lenidx:
-                break
-
-        # check we have data left to scan
-        if j >= lenidx:
-            break
-
-        # if we've seen some values, mark bin
-        if vc != 0:
-            bins[bc] = j
-            bc += 1
-            vc = 0
 
-    labels = np.resize(labels, bc + 1)
-    bins = np.resize(bins, bc)
+        bins[bc] = j
+        bc += 1
 
-    return bins, labels
+    return bins
 
 # add passing bin edges, instead of labels
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_add_bin(ndarray[float64_t, ndim=2] out,
                   ndarray[int32_t] counts,
                   ndarray[float64_t, ndim=2] values,
@@ -654,7 +633,7 @@ def group_add_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -667,7 +646,7 @@ def group_add_bin(ndarray[float64_t, ndim=2] out,
                     sumx[b, j] += val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -685,8 +664,8 @@ def group_add_bin(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = sumx[i, j]
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_prod_bin(ndarray[float64_t, ndim=2] out,
                   ndarray[int32_t] counts,
                   ndarray[float64_t, ndim=2] values,
@@ -708,7 +687,7 @@ def group_prod_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -721,7 +700,7 @@ def group_prod_bin(ndarray[float64_t, ndim=2] out,
                     prodx[b, j] *= val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -739,8 +718,8 @@ def group_prod_bin(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = prodx[i, j]
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_min_bin(ndarray[float64_t, ndim=2] out,
                    ndarray[int32_t] counts,
                    ndarray[float64_t, ndim=2] values,
@@ -765,7 +744,7 @@ def group_min_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -779,7 +758,7 @@ def group_min_bin(ndarray[float64_t, ndim=2] out,
                         minx[b, j] = val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -798,8 +777,8 @@ def group_min_bin(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = minx[i, j]
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_max_bin(ndarray[float64_t, ndim=2] out,
                   ndarray[int32_t] counts,
                   ndarray[float64_t, ndim=2] values,
@@ -822,7 +801,7 @@ def group_max_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -836,7 +815,7 @@ def group_max_bin(ndarray[float64_t, ndim=2] out,
                         maxx[b, j] = val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -856,8 +835,8 @@ def group_max_bin(ndarray[float64_t, ndim=2] out,
                 out[i, j] = maxx[i, j]
 
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_ohlc(ndarray[float64_t, ndim=2] out,
                   ndarray[int32_t] counts,
                   ndarray[float64_t, ndim=2] values,
@@ -884,7 +863,7 @@ def group_ohlc(ndarray[float64_t, ndim=2] out,
         raise NotImplementedError
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 if not got_first:
                     out[b, 0] = NA
                     out[b, 1] = NA
@@ -927,8 +906,8 @@ def group_ohlc(ndarray[float64_t, ndim=2] out,
             out[b, 3] = vclose
 
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_mean_bin(ndarray[float64_t, ndim=2] out,
                    ndarray[int32_t] counts,
                    ndarray[float64_t, ndim=2] values,
@@ -947,9 +926,12 @@ def group_mean_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups and i >= bins[b]:
                 b += 1
 
+            if b == ngroups:
+                break
+
             counts[b] += 1
             for j in range(K):
                 val = values[i, j]
@@ -960,7 +942,7 @@ def group_mean_bin(ndarray[float64_t, ndim=2] out,
                     sumx[b, j] += val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -979,8 +961,8 @@ def group_mean_bin(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = sumx[i, j] / count
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_var_bin(ndarray[float64_t, ndim=2] out,
                   ndarray[int32_t] counts,
                   ndarray[float64_t, ndim=2] values,
@@ -1001,7 +983,7 @@ def group_var_bin(ndarray[float64_t, ndim=2] out,
     b = 0
     if K > 1:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
@@ -1016,7 +998,7 @@ def group_var_bin(ndarray[float64_t, ndim=2] out,
                     sumxx[b, j] += val * val
     else:
         for i in range(N):
-            if b < ngroups - 1 and i >= bins[b]:
+            while b < ngroups - 1 and i >= bins[b]:
                 b += 1
 
             counts[b] += 1
diff --git a/pandas/src/sandbox.pyx b/pandas/src/sandbox.pyx
index e42b36e2c..cd21780b9 100644
--- a/pandas/src/sandbox.pyx
+++ b/pandas/src/sandbox.pyx
@@ -421,59 +421,115 @@ def int64_unique(ndarray[int64_t] arr):
 
     return np.sort(uniques[:j])
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def backfill_int64(ndarray[int64_t] old, ndarray[int64_t] new,
-                      limit=None):
-    cdef Py_ssize_t i, j, nleft, nright
-    cdef ndarray[int32_t, ndim=1] indexer
-    cdef object cur, prev
-    cdef int lim
-
-    nleft = len(old)
-    nright = len(new)
-    indexer = np.empty(nright, dtype=np.int32)
-    indexer.fill(-1)
-
-    if limit is None:
-        lim = nright
-    else:
-        # TODO: > 0?
-        lim = limit
+def group_add_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int32_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int32_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
 
-    if nleft == 0 or nright == 0 or new[0] > old[nleft - 1]:
-        return indexer
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
 
-    i = nleft - 1
-    j = nright - 1
+    ngroups = len(bins) + 1
+    N, K = (<object> values).shape
 
-    cur = old[nleft - 1]
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups and i >= bins[b]:
+                b += 1
 
-    while j > 0 and new[j] > cur:
-        j -= 1
+            if b == ngroups:
+                break
 
-    while True:
-        if j == 0:
-            break
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
 
-        if i == 0:
-            while j > 0 and new[j] <= cur:
-                print i, j
-                indexer[j] = i
-                j -= 1
-            break
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    sumx[b, j] += val
+    else:
+        for i in range(N):
+            while b < ngroups and i >= bins[b]:
+                b += 1
 
-        prev = old[i - 1]
+            if b == ngroups:
+                break
 
-        while j > 0 and prev < new[j] <= cur:
-            print i, j
+            counts[b] += 1
+            val = values[i, 0]
 
-            indexer[j] = i
-            j -= 1
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                sumx[b, 0] += val
 
-        print 'decrement', i, j
-        i -= 1
-        cur = prev
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j]
 
-    return indexer
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_add(ndarray[float64_t, ndim=2] out,
+              ndarray[int32_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int32_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    sumx[lab, j] += val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                sumx[lab, 0] += val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j]
 
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 49f410ef5..b9ace5ff7 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -286,128 +286,78 @@ def test_generate_bins():
     binner = np.array([0,3,6,9])
 
     for func in [lib.generate_bins_dt64, generate_bins_generic]:
-        bins, labels = func(values, binner, closed='left', label='left')
+        bins = func(values, binner, closed='left')
+        assert((bins == np.array([2, 5, 6])).all())
+
+        bins = func(values, binner, closed='left')
+        assert((bins == np.array([2, 5, 6])).all())
+
+        bins = func(values, binner, closed='right')
+        assert((bins == np.array([3, 6, 6])).all())
+
+        bins = func(values, binner, closed='right')
+        assert((bins == np.array([3, 6, 6])).all())
+
+class TestBinGroupers(unittest.TestCase):
+
+    def setUp(self):
+        self.obj = np.random.randn(10, 1)
+        self.labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
+        self.bins = np.array([3, 6], dtype=np.int32)
+
+    def test_group_bin_functions(self):
+        funcs = ['add', 'mean', 'prod', 'min', 'max', 'var']
+
+        np_funcs = {
+            'add': np.sum,
+            'mean': np.mean,
+            'prod': np.prod,
+            'min': np.min,
+            'max': np.max,
+            'var': lambda x: x.var(ddof=1) if len(x) >=2 else np.nan
+        }
+
+        # import pandas._sandbox as lib
+        for fname in funcs:
+            args = [getattr(lib, 'group_%s' % fname),
+                    getattr(lib, 'group_%s_bin' % fname),
+                    np_funcs[fname]]
+            self._check_versions(*args)
+
+    def _check_versions(self, irr_func, bin_func, np_func):
+        obj = self.obj
+
+        cts = np.zeros(3, dtype=np.int32)
+        exp = np.zeros((3, 1), np.float64)
+        irr_func(exp, cts, obj, self.labels)
+
+        # bin-based version
+        bins = np.array([3, 6], dtype=np.int32)
+        out  = np.zeros((3, 1), np.float64)
+        counts = np.zeros(len(out), dtype=np.int32)
+        bin_func(out, counts, obj, bins)
+
+        assert_almost_equal(out, exp)
+
+        # duplicate bins
+        bins = np.array([3, 9, 10], dtype=np.int32)
+        out  = np.zeros((4, 1), np.float64)
+        counts = np.zeros(len(out), dtype=np.int32)
+        bin_func(out, counts, obj, bins)
+        exp = np.array([np_func(obj[:3]), np_func(obj[3:9]),
+                        np_func(obj[9:]), np.nan],
+                       dtype=np.float64)
+        assert_almost_equal(out.squeeze(), exp)
+
+        bins = np.array([3, 6, 10, 10], dtype=np.int32)
+        out  = np.zeros((5, 1), np.float64)
+        counts = np.zeros(len(out), dtype=np.int32)
+        bin_func(out, counts, obj, bins)
+        exp = np.array([np_func(obj[:3]), np_func(obj[3:6]),
+                        np_func(obj[6:10]), np.nan, np.nan],
+                       dtype=np.float64)
+        assert_almost_equal(out.squeeze(), exp)
 
-        assert((bins == np.array([2, 5])).all())
-        assert((labels == np.array([0, 3, 6])).all())
-
-        bins, labels = func(values, binner, closed='left', label='right')
-
-        assert((bins == np.array([2, 5])).all())
-        assert((labels == np.array([3, 6, 9])).all())
-
-        bins, labels = func(values, binner, closed='right', label='left')
-
-        assert((bins == np.array([3])).all())
-        assert((labels == np.array([0, 3])).all())
-
-        bins, labels = func(values, binner, closed='right', label='right')
-
-        assert((bins == np.array([3])).all())
-        assert((labels == np.array([3, 6])).all())
-
-def test_group_add_bin():
-    # original group_add
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_add(exp, cts, obj, lab)
-
-    # bin-based group_add
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-    lib.group_add_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
-
-def test_group_mean_bin():
-    # original group_mean
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_mean(exp, cts, obj, lab)
-
-    # bin-based group_mean
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-    lib.group_mean_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
-
-def test_group_prod_bin():
-    # original group_prod
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_prod(exp, cts, obj, lab)
-
-    # bin-based group_prod
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-    lib.group_prod_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
-
-def test_group_min_bin():
-    # original group_min
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_min(exp, cts, obj, lab)
-
-    # bin-based group_min
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-    lib.group_min_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
-
-def test_group_max_bin():
-    # original group_max
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_max(exp, cts, obj, lab)
-
-    # bin-based group_max
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-    lib.group_max_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
-
-def test_group_var_bin():
-    # original group_var
-    obj = np.random.randn(10, 1)
-
-    lab = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)
-    cts = np.array([3, 3, 4], dtype=np.int32)
-    exp = np.zeros((3, 1), np.float64)
-    lib.group_var(exp, cts, obj, lab)
-
-    # bin-based group_var
-    bins = np.array([3, 6], dtype=np.int32)
-    out  = np.zeros((3, 1), np.float64)
-    counts = np.zeros(len(out), dtype=np.int32)
-
-    lib.group_var_bin(out, counts, obj, bins)
-
-    assert_almost_equal(out, exp)
 
 def test_group_ohlc():
     obj = np.random.randn(20)
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index dd7d8ec94..05ddefa09 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -1006,13 +1006,16 @@ def _generate_regular_range(start, end, periods, offset):
         stride = offset.us_stride()
         if periods is None:
             b = Timestamp(start).value
-            e = Timestamp(end).value + stride
+            e = Timestamp(end).value
+            e += stride - e % stride
         elif start is not None:
             b = Timestamp(start).value
             e = b + periods * stride
-        else:
+        elif end is not None:
             e = Timestamp(end).value + stride
             b = e - periods * stride
+        else:
+            raise NotImplementedError
 
         data = np.arange(b, e, stride, dtype=np.int64)
         data = data.view('M8[us]')
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 096c91dda..0f822a382 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -879,9 +879,7 @@ class Tick(DateOffset):
         return self._delta
 
     def us_stride(self):
-        return (self.delta.days * 24 * 60 * 60 * 1000000
-                + self.delta.seconds * 1000000
-                + self.delta.microseconds)
+        return _delta_to_microseconds(self.delta)
 
     def apply(self, other):
         if isinstance(other, (datetime, timedelta)):
@@ -892,6 +890,10 @@ class Tick(DateOffset):
     def rule_code(self):
         return 'T'
 
+def _delta_to_microseconds(delta):
+    return (delta.days * 24 * 60 * 60 * 1000000
+            + delta.seconds * 1000000
+            + delta.microseconds)
 
 class Day(Tick, CacheableOffset):
     _inc = timedelta(1)
diff --git a/pandas/tseries/resample.py b/pandas/tseries/resample.py
index 3d5a25f78..3980880e7 100644
--- a/pandas/tseries/resample.py
+++ b/pandas/tseries/resample.py
@@ -3,7 +3,7 @@ import numpy as np
 from pandas.core.groupby import BinGrouper
 from pandas.tseries.frequencies import to_offset
 from pandas.tseries.index import DatetimeIndex
-from pandas.tseries.offsets import DateOffset, Tick
+from pandas.tseries.offsets import DateOffset
 from pandas.tseries.period import PeriodIndex
 from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
@@ -36,6 +36,9 @@ class TimeGrouper(BinGrouper):
     begin = None
     end = None
     nperiods = None
+    binner = None
+
+    _filter_empty_groups = False
 
     def __init__(self, offset='Min', closed='left', label='left',
                  begin=None, end=None, nperiods=None, axis=None,
@@ -67,25 +70,31 @@ class TimeGrouper(BinGrouper):
             return
 
         if isinstance(self.axis, DatetimeIndex):
-            self.bins, self.binlabels = self._group_timestamps()
+            self.binner, self.bins, self.binlabels = self._group_timestamps()
         elif isinstance(self.axis, PeriodIndex):
-            self.bins, self.binlabels = self._group_periods()
+            self.binner, self.bins, self.binlabels = self._group_periods()
         else:
             raise ValueError('Invalid index: %s' % type(self.axis))
 
     def _group_timestamps(self):
         if self.kind is None or self.kind == 'timestamp':
-            binner = _generate_time_binner(self.axis, self.offset,
-                                           self.begin, self.end,
-                                           self.nperiods)
+            binner = self._generate_time_binner()
 
-            int_axis = self.axis.asi8
-            int_binner = com._ensure_int64(binner)
+            # a little hack
+            if (len(binner) > 2 and self.closed == 'right'
+                and binner[-2] == self.axis[-1]):
+                binner = binner[:-1]
 
             # general version, knowing nothing about relative frequencies
-            bins, labels = lib.generate_bins_dt64(int_axis, int_binner,
-                                                  self.closed, self.label)
-            return bins, labels.view('M8[us]')
+            bins = lib.generate_bins_dt64(self.axis.asi8, binner.asi8,
+                                          self.closed)
+
+            if self.label == 'right':
+                labels = binner[1:]
+            else:
+                labels = binner[:-1]
+
+            return binner, bins, labels
         elif self.kind == 'period':
             index = PeriodIndex(start=self.axis[0], end=self.axis[-1],
                                 freq=self.offset)
@@ -93,7 +102,7 @@ class TimeGrouper(BinGrouper):
             end_stamps = (index + 1).asfreq('D', 's').to_timestamp()
             bins = self.axis.searchsorted(end_stamps, side='left')
 
-            return bins, index
+            return index, bins, index
 
     def _group_periods(self):
         raise NotImplementedError
@@ -106,24 +115,16 @@ class TimeGrouper(BinGrouper):
         if not isinstance(offset, DateOffset):
             raise ValueError("Rule not a recognized offset")
 
-        if self.begin is None:
-            first = Timestamp(self.axis[0] - self.offset)
-        else:
-            first = Timestamp(self.offset.rollback(self.begin))
-
-        if self.end is None:
-            last = Timestamp(self.axis[-1] + self.offset)
-        else:
-            last = Timestamp(self.offset.rollforward(self.end))
-
-        if isinstance(offset, Tick):
-            return np.arange(first.value, last.value + 1,
-                             self.offset.us_stride(), dtype=np.int64)
-
-        result = DatetimeIndex(freq=offset, start=first, end=last,
+        first, last = _get_range_edges(self.axis, self.begin, self.end,
+                                       offset, closed=self.closed)
+        binner = DatetimeIndex(freq=offset, start=first, end=last,
                                periods=self.nperiods)
-        return result.asi8
 
+        return binner
+
+    @property
+    def downsamples(self):
+        return len(self.binlabels) < len(self.axis)
 
     @property
     def names(self):
@@ -142,39 +143,39 @@ class TimeGrouper(BinGrouper):
         return self.binlabels
 
 
-def _generate_time_binner(dtindex, offset, begin=None, end=None, nperiods=None):
-    if isinstance(offset, basestring):
-        offset = to_offset(offset)
+def _get_range_edges(axis, begin, end, offset, closed='left'):
+    # from pandas.tseries.tools import _delta_to_microseconds
+
+    # if isinstance(offset, Tick):
+    #     if begin is None:
+    #         if closed == 'left':
+    #             first = Timestamp(offset.rollback(axis[0]))
+    #         else:
+    #             first = Timestamp(axis[0] - offset)
+    #     else:
+    #         first = Timestamp(offset.rollback(begin))
+
+    #     if end is None:
+    #         if closed == 'left':
+    #             last = Timestamp(axis[-1] + offset)
+    #         else:
+    #             last = Timestamp(offset.rollforward(axis[-1]))
+    #     else:
+    #         last = Timestamp(offset.rollforward(end))
+    # else:
 
     if begin is None:
-        first = Timestamp(dtindex[0] - offset)
+        if closed == 'left':
+            first = Timestamp(offset.rollback(axis[0]))
+        else:
+            first = Timestamp(axis[0] - offset)
     else:
         first = Timestamp(offset.rollback(begin))
 
     if end is None:
-        last = Timestamp(dtindex[-1] + offset)
+        last = Timestamp(axis[-1] + offset)
+        # last = Timestamp(offset.rollforward(axis[-1]))
     else:
         last = Timestamp(offset.rollforward(end))
 
-    if isinstance(offset, Tick):
-        return np.arange(first.value, last.value+1, offset.us_stride(),
-                         dtype=np.int64)
-
-    result = DatetimeIndex(freq=offset, start=first, end=last,
-                           periods=nperiods)
-    return result.asi8
-
-
-def _generate_period_binner(dtindex, offset, begin=None, end=None,
-                            nperiods=None):
-    # if isinstance(offset, basestring):
-    #     offset = to_offset(offset)
-
-    first = dtindex[0]
-    last = dtindex[-1]
-    # if isinstance(offset, Tick):
-    #     return np.arange(first.value, last.value+1, offset.us_stride(),
-    #                      dtype=np.int64)
-
-    return PeriodIndex(freq=offset, start=first, end=last, periods=nperiods)
-
+    return first, last
diff --git a/pandas/tseries/tests/test_resample.py b/pandas/tseries/tests/test_resample.py
index 8177ac215..7fd1a6530 100644
--- a/pandas/tseries/tests/test_resample.py
+++ b/pandas/tseries/tests/test_resample.py
@@ -2,7 +2,7 @@ from datetime import datetime
 
 import numpy as np
 
-from pandas import Series, DataFrame
+from pandas import Series, DataFrame, isnull, notnull
 
 from pandas.tseries.index import date_range
 from pandas.tseries.offsets import Minute
@@ -12,7 +12,7 @@ from pandas.tseries.resample import DatetimeIndex, TimeGrouper
 import unittest
 import nose
 
-from pandas.util.testing import assert_series_equal, assert_almost_equal
+from pandas.util.testing import assert_series_equal
 
 class TestResample(unittest.TestCase):
 
@@ -34,6 +34,7 @@ class TestResample(unittest.TestCase):
         g = s.groupby(b)
 
         self.assertEquals(g.ngroups, 2593)
+        self.assert_(notnull(g.mean()).all())
 
         # construct expected val
         arr = [5] * 2592
@@ -53,14 +54,24 @@ class TestResample(unittest.TestCase):
         self.assertEquals(len(r.columns), 10)
         self.assertEquals(len(r.index), 2593)
 
-    def test_convert_basic(self):
-        s = self.series
+    def test_resample_basic(self):
+        rng = date_range('1/1/2000 00:00:00', '1/1/2000 00:13:00', freq='min')
+        s = Series(np.random.randn(14), index=rng)
+        result = s.resample('5min', how='mean', closed='right', label='right')
+        expected = Series([s[0], s[1:6].mean(), s[6:11].mean(), s[11:].mean()],
+                          index=date_range('1/1/2000', periods=4, freq='5min'))
+        assert_series_equal(result, expected)
 
-        result = s.convert('5Min', how='last')
+        result = s.resample('5min', how='mean', closed='left', label='right')
+        expected = Series([s[:5].mean(), s[5:10].mean(), s[10:].mean()],
+                          index=date_range('1/1/2000 00:05', periods=3,
+                                           freq='5min'))
+        assert_series_equal(result, expected)
 
+        s = self.series
+        result = s.resample('5Min', how='last')
         grouper = TimeGrouper(Minute(5), closed='right', label='right')
         expect = s.groupby(grouper).agg(lambda x: x[-1])
-
         assert_series_equal(result, expect)
 
         # from daily
@@ -70,7 +81,7 @@ class TestResample(unittest.TestCase):
         s = Series(np.random.rand(len(dti)), dti)
 
         # to weekly
-        result = s.convert('w-sun', how='last')
+        result = s.resample('w-sun', how='last')
 
         self.assertEquals(len(result), 3)
         self.assert_((result.index.dayofweek == [6,6,6]).all())
@@ -78,45 +89,45 @@ class TestResample(unittest.TestCase):
         self.assertEquals(result.irow(1), s['1/9/2005'])
         self.assertEquals(result.irow(2), s.irow(-1))
 
-        result = s.convert('W-MON', how='last')
+        result = s.resample('W-MON', how='last')
         self.assertEquals(len(result), 2)
         self.assert_((result.index.dayofweek == [0,0]).all())
         self.assertEquals(result.irow(0), s['1/3/2005'])
         self.assertEquals(result.irow(1), s['1/10/2005'])
 
-        result = s.convert('W-TUE', how='last')
+        result = s.resample('W-TUE', how='last')
         self.assertEquals(len(result), 2)
         self.assert_((result.index.dayofweek == [1,1]).all())
         self.assertEquals(result.irow(0), s['1/4/2005'])
         self.assertEquals(result.irow(1), s['1/10/2005'])
 
-        result = s.convert('W-WED', how='last')
+        result = s.resample('W-WED', how='last')
         self.assertEquals(len(result), 2)
         self.assert_((result.index.dayofweek == [2,2]).all())
         self.assertEquals(result.irow(0), s['1/5/2005'])
         self.assertEquals(result.irow(1), s['1/10/2005'])
 
-        result = s.convert('W-THU', how='last')
+        result = s.resample('W-THU', how='last')
         self.assertEquals(len(result), 2)
         self.assert_((result.index.dayofweek == [3,3]).all())
         self.assertEquals(result.irow(0), s['1/6/2005'])
         self.assertEquals(result.irow(1), s['1/10/2005'])
 
-        result = s.convert('W-FRI', how='last')
+        result = s.resample('W-FRI', how='last')
         self.assertEquals(len(result), 2)
         self.assert_((result.index.dayofweek == [4,4]).all())
         self.assertEquals(result.irow(0), s['1/7/2005'])
         self.assertEquals(result.irow(1), s['1/10/2005'])
 
         # to biz day
-        result = s.convert('B', how='last')
+        result = s.resample('B', how='last')
         self.assertEquals(len(result), 6)
         self.assert_((result.index.dayofweek == [0,1,2,3,4,0]).all())
         self.assertEquals(result.irow(0), s['1/3/2005'])
         self.assertEquals(result.irow(1), s['1/4/2005'])
         self.assertEquals(result.irow(5), s['1/10/2005'])
 
-    def test_convert_upsample(self):
+    def test_resample_upsample(self):
         # from daily
         dti = DatetimeIndex(start=datetime(2005,1,1), end=datetime(2005,1,10),
                             freq='D')
@@ -124,17 +135,17 @@ class TestResample(unittest.TestCase):
         s = Series(np.random.rand(len(dti)), dti)
 
         # to minutely, by padding
-        result = s.convert('Min', method='pad')
+        result = s.resample('Min', method='pad')
         self.assertEquals(len(result), 12961)
         self.assertEquals(result[0], s[0])
         self.assertEquals(result[-1], s[-1])
 
-    def test_convert_ohlc(self):
+    def test_resample_ohlc(self):
         s = self.series
 
         grouper = TimeGrouper(Minute(5), closed='right', label='right')
         expect = s.groupby(grouper).agg(lambda x: x[-1])
-        result = s.convert('5Min', how='ohlc')
+        result = s.resample('5Min', how='ohlc')
 
         self.assertEquals(len(result), len(expect))
         self.assertEquals(len(result.columns), 4)
@@ -151,37 +162,103 @@ class TestResample(unittest.TestCase):
         self.assertEquals(xs['low'], s[1:6].min())
         self.assertEquals(xs['close'], s[5])
 
-    def test_convert_reconvert(self):
+    def test_resample_reresample(self):
         dti = DatetimeIndex(start=datetime(2005,1,1), end=datetime(2005,1,10),
                             freq='D')
         s = Series(np.random.rand(len(dti)), dti)
-        result = s.convert('B').convert('8H')
+        result = s.resample('B').resample('8H')
         self.assertEquals(len(result), 22)
 
     def test_resample_timestamp_to_period(self):
         ts = _simple_ts('1/1/1990', '1/1/2000')
 
-        result = ts.convert('A-DEC', kind='period')
-        expected = ts.convert('A-DEC')
+        result = ts.resample('A-DEC', kind='period')
+        expected = ts.resample('A-DEC')
         expected.index = period_range('1990', '2000', freq='a-dec')
         assert_series_equal(result, expected)
 
-        result = ts.convert('M', kind='period')
-        expected = ts.convert('M')
+        result = ts.resample('A-JUN', kind='period')
+        expected = ts.resample('A-JUN')
+        expected.index = period_range('1990', '2000', freq='a-jun')
+        assert_series_equal(result, expected)
+
+        result = ts.resample('M', kind='period')
+        expected = ts.resample('M')
         expected.index = period_range('1990-01', '2000-01', freq='M')
         assert_series_equal(result, expected)
 
-        result = ts.convert('BM', kind='period')
-        expected = ts.convert('BM')
+        result = ts.resample('M', kind='period')
+        expected = ts.resample('M')
         expected.index = period_range('1990-01', '2000-01', freq='M')
         assert_series_equal(result, expected)
 
+    def test_ohlc_5min(self):
+        def _ohlc(group):
+            if isnull(group).all():
+                return np.repeat(np.nan, 4)
+            return [group[0], group.max(), group.min(), group[-1]]
+
+        rng = date_range('1/1/2000 00:00:00', '1/1/2000 5:59:50',
+                         freq='10s')
+        ts = Series(np.random.randn(len(rng)), index=rng)
+
+        resampled = ts.resample('5min', how='ohlc')
+
+        self.assert_((resampled.ix['1/1/2000 00:00'] == ts[0]).all())
+
+        exp = _ohlc(ts[1:31])
+        self.assert_((resampled.ix['1/1/2000 00:05'] == exp).all())
+
+        exp = _ohlc(ts['1/1/2000 5:55:01':])
+        self.assert_((resampled.ix['1/1/2000 6:00:00'] == exp).all())
+
+
 
 def _simple_ts(start, end, freq='D'):
     rng = date_range(start, end, freq=freq)
     return Series(np.random.randn(len(rng)), index=rng)
 
 
+class TestTimeGrouper(unittest.TestCase):
+
+    def setUp(self):
+        self.ts = Series(np.random.randn(1000),
+                         index=date_range('1/1/2000', periods=1000))
+
+    def test_apply(self):
+        grouper = TimeGrouper('A', label='right', closed='right')
+
+        grouped = self.ts.groupby(grouper)
+
+        f = lambda x: x.order()[-3:]
+
+        applied = grouped.apply(f)
+        expected = self.ts.groupby(lambda x: x.year).apply(f)
+
+        applied.index = applied.index.droplevel(0)
+        expected.index = expected.index.droplevel(0)
+        assert_series_equal(applied, expected)
+
+    def test_count(self):
+        self.ts[::3] = np.nan
+
+        grouper = TimeGrouper('A', label='right', closed='right')
+        result = self.ts.resample('A', how='count')
+
+        expected = self.ts.groupby(lambda x: x.year).count()
+        expected.index = result.index
+
+        assert_series_equal(result, expected)
+
+    def test_numpy_reduction(self):
+        result = self.ts.resample('A', how='prod', closed='right')
+
+        expected = self.ts.groupby(lambda x: x.year).agg(np.prod)
+        expected.index = result.index
+
+        assert_series_equal(result, expected)
+
+
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
                    exit=False)
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index 80e4814ad..f0b34d51e 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -219,27 +219,6 @@ class TestTimeSeries(unittest.TestCase):
         self.assertRaises(AssertionError, rng2.get_indexer, rng,
                           method='pad')
 
-
-    def test_ohlc_5min(self):
-        def _ohlc(group):
-            if isnull(group).all():
-                return np.repeat(np.nan, 4)
-            return [group[0], group.max(), group.min(), group[-1]]
-
-        rng = date_range('1/1/2000 00:00:00', '1/1/2000 5:59:50',
-                         freq='10s')
-        ts = Series(np.random.randn(len(rng)), index=rng)
-
-        converted = ts.convert('5min', how='ohlc')
-
-        self.assert_((converted.ix['1/1/2000 00:00'] == ts[0]).all())
-
-        exp = _ohlc(ts[1:31])
-        self.assert_((converted.ix['1/1/2000 00:05'] == exp).all())
-
-        exp = _ohlc(ts['1/1/2000 5:55:01':])
-        self.assert_((converted.ix['1/1/2000 6:00:00'] == exp).all())
-
     def test_frame_ctor_datetime64_column(self):
         rng = date_range('1/1/2000 00:00:00', '1/1/2000 1:59:50',
                          freq='10s')
@@ -436,6 +415,10 @@ class TestTimeSeries(unittest.TestCase):
         self.assert_(result.tz == rng.tz)
         self.assert_(result.freq == rng.freq)
 
+    def test_date_range_gen_error(self):
+        rng = date_range('1/1/2000 00:00', '1/1/2000 00:18', freq='5min')
+        self.assertEquals(len(rng), 4)
+
 def _skip_if_no_pytz():
     try:
         import pytz
@@ -898,44 +881,6 @@ class TestDatetime64(unittest.TestCase):
 
     # TODO: test merge & concat with datetime64 block
 
-class TestTimeGrouper(unittest.TestCase):
-
-    def setUp(self):
-        self.ts = Series(np.random.randn(1000),
-                         index=date_range('1/1/2000', periods=1000))
-
-    def test_apply(self):
-        grouper = TimeGrouper('A', label='right', closed='right')
-
-        grouped = self.ts.groupby(grouper)
-
-        f = lambda x: x.order()[-3:]
-
-        applied = grouped.apply(f)
-        expected = self.ts.groupby(lambda x: x.year).apply(f)
-
-        applied.index = applied.index.droplevel(0)
-        expected.index = expected.index.droplevel(0)
-        assert_series_equal(applied, expected)
-
-    def test_count(self):
-        self.ts[::3] = np.nan
-
-        grouper = TimeGrouper('A', label='right', closed='right')
-        result = self.ts.convert('A', how='count')
-
-        expected = self.ts.groupby(lambda x: x.year).count()
-        expected.index = result.index
-
-        assert_series_equal(result, expected)
-
-    def test_numpy_reduction(self):
-        result = self.ts.convert('A', how='prod', closed='right')
-
-        expected = self.ts.groupby(lambda x: x.year).agg(np.prod)
-        expected.index = result.index
-
-        assert_series_equal(result, expected)
 
 class TestNewOffsets(unittest.TestCase):
 
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index 05e37d6d8..f114a3825 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -21,6 +21,10 @@ except ImportError: # pragma: no cover
     print 'Please install python-dateutil via easy_install or some method!'
     raise # otherwise a 2nd import won't show the message
 
+def _delta_to_microseconds(delta):
+    return (delta.days * 24 * 60 * 60 * 1000000
+            + delta.seconds * 1000000
+            + delta.microseconds)
 
 def _infer_tzinfo(start, end):
     def _infer(a, b):
