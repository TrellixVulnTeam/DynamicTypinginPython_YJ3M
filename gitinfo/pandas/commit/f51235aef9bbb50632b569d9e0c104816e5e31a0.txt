commit f51235aef9bbb50632b569d9e0c104816e5e31a0
Author: immerrr <immerrr@gmail.com>
Date:   Wed Apr 16 21:39:55 2014 +0400

    CLN: rename Block.ref_locs -> mgr_locs to reduce confusion
    
    PERF: add BlockPlacement class to optimize range-like cases
    
    CLN: unify get_slice & take/reindex ops along axis0
    
    TST: add slicing/taking/reindexing tests for BlockManagers
    
    CLN: remove unused code

diff --git a/pandas/computation/tests/test_eval.py b/pandas/computation/tests/test_eval.py
old mode 100755
new mode 100644
diff --git a/pandas/core/format.py b/pandas/core/format.py
index c76693e16..43eb0e890 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -1292,7 +1292,7 @@ class CSVFormatter(object):
                                   float_format=self.float_format,
                                   date_format=self.date_format)
 
-            for col_loc, col in zip(b.ref_locs, d):
+            for col_loc, col in zip(b.mgr_locs, d):
                 # self.data is a preallocated list
                 self.data[col_loc] = col
 
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index c32ca065d..fcd2e65af 100755
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -1045,7 +1045,7 @@ class DataFrame(NDFrame):
         for block in selfsorted._data.blocks:
             newb = block2d_to_blocknd(
                 values=block.values.T,
-                placement=block.ref_locs, shape=shape,
+                placement=block.mgr_locs, shape=shape,
                 labels=[major_labels, minor_labels],
                 ref_items=selfsorted.columns)
             new_blocks.append(newb)
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 7c5c77a29..3f2ecd8af 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1689,7 +1689,7 @@ class NDFrame(PandasObject):
             labels, method, level, limit=limit, copy_if_needed=True)
         return self._reindex_with_indexers(
             {axis: [new_index, indexer]}, method=method, fill_value=fill_value,
-            limit=limit, copy=copy).__finalize__(self)
+            limit=limit, copy=copy)
 
     def _reindex_with_indexers(self, reindexers, method=None,
                                fill_value=np.nan, limit=None, copy=False,
@@ -1712,7 +1712,8 @@ class NDFrame(PandasObject):
             # TODO: speed up on homogeneous DataFrame objects
             new_data = new_data.reindex_indexer(index, indexer, axis=baxis,
                                                 fill_value=fill_value,
-                                                allow_dups=allow_dups)
+                                                allow_dups=allow_dups,
+                                                copy=copy)
 
         if copy and new_data is self._data:
             new_data = new_data.copy()
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index b284e3c63..f650b41ff 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -2243,7 +2243,7 @@ class NDFrameGroupBy(GroupBy):
             # see if we can cast the block back to the original dtype
             result = block._try_cast_result(result)
 
-            newb = make_block(result, placement=block.ref_locs)
+            newb = make_block(result, placement=block.mgr_locs)
             new_blocks.append(newb)
 
         if len(new_blocks) == 0:
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 9c5564941..7465fad39 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -3,7 +3,7 @@ import itertools
 import re
 import operator
 from datetime import datetime, timedelta
-from collections import defaultdict
+from collections import defaultdict, deque
 
 import numpy as np
 from pandas.core.base import PandasObject
@@ -14,7 +14,7 @@ from pandas.core.common import (_possibly_downcast_to_dtype, isnull, notnull,
                                 ABCSparseSeries, _infer_dtype_from_scalar,
                                 _is_null_datelike_scalar,
                                 is_timedelta64_dtype, is_datetime64_dtype,)
-from pandas.core.index import Index, MultiIndex, _ensure_index, _all_indexes_same
+from pandas.core.index import Index, Int64Index, MultiIndex, _ensure_index
 from pandas.core.indexing import (_maybe_convert_indices, _length_of_indexer)
 import pandas.core.common as com
 from pandas.sparse.array import _maybe_to_sparse, SparseArray
@@ -30,6 +30,10 @@ from pandas.compat import (range, lrange, lmap, callable, map, zip, u,
 from pandas.tseries.timedeltas import _coerce_scalar_to_timedelta_type
 
 
+
+from pandas.lib import BlockPlacement
+
+
 class Block(PandasObject):
 
     """
@@ -38,7 +42,7 @@ class Block(PandasObject):
 
     Index-ignorant; let the container take care of that
     """
-    __slots__ = ['_ref_locs', 'values', 'ndim']
+    __slots__ = ['_mgr_locs', 'values', 'ndim']
     is_numeric = False
     is_float = False
     is_integer = False
@@ -55,20 +59,19 @@ class Block(PandasObject):
     _ftype = 'dense'
 
     def __init__(self, values, placement, ndim=None, fastpath=False):
-
         if ndim is None:
             ndim = values.ndim
-
-        if values.ndim != ndim:
+        elif values.ndim != ndim:
             raise ValueError('Wrong number of dimensions')
+        self.ndim = ndim
 
-        if len(placement) != len(values):
-            raise ValueError('Wrong number of items passed %d, placement implies '
-                             '%d' % (len(values), len(placement)))
-
-        self._ref_locs = np.array(placement, dtype=np.int_, copy=True)
+        self.mgr_locs = placement
         self.values = values
-        self.ndim = ndim
+
+        if len(self.mgr_locs) != len(self.values):
+            raise ValueError('Wrong number of items passed %d,'
+                             ' placement implies %d' % (
+                                 len(self.values), len(self.mgr_locs)))
 
     @property
     def _consolidate_key(self):
@@ -88,8 +91,28 @@ class Block(PandasObject):
         return np.nan
 
     @property
-    def ref_locs(self):
-        return self._ref_locs
+    def mgr_locs(self):
+        return self._mgr_locs
+
+    def make_block_same_class(self, values, placement, copy=False,
+                              **kwargs):
+        """
+        Wrap given values in a block of same type as self.
+
+        `kwargs` are used in SparseBlock override.
+
+        """
+        if copy:
+            values = values.copy()
+        return make_block(values, placement, klass=self.__class__,
+                          fastpath=True)
+
+    @mgr_locs.setter
+    def mgr_locs(self, new_mgr_locs):
+        if not isinstance(new_mgr_locs, BlockPlacement):
+            new_mgr_locs = BlockPlacement(new_mgr_locs)
+
+        self._mgr_locs = new_mgr_locs
 
     def __unicode__(self):
 
@@ -104,7 +127,8 @@ class Block(PandasObject):
 
             shape = ' x '.join([com.pprint_thing(s) for s in self.shape])
             result = '%s: %s, %s, dtype: %s' % (
-                name, com.pprint_thing(self.ref_locs), shape, self.dtype)
+                name, com.pprint_thing(self.mgr_locs.indexer), shape,
+                self.dtype)
 
         return result
 
@@ -112,29 +136,37 @@ class Block(PandasObject):
         return len(self.values)
 
     def __getstate__(self):
-        return self.ref_locs, self.values
+        return self.mgr_locs.indexer, self.values
 
     def __setstate__(self, state):
-        self._ref_locs, self.values = state
+        self.mgr_locs = BlockPlacement(state[0])
+        self.values = state[1]
         self.ndim = self.values.ndim
 
     def _slice(self, slicer):
         """ return a slice of my values """
         return self.values[slicer]
 
-    def _getitem_block(self, slicer):
+    def getitem_block(self, slicer, new_mgr_locs=None):
         """
         Perform __getitem__-like, return result as block.
+
+        As of now, only supports slices that preserve dimensionality.
+
         """
-        if isinstance(slicer, tuple):
-            axis0_slicer = slicer[0]
-        else:
-            axis0_slicer = slicer
+        if new_mgr_locs is None:
+            if isinstance(slicer, tuple):
+                axis0_slicer = slicer[0]
+            else:
+                axis0_slicer = slicer
+            new_mgr_locs = self.mgr_locs[axis0_slicer]
+
+        new_values = self._slice(slicer)
+
+        if new_values.ndim != self.ndim:
+            raise ValueError("Only same dim slicing is allowed")
 
-        return self.__class__(values=self.values[slicer],
-                              ndim=self.ndim,
-                              fastpath=True,
-                              placement=self.ref_locs[axis0_slicer])
+        return self.make_block_same_class(new_values, new_mgr_locs)
 
     @property
     def shape(self):
@@ -152,12 +184,6 @@ class Block(PandasObject):
     def ftype(self):
         return "%s:%s" % (self.dtype, self._ftype)
 
-    def as_block(self, result):
-        """ if we are not a block, then wrap as a block, must have compatible shape """
-        if not isinstance(result, Block):
-            result = make_block(values=result, placement=self.ref_locs,)
-        return result
-
     def merge(self, other):
         return _merge_blocks([self, other])
 
@@ -175,31 +201,7 @@ class Block(PandasObject):
                                  fill_value=fill_value, mask_info=mask_info)
         return make_block(new_values,
                           ndim=self.ndim, fastpath=True,
-                          placement=self.ref_locs)
-
-    def reindex_items_from(self, indexer, method=None,
-                           fill_value=None, limit=None, copy=True):
-        """
-        Reindex to only those items contained in the input set of items
-
-        E.g. if you have ['a', 'b'], and the input items is ['b', 'c', 'd'],
-        then the resulting items will be ['b']
-
-        Returns
-        -------
-        reindexed : Block
-        """
-        if fill_value is None:
-            fill_value = self.fill_value
-
-        # single block only
-        assert self.ndim == 1
-        new_values = com.take_1d(self.values, indexer,
-                                 fill_value=fill_value)
-        block = make_block(new_values,
-                           ndim=self.ndim, fastpath=True,
-                           placement=np.arange(len(new_values)))
-        return block
+                          placement=self.mgr_locs)
 
     def get(self, item):
         loc = self.items.get_loc(item)
@@ -220,43 +222,18 @@ class Block(PandasObject):
 
     def delete(self, loc):
         """
-        Returns
-        -------
-        y : Block (new object)
+        Delete given loc(-s) from block in-place.
         """
-        new_values = np.delete(self.values, loc, 0)
-        return make_block(new_values,
-                          ndim=self.ndim, klass=self.__class__, fastpath=True,
-                          placement=np.delete(self.ref_locs, loc))
-
-    def split_block_at(self, item):
-        """
-        Split block into zero or more blocks around columns with given label,
-        for "deleting" a column without having to copy data by returning views
-        on the original array.
-
-        Returns
-        -------
-        generator of Block
-        """
-        loc = self.items.get_loc(item)
-
-        if type(loc) == slice or type(loc) == int:
-            mask = [True] * len(self)
-            mask[loc] = False
-        else:  # already a mask, inverted
-            mask = -loc
-
-        for s, e in com.split_ranges(mask):
-            # FIXME: drop this function
-            yield make_block(self.values[s:e],
-                             ndim=self.ndim,
-                             klass=self.__class__,
-                             fastpath=True)
+        self.values = np.delete(self.values, loc, 0)
+        self.mgr_locs = self.mgr_locs.delete(loc)
 
     def apply(self, func, **kwargs):
         """ apply the function to my values; return a block if we are not one """
-        return self.as_block(func(self.values))
+        result = func(self.values)
+        if not isinstance(result, Block):
+            result = make_block(values=result, placement=self.mgr_locs,)
+
+        return result
 
     def fillna(self, value, limit=None, inplace=False, downcast=None):
         if not self._can_hold_na:
@@ -308,7 +285,7 @@ class Block(PandasObject):
 
             nv = _possibly_downcast_to_dtype(values, dtypes)
             return [make_block(nv, ndim=self.ndim,
-                               fastpath=True, placement=self.ref_locs)]
+                               fastpath=True, placement=self.mgr_locs)]
 
         # ndim > 1
         if dtypes is None:
@@ -321,7 +298,7 @@ class Block(PandasObject):
         # item-by-item
         # this is expensive as it splits the blocks items-by-item
         blocks = []
-        for i, rl in enumerate(self.ref_locs):
+        for i, rl in enumerate(self.mgr_locs):
 
             if dtypes == 'infer':
                 dtype = 'infer'
@@ -364,7 +341,7 @@ class Block(PandasObject):
                 values = com._astype_nansafe(self.values.ravel(), dtype, copy=True)
                 values = values.reshape(self.values.shape)
             newb = make_block(values,
-                              ndim=self.ndim, placement=self.ref_locs,
+                              ndim=self.ndim, placement=self.mgr_locs,
                               fastpath=True, dtype=dtype, klass=klass)
         except:
             if raise_on_error is True:
@@ -387,38 +364,6 @@ class Block(PandasObject):
 
         return [self.copy()] if copy else [self]
 
-    def prepare_for_merge(self, **kwargs):
-        """ a regular block is ok to merge as is """
-        return self
-
-    def post_merge(self, items, **kwargs):
-        """ we are non-sparse block, try to convert to a sparse block(s) """
-        sparsified_mask = self.items.isin(items.keys())
-
-        if not sparsified_mask.any():
-            return self
-
-        new_blocks = []
-        for i in sparsified_mask.nonzero()[0]:
-            item = self.items[i]
-            ref_loc = self.ref_locs[i]
-
-            dtypes = set(items[item])
-            # this is a safe bet with multiple dtypes
-            dtype = list(dtypes)[0] if len(dtypes) == 1 else np.float64
-
-            new_blocks.append(make_block(
-                values=SparseArray(self.iget(i), dtype=dtype),
-                placement=[ref_loc]))
-
-        nonsparsified_locs = (~sparsified_mask).nonzero()[0]
-        if len(nonsparsified_locs):
-            new_blocks.append(make_block(
-                values=self.values[nonsparsified_locs],
-                placement=self.ref_locs[nonsparsified_locs]))
-
-        return new_blocks
-
     def _can_hold_element(self, value):
         raise NotImplementedError()
 
@@ -490,7 +435,7 @@ class Block(PandasObject):
             values = values.copy()
         return make_block(values, ndim=self.ndim,
                           klass=self.__class__, fastpath=True,
-                          placement=self.ref_locs)
+                          placement=self.mgr_locs)
 
     def replace(self, to_replace, value, inplace=False, filter=None,
                 regex=False):
@@ -500,7 +445,7 @@ class Block(PandasObject):
         compatibility."""
         mask = com.mask_missing(self.values, to_replace)
         if filter is not None:
-            filtered_out = ~Index(self.ref_locs, copy=False).isin(filter)
+            filtered_out = ~self.mgr_locs.isin(filter)
             mask[filtered_out.nonzero()[0]] = False
 
         if not mask.any():
@@ -573,7 +518,7 @@ class Block(PandasObject):
             values = self._try_coerce_result(values)
             values = self._try_cast_result(values, dtype)
             return [make_block(transf(values),
-                               ndim=self.ndim, placement=self._ref_locs,
+                               ndim=self.ndim, placement=self.mgr_locs,
                                fastpath=True)]
         except (ValueError, TypeError) as detail:
             raise
@@ -629,7 +574,7 @@ class Block(PandasObject):
             # need to go column by column
             new_blocks = []
             if self.ndim > 1:
-                for i, ref_loc in enumerate(self.ref_locs):
+                for i, ref_loc in enumerate(self.mgr_locs):
                     m = mask[i]
                     v = new_values[i]
 
@@ -660,7 +605,7 @@ class Block(PandasObject):
             else:
                 nv = _putmask_smart(new_values, mask, new)
                 new_blocks.append(make_block(values=nv,
-                                             placement=self.ref_locs,
+                                             placement=self.mgr_locs,
                                              fastpath=True))
 
             return new_blocks
@@ -669,7 +614,7 @@ class Block(PandasObject):
             return [self]
 
         return [make_block(new_values,
-                           placement=self.ref_locs, fastpath=True)]
+                           placement=self.mgr_locs, fastpath=True)]
 
     def interpolate(self, method='pad', axis=0, index=None,
                     values=None, inplace=False, limit=None,
@@ -750,7 +695,7 @@ class Block(PandasObject):
 
         blocks = [make_block(values,
                              ndim=self.ndim, klass=self.__class__,
-                             fastpath=True, placement=self.ref_locs)]
+                             fastpath=True, placement=self.mgr_locs)]
         return self._maybe_downcast(blocks, downcast)
 
     def _interpolate(self, method=None, index=None, values=None,
@@ -789,23 +734,37 @@ class Block(PandasObject):
 
         blocks = [make_block(interp_values,
                              ndim=self.ndim, klass=self.__class__,
-                             fastpath=True, placement=self.ref_locs)]
+                             fastpath=True, placement=self.mgr_locs)]
         return self._maybe_downcast(blocks, downcast)
 
-    def take(self, indexer, new_axis, axis=1):
-        if axis < 1:
-            raise AssertionError('axis must be at least 1, got %d' % axis)
-        new_values = com.take_nd(self.values, indexer, axis=axis,
-                                 allow_fill=False)
+    def take_nd(self, indexer, axis, new_mgr_locs=None, fill_tuple=None):
+        """
+        Take values according to indexer and return them as a block.bb
 
-        # need to preserve the ref_locs and just shift them
-        # GH6121
-        ref_locs = None
-        if not new_axis.is_unique:
-            ref_locs = self._ref_locs
+        """
+        if fill_tuple is None:
+            fill_value = self.fill_value
+            new_values = com.take_nd(self.get_values(), indexer, axis=axis,
+                                     allow_fill=False)
+        else:
+            fill_value = fill_tuple[0]
+            new_values = com.take_nd(self.get_values(), indexer, axis=axis,
+                                     allow_fill=True, fill_value=fill_value)
+
+        if new_mgr_locs is None:
+            if axis == 0:
+                slc = lib.indexer_as_slice(indexer)
+                if slc is not None:
+                    new_mgr_locs = self.mgr_locs[slc]
+                else:
+                    new_mgr_locs = self.mgr_locs[indexer]
+            else:
+                new_mgr_locs = self.mgr_locs
 
-        return [make_block(new_values, ndim=self.ndim,
-                           klass=self.__class__, placement=ref_locs, fastpath=True)]
+        if new_values.dtype != self.dtype:
+            return make_block(new_values, new_mgr_locs)
+        else:
+            return self.make_block_same_class(new_values, new_mgr_locs)
 
     def get_values(self, dtype=None):
         return self.values
@@ -815,7 +774,7 @@ class Block(PandasObject):
         new_values = com.diff(self.values, n, axis=1)
         return [make_block(values=new_values,
                            ndim=self.ndim, fastpath=True,
-                           placement=self.ref_locs)]
+                           placement=self.mgr_locs)]
 
     def shift(self, periods, axis=0):
         """ shift the block by periods, possibly upcast """
@@ -841,7 +800,7 @@ class Block(PandasObject):
 
         return [make_block(new_values,
                            ndim=self.ndim, fastpath=True,
-                           placement=self.ref_locs)]
+                           placement=self.mgr_locs)]
 
     def eval(self, func, other, raise_on_error=True, try_cast=False):
         """
@@ -933,7 +892,7 @@ class Block(PandasObject):
             result = self._try_cast_result(result)
 
         return [make_block(result, ndim=self.ndim,
-                           fastpath=True, placement=self.ref_locs)]
+                           fastpath=True, placement=self.mgr_locs)]
 
     def where(self, other, cond, align=True, raise_on_error=True,
               try_cast=False):
@@ -1024,7 +983,7 @@ class Block(PandasObject):
                 result = self._try_cast_result(result)
 
             return make_block(result,
-                              ndim=self.ndim, placement=self.ref_locs)
+                              ndim=self.ndim, placement=self.mgr_locs)
 
         # might need to separate out blocks
         axis = cond.ndim - 1
@@ -1038,7 +997,7 @@ class Block(PandasObject):
                 r = self._try_cast_result(
                     result.take(m.nonzero()[0], axis=axis))
                 result_blocks.append(make_block(r.T,
-                                                placement=self.ref_locs[m]))
+                                                placement=self.mgr_locs[m]))
 
         return result_blocks
 
@@ -1048,11 +1007,13 @@ class Block(PandasObject):
 
 
 class NumericBlock(Block):
+    __slots__ = ()
     is_numeric = True
     _can_hold_na = True
 
 
 class FloatOrComplexBlock(NumericBlock):
+    __slots__ = ()
 
     def equals(self, other):
         if self.dtype != other.dtype or self.shape != other.shape: return False
@@ -1060,6 +1021,7 @@ class FloatOrComplexBlock(NumericBlock):
         return ((left == right) | (np.isnan(left) & np.isnan(right))).all()
 
 class FloatBlock(FloatOrComplexBlock):
+    __slots__ = ()
     is_float = True
     _downcast_dtype = 'int64'
 
@@ -1100,6 +1062,7 @@ class FloatBlock(FloatOrComplexBlock):
 
 
 class ComplexBlock(FloatOrComplexBlock):
+    __slots__ = ()
     is_complex = True
 
     def _can_hold_element(self, element):
@@ -1120,6 +1083,7 @@ class ComplexBlock(FloatOrComplexBlock):
 
 
 class IntBlock(NumericBlock):
+    __slots__ = ()
     is_integer = True
     _can_hold_na = False
 
@@ -1140,6 +1104,7 @@ class IntBlock(NumericBlock):
 
 
 class TimeDeltaBlock(IntBlock):
+    __slots__ = ()
     is_timedelta = True
     _can_hold_na = True
     is_numeric = False
@@ -1224,6 +1189,7 @@ class TimeDeltaBlock(IntBlock):
 
 
 class BoolBlock(NumericBlock):
+    __slots__ = ()
     is_bool = True
     _can_hold_na = False
 
@@ -1251,7 +1217,9 @@ class BoolBlock(NumericBlock):
                                               inplace=inplace, filter=filter,
                                               regex=regex)
 
+
 class ObjectBlock(Block):
+    __slots__ = ()
     is_object = True
     _can_hold_na = True
 
@@ -1284,7 +1252,7 @@ class ObjectBlock(Block):
         blocks = []
         if by_item and not self._is_single_block:
 
-            for i, rl in enumerate(self.ref_locs):
+            for i, rl in enumerate(self.mgr_locs):
                 values = self.iget(i)
 
                 values = com._possibly_convert_objects(
@@ -1304,7 +1272,7 @@ class ObjectBlock(Block):
                 convert_numeric=convert_numeric
             ).reshape(self.values.shape)
             blocks.append(make_block(values,
-                                     ndim=self.ndim, placement=self.ref_locs))
+                                     ndim=self.ndim, placement=self.mgr_locs))
 
         return blocks
 
@@ -1456,17 +1424,17 @@ class ObjectBlock(Block):
         if filter is None:
             filt = slice(None)
         else:
-            filt = (Index(self.ref_locs, copy=False)
-                    .isin(filter).nonzero()[0])
+            filt = self.mgr_locs.isin(filter).nonzero()[0]
 
         new_values[filt] = f(new_values[filt])
 
         return [self if inplace else
                 make_block(new_values,
-                           fastpath=True, placement=self.ref_locs)]
+                           fastpath=True, placement=self.mgr_locs)]
 
 
 class DatetimeBlock(Block):
+    __slots__ = ()
     is_datetime = True
     _can_hold_na = True
 
@@ -1548,7 +1516,7 @@ class DatetimeBlock(Block):
         np.putmask(values, mask, value)
         return [self if inplace else
                 make_block(values,
-                           fastpath=True, placement=self.ref_locs)]
+                           fastpath=True, placement=self.mgr_locs)]
 
     def to_native_types(self, slicer=None, na_rep=None, date_format=None,
                         **kwargs):
@@ -1611,9 +1579,8 @@ class DatetimeBlock(Block):
 
 
 class SparseBlock(Block):
-
     """ implement as a list of sparse arrays of the same dtype """
-    __slots__ = ['_ref_locs', 'ndim', 'values']
+    __slots__ = ()
     is_sparse = True
     is_numeric = True
     _can_hold_na = True
@@ -1625,25 +1592,23 @@ class SparseBlock(Block):
                  ndim=None, fastpath=False,):
 
         # kludgetastic
-        if ndim is not None:
-            if ndim == 1:
-                ndim = 1
-            elif ndim > 2:
-                ndim = ndim
-        else:
+        if ndim is None:
             if len(placement) != 1:
                 ndim = 1
             else:
                 ndim = 2
         self.ndim = ndim
 
-        self._ref_locs = np.array(placement, dtype=np.int_, copy=True)
+        self.mgr_locs = placement
+
+        if not isinstance(values, SparseArray):
+            raise TypeError("values must be SparseArray")
 
         self.values = values
 
     @property
     def shape(self):
-        return (len(self.ref_locs), self.sp_index.length)
+        return (len(self.mgr_locs), self.sp_index.length)
 
     @property
     def itemsize(self):
@@ -1651,6 +1616,7 @@ class SparseBlock(Block):
 
     @property
     def fill_value(self):
+        #return np.nan
         return self.values.fill_value
 
     @fill_value.setter
@@ -1669,7 +1635,8 @@ class SparseBlock(Block):
         # reset the sparse values
         self.values = SparseArray(v, sparse_index=self.sp_index,
                                   kind=self.kind, dtype=v.dtype,
-                                  fill_value=self.fill_value, copy=False)
+                                  fill_value=self.values.fill_value,
+                                  copy=False)
 
     def iget(self, col):
         if col != 0:
@@ -1716,19 +1683,38 @@ class SparseBlock(Block):
         return values
 
     def copy(self, deep=True):
-        return self.make_block(values=self.values,
-                               sparse_index=self.sp_index,
-                               kind=self.kind, copy=deep,
-                               placement=self.ref_locs)
-
-    def make_block(self, values, placement,
-                   sparse_index=None, kind=None, dtype=None, fill_value=None,
-                   copy=False, fastpath=True):
+        return self.make_block_same_class(values=self.values,
+                                          sparse_index=self.sp_index,
+                                          kind=self.kind, copy=deep,
+                                          placement=self.mgr_locs)
+
+    def make_block_same_class(self, values, placement,
+                              sparse_index=None, kind=None, dtype=None,
+                              fill_value=None, copy=False, fastpath=True):
         """ return a new block """
         if dtype is None:
             dtype = self.dtype
         if fill_value is None:
-            fill_value = self.fill_value
+            fill_value = self.values.fill_value
+
+        # if not isinstance(values, SparseArray) and values.ndim != self.ndim:
+        #     raise ValueError("ndim mismatch")
+
+        if values.ndim == 2:
+            nitems = values.shape[0]
+
+            if nitems == 0:
+                # kludgy, but SparseBlocks cannot handle slices, where the
+                # output is 0-item, so let's convert it to a dense block: it
+                # won't take space since there's 0 items, plus it will preserve
+                # the dtype.
+                return make_block(np.empty(values.shape, dtype=dtype),
+                                  placement, fastpath=True,)
+            elif nitems > 1:
+                raise ValueError("Only 1-item 2d sparse blocks are supported")
+            else:
+                values = values.reshape(values.shape[1])
+
         new_values = SparseArray(values, sparse_index=sparse_index,
                                  kind=kind or self.kind, dtype=dtype,
                                  fill_value=fill_value, copy=copy)
@@ -1740,8 +1726,8 @@ class SparseBlock(Block):
 
         values = com.interpolate_2d(
             self.values.to_dense(), method, axis, limit, fill_value)
-        return self.make_block(values=values,
-                               placement=self.ref_locs)
+        return self.make_block_same_class(values=values,
+                                          placement=self.mgr_locs)
 
     def fillna(self, value, limit=None, inplace=False, downcast=None):
         # we may need to upcast our fill to match our dtype
@@ -1750,9 +1736,9 @@ class SparseBlock(Block):
         if issubclass(self.dtype.type, np.floating):
             value = float(value)
         values = self.values if inplace else self.values.copy()
-        return [self.make_block(values=values.get_values(value),
-                                fill_value=value, placement=self.ref_locs)]
-
+        return [self.make_block_same_class(values=values.get_values(value),
+                                           fill_value=value,
+                                           placement=self.mgr_locs)]
 
     def shift(self, periods, axis=0):
         """ shift the block by periods """
@@ -1770,15 +1756,7 @@ class SparseBlock(Block):
             new_values[:periods] = fill_value
         else:
             new_values[periods:] = fill_value
-        return [self.make_block(new_values, placement=self.ref_locs)]
-
-    def take(self, indexer, new_axis, axis=1):
-        """ going to take our items
-            along the long dimension"""
-        if axis < 1:
-            raise AssertionError('axis must be at least 1, got %d' % axis)
-
-        return [self.make_block(self.values.take(indexer))]
+        return [self.make_block_same_class(new_values, placement=self.mgr_locs)]
 
     def reindex_axis(self, indexer, method=None, axis=1, fill_value=None,
                      limit=None, mask_info=None):
@@ -1791,41 +1769,9 @@ class SparseBlock(Block):
         # taking on the 0th axis always here
         if fill_value is None:
             fill_value = self.fill_value
-        return self.make_block(self.values.take(indexer),
-                               fill_value=fill_value,
-                               placement=self.ref_locs)
-
-    def reindex_items_from(self, indexer, method=None,
-                           fill_value=None, limit=None, copy=True):
-        """
-        Reindex to only those items contained in the input set of items
-
-        E.g. if you have ['a', 'b'], and the input items is ['b', 'c', 'd'],
-        then the resulting items will be ['b']
-
-        Returns
-        -------
-        reindexed : Block
-        """
-
-        # 1-d always
-        if indexer is None:
-            indexer = np.arange(len(self.ref_locs))
-
-        # single block only
-        assert self.ndim == 1
-        new_values = com.take_1d(self.values.values, indexer)
-
-        # fill if needed
-        if method is not None or limit is not None:
-            if fill_value is None:
-                fill_value = self.fill_value
-            new_values = com.interpolate_2d(new_values, method=method,
-                                            limit=limit, fill_value=fill_value)
-
-        return self.make_block(new_values,
-                               copy=copy,
-                               placement=np.arange(len(indexer)))
+        return self.make_block_same_class(self.values.take(indexer),
+                                          fill_value=fill_value,
+                                          placement=self.mgr_locs)
 
     def sparse_reindex(self, new_index):
         """ sparse reindex and return a new block
@@ -1833,13 +1779,8 @@ class SparseBlock(Block):
         values = self.values
         values = values.sp_index.to_int_index().reindex(
             values.sp_values.astype('float64'), values.fill_value, new_index)
-        return self.make_block(values, sparse_index=new_index,
-                               placement=self.ref_locs)
-
-    def split_block_at(self, item):
-        if len(self.items) == 1 and item == self.items[0]:
-            return []
-        return super(SparseBlock, self).split_block_at(self, item)
+        return self.make_block_same_class(values, sparse_index=new_index,
+                               placement=self.mgr_locs)
 
     def _try_cast_result(self, result, dtype=None):
         return result
@@ -1953,15 +1894,15 @@ class BlockManager(PandasObject):
     This is *not* a public API class
     """
     __slots__ = ['axes', 'blocks', '_ndim', '_shape', '_known_consolidated',
-                 '_is_consolidated', '_has_sparse', '_ref_locs']
+                 '_is_consolidated', '_blknos', '_blklocs']
 
     def __init__(self, blocks, axes, do_integrity_check=True, fastpath=True):
         self.axes = [_ensure_index(ax) for ax in axes]
-        self.blocks = blocks
+        self.blocks = tuple(blocks)
 
         for block in blocks:
             if block.is_sparse:
-                if len(block.ref_locs) != 1:
+                if len(block.mgr_locs) != 1:
                     raise AssertionError("Sparse block refers to multiple items")
             else:
                 if self.ndim != block.ndim:
@@ -1972,10 +1913,9 @@ class BlockManager(PandasObject):
         if do_integrity_check:
             self._verify_integrity()
 
-        self._has_sparse = False
         self._consolidate_check()
 
-        self._rebuild_ref_locs()
+        self._rebuild_blknos_and_blklocs()
 
     def make_empty(self, axes=None):
         """ return an empty BlockManager with the items axis of len 0 """
@@ -2016,23 +1956,60 @@ class BlockManager(PandasObject):
 
         self.axes[axis] = new_labels
 
-    def _rebuild_ref_locs(self):
+    def rename_axis(self, mapper, axis, copy=True):
         """
-        Update mgr._ref_locs according to blk.ref_locs.
+        Rename one of axes.
+
+        Parameters
+        ----------
+        mapper : unary callable
+        axis : int
+        copy : boolean, default True
+
         """
-        blocks = np.empty(self.shape[0], dtype=np.object_)
-        blk_locs = np.empty(self.shape[0], dtype=np.int_)
-        blk_locs.fill(-1)
+        obj = self.copy(deep=copy)
+        obj.set_axis(axis, _transform_index(self.axes[axis], mapper))
+        return obj
 
-        for blk in self.blocks:
-            rl = blk.ref_locs
-            blocks[rl] = blk
-            blk_locs[rl] = np.arange(len(rl))
+    def add_prefix(self, prefix):
+        f = (str(prefix) + '%s').__mod__
+        return self.rename_axis(f, axis=0)
+
+    def add_suffix(self, suffix):
+        f = ('%s' + str(suffix)).__mod__
+        return self.rename_axis(f, axis=0)
+
+    @property
+    def _is_single_block(self):
+        if self.ndim == 1:
+            return True
+
+        if len(self.blocks) != 1:
+            return False
+
+        blk = self.blocks[0]
+        return (blk.mgr_locs.is_slice_like and
+                blk.mgr_locs.as_slice == slice(0, len(self), 1))
+
+    def _rebuild_blknos_and_blklocs(self):
+        """
+        Update mgr._blknos / mgr._blklocs.
+        """
+        new_blknos = np.empty(self.shape[0], dtype=np.int64)
+        new_blklocs = np.empty(self.shape[0], dtype=np.int64)
+        new_blknos.fill(-1)
+        new_blklocs.fill(-1)
+
+        for blkno, blk in enumerate(self.blocks):
+            rl = blk.mgr_locs
+            new_blknos[rl.indexer] = blkno
+            new_blklocs[rl.indexer] = np.arange(len(rl))
 
-        if (blk_locs == -1).any():
+        if (new_blknos == -1).any():
             raise AssertionError("Gaps in blk ref_locs")
 
-        self._ref_locs = lib.fast_zip([blocks, blk_locs])
+        self._blknos = new_blknos
+        self._blklocs = new_blklocs
 
     # make items read only for now
     def _get_items(self):
@@ -2055,14 +2032,16 @@ class BlockManager(PandasObject):
         return self._get_counts(lambda b: b.ftype)
 
     def get_dtypes(self):
-        return [rl[0].dtype for rl in self._ref_locs]
+        dtypes = np.array([blk.dtype for blk in self.blocks])
+        return dtypes.take(self._blknos)
 
     def get_ftypes(self):
-        return [rl[0].ftype for rl in self._ref_locs]
+        ftypes = np.array([blk.ftype for blk in self.blocks])
+        return ftypes.take(self._blknos)
 
     def __getstate__(self):
         block_values = [b.values for b in self.blocks]
-        block_items = [self.items.take(b.ref_locs) for b in self.blocks]
+        block_items = [self.items[b.mgr_locs.indexer] for b in self.blocks]
         axes_array = [ax for ax in self.axes]
         return axes_array, block_values, block_items
 
@@ -2083,15 +2062,14 @@ class BlockManager(PandasObject):
             blk = make_block(values,
                              placement=self.axes[0].get_indexer(items))
             blocks.append(blk)
-        self.blocks = blocks
+        self.blocks = tuple(blocks)
 
         self._post_setstate()
 
     def _post_setstate(self):
         self._is_consolidated = False
         self._known_consolidated = False
-        self._rebuild_ref_locs()
-        self._set_has_sparse()
+        self._rebuild_blknos_and_blklocs()
 
     def __len__(self):
         return len(self.items)
@@ -2110,7 +2088,7 @@ class BlockManager(PandasObject):
 
     def _verify_integrity(self):
         mgr_shape = self.shape
-        tot_items = sum(len(x.ref_locs) for x in self.blocks)
+        tot_items = sum(len(x.mgr_locs) for x in self.blocks)
         for block in self.blocks:
             if not block.is_sparse and block.shape[1:] != mgr_shape[1:]:
                 construction_error(tot_items, block.shape[1:], self.axes)
@@ -2140,10 +2118,14 @@ class BlockManager(PandasObject):
 
         result_blocks = []
 
+        # filter kwarg is used in replace-* family of methods
         if filter is not None:
-            # filter kwarg is used in replace-* family of methods
             filter_locs = set(self.items.get_indexer_for(filter))
-            kwargs['filter'] = filter_locs
+            if len(filter_locs) == len(self.items):
+                # All items are included, as if there were no filtering
+                filter = None
+            else:
+                kwargs['filter'] = filter_locs
 
         if f == 'where' and kwargs.get('align', True):
             align_copy = True
@@ -2167,13 +2149,12 @@ class BlockManager(PandasObject):
 
         for b in self.blocks:
             if filter is not None:
-                valid_locs = filter_locs.intersection(b.ref_locs)
-                if not valid_locs:
+                if not b.mgr_locs.isin(filter_locs).any():
                     result_blocks.append(b)
                     continue
 
             if aligned_args:
-                b_items = self.items.take(b.ref_locs)
+                b_items = self.items[b.mgr_locs.indexer]
 
                 for k, obj in aligned_args.items():
                     axis = getattr(obj, '_info_axis_number', 0)
@@ -2265,7 +2246,7 @@ class BlockManager(PandasObject):
                     else:
                         # get our mask for this element, sized to this
                         # particular block
-                        m = masks[i][b.ref_locs]
+                        m = masks[i][b.mgr_locs.indexer]
                         if m.any():
                             new_rb.extend(b.putmask(m, d, inplace=True))
                         else:
@@ -2289,10 +2270,6 @@ class BlockManager(PandasObject):
         ftypes = [blk.ftype for blk in self.blocks]
         self._is_consolidated = len(ftypes) == len(set(ftypes))
         self._known_consolidated = True
-        self._set_has_sparse()
-
-    def _set_has_sparse(self):
-        self._has_sparse = any((blk.is_sparse for blk in self.blocks))
 
     @property
     def is_mixed_type(self):
@@ -2337,14 +2314,15 @@ class BlockManager(PandasObject):
         if len(blocks) == 0:
             return self.make_empty()
 
-        indexer = np.sort(np.concatenate([b.ref_locs for b in blocks]))
+        # FIXME: optimization potential
+        indexer = np.sort(np.concatenate([b.mgr_locs.as_array for b in blocks]))
         inv_indexer = _invert_reordering(indexer)
         new_items = self.items.take(indexer)
 
         new_blocks = []
         for b in blocks:
             b = b.copy(deep=copy)
-            b._ref_locs = inv_indexer.take(b.ref_locs)
+            b.mgr_locs = inv_indexer.take(b.mgr_locs.as_array)
             new_blocks.append(b)
 
         new_axes = list(self.axes)
@@ -2352,36 +2330,22 @@ class BlockManager(PandasObject):
         return self.__class__(new_blocks, new_axes, do_integrity_check=False)
 
     def get_slice(self, slobj, axis=0):
-        new_axes = list(self.axes)
-        new_axes[axis] = new_axes[axis][slobj]
+        if axis >= self.ndim:
+            raise IndexError("Requested axis not found in manager")
 
         if axis == 0:
-            new_items = new_axes[0]
-
-            # we want to preserver the view of a single-block
-            if (len(self.blocks) == 1 and
-                (self.blocks[0]._ref_locs == np.arange(self.shape[0])).all()):
-                blk = self.blocks[0]
-                newb = make_block(blk._slice(slobj),
-                                  klass=blk.__class__, fastpath=True,
-                                  placement=np.arange(len(new_items)))
-
-                new_blocks = [newb]
-            else:
-                return self.reindex_indexer(
-                    new_items, indexer=np.arange(len(self.items))[slobj],
-                    axis=0, allow_dups=True)
+            new_blocks = self._slice_take_blocks_ax0(slobj)
         else:
-            slicer = [slice(None)] * self.ndim
+            slicer = [slice(None)] * (axis + 1)
             slicer[axis] = slobj
+            slicer = tuple(slicer)
+            new_blocks = [blk.getitem_block(slicer) for blk in self.blocks]
 
-            new_blocks = [make_block(block._slice(slicer),
-                                     klass=block.__class__,
-                                     fastpath=True,
-                                     placement=block.ref_locs)
-                          for block in self.blocks]
+        new_axes = list(self.axes)
+        new_axes[axis] = new_axes[axis][slobj]
 
-        bm = self.__class__(new_blocks, new_axes, do_integrity_check=False)
+        bm = self.__class__(new_blocks, new_axes, do_integrity_check=False,
+                            fastpath=True)
         bm._consolidate_inplace()
         return bm
 
@@ -2421,9 +2385,7 @@ class BlockManager(PandasObject):
         else:
             mgr = self
 
-        if (len(mgr.blocks) == 1 and
-            (mgr.blocks[0]._ref_locs is None or
-             (mgr.blocks[0]._ref_locs == np.arange(mgr.shape[0])).all())):
+        if self._is_single_block:
             return mgr.blocks[0].get_values()
         else:
             return mgr._interleave()
@@ -2436,12 +2398,25 @@ class BlockManager(PandasObject):
         dtype = _interleaved_dtype(self.blocks)
 
         result = np.empty(self.shape, dtype=dtype)
+
+        if result.shape[0] == 0:
+            # Workaround for numpy 1.7 bug:
+            #
+            #     >>> a = np.empty((0,10))
+            #     >>> a[slice(0,0)]
+            #     array([], shape=(0, 10), dtype=float64)
+            #     >>> a[[]]
+            #     Traceback (most recent call last):
+            #       File "<stdin>", line 1, in <module>
+            #     IndexError: index 0 is out of bounds for axis 0 with size 0
+            return result
+
         itemmask = np.zeros(self.shape[0])
 
         for blk in self.blocks:
-            rl = blk.ref_locs
-            result[rl] = blk.get_values(dtype)
-            itemmask[rl] = 1
+            rl = blk.mgr_locs
+            result[rl.indexer] = blk.get_values(dtype)
+            itemmask[rl.indexer] = 1
 
         if not itemmask.all():
             raise AssertionError('Some items were not contained in blocks')
@@ -2477,14 +2452,14 @@ class BlockManager(PandasObject):
             for blk in self.blocks:
                 newb = make_block(values=blk.values[slicer],
                                   klass=blk.__class__, fastpath=True,
-                                  placement=blk.ref_locs)
+                                  placement=blk.mgr_locs)
                 new_blocks.append(newb)
         elif len(self.blocks) == 1:
             block = self.blocks[0]
             vals = block.values[slicer]
             if copy:
                 vals = vals.copy()
-            new_blocks = [make_block(values=vals, placement=block.ref_locs,
+            new_blocks = [make_block(values=vals, placement=block.mgr_locs,
                                      klass=block.__class__, fastpath=True,)]
 
         return self.__class__(new_blocks, new_axes)
@@ -2515,8 +2490,8 @@ class BlockManager(PandasObject):
         result = np.empty(n, dtype=dtype)
         for blk in self.blocks:
             # Such assignment may incorrectly coerce NaT to None
-            # result[blk.ref_locs] = blk._slice((slice(None), loc))
-            for i, rl in enumerate(blk.ref_locs):
+            # result[blk.mgr_locs] = blk._slice((slice(None), loc))
+            for i, rl in enumerate(blk.mgr_locs):
                 result[rl] = blk._try_coerce_result(blk.iget((i, loc)))
 
         return result
@@ -2538,12 +2513,11 @@ class BlockManager(PandasObject):
 
     def _consolidate_inplace(self):
         if not self.is_consolidated():
-            self.blocks = _consolidate(self.blocks)
+            self.blocks = tuple(_consolidate(self.blocks))
 
             self._is_consolidated = True
             self._known_consolidated = True
-            self._set_has_sparse()
-            self._rebuild_ref_locs()
+            self._rebuild_blknos_and_blklocs()
 
     def get(self, item):
         """
@@ -2574,8 +2548,7 @@ class BlockManager(PandasObject):
                                         indexer=indexer, axis=0, allow_dups=True)
 
     def iget(self, i):
-        b, loc = self._ref_locs[i]
-        return b.iget(loc)
+        return self.blocks[self._blknos[i]].iget(self._blklocs[i])
 
     def get_scalar(self, tup):
         """
@@ -2583,8 +2556,10 @@ class BlockManager(PandasObject):
         """
         full_loc = list(ax.get_loc(x)
                         for ax, x in zip(self.axes, tup))
-        blk, blk_loc = self._ref_locs[full_loc[0]]
-        full_loc[0] = blk_loc
+        blk = self.blocks[self._blknos[full_loc[0]]]
+        full_loc[0] = self._blklocs[full_loc[0]]
+
+        # FIXME: this may return non-upcasted types?
         return blk.values[tuple(full_loc)]
 
     def delete(self, item):
@@ -2595,29 +2570,35 @@ class BlockManager(PandasObject):
 
         is_deleted = np.zeros(self.shape[0], dtype=np.bool_)
         is_deleted[indexer] = True
-        ref_loc_offset = is_deleted.cumsum()
+        ref_loc_offset = -is_deleted.cumsum()
 
-        new_items = self.items[~is_deleted]
-        new_blocks = []
+        is_blk_deleted = [False] * len(self.blocks)
 
-        for blk in self.blocks:
-            brl = blk.ref_locs
-            blk_del = is_deleted[brl]
-            blk_del_count = np.count_nonzero(blk_del)
+        if isinstance(indexer, int):
+            affected_start = indexer
+        else:
+            affected_start = is_deleted.nonzero()[0][0]
 
-            if blk_del_count == len(brl):
-                continue
+        for blkno, _ in _fast_count_smallints(self._blknos[affected_start:]):
+            blk = self.blocks[blkno]
+            bml = blk.mgr_locs
+            blk_del = is_deleted[bml.indexer].nonzero()[0]
 
-            blk._ref_locs -= ref_loc_offset[brl]
-            if blk_del_count != 0:
-                blk = blk._getitem_block(~blk_del)
+            if len(blk_del) == len(bml):
+                is_blk_deleted[blkno] = True
+                continue
+            elif len(blk_del) != 0:
+                blk.delete(blk_del)
+                bml = blk.mgr_locs
 
-            new_blocks.append(blk)
+            blk.mgr_locs = bml.add(ref_loc_offset[bml.indexer])
 
-        self.axes[0] = new_items
-        self.blocks = new_blocks
+        # FIXME: use Index.delete as soon as it uses fastpath=True
+        self.axes[0] = self.items[~is_deleted]
+        self.blocks = tuple(b for blkno, b in enumerate(self.blocks)
+                            if not is_blk_deleted[blkno])
         self._shape = None
-        self._rebuild_ref_locs()
+        self._rebuild_blknos_and_blklocs()
 
     def set(self, item, value, check=False):
         """
@@ -2626,22 +2607,22 @@ class BlockManager(PandasObject):
         if check, then validate that we are not setting the same data in-place
         """
         # FIXME: refactor, clearly separate broadcasting & zip-like assignment
-        is_sparse = isinstance(value, SparseArray)
+        value_is_sparse = isinstance(value, SparseArray)
 
-        if is_sparse:
+        if value_is_sparse:
             assert self.ndim == 2
 
-            def value_getitem(locs):
+            def value_getitem(placement):
                 return value
         else:
             if value.ndim == self.ndim - 1:
                 value = value.reshape((1,) + value.shape)
 
-                def value_getitem(locs):
+                def value_getitem(placement):
                     return value
             else:
-                def value_getitem(locs):
-                    return value[locs]
+                def value_getitem(placement):
+                    return value[placement.indexer]
             if value.shape[1:] != self.shape[1:]:
                 raise AssertionError('Shape of new values must be compatible '
                                      'with manager shape')
@@ -2656,49 +2637,72 @@ class BlockManager(PandasObject):
         if isinstance(loc, int):
             loc = [loc]
 
-        ref_locs = self._ref_locs[loc]
+        blknos = self._blknos[loc]
+        blklocs = self._blklocs[loc]
 
         unfit_mgr_locs = []
         unfit_val_locs = []
-        for blk, blk_locs, val_locs in ref_loc_groupby_block(ref_locs):
+        removed_blknos = []
+        for blkno, val_locs in _get_blkno_placements(blknos, len(self.blocks),
+                                                     group=True):
+            blk = self.blocks[blkno]
+            blk_locs = blklocs[val_locs.indexer]
             if blk.should_store(value):
                 blk.set(blk_locs, value_getitem(val_locs), check=check)
             else:
-                unfit_mgr_locs.append(blk.ref_locs[blk_locs])
+                unfit_mgr_locs.append(blk.mgr_locs.as_array[blk_locs])
                 unfit_val_locs.append(val_locs)
 
-                new_blk_ref_locs = np.delete(blk.ref_locs, blk_locs, axis=0)
-                new_blk_len = len(new_blk_ref_locs)
-                if not new_blk_len:
-                    self.blocks.remove(blk)
+                # If all block items are unfit, schedule the block for removal.
+                if len(val_locs) == len(blk.mgr_locs):
+                    removed_blknos.append(blkno)
                 else:
-                    blk.values = np.delete(blk.values, blk_locs, axis=0)
-                    blk._ref_locs = new_blk_ref_locs
-                    self._ref_locs[new_blk_ref_locs] = \
-                        lib.fast_zip([np.array([blk] * new_blk_len),
-                                      np.arange(new_blk_len)])
+                    self._blklocs[blk.mgr_locs.indexer] = -1
+                    blk.delete(blk_locs)
+                    self._blklocs[blk.mgr_locs.indexer] = np.arange(len(blk))
+
+        if len(removed_blknos):
+            # Remove blocks & update blknos accordingly
+            is_deleted = np.zeros(self.nblocks, dtype=np.bool_)
+            is_deleted[removed_blknos] = True
+
+            new_blknos = np.empty(self.nblocks, dtype=np.int_)
+            new_blknos.fill(-1)
+            new_blknos[~is_deleted] = np.arange(self.nblocks -
+                                                len(removed_blknos))
+            self._blknos = new_blknos.take(self._blknos, axis=0)
+            self.blocks = tuple(blk for i, blk in enumerate(self.blocks)
+                                if i not in set(removed_blknos))
 
         if unfit_val_locs:
-            unfit_val_locs = np.concatenate(unfit_val_locs)
             unfit_mgr_locs = np.concatenate(unfit_mgr_locs)
-            unfit_count = len(unfit_val_locs)
-
-            if is_sparse:
-                for mgr_loc in unfit_mgr_locs:
-                    new_block = make_block(values=value.copy(),
-                                           ndim=self.ndim,
-                                           placement=[mgr_loc])
-                    self.blocks.append(new_block)
-                    self._ref_locs[mgr_loc] = (new_block, 0)
+            unfit_count = len(unfit_mgr_locs)
+
+            new_blocks = []
+            if value_is_sparse:
+                # This code (ab-)uses the fact that sparse blocks contain only
+                # one item.
+                new_blocks.extend(
+                    make_block(values=value.copy(), ndim=self.ndim,
+                               placement=slice(mgr_loc, mgr_loc + 1))
+                    for mgr_loc in unfit_mgr_locs)
+
+                self._blknos[unfit_mgr_locs] = (np.arange(unfit_count) +
+                                                len(self.blocks))
+                self._blklocs[unfit_mgr_locs] = 0
+
             else:
-                new_block = make_block(values=value[unfit_val_locs],
-                                       ndim=self.ndim,
-                                       placement=unfit_mgr_locs)
+                # unfit_val_locs contains BlockPlacement objects
+                unfit_val_items = unfit_val_locs[0].append(unfit_val_locs[1:])
 
-                self.blocks.append(new_block)
-                self._ref_locs[unfit_mgr_locs] = lib.fast_zip([
-                    np.array([new_block] * unfit_count, dtype=np.object_),
-                    np.arange(unfit_count)])
+                new_blocks.append(
+                    make_block(values=value_getitem(unfit_val_items),
+                               ndim=self.ndim, placement=unfit_mgr_locs))
+
+                self._blknos[unfit_mgr_locs] = len(self.blocks)
+                self._blklocs[unfit_mgr_locs] = np.arange(unfit_count)
+
+            self.blocks += tuple(new_blocks)
 
             # Newly created block's dtype may already be present.
             self._known_consolidated = False
@@ -2723,132 +2727,169 @@ class BlockManager(PandasObject):
         if not isinstance(loc, int):
             raise TypeError("loc must be int")
 
-        new_items = self.items.insert(loc, item)
         block = make_block(values=value,
                            ndim=self.ndim,
-                           placement=[loc])
-        new_ref_locs = np.insert(self._ref_locs, loc, None, axis=0)
-        new_ref_locs[loc] = (block, 0)
+                           placement=slice(loc, loc+1))
 
-        for blk in self.blocks:
-            blk._ref_locs[blk._ref_locs >= loc] += 1
+        for blkno, count in _fast_count_smallints(self._blknos[loc:]):
+            blk = self.blocks[blkno]
+            if count == len(blk.mgr_locs):
+                blk.mgr_locs = blk.mgr_locs.add(1)
+            else:
+                new_mgr_locs = blk.mgr_locs.as_array.copy()
+                new_mgr_locs[new_mgr_locs >= loc] += 1
+                blk.mgr_locs = new_mgr_locs
+
+        if loc == self._blklocs.shape[0]:
+            # np.append is a lot faster (at least in numpy 1.7.1), let's use it
+            # if we can.
+            self._blklocs = np.append(self._blklocs, 0)
+            self._blknos = np.append(self._blknos, len(self.blocks))
+        else:
+            self._blklocs = np.insert(self._blklocs, loc, 0)
+            self._blknos = np.insert(self._blknos, loc, len(self.blocks))
 
-        self.blocks.append(block)
-        self.axes[0] = new_items
+        self.axes[0] = self.items.insert(loc, item)
+
+        self.blocks += (block,)
         self._shape = None
-        self._ref_locs = new_ref_locs
 
         self._known_consolidated = False
 
         if len(self.blocks) > 100:
             self._consolidate_inplace()
 
-    def reindex_axis(self, new_axis, axis, method=None, limit=None,
+    def reindex_axis(self, new_index, axis, method=None, limit=None,
                      fill_value=None, copy=True):
-        mgr = self if not copy else self.copy(deep=True)
-
-        new_axis = _ensure_index(new_axis)
-        new_axis, indexer = mgr.axes[axis].reindex(
-            new_axis, method=method, limit=limit, copy_if_needed=True)
+        """
+        Conform block manager to new index.
+        """
+        new_index = _ensure_index(new_index)
+        new_index, indexer = self.axes[axis].reindex(
+            new_index, method=method, limit=limit, copy_if_needed=True)
 
-        return mgr.reindex_indexer(new_axis, indexer, axis=axis,
-                                   fill_value=fill_value)
+        return self.reindex_indexer(new_index, indexer, axis=axis,
+                                    fill_value=fill_value, copy=copy)
 
     def reindex_indexer(self, new_axis, indexer, axis, fill_value=None,
-                        allow_dups=False):
+                        allow_dups=False, copy=True):
         """
+        Parameters
+        ----------
+        new_axis : Index
+        indexer : ndarray of int64 or None
+        axis : int
+        fill_value : object
+        allow_dups : bool
+
         pandas-indexer with -1's only.
         """
+
+        if indexer is None:
+            if new_axis is self.axes[axis] and not copy:
+                return self
+
+            result = self.copy(deep=copy)
+            result.axes = list(self.axes)
+            result.axes[axis] = new_axis
+            return result
+
+        self._consolidate_inplace()
+
         # trying to reindex on an axis with duplicates
         if (not allow_dups and not self.axes[axis].is_unique
-            and indexer is not None and len(indexer)):
+            and len(indexer)):
             raise ValueError("cannot reindex from a duplicate axis")
 
         if axis >= self.ndim:
-            raise AssertionError("Requested axis not found in manager")
-
-        # FIXME: this code comes from generic.py, see if any of that is needed
-        # elif (baxis == 0 and
-        #         index is not new_data.axes[baxis]):
-        #     new_data = new_data.reindex_items(index, copy=copy,
-        #                                       fill_value=fill_value)
-
-        # elif (baxis > 0 and index is not None and
-        #         index is not new_data.axes[baxis]):
-        #     new_data = new_data.copy(deep=copy)
-        #     new_data.set_axis(baxis, index)
+            raise IndexError("Requested axis not found in manager")
 
         if axis == 0:
-            new_blocks = self._get_blocks_for_items_indexer(indexer,
-                                                            fill_value)
+            new_blocks = self._slice_take_blocks_ax0(
+                indexer, fill_tuple=(fill_value,))
         else:
-            # TODO: is this faster than blk.reindex_axis?
-            # return self.apply('take',
-            #                   axes=new_axes,
-            #                   indexer=indexer,
-            #                   ref_items=new_axes[0],
-            #                   new_axis=new_axes[axis],
-            #                   axis=axis)
-            new_blocks = [blk.reindex_axis(indexer, axis=axis,
-                                           fill_value=fill_value)
+            new_blocks = [blk.take_nd(indexer, axis=axis,
+                                      fill_tuple=(fill_value if fill_value is not None else
+                                                  blk.fill_value,))
                           for blk in self.blocks]
 
         new_axes = list(self.axes)
         new_axes[axis] = new_axis
         return self.__class__(new_blocks, new_axes)
 
-    def _get_blocks_for_items_indexer(self, indexer, fill_value):
+    def _slice_take_blocks_ax0(self, slice_or_indexer, fill_tuple=None):
         """
-        Reindex blocks at axis=0 (overloaded for SingleBlockManager).
+        Slice/take blocks along axis=0.
+
+        Overloaded for SingleBlock
 
         Returns
         -------
         new_blocks : list of Block
 
         """
-        # fill_value[0] == None will group soon-to-be-added items under None
-        # fill_value[1] is an arbitrary integer (it's ignored)
-        new_ref_locs = com.take_1d(self._ref_locs, indexer,
-                                   fill_value=(None, 0))
-        new_blocks = []
-        for blk, blk_locs, mgr_locs in ref_loc_groupby_block(new_ref_locs):
-            if blk is None:
-                new_blocks.append(self._make_na_block(
+
+        allow_fill = fill_tuple is not None
+
+        sl_type, slobj, sllen = _preprocess_slice_or_indexer(
+            slice_or_indexer, self.shape[0], allow_fill=allow_fill)
+
+        if self._is_single_block:
+            blk = self.blocks[0]
+
+            if sl_type in ('slice', 'mask'):
+                return [blk.getitem_block(slobj,
+                                          new_mgr_locs=slice(0, sllen))]
+            elif not allow_fill or self.ndim == 1:
+                if allow_fill and fill_tuple[0] is None:
+                    _, fill_value = com._maybe_promote(blk.dtype)
+                    fill_tuple = (fill_value,)
+
+                return [blk.take_nd(slobj, axis=0,
+                                    new_mgr_locs=slice(0, sllen),
+                                    fill_tuple=fill_tuple)]
+
+        if sl_type in ('slice', 'mask'):
+            blknos = self._blknos[slobj]
+            blklocs = self._blklocs[slobj]
+        else:
+            blknos = com.take_1d(self._blknos, slobj, fill_value=-1,
+                                 allow_fill=allow_fill)
+            blklocs = com.take_1d(self._blklocs, slobj, fill_value=-1,
+                                  allow_fill=allow_fill)
+
+        # When filling blknos, make sure blknos is updated before appending to
+        # blocks list, that way new blkno is exactly len(blocks).
+        #
+        # FIXME: mgr_groupby_blknos must return mgr_locs in ascending order,
+        # pytables serialization will break otherwise.
+        blocks = []
+        for blkno, mgr_locs in _get_blkno_placements(blknos, len(self.blocks),
+                                                     group=True):
+            if blkno == -1:
+                # If we've got here, fill_tuple was not None.
+                fill_value = fill_tuple[0]
+
+                blocks.append(self._make_na_block(
                     placement=mgr_locs, fill_value=fill_value))
             else:
+                blk = self.blocks[blkno]
+
                 # Otherwise, slicing along items axis is necessary.
                 if blk.is_sparse:
-                    # If it's a sparse block, it's easy:
-                    #
-                    # - it can only contain 1 item
-                    # - if blk is here, the item wasn't deleted
-                    # - if blk wasn't handled above, the item is multiplied
-                    #
-                    # Hence the block is replicated.
+                    # A sparse block, it's easy, because there's only one item
+                    # and each mgr loc is a copy of that single item.
                     for mgr_loc in mgr_locs:
                         newblk = blk.copy(deep=True)
-                        newblk._ref_locs = np.array([mgr_loc])
-                        new_blocks.append(newblk)
+                        newblk.mgr_locs = slice(mgr_loc, mgr_loc + 1)
+                        blocks.append(newblk)
 
                 else:
-                    # FIXME: this hack makes sure post-reindex blocks enumerate
-                    # manager locs in ascending order.  It was implemented to
-                    # make pytables serialization test happy and should be
-                    # removed once the codebase successfully switches to
-                    # axis-oblivious blocks & blockmanagers.
-                    order = np.argsort(mgr_locs)
-                    blk_locs = blk_locs.take(order)
-                    mgr_locs = mgr_locs.take(order)
-
-                    new_values = com.take_1d(blk.values, blk_locs,
-                                             axis=0, allow_fill=False)
-                    newblk = blk.__class__(values=new_values,
-                                           ndim=blk.ndim,
-                                           fastpath=True,
-                                           placement=mgr_locs,)
-                    new_blocks.append(newblk)
-
-        return new_blocks
+                    blocks.append(blk.take_nd(
+                        blklocs[mgr_locs.indexer], axis=0,
+                        new_mgr_locs=mgr_locs, fill_tuple=None))
+
+        return blocks
 
     def _make_na_block(self, placement, fill_value=None):
         # TODO: infer dtypes other than float64 from fill_value
@@ -2891,13 +2932,14 @@ class BlockManager(PandasObject):
                                          right=other.items, rsuffix=rsuffix)
         new_items = _concat_indexes([l, r])
 
-        new_blocks = []
-        for blocks, offset in [(self.blocks, 0),
-                               (other.blocks, self.shape[0])]:
-            for blk in blocks:
-                blk = blk.copy(deep=False)
-                blk._ref_locs += offset
-                new_blocks.append(blk)
+        new_blocks = [blk.copy(deep=False)
+                      for blk in self.blocks]
+
+        offset = self.shape[0]
+        for blk in other.blocks:
+            blk = blk.copy(deep=False)
+            blk.mgr_locs = blk.mgr_locs.add(offset)
+            new_blocks.append(blk)
 
         new_axes = list(self.axes)
         new_axes[0] = new_items
@@ -2916,39 +2958,6 @@ class BlockManager(PandasObject):
                 return False
         return True
 
-    def rename_axis(self, mapper, axis, copy=True):
-        """
-        Rename one of axes.
-
-        Parameters
-        ----------
-        mapper : unary callable
-        axis : int
-        copy : boolean, default True
-
-        """
-        new_axis = _transform_index(self.axes[axis], mapper)
-
-        if axis != 0:
-            new_blocks = self.blocks
-        else:
-            new_blocks = []
-            for block in self.blocks:
-                newb = block.copy(deep=copy)
-                new_blocks.append(newb)
-
-        new_axes = list(self.axes)
-        new_axes[axis] = new_axis
-        return self.__class__(new_blocks, new_axes)
-
-    def add_prefix(self, prefix):
-        f = (('%s' % prefix) + '%s').__mod__
-        return self.rename_axis(f, axis=0)
-
-    def add_suffix(self, suffix):
-        f = ('%s' + ('%s' % suffix)).__mod__
-        return self.rename_axis(f, axis=0)
-
     def equals(self, other):
         self_axes, other_axes = self.axes, other.axes
         if len(self_axes) != len(other_axes):
@@ -2960,26 +2969,16 @@ class BlockManager(PandasObject):
         return all(block.equals(oblock) for block, oblock in
                    zip(self.blocks, other.blocks))
 
-    def group_blocks_by_ftype(self):
-        """
-        Combine blocks into map: ftype -> [blk0, blk1, ...].
-
-        """
-        bm = defaultdict(list)
-        for b in self.blocks:
-            bm[str(b.ftype)].append(b)
-        return bm
-
 
 class SingleBlockManager(BlockManager):
-
     """ manage a single block with """
+
     ndim = 1
     _is_consolidated = True
     _known_consolidated = True
-    __slots__ = ['axes', 'blocks']
+    __slots__ = ()
 
-    def __init__(self, block, axis, do_integrity_check=False, fastpath=True):
+    def __init__(self, block, axis, do_integrity_check=False, fastpath=False):
 
         if isinstance(axis, list):
             if len(axis) != 1:
@@ -2999,12 +2998,7 @@ class SingleBlockManager(BlockManager):
                     raise ValueError('Cannot create SingleBlockManager with '
                                      'more than 1 block')
                 block = block[0]
-            if not isinstance(block, Block):
-                block = make_block(block, ndim=1, fastpath=True,
-                                   placement=np.arange(len(axis)))
-
         else:
-
             self.axes = [_ensure_index(axis)]
 
             # create the block here
@@ -3021,9 +3015,10 @@ class SingleBlockManager(BlockManager):
                                      'more than 1 block')
                 block = block[0]
 
-            if not isinstance(block, Block):
-                block = make_block(block, axis, ndim=1,
-                                   fastpath=True, placement=None)
+        if not isinstance(block, Block):
+            block = make_block(block,
+                               placement=slice(0, len(axis)),
+                               ndim=1, fastpath=True)
 
         self.blocks = [block]
 
@@ -3038,26 +3033,6 @@ class SingleBlockManager(BlockManager):
     def _values(self):
         return self._block.values
 
-    @property
-    def _has_sparse(self):
-        return self._block.is_sparse
-
-    def _set_has_sparse(self):
-        # _has_sparse is a property, nothing to set here
-        pass
-
-    # def apply(self, f, axes=None, do_integrity_check=False, **kwargs):
-    #     """
-    #     fast path for SingleBlock Manager
-
-    #     ssee also BlockManager.apply
-    #     """
-    #     applied = getattr(self._block, f)(**kwargs)
-    #     bm = self.__class__(applied, axes or self.axes,
-    #                         do_integrity_check=do_integrity_check)
-    #     bm._consolidate_inplace()
-    #     return bm
-
     def reindex(self, new_axis, indexer=None, method=None, fill_value=None,
                 limit=None, copy=True):
         # if we are the same and don't copy, just return
@@ -3088,44 +3063,19 @@ class SingleBlockManager(BlockManager):
                                             limit=limit, fill_value=fill_value)
 
         if self._block.is_sparse:
-            make_block = self._block.make_block
+            make_block = self._block.make_block_same_class
 
         block = make_block(new_values, copy=copy,
-                           placement=np.arange(len(new_axis)))
+                           placement=slice(0, len(new_axis)))
 
-        # block = self._block.reindex_items_from(new_axis, indexer=indexer,
-        #                                        method=method,
-        #                                        fill_value=fill_value,
-        #                                        limit=limit, copy=copy)
         mgr = SingleBlockManager(block, new_axis)
         mgr._consolidate_inplace()
         return mgr
 
-    def _reindex_indexer_items(self, new_items, indexer, fill_value):
-        # equiv to a reindex
-        return self.reindex(new_items, indexer=indexer, fill_value=fill_value,
-                            copy=False)
-
-    def _delete_from_block(self, i, item):
-        super(SingleBlockManager, self)._delete_from_block(i, item)
-
-        # possibly need to merge split blocks
-        if len(self.blocks) > 1:
-            new_values = np.concatenate([b.values for b in self.blocks])
-            new_items = Index(np.concatenate([b.items for b in self.blocks]))
-
-            block = make_block(values=new_values, placement=None,
-                               dtype=self._block.dtype,)
-
-        elif len(self.blocks):
-            block = self.blocks[0]
-        else:
-            block = make_block(values=np.array([], dtype=self._block.dtype),
-                               placement=None)
-
-        self.blocks = [block]
+    def get_slice(self, slobj, axis=0):
+        if axis >= self.ndim:
+            raise IndexError("Requested axis not found in manager")
 
-    def get_slice(self, slobj):
         return self.__class__(self._block._slice(slobj),
                               self.index[slobj], fastpath=True)
 
@@ -3153,10 +3103,10 @@ class SingleBlockManager(BlockManager):
         return {self.ftype: 1}
 
     def get_dtypes(self):
-        return [self._block.dtype]
+        return np.array([self._block.dtype])
 
     def get_ftypes(self):
-        return [self._block.ftype]
+        return np.array([self._block.ftype])
 
     @property
     def values(self):
@@ -3185,15 +3135,9 @@ class SingleBlockManager(BlockManager):
 
         Ensures that self.blocks doesn't become empty.
         """
-        # Also, make sure dtype is preserved.
-        dtype = self._block.dtype
-
-        super(SingleBlockManager, self).delete(item)
-
-        if not self.blocks:
-            self.blocks = [make_block(values=np.empty(0, dtype=dtype),
-                                      placement=np.arange(len(self.items)),
-                                      ndim=1, dtype=dtype, fastpath=True)]
+        loc = self.items.get_loc(item)
+        self._block.delete(loc)
+        self.axes[0] = self.axes[0].delete(loc)
 
     def fast_xs(self, loc):
         """
@@ -3202,25 +3146,6 @@ class SingleBlockManager(BlockManager):
         """
         return self._block.values[loc]
 
-    def _get_blocks_for_items_indexer(self, indexer, fill_value):
-        """
-        Reindex blocks at axis=0 (overloaded for SingleBlockManager).
-
-        Returns
-        -------
-        new_blocks : list of Block
-
-        """
-        if indexer is None:
-            new_values = self._values.copy()
-        else:
-            new_values = com.take_1d(self._values, indexer,
-                                     fill_value=fill_value)
-
-        return [make_block(values=new_values,
-                           placement=np.arange(len(new_values)),
-                           ndim=self.ndim, fastpath=True)]
-
 
 def construction_error(tot_items, block_shape, axes, e=None):
     """ raise a helpful message about our construction """
@@ -3239,7 +3164,7 @@ def create_block_manager_from_blocks(blocks, axes):
             # basically "all items", but if there're many, don't bother
             # converting, it's an error anyway.
             blocks = [make_block(values=blocks[0],
-                                 placement=np.arange(len(axes[0])),)]
+                                 placement=slice(0, len(axes[0])))]
 
         mgr = BlockManager(blocks, axes)
         mgr._consolidate_inplace()
@@ -3526,15 +3451,17 @@ def _merge_blocks(blocks, dtype=None, _can_consolidate=True):
                 raise AssertionError("_merge_blocks are invalid!")
             dtype = blocks[0].dtype
 
-        new_ref_locs = np.concatenate([b.ref_locs for b in blocks])
+        # FIXME: optimization potential in case all mgrs contain slices and
+        # combination of those slices is a slice, too.
+        new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks])
         new_values = _vstack([b.values for b in blocks], dtype)
 
-        argsort = np.argsort(new_ref_locs)
+        argsort = np.argsort(new_mgr_locs)
         new_values = new_values[argsort]
-        new_ref_locs = new_ref_locs[argsort]
+        new_mgr_locs = new_mgr_locs[argsort]
 
         return make_block(new_values,
-                          fastpath=True, placement=new_ref_locs)
+                          fastpath=True, placement=new_mgr_locs)
 
     # no merge
     return blocks
@@ -3560,14 +3487,6 @@ def _vstack(to_stack, dtype):
         return np.vstack(to_stack)
 
 
-def _possibly_convert_to_indexer(loc):
-    if com._is_bool_indexer(loc):
-        loc = [i for i, v in enumerate(loc) if v]
-    elif isinstance(loc, slice):
-        loc = lrange(loc.start, loc.stop)
-    return loc
-
-
 def _possibly_compare(a, b, op):
     res = op(a, b)
     is_a_array = isinstance(a, np.ndarray)
@@ -3650,33 +3569,25 @@ def _invert_reordering(reordering, minlength=None):
     return inverted
 
 
-def ref_loc_groupby_block(ref_locs):
+def _get_blkno_placements(blknos, blk_count, group=True):
     """
-    Group given ref_locs by block.
+
+    Parameters
+    ----------
+    blknos : array of int64
+    blk_count : int
+    group : bool
 
     Returns
     -------
     iterator
-        Yield (block, block_locs, original_locs)
+        yield (BlockPlacement, blkno)
 
     """
-    if len(ref_locs) == 0:
-        return
 
-    blocks = com._ensure_object(lib.map_infer(ref_locs,
-                                              operator.itemgetter(0)))
-    indices = lib.map_infer(ref_locs, operator.itemgetter(1))
-
-    factorizer = Factorizer(len(blocks))
-    block_ids = factorizer.factorize(blocks, na_sentinel=-1)
-
-    for i in range(factorizer.get_count()):
-        locs = (block_ids == i).nonzero()[0]
-        yield blocks[locs[0]], indices[locs], locs
-
-    na_locs = (block_ids == -1).nonzero()[0]
-    if len(na_locs):
-        yield None, indices[na_locs], na_locs
+    # FIXME: blk_count is unused, but it may avoid the use of dicts in cython
+    for blkno, indexer in lib.get_blkno_indexers(blknos, group):
+        yield blkno, BlockPlacement(indexer)
 
 
 def items_overlap_with_suffix(left, lsuffix, right, rsuffix):
@@ -3774,14 +3685,14 @@ def concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy):
     copy : bool
 
     """
-    concat_plans = []
+    concat_plan = combine_concat_plans([get_mgr_concatenation_plan(mgr, indexers)
+                                        for mgr, indexers in mgrs_indexers],
+                                       concat_axis)
 
-    for mgr, indexers in mgrs_indexers:
-        plan = get_mgr_concatenation_plan(mgr, indexers)
-        concat_plans = combine_concat_plans(concat_plans, plan, concat_axis)
-
-    blocks = [concatenate_by_plan(plan, concat_axis, copy=copy)
-              for plan in concat_plans]
+    blocks = [make_block(concatenate_join_units(join_units, concat_axis,
+                                                copy=copy),
+                         placement=placement)
+              for placement, join_units in concat_plan]
 
     return BlockManager(blocks, axes)
 
@@ -3798,20 +3709,32 @@ def get_empty_dtype_and_na(join_units):
     na
     """
 
+    if len(join_units) == 1:
+        blk = join_units[0].block
+        if blk is None:
+            return np.float64, np.nan
+        else:
+            return blk.dtype, None
+
     has_none_blocks = False
-    dtypes = set()
-    upcast_classes = set()
-    null_upcast_classes = set()
-    for unit in join_units:
+    dtypes = [None] * len(join_units)
+
+    for i, unit in enumerate(join_units):
         if unit.block is None:
-            # This value is not supposed to be used anywhere, it's here to make
-            # sure "monotype" check (len(dtypes) == 1) fails and to indicate
-            # that upcasting is required.
             has_none_blocks = True
-            continue
+        else:
+            dtypes[i] = unit.dtype
+
+    if not has_none_blocks and len(set(dtypes)) == 1:
+        # Unanimous decision, nothing to upcast.
+        return dtypes[0], None
 
-        dtype = unit.dtype
-        dtypes.add(unit.dtype)
+    # dtypes = set()
+    upcast_classes = set()
+    null_upcast_classes = set()
+    for dtype, unit in zip(dtypes, join_units):
+        if dtype is None:
+            continue
 
         if issubclass(dtype.type, (np.object_, np.bool_)):
             upcast_cls = 'object'
@@ -3830,10 +3753,6 @@ def get_empty_dtype_and_na(join_units):
         else:
             upcast_classes.add(upcast_cls)
 
-    if not has_none_blocks and len(dtypes) == 1:
-        # Unanimous decision, nothing to upcast.
-        return next(iter(dtypes)), None
-
     if not upcast_classes:
         upcast_classes = null_upcast_classes
 
@@ -3850,11 +3769,13 @@ def get_empty_dtype_and_na(join_units):
         raise AssertionError("invalid dtype determination in get_concat_dtype")
 
 
-def concatenate_by_plan(plan, concat_axis, copy):
+def concatenate_join_units(join_units, concat_axis, copy):
     """
-    Make block from concatenation plan.
+    Concatenate values from several join units along selected axis.
     """
-    concat_start, join_units = plan
+    if concat_axis == 0 and len(join_units) > 1:
+        # Concatenating join units along ax0 is handled in _merge_blocks.
+        raise AssertionError("Concatenating join units along axis0")
 
     empty_dtype, upcasted_na = get_empty_dtype_and_na(join_units)
 
@@ -3864,20 +3785,20 @@ def concatenate_by_plan(plan, concat_axis, copy):
 
     if len(to_concat) == 1:
         # Only one block, nothing to concatenate.
-        if copy:
-            concat_values = to_concat[0].copy()
-        else:
-            concat_values = to_concat[0]
+        concat_values = to_concat[0]
+        if copy and concat_values.base is not None:
+            concat_values = concat_values.copy()
     else:
         concat_values = com._concat_compat(to_concat, axis=concat_axis)
 
-    rng = np.arange(concat_values.shape[0])
-
+    # FIXME: optimization potential: if len(join_units) == 1, single join unit
+    # is densified and sparsified back.
     if any(unit.is_sparse for unit in join_units):
-        concat_values = SparseArray(concat_values[0])
-
-    return make_block(concat_values,
-                      placement=rng + concat_start)
+        # If one of the units was sparse, concat_values are 2d and there's only
+        # one item.
+        return SparseArray(concat_values[0])
+    else:
+        return concat_values
 
 
 def get_mgr_concatenation_plan(mgr, indexers):
@@ -3891,7 +3812,7 @@ def get_mgr_concatenation_plan(mgr, indexers):
 
     Returns
     -------
-    plan : list of (start_loc, [JoinUnit]) tuples
+    plan : list of (BlockPlacement, JoinUnit) tuples
 
     """
     # Calculate post-reindex shape , save for item axis which will be separate
@@ -3899,109 +3820,124 @@ def get_mgr_concatenation_plan(mgr, indexers):
     mgr_shape = list(mgr.shape)
     for ax, indexer in indexers.items():
         mgr_shape[ax] = len(indexer)
+    mgr_shape = tuple(mgr_shape)
 
     if 0 in indexers:
-        indexer = indexers.pop(0)
-        ref_locs = com.take_1d(mgr._ref_locs, indexer, fill_value=(None, 0))
+        ax0_indexer = indexers.pop(0)
+        blknos = com.take_1d(mgr._blknos, ax0_indexer, fill_value=-1)
+        blklocs = com.take_1d(mgr._blklocs, ax0_indexer, fill_value=-1)
     else:
-        ref_locs = mgr._ref_locs
+
+        if mgr._is_single_block:
+            blk = mgr.blocks[0]
+            return [(blk.mgr_locs, JoinUnit(blk, mgr_shape, indexers))]
+
+        ax0_indexer = None
+        blknos = mgr._blknos
+        blklocs = mgr._blklocs
 
     plan = []
-    for blk, blk_locs, concat_locs in ref_loc_groupby_block(ref_locs):
-        # result_locs are assumed to be sorted
-        slices = locs_to_contiguous_sequences(concat_locs)
+    for blkno, placements in _get_blkno_placements(blknos, len(mgr.blocks),
+                                                   group=False):
+        assert placements.is_slice_like
 
-        for slc in slices:
-            join_unit_indexers = indexers.copy()
-            axis0_blk_indexer = blk_locs[slc]
+        join_unit_indexers = indexers.copy()
+
+        shape = list(mgr_shape)
+        shape[0] = len(placements)
+        shape = tuple(shape)
+
+        if blkno == -1:
+            unit = JoinUnit(None, shape)
+        else:
+            blk = mgr.blocks[blkno]
+            ax0_blk_indexer = blklocs[placements.indexer]
+
+            unit_no_ax0_reindexing = (
+                len(placements) == len(blk.mgr_locs) and
+                # Fastpath detection of join unit not needing to reindex its
+                # block: no ax0 reindexing took place and block placement was
+                # sequential before.
+                ((ax0_indexer is None
+                  and blk.mgr_locs.is_slice_like
+                  and blk.mgr_locs.as_slice.step == 1) or
+                 # Slow-ish detection: all indexer locs are sequential (and
+                 # length match is checked above).
+                 (np.diff(ax0_blk_indexer) == 1).all()))
 
             # Omit indexer if no item reindexing is required.
-            if (blk is None or
-                np.array_equal(axis0_blk_indexer, np.arange(blk.shape[0]))):
+            if unit_no_ax0_reindexing:
                 join_unit_indexers.pop(0, None)
             else:
-                join_unit_indexers[0] = axis0_blk_indexer
+                join_unit_indexers[0] = ax0_blk_indexer
 
-            blk_shape = copy.copy(mgr_shape)
-            blk_shape[0] = len(axis0_blk_indexer)
-            unit = JoinUnit(blk, join_unit_indexers, shape=blk_shape)
+            unit = JoinUnit(blk, shape, join_unit_indexers)
 
-            plan.append((concat_locs[slc.start], [unit]))
+        plan.append((placements, unit))
 
-    plan.sort()
     return plan
 
 
-def combine_concat_plans(existing_plan, new_plan, concat_axis):
+def combine_concat_plans(plans, concat_axis):
     """
     Combine multiple concatenation plans into one.
 
     existing_plan is updated in-place.
     """
-    if not existing_plan:
-        # Shortcut: nothing to combine with
-        return new_plan
-
-    if concat_axis == 0:
-        # Another shortcut: when concatenating along item axis, plans can be
-        # simply appended.
-        last_offset, last_units = existing_plan[-1]
-        plan_offset = last_offset + last_units[0].shape[0]
-        return existing_plan + [(off_i + plan_offset, units_i)
-                                for off_i, units_i in new_plan]
-
-    from collections import deque
-    old_items = deque(existing_plan)
-    new_items = deque(new_plan)
-    result = []
+    if len(plans) == 1:
+        for p in plans[0]:
+            yield p[0], [p[1]]
 
-    while new_items:
-        old_start, old_units = old_items.popleft()
-        new_start, new_units = new_items.popleft()
+    elif concat_axis == 0:
+        offset = 0
+        for plan in plans:
+            last_plc = None
 
-        assert old_start == new_start
+            for plc, unit in plan:
+                yield plc.add(offset), [unit]
+                last_plc = plc
 
-        old_len = old_units[0].shape[0]
-        new_len = new_units[0].shape[0]
+            if last_plc is not None:
+                offset += last_plc.as_slice.stop
 
-        # Trim either old or new part as necessary
-        common_len = min(old_len, new_len)
-        if new_len > common_len:
-            new_items.appendleft((new_start + common_len,
-                                  [trim_join_unit(unit, common_len)
-                                   for unit in new_units]))
-        elif old_len > common_len:
-            old_items.appendleft((old_start + common_len,
-                                  [trim_join_unit(unit, common_len)
-                                   for unit in old_units]))
+    else:
+        num_ended = [0]
+        def _next_or_none(seq):
+            retval = next(seq, None)
+            if retval is None:
+                num_ended[0] += 1
+            return retval
 
-        result.append((old_start, old_units + new_units))
+        plans = list(map(iter, plans))
+        next_items = list(map(_next_or_none, plans))
 
-    # The loop terminates when there's no new items, make sure that all old
-    # items are processed.
-    assert not old_items
+        while num_ended[0] != len(next_items):
+            if num_ended[0] > 0:
+                raise ValueError("Plan shapes are not aligned")
 
-    return result
+            placements, units = zip(*next_items)
 
+            lengths = list(map(len, placements))
+            min_len, max_len = min(lengths), max(lengths)
 
-def locs_to_contiguous_sequences(locs):
-    """
-    Return contiguous sequences found in locs as slices.
-    """
-    # FIXME: the code looks vaguely familiar, maybe there another version that
-    # can be reused instead
-    assert locs.ndim == 1
-    length = len(locs)
-
-    diff = np.diff(locs, axis=0)
-    break_locs = (diff != 1).nonzero()[0] + 1
+            if min_len == max_len:
+                yield placements[0], units
+                next_items[:] = map(_next_or_none, plans)
+            else:
+                yielded_placement = None
+                yielded_units = [None] * len(next_items)
+                for i, (plc, unit) in enumerate(next_items):
+                    yielded_units[i] = unit
+                    if len(plc) > min_len:
+                        # trim_join_unit updates unit in place, so only
+                        # placement needs to be sliced to skip min_len.
+                        next_items[i] = (plc[min_len:],
+                                         trim_join_unit(unit, min_len))
+                    else:
+                        yielded_placement = plc
+                        next_items[i] = _next_or_none(plans[i])
 
-    if len(break_locs) == 0:
-        return [slice(0, length)]
-    else:
-        return [slice(b, e)
-                for b, e in lib.fast_zip([np.r_[0, break_locs],
-                                          np.r_[break_locs, length]])]
+                yield yielded_placement, yielded_units
 
 
 def trim_join_unit(join_unit, length):
@@ -4012,23 +3948,29 @@ def trim_join_unit(join_unit, length):
     """
 
     if 0 not in join_unit.indexers:
-        join_unit.indexers[0] = np.arange(join_unit.shape[0])
+        extra_indexers = join_unit.indexers
 
-    extra_indexers = copy.copy(join_unit.indexers)
-    extra_shape = copy.copy(join_unit.shape)
+        if join_unit.block is None:
+            extra_block = None
+        else:
+            extra_block = join_unit.block.getitem_block(slice(length, None))
+            join_unit.block = join_unit.block.getitem_block(slice(length))
+    else:
+        extra_block = join_unit.block
 
-    extra_shape[0] = join_unit.shape[0] - length
-    extra_indexers[0] = extra_indexers[0][length:]
+        extra_indexers = copy.copy(join_unit.indexers)
+        extra_indexers[0] = extra_indexers[0][length:]
+        join_unit.indexers[0] = join_unit.indexers[0][:length]
 
-    join_unit.shape[0] = length
-    join_unit.indexers[0] = join_unit.indexers[0][:length]
+    extra_shape = (join_unit.shape[0] - length,) + join_unit.shape[1:]
+    join_unit.shape = (length,) + join_unit.shape[1:]
 
-    return JoinUnit(block=join_unit.block, indexers=extra_indexers,
+    return JoinUnit(block=extra_block, indexers=extra_indexers,
                     shape=extra_shape)
 
 
 class JoinUnit(object):
-    def __init__(self, block, indexers, shape):
+    def __init__(self, block, shape, indexers={}):
         # Passing shape explicitly is required for cases when block is None.
         self.block = block
         self.indexers = indexers
@@ -4061,42 +4003,66 @@ class JoinUnit(object):
 
     @cache_readonly
     def is_null(self):
-        return self.block is None or isnull(self.block.values).all()
+        if self.block is None:
+            return True
+
+        if not self.block._can_hold_na:
+            return False
+
+        # Usually it's enough to check but a small fraction of values to see if
+        # a block is NOT null, chunks should help in such cases.  1000 value
+        # was chosen rather arbitrarily.
+        values_flat = self.block.values.ravel()
+        total_len = values_flat.shape[0]
+        chunk_len = max(total_len // 40, 1000)
+        for i in range(0, total_len, chunk_len):
+            if not isnull(values_flat[i: i + chunk_len]).all():
+                return False
+
+        return True
 
     @cache_readonly
     def is_sparse(self):
         return self.block is not None and self.block.is_sparse
 
     def get_reindexed_values(self, empty_dtype, upcasted_na):
-        if upcasted_na is not None:
-            fill_value = upcasted_na
-        else:
-            # If upcasted_na is None, self.block should always exist.  If it
-            # doesn't (i.e. is None), then it's a bug in get_empty_dtype_and_na
-            # function.
+        if upcasted_na is None:
+            # No upcasting is necessary
             fill_value = self.block.fill_value
-
-        if self.is_null:
-            missing_arr = np.empty(self.shape, dtype=empty_dtype)
-            if np.prod(self.shape):
-                # NumPy 1.6 workaround: this statement gets strange if all
-                # blocks are of same dtype and some of them are empty: empty
-                # one are considered "null" so they must be filled, but no
-                # dtype upcasting happens and the dtype may not allow NaNs.
-                #
-                # In general, no one should get hurt when one tries to put
-                # incorrect values into empty array, but numpy 1.6 is strict
-                # about that.
-                missing_arr.fill(fill_value)
-            return missing_arr
+            values = self.block.get_values()
         else:
-            if upcasted_na is not None and self.block.is_bool:
+            fill_value = upcasted_na
+
+            if self.is_null:
+                missing_arr = np.empty(self.shape, dtype=empty_dtype)
+                if np.prod(self.shape):
+                    # NumPy 1.6 workaround: this statement gets strange if all
+                    # blocks are of same dtype and some of them are empty:
+                    # empty one are considered "null" so they must be filled,
+                    # but no dtype upcasting happens and the dtype may not
+                    # allow NaNs.
+                    #
+                    # In general, no one should get hurt when one tries to put
+                    # incorrect values into empty array, but numpy 1.6 is
+                    # strict about that.
+                    missing_arr.fill(fill_value)
+                return missing_arr
+
+            if self.block.is_bool:
                 # External code requested filling/upcasting, bool values must
                 # be upcasted to object to avoid being upcasted to numeric.
                 values = self.block.astype(np.object_).values
             else:
+                # No dtype upcasting is done here, it will be performed during
+                # concatenation itself.
                 values = self.block.get_values()
 
+        if not self.indexers:
+            # If there's no indexing to be done, we want to signal outside
+            # code that this array must be copied explicitly.  This is done
+            # by returning a view and checking `retval.base`.
+            return values.view()
+        else:
             for ax, indexer in self.indexers.items():
                 values = com.take_nd(values, indexer, axis=ax,
                                      fill_value=fill_value)
@@ -4104,20 +4070,26 @@ class JoinUnit(object):
             return values
 
 
-# def _align_kwargs(blocks, items, kwargs, align_keys, copy):
-#     aligned_objs = dict((k, kwargs[k]) for k in align_keys.items()
-#                         if hasattr(kwargs[k], 'reindex_axis'))
-
-#     if aligned_objs:
-#         kwargs = kwargs.copy()
-
-#     for b in blocks:
-#         if aligned_objs:
-#             b_items = items.take(b.ref_locs)
-
-#             for k, obj in aligned_objs.items():
-#                 axis = getattr(obj, '_info_axis_number', 0)
-#                 kwargs[k] = obj.reindex_axis(b_items, axis=axis,
-#                                              copy=copy)
-
-#         yield b, kwargs
+def _fast_count_smallints(arr):
+    """Faster version of set(arr) for sequences of small numbers."""
+    if len(arr) == 0:
+        # Handle empty arr case separately: numpy 1.6 chokes on that.
+        return np.empty((0, 2), dtype=arr.dtype)
+    else:
+        counts = np.bincount(arr)
+        nz = counts.nonzero()[0]
+        return np.c_[nz, counts[nz]]
+
+
+def _preprocess_slice_or_indexer(slice_or_indexer, length, allow_fill):
+    if isinstance(slice_or_indexer, slice):
+        return 'slice', slice_or_indexer, lib.slice_len(slice_or_indexer,
+                                                        length)
+    elif (isinstance(slice_or_indexer, np.ndarray) and
+          slice_or_indexer.dtype == np.bool_):
+        return 'mask', slice_or_indexer, slice_or_indexer.sum()
+    else:
+        indexer = np.asanyarray(slice_or_indexer, dtype=np.int_)
+        if not allow_fill:
+            indexer = _maybe_convert_indices(indexer, length)
+        return 'fancy', indexer, len(indexer)
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 3a977757b..196b80a83 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -447,7 +447,7 @@ def _unstack_frame(obj, level):
         new_blocks = []
         mask_blocks = []
         for blk in obj._data.blocks:
-            blk_items = obj._data.items.take(blk.ref_locs)
+            blk_items = obj._data.items[blk.mgr_locs.indexer]
             bunstacker = _Unstacker(blk.values.T, obj.index, level=level,
                                     value_columns=blk_items)
             new_items = bunstacker.get_new_columns()
diff --git a/pandas/io/packers.py b/pandas/io/packers.py
index 78f577566..7da86565b 100644
--- a/pandas/io/packers.py
+++ b/pandas/io/packers.py
@@ -356,7 +356,7 @@ def encode(obj):
             return {'typ': 'block_manager',
                     'klass': obj.__class__.__name__,
                     'axes': data.axes,
-                    'blocks': [{'items': data.items.take(b.ref_locs),
+                    'blocks': [{'items': data.items.take(b.mgr_locs),
                                 'values': convert(b.values),
                                 'shape': b.values.shape,
                                 'dtype': b.dtype.num,
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index e49ab3884..95daa2bbc 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -2669,7 +2669,7 @@ class BlockManagerFixed(GenericFixed):
         self.attrs.nblocks = len(data.blocks)
         for i, blk in enumerate(data.blocks):
             # I have no idea why, but writing values before items fixed #2299
-            blk_items = data.items.take(blk.ref_locs)
+            blk_items = data.items.take(blk.mgr_locs)
             self.write_array('block%d_values' % i, blk.values, items=blk_items)
             self.write_index('block%d_items' % i, blk_items)
 
@@ -3192,7 +3192,7 @@ class Table(Fixed):
             obj = _reindex_axis(obj, a[0], a[1])
 
         def get_blk_items(mgr, blocks):
-            return [mgr.items.take(blk.ref_locs) for blk in blocks]
+            return [mgr.items.take(blk.mgr_locs) for blk in blocks]
 
         # figure out data_columns and get out blocks
         block_obj = self.get_object(obj).consolidate()
@@ -3208,7 +3208,7 @@ class Table(Fixed):
                     axis=axis
                 )._data
 
-                blocks = mgr.blocks
+                blocks = list(mgr.blocks)
                 blk_items = get_blk_items(mgr, blocks)
                 for c in data_columns:
                     mgr = block_obj.reindex_axis([c], axis=axis)._data
diff --git a/pandas/lib.pyx b/pandas/lib.pyx
index a1fef095e..0bac4f801 100644
--- a/pandas/lib.pyx
+++ b/pandas/lib.pyx
@@ -19,6 +19,17 @@ from cpython cimport (PyDict_New, PyDict_GetItem, PyDict_SetItem,
                       PyTuple_New,
                       PyObject_SetAttrString)
 
+cdef extern from "Python.h":
+    ctypedef struct PySliceObject:
+        pass
+
+    cdef int PySlice_GetIndicesEx(
+        PySliceObject* s, Py_ssize_t length,
+        Py_ssize_t *start, Py_ssize_t *stop, Py_ssize_t *step,
+        Py_ssize_t *slicelength) except -1
+
+
+
 cimport cpython
 
 isnan = np.isnan
@@ -1232,6 +1243,419 @@ def indices_fast(object index, ndarray[int64_t] labels, list keys,
 
     return result
 
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def get_blkno_indexers(int64_t[:] blknos, bint group=True):
+    """
+    Enumerate contiguous runs of integers in ndarray.
+
+    Iterate over elements of `blknos` yielding ``(blkno, slice(start, stop))``
+    pairs for each contiguous run found.
+
+    If `group` is True and there is more than one run for a certain blkno,
+    ``(blkno, array)`` with an array containing positions of all elements equal
+    to blkno.
+
+    Returns
+    -------
+    iter : iterator of (int, slice or array)
+
+    """
+    # There's blkno in this function's name because it's used in block &
+    # blockno handling.
+    cdef:
+        int64_t cur_blkno
+        Py_ssize_t i, start, stop, n, diff
+
+        list group_order
+        dict group_slices
+        int64_t[:] res_view
+
+    n = blknos.shape[0]
+
+    if n > 0:
+        start = 0
+        cur_blkno = blknos[start]
+
+        if group == False:
+            for i in range(1, n):
+                if blknos[i] != cur_blkno:
+                    yield cur_blkno, slice(start, i)
+
+                    start = i
+                    cur_blkno = blknos[i]
+
+            yield cur_blkno, slice(start, n)
+        else:
+            group_order = []
+            group_dict = {}
+
+            for i in range(1, n):
+                if blknos[i] != cur_blkno:
+                    if cur_blkno not in group_dict:
+                        group_order.append(cur_blkno)
+                        group_dict[cur_blkno] = [(start, i)]
+                    else:
+                        group_dict[cur_blkno].append((start, i))
+
+                    start = i
+                    cur_blkno = blknos[i]
+
+            if cur_blkno not in group_dict:
+                group_order.append(cur_blkno)
+                group_dict[cur_blkno] = [(start, n)]
+            else:
+                group_dict[cur_blkno].append((start, n))
+
+            for blkno in group_order:
+                slices = group_dict[blkno]
+                if len(slices) == 1:
+                    yield blkno, slice(slices[0][0], slices[0][1])
+                else:
+                    tot_len = sum(stop - start for start, stop in slices)
+                    result = np.empty(tot_len, dtype=np.int64)
+                    res_view = result
+
+                    i = 0
+                    for start, stop in slices:
+                        for diff in range(start, stop):
+                            res_view[i] = diff
+                            i += 1
+
+                    yield blkno, result
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+cpdef slice indexer_as_slice(int64_t[:] vals):
+    cdef:
+        Py_ssize_t i, n, start, stop
+        int64_t d
+
+    if vals is None:
+        raise TypeError("vals must be ndarray")
+
+    n = vals.shape[0]
+
+    if n == 0 or vals[0] < 0:
+        return None
+
+    if n == 1:
+        return slice(vals[0], vals[0] + 1, 1)
+
+    if vals[1] < 0:
+        return None
+
+    # n > 2
+    d = vals[1] - vals[0]
+
+    if d == 0:
+        return None
+
+    for i in range(2, n):
+        if vals[i] < 0 or vals[i] - vals[i-1] != d:
+            return None
+
+    start = vals[0]
+    stop = start + n * d
+    if stop < 0 and d < 0:
+        return slice(start, None, d)
+    else:
+        return slice(start, stop, d)
+
+
+cpdef slice_canonize(slice s):
+    """
+    Convert slice to canonical bounded form.
+    """
+    cdef:
+        Py_ssize_t start, stop, step, length
+
+    if s.step is None:
+        step = 1
+    else:
+        step = <Py_ssize_t>s.step
+        if step == 0:
+            raise ValueError("slice step cannot be zero")
+
+    if step > 0:
+        if s.stop is None:
+            raise ValueError("unbounded slice")
+
+        stop = <Py_ssize_t>s.stop
+        if s.start is None:
+            start = 0
+        else:
+            start = <Py_ssize_t>s.start
+            if start > stop:
+                start = stop
+    elif step < 0:
+        if s.start is None:
+            raise ValueError("unbounded slice")
+
+        start = <Py_ssize_t>s.start
+        if s.stop is None:
+            stop = -1
+        else:
+            stop = <Py_ssize_t>s.stop
+            if stop > start:
+                stop = start
+
+    if start < 0 or (stop < 0 and s.stop is not None):
+        raise ValueError("unbounded slice")
+
+    if stop < 0:
+        return slice(start, None, step)
+    else:
+        return slice(start, stop, step)
+
+
+cpdef slice_get_indices_ex(slice slc, Py_ssize_t objlen=INT64_MAX):
+    """
+    Get (start, stop, step, length) tuple for a slice.
+
+    If `objlen` is not specified, slice must be bounded, otherwise the result
+    will be wrong.
+
+    """
+    cdef:
+        Py_ssize_t start, stop, step, length
+
+    if slc is None:
+        raise TypeError("slc should be a slice")
+
+    PySlice_GetIndicesEx(<PySliceObject*>slc, objlen,
+                         &start, &stop, &step, &length)
+    return start, stop, step, length
+
+
+cpdef Py_ssize_t slice_len(slice slc, Py_ssize_t objlen=INT64_MAX) except -1:
+    """
+    Get length of a bounded slice.
+
+    The slice must not have any "open" bounds that would create dependency on
+    container size, i.e.:
+    - if ``s.step is None or s.step > 0``, ``s.stop`` is not ``None``
+    - if ``s.step < 0``, ``s.start`` is not ``None``
+
+    Otherwise, the result is unreliable.
+
+    """
+    cdef:
+        Py_ssize_t start, stop, step, length
+
+    if slc is None:
+        raise TypeError("slc must be slice")
+
+    PySlice_GetIndicesEx(<PySliceObject*>slc, objlen,
+                         &start, &stop, &step, &length)
+
+    return length
+
+
+def slice_getitem(slice slc not None, ind):
+    cdef:
+        Py_ssize_t s_start, s_stop, s_step, s_len
+        Py_ssize_t ind_start, ind_stop, ind_step, ind_len
+
+    s_start, s_stop, s_step, s_len = slice_get_indices_ex(slc)
+
+    if isinstance(ind, slice):
+        ind_start, ind_stop, ind_step, ind_len = slice_get_indices_ex(ind,
+                                                                      s_len)
+
+        if ind_step > 0 and ind_len == s_len:
+            # short-cut for no-op slice
+            if ind_len == s_len:
+                return slc
+
+        if ind_step < 0:
+            s_start = s_stop - s_step
+            ind_step = -ind_step
+
+        s_step *= ind_step
+        s_stop = s_start + ind_stop * s_step
+        s_start = s_start + ind_start * s_step
+
+        if s_step < 0 and s_stop < 0:
+            return slice(s_start, None, s_step)
+        else:
+            return slice(s_start, s_stop, s_step)
+
+    else:
+        return np.arange(s_start, s_stop, s_step)[ind]
+
+
+cdef class BlockPlacement:
+    # __slots__ = '_as_slice', '_as_array', '_len'
+    cdef slice _as_slice
+    cdef object _as_array
+
+    cdef bint _has_slice, _has_array, _is_known_slice_like
+
+    def __init__(self, val):
+        cdef slice slc
+
+        self._has_slice = False
+        self._has_array = False
+
+        if isinstance(val, slice):
+            slc = slice_canonize(val)
+
+            if slc.start != slc.stop:
+                self._as_slice = slc
+                self._has_slice = True
+            else:
+                arr = np.empty(0, dtype=np.int64)
+                self._as_array = arr
+                self._has_array = True
+        else:
+            # Cython memoryview interface requires ndarray to be writeable.
+            arr = np.require(val, dtype=np.int64, requirements='W')
+            assert arr.ndim == 1
+            self._as_array = arr
+            self._has_array = True
+
+    def __unicode__(self):
+        cdef slice s = self._ensure_has_slice()
+        if s is not None:
+            v = self._as_slice
+        else:
+            v = self._as_array
+
+        return '%s(%r)' % (self.__class__.__name__, v)
+
+    def __len__(self):
+        cdef slice s = self._ensure_has_slice()
+        if s is not None:
+            return slice_len(s)
+        else:
+            return len(self._as_array)
+
+    def __iter__(self):
+        cdef slice s = self._ensure_has_slice()
+        cdef Py_ssize_t start, stop, step, _
+        if s is not None:
+            start, stop, step, _ = slice_get_indices_ex(s)
+            return iter(range(start, stop, step))
+        else:
+            return iter(self._as_array)
+
+    @property
+    def as_slice(self):
+        cdef slice s = self._ensure_has_slice()
+        if s is None:
+            raise TypeError('Not slice-like')
+        else:
+            return s
+
+    @property
+    def indexer(self):
+        cdef slice s = self._ensure_has_slice()
+        if s is not None:
+            return s
+        else:
+            return self._as_array
+
+    def isin(self, arr):
+        from pandas.core.index import Int64Index
+        return Int64Index(self.as_array, copy=False).isin(arr)
+
+    @property
+    def as_array(self):
+        cdef Py_ssize_t start, stop, end, _
+        if not self._has_array:
+            start, stop, step, _ = slice_get_indices_ex(self._as_slice)
+            self._as_array = np.arange(start, stop, step,
+                                       dtype=np.int_)
+            self._has_array = True
+        return self._as_array
+
+    @property
+    def is_slice_like(self):
+        cdef slice s = self._ensure_has_slice()
+        return s is not None
+
+    def __getitem__(self, loc):
+        cdef slice s = self._ensure_has_slice()
+        if s is not None:
+            val = slice_getitem(s, loc)
+        else:
+            val = self._as_array[loc]
+
+        if not isinstance(val, slice) and val.ndim == 0:
+            return val
+
+        return BlockPlacement(val)
+
+    def delete(self, loc):
+        return BlockPlacement(np.delete(self.as_array, loc, axis=0))
+
+    def append(self, others):
+        if len(others) == 0:
+            return self
+
+        return BlockPlacement(np.concatenate([self.as_array] +
+                                             [o.as_array for o in others]))
+
+    cdef iadd(self, other):
+        cdef slice s = self._ensure_has_slice()
+        cdef Py_ssize_t other_int, start, stop, step, l
+
+        if isinstance(other, int) and s is not None:
+            other_int = <Py_ssize_t>other
+
+            if other_int == 0:
+                return self
+
+            start, stop, step, l = slice_get_indices_ex(s)
+            start += other_int
+            stop += other_int
+
+            if ((step > 0 and start < 0) or
+                (step < 0 and stop < step)):
+                raise ValueError("iadd causes length change")
+
+            if stop < 0:
+                self._as_slice = slice(start, None, step)
+            else:
+                self._as_slice = slice(start, stop, step)
+
+            self._has_array = False
+            self._as_array = None
+        else:
+            newarr = self.as_array + other
+            if (newarr < 0).any():
+                raise ValueError("iadd causes length change")
+
+            self._as_array = newarr
+            self._has_array = True
+            self._has_slice = False
+            self._as_slice = None
+
+        return self
+
+    cdef BlockPlacement copy(self):
+        cdef slice s = self._ensure_has_slice()
+        if s is not None:
+            return BlockPlacement(s)
+        else:
+            return BlockPlacement(self._as_array)
+
+    def add(self, other):
+        return self.copy().iadd(other)
+
+    def sub(self, other):
+        return self.add(-other)
+
+    cdef slice _ensure_has_slice(self):
+        if not self._has_slice:
+            self._as_slice = indexer_as_slice(self._as_array)
+            self._has_slice = True
+        return self._as_slice
+
+
 include "reduce.pyx"
 include "properties.pyx"
 include "inference.pyx"
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index 5d0aa992b..2aac364d1 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -11822,8 +11822,8 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
         df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=df_float.columns)
         df        = pd.concat([ df_float, df_int, df_bool, df_object, df_dt ], axis=1)
 
-        result = df._data._ref_locs
-        self.assertEqual(len(result), len(df.columns))
+        self.assertEqual(len(df._data._blknos), len(df.columns))
+        self.assertEqual(len(df._data._blklocs), len(df.columns))
 
         # testing iget
         for i in range(len(df.columns)):
diff --git a/pandas/tests/test_indexing.py b/pandas/tests/test_indexing.py
index 261e1dd2a..a105b1779 100644
--- a/pandas/tests/test_indexing.py
+++ b/pandas/tests/test_indexing.py
@@ -1015,7 +1015,7 @@ class TestIndexing(tm.TestCase):
         columns = list(range(0,8,2))
         df = DataFrame(arr,index=index,columns=columns)
 
-        df._data.blocks[0].ref_locs
+        df._data.blocks[0].mgr_locs
         result = df.iloc[1:5,2:4]
         str(result)
         result.dtypes
diff --git a/pandas/tests/test_internals.py b/pandas/tests/test_internals.py
index 1e4c621dd..b91384a84 100644
--- a/pandas/tests/test_internals.py
+++ b/pandas/tests/test_internals.py
@@ -18,58 +18,85 @@ from pandas.compat import zip, u
 def assert_block_equal(left, right):
     assert_almost_equal(left.values, right.values)
     assert(left.dtype == right.dtype)
-    assert_almost_equal(left.ref_locs, right.ref_locs)
+    assert_almost_equal(left.mgr_locs, right.mgr_locs)
 
 
-def get_numeric_mat(n, k, dtype):
-    return np.repeat(np.atleast_2d(np.arange(k, dtype=dtype)), n, axis=0)
+def get_numeric_mat(shape):
+    arr = np.arange(shape[0])
+    return np.lib.stride_tricks.as_strided(
+        x=arr, shape=shape,
+        strides=(arr.itemsize,) + (0,) * (len(shape) - 1)).copy()
 
 
 N = 10
 
 
-def create_block(typestr, placement, num_rows=None, num_offset=None):
-    placement = np.asanyarray(placement)
+def create_block(typestr, placement, item_shape=None, num_offset=0):
+    """
+    Supported typestr:
+
+        * float, f8, f4, f2
+        * int, i8, i4, i2, i1
+        * uint, u8, u4, u2, u1
+        * complex, c16, c8
+        * bool
+        * object, string, O
+        * datetime, dt
+        * sparse (SparseArray with fill_value=0.0)
+        * sparse_na (SparseArray with fill_value=np.nan)
 
-    if num_offset is None:
-        num_offset = 0
+    """
+    placement = BlockPlacement(placement)
+    num_items = len(placement)
 
-    if num_rows is None:
-        num_rows = N
+    if item_shape is None:
+        item_shape = (N,)
+
+    shape = (num_items,) + item_shape
+
+    mat = get_numeric_mat(shape)
 
     if typestr in ('float', 'f8', 'f4', 'f2',
                    'int', 'i8', 'i4', 'i2', 'i1',
                    'uint', 'u8', 'u4', 'u2', 'u1'):
-        values = get_numeric_mat(num_rows, len(placement),
-                                 dtype=np.dtype(typestr)).T + num_offset
+        values = mat.astype(typestr) + num_offset
     elif typestr in ('complex', 'c16', 'c8'):
-        values = get_numeric_mat(num_rows, len(placement),
-                                 dtype=np.dtype(typestr)).T + num_offset
-        values *= 1.j
+        values = 1.j * (mat.astype(typestr) + num_offset)
     elif typestr in ('object', 'string', 'O'):
-        values = np.repeat(
-            np.array([['A%s' % i
-                       for i in np.arange(len(placement)) + num_offset]]),
-            num_rows, axis=0).T
+        values = np.reshape(['A%d' % i for i in mat.ravel() + num_offset],
+                            shape)
     elif typestr in ('bool'):
-        values = np.ones((num_rows, len(placement)), dtype=np.bool_).T
+        values = np.ones(shape, dtype=np.bool_)
     elif typestr in ('datetime', 'dt'):
-        values = (randn(num_rows, len(placement)).astype(int)
-                  .astype('M8[ns]')).T
-    elif typestr in ('sparse',):
+        values = (mat * 1e9).astype('M8[ns]')
+    elif typestr in ('sparse', 'sparse_na'):
         # FIXME: doesn't support num_rows != 10
-        assert len(placement) == 1
-        assert num_rows == 10
-        values = SparseArray([0, 0, 1, 2, 3, 0, 4, 5, 0, 6], fill_value=0)
+        assert shape[-1] == 10
+        assert all(s == 1 for s in shape[:-1])
+        if typestr.endswith('_na'):
+            fill_value = np.nan
+        else:
+            fill_value = 0.0
+        values = SparseArray([fill_value, fill_value, 1, 2, 3, fill_value,
+                              4, 5, fill_value, 6], fill_value=fill_value)
         arr = values.sp_values.view()
         arr += (num_offset - 1)
     else:
         raise ValueError('Unsupported typestr: "%s"' % typestr)
 
-    return make_block(values, placement=placement)
+    return make_block(values, placement=placement, ndim=len(shape))
+
+
+def create_single_mgr(typestr, num_rows=None):
+    if num_rows is None:
+        num_rows = N
+
+    return SingleBlockManager(
+        create_block(typestr, placement=slice(0, num_rows), item_shape=()),
+        np.arange(num_rows))
 
 
-def create_mgr(descr, num_rows=None):
+def create_mgr(descr, item_shape=None):
     """
     Construct BlockManager from string description.
 
@@ -80,17 +107,7 @@ def create_mgr(descr, num_rows=None):
 
     Rules are rather simple:
 
-    * supported datatypes:
-
-      * float, f8, f4, f2
-      * int, i8, i4, i2, i1
-      * uint, u8, u4, u2, u1
-      * complex, c16, c8
-      * bool
-      * object, string, O
-      * datetime, dt
-      * sparse
-
+    * see list of supported datatypes in `create_block` method
     * components are semicolon-separated
     * each component is `NAME,NAME,NAME: DTYPE_ID`
     * whitespace around colons & semicolons are removed
@@ -100,8 +117,8 @@ def create_mgr(descr, num_rows=None):
         'a:f8-1; b:f8-2; c:f8-foobar'
 
     """
-    if num_rows is None:
-        num_rows = N
+    if item_shape is None:
+        item_shape = (N,)
 
     offset = 0
     mgr_items = []
@@ -126,12 +143,12 @@ def create_mgr(descr, num_rows=None):
     num_offset = 0
     for blockstr, placement in block_placements.items():
         typestr = blockstr.split('-')[0]
-        blocks.append(create_block(typestr, placement, num_rows=num_rows,
+        blocks.append(create_block(typestr, placement, item_shape=item_shape,
                                    num_offset=num_offset,))
         num_offset += len(placement)
 
-    return BlockManager(sorted(blocks, key=lambda b: b.ref_locs[0]),
-                        [mgr_items, np.arange(num_rows)])
+    return BlockManager(sorted(blocks, key=lambda b: b.mgr_locs[0]),
+                        [mgr_items] + [np.arange(n) for n in item_shape])
 
 
 
@@ -169,8 +186,8 @@ class TestBlock(tm.TestCase):
         _check(self.oblock)
         _check(self.bool_block)
 
-    def test_ref_locs(self):
-        assert_almost_equal(self.fblock.ref_locs, [0, 2, 4])
+    def test_mgr_locs(self):
+        assert_almost_equal(self.fblock.mgr_locs, [0, 2, 4])
 
     def test_attrs(self):
         self.assertEqual(self.fblock.shape, self.fblock.values.shape)
@@ -188,7 +205,7 @@ class TestBlock(tm.TestCase):
         bblock = make_block(bvals,
                             ref_cols.get_indexer(['a', 'd']))
         merged = ablock.merge(bblock)
-        assert_almost_equal(merged.ref_locs, [0, 1, 2, 3])
+        assert_almost_equal(merged.mgr_locs, [0, 1, 2, 3])
         assert_almost_equal(merged.values[[0, 2]], avals)
         assert_almost_equal(merged.values[[1, 3]], bvals)
 
@@ -199,33 +216,9 @@ class TestBlock(tm.TestCase):
         self.assertIsNot(cop, self.fblock)
         assert_block_equal(self.fblock, cop)
 
-    def test_items(self):
-        raise nose.SkipTest('items are removed from Block')
-        cols = self.fblock.items
-        self.assert_numpy_array_equal(cols, ['a', 'c', 'e'])
-
-        cols2 = self.fblock.items
-        # disabled: items are generated
-        # self.assertIs(cols, cols2)
-
-    def test_assign_ref_items(self):
-        raise nose.SkipTest('ref_items are removed from Block')
-        new_cols = Index(['foo', 'bar', 'baz', 'quux', 'hi'])
-        self.fblock.set_ref_items(new_cols)
-        self.assert_numpy_array_equal(self.fblock.items, ['foo', 'baz', 'hi'])
-
     def test_reindex_index(self):
         pass
 
-    def test_reindex_items_from(self):
-        raise nose.SkipTest('reindex_items_from is removed from Block')
-        new_cols = Index(['e', 'b', 'c', 'f'])
-        reindexed = self.fblock.reindex_items_from(new_cols)
-        assert_almost_equal(reindexed.ref_locs, [0, 2])
-        self.assertEquals(reindexed.values.shape[0], 2)
-        self.assert_((reindexed.values[0] == 2).all())
-        self.assert_((reindexed.values[1] == 1).all())
-
     def test_reindex_cast(self):
         pass
 
@@ -233,19 +226,23 @@ class TestBlock(tm.TestCase):
         pass
 
     def test_delete(self):
-        newb = self.fblock.delete(0)
-        assert_almost_equal(newb.ref_locs, [2, 4])
+        newb = self.fblock.copy()
+        newb.delete(0)
+        assert_almost_equal(newb.mgr_locs, [2, 4])
         self.assert_((newb.values[0] == 1).all())
 
-        newb = self.fblock.delete(1)
-        assert_almost_equal(newb.ref_locs, [0, 4])
+        newb = self.fblock.copy()
+        newb.delete(1)
+        assert_almost_equal(newb.mgr_locs, [0, 4])
         self.assert_((newb.values[1] == 2).all())
 
-        newb = self.fblock.delete(2)
-        assert_almost_equal(newb.ref_locs, [0, 2])
+        newb = self.fblock.copy()
+        newb.delete(2)
+        assert_almost_equal(newb.mgr_locs, [0, 2])
         self.assert_((newb.values[1] == 1).all())
 
-        self.assertRaises(Exception, self.fblock.delete, 3)
+        newb = self.fblock.copy()
+        self.assertRaises(Exception, newb.delete, 3)
 
     def test_split_block_at(self):
 
@@ -270,11 +267,6 @@ class TestBlock(tm.TestCase):
         bs = list(bblock.split_block_at('f'))
         self.assertEqual(len(bs), 0)
 
-    def test_unicode_repr(self):
-        raise nose.SkipTest('No items to test unicode on...')
-        str_repr = repr(create_block('object', [0, 1],
-                                     ref_items=['b', u("\u05d0")]))
-
     def test_get(self):
         pass
 
@@ -326,23 +318,16 @@ class TestBlockManager(tm.TestCase):
 
         axes, blocks = tmp_mgr.axes, tmp_mgr.blocks
 
-        blocks[0]._ref_locs = np.array([0])
-        blocks[1]._ref_locs = np.array([0])
+        blocks[0].mgr_locs = np.array([0])
+        blocks[1].mgr_locs = np.array([0])
         # test trying to create block manager with overlapping ref locs
         self.assertRaises(AssertionError, BlockManager, blocks, axes)
 
-        blocks[0]._ref_locs = np.array([0])
-        blocks[1]._ref_locs = np.array([1])
+        blocks[0].mgr_locs = np.array([0])
+        blocks[1].mgr_locs = np.array([1])
         mgr = BlockManager(blocks, axes)
         mgr.iget(1)
 
-        # invalidate the _ref_locs
-        for b in blocks:
-            b._ref_locs = None
-        mgr._ref_locs = None
-        mgr._items_map = None
-        self.assertRaises(Exception, mgr._rebuild_ref_locs)
-
     def test_contains(self):
         self.assertIn('a', self.mgr)
         self.assertNotIn('baz', self.mgr)
@@ -386,7 +371,7 @@ class TestBlockManager(tm.TestCase):
         assert_almost_equal(mgr.get('c'), values[2])
 
     def test_set(self):
-        mgr = create_mgr('a,b,c: int', num_rows=3)
+        mgr = create_mgr('a,b,c: int', item_shape=(3,))
 
         mgr.set('d', np.array(['foo'] * 3))
         mgr.set('b', np.array(['bar'] * 3))
@@ -581,7 +566,7 @@ class TestBlockManager(tm.TestCase):
 
         cons = self.mgr.consolidate()
         self.assertEquals(cons.nblocks, 1)
-        assert_almost_equal(cons.blocks[0].ref_locs,
+        assert_almost_equal(cons.blocks[0].mgr_locs,
                             np.arange(len(cons.items)))
 
     def test_reindex_index(self):
@@ -618,7 +603,7 @@ class TestBlockManager(tm.TestCase):
     def test_get_numeric_data(self):
         mgr = create_mgr('int: int; float: float; complex: complex;'
                          'str: object; bool: bool; obj: object; dt: datetime',
-                         num_rows=3)
+                         item_shape=(3,))
         mgr.set('obj', np.array([1, 2, 3], dtype=np.object_))
 
         numeric = mgr.get_numeric_data()
@@ -637,7 +622,7 @@ class TestBlockManager(tm.TestCase):
     def test_get_bool_data(self):
         mgr = create_mgr('int: int; float: float; complex: complex;'
                          'str: object; bool: bool; obj: object; dt: datetime',
-                         num_rows=3)
+                         item_shape=(3,))
         mgr.set('obj', np.array([True, False, True], dtype=np.object_))
 
         bools = mgr.get_bool_data()
@@ -672,7 +657,334 @@ class TestBlockManager(tm.TestCase):
         bm2 = BlockManager(bm1.blocks[::-1], bm1.axes)
         self.assertTrue(bm1.equals(bm2))
 
+    def test_single_mgr_ctor(self):
+        mgr = create_single_mgr('f8', num_rows=5)
+        self.assertEquals(mgr.as_matrix().tolist(), [0., 1., 2., 3., 4.])
+
+
+class TestIndexing(object):
+    # Nosetests-style data-driven tests.
+    #
+    # This test applies different indexing routines to block managers and
+    # compares the outcome to the result of same operations on np.ndarray.
+    #
+    # NOTE: sparse (SparseBlock with fill_value != np.nan) fail a lot of tests
+    #       and are disabled.
+
+    MANAGERS = [
+        create_single_mgr('f8', N),
+        create_single_mgr('i8', N),
+        #create_single_mgr('sparse', N),
+        create_single_mgr('sparse_na', N),
+
+        # 2-dim
+        create_mgr('a,b,c,d,e,f: f8', item_shape=(N,)),
+        create_mgr('a,b,c,d,e,f: i8', item_shape=(N,)),
+        create_mgr('a,b: f8; c,d: i8; e,f: string', item_shape=(N,)),
+        create_mgr('a,b: f8; c,d: i8; e,f: f8', item_shape=(N,)),
+        #create_mgr('a: sparse', item_shape=(N,)),
+        create_mgr('a: sparse_na', item_shape=(N,)),
+
+        # 3-dim
+        create_mgr('a,b,c,d,e,f: f8', item_shape=(N, N)),
+        create_mgr('a,b,c,d,e,f: i8', item_shape=(N, N)),
+        create_mgr('a,b: f8; c,d: i8; e,f: string', item_shape=(N, N)),
+        create_mgr('a,b: f8; c,d: i8; e,f: f8', item_shape=(N, N)),
+        # create_mgr('a: sparse', item_shape=(1, N)),
+    ]
+
+    # MANAGERS = [MANAGERS[6]]
+
+    def test_get_slice(self):
+        def assert_slice_ok(mgr, axis, slobj):
+            # import pudb; pudb.set_trace()
+            mat = mgr.as_matrix()
+            sliced = mgr.get_slice(slobj, axis=axis)
+            mat_slobj = (slice(None),) * axis + (slobj,)
+            assert_almost_equal(mat[mat_slobj], sliced.as_matrix())
+            assert_almost_equal(mgr.axes[axis][slobj], sliced.axes[axis])
+
+        for mgr in self.MANAGERS:
+            for ax in range(mgr.ndim):
+                # slice
+                yield assert_slice_ok, mgr, ax, slice(None)
+                yield assert_slice_ok, mgr, ax, slice(3)
+                yield assert_slice_ok, mgr, ax, slice(100)
+                yield assert_slice_ok, mgr, ax, slice(1, 4)
+                yield assert_slice_ok, mgr, ax, slice(3, 0, -2)
+
+                # boolean mask
+                yield assert_slice_ok, mgr, ax, np.array([], dtype=np.bool_)
+                yield (assert_slice_ok, mgr, ax,
+                       np.ones(mgr.shape[ax], dtype=np.bool_))
+                yield (assert_slice_ok, mgr, ax,
+                       np.zeros(mgr.shape[ax], dtype=np.bool_))
+
+                if mgr.shape[ax] >= 3:
+                    yield (assert_slice_ok, mgr, ax,
+                           np.arange(mgr.shape[ax]) % 3 == 0)
+                    yield (assert_slice_ok, mgr, ax,
+                           np.array([True, True, False], dtype=np.bool_))
+
+                # fancy indexer
+                yield assert_slice_ok, mgr, ax, []
+                yield assert_slice_ok, mgr, ax, lrange(mgr.shape[ax])
+
+                if mgr.shape[ax] >= 3:
+                    yield assert_slice_ok, mgr, ax, [0, 1, 2]
+                    yield assert_slice_ok, mgr, ax, [-1, -2, -3]
+
+    def test_take(self):
+        def assert_take_ok(mgr, axis, indexer):
+            mat = mgr.as_matrix()
+            taken = mgr.take(indexer, axis)
+            assert_almost_equal(np.take(mat, indexer, axis),
+                                taken.as_matrix())
+            assert_almost_equal(mgr.axes[axis].take(indexer),
+                                taken.axes[axis])
+
+        for mgr in self.MANAGERS:
+            for ax in range(mgr.ndim):
+                # take/fancy indexer
+                yield assert_take_ok, mgr, ax, []
+                yield assert_take_ok, mgr, ax, [0, 0, 0]
+                yield assert_take_ok, mgr, ax, lrange(mgr.shape[ax])
+
+                if mgr.shape[ax] >= 3:
+                    yield assert_take_ok, mgr, ax, [0, 1, 2]
+                    yield assert_take_ok, mgr, ax, [-1, -2, -3]
+
+    def test_reindex_axis(self):
+        def assert_reindex_axis_is_ok(mgr, axis, new_labels,
+                                      fill_value):
+            mat = mgr.as_matrix()
+            indexer = mgr.axes[axis].get_indexer_for(new_labels)
+
+            reindexed = mgr.reindex_axis(new_labels, axis,
+                                         fill_value=fill_value)
+            assert_almost_equal(com.take_nd(mat, indexer, axis,
+                                            fill_value=fill_value),
+                                reindexed.as_matrix())
+            assert_almost_equal(reindexed.axes[axis], new_labels)
+
+        for mgr in self.MANAGERS:
+            for ax in range(mgr.ndim):
+                for fill_value in (None, np.nan, 100.):
+                    yield assert_reindex_axis_is_ok, mgr, ax, [], fill_value
+                    yield (assert_reindex_axis_is_ok, mgr, ax,
+                           mgr.axes[ax], fill_value)
+                    yield (assert_reindex_axis_is_ok, mgr, ax,
+                           mgr.axes[ax][[0, 0, 0]], fill_value)
+                    yield (assert_reindex_axis_is_ok, mgr, ax,
+                           ['foo', 'bar', 'baz'], fill_value)
+                    yield (assert_reindex_axis_is_ok, mgr, ax,
+                           ['foo', mgr.axes[ax][0], 'baz'], fill_value)
+
+                    if mgr.shape[ax] >= 3:
+                        yield (assert_reindex_axis_is_ok, mgr, ax,
+                               mgr.axes[ax][:-3], fill_value)
+                        yield (assert_reindex_axis_is_ok, mgr, ax,
+                               mgr.axes[ax][-3::-1], fill_value)
+                        yield (assert_reindex_axis_is_ok, mgr, ax,
+                               mgr.axes[ax][[0, 1, 2, 0, 1, 2]], fill_value)
+
+    def test_reindex_indexer(self):
+        def assert_reindex_indexer_is_ok(mgr, axis, new_labels, indexer,
+                                         fill_value):
+            mat = mgr.as_matrix()
+            reindexed_mat = com.take_nd(mat, indexer, axis,
+                                        fill_value=fill_value)
+            reindexed = mgr.reindex_indexer(new_labels, indexer, axis,
+                                            fill_value=fill_value)
+            assert_almost_equal(reindexed_mat, reindexed.as_matrix())
+            assert_almost_equal(reindexed.axes[axis], new_labels)
+
+        for mgr in self.MANAGERS:
+            for ax in range(mgr.ndim):
+                for fill_value in (None, np.nan, 100.):
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           [], [], fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           mgr.axes[ax], np.arange(mgr.shape[ax]), fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           ['foo'] * mgr.shape[ax], np.arange(mgr.shape[ax]),
+                           fill_value)
+
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           mgr.axes[ax][::-1], np.arange(mgr.shape[ax]),
+                           fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           mgr.axes[ax], np.arange(mgr.shape[ax])[::-1],
+                           fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           ['foo', 'bar', 'baz'], [0, 0, 0], fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           ['foo', 'bar', 'baz'], [-1, 0, -1], fill_value)
+                    yield (assert_reindex_indexer_is_ok, mgr, ax,
+                           ['foo', mgr.axes[ax][0], 'baz'], [-1, -1, -1],
+                           fill_value)
+
+                    if mgr.shape[ax] >= 3:
+                        yield (assert_reindex_indexer_is_ok, mgr, ax,
+                               ['foo', 'bar', 'baz'], [0, 1, 2], fill_value)
+
+
+    # test_get_slice(slice_like, axis)
+    # take(indexer, axis)
+    # reindex_axis(new_labels, axis)
+    # reindex_indexer(new_labels, indexer, axis)
+
+
+
+
+class TestBlockPlacement(tm.TestCase):
+    _multiprocess_can_split_ = True
+
+    def test_slice_len(self):
+        self.assertEquals(len(BlockPlacement(slice(0, 4))), 4)
+        self.assertEquals(len(BlockPlacement(slice(0, 4, 2))), 2)
+        self.assertEquals(len(BlockPlacement(slice(0, 3, 2))), 2)
+
+        self.assertEquals(len(BlockPlacement(slice(0, 1, 2))), 1)
+        self.assertEquals(len(BlockPlacement(slice(1, 0, -1))), 1)
+
+    def test_zero_step_raises(self):
+        self.assertRaises(ValueError, BlockPlacement, slice(1, 1, 0))
+        self.assertRaises(ValueError, BlockPlacement, slice(1, 2, 0))
+
+    def test_unbounded_slice_raises(self):
+        def assert_unbounded_slice_error(slc):
+            # assertRaisesRegexp is not available in py2.6
+            # self.assertRaisesRegexp(ValueError, "unbounded slice",
+            #                         lambda: BlockPlacement(slc))
+            self.assertRaises(ValueError, BlockPlacement, slc)
+
+        assert_unbounded_slice_error(slice(None, None))
+        assert_unbounded_slice_error(slice(10, None))
+        assert_unbounded_slice_error(slice(None, None, -1))
+        assert_unbounded_slice_error(slice(None, 10, -1))
+
+        # These are "unbounded" because negative index will change depending on
+        # container shape.
+        assert_unbounded_slice_error(slice(-1, None))
+        assert_unbounded_slice_error(slice(None, -1))
+        assert_unbounded_slice_error(slice(-1, -1))
+        assert_unbounded_slice_error(slice(-1, None, -1))
+        assert_unbounded_slice_error(slice(None, -1, -1))
+        assert_unbounded_slice_error(slice(-1, -1, -1))
+
+    def test_not_slice_like_slices(self):
+        def assert_not_slice_like(slc):
+            self.assertTrue(not BlockPlacement(slc).is_slice_like)
+
+        assert_not_slice_like(slice(0, 0))
+        assert_not_slice_like(slice(100, 0))
+
+        assert_not_slice_like(slice(100, 100, -1))
+        assert_not_slice_like(slice(0, 100, -1))
+
+        self.assertTrue(not BlockPlacement(slice(0, 0)).is_slice_like)
+        self.assertTrue(not BlockPlacement(slice(100, 100)).is_slice_like)
+
+    def test_array_to_slice_conversion(self):
+        def assert_as_slice_equals(arr, slc):
+            self.assertEquals(BlockPlacement(arr).as_slice, slc)
+
+        assert_as_slice_equals([0], slice(0, 1, 1))
+        assert_as_slice_equals([100], slice(100, 101, 1))
+
+        assert_as_slice_equals([0, 1, 2], slice(0, 3, 1))
+        assert_as_slice_equals([0, 5, 10], slice(0, 15, 5))
+        assert_as_slice_equals([0, 100], slice(0, 200, 100))
+
+        assert_as_slice_equals([2, 1], slice(2, 0, -1))
+        assert_as_slice_equals([2, 1, 0], slice(2, None, -1))
+        assert_as_slice_equals([100, 0], slice(100, None, -100))
+
+    def test_not_slice_like_arrays(self):
+        def assert_not_slice_like(arr):
+            self.assertTrue(not BlockPlacement(arr).is_slice_like)
+
+        assert_not_slice_like([])
+        assert_not_slice_like([-1])
+        assert_not_slice_like([-1, -2, -3])
+        assert_not_slice_like([-10])
+        assert_not_slice_like([-1])
+        assert_not_slice_like([-1, 0, 1, 2])
+        assert_not_slice_like([-2, 0, 2, 4])
+        assert_not_slice_like([1, 0, -1])
+        assert_not_slice_like([1, 1, 1])
+
+    def test_slice_iter(self):
+        self.assertEquals(list(BlockPlacement(slice(0, 3))), [0, 1, 2])
+        self.assertEquals(list(BlockPlacement(slice(0, 0))), [])
+        self.assertEquals(list(BlockPlacement(slice(3, 0))), [])
+
+        self.assertEquals(list(BlockPlacement(slice(3, 0, -1))), [3, 2, 1])
+        self.assertEquals(list(BlockPlacement(slice(3, None, -1))),
+                          [3, 2, 1, 0])
+
+    def test_slice_to_array_conversion(self):
+        def assert_as_array_equals(slc, asarray):
+            np.testing.assert_array_equal(
+                BlockPlacement(slc).as_array,
+                np.asarray(asarray))
+
+        assert_as_array_equals(slice(0, 3), [0, 1, 2])
+        assert_as_array_equals(slice(0, 0), [])
+        assert_as_array_equals(slice(3, 0), [])
+
+        assert_as_array_equals(slice(3, 0, -1), [3, 2, 1])
+        assert_as_array_equals(slice(3, None, -1), [3, 2, 1, 0])
+        assert_as_array_equals(slice(31, None, -10), [31, 21, 11, 1])
+
+    def test_blockplacement_add(self):
+        bpl = BlockPlacement(slice(0, 5))
+        self.assertEquals(bpl.add(1).as_slice, slice(1, 6, 1))
+        self.assertEquals(bpl.add(np.arange(5)).as_slice,
+                          slice(0, 10, 2))
+        self.assertEquals(list(bpl.add(np.arange(5, 0, -1))),
+                          [5, 5, 5, 5, 5])
+
+    def test_blockplacement_add_int(self):
+        def assert_add_equals(val, inc, result):
+            self.assertEquals(list(BlockPlacement(val).add(inc)),
+                              result)
+
+        assert_add_equals(slice(0, 0), 0, [])
+        assert_add_equals(slice(1, 4), 0, [1, 2, 3])
+        assert_add_equals(slice(3, 0, -1), 0, [3, 2, 1])
+        assert_add_equals(slice(2, None, -1), 0, [2, 1, 0])
+        assert_add_equals([1, 2, 4], 0, [1, 2, 4])
+
+        assert_add_equals(slice(0, 0), 10, [])
+        assert_add_equals(slice(1, 4), 10, [11, 12, 13])
+        assert_add_equals(slice(3, 0, -1), 10, [13, 12, 11])
+        assert_add_equals(slice(2, None, -1), 10, [12, 11, 10])
+        assert_add_equals([1, 2, 4], 10, [11, 12, 14])
+
+        assert_add_equals(slice(0, 0), -1, [])
+        assert_add_equals(slice(1, 4), -1, [0, 1, 2])
+        assert_add_equals(slice(3, 0, -1), -1, [2, 1, 0])
+        assert_add_equals([1, 2, 4], -1, [0, 1, 3])
+
+        self.assertRaises(ValueError,
+                          lambda: BlockPlacement(slice(1, 4)).add(-10))
+        self.assertRaises(ValueError,
+                          lambda: BlockPlacement([1, 2, 4]).add(-10))
+        self.assertRaises(ValueError,
+                          lambda: BlockPlacement(slice(2, None, -1)).add(-1))
+
+    # def test_blockplacement_array_add(self):
+
+    #     assert_add_equals(slice(0, 2), [0, 1, 1], [0, 2, 3])
+    #     assert_add_equals(slice(2, None, -1), [1, 1, 0], [3, 2, 0])
+
+
 if __name__ == '__main__':
     import nose
     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
                    exit=False)
+
+
+    
diff --git a/vb_suite/eval.py b/vb_suite/eval.py
index 3b0efa9e8..36aa702b5 100644
--- a/vb_suite/eval.py
+++ b/vb_suite/eval.py
@@ -55,7 +55,7 @@ eval_frame_mult_one_thread = \
               start_date=datetime(2013, 7, 26))
 
 eval_frame_mult_python = \
-    Benchmark("pdl.eval('df * df2 * df3 * df4', engine='python')",
+    Benchmark("pd.eval('df * df2 * df3 * df4', engine='python')",
               common_setup,
               name='eval_frame_mult_python', start_date=datetime(2013, 7, 21))
 
@@ -102,7 +102,7 @@ eval_frame_chained_cmp_one_thread = \
               name='eval_frame_chained_cmp_one_thread',
               start_date=datetime(2013, 7, 26))
 
-setup = common_setup
+# setup = common_setup
 eval_frame_chained_cmp_python = \
     Benchmark("pd.eval('df < df2 < df3 < df4', engine='python')",
               common_setup, name='eval_frame_chained_cmp_python',
