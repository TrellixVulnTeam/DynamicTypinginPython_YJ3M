commit d63ac05a71dee525f51091a0d7be110f178d587b
Author: Phillip Cloud <cpcloud@gmail.com>
Date:   Wed Aug 28 12:06:19 2013 -0400

    CLN/ENH: add parse_dates arg and use TextReader

diff --git a/pandas/io/html.py b/pandas/io/html.py
index df94e0ffa..e2b3eca9c 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -7,15 +7,18 @@ import os
 import re
 import numbers
 import collections
+import warnings
 
+from itertools import repeat
 from distutils.version import LooseVersion
 
 import numpy as np
 
-from pandas import DataFrame, MultiIndex, isnull
 from pandas.io.common import _is_url, urlopen, parse_url
-from pandas.compat import range, lrange, lmap, u, map
-from pandas import compat
+from pandas.io.parsers import TextParser
+from pandas.compat import lrange, lmap, u
+from pandas.core import common as com
+from pandas import compat, Series
 
 
 try:
@@ -67,7 +70,7 @@ def _remove_whitespace(s, regex=_RE_WHITESPACE):
     return regex.sub(' ', s.strip())
 
 
-def _get_skiprows_iter(skiprows):
+def _get_skiprows(skiprows):
     """Get an iterator given an integer, slice or container.
 
     Parameters
@@ -92,10 +95,10 @@ def _get_skiprows_iter(skiprows):
     """
     if isinstance(skiprows, slice):
         return lrange(skiprows.start or 0, skiprows.stop, skiprows.step or 1)
-    elif isinstance(skiprows, numbers.Integral):
-        return lrange(skiprows)
-    elif isinstance(skiprows, collections.Container):
+    elif isinstance(skiprows, numbers.Integral) or com.is_list_like(skiprows):
         return skiprows
+    elif skiprows is None:
+        return 0
     else:
         raise TypeError('{0} is not a valid type for skipping'
                         ' rows'.format(type(skiprows)))
@@ -583,101 +586,34 @@ class _LxmlFrameParser(_HtmlFrameParser):
                 table.xpath(expr)]
 
 
-def _data_to_frame(data, header, index_col, infer_types, skiprows):
-    """Parse a BeautifulSoup table into a DataFrame.
+def _nan_list(n):
+    return list(repeat(np.nan, n))
 
-    Parameters
-    ----------
-    data : tuple of lists
-        The raw data to be placed into a DataFrame. This is a list of lists of
-        strings or unicode. If it helps, it can be thought of as a matrix of
-        strings instead.
-
-    header : int or None
-        An integer indicating the row to use for the column header or None
-        indicating no header will be used.
-
-    index_col : int or None
-        An integer indicating the column to use for the index or None
-        indicating no column will be used.
-
-    infer_types : bool
-        Whether to convert numbers and dates.
-
-    skiprows : collections.Container or int or slice
-        Iterable used to skip rows.
-
-    Returns
-    -------
-    df : DataFrame
-        A DataFrame containing the data from `data`
-
-    Raises
-    ------
-    ValueError
-        * If `skiprows` is not found in the rows of the parsed DataFrame.
 
-    Raises
-    ------
-    ValueError
-        * If `skiprows` is not found in the rows of the parsed DataFrame.
+def _expand_elements(body):
+    lens = Series(lmap(len, body))
+    lens_max = lens.max()
+    not_max = lens[lens != lens_max]
 
-    See Also
-    --------
-    read_html
+    for ind, length in not_max.iteritems():
+        body[ind] += _nan_list(lens_max - length)
 
-    Notes
-    -----
-    The `data` parameter is guaranteed not to be a list of empty lists.
-    """
-    thead, tbody, tfoot = data
-    columns = thead or None
-    df = DataFrame(tbody, columns=columns)
 
-    if skiprows is not None:
-        it = _get_skiprows_iter(skiprows)
+def _data_to_frame(data, header, index_col, skiprows, infer_types,
+                   parse_dates):
+    head, body, _ = data  # _ is footer which is rarely used: ignore for now
+    _expand_elements(body)
+    body = [head] + body
+    import ipdb; ipdb.set_trace()
+    tp = TextParser(body, header=header, index_col=index_col,
+                    skiprows=_get_skiprows(skiprows),
+                    parse_dates=parse_dates, tupleize_cols=False)
+    df = tp.read()
 
-        try:
-            df = df.drop(it)
-        except ValueError:
-            raise ValueError('Labels {0} not found when trying to skip'
-                             ' rows'.format(it))
-
-    # convert to numbers/dates where possible
-    # must be sequential since dates trump numbers if both args are given
-    if infer_types:
-        df = df.convert_objects(convert_numeric=True)
+    if infer_types:  # remove in 0.14
         df = df.convert_objects(convert_dates='coerce')
-
-    if header is not None:
-        header_rows = df.iloc[header]
-
-        if header_rows.ndim == 2:
-            names = header_rows.index
-            df.columns = MultiIndex.from_arrays(header_rows.values,
-                                                names=names)
-        else:
-            df.columns = header_rows
-
-        df = df.drop(df.index[header])
-
-    if index_col is not None:
-        cols = df.columns[index_col]
-
-        try:
-            cols = cols.tolist()
-        except AttributeError:
-            pass
-
-        # drop by default
-        df.set_index(cols, inplace=True)
-        if df.index.nlevels == 1:
-            if isnull(df.index.name) or not df.index.name:
-                df.index.name = None
-        else:
-            names = [name or None for name in df.index.names]
-            df.index = MultiIndex.from_tuples(df.index.values, names=names)
-
+    else:
+        df = df.applymap(compat.text_type)
     return df
 
 
@@ -750,7 +686,8 @@ def _validate_parser_flavor(flavor):
     return flavor
 
 
-def _parse(flavor, io, match, header, index_col, skiprows, infer_types, attrs):
+def _parse(flavor, io, match, header, index_col, skiprows, infer_types,
+           parse_dates, attrs):
     # bonus: re.compile is idempotent under function iteration so you can pass
     # a compiled regex to it and it will return itself
     flavor = _validate_parser_flavor(flavor)
@@ -771,12 +708,12 @@ def _parse(flavor, io, match, header, index_col, skiprows, infer_types, attrs):
     else:
         raise retained
 
-    return [_data_to_frame(table, header, index_col, infer_types, skiprows)
-            for table in tables]
+    return [_data_to_frame(table, header, index_col, skiprows, infer_types,
+                           parse_dates) for table in tables]
 
 
-def read_html(io, match='.+', flavor=None, header=None, index_col=None,
-              skiprows=None, infer_types=True, attrs=None):
+def read_html(io, match='.+', flavor=None, header=0, index_col=None,
+              skiprows=None, infer_types=None, attrs=None, parse_dates=False):
     r"""Read an HTML table into a DataFrame.
 
     Parameters
@@ -801,7 +738,7 @@ def read_html(io, match='.+', flavor=None, header=None, index_col=None,
         compatibility. The default of ``None`` tries to use ``lxml`` to parse
         and if that fails it falls back on ``bs4`` + ``html5lib``.
 
-    header : int or array-like or None, optional, default ``None``
+    header : int or array-like, optional, default ``0``
         The row (or rows for a MultiIndex) to use to make the columns headers.
         Note that this row will be removed from the data.
 
@@ -828,9 +765,7 @@ def read_html(io, match='.+', flavor=None, header=None, index_col=None,
         it is treated as "skip :math:`n` rows", *not* as "skip the
         :math:`n^\textrm{th}` row".
 
-    infer_types : bool, optional, default ``True``
-        Whether to convert numeric types and date-appearing strings to numbers
-        and dates, respectively.
+    infer_types : bool or None, optional, default ``None``, deprecated since 0.13, removed in 0.14
 
     attrs : dict or None, optional, default ``None``
         This is a dictionary of attributes that you can pass to use to identify
@@ -896,8 +831,13 @@ def read_html(io, match='.+', flavor=None, header=None, index_col=None,
     """
     # Type check here. We don't want to parse only to fail because of an
     # invalid value of an integer skiprows.
+    if infer_types is not None:
+        warnings.warn("infer_types will be removed in 0.14", UserWarning)
+    else:
+        infer_types = True  # remove in 0.14
+
     if isinstance(skiprows, numbers.Integral) and skiprows < 0:
         raise AssertionError('cannot skip rows starting from the end of the '
                              'data (you passed a negative value)')
     return _parse(flavor, io, match, header, index_col, skiprows, infer_types,
-                  attrs)
+                  parse_dates, attrs)
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 3ef3cbf85..0e1ffee42 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -1468,22 +1468,23 @@ class PythonParser(ParserBase):
                 col = self.orig_names[col]
             clean_conv[col] = f
 
-        return self._convert_to_ndarrays(data, self.na_values, self.na_fvalues, self.verbose,
-                                         clean_conv)
+        return self._convert_to_ndarrays(data, self.na_values, self.na_fvalues,
+                                         self.verbose, clean_conv)
 
     def _infer_columns(self):
+        #import ipdb; ipdb.set_trace()
         names = self.names
 
         if self.header is not None:
             header = self.header
 
             # we have a mi columns, so read and extra line
-            if isinstance(header,(list,tuple,np.ndarray)):
+            if isinstance(header, (list, tuple, np.ndarray)):
                 have_mi_columns = True
-                header = list(header) + [header[-1]+1]
+                header = list(header) + [header[-1] + 1]
             else:
                 have_mi_columns = False
-                header = [ header ]
+                header = [header]
 
             columns = []
             for level, hr in enumerate(header):
@@ -1498,7 +1499,7 @@ class PythonParser(ParserBase):
 
                 this_columns = []
                 for i, c in enumerate(line):
-                    if c == '':
+                    if not c:
                         if have_mi_columns:
                             this_columns.append('Unnamed: %d_level_%d' % (i,level))
                         else:
diff --git a/pandas/io/tests/test_html.py b/pandas/io/tests/test_html.py
index 44e4b5cfd..6ce0855c1 100644
--- a/pandas/io/tests/test_html.py
+++ b/pandas/io/tests/test_html.py
@@ -1,33 +1,30 @@
 from __future__ import print_function
+
 import os
 import re
-from unittest import TestCase
 import warnings
+
+try:
+    from importlib import import_module
+except ImportError:
+    import_module = __import__
+
+from unittest import TestCase
 from distutils.version import LooseVersion
-from pandas.io.common import URLError
 
 import nose
-from nose.tools import assert_raises
 
 import numpy as np
 from numpy.random import rand
 from numpy.testing.decorators import slow
-from pandas.compat import map, zip, StringIO
-import pandas.compat as compat
-
-try:
-    from importlib import import_module
-except ImportError:
-    import_module = __import__
-
-from pandas.io.html import read_html
-from pandas.io.common import urlopen
 
 from pandas import DataFrame, MultiIndex, read_csv, Timestamp
-from pandas.util.testing import (assert_frame_equal, network,
-                                 get_data_path)
+from pandas.compat import map, zip, StringIO, string_types
+from pandas.io.common import URLError, urlopen
+from pandas.io.html import read_html
 
-from pandas.util.testing import makeCustomDataframe as mkdf
+import pandas.util.testing as tm
+from pandas.util.testing import makeCustomDataframe as mkdf, network
 
 
 def _have_module(module_name):
@@ -40,11 +37,11 @@ def _have_module(module_name):
 
 def _skip_if_no(module_name):
     if not _have_module(module_name):
-        raise nose.SkipTest("{0} not found".format(module_name))
+        raise nose.SkipTest("{0!r} not found".format(module_name))
 
 
 def _skip_if_none_of(module_names):
-    if isinstance(module_names, compat.string_types):
+    if isinstance(module_names, string_types):
         _skip_if_no(module_names)
         if module_names == 'bs4':
             import bs4
@@ -54,17 +51,14 @@ def _skip_if_none_of(module_names):
         not_found = [module_name for module_name in module_names if not
                      _have_module(module_name)]
         if set(not_found) & set(module_names):
-            raise nose.SkipTest("{0} not found".format(not_found))
+            raise nose.SkipTest("{0!r} not found".format(not_found))
         if 'bs4' in module_names:
             import bs4
             if bs4.__version__ == LooseVersion('4.2.0'):
                 raise nose.SkipTest("Bad version of bs4: 4.2.0")
 
 
-DATA_PATH = get_data_path()
-
-def isframe(x):
-    return isinstance(x, DataFrame)
+DATA_PATH = tm.get_data_path()
 
 
 def assert_framelist_equal(list1, list2, *args, **kwargs):
@@ -72,10 +66,12 @@ def assert_framelist_equal(list1, list2, *args, **kwargs):
                                       'len(list1) == {0}, '
                                       'len(list2) == {1}'.format(len(list1),
                                                                  len(list2)))
-    assert all(map(lambda x, y: isframe(x) and isframe(y), list1, list2)), \
-        'not all list elements are DataFrames'
+    msg = 'not all list elements are DataFrames'
+    both_frames = all(map(lambda x, y: isinstance(x, DataFrame) and
+                          isinstance(y, DataFrame), list1, list2))
+    assert both_frames, msg
     for frame_i, frame_j in zip(list1, list2):
-        assert_frame_equal(frame_i, frame_j, *args, **kwargs)
+        tm.assert_frame_equal(frame_i, frame_j, *args, **kwargs)
         assert not frame_i.empty, 'frames are both empty'
 
 
@@ -83,9 +79,9 @@ def test_bs4_version_fails():
     _skip_if_none_of(('bs4', 'html5lib'))
     import bs4
     if bs4.__version__ == LooseVersion('4.2.0'):
-        assert_raises(AssertionError, read_html, os.path.join(DATA_PATH,
-                                                              "spam.html"),
-                      flavor='bs4')
+        tm.assert_raises(AssertionError, read_html, os.path.join(DATA_PATH,
+                                                                 "spam.html"),
+                         flavor='bs4')
 
 
 class TestReadHtmlBase(TestCase):
@@ -116,7 +112,7 @@ class TestReadHtmlBase(TestCase):
                                  index_col=0)[0]
         print(df.dtypes)
         print(res.dtypes)
-        assert_frame_equal(res, df)
+        tm.assert_frame_equal(res, df)
 
     @network
     def test_banklist_url(self):
@@ -145,13 +141,20 @@ class TestReadHtmlBase(TestCase):
 
         assert_framelist_equal(df1, df2)
 
-    def test_spam(self):
+    def test_spam_no_types(self):
         df1 = self.run_read_html(self.spam_data, '.*Water.*',
                                  infer_types=False)
         df2 = self.run_read_html(self.spam_data, 'Unit', infer_types=False)
 
         assert_framelist_equal(df1, df2)
-        print(df1[0])
+
+        self.assertEqual(df1[0].ix[0, 0], 'Proximates')
+        self.assertEqual(df1[0].columns[0], 'Nutrient')
+
+    def test_spam_with_types(self):
+        df1 = self.run_read_html(self.spam_data, '.*Water.*')
+        df2 = self.run_read_html(self.spam_data, 'Unit')
+        assert_framelist_equal(df1, df2)
 
         self.assertEqual(df1[0].ix[0, 0], 'Proximates')
         self.assertEqual(df1[0].columns[0], 'Nutrient')
@@ -167,9 +170,8 @@ class TestReadHtmlBase(TestCase):
             self.assert_(isinstance(df, DataFrame))
 
     def test_spam_header(self):
-        df = self.run_read_html(self.spam_data, '.*Water.*', header=0)
         df = self.run_read_html(self.spam_data, '.*Water.*', header=1)[0]
-        self.assertEqual(df.columns[0], 'Water')
+        self.assertEqual(df.columns[0], 'Proximates')
         self.assertFalse(df.empty)
 
     def test_skiprows_int(self):
@@ -179,10 +181,10 @@ class TestReadHtmlBase(TestCase):
         assert_framelist_equal(df1, df2)
 
     def test_skiprows_xrange(self):
-        df1 = [self.run_read_html(self.spam_data, '.*Water.*').pop()[2:]]
-        df2 = self.run_read_html(self.spam_data, 'Unit', skiprows=range(2))
-
-        assert_framelist_equal(df1, df2)
+        df1 = self.run_read_html(self.spam_data, '.*Water.*',
+                                 skiprows=range(2))[0]
+        df2 = self.run_read_html(self.spam_data, 'Unit', skiprows=range(2))[0]
+        tm.assert_frame_equal(df1, df2)
 
     def test_skiprows_list(self):
         df1 = self.run_read_html(self.spam_data, '.*Water.*', skiprows=[1, 2])
@@ -226,7 +228,7 @@ class TestReadHtmlBase(TestCase):
         assert_framelist_equal(df1, df2)
 
     def test_skiprows_invalid(self):
-        self.assertRaises(ValueError, self.run_read_html, self.spam_data,
+        self.assertRaises(TypeError, self.run_read_html, self.spam_data,
                           '.*Water.*', skiprows='asdf')
 
     def test_index(self):
@@ -237,8 +239,8 @@ class TestReadHtmlBase(TestCase):
     def test_header_and_index_no_types(self):
         df1 = self.run_read_html(self.spam_data, '.*Water.*', header=1,
                                  index_col=0, infer_types=False)
-        df2 = self.run_read_html(self.spam_data, 'Unit', header=1, index_col=0,
-                                 infer_types=False)
+        df2 = self.run_read_html(self.spam_data, 'Unit', header=1,
+                                 index_col=0, infer_types=False)
         assert_framelist_equal(df1, df2)
 
     def test_header_and_index_with_types(self):
@@ -336,6 +338,7 @@ class TestReadHtmlBase(TestCase):
 
     @slow
     def test_multiindex_header_skiprows(self):
+        import ipdb; ipdb.set_trace()
         df = self._bank_data(header=[0, 1], skiprows=1)[0]
         self.assert_(isinstance(df.columns, MultiIndex))
 
@@ -343,6 +346,7 @@ class TestReadHtmlBase(TestCase):
     def test_multiindex_header_index_skiprows(self):
         df = self._bank_data(header=[0, 1], index_col=[0, 1], skiprows=1)[0]
         self.assert_(isinstance(df.index, MultiIndex))
+        self.assert_(isinstance(df.columns, MultiIndex))
 
     @slow
     def test_regex_idempotency(self):
@@ -382,6 +386,7 @@ class TestReadHtmlBase(TestCase):
     @slow
     def test_banklist_header(self):
         from pandas.io.html import _remove_whitespace
+
         def try_remove_ws(x):
             try:
                 return _remove_whitespace(x)
@@ -412,8 +417,8 @@ class TestReadHtmlBase(TestCase):
         dfnew = df.applymap(try_remove_ws).replace(old, new)
         gtnew = ground_truth.applymap(try_remove_ws)
         converted = dfnew.convert_objects(convert_numeric=True)
-        assert_frame_equal(converted.convert_objects(convert_dates='coerce'),
-                           gtnew)
+        tm.assert_frame_equal(converted.convert_objects(convert_dates='coerce'),
+                              gtnew)
 
     @slow
     def test_gold_canyon(self):
@@ -446,7 +451,8 @@ class TestReadHtmlLxml(TestCase):
     def test_banklist_data_fail(self):
         from lxml.etree import XMLSyntaxError
         banklist_data = os.path.join(DATA_PATH, 'banklist.html')
-        self.assertRaises(XMLSyntaxError, self.run_read_html, banklist_data, flavor=['lxml'])
+        self.assertRaises(XMLSyntaxError, self.run_read_html, banklist_data,
+                          flavor=['lxml'])
 
     def test_works_on_valid_markup(self):
         filename = os.path.join(DATA_PATH, 'valid_markup.html')
