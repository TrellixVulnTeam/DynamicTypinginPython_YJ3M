commit 81dcf1018632e523ae5e83d7b394de3e433b10c3
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Sun Mar 4 15:43:44 2012 -0500

    ENH: use khash for integer Series.value_counts, move to algorithms.py, vbenchmarks GH #861

diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 649b2118f..8d86f9756 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -92,6 +92,33 @@ def factorize(values, sort=False, order=None, na_sentinel=-1):
 
     return labels, uniques, counts
 
+def value_counts(values, sort=True, ascending=False):
+    """
+    Compute a histogram of the counts of non-null values
+
+    Returns
+    -------
+    value_counts : Series
+    """
+    from collections import defaultdict
+    if com.is_integer_dtype(values.dtype):
+        values = com._ensure_int64(values)
+        keys, counts = lib.value_count_int64(values)
+        result = Series(counts, index=keys)
+    else:
+        counter = defaultdict(lambda: 0)
+        values = values[com.notnull(values)]
+        for value in values:
+            counter[value] += 1
+        result = Series(counter)
+
+    if sort:
+        result.sort()
+        if not ascending:
+            result = result[::-1]
+
+    return result
+
 
 def _get_hash_table_and_cast(values):
     if com.is_float_dtype(values):
diff --git a/pandas/core/series.py b/pandas/core/series.py
index fa51be1a4..40d22e10d 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -5,6 +5,7 @@ Data structure for 1-dimensional cross-sectional and time series data
 # pylint: disable=E1101,E1103
 # pylint: disable=W0703,W0622,W0613,W0201
 
+from collections import defaultdict
 from itertools import izip
 import operator
 from distutils.version import LooseVersion
@@ -915,11 +916,8 @@ copy : boolean, default False
         -------
         counts : Series
         """
-        from collections import defaultdict
-        counter = defaultdict(lambda: 0)
-        for value in self.dropna().values:
-            counter[value] += 1
-        return Series(counter).order(ascending=False)
+        import pandas.core.algorithms as algos
+        return algos.value_counts(self.values, sort=True, ascending=False)
 
     def unique(self):
         """
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
index a5ecd48e1..d32e91f9f 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/src/hashtable.pyx
@@ -401,6 +401,11 @@ cdef class Int64HashTable:
     def __dealloc__(self):
         kh_destroy_int64(self.table)
 
+    cdef inline bint has_key(self, int64_t val):
+        cdef khiter_t k
+        k = kh_get_int64(self.table, val)
+        return k != self.table.n_buckets
+
     cpdef get_item(self, int64_t val):
         cdef khiter_t k
         k = kh_get_int64(self.table, val)
@@ -577,6 +582,39 @@ cdef class Int64HashTable:
 
         return uniques
 
+def value_count_int64(ndarray[int64_t] values):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        kh_int64_t *table
+        int ret = 0
+        list uniques = []
+
+    table = kh_init_int64()
+    kh_resize_int64(table, n)
+
+    for i in range(n):
+        val = values[i]
+        k = kh_get_int64(table, val)
+        if k != table.n_buckets:
+            table.vals[k] += 1
+        else:
+            k = kh_put_int64(table, val, &ret)
+            table.vals[k] = 1
+
+    # for (k = kh_begin(h); k != kh_end(h); ++k)
+    # 	if (kh_exist(h, k)) kh_value(h, k) = 1;
+    i = 0
+    result_keys = np.empty(table.n_occupied, dtype=np.int64)
+    result_counts = np.zeros(table.n_occupied, dtype=np.int64)
+    for k in range(table.n_buckets):
+        if kh_exist_int64(table, k):
+            result_keys[i] = table.keys[k]
+            result_counts[i] = table.vals[k]
+            i += 1
+    kh_destroy_int64(table)
+
+    return result_keys, result_counts
+
 ONAN = np.nan
 
 cdef class Float64HashTable:
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 7be587c9a..71ac711ba 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -111,3 +111,13 @@ df = DataFrame({'key1': np.random.randint(0, 500, size=100000),
 
 groupby_multi_size = Benchmark("df.groupby(['key1', 'key2']).size()",
                                setup, start_date=datetime(2011, 10, 1))
+
+#----------------------------------------------------------------------
+# Series.value_counts
+
+setup = common_setup + """
+s = Series(np.random.randint(0, 1000, size=100000))
+"""
+
+series_value_counts_int64 = Benchmark('s.value_counts()', setup,
+                                      start_date=datetime(2011, 10, 21))
