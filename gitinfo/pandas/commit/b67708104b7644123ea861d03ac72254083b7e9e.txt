commit b67708104b7644123ea861d03ac72254083b7e9e
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Wed Oct 19 13:26:20 2011 -0400

    BUG: parser refactoring, fix GH #257 and #258

diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 8ed64a0f6..61c2e0575 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -2,15 +2,13 @@
 Module contains tools for processing files into DataFrames or other objects
 """
 
-from datetime import datetime
-from itertools import izip
 import numpy as np
 
 from pandas.core.index import Index, MultiIndex
 from pandas.core.frame import DataFrame
-
 import pandas._tseries as lib
 
+
 def read_csv(filepath_or_buffer, sep=None, header=0, index_col=None, names=None,
              skiprows=None, na_values=None, parse_dates=False,
              date_parser=None):
@@ -53,7 +51,7 @@ def read_csv(filepath_or_buffer, sep=None, header=0, index_col=None, names=None,
     return _simple_parser(lines,
                           header=header,
                           index_col=index_col,
-                          colNames=names,
+                          names=names,
                           na_values=na_values,
                           parse_dates=parse_dates,
                           date_parser=date_parser)
@@ -116,13 +114,18 @@ parsed : DataFrame
 """ % (_parser_params % _table_sep)
 
 
-def _simple_parser(lines, colNames=None, header=0, index_col=0,
+def _simple_parser(lines, names=None, header=0, index_col=0,
                    na_values=None, date_parser=None, parse_dates=True):
     """
     Workhorse function for processing nested list into DataFrame
 
     Should be replaced by np.genfromtxt eventually?
     """
+    passed_names = names is not None
+    if passed_names:
+        names = list(names)
+        header = None
+
     if header is not None:
         columns = []
         for i, c in enumerate(lines[header]):
@@ -141,32 +144,51 @@ def _simple_parser(lines, colNames=None, header=0, index_col=0,
             counts[col] = cur_count + 1
     else:
         ncols = len(lines[0])
-        if not colNames:
+        if not names:
             columns = ['X.%d' % (i + 1) for i in range(ncols)]
         else:
-            assert(len(colNames) == ncols)
-            columns = colNames
+            columns = names
         content = lines
 
+    # spaghetti
+
+    # implicitly index_col=0 b/c 1 fewer column names
+    index_name = None
+    implicit_first_col = (len(content) > 0 and
+                          len(content[0]) == len(columns) + 1)
+
+    if implicit_first_col:
+        if index_col is None:
+            index_col = 0
+        index_name = None
+    elif np.isscalar(index_col):
+        if passed_names:
+            index_name = None
+        else:
+            index_name = columns.pop(index_col)
+    elif index_col is not None:
+        if not passed_names:
+            cp_cols = list(columns)
+            index_name = []
+            for i in index_col:
+                name = cp_cols[i]
+                columns.remove(name)
+                index_name.append(name)
+        else:
+            index_name=None
+
     if len(content) == 0: # pragma: no cover
         if index_col is not None:
             if np.isscalar(index_col):
-                index = Index([], name=columns.pop(index_col))
+                index = Index([], name=index_name)
             else:
-                cp_cols = list(columns)
-                names = []
-                for i in index_col:
-                    name = cp_cols[i]
-                    columns.remove(name)
-                    names.append(name)
                 index = MultiIndex.fromarrays([[]] * len(index_col),
-                                              names=names)
+                                              names=index_name)
         else:
             index = Index([])
 
         return DataFrame(index=index, columns=columns)
 
-
     # common NA values
     # no longer excluding inf representations
     # '1.#INF','-1.#INF', '1.#INF000000',
@@ -178,42 +200,29 @@ def _simple_parser(lines, colNames=None, header=0, index_col=0,
     else:
         na_values = set(list(na_values)) | NA_VALUES
 
-
     zipped_content = list(lib.to_object_array(content).T)
 
-    if index_col is None and len(content[0]) == len(columns) + 1:
-        index_col = 0
-
     # no index column specified, so infer that's what is wanted
     if index_col is not None:
         if np.isscalar(index_col):
             index = zipped_content.pop(index_col)
-
-            if len(content[0]) == len(columns) + 1:
-                name = None
-            else:
-                name = columns.pop(index_col)
-
         else: # given a list of index
-            idx_names = []
             index = []
             for idx in index_col:
-                idx_names.append(columns[idx])
                 index.append(zipped_content[idx])
             #remove index items from content and columns, don't pop in loop
             for i in range(len(index_col)):
-                columns.remove(idx_names[i])
                 zipped_content.remove(index[i])
 
         if np.isscalar(index_col):
             if parse_dates:
                 index = lib.try_parse_dates(index, parser=date_parser)
-            index = Index(_convert_types(index, na_values), name=name)
+            index = Index(_convert_types(index, na_values),
+                          name=index_name)
         else:
             arrays = _maybe_convert_int_mindex(index, parse_dates,
                                                date_parser)
-            index = MultiIndex.from_arrays(arrays,
-                                                 names=idx_names)
+            index = MultiIndex.from_arrays(arrays, names=index_name)
     else:
         index = Index(np.arange(len(content)))
 
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index c98289246..632d715ab 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -113,10 +113,26 @@ c,4,5
                     [6,7,8,9,10],
                     [11,12,13,14,15]]
         assert_almost_equal(df.values, expected)
+        assert_almost_equal(df.values, df2.values)
         self.assert_(np.array_equal(df.columns,
                                     ['X.1', 'X.2', 'X.3', 'X.4', 'X.5']))
         self.assert_(np.array_equal(df2.columns, names))
 
+    def test_header_with_index_col(self):
+        data = """foo,1,2,3
+bar,4,5,6
+baz,7,8,9
+"""
+        names = ['A', 'B', 'C']
+        df = read_csv(StringIO(data), names=names)
+
+        self.assertEqual(names, ['A', 'B', 'C'])
+
+        data = [[1,2,3],[4,5,6],[7,8,9]]
+        expected = DataFrame(data, index=['foo','bar','baz'],
+                             columns=['A','B','C'])
+        assert_frame_equal(df, expected)
+
     def test_read_csv_dataframe(self):
         df = read_csv(self.csv1, index_col=0, parse_dates=True)
         df2 = read_table(self.csv1, sep=',', index_col=0, parse_dates=True)
@@ -196,53 +212,52 @@ baz,7,8,9
         self.assert_(data.index.equals(Index(['foo', 'bar', 'baz'])))
 
 
-def test_convert_sql_column_floats():
-    arr = np.array([1.5, None, 3, 4.2], dtype=object)
-    result = lib.convert_sql_column(arr)
-    expected = np.array([1.5, np.nan, 3, 4.2], dtype='f8')
-    assert_same_values_and_dtype(result, expected)
-
-def test_convert_sql_column_strings():
-    arr = np.array(['1.5', None, '3', '4.2'], dtype=object)
-    result = lib.convert_sql_column(arr)
-    expected = np.array(['1.5', np.nan, '3', '4.2'], dtype=object)
-    assert_same_values_and_dtype(result, expected)
-
-def test_convert_sql_column_unicode():
-    arr = np.array([u'1.5', None, u'3', u'4.2'], dtype=object)
-    result = lib.convert_sql_column(arr)
-    expected = np.array([u'1.5', np.nan, u'3', u'4.2'], dtype=object)
-    assert_same_values_and_dtype(result, expected)
-
-def test_convert_sql_column_ints():
-    arr = np.array([1, 2, 3, 4], dtype='O')
-    arr2 = np.array([1, 2, 3, 4], dtype='i4').astype('O')
-    result = lib.convert_sql_column(arr)
-    result2 = lib.convert_sql_column(arr2)
-    expected = np.array([1, 2, 3, 4], dtype='i8')
-    assert_same_values_and_dtype(result, expected)
-    assert_same_values_and_dtype(result2, expected)
-
-def test_convert_sql_column_bools():
-    arr = np.array([True, False, True, False], dtype='O')
-    result = lib.convert_sql_column(arr)
-    expected = np.array([True, False, True, False], dtype=bool)
-    assert_same_values_and_dtype(result, expected)
-
-    arr = np.array([True, False, None, False], dtype='O')
-    result = lib.convert_sql_column(arr)
-    expected = np.array([True, False, np.nan, False], dtype=object)
-    assert_same_values_and_dtype(result, expected)
-
-def test_convert_sql_column_decimals():
-    from decimal import Decimal
-    arr = np.array([Decimal('1.5'), None, Decimal('3'), Decimal('4.2')])
-    result = lib.convert_sql_column(arr)
-    expected = np.array([1.5, np.nan, 3, 4.2], dtype='f8')
-    assert_same_values_and_dtype(result, expected)
-
-def test_convert_sql_column_other():
-    arr = np.array([1.5, None, 3, 4.2])
+class TestParseSQL(unittest.TestCase):
+
+    def test_convert_sql_column_floats(self):
+        arr = np.array([1.5, None, 3, 4.2], dtype=object)
+        result = lib.convert_sql_column(arr)
+        expected = np.array([1.5, np.nan, 3, 4.2], dtype='f8')
+        assert_same_values_and_dtype(result, expected)
+
+    def test_convert_sql_column_strings(self):
+        arr = np.array(['1.5', None, '3', '4.2'], dtype=object)
+        result = lib.convert_sql_column(arr)
+        expected = np.array(['1.5', np.nan, '3', '4.2'], dtype=object)
+        assert_same_values_and_dtype(result, expected)
+
+    def test_convert_sql_column_unicode(self):
+        arr = np.array([u'1.5', None, u'3', u'4.2'], dtype=object)
+        result = lib.convert_sql_column(arr)
+        expected = np.array([u'1.5', np.nan, u'3', u'4.2'], dtype=object)
+        assert_same_values_and_dtype(result, expected)
+
+    def test_convert_sql_column_ints(self):
+        arr = np.array([1, 2, 3, 4], dtype='O')
+        arr2 = np.array([1, 2, 3, 4], dtype='i4').astype('O')
+        result = lib.convert_sql_column(arr)
+        result2 = lib.convert_sql_column(arr2)
+        expected = np.array([1, 2, 3, 4], dtype='i8')
+        assert_same_values_and_dtype(result, expected)
+        assert_same_values_and_dtype(result2, expected)
+
+    def test_convert_sql_column_bools(self):
+        arr = np.array([True, False, True, False], dtype='O')
+        result = lib.convert_sql_column(arr)
+        expected = np.array([True, False, True, False], dtype=bool)
+        assert_same_values_and_dtype(result, expected)
+
+        arr = np.array([True, False, None, False], dtype='O')
+        result = lib.convert_sql_column(arr)
+        expected = np.array([True, False, np.nan, False], dtype=object)
+        assert_same_values_and_dtype(result, expected)
+
+    def test_convert_sql_column_decimals(self):
+        from decimal import Decimal
+        arr = np.array([Decimal('1.5'), None, Decimal('3'), Decimal('4.2')])
+        result = lib.convert_sql_column(arr)
+        expected = np.array([1.5, np.nan, 3, 4.2], dtype='f8')
+        assert_same_values_and_dtype(result, expected)
 
 def assert_same_values_and_dtype(res, exp):
     assert(res.dtype == exp.dtype)
