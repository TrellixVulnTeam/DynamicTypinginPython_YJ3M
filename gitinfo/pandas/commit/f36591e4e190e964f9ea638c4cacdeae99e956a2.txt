commit f36591e4e190e964f9ea638c4cacdeae99e956a2
Author: Wouter Overmeire <lodagro@gmail.com>
Date:   Fri Oct 19 23:47:10 2012 +0200

    STY: pep8

diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index d5380b66a..cb7314a26 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -9,6 +9,7 @@ import pandas.core.common as com
 import pandas.lib as lib
 import pandas._algos as _algos
 
+
 def match(to_match, values, na_sentinel=-1):
     """
     Compute locations of to_match into values
@@ -36,6 +37,7 @@ def match(to_match, values, na_sentinel=-1):
     f = lambda htype, caster: _match_generic(to_match, values, htype, caster)
     return _hashtable_algo(f, values.dtype)
 
+
 def unique(values):
     """
     Compute unique values (not necessarily sorted) efficiently from input array
@@ -62,6 +64,7 @@ def count(values, uniques=None):
     else:
         return _hashtable_algo(f, values.dtype)
 
+
 def _hashtable_algo(f, dtype):
     """
     f(HashTable, type_caster) -> result
@@ -83,6 +86,7 @@ def _count_generic(values, table_type, type_caster):
 
     return Series(counts, index=uniques)
 
+
 def _match_generic(values, index, table_type, type_caster):
     values = type_caster(values)
     index = type_caster(index)
@@ -90,6 +94,7 @@ def _match_generic(values, index, table_type, type_caster):
     table.map_locations(index)
     return table.lookup(values)
 
+
 def _unique_generic(values, table_type, type_caster):
     values = type_caster(values)
     table = table_type(min(len(values), 1000000))
@@ -138,6 +143,7 @@ def factorize(values, sort=False, order=None, na_sentinel=-1):
 
     return labels, uniques, counts
 
+
 def value_counts(values, sort=True, ascending=False):
     """
     Compute a histogram of the counts of non-null values
@@ -192,6 +198,7 @@ def rank(values, axis=0, method='average', na_option='keep',
                   ascending=ascending)
     return ranks
 
+
 def quantile(x, q, interpolation_method='fraction'):
     """
     Compute sample quantile or quantiles of the input array. For example, q=0.5
@@ -254,8 +261,8 @@ def quantile(x, q, interpolation_method='fraction'):
             elif interpolation_method == 'higher':
                 score = values[np.ceil(idx)]
             else:
-                raise ValueError("interpolation_method can only be 'fraction', " \
-                                 "'lower' or 'higher'")
+                raise ValueError("interpolation_method can only be 'fraction' "
+                                 ", 'lower' or 'higher'")
 
         return score
 
@@ -265,11 +272,12 @@ def quantile(x, q, interpolation_method='fraction'):
         q = np.asarray(q, np.float64)
         return _algos.arrmap_float64(q, _get_score)
 
+
 def _interpolate(a, b, fraction):
     """Returns the point at the given fraction between a and b, where
     'fraction' must be between 0 and 1.
     """
-    return a + (b - a)*fraction
+    return a + (b - a) * fraction
 
 
 def _get_data_algo(values, func_map):
@@ -287,6 +295,7 @@ def _get_data_algo(values, func_map):
         values = com._ensure_object(values)
     return f, values
 
+
 def group_position(*args):
     """
     Get group position
@@ -303,19 +312,19 @@ def group_position(*args):
 
 
 _rank1d_functions = {
-    'float64' : lib.rank_1d_float64,
-    'int64' : lib.rank_1d_int64,
-    'generic' : lib.rank_1d_generic
+    'float64': lib.rank_1d_float64,
+    'int64': lib.rank_1d_int64,
+    'generic': lib.rank_1d_generic
 }
 
 _rank2d_functions = {
-    'float64' : lib.rank_2d_float64,
-    'int64' : lib.rank_2d_int64,
-    'generic' : lib.rank_2d_generic
+    'float64': lib.rank_2d_float64,
+    'int64': lib.rank_2d_int64,
+    'generic': lib.rank_2d_generic
 }
 
 _hashtables = {
-    'float64' : lib.Float64HashTable,
-    'int64' : lib.Int64HashTable,
-    'generic' : lib.PyObjectHashTable
+    'float64': lib.Float64HashTable,
+    'int64': lib.Int64HashTable,
+    'generic': lib.PyObjectHashTable
 }
diff --git a/pandas/core/categorical.py b/pandas/core/categorical.py
index 34b05d1a2..1ff23bcce 100644
--- a/pandas/core/categorical.py
+++ b/pandas/core/categorical.py
@@ -24,6 +24,7 @@ def _cat_compare_op(op):
 
     return f
 
+
 class Categorical(object):
     """
     Represents a categorical variable in classic R / S-plus fashion
@@ -60,6 +61,7 @@ class Categorical(object):
                            name=getattr(data, 'name', None))
 
     _levels = None
+
     def _set_levels(self, levels):
         from pandas.core.index import _ensure_index
 
@@ -95,7 +97,8 @@ class Categorical(object):
 
         indent = ' ' * (levstring.find('[') + len(levheader) + 1)
         lines = levstring.split('\n')
-        levstring = '\n'.join([lines[0]] + [indent + x.lstrip() for x in lines[1:]])
+        levstring = '\n'.join([lines[0]] +
+                              [indent + x.lstrip() for x in lines[1:]])
 
         return temp % ('' if self.name is None else self.name,
                        repr(values), levheader + levstring)
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 8e851c671..c400a5e11 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -30,15 +30,18 @@ from pandas.util.py3compat import StringIO, BytesIO
 try:
     np.seterr(all='ignore')
     # np.set_printoptions(suppress=True)
-except Exception: # pragma: no cover
+except Exception:  # pragma: no cover
     pass
 
+
 class PandasError(Exception):
     pass
 
+
 class AmbiguousIndexError(PandasError, KeyError):
     pass
 
+
 def isnull(obj):
     '''
     Replacement for numpy.isnan / -numpy.isfinite which is suitable
@@ -66,6 +69,7 @@ def isnull(obj):
     else:
         return obj is None
 
+
 def _isnull_ndarraylike(obj):
     from pandas import Series
     values = np.asarray(obj)
@@ -90,6 +94,7 @@ def _isnull_ndarraylike(obj):
         result = -np.isfinite(obj)
     return result
 
+
 def notnull(obj):
     '''
     Replacement for numpy.isfinite / -numpy.isnan which is suitable
@@ -108,6 +113,7 @@ def notnull(obj):
         return not res
     return -res
 
+
 def mask_missing(arr, values_to_mask):
     """
     Return a masking array of same size/shape as arr
@@ -139,6 +145,7 @@ def mask_missing(arr, values_to_mask):
 
     return mask
 
+
 def _pickle_array(arr):
     arr = arr.view(np.ndarray)
 
@@ -147,10 +154,12 @@ def _pickle_array(arr):
 
     return buf.getvalue()
 
+
 def _unpickle_array(bytes):
     arr = read_array(BytesIO(bytes))
     return arr
 
+
 def _view_wrapper(f, wrap_dtype, na_override=None):
     def wrapper(arr, indexer, out, fill_value=np.nan):
         if na_override is not None and np.isnan(fill_value):
@@ -162,45 +171,46 @@ def _view_wrapper(f, wrap_dtype, na_override=None):
 
 
 _take1d_dict = {
-    'float64' : _algos.take_1d_float64,
-    'int32' : _algos.take_1d_int32,
-    'int64' : _algos.take_1d_int64,
-    'object' : _algos.take_1d_object,
-    'bool' : _view_wrapper(_algos.take_1d_bool, np.uint8),
-    'datetime64[ns]' : _view_wrapper(_algos.take_1d_int64, np.int64,
-                                     na_override=lib.iNaT),
+    'float64': _algos.take_1d_float64,
+    'int32': _algos.take_1d_int32,
+    'int64': _algos.take_1d_int64,
+    'object': _algos.take_1d_object,
+    'bool': _view_wrapper(_algos.take_1d_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(_algos.take_1d_int64, np.int64,
+                                    na_override=lib.iNaT),
 }
 
 _take2d_axis0_dict = {
-    'float64' : _algos.take_2d_axis0_float64,
-    'int32' : _algos.take_2d_axis0_int32,
-    'int64' : _algos.take_2d_axis0_int64,
-    'object' : _algos.take_2d_axis0_object,
-    'bool' : _view_wrapper(_algos.take_2d_axis0_bool, np.uint8),
-    'datetime64[ns]' : _view_wrapper(_algos.take_2d_axis0_int64, np.int64,
-                                     na_override=lib.iNaT),
+    'float64': _algos.take_2d_axis0_float64,
+    'int32': _algos.take_2d_axis0_int32,
+    'int64': _algos.take_2d_axis0_int64,
+    'object': _algos.take_2d_axis0_object,
+    'bool': _view_wrapper(_algos.take_2d_axis0_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(_algos.take_2d_axis0_int64, np.int64,
+                                    na_override=lib.iNaT),
 }
 
 _take2d_axis1_dict = {
-    'float64' : _algos.take_2d_axis1_float64,
-    'int32' : _algos.take_2d_axis1_int32,
-    'int64' : _algos.take_2d_axis1_int64,
-    'object' : _algos.take_2d_axis1_object,
-    'bool' : _view_wrapper(_algos.take_2d_axis1_bool, np.uint8),
-    'datetime64[ns]' : _view_wrapper(_algos.take_2d_axis1_int64, np.int64,
+    'float64': _algos.take_2d_axis1_float64,
+    'int32': _algos.take_2d_axis1_int32,
+    'int64': _algos.take_2d_axis1_int64,
+    'object': _algos.take_2d_axis1_object,
+    'bool': _view_wrapper(_algos.take_2d_axis1_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(_algos.take_2d_axis1_int64, np.int64,
                                      na_override=lib.iNaT),
 }
 
 _take2d_multi_dict = {
-    'float64' : _algos.take_2d_multi_float64,
-    'int32' : _algos.take_2d_multi_int32,
-    'int64' : _algos.take_2d_multi_int64,
-    'object' : _algos.take_2d_multi_object,
-    'bool' : _view_wrapper(_algos.take_2d_multi_bool, np.uint8),
-    'datetime64[ns]' : _view_wrapper(_algos.take_2d_multi_int64, np.int64,
-                                     na_override=lib.iNaT),
+    'float64': _algos.take_2d_multi_float64,
+    'int32': _algos.take_2d_multi_int32,
+    'int64': _algos.take_2d_multi_int64,
+    'object': _algos.take_2d_multi_object,
+    'bool': _view_wrapper(_algos.take_2d_multi_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(_algos.take_2d_multi_int64, np.int64,
+                                    na_override=lib.iNaT),
 }
 
+
 def _get_take2d_function(dtype_str, axis=0):
     if axis == 0:
         return _take2d_axis0_dict[dtype_str]
@@ -208,9 +218,10 @@ def _get_take2d_function(dtype_str, axis=0):
         return _take2d_axis1_dict[dtype_str]
     elif axis == 'multi':
         return _take2d_multi_dict[dtype_str]
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('bad axis: %s' % axis)
 
+
 def take_1d(arr, indexer, out=None, fill_value=np.nan):
     """
     Specialized Cython take which sets NaN values in one pass
@@ -258,6 +269,7 @@ def take_1d(arr, indexer, out=None, fill_value=np.nan):
 
     return out
 
+
 def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan, out=None):
 
     dtype_str = arr.dtype.name
@@ -266,7 +278,7 @@ def take_2d_multi(arr, row_idx, col_idx, fill_value=np.nan, out=None):
 
     if dtype_str in ('int32', 'int64', 'bool'):
         row_mask = row_idx == -1
-        col_mask=  col_idx == -1
+        col_mask = col_idx == -1
         needs_masking = row_mask.any() or col_mask.any()
 
         if needs_masking:
@@ -348,15 +360,18 @@ def take_2d(arr, indexer, out=None, mask=None, needs_masking=None, axis=0,
                              fill_value=fill_value)
         return result
 
+
 def ndtake(arr, indexer, axis=0, out=None):
     return arr.take(_ensure_platform_int(indexer), axis=axis, out=out)
 
+
 def mask_out_axis(arr, mask, axis, fill_value=np.nan):
     indexer = [slice(None)] * arr.ndim
     indexer[axis] = mask
 
     arr[tuple(indexer)] = fill_value
 
+
 def take_fast(arr, indexer, mask, needs_masking, axis=0, out=None,
               fill_value=np.nan):
     if arr.ndim == 2:
@@ -369,6 +384,7 @@ def take_fast(arr, indexer, mask, needs_masking, axis=0, out=None,
                          out_passed=out is not None, fill_value=fill_value)
     return result
 
+
 def _maybe_mask(result, mask, needs_masking, axis=0, out_passed=False,
                 fill_value=np.nan):
     if needs_masking:
@@ -380,6 +396,7 @@ def _maybe_mask(result, mask, needs_masking, axis=0, out_passed=False,
             mask_out_axis(result, mask, axis, fill_value)
     return result
 
+
 def _maybe_upcast(values):
     if issubclass(values.dtype.type, np.integer):
         values = values.astype(float)
@@ -388,11 +405,13 @@ def _maybe_upcast(values):
 
     return values
 
+
 def _need_upcast(values):
     if issubclass(values.dtype.type, (np.integer, np.bool_)):
         return True
     return False
 
+
 def _interp_wrapper(f, wrap_dtype, na_override=None):
     def wrapper(arr, mask, limit=None):
         view = arr.view(wrap_dtype)
@@ -401,8 +420,11 @@ def _interp_wrapper(f, wrap_dtype, na_override=None):
 
 _pad_1d_datetime = _interp_wrapper(_algos.pad_inplace_int64, np.int64)
 _pad_2d_datetime = _interp_wrapper(_algos.pad_2d_inplace_int64, np.int64)
-_backfill_1d_datetime = _interp_wrapper(_algos.backfill_inplace_int64, np.int64)
-_backfill_2d_datetime = _interp_wrapper(_algos.backfill_2d_inplace_int64, np.int64)
+_backfill_1d_datetime = _interp_wrapper(_algos.backfill_inplace_int64,
+                                        np.int64)
+_backfill_2d_datetime = _interp_wrapper(_algos.backfill_2d_inplace_int64,
+                                        np.int64)
+
 
 def pad_1d(values, limit=None, mask=None):
     if is_float_dtype(values):
@@ -411,7 +433,7 @@ def pad_1d(values, limit=None, mask=None):
         _method = _pad_1d_datetime
     elif values.dtype == np.object_:
         _method = _algos.pad_inplace_object
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
     if mask is None:
@@ -419,6 +441,7 @@ def pad_1d(values, limit=None, mask=None):
     mask = mask.view(np.uint8)
     _method(values, mask, limit=limit)
 
+
 def backfill_1d(values, limit=None, mask=None):
     if is_float_dtype(values):
         _method = _algos.backfill_inplace_float64
@@ -426,7 +449,7 @@ def backfill_1d(values, limit=None, mask=None):
         _method = _backfill_1d_datetime
     elif values.dtype == np.object_:
         _method = _algos.backfill_inplace_object
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
     if mask is None:
@@ -435,6 +458,7 @@ def backfill_1d(values, limit=None, mask=None):
 
     _method(values, mask, limit=limit)
 
+
 def pad_2d(values, limit=None, mask=None):
     if is_float_dtype(values):
         _method = _algos.pad_2d_inplace_float64
@@ -442,7 +466,7 @@ def pad_2d(values, limit=None, mask=None):
         _method = _pad_2d_datetime
     elif values.dtype == np.object_:
         _method = _algos.pad_2d_inplace_object
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
     if mask is None:
@@ -455,6 +479,7 @@ def pad_2d(values, limit=None, mask=None):
         # for test coverage
         pass
 
+
 def backfill_2d(values, limit=None, mask=None):
     if is_float_dtype(values):
         _method = _algos.backfill_2d_inplace_float64
@@ -462,7 +487,7 @@ def backfill_2d(values, limit=None, mask=None):
         _method = _backfill_2d_datetime
     elif values.dtype == np.object_:
         _method = _algos.backfill_2d_inplace_object
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
     if mask is None:
@@ -475,6 +500,7 @@ def backfill_2d(values, limit=None, mask=None):
         # for test coverage
         pass
 
+
 def _consensus_name_attr(objs):
     name = objs[0].name
     for obj in objs[1:]:
@@ -485,6 +511,7 @@ def _consensus_name_attr(objs):
 #----------------------------------------------------------------------
 # Lots of little utilities
 
+
 def _infer_dtype(value):
     if isinstance(value, (float, np.floating)):
         return np.float_
@@ -495,15 +522,17 @@ def _infer_dtype(value):
     else:
         return np.object_
 
+
 def _possibly_cast_item(obj, item, dtype):
     chunk = obj[item]
 
     if chunk.values.dtype != dtype:
         if dtype in (np.object_, np.bool_):
             obj[item] = chunk.astype(np.object_)
-        elif not issubclass(dtype, (np.integer, np.bool_)): # pragma: no cover
+        elif not issubclass(dtype, (np.integer, np.bool_)):  # pragma: no cover
             raise ValueError("Unexpected dtype encountered: %s" % dtype)
 
+
 def _is_bool_indexer(key):
     if isinstance(key, np.ndarray) and key.dtype == np.object_:
         key = np.asarray(key)
@@ -519,21 +548,24 @@ def _is_bool_indexer(key):
     elif isinstance(key, list):
         try:
             return np.asarray(key).dtype == np.bool_
-        except TypeError: # pragma: no cover
+        except TypeError:  # pragma: no cover
             return False
 
     return False
 
+
 def _default_index(n):
     from pandas.core.index import Index
     return Index(np.arange(n))
 
+
 def ensure_float(arr):
     if issubclass(arr.dtype.type, np.integer):
         arr = arr.astype(float)
 
     return arr
 
+
 def _mut_exclusive(arg1, arg2):
     if arg1 is not None and arg2 is not None:
         raise Exception('mutually exclusive arguments')
@@ -542,18 +574,21 @@ def _mut_exclusive(arg1, arg2):
     else:
         return arg2
 
+
 def _any_none(*args):
     for arg in args:
         if arg is None:
             return True
     return False
 
+
 def _all_not_none(*args):
     for arg in args:
         if arg is None:
             return False
     return True
 
+
 def _try_sort(iterable):
     listed = list(iterable)
     try:
@@ -561,17 +596,20 @@ def _try_sort(iterable):
     except Exception:
         return listed
 
+
 def _count_not_none(*args):
     return sum(x is not None for x in args)
 
 #------------------------------------------------------------------------------
 # miscellaneous python tools
 
+
 def rands(n):
     """Generates a random alphanumeric string of length *n*"""
     from random import Random
     import string
-    return ''.join(Random().sample(string.ascii_letters+string.digits, n))
+    return ''.join(Random().sample(string.ascii_letters + string.digits, n))
+
 
 def adjoin(space, *lists):
     """
@@ -595,6 +633,7 @@ def adjoin(space, *lists):
         out_lines.append(_join_unicode(lines))
     return _join_unicode(out_lines, sep='\n')
 
+
 def _join_unicode(lines, sep=''):
     try:
         return sep.join(lines)
@@ -603,6 +642,7 @@ def _join_unicode(lines, sep=''):
         return sep.join([x.decode('utf-8') if isinstance(x, str) else x
                          for x in lines])
 
+
 def iterpairs(seq):
     """
     Parameters
@@ -625,10 +665,12 @@ def iterpairs(seq):
 
     return itertools.izip(seq_it, seq_it_next)
 
+
 def indent(string, spaces=4):
     dent = ' ' * spaces
     return '\n'.join([dent + x for x in string.split('\n')])
 
+
 def banner(message):
     """
     Return 80-char width message declaration with = bars on top and bottom.
@@ -636,6 +678,7 @@ def banner(message):
     bar = '=' * 80
     return '%s\n%s\n%s' % (bar, message, bar)
 
+
 class groupby(dict):
     """
     A simple groupby different from the one in itertools.
@@ -643,7 +686,7 @@ class groupby(dict):
     Does not require the sequence elements to be sorted by keys,
     however it is slower.
     """
-    def __init__(self, seq, key=lambda x:x):
+    def __init__(self, seq, key=lambda x: x):
         for value in seq:
             k = key(value)
             self.setdefault(k, []).append(value)
@@ -654,6 +697,7 @@ class groupby(dict):
         def __iter__(self):
             return iter(dict.items(self))
 
+
 def map_indices_py(arr):
     """
     Returns a dictionary with (element, index) pairs for each element in the
@@ -661,6 +705,7 @@ def map_indices_py(arr):
     """
     return dict([(x, i) for i, x in enumerate(arr)])
 
+
 def union(*seqs):
     result = set([])
     for seq in seqs:
@@ -669,9 +714,11 @@ def union(*seqs):
         result |= seq
     return type(seqs[0])(list(result))
 
+
 def difference(a, b):
     return type(a)(list(set(a) - set(b)))
 
+
 def intersection(*seqs):
     result = set(seqs[0])
     for seq in seqs:
@@ -680,6 +727,7 @@ def intersection(*seqs):
         result &= seq
     return type(seqs[0])(list(result))
 
+
 def _asarray_tuplesafe(values, dtype=None):
     from pandas.core.index import Index
 
@@ -707,6 +755,7 @@ def _asarray_tuplesafe(values, dtype=None):
 
     return result
 
+
 def _index_labels_to_array(labels):
     if isinstance(labels, (basestring, tuple)):
         labels = [labels]
@@ -714,31 +763,37 @@ def _index_labels_to_array(labels):
     if not isinstance(labels, (list, np.ndarray)):
         try:
             labels = list(labels)
-        except TypeError: # non-iterable
+        except TypeError:  # non-iterable
             labels = [labels]
 
     labels = _asarray_tuplesafe(labels)
 
     return labels
 
+
 def _maybe_make_list(obj):
     if obj is not None and not isinstance(obj, (tuple, list)):
         return [obj]
     return obj
 
+
 def is_integer(obj):
     return isinstance(obj, (int, long, np.integer))
 
+
 def is_float(obj):
     return isinstance(obj, (float, np.floating))
 
+
 def is_iterator(obj):
     # python 3 generators have __next__ instead of next
     return hasattr(obj, 'next') or hasattr(obj, '__next__')
 
+
 def is_number(obj):
     return isinstance(obj, (np.number, int, long, float))
 
+
 def is_integer_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
         tipo = arr_or_dtype.type
@@ -747,6 +802,7 @@ def is_integer_dtype(arr_or_dtype):
     return (issubclass(tipo, np.integer) and not
             issubclass(tipo, np.datetime64))
 
+
 def is_datetime64_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
         tipo = arr_or_dtype.type
@@ -754,6 +810,7 @@ def is_datetime64_dtype(arr_or_dtype):
         tipo = arr_or_dtype.dtype.type
     return issubclass(tipo, np.datetime64)
 
+
 def is_float_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
         tipo = arr_or_dtype.type
@@ -761,9 +818,11 @@ def is_float_dtype(arr_or_dtype):
         tipo = arr_or_dtype.dtype.type
     return issubclass(tipo, np.floating)
 
+
 def is_list_like(arg):
     return hasattr(arg, '__iter__') and not isinstance(arg, basestring)
 
+
 def _is_sequence(x):
     try:
         iter(x)
@@ -797,6 +856,7 @@ def _astype_nansafe(arr, dtype):
 
     return arr.astype(dtype)
 
+
 def _clean_fill_method(method):
     method = method.lower()
     if method == 'ffill':
@@ -804,11 +864,12 @@ def _clean_fill_method(method):
     if method == 'bfill':
         method = 'backfill'
     if method not in ['pad', 'backfill']:
-        msg = ('Invalid fill method. Expecting pad (ffill) or backfill (bfill).'
-               ' Got %s' % method)
+        msg = ('Invalid fill method. Expecting pad (ffill) or backfill '
+               '(bfill). Got %s' % method)
         raise ValueError(msg)
     return method
 
+
 def _all_none(*args):
     for arg in args:
         if arg is not None:
@@ -853,6 +914,7 @@ def load(path):
     finally:
         f.close()
 
+
 class UTF8Recoder:
     """
     Iterator that reads an encoded stream and reencodes the input to UTF-8
@@ -866,6 +928,7 @@ class UTF8Recoder:
     def next(self):
         return self.reader.next().encode("utf-8")
 
+
 def _get_handle(path, mode, encoding=None):
     if py3compat.PY3:  # pragma: no cover
         if encoding:
@@ -916,11 +979,11 @@ else:
             self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
             self.stream = f
             self.encoder = codecs.getincrementalencoder(encoding)()
-            self.quoting=kwds.get("quoting",None)
+            self.quoting = kwds.get("quoting", None)
 
         def writerow(self, row):
             def _check_as_is(x):
-                return (self.quoting == csv.QUOTE_NONNUMERIC and \
+                return (self.quoting == csv.QUOTE_NONNUMERIC and
                         is_number(x)) or isinstance(x, str)
 
             row = [x if _check_as_is(x)
@@ -940,6 +1003,7 @@ else:
 
 _NS_DTYPE = np.dtype('M8[ns]')
 
+
 def _concat_compat(to_concat, axis=0):
     # filter empty arrays
     to_concat = [x for x in to_concat if x.shape[axis] > 0]
@@ -955,8 +1019,8 @@ def _concat_compat(to_concat, axis=0):
 # Unicode consolidation
 # ---------------------
 #
-# pprinting utility functions for generating Unicode text or bytes(3.x)/str(2.x)
-# representations of objects.
+# pprinting utility functions for generating Unicode text or
+# bytes(3.x)/str(2.x) representations of objects.
 # Try to use these as much as possible rather then rolling your own.
 #
 # When to use
@@ -973,21 +1037,24 @@ def _concat_compat(to_concat, axis=0):
 #    console_encode() should (hopefully) choose the right encoding for you
 #    based on the encoding set in fmt.print_config.encoding.
 #
-# 3) if you need to write something out to file, use pprint_thing_encoded(encoding).
+# 3) if you need to write something out to file, use
+#    pprint_thing_encoded(encoding).
 #
-#    If no encoding is specified, it defaults to utf-8. SInce encoding pure ascii with
-#    utf-8 is a no-op you can safely use the default utf-8 if you're working with
-#    straight ascii.
+#    If no encoding is specified, it defaults to utf-8. Since encoding pure
+#    ascii with utf-8 is a no-op you can safely use the default utf-8 if you're
+#    working with straight ascii.
 
-def _pprint_seq(seq,_nest_lvl=0):
+
+def _pprint_seq(seq, _nest_lvl=0):
     """
     internal. pprinter for iterables. you should probably use pprint_thing()
     rather then calling this directly.
     """
-    fmt=u"[%s]" if hasattr(seq,'__setitem__') else u"(%s)"
-    return fmt % ", ".join(pprint_thing(e,_nest_lvl+1) for e in seq)
+    fmt = u"[%s]" if hasattr(seq, '__setitem__') else u"(%s)"
+    return fmt % ", ".join(pprint_thing(e, _nest_lvl + 1) for e in seq)
+
 
-def pprint_thing(thing,_nest_lvl=0):
+def pprint_thing(thing, _nest_lvl=0):
     """
     This function is the sanctioned way of converting objects
     to a unicode representation.
@@ -1011,7 +1078,7 @@ def pprint_thing(thing,_nest_lvl=0):
     if thing is None:
         result = ''
     elif _is_sequence(thing) and _nest_lvl < print_config.pprint_nest_depth:
-        result = _pprint_seq(thing,_nest_lvl)
+        result = _pprint_seq(thing, _nest_lvl)
     else:
         # when used internally in the package, everything
         # passed in should be a unicode object or have a unicode
@@ -1021,17 +1088,19 @@ def pprint_thing(thing,_nest_lvl=0):
         # so we resort to utf-8 with replacing errors
 
         try:
-            result = unicode(thing) # we should try this first
+            result = unicode(thing)  # we should try this first
         except UnicodeDecodeError:
             # either utf-8 or we replace errors
-            result = str(thing).decode('utf-8',"replace")
+            result = str(thing).decode('utf-8', "replace")
 
-    return unicode(result) # always unicode
+    return unicode(result)  # always unicode
 
-def pprint_thing_encoded(object,encoding='utf-8',errors='replace'):
-    value=pprint_thing(object) # get unicode representation of object
+
+def pprint_thing_encoded(object, encoding='utf-8', errors='replace'):
+    value = pprint_thing(object)  # get unicode representation of object
     return value.encode(encoding, errors)
 
+
 def console_encode(object):
     from pandas.core.format import print_config
     """
@@ -1041,4 +1110,4 @@ def console_encode(object):
     set in print_config.encoding. Use this everywhere
     where you output to the console.
     """
-    return pprint_thing_encoded(object,print_config.encoding)
+    return pprint_thing_encoded(object, print_config.encoding)
diff --git a/pandas/core/daterange.py b/pandas/core/daterange.py
index 4bf6ee5a1..bfed7fcc6 100644
--- a/pandas/core/daterange.py
+++ b/pandas/core/daterange.py
@@ -37,7 +37,7 @@ class DateRange(Index):
         # for backwards compatibility
         if len(aug_state) > 2:
             tzinfo = aug_state[2]
-        else: # pragma: no cover
+        else:  # pragma: no cover
             tzinfo = None
 
         self.offset = offset
diff --git a/pandas/core/format.py b/pandas/core/format.py
index 3bc379220..215287696 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -56,6 +56,7 @@ docstring_to_string = """
     -------
     formatted : string (or unicode, depending on data and options)"""
 
+
 class SeriesFormatter(object):
 
     def __init__(self, series, buf=None, header=True, length=True,
@@ -152,6 +153,7 @@ else:
         except UnicodeError:
             return len(x)
 
+
 class DataFrameFormatter(object):
     """
     Render a DataFrame
@@ -243,9 +245,9 @@ class DataFrameFormatter(object):
                     return x.decode('utf-8')
                 strcols = map(lambda col: map(make_unicode, col), strcols)
             else:
-                # generally everything is plain strings, which has ascii
-                # encoding.  problem is when there is a char with value over 127
-                # - everything then gets converted to unicode.
+                # Generally everything is plain strings, which has ascii
+                # encoding.  Problem is when there is a char with value over
+                # 127. Everything then gets converted to unicode.
                 try:
                     map(lambda col: map(str, col), strcols)
                 except UnicodeError:
@@ -299,7 +301,7 @@ class DataFrameFormatter(object):
         nlevels = frame.index.nlevels
         for i, row in enumerate(izip(*strcols)):
             if i == nlevels:
-                self.buf.write('\\hline\n') # End of header
+                self.buf.write('\\hline\n')  # End of header
             crow = [(x.replace('_', '\\_')
                      .replace('%', '\\%')
                      .replace('&', '\\&') if x else '{}') for x in row]
@@ -424,6 +426,7 @@ class HTMLFormatter(object):
 
         _bold_row = self.fmt.kwds.get('bold_rows', False)
         _temp = '<strong>%s</strong>'
+
         def _maybe_bold_row(x):
             if _bold_row:
                 return ([_temp % y for y in x] if isinstance(x, tuple)
@@ -432,7 +435,6 @@ class HTMLFormatter(object):
                 return x
         self._maybe_bold_row = _maybe_bold_row
 
-
     def write(self, s, indent=0):
         self.elements.append(' ' * indent + _str(s))
 
@@ -474,7 +476,7 @@ class HTMLFormatter(object):
         indent = 0
         frame = self.frame
 
-        _classes = ['dataframe'] # Default class.
+        _classes = ['dataframe']  # Default class.
         if self.classes is not None:
             if isinstance(self.classes, str):
                 self.classes = self.classes.split()
@@ -485,12 +487,12 @@ class HTMLFormatter(object):
                    indent)
 
         if len(frame.columns) == 0 or len(frame.index) == 0:
-            self.write('<tbody>', indent  + self.indent_delta)
+            self.write('<tbody>', indent + self.indent_delta)
             self.write_tr([repr(frame.index),
                            'Empty %s' % type(frame).__name__],
                           indent + (2 * self.indent_delta),
                           self.indent_delta)
-            self.write('</tbody>', indent  + self.indent_delta)
+            self.write('</tbody>', indent + self.indent_delta)
         else:
             indent += self.indent_delta
             indent = self._write_header(indent)
@@ -535,10 +537,10 @@ class HTMLFormatter(object):
 
             levels = self.columns.format(sparsify=True, adjoin=False,
                                          names=False)
-            col_values = self.columns.values
             level_lengths = _get_level_lengths(levels)
 
-            for lnum, (records, values) in enumerate(zip(level_lengths, levels)):
+            for lnum, (records, values) in enumerate(
+                    zip(level_lengths, levels)):
                 name = self.columns.names[lnum]
                 row = ['' if name is None else str(name)]
 
@@ -659,6 +661,7 @@ def _get_level_lengths(levels):
 
     def _make_grouper():
         record = {'count': 0}
+
         def grouper(x):
             if x != '':
                 record['count'] += 1
@@ -771,6 +774,7 @@ class GenericArrayFormatter(object):
 
         return fmt_values
 
+
 class FloatArrayFormatter(GenericArrayFormatter):
     """
 
@@ -797,7 +801,7 @@ class FloatArrayFormatter(GenericArrayFormatter):
             if len(fmt_values) > 0:
                 maxlen = max(len(x) for x in fmt_values)
             else:
-                maxlen =0
+                maxlen = 0
 
             too_long = maxlen > self.digits + 5
 
@@ -805,7 +809,7 @@ class FloatArrayFormatter(GenericArrayFormatter):
 
             # this is pretty arbitrary for now
             has_large_values = (abs_vals > 1e8).any()
-            has_small_values = ((abs_vals < 10**(-self.digits)) &
+            has_small_values = ((abs_vals < 10 ** (-self.digits)) &
                                 (abs_vals > 0)).any()
 
             if too_long and has_large_values:
@@ -842,6 +846,7 @@ class Datetime64Formatter(GenericArrayFormatter):
         fmt_values = [formatter(x) for x in self.values]
         return _make_fixed_width(fmt_values, self.justify)
 
+
 def _format_datetime64(x, tz=None):
     if isnull(x):
         return 'NaT'
@@ -882,6 +887,7 @@ def _make_fixed_width(strings, justify='right', minimum=None):
 
     return [just(x) for x in strings]
 
+
 def _trim_zeros(str_floats, na_rep='NaN'):
     """
     Trims zeros and decimal points
@@ -912,6 +918,7 @@ def single_column_table(column, align=None, style=None):
     table += '</tbody></table>'
     return table
 
+
 def single_row_table(row):  # pragma: no cover
     table = '<table><tbody><tr>'
     for i in row:
@@ -919,6 +926,7 @@ def single_row_table(row):  # pragma: no cover
     table += '</tr></tbody></table>'
     return table
 
+
 def _has_names(index):
     if isinstance(index, MultiIndex):
         return any([x is not None for x in index.names])
@@ -926,10 +934,10 @@ def _has_names(index):
         return index.name is not None
 
 
-
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Global formatting options
 
+
 def set_printoptions(precision=None, column_space=None, max_rows=None,
                      max_columns=None, colheader_justify=None,
                      max_colwidth=None, notebook_repr_html=None,
@@ -985,9 +993,11 @@ def set_printoptions(precision=None, column_space=None, max_rows=None,
     if encoding is not None:
         print_config.encoding = encoding
 
+
 def reset_printoptions():
     print_config.reset()
 
+
 class EngFormatter(object):
     """
     Formats float values according to engineering format.
@@ -1052,7 +1062,7 @@ class EngFormatter(object):
             dnum = -dnum
 
         if dnum != 0:
-            pow10 = decimal.Decimal(int(math.floor(dnum.log10()/3)*3))
+            pow10 = decimal.Decimal(int(math.floor(dnum.log10() / 3) * 3))
         else:
             pow10 = decimal.Decimal(0)
 
@@ -1068,16 +1078,17 @@ class EngFormatter(object):
             else:
                 prefix = 'E+%02d' % int_pow10
 
-        mant = sign*dnum/(10**pow10)
+        mant = sign * dnum / (10 ** pow10)
 
         if self.accuracy is None:  # pragma: no cover
             format_str = u"% g%s"
         else:
-            format_str = (u"%% .%if%%s" % self.accuracy )
+            format_str = (u"%% .%if%%s" % self.accuracy)
 
         formatted = format_str % (mant, prefix)
 
-        return formatted #.strip()
+        return formatted  #.strip()
+
 
 def set_eng_float_format(precision=None, accuracy=3, use_eng_prefix=False):
     """
@@ -1087,10 +1098,10 @@ def set_eng_float_format(precision=None, accuracy=3, use_eng_prefix=False):
 
     See also EngFormatter.
     """
-    if precision is not None: # pragma: no cover
+    if precision is not None:  # pragma: no cover
         import warnings
         warnings.warn("'precision' parameter in set_eng_float_format is "
-                      "being renamed to 'accuracy'" , FutureWarning)
+                      "being renamed to 'accuracy'", FutureWarning)
         accuracy = precision
 
     print_config.float_format = EngFormatter(accuracy, use_eng_prefix)
@@ -1126,17 +1137,17 @@ class _GlobalPrintConfig(object):
 
         encoding = None
         try:
-            encoding=sys.stdin.encoding
+            encoding = sys.stdin.encoding
         except AttributeError:
             pass
 
-        if not encoding or encoding =='ascii': # try again for something better
+        if not encoding or encoding == 'ascii':  # try again for better
             try:
                 encoding = locale.getpreferredencoding()
             except Exception:
                 pass
 
-        if not encoding: # when all else fails. this will usually be "ascii"
+        if not encoding:  # when all else fails. this will usually be "ascii"
                 encoding = sys.getdefaultencoding()
 
         return encoding
@@ -1162,7 +1173,7 @@ if __name__ == '__main__':
                       599502.4276,   620921.8593,   620898.5294,   552427.1093,
                       555221.2193,   519639.7059,   388175.7   ,   379199.5854,
                       614898.25  ,   504833.3333,   560600.    ,   941214.2857,
-                      1134250.    ,  1219550.    ,   855736.85  ,  1042615.4286,
+                      1134250.    ,  1219550.    ,   855736.85 ,  1042615.4286,
                       722621.3043,   698167.1818,   803750.    ])
     fmt = FloatArrayFormatter(arr, digits=7)
     print fmt.get_result()
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index c18010ab9..06a290b6e 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -24,8 +24,8 @@ from numpy import nan
 import numpy as np
 import numpy.ma as ma
 
-from pandas.core.common import (isnull, notnull, PandasError, _try_sort,\
-                                _default_index,_is_sequence)
+from pandas.core.common import (isnull, notnull, PandasError, _try_sort,
+                                _default_index, _is_sequence)
 from pandas.core.generic import NDFrame
 from pandas.core.index import Index, MultiIndex, _ensure_index
 from pandas.core.indexing import _NDFrameIndexer, _maybe_droplevels
@@ -170,11 +170,14 @@ merged : DataFrame
 
 # Custom error class for update
 
-class DataConflictError(Exception): pass
+
+class DataConflictError(Exception):
+    pass
 
 #----------------------------------------------------------------------
 # Factory helper methods
 
+
 def _arith_method(op, name, default_axis='columns'):
     def na_op(x, y):
         try:
@@ -228,6 +231,7 @@ def _arith_method(op, name, default_axis='columns'):
 
     return f
 
+
 def _flex_comp_method(op, name, default_axis='columns'):
 
     def na_op(x, y):
@@ -733,7 +737,9 @@ class DataFrame(NDFrame):
         lvals = left.values
         rvals = right.values
         if isinstance(other, DataFrame):
-            return DataFrame(np.dot(lvals, rvals), index=self.index, columns=other.columns)
+            return DataFrame(np.dot(lvals, rvals),
+                             index=self.index,
+                             columns=other.columns)
         elif isinstance(other, Series):
             return Series(np.dot(lvals, rvals), index=left.index)
         else:
@@ -798,8 +804,8 @@ class DataFrame(NDFrame):
         elif outtype.lower().startswith('l'):
             return dict((k, v.tolist()) for k, v in self.iteritems())
         elif outtype.lower().startswith('s'):
-            return dict((k, v) for k,v in self.iteritems())
-        else: # pragma: no cover
+            return dict((k, v) for k, v in self.iteritems())
+        else:  # pragma: no cover
             raise ValueError("outtype %s not understood" % outtype)
 
     @classmethod
@@ -833,7 +839,8 @@ class DataFrame(NDFrame):
             columns = list(columns)
 
             if len(algos.unique(columns)) < len(columns):
-                raise ValueError('Non-unique columns not yet supported in from_records')
+                raise ValueError('Non-unique columns not yet supported in '
+                                 'from_records')
 
         if names is not None:  # pragma: no cover
             columns = names
@@ -1166,7 +1173,6 @@ class DataFrame(NDFrame):
             f = com._get_handle(path_or_buf, mode, encoding=encoding)
             close = True
 
-
         if quoting is None:
             quoting = csv.QUOTE_MINIMAL
 
@@ -2019,7 +2025,7 @@ class DataFrame(NDFrame):
             new_values = self._data.fast_2d_xs(loc, copy=copy)
             return Series(new_values, index=self.columns,
                           name=self.index[loc])
-        else: # isinstance(loc, slice) or loc.dtype == np.bool_:
+        else:  # isinstance(loc, slice) or loc.dtype == np.bool_:
             result = self[loc]
             result.index = new_index
             return result
@@ -2488,15 +2494,15 @@ class DataFrame(NDFrame):
             labels are inserted into. By default it is inserted into the first
             level.
         col_fill : object, default ''
-            If the columns have multiple levels, determines how the other levels
-            are named. If None then the index name is repeated.
+            If the columns have multiple levels, determines how the other
+            levels are named. If None then the index name is repeated.
 
         Returns
         -------
         resetted : DataFrame
         """
         if inplace:
-            new_obj  = self
+            new_obj = self
         else:
             new_obj = self.copy()
 
@@ -2786,7 +2792,7 @@ class DataFrame(NDFrame):
         -------
         sorted : DataFrame
         """
-        if column is not None: # pragma: no cover
+        if column is not None:  # pragma: no cover
             import warnings
             warnings.warn("column is deprecated, use columns", FutureWarning)
             columns = column
@@ -3048,7 +3054,7 @@ class DataFrame(NDFrame):
                 return self
 
             if isinstance(to_replace, dict):
-                if isinstance(value, dict): # {'A' : np.nan} -> {'A' : 0}
+                if isinstance(value, dict):  # {'A' : np.nan} -> {'A' : 0}
                     return self._replace_both_dict(to_replace, value, inplace)
 
                 elif not isinstance(value, (list, np.ndarray)):
@@ -3067,7 +3073,7 @@ class DataFrame(NDFrame):
                     new_data = self._data if inplace else self.copy()._data
                     new_data._replace_list(to_replace, value)
 
-                else: # [np.nan, ''] -> 0
+                else:  # [np.nan, ''] -> 0
                     new_data = self._data.replace(to_replace, value,
                                                   inplace=inplace)
 
@@ -3077,9 +3083,9 @@ class DataFrame(NDFrame):
                 else:
                     return self._constructor(new_data)
             else:
-                if isinstance(value, dict): # np.nan -> {'A' : 0, 'B' : -1}
+                if isinstance(value, dict):  # np.nan -> {'A' : 0, 'B' : -1}
                     return self._replace_dest_dict(to_replace, value, inplace)
-                elif not isinstance(value, (list, np.ndarray)): # np.nan -> 0
+                elif not isinstance(value, (list, np.ndarray)):  # np.nan -> 0
                     new_data = self._data.replace(to_replace, value,
                                                   inplace=inplace)
                     if inplace:
@@ -3089,7 +3095,7 @@ class DataFrame(NDFrame):
                         return self._constructor(new_data)
 
             raise ValueError('Invalid to_replace type: %s' %
-                             type(to_replace)) # pragma: no cover
+                             type(to_replace))  # pragma: no cover
 
     def _interpolate(self, to_replace, method, axis, inplace, limit):
         if self._is_mixed_type and axis == 1:
@@ -3833,7 +3839,7 @@ class DataFrame(NDFrame):
                     if hasattr(e, 'args'):
                         k = res_index[i]
                         e.args = e.args + ('occurred at index %s' % str(k),)
-                except NameError: # pragma: no cover
+                except NameError:  # pragma: no cover
                     # no k defined yet
                     pass
                 raise
@@ -4076,7 +4082,7 @@ class DataFrame(NDFrame):
             correl = np.empty((K, K), dtype=float)
             mask = np.isfinite(mat)
             for i, ac in enumerate(mat):
-                for j, bc  in enumerate(mat):
+                for j, bc in enumerate(mat):
                     valid = mask[i] & mask[j]
                     if not valid.any():
                         c = np.nan
@@ -4183,7 +4189,7 @@ class DataFrame(NDFrame):
                                   for k, v in self.iteritems()),
                                   columns=self.columns)
 
-        lb = .5 * (1. - percentile_width/100.)
+        lb = .5 * (1. - percentile_width / 100.)
         ub = 1. - lb
 
         def pretty_name(x):
@@ -4448,7 +4454,6 @@ class DataFrame(NDFrame):
         return self._reduce(nanops.nanskew, axis=axis, skipna=skipna,
                             numeric_only=None)
 
-
     @Substitution(name='unbiased kurtosis', shortname='kurt',
                   na_action=_doc_exclude_na, extras='')
     @Appender(_stat_doc)
@@ -4971,6 +4976,7 @@ def _to_sdict(data, columns, coerce_float=False):
         data = map(tuple, data)
         return _list_to_sdict(data, columns, coerce_float=coerce_float)
 
+
 def _list_to_sdict(data, columns, coerce_float=False):
     if len(data) > 0 and isinstance(data[0], tuple):
         content = list(lib.to_object_array_tuples(data).T)
@@ -4984,6 +4990,7 @@ def _list_to_sdict(data, columns, coerce_float=False):
     return _convert_object_array(content, columns,
                                  coerce_float=coerce_float)
 
+
 def _list_of_series_to_sdict(data, columns, coerce_float=False):
     from pandas.core.index import _get_combined_index
 
@@ -5038,6 +5045,7 @@ def _convert_object_array(content, columns, coerce_float=False):
                  for c, vals in zip(columns, content))
     return sdict, columns
 
+
 def _get_names_from_index(data):
     index = range(len(data))
     has_some_name = any([s.name is not None for s in data])
@@ -5055,6 +5063,7 @@ def _get_names_from_index(data):
 
     return index
 
+
 def _homogenize(data, index, columns, dtype=None):
     from pandas.core.series import _sanitize_array
 
@@ -5109,6 +5118,7 @@ def _homogenize(data, index, columns, dtype=None):
 def _put_str(s, space):
     return ('%s' % s)[:space].ljust(space)
 
+
 def install_ipython_completers():  # pragma: no cover
     """Register the DataFrame type with IPython's tab completion machinery, so
     that it knows about accessing column names as attributes."""
@@ -5136,6 +5146,7 @@ import pandas.tools.plotting as gfx
 DataFrame.plot = gfx.plot_frame
 DataFrame.hist = gfx.hist_frame
 
+
 def boxplot(self, column=None, by=None, ax=None, fontsize=None,
             rot=0, grid=True, **kwds):
     """
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index f0c70522d..97b10e532 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -8,6 +8,7 @@ from pandas.tseries.index import DatetimeIndex
 from pandas.tseries.offsets import DateOffset
 import pandas.core.common as com
 
+
 class PandasError(Exception):
     pass
 
@@ -15,8 +16,8 @@ class PandasError(Exception):
 class PandasObject(object):
 
     _AXIS_NUMBERS = {
-        'index' : 0,
-        'columns' : 1
+        'index': 0,
+        'columns': 1
     }
 
     _AXIS_ALIASES = {}
@@ -274,7 +275,7 @@ class PandasObject(object):
         else:
             new_axis = axis
 
-        return self.reindex(**{axis_name : new_axis})
+        return self.reindex(**{axis_name: new_axis})
 
     def drop(self, labels, axis=0, level=None):
         """
@@ -300,7 +301,7 @@ class PandasObject(object):
         else:
             new_axis = axis.drop(labels)
 
-        return self.reindex(**{axis_name : new_axis})
+        return self.reindex(**{axis_name: new_axis})
 
     def sort_index(self, axis=0, ascending=True):
         """
@@ -326,7 +327,7 @@ class PandasObject(object):
             sort_index = sort_index[::-1]
 
         new_axis = labels.take(sort_index)
-        return self.reindex(**{axis_name : new_axis})
+        return self.reindex(**{axis_name: new_axis})
 
     @property
     def ix(self):
@@ -483,13 +484,13 @@ class NDFrame(PandasObject):
         self._item_cache.clear()
 
     def _set_item(self, key, value):
-        if hasattr(self,'columns') and isinstance(self.columns, MultiIndex):
+        if hasattr(self, 'columns') and isinstance(self.columns, MultiIndex):
             # Pad the key with empty strings if lower levels of the key
             # aren't specified:
             if not isinstance(key, tuple):
                 key = (key,)
             if len(key) != self.columns.nlevels:
-                key += ('',)*(self.columns.nlevels - len(key))
+                key += ('',) * (self.columns.nlevels - len(key))
         self._data.set(key, value)
 
         try:
@@ -504,7 +505,7 @@ class NDFrame(PandasObject):
         deleted = False
 
         maybe_shortcut = False
-        if hasattr(self,'columns') and isinstance(self.columns, MultiIndex):
+        if hasattr(self, 'columns') and isinstance(self.columns, MultiIndex):
             try:
                 maybe_shortcut = key not in self.columns._engine
             except TypeError:
@@ -513,10 +514,10 @@ class NDFrame(PandasObject):
         if maybe_shortcut:
             # Allow shorthand to delete all columns whose first len(key)
             # elements match key:
-            if not isinstance(key,tuple):
+            if not isinstance(key, tuple):
                 key = (key,)
             for col in self.columns:
-                if isinstance(col,tuple) and col[:len(key)] == key:
+                if isinstance(col, tuple) and col[:len(key)] == key:
                     del self[col]
                     deleted = True
         if not deleted:
@@ -702,7 +703,7 @@ class NDFrame(PandasObject):
             if skipna:
                 np.putmask(result, mask, np.nan)
         else:
-            result = np.maximum.accumulate(y,axis)
+            result = np.maximum.accumulate(y, axis)
         return self._wrap_array(result, self.axes, copy=False)
 
     def cummin(self, axis=None, skipna=True):
@@ -738,7 +739,7 @@ class NDFrame(PandasObject):
             if skipna:
                 np.putmask(result, mask, np.nan)
         else:
-            result = np.minimum.accumulate(y,axis)
+            result = np.minimum.accumulate(y, axis)
         return self._wrap_array(result, self.axes, copy=False)
 
     def copy(self, deep=True):
@@ -934,6 +935,7 @@ class NDFrame(PandasObject):
 
 # Good for either Series or DataFrame
 
+
 def truncate(self, before=None, after=None, copy=True):
     """Function truncate a sorted DataFrame / Series before and/or after
     some particular dates.
@@ -965,4 +967,3 @@ def truncate(self, before=None, after=None, copy=True):
         result = result.copy()
 
     return result
-
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index f0f6f7b2a..e158e164e 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -18,12 +18,15 @@ import pandas.lib as lib
 class GroupByError(Exception):
     pass
 
+
 class DataError(GroupByError):
     pass
 
+
 class SpecificationError(GroupByError):
     pass
 
+
 def _groupby_function(name, alias, npfunc, numeric_only=True):
     def f(self):
         try:
@@ -36,6 +39,7 @@ def _groupby_function(name, alias, npfunc, numeric_only=True):
 
     return f
 
+
 def _first_compat(x, axis=0):
     x = np.asarray(x)
     x = x[com.notnull(x)]
@@ -43,6 +47,7 @@ def _first_compat(x, axis=0):
         return np.nan
     return x[0]
 
+
 def _last_compat(x, axis=0):
     x = np.asarray(x)
     x = x[com.notnull(x)]
@@ -166,7 +171,7 @@ class GroupBy(object):
     @property
     def name(self):
         if self._selection is None:
-            return None # 'result'
+            return None  # 'result'
         else:
             return self._selection
 
@@ -205,6 +210,7 @@ class GroupBy(object):
 
             def curried_with_axis(x):
                 return f(x, *args, **kwargs_with_axis)
+
             def curried(x):
                 return f(x, *args, **kwargs)
 
@@ -458,6 +464,7 @@ class GroupBy(object):
 
         return result
 
+
 def _generate_groups(obj, group_index, ngroups, axis=0):
     if isinstance(obj, NDFrame) and not isinstance(obj, DataFrame):
         factory = obj._constructor
@@ -468,23 +475,26 @@ def _generate_groups(obj, group_index, ngroups, axis=0):
     return generate_groups(obj, group_index, ngroups,
                            axis=axis, factory=factory)
 
+
 @Appender(GroupBy.__doc__)
 def groupby(obj, by, **kwds):
     if isinstance(obj, Series):
         klass = SeriesGroupBy
     elif isinstance(obj, DataFrame):
         klass = DataFrameGroupBy
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise TypeError('invalid type: %s' % type(obj))
 
     return klass(obj, by, **kwds)
 
+
 def _get_axes(group):
     if isinstance(group, Series):
         return [group.index]
     else:
         return group.axes
 
+
 def _is_indexed_like(obj, axes):
     if isinstance(obj, Series):
         if len(axes) > 1:
@@ -495,6 +505,7 @@ def _is_indexed_like(obj, axes):
 
     return False
 
+
 class Grouper(object):
     """
 
@@ -531,7 +542,7 @@ class Grouper(object):
             groups = indices.keys()
             try:
                 groups = sorted(groups)
-            except Exception: # pragma: no cover
+            except Exception:  # pragma: no cover
                 pass
 
             for name in groups:
@@ -658,29 +669,29 @@ class Grouper(object):
     # Aggregation functions
 
     _cython_functions = {
-        'add' : lib.group_add,
-        'prod' : lib.group_prod,
-        'min' : lib.group_min,
-        'max' : lib.group_max,
-        'mean' : lib.group_mean,
-        'median' : lib.group_median,
-        'var' : lib.group_var,
-        'std' : lib.group_var,
+        'add': lib.group_add,
+        'prod': lib.group_prod,
+        'min': lib.group_min,
+        'max': lib.group_max,
+        'mean': lib.group_mean,
+        'median': lib.group_median,
+        'var': lib.group_var,
+        'std': lib.group_var,
         'first': lambda a, b, c, d: lib.group_nth(a, b, c, d, 1),
         'last': lib.group_last
     }
 
     _cython_object_functions = {
-        'first' : lambda a, b, c, d: lib.group_nth_object(a, b, c, d, 1),
-        'last' : lib.group_last_object
+        'first': lambda a, b, c, d: lib.group_nth_object(a, b, c, d, 1),
+        'last': lib.group_last_object
     }
 
     _cython_transforms = {
-        'std' : np.sqrt
+        'std': np.sqrt
     }
 
     _cython_arity = {
-        'ohlc' : 4, # OHLC
+        'ohlc': 4,  # OHLC
     }
 
     _name_functions = {}
@@ -840,18 +851,17 @@ def generate_bins_generic(values, binner, closed):
     if values[0] < binner[0]:
         raise ValueError("Values falls before first bin")
 
-    if values[lenidx-1] > binner[lenbin-1]:
+    if values[lenidx - 1] > binner[lenbin - 1]:
         raise ValueError("Values falls after last bin")
 
-    bins   = np.empty(lenbin - 1, dtype=np.int64)
+    bins = np.empty(lenbin - 1, dtype=np.int64)
 
-    j  = 0 # index into values
-    bc = 0 # bin count
+    j = 0  # index into values
+    bc = 0  # bin count
 
-    # linear scan, presume nothing about values/binner except that it
-    # fits ok
-    for i in range(0, lenbin-1):
-        r_bin = binner[i+1]
+    # linear scan, presume nothing about values/binner except that it fits ok
+    for i in range(0, lenbin - 1):
+        r_bin = binner[i + 1]
 
         # count values in current bin, advance to next bin
         while j < lenidx and (values[j] < r_bin or
@@ -921,25 +931,25 @@ class BinGrouper(Grouper):
     # cython aggregation
 
     _cython_functions = {
-        'add' : lib.group_add_bin,
-        'prod' : lib.group_prod_bin,
-        'mean' : lib.group_mean_bin,
-        'min' : lib.group_min_bin,
-        'max' : lib.group_max_bin,
-        'var' : lib.group_var_bin,
-        'std' : lib.group_var_bin,
-        'ohlc' : lib.group_ohlc,
+        'add': lib.group_add_bin,
+        'prod': lib.group_prod_bin,
+        'mean': lib.group_mean_bin,
+        'min': lib.group_min_bin,
+        'max': lib.group_max_bin,
+        'var': lib.group_var_bin,
+        'std': lib.group_var_bin,
+        'ohlc': lib.group_ohlc,
         'first': lambda a, b, c, d: lib.group_nth_bin(a, b, c, d, 1),
         'last': lib.group_last_bin
     }
 
     _cython_object_functions = {
-        'first' : lambda a, b, c, d: lib.group_nth_bin_object(a, b, c, d, 1),
-        'last' : lib.group_last_bin_object
+        'first': lambda a, b, c, d: lib.group_nth_bin_object(a, b, c, d, 1),
+        'last': lib.group_last_bin_object
     }
 
     _name_functions = {
-        'ohlc' : lambda *args: ['open', 'high', 'low', 'close']
+        'ohlc': lambda *args: ['open', 'high', 'low', 'close']
     }
 
     _filter_empty_groups = True
@@ -1105,6 +1115,7 @@ class Grouping(object):
             self._counts = counts
 
     _groups = None
+
     @property
     def groups(self):
         if self._groups is None:
@@ -1184,9 +1195,11 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True):
 
     return grouper, exclusions
 
+
 def _is_label_like(val):
     return isinstance(val, basestring) or np.isscalar(val)
 
+
 def _convert_grouper(axis, grouper):
     if isinstance(grouper, dict):
         return grouper.get
@@ -1201,6 +1214,7 @@ def _convert_grouper(axis, grouper):
     else:
         return grouper
 
+
 class SeriesGroupBy(GroupBy):
 
     def aggregate(self, func_or_funcs, *args, **kwargs):
@@ -1257,7 +1271,7 @@ class SeriesGroupBy(GroupBy):
         if isinstance(func_or_funcs, basestring):
             return getattr(self, func_or_funcs)(*args, **kwargs)
 
-        if hasattr(func_or_funcs,'__iter__'):
+        if hasattr(func_or_funcs, '__iter__'):
             ret = self._aggregate_multiple_funcs(func_or_funcs)
         else:
             cyfunc = _intercept_cython(func_or_funcs)
@@ -1393,6 +1407,7 @@ class SeriesGroupBy(GroupBy):
 
         return result
 
+
 class NDFrameGroupBy(GroupBy):
 
     def _iterate_slices(self):
@@ -1686,7 +1701,7 @@ class NDFrameGroupBy(GroupBy):
                 if (isinstance(values[0], Series) and
                     not _all_indexes_same([x.index for x in values])):
                     return self._concat_objects(keys, values,
-                                                not_indexed_same=not_indexed_same)
+                            not_indexed_same=not_indexed_same)
 
                 if self.axis == 0:
                     stacked_values = np.vstack([np.asarray(x)
@@ -1743,7 +1758,7 @@ class NDFrameGroupBy(GroupBy):
                 res = group.apply(wrapper, axis=self.axis)
             except TypeError:
                 return self._transform_item_by_item(obj, wrapper)
-            except Exception: # pragma: no cover
+            except Exception:  # pragma: no cover
                 res = wrapper(group)
 
             # broadcasting
@@ -1892,6 +1907,7 @@ class DataFrameGroupBy(NDFrameGroupBy):
 from pandas.tools.plotting import boxplot_frame_groupby
 DataFrameGroupBy.boxplot = boxplot_frame_groupby
 
+
 class PanelGroupBy(NDFrameGroupBy):
 
     def _iterate_slices(self):
@@ -2023,6 +2039,7 @@ def generate_groups(data, group_index, ngroups, axis=0, factory=lambda x: x):
         assert(start < end)
         yield i, _get_slice(slice(start, end))
 
+
 def get_group_index(label_list, shape):
     """
     For the particular label_list, gets the offsets into the hypothetical list
@@ -2036,7 +2053,7 @@ def get_group_index(label_list, shape):
     group_index = np.zeros(n, dtype=np.int64)
     mask = np.zeros(n, dtype=bool)
     for i in xrange(len(shape)):
-        stride = np.prod([x for x in shape[i+1:]], dtype=np.int64)
+        stride = np.prod([x for x in shape[i + 1:]], dtype=np.int64)
         group_index += com._ensure_int64(label_list[i]) * stride
         mask |= label_list[i] < 0
 
@@ -2051,6 +2068,7 @@ def _int64_overflow_possible(shape):
 
     return the_prod >= _INT64_MAX
 
+
 def decons_group_index(comp_labels, shape):
     # reconstruct labels
     label_list = []
@@ -2099,6 +2117,7 @@ def _lexsort_indexer(keys):
         shape.append(len(rizer.uniques))
     return _indexer_from_factorized(labels, shape)
 
+
 class _KeyMapper(object):
     """
     Ease my suffering. Map compressed group id -> key tuple
@@ -2139,6 +2158,7 @@ def _get_indices_dict(label_list, keys):
 #----------------------------------------------------------------------
 # sorting levels...cleverly?
 
+
 def _compress_group_index(group_index, sort=True):
     """
     Group_index is offsets into cartesian product of all possible labels. This
@@ -2162,6 +2182,7 @@ def _compress_group_index(group_index, sort=True):
 
     return comp_ids, obs_group_ids
 
+
 def _reorder_by_uniques(uniques, labels):
     # sorter is index where elements ought to go
     sorter = uniques.argsort()
@@ -2196,15 +2217,19 @@ _cython_table = {
     np.var: 'var'
 }
 
+
 def _intercept_function(func):
     return _func_table.get(func, func)
 
+
 def _intercept_cython(func):
     return _cython_table.get(func)
 
+
 def _groupby_indices(values):
     return lib.groupby_indices(com._ensure_object(values))
 
+
 def numpy_groupby(data, labels, axis=0):
     s = np.argsort(labels)
     keys, inv = np.unique(labels, return_inverse=True)
@@ -2222,6 +2247,7 @@ def numpy_groupby(data, labels, axis=0):
 from pandas.util import py3compat
 import sys
 
+
 def install_ipython_completers():  # pragma: no cover
     """Register the DataFrame type with IPython's tab completion machinery, so
     that it knows about accessing column names as attributes."""
@@ -2229,7 +2255,7 @@ def install_ipython_completers():  # pragma: no cover
 
     @complete_object.when_type(DataFrameGroupBy)
     def complete_dataframe(obj, prev_completions):
-        return prev_completions + [c for c in obj.obj.columns \
+        return prev_completions + [c for c in obj.obj.columns
                     if isinstance(c, basestring) and py3compat.isidentifier(c)]
 
 
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 08d1c593d..c94b3baee 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -27,7 +27,7 @@ def _indexOp(opname):
         result = func(other)
         try:
             return result.view(np.ndarray)
-        except: # pragma: no cover
+        except:  # pragma: no cover
             return result
     return wrapper
 
@@ -137,7 +137,7 @@ class Index(np.ndarray):
             prepr = com.pprint_thing(self)
         else:
             prepr = com.pprint_thing_encoded(self)
-        return  'Index(%s, dtype=%s)' % (prepr,self.dtype)
+        return 'Index(%s, dtype=%s)' % (prepr, self.dtype)
 
     def astype(self, dtype):
         return Index(self.values.astype(dtype), name=self.name,
@@ -570,7 +570,7 @@ class Index(np.ndarray):
                 # contained in
                 try:
                     result = np.sort(self.values)
-                except TypeError: # pragma: no cover
+                except TypeError:  # pragma: no cover
                     result = self.values
 
         # for subclasses
@@ -1027,7 +1027,7 @@ class Index(np.ndarray):
                 lidx = self._left_indexer_unique(ov, sv)
                 ridx = None
             elif how == 'inner':
-                join_index, lidx, ridx = self._inner_indexer(sv,ov)
+                join_index, lidx, ridx = self._inner_indexer(sv, ov)
                 join_index = self._wrap_joined_index(join_index, other)
             elif how == 'outer':
                 join_index, lidx, ridx = self._outer_indexer(sv, ov)
@@ -1229,8 +1229,6 @@ class Int64Index(Index):
         return Int64Index(joined, name=name)
 
 
-
-
 class MultiIndex(Index):
     """
     Implements multi-level, a.k.a. hierarchical, index object for pandas
@@ -1291,11 +1289,12 @@ class MultiIndex(Index):
 
     def __array_finalize__(self, obj):
         """
-        Update custom MultiIndex attributes when a new array is created by numpy,
-        e.g. when calling ndarray.view()
+        Update custom MultiIndex attributes when a new array is created by
+        numpy, e.g. when calling ndarray.view()
         """
         if not isinstance(obj, type(self)):
-            # Only relevant if this array is being created from an Index instance.
+            # Only relevant if this array is being created from an Index
+            # instance.
             return
 
         self.levels = list(getattr(obj, 'levels', []))
@@ -1345,7 +1344,7 @@ class MultiIndex(Index):
         index = values.view(MultiIndex)
         index.levels = levels
         index.labels = labels
-        index.names  = names
+        index.names = names
         index.sortorder = sortorder
         return index
 
@@ -1412,7 +1411,7 @@ class MultiIndex(Index):
         shape = [len(lev) for lev in self.levels]
         group_index = np.zeros(len(self), dtype='i8')
         for i in xrange(len(shape)):
-            stride = np.prod([x for x in shape[i+1:]], dtype='i8')
+            stride = np.prod([x for x in shape[i + 1:]], dtype='i8')
             group_index += self.labels[i] * stride
 
         if len(np.unique(group_index)) < len(group_index):
@@ -1587,7 +1586,7 @@ class MultiIndex(Index):
 
         if isinstance(tuples, np.ndarray):
             if isinstance(tuples, Index):
-               tuples = tuples.values
+                tuples = tuples.values
 
             arrays = list(lib.tuples_to_object_array(tuples).T)
         elif isinstance(tuples, list):
@@ -2430,10 +2429,12 @@ def _ensure_index(index_like):
 
     return Index(index_like)
 
+
 def _validate_join_method(method):
     if method not in ['left', 'right', 'inner', 'outer']:
         raise Exception('do not recognize join method %s' % method)
 
+
 # TODO: handle index names!
 def _get_combined_index(indexes, intersect=False):
     indexes = _get_distinct_indexes(indexes)
@@ -2507,6 +2508,7 @@ def _sanitize_and_check(indexes):
     else:
         return indexes, 'array'
 
+
 def _handle_legacy_indexes(indexes):
     from pandas.core.daterange import DateRange
     from pandas.tseries.index import DatetimeIndex
@@ -2526,6 +2528,7 @@ def _handle_legacy_indexes(indexes):
 
     return converted
 
+
 def _get_consensus_names(indexes):
     consensus_name = indexes[0].names
     for index in indexes[1:]:
@@ -2534,6 +2537,7 @@ def _get_consensus_names(indexes):
             break
     return consensus_name
 
+
 def _maybe_box(idx):
     from pandas.tseries.api import DatetimeIndex, PeriodIndex
     klasses = DatetimeIndex, PeriodIndex
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 13fa0b2af..531431c06 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -10,6 +10,7 @@ import numpy as np
 # "null slice"
 _NS = slice(None, None)
 
+
 def _is_sequence(x):
     try:
         iter(x)
@@ -18,6 +19,7 @@ def _is_sequence(x):
     except Exception:
         return False
 
+
 class IndexingError(Exception):
     pass
 
@@ -587,6 +589,7 @@ class _NDFrameIndexer(object):
 # 32-bit floating point machine epsilon
 _eps = np.finfo('f4').eps
 
+
 def _is_index_slice(obj):
     def _is_valid_index(x):
         return (com.is_integer(x) or com.is_float(x)
@@ -599,6 +602,7 @@ def _is_index_slice(obj):
 
     return not both_none and (_crit(obj.start) and _crit(obj.stop))
 
+
 def _is_int_slice(obj):
     def _is_valid_index(x):
         return com.is_integer(x)
@@ -610,6 +614,7 @@ def _is_int_slice(obj):
 
     return not both_none and (_crit(obj.start) and _crit(obj.stop))
 
+
 def _is_float_slice(obj):
     def _is_valid_index(x):
         return com.is_float(x)
diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index c0f2ba765..9a1785b95 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -12,11 +12,13 @@ try:
 except ImportError:  # pragma: no cover
     _USE_BOTTLENECK = False
 
+
 def _bottleneck_switch(bn_name, alt, zero_value=None, **kwargs):
     try:
         bn_func = getattr(bn, bn_name)
     except (AttributeError, NameError):  # pragma: no cover
         bn_func = None
+
     def f(values, axis=None, skipna=True, **kwds):
         if len(kwargs) > 0:
             for k, v in kwargs.iteritems():
@@ -46,6 +48,7 @@ def _bottleneck_switch(bn_name, alt, zero_value=None, **kwargs):
 
     return f
 
+
 def _has_infs(result):
     if isinstance(result, np.ndarray):
         if result.dtype == 'f8':
@@ -57,6 +60,7 @@ def _has_infs(result):
     else:
         return np.isinf(result) or np.isneginf(result)
 
+
 def nanany(values, axis=None, skipna=True):
     mask = isnull(values)
 
@@ -65,6 +69,7 @@ def nanany(values, axis=None, skipna=True):
         np.putmask(values, mask, False)
     return values.any(axis)
 
+
 def nanall(values, axis=None, skipna=True):
     mask = isnull(values)
 
@@ -73,6 +78,7 @@ def nanall(values, axis=None, skipna=True):
         np.putmask(values, mask, True)
     return values.all(axis)
 
+
 def _nansum(values, axis=None, skipna=True):
     mask = isnull(values)
 
@@ -85,6 +91,7 @@ def _nansum(values, axis=None, skipna=True):
 
     return the_sum
 
+
 def _nanmean(values, axis=None, skipna=True):
     mask = isnull(values)
 
@@ -104,6 +111,7 @@ def _nanmean(values, axis=None, skipna=True):
         the_mean = the_sum / count if count > 0 else np.nan
     return the_mean
 
+
 def _nanmedian(values, axis=None, skipna=True):
     def get_median(x):
         mask = notnull(x)
@@ -119,6 +127,7 @@ def _nanmedian(values, axis=None, skipna=True):
     else:
         return get_median(values)
 
+
 def _nanvar(values, axis=None, skipna=True, ddof=1):
     mask = isnull(values)
 
@@ -135,6 +144,7 @@ def _nanvar(values, axis=None, skipna=True, ddof=1):
     XX = _ensure_numeric((values ** 2).sum(axis))
     return np.fabs((XX - X ** 2 / count) / (count - ddof))
 
+
 def _nanmin(values, axis=None, skipna=True):
     mask = isnull(values)
     if skipna and not issubclass(values.dtype.type,
@@ -160,6 +170,7 @@ def _nanmin(values, axis=None, skipna=True):
 
     return _maybe_null_out(result, axis, mask)
 
+
 def _nanmax(values, axis=None, skipna=True):
     mask = isnull(values)
     if skipna and not issubclass(values.dtype.type,
@@ -186,6 +197,7 @@ def _nanmax(values, axis=None, skipna=True):
 
     return _maybe_null_out(result, axis, mask)
 
+
 def nanargmax(values, axis=None, skipna=True):
     """
     Returns -1 in the NA case
@@ -198,6 +210,7 @@ def nanargmax(values, axis=None, skipna=True):
     result = _maybe_arg_null_out(result, axis, mask, skipna)
     return result
 
+
 def nanargmin(values, axis=None, skipna=True):
     """
     Returns -1 in the NA case
@@ -217,6 +230,7 @@ nanvar = _bottleneck_switch('nanvar', _nanvar, ddof=1)
 nanmin = _bottleneck_switch('nanmin', _nanmin)
 nanmax = _bottleneck_switch('nanmax', _nanmax)
 
+
 def nanskew(values, axis=None, skipna=True):
     if not isinstance(values.dtype.type, np.floating):
         values = values.astype('f8')
@@ -249,6 +263,7 @@ def nanskew(values, axis=None, skipna=True):
             return np.nan
         return result
 
+
 def nankurt(values, axis=None, skipna=True):
     if not isinstance(values.dtype.type, np.floating):
         values = values.astype('f8')
@@ -269,8 +284,8 @@ def nankurt(values, axis=None, skipna=True):
     C = _zero_out_fperr(C)
     D = _zero_out_fperr(D)
 
-    result = (((count*count - 1.)*D / (B*B) - 3*((count-1.)**2)) /
-              ((count - 2.)*(count-3.)))
+    result = (((count * count - 1.) * D / (B * B) - 3 * ((count - 1.) ** 2)) /
+              ((count - 2.) * (count - 3.)))
     if isinstance(result, np.ndarray):
         result = np.where(B == 0, 0, result)
         result[count < 4] = np.nan
@@ -281,6 +296,7 @@ def nankurt(values, axis=None, skipna=True):
             return np.nan
         return result
 
+
 def nanprod(values, axis=None, skipna=True):
     mask = isnull(values)
     if skipna and not issubclass(values.dtype.type, np.integer):
@@ -289,6 +305,7 @@ def nanprod(values, axis=None, skipna=True):
     result = values.prod(axis)
     return _maybe_null_out(result, axis, mask)
 
+
 def _maybe_arg_null_out(result, axis, mask, skipna):
     # helper function for nanargmin/nanargmax
     if axis is None:
@@ -307,6 +324,7 @@ def _maybe_arg_null_out(result, axis, mask, skipna):
             result[na_mask] = -1
     return result
 
+
 def _get_counts(mask, axis):
     if axis is not None:
         count = (mask.shape[axis] - mask.sum(axis)).astype(float)
@@ -315,6 +333,7 @@ def _get_counts(mask, axis):
 
     return count
 
+
 def _maybe_null_out(result, axis, mask):
     if axis is not None:
         null_mask = (mask.shape[axis] - mask.sum(axis)) == 0
@@ -328,12 +347,14 @@ def _maybe_null_out(result, axis, mask):
 
     return result
 
+
 def _zero_out_fperr(arg):
     if isinstance(arg, np.ndarray):
         return np.where(np.abs(arg) < 1e-14, 0, arg)
     else:
         return 0 if np.abs(arg) < 1e-14 else arg
 
+
 def nancorr(a, b, method='pearson'):
     """
     a, b: ndarrays
@@ -351,27 +372,31 @@ def nancorr(a, b, method='pearson'):
     f = get_corr_func(method)
     return f(a, b)
 
+
 def get_corr_func(method):
     if method in ['kendall', 'spearman']:
         from scipy.stats import kendalltau, spearmanr
 
     def _pearson(a, b):
         return np.corrcoef(a, b)[0, 1]
+
     def _kendall(a, b):
         rs = kendalltau(a, b)
         if isinstance(rs, tuple):
             return rs[0]
         return rs
+
     def _spearman(a, b):
         return spearmanr(a, b)[0]
 
     _cor_methods = {
-        'pearson' : _pearson,
-        'kendall' : _kendall,
-        'spearman' : _spearman
+        'pearson': _pearson,
+        'kendall': _kendall,
+        'spearman': _spearman
     }
     return _cor_methods[method]
 
+
 def nancov(a, b):
     assert(len(a) == len(b))
 
@@ -385,6 +410,7 @@ def nancov(a, b):
 
     return np.cov(a, b)[0, 1]
 
+
 def _ensure_numeric(x):
     if isinstance(x, np.ndarray):
         if x.dtype == np.object_:
@@ -401,6 +427,7 @@ def _ensure_numeric(x):
 
 import operator
 
+
 def make_nancomp(op):
     def f(x, y):
         xmask = isnull(x)
@@ -424,6 +451,7 @@ nanle = make_nancomp(operator.le)
 naneq = make_nancomp(operator.eq)
 nanne = make_nancomp(operator.ne)
 
+
 def unique1d(values):
     """
     Hash table-based unique
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index 211434ab0..0efbb5284 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -29,7 +29,7 @@ def _ensure_like_indices(time, panels):
     """
     n_time = len(time)
     n_panel = len(panels)
-    u_panels = np.unique(panels) # this sorts!
+    u_panels = np.unique(panels)  # this sorts!
     u_time = np.unique(time)
     if len(u_time) == n_time:
         time = np.tile(u_time, len(u_panels))
@@ -37,6 +37,7 @@ def _ensure_like_indices(time, panels):
         panels = np.repeat(u_panels, len(u_time))
     return time, panels
 
+
 def panel_index(time, panels, names=['time', 'panel']):
     """
     Returns a multi-index suitable for a panel-like DataFrame
@@ -84,9 +85,11 @@ def panel_index(time, panels, names=['time', 'panel']):
     levels = [time_factor.levels, panel_factor.levels]
     return MultiIndex(levels, labels, sortorder=None, names=names)
 
+
 class PanelError(Exception):
     pass
 
+
 def _arith_method(func, name):
     # work only for scalars
 
@@ -99,6 +102,7 @@ def _arith_method(func, name):
     f.__name__ = name
     return f
 
+
 def _panel_arith_method(op, name):
     @Substitution(op)
     def f(self, other, axis='items'):
@@ -144,20 +148,20 @@ If all values are NA, result will be NA"""
 
 class Panel(NDFrame):
     _AXIS_NUMBERS = {
-        'items' : 0,
-        'major_axis' : 1,
-        'minor_axis' : 2
+        'items': 0,
+        'major_axis': 1,
+        'minor_axis': 2
     }
 
     _AXIS_ALIASES = {
-        'major' : 'major_axis',
-        'minor' : 'minor_axis'
+        'major': 'major_axis',
+        'minor': 'minor_axis'
     }
 
     _AXIS_NAMES = {
-        0 : 'items',
-        1 : 'major_axis',
-        2 : 'minor_axis'
+        0: 'items',
+        1: 'major_axis',
+        2: 'minor_axis'
     }
 
     # major
@@ -223,7 +227,7 @@ class Panel(NDFrame):
             mgr = self._init_matrix(data, passed_axes, dtype=dtype, copy=copy)
             copy = False
             dtype = None
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise PandasError('Panel constructor not properly called!')
 
         NDFrame.__init__(self, mgr, axes=axes, copy=copy, dtype=dtype)
@@ -259,7 +263,7 @@ class Panel(NDFrame):
             minor = _extract_axis(data, axis=1)
 
         axes = [items, major, minor]
-        reshaped_data = data.copy() # shallow
+        reshaped_data = data.copy()  # shallow
 
         item_shape = len(major), len(minor)
         for item in items:
@@ -364,7 +368,6 @@ class Panel(NDFrame):
         block = make_block(values, items, items)
         return BlockManager([block], fixed_axes)
 
-
     #----------------------------------------------------------------------
     # Array interface
 
@@ -561,7 +564,8 @@ class Panel(NDFrame):
             return result.set_value(item, major, minor, value)
 
     def _box_item_values(self, key, values):
-        return DataFrame(values, index=self.major_axis, columns=self.minor_axis)
+        return DataFrame(values, index=self.major_axis,
+                         columns=self.minor_axis)
 
     def __getattr__(self, name):
         """After regular attribute access, try looking up the name of an item.
@@ -617,13 +621,13 @@ class Panel(NDFrame):
         # old Panel pickle
         if isinstance(state, BlockManager):
             self._data = state
-        elif len(state) == 4: # pragma: no cover
+        elif len(state) == 4:  # pragma: no cover
             self._unpickle_panel_compat(state)
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise ValueError('unrecognized pickle')
         self._item_cache = {}
 
-    def _unpickle_panel_compat(self, state): # pragma: no cover
+    def _unpickle_panel_compat(self, state):  # pragma: no cover
         "Unpickle the panel"
         _unpickle = com._unpickle_array
         vals, items, major, minor = state
@@ -999,7 +1003,7 @@ class Panel(NDFrame):
         if i == j:
             raise ValueError('Cannot specify the same axis')
 
-        mapping = {i : j, j : i}
+        mapping = {i: j, j: i}
 
         new_axes = (self._get_axis(mapping.get(k, k))
                     for k in range(3))
@@ -1267,7 +1271,7 @@ class Panel(NDFrame):
         beg_slice, end_slice = index.slice_locs(before, after)
         new_index = index[beg_slice:end_slice]
 
-        return self.reindex(**{axis : new_index})
+        return self.reindex(**{axis: new_index})
 
     def join(self, other, how='left', lsuffix='', rsuffix=''):
         """
@@ -1303,8 +1307,8 @@ class Panel(NDFrame):
             return self._constructor(merged_data)
         else:
             if lsuffix or rsuffix:
-                raise ValueError('Suffixes not supported when passing multiple '
-                                 'panels')
+                raise ValueError('Suffixes not supported when passing '
+                                 'multiple panels')
 
             if how == 'left':
                 how = 'outer'
@@ -1364,6 +1368,7 @@ class Panel(NDFrame):
 WidePanel = Panel
 LongPanel = DataFrame
 
+
 def _prep_ndarray(values, copy=True):
     if not isinstance(values, np.ndarray):
         values = np.asarray(values)
@@ -1376,6 +1381,7 @@ def _prep_ndarray(values, copy=True):
     assert(values.ndim == 3)
     return values
 
+
 def _homogenize_dict(frames, intersect=True, dtype=None):
     """
     Conform set of DataFrame-like objects to either an intersection
@@ -1446,6 +1452,7 @@ def _extract_axis(data, axis=0, intersect=False):
 def _monotonic(arr):
     return not (arr[1:] < arr[:-1]).any()
 
+
 def install_ipython_completers():  # pragma: no cover
     """Register the Panel type with IPython's tab completion machinery, so
     that it knows about accessing column names as attributes."""
@@ -1463,4 +1470,3 @@ if "IPython" in sys.modules:  # pragma: no cover
         install_ipython_completers()
     except Exception:
         pass
-
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index d8c9087a3..044870fc4 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -74,7 +74,7 @@ class _Unstacker(object):
 
         v = self.level
         lshape = self.index.levshape
-        self.full_shape = np.prod(lshape[:v] + lshape[v+1:]), lshape[v]
+        self.full_shape = np.prod(lshape[:v] + lshape[v + 1:]), lshape[v]
 
         self._make_sorted_values_labels()
         self._make_selectors()
@@ -84,8 +84,8 @@ class _Unstacker(object):
 
         labs = self.index.labels
         levs = self.index.levels
-        to_sort =  labs[:v] + labs[v+1:] + [labs[v]]
-        sizes = [len(x) for x in levs[:v] + levs[v+1:] + [levs[v]]]
+        to_sort = labs[:v] + labs[v + 1:] + [labs[v]]
+        sizes = [len(x) for x in levs[:v] + levs[v + 1:] + [levs[v]]]
 
         group_index = get_group_index(to_sort, sizes)
         max_groups = np.prod(sizes)
@@ -93,7 +93,7 @@ class _Unstacker(object):
             comp_index, obs_ids = _compress_group_index(group_index)
             ngroups = len(obs_ids)
         else:
-            comp_index, ngroups  = group_index, max_groups
+            comp_index, ngroups = group_index, max_groups
 
         indexer = lib.groupsort_indexer(comp_index, ngroups)[0]
         indexer = _ensure_platform_int(indexer)
@@ -280,6 +280,7 @@ def _unstack_multiple(data, clocs):
 
     return unstacked
 
+
 def pivot(self, index=None, columns=None, values=None):
     """
     See DataFrame.pivot
@@ -292,6 +293,7 @@ def pivot(self, index=None, columns=None, values=None):
                          index=[self[index], self[columns]])
         return indexed.unstack(columns)
 
+
 def pivot_simple(index, columns, values):
     """
     Produce 'pivot' table based on 3 columns of this DataFrame.
@@ -324,6 +326,7 @@ def pivot_simple(index, columns, values):
     series = series.sortlevel(0)
     return series.unstack()
 
+
 def _slow_pivot(index, columns, values):
     """
     Produce 'pivot' table based on 3 columns of this DataFrame.
@@ -349,6 +352,7 @@ def _slow_pivot(index, columns, values):
 
     return DataFrame(tree)
 
+
 def unstack(obj, level):
     if isinstance(level, (tuple, list)):
         return _unstack_multiple(obj, level)
@@ -362,11 +366,12 @@ def unstack(obj, level):
         unstacker = _Unstacker(obj.values, obj.index, level=level)
         return unstacker.get_result()
 
+
 def _unstack_frame(obj, level):
     from pandas.core.internals import BlockManager, make_block
 
     if obj._is_mixed_type:
-        unstacker = _Unstacker(np.empty(obj.shape, dtype=bool), # dummy
+        unstacker = _Unstacker(np.empty(obj.shape, dtype=bool),  # dummy
                                obj.index, level=level,
                                value_columns=obj.columns)
         new_columns = unstacker.get_new_columns()
@@ -395,6 +400,7 @@ def _unstack_frame(obj, level):
                                value_columns=obj.columns)
         return unstacker.get_result()
 
+
 def stack(frame, level=-1, dropna=True):
     """
     Convert DataFrame to Series with multi-level Index. Columns become the
@@ -437,6 +443,7 @@ def stack(frame, level=-1, dropna=True):
         new_index = new_index[mask]
     return Series(new_values, index=new_index)
 
+
 def _stack_multi_columns(frame, level=-1, dropna=True):
     this = frame.copy()
 
@@ -491,7 +498,7 @@ def _stack_multi_columns(frame, level=-1, dropna=True):
     else:
         new_levels = [this.index]
         new_labels = [np.arange(N).repeat(levsize)]
-        new_names = [this.index.name] # something better?
+        new_names = [this.index.name]  # something better?
 
     new_levels.append(frame.columns.levels[level])
     new_labels.append(np.tile(np.arange(levsize), N))
@@ -704,8 +711,8 @@ def make_axis_dummies(frame, axis='minor', transform=None):
         Column names taken from chosen axis
     """
     numbers = {
-        'major' : 0,
-        'minor' : 1
+        'major': 0,
+        'minor': 1
     }
     num = numbers.get(axis, axis)
 
@@ -722,6 +729,7 @@ def make_axis_dummies(frame, axis='minor', transform=None):
 
     return DataFrame(values, columns=items, index=frame.index)
 
+
 def block2d_to_block3d(values, items, shape, major_labels, minor_labels,
                        ref_items=None):
     """
diff --git a/pandas/core/series.py b/pandas/core/series.py
index eca177c4c..7a7fc7159 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -44,6 +44,7 @@ _SHOW_WARNINGS = True
 #----------------------------------------------------------------------
 # Wrapper function for Series arithmetic methods
 
+
 def _arith_method(op, name):
     """
     Wrapper function for Series arithmetic operations, to avoid
@@ -124,7 +125,7 @@ def _comp_method(op, name):
             name = _maybe_match_name(self, other)
             return Series(na_op(self.values, other.values),
                           index=self.index, name=name)
-        elif isinstance(other, DataFrame): # pragma: no cover
+        elif isinstance(other, DataFrame):  # pragma: no cover
             return NotImplemented
         elif isinstance(other, np.ndarray):
             return Series(na_op(self.values, np.asarray(other)),
@@ -160,8 +161,8 @@ def _bool_method(op, name):
 
             if isinstance(y, np.ndarray):
                 if (x.dtype == np.bool_ and
-                    y.dtype == np.bool_): # pragma: no cover
-                    result = op(x, y) # when would this be hit?
+                    y.dtype == np.bool_):  # pragma: no cover
+                    result = op(x, y)  # when would this be hit?
                 else:
                     x = com._ensure_object(x)
                     y = com._ensure_object(y)
@@ -187,7 +188,6 @@ def _bool_method(op, name):
     return wrapper
 
 
-
 def _radd_compat(left, right):
     radd = lambda x, y: y + x
     # GH #353, NumPy 1.5.1 workaround
@@ -196,7 +196,7 @@ def _radd_compat(left, right):
     except TypeError:
         cond = (_np_version_under1p6 and
                 left.dtype == np.object_)
-        if cond: # pragma: no cover
+        if cond:  # pragma: no cover
             output = np.empty_like(left)
             output.flat[:] = [radd(x, right) for x in left.flat]
         else:
@@ -239,6 +239,7 @@ def _flex_method(op, name):
     f.__name__ = name
     return f
 
+
 def _unbox(func):
     @Appender(func.__doc__)
     def f(self, *args, **kwargs):
@@ -288,9 +289,10 @@ def _make_stat_func(nanop, name, shortname, na_action=_doc_exclude_na,
 #----------------------------------------------------------------------
 # Series class
 
+
 class Series(np.ndarray, generic.PandasObject):
     _AXIS_NUMBERS = {
-        'index' : 0
+        'index': 0
     }
 
     _AXIS_NAMES = dict((v, k) for k, v in _AXIS_NUMBERS.iteritems())
@@ -322,7 +324,8 @@ class Series(np.ndarray, generic.PandasObject):
                 elif isinstance(index, PeriodIndex):
                     data = [data.get(i, nan) for i in index]
                 else:
-                    data = lib.fast_multiget(data, index.values, default=np.nan)
+                    data = lib.fast_multiget(data, index.values,
+                                             default=np.nan)
             except TypeError:
                 data = [data.get(i, nan) for i in index]
         elif isinstance(data, types.GeneratorType):
@@ -830,7 +833,7 @@ copy : boolean, default False
             if name is None:
                 df = DataFrame(self)
             else:
-                df = DataFrame({name : self})
+                df = DataFrame({name: self})
 
             return df.reset_index(level=level, drop=drop)
 
@@ -1153,7 +1156,7 @@ copy : boolean, default False
 
     @Substitution(name='standard deviation', shortname='stdev',
                   na_action=_doc_exclude_na, extras='')
-    @Appender(_stat_doc + 
+    @Appender(_stat_doc +
         """
         Normalized by N-1 (unbiased estimator).
         """)
@@ -1166,7 +1169,7 @@ copy : boolean, default False
 
     @Substitution(name='variance', shortname='var',
                   na_action=_doc_exclude_na, extras='')
-    @Appender(_stat_doc + 
+    @Appender(_stat_doc +
         """
         Normalized by N-1 (unbiased estimator).
         """)
@@ -1432,7 +1435,7 @@ copy : boolean, default False
                          lib.Timestamp(top), freq]
         else:
 
-            lb = .5 * (1. - percentile_width/100.)
+            lb = .5 * (1. - percentile_width / 100.)
             ub = 1. - lb
 
             def pretty_name(x):
@@ -1567,7 +1570,7 @@ copy : boolean, default False
         """
         return np.where(self < threshold, threshold, self)
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Combination
 
     def append(self, to_append, verify_integrity=False):
@@ -1993,6 +1996,7 @@ copy : boolean, default False
 
         if na_action == 'ignore':
             mask = isnull(values)
+
             def map_f(values, f):
                 return lib.map_infer_mask(values, f, mask.view(np.uint8))
         else:
@@ -2245,7 +2249,6 @@ copy : boolean, default False
 
         return result
 
-
     def replace(self, to_replace, value=None, method='pad', inplace=False,
                 limit=None):
         """
@@ -2282,15 +2285,15 @@ copy : boolean, default False
         """
         result = self.copy() if not inplace else self
 
-        def _rep_one(s, to_rep, v): # replace single value
+        def _rep_one(s, to_rep, v):  # replace single value
             mask = com.mask_missing(s.values, to_rep)
             np.putmask(s.values, mask, v)
             return s
 
-        def _rep_dict(rs, to_rep): # replace {[src] -> dest}
+        def _rep_dict(rs, to_rep):  # replace {[src] -> dest}
 
             all_src = set()
-            dd = {} # group by unique destination value
+            dd = {}  # group by unique destination value
             for s, d in to_rep.iteritems():
                 dd.setdefault(d, []).append(s)
                 all_src.add(s)
@@ -2298,12 +2301,12 @@ copy : boolean, default False
             if any(d in all_src for d in dd.keys()):
                 # don't clobber each other at the cost of temporaries
                 masks = {}
-                for d, sset in dd.iteritems(): # now replace by each dest
+                for d, sset in dd.iteritems():  # now replace by each dest
                     masks[d] = com.mask_missing(rs.values, sset)
 
                 for d, m in masks.iteritems():
                     np.putmask(rs.values, m, d)
-            else: # if no risk of clobbering then simple
+            else:  # if no risk of clobbering then simple
                 for d, sset in dd.iteritems():
                     _rep_one(rs, sset, d)
             return rs
@@ -2316,17 +2319,17 @@ copy : boolean, default False
 
         if isinstance(to_replace, (list, np.ndarray)):
 
-            if isinstance(value, (list, np.ndarray)): # check same length
+            if isinstance(value, (list, np.ndarray)):  # check same length
                 vl, rl = len(value), len(to_replace)
                 if vl == rl:
                     return _rep_dict(result, dict(zip(to_replace, value)))
                 raise ValueError('Got %d to replace but %d values' % (rl, vl))
 
-            elif value is not None: # otherwise all replaced with same value
+            elif value is not None:  # otherwise all replaced with same value
 
                 return _rep_one(result, to_replace, value)
 
-            else: # method
+            else:  # method
                 if method is None:  # pragma: no cover
                     raise ValueError('must specify a fill method')
                 fill_f = _get_fill_func(method)
@@ -2339,7 +2342,6 @@ copy : boolean, default False
                                     name=self.name)
                 return result
 
-
         raise ValueError('Unrecognized to_replace type %s' %
                          type(to_replace))
 
@@ -2746,9 +2748,10 @@ copy : boolean, default False
 
 _INDEX_TYPES = ndarray, Index, list, tuple
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Supplementary functions
 
+
 def remove_na(arr):
     """
     Return array containing only true/non-NaN values, possibly empty.
@@ -2769,7 +2772,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
         except (ValueError, TypeError):
             if dtype is not None and raise_cast_failure:
                 raise
-            else: # pragma: no cover
+            else:  # pragma: no cover
                 subarr = np.array(data, dtype=object, copy=copy)
         return subarr
 
@@ -2801,7 +2804,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
             try:
                 subarr = _try_cast(data)
             except Exception:
-                if raise_cast_failure: # pragma: no cover
+                if raise_cast_failure:  # pragma: no cover
                     raise
                 subarr = np.array(data, dtype=object, copy=copy)
                 subarr = lib.maybe_convert_objects(subarr)
@@ -2846,6 +2849,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
 
     return subarr
 
+
 def _dtype_from_scalar(val):
     if isinstance(val, np.datetime64):
         # ugly hacklet
@@ -2853,6 +2857,7 @@ def _dtype_from_scalar(val):
         return val, np.dtype('M8[ns]')
     return val, type(val)
 
+
 def _get_rename_function(mapper):
     if isinstance(mapper, (dict, Series)):
         def f(x):
@@ -2865,9 +2870,8 @@ def _get_rename_function(mapper):
 
     return f
 
-def _resolve_offset(freq, kwds):
-    from pandas.core.datetools import getOffset
 
+def _resolve_offset(freq, kwds):
     if 'timeRule' in kwds or 'offset' in kwds:
         offset = kwds.get('offset', None)
         offset = kwds.get('timeRule', offset)
@@ -2886,6 +2890,7 @@ def _resolve_offset(freq, kwds):
 
     return offset
 
+
 def _get_fill_func(method):
     method = com._clean_fill_method(method)
     if method == 'pad':
@@ -2904,6 +2909,7 @@ Series.hist = _gfx.hist_series
 
 # Put here, otherwise monkey-patching in methods fails
 
+
 class TimeSeries(Series):
 
     def _repr_footer(self):
diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index 3172c5a39..cdbeffbba 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -9,6 +9,7 @@ import pandas.lib as lib
 import pandas.core.common as com
 import operator
 
+
 class repeat(object):
     def __init__(self, obj):
         self.obj = obj
@@ -16,6 +17,7 @@ class repeat(object):
     def __getitem__(self, i):
         return self.obj
 
+
 class azip(object):
     def __init__(self, *args):
         self.cols = []
@@ -328,6 +330,7 @@ def str_replace(arr, pat, repl, n=0, case=True, flags=0):
 
     return _na_map(f, arr)
 
+
 def str_repeat(arr, repeats):
     """
     Duplicate each string in the array by indicated number of times
@@ -358,6 +361,7 @@ def str_repeat(arr, repeats):
         result = lib.vec_binop(arr, repeats, rep)
         return result
 
+
 def str_match(arr, pat, flags=0):
     """
     Find groups in each string (from beginning) using passed regular expression
@@ -374,6 +378,7 @@ def str_match(arr, pat, flags=0):
     matches : array
     """
     regex = re.compile(pat, flags=flags)
+
     def f(x):
         m = regex.match(x)
         if m:
@@ -384,7 +389,6 @@ def str_match(arr, pat, flags=0):
     return _na_map(f, arr)
 
 
-
 def str_join(arr, sep):
     """
     Join lists contained as elements in array, a la str.join
@@ -412,7 +416,6 @@ def str_len(arr):
     return _na_map(len, arr)
 
 
-
 def str_findall(arr, pat, flags=0):
     """
     Find all occurrences of pattern or regular expression
@@ -582,6 +585,7 @@ def str_wrap(arr, width=80):
     """
     raise NotImplementedError
 
+
 def str_get(arr, i):
     """
     Extract element from lists, tuples, or strings in each element in the array
@@ -598,6 +602,7 @@ def str_get(arr, i):
     f = lambda x: x[i]
     return _na_map(f, arr)
 
+
 def str_decode(arr, encoding):
     """
     Decode character string to unicode using indicated encoding
@@ -613,6 +618,7 @@ def str_decode(arr, encoding):
     f = lambda x: x.decode(encoding)
     return _na_map(f, arr)
 
+
 def str_encode(arr, encoding):
     """
     Encode character string to unicode using indicated encoding
@@ -628,6 +634,7 @@ def str_encode(arr, encoding):
     f = lambda x: x.encode(encoding)
     return _na_map(f, arr)
 
+
 def _noarg_wrapper(f):
     def wrapper(self):
         result = f(self.series)
@@ -661,6 +668,7 @@ def _pat_wrapper(f, flags=False, na=False):
 
     return wrapper
 
+
 def copy(source):
     "Copy a docstring from another source function (if present)"
     def do_copy(target):
diff --git a/pandas/io/data.py b/pandas/io/data.py
index 8753d1dab..e4c1ae9a9 100644
--- a/pandas/io/data.py
+++ b/pandas/io/data.py
@@ -82,13 +82,14 @@ def get_quote_yahoo(symbols):
     if not isinstance(symbols, list):
         raise TypeError, "symbols must be a list"
     # for codes see: http://www.gummy-stuff.org/Yahoo-data.htm
-    codes = {'symbol':'s','last':'l1','change_pct':'p2','PE':'r','time':'t1','short_ratio':'s7'}
-    request = str.join('',codes.values()) # code request string
+    codes = {'symbol': 's', 'last': 'l1', 'change_pct': 'p2', 'PE': 'r',
+             'time': 't1', 'short_ratio': 's7'}
+    request = str.join('',codes.values())  # code request string
     header = codes.keys()
 
     data = dict(zip(codes.keys(), [[] for i in range(len(codes))]))
 
-    urlStr = 'http://finance.yahoo.com/d/quotes.csv?s=%s&f=%s' % (str.join('+',symbols), request)
+    urlStr = 'http://finance.yahoo.com/d/quotes.csv?s=%s&f=%s' % (str.join('+', symbols), request)
 
     try:
         lines = urllib2.urlopen(urlStr).readlines()
@@ -178,8 +179,8 @@ def get_data_fred(name=None, start=dt.datetime(2010, 1, 1),
 
     url = fred_URL + '%s' % name + \
       '/downloaddata/%s' % name + '.csv'
-    data = read_csv(urllib.urlopen(url), index_col=0, parse_dates=True, header=None,
-                    skiprows=1, names=["DATE", name])
+    data = read_csv(urllib.urlopen(url), index_col=0, parse_dates=True,
+                    header=None, skiprows=1, names=["DATE", name])
     return data.truncate(start, end)
 
 
@@ -197,10 +198,10 @@ def get_data_famafrench(name, start=None, end=None):
 
     datasets = {}
     for i in range(len(file_edges) - 1):
-        dataset = [d.split() for d in data[(file_edges[i] + 1):file_edges[i+1]]]
+        dataset = [d.split() for d in data[(file_edges[i] + 1):file_edges[i + 1]]]
         if(len(dataset) > 10):
             ncol = np.median(np.array([len(d) for d in dataset]))
-            header_index = np.where(np.array([len(d) for d in dataset]) == (ncol-1))[0][-1]
+            header_index = np.where(np.array([len(d) for d in dataset]) == (ncol - 1))[0][-1]
             header = dataset[header_index]
             # to ensure the header is unique
             header = [str(j + 1) + " " + header[j] for j in range(len(header))]
diff --git a/pandas/io/date_converters.py b/pandas/io/date_converters.py
index b9325b97b..ce670eec7 100644
--- a/pandas/io/date_converters.py
+++ b/pandas/io/date_converters.py
@@ -2,17 +2,20 @@
 import numpy as np
 import pandas.lib as lib
 
+
 def parse_date_time(date_col, time_col):
     date_col = _maybe_cast(date_col)
     time_col = _maybe_cast(time_col)
     return lib.try_parse_date_and_time(date_col, time_col)
 
+
 def parse_date_fields(year_col, month_col, day_col):
     year_col = _maybe_cast(year_col)
     month_col = _maybe_cast(month_col)
     day_col = _maybe_cast(day_col)
     return lib.try_parse_year_month_day(year_col, month_col, day_col)
 
+
 def parse_all_fields(year_col, month_col, day_col, hour_col, minute_col,
                      second_col):
     year_col = _maybe_cast(year_col)
@@ -24,6 +27,7 @@ def parse_all_fields(year_col, month_col, day_col, hour_col, minute_col,
     return lib.try_parse_datetime_components(year_col, month_col, day_col,
                                              hour_col, minute_col, second_col)
 
+
 def generic_parser(parse_func, *cols):
     N = _check_columns(cols)
     results = np.empty(N, dtype=object)
@@ -34,11 +38,13 @@ def generic_parser(parse_func, *cols):
 
     return results
 
+
 def _maybe_cast(arr):
     if not arr.dtype.type == np.object_:
         arr = np.array(arr, dtype=object)
     return arr
 
+
 def _check_columns(cols):
     assert(len(cols) > 0)
 
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index db8c4a132..76002917e 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -26,6 +26,7 @@ from pandas.io.date_converters import generic_parser
 
 from pandas.util.decorators import Appender
 
+
 class DateConversionError(Exception):
     pass
 
@@ -146,11 +147,12 @@ def _is_url(url):
     Very naive check to see if url is an http(s), ftp, or file location.
     """
     parsed_url = urlparse(url)
-    if parsed_url.scheme in ['http','file', 'ftp', 'https']:
+    if parsed_url.scheme in ['http', 'file', 'ftp', 'https']:
         return True
     else:
         return False
 
+
 def _read(cls, filepath_or_buffer, kwds):
     "Generic reader of line files."
     encoding = kwds.get('encoding', None)
@@ -176,7 +178,7 @@ def _read(cls, filepath_or_buffer, kwds):
         try:
             # universal newline mode
             f = com._get_handle(filepath_or_buffer, 'U', encoding=encoding)
-        except Exception: # pragma: no cover
+        except Exception:  # pragma: no cover
             f = com._get_handle(filepath_or_buffer, 'r', encoding=encoding)
 
     if kwds.get('date_parser', None) is not None:
@@ -199,6 +201,7 @@ def _read(cls, filepath_or_buffer, kwds):
 
     return parser.get_chunk()
 
+
 @Appender(_read_csv_doc)
 def read_csv(filepath_or_buffer,
              sep=',',
@@ -249,6 +252,7 @@ def read_csv(filepath_or_buffer,
 
     return _read(TextParser, filepath_or_buffer, kdict)
 
+
 @Appender(_read_table_doc)
 def read_table(filepath_or_buffer,
                sep='\t',
@@ -299,6 +303,7 @@ def read_table(filepath_or_buffer,
 
     return _read(TextParser, filepath_or_buffer, kdict)
 
+
 @Appender(_read_fwf_doc)
 def read_fwf(filepath_or_buffer,
              colspecs=None,
@@ -353,13 +358,14 @@ def read_fwf(filepath_or_buffer,
     if widths is not None:
         colspecs, col = [], 0
         for w in widths:
-            colspecs.append( (col, col+w) )
+            colspecs.append((col, col+w))
             col += w
         kdict['colspecs'] = colspecs
 
     kdict['thousands'] = thousands
     return _read(FixedWidthFieldParser, filepath_or_buffer, kdict)
 
+
 def read_clipboard(**kwargs):  # pragma: no cover
     """
     Read text from clipboard and pass to read_table. See read_table for the
@@ -373,7 +379,8 @@ def read_clipboard(**kwargs):  # pragma: no cover
     text = clipboard_get()
     return read_table(StringIO(text), **kwargs)
 
-def to_clipboard(obj): # pragma: no cover
+
+def to_clipboard(obj):  # pragma: no cover
     """
     Attempt to write text representation of object to the system clipboard
 
@@ -387,6 +394,7 @@ def to_clipboard(obj): # pragma: no cover
     from pandas.util.clipboard import clipboard_set
     clipboard_set(str(obj))
 
+
 class BufferedReader(object):
     """
     For handling different kinds of files, e.g. zip files where reading out a
@@ -394,7 +402,8 @@ class BufferedReader(object):
     """
 
     def __init__(self, fh, delimiter=','):
-        pass # pragma: no coverage
+        pass  # pragma: no coverage
+
 
 class BufferedCSVReader(BufferedReader):
     pass
@@ -817,7 +826,7 @@ class TextParser(object):
         self._first_chunk = False
 
         columns = list(self.orig_columns)
-        if len(content) == 0: # pragma: no cover
+        if len(content) == 0:  # pragma: no cover
             if self.index_col is not None:
                 if np.isscalar(self.index_col):
                     index = Index([], name=self.index_name)
@@ -903,7 +912,7 @@ class TextParser(object):
             index = data.pop(i)
             if not self._implicit_index:
                 columns.pop(i)
-        else: # given a list of index
+        else:  # given a list of index
             to_remove = []
             index = []
             for idx in self.index_col:
@@ -938,7 +947,7 @@ class TextParser(object):
             name = _get_name(self.index_col)
             index = data.pop(name)
             col_names.remove(name)
-        else: # given a list of index
+        else:  # given a list of index
             to_remove = []
             index = []
             for idx in self.index_col:
@@ -1085,7 +1094,7 @@ class TextParser(object):
                 lines.extend(source[self.pos:])
                 self.pos = len(source)
             else:
-                lines.extend(source[self.pos:self.pos+rows])
+                lines.extend(source[self.pos:self.pos + rows])
                 self.pos += rows
         else:
             new_rows = []
@@ -1121,6 +1130,7 @@ class TextParser(object):
         lines = self._check_comments(lines)
         return self._check_thousands(lines)
 
+
 def _get_na_values(col, na_values):
     if isinstance(na_values, dict):
         if col in na_values:
@@ -1130,6 +1140,7 @@ def _get_na_values(col, na_values):
     else:
         return na_values
 
+
 def _convert_to_ndarrays(dct, na_values, verbose=False):
     result = {}
     for c, values in dct.iteritems():
@@ -1140,6 +1151,7 @@ def _convert_to_ndarrays(dct, na_values, verbose=False):
             print 'Filled %d NA values in column %s' % (na_count, str(c))
     return result
 
+
 def _convert_types(values, na_values):
     na_count = 0
     if issubclass(values.dtype.type, (np.number, np.bool_)):
@@ -1162,6 +1174,7 @@ def _convert_types(values, na_values):
 
     return result, na_count
 
+
 def _try_convert_dates(parser, colspec, data_dict, columns):
     colspec = _get_col_names(colspec, columns)
     new_name = '_'.join([str(x) for x in colspec])
@@ -1173,6 +1186,7 @@ def _try_convert_dates(parser, colspec, data_dict, columns):
         new_col = parser(_concat_date_cols(to_parse))
     return new_name, new_col, colspec
 
+
 def _get_col_names(colspec, columns):
     colset = set(columns)
     colnames = []
@@ -1201,7 +1215,7 @@ class FixedWidthReader(object):
     def __init__(self, f, colspecs, filler, thousands=None):
         self.f = f
         self.colspecs = colspecs
-        self.filler = filler # Empty characters between fields.
+        self.filler = filler  # Empty characters between fields.
         self.thousands = thousands
 
         assert isinstance(colspecs, (tuple, list))
@@ -1323,8 +1337,8 @@ class ExcelFile(object):
         if skipfooter is not None:
             skip_footer = skipfooter
 
-        choose = {True:self._parse_xlsx,
-                  False:self._parse_xls}
+        choose = {True: self._parse_xlsx,
+                  False: self._parse_xls}
         return choose[self.use_xlsx](sheetname, header=header,
                                      skiprows=skiprows, index_col=index_col,
                                      parse_cols=parse_cols,
@@ -1399,7 +1413,7 @@ class ExcelFile(object):
                     if typ == XL_CELL_DATE:
                         dt = xldate_as_tuple(value, datemode)
                         # how to produce this first case?
-                        if dt[0] < MINYEAR: # pragma: no cover
+                        if dt[0] < MINYEAR:  # pragma: no cover
                             value = time(*dt[3:])
                         else:
                             value = datetime(*dt)
@@ -1436,6 +1450,7 @@ def _trim_excel_header(row):
         row = row[1:]
     return row
 
+
 class ExcelWriter(object):
     """
     Class for writing DataFrame objects into excel sheets, uses xlwt for xls,
@@ -1456,7 +1471,7 @@ class ExcelWriter(object):
             self.fm_date = xlwt.easyxf(num_format_str='YYYY-MM-DD')
         else:
             from openpyxl.workbook import Workbook
-            self.book = Workbook(optimized_write = True)
+            self.book = Workbook(optimized_write=True)
         self.path = path
         self.sheets = {}
         self.cur_sheet = None
@@ -1498,15 +1513,15 @@ class ExcelWriter(object):
         for i, val in enumerate(row):
             if isinstance(val, (datetime.datetime, datetime.date)):
                 if isinstance(val, datetime.datetime):
-                    sheetrow.write(i,val, self.fm_datetime)
+                    sheetrow.write(i, val, self.fm_datetime)
                 else:
-                    sheetrow.write(i,val, self.fm_date)
+                    sheetrow.write(i, val, self.fm_date)
             elif isinstance(val, np.int64):
-                sheetrow.write(i,int(val))
+                sheetrow.write(i, int(val))
             elif isinstance(val, np.bool8):
-                sheetrow.write(i,bool(val))
+                sheetrow.write(i, bool(val))
             else:
-                sheetrow.write(i,val)
+                sheetrow.write(i, val)
         row_idx += 1
         if row_idx == 1000:
             sheet.flush_row_data()
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index c82dab08a..af480b5a6 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -29,44 +29,45 @@ from contextlib import contextmanager
 
 # reading and writing the full object in one go
 _TYPE_MAP = {
-    Series     : 'series',
-    SparseSeries : 'sparse_series',
-    TimeSeries : 'series',
-    DataFrame  : 'frame',
-    SparseDataFrame : 'sparse_frame',
-    Panel  : 'wide',
-    SparsePanel : 'sparse_panel'
+    Series: 'series',
+    SparseSeries: 'sparse_series',
+    TimeSeries: 'series',
+    DataFrame: 'frame',
+    SparseDataFrame: 'sparse_frame',
+    Panel: 'wide',
+    SparsePanel: 'sparse_panel'
 }
 
 _NAME_MAP = {
-    'series' : 'Series',
-    'time_series' : 'TimeSeries',
-    'sparse_series' : 'SparseSeries',
-    'frame' : 'DataFrame',
-    'sparse_frame' : 'SparseDataFrame',
-    'frame_table' : 'DataFrame (Table)',
-    'wide' : 'Panel',
-    'sparse_panel' : 'SparsePanel',
-    'wide_table' : 'Panel (Table)',
-    'long' : 'LongPanel',
+    'series': 'Series',
+    'time_series': 'TimeSeries',
+    'sparse_series': 'SparseSeries',
+    'frame': 'DataFrame',
+    'sparse_frame': 'SparseDataFrame',
+    'frame_table': 'DataFrame (Table)',
+    'wide': 'Panel',
+    'sparse_panel': 'SparsePanel',
+    'wide_table': 'Panel (Table)',
+    'long': 'LongPanel',
     # legacy h5 files
-    'Series' : 'Series',
-    'TimeSeries' : 'TimeSeries',
-    'DataFrame' : 'DataFrame',
-    'DataMatrix' : 'DataMatrix'
+    'Series': 'Series',
+    'TimeSeries': 'TimeSeries',
+    'DataFrame': 'DataFrame',
+    'DataMatrix': 'DataMatrix'
 }
 
 # legacy handlers
 _LEGACY_MAP = {
-    'Series' : 'legacy_series',
-    'TimeSeries' : 'legacy_series',
-    'DataFrame' : 'legacy_frame',
-    'DataMatrix' : 'legacy_frame',
-    'WidePanel' : 'wide_table',
+    'Series': 'legacy_series',
+    'TimeSeries': 'legacy_series',
+    'DataFrame': 'legacy_frame',
+    'DataMatrix': 'legacy_frame',
+    'WidePanel': 'wide_table',
 }
 
 # oh the troubles to reduce import time
 _table_mod = None
+
 def _tables():
     global _table_mod
     if _table_mod is None:
@@ -74,6 +75,7 @@ def _tables():
         _table_mod = tables
     return _table_mod
 
+
 @contextmanager
 def get_store(path, mode='a', complevel=None, complib=None,
               fletcher32=False):
@@ -120,6 +122,7 @@ def get_store(path, mode='a', complevel=None, complib=None,
         if store is not None:
             store.close()
 
+
 class HDFStore(object):
     """
     dict-like IO interface for storing pandas objects in PyTables
@@ -167,7 +170,7 @@ class HDFStore(object):
                  fletcher32=False):
         try:
             import tables as _
-        except ImportError: # pragma: no cover
+        except ImportError:  # pragma: no cover
             raise Exception('HDFStore requires PyTables')
 
         self.path = path
@@ -226,7 +229,7 @@ class HDFStore(object):
             See HDFStore docstring or tables.openFile for info about modes
         """
         self.mode = mode
-        if warn and mode == 'w': # pragma: no cover
+        if warn and mode == 'w':  # pragma: no cover
             while True:
                 response = raw_input("Re-opening as mode='w' will delete the "
                                      "current file. Continue (y/n)?")
@@ -328,8 +331,8 @@ class HDFStore(object):
         value : {Series, DataFrame, Panel}
         table : boolean, default False
             Write as a PyTables Table structure which may perform worse but
-            allow more flexible operations like searching / selecting subsets of
-            the data
+            allow more flexible operations like searching / selecting subsets
+            of the data
         append : boolean, default False
             For table data structures, append the input data to the existing
             table
@@ -342,7 +345,7 @@ class HDFStore(object):
                              comp=compression)
 
     def _get_handler(self, op, kind):
-        return getattr(self,'_%s_%s' % (op, kind))
+        return getattr(self, '_%s_%s' % (op, kind))
 
     def remove(self, key, where=None):
         """
@@ -666,7 +669,8 @@ class HDFStore(object):
         if 'name' in node._v_attrs:
             name = node._v_attrs.name
 
-        index_class = _alias_to_class(getattr(node._v_attrs, 'index_class', ''))
+        index_class = _alias_to_class(getattr(node._v_attrs,
+                                              'index_class', ''))
         factory = _get_index_factory(index_class)
 
         kwargs = {}
@@ -714,7 +718,6 @@ class HDFStore(object):
                 getattr(group, key)._v_attrs.transposed = transposed
                 return
 
-
         if value.dtype.type == np.object_:
             vlarr = self.handle.createVLArray(group, key,
                                               _tables().ObjectAtom())
@@ -749,12 +752,12 @@ class HDFStore(object):
 
         if 'table' not in group:
             # create the table
-            desc = {'index'  : index_t,
-                    'column' : col_t,
-                    'values' : _tables().FloatCol(shape=(len(values)))}
+            desc = {'index': index_t,
+                    'column': col_t,
+                    'values': _tables().FloatCol(shape=(len(values)))}
 
-            options = {'name' : 'table',
-                       'description' : desc}
+            options = {'name': 'table',
+                       'description': desc}
 
             if compression:
                 complevel = self.complevel
@@ -783,7 +786,7 @@ class HDFStore(object):
         table._v_attrs.index_kind = index_kind
         table._v_attrs.columns_kind = cols_kind
         if append:
-            existing_fields = getattr(table._v_attrs,'fields',None)
+            existing_fields = getattr(table._v_attrs, 'fields', None)
             if (existing_fields is not None and
                 existing_fields != list(items)):
                 raise Exception("appended items do not match existing items"
@@ -809,7 +812,7 @@ class HDFStore(object):
                     row['values'] = v
                     row.append()
             self.handle.flush()
-        except (ValueError), detail: # pragma: no cover
+        except (ValueError), detail:  # pragma: no cover
             print "value_error in _write_table -> %s" % str(detail)
             try:
                 self.handle.flush()
@@ -918,6 +921,7 @@ class HDFStore(object):
             wp = wp.reindex(minor=new_minor)
         return wp
 
+
     def _delete_from_table(self, group, where = None):
         table = getattr(group, 'table')
 
@@ -933,6 +937,7 @@ class HDFStore(object):
         self.handle.flush()
         return len(s.values)
 
+
 def _convert_index(index):
     if isinstance(index, DatetimeIndex):
         converted = index.asi8
@@ -960,7 +965,7 @@ def _convert_index(index):
         converted = np.array([time.mktime(v.timetuple()) for v in values],
                             dtype=np.int32)
         return converted, 'date', _tables().Time32Col()
-    elif inferred_type =='string':
+    elif inferred_type == 'string':
         converted = np.array(list(values), dtype=np.str_)
         itemsize = converted.dtype.itemsize
         return converted, 'string', _tables().StringCol(itemsize)
@@ -974,10 +979,11 @@ def _convert_index(index):
     elif inferred_type == 'floating':
         atom = _tables().Float64Col()
         return np.asarray(values, dtype=np.float64), 'float', atom
-    else: # pragma: no cover
+    else:  # pragma: no cover
         atom = _tables().ObjectAtom()
         return np.asarray(values, dtype='O'), 'object', atom
 
+
 def _read_array(group, key):
     import tables
     node = getattr(group, key)
@@ -1006,6 +1012,7 @@ def _read_array(group, key):
     else:
         return ret
 
+
 def _unconvert_index(data, kind):
     if kind == 'datetime64':
         index = DatetimeIndex(data)
@@ -1018,19 +1025,21 @@ def _unconvert_index(data, kind):
         index = np.array(data)
     elif kind == 'object':
         index = np.array(data[0])
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('unrecognized index type %s' % kind)
     return index
 
+
 def _unconvert_index_legacy(data, kind, legacy=False):
     if kind == 'datetime':
         index = lib.time64_to_datetime(data)
     elif kind in ('string', 'integer'):
         index = np.array(data, dtype=object)
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('unrecognized index type %s' % kind)
     return index
 
+
 def _maybe_convert(values, val_kind):
     if _need_convert(val_kind):
         conv = _get_converter(val_kind)
@@ -1038,19 +1047,22 @@ def _maybe_convert(values, val_kind):
         values = conv(values)
     return values
 
+
 def _get_converter(kind):
     if kind == 'datetime64':
         return lambda x: np.array(x, dtype='M8[ns]')
     if kind == 'datetime':
         return lib.convert_timestamps
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('invalid kind %s' % kind)
 
+
 def _need_convert(kind):
     if kind in ('datetime', 'datetime64'):
         return True
     return False
 
+
 def _is_table_type(group):
     try:
         return 'table' in group._v_attrs.pandas_type
@@ -1058,21 +1070,24 @@ def _is_table_type(group):
         # new node, e.g.
         return False
 
-_index_type_map = {DatetimeIndex : 'datetime',
-                   PeriodIndex : 'period'}
+_index_type_map = {DatetimeIndex: 'datetime',
+                   PeriodIndex: 'period'}
 
 _reverse_index_map = {}
 for k, v in _index_type_map.iteritems():
     _reverse_index_map[v] = k
 
+
 def _class_to_alias(cls):
     return _index_type_map.get(cls, '')
 
+
 def _alias_to_class(alias):
-    if isinstance(alias, type): # pragma: no cover
-        return alias # compat: for a short period of time master stored types
+    if isinstance(alias, type):  # pragma: no cover
+        return alias  # compat: for a short period of time master stored types
     return _reverse_index_map.get(alias, Index)
 
+
 class Selection(object):
     """
     Carries out a selection operation on a tables.Table object.
@@ -1109,18 +1124,18 @@ class Selection(object):
     def generate(self, where):
         # and condictions
         for c in where:
-            op = c.get('op',None)
+            op = c.get('op', None)
             value = c['value']
             field = c['field']
 
             if field == 'index' and self.index_kind == 'datetime64':
                 val = lib.Timestamp(value).value
-                self.conditions.append('(%s %s %s)' % (field,op,val))
+                self.conditions.append('(%s %s %s)' % (field, op, val))
             elif field == 'index' and isinstance(value, datetime):
                 value = time.mktime(value.timetuple())
-                self.conditions.append('(%s %s %s)' % (field,op,value))
+                self.conditions.append('(%s %s %s)' % (field, op, value))
             else:
-                self.generate_multiple_conditions(op,value,field)
+                self.generate_multiple_conditions(op, value, field)
 
         if len(self.conditions):
             self.the_condition = '(' + ' & '.join(self.conditions) + ')'
@@ -1129,15 +1144,15 @@ class Selection(object):
 
         if op and op == 'in' or isinstance(value, (list, np.ndarray)):
             if len(value) <= 61:
-                l = '(' + ' | '.join([ "(%s == '%s')" % (field,v)
-                                       for v in value ]) + ')'
+                l = '(' + ' | '.join([ "(%s == '%s')" % (field, v)
+                                       for v in value]) + ')'
                 self.conditions.append(l)
             else:
                 self.column_filter = set(value)
         else:
             if op is None:
                 op = '=='
-            self.conditions.append('(%s %s "%s")' % (field,op,value))
+            self.conditions.append('(%s %s "%s")' % (field, op, value))
 
     def select(self):
         """
@@ -1155,6 +1170,7 @@ class Selection(object):
         """
         self.values = self.table.getWhereList(self.the_condition)
 
+
 def _get_index_factory(klass):
     if klass == DatetimeIndex:
         def f(values, freq=None, tz=None):
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index 6d1628fd8..021f80c06 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -10,9 +10,10 @@ import traceback
 from pandas.core.datetools import format as date_format
 from pandas.core.api import DataFrame, isnull
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Helper execution function
 
+
 def execute(sql, con, retry=True, cur=None, params=None):
     """
     Execute the given SQL query using the provided connection object.
@@ -44,17 +45,19 @@ def execute(sql, con, retry=True, cur=None, params=None):
         print 'Error on sql %s' % sql
         raise
 
+
 def _safe_fetch(cur):
     try:
         result = cur.fetchall()
         if not isinstance(result, list):
             result = list(result)
         return result
-    except Exception, e: # pragma: no cover
+    except Exception, e:  # pragma: no cover
         excName = e.__class__.__name__
         if excName == 'OperationalError':
             return []
 
+
 def tquery(sql, con=None, cur=None, retry=True):
     """
     Returns list of tuples corresponding to each row in given sql
@@ -98,6 +101,7 @@ def tquery(sql, con=None, cur=None, retry=True):
 
     return result
 
+
 def uquery(sql, con=None, cur=None, retry=True, params=()):
     """
     Does the same thing as tquery, but instead of returning results, it
@@ -119,6 +123,7 @@ def uquery(sql, con=None, cur=None, retry=True, params=()):
             return uquery(sql, con, retry=False)
     return result
 
+
 def read_frame(sql, con, index_col=None, coerce_float=True):
     """
     Returns a DataFrame corresponding to the result set of the query
@@ -152,6 +157,7 @@ def read_frame(sql, con, index_col=None, coerce_float=True):
 
 frame_query = read_frame
 
+
 def write_frame(frame, name=None, con=None, flavor='sqlite', append=False):
     """
     Write records stored in a DataFrame to SQLite. The index will currently be
@@ -170,11 +176,13 @@ def write_frame(frame, name=None, con=None, flavor='sqlite', append=False):
     data = [tuple(x) for x in frame.values]
     con.executemany(insert_sql, data)
 
+
 def has_table(name, con):
     sqlstr = "SELECT name FROM sqlite_master WHERE type='table' AND name='%s'" % name
     rs = tquery(sqlstr, con)
     return len(rs) > 0
 
+
 def get_sqlite_schema(frame, name, dtypes=None, keys=None):
     template = """
 CREATE TABLE %(name)s (
@@ -206,26 +214,25 @@ CREATE TABLE %(name)s (
         if isinstance(keys, basestring):
             keys = (keys,)
         keystr = ', PRIMARY KEY (%s)' % ','.join(keys)
-    return template % {'name' : name, 'columns' : columns, 'keystr' : keystr}
-
+    return template % {'name': name, 'columns': columns, 'keystr': keystr}
 
 
-
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Query formatting
 
 _formatters = {
-    datetime : lambda dt: "'%s'" % date_format(dt),
-    str : lambda x: "'%s'" % x,
-    np.str_ : lambda x: "'%s'" % x,
-    unicode : lambda x: "'%s'" % x,
-    float : lambda x: "%.8f" % x,
-    int : lambda x: "%s" % x,
-    type(None) : lambda x: "NULL",
-    np.float64 : lambda x: "%.10f" % x,
-    bool : lambda x: "'%s'" % x,
+    datetime: lambda dt: "'%s'" % date_format(dt),
+    str: lambda x: "'%s'" % x,
+    np.str_: lambda x: "'%s'" % x,
+    unicode: lambda x: "'%s'" % x,
+    float: lambda x: "%.8f" % x,
+    int: lambda x: "%s" % x,
+    type(None): lambda x: "NULL",
+    np.float64: lambda x: "%.10f" % x,
+    bool: lambda x: "'%s'" % x,
 }
 
+
 def format_query(sql, *args):
     """
 
diff --git a/pandas/rpy/base.py b/pandas/rpy/base.py
index 070d457ed..0c8044868 100644
--- a/pandas/rpy/base.py
+++ b/pandas/rpy/base.py
@@ -1,5 +1,6 @@
 import pandas.rpy.util as util
 
+
 class lm(object):
     """
     Examples
@@ -10,4 +11,3 @@ class lm(object):
     def __init__(self, formula, data):
         pass
 
-
diff --git a/pandas/rpy/common.py b/pandas/rpy/common.py
index f81ec7ef3..481714b94 100644
--- a/pandas/rpy/common.py
+++ b/pandas/rpy/common.py
@@ -16,6 +16,7 @@ import rpy2.robjects as robj
 __all__ = ['convert_robj', 'load_data', 'convert_to_r_dataframe',
            'convert_to_r_matrix']
 
+
 def load_data(name, package=None, convert=True):
     if package:
         importr(package)
@@ -29,15 +30,18 @@ def load_data(name, package=None, convert=True):
     else:
         return robj
 
+
 def _rclass(obj):
     """
     Return R class name for input object
     """
     return r['class'](obj)[0]
 
+
 def _is_null(obj):
     return _rclass(obj) == 'NULL'
 
+
 def _convert_list(obj):
     """
     Convert named Vector to dict
@@ -45,6 +49,7 @@ def _convert_list(obj):
     values = [convert_robj(x) for x in obj]
     return dict(zip(obj.names, values))
 
+
 def _convert_array(obj):
     """
     Convert Array to ndarray
@@ -59,7 +64,6 @@ def _convert_array(obj):
     if len(dim) == 3:
         arr = values.reshape(dim[-1:] + dim[:-1]).swapaxes(1, 2)
 
-
     if obj.names is not None:
         name_list = [list(x) for x in obj.names]
         if len(dim) == 2:
@@ -73,6 +77,7 @@ def _convert_array(obj):
     else:
         return arr
 
+
 def _convert_vector(obj):
     if isinstance(obj, robj.IntVector):
         return _convert_int_vector(obj)
@@ -83,6 +88,7 @@ def _convert_vector(obj):
 
 NA_INTEGER = -2147483648
 
+
 def _convert_int_vector(obj):
     arr = np.asarray(obj)
     mask = arr == NA_INTEGER
@@ -91,6 +97,7 @@ def _convert_int_vector(obj):
         arr[mask] = np.nan
     return arr
 
+
 def _convert_str_vector(obj):
     arr = np.asarray(obj, dtype=object)
     mask = arr == robj.NA_Character
@@ -98,6 +105,7 @@ def _convert_str_vector(obj):
         arr[mask] = np.nan
     return arr
 
+
 def _convert_DataFrame(rdf):
     columns = list(rdf.colnames)
     rows = np.array(rdf.rownames)
@@ -125,6 +133,7 @@ def _convert_DataFrame(rdf):
 
     return pd.DataFrame(data, index=_check_int(rows), columns=columns)
 
+
 def _convert_Matrix(mat):
     columns = mat.colnames
     rows = mat.rownames
@@ -135,6 +144,7 @@ def _convert_Matrix(mat):
     return pd.DataFrame(np.array(mat), index=_check_int(index),
                         columns=columns)
 
+
 def _check_int(vec):
     try:
         # R observation numbers come through as strings
@@ -145,8 +155,8 @@ def _check_int(vec):
     return vec
 
 _pandas_converters = [
-    (robj.DataFrame , _convert_DataFrame),
-    (robj.Matrix , _convert_Matrix),
+    (robj.DataFrame, _convert_DataFrame),
+    (robj.Matrix, _convert_Matrix),
     (robj.StrVector, _convert_vector),
     (robj.FloatVector, _convert_vector),
     (robj.Array, _convert_array),
@@ -154,8 +164,8 @@ _pandas_converters = [
 ]
 
 _converters = [
-    (robj.DataFrame , lambda x: _convert_DataFrame(x).toRecords(index=False)),
-    (robj.Matrix , lambda x: _convert_Matrix(x).toRecords(index=False)),
+    (robj.DataFrame, lambda x: _convert_DataFrame(x).toRecords(index=False)),
+    (robj.Matrix, lambda x: _convert_Matrix(x).toRecords(index=False)),
     (robj.IntVector, _convert_vector),
     (robj.StrVector, _convert_vector),
     (robj.FloatVector, _convert_vector),
@@ -163,6 +173,7 @@ _converters = [
     (robj.Vector, _convert_list),
 ]
 
+
 def convert_robj(obj, use_pandas=True):
     """
     Convert rpy2 object to a pandas-friendly form
@@ -206,6 +217,7 @@ NA_TYPES = {np.float64: robj.NA_Real,
             np.str: robj.NA_Character,
             np.bool: robj.NA_Logical}
 
+
 def convert_to_r_dataframe(df, strings_as_factors=False):
     """
     Convert a pandas DataFrame to a R data.frame.
@@ -270,7 +282,6 @@ def convert_to_r_matrix(df, strings_as_factors=False):
         raise TypeError("Conversion to matrix only possible with non-mixed "
                         "type DataFrames")
 
-
     r_dataframe = convert_to_r_dataframe(df, strings_as_factors)
     as_matrix = robj.baseenv.get("as.matrix")
     r_matrix = as_matrix(r_dataframe)
@@ -282,18 +293,20 @@ def test_convert_list():
     obj = r('list(a=1, b=2, c=3)')
 
     converted = convert_robj(obj)
-    expected = {'a' : [1], 'b' : [2], 'c' : [3]}
+    expected = {'a': [1], 'b': [2], 'c': [3]}
 
     _test.assert_dict_equal(converted, expected)
 
+
 def test_convert_nested_list():
     obj = r('list(a=list(foo=1, bar=2))')
 
     converted = convert_robj(obj)
-    expected = {'a' : {'foo' : [1], 'bar' : [2]}}
+    expected = {'a': {'foo': [1], 'bar': [2]}}
 
     _test.assert_dict_equal(converted, expected)
 
+
 def test_convert_frame():
     # built-in dataset
     df = r['faithful']
@@ -303,6 +316,7 @@ def test_convert_frame():
     assert np.array_equal(converted.columns, ['eruptions', 'waiting'])
     assert np.array_equal(converted.index, np.arange(1, 273))
 
+
 def _test_matrix():
     r('mat <- matrix(rnorm(9), ncol=3)')
     r('colnames(mat) <- c("one", "two", "three")')
@@ -310,6 +324,7 @@ def _test_matrix():
 
     return r['mat']
 
+
 def test_convert_matrix():
     mat = _test_matrix()
 
@@ -318,6 +333,7 @@ def test_convert_matrix():
     assert np.array_equal(converted.index, ['a', 'b', 'c'])
     assert np.array_equal(converted.columns, ['one', 'two', 'three'])
 
+
 def test_convert_r_dataframe():
 
     is_na = robj.baseenv.get("is.na")
@@ -350,6 +366,7 @@ def test_convert_r_dataframe():
             else:
                 assert original == converted
 
+
 def test_convert_r_matrix():
 
     is_na = robj.baseenv.get("is.na")
diff --git a/pandas/rpy/mass.py b/pandas/rpy/mass.py
index 1a663e572..12fbbdfa4 100644
--- a/pandas/rpy/mass.py
+++ b/pandas/rpy/mass.py
@@ -1,4 +1,2 @@
-
 class rlm(object):
     pass
-
diff --git a/pandas/rpy/vars.py b/pandas/rpy/vars.py
index 3993423b3..4756b2779 100644
--- a/pandas/rpy/vars.py
+++ b/pandas/rpy/vars.py
@@ -1,5 +1,6 @@
 import pandas.rpy.util as util
 
+
 class VAR(object):
     """
 
@@ -17,4 +18,3 @@ class VAR(object):
     def __init__(y, p=1, type="none", season=None, exogen=None,
                  lag_max=None, ic=None):
         pass
-
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index f0f02d317..c7e783dee 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -35,12 +35,13 @@ def _sparse_op_wrap(op, name):
             return SparseArray(op(self.sp_values, other),
                                sparse_index=self.sp_index,
                                fill_value=new_fill_value)
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise TypeError('operation with %s not supported' % type(other))
 
     wrapper.__name__ = name
     return wrapper
 
+
 def _sparse_array_op(left, right, op, name):
     if np.isnan(left.fill_value):
         sparse_op = lambda a, b: _sparse_nanop(a, b, name)
@@ -61,6 +62,7 @@ def _sparse_array_op(left, right, op, name):
     return SparseArray(result, sparse_index=result_index,
                        fill_value=fill_value)
 
+
 def _sparse_nanop(this, other, name):
     sparse_op = getattr(splib, 'sparse_nan%s' % name)
     result, result_index = sparse_op(this.sp_values,
@@ -70,6 +72,7 @@ def _sparse_nanop(this, other, name):
 
     return result, result_index
 
+
 def _sparse_fillop(this, other, name):
     sparse_op = getattr(splib, 'sparse_%s' % name)
     result, result_index = sparse_op(this.sp_values,
@@ -399,6 +402,7 @@ to sparse
             nsparse = self.sp_index.ngaps
             return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)
 
+
 def make_sparse(arr, kind='block', fill_value=nan):
     """
     Convert ndarray to sparse format
@@ -428,7 +432,7 @@ def make_sparse(arr, kind='block', fill_value=nan):
         index = BlockIndex(length, locs, lens)
     elif kind == 'integer':
         index = IntIndex(length, indices)
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError('must be block or integer type')
 
     sparsified_values = arr[mask]
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index c26a37852..0df726fcb 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -1,6 +1,6 @@
 """
-Data structures for sparse float data. Life is made simpler by dealing only with
-float64 data
+Data structures for sparse float data. Life is made simpler by dealing only
+with float64 data
 """
 
 # pylint: disable=E1101,E1103,W0231,E0202
@@ -42,6 +42,7 @@ class _SparseMockBlockManager(object):
     def axes(self):
         return [self.sp_frame.columns, self.sp_frame.index]
 
+
 class SparseDataFrame(DataFrame):
     """
     DataFrame containing sparse floating point data in the form of SparseSeries
@@ -291,10 +292,11 @@ class SparseDataFrame(DataFrame):
             new_columns = self.columns[:loc]
         else:
             new_columns = Index(np.concatenate((self.columns[:loc],
-                                               self.columns[loc+1:])))
+                                               self.columns[loc + 1:])))
         self.columns = new_columns
 
     _index = None
+
     def _set_index(self, index):
         self._index = _ensure_index(index)
         for v in self._series.values():
@@ -337,7 +339,7 @@ class SparseDataFrame(DataFrame):
                 if com._is_bool_indexer(key):
                     key = np.asarray(key, dtype=bool)
                 return self._getitem_array(key)
-            else: # pragma: no cover
+            else:  # pragma: no cover
                 raise
 
     @Appender(DataFrame.get_value.__doc__, indents=0)
@@ -575,7 +577,7 @@ class SparseDataFrame(DataFrame):
 
         for col in self.columns:
             new_col = mapper(col)
-            if new_col in new_series: # pragma: no cover
+            if new_col in new_series:  # pragma: no cover
                 raise Exception('Non-unique mapping!')
             new_series[new_col] = self[col]
             new_columns.append(new_col)
@@ -626,7 +628,7 @@ class SparseDataFrame(DataFrame):
     def _join_index(self, other, how, lsuffix, rsuffix):
         if isinstance(other, Series):
             assert(other.name is not None)
-            other = SparseDataFrame({other.name : other},
+            other = SparseDataFrame({other.name: other},
                                     default_fill_value=self.default_fill_value)
 
         join_index = self.index.join(other.index, how=how)
@@ -786,6 +788,7 @@ class SparseDataFrame(DataFrame):
             return self._constructor(new_series, index=self.index,
                                      columns=self.columns)
 
+
 def stack_sparse_frame(frame):
     """
     Only makes sense when fill_value is NaN
diff --git a/pandas/sparse/list.py b/pandas/sparse/list.py
index 62c9d096d..9f59b9108 100644
--- a/pandas/sparse/list.py
+++ b/pandas/sparse/list.py
@@ -3,6 +3,7 @@ import numpy as np
 from pandas.sparse.array import SparseArray
 import pandas._sparse as splib
 
+
 class SparseList(object):
     """
     Data structure for accumulating data to be converted into a
diff --git a/pandas/sparse/panel.py b/pandas/sparse/panel.py
index b843b653a..bd5a2785a 100644
--- a/pandas/sparse/panel.py
+++ b/pandas/sparse/panel.py
@@ -1,6 +1,6 @@
 """
-Data structures for sparse float data. Life is made simpler by dealing only with
-float64 data
+Data structures for sparse float data. Life is made simpler by dealing only
+with float64 data
 """
 
 # pylint: disable=E1101,E1103,W0231
@@ -15,6 +15,7 @@ from pandas.util.decorators import deprecate
 
 import pandas.core.common as com
 
+
 class SparsePanelAxis(object):
 
     def __init__(self, cache_field, frame_attr):
@@ -97,7 +98,7 @@ class SparsePanel(Panel):
         self.major_axis = major_axis
         self.minor_axis = minor_axis
 
-    def _consolidate_inplace(self): # pragma: no cover
+    def _consolidate_inplace(self):  # pragma: no cover
         # do nothing when DataFrame calls this method
         pass
 
@@ -135,6 +136,7 @@ class SparsePanel(Panel):
     # need a special property for items to make the field assignable
 
     _items = None
+
     def _get_items(self):
         return self._items
 
@@ -262,7 +264,7 @@ class SparsePanel(Panel):
 
             # values are stacked column-major
             indexer = minor * N + major
-            counts.put(indexer, counts.take(indexer) + 1) # cuteness
+            counts.put(indexer, counts.take(indexer) + 1)  # cuteness
 
             d_values[item] = values
             d_indexer[item] = indexer
@@ -445,6 +447,7 @@ class SparsePanel(Panel):
 
 SparseWidePanel = SparsePanel
 
+
 def _convert_frames(frames, index, columns, fill_value=np.nan, kind='block'):
     from pandas.core.panel import _get_combined_index
     output = {}
diff --git a/pandas/sparse/series.py b/pandas/sparse/series.py
index dfe78a81c..70d356075 100644
--- a/pandas/sparse/series.py
+++ b/pandas/sparse/series.py
@@ -1,6 +1,6 @@
 """
-Data structures for sparse float data. Life is made simpler by dealing only with
-float64 data
+Data structures for sparse float data. Life is made simpler by dealing only
+with float64 data
 """
 
 # pylint: disable=E1101,E1103,W0231
@@ -25,9 +25,10 @@ import pandas._sparse as splib
 
 from pandas.util.decorators import Appender
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Wrapper function for Series arithmetic methods
 
+
 def _sparse_op_wrap(op, name):
     """
     Wrapper function for Series arithmetic operations, to avoid
@@ -49,12 +50,13 @@ def _sparse_op_wrap(op, name):
                                 sparse_index=self.sp_index,
                                 fill_value=new_fill_value,
                                 name=self.name)
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise TypeError('operation with %s not supported' % type(other))
 
     wrapper.__name__ = name
     return wrapper
 
+
 def _sparse_series_op(left, right, op, name):
     left, right = left.align(right, join='outer', copy=False)
     new_index = left.index
@@ -67,6 +69,7 @@ def _sparse_series_op(left, right, op, name):
 
     return result
 
+
 class SparseSeries(SparseArray, Series):
     __array_priority__ = 15
 
@@ -98,7 +101,7 @@ class SparseSeries(SparseArray, Series):
             data = Series(data)
             values, sparse_index = make_sparse(data, kind=kind,
                                                fill_value=fill_value)
-        elif np.isscalar(data): # pragma: no cover
+        elif np.isscalar(data):  # pragma: no cover
             if index is None:
                 raise Exception('must pass index!')
 
@@ -200,7 +203,6 @@ to sparse
         nd_state, own_state = state
         ndarray.__setstate__(self, nd_state)
 
-
         index, fill_value, sp_index = own_state[:3]
         name = None
         if len(own_state) > 3:
@@ -540,5 +542,6 @@ to sparse
         dense_combined = self.to_dense().combine_first(other)
         return dense_combined.to_sparse(fill_value=self.fill_value)
 
+
 class SparseTimeSeries(SparseSeries, TimeSeries):
     pass
diff --git a/pandas/stats/common.py b/pandas/stats/common.py
index 492a7a767..c3034dbc3 100644
--- a/pandas/stats/common.py
+++ b/pandas/stats/common.py
@@ -13,14 +13,14 @@ def _get_cluster_type(cluster_type):
         raise Exception('Unrecognized cluster type: %s' % cluster_type)
 
 _CLUSTER_TYPES = {
-    0 : 'time',
-    1 : 'entity'
+    0: 'time',
+    1: 'entity'
 }
 
 _WINDOW_TYPES = {
-    0 : 'full_sample',
-    1 : 'rolling',
-    2 : 'expanding'
+    0: 'full_sample',
+    1: 'rolling',
+    2: 'expanding'
 }
 
 
@@ -37,6 +37,7 @@ def _get_window_type(window_type):
     else:  # pragma: no cover
         raise Exception('Unrecognized window type: %s' % window_type)
 
+
 def banner(text, width=80):
     """
 
diff --git a/pandas/stats/fama_macbeth.py b/pandas/stats/fama_macbeth.py
index 586642f81..2c8a3a65b 100644
--- a/pandas/stats/fama_macbeth.py
+++ b/pandas/stats/fama_macbeth.py
@@ -6,6 +6,7 @@ from pandas.core.api import Series, DataFrame
 import pandas.stats.common as common
 from pandas.util.decorators import cache_readonly
 
+
 def fama_macbeth(**kwargs):
     """Runs Fama-MacBeth regression.
 
@@ -24,6 +25,7 @@ def fama_macbeth(**kwargs):
 
     return klass(**kwargs)
 
+
 class FamaMacBeth(object):
     def __init__(self, y, x, intercept=True, nw_lags=None,
                  nw_lags_beta=None,
@@ -79,16 +81,16 @@ class FamaMacBeth(object):
     @cache_readonly
     def _results(self):
         return {
-            'mean_beta' : self._mean_beta_raw,
-            'std_beta' : self._std_beta_raw,
-            't_stat' : self._t_stat_raw,
+            'mean_beta': self._mean_beta_raw,
+            'std_beta': self._std_beta_raw,
+            't_stat': self._t_stat_raw,
         }
 
     @cache_readonly
     def _coef_table(self):
         buffer = StringIO()
         buffer.write('%13s %13s %13s %13s %13s %13s\n' %
-            ('Variable','Beta', 'Std Err','t-stat','CI 2.5%','CI 97.5%'))
+            ('Variable', 'Beta', 'Std Err', 't-stat', 'CI 2.5%', 'CI 97.5%'))
         template = '%13s %13.4f %13.4f %13.2f %13.4f %13.4f\n'
 
         for i, name in enumerate(self._cols):
@@ -128,13 +130,14 @@ Formula: Y ~ %(formulaRHS)s
 --------------------------------End of Summary---------------------------------
 """
         params = {
-            'formulaRHS' : ' + '.join(self._cols),
-            'nu' : len(self._beta_raw),
-            'coefTable' : self._coef_table,
+            'formulaRHS': ' + '.join(self._cols),
+            'nu': len(self._beta_raw),
+            'coefTable': self._coef_table,
         }
 
         return template % params
 
+
 class MovingFamaMacBeth(FamaMacBeth):
     def __init__(self, y, x, window_type='rolling', window=10,
                  intercept=True, nw_lags=None, nw_lags_beta=None,
@@ -197,11 +200,12 @@ class MovingFamaMacBeth(FamaMacBeth):
     @cache_readonly
     def _results(self):
         return {
-            'mean_beta' : self._mean_beta_raw[-1],
-            'std_beta' : self._std_beta_raw[-1],
-            't_stat' : self._t_stat_raw[-1],
+            'mean_beta': self._mean_beta_raw[-1],
+            'std_beta': self._std_beta_raw[-1],
+            't_stat': self._t_stat_raw[-1],
         }
 
+
 def _calc_t_stat(beta, nw_lags_beta):
     N = len(beta)
     B = beta - beta.mean(0)
diff --git a/pandas/stats/interface.py b/pandas/stats/interface.py
index 603d3b828..ff87aa1c9 100644
--- a/pandas/stats/interface.py
+++ b/pandas/stats/interface.py
@@ -3,6 +3,7 @@ from pandas.stats.ols import OLS, MovingOLS
 from pandas.stats.plm import PanelOLS, MovingPanelOLS, NonPooledPanelOLS
 import pandas.stats.common as common
 
+
 def ols(**kwargs):
     """Returns the appropriate OLS object depending on whether you need
     simple or panel OLS, and a full-sample or rolling/expanding OLS.
diff --git a/pandas/stats/math.py b/pandas/stats/math.py
index c04843549..1b926fa5e 100644
--- a/pandas/stats/math.py
+++ b/pandas/stats/math.py
@@ -6,6 +6,7 @@ from __future__ import division
 import numpy as np
 import numpy.linalg as linalg
 
+
 def rank(X, cond=1.0e-12):
     """
     Return the rank of a matrix X based on its generalized inverse,
@@ -20,6 +21,7 @@ def rank(X, cond=1.0e-12):
     else:
         return int(not np.alltrue(np.equal(X, 0.)))
 
+
 def solve(a, b):
     """Returns the solution of A X = B."""
     try:
@@ -27,6 +29,7 @@ def solve(a, b):
     except linalg.LinAlgError:
         return np.dot(linalg.pinv(a), b)
 
+
 def inv(a):
     """Returns the inverse of A."""
     try:
@@ -34,10 +37,12 @@ def inv(a):
     except linalg.LinAlgError:
         return np.linalg.pinv(a)
 
+
 def is_psd(m):
     eigvals = linalg.eigvals(m)
     return np.isreal(eigvals).all() and (eigvals >= 0).all()
 
+
 def newey_west(m, max_lags, nobs, df, nw_overlap=False):
     """
     Compute Newey-West adjusted covariance matrix, taking into account
@@ -84,6 +89,7 @@ def newey_west(m, max_lags, nobs, df, nw_overlap=False):
 
     return Xeps
 
+
 def calc_F(R, r, beta, var_beta, nobs, df):
     """
     Computes the standard F-test statistic for linear restriction
@@ -120,4 +126,3 @@ def calc_F(R, r, beta, var_beta, nobs, df):
     p_value = 1 - f.cdf(F, q, nobs - df)
 
     return F, (q, nobs - df), p_value
-
diff --git a/pandas/stats/misc.py b/pandas/stats/misc.py
index 7fc2892ad..e81319cb7 100644
--- a/pandas/stats/misc.py
+++ b/pandas/stats/misc.py
@@ -6,7 +6,7 @@ from pandas.core.series import remove_na
 
 
 def zscore(series):
-    return (series - series.mean()) / np.std(series, ddof = 0)
+    return (series - series.mean()) / np.std(series, ddof=0)
 
 
 def correl_ts(frame1, frame2):
@@ -36,6 +36,7 @@ def correl_ts(frame1, frame2):
 
     return Series(results)
 
+
 def correl_xs(frame1, frame2):
     return correl_ts(frame1.T, frame2.T)
 
@@ -124,6 +125,7 @@ def bucket(series, k, by=None):
 
     return DataFrame(mat, index=series.index, columns=np.arange(k) + 1)
 
+
 def _split_quantile(arr, k):
     arr = np.asarray(arr)
     mask = np.isfinite(arr)
@@ -132,6 +134,7 @@ def _split_quantile(arr, k):
 
     return np.array_split(np.arange(n)[mask].take(order), k)
 
+
 def bucketcat(series, cats):
     """
     Produce DataFrame representing quantiles of a Series
@@ -162,6 +165,7 @@ def bucketcat(series, cats):
 
     return DataFrame(data, columns=unique_labels)
 
+
 def bucketpanel(series, bins=None, by=None, cat=None):
     """
     Bucket data by two Series to create summary panel
@@ -198,7 +202,9 @@ def bucketpanel(series, bins=None, by=None, cat=None):
         xcat, ycat = cat
         return _bucketpanel_cat(series, xcat, ycat)
     else:
-        raise Exception('must specify either values or categories to bucket by')
+        raise Exception('must specify either values or categories '
+                        'to bucket by')
+
 
 def _bucketpanel_by(series, xby, yby, xbins, ybins):
     xby = xby.reindex(series.index)
@@ -229,6 +235,7 @@ def _bucketpanel_by(series, xby, yby, xbins, ybins):
 
     return bucketed.rename(columns=relabel)
 
+
 def _bucketpanel_cat(series, xcat, ycat):
     xlabels, xmapping = _intern(xcat)
     ylabels, ymapping = _intern(ycat)
@@ -256,6 +263,7 @@ def _bucketpanel_cat(series, xcat, ycat):
 
     return result
 
+
 def _intern(values):
     # assumed no NaN values
     values = np.asarray(values)
@@ -273,6 +281,7 @@ def _uniquify(xlabels, ylabels, xbins, ybins):
 
     return _xpiece + _ypiece
 
+
 def _bucket_labels(series, k):
     arr = np.asarray(series)
     mask = np.isfinite(arr)
diff --git a/pandas/stats/moments.py b/pandas/stats/moments.py
index cfdc4aa8a..b805a9dca 100644
--- a/pandas/stats/moments.py
+++ b/pandas/stats/moments.py
@@ -26,7 +26,7 @@ __all__ = ['rolling_count', 'rolling_max', 'rolling_min',
            'expanding_skew', 'expanding_kurt', 'expanding_quantile',
            'expanding_median', 'expanding_apply', 'expanding_corr_pairwise']
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Docs
 
 _doc_template = """
@@ -73,8 +73,8 @@ Notes
 Either center of mass or span must be specified
 
 EWMA is sometimes specified using a "span" parameter s, we have have that the
-decay parameter \alpha is related to the span as :math:`\alpha = 1 - 2 / (s + 1)
-= c / (1 + c)`
+decay parameter \alpha is related to the span as
+:math:`\alpha = 1 - 2 / (s + 1) = c / (1 + c)`
 
 where c is the center of mass. Given a span, the associated center of mass is
 :math:`c = (s - 1) / 2`
@@ -122,6 +122,8 @@ arg2 : Series, DataFrame, or ndarray"""
 _bias_doc = r"""bias : boolean, default False
     Use a standard estimation bias correction
 """
+
+
 def rolling_count(arg, window, freq=None, time_rule=None):
     """
     Rolling count of number of non-NaN observations inside provided window.
@@ -151,6 +153,7 @@ def rolling_count(arg, window, freq=None, time_rule=None):
 
     return return_hook(result)
 
+
 @Substitution("Unbiased moving covariance", _binary_arg_flex, _flex_retval)
 @Appender(_doc_template)
 def rolling_cov(arg1, arg2, window, min_periods=None, time_rule=None):
@@ -161,16 +164,18 @@ def rolling_cov(arg1, arg2, window, min_periods=None, time_rule=None):
         return (mean(X * Y) - mean(X) * mean(Y)) * bias_adj
     return _flex_binary_moment(arg1, arg2, _get_cov)
 
+
 @Substitution("Moving sample correlation", _binary_arg_flex, _flex_retval)
 @Appender(_doc_template)
 def rolling_corr(arg1, arg2, window, min_periods=None, time_rule=None):
     def _get_corr(a, b):
         num = rolling_cov(a, b, window, min_periods, time_rule)
-        den  = (rolling_std(a, window, min_periods, time_rule) *
+        den = (rolling_std(a, window, min_periods, time_rule) *
                 rolling_std(b, window, min_periods, time_rule))
         return num / den
     return _flex_binary_moment(arg1, arg2, _get_corr)
 
+
 def _flex_binary_moment(arg1, arg2, f):
     if isinstance(arg1, np.ndarray) and isinstance(arg2, np.ndarray):
         X, Y = _prep_binary(arg1, arg2)
@@ -197,6 +202,7 @@ def _flex_binary_moment(arg1, arg2, f):
     else:
         return _flex_binary_moment(arg2, arg1, f)
 
+
 def rolling_corr_pairwise(df, window, min_periods=None):
     """
     Computes pairwise rolling correlation matrices as Panel whose items are
@@ -226,6 +232,7 @@ def rolling_corr_pairwise(df, window, min_periods=None):
 
     return Panel.from_dict(all_results).swapaxes('items', 'major')
 
+
 def _rolling_moment(arg, window, func, minp, axis=0, freq=None,
                     time_rule=None, **kwargs):
     """
@@ -255,6 +262,7 @@ def _rolling_moment(arg, window, func, minp, axis=0, freq=None,
 
     return return_hook(result)
 
+
 def _process_data_structure(arg, kill_inf=True):
     if isinstance(arg, DataFrame):
         return_hook = lambda v: type(arg)(v, index=arg.index,
@@ -276,9 +284,10 @@ def _process_data_structure(arg, kill_inf=True):
 
     return return_hook, values
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Exponential moving moments
 
+
 def _get_center_of_mass(com, span):
     if span is not None:
         if com is not None:
@@ -292,6 +301,7 @@ def _get_center_of_mass(com, span):
 
     return float(com)
 
+
 @Substitution("Exponentially-weighted moving average", _unary_arg, "")
 @Appender(_ewm_doc)
 def ewma(arg, com=None, span=None, min_periods=0, freq=None, time_rule=None,
@@ -309,10 +319,12 @@ def ewma(arg, com=None, span=None, min_periods=0, freq=None, time_rule=None,
     output = np.apply_along_axis(_ewma, 0, values)
     return return_hook(output)
 
+
 def _first_valid_index(arr):
     # argmax scans from left
     return notnull(arr).argmax() if len(arr) else 0
 
+
 @Substitution("Exponentially-weighted moving variance", _unary_arg, _bias_doc)
 @Appender(_ewm_doc)
 def ewmvar(arg, com=None, span=None, min_periods=0, bias=False,
@@ -328,6 +340,7 @@ def ewmvar(arg, com=None, span=None, min_periods=0, bias=False,
 
     return result
 
+
 @Substitution("Exponentially-weighted moving std", _unary_arg, _bias_doc)
 @Appender(_ewm_doc)
 def ewmstd(arg, com=None, span=None, min_periods=0, bias=False,
@@ -338,6 +351,7 @@ def ewmstd(arg, com=None, span=None, min_periods=0, bias=False,
 
 ewmvol = ewmstd
 
+
 @Substitution("Exponentially-weighted moving covariance", _binary_arg, "")
 @Appender(_ewm_doc)
 def ewmcov(arg1, arg2, com=None, span=None, min_periods=0, bias=False,
@@ -349,13 +363,14 @@ def ewmcov(arg1, arg2, com=None, span=None, min_periods=0, bias=False,
 
     mean = lambda x: ewma(x, com=com, span=span, min_periods=min_periods)
 
-    result = (mean(X*Y) - mean(X) * mean(Y))
+    result = (mean(X * Y) - mean(X) * mean(Y))
     com = _get_center_of_mass(com, span)
     if not bias:
         result *= (1.0 + 2.0 * com) / (2.0 * com)
 
     return result
 
+
 @Substitution("Exponentially-weighted moving " "correlation", _binary_arg, "")
 @Appender(_ewm_doc)
 def ewmcorr(arg1, arg2, com=None, span=None, min_periods=0,
@@ -368,7 +383,7 @@ def ewmcorr(arg1, arg2, com=None, span=None, min_periods=0,
     mean = lambda x: ewma(x, com=com, span=span, min_periods=min_periods)
     var = lambda x: ewmvar(x, com=com, span=span, min_periods=min_periods,
                            bias=True)
-    return (mean(X*Y) - mean(X)*mean(Y)) / _zsqrt(var(X) * var(Y))
+    return (mean(X * Y) - mean(X) * mean(Y)) / _zsqrt(var(X) * var(Y))
 
 
 def _zsqrt(x):
@@ -384,6 +399,7 @@ def _zsqrt(x):
 
     return result
 
+
 def _prep_binary(arg1, arg2):
     if not isinstance(arg2, type(arg1)):
         raise Exception('Input arrays must be of the same type!')
@@ -397,6 +413,7 @@ def _prep_binary(arg1, arg2):
 #----------------------------------------------------------------------
 # Python interface to Cython functions
 
+
 def _conv_timerule(arg, freq, time_rule):
     if time_rule is not None:
         import warnings
@@ -412,6 +429,7 @@ def _conv_timerule(arg, freq, time_rule):
 
     return arg
 
+
 def _require_min_periods(p):
     def _check_func(minp, window):
         if minp is None:
@@ -420,12 +438,14 @@ def _require_min_periods(p):
             return max(p, minp)
     return _check_func
 
+
 def _use_window(minp, window):
     if minp is None:
         return window
     else:
         return minp
 
+
 def _rolling_func(func, desc, check_minp=_use_window):
     @Substitution(desc, _unary_arg, _type_of_input)
     @Appender(_doc_template)
@@ -455,6 +475,7 @@ rolling_skew = _rolling_func(lib.roll_skew, 'Unbiased moving skewness',
 rolling_kurt = _rolling_func(lib.roll_kurt, 'Unbiased moving kurtosis',
                              check_minp=_require_min_periods(4))
 
+
 def rolling_quantile(arg, window, quantile, min_periods=None, freq=None,
                      time_rule=None):
     """Moving quantile
@@ -480,6 +501,7 @@ def rolling_quantile(arg, window, quantile, min_periods=None, freq=None,
     return _rolling_moment(arg, window, call_cython, min_periods,
                            freq=freq, time_rule=time_rule)
 
+
 def rolling_apply(arg, window, func, min_periods=None, freq=None,
                   time_rule=None):
     """Generic moving function application
diff --git a/pandas/stats/ols.py b/pandas/stats/ols.py
index 0192dced6..d19898990 100644
--- a/pandas/stats/ols.py
+++ b/pandas/stats/ols.py
@@ -21,6 +21,7 @@ import pandas.stats.moments as moments
 
 _FP_ERR = 1e-8
 
+
 class OLS(object):
     """
     Runs a full sample ordinary least squares regression.
@@ -221,7 +222,7 @@ class OLS(object):
             eqs = hypothesis.split(',')
         elif isinstance(hypothesis, list):
             eqs = hypothesis
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise Exception('hypothesis must be either string or list')
         for equation in eqs:
             row = np.zeros(len(x_names))
@@ -438,7 +439,7 @@ class OLS(object):
             else:
                 x = x.fillna(value=fill_value, method=fill_method, axis=axis)
             if isinstance(x, Series):
-                x = DataFrame({'x' : x})
+                x = DataFrame({'x': x})
             if self._intercept:
                 x['intercept'] = 1.
 
@@ -500,10 +501,10 @@ class OLS(object):
         """Returns the formatted results of the OLS as a DataFrame."""
         results = self._results
         beta = results['beta']
-        data = {'beta' : results['beta'],
-                't-stat' : results['t_stat'],
-                'p-value' : results['p_value'],
-                'std err' : results['std_err']}
+        data = {'beta': results['beta'],
+                't-stat': results['t_stat'],
+                'p-value': results['p_value'],
+                'std err': results['std_err']}
         return DataFrame(data, beta.index).T
 
     @cache_readonly
@@ -538,7 +539,7 @@ Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
 
         f_stat = results['f_stat']
 
-        bracketed = ['<%s>' %str(c) for c in results['beta'].index]
+        bracketed = ['<%s>' % str(c) for c in results['beta'].index]
 
         formula = StringIO()
         formula.write(bracketed[0])
@@ -554,21 +555,21 @@ Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
             formula.write(' + ' + coef)
 
         params = {
-            'bannerTop' : scom.banner('Summary of Regression Analysis'),
-            'bannerCoef' : scom.banner('Summary of Estimated Coefficients'),
-            'bannerEnd' : scom.banner('End of Summary'),
-            'formula' : formula.getvalue(),
-            'r2' : results['r2'],
-            'r2_adj' : results['r2_adj'],
-            'nobs' : results['nobs'],
-            'df'  : results['df'],
-            'df_model'  : results['df_model'],
-            'df_resid'  : results['df_resid'],
-            'coef_table' : coef_table,
-            'rmse' : results['rmse'],
-            'f_stat' : f_stat['f-stat'],
-            'f_stat_shape' : '(%d, %d)' % (f_stat['DF X'], f_stat['DF Resid']),
-            'f_stat_p_value' : f_stat['p-value'],
+            'bannerTop': scom.banner('Summary of Regression Analysis'),
+            'bannerCoef': scom.banner('Summary of Estimated Coefficients'),
+            'bannerEnd': scom.banner('End of Summary'),
+            'formula': formula.getvalue(),
+            'r2': results['r2'],
+            'r2_adj': results['r2_adj'],
+            'nobs': results['nobs'],
+            'df': results['df'],
+            'df_model': results['df_model'],
+            'df_resid': results['df_resid'],
+            'coef_table': coef_table,
+            'rmse': results['rmse'],
+            'f_stat': f_stat['f-stat'],
+            'f_stat_shape': '(%d, %d)' % (f_stat['DF X'], f_stat['DF Resid']),
+            'f_stat_p_value': f_stat['p-value'],
         }
 
         return template % params
@@ -576,7 +577,6 @@ Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
     def __repr__(self):
         return self.summary
 
-
     @cache_readonly
     def _time_obs_count(self):
         # XXX
@@ -630,7 +630,7 @@ class MovingOLS(OLS):
         self._window = int(window)
         self._min_periods = min_periods
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # "Public" results
 
     @cache_readonly
@@ -745,7 +745,7 @@ class MovingOLS(OLS):
         return Series(self._y_predict_raw[self._valid_obs_labels],
                       index=self._result_index)
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # "raw" attributes, calculations
 
     @property
@@ -833,9 +833,10 @@ class MovingOLS(OLS):
         slicer = lambda df, dt: df.truncate(dt, dt).values
         if not self._panel_model:
             _get_index = x.index.get_loc
+
             def slicer(df, dt):
                 i = _get_index(dt)
-                return df.values[i:i+1, :]
+                return df.values[i:i + 1, :]
 
         last = np.zeros((K, K))
 
@@ -858,9 +859,10 @@ class MovingOLS(OLS):
         x_slicer = lambda df, dt: df.truncate(dt, dt).values
         if not self._panel_model:
             _get_index = x.index.get_loc
+
             def x_slicer(df, dt):
                 i = _get_index(dt)
-                return df.values[i:i+1]
+                return df.values[i:i + 1]
 
         _y_get_index = y.index.get_loc
         _values = y.values
@@ -871,7 +873,7 @@ class MovingOLS(OLS):
         else:
             def y_slicer(df, dt):
                 i = _y_get_index(dt)
-                return _values[i:i+1]
+                return _values[i:i + 1]
 
         last = np.zeros(len(x.columns))
         for i, date in enumerate(dates):
@@ -996,7 +998,7 @@ class MovingOLS(OLS):
                                                      after=date))
                 weights_slice = weights.truncate(prior_date, date)
                 demeaned = Y_slice - np.average(Y_slice, weights=weights_slice)
-                SS_total = (weights_slice*demeaned**2).sum()
+                SS_total = (weights_slice * demeaned ** 2).sum()
             else:
                 SS_total = ((Y_slice - Y_slice.mean()) ** 2).sum()
 
@@ -1008,9 +1010,9 @@ class MovingOLS(OLS):
             uncentered_sst.append(SST_uncentered)
 
         return {
-            'sse' : np.array(sse),
-            'centered_tss' : np.array(sst),
-            'uncentered_tss' : np.array(uncentered_sst),
+            'sse': np.array(sse),
+            'centered_tss': np.array(sst),
+            'uncentered_tss': np.array(uncentered_sst),
         }
 
     @cache_readonly
@@ -1166,7 +1168,7 @@ class MovingOLS(OLS):
                 value = value[self.beta.index[-1]]
             elif isinstance(value, DataFrame):
                 value = value.xs(self.beta.index[-1])
-            else: # pragma: no cover
+            else:  # pragma: no cover
                 raise Exception('Problem retrieving %s' % result)
             results[result] = value
 
@@ -1226,6 +1228,7 @@ class MovingOLS(OLS):
         return self._nobs_raw >= max(self._min_periods,
                                      len(self._x.columns) + 1)
 
+
 def _safe_update(d, other):
     """
     Combine dictionaries with non-overlapping keys
@@ -1236,6 +1239,7 @@ def _safe_update(d, other):
 
         d[k] = v
 
+
 def _filter_data(lhs, rhs, weights=None):
     """
     Cleans the input for single OLS.
@@ -1257,7 +1261,7 @@ def _filter_data(lhs, rhs, weights=None):
         lhs = Series(lhs, index=rhs.index)
 
     rhs = _combine_rhs(rhs)
-    lhs = DataFrame({'__y__' : lhs}, dtype=float)
+    lhs = DataFrame({'__y__': lhs}, dtype=float)
     pre_filt_rhs = rhs.dropna(how='any')
 
     combined = rhs.join(lhs, how='outer')
@@ -1294,12 +1298,12 @@ def _combine_rhs(rhs):
     elif isinstance(rhs, dict):
         for name, value in rhs.iteritems():
             if isinstance(value, Series):
-                _safe_update(series, {name : value})
+                _safe_update(series, {name: value})
             elif isinstance(value, (dict, DataFrame)):
                 _safe_update(series, value)
-            else: # pragma: no cover
+            else:  # pragma: no cover
                 raise Exception('Invalid RHS data type: %s' % type(value))
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise Exception('Invalid RHS type: %s' % type(rhs))
 
     if not isinstance(series, DataFrame):
@@ -1311,7 +1315,7 @@ def _combine_rhs(rhs):
 # MovingOLS and MovingPanelOLS
 def _y_converter(y):
     y = y.values.squeeze()
-    if y.ndim == 0: # pragma: no cover
+    if y.ndim == 0:  # pragma: no cover
         return np.array([y])
     else:
         return y
@@ -1327,4 +1331,3 @@ def f_stat_to_dict(result):
     result['p-value'] = p_value
 
     return result
-
diff --git a/pandas/stats/plm.py b/pandas/stats/plm.py
index 7b6f85b12..7dde37822 100644
--- a/pandas/stats/plm.py
+++ b/pandas/stats/plm.py
@@ -20,6 +20,7 @@ import pandas.stats.common as com
 import pandas.stats.math as math
 from pandas.util.decorators import cache_readonly
 
+
 class PanelOLS(OLS):
     """Implements panel OLS.
 
@@ -54,7 +55,7 @@ class PanelOLS(OLS):
         self._T = len(self._index)
 
     def log(self, msg):
-        if self._verbose: # pragma: no cover
+        if self._verbose:  # pragma: no cover
             print msg
 
     def _prepare_data(self):
@@ -268,7 +269,7 @@ class PanelOLS(OLS):
                 else:
                     to_exclude = mapped_name = dummies.columns[0]
 
-                if mapped_name not in dummies.columns: # pragma: no cover
+                if mapped_name not in dummies.columns:  # pragma: no cover
                     raise Exception('%s not in %s' % (to_exclude,
                                                       dummies.columns))
 
@@ -337,7 +338,7 @@ class PanelOLS(OLS):
         if self._use_centered_tss:
             SST = ((Y - np.mean(Y)) ** 2).sum()
         else:
-            SST = (Y**2).sum()
+            SST = (Y ** 2).sum()
 
         return 1 - SSE / SST
 
@@ -427,6 +428,7 @@ class PanelOLS(OLS):
     def _nobs(self):
         return len(self._y)
 
+
 def _convertDummies(dummies, mapping):
     # cleans up the names of the generated dummies
     new_items = []
@@ -446,6 +448,7 @@ def _convertDummies(dummies, mapping):
 
     return dummies
 
+
 def _is_numeric(df):
     for col in df:
         if df[col].dtype.name == 'object':
@@ -453,6 +456,7 @@ def _is_numeric(df):
 
     return True
 
+
 def add_intercept(panel, name='intercept'):
     """
     Add column of ones to input panel
@@ -471,6 +475,7 @@ def add_intercept(panel, name='intercept'):
 
     return panel.consolidate()
 
+
 class MovingPanelOLS(MovingOLS, PanelOLS):
     """Implements rolling/expanding panel OLS.
 
@@ -648,13 +653,14 @@ class MovingPanelOLS(MovingOLS, PanelOLS):
         # TODO: write unit tests for this
 
         rank_threshold = len(self._x.columns) + 1
-        if self._min_obs < rank_threshold: # pragma: no cover
+        if self._min_obs < rank_threshold:  # pragma: no cover
             warnings.warn('min_obs is smaller than rank of X matrix')
 
         enough_observations = self._nobs_raw >= self._min_obs
         enough_time_periods = self._window_time_obs >= self._min_periods
         return enough_time_periods & enough_observations
 
+
 def create_ols_dict(attr):
     def attr_getter(self):
         d = {}
@@ -666,9 +672,11 @@ def create_ols_dict(attr):
 
     return attr_getter
 
+
 def create_ols_attr(attr):
     return property(create_ols_dict(attr))
 
+
 class NonPooledPanelOLS(object):
     """Implements non-pooled panel OLS.
 
@@ -774,6 +782,7 @@ def _var_beta_panel(y, x, beta, xx, rmse, cluster_axis,
 
         return np.dot(xx_inv, np.dot(xox, xx_inv))
 
+
 def _xx_time_effects(x, y):
     """
     Returns X'X - (X'T) (T'T)^-1 (T'X)
@@ -790,5 +799,3 @@ def _xx_time_effects(x, y):
     count = count[selector]
 
     return xx - np.dot(xt.T / count, xt)
-
-
diff --git a/pandas/stats/var.py b/pandas/stats/var.py
index e2b5a2ce7..a4eb8920a 100644
--- a/pandas/stats/var.py
+++ b/pandas/stats/var.py
@@ -10,6 +10,7 @@ import pandas.stats.common as common
 from pandas.stats.math import inv
 from pandas.stats.ols import _combine_rhs
 
+
 class VAR(object):
     """
     Estimates VAR(p) regression on multivariate time series data
@@ -164,8 +165,8 @@ class VAR(object):
         p_value_mat = DataFrame(p_value_dict)
 
         return {
-            'f-stat' : f_stat_mat,
-            'p-value' : p_value_mat,
+            'f-stat': f_stat_mat,
+            'p-value': p_value_mat,
         }
 
     @cache_readonly
@@ -226,13 +227,13 @@ BIC:                            %(bic).3f
 %(banner_end)s
 """
         params = {
-            'banner_top' : common.banner('Summary of VAR'),
-            'banner_coef' : common.banner('Summary of Estimated Coefficients'),
-            'banner_end' : common.banner('End of Summary'),
-            'coef_table' : self.beta,
-            'aic' : self.aic,
-            'bic' : self.bic,
-            'nobs' : self._nobs,
+            'banner_top': common.banner('Summary of VAR'),
+            'banner_coef': common.banner('Summary of Estimated Coefficients'),
+            'banner_end': common.banner('End of Summary'),
+            'coef_table': self.beta,
+            'aic': self.aic,
+            'bic': self.bic,
+            'nobs': self._nobs,
         }
 
         return template % params
@@ -410,8 +411,8 @@ BIC:                            %(bic).3f
         k = self._p * (self._k * self._p + 1)
         n = self._nobs * self._k
 
-        return {'aic' : 2 * k + n * np.log(RSS / n),
-                'bic' : n * np.log(RSS / n) + k * np.log(n)}
+        return {'aic': 2 * k + n * np.log(RSS / n),
+                'bic': n * np.log(RSS / n) + k * np.log(n)}
 
     @cache_readonly
     def _k(self):
@@ -478,6 +479,7 @@ BIC:                            %(bic).3f
     def __repr__(self):
         return self.summary
 
+
 def lag_select(data, max_lags=5, ic=None):
     """
     Select number of lags based on a variety of information criteria
@@ -496,6 +498,7 @@ def lag_select(data, max_lags=5, ic=None):
     """
     pass
 
+
 class PanelVAR(VAR):
     """
     Performs Vector Autoregression on panel data.
@@ -567,14 +570,17 @@ def _prep_panel_data(data):
 
     return Panel.fromDict(data)
 
+
 def _drop_incomplete_rows(array):
     mask = np.isfinite(array).all(1)
     indices = np.arange(len(array))[mask]
     return array.take(indices, 0)
 
+
 def _make_param_name(lag, name):
     return 'L%d.%s' % (lag, name)
 
+
 def chain_dot(*matrices):
     """
     Returns the dot product of the given matrices.
diff --git a/pandas/tools/describe.py b/pandas/tools/describe.py
index 43e3051da..eca5a800b 100644
--- a/pandas/tools/describe.py
+++ b/pandas/tools/describe.py
@@ -1,5 +1,6 @@
 from pandas.core.series import Series
 
+
 def value_range(df):
     """
     Return the minimum and maximum of a dataframe in a series object
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index 4a50016c3..d92ed1cb0 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -21,6 +21,7 @@ import pandas.core.common as com
 
 import pandas.lib as lib
 
+
 @Substitution('\nleft : DataFrame')
 @Appender(_merge_doc, indents=0)
 def merge(left, right, how='inner', on=None, left_on=None, right_on=None,
@@ -145,6 +146,7 @@ def ordered_merge(left, right, on=None, left_by=None, right_by=None,
 # TODO: transformations??
 # TODO: only copy DataFrames when modification necessary
 
+
 class _MergeOperation(object):
     """
     Perform a database (SQL) merge operation between two DataFrame objects
@@ -182,7 +184,8 @@ class _MergeOperation(object):
         # this is a bit kludgy
         ldata, rdata = self._get_merge_data()
 
-        # TODO: more efficiently handle group keys to avoid extra consolidation!
+        # TODO: more efficiently handle group keys to avoid extra
+        #       consolidation!
         join_op = _BlockJoinOperation([ldata, rdata], join_index,
                                       [left_indexer, right_indexer], axis=1,
                                       copy=self.copy)
@@ -427,7 +430,7 @@ def _get_join_indexers(left_keys, right_keys, sort=False, how='inner'):
     for x in group_sizes:
         max_groups *= long(x)
 
-    if max_groups > 2**63:  # pragma: no cover
+    if max_groups > 2 ** 63:  # pragma: no cover
         raise MergeError('Combinatorial explosion! (boom)')
 
     left_group_key, right_group_key, max_groups = \
@@ -437,7 +440,6 @@ def _get_join_indexers(left_keys, right_keys, sort=False, how='inner'):
     return join_func(left_group_key, right_group_key, max_groups)
 
 
-
 class _OrderedMerge(_MergeOperation):
 
     def __init__(self, left, right, on=None, by=None, left_on=None,
@@ -452,10 +454,9 @@ class _OrderedMerge(_MergeOperation):
                                  left_index=left_index,
                                  right_index=right_index,
                                  how='outer', suffixes=suffixes,
-                                 sort=True # sorts when factorizing
+                                 sort=True  # sorts when factorizing
                                  )
 
-
     def get_result(self):
         join_index, left_indexer, right_indexer = self._get_join_info()
 
@@ -503,6 +504,7 @@ def _get_multiindex_indexer(join_keys, index, sort=False):
 
     return left_indexer, right_indexer
 
+
 def _get_single_indexer(join_key, index, sort=False):
     left_key, right_key, count = _factorize_keys(join_key, index, sort=sort)
 
@@ -513,6 +515,7 @@ def _get_single_indexer(join_key, index, sort=False):
 
     return left_indexer, right_indexer
 
+
 def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):
     join_index = left_ax
     left_indexer = None
@@ -544,10 +547,10 @@ def _right_outer_join(x, y, max_groups):
     return left_indexer, right_indexer
 
 _join_functions = {
-    'inner' : lib.inner_join,
-    'left' : lib.left_outer_join,
-    'right' : _right_outer_join,
-    'outer' : lib.full_outer_join,
+    'inner': lib.inner_join,
+    'left': lib.left_outer_join,
+    'right': _right_outer_join,
+    'outer': lib.full_outer_join,
 }
 
 
@@ -584,6 +587,7 @@ def _factorize_keys(lk, rk, sort=True):
 
     return llab, rlab, count
 
+
 def _sort_labels(uniques, left, right):
     if not isinstance(uniques, np.ndarray):
         # tuplesafe
@@ -602,6 +606,7 @@ def _sort_labels(uniques, left, right):
 
     return new_left, new_right
 
+
 class _BlockJoinOperation(object):
     """
     BlockJoinOperation made generic for N DataFrames
@@ -713,7 +718,6 @@ class _BlockJoinOperation(object):
         return make_block(out, new_block_items, self.result_items)
 
 
-
 class _JoinUnit(object):
     """
     Blocks plus indexer
@@ -762,6 +766,7 @@ class _JoinUnit(object):
         result.ref_items = ref_items
         return result
 
+
 def _may_need_upcasting(blocks):
     for block in blocks:
         if isinstance(block, (IntBlock, BoolBlock)):
@@ -788,18 +793,21 @@ def _upcast_blocks(blocks):
     # use any ref_items
     return _consolidate(new_blocks, newb.ref_items)
 
+
 def _get_all_block_kinds(blockmaps):
     kinds = set()
     for mapping in blockmaps:
         kinds |= set(mapping)
     return kinds
 
+
 def _get_merge_block_kinds(blockmaps):
     kinds = set()
     for _, mapping in blockmaps:
         kinds |= set(mapping)
     return kinds
 
+
 def _get_block_dtype(blocks):
     if len(blocks) == 0:
         return object
@@ -816,6 +824,7 @@ def _get_block_dtype(blocks):
 #----------------------------------------------------------------------
 # Concatenate DataFrame objects
 
+
 def concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,
            keys=None, levels=None, names=None, verify_integrity=False):
     """
@@ -884,8 +893,8 @@ class _Concatenator(object):
         elif join == 'inner':
             self.intersect = True
         else:  # pragma: no cover
-            raise ValueError('Only can inner (intersect) or outer (union) join '
-                             'the other axis')
+            raise ValueError('Only can inner (intersect) or outer (union) '
+                             'join the other axis')
 
         if isinstance(objs, dict):
             if keys is None:
@@ -1149,6 +1158,7 @@ class _Concatenator(object):
 def _concat_indexes(indexes):
     return indexes[0].append(indexes[1:])
 
+
 def _make_concat_multiindex(indexes, keys, levels=None, names=None):
     if ((levels is None and isinstance(keys[0], tuple)) or
         (levels is not None and len(levels) > 1)):
@@ -1250,6 +1260,5 @@ def _should_fill(lname, rname):
     return lname == rname
 
 
-
 def _any(x):
     return x is not None and len(x) > 0 and any([y is not None for y in x])
diff --git a/pandas/tools/pivot.py b/pandas/tools/pivot.py
index 146cba827..bed1fe221 100644
--- a/pandas/tools/pivot.py
+++ b/pandas/tools/pivot.py
@@ -126,6 +126,7 @@ def pivot_table(data, values=None, rows=None, cols=None, aggfunc='mean',
 
 DataFrame.pivot_table = pivot_table
 
+
 def _add_margins(table, data, values, rows=None, cols=None, aggfunc=np.mean):
     grand_margin = {}
     for k, v in data[values].iteritems():
@@ -142,7 +143,6 @@ def _add_margins(table, data, values, rows=None, cols=None, aggfunc=np.mean):
         table_pieces = []
         margin_keys = []
 
-
         def _all_key(key):
             return (key, 'All') + ('',) * (len(cols) - 1)
 
@@ -199,6 +199,7 @@ def _add_margins(table, data, values, rows=None, cols=None, aggfunc=np.mean):
 
     return result
 
+
 def _convert_by(by):
     if by is None:
         by = []
@@ -209,6 +210,7 @@ def _convert_by(by):
         by = list(by)
     return by
 
+
 def crosstab(rows, cols, values=None, rownames=None, colnames=None,
              aggfunc=None, margins=False):
     """
@@ -284,6 +286,7 @@ def crosstab(rows, cols, values=None, rownames=None, colnames=None,
                                aggfunc=aggfunc, margins=margins)
         return table
 
+
 def _get_names(arrs, names, prefix='row'):
     if names is None:
         names = []
diff --git a/pandas/tools/plotting.py b/pandas/tools/plotting.py
index 34754a23b..a2cc21e23 100644
--- a/pandas/tools/plotting.py
+++ b/pandas/tools/plotting.py
@@ -15,14 +15,15 @@ from pandas.tseries.period import PeriodIndex
 from pandas.tseries.frequencies import get_period_alias, get_base_alias
 from pandas.tseries.offsets import DateOffset
 
-try: # mpl optional
+try:  # mpl optional
     import pandas.tseries.converter as conv
     conv.register()
 except ImportError:
     pass
 
+
 def _get_standard_kind(kind):
-    return {'density' : 'kde'}.get(kind, kind)
+    return {'density': 'kde'}.get(kind, kind)
 
 
 def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,
@@ -120,8 +121,8 @@ def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,
             # ax.grid(b=grid)
 
     axes[0, 0].yaxis.set_visible(False)
-    axes[n-1, n-1].xaxis.set_visible(False)
-    axes[n-1, n-1].yaxis.set_visible(False)
+    axes[n - 1, n - 1].xaxis.set_visible(False)
+    axes[n - 1, n - 1].yaxis.set_visible(False)
     axes[0, n - 1].yaxis.tick_right()
 
     for ax in axes.flat:
@@ -135,10 +136,12 @@ def _gca():
     import matplotlib.pyplot as plt
     return plt.gca()
 
+
 def _gcf():
     import matplotlib.pyplot as plt
     return plt.gcf()
 
+
 def _get_marker_compat(marker):
     import matplotlib.lines as mlines
     import matplotlib as mpl
@@ -148,6 +151,7 @@ def _get_marker_compat(marker):
         return 'o'
     return marker
 
+
 def radviz(frame, class_column, ax=None, **kwds):
     """RadViz - a multivariate data visualization algorithm
 
@@ -232,6 +236,7 @@ def radviz(frame, class_column, ax=None, **kwds):
     ax.axis('equal')
     return ax
 
+
 def andrews_curves(data, class_column, ax=None, samples=200):
     """
     Parameters:
@@ -243,6 +248,7 @@ def andrews_curves(data, class_column, ax=None, samples=200):
     from math import sqrt, pi, sin, cos
     import matplotlib.pyplot as plt
     import random
+
     def function(amplitudes):
         def f(x):
             x1 = amplitudes[0]
@@ -256,6 +262,7 @@ def andrews_curves(data, class_column, ax=None, samples=200):
                 result += amplitudes[-1] * sin(harmonic * x)
             return result
         return f
+
     def random_color(column):
         random.seed(column)
         return [random.random() for _ in range(3)]
@@ -280,6 +287,7 @@ def andrews_curves(data, class_column, ax=None, samples=200):
     ax.grid()
     return ax
 
+
 def bootstrap_plot(series, fig=None, size=50, samples=500, **kwds):
     """Bootstrap plot.
 
@@ -289,7 +297,8 @@ def bootstrap_plot(series, fig=None, size=50, samples=500, **kwds):
     fig: matplotlib figure object, optional
     size: number of data points to consider during each sampling
     samples: number of times the bootstrap procedure is performed
-    kwds: optional keyword arguments for plotting commands, must be accepted by both hist and plot
+    kwds: optional keyword arguments for plotting commands, must be accepted
+        by both hist and plot
 
     Returns:
     --------
@@ -336,6 +345,7 @@ def bootstrap_plot(series, fig=None, size=50, samples=500, **kwds):
         plt.setp(axis.get_yticklabels(), fontsize=8)
     return fig
 
+
 def parallel_coordinates(data, class_column, cols=None, ax=None, **kwds):
     """Parallel coordinates plotting.
 
@@ -353,6 +363,7 @@ def parallel_coordinates(data, class_column, cols=None, ax=None, **kwds):
     """
     import matplotlib.pyplot as plt
     import random
+
     def random_color(column):
         random.seed(column)
         return [random.random() for _ in range(3)]
@@ -392,6 +403,7 @@ def parallel_coordinates(data, class_column, cols=None, ax=None, **kwds):
     ax.grid()
     return ax
 
+
 def lag_plot(series, ax=None, **kwds):
     """Lag plot for time series.
 
@@ -416,6 +428,7 @@ def lag_plot(series, ax=None, **kwds):
     ax.scatter(y1, y2, **kwds)
     return ax
 
+
 def autocorrelation_plot(series, ax=None):
     """Autocorrelation plot for time series.
 
@@ -435,23 +448,25 @@ def autocorrelation_plot(series, ax=None):
         ax = plt.gca(xlim=(1, n), ylim=(-1.0, 1.0))
     mean = np.mean(data)
     c0 = np.sum((data - mean) ** 2) / float(n)
+
     def r(h):
         return ((data[:n - h] - mean) * (data[h:] - mean)).sum() / float(n) / c0
     x = np.arange(n) + 1
     y = map(r, x)
     z95 = 1.959963984540054
     z99 = 2.5758293035489004
-    ax.axhline(y=z99/np.sqrt(n), linestyle='--', color='grey')
-    ax.axhline(y=z95/np.sqrt(n), color='grey')
+    ax.axhline(y=z99 / np.sqrt(n), linestyle='--', color='grey')
+    ax.axhline(y=z95 / np.sqrt(n), color='grey')
     ax.axhline(y=0.0, color='black')
-    ax.axhline(y=-z95/np.sqrt(n), color='grey')
-    ax.axhline(y=-z99/np.sqrt(n), linestyle='--', color='grey')
+    ax.axhline(y=-z95 / np.sqrt(n), color='grey')
+    ax.axhline(y=-z99 / np.sqrt(n), linestyle='--', color='grey')
     ax.set_xlabel("Lag")
     ax.set_ylabel("Autocorrelation")
     ax.plot(x, y)
     ax.grid()
     return ax
 
+
 def grouped_hist(data, column=None, by=None, ax=None, bins=50, log=False,
                  figsize=None, layout=None, sharex=False, sharey=False,
                  rot=90):
@@ -590,7 +605,7 @@ class MPLPlot(object):
             orig_ax, new_ax = ax, ax.twinx()
             orig_ax.right_ax, new_ax.left_ax = new_ax, orig_ax
 
-            if len(orig_ax.get_lines()) == 0: # no data on left y
+            if len(orig_ax.get_lines()) == 0:  # no data on left y
                 orig_ax.get_yaxis().set_visible(False)
 
             if len(new_ax.get_lines()) == 0:
@@ -795,6 +810,7 @@ class MPLPlot(object):
 
         return style or None
 
+
 class KdePlot(MPLPlot):
     def __init__(self, data, **kwargs):
         MPLPlot.__init__(self, data, **kwargs)
@@ -830,6 +846,7 @@ class KdePlot(MPLPlot):
             for ax in self.axes:
                 ax.legend(loc='best')
 
+
 class LinePlot(MPLPlot):
 
     def __init__(self, data, **kwargs):
@@ -861,9 +878,9 @@ class LinePlot(MPLPlot):
 
         ax = self._get_ax(0)
         ax_freq = getattr(ax, 'freq', None)
-        if freq is None: # convert irregular if axes has freq info
+        if freq is None:  # convert irregular if axes has freq info
             freq = ax_freq
-        else: # do not use tsplot if irregular was plotted first
+        else:  # do not use tsplot if irregular was plotted first
             if (ax_freq is None) and (len(ax.get_lines()) > 0):
                 return False
 
@@ -887,6 +904,7 @@ class LinePlot(MPLPlot):
             x = self._get_xticks(convert_period=True)
 
             has_colors, colors = self._get_colors()
+
             def _maybe_add_color(kwargs, style, i):
                 if (not has_colors and
                     (style is None or re.match('[a-z]+', style) is None)
@@ -945,14 +963,14 @@ class LinePlot(MPLPlot):
             return label
 
         if isinstance(data, Series):
-            ax = self._get_ax(0) #self.axes[0]
+            ax = self._get_ax(0)  # self.axes[0]
             style = self.style or ''
             label = com.pprint_thing(self.label)
             kwds = kwargs.copy()
             _maybe_add_color(kwds, style, 0)
 
-            newlines = tsplot(data, plotf, ax=ax, label=label, style=self.style,
-                             **kwds)
+            newlines = tsplot(data, plotf, ax=ax, label=label,
+                              style=self.style, **kwds)
             ax.grid(self.grid)
             lines.append(newlines[0])
             leg_label = to_leg_label(label, 0)
@@ -1059,7 +1077,7 @@ class LinePlot(MPLPlot):
 
 class BarPlot(MPLPlot):
 
-    _default_rot = {'bar' : 90, 'barh' : 0}
+    _default_rot = {'bar': 90, 'barh': 0}
 
     def __init__(self, data, **kwargs):
         self.stacked = kwargs.pop('stacked', False)
@@ -1088,7 +1106,7 @@ class BarPlot(MPLPlot):
         rects = []
         labels = []
 
-        ax = self._get_ax(0) #self.axes[0]
+        ax = self._get_ax(0)  # self.axes[0]
 
         bar_f = self.bar_f
 
@@ -1102,7 +1120,7 @@ class BarPlot(MPLPlot):
             kwds['color'] = colors[i % len(colors)]
 
             if self.subplots:
-                ax = self._get_ax(i) #self.axes[i]
+                ax = self._get_ax(i)  # self.axes[i]
                 rect = bar_f(ax, self.ax_pos, y, 0.5, start=pos_prior, **kwds)
                 ax.set_title(label)
             elif self.stacked:
@@ -1149,6 +1167,7 @@ class BarPlot(MPLPlot):
         #if self.subplots and self.legend:
         #    self.axes[0].legend(loc='best')
 
+
 class BoxPlot(MPLPlot):
     pass
 
@@ -1159,9 +1178,10 @@ class HistPlot(MPLPlot):
 
 def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,
                sharey=False, use_index=True, figsize=None, grid=False,
-               legend=True, rot=None, ax=None, style=None, title=None, xlim=None,
-               ylim=None, logy=False, xticks=None, yticks=None, kind='line',
-               sort_columns=False, fontsize=None, secondary_y=False, **kwds):
+               legend=True, rot=None, ax=None, style=None, title=None,
+               xlim=None, ylim=None, logy=False, xticks=None, yticks=None,
+               kind='line', sort_columns=False, fontsize=None,
+               secondary_y=False, **kwds):
 
     """
     Make line or bar plot of DataFrame's series with the index on the x-axis
@@ -1255,6 +1275,7 @@ def plot_frame(frame=None, x=None, y=None, subplots=False, sharex=True,
     else:
         return plot_obj.axes[0]
 
+
 def plot_series(series, label=None, kind='line', use_index=True, rot=None,
                 xticks=None, yticks=None, xlim=None, ylim=None,
                 ax=None, style=None, grid=None, logy=False, secondary_y=False,
@@ -1330,6 +1351,7 @@ def plot_series(series, label=None, kind='line', use_index=True, rot=None,
 
     return plot_obj.ax
 
+
 def boxplot(data, column=None, by=None, ax=None, fontsize=None,
             rot=0, grid=True, figsize=None, **kwds):
     """
@@ -1354,7 +1376,7 @@ def boxplot(data, column=None, by=None, ax=None, fontsize=None,
     """
     from pandas import Series, DataFrame
     if isinstance(data, Series):
-        data = DataFrame({'x' : data})
+        data = DataFrame({'x': data})
         column = 'x'
 
     def plot_group(grouped, ax):
@@ -1411,6 +1433,7 @@ def boxplot(data, column=None, by=None, ax=None, fontsize=None,
     fig.subplots_adjust(bottom=0.15, top=0.9, left=0.1, right=0.9, wspace=0.2)
     return ret
 
+
 def format_date_labels(ax, rot):
     # mini version of autofmt_xdate
     try:
@@ -1419,7 +1442,7 @@ def format_date_labels(ax, rot):
             label.set_rotation(rot)
         fig = ax.get_figure()
         fig.subplots_adjust(bottom=0.2)
-    except Exception: # pragma: no cover
+    except Exception:  # pragma: no cover
         pass
 
 
@@ -1515,6 +1538,7 @@ def hist_frame(data, grid=True, xlabelsize=None, xrot=None,
 
     return axes
 
+
 def hist_series(self, ax=None, grid=True, xlabelsize=None, xrot=None,
                 ylabelsize=None, yrot=None, **kwds):
     """
@@ -1563,6 +1587,7 @@ def hist_series(self, ax=None, grid=True, xlabelsize=None, xrot=None,
 
     return ax
 
+
 def boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None,
                           rot=0, grid=True, figsize=None, **kwds):
     """
@@ -1628,6 +1653,7 @@ def boxplot_frame_groupby(grouped, subplots=True, column=None, fontsize=None,
                          grid=grid, figsize=figsize, **kwds)
     return ret
 
+
 def _grouped_plot(plotf, data, column=None, by=None, numeric_only=True,
                   figsize=None, sharex=True, sharey=True, layout=None,
                   rot=0, ax=None):
@@ -1672,6 +1698,7 @@ def _grouped_plot(plotf, data, column=None, by=None, numeric_only=True,
 
     return fig, axes
 
+
 def _grouped_plot_by_column(plotf, data, columns=None, by=None,
                             numeric_only=True, grid=False,
                             figsize=None, ax=None):
@@ -1710,6 +1737,7 @@ def _grouped_plot_by_column(plotf, data, columns=None, by=None,
 
     return fig, axes
 
+
 def _get_layout(nplots):
     if nplots == 1:
         return (1, 1)
@@ -1729,6 +1757,7 @@ def _get_layout(nplots):
 
 # copied from matplotlib/pyplot.py for compatibility with matplotlib < 1.0
 
+
 def _subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,
               subplot_kw=None, ax=None, secondary_y=False, data=None,
               **fig_kw):
@@ -1817,7 +1846,7 @@ def _subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,
 
     # Create empty object array to hold all axes.  It's easiest to make it 1-d
     # so we can just append subplots upon creation, and then
-    nplots = nrows*ncols
+    nplots = nrows * ncols
     axarr = np.empty(nplots, dtype=object)
 
     def on_right(i):
@@ -1854,18 +1883,18 @@ def _subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,
     if nplots > 1:
         if sharex and nrows > 1:
             for i, ax in enumerate(axarr):
-                if np.ceil(float(i + 1) / ncols) < nrows: # only last row
+                if np.ceil(float(i + 1) / ncols) < nrows:  # only last row
                     [label.set_visible(False) for label in ax.get_xticklabels()]
         if sharey and ncols > 1:
             for i, ax in enumerate(axarr):
-                if (i % ncols) != 0: # only first column
+                if (i % ncols) != 0:  # only first column
                     [label.set_visible(False) for label in ax.get_yticklabels()]
 
     if squeeze:
         # Reshape the array to have the final desired dimension (nrow,ncol),
         # though discarding unneeded dimensions that equal 1.  If we only have
         # one subplot, just return it instead of a 1-element array.
-        if nplots==1:
+        if nplots == 1:
             axes = axarr[0]
         else:
             axes = axarr.reshape(nrows, ncols).squeeze()
diff --git a/pandas/tools/tile.py b/pandas/tools/tile.py
index 7a2101a96..cc4c4192c 100644
--- a/pandas/tools/tile.py
+++ b/pandas/tools/tile.py
@@ -66,7 +66,7 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,
     if not np.iterable(bins):
         if np.isscalar(bins) and bins < 1:
             raise ValueError("`bins` should be a positive integer.")
-        try: # for array-like
+        try:  # for array-like
             sz = x.size
         except AttributeError:
             x = np.asarray(x)
@@ -79,13 +79,13 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,
             rng = (nanops.nanmin(x), nanops.nanmax(x))
         mn, mx = [mi + 0.0 for mi in rng]
 
-        if mn == mx: # adjust end points before binning
+        if mn == mx:  # adjust end points before binning
             mn -= .001 * mn
             mx += .001 * mx
-            bins = np.linspace(mn, mx, bins+1, endpoint=True)
-        else: # adjust end points after binning
-            bins = np.linspace(mn, mx, bins+1, endpoint=True)
-            adj = (mx - mn) * 0.001 # 0.1% of the range
+            bins = np.linspace(mn, mx, bins + 1, endpoint=True)
+        else:  # adjust end points after binning
+            bins = np.linspace(mn, mx, bins + 1, endpoint=True)
+            adj = (mx - mn) * 0.001  # 0.1% of the range
             if right:
                 bins[0] -= adj
             else:
@@ -101,7 +101,6 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,
                          include_lowest=include_lowest)
 
 
-
 def qcut(x, q, labels=None, retbins=False, precision=3):
     """
     Quantile-based discretization function. Discretize variable into
@@ -210,6 +209,7 @@ def _format_label(x, precision=3):
     else:
         return str(x)
 
+
 def _trim_zeros(x):
     while len(x) > 1 and x[-1] == '0':
         x = x[:-1]
diff --git a/pandas/tseries/converter.py b/pandas/tseries/converter.py
index e3a49d03f..78455a5e4 100644
--- a/pandas/tseries/converter.py
+++ b/pandas/tseries/converter.py
@@ -20,6 +20,7 @@ import pandas.tseries.frequencies as frequencies
 from pandas.tseries.frequencies import FreqGroup
 from pandas.tseries.period import Period, PeriodIndex
 
+
 def register():
     units.registry[pydt.time] = TimeConverter()
     units.registry[lib.Timestamp] = DatetimeConverter()
@@ -27,11 +28,13 @@ def register():
     units.registry[pydt.datetime] = DatetimeConverter()
     units.registry[Period] = PeriodConverter()
 
+
 def _to_ordinalf(tm):
     tot_sec = (tm.hour * 3600 + tm.minute * 60 + tm.second +
                float(tm.microsecond / 1e6))
     return tot_sec
 
+
 def time2num(d):
     if isinstance(d, basestring):
         parsed = tools.to_datetime(d)
@@ -42,6 +45,7 @@ def time2num(d):
         return _to_ordinalf(d)
     return d
 
+
 class TimeConverter(units.ConversionInterface):
 
     @staticmethod
@@ -69,6 +73,7 @@ class TimeConverter(units.ConversionInterface):
     def default_units(x, axis):
         return 'time'
 
+
 ### time formatter
 class TimeFormatter(Formatter):
 
@@ -90,6 +95,7 @@ class TimeFormatter(Formatter):
 
 ### Period Conversion
 
+
 class PeriodConverter(dates.DateConverter):
 
     @staticmethod
@@ -106,6 +112,7 @@ class PeriodConverter(dates.DateConverter):
             return [get_datevalue(x, axis.freq) for x in values]
         return values
 
+
 def get_datevalue(date, freq):
     if isinstance(date, Period):
         return date.asfreq(freq).ordinal
@@ -119,9 +126,10 @@ def get_datevalue(date, freq):
     raise ValueError("Unrecognizable date '%s'" % date)
 
 HOURS_PER_DAY = 24.
-MINUTES_PER_DAY  = 60.*HOURS_PER_DAY
-SECONDS_PER_DAY =  60.*MINUTES_PER_DAY
-MUSECONDS_PER_DAY = 1e6*SECONDS_PER_DAY
+MINUTES_PER_DAY = 60. * HOURS_PER_DAY
+SECONDS_PER_DAY = 60. * MINUTES_PER_DAY
+MUSECONDS_PER_DAY = 1e6 * SECONDS_PER_DAY
+
 
 def _dt_to_float_ordinal(dt):
     """
@@ -132,6 +140,7 @@ def _dt_to_float_ordinal(dt):
     base = dates.date2num(dt)
     return base
 
+
 ### Datetime Conversion
 class DatetimeConverter(dates.DateConverter):
 
@@ -184,8 +193,8 @@ class DatetimeConverter(dates.DateConverter):
         datemin = pydt.date(2000, 1, 1)
         datemax = pydt.date(2010, 1, 1)
 
-        return units.AxisInfo( majloc=majloc, majfmt=majfmt, label='',
-                               default_limits=(datemin, datemax))
+        return units.AxisInfo(majloc=majloc, majfmt=majfmt, label='',
+                              default_limits=(datemin, datemax))
 
 
 class PandasAutoDateFormatter(dates.AutoDateFormatter):
@@ -196,23 +205,23 @@ class PandasAutoDateFormatter(dates.AutoDateFormatter):
         if self._tz is dates.UTC:
             self._tz._utcoffset = self._tz.utcoffset(None)
         self.scaled = {
-           365.0  : '%Y',
-           30.    : '%b %Y',
-           1.0    : '%b %d %Y',
-           1. / 24. : '%H:%M:%S',
-           1. / 24. / 3600. / 1000. : '%H:%M:%S.%f'
+           365.0: '%Y',
+           30.: '%b %Y',
+           1.0: '%b %d %Y',
+           1. / 24.: '%H:%M:%S',
+           1. / 24. / 3600. / 1000.: '%H:%M:%S.%f'
            }
 
     def _get_fmt(self, x):
 
-        scale = float( self._locator._get_unit() )
+        scale = float(self._locator._get_unit())
 
         fmt = self.defaultfmt
 
         for k in sorted(self.scaled):
-           if k >= scale:
-              fmt = self.scaled[k]
-              break
+            if k >= scale:
+                fmt = self.scaled[k]
+                break
 
         return fmt
 
@@ -221,6 +230,7 @@ class PandasAutoDateFormatter(dates.AutoDateFormatter):
         self._formatter = dates.DateFormatter(fmt, self._tz)
         return self._formatter(x, pos)
 
+
 class PandasAutoDateLocator(dates.AutoDateLocator):
 
     def get_locator(self, dmin, dmax):
@@ -245,6 +255,7 @@ class PandasAutoDateLocator(dates.AutoDateLocator):
     def _get_unit(self):
         return MilliSecondLocator.get_unit_generic(self._freq)
 
+
 class MilliSecondLocator(dates.DateLocator):
 
     UNIT = 1. / (24 * 3600 * 1000)
@@ -265,10 +276,12 @@ class MilliSecondLocator(dates.DateLocator):
 
     def __call__(self):
         # if no data have been set, this will tank with a ValueError
-        try: dmin, dmax = self.viewlim_to_dt()
-        except ValueError: return []
+        try:
+            dmin, dmax = self.viewlim_to_dt()
+        except ValueError:
+            return []
 
-        if dmin>dmax:
+        if dmin > dmax:
             dmax, dmin = dmin, dmax
         delta = relativedelta(dmax, dmin)
 
@@ -276,13 +289,13 @@ class MilliSecondLocator(dates.DateLocator):
         try:
             start = dmin - delta
         except ValueError:
-            start = _from_ordinal( 1.0 )
+            start = _from_ordinal(1.0)
 
         try:
             stop = dmax + delta
         except ValueError:
             # The magic number!
-            stop = _from_ordinal( 3652059.9999999 )
+            stop = _from_ordinal(3652059.9999999)
 
         nmax, nmin = dates.date2num((dmax, dmin))
 
@@ -306,7 +319,7 @@ class MilliSecondLocator(dates.DateLocator):
 
         freq = '%dL' % self._get_interval()
         tz = self.tz.tzname(None)
-        st = _from_ordinal(dates.date2num(dmin)) # strip tz
+        st = _from_ordinal(dates.date2num(dmin))  # strip tz
         ed = _from_ordinal(dates.date2num(dmax))
         all_dates = date_range(start=st, end=ed, freq=freq, tz=tz).asobject
 
@@ -328,7 +341,7 @@ class MilliSecondLocator(dates.DateLocator):
         Set the view limits to include the data range.
         """
         dmin, dmax = self.datalim_to_dt()
-        if dmin>dmax:
+        if dmin > dmax:
             dmax, dmin = dmin, dmax
 
         delta = relativedelta(dmax, dmin)
@@ -343,7 +356,7 @@ class MilliSecondLocator(dates.DateLocator):
             stop = dmax + delta
         except ValueError:
             # The magic number!
-            stop = _from_ordinal( 3652059.9999999 )
+            stop = _from_ordinal(3652059.9999999)
 
         dmin, dmax = self.datalim_to_dt()
 
@@ -357,18 +370,19 @@ def _from_ordinal(x, tz=None):
     ix = int(x)
     dt = datetime.fromordinal(ix)
     remainder = float(x) - ix
-    hour, remainder = divmod(24*remainder, 1)
-    minute, remainder = divmod(60*remainder, 1)
-    second, remainder = divmod(60*remainder, 1)
-    microsecond = int(1e6*remainder)
-    if microsecond<10: microsecond=0 # compensate for rounding errors
+    hour, remainder = divmod(24 * remainder, 1)
+    minute, remainder = divmod(60 * remainder, 1)
+    second, remainder = divmod(60 * remainder, 1)
+    microsecond = int(1e6 * remainder)
+    if microsecond < 10:
+        microsecond = 0  # compensate for rounding errors
     dt = datetime(dt.year, dt.month, dt.day, int(hour), int(minute),
                   int(second), microsecond)
     if tz is not None:
         dt = dt.astimezone(tz)
 
     if microsecond > 999990:  # compensate for rounding errors
-        dt += timedelta(microseconds = 1e6 - microsecond)
+        dt += timedelta(microseconds=1e6 - microsecond)
 
     return dt
 
@@ -378,6 +392,7 @@ def _from_ordinal(x, tz=None):
 #---- --- Locators ---
 ##### -------------------------------------------------------------------------
 
+
 def _get_default_annual_spacing(nyears):
     """
     Returns a default spacing between consecutive ticks for annual data.
@@ -399,6 +414,7 @@ def _get_default_annual_spacing(nyears):
         (min_spacing, maj_spacing) = (factor * 20, factor * 100)
     return (min_spacing, maj_spacing)
 
+
 def period_break(dates, period):
     """
     Returns the indices where the given period changes.
@@ -414,6 +430,7 @@ def period_break(dates, period):
     previous = getattr(dates - 1, period)
     return (current - previous).nonzero()[0]
 
+
 def has_level_label(label_flags, vmin):
     """
     Returns true if the ``label_flags`` indicate there is at least one label
@@ -429,6 +446,7 @@ def has_level_label(label_flags, vmin):
     else:
         return True
 
+
 def _daily_finder(vmin, vmax, freq):
     periodsperday = -1
 
@@ -439,7 +457,7 @@ def _daily_finder(vmin, vmax, freq):
             periodsperday = 24 * 60
         elif freq == FreqGroup.FR_HR:
             periodsperday = 24
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise ValueError("unexpected frequency: %s" % freq)
         periodsperyear = 365 * periodsperday
         periodspermonth = 28 * periodsperday
@@ -453,7 +471,7 @@ def _daily_finder(vmin, vmax, freq):
     elif frequencies.get_freq_group(freq) == FreqGroup.FR_WK:
         periodsperyear = 52
         periodspermonth = 3
-    else: # pragma: no cover
+    else:  # pragma: no cover
         raise ValueError("unexpected frequency")
 
     # save this for later usage
@@ -489,7 +507,7 @@ def _daily_finder(vmin, vmax, freq):
 
         def _hour_finder(label_interval, force_year_start):
             _hour = dates_.hour
-            _prev_hour = (dates_-1).hour
+            _prev_hour = (dates_ - 1).hour
             hour_start = (_hour - _prev_hour) != 0
             info_maj[day_start] = True
             info_min[hour_start & (_hour % label_interval == 0)] = True
@@ -503,7 +521,7 @@ def _daily_finder(vmin, vmax, freq):
         def _minute_finder(label_interval):
             hour_start = period_break(dates_, 'hour')
             _minute = dates_.minute
-            _prev_minute = (dates_-1).minute
+            _prev_minute = (dates_ - 1).minute
             minute_start = (_minute - _prev_minute) != 0
             info_maj[hour_start] = True
             info_min[minute_start & (_minute % label_interval == 0)] = True
@@ -516,7 +534,7 @@ def _daily_finder(vmin, vmax, freq):
         def _second_finder(label_interval):
             minute_start = period_break(dates_, 'minute')
             _second = dates_.second
-            _prev_second = (dates_-1).second
+            _prev_second = (dates_ - 1).second
             second_start = (_second - _prev_second) != 0
             info['maj'][minute_start] = True
             info['min'][second_start & (_second % label_interval == 0)] = True
@@ -748,6 +766,7 @@ def _quarterly_finder(vmin, vmax, freq):
     #..............
     return info
 
+
 def _annual_finder(vmin, vmax, freq):
     (vmin, vmax) = (int(vmin), int(vmax + 1))
     span = vmax - vmin + 1
@@ -767,6 +786,7 @@ def _annual_finder(vmin, vmax, freq):
     #..............
     return info
 
+
 def get_finder(freq):
     if isinstance(freq, basestring):
         freq = frequencies.get_freq(freq)
@@ -776,14 +796,15 @@ def get_finder(freq):
         return _annual_finder
     elif fgroup == FreqGroup.FR_QTR:
         return _quarterly_finder
-    elif freq ==FreqGroup.FR_MTH:
+    elif freq == FreqGroup.FR_MTH:
         return _monthly_finder
     elif ((freq >= FreqGroup.FR_BUS) or fgroup == FreqGroup.FR_WK):
         return _daily_finder
-    else: # pragma: no cover
+    else:  # pragma: no cover
         errmsg = "Unsupported frequency: %s" % (freq)
         raise NotImplementedError(errmsg)
 
+
 class TimeSeries_DateLocator(Locator):
     """
     Locates the ticks along an axis controlled by a :class:`Series`.
@@ -839,7 +860,7 @@ class TimeSeries_DateLocator(Locator):
             vmin, vmax = vmax, vmin
         if self.isdynamic:
             locs = self._get_default_locs(vmin, vmax)
-        else: # pragma: no cover
+        else:  # pragma: no cover
             base = self.base
             (d, m) = divmod(vmin, base)
             vmin = (d + 1) * base
@@ -921,11 +942,10 @@ class TimeSeries_DateFormatter(Formatter):
         if vmax < vmin:
             (vmin, vmax) = (vmax, vmin)
         self._set_default_format(vmin, vmax)
-    #
+
     def __call__(self, x, pos=0):
         if self.formatdict is None:
             return ''
         else:
             fmt = self.formatdict.pop(x, '')
             return Period(ordinal=int(x), freq=self.freq).strftime(fmt)
-
diff --git a/pandas/tseries/frequencies.py b/pandas/tseries/frequencies.py
index 77f03dc4d..b9b2d28e1 100644
--- a/pandas/tseries/frequencies.py
+++ b/pandas/tseries/frequencies.py
@@ -9,6 +9,7 @@ import pandas.tseries.offsets as offsets
 import pandas.core.common as com
 import pandas.lib as lib
 
+
 class FreqGroup(object):
     FR_ANN = 1000
     FR_QTR = 2000
@@ -20,6 +21,7 @@ class FreqGroup(object):
     FR_MIN = 8000
     FR_SEC = 9000
 
+
 def get_to_timestamp_base(base):
     if base <= FreqGroup.FR_WK:
         return FreqGroup.FR_DAY
@@ -27,18 +29,21 @@ def get_to_timestamp_base(base):
         return FreqGroup.FR_SEC
     return base
 
+
 def get_freq_group(freq):
     if isinstance(freq, basestring):
         base, mult = get_freq_code(freq)
         freq = base
     return (freq // 1000) * 1000
 
+
 def get_freq(freq):
     if isinstance(freq, basestring):
         base, mult = get_freq_code(freq)
         freq = base
     return freq
 
+
 def get_freq_code(freqstr):
     """
 
@@ -93,171 +98,171 @@ from pandas.tseries.offsets import (Day, BDay, Hour, Minute, Second, Milli,
                                     QuarterEnd, BQuarterBegin, BQuarterEnd)
 
 _offset_map = {
-    'D'     : Day(),
-    'B'     : BDay(),
-    'H'     : Hour(),
-    'T'     : Minute(),
-    'S'     : Second(),
-    'L'     : Milli(),
-    'U'     : Micro(),
-    None    : None,
+    'D': Day(),
+    'B': BDay(),
+    'H': Hour(),
+    'T': Minute(),
+    'S': Second(),
+    'L': Milli(),
+    'U': Micro(),
+    None: None,
 
     # Monthly - Calendar
-    'M'      : MonthEnd(),
-    'MS'     : MonthBegin(),
+    'M': MonthEnd(),
+    'MS': MonthBegin(),
 
     # Monthly - Business
-    'BM'     : BMonthEnd(),
-    'BMS'    : BMonthBegin(),
+    'BM': BMonthEnd(),
+    'BMS': BMonthBegin(),
 
     # Annual - Calendar
-    'A-JAN' : YearEnd(month=1),
-    'A-FEB' : YearEnd(month=2),
-    'A-MAR' : YearEnd(month=3),
-    'A-APR' : YearEnd(month=4),
-    'A-MAY' : YearEnd(month=5),
-    'A-JUN' : YearEnd(month=6),
-    'A-JUL' : YearEnd(month=7),
-    'A-AUG' : YearEnd(month=8),
-    'A-SEP' : YearEnd(month=9),
-    'A-OCT' : YearEnd(month=10),
-    'A-NOV' : YearEnd(month=11),
-    'A-DEC' : YearEnd(month=12),
+    'A-JAN': YearEnd(month=1),
+    'A-FEB': YearEnd(month=2),
+    'A-MAR': YearEnd(month=3),
+    'A-APR': YearEnd(month=4),
+    'A-MAY': YearEnd(month=5),
+    'A-JUN': YearEnd(month=6),
+    'A-JUL': YearEnd(month=7),
+    'A-AUG': YearEnd(month=8),
+    'A-SEP': YearEnd(month=9),
+    'A-OCT': YearEnd(month=10),
+    'A-NOV': YearEnd(month=11),
+    'A-DEC': YearEnd(month=12),
 
     # Annual - Calendar (start)
-    'AS-JAN' : YearBegin(month=1),
-    'AS-FEB' : YearBegin(month=2),
-    'AS-MAR' : YearBegin(month=3),
-    'AS-APR' : YearBegin(month=4),
-    'AS-MAY' : YearBegin(month=5),
-    'AS-JUN' : YearBegin(month=6),
-    'AS-JUL' : YearBegin(month=7),
-    'AS-AUG' : YearBegin(month=8),
-    'AS-SEP' : YearBegin(month=9),
-    'AS-OCT' : YearBegin(month=10),
-    'AS-NOV' : YearBegin(month=11),
-    'AS-DEC' : YearBegin(month=12),
+    'AS-JAN': YearBegin(month=1),
+    'AS-FEB': YearBegin(month=2),
+    'AS-MAR': YearBegin(month=3),
+    'AS-APR': YearBegin(month=4),
+    'AS-MAY': YearBegin(month=5),
+    'AS-JUN': YearBegin(month=6),
+    'AS-JUL': YearBegin(month=7),
+    'AS-AUG': YearBegin(month=8),
+    'AS-SEP': YearBegin(month=9),
+    'AS-OCT': YearBegin(month=10),
+    'AS-NOV': YearBegin(month=11),
+    'AS-DEC': YearBegin(month=12),
 
     # Annual - Business
-    'BA-JAN' : BYearEnd(month=1),
-    'BA-FEB' : BYearEnd(month=2),
-    'BA-MAR' : BYearEnd(month=3),
-    'BA-APR' : BYearEnd(month=4),
-    'BA-MAY' : BYearEnd(month=5),
-    'BA-JUN' : BYearEnd(month=6),
-    'BA-JUL' : BYearEnd(month=7),
-    'BA-AUG' : BYearEnd(month=8),
-    'BA-SEP' : BYearEnd(month=9),
-    'BA-OCT' : BYearEnd(month=10),
-    'BA-NOV' : BYearEnd(month=11),
-    'BA-DEC' : BYearEnd(month=12),
+    'BA-JAN': BYearEnd(month=1),
+    'BA-FEB': BYearEnd(month=2),
+    'BA-MAR': BYearEnd(month=3),
+    'BA-APR': BYearEnd(month=4),
+    'BA-MAY': BYearEnd(month=5),
+    'BA-JUN': BYearEnd(month=6),
+    'BA-JUL': BYearEnd(month=7),
+    'BA-AUG': BYearEnd(month=8),
+    'BA-SEP': BYearEnd(month=9),
+    'BA-OCT': BYearEnd(month=10),
+    'BA-NOV': BYearEnd(month=11),
+    'BA-DEC': BYearEnd(month=12),
 
     # Annual - Business (Start)
-    'BAS-JAN' : BYearBegin(month=1),
-    'BAS-FEB' : BYearBegin(month=2),
-    'BAS-MAR' : BYearBegin(month=3),
-    'BAS-APR' : BYearBegin(month=4),
-    'BAS-MAY' : BYearBegin(month=5),
-    'BAS-JUN' : BYearBegin(month=6),
-    'BAS-JUL' : BYearBegin(month=7),
-    'BAS-AUG' : BYearBegin(month=8),
-    'BAS-SEP' : BYearBegin(month=9),
-    'BAS-OCT' : BYearBegin(month=10),
-    'BAS-NOV' : BYearBegin(month=11),
-    'BAS-DEC' : BYearBegin(month=12),
+    'BAS-JAN': BYearBegin(month=1),
+    'BAS-FEB': BYearBegin(month=2),
+    'BAS-MAR': BYearBegin(month=3),
+    'BAS-APR': BYearBegin(month=4),
+    'BAS-MAY': BYearBegin(month=5),
+    'BAS-JUN': BYearBegin(month=6),
+    'BAS-JUL': BYearBegin(month=7),
+    'BAS-AUG': BYearBegin(month=8),
+    'BAS-SEP': BYearBegin(month=9),
+    'BAS-OCT': BYearBegin(month=10),
+    'BAS-NOV': BYearBegin(month=11),
+    'BAS-DEC': BYearBegin(month=12),
 
     # Quarterly - Calendar
     # 'Q'     : QuarterEnd(startingMonth=3),
-    'Q-JAN' : QuarterEnd(startingMonth=1),
-    'Q-FEB' : QuarterEnd(startingMonth=2),
-    'Q-MAR' : QuarterEnd(startingMonth=3),
-    'Q-APR' : QuarterEnd(startingMonth=4),
-    'Q-MAY' : QuarterEnd(startingMonth=5),
-    'Q-JUN' : QuarterEnd(startingMonth=6),
-    'Q-JUL' : QuarterEnd(startingMonth=7),
-    'Q-AUG' : QuarterEnd(startingMonth=8),
-    'Q-SEP' : QuarterEnd(startingMonth=9),
-    'Q-OCT' : QuarterEnd(startingMonth=10),
-    'Q-NOV' : QuarterEnd(startingMonth=11),
-    'Q-DEC' : QuarterEnd(startingMonth=12),
+    'Q-JAN': QuarterEnd(startingMonth=1),
+    'Q-FEB': QuarterEnd(startingMonth=2),
+    'Q-MAR': QuarterEnd(startingMonth=3),
+    'Q-APR': QuarterEnd(startingMonth=4),
+    'Q-MAY': QuarterEnd(startingMonth=5),
+    'Q-JUN': QuarterEnd(startingMonth=6),
+    'Q-JUL': QuarterEnd(startingMonth=7),
+    'Q-AUG': QuarterEnd(startingMonth=8),
+    'Q-SEP': QuarterEnd(startingMonth=9),
+    'Q-OCT': QuarterEnd(startingMonth=10),
+    'Q-NOV': QuarterEnd(startingMonth=11),
+    'Q-DEC': QuarterEnd(startingMonth=12),
 
     # Quarterly - Calendar (Start)
-    'QS'     : QuarterBegin(startingMonth=1),
-    'QS-JAN' : QuarterBegin(startingMonth=1),
-    'QS-FEB' : QuarterBegin(startingMonth=2),
-    'QS-MAR' : QuarterBegin(startingMonth=3),
-    'QS-APR' : QuarterBegin(startingMonth=4),
-    'QS-MAY' : QuarterBegin(startingMonth=5),
-    'QS-JUN' : QuarterBegin(startingMonth=6),
-    'QS-JUL' : QuarterBegin(startingMonth=7),
-    'QS-AUG' : QuarterBegin(startingMonth=8),
-    'QS-SEP' : QuarterBegin(startingMonth=9),
-    'QS-OCT' : QuarterBegin(startingMonth=10),
-    'QS-NOV' : QuarterBegin(startingMonth=11),
-    'QS-DEC' : QuarterBegin(startingMonth=12),
+    'QS': QuarterBegin(startingMonth=1),
+    'QS-JAN': QuarterBegin(startingMonth=1),
+    'QS-FEB': QuarterBegin(startingMonth=2),
+    'QS-MAR': QuarterBegin(startingMonth=3),
+    'QS-APR': QuarterBegin(startingMonth=4),
+    'QS-MAY': QuarterBegin(startingMonth=5),
+    'QS-JUN': QuarterBegin(startingMonth=6),
+    'QS-JUL': QuarterBegin(startingMonth=7),
+    'QS-AUG': QuarterBegin(startingMonth=8),
+    'QS-SEP': QuarterBegin(startingMonth=9),
+    'QS-OCT': QuarterBegin(startingMonth=10),
+    'QS-NOV': QuarterBegin(startingMonth=11),
+    'QS-DEC': QuarterBegin(startingMonth=12),
 
     # Quarterly - Business
-    'BQ-JAN' : BQuarterEnd(startingMonth=1),
-    'BQ-FEB' : BQuarterEnd(startingMonth=2),
-    'BQ-MAR' : BQuarterEnd(startingMonth=3),
-
-    'BQ'     : BQuarterEnd(startingMonth=12),
-    'BQ-APR' : BQuarterEnd(startingMonth=4),
-    'BQ-MAY' : BQuarterEnd(startingMonth=5),
-    'BQ-JUN' : BQuarterEnd(startingMonth=6),
-    'BQ-JUL' : BQuarterEnd(startingMonth=7),
-    'BQ-AUG' : BQuarterEnd(startingMonth=8),
-    'BQ-SEP' : BQuarterEnd(startingMonth=9),
-    'BQ-OCT' : BQuarterEnd(startingMonth=10),
-    'BQ-NOV' : BQuarterEnd(startingMonth=11),
-    'BQ-DEC' : BQuarterEnd(startingMonth=12),
+    'BQ-JAN': BQuarterEnd(startingMonth=1),
+    'BQ-FEB': BQuarterEnd(startingMonth=2),
+    'BQ-MAR': BQuarterEnd(startingMonth=3),
+
+    'BQ': BQuarterEnd(startingMonth=12),
+    'BQ-APR': BQuarterEnd(startingMonth=4),
+    'BQ-MAY': BQuarterEnd(startingMonth=5),
+    'BQ-JUN': BQuarterEnd(startingMonth=6),
+    'BQ-JUL': BQuarterEnd(startingMonth=7),
+    'BQ-AUG': BQuarterEnd(startingMonth=8),
+    'BQ-SEP': BQuarterEnd(startingMonth=9),
+    'BQ-OCT': BQuarterEnd(startingMonth=10),
+    'BQ-NOV': BQuarterEnd(startingMonth=11),
+    'BQ-DEC': BQuarterEnd(startingMonth=12),
 
     # Quarterly - Business (Start)
-    'BQS-JAN' : BQuarterBegin(startingMonth=1),
-    'BQS'     : BQuarterBegin(startingMonth=1),
-    'BQS-FEB' : BQuarterBegin(startingMonth=2),
-    'BQS-MAR' : BQuarterBegin(startingMonth=3),
-    'BQS-APR' : BQuarterBegin(startingMonth=4),
-    'BQS-MAY' : BQuarterBegin(startingMonth=5),
-    'BQS-JUN' : BQuarterBegin(startingMonth=6),
-    'BQS-JUL' : BQuarterBegin(startingMonth=7),
-    'BQS-AUG' : BQuarterBegin(startingMonth=8),
-    'BQS-SEP' : BQuarterBegin(startingMonth=9),
-    'BQS-OCT' : BQuarterBegin(startingMonth=10),
-    'BQS-NOV' : BQuarterBegin(startingMonth=11),
-    'BQS-DEC' : BQuarterBegin(startingMonth=12),
+    'BQS-JAN': BQuarterBegin(startingMonth=1),
+    'BQS': BQuarterBegin(startingMonth=1),
+    'BQS-FEB': BQuarterBegin(startingMonth=2),
+    'BQS-MAR': BQuarterBegin(startingMonth=3),
+    'BQS-APR': BQuarterBegin(startingMonth=4),
+    'BQS-MAY': BQuarterBegin(startingMonth=5),
+    'BQS-JUN': BQuarterBegin(startingMonth=6),
+    'BQS-JUL': BQuarterBegin(startingMonth=7),
+    'BQS-AUG': BQuarterBegin(startingMonth=8),
+    'BQS-SEP': BQuarterBegin(startingMonth=9),
+    'BQS-OCT': BQuarterBegin(startingMonth=10),
+    'BQS-NOV': BQuarterBegin(startingMonth=11),
+    'BQS-DEC': BQuarterBegin(startingMonth=12),
 
     # Weekly
-    'W-MON' : Week(weekday=0),
-    'W-TUE' : Week(weekday=1),
-    'W-WED' : Week(weekday=2),
-    'W-THU' : Week(weekday=3),
-    'W-FRI' : Week(weekday=4),
-    'W-SAT' : Week(weekday=5),
-    'W-SUN' : Week(weekday=6),
+    'W-MON': Week(weekday=0),
+    'W-TUE': Week(weekday=1),
+    'W-WED': Week(weekday=2),
+    'W-THU': Week(weekday=3),
+    'W-FRI': Week(weekday=4),
+    'W-SAT': Week(weekday=5),
+    'W-SUN': Week(weekday=6),
 
 }
 
 _offset_to_period_map = {
-    'WEEKDAY' : 'D',
-    'EOM' : 'M',
-    'BM' : 'M',
-    'BQS' : 'Q',
-    'QS' : 'Q',
-    'BQ' : 'Q',
-    'BA' : 'A',
-    'AS' : 'A',
-    'BAS' : 'A',
-    'MS' : 'M',
-    'D' : 'D',
-    'B' : 'B',
-    'T' : 'T',
-    'S' : 'S',
-    'H' : 'H',
-    'Q' : 'Q',
-    'A' : 'A',
-    'W' : 'W',
-    'M' : 'M'
+    'WEEKDAY': 'D',
+    'EOM': 'M',
+    'BM': 'M',
+    'BQS': 'Q',
+    'QS': 'Q',
+    'BQ': 'Q',
+    'BA': 'A',
+    'AS': 'A',
+    'BAS': 'A',
+    'MS': 'M',
+    'D': 'D',
+    'B': 'B',
+    'T': 'T',
+    'S': 'S',
+    'H': 'H',
+    'Q': 'Q',
+    'A': 'A',
+    'W': 'W',
+    'M': 'M'
 }
 
 need_suffix = ['QS', 'BQ', 'BQS', 'AS', 'BA', 'BAS']
@@ -276,6 +281,7 @@ _days = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']
 for _d in _days:
     _offset_to_period_map['W-%s' % _d] = 'W-%s' % _d
 
+
 def get_period_alias(offset_str):
     """ alias to closest period strings BQ->Q etc"""
     return _offset_to_period_map.get(offset_str, None)
@@ -299,25 +305,25 @@ _rule_aliases = {
     'Q@JAN': 'BQ-JAN',
     'Q@FEB': 'BQ-FEB',
     'Q@MAR': 'BQ-MAR',
-    'Q' : 'Q-DEC',
-
-    'A'     : 'A-DEC', # YearEnd(month=12),
-    'AS'     : 'AS-JAN', # YearBegin(month=1),
-    'BA'     : 'BA-DEC', # BYearEnd(month=12),
-    'BAS'     : 'BAS-JAN', # BYearBegin(month=1),
-
-    'A@JAN' : 'BA-JAN',
-    'A@FEB' : 'BA-FEB',
-    'A@MAR' : 'BA-MAR',
-    'A@APR' : 'BA-APR',
-    'A@MAY' : 'BA-MAY',
-    'A@JUN' : 'BA-JUN',
-    'A@JUL' : 'BA-JUL',
-    'A@AUG' : 'BA-AUG',
-    'A@SEP' : 'BA-SEP',
-    'A@OCT' : 'BA-OCT',
-    'A@NOV' : 'BA-NOV',
-    'A@DEC' : 'BA-DEC',
+    'Q': 'Q-DEC',
+
+    'A': 'A-DEC',  # YearEnd(month=12),
+    'AS': 'AS-JAN',  # YearBegin(month=1),
+    'BA': 'BA-DEC',  # BYearEnd(month=12),
+    'BAS': 'BAS-JAN',  # BYearBegin(month=1),
+
+    'A@JAN': 'BA-JAN',
+    'A@FEB': 'BA-FEB',
+    'A@MAR': 'BA-MAR',
+    'A@APR': 'BA-APR',
+    'A@MAY': 'BA-MAY',
+    'A@JUN': 'BA-JUN',
+    'A@JUL': 'BA-JUL',
+    'A@AUG': 'BA-AUG',
+    'A@SEP': 'BA-SEP',
+    'A@OCT': 'BA-OCT',
+    'A@NOV': 'BA-NOV',
+    'A@DEC': 'BA-DEC',
 
     # lite aliases
     'Min': 'T',
@@ -407,6 +413,7 @@ def to_offset(freqstr):
 # hack to handle WOM-1MON
 opattern = re.compile(r'([\-]?\d*)\s*([A-Za-z]+([\-@]\d*[A-Za-z]+)?)')
 
+
 def _base_and_stride(freqstr):
     """
     Return base freq and stride info from string representation
@@ -431,6 +438,7 @@ def _base_and_stride(freqstr):
 
     return (base, stride)
 
+
 def get_base_alias(freqstr):
     """
     Returns the base frequency alias, e.g., '5D' -> 'D'
@@ -473,6 +481,7 @@ getOffset = get_offset
 def hasOffsetName(offset):
     return offset in _offset_names
 
+
 def get_offset_name(offset):
     """
     Return rule name associated with a DateOffset object
@@ -488,6 +497,7 @@ def get_offset_name(offset):
     else:
         raise Exception('Bad rule given: %s!' % offset)
 
+
 def get_legacy_offset_name(offset):
     """
     Return the pre pandas 0.8.0 name for the date offset
@@ -497,6 +507,7 @@ def get_legacy_offset_name(offset):
 
 get_offset_name = get_offset_name
 
+
 def get_standard_freq(freq):
     """
     Return the standardized frequency string
@@ -518,49 +529,49 @@ def get_standard_freq(freq):
 _period_code_map = {
     # Annual freqs with various fiscal year ends.
     # eg, 2005 for A-FEB runs Mar 1, 2004 to Feb 28, 2005
-    "A-DEC" : 1000,  # Annual - December year end
-    "A-JAN" : 1001,  # Annual - January year end
-    "A-FEB" : 1002,  # Annual - February year end
-    "A-MAR" : 1003,  # Annual - March year end
-    "A-APR" : 1004,  # Annual - April year end
-    "A-MAY" : 1005,  # Annual - May year end
-    "A-JUN" : 1006,  # Annual - June year end
-    "A-JUL" : 1007,  # Annual - July year end
-    "A-AUG" : 1008,  # Annual - August year end
-    "A-SEP" : 1009,  # Annual - September year end
-    "A-OCT" : 1010,  # Annual - October year end
-    "A-NOV" : 1011,  # Annual - November year end
+    "A-DEC": 1000,  # Annual - December year end
+    "A-JAN": 1001,  # Annual - January year end
+    "A-FEB": 1002,  # Annual - February year end
+    "A-MAR": 1003,  # Annual - March year end
+    "A-APR": 1004,  # Annual - April year end
+    "A-MAY": 1005,  # Annual - May year end
+    "A-JUN": 1006,  # Annual - June year end
+    "A-JUL": 1007,  # Annual - July year end
+    "A-AUG": 1008,  # Annual - August year end
+    "A-SEP": 1009,  # Annual - September year end
+    "A-OCT": 1010,  # Annual - October year end
+    "A-NOV": 1011,  # Annual - November year end
 
     # Quarterly frequencies with various fiscal year ends.
     # eg, Q42005 for Q-OCT runs Aug 1, 2005 to Oct 31, 2005
-    "Q-DEC" : 2000 ,    # Quarterly - December year end
-    "Q-JAN" : 2001,    # Quarterly - January year end
-    "Q-FEB" : 2002,    # Quarterly - February year end
-    "Q-MAR" : 2003,    # Quarterly - March year end
-    "Q-APR" : 2004,    # Quarterly - April year end
-    "Q-MAY" : 2005,    # Quarterly - May year end
-    "Q-JUN" : 2006,    # Quarterly - June year end
-    "Q-JUL" : 2007,    # Quarterly - July year end
-    "Q-AUG" : 2008,    # Quarterly - August year end
-    "Q-SEP" : 2009,    # Quarterly - September year end
-    "Q-OCT" : 2010,    # Quarterly - October year end
-    "Q-NOV" : 2011,    # Quarterly - November year end
-
-    "M"     : 3000,   # Monthly
-
-    "W-SUN" : 4000,    # Weekly - Sunday end of week
-    "W-MON" : 4001,    # Weekly - Monday end of week
-    "W-TUE" : 4002,    # Weekly - Tuesday end of week
-    "W-WED" : 4003,    # Weekly - Wednesday end of week
-    "W-THU" : 4004,    # Weekly - Thursday end of week
-    "W-FRI" : 4005,    # Weekly - Friday end of week
-    "W-SAT" : 4006,    # Weekly - Saturday end of week
-
-    "B"      : 5000,   # Business days
-    "D"      : 6000,   # Daily
-    "H"      : 7000,   # Hourly
-    "T"      : 8000,   # Minutely
-    "S"      : 9000,   # Secondly
+    "Q-DEC": 2000,    # Quarterly - December year end
+    "Q-JAN": 2001,    # Quarterly - January year end
+    "Q-FEB": 2002,    # Quarterly - February year end
+    "Q-MAR": 2003,    # Quarterly - March year end
+    "Q-APR": 2004,    # Quarterly - April year end
+    "Q-MAY": 2005,    # Quarterly - May year end
+    "Q-JUN": 2006,    # Quarterly - June year end
+    "Q-JUL": 2007,    # Quarterly - July year end
+    "Q-AUG": 2008,    # Quarterly - August year end
+    "Q-SEP": 2009,    # Quarterly - September year end
+    "Q-OCT": 2010,    # Quarterly - October year end
+    "Q-NOV": 2011,    # Quarterly - November year end
+
+    "M": 3000,        # Monthly
+
+    "W-SUN": 4000,    # Weekly - Sunday end of week
+    "W-MON": 4001,    # Weekly - Monday end of week
+    "W-TUE": 4002,    # Weekly - Tuesday end of week
+    "W-WED": 4003,    # Weekly - Wednesday end of week
+    "W-THU": 4004,    # Weekly - Thursday end of week
+    "W-FRI": 4005,    # Weekly - Friday end of week
+    "W-SAT": 4006,    # Weekly - Saturday end of week
+
+    "B": 5000,        # Business days
+    "D": 6000,        # Daily
+    "H": 7000,        # Hourly
+    "T": 8000,        # Minutely
+    "S": 9000,        # Secondly
 }
 
 _reverse_period_code_map = {}
@@ -569,11 +580,12 @@ for _k, _v in _period_code_map.iteritems():
 
 # Additional aliases
 _period_code_map.update({
-    "Q"     : 2000,    # Quarterly - December year end (default quarterly)
-    "A"     : 1000,  # Annual
-    "W"     : 4000,    # Weekly
+    "Q": 2000,  # Quarterly - December year end (default quarterly)
+    "A": 1000,  # Annual
+    "W": 4000,  # Weekly
 })
 
+
 def _period_alias_dictionary():
     """
     Build freq alias dictionary to support freqs from original c_dates.c file
@@ -613,18 +625,18 @@ def _period_alias_dictionary():
                   "QTR-E", "QUARTER-E", "QUARTERLY-E"]
 
     month_names = [
-        [ "DEC", "DECEMBER" ],
-        [ "JAN", "JANUARY" ],
-        [ "FEB", "FEBRUARY" ],
-        [ "MAR", "MARCH" ],
-        [ "APR", "APRIL" ],
-        [ "MAY", "MAY" ],
-        [ "JUN", "JUNE" ],
-        [ "JUL", "JULY" ],
-        [ "AUG", "AUGUST" ],
-        [ "SEP", "SEPTEMBER" ],
-        [ "OCT", "OCTOBER" ],
-        [ "NOV", "NOVEMBER" ] ]
+        ["DEC", "DECEMBER"],
+        ["JAN", "JANUARY"],
+        ["FEB", "FEBRUARY"],
+        ["MAR", "MARCH"],
+        ["APR", "APRIL"],
+        ["MAY", "MAY"],
+        ["JUN", "JUNE"],
+        ["JUL", "JULY"],
+        ["AUG", "AUGUST"],
+        ["SEP", "SEPTEMBER"],
+        ["OCT", "OCTOBER"],
+        ["NOV", "NOVEMBER"]]
 
     seps = ["@", "-"]
 
@@ -647,13 +659,13 @@ def _period_alias_dictionary():
     W_prefixes = ["W", "WK", "WEEK", "WEEKLY"]
 
     day_names = [
-        [ "SUN", "SUNDAY" ],
-        [ "MON", "MONDAY" ],
-        [ "TUE", "TUESDAY" ],
-        [ "WED", "WEDNESDAY" ],
-        [ "THU", "THURSDAY" ],
-        [ "FRI", "FRIDAY" ],
-        [ "SAT", "SATURDAY" ] ]
+        ["SUN", "SUNDAY"],
+        ["MON", "MONDAY"],
+        ["TUE", "TUESDAY"],
+        ["WED", "WEDNESDAY"],
+        ["THU", "THURSDAY"],
+        ["FRI", "FRIDAY"],
+        ["SAT", "SATURDAY"]]
 
     for k in W_prefixes:
         alias_dict[k] = 'W'
@@ -666,24 +678,27 @@ def _period_alias_dictionary():
     return alias_dict
 
 _reso_period_map = {
-    "year"    : "A",
-    "quarter" : "Q",
-    "month"   : "M",
-    "day"     : "D",
-    "hour"    : "H",
-    "minute"  : "T",
-    "second"  : "S",
+    "year": "A",
+    "quarter": "Q",
+    "month": "M",
+    "day": "D",
+    "hour": "H",
+    "minute": "T",
+    "second": "S",
 }
 
+
 def _infer_period_group(freqstr):
     return _period_group(_reso_period_map[freqstr])
 
+
 def _period_group(freqstr):
     base, mult = get_freq_code(freqstr)
     return base // 1000 * 1000
 
 _period_alias_dict = _period_alias_dictionary()
 
+
 def _period_str_to_code(freqstr):
     # hack
     freqstr = _rule_aliases.get(freqstr, freqstr)
@@ -697,7 +712,6 @@ def _period_str_to_code(freqstr):
         return _period_code_map[alias]
 
 
-
 def infer_freq(index, warn=True):
     """
     Infer the most likely frequency given the input index. If the frequency is
@@ -727,6 +741,7 @@ _ONE_MINUTE = 60 * _ONE_SECOND
 _ONE_HOUR = 60 * _ONE_MINUTE
 _ONE_DAY = 24 * _ONE_HOUR
 
+
 class _FrequencyInferer(object):
     """
     Not sure if I can avoid the state machine here
@@ -853,7 +868,7 @@ class _FrequencyInferer(object):
         quarterly_rule = self._get_quarterly_rule()
         if quarterly_rule:
             nquarters = self.mdiffs[0] / 3
-            mod_dict = {0 : 12, 2 : 11, 1 : 10}
+            mod_dict = {0: 12, 2: 11, 1: 10}
             month = _month_aliases[mod_dict[self.rep_stamp.month % 3]]
             return _maybe_add_count('%s-%s' % (quarterly_rule, month),
                                     nquarters)
@@ -907,12 +922,14 @@ class _FrequencyInferer(object):
 
 import pandas.core.algorithms as algos
 
+
 def _maybe_add_count(base, count):
     if count > 1:
         return '%d%s' % (count, base)
     else:
         return base
 
+
 def is_subperiod(source, target):
     """
     Returns True if downsampling is possible between source and target
@@ -959,6 +976,7 @@ def is_subperiod(source, target):
     elif target == 'S':
         return source in ['S']
 
+
 def is_superperiod(source, target):
     """
     Returns True if upsampling is possible between source and target
@@ -1009,6 +1027,7 @@ def is_superperiod(source, target):
     elif source == 'S':
         return target in ['S']
 
+
 def _get_rule_month(source, default='DEC'):
     source = source.upper()
     if '-' not in source:
@@ -1016,15 +1035,18 @@ def _get_rule_month(source, default='DEC'):
     else:
         return source.split('-')[1]
 
+
 def _is_annual(rule):
     rule = rule.upper()
     return rule == 'A' or rule.startswith('A-')
 
+
 def _quarter_months_conform(source, target):
     snum = _month_numbers[source]
     tnum = _month_numbers[target]
     return snum % 3 == tnum % 3
 
+
 def _is_quarterly(rule):
     rule = rule.upper()
     return rule == 'Q' or rule.startswith('Q-')
@@ -1043,9 +1065,9 @@ MONTHS = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL',
 _month_numbers = dict((k, i) for i, k in enumerate(MONTHS))
 
 
-
 _weekday_rule_aliases = dict((k, v) for k, v in enumerate(DAYS))
 _month_aliases = dict((k + 1, v) for k, v in enumerate(MONTHS))
 
+
 def _is_multiple(us, mult):
     return us % mult == 0
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 6d5fd6f56..597f3573d 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -20,6 +20,7 @@ from pandas.lib import Timestamp
 import pandas.lib as lib
 import pandas._algos as _algos
 
+
 def _utc():
     import pytz
     return pytz.utc
@@ -38,6 +39,7 @@ def _field_accessor(name, field):
     f.__name__ = name
     return property(f)
 
+
 def _join_i8_wrapper(joinf, with_indexers=True):
     @staticmethod
     def wrapper(left, right):
@@ -73,6 +75,7 @@ def _dt_index_cmp(opname):
 
     return wrapper
 
+
 def _ensure_datetime64(other):
     if isinstance(other, np.datetime64):
         return other
@@ -87,6 +90,7 @@ _midnight = time(0, 0)
 _NS_DTYPE = np.dtype('M8[ns]')
 _INT64_DTYPE = np.dtype(np.int64)
 
+
 class DatetimeIndex(Int64Index):
     """
     Immutable ndarray of datetime64 data, represented internally as int64, and
@@ -115,8 +119,8 @@ class DatetimeIndex(Int64Index):
 
     _inner_indexer = _join_i8_wrapper(_algos.inner_join_indexer_int64)
     _outer_indexer = _join_i8_wrapper(_algos.outer_join_indexer_int64)
-    _left_indexer  = _join_i8_wrapper(_algos.left_join_indexer_int64)
-    _left_indexer_unique  = _join_i8_wrapper(
+    _left_indexer = _join_i8_wrapper(_algos.left_join_indexer_int64)
+    _left_indexer_unique = _join_i8_wrapper(
         _algos.left_join_indexer_unique_int64, with_indexers=False)
     _arrmap = None
 
@@ -308,7 +312,6 @@ class DatetimeIndex(Int64Index):
             else:
                 _normalized = _normalized and end.time() == _midnight
 
-
         if hasattr(offset, 'delta') and offset != offsets.Day():
             if inferred_tz is None and tz is not None:
                 # naive dates
@@ -325,7 +328,6 @@ class DatetimeIndex(Int64Index):
                 if end.tz is None and start.tz is not None:
                     end = end.tz_localize(start.tz)
 
-
             if (offset._should_cache() and
                 not (offset._normalize_cache and not _normalized) and
                 _naive_in_cache_range(start, end)):
@@ -842,9 +844,11 @@ class DatetimeIndex(Int64Index):
         if isinstance(other, DatetimeIndex):
             if self.tz is not None:
                 if other.tz is None:
-                    raise Exception('Cannot join tz-naive with tz-aware DatetimeIndex')
+                    raise Exception('Cannot join tz-naive with tz-aware '
+                                    'DatetimeIndex')
             elif other.tz is not None:
-                raise Exception('Cannot join tz-naive with tz-aware DatetimeIndex')
+                raise Exception('Cannot join tz-naive with tz-aware '
+                                'DatetimeIndex')
 
             if self.tz != other.tz:
                 this = self.tz_convert('UTC')
@@ -920,7 +924,7 @@ class DatetimeIndex(Int64Index):
                               freq=left.offset)
 
     def __array_finalize__(self, obj):
-        if self.ndim == 0: # pragma: no cover
+        if self.ndim == 0:  # pragma: no cover
             return self.item()
 
         self.offset = getattr(obj, 'offset', None)
@@ -978,8 +982,8 @@ class DatetimeIndex(Int64Index):
 
     def _partial_date_slice(self, reso, parsed):
         if not self.is_monotonic:
-            raise TimeSeriesError('Partial indexing only valid for ordered time'
-                                  ' series')
+            raise TimeSeriesError('Partial indexing only valid for ordered '
+                                  'time series.')
 
         if reso == 'year':
             t1 = Timestamp(datetime(parsed.year, 1, 1))
@@ -989,7 +993,7 @@ class DatetimeIndex(Int64Index):
             t1 = Timestamp(datetime(parsed.year, parsed.month, 1))
             t2 = Timestamp(datetime(parsed.year, parsed.month, d))
         elif reso == 'quarter':
-            qe = (((parsed.month - 1) + 2) % 12) + 1 # two months ahead
+            qe = (((parsed.month - 1) + 2) % 12) + 1  # two months ahead
             d = lib.monthrange(parsed.year, qe)[1]   # at end of month
             t1 = Timestamp(datetime(parsed.year, parsed.month, 1))
             t2 = Timestamp(datetime(parsed.year, qe, d))
@@ -1361,7 +1365,6 @@ class DatetimeIndex(Int64Index):
         start_micros = _time_to_micros(start_time)
         end_micros = _time_to_micros(end_time)
 
-
         if include_start and include_end:
             lop = rop = operator.le
         elif include_start:
@@ -1523,10 +1526,10 @@ def _to_m8(key):
     return np.int64(lib.pydt_to_i8(key)).view(_NS_DTYPE)
 
 
-
 def _str_to_dt_array(arr, offset=None, dayfirst=None, yearfirst=None):
     def parser(x):
-        result = parse_time_string(x, offset, dayfirst=dayfirst, yearfirst=None)
+        result = parse_time_string(x, offset, dayfirst=dayfirst,
+                                   yearfirst=None)
         return result[0]
 
     arr = np.asarray(arr, dtype=object)
@@ -1535,7 +1538,7 @@ def _str_to_dt_array(arr, offset=None, dayfirst=None, yearfirst=None):
 
 
 _CACHE_START = Timestamp(datetime(1950, 1, 1))
-_CACHE_END   = Timestamp(datetime(2030, 1, 1))
+_CACHE_END = Timestamp(datetime(2030, 1, 1))
 
 _daterange_cache = {}
 
@@ -1548,13 +1551,16 @@ def _naive_in_cache_range(start, end):
             return False
         return _in_range(start, end, _CACHE_START, _CACHE_END)
 
+
 def _in_range(start, end, rng_start, rng_end):
     return start > rng_start and end < rng_end
 
+
 def _time_to_micros(time):
     seconds = time.hour * 60 * 60 + 60 * time.minute + time.second
     return 1000000 * seconds + time.microsecond
 
+
 def _utc_naive(dt):
     if dt is None:
         return dt
diff --git a/pandas/tseries/interval.py b/pandas/tseries/interval.py
index 58c16dcf0..104e088ee 100644
--- a/pandas/tseries/interval.py
+++ b/pandas/tseries/interval.py
@@ -2,6 +2,7 @@ import numpy as np
 
 from pandas.core.index import Index
 
+
 class Interval(object):
     """
     Represents an interval of time defined by two timestamps
@@ -11,6 +12,7 @@ class Interval(object):
         self.start = start
         self.end = end
 
+
 class PeriodInterval(object):
     """
     Represents an interval of time defined by two Period objects (time ordinals)
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 05861d937..1e3c17b7e 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -164,7 +164,7 @@ class DateOffset(object):
             raise TypeError('Cannot subtract datetime from offset!')
         elif type(other) == type(self):
             return self.__class__(self.n - other.n, **self.kwds)
-        else: # pragma: no cover
+        else:  # pragma: no cover
             raise TypeError('Cannot subtract %s from %s'
                             % (type(other), type(self)))
 
@@ -228,6 +228,7 @@ class DateOffset(object):
 
         return fstr
 
+
 class BusinessDay(CacheableOffset, DateOffset):
     """
     DateOffset subclass representing possibly n business days
@@ -343,6 +344,7 @@ class BusinessDay(CacheableOffset, DateOffset):
         else:
             raise Exception('Only know how to combine business day with '
                             'datetime or timedelta!')
+
     @classmethod
     def onOffset(cls, dt):
         return dt.weekday() < 5
@@ -379,7 +381,7 @@ class MonthBegin(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        if other.day > 1 and n <= 0: #then roll forward if n<=0
+        if other.day > 1 and n <= 0:  # then roll forward if n<=0
             n += 1
 
         other = other + relativedelta(months=n, day=1)
@@ -436,7 +438,7 @@ class BusinessMonthBegin(DateOffset, CacheableOffset):
             # as if rolled forward already
             n += 1
         elif other.day < first and n > 0:
-            other = other + timedelta(days=first-other.day)
+            other = other + timedelta(days=first - other.day)
             n -= 1
 
         other = other + relativedelta(months=n)
@@ -525,6 +527,7 @@ _weekday_dict = {
     6: 'SUN'
 }
 
+
 class WeekOfMonth(DateOffset, CacheableOffset):
     """
     Describes monthly dates like "the Tuesday of the 2nd week of each month"
@@ -631,7 +634,7 @@ class BQuarterEnd(DateOffset, CacheableOffset):
         elif n <= 0 and other.day > lastBDay and monthsToGo == 0:
             n = n + 1
 
-        other = other + relativedelta(months=monthsToGo + 3*n, day=31)
+        other = other + relativedelta(months=monthsToGo + 3 * n, day=31)
 
         if other.weekday() > 4:
             other = other - BDay()
@@ -686,7 +689,7 @@ class BQuarterBegin(DateOffset, CacheableOffset):
 
         monthsSince = (other.month - self.startingMonth) % 3
 
-        if n <= 0 and monthsSince != 0: # make sure to roll forward so negate
+        if n <= 0 and monthsSince != 0:  # make sure to roll forward so negate
             monthsSince = monthsSince - 3
 
         # roll forward if on same month later than first bday
@@ -697,7 +700,7 @@ class BQuarterBegin(DateOffset, CacheableOffset):
             n = n - 1
 
         # get the first bday for result
-        other = other + relativedelta(months=3*n - monthsSince)
+        other = other + relativedelta(months=3 * n - monthsSince)
         wkday, _ = lib.monthrange(other.year, other.month)
         first = _get_firstbday(wkday)
         result = datetime(other.year, other.month, first,
@@ -741,7 +744,7 @@ class QuarterEnd(DateOffset, CacheableOffset):
         if n > 0 and not (other.day >= days_in_month and monthsToGo == 0):
             n = n - 1
 
-        other = other + relativedelta(months=monthsToGo + 3*n, day=31)
+        other = other + relativedelta(months=monthsToGo + 3 * n, day=31)
 
         return other
 
@@ -783,7 +786,7 @@ class QuarterBegin(DateOffset, CacheableOffset):
             # after start, so come back an extra period as if rolled forward
             n = n + 1
 
-        other = other + relativedelta(months=3*n - monthsSince, day=1)
+        other = other + relativedelta(months=3 * n - monthsSince, day=1)
         return other
 
     @property
@@ -860,18 +863,17 @@ class BYearBegin(DateOffset, CacheableOffset):
 
         years = n
 
-
-        if n > 0: # roll back first for positive n
+        if n > 0:  # roll back first for positive n
             if (other.month < self.month or
                 (other.month == self.month and other.day < first)):
                 years -= 1
-        elif n <= 0: # roll forward
+        elif n <= 0:  # roll forward
             if (other.month > self.month or
                 (other.month == self.month and other.day > first)):
                 years += 1
 
         # set first bday for result
-        other = other + relativedelta(years = years)
+        other = other + relativedelta(years=years)
         wkday, days_in_month = lib.monthrange(other.year, self.month)
         first = _get_firstbday(wkday)
         return datetime(other.year, self.month, first)
@@ -909,6 +911,7 @@ class YearEnd(DateOffset, CacheableOffset):
             return datetime(year, self.month, days_in_month,
                             date.hour, date.minute, date.second,
                             date.microsecond)
+
         def _decrement(date):
             year = date.year if date.month > self.month else date.year - 1
             _, days_in_month = lib.monthrange(year, self.month)
@@ -967,7 +970,7 @@ class YearBegin(DateOffset, CacheableOffset):
                              other.microsecond)
             if n <= 0:
                 n = n + 1
-        other = other + relativedelta(years = n, day=1)
+        other = other + relativedelta(years=n, day=1)
         return other
 
     @classmethod
@@ -1039,10 +1042,12 @@ class Tick(DateOffset):
             raise TypeError('Unhandled type: %s' % type(other))
 
     _rule_base = 'undefined'
+
     @property
     def rule_code(self):
         return self._rule_base
 
+
 def _delta_to_tick(delta):
     if delta.microseconds == 0:
         if delta.seconds == 0:
@@ -1064,6 +1069,7 @@ def _delta_to_tick(delta):
         else:  # pragma: no cover
             return Nano(nanos)
 
+
 def _delta_to_nanoseconds(delta):
     if isinstance(delta, Tick):
         delta = delta.delta
@@ -1071,6 +1077,7 @@ def _delta_to_nanoseconds(delta):
             + delta.seconds * 1000000
             + delta.microseconds) * 1000
 
+
 class Day(Tick, CacheableOffset):
     _inc = timedelta(1)
     _rule_base = 'D'
@@ -1079,25 +1086,31 @@ class Day(Tick, CacheableOffset):
 
         return False
 
+
 class Hour(Tick):
     _inc = timedelta(0, 3600)
     _rule_base = 'H'
 
+
 class Minute(Tick):
     _inc = timedelta(0, 60)
     _rule_base = 'T'
 
+
 class Second(Tick):
     _inc = timedelta(0, 1)
     _rule_base = 'S'
 
+
 class Milli(Tick):
     _rule_base = 'L'
 
+
 class Micro(Tick):
     _inc = timedelta(microseconds=1)
     _rule_base = 'U'
 
+
 class Nano(Tick):
     _inc = 1
     _rule_base = 'N'
@@ -1114,9 +1127,9 @@ def _get_firstbday(wkday):
     If it's a saturday or sunday, increment first business day to reflect this
     """
     first = 1
-    if wkday == 5: # on Saturday
+    if wkday == 5:  # on Saturday
         first = 3
-    elif wkday == 6: # on Sunday
+    elif wkday == 6:  # on Sunday
         first = 2
     return first
 
diff --git a/pandas/tseries/period.py b/pandas/tseries/period.py
index 88991b57d..d7557e38c 100644
--- a/pandas/tseries/period.py
+++ b/pandas/tseries/period.py
@@ -28,6 +28,7 @@ def _period_field_accessor(name, alias):
     f.__name__ = name
     return property(f)
 
+
 def _field_accessor(name, alias):
     def f(self):
         base, mult = _gfc(self.freq)
@@ -386,6 +387,7 @@ class Period(object):
         base, mult = _gfc(self.freq)
         return plib.period_format(self.ordinal, base, fmt)
 
+
 def _get_date_and_freq(value, freq):
     value = value.upper()
     dt, _, reso = parse_time_string(value, freq)
@@ -418,6 +420,7 @@ def _get_ordinals(data, freq):
     else:
         return lib.map_infer(data, f)
 
+
 def dt64arr_to_periodarr(data, freq):
     if data.dtype != np.dtype('M8[ns]'):
         raise ValueError('Wrong dtype: %s' % data.dtype)
@@ -828,7 +831,7 @@ class PeriodIndex(Int64Index):
 
                 # if our data is higher resolution than requested key, slice
                 if grp < freqn:
-                    iv = Period(asdt, freq=(grp,1))
+                    iv = Period(asdt, freq=(grp, 1))
                     ord1 = iv.asfreq(self.freq, how='S').ordinal
                     ord2 = iv.asfreq(self.freq, how='E').ordinal
 
@@ -836,7 +839,7 @@ class PeriodIndex(Int64Index):
                         raise KeyError(key)
 
                     pos = np.searchsorted(self.values, [ord1, ord2])
-                    key = slice(pos[0], pos[1]+1)
+                    key = slice(pos[0], pos[1] + 1)
                     return series[key]
                 else:
                     key = Period(asdt, freq=self.freq)
@@ -993,7 +996,7 @@ class PeriodIndex(Int64Index):
         return header + ['%s' % Period(x, freq=self.freq) for x in self]
 
     def __array_finalize__(self, obj):
-        if self.ndim == 0: # pragma: no cover
+        if self.ndim == 0:  # pragma: no cover
             return self.item()
 
         self.freq = getattr(obj, 'freq', None)
@@ -1088,10 +1091,11 @@ def _get_ordinal_range(start, end, periods, freq):
             data = np.arange(start.ordinal, start.ordinal + periods,
                              dtype=np.int64)
     else:
-        data = np.arange(start.ordinal, end.ordinal+1, dtype=np.int64)
+        data = np.arange(start.ordinal, end.ordinal + 1, dtype=np.int64)
 
     return data, freq
 
+
 def _range_from_fields(year=None, month=None, quarter=None, day=None,
                        hour=None, minute=None, second=None, freq=None):
     if hour is None:
@@ -1131,6 +1135,7 @@ def _range_from_fields(year=None, month=None, quarter=None, day=None,
 
     return np.array(ordinals, dtype=np.int64), freq
 
+
 def _make_field_arrays(*fields):
     length = None
     for x in fields:
@@ -1157,6 +1162,7 @@ def _ordinal_from_fields(year, month, quarter, day, hour, minute,
 
     return plib.period_ordinal(year, month, day, hour, minute, second, base)
 
+
 def _quarter_to_myear(year, quarter, freq):
     if quarter is not None:
         if quarter <= 0 or quarter > 4:
@@ -1179,9 +1185,11 @@ def _validate_end_alias(how):
         raise ValueError('How must be one of S or E')
     return how
 
+
 def pnow(freq=None):
     return Period(datetime.now(), freq=freq)
 
+
 def period_range(start=None, end=None, periods=None, freq='D', name=None):
     """
     Return a fixed frequency datetime index, with day (calendar) as the default
@@ -1206,6 +1214,7 @@ def period_range(start=None, end=None, periods=None, freq='D', name=None):
     return PeriodIndex(start=start, end=end, periods=periods,
                        freq=freq, name=name)
 
+
 def _period_rule_to_timestamp_rule(freq, how='end'):
     how = how.lower()
     if how in ('end', 'e'):
diff --git a/pandas/tseries/plotting.py b/pandas/tseries/plotting.py
index 6f1772dd3..70b36ff7e 100644
--- a/pandas/tseries/plotting.py
+++ b/pandas/tseries/plotting.py
@@ -26,6 +26,7 @@ units.registry[Period] = PeriodConverter()
 #----------------------------------------------------------------------
 # Plotting functions and monkey patches
 
+
 def tsplot(series, plotf, **kwargs):
     """
     Plots a Series on the given Matplotlib axes or the current axes
@@ -49,7 +50,7 @@ def tsplot(series, plotf, **kwargs):
 
     freq = _get_freq(ax, series)
     # resample against axes freq if necessary
-    if freq is None: # pragma: no cover
+    if freq is None:  # pragma: no cover
         raise ValueError('Cannot use dynamic axis without frequency info')
     else:
         # Convert DatetimeIndex to PeriodIndex
@@ -74,7 +75,7 @@ def tsplot(series, plotf, **kwargs):
     if style is not None:
         args.append(style)
 
-    lines = plotf(ax, *args,  **kwargs)
+    lines = plotf(ax, *args, **kwargs)
     label = kwargs.get('label', None)
 
     # set date formatter, locators and rescale limits
@@ -84,14 +85,15 @@ def tsplot(series, plotf, **kwargs):
 
     return lines
 
+
 def _maybe_resample(series, ax, freq, plotf, kwargs):
     ax_freq = _get_ax_freq(ax)
     if ax_freq is not None and freq != ax_freq:
-        if frequencies.is_superperiod(freq, ax_freq): # upsample input
+        if frequencies.is_superperiod(freq, ax_freq):  # upsample input
             series = series.copy()
             series.index = series.index.asfreq(ax_freq)
             freq = ax_freq
-        elif _is_sup(freq, ax_freq): # one is weekly
+        elif _is_sup(freq, ax_freq):  # one is weekly
             how = kwargs.pop('how', 'last')
             series = series.resample('D', how=how).dropna()
             series = series.resample(ax_freq, how=how).dropna()
@@ -103,6 +105,7 @@ def _maybe_resample(series, ax, freq, plotf, kwargs):
             raise ValueError('Incompatible frequency conversion')
     return freq, ax_freq, series
 
+
 def _get_ax_freq(ax):
     ax_freq = getattr(ax, 'freq', None)
     if ax_freq is None:
@@ -112,14 +115,17 @@ def _get_ax_freq(ax):
             ax_freq = getattr(ax.right_ax, 'freq', None)
     return ax_freq
 
+
 def _is_sub(f1, f2):
     return ((f1.startswith('W') and frequencies.is_subperiod('D', f2)) or
             (f2.startswith('W') and frequencies.is_subperiod(f1, 'D')))
 
+
 def _is_sup(f1, f2):
     return ((f1.startswith('W') and frequencies.is_superperiod('D', f2)) or
             (f2.startswith('W') and frequencies.is_superperiod(f1, 'D')))
 
+
 def _upsample_others(ax, freq, plotf, kwargs):
     legend = ax.get_legend()
     lines, labels = _replot_ax(ax, freq, plotf, kwargs)
@@ -142,6 +148,7 @@ def _upsample_others(ax, freq, plotf, kwargs):
             title = None
         ax.legend(lines, labels, loc='best', title=title)
 
+
 def _replot_ax(ax, freq, plotf, kwargs):
     data = getattr(ax, '_plot_data', None)
     ax._plot_data = []
@@ -162,6 +169,7 @@ def _replot_ax(ax, freq, plotf, kwargs):
 
     return lines, labels
 
+
 def _decorate_axes(ax, freq, kwargs):
     ax.freq = freq
     xaxis = ax.get_xaxis()
@@ -173,6 +181,7 @@ def _decorate_axes(ax, freq, kwargs):
     ax.view_interval = None
     ax.date_axis_info = None
 
+
 def _maybe_mask(series):
     mask = isnull(series)
     if mask.any():
@@ -183,6 +192,7 @@ def _maybe_mask(series):
         args = [series.index, series]
     return args
 
+
 def _get_freq(ax, series):
     # get frequency from data
     freq = getattr(series.index, 'freq', None)
@@ -205,6 +215,7 @@ def _get_freq(ax, series):
 
     return freq
 
+
 def _get_xlim(lines):
     left, right = np.inf, -np.inf
     for l in lines:
@@ -213,6 +224,7 @@ def _get_xlim(lines):
         right = max(x[-1].ordinal, right)
     return left, right
 
+
 def get_datevalue(date, freq):
     if isinstance(date, Period):
         return date.asfreq(freq).ordinal
@@ -228,6 +240,7 @@ def get_datevalue(date, freq):
 # Patch methods for subplot. Only format_dateaxis is currently used.
 # Do we need the rest for convenience?
 
+
 def format_dateaxis(subplot, freq):
     """
     Pretty-formats the date axis (x-axis).
diff --git a/pandas/tseries/resample.py b/pandas/tseries/resample.py
index be5098ded..1fb1725f1 100644
--- a/pandas/tseries/resample.py
+++ b/pandas/tseries/resample.py
@@ -184,7 +184,7 @@ class TimeGrouper(CustomGrouper):
         # Determine if we're downsampling
         if axlabels.freq is not None or axlabels.inferred_freq is not None:
             if len(grouper.binlabels) < len(axlabels) or self.how is not None:
-                grouped  = obj.groupby(grouper, axis=self.axis)
+                grouped = obj.groupby(grouper, axis=self.axis)
                 result = grouped.aggregate(self._agg_method)
             else:
                 # upsampling shortcut
@@ -193,7 +193,7 @@ class TimeGrouper(CustomGrouper):
                                      limit=self.limit)
         else:
             # Irregular data, have to use groupby
-            grouped  = obj.groupby(grouper, axis=self.axis)
+            grouped = obj.groupby(grouper, axis=self.axis)
             result = grouped.aggregate(self._agg_method)
 
             if self.fill_method is not None:
@@ -265,7 +265,6 @@ def _take_new_index(obj, indexer, new_index, axis=0):
         raise NotImplementedError
 
 
-
 def _get_range_edges(axis, offset, closed='left', base=0):
     if isinstance(offset, basestring):
         offset = to_offset(offset)
@@ -278,7 +277,7 @@ def _get_range_edges(axis, offset, closed='left', base=0):
                                           closed=closed, base=base)
 
     first, last = axis[0], axis[-1]
-    if not isinstance(offset, Tick):# and first.time() != last.time():
+    if not isinstance(offset, Tick):  # and first.time() != last.time():
         # hack!
         first = tools.normalize_date(first)
         last = tools.normalize_date(last)
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index 36a9f32bd..9e1c451c4 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -17,9 +17,10 @@ try:
         dateutil.__version__ == '2.0'):  # pragma: no cover
         raise Exception('dateutil 2.0 incompatible with Python 2.x, you must '
                         'install version 1.5 or 2.1+!')
-except ImportError: # pragma: no cover
+except ImportError:  # pragma: no cover
     print 'Please install python-dateutil via easy_install or some method!'
-    raise # otherwise a 2nd import won't show the message
+    raise  # otherwise a 2nd import won't show the message
+
 
 def _infer_tzinfo(start, end):
     def _infer(a, b):
@@ -124,7 +125,6 @@ class DateParseError(ValueError):
     pass
 
 
-
 # patterns for quarters like '4Q2005', '05Q1'
 qpat1full = re.compile(r'(\d)Q(\d\d\d\d)')
 qpat2full = re.compile(r'(\d\d\d\d)Q(\d)')
@@ -132,6 +132,7 @@ qpat1 = re.compile(r'(\d)Q(\d\d)')
 qpat2 = re.compile(r'(\d\d)Q(\d)')
 ypat = re.compile(r'(\d\d\d\d)$')
 
+
 def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
     """
     Try hard to parse datetime string, leveraging dateutil plus some extra
@@ -161,7 +162,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
 
     arg = arg.upper()
 
-    default = datetime(1,1,1).replace(hour=0, minute=0,
+    default = datetime(1, 1, 1).replace(hour=0, minute=0,
                                       second=0, microsecond=0)
 
     # special handling for possibilities eg, 2Q2005, 2Q05, 2005Q1, 05Q1
@@ -239,7 +240,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
                  "minute", "second", "microsecond"]:
         can_be_zero = ['hour', 'minute', 'second', 'microsecond']
         value = getattr(parsed, attr)
-        if value is not None and value != 0: # or attr in can_be_zero):
+        if value is not None and value != 0:  # or attr in can_be_zero):
             repl[attr] = value
             if not stopped:
                 reso = attr
@@ -249,6 +250,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
     ret = default.replace(**repl)
     return ret, parsed, reso  # datetime, resolution
 
+
 def _attempt_monthly(val):
     pats = ['%Y-%m', '%m-%Y', '%b %Y', '%b-%Y']
     for pat in pats:
@@ -269,7 +271,7 @@ def _try_parse_monthly(arg):
         add_base = True
         y = int(arg[:2])
         m = int(arg[2:4])
-    elif len(arg) >= 6: # 201201
+    elif len(arg) >= 6:  # 201201
         y = int(arg[:4])
         m = int(arg[4:6])
     if add_base:
@@ -287,6 +289,7 @@ def format(dt):
 
 OLE_TIME_ZERO = datetime(1899, 12, 30, 0, 0, 0)
 
+
 def ole2datetime(oledt):
     """function for converting excel date to normal date format"""
     val = float(oledt)
diff --git a/pandas/tseries/util.py b/pandas/tseries/util.py
index 4b2977123..0702bc403 100644
--- a/pandas/tseries/util.py
+++ b/pandas/tseries/util.py
@@ -3,6 +3,7 @@ import numpy as np
 from pandas.core.frame import DataFrame
 import pandas.core.nanops as nanops
 
+
 def pivot_annual(series, freq=None):
     """
     Group a series by years, taking leap years into account.
@@ -71,6 +72,7 @@ def pivot_annual(series, freq=None):
 
     return DataFrame(values, index=years, columns=columns)
 
+
 def isleapyear(year):
     """
     Returns true if year is a leap year.
diff --git a/pandas/util/clipboard.py b/pandas/util/clipboard.py
index b21800015..4136df072 100644
--- a/pandas/util/clipboard.py
+++ b/pandas/util/clipboard.py
@@ -7,6 +7,7 @@ Used under the terms of the BSD license
 import subprocess
 import sys
 
+
 def clipboard_get():
     """ Get text from the clipboard.
     """
@@ -22,6 +23,7 @@ def clipboard_get():
             pass
     return tkinter_clipboard_get()
 
+
 def clipboard_set(text):
     """ Get text from the clipboard.
     """
@@ -37,6 +39,7 @@ def clipboard_set(text):
             pass
     xsel_clipboard_set(text)
 
+
 def win32_clipboard_get():
     """ Get the current clipboard's text on Windows.
 
@@ -54,6 +57,7 @@ def win32_clipboard_get():
     win32clipboard.CloseClipboard()
     return text
 
+
 def osx_clipboard_get():
     """ Get the clipboard's text on OS X.
     """
@@ -64,6 +68,7 @@ def osx_clipboard_get():
     text = text.replace('\r', '\n')
     return text
 
+
 def tkinter_clipboard_get():
     """ Get the clipboard's text using Tkinter.
 
@@ -83,6 +88,7 @@ def tkinter_clipboard_get():
     root.destroy()
     return text
 
+
 def win32_clipboard_set(text):
     # idiosyncratic win32 import issues
     import pywintypes as _
@@ -94,9 +100,11 @@ def win32_clipboard_set(text):
     finally:
         win32clipboard.CloseClipboard()
 
+
 def _fix_line_endings(text):
     return '\r\n'.join(text.splitlines())
 
+
 def osx_clipboard_set(text):
     """ Get the clipboard's text on OS X.
     """
@@ -104,6 +112,7 @@ def osx_clipboard_set(text):
         stdin=subprocess.PIPE)
     p.communicate(input=text)
 
+
 def xsel_clipboard_set(text):
     from subprocess import Popen, PIPE
     p = Popen(['xsel', '-bi'], stdin=PIPE)
diff --git a/pandas/util/compat.py b/pandas/util/compat.py
index 213f06552..894f94d11 100644
--- a/pandas/util/compat.py
+++ b/pandas/util/compat.py
@@ -9,6 +9,6 @@ except ImportError:  # python 2.5
         pools = map(tuple, args) * kwds.get('repeat', 1)
         result = [[]]
         for pool in pools:
-            result = [x+[y] for x in result for y in pool]
+            result = [x + [y] for x in result for y in pool]
         for prod in result:
             yield tuple(prod)
diff --git a/pandas/util/counter.py b/pandas/util/counter.py
index f23f6e6fb..29e8906fd 100644
--- a/pandas/util/counter.py
+++ b/pandas/util/counter.py
@@ -8,9 +8,10 @@ from operator import itemgetter as _itemgetter
 try:
     from collections import Mapping
 except:
-    # ABCs were only introduced in Python 2.6, so this is a hack for Python 2.5:
+    # ABCs were only introduced in Python 2.6, so this is a hack for Python 2.5
     Mapping = dict
 
+
 class Counter(dict):
     '''Dict subclass for counting hashable items.  Sometimes called a bag
     or multiset.  Elements are stored as dictionary keys and their counts
@@ -50,8 +51,8 @@ class Counter(dict):
     in the counter until the entry is deleted or the counter is cleared:
 
     >>> c = Counter('aaabbc')
-    >>> c['b'] -= 2                     # reduce the count of 'b' by two
-    >>> c.most_common()                 # 'b' is still in, but its count is zero
+    >>> c['b'] -= 2                    # reduce the count of 'b' by two
+    >>> c.most_common()                # 'b' is still in, but its count is zero
     [('a', 3), ('c', 1), ('b', 0)]
 
     '''
@@ -67,10 +68,10 @@ class Counter(dict):
         from an input iterable.  Or, initialize the count from another mapping
         of elements to their counts.
 
-        >>> c = Counter()                           # a new, empty counter
-        >>> c = Counter('gallahad')                 # a new counter from an iterable
-        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping
-        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args
+        >>> c = Counter()                     # a new, empty counter
+        >>> c = Counter('gallahad')           # a new counter from an iterable
+        >>> c = Counter({'a': 4, 'b': 2})     # a new counter from a mapping
+        >>> c = Counter(a=4, b=2)             # a new counter from keyword args
 
         '''
         super(Counter, self).__init__()
@@ -152,7 +153,8 @@ class Counter(dict):
                     for elem, count in iterable.iteritems():
                         self[elem] = self_get(elem, 0) + count
                 else:
-                    super(Counter, self).update(iterable) # fast path when counter is empty
+                    # fast path when counter is empty
+                    super(Counter, self).update(iterable)
             else:
                 self_get = self.get
                 for elem in iterable:
@@ -195,7 +197,9 @@ class Counter(dict):
         return self.__class__, (dict(self),)
 
     def __delitem__(self, elem):
-        'Like dict.__delitem__() but does not raise KeyError for missing values.'
+        """
+        Like dict.__delitem__() but does not raise KeyError for missing values.
+        """
         if elem in self:
             super(Counter, self).__delitem__(elem)
 
diff --git a/pandas/util/decorators.py b/pandas/util/decorators.py
index 5cd87a1e9..bef3ffc56 100644
--- a/pandas/util/decorators.py
+++ b/pandas/util/decorators.py
@@ -3,8 +3,10 @@ from pandas.lib import cache_readonly
 import sys
 import warnings
 
+
 def deprecate(name, alternative):
     alt_name = alternative.func_name
+
     def wrapper(*args, **kwargs):
         warnings.warn("%s is deprecated. Use %s instead" % (name, alt_name),
                       FutureWarning)
@@ -14,6 +16,7 @@ def deprecate(name, alternative):
 # Substitution and Appender are derived from matplotlib.docstring (1.1.0)
 # module http://matplotlib.sourceforge.net/users/license.html
 
+
 class Substitution(object):
     """
     A decorator to take a function's docstring and perform string
@@ -66,6 +69,7 @@ class Substitution(object):
         result.params = params
         return result
 
+
 class Appender(object):
     """
     A function decorator that will append an addendum to the docstring
@@ -99,12 +103,14 @@ class Appender(object):
         func.__doc__ = ''.join(docitems)
         return func
 
+
 def indent(text, indents=1):
     if not text or type(text) != str:
         return ''
     jointext = ''.join(['\n'] + ['    '] * indents)
     return jointext.join(text.split('\n'))
 
+
 def suppress_stdout(f):
     def wrapped(*args, **kwargs):
         try:
@@ -120,6 +126,7 @@ class KnownFailureTest(Exception):
     '''Raise this exception to mark a test as a known failing test.'''
     pass
 
+
 def knownfailureif(fail_condition, msg=None):
     """
     Make function raise KnownFailureTest exception if given condition is true.
@@ -163,6 +170,7 @@ def knownfailureif(fail_condition, msg=None):
         # Local import to avoid a hard nose dependency and only incur the
         # import time overhead at actual test-time.
         import nose
+
         def knownfailer(*args, **kwargs):
             if fail_val():
                 raise KnownFailureTest, msg
diff --git a/pandas/util/py3compat.py b/pandas/util/py3compat.py
index 9a602155e..0b00e5211 100644
--- a/pandas/util/py3compat.py
+++ b/pandas/util/py3compat.py
@@ -16,6 +16,7 @@ else:
     # Python 2
     import re
     _name_re = re.compile(r"[a-zA-Z_][a-zA-Z0-9_]*$")
+
     def isidentifier(s, dotted=False):
         return bool(_name_re.match(s))
 
@@ -34,4 +35,3 @@ try:
     from io import BytesIO
 except:
     from cStringIO import StringIO as BytesIO
-
diff --git a/pandas/util/terminal.py b/pandas/util/terminal.py
index 4278f35ba..312f54b52 100644
--- a/pandas/util/terminal.py
+++ b/pandas/util/terminal.py
@@ -14,28 +14,29 @@ on linux, os x, windows and cygwin (windows).
 
 import os
 
-__all__=['get_terminal_size']
+__all__ = ['get_terminal_size']
 
 
 def get_terminal_size():
-   import platform
-   current_os = platform.system()
-   tuple_xy=None
-   if current_os == 'Windows':
-       tuple_xy = _get_terminal_size_windows()
-       if tuple_xy is None:
-          tuple_xy = _get_terminal_size_tput()
-          # needed for window's python in cygwin's xterm!
-   if current_os == 'Linux' or \
-      current_os == 'Darwin' or \
-      current_os.startswith('CYGWIN'):
-       tuple_xy = _get_terminal_size_linux()
-   if tuple_xy is None:
-       tuple_xy = (80, 25)      # default value
-   return tuple_xy
+    import platform
+    current_os = platform.system()
+    tuple_xy = None
+    if current_os == 'Windows':
+        tuple_xy = _get_terminal_size_windows()
+        if tuple_xy is None:
+            tuple_xy = _get_terminal_size_tput()
+            # needed for window's python in cygwin's xterm!
+    if current_os == 'Linux' or \
+       current_os == 'Darwin' or \
+       current_os.startswith('CYGWIN'):
+        tuple_xy = _get_terminal_size_linux()
+    if tuple_xy is None:
+        tuple_xy = (80, 25)      # default value
+    return tuple_xy
+
 
 def _get_terminal_size_windows():
-    res=None
+    res = None
     try:
         from ctypes import windll, create_string_buffer
 
@@ -58,32 +59,36 @@ def _get_terminal_size_windows():
     else:
         return None
 
+
 def _get_terminal_size_tput():
     # get terminal width
     # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width
     # -height-of-a-terminal-window
     try:
-       import subprocess
-       proc = subprocess.Popen(["tput", "cols"],
-                               stdin=subprocess.PIPE,
-                               stdout=subprocess.PIPE)
-       output=proc.communicate(input=None)
-       cols=int(output[0])
-       proc=subprocess.Popen(["tput", "lines"],
-                             stdin=subprocess.PIPE,
-                             stdout=subprocess.PIPE)
-       output=proc.communicate(input=None)
-       rows=int(output[0])
-       return (cols,rows)
+        import subprocess
+        proc = subprocess.Popen(["tput", "cols"],
+                                stdin=subprocess.PIPE,
+                                stdout=subprocess.PIPE)
+        output = proc.communicate(input=None)
+        cols = int(output[0])
+        proc = subprocess.Popen(["tput", "lines"],
+                                stdin=subprocess.PIPE,
+                                stdout=subprocess.PIPE)
+        output = proc.communicate(input=None)
+        rows = int(output[0])
+        return (cols, rows)
     except:
-       return None
+        return None
 
 
 def _get_terminal_size_linux():
     def ioctl_GWINSZ(fd):
         try:
-            import fcntl, termios, struct, os
-            cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ,'1234'))
+            import fcntl
+            import termios
+            import struct
+            import os
+            cr = struct.unpack('hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))
         except:
             return None
         return cr
@@ -97,12 +102,12 @@ def _get_terminal_size_linux():
             pass
     if not cr or cr == (0, 0):
         try:
-           from os import environ as env
-           cr = (env['LINES'], env['COLUMNS'])
+            from os import environ as env
+            cr = (env['LINES'], env['COLUMNS'])
         except:
             return None
     return int(cr[1]), int(cr[0])
 
 if __name__ == "__main__":
     sizex, sizey = get_terminal_size()
-    print  'width =', sizex, 'height =', sizey
+    print 'width =', sizex, 'height =', sizey
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 904426731..866f39490 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -33,17 +33,20 @@ Panel = panel.Panel
 N = 30
 K = 4
 
+
 def rands(n):
     choices = string.ascii_letters + string.digits
     return ''.join([random.choice(choices) for _ in xrange(n)])
 
+
 def randu(n):
-    choices = u"".join(map(unichr,range(1488,1488+26))) + string.digits
+    choices = u"".join(map(unichr, range(1488, 1488 + 26))) + string.digits
     return ''.join([random.choice(choices) for _ in xrange(n)])
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Console debugging tools
 
+
 def debug(f, *args, **kwargs):
     from pdb import Pdb as OldPdb
     try:
@@ -55,10 +58,12 @@ def debug(f, *args, **kwargs):
     pdb = Pdb(**kw)
     return pdb.runcall(f, *args, **kwargs)
 
+
 def pudebug(f, *args, **kwargs):
     import pudb
     return pudb.runcall(f, *args, **kwargs)
 
+
 def set_trace():
     from IPython.core.debugger import Pdb
     try:
@@ -67,17 +72,20 @@ def set_trace():
         from pdb import Pdb as OldPdb
         OldPdb().set_trace(sys._getframe().f_back)
 
-#-------------------------------------------------------------------------------
+#------------------------------------------------------------------------------
 # Comparators
 
+
 def equalContents(arr1, arr2):
     """Checks if the set of unique elements of arr1 and arr2 are equivalent.
     """
     return frozenset(arr1) == frozenset(arr2)
 
+
 def isiterable(obj):
     return hasattr(obj, '__iter__')
 
+
 def assert_almost_equal(a, b):
     if isinstance(a, dict) or isinstance(b, dict):
         return assert_dict_equal(a, b)
@@ -109,13 +117,15 @@ def assert_almost_equal(a, b):
                 a, b, decimal=5, err_msg=err_msg(a, b), verbose=False)
         else:
             np.testing.assert_almost_equal(
-                1, a/b, decimal=5, err_msg=err_msg(a, b), verbose=False)
+                1, a / b, decimal=5, err_msg=err_msg(a, b), verbose=False)
     else:
         assert(a == b)
 
+
 def is_sorted(seq):
     return assert_almost_equal(seq, np.sort(np.array(seq)))
 
+
 def assert_dict_equal(a, b, compare_keys=True):
     a_keys = frozenset(a.keys())
     b_keys = frozenset(b.keys())
@@ -126,6 +136,7 @@ def assert_dict_equal(a, b, compare_keys=True):
     for k in a_keys:
         assert_almost_equal(a[k], b[k])
 
+
 def assert_series_equal(left, right, check_dtype=True,
                         check_index_type=False,
                         check_index_freq=False,
@@ -144,6 +155,7 @@ def assert_series_equal(left, right, check_dtype=True,
         assert(getattr(left, 'freqstr', None) ==
                getattr(right, 'freqstr', None))
 
+
 def assert_frame_equal(left, right, check_index_type=False,
                        check_column_type=False,
                        check_frame_type=False):
@@ -167,6 +179,7 @@ def assert_frame_equal(left, right, check_index_type=False,
         assert(left.columns.dtype == right.columns.dtype)
         assert(left.columns.inferred_type == right.columns.inferred_type)
 
+
 def assert_panel_equal(left, right, check_panel_type=False):
     if check_panel_type:
         assert(type(left) == type(right))
@@ -182,102 +195,125 @@ def assert_panel_equal(left, right, check_panel_type=False):
     for col in right:
         assert(col in left)
 
+
 def assert_contains_all(iterable, dic):
     for k in iterable:
         assert(k in dic)
 
+
 def getCols(k):
     return string.ascii_uppercase[:k]
 
+
 def makeStringIndex(k):
     return Index([rands(10) for _ in xrange(k)])
 
+
 def makeUnicodeIndex(k):
     return Index([randu(10) for _ in xrange(k)])
 
+
 def makeIntIndex(k):
     return Index(range(k))
 
+
 def makeFloatIndex(k):
     values = sorted(np.random.random_sample(k)) - np.random.random_sample(1)
     return Index(values * (10 ** np.random.randint(0, 9)))
 
+
 def makeFloatSeries():
     index = makeStringIndex(N)
     return Series(randn(N), index=index)
 
+
 def makeStringSeries():
     index = makeStringIndex(N)
     return Series(randn(N), index=index)
 
+
 def makeObjectSeries():
     dateIndex = makeDateIndex(N)
     dateIndex = Index(dateIndex, dtype=object)
     index = makeStringIndex(N)
     return Series(dateIndex, index=index)
 
+
 def getSeriesData():
     index = makeStringIndex(N)
     return dict((c, Series(randn(N), index=index)) for c in getCols(K))
 
+
 def makeDataFrame():
     data = getSeriesData()
     return DataFrame(data)
 
+
 def getArangeMat():
     return np.arange(N * K).reshape((N, K))
 
+
 def getMixedTypeDict():
     index = Index(['a', 'b', 'c', 'd', 'e'])
 
     data = {
-        'A' : [0., 1., 2., 3., 4.],
-        'B' : [0., 1., 0., 1., 0.],
-        'C' : ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'],
-        'D' : bdate_range('1/1/2009', periods=5)
+        'A': [0., 1., 2., 3., 4.],
+        'B': [0., 1., 0., 1., 0.],
+        'C': ['foo1', 'foo2', 'foo3', 'foo4', 'foo5'],
+        'D': bdate_range('1/1/2009', periods=5)
     }
 
     return index, data
 
+
 def makeDateIndex(k):
-    dt = datetime(2000,1,1)
+    dt = datetime(2000, 1, 1)
     dr = bdate_range(dt, periods=k)
     return DatetimeIndex(dr)
 
+
 def makePeriodIndex(k):
-    dt = datetime(2000,1,1)
+    dt = datetime(2000, 1, 1)
     dr = PeriodIndex(start=dt, periods=k, freq='B')
     return dr
 
+
 def makeTimeSeries(nper=None):
     if nper is None:
         nper = N
     return Series(randn(nper), index=makeDateIndex(nper))
 
+
 def makePeriodSeries(nper=None):
     if nper is None:
         nper = N
     return Series(randn(nper), index=makePeriodIndex(nper))
 
+
 def getTimeSeriesData():
     return dict((c, makeTimeSeries()) for c in getCols(K))
 
+
 def makeTimeDataFrame():
     data = getTimeSeriesData()
     return DataFrame(data)
 
+
 def getPeriodData():
     return dict((c, makePeriodSeries()) for c in getCols(K))
 
+
 def makePeriodFrame():
     data = getPeriodData()
     return DataFrame(data)
 
+
 def makePanel():
     cols = ['Item' + c for c in string.ascii_uppercase[:K - 1]]
     data = dict((c, makeTimeDataFrame()) for c in cols)
     return Panel.fromDict(data)
 
+
 def add_nans(panel):
     I, J, N = panel.shape
     for i, item in enumerate(panel.items):
@@ -285,6 +321,7 @@ def add_nans(panel):
         for j, col in enumerate(dm.columns):
             dm[col][:i + j] = np.NaN
 
+
 class TestSubDict(dict):
     def __init__(self, *args, **kwargs):
         dict.__init__(self, *args, **kwargs)
@@ -327,7 +364,7 @@ def package_check(pkg_name, version=None, app='pandas', checker=LooseVersion,
     else:
         msg = 'module requires %s' % pkg_name
     if version:
-      msg += ' with version >= %s' % (version,)
+        msg += ' with version >= %s' % (version,)
     try:
         mod = __import__(pkg_name)
     except ImportError:
@@ -341,6 +378,7 @@ def package_check(pkg_name, version=None, app='pandas', checker=LooseVersion,
     if checker(have_version) < checker(version):
         raise exc_failed_check(msg)
 
+
 def skip_if_no_package(*args, **kwargs):
     """Raise SkipTest if package_check fails
 
@@ -357,6 +395,8 @@ def skip_if_no_package(*args, **kwargs):
 #
 # Additional tags decorators for nose
 #
+
+
 def network(t):
     """
     Label a test as requiring network connection.
@@ -411,14 +451,15 @@ class SimpleMock(object):
         attrs = kwds.get("attrs", {})
         for k, v in zip(args[::2], args[1::2]):
             # dict comprehensions break 2.6
-            attrs[k]=v
+            attrs[k] = v
         self.attrs = attrs
         self.obj = obj
 
-    def __getattribute__(self,name):
+    def __getattribute__(self, name):
         attrs = object.__getattribute__(self, "attrs")
         obj = object.__getattribute__(self, "obj")
-        return attrs.get(name, type(obj).__getattribute__(obj,name))
+        return attrs.get(name, type(obj).__getattribute__(obj, name))
+
 
 @contextmanager
 def stdin_encoding(encoding=None):
