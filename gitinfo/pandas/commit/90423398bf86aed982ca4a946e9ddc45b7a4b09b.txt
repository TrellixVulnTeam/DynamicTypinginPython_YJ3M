commit 90423398bf86aed982ca4a946e9ddc45b7a4b09b
Author: Phillip Cloud <cpcloud@gmail.com>
Date:   Sat Jun 15 21:34:56 2013 -0400

    ENH: add new computation module and toplevel eval function

diff --git a/pandas/__init__.py b/pandas/__init__.py
index 03681d3fa..c4c012d6c 100644
--- a/pandas/__init__.py
+++ b/pandas/__init__.py
@@ -42,6 +42,7 @@ from pandas.sparse.api import *
 from pandas.stats.api import *
 from pandas.tseries.api import *
 from pandas.io.api import *
+from pandas.computation.api import *
 
 from pandas.util.testing import debug
 
diff --git a/pandas/computation/__init__.py b/pandas/computation/__init__.py
new file mode 100644
index 000000000..e69de29bb
diff --git a/pandas/computation/align.py b/pandas/computation/align.py
new file mode 100644
index 000000000..529fe84fd
--- /dev/null
+++ b/pandas/computation/align.py
@@ -0,0 +1,220 @@
+from functools import partial, wraps
+from itertools import izip
+
+import numpy as np
+
+import pandas as pd
+import pandas.core.common as com
+from pandas.computation.ops import is_const
+from pandas.computation.common import flatten
+
+
+def _align_core_single_unary_op(term):
+    if isinstance(term.value, np.ndarray) and not com.is_series(term.value):
+        typ = partial(np.asanyarray, dtype=term.value.dtype)
+    else:
+        typ = type(term.value)
+    ret = typ,
+
+    if not hasattr(term.value, 'axes'):
+        ret += None,
+    else:
+        ret += _zip_axes_from_type(typ, term.value.axes),
+    return ret
+
+
+def _zip_axes_from_type(typ, new_axes):
+    axes = {}
+    for ax_ind, ax_name in typ._AXIS_NAMES.iteritems():
+        axes[ax_name] = new_axes[ax_ind]
+    return axes
+
+
+def _maybe_promote_shape(values, naxes):
+    # test to see if we have an array else leave since must be a number
+    if not isinstance(values, np.ndarray):
+        return values
+
+    ndims = values.ndim
+    if ndims > naxes:
+        raise AssertionError('cannot have more dims than axes, '
+                             '{0} > {1}'.format(ndims, naxes))
+    if ndims == naxes:
+        return values
+
+    ndim = set(xrange(ndims))
+    nax = set(xrange(naxes))
+
+    axes_slice = [slice(None)] * naxes
+
+    # symmetric difference of numaxes and ndims
+    slices = nax - ndim
+
+    if ndims == naxes:
+        if slices:
+            raise AssertionError('slices should be empty if ndims == naxes '
+                                 '{0}'.format(slices))
+    else:
+        if not slices:
+            raise AssertionError('slices should NOT be empty if ndim != naxes '
+                                 '{0}'.format(slices))
+
+    for sl in slices:
+        axes_slice[sl] = np.newaxis
+
+    return values[tuple(axes_slice)]
+
+
+def _any_pandas_objects(terms):
+    """Check a sequence of terms for instances of PandasObject."""
+    return any(com.is_pd_obj(term.value) for term in terms)
+
+
+def _filter_special_cases(f):
+    @wraps(f)
+    def wrapper(terms):
+        # single unary operand
+        if len(terms) == 1:
+            return _align_core_single_unary_op(terms[0])
+
+        # only scalars
+        elif all(term.isscalar for term in terms):
+            return np.result_type(*(term.value for term in terms)), None
+
+        # single element ndarrays
+        all_has_size = all(hasattr(term.value, 'size') for term in terms)
+        if (all_has_size and all(term.value.size == 1 for term in terms)):
+            return np.result_type(*(term.value for term in terms)), None
+
+        # no pandas so just punt to the evaluator
+        if not _any_pandas_objects(terms):
+            return np.result_type(*(term.value for term in terms)), None
+
+        return f(terms)
+    return wrapper
+
+
+@_filter_special_cases
+def _align_core(terms):
+    term_index = [i for i, term in enumerate(terms) if hasattr(term.value,
+                                                               'axes')]
+    term_dims = [terms[i].value.ndim for i in term_index]
+    ndims = pd.Series(dict(zip(term_index, term_dims)))
+
+    # initial axes are the axes of the largest-axis'd term
+    biggest = terms[ndims.idxmax()].value
+    typ = biggest._constructor
+    axes = biggest.axes
+    naxes = len(axes)
+
+    for term in (terms[i] for i in term_index):
+        for axis, items in enumerate(term.value.axes):
+            if com.is_series(term.value) and naxes > 1:
+                ax, itm = naxes - 1, term.value.index
+            else:
+                ax, itm = axis, items
+            axes[ax] = axes[ax].join(itm, how='outer')
+
+    for i, ndim in ndims.iteritems():
+        for axis, items in izip(xrange(ndim), axes):
+            ti = terms[i].value
+
+            if hasattr(ti, 'reindex_axis'):
+                transpose = com.is_series(ti) and naxes > 1
+
+                if transpose:
+                    f = partial(ti.reindex, index=axes[naxes - 1], copy=False)
+                else:
+                    f = partial(ti.reindex_axis, items, axis=axis, copy=False)
+
+                if pd.lib.is_bool_array(ti.values):
+                    r = f(fill_value=True)
+                else:
+                    r = f()
+
+                terms[i].update(r)
+
+        res = _maybe_promote_shape(terms[i].value.T if transpose else
+                                   terms[i].value, naxes)
+        res = res.T if transpose else res
+
+        try:
+            v = res.values
+        except AttributeError:
+            v = res
+        terms[i].update(v)
+
+    return typ, _zip_axes_from_type(typ, axes)
+
+
+def _filter_terms(flat):
+    # numeric literals
+    literals = set(filter(is_const, flat))
+
+    # these are strings which are variable names
+    names = set(flat) - literals
+
+    # literals are not names and names are not literals, so intersection should
+    # be empty
+    if literals & names:
+        raise ValueError('literals cannot be names and names cannot be '
+                         'literals')
+    return names, literals
+
+
+def _align(terms, env):
+
+    # flatten the parse tree (a nested list)
+    terms = list(flatten(terms))
+
+    # separate names and literals
+    names, literals = _filter_terms(terms)
+
+    if not names:  # only literals so just promote to a common type
+        return np.result_type(*literals).type, None
+
+    # if all resolved variables are numeric scalars
+    if all(term.isscalar for term in terms):
+        return np.result_type(*(term.value for term in terms)).type, None
+
+    # perform the main alignment
+    typ, axes = _align_core(terms)
+    return typ, axes
+
+
+def _reconstruct_object(typ, obj, axes, dtype):
+    """Reconstruct an object given its type, raw value, and possibly empty
+    (None) axes.
+
+    Parameters
+    ----------
+    typ : object
+        A type
+    obj : object
+        The value to use in the type constructor
+    axes : dict
+        The axes to use to construct the resulting pandas object
+
+    Returns
+    -------
+    reconst : typ
+        An object of type ``typ`` with the value `obj` and possible axes
+        `axes`.
+    """
+    #import ipdb; ipdb.set_trace()
+    try:
+        typ = typ.type
+    except AttributeError:
+        pass
+
+    if (not isinstance(typ, partial) and
+        issubclass(typ, pd.core.generic.PandasObject)):
+        return typ(obj, dtype=dtype, **axes)
+
+    ret_value = typ(obj).astype(dtype)
+
+    try:
+        ret = ret_value.item()
+    except ValueError:
+        ret = ret_value
+    return ret
diff --git a/pandas/computation/api.py b/pandas/computation/api.py
new file mode 100644
index 000000000..db8269a49
--- /dev/null
+++ b/pandas/computation/api.py
@@ -0,0 +1,2 @@
+from pandas.computation.eval import eval
+from pandas.computation.expr import Expr
diff --git a/pandas/computation/common.py b/pandas/computation/common.py
new file mode 100644
index 000000000..4061984dd
--- /dev/null
+++ b/pandas/computation/common.py
@@ -0,0 +1,11 @@
+import collections
+from pandas.core.common import is_string
+
+
+def flatten(l):
+    for el in l:
+        if isinstance(el, collections.Iterable) and not is_string(el):
+            for s in flatten(el):
+                yield s
+        else:
+            yield el
diff --git a/pandas/computation/engines.py b/pandas/computation/engines.py
new file mode 100644
index 000000000..7f500dccb
--- /dev/null
+++ b/pandas/computation/engines.py
@@ -0,0 +1,80 @@
+import abc
+
+from pandas.computation.align import _align, _reconstruct_object
+
+
+class AbstractEngine(object):
+    """"""
+    __metaclass__ = abc.ABCMeta
+
+    has_neg_frac = False
+
+    def __init__(self, expr):
+        self.expr = expr
+        self.aligned_axes = None
+        self.result_type = None
+
+    @abc.abstractmethod
+    def convert(self):
+        """Convert an expression for evaluation."""
+        pass
+
+    def evaluate(self):
+        if not self._is_aligned:
+            self.result_type, self.aligned_axes = _align(self.expr.terms,
+                                                         self.expr.env)
+
+        res = self._evaluate(self.expr.env)
+        return _reconstruct_object(self.result_type, res, self.aligned_axes,
+                                   self.expr.terms.return_type)
+
+    @property
+    def _is_aligned(self):
+        return self.aligned_axes is not None and self.result_type is not None
+
+    @abc.abstractmethod
+    def _evaluate(self, env):
+        """Return an evaluated expression."""
+        pass
+
+
+class NumExprEngine(AbstractEngine):
+    """NumExpr engine class"""
+    has_neg_frac = True
+
+    def __init__(self, expr):
+        super(NumExprEngine, self).__init__(expr)
+
+    def convert(self):
+        """Return a string"""
+        return '%s' % self.expr
+
+    def _evaluate(self, env):
+        import numexpr as ne
+
+        try:
+            return ne.evaluate(self.convert(), local_dict=env.locals,
+                               global_dict=env.globals,
+                               truediv=self.expr.truediv)
+        except KeyError as e:
+            raise NameError('{0!r} is not defined'.format(e.message))
+
+
+class PythonEngine(AbstractEngine):
+    """Use NumPy even if numexpr is installed"""
+    has_neg_frac = False
+
+    def __init__(self, expr):
+        super(PythonEngine, self).__init__(expr)
+
+    def convert(self):
+        pass
+
+    def evaluate(self):
+        return self.expr(self.expr.env)
+
+    def _evaluate(self, env):
+        pass
+
+
+_engines = {'numexpr': NumExprEngine, 'python': PythonEngine}
diff --git a/pandas/computation/eval.py b/pandas/computation/eval.py
new file mode 100644
index 000000000..1a681e37d
--- /dev/null
+++ b/pandas/computation/eval.py
@@ -0,0 +1,85 @@
+#!/usr/bin/env python
+
+import numbers
+
+import numpy as np
+
+import six
+
+from pandas.computation.expr import Expr, Scope
+from pandas.computation.engines import _engines
+
+
+def eval(expr, engine='numexpr', truediv=True, local_dict=None,
+         global_dict=None):
+    """Evaluate a Python expression as a string using various backends.
+
+    The following arithmetic operations are supported: +, -, *, /, **, %, //
+    (python engine only) along with the following boolean operations: | (or), &
+    (and), and ~ (not). All Pandas objects are supported and behave as they
+    would with in-Python evaluation.
+
+    Parameters
+    ----------
+    expr : string or Expr object
+        The expression to evaluate. This can be either a string or an ``Expr``
+        object.
+    engine : string, optional, default 'numexpr', {'python', 'numexpr', 'pytables'}
+        The engine used to evaluate the expression. Supported engines are
+
+        - 'numexpr': This default engine evaluates pandas objects using numexpr
+                     for large speed ups in complex expressions with large
+                     frames.
+        - 'python': Performs operations as if you had eval'd in top level
+                    python
+        - 'pytables': Engine used for evaluating expressions for selection of
+                      objects from PyTables HDF5 tables.
+
+    truediv : bool, optional, default True
+        Whether to use true division, like in Python >= 3
+    local_dict : dict or None, optional, default None
+        A dictionary of local variables, taken from locals() by default.
+    global_dict : dict or None, optional, default None
+        A dictionary of global variables, taken from globals() by default.
+
+    Returns
+    -------
+    obj : ndarray, scalar, DataFrame, Series, or Panel
+
+    Notes
+    -----
+    * The benefits of using ``eval`` are that very large frames that are terms in
+      long expressions are sped up, sometimes by as much as 10x.
+
+    See :ref:`Enhancing performance <enhancingperf.eval>` for more details.
+    """
+    # make sure we're passed a valid engine
+    if not engine in _engines:
+        raise KeyError('Invalid engine {0} passed, valid engines are'
+                       ' {1}'.format(_engines.keys()))
+
+    eng = _engines[engine]
+
+    if isinstance(expr, six.string_types):
+        # need to go 2 up in the call stack from the constructor since we want
+        # the calling scope's variables
+        env = Scope(global_dict, local_dict, frame_level=2)
+        parsed_expr = Expr(expr, engine, env, truediv)
+    elif isinstance(expr, Expr):
+        parsed_expr = expr
+    else:
+        raise TypeError("eval only accepts strings and Expr objects, you "
+                        "passed a {0!r}".format(expr.__class__.__name__))
+
+
+    # construct the engine and evaluate
+    ret = eng(parsed_expr).evaluate()
+
+    # sanity check for a number
+    # TODO: eventually take out
+    # TODO: pytables engine will probably need a string check
+    if np.isscalar(ret):
+        if not isinstance(ret, (np.number, np.bool_, numbers.Number)):
+            raise TypeError('scalar result must be numeric or bool, passed '
+                            'type is {0!r}'.format(ret.__class__.__name__))
+    return ret
diff --git a/pandas/computation/expr.py b/pandas/computation/expr.py
new file mode 100644
index 000000000..6d33f6ac5
--- /dev/null
+++ b/pandas/computation/expr.py
@@ -0,0 +1,150 @@
+import ast
+import sys
+from functools import partial
+
+from pandas.core.base import StringMixin
+from pandas.computation.ops import BinOp, UnaryOp, _reductions, _mathops
+from pandas.computation.ops import _cmp_ops_syms, _bool_ops_syms
+from pandas.computation.ops import _arith_ops_syms, _unary_ops_syms
+from pandas.computation.ops import Term, Constant
+
+
+class Scope(object):
+    __slots__ = 'globals', 'locals'
+
+    def __init__(self, gbls=None, lcls=None, frame_level=1):
+        frame = sys._getframe(frame_level)
+
+        try:
+            self.globals = gbls or frame.f_globals.copy()
+            self.locals = lcls or frame.f_locals.copy()
+        finally:
+            del frame
+
+
+class ExprParserError(Exception):
+    pass
+
+
+class ExprVisitor(ast.NodeVisitor):
+    """Custom ast walker
+    """
+    bin_ops = _cmp_ops_syms + _bool_ops_syms + _arith_ops_syms
+    bin_op_nodes = ('Gt', 'Lt', 'GtE', 'LtE', 'Eq', 'NotEq', 'BitAnd', 'BitOr',
+                    'Add', 'Sub', 'Mult', 'Div', 'Pow', 'FloorDiv', 'Mod')
+    bin_op_nodes_map = dict(zip(bin_ops, bin_op_nodes))
+
+    unary_ops = _unary_ops_syms
+    unary_op_nodes = 'UAdd', 'USub', 'Invert'
+    unary_op_nodes_map = dict(zip(unary_ops, unary_op_nodes))
+
+    def __init__(self, env):
+        for bin_op in self.bin_ops:
+            setattr(self, 'visit_{0}'.format(self.bin_op_nodes_map[bin_op]),
+                    lambda node, bin_op=bin_op: partial(BinOp, bin_op))
+
+        for unary_op in self.unary_ops:
+            setattr(self,
+                    'visit_{0}'.format(self.unary_op_nodes_map[unary_op]),
+                    lambda node, unary_op=unary_op: partial(UnaryOp, unary_op))
+        self.env = env
+
+    def visit(self, node):
+        if not (isinstance(node, ast.AST) or isinstance(node, basestring)):
+            raise TypeError('"node" must be an AST node or a string, you'
+                            ' passed a(n) {0}'.format(node.__class__))
+        if isinstance(node, basestring):
+            node = ast.fix_missing_locations(ast.parse(node))
+        return super(ExprVisitor, self).visit(node)
+
+    def visit_Module(self, node):
+        if len(node.body) != 1:
+            raise ExprParserError('only a single expression is allowed')
+
+        expr = node.body[0]
+        if not isinstance(expr, ast.Expr):
+            raise SyntaxError('only expressions are allowed')
+
+        return self.visit(expr)
+
+    def visit_Expr(self, node):
+        return self.visit(node.value)
+
+    def visit_BinOp(self, node):
+        op = self.visit(node.op)
+        left = self.visit(node.left)
+        right = self.visit(node.right)
+        return op(left, right)
+
+    def visit_UnaryOp(self, node):
+        if isinstance(node.op, ast.Not):
+            raise NotImplementedError("not operator not yet supported")
+        op = self.visit(node.op)
+        return op(self.visit(node.operand))
+
+    def visit_Name(self, node):
+        return Term(node.id, self.env)
+
+    def visit_Num(self, node):
+        return Constant(node.n, self.env)
+
+    def visit_Compare(self, node):
+        ops = node.ops
+        comps = node.comparators
+        if len(ops) != 1:
+            raise ExprParserError('chained comparisons not supported')
+        return self.visit(ops[0])(self.visit(node.left), self.visit(comps[0]))
+
+    def visit_Call(self, node):
+        if not isinstance(node.func, ast.Name):
+            raise TypeError("Only named functions are supported")
+
+        valid_ops = _reductions + _mathops
+
+        if node.func.id not in valid_ops:
+            raise ValueError("Only {0} are supported".format(valid_ops))
+
+        raise NotImplementedError("function calls not yet supported")
+
+    def visit_Attribute(self, node):
+        raise NotImplementedError("attribute access is not yet supported")
+
+    def visit_BoolOp(self, node):
+        raise NotImplementedError("boolean operators are not yet supported")
+
+
+class Expr(StringMixin):
+    """Expr object"""
+    def __init__(self, expr, engine='numexpr', env=None, truediv=True):
+        self.expr = expr
+        self.env = env or Scope(frame_level=2)
+        self._visitor = ExprVisitor(self.env)
+        self.terms = self.parse()
+        self.engine = engine
+        self.truediv = truediv
+
+    def __call__(self, env):
+        env.locals['truediv'] = self.truediv
+        return self.terms(env)
+
+    def __unicode__(self):
+        return unicode(self.terms)
+
+    def parse(self):
+        """return a Termset"""
+        return self._visitor.visit(self.expr)
+
+    def align(self):
+        """align a set of Terms"""
+        return self.terms.align(self.env)
+
+
+def isexpr(s, check_names=True):
+    try:
+        Expr(s)
+    except SyntaxError:
+        return False
+    except NameError:
+        return not check_names
+    else:
+        return True
diff --git a/pandas/core/expressions.py b/pandas/computation/expressions.py
similarity index 67%
rename from pandas/core/expressions.py
rename to pandas/computation/expressions.py
index b1bd104ce..45c9a2d52 100644
--- a/pandas/core/expressions.py
+++ b/pandas/computation/expressions.py
@@ -5,6 +5,7 @@ Expressions
 Offer fast expression evaluation thru numexpr
 
 """
+
 import numpy as np
 from pandas.core.common import _values_from_object
 
@@ -15,17 +16,19 @@ except ImportError:  # pragma: no cover
     _NUMEXPR_INSTALLED = False
 
 _USE_NUMEXPR = _NUMEXPR_INSTALLED
-_evaluate    = None
-_where       = None
+_evaluate = None
+_where = None
 
 # the set of dtypes that we will allow pass to numexpr
-_ALLOWED_DTYPES = dict(evaluate = set(['int64','int32','float64','float32','bool']),
-                       where    = set(['int64','float64','bool']))
+_ALLOWED_DTYPES = dict(
+    evaluate=set(['int64', 'int32', 'float64', 'float32', 'bool']),
+    where=set(['int64', 'float64', 'bool']))
 
 # the minimum prod shape that we will use numexpr
-_MIN_ELEMENTS   = 10000
+_MIN_ELEMENTS = 10000
+
 
-def set_use_numexpr(v = True):
+def set_use_numexpr(v=True):
     # set/unset to use numexpr
     global _USE_NUMEXPR
     if _NUMEXPR_INSTALLED:
@@ -35,26 +38,25 @@ def set_use_numexpr(v = True):
     global _evaluate, _where
     if not _USE_NUMEXPR:
         _evaluate = _evaluate_standard
-        _where    = _where_standard
+        _where = _where_standard
     else:
         _evaluate = _evaluate_numexpr
-        _where    = _where_numexpr
+        _where = _where_numexpr
 
-def set_numexpr_threads(n = None):
+
+def set_numexpr_threads(n=None):
     # if we are using numexpr, set the threads to n
     # otherwise reset
-    try:
-        if _NUMEXPR_INSTALLED and _USE_NUMEXPR:
-            if n is None:
-                n = ne.detect_number_of_cores()
-            ne.set_num_threads(n)
-    except:
-        pass
+    if _NUMEXPR_INSTALLED and _USE_NUMEXPR:
+        if n is None:
+            n = ne.detect_number_of_cores()
+        ne.set_num_threads(n)
 
 
 def _evaluate_standard(op, op_str, a, b, raise_on_error=True, **eval_kwargs):
     """ standard evaluation """
-    return op(a,b)
+    return op(a, b)
+
 
 def _can_use_numexpr(op, op_str, a, b, dtype_check):
     """ return a boolean if we WILL be using numexpr """
@@ -65,13 +67,13 @@ def _can_use_numexpr(op, op_str, a, b, dtype_check):
 
             # check for dtype compatiblity
             dtypes = set()
-            for o in [ a, b ]:
-                if hasattr(o,'get_dtype_counts'):
+            for o in [a, b]:
+                if hasattr(o, 'get_dtype_counts'):
                     s = o.get_dtype_counts()
                     if len(s) > 1:
                         return False
                     dtypes |= set(s.index)
-                elif isinstance(o,np.ndarray):
+                elif isinstance(o, np.ndarray):
                     dtypes |= set([o.dtype.name])
 
             # allowed are a superset
@@ -80,52 +82,54 @@ def _can_use_numexpr(op, op_str, a, b, dtype_check):
 
     return False
 
-def _evaluate_numexpr(op, op_str, a, b, raise_on_error = False, **eval_kwargs):
+
+def _evaluate_numexpr(op, op_str, a, b, raise_on_error=False, **eval_kwargs):
     result = None
 
     if _can_use_numexpr(op, op_str, a, b, 'evaluate'):
         try:
             a_value, b_value = a, b
-            if hasattr(a_value,'values'):
+            if hasattr(a_value, 'values'):
                 a_value = a_value.values
-            if hasattr(b_value,'values'):
+            if hasattr(b_value, 'values'):
                 b_value = b_value.values
             result = ne.evaluate('a_value %s b_value' % op_str,
-                                 local_dict={ 'a_value' : a_value,
-                                              'b_value' : b_value },
+                                 local_dict={'a_value': a_value,
+                                             'b_value': b_value},
                                  casting='safe', **eval_kwargs)
         except (ValueError) as detail:
             if 'unknown type object' in str(detail):
                 pass
         except (Exception) as detail:
             if raise_on_error:
-                raise TypeError(str(detail))
+                raise
 
     if result is None:
-        result = _evaluate_standard(op,op_str,a,b,raise_on_error)
+        result = _evaluate_standard(op, op_str, a, b, raise_on_error)
 
     return result
 
 def _where_standard(cond, a, b, raise_on_error=True):
-    return np.where(_values_from_object(cond), _values_from_object(a), _values_from_object(b))
+    return np.where(_values_from_object(cond), _values_from_object(a),
+                    _values_from_object(b))
 
-def _where_numexpr(cond, a, b, raise_on_error = False):
+def _where_numexpr(cond, a, b, raise_on_error=False):
     result = None
 
     if _can_use_numexpr(None, 'where', a, b, 'where'):
 
         try:
             cond_value, a_value, b_value = cond, a, b
-            if hasattr(cond_value,'values'):
+            if hasattr(cond_value, 'values'):
                 cond_value = cond_value.values
-            if hasattr(a_value,'values'):
+            if hasattr(a_value, 'values'):
                 a_value = a_value.values
-            if hasattr(b_value,'values'):
+            if hasattr(b_value, 'values'):
                 b_value = b_value.values
-            result = ne.evaluate('where(cond_value,a_value,b_value)',
-                                 local_dict={ 'cond_value' : cond_value,
-                                              'a_value' : a_value,
-                                              'b_value' : b_value },
+            result = ne.evaluate('where(cond_value, a_value, b_value)',
+                                 local_dict={'cond_value': cond_value,
+                                             'a_value': a_value,
+                                             'b_value': b_value},
                                  casting='safe')
         except (ValueError) as detail:
             if 'unknown type object' in str(detail):
@@ -135,7 +139,7 @@ def _where_numexpr(cond, a, b, raise_on_error = False):
                 raise TypeError(str(detail))
 
     if result is None:
-        result = _where_standard(cond,a,b,raise_on_error)
+        result = _where_standard(cond, a, b, raise_on_error)
 
     return result
 
@@ -143,7 +147,9 @@ def _where_numexpr(cond, a, b, raise_on_error = False):
 # turn myself on
 set_use_numexpr(True)
 
-def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True, **eval_kwargs):
+
+def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True,
+             **eval_kwargs):
     """ evaluate and return the expression of the op on a and b
 
         Parameters
@@ -153,15 +159,18 @@ def evaluate(op, op_str, a, b, raise_on_error=False, use_numexpr=True, **eval_kw
         op_str: the string version of the op
         a :     left operand
         b :     right operand
-        raise_on_error : pass the error to the higher level if indicated (default is False),
-                         otherwise evaluate the op with and return the results
+        raise_on_error : pass the error to the higher level if indicated
+                         (default is False), otherwise evaluate the op with and
+                         return the results
         use_numexpr : whether to try to use numexpr (default True)
         """
 
     if use_numexpr:
-        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error, **eval_kwargs)
+        return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,
+                         **eval_kwargs)
     return _evaluate_standard(op, op_str, a, b, raise_on_error=raise_on_error)
 
+
 def where(cond, a, b, raise_on_error=False, use_numexpr=True):
     """ evaluate the where condition cond on a and b
 
@@ -171,8 +180,9 @@ def where(cond, a, b, raise_on_error=False, use_numexpr=True):
         cond : a boolean array
         a :    return if cond is True
         b :    return if cond is False
-        raise_on_error : pass the error to the higher level if indicated (default is False),
-                         otherwise evaluate the op with and return the results
+        raise_on_error : pass the error to the higher level if indicated
+                         (default is False), otherwise evaluate the op with and
+                         return the results
         use_numexpr : whether to try to use numexpr (default True)
         """
 
diff --git a/pandas/computation/ops.py b/pandas/computation/ops.py
new file mode 100644
index 000000000..ca5f6d487
--- /dev/null
+++ b/pandas/computation/ops.py
@@ -0,0 +1,255 @@
+import operator as op
+
+import numpy as np
+from pandas.util.py3compat import PY3
+import pandas.core.common as com
+from pandas.core.base import StringMixin
+from pandas.computation.common import flatten
+
+
+_reductions = 'sum', 'prod'
+_mathops = ('sin', 'cos', 'exp', 'log', 'expm1', 'log1p', 'pow', 'div', 'sqrt',
+            'inv', 'sinh', 'cosh', 'tanh', 'arcsin', 'arccos', 'arctan',
+            'arccosh', 'arcsinh', 'arctanh', 'arctan2', 'abs')
+
+
+class OperatorError(Exception):
+    pass
+
+
+class UnaryOperatorError(OperatorError):
+    pass
+
+
+class BinaryOperatorError(OperatorError):
+    pass
+
+
+def _resolve_name(env, key):
+    res = env.locals.get(key, env.globals.get(key))
+
+    if res is None:
+        if not isinstance(key, basestring):
+            return key
+
+        raise NameError('name {0!r} is not defined'.format(key))
+
+    return res
+
+
+def _update_name(env, key, value):
+    if isinstance(key, basestring):
+        try:
+            del env.locals[key]
+            env.locals[key] = value
+        except KeyError:
+            try:
+                del env.globals[key]
+                env.globals[key] = value
+            except KeyError:
+                raise NameError('name {0!r} is not defined'.format(key))
+
+
+class Term(StringMixin):
+    def __init__(self, name, env):
+        self.name = name
+        self.env = env
+        self.value = _resolve_name(self.env, self.name)
+
+        try:
+            # ndframe potentially very slow for large, mixed dtype frames
+            self.type = self.value.values.dtype
+        except AttributeError:
+            try:
+                # ndarray
+                self.type = self.value.dtype
+            except AttributeError:
+                # scalar
+                self.type = type(self.value)
+
+    def __unicode__(self):
+        return com.pprint_thing(self.name)
+
+    def update(self, value):
+        _update_name(self.env, self.name, value)
+        self.value = value
+
+    @property
+    def isscalar(self):
+        return np.isscalar(self.value)
+
+
+class Constant(Term):
+    def __init__(self, value, env):
+        super(Constant, self).__init__(value, env)
+
+
+def _print_operand(opr):
+    return opr.name if is_term(opr) else unicode(opr)
+
+
+class Op(StringMixin):
+    """Hold an operator of unknown arity
+    """
+    def __init__(self, op, operands):
+        self.op = op
+        self.operands = operands
+
+    def __iter__(self):
+        return iter(self.operands)
+
+    def __unicode__(self):
+        """Print a generic n-ary operator and its operands using infix
+        notation"""
+        # recurse over the operands
+        parened = ('({0})'.format(_print_operand(opr))
+                   for opr in self.operands)
+        return com.pprint_thing(' {0} '.format(self.op).join(parened))
+
+    @property
+    def return_type(self):
+        # clobber types to bool if the op is a boolean operator
+        if self.op in (_cmp_ops_syms + _bool_ops_syms):
+            return np.bool_
+        return np.result_type(*(term.type for term in flatten(self)))
+
+
+_cmp_ops_syms = '>', '<', '>=', '<=', '==', '!=', '='
+_cmp_ops_funcs = op.gt, op.lt, op.ge, op.le, op.eq, op.ne, op.eq
+_cmp_ops_dict = dict(zip(_cmp_ops_syms, _cmp_ops_funcs))
+
+_bool_ops_syms = '&', '|'
+_bool_ops_funcs = op.and_, op.or_
+_bool_ops_dict = dict(zip(_bool_ops_syms, _bool_ops_funcs))
+
+_arith_ops_syms = '+', '-', '*', '/', '**', '//', '%'
+_arith_ops_funcs = (op.add, op.sub, op.mul, op.truediv if PY3 else op.div,
+                    op.pow, op.floordiv, op.mod)
+_arith_ops_dict = dict(zip(_arith_ops_syms, _arith_ops_funcs))
+
+_binary_ops_dict = {}
+
+for d in (_cmp_ops_dict, _bool_ops_dict, _arith_ops_dict):
+    _binary_ops_dict.update(d)
+
+
+def _cast_inplace(terms, dtype):
+    dt = np.dtype(dtype)
+    for term in terms:
+        # cast all the way down the tree since operands must be
+        try:
+            _cast_inplace(term.operands, dtype)
+        except AttributeError:
+            # we've bottomed out so actually do the cast
+            try:
+                new_value = term.value.astype(dt)
+            except AttributeError:
+                new_value = dt.type(term.value)
+            term.update(new_value)
+
+
+def is_term(obj):
+    return isinstance(obj, Term)
+
+
+def is_const(obj):
+    return isinstance(obj, Constant)
+
+
+class BinOp(Op):
+    """Hold a binary operator and its operands
+
+    Parameters
+    ----------
+    op : str or Op
+    left : str or Op
+    right : str or Op
+    """
+    def __init__(self, op, lhs, rhs):
+        super(BinOp, self).__init__(op, (lhs, rhs))
+        self.lhs = lhs
+        self.rhs = rhs
+
+        try:
+            self.func = _binary_ops_dict[op]
+        except KeyError:
+            keys = _binary_ops_dict.keys()
+            raise BinaryOperatorError('Invalid binary operator {0}, valid'
+                                      ' operators are {1}'.format(op, keys))
+
+    def __call__(self, env):
+        # handle truediv
+        if self.op == '/' and env.locals['truediv']:
+            self.func = op.truediv
+
+        # recurse over the left nodes
+        try:
+            left = self.lhs(env)
+        except TypeError:
+            left = self.lhs
+
+        # recurse over the right nodes
+        try:
+            right = self.rhs(env)
+        except TypeError:
+            right = self.rhs
+
+        # base cases
+        if is_term(left) and is_term(right):
+            res = self.func(left.value, right.value)
+        elif not is_term(left) and is_term(right):
+            res = self.func(left, right.value)
+        elif is_term(left) and not is_term(right):
+            res = self.func(left.value, right)
+        elif not (is_term(left) or is_term(right)):
+            res = self.func(left, right)
+
+        return res
+
+
+class Mod(BinOp):
+    def __init__(self, lhs, rhs):
+        super(Mod, self).__init__('%', lhs, rhs)
+        _cast_inplace(self.operands, np.float_)
+
+
+_unary_ops_syms = '+', '-', '~'
+_unary_ops_funcs = op.pos, op.neg, op.invert
+_unary_ops_dict = dict(zip(_unary_ops_syms, _unary_ops_funcs))
+
+
+class UnaryOp(Op):
+    """Hold a unary operator and its operands
+    """
+    def __init__(self, op, operand):
+        super(UnaryOp, self).__init__(op, (operand,))
+        self.operand = operand
+
+        try:
+            self.func = _unary_ops_dict[op]
+        except KeyError:
+            raise UnaryOperatorError('Invalid unary operator {0}, valid '
+                                     'operators are '
+                                     '{1}'.format(op, _unary_ops_syms))
+
+    def __call__(self, env):
+        operand = self.operand
+
+        # recurse if operand is an Op
+        try:
+            operand = self.operand(env)
+        except TypeError:
+            operand = self.operand
+
+        v = operand.value if is_term(operand) else operand
+
+        try:
+            res = self.func(v)
+        except TypeError:
+            res = self.func(v.values)
+
+        return res
+
+    def __unicode__(self):
+        return com.pprint_thing('{0}({1})'.format(self.op, self.operand))
+
diff --git a/pandas/computation/tests/__init__.py b/pandas/computation/tests/__init__.py
new file mode 100644
index 000000000..e69de29bb
diff --git a/pandas/computation/tests/test_eval.py b/pandas/computation/tests/test_eval.py
new file mode 100644
index 000000000..fc1cccf32
--- /dev/null
+++ b/pandas/computation/tests/test_eval.py
@@ -0,0 +1,648 @@
+#!/usr/bin/env python
+
+import unittest
+import itertools
+from itertools import product
+
+import nose
+from nose.tools import assert_raises, assert_tuple_equal
+from nose.tools import assert_true, assert_false
+
+from numpy.random import randn, rand
+import numpy as np
+from numpy.testing import assert_array_equal, assert_allclose
+from numpy.testing.decorators import slow
+
+import pandas as pd
+from pandas.core import common as com
+from pandas import DataFrame, Series
+from pandas.util.testing import makeCustomDataframe as mkdf
+from pandas.computation.engines import _engines, _reconstruct_object
+from pandas.computation.align import _align_core
+from pandas.computation.ops import _binary_ops_dict, _unary_ops_dict, Term
+import pandas.computation.expr as expr
+from pandas.computation.expressions import _USE_NUMEXPR
+from pandas.computation.eval import Scope
+from pandas.util.testing import assert_frame_equal, randbool
+from pandas.util.py3compat import PY3
+
+
+def skip_numexpr_engine(engine):
+    if not _USE_NUMEXPR and engine == 'numexpr':
+        raise nose.SkipTest
+
+
+def engine_has_neg_frac(engine):
+    return _engines[engine].has_neg_frac
+
+
+def fractional(x):
+    frac, _ = np.modf(np.asanyarray(x))
+    return frac
+
+
+def hasfractional(x):
+    return np.any(fractional(x))
+
+
+def _eval_from_expr(lhs, cmp1, rhs, binop, cmp2):
+    f1 = _binary_ops_dict[cmp1]
+    f2 = _binary_ops_dict[cmp2]
+    bf = _binary_ops_dict[binop]
+    env = Scope()
+    typ, axes = _align_core((Term('lhs', env), Term('rhs', env)))
+    lhs, rhs = env.locals['lhs'], env.locals['rhs']
+    return _reconstruct_object(typ, bf(f1(lhs, rhs), f2(lhs, rhs)), axes)
+
+
+def _eval_single_bin(lhs, cmp1, rhs, has_neg_frac):
+    c = _binary_ops_dict[cmp1]
+    if has_neg_frac:
+        try:
+            result = c(lhs, rhs)
+        except ValueError:
+            result = np.nan
+    else:
+        result = c(lhs, rhs)
+    return result
+
+
+def isframe(x):
+    return isinstance(x, pd.DataFrame)
+
+
+def isseries(x):
+    return isinstance(x, pd.Series)
+
+
+def are_compatible_types(op, lhs, rhs):
+    if op in ('&', '|'):
+        if isframe(lhs) and isseries(rhs) or isframe(rhs) and isseries(lhs):
+            return False
+    return True
+
+
+def _eval_bin_and_unary(unary, lhs, arith1, rhs):
+    binop = _binary_ops_dict[arith1]
+    unop = expr._unary_ops_dict[unary]
+    return unop(binop(lhs, rhs))
+
+
+def _series_and_2d_ndarray(lhs, rhs):
+    return (com.is_series(lhs) and isinstance(rhs, np.ndarray) and rhs.ndim > 1
+            or com.is_series(rhs) and isinstance(lhs, np.ndarray) and lhs.ndim
+            > 1)
+
+
+# Smoke testing
+class TestBasicEval(unittest.TestCase):
+
+    @classmethod
+    def setUpClass(self):
+        self.cmp_ops = expr._cmp_ops_syms
+        self.cmp2_ops = self.cmp_ops[::-1]
+        self.bin_ops = expr._bool_ops_syms
+        self.arith_ops = tuple(o for o in expr._arith_ops_syms if o != '//')
+        self.unary_ops = '+', '-'
+
+    def set_current_engine(self):
+        self.engine = 'numexpr'
+
+    def setup_data(self):
+        nan_df = DataFrame(rand(10, 5))
+        nan_df[nan_df > 0.5] = np.nan
+        self.lhses = (DataFrame(randn(10, 5)), Series(randn(5)), randn(),
+                      np.float64(randn()), randn(10, 5), randn(5), np.nan,
+                      Series([1, 2, np.nan, np.nan, 5]), nan_df)
+        self.rhses = (DataFrame(randn(10, 5)), Series(randn(5)), randn(),
+                      np.float64(randn()), randn(10, 5), randn(5), np.nan,
+                      Series([1, 2, np.nan, np.nan, 5]), nan_df)
+
+    def setUp(self):
+        try:
+            import numexpr as ne
+            self.ne = ne
+        except ImportError:
+            raise nose.SkipTest
+        self.set_current_engine()
+        self.setup_data()
+        self.current_engines = filter(lambda x: x != self.engine,
+                                      _engines.iterkeys())
+
+    @slow
+    def test_complex_cmp_ops(self):
+        self.setUp()
+        lhses, rhses = self.lhses, self.rhses
+        args = itertools.product(lhses, self.cmp_ops, rhses, self.bin_ops,
+                                 self.cmp2_ops)
+        for lhs, cmp1, rhs, binop, cmp2 in args:
+            self._create_cmp_op_t(lhs, cmp1, rhs, binop, cmp2)
+
+    def test_simple_cmp_ops(self):
+        bool_lhses = (DataFrame(randbool(size=(10, 5))),
+                      Series(randbool((5,))), randbool())
+        bool_rhses = (DataFrame(randbool(size=(10, 5))),
+                      Series(randbool((5,))), randbool())
+        args = itertools.product(bool_lhses, bool_rhses, self.cmp_ops)
+        for lhs, rhs, cmp_op in args:
+            self._create_simple_cmp_op_t(lhs, rhs, cmp_op)
+
+    def test_binary_arith_ops(self):
+        self.setUp()
+        lhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        rhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        args = itertools.product(lhses, self.arith_ops, rhses)
+        for lhs, op, rhs in args:
+            self._create_arith_op_t(lhs, op, rhs)
+
+    def test_unary_arith_ops(self):
+        self.setUp()
+        lhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        rhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        aops = tuple(aop for aop in self.arith_ops if aop not in '+-')
+        args = itertools.product(self.unary_ops, lhses, aops, rhses)
+        for unary_op, lhs, arith_op, rhs in args:
+            self._create_unary_arith_op_t(unary_op, lhs, arith_op, rhs)
+
+    def test_invert(self):
+        self.setUp()
+        lhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        rhses = DataFrame(randn(10, 5)), Series(randn(5)), randn()
+        args = itertools.product(lhses, self.cmp_ops, rhses)
+        for lhs, op, rhs in args:
+            self._create_invert_op_t(lhs, op, rhs)
+
+    def _create_cmp_op_t(self, lhs, cmp1, rhs, binop, cmp2):
+        ex = '(lhs {cmp1} rhs) {binop} (lhs {cmp2} rhs)'.format(cmp1=cmp1,
+                                                                binop=binop,
+                                                                cmp2=cmp2)
+        if _series_and_2d_ndarray(lhs, rhs):
+            self.assertRaises(Exception, _eval_from_expr, lhs, cmp1, rhs,
+                              binop, cmp2)
+            self.assertRaises(Exception, pd.eval, ex, engine=self.engine)
+        else:
+            expected = _eval_from_expr(lhs, cmp1, rhs, binop, cmp2)
+            result = pd.eval(ex, engine=self.engine)
+            assert_array_equal(result, expected)
+
+    def _create_simple_cmp_op_t(self, lhs, rhs, cmp1):
+        ex = 'lhs {0} rhs'.format(cmp1)
+
+        if are_compatible_types(cmp1, lhs, rhs):
+            expected = _eval_single_bin(lhs, cmp1, rhs,
+                                        engine_has_neg_frac(self.engine))
+            result = pd.eval(ex, engine=self.engine)
+            assert_array_equal(result, expected)
+        else:
+            assert_raises(TypeError, _eval_single_bin, lhs, cmp1, rhs,
+                          engine_has_neg_frac(self.engine))
+
+    def _create_arith_op_t(self, lhs, arith1, rhs):
+        ex = 'lhs {0} rhs'.format(arith1)
+        nan_frac_neg = (arith1 == '**' and np.any(lhs < 0) and
+                        hasfractional(rhs) and np.isscalar(lhs) and
+                        np.isscalar(rhs) and
+                        not (isinstance(lhs, tuple(np.typeDict.values()))
+                             or isinstance(rhs, tuple(np.typeDict.values()))))
+        if nan_frac_neg and not engine_has_neg_frac(self.engine):
+                assert_raises(ValueError, pd.eval, ex, engine=self.engine,
+                              local_dict=locals(), global_dict=globals())
+        else:
+            result = pd.eval(ex, engine=self.engine)
+
+            if arith1 != '//':
+                expected = _eval_single_bin(lhs, arith1, rhs,
+                                            engine_has_neg_frac(self.engine))
+                # roundoff error with modulus
+                if arith1 == '%':
+                    assert_allclose(result, expected)
+                else:
+                    assert_array_equal(result, expected)
+
+            # sanity check on recursive parsing
+            try:
+                ghs = rhs.copy()
+            except AttributeError:
+                ghs = rhs
+
+        if nan_frac_neg and not engine_has_neg_frac(self.engine):
+            assert_raises(ValueError, pd.eval, ex, engine=self.engine,
+                          local_dict=locals(), global_dict=globals())
+        else:
+            if arith1 == '**':
+                ex = '(lhs {0} rhs) {0} ghs'.format(arith1)
+            else:
+                ex = 'lhs {0} rhs {0} ghs'.format(arith1)
+            result = pd.eval(ex, engine=self.engine)
+
+            try:
+                nlhs = _eval_single_bin(lhs, arith1, rhs,
+                                        engine_has_neg_frac(self.engine))
+            except ValueError:
+                assert_raises(ValueError, _eval_single_bin, lhs, arith1, rhs,
+                              engine_has_neg_frac(self.engine))
+            else:
+                try:
+                    nlhs, ghs = nlhs.align(ghs)
+                except:
+                    pass
+                if arith1 != '//':
+                    expected = self.ne.evaluate('nlhs {0} ghs'.format(arith1))
+
+                    # roundoff error with modulus
+                    if arith1 == '%':
+                        assert_allclose(result, expected)
+                    else:
+                        assert_array_equal(result, expected)
+
+    def _create_invert_op_t(self, lhs, cmp1, rhs):
+        # simple
+        for el in (lhs, rhs):
+            try:
+                elb = el.astype(bool)
+            except AttributeError:
+                elb = np.array([bool(el)])
+            expected = ~elb
+            result = pd.eval('~elb', engine=self.engine)
+            assert_array_equal(expected, result)
+
+            for engine in self.current_engines:
+                assert_array_equal(result, pd.eval('~elb', engine=engine))
+
+        # compound
+        ex = '~(lhs {0} rhs)'.format(cmp1)
+        if np.isscalar(lhs) and np.isscalar(rhs):
+            lhs, rhs = map(lambda x: np.array([x]), (lhs, rhs))
+        expected = ~_eval_single_bin(lhs, cmp1, rhs,
+                                     engine_has_neg_frac(self.engine))
+        result = pd.eval(ex, engine=self.engine)
+        assert_array_equal(expected, result)
+
+        # make sure the other engines work
+        for engine in self.current_engines:
+            ev = pd.eval(ex, engine=self.engine)
+            assert_array_equal(ev, result)
+
+    def _create_unary_arith_op_t(self, unary_op, lhs, arith1, rhs):
+        # simple
+        ex = '{0}lhs'.format(unary_op, arith1)
+        f = _unary_ops_dict[unary_op]
+        bad_types = tuple(np.typeDict.values())
+
+        nan_frac_neg = (arith1 == '**' and
+                        np.any(lhs < 0) and
+                        hasfractional(rhs) and
+                        np.isscalar(lhs) and np.isscalar(rhs) and
+                        not (isinstance(lhs, bad_types) or
+                             isinstance(rhs, bad_types))
+                        and not engine_has_neg_frac(self.engine))
+        try:
+            expected = f(lhs.values)
+        except AttributeError:
+            expected = f(lhs)
+        result = pd.eval(ex, engine=self.engine)
+        assert_array_equal(result, expected)
+
+        for engine in self.current_engines:
+            assert_array_equal(result, pd.eval(ex, engine=engine))
+
+        ex = '{0}(lhs {1} rhs)'.format(unary_op, arith1)
+
+        if nan_frac_neg:
+            assert_raises(ValueError, pd.eval, ex, engine=self.engine,
+                          local_dict=locals(), global_dict=globals())
+        else:
+            # compound
+            result = pd.eval(ex, engine=self.engine)
+
+            #(lhs, rhs), _ = _align((lhs, rhs))
+            #if arith1 != '//':
+                #expected = self.ne.evaluate(ex)
+                #assert_array_equal(result, expected)
+            #else:
+                #assert_raises(TypeError, self.ne.evaluate, ex)
+
+            #for engine in self.current_engines:
+                #if arith1 != '//':
+                    #if engine_has_neg_frac(engine):
+                        #assert_array_equal(result, pd.eval(ex, engine=engine))
+                #else:
+                    #assert_raises(TypeError, pd.eval, ex, engine=engine,
+                                  #local_dict=locals(), global_dict=globals())
+
+
+class TestBasicEvalPython(TestBasicEval):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.cmp_ops = expr._cmp_ops_syms
+        cls.cmp2_ops = cls.cmp_ops[::-1]
+        cls.bin_ops = expr._bool_ops_syms
+        cls.arith_ops = expr._arith_ops_syms
+        cls.unary_ops = '+', '-'
+
+    def set_current_engine(self):
+        self.engine = 'python'
+
+
+def test_syntax_error_exprs():
+    for engine in _engines:
+        e = 's +'
+        assert_raises(SyntaxError, pd.eval, e, engine=engine)
+
+
+def test_name_error_exprs():
+    for engine in _engines:
+        e = 's + t'
+        assert_raises(NameError, pd.eval, e, engine=engine)
+
+
+def test_align_nested_unary_op():
+    for engine in _engines:
+        yield check_align_nested_unary_op, engine
+
+
+f = lambda *args, **kwargs: np.random.randn()
+
+
+def check_align_nested_unary_op(engine):
+    skip_numexpr_engine(engine)
+    s = 'df * ~2'
+    df = mkdf(10, 10, data_gen_f=f)
+    res = pd.eval(s, engine)
+    assert_frame_equal(res, df * ~2)
+
+
+def check_basic_frame_alignment(engine):
+    df = mkdf(10, 10, data_gen_f=f)
+    df2 = mkdf(20, 10, data_gen_f=f)
+    res = pd.eval('df + df2', engine=engine)
+    assert_frame_equal(res, df + df2)
+
+
+def test_basic_frame_alignment():
+    for engine in _engines:
+        yield check_basic_frame_alignment, engine
+
+
+def check_medium_complex_frame_alignment(engine, r1, r2, c1, c2):
+    skip_numexpr_engine(engine)
+    df = mkdf(5, 2, data_gen_f=f, r_idx_type=r1, c_idx_type=c1)
+    df2 = mkdf(10, 2, data_gen_f=f, r_idx_type=r2, c_idx_type=c2)
+    df3 = mkdf(15, 2, data_gen_f=f, r_idx_type=r2, c_idx_type=c2)
+    res = pd.eval('df + df2 + df3', engine=engine)
+    assert_frame_equal(res, df + df2 + df3)
+
+
+@slow
+def test_medium_complex_frame_alignment():
+    args = product(_engines, *([INDEX_TYPES[:4]] * 4))
+    for engine, r1, r2, c1, c2 in args:
+        check_medium_complex_frame_alignment(engine, r1, r2, c1, c2)
+
+
+def check_basic_frame_series_alignment(engine, r_idx_type, c_idx_type,
+                                       index_name):
+    skip_numexpr_engine(engine)
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type=r_idx_type,
+              c_idx_type=c_idx_type)
+    index = getattr(df, index_name)
+    s = Series(np.random.randn(5), index[:5])
+
+    if r_idx_type != 'p' and c_idx_type == 'p' and index_name == 'index':
+        assert_raises(ValueError, pd.eval, 'df + s', local_dict=locals())
+        assert_raises(ValueError, df.add, s, axis=1)
+    else:
+        res = pd.eval('df + s', engine=engine)
+        expected = df + s
+        assert_frame_equal(res, expected)
+
+
+def check_not_both_period_fails_otherwise_succeeds(lhs, rhs, r_idx_type,
+                                                   c_idx_type, index_name, s,
+                                                   df, *terms):
+    if r_idx_type != 'p' and c_idx_type == 'p' and index_name == 'index':
+        assert_raises(ValueError, pd.eval, lhs, local_dict=locals())
+        assert_raises(ValueError, pd.eval, rhs, local_dict=locals())
+    else:
+        a, b = pd.eval(lhs), pd.eval(rhs)
+        assert_frame_equal(a, b)
+
+
+def check_basic_series_frame_alignment(engine, r_idx_type, c_idx_type,
+                                       index_name):
+    skip_numexpr_engine(engine)
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type=r_idx_type,
+              c_idx_type=c_idx_type)
+    index = getattr(df, index_name)
+    s = Series(np.random.randn(5), index[:5])
+
+    if r_idx_type != 'p' and c_idx_type == 'p' and index_name == 'index':
+        assert_raises(ValueError, pd.eval, 's + df', local_dict=locals())
+        assert_raises(ValueError, df.add, s, axis=1)
+    else:
+        res = pd.eval('s + df', engine=engine)
+        expected = s + df
+        assert_frame_equal(res, expected)
+
+
+@slow
+def check_basic_series_frame_alignment_datetime(engine, r_idx_type, c_idx_type,
+                                                index_name):
+    skip_numexpr_engine(engine)
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type=r_idx_type,
+              c_idx_type=c_idx_type)
+    index = getattr(df, index_name)
+    s = Series(np.random.randn(5), index[:5])
+    if r_idx_type != 'p' and c_idx_type == 'p' and index_name == 'index':
+        assert_raises(ValueError, pd.eval, 's + df', local_dict=locals())
+        assert_raises(ValueError, df.add, s, axis=1)
+    else:
+        res = pd.eval('s + df', engine=engine)
+        expected = s + df
+        assert_frame_equal(res, expected)
+
+    if r_idx_type != 'p' and c_idx_type == 'p' and index_name == 'index':
+        assert_raises(ValueError, pd.eval, 'df + s', local_dict=locals())
+        assert_raises(ValueError, df.add, s, axis=1)
+    else:
+        res = pd.eval('df + s', engine=engine)
+        expected = df + s
+        assert_frame_equal(res, expected)
+
+
+def check_series_frame_commutativity(engine, r_idx_type, c_idx_type, op,
+                                     index_name):
+    skip_numexpr_engine(engine)
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type=r_idx_type,
+              c_idx_type=c_idx_type)
+    index = getattr(df, index_name)
+    s = Series(np.random.randn(5), index[:5])
+
+    lhs = 's {0} df'.format(op)
+    rhs = 'df {0} s'.format(op)
+    check_not_both_period_fails_otherwise_succeeds(lhs, rhs, r_idx_type,
+                                                   c_idx_type, index_name, s,
+                                                   df)
+
+
+INDEX_TYPES = 'i', 'f', 's', 'u', # 'dt',  # 'p'
+
+
+@slow
+def test_series_frame_commutativity():
+    args = product(_engines, INDEX_TYPES, INDEX_TYPES, ('+', '*'), ('index',
+                                                                    'columns'))
+    for engine, r_idx_type, c_idx_type, op, index_name in args:
+        check_series_frame_commutativity(engine, r_idx_type, c_idx_type, op,
+                                         index_name)
+
+
+def test_basic_frame_series_alignment():
+    args = product(_engines, INDEX_TYPES, INDEX_TYPES, ('index', 'columns'))
+    for engine, r_idx_type, c_idx_type, index_name in args:
+        check_basic_frame_series_alignment(engine, r_idx_type, c_idx_type,
+                                           index_name)
+
+
+@slow
+def test_basic_series_frame_alignment_datetime():
+    idx_types = INDEX_TYPES
+    args = product(_engines, idx_types, idx_types, ('index', 'columns'))
+    for engine, r_idx_type, c_idx_type, index_name in args:
+        check_basic_series_frame_alignment_datetime(engine, r_idx_type,
+                                                    c_idx_type, index_name)
+
+
+def test_basic_series_frame_alignment():
+    args = product(_engines, INDEX_TYPES, INDEX_TYPES, ('index', 'columns'))
+    for engine, r_idx_type, c_idx_type, index_name in args:
+        check_basic_series_frame_alignment(engine, r_idx_type, c_idx_type,
+                                           index_name)
+
+
+def check_complex_series_frame_alignment(engine, index_name, obj, r1, r2, c1,
+                                         c2):
+    skip_numexpr_engine(engine)
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type=r1, c_idx_type=c1)
+    df2 = mkdf(20, 10, data_gen_f=f, r_idx_type=r2, c_idx_type=c2)
+    index = getattr(locals()[obj], index_name)
+    s = Series(np.random.randn(5), index[:5])
+    if engine != 'python':
+        expected = df2.add(s, axis=1).add(df)
+    else:
+        expected = df2 + s + df
+    res = pd.eval('df2 + s + df', engine=engine)
+    expected = df2 + s + df
+    assert_tuple_equal(res.shape, expected.shape)
+    assert_frame_equal(res, expected)
+
+
+@slow
+def test_complex_series_frame_alignment():
+    args = product(_engines, ('index', 'columns'), ('df', 'df2'),
+                   *([INDEX_TYPES[:4]] * 4))
+    for engine, index_name, obj, r1, r2, c1, c2 in args:
+        check_complex_series_frame_alignment(engine, index_name, obj, r1, r2,
+                                             c1, c2)
+
+
+def check_datetime_index_rows_punts_to_python(engine):
+    df = mkdf(10, 10, data_gen_f=f, r_idx_type='dt', c_idx_type='dt')
+    index = getattr(df, 'index')
+    s = Series(np.random.randn(5), index[:5])
+    env = Scope(globals(), locals())
+
+
+def test_datetime_index_rows_punts_to_python():
+    for engine in _engines:
+        check_datetime_index_rows_punts_to_python(engine)
+
+
+def test_truediv():
+    for engine in _engines:
+        check_truediv(engine)
+
+
+def check_truediv(engine):
+    s = np.array([1])
+    ex = 's / 1'
+
+    if PY3:
+        res = pd.eval(ex, truediv=False)
+        assert_array_equal(res, np.array([1.0]))
+
+        res = pd.eval(ex, truediv=True)
+        assert_array_equal(res, np.array([1.0]))
+    else:
+        res = pd.eval(ex, truediv=False)
+        assert_array_equal(res, np.array([1]))
+
+        res = pd.eval(ex, truediv=True)
+        assert_array_equal(res, np.array([1.0]))
+
+
+__var_s = randn(10)
+
+
+def check_global_scope(engine):
+    e = '__var_s * 2'
+    assert_array_equal(__var_s * 2, pd.eval(e, engine=engine))
+
+
+def test_global_scope():
+    for engine in _engines:
+        yield check_global_scope, engine
+
+
+def check_is_expr(engine):
+    s = 1
+    valid = 's + 1'
+    invalid = 's +'
+    assert_true(expr.isexpr(valid, check_names=True))
+    assert_true(expr.isexpr(valid, check_names=False))
+    assert_false(expr.isexpr(invalid, check_names=False))
+    assert_false(expr.isexpr(invalid, check_names=True))
+
+
+def test_is_expr():
+    for engine in _engines:
+        check_is_expr(engine)
+
+
+def check_not_fails(engine):
+    x = True
+    assert_raises(NotImplementedError, pd.eval, 'not x', engine=engine,
+                  local_dict={'x': x})
+
+
+def test_not_fails():
+    for engine in _engines:
+        check_not_fails(engine)
+
+
+def check_and_fails(engine):
+    x, y = False, True
+    assert_raises(NotImplementedError, pd.eval, 'x and y', engine=engine,
+                  local_dict={'x': x, 'y': y})
+
+
+def test_and_fails():
+    for engine in _engines:
+        check_and_fails(engine)
+
+
+def check_or_fails(engine):
+    x, y = True, False
+    assert_raises(NotImplementedError, pd.eval, 'x or y', engine=engine,
+                  local_dict={'x': x, 'y': y})
+
+
+def test_or_fails():
+    for engine in _engines:
+        check_or_fails(engine)
+
+
+if __name__ == '__main__':
+    nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
+                   exit=False)
diff --git a/pandas/core/base.py b/pandas/core/base.py
index a2f7f0405..fb0d56113 100644
--- a/pandas/core/base.py
+++ b/pandas/core/base.py
@@ -48,6 +48,7 @@ class StringMixin(object):
         """
         return str(self)
 
+
 class PandasObject(StringMixin):
     """baseclass for various pandas objects"""
 
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 34aaa08b5..89407121f 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -18,6 +18,7 @@ from datetime import timedelta
 
 from pandas.core.config import get_option
 from pandas.core import array as pa
+import pandas as pd
 
 class PandasError(Exception):
     pass
@@ -1656,6 +1657,29 @@ def is_bool(obj):
     return isinstance(obj, (bool, np.bool_))
 
 
+def is_string(obj):
+    return isinstance(obj, (basestring, np.str_, np.unicode_))
+
+
+def is_series(obj):
+    return isinstance(obj, pd.Series)
+
+
+def is_frame(obj):
+    return isinstance(obj, pd.DataFrame)
+
+
+def is_panel(obj):
+    return isinstance(obj, pd.Panel)
+
+
+def is_pd_obj(obj):
+    return isinstance(obj, pd.core.generic.PandasObject)
+
+
+def is_ndframe(obj):
+    return isinstance(obj, pd.core.generic.NDFrame)
+
 def is_integer(obj):
     return isinstance(obj, (int, long, np.integer))
 
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index f56b6bc00..c957ec9d3 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -35,8 +35,8 @@ from pandas.core.internals import (BlockManager,
                                    create_block_manager_from_arrays,
                                    create_block_manager_from_blocks)
 from pandas.core.series import Series, _radd_compat
-import pandas.core.expressions as expressions
 from pandas.sparse.array import SparseArray
+import pandas.computation.expressions as expressions
 from pandas.compat.scipy import scoreatpercentile as _quantile
 from pandas.compat import(range, zip, lrange, lmap, lzip, StringIO, u,
                           OrderedDict, raise_with_traceback)
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index c265d1590..11ce27b07 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -18,8 +18,7 @@ import pandas.core.common as com
 from pandas.sparse.array import _maybe_to_sparse, SparseArray
 import pandas.lib as lib
 import pandas.tslib as tslib
-import pandas.core.expressions as expressions
-from pandas.util.decorators import cache_readonly
+import pandas.computation.expressions as expressions
 
 from pandas.tslib import Timestamp
 from pandas import compat
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index c8224f761..6e7f72195 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -32,7 +32,6 @@ from pandas.core.index import _ensure_index
 from pandas.tseries.timedeltas import _coerce_scalar_to_timedelta_type
 import pandas.core.common as com
 from pandas.tools.merge import concat
-from pandas import compat
 from pandas.io.common import PerformanceWarning
 from pandas.core.config import get_option
 
@@ -222,9 +221,12 @@ def get_store(path, **kwargs):
 
     Examples
     --------
+    >>> from pandas import DataFrame
+    >>> from numpy.random import randn
+    >>> bar = DataFrame(randn(10, 4))
     >>> with get_store('test.h5') as store:
-    >>>     store['foo'] = bar   # write to HDF5
-    >>>     bar = store['foo']   # retrieve
+    ...     store['foo'] = bar   # write to HDF5
+    ...     bar = store['foo']   # retrieve
     """
     store = None
     try:
@@ -237,7 +239,8 @@ def get_store(path, **kwargs):
 
 # interface to/from ###
 
-def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None, append=None, **kwargs):
+def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None,
+           append=None, **kwargs):
     """ store this object, close it if we opened it """
     if append:
         f = lambda store: store.append(key, value, **kwargs)
@@ -332,6 +335,9 @@ class HDFStore(StringMixin):
 
     Examples
     --------
+    >>> from pandas import DataFrame
+    >>> from numpy.random import randn
+    >>> bar = DataFrame(randn(10, 4))
     >>> store = HDFStore('test.h5')
     >>> store['foo'] = bar   # write to HDF5
     >>> bar = store['foo']   # retrieve
@@ -341,9 +347,9 @@ class HDFStore(StringMixin):
     def __init__(self, path, mode=None, complevel=None, complib=None,
                  fletcher32=False, **kwargs):
         try:
-            import tables as _
+            import tables
         except ImportError:  # pragma: no cover
-            raise Exception('HDFStore requires PyTables')
+            raise ImportError('HDFStore requires PyTables')
 
         self._path = path
         if mode is None:
@@ -523,7 +529,8 @@ class HDFStore(StringMixin):
             raise KeyError('No object named %s in the file' % key)
         return self._read_group(group)
 
-    def select(self, key, where=None, start=None, stop=None, columns=None, iterator=False, chunksize=None, auto_close=False, **kwargs):
+    def select(self, key, where=None, start=None, stop=None, columns=None,
+               iterator=False, chunksize=None, auto_close=False, **kwargs):
         """
         Retrieve pandas object stored in file, optionally based on where
         criteria
@@ -554,17 +561,22 @@ class HDFStore(StringMixin):
 
         # what we are actually going to do for a chunk
         def func(_start, _stop):
-            return s.read(where=where, start=_start, stop=_stop, columns=columns, **kwargs)
+            return s.read(where=where, start=_start, stop=_stop,
+                          columns=columns, **kwargs)
 
         if iterator or chunksize is not None:
             if not s.is_table:
                 raise TypeError(
                     "can only use an iterator or chunksize on a table")
-            return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop, chunksize=chunksize, auto_close=auto_close)
+            return TableIterator(self, func, nrows=s.nrows, start=start,
+                                 stop=stop, chunksize=chunksize,
+                                 auto_close=auto_close)
 
-        return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop, auto_close=auto_close).get_values()
+        return TableIterator(self, func, nrows=s.nrows, start=start, stop=stop,
+                             auto_close=auto_close).get_values()
 
-    def select_as_coordinates(self, key, where=None, start=None, stop=None, **kwargs):
+    def select_as_coordinates(
+            self, key, where=None, start=None, stop=None, **kwargs):
         """
         return the selection as an Index
 
@@ -599,7 +611,9 @@ class HDFStore(StringMixin):
         """
         return self.get_storer(key).read_column(column=column, **kwargs)
 
-    def select_as_multiple(self, keys, where=None, selector=None, columns=None, start=None, stop=None, iterator=False, chunksize=None, auto_close=False, **kwargs):
+    def select_as_multiple(self, keys, where=None, selector=None, columns=None,
+                           start=None, stop=None, iterator=False,
+                           chunksize=None, auto_close=False, **kwargs):
         """ Retrieve pandas objects from multiple tables
 
         Parameters
@@ -624,10 +638,10 @@ class HDFStore(StringMixin):
             return self.select(key=keys, where=where, columns=columns, start=start, stop=stop, iterator=iterator, chunksize=chunksize, **kwargs)
 
         if not isinstance(keys, (list, tuple)):
-            raise Exception("keys must be a list/tuple")
+            raise TypeError("keys must be a list/tuple")
 
-        if len(keys) == 0:
-            raise Exception("keys must have a non-zero length")
+        if not len(keys):
+            raise ValueError("keys must have a non-zero length")
 
         if selector is None:
             selector = keys[0]
@@ -642,7 +656,8 @@ class HDFStore(StringMixin):
                 raise TypeError("Invalid table [%s]" % k)
             if not t.is_table:
                 raise TypeError(
-                    "object [%s] is not a table, and cannot be used in all select as multiple" % t.pathname)
+                    "object [%s] is not a table, and cannot be used in all select as multiple" %
+                    t.pathname)
 
             if nrows is None:
                 nrows = t.nrows
@@ -655,7 +670,7 @@ class HDFStore(StringMixin):
             c = self.select_as_coordinates(
                 selector, where, start=start, stop=stop)
             nrows = len(c)
-        except (Exception) as detail:
+        except Exception:
             raise ValueError("invalid selector [%s]" % selector)
 
         def func(_start, _stop):
@@ -777,8 +792,8 @@ class HDFStore(StringMixin):
         data in the table, so be careful
         """
         if columns is not None:
-            raise Exception(
-                "columns is not a supported keyword in append, try data_columns")
+            raise TypeError("columns is not a supported keyword in append, "
+                            "try data_columns")
 
         if dropna is None:
             dropna = get_option("io.hdf.dropna_table")
@@ -809,8 +824,9 @@ class HDFStore(StringMixin):
 
         """
         if axes is not None:
-            raise Exception(
-                "axes is currently not accepted as a paremter to append_to_multiple; you can create the tables indepdently instead")
+            raise TypeError("axes is currently not accepted as a parameter to"
+                            " append_to_multiple; you can create the "
+                            "tables indepdently instead")
 
         if not isinstance(d, dict):
             raise ValueError(
@@ -876,7 +892,7 @@ class HDFStore(StringMixin):
         # version requirements
         _tables()
         if not _table_supports_index:
-            raise Exception("PyTables >= 2.3 is required for table indexing")
+            raise ValueError("PyTables >= 2.3 is required for table indexing")
 
         s = self.get_storer(key)
         if s is None:
@@ -930,7 +946,11 @@ class HDFStore(StringMixin):
 
         """
         new_store = HDFStore(
-            file, mode=mode, complib=complib, complevel=complevel, fletcher32 = fletcher32)
+            file,
+            mode=mode,
+            complib=complib,
+            complevel=complevel,
+            fletcher32=fletcher32)
         if keys is None:
             keys = list(self.keys())
         if not isinstance(keys, (tuple, list)):
@@ -1142,7 +1162,8 @@ class TableIterator(object):
         kwargs : the passed kwargs
         """
 
-    def __init__(self, store, func, nrows, start=None, stop=None, chunksize=None, auto_close=False):
+    def __init__(self, store, func, nrows, start=None, stop=None,
+                 chunksize=None, auto_close=False):
         self.store = store
         self.func = func
         self.nrows = nrows or 0
@@ -1251,7 +1272,12 @@ class IndexCol(StringMixin):
 
     def __unicode__(self):
         temp = tuple(
-            map(pprint_thing, (self.name, self.cname, self.axis, self.pos, self.kind)))
+            map(pprint_thing,
+                    (self.name,
+                     self.cname,
+                     self.axis,
+                     self.pos,
+                     self.kind)))
         return "name->%s,cname->%s,axis->%s,pos->%s,kind->%s" % temp
 
     def __eq__(self, other):
@@ -1361,9 +1387,7 @@ class IndexCol(StringMixin):
         """ validate this column: return the compared against itemsize """
 
         # validate this column for string truncation (or reset to the max size)
-        dtype = getattr(self, 'dtype', None)
         if _ensure_decoded(self.kind) == u('string'):
-
             c = self.col
             if c is not None:
                 if itemsize is None:
@@ -1467,7 +1491,8 @@ class DataCol(IndexCol):
     _info_fields = ['tz']
 
     @classmethod
-    def create_for_block(cls, i=None, name=None, cname=None, version=None, **kwargs):
+    def create_for_block(
+            cls, i=None, name=None, cname=None, version=None, **kwargs):
         """ return a new datacol with the block i """
 
         if cname is None:
@@ -1487,7 +1512,8 @@ class DataCol(IndexCol):
 
         return cls(name=name, cname=cname, **kwargs)
 
-    def __init__(self, values=None, kind=None, typ=None, cname=None, data=None, block=None, **kwargs):
+    def __init__(self, values=None, kind=None, typ=None,
+                 cname=None, data=None, block=None, **kwargs):
         super(DataCol, self).__init__(
             values=values, kind=kind, typ=typ, cname=cname, **kwargs)
         self.dtype = None
@@ -1540,7 +1566,8 @@ class DataCol(IndexCol):
             if self.typ is None:
                 self.typ = getattr(self.description, self.cname, None)
 
-    def set_atom(self, block, existing_col, min_itemsize, nan_rep, info, encoding=None, **kwargs):
+    def set_atom(self, block, existing_col, min_itemsize,
+                 nan_rep, info, encoding=None, **kwargs):
         """ create and setup my atom from the block b """
 
         self.values = list(block.items)
@@ -1596,7 +1623,11 @@ class DataCol(IndexCol):
         # end up here ###
         elif inferred_type == 'string' or dtype == 'object':
             self.set_atom_string(
-                block, existing_col, min_itemsize, nan_rep, encoding)
+                block,
+                existing_col,
+                min_itemsize,
+                nan_rep,
+                encoding)
         else:
             self.set_atom_data(block)
 
@@ -1605,7 +1636,8 @@ class DataCol(IndexCol):
     def get_atom_string(self, block, itemsize):
         return _tables().StringCol(itemsize=itemsize, shape=block.shape[0])
 
-    def set_atom_string(self, block, existing_col, min_itemsize, nan_rep, encoding):
+    def set_atom_string(
+            self, block, existing_col, min_itemsize, nan_rep, encoding):
         # fill nan items with myself
         block = block.fillna(nan_rep)[0]
         data = block.values
@@ -1701,13 +1733,13 @@ class DataCol(IndexCol):
             if (existing_fields is not None and
                     existing_fields != list(self.values)):
                 raise ValueError("appended items do not match existing items"
-                                " in table!")
+                                 " in table!")
 
             existing_dtype = getattr(self.attrs, self.dtype_attr, None)
             if (existing_dtype is not None and
                     existing_dtype != self.dtype):
                 raise ValueError("appended items dtype do not match existing items dtype"
-                                " in table!")
+                                 " in table!")
 
     def convert(self, values, nan_rep, encoding):
         """ set the data from this selection (and convert to the correct dtype if we can) """
@@ -1855,6 +1887,9 @@ class Fixed(StringMixin):
             return "%-12.12s (shape->%s)" % (self.pandas_type, s)
         return self.pandas_type
 
+    def __str__(self):
+        return self.__repr__()
+
     def set_object_info(self):
         """ set my pandas type & version """
         self.attrs.pandas_type = str(self.pandas_kind)
@@ -2058,7 +2093,7 @@ class GenericFixed(Fixed):
             _, index = self.read_index_node(getattr(self.group, key))
             return index
         else:  # pragma: no cover
-            raise Exception('unrecognized index variety: %s' % variety)
+            raise TypeError('unrecognized index variety: %s' % variety)
 
     def write_index(self, key, index):
         if isinstance(index, MultiIndex):
@@ -2241,7 +2276,7 @@ class GenericFixed(Fixed):
                 warnings.warn(ws, PerformanceWarning)
 
             vlarr = self._handle.createVLArray(self.group, key,
-                                              _tables().ObjectAtom())
+                                               _tables().ObjectAtom())
             vlarr.append(value)
         elif value.dtype.type == np.datetime64:
             self._handle.createArray(self.group, key, value.view('i8'))
@@ -2381,8 +2416,7 @@ class SparsePanelFixed(GenericFixed):
         sdict = {}
         for name in items:
             key = 'sparse_frame_%s' % name
-            node = getattr(self.group, key)
-            s = SparseFrameFixed(self.parent, getattr(self.group, key))
+            s = SparseFrameStorer(self.parent, getattr(self.group, key))
             s.infer_axes()
             sdict[name] = s.read()
         return SparsePanel(sdict, items=items, default_kind=self.default_kind,
@@ -2574,7 +2608,8 @@ class Table(Fixed):
                     oax = ov[i]
                     if sax != oax:
                         raise ValueError(
-                            "invalid combinate of [%s] on appending data [%s] vs current table [%s]" % (c, sax, oax))
+                            "invalid combinate of [%s] on appending data [%s] vs current table [%s]" %
+                            (c, sax, oax))
 
                 # should never get here
                 raise Exception(
@@ -2706,14 +2741,14 @@ class Table(Fixed):
                 continue
             if k not in q:
                 raise ValueError(
-                    "min_itemsize has the key [%s] which is not an axis or data_column" % k)
+                    "min_itemsize has the key [%s] which is not an axis or data_column" %
+                    k)
 
     @property
     def indexables(self):
         """ create/cache the indexables if they don't exist """
         if self._indexables is None:
 
-            d = self.description
             self._indexables = []
 
             # index columns
@@ -2848,7 +2883,8 @@ class Table(Fixed):
         # return valid columns in the order of our axis
         return [c for c in data_columns if c in axis_labels]
 
-    def create_axes(self, axes, obj, validate=True, nan_rep=None, data_columns=None, min_itemsize=None, **kwargs):
+    def create_axes(self, axes, obj, validate=True, nan_rep=None,
+                    data_columns=None, min_itemsize=None, **kwargs):
         """ create and return the axes
               leagcy tables create an indexable column, indexable index, non-indexable fields
 
@@ -2869,8 +2905,7 @@ class Table(Fixed):
             try:
                 axes = _AXES_MAP[type(obj)]
             except:
-                raise TypeError(
-                    "cannot properly create the storer for: [group->%s,value->%s]" %
+                raise TypeError("cannot properly create the storer for: [group->%s,value->%s]" %
                                 (self.group._v_name, type(obj)))
 
         # map axes to numbers
@@ -2995,8 +3030,7 @@ class Table(Fixed):
                 try:
                     existing_col = existing_table.values_axes[i]
                 except:
-                    raise ValueError(
-                        "Incompatible appended table [%s] with existing table [%s]" %
+                    raise ValueError("Incompatible appended table [%s] with existing table [%s]" %
                                     (blocks, existing_table.values_axes))
             else:
                 existing_col = None
@@ -3070,7 +3104,8 @@ class Table(Fixed):
 
         return obj
 
-    def create_description(self, complib=None, complevel=None, fletcher32=False, expectedrows=None):
+    def create_description(
+            self, complib=None, complevel=None, fletcher32=False, expectedrows=None):
         """ create the description of the table from the axes & values """
 
         # expected rows estimate
@@ -3119,8 +3154,8 @@ class Table(Fixed):
             return False
 
         if where is not None:
-            raise Exception(
-                "read_column does not currently accept a where clause")
+            raise TypeError("read_column does not currently accept a where "
+                            "clause")
 
         # find the axes
         for a in self.axes:
@@ -3128,7 +3163,8 @@ class Table(Fixed):
 
                 if not a.is_data_indexable:
                     raise ValueError(
-                        "column [%s] can not be extracted individually; it is not data indexable" % column)
+                        "column [%s] can not be extracted individually; it is not data indexable" %
+                        column)
 
                 # column must be an indexable or a data column
                 c = getattr(self.table.cols, column)
@@ -3174,7 +3210,7 @@ class LegacyTable(Table):
     ndim = 3
 
     def write(self, **kwargs):
-        raise Exception("write operations are not allowed on legacy tables!")
+        raise TypeError("write operations are not allowed on legacy tables!")
 
     def read(self, where=None, columns=None, **kwargs):
         """ we have n indexable columns, with an arbitrary number of data axes """
@@ -3626,9 +3662,9 @@ class GenericTable(AppendableFrameTable):
         self.levels = []
         t = self.table
         self.index_axes = [a.infer(t)
-                                         for a in self.indexables if a.is_an_indexable]
+                           for a in self.indexables if a.is_an_indexable]
         self.values_axes = [a.infer(t)
-                                         for a in self.indexables if not a.is_an_indexable]
+                            for a in self.indexables if not a.is_an_indexable]
         self.data_columns = [a.name for a in self.values_axes]
 
     @property
@@ -3755,7 +3791,7 @@ def _convert_index(index, encoding=None):
                         index_name=index_name)
 
     if isinstance(index, MultiIndex):
-        raise Exception('MultiIndex not supported here!')
+        raise TypeError('MultiIndex not supported here!')
 
     inferred_type = lib.infer_dtype(index)
 
@@ -3904,32 +3940,13 @@ def _need_convert(kind):
     return False
 
 
-class Term(StringMixin):
-
-    """create a term object that holds a field, op, and value
-
-    Parameters
-    ----------
-    field : dict, string term expression, or the field to operate (must be a valid index/column type of DataFrame/Panel)
-    op    : a valid op (defaults to '=') (optional)
-            >, >=, <, <=, =, != (not equal) are allowed
-    value : a value or list of values (required)
-    queryables : a kinds map (dict of column name -> kind), or None i column is non-indexable
-    encoding : an encoding that will encode the query terms
+class Coordinates(object):
 
-    Returns
-    -------
-    a Term object
+    """ holds a returned coordinates list, useful to select the same rows from different tables
 
-    Examples
-    --------
-    >>> Term(dict(field = 'index', op = '>', value = '20121114'))
-    >>> Term('index', '20121114')
-    >>> Term('index', '>', '20121114')
-    >>> Term('index', ['20121114','20121114'])
-    >>> Term('index', datetime(2012,11,14))
-    >>> Term('major_axis>20121114')
-    >>> Term('minor_axis', ['A','U'])
+    coordinates : holds the array of coordinates
+    group       : the source group
+    where       : the source where
     """
 
     _ops = ['<=', '<', '>=', '>', '!=', '==', '=']
@@ -4134,23 +4151,13 @@ class Term(StringMixin):
         return TermValue(v, stringify(v), u('string'))
 
 
-class TermValue(object):
-
-    """ hold a term value the we use to construct a condition/filter """
 
-    def __init__(self, value, converted, kind):
-        self.value = value
-        self.converted = converted
-        self.kind = kind
+    def __len__(self):
+        return len(self.values)
 
-    def tostring(self, encoding):
-        """ quote the string if not encoded
-            else encode and return """
-        if self.kind == u('string'):
-            if encoding is not None:
-                return self.converted
-            return '"%s"' % self.converted
-        return self.converted
+    def __getitem__(self, key):
+        """ return a new coordinates object, sliced by the key """
+        return Coordinates(self.values[key], self.group, self.where)
 
 
 class Selection(object):
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index 861b4dd75..6a325db8a 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -2,9 +2,10 @@ from __future__ import print_function
 from pandas.compat import range, lrange, u
 import nose
 import unittest
-import os
 import sys
+import os
 import warnings
+from contextlib import contextmanager
 
 import datetime
 import numpy as np
@@ -25,7 +26,6 @@ from pandas import concat, Timestamp
 from pandas import compat, _np_version_under1p7
 from pandas.core import common as com
 
-from numpy.testing.decorators import slow
 
 try:
     import tables
@@ -42,12 +42,12 @@ _multiprocess_can_split_ = False
 # contextmanager to ensure the file cleanup
 def safe_remove(path):
     if path is not None:
-        import os
         try:
             os.remove(path)
         except:
             pass
 
+
 def safe_close(store):
     try:
         if store is not None:
@@ -55,7 +55,6 @@ def safe_close(store):
     except:
         pass
 
-from contextlib import contextmanager
 
 @contextmanager
 def ensure_clean(path, mode='a', complevel=None, complib=None,
@@ -1328,6 +1327,7 @@ class TestHDFStore(unittest.TestCase):
             store.append('df', df)
             rows = store.root.df.table.nrows
             recons = store.select('df')
+            assert isinstance(recons, DataFrame)
 
         print("\nbig_table frame [%s] -> %5.2f" % (rows, time.time() - x))
 
@@ -1382,7 +1382,7 @@ class TestHDFStore(unittest.TestCase):
 
         with ensure_clean(self.path, mode='w') as store:
             start_time = time.time()
-            store = HDFStore(fn, mode='w')
+            store = HDFStore(self.path, mode='w')
             store.put('df', df)
 
             print(df.get_dtype_counts())
@@ -1410,6 +1410,7 @@ class TestHDFStore(unittest.TestCase):
             store.append('wp', wp)
             rows = store.root.wp.table.nrows
             recons = store.select('wp')
+            assert isinstance(recons, Panel)
 
         print("\nbig_table panel [%s] -> %5.2f" % (rows, time.time() - x))
 
@@ -1654,7 +1655,6 @@ class TestHDFStore(unittest.TestCase):
             expected.sort()
             tm.assert_series_equal(result,expected)
 
-
     def test_table_mixed_dtypes(self):
 
         # frame
@@ -2898,7 +2898,6 @@ class TestHDFStore(unittest.TestCase):
             expected = df[df.int!=2]
             assert_frame_equal(result,expected)
 
-
     def test_read_column(self):
 
         df = tm.makeTimeDataFrame()
@@ -3190,7 +3189,6 @@ class TestHDFStore(unittest.TestCase):
             again = store['obj']
             comparator(again, obj, **kwargs)
 
-
     def _check_roundtrip_table(self, obj, comparator, compression=False):
         options = {}
         if compression:
@@ -3296,6 +3294,7 @@ class TestHDFStore(unittest.TestCase):
         try:
             store = HDFStore(tm.get_data_path('legacy_hdf/pytables_native.h5'), 'r')
             d2 = store['detector/readout']
+            assert isinstance(d2, DataFrame)
         finally:
             safe_close(store)
 
@@ -3303,6 +3302,7 @@ class TestHDFStore(unittest.TestCase):
             store = HDFStore(tm.get_data_path('legacy_hdf/pytables_native2.h5'), 'r')
             str(store)
             d1 = store['detector']
+            assert isinstance(d1, DataFrame)
         finally:
             safe_close(store)
 
@@ -3352,11 +3352,18 @@ class TestHDFStore(unittest.TestCase):
     def test_legacy_0_11_read(self):
         # legacy from 0.11
         try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/legacy_table_0.11.h5'), 'r')
+            path = os.path.join('legacy_hdf', 'legacy_table_0.11.h5')
+            store = HDFStore(tm.get_data_path(path), 'r')
             str(store)
+            assert 'df' in store
+            assert 'df1' in store
+            assert 'mi' in store
             df = store.select('df')
             df1 = store.select('df1')
             mi = store.select('mi')
+            assert isinstance(df, DataFrame)
+            assert isinstance(df1, DataFrame)
+            assert isinstance(mi, DataFrame)
         finally:
             safe_close(store)
 
@@ -3364,10 +3371,9 @@ class TestHDFStore(unittest.TestCase):
 
         def do_copy(f = None, new_f = None, keys = None, propindexes = True, **kwargs):
             try:
-                import os
-
                 if f is None:
-                    f = tm.get_data_path('legacy_hdf/legacy_0.10.h5')
+                    f = tm.get_data_path(os.path.join('legacy_hdf',
+                                                      'legacy_0.10.h5'))
 
 
                 store = HDFStore(f, 'r')
@@ -3437,6 +3443,7 @@ class TestHDFStore(unittest.TestCase):
 
         df = DataFrame(dict(A = 'foo', B = 'bar'),index=lrange(10))
         store.append('df', df, data_columns = ['B'], min_itemsize={'A' : 200 })
+        store.append('wp', wp)
 
         store.close()
 
@@ -3524,6 +3531,7 @@ def _test_sort(obj):
     else:
         raise ValueError('type not supported here')
 
+
 if __name__ == '__main__':
     import nose
     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
diff --git a/pandas/tests/test_common.py b/pandas/tests/test_common.py
index e2051eba7..96131d782 100644
--- a/pandas/tests/test_common.py
+++ b/pandas/tests/test_common.py
@@ -17,6 +17,11 @@ import pandas.core.common as com
 import pandas.util.testing as tm
 import pandas.core.config as cf
 
+import numpy as np
+from numpy.random import randn
+
+from pandas.tslib import iNaT
+
 _multiprocess_can_split_ = True
 
 
@@ -42,6 +47,7 @@ def test_is_sequence():
 
     assert(not is_seq(A()))
 
+
 def test_notnull():
     assert notnull(1.)
     assert not notnull(None)
@@ -107,6 +113,61 @@ def test_isnull_lists():
     assert(not result.any())
 
 
+def test_is_string():
+    class MyString(str):
+        pass
+
+    class MyUnicode(unicode):
+        pass
+
+    strings = ('s', np.str_('a'), np.unicode_('unicode_string'),
+               MyString('a _string blah'), u'asdf', MyUnicode(u'asdf'))
+    not_strings = [], 1, {}, set(), np.array(['1']), np.array([u'1'])
+
+    for string in strings:
+        assert com.is_string(string), '{0} is not a string'.format(string)
+
+    for not_string in not_strings:
+        assert not com.is_string(not_string), ('{0} is a '
+                                               'string'.format(not_string))
+
+
+def test_is_frame():
+    df = DataFrame(randn(2, 1))
+    assert com.is_frame(df)
+    assert not com.is_frame('s')
+
+
+def test_is_series():
+    s = Series(randn(2))
+    assert com.is_series(s)
+    assert not com.is_series(s.values)
+
+
+def test_is_panel():
+    p = Panel(randn(2, 3, 4))
+    assert com.is_panel(p)
+    assert not com.is_panel(2)
+
+
+def test_is_pd_obj():
+    df = DataFrame(randn(2, 1))
+    s = Series(randn(2))
+    p = Panel(randn(2, 3, 4))
+    for obj in (df, s, p):
+        assert com.is_pd_obj(obj)
+        assert not com.is_pd_obj(obj.values)
+
+
+def test_is_ndframe():
+    df = DataFrame(randn(2, 1))
+    p = Panel(randn(2, 3, 4))
+    # should add series after @jreback's ndframe to series pr
+    for obj in (df, p):
+        assert com.is_ndframe(obj)
+        assert not com.is_ndframe(obj.values)
+
+
 def test_isnull_datetime():
     assert (not isnull(datetime.now()))
     assert notnull(datetime.now())
@@ -121,11 +182,13 @@ def test_isnull_datetime():
     assert(mask[0])
     assert(not mask[1:].any())
 
+
 def test_datetimeindex_from_empty_datetime64_array():
     for unit in [ 'ms', 'us', 'ns' ]:
         idx = DatetimeIndex(np.array([], dtype='datetime64[%s]' % unit))
         assert(len(idx) == 0)
 
+
 def test_nan_to_nat_conversions():
 
     df = DataFrame(dict({
@@ -144,6 +207,7 @@ def test_nan_to_nat_conversions():
     if LooseVersion(np.__version__) >= '1.7.0':
         assert(s[8].value == np.datetime64('NaT').astype(np.int64))
 
+
 def test_any_none():
     assert(com._any_none(1, 2, 3, None))
     assert(not com._any_none(1, 2, 3, 4))
@@ -308,6 +372,7 @@ def test_ensure_int32():
     result = com._ensure_int32(values)
     assert(result.dtype == np.int32)
 
+
 def test_ensure_platform_int():
 
     # verify that when we create certain types of indices
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 1572ca481..8646d2613 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -926,7 +926,8 @@ class DatetimeIndex(Int64Index):
         See Index.join
         """
         if (not isinstance(other, DatetimeIndex) and len(other) > 0 and
-            other.inferred_type != 'mixed-integer'):
+            other.inferred_type not in ('floating', 'mixed-integer',
+                                        'mixed-integer-float', 'mixed')):
             try:
                 other = DatetimeIndex(other)
             except TypeError:
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index abc13fb2a..eeb5ca436 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -15,7 +15,7 @@ from functools import wraps, partial
 from contextlib import contextmanager
 from distutils.version import LooseVersion
 
-from numpy.random import randn
+from numpy.random import randn, rand
 import numpy as np
 
 from pandas.core.common import isnull, _is_sequence
@@ -48,6 +48,9 @@ K = 4
 _RAISE_NETWORK_ERROR_DEFAULT = False
 
 
+def randbool(size=(), p=0.5):
+    return rand(*size) <= p
+
 def rands(n):
     choices = string.ascii_letters + string.digits
     return ''.join(random.choice(choices) for _ in range(n))
diff --git a/setup.py b/setup.py
index b7df339da..955dedb74 100755
--- a/setup.py
+++ b/setup.py
@@ -83,7 +83,7 @@ try:
 except ImportError:
     cython = False
 
-from os.path import splitext, basename, join as pjoin
+from os.path import join as pjoin
 
 
 class build_ext(_build_ext):
@@ -506,6 +506,7 @@ setup(name=DISTNAME,
       maintainer=AUTHOR,
       packages=['pandas',
                 'pandas.compat',
+                'pandas.computation',
                 'pandas.core',
                 'pandas.io',
                 'pandas.rpy',
diff --git a/vb_suite/binary_ops.py b/vb_suite/binary_ops.py
index 547743445..3f076f9f9 100644
--- a/vb_suite/binary_ops.py
+++ b/vb_suite/binary_ops.py
@@ -21,7 +21,7 @@ frame_add = \
               start_date=datetime(2012, 1, 1))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_numexpr_threads(1)
@@ -32,7 +32,7 @@ frame_add_st = \
               start_date=datetime(2013, 2, 26))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_use_numexpr(False)
@@ -53,7 +53,7 @@ frame_mult = \
               start_date=datetime(2012, 1, 1))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_numexpr_threads(1)
@@ -63,7 +63,7 @@ frame_mult_st = \
               start_date=datetime(2013, 2, 26))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_use_numexpr(False)
@@ -84,7 +84,7 @@ frame_multi_and = \
               start_date=datetime(2012, 1, 1))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_numexpr_threads(1)
@@ -94,7 +94,7 @@ frame_multi_and_st = \
               start_date=datetime(2013, 2, 26))
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(20000, 100))
 df2 = DataFrame(np.random.randn(20000, 100))
 expr.set_use_numexpr(False)
diff --git a/vb_suite/indexing.py b/vb_suite/indexing.py
index 1264ae053..2fb5a22ce 100644
--- a/vb_suite/indexing.py
+++ b/vb_suite/indexing.py
@@ -118,7 +118,7 @@ indexing_dataframe_boolean_st = \
 
 
 setup = common_setup + """
-import pandas.core.expressions as expr
+import pandas.computation.expressions as expr
 df  = DataFrame(np.random.randn(50000, 100))
 df2 = DataFrame(np.random.randn(50000, 100))
 expr.set_use_numexpr(False)
