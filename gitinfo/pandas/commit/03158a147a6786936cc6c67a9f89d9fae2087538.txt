commit 03158a147a6786936cc6c67a9f89d9fae2087538
Author: jreback <jeff@reback.net>
Date:   Mon Jul 22 13:29:48 2013 -0400

    BLD: pep8 major changes

diff --git a/pandas/core/common.py b/pandas/core/common.py
index 1964aada8..7af4be1c3 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -27,9 +27,11 @@ try:
 except Exception:  # pragma: no cover
     pass
 
+
 class PandasError(Exception):
     pass
 
+
 class AmbiguousIndexError(PandasError, KeyError):
     pass
 
@@ -38,30 +40,39 @@ _np_version = np.version.short_version
 _np_version_under1p6 = LooseVersion(_np_version) < '1.6'
 _np_version_under1p7 = LooseVersion(_np_version) < '1.7'
 
-_POSSIBLY_CAST_DTYPES = set([ np.dtype(t) for t in ['M8[ns]','m8[ns]','O','int8','uint8','int16','uint16','int32','uint32','int64','uint64'] ])
+_POSSIBLY_CAST_DTYPES = set([np.dtype(t)
+                            for t in ['M8[ns]', 'm8[ns]', 'O', 'int8', 'uint8', 'int16', 'uint16', 'int32', 'uint32', 'int64', 'uint64']])
+
 _NS_DTYPE = np.dtype('M8[ns]')
 _TD_DTYPE = np.dtype('m8[ns]')
 _INT64_DTYPE = np.dtype(np.int64)
-_DATELIKE_DTYPES = set([ np.dtype(t) for t in ['M8[ns]','m8[ns]'] ])
+_DATELIKE_DTYPES = set([np.dtype(t) for t in ['M8[ns]', 'm8[ns]']])
+
 
 def is_series(obj):
-    return getattr(obj, '_typ' ,None) == 'series'
+    return getattr(obj, '_typ', None) == 'series'
+
 
 def is_sparse_series(obj):
-    return getattr(obj, '_subtyp', None) in ('sparse_series','sparse_time_series')
+    return getattr(obj, '_subtyp', None) in ('sparse_series', 'sparse_time_series')
+
 
 def is_sparse_array_like(obj):
-    return getattr(obj, '_subtyp', None) in ['sparse_array','sparse_series','sparse_array']
+    return getattr(obj, '_subtyp', None) in ['sparse_array', 'sparse_series', 'sparse_array']
+
 
 def is_dataframe(obj):
     return getattr(obj, '_typ', None) == 'dataframe'
 
+
 def is_panel(obj):
     return getattr(obj, '_typ', None) == 'panel'
 
+
 def is_generic(obj):
     return getattr(obj, '_data', None) is not None
 
+
 def isnull(obj):
     """Detect missing values (NaN in numeric arrays, None/NaN in object arrays)
 
@@ -119,6 +130,7 @@ def _isnull_old(obj):
 
 _isnull = _isnull_new
 
+
 def _use_inf_as_null(key):
     '''Option change callback for null/inf behaviour
     Choose which replacement for numpy.isnan / -numpy.isfinite is used.
@@ -148,7 +160,7 @@ def _use_inf_as_null(key):
 def _isnull_ndarraylike(obj):
 
     values = obj
-    dtype  = values.dtype
+    dtype = values.dtype
 
     if dtype.kind in ('O', 'S', 'U'):
         # Working around NumPy ticket 1542
@@ -163,7 +175,7 @@ def _isnull_ndarraylike(obj):
 
     elif dtype in _DATELIKE_DTYPES:
         # this is the NaT pattern
-        v = getattr(values,'asi8',None)
+        v = getattr(values, 'asi8', None)
         if v is None:
             v = values.view('i8')
         result = v == tslib.iNaT
@@ -176,9 +188,10 @@ def _isnull_ndarraylike(obj):
 
     return result
 
+
 def _isnull_ndarraylike_old(obj):
     values = obj
-    dtype  = values.dtype
+    dtype = values.dtype
 
     if dtype.kind in ('O', 'S', 'U'):
         # Working around NumPy ticket 1542
@@ -193,7 +206,7 @@ def _isnull_ndarraylike_old(obj):
 
     elif dtype in _DATELIKE_DTYPES:
         # this is the NaT pattern
-        v = getattr(values,'asi8',None)
+        v = getattr(values, 'asi8', None)
         if v is None:
             v = values.view('i8')
         result = v == tslib.iNaT
@@ -206,6 +219,7 @@ def _isnull_ndarraylike_old(obj):
 
     return result
 
+
 def notnull(obj):
     """Replacement for numpy.isfinite / -numpy.isnan which is suitable for use
     on object arrays.
@@ -250,9 +264,9 @@ def mask_missing(arr, values_to_mask):
 
             # if x is a string and mask is not, then we get a scalar
             # return value, which is not good
-            if not isinstance(mask,np.ndarray):
+            if not isinstance(mask, np.ndarray):
                 m = mask
-                mask = np.empty(arr.shape,dtype=np.bool)
+                mask = np.empty(arr.shape, dtype=np.bool)
                 mask.fill(m)
         else:
             mask = mask | (arr == x)
@@ -357,11 +371,11 @@ _take_1d_dict = {
     ('float64', 'float64'): algos.take_1d_float64_float64,
     ('object', 'object'): algos.take_1d_object_object,
     ('bool', 'bool'):
-        _view_wrapper(algos.take_1d_bool_bool, np.uint8, np.uint8),
+    _view_wrapper(algos.take_1d_bool_bool, np.uint8, np.uint8),
     ('bool', 'object'):
-        _view_wrapper(algos.take_1d_bool_object, np.uint8, None),
-    ('datetime64[ns]','datetime64[ns]'):
-        _view_wrapper(algos.take_1d_int64_int64, np.int64, np.int64, np.int64)
+    _view_wrapper(algos.take_1d_bool_object, np.uint8, None),
+    ('datetime64[ns]', 'datetime64[ns]'):
+    _view_wrapper(algos.take_1d_int64_int64, np.int64, np.int64, np.int64)
 }
 
 
@@ -384,12 +398,12 @@ _take_2d_axis0_dict = {
     ('float64', 'float64'): algos.take_2d_axis0_float64_float64,
     ('object', 'object'): algos.take_2d_axis0_object_object,
     ('bool', 'bool'):
-        _view_wrapper(algos.take_2d_axis0_bool_bool, np.uint8, np.uint8),
+    _view_wrapper(algos.take_2d_axis0_bool_bool, np.uint8, np.uint8),
     ('bool', 'object'):
-        _view_wrapper(algos.take_2d_axis0_bool_object, np.uint8, None),
-    ('datetime64[ns]','datetime64[ns]'):
-        _view_wrapper(algos.take_2d_axis0_int64_int64, np.int64, np.int64,
-                      fill_wrap=np.int64)
+    _view_wrapper(algos.take_2d_axis0_bool_object, np.uint8, None),
+    ('datetime64[ns]', 'datetime64[ns]'):
+    _view_wrapper(algos.take_2d_axis0_int64_int64, np.int64, np.int64,
+                  fill_wrap=np.int64)
 }
 
 
@@ -412,12 +426,12 @@ _take_2d_axis1_dict = {
     ('float64', 'float64'): algos.take_2d_axis1_float64_float64,
     ('object', 'object'): algos.take_2d_axis1_object_object,
     ('bool', 'bool'):
-        _view_wrapper(algos.take_2d_axis1_bool_bool, np.uint8, np.uint8),
+    _view_wrapper(algos.take_2d_axis1_bool_bool, np.uint8, np.uint8),
     ('bool', 'object'):
-        _view_wrapper(algos.take_2d_axis1_bool_object, np.uint8, None),
-    ('datetime64[ns]','datetime64[ns]'):
-        _view_wrapper(algos.take_2d_axis1_int64_int64, np.int64, np.int64,
-                      fill_wrap=np.int64)
+    _view_wrapper(algos.take_2d_axis1_bool_object, np.uint8, None),
+    ('datetime64[ns]', 'datetime64[ns]'):
+    _view_wrapper(algos.take_2d_axis1_int64_int64, np.int64, np.int64,
+                  fill_wrap=np.int64)
 }
 
 
@@ -440,12 +454,12 @@ _take_2d_multi_dict = {
     ('float64', 'float64'): algos.take_2d_multi_float64_float64,
     ('object', 'object'): algos.take_2d_multi_object_object,
     ('bool', 'bool'):
-        _view_wrapper(algos.take_2d_multi_bool_bool, np.uint8, np.uint8),
+    _view_wrapper(algos.take_2d_multi_bool_bool, np.uint8, np.uint8),
     ('bool', 'object'):
-        _view_wrapper(algos.take_2d_multi_bool_object, np.uint8, None),
-    ('datetime64[ns]','datetime64[ns]'):
-        _view_wrapper(algos.take_2d_multi_int64_int64, np.int64, np.int64,
-                      fill_wrap=np.int64)
+    _view_wrapper(algos.take_2d_multi_bool_object, np.uint8, None),
+    ('datetime64[ns]', 'datetime64[ns]'):
+    _view_wrapper(algos.take_2d_multi_int64_int64, np.int64, np.int64,
+                  fill_wrap=np.int64)
 }
 
 
@@ -686,7 +700,7 @@ def diff(arr, n, axis=0):
                 lag = lag.copy()
                 lag[mask] = 0
 
-            result = res-lag
+            result = res - lag
             result[mask] = na
             out_arr[res_indexer] = result
         else:
@@ -704,10 +718,11 @@ def _infer_dtype_from_scalar(val):
     # a 1-element ndarray
     if isinstance(val, pa.Array):
         if val.ndim != 0:
-            raise ValueError("invalid ndarray passed to _infer_dtype_from_scalar")
+            raise ValueError(
+                "invalid ndarray passed to _infer_dtype_from_scalar")
 
         dtype = val.dtype
-        val   = val.item()
+        val = val.item()
 
     elif isinstance(val, compat.string_types):
 
@@ -721,7 +736,7 @@ def _infer_dtype_from_scalar(val):
 
     elif isinstance(val, np.datetime64):
         # ugly hacklet
-        val   = lib.Timestamp(val).value
+        val = lib.Timestamp(val).value
         dtype = np.dtype('M8[ns]')
 
     elif is_bool(val):
@@ -746,11 +761,12 @@ def _maybe_cast_scalar(dtype, value):
         return tslib.iNaT
     return value
 
+
 def _maybe_promote(dtype, fill_value=np.nan):
 
     # if we passed an array here, determine the fill value by dtype
-    if isinstance(fill_value,np.ndarray):
-        if issubclass(fill_value.dtype.type, (np.datetime64,np.timedelta64)):
+    if isinstance(fill_value, np.ndarray):
+        if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)):
             fill_value = tslib.iNaT
         else:
 
@@ -761,7 +777,7 @@ def _maybe_promote(dtype, fill_value=np.nan):
             fill_value = np.nan
 
     # returns tuple of (dtype, fill_value)
-    if issubclass(dtype.type, (np.datetime64,np.timedelta64)):
+    if issubclass(dtype.type, (np.datetime64, np.timedelta64)):
         # for now: refuse to upcast datetime64
         # (this is because datetime64 will not implicitly upconvert
         #  to object correctly as of numpy 1.6.1)
@@ -818,6 +834,7 @@ def _maybe_upcast_putmask(result, mask, other, dtype=None, change=None):
     if mask.any():
 
         other = _maybe_cast_scalar(result.dtype, other)
+
         def changeit():
 
             # try to directly set by expanding our array to full
@@ -833,8 +850,10 @@ def _maybe_upcast_putmask(result, mask, other, dtype=None, change=None):
             except:
                 pass
 
-            # we are forced to change the dtype of the result as the input isn't compatible
-            r, fill_value = _maybe_upcast(result, fill_value=other, dtype=dtype, copy=True)
+            # we are forced to change the dtype of the result as the input
+            # isn't compatible
+            r, fill_value = _maybe_upcast(
+                result, fill_value=other, dtype=dtype, copy=True)
             np.putmask(r, mask, other)
 
             # we need to actually change the dtype here
@@ -843,7 +862,8 @@ def _maybe_upcast_putmask(result, mask, other, dtype=None, change=None):
                 # if we are trying to do something unsafe
                 # like put a bigger dtype in a smaller one, use the smaller one
                 if change.dtype.itemsize < r.dtype.itemsize:
-                    raise Exception("cannot change dtype of input to smaller size")
+                    raise Exception(
+                        "cannot change dtype of input to smaller size")
                 change.dtype = r.dtype
                 change[:] = r
 
@@ -853,12 +873,12 @@ def _maybe_upcast_putmask(result, mask, other, dtype=None, change=None):
         # if we have nans in the False portion of our mask then we need to upcast (possibily)
         # otherwise we DON't want to upcast (e.g. if we are have values, say integers in
         # the success portion then its ok to not upcast)
-        new_dtype, fill_value = _maybe_promote(result.dtype,other)
+        new_dtype, fill_value = _maybe_promote(result.dtype, other)
         if new_dtype != result.dtype:
 
             # we have a scalar or len 0 ndarray
             # and its nan and we are changing some values
-            if np.isscalar(other) or (isinstance(other,np.ndarray) and other.ndim < 1):
+            if np.isscalar(other) or (isinstance(other, np.ndarray) and other.ndim < 1):
                 if isnull(other):
                     return changeit()
 
@@ -875,6 +895,7 @@ def _maybe_upcast_putmask(result, mask, other, dtype=None, change=None):
 
     return result, False
 
+
 def _maybe_upcast_indexer(result, indexer, other, dtype=None):
     """ a safe version of setitem that (potentially upcasts the result
         return the result and a changed flag
@@ -882,9 +903,11 @@ def _maybe_upcast_indexer(result, indexer, other, dtype=None):
 
     other = _maybe_cast_scalar(result.dtype, other)
     original_dtype = result.dtype
+
     def changeit():
         # our type is wrong here, need to upcast
-        r, fill_value = _maybe_upcast(result, fill_value=other, dtype=dtype, copy=True)
+        r, fill_value = _maybe_upcast(
+            result, fill_value=other, dtype=dtype, copy=True)
         try:
             r[indexer] = other
         except:
@@ -893,10 +916,10 @@ def _maybe_upcast_indexer(result, indexer, other, dtype=None):
             r[indexer] = fill_value
 
         # if we have changed to floats, might want to cast back if we can
-        r = _possibly_downcast_to_dtype(r,original_dtype)
+        r = _possibly_downcast_to_dtype(r, original_dtype)
         return r, True
 
-    new_dtype, fill_value = _maybe_promote(original_dtype,other)
+    new_dtype, fill_value = _maybe_promote(original_dtype, other)
     if new_dtype != result.dtype:
         return changeit()
 
@@ -907,6 +930,7 @@ def _maybe_upcast_indexer(result, indexer, other, dtype=None):
 
     return result, False
 
+
 def _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
     """ provide explicty type promotion and coercion
 
@@ -945,9 +969,9 @@ def _possibly_downcast_to_dtype(result, dtype):
         return result
 
     try:
-        if issubclass(dtype.type,np.floating):
+        if issubclass(dtype.type, np.floating):
             return result.astype(dtype)
-        elif dtype == np.bool_ or issubclass(dtype.type,np.integer):
+        elif dtype == np.bool_ or issubclass(dtype.type, np.integer):
             if issubclass(result.dtype.type, np.number) and notnull(result).all():
                 new_result = result.astype(dtype)
                 if (new_result == result).all():
@@ -957,6 +981,7 @@ def _possibly_downcast_to_dtype(result, dtype):
 
     return result
 
+
 def _lcd_dtypes(a_dtype, b_dtype):
     """ return the lcd dtype to hold these types """
 
@@ -984,6 +1009,7 @@ def _lcd_dtypes(a_dtype, b_dtype):
             return np.float64
     return np.object
 
+
 def _fill_zeros(result, y, fill):
     """ if we have an integer value (or array in y)
         and we have 0's, fill them with the fill,
@@ -992,7 +1018,7 @@ def _fill_zeros(result, y, fill):
     if fill is not None:
         if not isinstance(y, np.ndarray):
             dtype, value = _infer_dtype_from_scalar(y)
-            y = pa.empty(result.shape,dtype=dtype)
+            y = pa.empty(result.shape, dtype=dtype)
             y.fill(value)
 
         if is_integer_dtype(y):
@@ -1000,11 +1026,13 @@ def _fill_zeros(result, y, fill):
             mask = y.ravel() == 0
             if mask.any():
                 shape = result.shape
-                result, changed = _maybe_upcast_putmask(result.ravel(),mask,fill)
+                result, changed = _maybe_upcast_putmask(
+                    result.ravel(), mask, fill)
                 result = result.reshape(shape)
 
     return result
 
+
 def _interp_wrapper(f, wrap_dtype, na_override=None):
     def wrapper(arr, mask, limit=None):
         view = arr.view(wrap_dtype)
@@ -1022,10 +1050,10 @@ _backfill_2d_datetime = _interp_wrapper(algos.backfill_2d_inplace_int64,
 
 def pad_1d(values, limit=None, mask=None):
 
-    dtype   = values.dtype.name
+    dtype = values.dtype.name
     _method = None
     if is_float_dtype(values):
-        _method = getattr(algos,'pad_inplace_%s' % dtype,None)
+        _method = getattr(algos, 'pad_inplace_%s' % dtype, None)
     elif is_datetime64_dtype(values):
         _method = _pad_1d_datetime
     elif values.dtype == np.object_:
@@ -1042,10 +1070,10 @@ def pad_1d(values, limit=None, mask=None):
 
 def backfill_1d(values, limit=None, mask=None):
 
-    dtype   = values.dtype.name
+    dtype = values.dtype.name
     _method = None
     if is_float_dtype(values):
-        _method = getattr(algos,'backfill_inplace_%s' % dtype,None)
+        _method = getattr(algos, 'backfill_inplace_%s' % dtype, None)
     elif is_datetime64_dtype(values):
         _method = _backfill_1d_datetime
     elif values.dtype == np.object_:
@@ -1063,10 +1091,10 @@ def backfill_1d(values, limit=None, mask=None):
 
 def pad_2d(values, limit=None, mask=None):
 
-    dtype   = values.dtype.name
+    dtype = values.dtype.name
     _method = None
     if is_float_dtype(values):
-        _method = getattr(algos,'pad_2d_inplace_%s' % dtype,None)
+        _method = getattr(algos, 'pad_2d_inplace_%s' % dtype, None)
     elif is_datetime64_dtype(values):
         _method = _pad_2d_datetime
     elif values.dtype == np.object_:
@@ -1088,10 +1116,10 @@ def pad_2d(values, limit=None, mask=None):
 
 def backfill_2d(values, limit=None, mask=None):
 
-    dtype   = values.dtype.name
+    dtype = values.dtype.name
     _method = None
     if is_float_dtype(values):
-        _method = getattr(algos,'backfill_2d_inplace_%s' % dtype,None)
+        _method = getattr(algos, 'backfill_2d_inplace_%s' % dtype, None)
     elif is_datetime64_dtype(values):
         _method = _backfill_2d_datetime
     elif values.dtype == np.object_:
@@ -1110,6 +1138,7 @@ def backfill_2d(values, limit=None, mask=None):
         # for test coverage
         pass
 
+
 def interpolate_2d(values, method='pad', axis=0, limit=None, missing=None):
     """ perform an actual interpolation of values, values will be make 2-d if needed
         fills inplace, returns the result """
@@ -1139,6 +1168,7 @@ def interpolate_2d(values, method='pad', axis=0, limit=None, missing=None):
 
     return values
 
+
 def _consensus_name_attr(objs):
     name = objs[0].name
     for obj in objs[1:]:
@@ -1149,6 +1179,7 @@ def _consensus_name_attr(objs):
 #----------------------------------------------------------------------
 # Lots of little utilities
 
+
 def _maybe_box(indexer, values, obj, key):
 
     # if we have multiples coming back, box em
@@ -1158,40 +1189,45 @@ def _maybe_box(indexer, values, obj, key):
     # return the value
     return values
 
+
 def _values_from_object(o):
     """ return my values or the object if we are say an ndarray """
-    f = getattr(o,'get_values',None)
+    f = getattr(o, 'get_values', None)
     if f is not None:
         o = f()
     return o
 
+
 def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True):
     """ if we have an object dtype, try to coerce dates and/or numers """
 
     # if we have passed in a list or scalar
-    if isinstance(values, (list,tuple)):
-        values = np.array(values,dtype=np.object_)
-    if not hasattr(values,'dtype'):
-        values = np.array([values],dtype=np.object_)
+    if isinstance(values, (list, tuple)):
+        values = np.array(values, dtype=np.object_)
+    if not hasattr(values, 'dtype'):
+        values = np.array([values], dtype=np.object_)
 
     # convert dates
     if convert_dates and values.dtype == np.object_:
 
         # we take an aggressive stance and convert to datetime64[ns]
         if convert_dates == 'coerce':
-            new_values = _possibly_cast_to_datetime(values, 'M8[ns]', coerce = True)
+            new_values = _possibly_cast_to_datetime(
+                values, 'M8[ns]', coerce=True)
 
             # if we are all nans then leave me alone
             if not isnull(new_values).all():
                 values = new_values
 
         else:
-            values = lib.maybe_convert_objects(values, convert_datetime=convert_dates)
+            values = lib.maybe_convert_objects(
+                values, convert_datetime=convert_dates)
 
     # convert to numeric
     if convert_numeric and values.dtype == np.object_:
         try:
-            new_values = lib.maybe_convert_numeric(values,set(),coerce_numeric=True)
+            new_values = lib.maybe_convert_numeric(
+                values, set(), coerce_numeric=True)
 
             # if we are all nans then leave me alone
             if not isnull(new_values).all():
@@ -1202,21 +1238,24 @@ def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True):
 
     return values
 
+
 def _possibly_castable(arr):
     return arr.dtype not in _POSSIBLY_CAST_DTYPES
 
+
 def _possibly_convert_platform(values):
     """ try to do platform conversion, allow ndarray or list here """
 
-    if isinstance(values, (list,tuple)):
+    if isinstance(values, (list, tuple)):
         values = lib.list_to_object_array(values)
-    if getattr(values,'dtype',None) == np.object_:
-        if hasattr(values,'values'):
+    if getattr(values, 'dtype', None) == np.object_:
+        if hasattr(values, 'values'):
             values = values.values
         values = lib.maybe_convert_objects(values)
 
     return values
 
+
 def _possibly_cast_to_timedelta(value, coerce=True):
     """ try to cast to timedelta64, if already a timedeltalike, then make
         sure that we are [ns] (as numpy 1.6.2 is very buggy in this regards,
@@ -1261,36 +1300,41 @@ def _possibly_cast_to_timedelta(value, coerce=True):
         return np.array([ convert(v,dtype) for v in value ], dtype='m8[ns]')
 
     # deal with numpy not being able to handle certain timedelta operations
-    if (isinstance(value,np.ndarray) or is_series(value)) and value.dtype.kind == 'm':
+    if (isinstance(value, np.ndarray) or is_series(value)) and value.dtype.kind == 'm':
         if value.dtype != 'timedelta64[ns]':
             value = value.astype('timedelta64[ns]')
         return value
 
-    # we don't have a timedelta, but we want to try to convert to one (but don't force it)
+    # we don't have a timedelta, but we want to try to convert to one (but
+    # don't force it)
     if coerce:
-        new_value = tslib.array_to_timedelta64(_values_from_object(value).astype(object), coerce=False)
+        new_value = tslib.array_to_timedelta64(
+            _values_from_object(value).astype(object), coerce=False)
         if new_value.dtype == 'i8':
-            value = np.array(new_value,dtype='timedelta64[ns]')
+            value = np.array(new_value, dtype='timedelta64[ns]')
 
     return value
 
-def _possibly_cast_to_datetime(value, dtype, coerce = False):
+
+def _possibly_cast_to_datetime(value, dtype, coerce=False):
     """ try to cast the array/value to a datetimelike dtype, converting float nan to iNaT """
 
     if dtype is not None:
         if isinstance(dtype, compat.string_types):
             dtype = np.dtype(dtype)
 
-        is_datetime64  = is_datetime64_dtype(dtype)
+        is_datetime64 = is_datetime64_dtype(dtype)
         is_timedelta64 = is_timedelta64_dtype(dtype)
 
         if is_datetime64 or is_timedelta64:
 
             # force the dtype if needed
             if is_datetime64 and dtype != _NS_DTYPE:
-                  raise TypeError("cannot convert datetimelike to dtype [%s]" % dtype)
+                raise TypeError(
+                    "cannot convert datetimelike to dtype [%s]" % dtype)
             elif is_timedelta64 and dtype != _TD_DTYPE:
-                raise TypeError("cannot convert timedeltalike to dtype [%s]" % dtype)
+                raise TypeError(
+                    "cannot convert timedeltalike to dtype [%s]" % dtype)
 
             if np.isscalar(value):
                 if value == tslib.iNaT or isnull(value):
@@ -1325,15 +1369,15 @@ def _possibly_cast_to_datetime(value, dtype, coerce = False):
             # don't change the value unless we find a datetime set
             v = value
             if not is_list_like(v):
-                v = [ v ]
+                v = [v]
             if len(v):
                 inferred_type = lib.infer_dtype(v)
-                if inferred_type in ['datetime','datetime64']:
+                if inferred_type in ['datetime', 'datetime64']:
                     try:
                         value = tslib.array_to_datetime(np.array(v))
                     except:
                         pass
-                elif inferred_type in ['timedelta','timedelta64']:
+                elif inferred_type in ['timedelta', 'timedelta64']:
                     value = _possibly_cast_to_timedelta(value)
 
     return value
@@ -1361,6 +1405,7 @@ def _is_bool_indexer(key):
 
     return False
 
+
 def _default_index(n):
     from pandas.core.index import Int64Index
     values = np.arange(n, dtype=np.int64)
@@ -1508,6 +1553,7 @@ def banner(message):
     bar = '=' * 80
     return '%s\n%s\n%s' % (bar, message, bar)
 
+
 def _long_prod(vals):
     result = long(1)
     for x in vals:
@@ -1516,12 +1562,14 @@ def _long_prod(vals):
 
 
 class groupby(dict):
+
     """
     A simple groupby different from the one in itertools.
 
     Does not require the sequence elements to be sorted by keys,
     however it is slower.
     """
+
     def __init__(self, seq, key=lambda x: x):
         for value in seq:
             k = key(value)
@@ -1688,9 +1736,11 @@ def is_timedelta64_dtype(arr_or_dtype):
         tipo = arr_or_dtype.dtype.type
     return issubclass(tipo, np.timedelta64)
 
+
 def needs_i8_conversion(arr_or_dtype):
     return is_datetime64_dtype(arr_or_dtype) or is_timedelta64_dtype(arr_or_dtype)
 
+
 def is_float_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
         tipo = arr_or_dtype.type
@@ -1698,6 +1748,7 @@ def is_float_dtype(arr_or_dtype):
         tipo = arr_or_dtype.dtype.type
     return issubclass(tipo, np.floating)
 
+
 def is_complex_dtype(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
         tipo = arr_or_dtype.type
@@ -1722,6 +1773,7 @@ def is_re_compilable(obj):
 def is_list_like(arg):
     return hasattr(arg, '__iter__') and not isinstance(arg, compat.string_types)
 
+
 def _is_sequence(x):
     try:
         iter(x)
@@ -1752,7 +1804,8 @@ def _astype_nansafe(arr, dtype, copy=True):
         elif dtype == np.int64:
             return arr.view(dtype)
         elif dtype != _NS_DTYPE:
-            raise TypeError("cannot astype a datetimelike from [%s] to [%s]" % (arr.dtype,dtype))
+            raise TypeError(
+                "cannot astype a datetimelike from [%s] to [%s]" % (arr.dtype, dtype))
         return arr.astype(_NS_DTYPE)
     elif is_timedelta64_dtype(arr):
         if dtype == np.int64:
@@ -1799,10 +1852,13 @@ def _all_none(*args):
             return False
     return True
 
+
 class UTF8Recoder:
+
     """
     Iterator that reads an encoded stream and reencodes the input to UTF-8
     """
+
     def __init__(self, f, encoding):
         self.reader = codecs.getreader(encoding)(f)
 
@@ -1855,6 +1911,7 @@ if compat.PY3:  # pragma: no cover
         return csv.writer(f, dialect=dialect, **kwds)
 else:
     class UnicodeReader:
+
         """
         A CSV reader which will iterate over lines in the CSV file "f",
         which is encoded in the given encoding.
@@ -1878,6 +1935,7 @@ else:
             return self
 
     class UnicodeWriter:
+
         """
         A CSV writer which will write rows to CSV file "f",
         which is encoded in the given encoding.
@@ -1936,7 +1994,8 @@ def _concat_compat(to_concat, axis=0):
     to_concat = [x for x in to_concat if x.shape[axis] > 0]
 
     # return the empty np array, if nothing to concatenate, #3121
-    if not to_concat: return np.array([], dtype=object)
+    if not to_concat:
+        return np.array([], dtype=object)
 
     is_datetime64 = [x.dtype == _NS_DTYPE for x in to_concat]
     if all(is_datetime64):
@@ -1958,6 +2017,7 @@ def _to_pydatetime(x):
 
     return x
 
+
 def _where_compat(mask, arr1, arr2):
     if arr1.dtype == _NS_DTYPE and arr2.dtype == _NS_DTYPE:
         new_vals = np.where(mask, arr1.view('i8'), arr2.view('i8'))
@@ -1971,12 +2031,14 @@ def _where_compat(mask, arr1, arr2):
 
     return np.where(mask, arr1, arr2)
 
+
 def sentinal_factory():
     class Sentinal(object):
         pass
 
     return Sentinal()
 
+
 def in_interactive_session():
     """ check if we're running in an interactive shell
 
@@ -1999,28 +2061,30 @@ def in_qtconsole():
     """
     try:
         ip = get_ipython()
-        front_end = (ip.config.get('KernelApp',{}).get('parent_appname',"") or
-                         ip.config.get('IPKernelApp',{}).get('parent_appname',""))
+        front_end = (ip.config.get('KernelApp', {}).get('parent_appname', "") or
+                     ip.config.get('IPKernelApp', {}).get('parent_appname', ""))
         if 'qtconsole' in front_end.lower():
             return True
     except:
         return False
     return False
 
+
 def in_ipnb():
     """
     check if we're inside an IPython Notebook
     """
     try:
         ip = get_ipython()
-        front_end = (ip.config.get('KernelApp',{}).get('parent_appname',"") or
-                         ip.config.get('IPKernelApp',{}).get('parent_appname',""))
+        front_end = (ip.config.get('KernelApp', {}).get('parent_appname', "") or
+                     ip.config.get('IPKernelApp', {}).get('parent_appname', ""))
         if 'notebook' in front_end.lower():
             return True
     except:
         return False
     return False
 
+
 def in_ipython_frontend():
     """
     check if we're inside an an IPython zmq frontend
@@ -2078,19 +2142,19 @@ def _pprint_seq(seq, _nest_lvl=0, **kwds):
 
     s = iter(seq)
     r = []
-    for i in range(min(nitems,len(seq))): # handle sets, no slicing
+    for i in range(min(nitems, len(seq))):  # handle sets, no slicing
         r.append(pprint_thing(next(s), _nest_lvl + 1, **kwds))
     body = ", ".join(r)
 
     if nitems < len(seq):
-        body+= ", ..."
-    elif isinstance(seq,tuple) and len(seq) == 1:
+        body += ", ..."
+    elif isinstance(seq, tuple) and len(seq) == 1:
         body += ','
 
     return fmt % body
 
 
-def _pprint_dict(seq, _nest_lvl=0,**kwds):
+def _pprint_dict(seq, _nest_lvl=0, **kwds):
     """
     internal. pprinter for iterables. you should probably use pprint_thing()
     rather then calling this directly.
@@ -2138,10 +2202,10 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
     result - unicode object on py2, str on py3. Always Unicode.
 
     """
-    def as_escaped_unicode(thing,escape_chars=escape_chars):
+    def as_escaped_unicode(thing, escape_chars=escape_chars):
         # Unicode is fine, else we try to decode using utf-8 and 'replace'
         # if that's not it either, we have no way of knowing and the user
-        #should deal with it himself.
+        # should deal with it himself.
 
         try:
             result = compat.text_type(thing)  # we should try this first
@@ -2170,7 +2234,7 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
         return compat.text_type(thing)
     elif (isinstance(thing, dict) and
           _nest_lvl < get_option("display.pprint_nest_depth")):
-        result = _pprint_dict(thing, _nest_lvl,quote_strings=True)
+        result = _pprint_dict(thing, _nest_lvl, quote_strings=True)
     elif _is_sequence(thing) and _nest_lvl < \
             get_option("display.pprint_nest_depth"):
         result = _pprint_seq(thing, _nest_lvl, escape_chars=escape_chars,
@@ -2203,6 +2267,7 @@ def console_encode(object, **kwds):
     return pprint_thing_encoded(object,
                                 get_option("display.encoding"))
 
+
 def load(path):  # TODO remove in 0.13
     """
     Load pickled pandas object (or any other pickled object) from the specified
@@ -2225,6 +2290,7 @@ def load(path):  # TODO remove in 0.13
     from pandas.io.pickle import read_pickle
     return read_pickle(path)
 
+
 def save(obj, path):  # TODO remove in 0.13
     '''
     Pickle (serialize) object to input file path
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 5c943a7eb..aa730ce1e 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -190,11 +190,12 @@ class DataConflictError(Exception):
 # Factory helper methods
 
 
-def _arith_method(op, name, str_rep = None, default_axis='columns', fill_zeros=None, **eval_kwargs):
+def _arith_method(op, name, str_rep=None, default_axis='columns', fill_zeros=None, **eval_kwargs):
     def na_op(x, y):
         try:
-            result = expressions.evaluate(op, str_rep, x, y, raise_on_error=True, **eval_kwargs)
-            result = com._fill_zeros(result,y,fill_zeros)
+            result = expressions.evaluate(
+                op, str_rep, x, y, raise_on_error=True, **eval_kwargs)
+            result = com._fill_zeros(result, y, fill_zeros)
 
         except TypeError:
             xrav = x.ravel()
@@ -207,7 +208,7 @@ def _arith_method(op, name, str_rep = None, default_axis='columns', fill_zeros=N
                 mask = notnull(xrav)
                 result[mask] = op(xrav[mask], y)
 
-            result, changed = com._maybe_upcast_putmask(result,-mask,np.nan)
+            result, changed = com._maybe_upcast_putmask(result, -mask, np.nan)
             result = result.reshape(x.shape)
 
         return result
@@ -246,7 +247,7 @@ def _arith_method(op, name, str_rep = None, default_axis='columns', fill_zeros=N
     return f
 
 
-def _flex_comp_method(op, name, str_rep = None, default_axis='columns'):
+def _flex_comp_method(op, name, str_rep=None, default_axis='columns'):
 
     def na_op(x, y):
         try:
@@ -324,7 +325,7 @@ def _comp_method(func, name, str_rep):
 
             # straight boolean comparisions we want to allow all columns
             # (regardless of dtype to pass thru)
-            return self._combine_const(other, func, raise_on_error = False).fillna(True).astype(bool)
+            return self._combine_const(other, func, raise_on_error=False).fillna(True).astype(bool)
 
     f.__name__ = name
 
@@ -336,6 +337,7 @@ def _comp_method(func, name, str_rep):
 
 
 class DataFrame(NDFrame):
+
     """ Two-dimensional size-mutable, potentially heterogeneous tabular data
     structure with labeled axes (rows and columns). Arithmetic operations
     align on both row and column labels. Can be thought of as a dict-like
@@ -389,7 +391,8 @@ class DataFrame(NDFrame):
             data = data._data
 
         if isinstance(data, BlockManager):
-            mgr = self._init_mgr(data, axes = dict(index=index, columns=columns), dtype=dtype, copy=copy)
+            mgr = self._init_mgr(
+                data, axes=dict(index=index, columns=columns), dtype=dtype, copy=copy)
         elif isinstance(data, dict):
             mgr = self._init_dict(data, index, columns, dtype=dtype)
         elif isinstance(data, ma.MaskedArray):
@@ -401,7 +404,7 @@ class DataFrame(NDFrame):
                 data = data.copy()
             mgr = self._init_ndarray(data, index, columns, dtype=dtype,
                                      copy=copy)
-        elif isinstance(data, (np.ndarray,Series)):
+        elif isinstance(data, (np.ndarray, Series)):
             if data.dtype.names:
                 data_columns, data = _rec_to_dict(data)
                 if columns is None:
@@ -481,7 +484,7 @@ class DataFrame(NDFrame):
                         continue
 
                     if dtype is None:
-                        # #1783
+                        # 1783
                         v = np.empty(len(index), dtype=object)
                     else:
                         v = np.empty(len(index), dtype=dtype)
@@ -513,7 +516,7 @@ class DataFrame(NDFrame):
 
             # zero len case (GH #2234)
             if not len(values) and len(columns):
-                values = np.empty((0,1), dtype=object)
+                values = np.empty((0, 1), dtype=object)
 
         values = _prep_ndarray(values, copy=copy)
 
@@ -536,7 +539,7 @@ class DataFrame(NDFrame):
         else:
             columns = _ensure_index(columns)
 
-        return create_block_manager_from_blocks([ values.T ], [ columns, index ])
+        return create_block_manager_from_blocks([values.T], [columns, index])
 
     @property
     def _verbose_info(self):
@@ -559,7 +562,7 @@ class DataFrame(NDFrame):
         max_rows = get_option("display.max_rows")
         return len(self) <= max_rows
 
-    def _repr_fits_horizontal_(self,ignore_width=False):
+    def _repr_fits_horizontal_(self, ignore_width=False):
         """
         Check if full repr fits in horizontal boundaries imposed by the display
         options width and max_columns. In case off non-interactive session, no
@@ -576,15 +579,15 @@ class DataFrame(NDFrame):
 
         # exceed max columns
         if ((max_columns and nb_columns > max_columns) or
-            ((not ignore_width) and width and nb_columns > (width // 2))):
+                ((not ignore_width) and width and nb_columns > (width // 2))):
             return False
 
         if (ignore_width  # used by repr_html under IPython notebook
-            or not com.in_interactive_session()): # scripts ignore terminal dims
+                or not com.in_interactive_session()):  # scripts ignore terminal dims
             return True
 
         if (get_option('display.width') is not None or
-            com.in_ipython_frontend()):
+                com.in_ipython_frontend()):
             # check at least the column row for excessive width
             max_rows = 1
         else:
@@ -599,9 +602,9 @@ class DataFrame(NDFrame):
         # and to_string on entire frame may be expensive
         d = self
 
-        if not (max_rows is None): # unlimited rows
+        if not (max_rows is None):  # unlimited rows
             # min of two, where one may be None
-            d=d.iloc[:min(max_rows,len(d))]
+            d = d.iloc[:min(max_rows, len(d))]
         else:
             return True
 
@@ -636,7 +639,7 @@ class DataFrame(NDFrame):
             # of terminal, then use expand_repr
             if (fits_vertical and
                 expand_repr and
-                len(self.columns) <= max_columns):
+                    len(self.columns) <= max_columns):
                 self.to_string(buf=buf, line_width=width)
             else:
                 max_info_rows = get_option('display.max_info_rows')
@@ -670,7 +673,8 @@ class DataFrame(NDFrame):
             fits_vertical = self._repr_fits_vertical_()
             fits_horizontal = False
             if fits_vertical:
-                fits_horizontal = self._repr_fits_horizontal_(ignore_width=ipnbh)
+                fits_horizontal = self._repr_fits_horizontal_(
+                    ignore_width=ipnbh)
 
             if fits_horizontal and fits_vertical:
                 return ('<div style="max-height:1000px;'
@@ -723,7 +727,7 @@ class DataFrame(NDFrame):
         """
         columns = self.columns
         for k, v in zip(self.index, self.values):
-            s = Series(v,index=columns,name=k)
+            s = Series(v, index=columns, name=k)
             yield k, s
 
     def itertuples(self, index=True):
@@ -777,7 +781,8 @@ class DataFrame(NDFrame):
 
     # currently causes a floating point exception to occur - so sticking with unaccelerated for now
     # __mod__ = _arith_method(operator.mod, '__mod__', '%', default_axis=None, fill_zeros=np.nan)
-    __mod__ = _arith_method(operator.mod, '__mod__', default_axis=None, fill_zeros=np.nan)
+    __mod__ = _arith_method(
+        operator.mod, '__mod__', default_axis=None, fill_zeros=np.nan)
 
     __radd__ = _arith_method(_radd_compat, '__radd__', default_axis=None)
     __rmul__ = _arith_method(operator.mul, '__rmul__', default_axis=None)
@@ -806,8 +811,8 @@ class DataFrame(NDFrame):
     # Comparison methods
     __eq__ = _comp_method(operator.eq, '__eq__', '==')
     __ne__ = _comp_method(operator.ne, '__ne__', '!=')
-    __lt__ = _comp_method(operator.lt, '__lt__', '<' )
-    __gt__ = _comp_method(operator.gt, '__gt__', '>' )
+    __lt__ = _comp_method(operator.lt, '__lt__', '<')
+    __gt__ = _comp_method(operator.gt, '__gt__', '>')
     __le__ = _comp_method(operator.le, '__le__', '<=')
     __ge__ = _comp_method(operator.ge, '__ge__', '>=')
 
@@ -1004,9 +1009,11 @@ class DataFrame(NDFrame):
 
                 # reorder according to the columns
                 if len(columns) and len(arr_columns):
-                    indexer     = _ensure_index(arr_columns).get_indexer(columns)
-                    arr_columns = _ensure_index([ arr_columns[i] for i in indexer ])
-                    arrays      = [ arrays[i] for i in indexer ]
+                    indexer = _ensure_index(
+                        arr_columns).get_indexer(columns)
+                    arr_columns = _ensure_index(
+                        [arr_columns[i] for i in indexer])
+                    arrays = [arrays[i] for i in indexer]
 
         elif isinstance(data, (np.ndarray, DataFrame)):
             arrays, columns = _to_arrays(data, columns)
@@ -1089,7 +1096,7 @@ class DataFrame(NDFrame):
                 else:
                     ix_vals = [self.index.values]
 
-            arrays = ix_vals+ [self[c].values for c in self.columns]
+            arrays = ix_vals + [self[c].values for c in self.columns]
 
             count = 0
             index_names = list(self.index.names)
@@ -1202,7 +1209,7 @@ class DataFrame(NDFrame):
         from pandas.io.parsers import read_table
         return read_table(path, header=header, sep=sep,
                           parse_dates=parse_dates, index_col=index_col,
-                          encoding=encoding,tupleize_cols=False)
+                          encoding=encoding, tupleize_cols=False)
 
     def to_sparse(self, fill_value=None, kind='block'):
         """
@@ -1263,7 +1270,7 @@ class DataFrame(NDFrame):
         new_blocks = []
         for block in selfsorted._data.blocks:
             newb = block2d_to_blocknd(block.values.T, block.items, shape,
-                                      [ major_labels, minor_labels ],
+                                      [major_labels, minor_labels],
                                       ref_items=selfsorted.columns)
             new_blocks.append(newb)
 
@@ -1337,11 +1344,12 @@ class DataFrame(NDFrame):
         formatter = fmt.CSVFormatter(self, path_or_buf,
                                      line_terminator=line_terminator,
                                      sep=sep, encoding=encoding,
-                                     quoting=quoting,na_rep=na_rep,
+                                     quoting=quoting, na_rep=na_rep,
                                      float_format=float_format, cols=cols,
                                      header=header, index=index,
-                                     index_label=index_label,mode=mode,
-                                     chunksize=chunksize,engine=kwds.get("engine"),
+                                     index_label=index_label, mode=mode,
+                                     chunksize=chunksize, engine=kwds.get(
+                                         "engine"),
                                      tupleize_cols=tupleize_cols)
         formatter.save()
 
@@ -1405,8 +1413,9 @@ class DataFrame(NDFrame):
         if need_save:
             excel_writer.save()
 
-    def to_stata(self, fname, convert_dates=None, write_index=True, encoding="latin-1",
-                 byteorder=None):
+    def to_stata(
+        self, fname, convert_dates=None, write_index=True, encoding="latin-1",
+            byteorder=None):
         """
         A class for writing Stata binary dta files from array-like objects
 
@@ -1436,7 +1445,8 @@ class DataFrame(NDFrame):
         >>> writer.write_file()
         """
         from pandas.io.stata import StataWriter
-        writer = StataWriter(fname,self,convert_dates=convert_dates, encoding=encoding, byteorder=byteorder)
+        writer = StataWriter(
+            fname, self, convert_dates=convert_dates, encoding=encoding, byteorder=byteorder)
         writer.write_file()
 
     def to_sql(self, name, con, flavor='sqlite', if_exists='fail', **kwargs):
@@ -1454,7 +1464,8 @@ class DataFrame(NDFrame):
             append: If table exists, insert data. Create if does not exist.
         """
         from pandas.io.sql import write_frame
-        write_frame(self, name, con, flavor=flavor, if_exists=if_exists, **kwargs)
+        write_frame(
+            self, name, con, flavor=flavor, if_exists=if_exists, **kwargs)
 
     @Appender(fmt.docstring_to_string, indents=1)
     def to_string(self, buf=None, columns=None, col_space=None, colSpace=None,
@@ -1604,10 +1615,12 @@ class DataFrame(NDFrame):
 
         # hack
         if max_cols is None:
-            max_cols = get_option('display.max_info_columns',len(self.columns)+1)
+            max_cols = get_option(
+                'display.max_info_columns', len(self.columns) + 1)
 
         if verbose and len(self.columns) <= max_cols:
-            lines.append('Data columns (total %d columns):' % len(self.columns))
+            lines.append('Data columns (total %d columns):' %
+                         len(self.columns))
             space = max([len(com.pprint_thing(k)) for k in self.columns]) + 4
             counts = self.count()
             if len(cols) != len(counts):
@@ -1633,7 +1646,7 @@ class DataFrame(NDFrame):
         return self.apply(lambda x: x.ftype, reduce=False)
 
     def transpose(self):
-        return super(DataFrame, self).transpose(1,0)
+        return super(DataFrame, self).transpose(1, 0)
 
     T = property(transpose)
 
@@ -1729,10 +1742,10 @@ class DataFrame(NDFrame):
             return result.set_value(index, col, value)
 
     def irow(self, i, copy=False):
-        return self._ixs(i,axis=0)
+        return self._ixs(i, axis=0)
 
     def icol(self, i):
-        return self._ixs(i,axis=1)
+        return self._ixs(i, axis=1)
 
     def _ixs(self, i, axis=0, copy=False):
         """
@@ -1786,11 +1799,12 @@ class DataFrame(NDFrame):
                     return self.take(i, axis=1, convert=True)
 
                 values = self._data.iget(i)
-                return self._constructor_sliced.from_array(values, index=self.index,
-                                                           name=label, fastpath=True)
+                return self._constructor_sliced.from_array(
+                    values, index=self.index,
+                    name=label, fastpath=True)
 
     def iget_value(self, i, j):
-        return self.iat[i,j]
+        return self.iat[i, j]
 
     def __getitem__(self, key):
 
@@ -1870,7 +1884,8 @@ class DataFrame(NDFrame):
 
     def _slice(self, slobj, axis=0, raise_on_error=False):
         axis = self._get_block_manager_axis(axis)
-        new_data = self._data.get_slice(slobj, axis=axis, raise_on_error=raise_on_error)
+        new_data = self._data.get_slice(
+            slobj, axis=axis, raise_on_error=raise_on_error)
         return self._constructor(new_data)
 
     def _box_item_values(self, key, values):
@@ -1928,7 +1943,8 @@ class DataFrame(NDFrame):
 
         if self._is_mixed_type:
             if not self._is_numeric_mixed_type:
-                raise ValueError('Cannot do boolean setting on mixed-type frame')
+                raise ValueError(
+                    'Cannot do boolean setting on mixed-type frame')
 
         self.where(-key, value, inplace=True)
 
@@ -1958,7 +1974,8 @@ class DataFrame(NDFrame):
         value : int, Series, or array-like
         """
         value = self._sanitize_column(column, value)
-        self._data.insert(loc, column, value, allow_duplicates=allow_duplicates)
+        self._data.insert(
+            loc, column, value, allow_duplicates=allow_duplicates)
 
     def _sanitize_column(self, key, value):
         # Need to make sure new columns (which go into the BlockManager as new
@@ -2021,7 +2038,8 @@ class DataFrame(NDFrame):
             else:
                 # upcast the scalar
                 dtype, value = _infer_dtype_from_scalar(value)
-                value = np.array(np.repeat(value, len(self.index)), dtype=dtype)
+                value = np.array(
+                    np.repeat(value, len(self.index)), dtype=dtype)
 
             value = com._possibly_cast_to_datetime(value, dtype)
         return np.atleast_2d(np.asarray(value))
@@ -2218,26 +2236,26 @@ class DataFrame(NDFrame):
     # Reindexing and alignment
 
     def _reindex_axes(self, axes, level, limit, method, fill_value, copy, takeable=False):
-      frame = self
+        frame = self
 
-      columns = axes['columns']
-      if columns is not None:
-          frame = frame._reindex_columns(columns, copy, level,
-                                         fill_value, limit, takeable=takeable)
+        columns = axes['columns']
+        if columns is not None:
+            frame = frame._reindex_columns(columns, copy, level,
+                                           fill_value, limit, takeable=takeable)
 
-      index = axes['index']
-      if index is not None:
-          frame = frame._reindex_index(index, method, copy, level,
-                                       fill_value, limit, takeable=takeable)
+        index = axes['index']
+        if index is not None:
+            frame = frame._reindex_index(index, method, copy, level,
+                                         fill_value, limit, takeable=takeable)
 
-      return frame
+        return frame
 
     def _reindex_index(self, new_index, method, copy, level, fill_value=NA,
                        limit=None, takeable=False):
         new_index, indexer = self.index.reindex(new_index, method, level,
                                                 limit=limit, copy_if_needed=True,
                                                 takeable=takeable)
-        return self._reindex_with_indexers({ 0 : [ new_index, indexer ] },
+        return self._reindex_with_indexers({0: [new_index, indexer]},
                                            copy=copy, fill_value=fill_value)
 
     def _reindex_columns(self, new_columns, copy, level, fill_value=NA,
@@ -2245,7 +2263,7 @@ class DataFrame(NDFrame):
         new_columns, indexer = self.columns.reindex(new_columns, level=level,
                                                     limit=limit, copy_if_needed=True,
                                                     takeable=takeable)
-        return self._reindex_with_indexers({ 1 : [ new_columns, indexer ] },
+        return self._reindex_with_indexers({1: [new_columns, indexer]},
                                            copy=copy, fill_value=fill_value)
 
     def _reindex_multi(self, axes, copy, fill_value):
@@ -2261,9 +2279,9 @@ class DataFrame(NDFrame):
             return self._constructor(new_values, index=new_index,
                                      columns=new_columns)
         elif row_indexer is not None:
-            return self._reindex_with_indexers({ 0 : [ new_index,   row_indexer ] }, copy=copy, fill_value=fill_value)
+            return self._reindex_with_indexers({0: [new_index,   row_indexer]}, copy=copy, fill_value=fill_value)
         elif col_indexer is not None:
-            return self._reindex_with_indexers({ 1 : [ new_columns, col_indexer ] }, copy=copy, fill_value=fill_value)
+            return self._reindex_with_indexers({1: [new_columns, col_indexer]}, copy=copy, fill_value=fill_value)
         else:
             return self.copy() if copy else self
 
@@ -2421,7 +2439,8 @@ class DataFrame(NDFrame):
                 mask = labels == -1
                 values = values.take(labels)
                 if mask.any():
-                    values, changed = com._maybe_upcast_putmask(values,mask,np.nan)
+                    values, changed = com._maybe_upcast_putmask(
+                        values, mask, np.nan)
 
             return values
 
@@ -2506,7 +2525,8 @@ class DataFrame(NDFrame):
         # check/convert indicies here
         if convert:
             axis = self._get_axis_number(axis)
-            indices = _maybe_convert_indices(indices, len(self._get_axis(axis)))
+            indices = _maybe_convert_indices(
+                indices, len(self._get_axis(axis)))
 
         if self._is_mixed_type:
             if axis == 0:
@@ -3012,10 +3032,11 @@ class DataFrame(NDFrame):
         if fill_value is not None:
             raise NotImplementedError
 
-        new_data = left._data.eval(func, right, axes = [left.columns, self.index])
+        new_data = left._data.eval(
+            func, right, axes=[left.columns, self.index])
         return self._constructor(new_data)
 
-    def _combine_const(self, other, func, raise_on_error = True):
+    def _combine_const(self, other, func, raise_on_error=True):
         if self.empty:
             return self
 
@@ -3028,7 +3049,7 @@ class DataFrame(NDFrame):
                             'DataFrame objects')
 
         def _compare(a, b):
-            return dict([ (col,func(a[col], b[col])) for col in a.columns ])
+            return dict([(col, func(a[col], b[col])) for col in a.columns])
         new_data = expressions.evaluate(_compare, str_rep, self, other)
 
         return self._constructor(data=new_data, index=self.index,
@@ -3039,7 +3060,7 @@ class DataFrame(NDFrame):
             self, other = self.align(other, 'outer', level=level)
 
         def _compare(a, b):
-            return dict([ (col,func(a[col], b[col])) for col in a.columns ])
+            return dict([(col, func(a[col], b[col])) for col in a.columns])
         new_data = expressions.evaluate(_compare, str_rep, self, other)
 
         return self._constructor(data=new_data, index=self.index,
@@ -3105,7 +3126,7 @@ class DataFrame(NDFrame):
             # if we have different dtypes, possibily promote
             new_dtype = this_dtype
             if this_dtype != other_dtype:
-                new_dtype = com._lcd_dtypes(this_dtype,other_dtype)
+                new_dtype = com._lcd_dtypes(this_dtype, other_dtype)
                 series = series.astype(new_dtype)
                 otherSeries = otherSeries.astype(new_dtype)
 
@@ -3157,8 +3178,8 @@ class DataFrame(NDFrame):
         combined : DataFrame
         """
         def combiner(x, y, needs_i8_conversion=False):
-            x_values = x.values if hasattr(x,'values') else x
-            y_values = y.values if hasattr(y,'values') else y
+            x_values = x.values if hasattr(x, 'values') else x
+            y_values = y.values if hasattr(y, 'values') else y
             if needs_i8_conversion:
                 mask = isnull(x)
                 x_values = x_values.view('i8')
@@ -3218,7 +3239,8 @@ class DataFrame(NDFrame):
                 else:
                     mask = notnull(this)
 
-            self[col] = expressions.where(mask, this, that, raise_on_error=True)
+            self[col] = expressions.where(
+                mask, this, that, raise_on_error=True)
 
     #----------------------------------------------------------------------
     # Misc methods
@@ -3613,7 +3635,6 @@ class DataFrame(NDFrame):
                     pass
                 raise e
 
-
         if len(results) > 0 and _is_sequence(results[0]):
             if not isinstance(results[0], Series):
                 index = res_columns
@@ -3909,7 +3930,7 @@ class DataFrame(NDFrame):
                 baseCov.fill(np.nan)
             else:
                 baseCov = np.cov(mat.T)
-            baseCov = baseCov.reshape((len(cols),len(cols)))
+            baseCov = baseCov.reshape((len(cols), len(cols)))
         else:
             baseCov = _algos.nancorr(com._ensure_float64(mat), cov=True,
                                      minp=min_periods)
@@ -4466,7 +4487,7 @@ class DataFrame(NDFrame):
 
         # GH 2747 (arguments were reversed)
         if lower is not None and upper is not None:
-            lower, upper = min(lower,upper), max(lower,upper)
+            lower, upper = min(lower, upper), max(lower, upper)
 
         return self.apply(lambda x: x.clip(lower=lower, upper=upper))
 
@@ -4676,7 +4697,8 @@ class DataFrame(NDFrame):
         """
         return self.mul(other, fill_value=1.)
 
-DataFrame._setup_axes(['index', 'columns'], info_axis=1, stat_axis=0, axes_are_reversed=True)
+DataFrame._setup_axes(
+    ['index', 'columns'], info_axis=1, stat_axis=0, axes_are_reversed=True)
 
 _EMPTY_SERIES = Series([])
 
@@ -4767,6 +4789,7 @@ def _arrays_to_mgr(arrays, arr_names, index, columns, dtype=None):
 
     return create_block_manager_from_arrays(arrays, arr_names, axes)
 
+
 def extract_index(data):
     from pandas.core.index import _union_indexes
 
@@ -4830,8 +4853,8 @@ def _prep_ndarray(values, copy=True):
         # we could have a 1-dim or 2-dim list here
         # this is equiv of np.asarray, but does object conversion
         # and platform dtype preservation
-        if com.is_list_like(values[0]) or hasattr(values[0],'len'):
-            values = np.array([ convert(v) for v in values])
+        if com.is_list_like(values[0]) or hasattr(values[0], 'len'):
+            values = np.array([convert(v) for v in values])
         else:
             values = convert(values)
 
@@ -5027,6 +5050,7 @@ def _homogenize(data, index, dtype=None):
 
     return homogenized
 
+
 def _from_nested_dict(data):
     # TODO: this should be seriously cythonized
     new_data = OrderedDict()
@@ -5082,11 +5106,11 @@ def boxplot(self, column=None, by=None, ax=None, fontsize=None,
         Can be any valid input to groupby
     by : string or sequence
         Column in the DataFrame to group by
-	ax : matplotlib axis object, default None
+        ax : matplotlib axis object, default None
     fontsize : int or string
-	rot : int, default None
+        rot : int, default None
         Rotation for ticks
-	grid : boolean, default None (matlab style default)
+        grid : boolean, default None (matlab style default)
         Axis grid lines
 
     Returns
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 3397e2fdd..fc795912a 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -21,7 +21,9 @@ from pandas.core.common import (isnull, notnull, is_list_like,
                                 _infer_dtype_from_scalar, _maybe_promote)
 from pandas.core.base import PandasObject
 
+
 class NDFrame(PandasObject):
+
     """
     N-dimensional analogue of DataFrame. Store multi-dimensional in a
     size-mutable, labeled data structure
@@ -32,9 +34,10 @@ class NDFrame(PandasObject):
     axes : list
     copy : boolean, default False
     """
-    _internal_names     = ['_data','name','_subtyp','_index','_default_kind','_default_fill_value']
+    _internal_names = [
+        '_data', 'name', '_subtyp', '_index', '_default_kind', '_default_fill_value']
     _internal_names_set = set(_internal_names)
-    _prop_attributes    = []
+    _prop_attributes = []
 
     def __init__(self, data, axes=None, copy=False, dtype=None, fastpath=False):
 
@@ -55,7 +58,8 @@ class NDFrame(PandasObject):
         """ passed a manager and a axes dict """
         for a, axe in axes.items():
             if axe is not None:
-                mgr = mgr.reindex_axis(axe, axis=self._get_block_manager_axis(a), copy=False)
+                mgr = mgr.reindex_axis(
+                    axe, axis=self._get_block_manager_axis(a), copy=False)
 
         # do not copy BlockManager unless explicitly done
         if copy and dtype is None:
@@ -75,7 +79,7 @@ class NDFrame(PandasObject):
 
     def __hash__(self):
         raise TypeError('{0!r} objects are mutable, thus they cannot be'
-                              ' hashed'.format(self.__class__.__name__))
+                        ' hashed'.format(self.__class__.__name__))
 
     def __unicode__(self):
         # unicode representation based upon iterating over self
@@ -91,8 +95,9 @@ class NDFrame(PandasObject):
     # Axis
 
     @classmethod
-    def _setup_axes(cls, axes, info_axis = None, stat_axis = None, aliases = None, slicers = None,
-                    axes_are_reversed = False, build_axes = True, ns = None):
+    def _setup_axes(
+        cls, axes, info_axis=None, stat_axis=None, aliases=None, slicers=None,
+            axes_are_reversed=False, build_axes=True, ns=None):
         """ provide axes setup for the major PandasObjects
 
             axes : the names of the axes in order (lowest to highest)
@@ -104,47 +109,48 @@ class NDFrame(PandasObject):
             build_axes : setup the axis properties (default True)
             """
 
-        cls._AXIS_ORDERS  = axes
-        cls._AXIS_NUMBERS = dict([(a, i) for i, a in enumerate(axes) ])
-        cls._AXIS_LEN     = len(axes)
+        cls._AXIS_ORDERS = axes
+        cls._AXIS_NUMBERS = dict([(a, i) for i, a in enumerate(axes)])
+        cls._AXIS_LEN = len(axes)
         cls._AXIS_ALIASES = aliases or dict()
-        cls._AXIS_IALIASES = dict([ (v,k) for k, v in cls._AXIS_ALIASES.items() ])
-        cls._AXIS_NAMES    = dict([(i, a) for i, a in enumerate(axes) ])
+        cls._AXIS_IALIASES = dict([(v, k)
+                                  for k, v in cls._AXIS_ALIASES.items()])
+        cls._AXIS_NAMES = dict([(i, a) for i, a in enumerate(axes)])
         cls._AXIS_SLICEMAP = slicers or None
         cls._AXIS_REVERSED = axes_are_reversed
 
         # typ
-        setattr(cls,'_typ',cls.__name__.lower())
+        setattr(cls, '_typ', cls.__name__.lower())
 
         # indexing support
         cls._ix = None
 
         if info_axis is not None:
             cls._info_axis_number = info_axis
-            cls._info_axis_name   = axes[info_axis]
+            cls._info_axis_name = axes[info_axis]
 
         if stat_axis is not None:
             cls._stat_axis_number = stat_axis
-            cls._stat_axis_name   = axes[stat_axis]
+            cls._stat_axis_name = axes[stat_axis]
 
         # setup the actual axis
         if build_axes:
 
             def set_axis(a, i):
-                setattr(cls,a,lib.AxisProperty(i))
+                setattr(cls, a, lib.AxisProperty(i))
 
             if axes_are_reversed:
-                m = cls._AXIS_LEN-1
+                m = cls._AXIS_LEN - 1
                 for i, a in cls._AXIS_NAMES.items():
-                    set_axis(a,m-i)
+                    set_axis(a, m - i)
             else:
                 for i, a in cls._AXIS_NAMES.items():
-                    set_axis(a,i)
+                    set_axis(a, i)
 
         # addtl parms
         if isinstance(ns, dict):
             for k, v in ns.items():
-                setattr(cls,k,v)
+                setattr(cls, k, v)
 
     def _construct_axes_dict(self, axes=None, **kwargs):
         """ return an axes dictionary for myself """
@@ -180,7 +186,8 @@ class NDFrame(PandasObject):
             if alias is not None:
                 if a in kwargs:
                     if alias in kwargs:
-                        raise Exception("arguments are multually exclusive for [%s,%s]" % (a,alias))
+                        raise Exception(
+                            "arguments are multually exclusive for [%s,%s]" % (a, alias))
                     continue
                 if alias in kwargs:
                     kwargs[a] = kwargs.pop(alias)
@@ -195,7 +202,7 @@ class NDFrame(PandasObject):
                         raise AssertionError(
                             "not enough arguments specified!")
 
-        axes = dict([ (a,kwargs.get(a)) for a in self._AXIS_ORDERS])
+        axes = dict([(a, kwargs.get(a)) for a in self._AXIS_ORDERS])
         return axes, kwargs
 
     @classmethod
@@ -241,8 +248,8 @@ class NDFrame(PandasObject):
         """ map the axis to the block_manager axis """
         axis = self._get_axis_number(axis)
         if self._AXIS_REVERSED:
-            m = self._AXIS_LEN-1
-            return m-axis
+            m = self._AXIS_LEN - 1
+            return m - axis
         return axis
 
     @property
@@ -322,9 +329,12 @@ class NDFrame(PandasObject):
         """
 
         # construct the args
-        axes, kwargs = self._construct_axes_from_arguments(args, kwargs, require_all=True)
-        axes_names   = tuple([ self._get_axis_name(  axes[a]) for a in self._AXIS_ORDERS ])
-        axes_numbers = tuple([ self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS ])
+        axes, kwargs = self._construct_axes_from_arguments(
+            args, kwargs, require_all=True)
+        axes_names = tuple([self._get_axis_name(axes[a])
+                            for a in self._AXIS_ORDERS])
+        axes_numbers = tuple([self._get_axis_number(axes[a])
+                             for a in self._AXIS_ORDERS])
 
         # we must have unique axes
         if len(axes) != len(set(axes)):
@@ -374,7 +384,7 @@ class NDFrame(PandasObject):
     def squeeze(self):
         """ squeeze length 1 dimensions """
         try:
-            return self.ix[tuple([ slice(None) if len(a) > 1 else a[0] for a in self.axes ])]
+            return self.ix[tuple([slice(None) if len(a) > 1 else a[0] for a in self.axes])]
         except:
             return self
 
@@ -432,7 +442,7 @@ class NDFrame(PandasObject):
     # Comparisons
 
     def _indexed_same(self, other):
-        return all([ self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS])
+        return all([self._get_axis(a).equals(other._get_axis(a)) for a in self._AXIS_ORDERS])
 
     def reindex(self, *args, **kwds):
         raise NotImplementedError
@@ -528,11 +538,11 @@ class NDFrame(PandasObject):
                 for k in self._internal_names:
                     if k in state:
                         v = state[k]
-                        object.__setattr__(self,k,v)
+                        object.__setattr__(self, k, v)
 
                 for k, v in state.items():
                     if k not in self._internal_names:
-                        object.__setattr__(self,k,v)
+                        object.__setattr__(self, k, v)
 
             else:
                 self._unpickle_series_compat(state)
@@ -643,8 +653,9 @@ class NDFrame(PandasObject):
         """
 
         from pandas.io import json
-        return json.to_json(path_or_buf=path_or_buf, obj=self, orient=orient, date_format=date_format,
-                            double_precision=double_precision, force_ascii=force_ascii)
+        return json.to_json(
+            path_or_buf=path_or_buf, obj=self, orient=orient, date_format=date_format,
+            double_precision=double_precision, force_ascii=force_ascii)
 
     #----------------------------------------------------------------------
     # Fancy Indexing
@@ -653,14 +664,14 @@ class NDFrame(PandasObject):
     def _create_indexer(cls, name, indexer):
         """ create an indexer like _name in the class """
         iname = '_%s' % name
-        setattr(cls,iname,None)
+        setattr(cls, iname, None)
 
         def _indexer(self):
-            if getattr(self,iname,None) is None:
-                setattr(self,iname,indexer(self, name))
-            return getattr(self,iname)
+            if getattr(self, iname, None) is None:
+                setattr(self, iname, indexer(self, name))
+            return getattr(self, iname)
 
-        setattr(cls,name,property(_indexer))
+        setattr(cls, name, property(_indexer))
 
     def get(self, key, default=None):
         """
@@ -754,7 +765,8 @@ class NDFrame(PandasObject):
         # check/convert indicies here
         if convert:
             axis = self._get_axis_number(axis)
-            indices = _maybe_convert_indices(indices, len(self._get_axis(axis)))
+            indices = _maybe_convert_indices(
+                indices, len(self._get_axis(axis)))
 
         if axis == 0:
             labels = self._get_axis(axis)
@@ -778,12 +790,13 @@ class NDFrame(PandasObject):
         -------
         selection : type of caller
         """
-        axis        = self._get_axis_number(axis)
-        axis_name   = self._get_axis_name(axis)
+        axis = self._get_axis_number(axis)
+        axis_name = self._get_axis_name(axis)
         axis_values = self._get_axis(axis)
 
         if len(axis_values) > 0:
-            new_axis = axis_values[np.asarray([bool(crit(label)) for label in axis_values])]
+            new_axis = axis_values[
+                np.asarray([bool(crit(label)) for label in axis_values])]
         else:
             new_axis = axis_values
 
@@ -953,12 +966,12 @@ class NDFrame(PandasObject):
 
         # construct the args
         axes, kwargs = self._construct_axes_from_arguments(args, kwargs)
-        method     = kwargs.get('method')
-        level      = kwargs.get('level')
-        copy       = kwargs.get('copy',True)
-        limit      = kwargs.get('limit')
-        fill_value = kwargs.get('fill_value',np.nan)
-        takeable   = kwargs.get('takeable',False)
+        method = kwargs.get('method')
+        level = kwargs.get('level')
+        copy = kwargs.get('copy', True)
+        limit = kwargs.get('limit')
+        fill_value = kwargs.get('fill_value', np.nan)
+        takeable = kwargs.get('takeable', False)
 
         self._consolidate_inplace()
 
@@ -980,15 +993,18 @@ class NDFrame(PandasObject):
         obj = self
         for a in self._AXIS_ORDERS:
             labels = axes[a]
-            if labels is None: continue
+            if labels is None:
+                continue
 
             # convert to an index if we are not a multi-selection
             if level is None:
                 labels = _ensure_index(labels)
 
-            axis   = self._get_axis_number(a)
-            new_index, indexer = self._get_axis(a).reindex(labels, level=level, limit=limit, takeable=takeable)
-            obj    = obj._reindex_with_indexers({ axis : [ labels, indexer ] }, method, fill_value, copy)
+            axis = self._get_axis_number(a)
+            new_index, indexer = self._get_axis(a).reindex(
+                labels, level=level, limit=limit, takeable=takeable)
+            obj = obj._reindex_with_indexers(
+                {axis: [labels, indexer]}, method, fill_value, copy)
 
         return obj
 
@@ -1038,11 +1054,11 @@ class NDFrame(PandasObject):
         """
         self._consolidate_inplace()
 
-        axis_name   = self._get_axis_name(axis)
+        axis_name = self._get_axis_name(axis)
         axis_values = self._get_axis(axis_name)
         new_index, indexer = axis_values.reindex(labels, method, level,
                                                  limit=limit, copy_if_needed=True)
-        return self._reindex_with_indexers({ axis : [ new_index, indexer ] }, method, fill_value, copy)
+        return self._reindex_with_indexers({axis: [new_index, indexer]}, method, fill_value, copy)
 
     def _reindex_with_indexers(self, reindexers, method=None, fill_value=np.nan, copy=False):
 
@@ -1054,8 +1070,9 @@ class NDFrame(PandasObject):
 
             # reindex the axis
             if method is not None:
-                new_data = new_data.reindex_axis(index, method=method, axis=baxis,
-                                                 fill_value=fill_value, copy=copy)
+                new_data = new_data.reindex_axis(
+                    index, method=method, axis=baxis,
+                    fill_value=fill_value, copy=copy)
 
             elif indexer is not None:
                 # TODO: speed up on homogeneous DataFrame objects
@@ -1069,7 +1086,7 @@ class NDFrame(PandasObject):
 
             elif baxis > 0 and index is not None and index is not new_data.axes[baxis]:
                 new_data = new_data.copy(deep=copy)
-                new_data.set_axis(baxis,index)
+                new_data.set_axis(baxis, index)
 
         if copy and new_data is self._data:
             new_data = new_data.copy()
@@ -1107,11 +1124,11 @@ class NDFrame(PandasObject):
 
         if axis is None:
             axis = self._info_axis_name
-        axis_name   = self._get_axis_name(axis)
+        axis_name = self._get_axis_name(axis)
         axis_values = self._get_axis(axis_name)
 
         if items is not None:
-            return self.reindex(**{ axis_name : [r for r in items if r in axis_values ] })
+            return self.reindex(**{axis_name: [r for r in items if r in axis_values]})
         elif like:
             matchf = lambda x: (like in x if isinstance(x, basestring)
                                 else like in str(x))
@@ -1128,7 +1145,7 @@ class NDFrame(PandasObject):
     def _propogate_attributes(self, other):
         """ propogate attributes from other to self"""
         for name in self._prop_attributes:
-            object.__setattr__(self,name,getattr(other,name,None))
+            object.__setattr__(self, name, getattr(other, name, None))
         return self
 
     def __getattr__(self, name):
@@ -1280,14 +1297,15 @@ class NDFrame(PandasObject):
         bd = dict()
         for b in self._data.blocks:
             b = b.reindex_items_from(columns or b.items)
-            bd[str(b.dtype)] = self._constructor(BlockManager([ b ], [ b.items, self.index ]))
+            bd[str(b.dtype)] = self._constructor(
+                BlockManager([b], [b.items, self.index]))
         return bd
 
     @property
     def blocks(self):
         return self.as_blocks()
 
-    def astype(self, dtype, copy = True, raise_on_error = True):
+    def astype(self, dtype, copy=True, raise_on_error=True):
         """
         Cast object to input numpy.dtype
         Return a copy when copy = True (be really careful with this!)
@@ -1302,7 +1320,8 @@ class NDFrame(PandasObject):
         casted : type of caller
         """
 
-        mgr = self._data.astype(dtype, copy = copy, raise_on_error = raise_on_error)
+        mgr = self._data.astype(
+            dtype, copy=copy, raise_on_error=raise_on_error)
         return self._constructor(mgr)._propogate_attributes(self)
 
     def copy(self, deep=True):
@@ -1384,8 +1403,9 @@ class NDFrame(PandasObject):
         self._consolidate_inplace()
 
         axis = self._get_axis_number(axis)
-        if axis+1 > self._AXIS_LEN:
-            raise ValueError("invalid axis passed for object type {0}".format(type(self)))
+        if axis + 1 > self._AXIS_LEN:
+            raise ValueError(
+                "invalid axis passed for object type {0}".format(type(self)))
 
         if value is None:
             if method is None:
@@ -1396,11 +1416,11 @@ class NDFrame(PandasObject):
                 return self.T.fillna(method=method, limit=limit).T
 
             method = com._clean_fill_method(method)
-            new_data = self._data.interpolate(method  = method,
-                                              axis    = axis,
-                                              limit   = limit,
-                                              inplace = inplace,
-                                              coerce  = True)
+            new_data = self._data.interpolate(method=method,
+                                              axis=axis,
+                                              limit=limit,
+                                              inplace=inplace,
+                                              coerce=True)
         else:
             if method is not None:
                 raise ValueError('cannot specify both a fill method and value')
@@ -1563,7 +1583,7 @@ class NDFrame(PandasObject):
             items = to_replace.items()
             keys, values = itertools.izip(*items)
 
-            are_mappings = [ is_dictlike(v) for v in values]
+            are_mappings = [is_dictlike(v) for v in values]
 
             if any(are_mappings):
                 if not all(are_mappings):
@@ -1599,7 +1619,8 @@ class NDFrame(PandasObject):
                                                         inplace=inplace,
                                                         regex=regex)
 
-                elif not isinstance(value, (list, np.ndarray)):  # {'A': NA} -> 0
+                # {'A': NA} -> 0
+                elif not isinstance(value, (list, np.ndarray)):
                     new_data = self._data
                     for k, src in to_replace.iteritems():
                         if k in self:
@@ -1981,7 +2002,7 @@ class NDFrame(PandasObject):
         (left, right) : (type of input, type of other)
             Aligned objects
         """
-        from pandas import DataFrame,Series
+        from pandas import DataFrame, Series
 
         if isinstance(other, DataFrame):
             return self._align_frame(other, join=join, axis=axis, level=level,
@@ -2016,14 +2037,13 @@ class NDFrame(PandasObject):
                     self.columns.join(other.columns, how=join, level=level,
                                       return_indexers=True)
 
-        left  = self._reindex_with_indexers({ 0 : [ join_index,   ilidx ],
-                                              1 : [ join_columns, clidx ] },
-                                            copy=copy, fill_value=fill_value)
-        right = other._reindex_with_indexers({ 0 : [ join_index,   iridx ],
-                                               1 : [ join_columns, cridx ] },
+        left = self._reindex_with_indexers({0: [join_index,   ilidx],
+                                            1: [join_columns, clidx]},
+                                           copy=copy, fill_value=fill_value)
+        right = other._reindex_with_indexers({0: [join_index,   iridx],
+                                              1: [join_columns, cridx]},
                                              copy=copy, fill_value=fill_value)
 
-
         if method is not None:
             left = left.fillna(axis=fill_axis, method=method, limit=limit)
             right = right.fillna(axis=fill_axis, method=method, limit=limit)
@@ -2101,7 +2121,8 @@ class NDFrame(PandasObject):
                 raise ValueError('where requires an ndarray like object for its '
                                  'condition')
             if cond.shape != self.shape:
-                raise ValueError('Array conditional must be same shape as self')
+                raise ValueError(
+                    'Array conditional must be same shape as self')
             cond = self._constructor(cond, **self._construct_axes_dict())
 
         if inplace:
@@ -2127,7 +2148,7 @@ class NDFrame(PandasObject):
             if self.ndim == 1:
 
                 # try to set the same dtype as ourselves
-                new_other = np.array(other,dtype=self.dtype)
+                new_other = np.array(other, dtype=self.dtype)
                 if not (new_other == np.array(other)).all():
                     other = np.array(other)
 
@@ -2140,7 +2161,7 @@ class NDFrame(PandasObject):
 
                 other = np.array(other)
 
-        if isinstance(other,np.ndarray):
+        if isinstance(other, np.ndarray):
 
             if other.shape != self.shape:
 
@@ -2172,13 +2193,14 @@ class NDFrame(PandasObject):
                         if not try_quick:
 
                             dtype, fill_value = _maybe_promote(other.dtype)
-                            new_other = np.empty(len(icond),dtype=dtype)
+                            new_other = np.empty(len(icond), dtype=dtype)
                             new_other.fill(fill_value)
                             com._maybe_upcast_putmask(new_other, icond, other)
                             other = new_other
 
                     else:
-                        raise ValueError('Length of replacements must equal series length')
+                        raise ValueError(
+                            'Length of replacements must equal series length')
 
                 else:
                     raise ValueError('other must be the same shape as self '
@@ -2189,11 +2211,13 @@ class NDFrame(PandasObject):
                 other = self._constructor(other, **self._construct_axes_dict())
 
         if inplace:
-            # we may have different type blocks come out of putmask, so reconstruct the block manager
-            self._data = self._data.putmask(cond,other,inplace=True)
+            # we may have different type blocks come out of putmask, so
+            # reconstruct the block manager
+            self._data = self._data.putmask(cond, other, inplace=True)
 
         else:
-            new_data = self._data.where(other, cond, raise_on_error=raise_on_error, try_cast=try_cast)
+            new_data = self._data.where(
+                other, cond, raise_on_error=raise_on_error, try_cast=try_cast)
 
             return self._constructor(new_data)
 
@@ -2602,5 +2626,4 @@ class NDFrame(PandasObject):
 
 # install the indexerse
 for _name, _indexer in indexing.get_indexers_list():
-    NDFrame._create_indexer(_name,_indexer)
-
+    NDFrame._create_indexer(_name, _indexer)
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 3811cdfa6..87940615f 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -24,7 +24,9 @@ from pandas import compat
 from pandas.compat import range, lrange, lmap, callable, map, zip
 from pandas.util import rwproperty
 
+
 class Block(PandasObject):
+
     """
     Canonical n-dimensional unit of homogeneous dtype contained in a pandas
     data structure
@@ -101,14 +103,14 @@ class Block(PandasObject):
 
     def reset_ref_locs(self):
         """ reset the block ref_locs """
-        self._ref_locs = np.empty(len(self.items),dtype='int64')
+        self._ref_locs = np.empty(len(self.items), dtype='int64')
 
     def set_ref_locs(self, placement):
         """ explicity set the ref_locs indexer, only necessary for duplicate indicies """
         if placement is None:
             self._ref_locs = None
         else:
-            self._ref_locs = np.array(placement,dtype='int64', copy=True)
+            self._ref_locs = np.array(placement, dtype='int64', copy=True)
 
     def set_ref_items(self, ref_items, maybe_rename=True):
         """
@@ -179,12 +181,13 @@ class Block(PandasObject):
             values = values.copy()
         if ref_items is None:
             ref_items = self.ref_items
-        return make_block(values, self.items, ref_items, ndim=self.ndim, klass=self.__class__,
-                          fastpath=True, placement=self._ref_locs)
+        return make_block(
+            values, self.items, ref_items, ndim=self.ndim, klass=self.__class__,
+            fastpath=True, placement=self._ref_locs)
 
     @property
     def ftype(self):
-        return "%s:%s" % (self.dtype,self._ftype)
+        return "%s:%s" % (self.dtype, self._ftype)
 
     def merge(self, other):
         if not self.ref_items.equals(other.ref_items):
@@ -205,8 +208,9 @@ class Block(PandasObject):
             fill_value = self.fill_value
         new_values = com.take_nd(self.values, indexer, axis,
                                  fill_value=fill_value, mask_info=mask_info)
-        return make_block(new_values, self.items, self.ref_items, ndim=self.ndim, fastpath=True,
-                          placement=self._ref_locs)
+        return make_block(
+            new_values, self.items, self.ref_items, ndim=self.ndim, fastpath=True,
+            placement=self._ref_locs)
 
     def reindex_items_from(self, new_ref_items, copy=True):
         """
@@ -282,7 +286,7 @@ class Block(PandasObject):
             yield make_block(self.values[s:e],
                              self.items[s:e].copy(),
                              self.ref_items,
-                             ndim = self.ndim,
+                             ndim=self.ndim,
                              klass=self.__class__,
                              fastpath=True)
 
@@ -299,12 +303,13 @@ class Block(PandasObject):
         value = self._try_fill(value)
         np.putmask(new_values, mask, value)
 
-        block = make_block(new_values, self.items, self.ref_items, fastpath=True)
+        block = make_block(
+            new_values, self.items, self.ref_items, fastpath=True)
         if downcast:
             block = block.downcast()
         return block
 
-    def downcast(self, dtypes = None):
+    def downcast(self, dtypes=None):
         """ try to downcast each item to the dict of dtypes if present """
 
         if dtypes is None:
@@ -314,15 +319,15 @@ class Block(PandasObject):
         blocks = []
         for i, item in enumerate(self.items):
 
-            dtype = dtypes.get(item,self._downcast_dtype)
+            dtype = dtypes.get(item, self._downcast_dtype)
             if dtype is None:
                 nv = _block_shape(values[i])
-                blocks.append(make_block(nv, [ item ], self.ref_items))
+                blocks.append(make_block(nv, [item], self.ref_items))
                 continue
 
             nv = _possibly_downcast_to_dtype(values[i], np.dtype(dtype))
             nv = _block_shape(nv)
-            blocks.append(make_block(nv, [ item ], self.ref_items))
+            blocks.append(make_block(nv, [item], self.ref_items))
 
         return blocks
 
@@ -346,8 +351,9 @@ class Block(PandasObject):
             # force the copy here
             if values is None:
                 values = com._astype_nansafe(self.values, dtype, copy=True)
-            newb = make_block(values, self.items, self.ref_items, ndim=self.ndim,
-                              fastpath=True, dtype=dtype, klass=klass)
+            newb = make_block(
+                values, self.items, self.ref_items, ndim=self.ndim,
+                fastpath=True, dtype=dtype, klass=klass)
         except:
             if raise_on_error is True:
                 raise
@@ -361,7 +367,7 @@ class Block(PandasObject):
                                                self.itemsize, newb.dtype.name, newb.itemsize))
         return newb
 
-    def convert(self, copy = True, **kwargs):
+    def convert(self, copy=True, **kwargs):
         """ attempt to coerce any object types to better types
             return a copy of the block (if copy = True)
             by definition we are not an ObjectBlock here!  """
@@ -383,9 +389,10 @@ class Block(PandasObject):
                 dtypes = set(items[item])
 
                 # this is a safe bet with multiple dtypes
-                dtype  = list(dtypes)[0] if len(dtypes) == 1 else np.float64
+                dtype = list(dtypes)[0] if len(dtypes) == 1 else np.float64
 
-                b = make_block(SparseArray(self.get(item), dtype=dtype), [ item ], self.ref_items)
+                b = make_block(
+                    SparseArray(self.get(item), dtype=dtype), [item], self.ref_items)
                 new_blocks.append(b)
 
             return new_blocks
@@ -419,8 +426,8 @@ class Block(PandasObject):
 
         values = self.values
         if slicer is not None:
-            values = values[:,slicer]
-        values = np.array(values,dtype=object)
+            values = values[:, slicer]
+        values = np.array(values, dtype=object)
         mask = isnull(values)
         values[mask] = na_rep
         return values.tolist()
@@ -439,8 +446,8 @@ class Block(PandasObject):
 
         if not mask.any():
             if inplace:
-                return [ self ]
-            return [ self.copy() ]
+                return [self]
+            return [self.copy()]
         return self.putmask(mask, value, inplace=inplace)
 
     def putmask(self, mask, new, inplace=False):
@@ -457,7 +464,8 @@ class Block(PandasObject):
         # may need to align the mask
         if hasattr(mask, 'reindex_axis'):
             axis = getattr(mask, '_info_axis_number', 0)
-            mask = mask.reindex_axis(self.items, axis=axis, copy=False).values.T
+            mask = mask.reindex_axis(
+                self.items, axis=axis, copy=False).values.T
 
         if self._can_hold_element(new):
             new = self._try_cast(new)
@@ -469,7 +477,7 @@ class Block(PandasObject):
             # need to go column by column
             new_blocks = []
 
-            def create_block(v,m,n,item,reshape=True):
+            def create_block(v, m, n, item, reshape=True):
                 """ return a new block, try to preserve dtype if possible """
 
                 # n should the length of the mask or a scalar here
@@ -483,8 +491,8 @@ class Block(PandasObject):
                     nn = n[m]
                     nn_at = nn.astype(self.dtype)
                     if (nn == nn_at).all():
-                            nv = v.copy()
-                            nv[mask] = nn_at
+                        nv = v.copy()
+                        nv[mask] = nn_at
                 except:
                     pass
 
@@ -496,7 +504,7 @@ class Block(PandasObject):
 
                 if reshape:
                     nv = _block_shape(nv)
-                    return make_block(nv, [ item ], self.ref_items)
+                    return make_block(nv, [item], self.ref_items)
                 else:
                     return make_block(nv, item, self.ref_items)
 
@@ -508,7 +516,8 @@ class Block(PandasObject):
                     # need a new block
                     if m.any():
 
-                        n = new[i] if isinstance(new, np.ndarray) else np.array(new)
+                        n = new[i] if isinstance(
+                            new, np.ndarray) else np.array(new)
 
                         # type of the new block
                         dtype, _ = com._maybe_promote(n.dtype)
@@ -516,23 +525,25 @@ class Block(PandasObject):
                         # we need to exiplicty astype here to make a copy
                         n = n.astype(dtype)
 
-                        block = create_block(v,m,n,item)
+                        block = create_block(v, m, n, item)
 
                     else:
                         nv = v if inplace else v.copy()
                         nv = _block_shape(nv)
-                        block = make_block(nv, Index([ item ]), self.ref_items, fastpath=True)
+                        block = make_block(
+                            nv, Index([item]), self.ref_items, fastpath=True)
 
                     new_blocks.append(block)
 
             else:
 
-                new_blocks.append(create_block(new_values,mask,new,self.items,reshape=False))
+                new_blocks.append(
+                    create_block(new_values, mask, new, self.items, reshape=False))
 
             return new_blocks
 
         if inplace:
-            return [ self ]
+            return [self]
 
         return make_block(new_values, self.items, self.ref_items, fastpath=True)
 
@@ -583,7 +594,7 @@ class Block(PandasObject):
             new_values[:, periods:] = fill_value
         return make_block(new_values, self.items, self.ref_items, ndim=self.ndim, fastpath=True)
 
-    def eval(self, func, other, raise_on_error = True, try_cast = False):
+    def eval(self, func, other, raise_on_error=True, try_cast=False):
         """
         evaluate the block; return result block from the result
 
@@ -603,7 +614,8 @@ class Block(PandasObject):
         # see if we can align other
         if hasattr(other, 'reindex_axis'):
             axis = getattr(other, '_info_axis_number', 0)
-            other = other.reindex_axis(self.items, axis=axis, copy=False).values
+            other = other.reindex_axis(
+                self.items, axis=axis, copy=False).values
 
         # make sure that we can broadcast
         is_transposed = False
@@ -613,16 +625,16 @@ class Block(PandasObject):
                 is_transposed = True
 
         values, other = self._try_coerce_args(values, other)
-        args = [ values, other ]
+        args = [values, other]
         try:
             result = self._try_coerce_result(func(*args))
         except (Exception) as detail:
             if raise_on_error:
                 raise TypeError('Could not operate [%s] with block values [%s]'
-                                % (repr(other),str(detail)))
+                                % (repr(other), str(detail)))
             else:
                 # return the values
-                result = np.empty(values.shape,dtype='O')
+                result = np.empty(values.shape, dtype='O')
                 result.fill(np.nan)
 
         if not isinstance(result, np.ndarray):
@@ -638,7 +650,7 @@ class Block(PandasObject):
 
         return make_block(result, self.items, self.ref_items, ndim=self.ndim, fastpath=True)
 
-    def where(self, other, cond, raise_on_error = True, try_cast = False):
+    def where(self, other, cond, raise_on_error=True, try_cast=False):
         """
         evaluate the block; return result block(s) from the result
 
@@ -657,8 +669,8 @@ class Block(PandasObject):
         values = self.values
 
         # see if we can align other
-        if hasattr(other,'reindex_axis'):
-            axis = getattr(other,'_info_axis_number',0)
+        if hasattr(other, 'reindex_axis'):
+            axis = getattr(other, '_info_axis_number', 0)
             other = other.reindex_axis(self.items, axis=axis, copy=True).values
 
         # make sure that we can broadcast
@@ -669,10 +681,11 @@ class Block(PandasObject):
                 is_transposed = True
 
         # see if we can align cond
-        if not hasattr(cond,'shape'):
-            raise ValueError("where must have a condition that is ndarray like")
-        if hasattr(cond,'reindex_axis'):
-            axis = getattr(cond,'_info_axis_number',0)
+        if not hasattr(cond, 'shape'):
+            raise ValueError(
+                "where must have a condition that is ndarray like")
+        if hasattr(cond, 'reindex_axis'):
+            axis = getattr(cond, '_info_axis_number', 0)
             cond = cond.reindex_axis(self.items, axis=axis, copy=True).values
         else:
             cond = cond.values
@@ -681,10 +694,10 @@ class Block(PandasObject):
         if hasattr(values, 'ndim'):
             if values.ndim != cond.ndim or values.shape == cond.shape[::-1]:
                 values = values.T
-                is_transposed =  not is_transposed
+                is_transposed = not is_transposed
 
         # our where function
-        def func(c,v,o):
+        def func(c, v, o):
             if c.ravel().all():
                 return v
 
@@ -694,15 +707,15 @@ class Block(PandasObject):
             except (Exception) as detail:
                 if raise_on_error:
                     raise TypeError('Could not operate [%s] with block values [%s]'
-                                    % (repr(o),str(detail)))
+                                    % (repr(o), str(detail)))
                 else:
                     # return the values
-                    result = np.empty(v.shape,dtype='float64')
+                    result = np.empty(v.shape, dtype='float64')
                     result.fill(np.nan)
                     return result
 
         # see if we can operate on the entire block, or need item-by-item
-        result = func(cond,values,other)
+        result = func(cond, values, other)
         if self._can_hold_na:
 
             if not isinstance(result, np.ndarray):
@@ -735,6 +748,7 @@ class Block(PandasObject):
 
         return result_blocks
 
+
 class NumericBlock(Block):
     is_numeric = True
     _can_hold_na = True
@@ -742,6 +756,7 @@ class NumericBlock(Block):
     def _try_cast_result(self, result):
         return _possibly_downcast_to_dtype(result, self.dtype)
 
+
 class FloatBlock(NumericBlock):
     _downcast_dtype = 'int64'
 
@@ -761,13 +776,14 @@ class FloatBlock(NumericBlock):
 
         values = self.values
         if slicer is not None:
-            values = values[:,slicer]
-        values = np.array(values,dtype=object)
+            values = values[:, slicer]
+        values = np.array(values, dtype=object)
         mask = isnull(values)
         values[mask] = na_rep
         if float_format:
             imask = (-mask).ravel()
-            values.flat[imask] = np.array([ float_format % val for val in values.ravel()[imask] ])
+            values.flat[imask] = np.array(
+                [float_format % val for val in values.ravel()[imask]])
         return values.tolist()
 
     def should_store(self, value):
@@ -858,17 +874,21 @@ class ObjectBlock(Block):
             for i, c in enumerate(self.items):
                 values = self.iget(i)
 
-                values = com._possibly_convert_objects(values, convert_dates=convert_dates, convert_numeric=convert_numeric)
+                values = com._possibly_convert_objects(
+                    values, convert_dates=convert_dates, convert_numeric=convert_numeric)
                 values = _block_shape(values)
                 items = self.items.take([i])
                 placement = None if is_unique else [i]
-                newb = make_block(values, items, self.ref_items, ndim=self.ndim, placement=placement)
+                newb = make_block(
+                    values, items, self.ref_items, ndim=self.ndim, placement=placement)
                 blocks.append(newb)
 
         else:
 
-            values = com._possibly_convert_objects(self.values, convert_dates=convert_dates, convert_numeric=convert_numeric)
-            blocks.append(make_block(values, self.items, self.ref_items, ndim = self.ndim))
+            values = com._possibly_convert_objects(
+                self.values, convert_dates=convert_dates, convert_numeric=convert_numeric)
+            blocks.append(
+                make_block(values, self.items, self.ref_items, ndim=self.ndim))
 
         return blocks
 
@@ -951,7 +971,7 @@ class ObjectBlock(Block):
                                                       inplace=inplace,
                                                       filter=filter, regex=regex)
             if not isinstance(result, list):
-                result = [ result]
+                result = [result]
             return result
 
         new_values = self.values if inplace else self.values.copy()
@@ -1026,7 +1046,8 @@ class DatetimeBlock(Block):
         """ reverse of try_coerce_args """
         if isinstance(result, np.ndarray):
             if result.dtype == 'i8':
-                result = tslib.array_to_datetime(result.astype(object).ravel()).reshape(result.shape)
+                result = tslib.array_to_datetime(
+                    result.astype(object).ravel()).reshape(result.shape)
         elif isinstance(result, np.integer):
             result = lib.Timestamp(result)
         return result
@@ -1042,18 +1063,20 @@ class DatetimeBlock(Block):
 
         values = self.values
         if slicer is not None:
-            values = values[:,slicer]
+            values = values[:, slicer]
         mask = isnull(values)
 
-        rvalues = np.empty(values.shape,dtype=object)
+        rvalues = np.empty(values.shape, dtype=object)
         if na_rep is None:
             na_rep = 'NaT'
         rvalues[mask] = na_rep
         imask = (-mask).ravel()
         if self.dtype == 'datetime64[ns]':
-            rvalues.flat[imask] = np.array([ Timestamp(val)._repr_base for val in values.ravel()[imask] ],dtype=object)
+            rvalues.flat[imask] = np.array(
+                [Timestamp(val)._repr_base for val in values.ravel()[imask]], dtype=object)
         elif self.dtype == 'timedelta64[ns]':
-            rvalues.flat[imask] = np.array([ lib.repr_timedelta64(val) for val in values.ravel()[imask] ],dtype=object)
+            rvalues.flat[imask] = np.array([lib.repr_timedelta64(val)
+                                           for val in values.ravel()[imask]], dtype=object)
         return rvalues.tolist()
 
     def should_store(self, value):
@@ -1083,14 +1106,16 @@ class DatetimeBlock(Block):
 
         self.values[loc] = value
 
-    def get_values(self, dtype = None):
+    def get_values(self, dtype=None):
         if dtype == object:
             flat_i8 = self.values.ravel().view(np.int64)
             res = tslib.ints_to_pydatetime(flat_i8)
             return res.reshape(self.values.shape)
         return self.values
 
+
 class SparseBlock(Block):
+
     """ implement as a list of sparse arrays of the same dtype """
     __slots__ = ['items', 'ref_items', '_ref_locs', 'ndim', 'values']
     is_sparse = True
@@ -1126,7 +1151,7 @@ class SparseBlock(Block):
 
     @property
     def shape(self):
-        return (len(self.items),self.sp_index.length)
+        return (len(self.items), self.sp_index.length)
 
     @property
     def itemsize(self):
@@ -1150,7 +1175,8 @@ class SparseBlock(Block):
     @rwproperty.setproperty
     def sp_values(self, v):
         # reset the sparse values
-        self.values = SparseArray(v,sparse_index=self.sp_index,kind=self.kind,dtype=v.dtype,fill_value=self.fill_value,copy=False)
+        self.values = SparseArray(
+            v, sparse_index=self.sp_index, kind=self.kind, dtype=v.dtype, fill_value=self.fill_value, copy=False)
 
     @property
     def sp_index(self):
@@ -1200,8 +1226,9 @@ class SparseBlock(Block):
     def get_merge_length(self):
         return 1
 
-    def make_block(self, values, items=None, ref_items=None, sparse_index=None, kind=None, dtype=None, fill_value=None,
-                   copy=False, fastpath=True):
+    def make_block(
+        self, values, items=None, ref_items=None, sparse_index=None, kind=None, dtype=None, fill_value=None,
+            copy=False, fastpath=True):
         """ return a new block """
         if dtype is None:
             dtype = self.dtype
@@ -1211,13 +1238,15 @@ class SparseBlock(Block):
             items = self.items
         if ref_items is None:
             ref_items = self.ref_items
-        new_values = SparseArray(values,sparse_index=sparse_index,kind=kind or self.kind,dtype=dtype,fill_value=fill_value,copy=copy)
+        new_values = SparseArray(values, sparse_index=sparse_index,
+                                 kind=kind or self.kind, dtype=dtype, fill_value=fill_value, copy=copy)
         return make_block(new_values, items, ref_items, ndim=self.ndim, fastpath=fastpath)
 
     def interpolate(self, method='pad', axis=0, inplace=False,
                     limit=None, missing=None, **kwargs):
 
-        values = com.interpolate_2d(self.values.to_dense(), method, axis, limit, missing)
+        values = com.interpolate_2d(
+            self.values.to_dense(), method, axis, limit, missing)
         return self.make_block(values, self.items, self.ref_items)
 
     def fillna(self, value, inplace=False, downcast=None):
@@ -1225,7 +1254,7 @@ class SparseBlock(Block):
         if issubclass(self.dtype.type, np.floating):
             value = float(value)
         values = self.values if inplace else self.values.copy()
-        return self.make_block(values.get_values(value),fill_value=value)
+        return self.make_block(values.get_values(value), fill_value=value)
 
     def shift(self, indexer, periods):
         """ shift the block by periods """
@@ -1258,7 +1287,7 @@ class SparseBlock(Block):
         # taking on the 0th axis always here
         if fill_value is None:
             fill_value = self.fill_value
-        return self.make_block(self.values.take(indexer),items=self.items,fill_value=fill_value)
+        return self.make_block(self.values.take(indexer), items=self.items, fill_value=fill_value)
 
     def reindex_items_from(self, new_ref_items, copy=True):
         """
@@ -1276,27 +1305,29 @@ class SparseBlock(Block):
         if self.ndim >= 2:
             if self.items[0] not in self.ref_items:
                 return None
-            return self.make_block(self.values,ref_items=new_ref_items,copy=copy)
+            return self.make_block(self.values, ref_items=new_ref_items, copy=copy)
 
         # 1-d
         new_ref_items, indexer = self.items.reindex(new_ref_items)
         if indexer is None:
             indexer = np.arange(len(self.items))
 
-        return self.make_block(com.take_1d(self.values.values, indexer),items=new_ref_items,ref_items=new_ref_items,copy=copy)
+        return self.make_block(com.take_1d(self.values.values, indexer), items=new_ref_items, ref_items=new_ref_items, copy=copy)
 
     def sparse_reindex(self, new_index):
         """ sparse reindex and return a new block
             current reindex only works for float64 dtype! """
         values = self.values
-        values = values.sp_index.to_int_index().reindex(values.sp_values.astype('float64'),values.fill_value,new_index)
-        return self.make_block(values,sparse_index=new_index)
+        values = values.sp_index.to_int_index().reindex(
+            values.sp_values.astype('float64'), values.fill_value, new_index)
+        return self.make_block(values, sparse_index=new_index)
 
     def split_block_at(self, item):
         if len(self.items) == 1 and item == self.items[0]:
             return []
         return super(SparseBlock, self).split_block_at(self, item)
 
+
 def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fastpath=False, placement=None):
 
     if klass is None:
@@ -1327,7 +1358,8 @@ def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fast
                     # we have an object array that has been inferred as datetime, so
                     # convert it
                     try:
-                        values = tslib.array_to_datetime(flat).reshape(values.shape)
+                        values = tslib.array_to_datetime(
+                            flat).reshape(values.shape)
                         klass = DatetimeBlock
                     except:  # it already object, so leave it
                         pass
@@ -1341,6 +1373,7 @@ def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fast
 
 
 class BlockManager(PandasObject):
+
     """
     Core internal data structure to implement DataFrame
 
@@ -1356,7 +1389,8 @@ class BlockManager(PandasObject):
     -----
     This is *not* a public API class
     """
-    __slots__ = ['axes', 'blocks', '_ndim', '_shape', '_known_consolidated', '_is_consolidated', '_has_sparse', '_ref_locs', '_items_map']
+    __slots__ = ['axes', 'blocks', '_ndim', '_shape', '_known_consolidated',
+                 '_is_consolidated', '_has_sparse', '_ref_locs', '_items_map']
 
     def __init__(self, blocks, axes, do_integrity_check=True, fastpath=True):
         self.axes = [_ensure_index(ax) for ax in axes]
@@ -1391,13 +1425,13 @@ class BlockManager(PandasObject):
 
     @property
     def shape(self):
-        if getattr(self,'_shape',None) is None:
+        if getattr(self, '_shape', None) is None:
             self._shape = tuple(len(ax) for ax in self.axes)
         return self._shape
 
     @property
     def ndim(self):
-        if getattr(self,'_ndim',None) is None:
+        if getattr(self, '_ndim', None) is None:
             self._ndim = len(self.axes)
         return self._ndim
 
@@ -1425,7 +1459,6 @@ class BlockManager(PandasObject):
             # set/reset ref_locs based on the new index
             self._set_ref_locs(labels=value, do_refs=True)
 
-
     def _reset_ref_locs(self):
         """ take the current _ref_locs and reset ref_locs on the blocks
             to correctly map, ignoring Nones;
@@ -1440,7 +1473,7 @@ class BlockManager(PandasObject):
                 b.reset_ref_locs()
         self._rebuild_ref_locs()
 
-        self._ref_locs  = None
+        self._ref_locs = None
         self._items_map = None
 
     def _rebuild_ref_locs(self):
@@ -1483,10 +1516,10 @@ class BlockManager(PandasObject):
 
         # we are going to a non-unique index
         # we have ref_locs on the block at this point
-        if (not is_unique and do_refs) or do_refs=='force':
+        if (not is_unique and do_refs) or do_refs == 'force':
 
             # create the items map
-            im = getattr(self,'_items_map',None)
+            im = getattr(self, '_items_map', None)
             if im is None:
 
                 im = dict()
@@ -1499,25 +1532,25 @@ class BlockManager(PandasObject):
                     except:
                         raise AssertionError("cannot create BlockManager._ref_locs because "
                                              "block [%s] with duplicate items [%s] "
-                                             "does not have _ref_locs set" % (block,labels))
+                                             "does not have _ref_locs set" % (block, labels))
 
-                    m = maybe_create_block_in_items_map(im,block)
+                    m = maybe_create_block_in_items_map(im, block)
                     for i, item in enumerate(block.items):
                         m[i] = rl[i]
 
                 self._items_map = im
 
             # create the _ref_loc map here
-            rl = [ None] * len(labels)
+            rl = [None] * len(labels)
             for block, items in im.items():
                 for i, loc in enumerate(items):
-                    rl[loc] = (block,i)
+                    rl[loc] = (block, i)
             self._ref_locs = rl
             return rl
 
         # return our cached _ref_locs (or will compute again
         # when we recreate the block manager if needed
-        return getattr(self,'_ref_locs',None)
+        return getattr(self, '_ref_locs', None)
 
     def get_items_map(self, use_cached=True):
         """
@@ -1529,7 +1562,7 @@ class BlockManager(PandasObject):
 
         # cache check
         if use_cached:
-            im = getattr(self,'_items_map',None)
+            im = getattr(self, '_items_map', None)
             if im is not None:
                 return im
 
@@ -1542,17 +1575,16 @@ class BlockManager(PandasObject):
             axis = self.axes[0]
             for block in self.blocks:
 
-                m = maybe_create_block_in_items_map(im,block)
+                m = maybe_create_block_in_items_map(im, block)
                 for i, item in enumerate(block.items):
                     m[i] = axis.get_loc(item)
 
-
         # use the ref_locs to construct the map
         else:
 
             for i, (block, idx) in enumerate(rl):
 
-                m = maybe_create_block_in_items_map(im,block)
+                m = maybe_create_block_in_items_map(im, block)
                 m[idx] = i
 
         self._items_map = im
@@ -1568,7 +1600,7 @@ class BlockManager(PandasObject):
         self._consolidate_inplace()
         counts = dict()
         for b in self.blocks:
-            counts[b.dtype.name] = counts.get(b.dtype.name,0) + b.shape[0]
+            counts[b.dtype.name] = counts.get(b.dtype.name, 0) + b.shape[0]
         return counts
 
     def get_ftype_counts(self):
@@ -1576,7 +1608,7 @@ class BlockManager(PandasObject):
         self._consolidate_inplace()
         counts = dict()
         for b in self.blocks:
-            counts[b.ftype] = counts.get(b.ftype,0) + b.shape[0]
+            counts[b.ftype] = counts.get(b.ftype, 0) + b.shape[0]
         return counts
 
     def __getstate__(self):
@@ -1629,7 +1661,8 @@ class BlockManager(PandasObject):
                 raise AssertionError("Block ref_items must be BlockManager "
                                      "items")
             if not block.is_sparse and block.values.shape[1:] != mgr_shape[1:]:
-                construction_error(tot_items,block.values.shape[1:],self.axes)
+                construction_error(
+                    tot_items, block.values.shape[1:], self.axes)
         if len(self.items) != tot_items:
             raise AssertionError('Number of manager items must equal union of '
                                  'block items\n# manager items: {0}, # '
@@ -1646,9 +1679,9 @@ class BlockManager(PandasObject):
         filter : list, if supplied, only call the block if the filter is in the block
         """
 
-        axes = kwargs.pop('axes',None)
+        axes = kwargs.pop('axes', None)
         filter = kwargs.get('filter')
-        do_integrity_check = kwargs.pop('do_integrity_check',False)
+        do_integrity_check = kwargs.pop('do_integrity_check', False)
         result_blocks = []
         for blk in self.blocks:
             if filter is not None:
@@ -1659,13 +1692,14 @@ class BlockManager(PandasObject):
             if callable(f):
                 applied = f(blk, *args, **kwargs)
             else:
-                applied = getattr(blk,f)(*args, **kwargs)
+                applied = getattr(blk, f)(*args, **kwargs)
 
-            if isinstance(applied,list):
+            if isinstance(applied, list):
                 result_blocks.extend(applied)
             else:
                 result_blocks.append(applied)
-        bm = self.__class__(result_blocks, axes or self.axes, do_integrity_check=do_integrity_check)
+        bm = self.__class__(
+            result_blocks, axes or self.axes, do_integrity_check=do_integrity_check)
         bm._consolidate_inplace()
         return bm
 
@@ -1707,18 +1741,19 @@ class BlockManager(PandasObject):
 
         # figure out our mask a-priori to avoid repeated replacements
         values = self.as_matrix()
+
         def comp(s):
             if isnull(s):
                 return isnull(values)
             return values == s
-        masks = [ comp(s) for i, s in enumerate(src_lst) ]
+        masks = [comp(s) for i, s in enumerate(src_lst)]
 
         result_blocks = []
         for blk in self.blocks:
 
             # its possible to get multiple result blocks here
             # replace ALWAYS will return a list
-            rb = [ blk if inplace else blk.copy() ]
+            rb = [blk if inplace else blk.copy()]
             for i, (s, d) in enumerate(zip(src_lst, dest_lst)):
                 new_rb = []
                 for b in rb:
@@ -1763,11 +1798,10 @@ class BlockManager(PandasObject):
                         is_sparse[i].append(blk.dtype)
 
         if len(is_sparse):
-            return self.apply('post_merge', items = is_sparse)
+            return self.apply('post_merge', items=is_sparse)
 
         return self
 
-
     def is_consolidated(self):
         """
         Return True if more than one block with the same dtype
@@ -1795,7 +1829,7 @@ class BlockManager(PandasObject):
     def is_numeric_mixed_type(self):
         # Warning, consolidation needs to get checked upstairs
         self._consolidate_inplace()
-        return all([ block.is_numeric for block in self.blocks ])
+        return all([block.is_numeric for block in self.blocks])
 
     def get_block_map(self, copy=False, typ=None, columns=None, is_numeric=False, is_bool=False):
         """ return a dictionary mapping the ftype -> block list
@@ -1831,6 +1865,7 @@ class BlockManager(PandasObject):
             return b
 
         maybe_copy = lambda b: b.copy() if copy else b
+
         def maybe_copy(b):
             if copy:
                 b = b.copy()
@@ -1872,7 +1907,8 @@ class BlockManager(PandasObject):
         copy : boolean, default False
             Whether to copy the blocks
         """
-        blocks = self.get_block_map(typ='list', copy=copy, columns=columns, **kwargs)
+        blocks = self.get_block_map(
+            typ='list', copy=copy, columns=columns, **kwargs)
         if len(blocks) == 0:
             return self.__class__.make_empty()
 
@@ -2128,17 +2164,17 @@ class BlockManager(PandasObject):
             if com.is_integer(indexer):
 
                 b, loc = ref_locs[indexer]
-                values = [ b.iget(loc) ]
-                index = Index([ self.items[indexer] ])
+                values = [b.iget(loc)]
+                index = Index([self.items[indexer]])
 
             # we have a multiple result, potentially across blocks
             else:
 
-                values = [ block.iget(i) for block, i in ref_locs[indexer] ]
+                values = [block.iget(i) for block, i in ref_locs[indexer]]
                 index = self.items[indexer]
 
             # create and return a new block manager
-            axes  = [ index ] + self.axes[1:]
+            axes = [index] + self.axes[1:]
             blocks = form_blocks(values, index, axes)
             mgr = BlockManager(blocks, axes)
             mgr._consolidate_inplace()
@@ -2230,7 +2266,8 @@ class BlockManager(PandasObject):
                     for i, (l, arr) in enumerate(zip(loc, value)):
 
                         # insert the item
-                        self.insert(l, item, arr[None, :], allow_duplicates=True)
+                        self.insert(
+                            l, item, arr[None, :], allow_duplicates=True)
 
                         # reset the _ref_locs on indiviual blocks
                         # rebuild ref_locs
@@ -2240,7 +2277,6 @@ class BlockManager(PandasObject):
 
                     self._rebuild_ref_locs()
 
-
                 else:
                     for i, (item, arr) in enumerate(zip(subset, value)):
                         _set_item(item, arr[None, :])
@@ -2278,7 +2314,7 @@ class BlockManager(PandasObject):
         self._known_consolidated = False
 
         # clear the internal ref_loc mappings if necessary
-        if loc != len(self.items)-1 and new_items.is_unique:
+        if loc != len(self.items) - 1 and new_items.is_unique:
             self.set_items_clear(new_items)
 
     def set_items_norename(self, value):
@@ -2297,7 +2333,7 @@ class BlockManager(PandasObject):
         # possibily convert to an indexer
         loc = _possibly_convert_to_indexer(loc)
 
-        if isinstance(loc, (list,tuple,np.ndarray)):
+        if isinstance(loc, (list, tuple, np.ndarray)):
             for l in loc:
                 for i, b in enumerate(self.blocks):
                     if item in b.items:
@@ -2315,11 +2351,13 @@ class BlockManager(PandasObject):
         so after this function, _ref_locs and _items_map (if used)
         are correct for the items, None fills holes in _ref_locs
         """
-        block     = self.blocks.pop(i)
-        ref_locs  = self._set_ref_locs()
-        prev_items_map = self._items_map.pop(block) if ref_locs is not None else None
+        block = self.blocks.pop(i)
+        ref_locs = self._set_ref_locs()
+        prev_items_map = self._items_map.pop(
+            block) if ref_locs is not None else None
 
-        # if we can't consolidate, then we are removing this block in its entirey
+        # if we can't consolidate, then we are removing this block in its
+        # entirey
         if block._can_consolidate:
 
             # compute the split mask
@@ -2346,7 +2384,8 @@ class BlockManager(PandasObject):
                 if ref_locs is not None:
 
                     # fill the item_map out for this sub-block
-                    m = maybe_create_block_in_items_map(self._items_map,sblock)
+                    m = maybe_create_block_in_items_map(
+                        self._items_map, sblock)
                     for j, itm in enumerate(sblock.items):
 
                         # is this item masked (e.g. was deleted)?
@@ -2438,7 +2477,8 @@ class BlockManager(PandasObject):
                                      'axis == 0')
             return self.reindex_items(new_axis, copy=copy, fill_value=fill_value)
 
-        new_axis, indexer = cur_axis.reindex(new_axis, method, copy_if_needed=True)
+        new_axis, indexer = cur_axis.reindex(
+            new_axis, method, copy_if_needed=True)
         return self.reindex_indexer(new_axis, indexer, axis=axis, fill_value=fill_value)
 
     def reindex_indexer(self, new_axis, indexer, axis=1, fill_value=None):
@@ -2450,7 +2490,8 @@ class BlockManager(PandasObject):
 
         new_blocks = []
         for block in self.blocks:
-            newb = block.reindex_axis(indexer, axis=axis, fill_value=fill_value)
+            newb = block.reindex_axis(
+                indexer, axis=axis, fill_value=fill_value)
             new_blocks.append(newb)
 
         new_axes = list(self.axes)
@@ -2557,7 +2598,7 @@ class BlockManager(PandasObject):
         n = len(self.axes[axis])
 
         if verify:
-           indexer = _maybe_convert_indices(indexer, n)
+            indexer = _maybe_convert_indices(indexer, n)
 
         if ((indexer == -1) | (indexer >= n)).any():
             raise Exception('Indices must be nonzero and less than '
@@ -2568,7 +2609,7 @@ class BlockManager(PandasObject):
             new_index = self.axes[axis].take(indexer)
 
         new_axes[axis] = new_index
-        return self.apply('take',axes=new_axes,indexer=indexer,ref_items=new_axes[0],axis=axis)
+        return self.apply('take', axes=new_axes, indexer=indexer, ref_items=new_axes[0], axis=axis)
 
     def merge(self, other, lsuffix=None, rsuffix=None):
         if not self._is_indexed_like(other):
@@ -2622,7 +2663,8 @@ class BlockManager(PandasObject):
 
         index = self.axes[axis]
         if isinstance(index, MultiIndex):
-            new_axis = MultiIndex.from_tuples([tuple(mapper(y) for y in x) for x in index], names=index.names)
+            new_axis = MultiIndex.from_tuples(
+                [tuple(mapper(y) for y in x) for x in index], names=index.names)
         else:
             new_axis = Index([mapper(x) for x in index], name=index.name)
 
@@ -2686,33 +2728,38 @@ class BlockManager(PandasObject):
             raise AssertionError('Some items were not in any block')
         return result
 
+
 class SingleBlockManager(BlockManager):
+
     """ manage a single block with """
     ndim = 1
     _is_consolidated = True
     _known_consolidated = True
-    __slots__ = ['axes', 'blocks', '_block', '_values', '_shape', '_has_sparse']
+    __slots__ = ['axes', 'blocks', '_block',
+                 '_values', '_shape', '_has_sparse']
 
     def __init__(self, block, axis, do_integrity_check=False, fastpath=True):
 
         if isinstance(axis, list):
             if len(axis) != 1:
-                raise ValueError("cannot create SingleBlockManager with more than 1 axis")
+                raise ValueError(
+                    "cannot create SingleBlockManager with more than 1 axis")
             axis = axis[0]
 
         # passed from constructor, single block, single axis
         if fastpath:
-            self.axes = [ axis ]
+            self.axes = [axis]
             if isinstance(block, list):
                 if len(block) != 1:
-                    raise ValueError("cannot create SingleBlockManager with more than 1 block")
+                    raise ValueError(
+                        "cannot create SingleBlockManager with more than 1 block")
                 block = block[0]
             if not isinstance(block, Block):
                 block = make_block(block, axis, axis, ndim=1, fastpath=True)
 
         else:
 
-            self.axes   = [ _ensure_index(axis) ]
+            self.axes = [_ensure_index(axis)]
 
             # create the block here
             if isinstance(block, list):
@@ -2720,18 +2767,19 @@ class SingleBlockManager(BlockManager):
                 # provide consolidation to the interleaved_dtype
                 if len(block) > 1:
                     dtype = _interleaved_dtype(block)
-                    block = [ b.astype(dtype) for b in block ]
+                    block = [b.astype(dtype) for b in block]
                     block = _consolidate(block, axis)
 
                 if len(block) != 1:
-                    raise ValueError("cannot create SingleBlockManager with more than 1 block")
+                    raise ValueError(
+                        "cannot create SingleBlockManager with more than 1 block")
                 block = block[0]
 
             if not isinstance(block, Block):
                 block = make_block(block, axis, axis, ndim=1, fastpath=True)
 
-        self.blocks = [ block ]
-        self._block  = self.blocks[0]
+        self.blocks = [block]
+        self._block = self.blocks[0]
         self._values = self._block.values
         self._has_sparse = self._block.is_sparse
 
@@ -2741,7 +2789,7 @@ class SingleBlockManager(BlockManager):
 
     @property
     def shape(self):
-        if getattr(self,'_shape',None) is None:
+        if getattr(self, '_shape', None) is None:
             self._shape = tuple([len(self.axes[0])])
         return self._shape
 
@@ -2818,11 +2866,12 @@ class SingleBlockManager(BlockManager):
     def _consolidate_inplace(self):
         pass
 
+
 def construction_error(tot_items, block_shape, axes):
     """ raise a helpful message about our construction """
     raise ValueError("Shape of passed values is %s, indices imply %s" % (
-            tuple(map(int, [tot_items] + list(block_shape))),
-            tuple(map(int, [len(ax) for ax in axes]))))
+        tuple(map(int, [tot_items] + list(block_shape))),
+        tuple(map(int, [len(ax) for ax in axes]))))
 
 
 def create_block_manager_from_blocks(blocks, axes):
@@ -2831,16 +2880,18 @@ def create_block_manager_from_blocks(blocks, axes):
         # if we are passed values, make the blocks
         if len(blocks) == 1 and not isinstance(blocks[0], Block):
             placement = None if axes[0].is_unique else np.arange(len(axes[0]))
-            blocks = [ make_block(blocks[0], axes[0], axes[0], placement=placement) ]
+            blocks = [
+                make_block(blocks[0], axes[0], axes[0], placement=placement)]
 
         mgr = BlockManager(blocks, axes)
         mgr._consolidate_inplace()
         return mgr
 
     except (ValueError):
-        blocks = [ getattr(b,'values',b) for b in blocks ]
+        blocks = [getattr(b, 'values', b) for b in blocks]
         tot_items = sum(b.shape[0] for b in blocks)
-        construction_error(tot_items,blocks[0].shape[1:],axes)
+        construction_error(tot_items, blocks[0].shape[1:], axes)
+
 
 def create_block_manager_from_arrays(arrays, names, axes):
     try:
@@ -2849,14 +2900,15 @@ def create_block_manager_from_arrays(arrays, names, axes):
         mgr._consolidate_inplace()
         return mgr
     except (ValueError):
-        construction_error(len(arrays),arrays[0].shape[1:],axes)
+        construction_error(len(arrays), arrays[0].shape[1:], axes)
+
 
-def maybe_create_block_in_items_map(im,block):
+def maybe_create_block_in_items_map(im, block):
     """ create/return the block in an items_map """
     try:
         return im[block]
     except:
-        im[block] = l = [ None ] * len(block.items)
+        im[block] = l = [None] * len(block.items)
     return l
 
 
@@ -2867,7 +2919,7 @@ def form_blocks(arrays, names, axes):
 
     if len(arrays) < len(items):
         nn = set(names)
-        extra_items = Index([ i for i in items if i not in nn ])
+        extra_items = Index([i for i in items if i not in nn])
     else:
         extra_items = []
 
@@ -2915,7 +2967,8 @@ def form_blocks(arrays, names, axes):
         blocks.extend(float_blocks)
 
     if len(complex_items):
-        complex_blocks = _simple_blockify(complex_items, items, np.complex128, is_unique=is_unique)
+        complex_blocks = _simple_blockify(
+            complex_items, items, np.complex128, is_unique=is_unique)
         blocks.extend(complex_blocks)
 
     if len(int_items):
@@ -2923,15 +2976,18 @@ def form_blocks(arrays, names, axes):
         blocks.extend(int_blocks)
 
     if len(datetime_items):
-        datetime_blocks = _simple_blockify(datetime_items, items, _NS_DTYPE, is_unique=is_unique)
+        datetime_blocks = _simple_blockify(
+            datetime_items, items, _NS_DTYPE, is_unique=is_unique)
         blocks.extend(datetime_blocks)
 
     if len(bool_items):
-        bool_blocks = _simple_blockify(bool_items, items, np.bool_, is_unique=is_unique)
+        bool_blocks = _simple_blockify(
+            bool_items, items, np.bool_, is_unique=is_unique)
         blocks.extend(bool_blocks)
 
     if len(object_items) > 0:
-        object_blocks = _simple_blockify(object_items, items, np.object_, is_unique=is_unique)
+        object_blocks = _simple_blockify(
+            object_items, items, np.object_, is_unique=is_unique)
         blocks.extend(object_blocks)
 
     if len(sparse_items) > 0:
@@ -2946,7 +3002,8 @@ def form_blocks(arrays, names, axes):
         block_values.fill(np.nan)
 
         placement = None if is_unique else np.arange(len(extra_items))
-        na_block = make_block(block_values, extra_items, items, placement=placement)
+        na_block = make_block(
+            block_values, extra_items, items, placement=placement)
         blocks.append(na_block)
 
     return blocks
@@ -2961,11 +3018,12 @@ def _simple_blockify(tuples, ref_items, dtype, is_unique=True):
         values = values.astype(dtype)
 
     if is_unique:
-        placement=None
+        placement = None
     block = make_block(values, block_items, ref_items, placement=placement)
-    return [ block ]
+    return [block]
 
-def _multi_blockify(tuples, ref_items, dtype = None, is_unique=True):
+
+def _multi_blockify(tuples, ref_items, dtype=None, is_unique=True):
     """ return an array of blocks that potentially have different dtypes """
 
     # group by dtype
@@ -2974,30 +3032,34 @@ def _multi_blockify(tuples, ref_items, dtype = None, is_unique=True):
     new_blocks = []
     for dtype, tup_block in grouper:
 
-        block_items, values, placement = _stack_arrays(list(tup_block), ref_items, dtype)
+        block_items, values, placement = _stack_arrays(
+            list(tup_block), ref_items, dtype)
         if is_unique:
-            placement=None
+            placement = None
         block = make_block(values, block_items, ref_items, placement=placement)
         new_blocks.append(block)
 
     return new_blocks
 
-def _sparse_blockify(tuples, ref_items, dtype = None):
+
+def _sparse_blockify(tuples, ref_items, dtype=None):
     """ return an array of blocks that potentially have different dtypes (and are sparse) """
 
     new_blocks = []
     for i, names, array in tuples:
 
-        if not isinstance(names, (list,tuple)):
-            names = [ names ]
+        if not isinstance(names, (list, tuple)):
+            names = [names]
         items = ref_items[ref_items.isin(names)]
 
         array = _maybe_to_sparse(array)
-        block = make_block(array, items, ref_items, klass=SparseBlock, fastpath=True)
+        block = make_block(
+            array, items, ref_items, klass=SparseBlock, fastpath=True)
         new_blocks.append(block)
 
     return new_blocks
 
+
 def _stack_arrays(tuples, ref_items, dtype):
 
     # fml
@@ -3026,7 +3088,7 @@ def _stack_arrays(tuples, ref_items, dtype):
     if ref_items.is_unique:
         items = ref_items[ref_items.isin(names)]
     else:
-        items = _ensure_index([ n for n in names if n in ref_items ])
+        items = _ensure_index([n for n in names if n in ref_items])
         if len(items) != len(stacked):
             raise Exception("invalid names passed _stack_arrays")
 
@@ -3045,7 +3107,8 @@ def _blocks_to_series_dict(blocks, index=None):
 
 
 def _interleaved_dtype(blocks):
-    if not len(blocks): return None
+    if not len(blocks):
+        return None
 
     counts = defaultdict(lambda: [])
     for x in blocks:
@@ -3079,7 +3142,7 @@ def _interleaved_dtype(blocks):
         # if we are mixing unsigned and signed, then return
         # the next biggest int type (if we can)
         lcd = _lcd_dtype(counts[IntBlock])
-        kinds = set([ i.dtype.kind for i in counts[IntBlock] ])
+        kinds = set([i.dtype.kind for i in counts[IntBlock]])
         if len(kinds) == 1:
             return lcd
 
@@ -3088,7 +3151,7 @@ def _interleaved_dtype(blocks):
 
         # return 1 bigger on the itemsize if unsinged
         if lcd.kind == 'u':
-            return np.dtype('int%s' % (lcd.itemsize*8*2))
+            return np.dtype('int%s' % (lcd.itemsize * 8 * 2))
         return lcd
 
     elif have_dt64 and not have_float and not have_complex:
@@ -3098,6 +3161,7 @@ def _interleaved_dtype(blocks):
     else:
         return _lcd_dtype(counts[FloatBlock] + counts[SparseBlock])
 
+
 def _consolidate(blocks, items):
     """
     Merge blocks having same dtype, exclude non-consolidating blocks
@@ -3109,7 +3173,8 @@ def _consolidate(blocks, items):
 
     new_blocks = []
     for (_can_consolidate, dtype), group_blocks in grouper:
-        merged_blocks = _merge_blocks(list(group_blocks), items, dtype=dtype, _can_consolidate=_can_consolidate)
+        merged_blocks = _merge_blocks(
+            list(group_blocks), items, dtype=dtype, _can_consolidate=_can_consolidate)
         if isinstance(merged_blocks, list):
             new_blocks.extend(merged_blocks)
         else:
@@ -3118,19 +3183,18 @@ def _consolidate(blocks, items):
     return new_blocks
 
 
-
-def _merge_blocks(blocks, items, dtype=None, _can_consolidate = True):
+def _merge_blocks(blocks, items, dtype=None, _can_consolidate=True):
     if len(blocks) == 1:
         return blocks[0]
 
     if _can_consolidate:
 
         if dtype is None:
-            if len(set([ b.dtype for b in blocks ])) != 1:
+            if len(set([b.dtype for b in blocks])) != 1:
                 raise AssertionError("_merge_blocks are invalid!")
             dtype = blocks[0].dtype
 
-        new_values = _vstack([ b.values for b in blocks ], dtype)
+        new_values = _vstack([b.values for b in blocks], dtype)
         new_items = blocks[0].items.append([b.items for b in blocks[1:]])
         new_block = make_block(new_values, new_items, items)
 
@@ -3139,14 +3203,15 @@ def _merge_blocks(blocks, items, dtype=None, _can_consolidate = True):
             return new_block.reindex_items_from(items)
 
         # merge the ref_locs
-        new_ref_locs = [ b._ref_locs for b in blocks ]
-        if all([ x is not None for x in new_ref_locs ]):
+        new_ref_locs = [b._ref_locs for b in blocks]
+        if all([x is not None for x in new_ref_locs]):
             new_block.set_ref_locs(np.concatenate(new_ref_locs))
         return new_block
 
     # no merge
     return blocks
 
+
 def _block_shape(values, ndim=1, shape=None):
     """ guarantee the shape of the values to be at least 1 d """
     if values.ndim == ndim:
@@ -3155,6 +3220,7 @@ def _block_shape(values, ndim=1, shape=None):
         values = values.reshape(tuple((1,) + shape))
     return values
 
+
 def _vstack(to_stack, dtype):
 
     # work around NumPy 1.6 bug
@@ -3165,6 +3231,7 @@ def _vstack(to_stack, dtype):
     else:
         return np.vstack(to_stack)
 
+
 def _possibly_convert_to_indexer(loc):
     if com._is_bool_indexer(loc):
         loc = [i for i, v in enumerate(loc) if v]
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index 71648f55a..bca6f985a 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -145,6 +145,7 @@ def _comp_method(func, name):
 
 
 class Panel(NDFrame):
+
     """
     Represents wide format panel data, stored as 3-dimensional array
 
@@ -163,72 +164,9 @@ class Panel(NDFrame):
         Copy data from inputs. Only affects DataFrame / 2d ndarray input
     """
 
-<<<<<<< HEAD
-    _AXIS_ORDERS = ['items', 'major_axis', 'minor_axis']
-    _AXIS_NUMBERS = dict((a, i) for i, a in enumerate(_AXIS_ORDERS))
-    _AXIS_ALIASES = {
-        'major': 'major_axis',
-        'minor': 'minor_axis'
-    }
-    _AXIS_NAMES = dict(enumerate(_AXIS_ORDERS))
-    _AXIS_SLICEMAP = {
-        'major_axis': 'index',
-        'minor_axis': 'columns'
-    }
-    _AXIS_LEN = len(_AXIS_ORDERS)
-
-    # major
-    _default_stat_axis = 1
-
-    # info axis
-    _het_axis = 0
-    _info_axis = _AXIS_ORDERS[_het_axis]
-
-    items = lib.AxisProperty(0)
-    major_axis = lib.AxisProperty(1)
-    minor_axis = lib.AxisProperty(2)
-
-    # return the type of the slice constructor
-    _constructor_sliced = DataFrame
-
-    def _construct_axes_dict(self, axes=None, **kwargs):
-        """ Return an axes dictionary for myself """
-        d = dict([(a, getattr(self, a)) for a in (axes or self._AXIS_ORDERS)])
-        d.update(kwargs)
-        return d
-
-    @staticmethod
-    def _construct_axes_dict_from(self, axes, **kwargs):
-        """ Return an axes dictionary for the passed axes """
-        d = dict([(a, ax) for a, ax in zip(self._AXIS_ORDERS, axes)])
-        d.update(kwargs)
-        return d
-
-    def _construct_axes_dict_for_slice(self, axes=None, **kwargs):
-        """ Return an axes dictionary for myself """
-        d = dict([(self._AXIS_SLICEMAP[a], getattr(self, a))
-                 for a in (axes or self._AXIS_ORDERS)])
-        d.update(kwargs)
-        return d
-
-    __add__ = _arith_method(operator.add, '__add__')
-    __sub__ = _arith_method(operator.sub, '__sub__')
-    __truediv__ = _arith_method(operator.truediv, '__truediv__')
-    __floordiv__ = _arith_method(operator.floordiv, '__floordiv__')
-    __mul__ = _arith_method(operator.mul, '__mul__')
-    __pow__ = _arith_method(operator.pow, '__pow__')
-
-    __radd__ = _arith_method(operator.add, '__radd__')
-    __rmul__ = _arith_method(operator.mul, '__rmul__')
-    __rsub__ = _arith_method(lambda x, y: y - x, '__rsub__')
-    __rtruediv__ = _arith_method(lambda x, y: y / x, '__rtruediv__')
-    __rfloordiv__ = _arith_method(lambda x, y: y // x, '__rfloordiv__')
-    __rpow__ = _arith_method(lambda x, y: y ** x, '__rpow__')
-=======
     @property
     def _constructor(self):
         return type(self)
->>>>>>> ENH/CLN: refactor of common code from frame/panel to generic.py
 
     _constructor_sliced = DataFrame
 
@@ -267,12 +205,7 @@ class Panel(NDFrame):
         NDFrame.__init__(self, mgr, axes=axes, copy=copy, dtype=dtype)
 
     def _init_dict(self, data, axes, dtype=None):
-<<<<<<< HEAD
-        haxis = axes.pop(self._het_axis)
-=======
-        from pandas.util.compat import OrderedDict
         haxis = axes.pop(self._info_axis_number)
->>>>>>> ENH/CLN: refactor of common code from frame/panel to generic.py
 
         # prefilter if haxis passed
         if haxis is not None:
@@ -281,7 +214,7 @@ class Panel(NDFrame):
                                in compat.iteritems(data) if k in haxis)
         else:
             ks = list(data.keys())
-            if not isinstance(data,OrderedDict):
+            if not isinstance(data, OrderedDict):
                 ks = _try_sort(ks)
             haxis = Index(ks)
 
@@ -339,7 +272,6 @@ class Panel(NDFrame):
         -------
         Panel
         """
-
         orient = orient.lower()
         if orient == 'minor':
             new_data = OrderedDefaultdict(dict)
@@ -352,7 +284,7 @@ class Panel(NDFrame):
 
         d = cls._homogenize_dict(cls, data, intersect=intersect, dtype=dtype)
         ks = list(d['data'].keys())
-        if not isinstance(d['data'],OrderedDict):
+        if not isinstance(d['data'], OrderedDict):
             ks = list(sorted(ks))
         d[cls._info_axis_name] = Index(ks)
         return cls(**d)
@@ -416,7 +348,7 @@ class Panel(NDFrame):
                 ax = _ensure_index(ax)
             fixed_axes.append(ax)
 
-        return create_block_manager_from_blocks([ values ], fixed_axes)
+        return create_block_manager_from_blocks([values], fixed_axes)
 
     #----------------------------------------------------------------------
     # Comparison methods
@@ -602,7 +534,7 @@ class Panel(NDFrame):
             axes = self._expand_axes(args)
             d = self._construct_axes_dict_from(self, axes, copy=False)
             result = self.reindex(**d)
-            args  = list(args)
+            args = list(args)
             likely_dtype, args[-1] = _infer_dtype_from_scalar(args[-1])
             made_bigger = not np.array_equal(
                 axes[0], self._info_axis)
@@ -906,7 +838,7 @@ class Panel(NDFrame):
 
         # xs cannot handle a non-scalar key, so just reindex here
         if _is_list_like(key):
-            return self.reindex(**{ self._get_axis_name(axis) : key })
+            return self.reindex(**{self._get_axis_name(axis): key})
 
         return self.xs(key, axis=axis)
 
@@ -1192,7 +1124,7 @@ class Panel(NDFrame):
         if not isinstance(other, self._constructor):
             other = self._constructor(other)
 
-        axis_name   = self._info_axis_name
+        axis_name = self._info_axis_name
         axis_values = self._info_axis
         other = other.reindex(**{axis_name: axis_values})
 
@@ -1257,7 +1189,8 @@ class Panel(NDFrame):
         """
 
         result = dict()
-        if isinstance(frames,OrderedDict): # caller differs dict/ODict, presered type
+        # caller differs dict/ODict, presered type
+        if isinstance(frames, OrderedDict):
             result = OrderedDict()
 
         adj_frames = OrderedDict()
@@ -1366,7 +1299,7 @@ Return %(desc)s over requested axis
 Parameters
 ----------
 axis : {""" + ', '.join(cls._AXIS_ORDERS) + "} or {" \
-+ ', '.join([str(i) for i in range(cls._AXIS_LEN)]) + """}
+            + ', '.join([str(i) for i in range(cls._AXIS_LEN)]) + """}
 skipna : boolean, default True
     Exclude NA/null values. If an entire row/column is NA, the result
     will be NA
@@ -1440,13 +1373,13 @@ If all values are NA, result will be NA"""
             return self._reduce(nanops.nanmin, axis=axis, skipna=skipna)
         cls.min = min
 
-Panel._setup_axes(axes      = ['items', 'major_axis', 'minor_axis'],
-                  info_axis = 0,
-                  stat_axis = 1,
-                  aliases   = { 'major': 'major_axis',
-                                'minor': 'minor_axis' },
-                  slicers   = { 'major_axis': 'index',
-                                'minor_axis': 'columns' })
+Panel._setup_axes(axes=['items', 'major_axis', 'minor_axis'],
+                  info_axis=0,
+                  stat_axis=1,
+                  aliases={'major': 'major_axis',
+                           'minor': 'minor_axis'},
+                  slicers={'major_axis': 'index',
+                           'minor_axis': 'columns'})
 Panel._add_aggregate_operations()
 
 WidePanel = Panel
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 4836747ce..d0bca3ed0 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -22,8 +22,9 @@ from pandas.core.common import (isnull, notnull, _is_bool_indexer,
                                 is_sparse_array_like)
 from pandas.core.index import (Index, MultiIndex, InvalidIndexError,
                                _ensure_index, _handle_legacy_indexes)
-from pandas.core.indexing import (_SeriesIndexer, _check_bool_indexer, _check_slice_bounds,
-                                  _is_index_slice, _maybe_convert_indices)
+from pandas.core.indexing import (
+    _SeriesIndexer, _check_bool_indexer, _check_slice_bounds,
+    _is_index_slice, _maybe_convert_indices)
 from pandas.core import generic
 from pandas.core.internals import SingleBlockManager
 from pandas.tseries.index import DatetimeIndex
@@ -69,18 +70,18 @@ def _arith_method(op, name, fill_zeros=None):
         try:
 
             result = op(x, y)
-            result = com._fill_zeros(result,y,fill_zeros)
+            result = com._fill_zeros(result, y, fill_zeros)
 
         except TypeError:
             result = pa.empty(len(x), dtype=x.dtype)
-            if isinstance(y, (pa.Array,Series)):
+            if isinstance(y, (pa.Array, Series)):
                 mask = notnull(x) & notnull(y)
                 result[mask] = op(x[mask], y[mask])
             else:
                 mask = notnull(x)
                 result[mask] = op(x[mask], y)
 
-            result, changed = com._maybe_upcast_putmask(result,-mask,pa.NA)
+            result, changed = com._maybe_upcast_putmask(result, -mask, pa.NA)
 
         return result
 
@@ -243,19 +244,19 @@ def _arith_method(op, name, fill_zeros=None):
             arr = na_op(lvalues, rvalues)
 
             name = _maybe_match_name(self, other)
-            return self._constructor(wrap_results(arr), index=join_idx, name=name,dtype=dtype)
+            return self._constructor(wrap_results(arr), index=join_idx, name=name, dtype=dtype)
         elif isinstance(other, DataFrame):
             return NotImplemented
         else:
             # scalars
-            if hasattr(lvalues,'values'):
+            if hasattr(lvalues, 'values'):
                 lvalues = lvalues.values
             return self._constructor(wrap_results(na_op(lvalues, rvalues)),
-                          index=self.index, name=self.name, dtype=dtype)
+                                     index=self.index, name=self.name, dtype=dtype)
     return wrapper
 
 
-def _comp_method(op, name, masker = False):
+def _comp_method(op, name, masker=False):
     """
     Wrapper function for Series arithmetic operations, to avoid
     code duplication.
@@ -265,7 +266,7 @@ def _comp_method(op, name, masker = False):
             if isinstance(y, list):
                 y = lib.list_to_object_array(y)
 
-            if isinstance(y, (pa.Array,Series)):
+            if isinstance(y, (pa.Array, Series)):
                 if y.dtype != np.object_:
                     result = lib.vec_compare(x, y.astype(np.object_), op)
                 else:
@@ -285,14 +286,14 @@ def _comp_method(op, name, masker = False):
             if len(self) != len(other):
                 raise ValueError('Series lengths must match to compare')
             return self._constructor(na_op(self.values, other.values),
-                          index=self.index, name=name)
+                                     index=self.index, name=name)
         elif isinstance(other, DataFrame):  # pragma: no cover
             return NotImplemented
-        elif isinstance(other, (pa.Array,Series)):
+        elif isinstance(other, (pa.Array, Series)):
             if len(self) != len(other):
                 raise ValueError('Lengths must match to compare')
             return self._constructor(na_op(self.values, np.asarray(other)),
-                          index=self.index, name=self.name)
+                                     index=self.index, name=self.name)
         else:
 
             mask = isnull(self)
@@ -334,7 +335,7 @@ def _bool_method(op, name):
             if isinstance(y, list):
                 y = lib.list_to_object_array(y)
 
-            if isinstance(y, (pa.Array,Series)):
+            if isinstance(y, (pa.Array, Series)):
                 if (x.dtype == np.bool_ and
                         y.dtype == np.bool_):  # pragma: no cover
                     result = op(x, y)  # when would this be hit?
@@ -353,13 +354,13 @@ def _bool_method(op, name):
         if isinstance(other, Series):
             name = _maybe_match_name(self, other)
             return self._constructor(na_op(self.values, other.values),
-                          index=self.index, name=name)
+                                     index=self.index, name=name)
         elif isinstance(other, DataFrame):
             return NotImplemented
         else:
             # scalars
             return self._constructor(na_op(self.values, other),
-                          index=self.index, name=self.name)
+                                     index=self.index, name=self.name)
     return wrapper
 
 
@@ -379,15 +380,18 @@ def _radd_compat(left, right):
 
     return output
 
+
 def _coerce_method(converter):
     """ install the scalar coercion methods """
 
     def wrapper(self):
         if len(self) == 1:
             return converter(self.iloc[0])
-        raise TypeError("cannot convert the series to {0}".format(str(converter)))
+        raise TypeError(
+            "cannot convert the series to {0}".format(str(converter)))
     return wrapper
 
+
 def _maybe_match_name(a, b):
     name = None
     if a.name == b.name:
@@ -426,7 +430,7 @@ def _flex_method(op, name):
                                level=level, fill_value=fill_value)
         else:
             return self._constructor(op(self.values, other), self.index,
-                          name=self.name)
+                                     name=self.name)
 
     f.__name__ = name
     return f
@@ -480,6 +484,8 @@ def _make_stat_func(nanop, name, shortname, na_action=_doc_exclude_na,
 
 #----------------------------------------------------------------------
 # Series class
+
+
 class Series(generic.NDFrame):
 
     """
@@ -508,7 +514,7 @@ class Series(generic.NDFrame):
         If None, dtype will be inferred
     copy : boolean, default False, copy input data
     """
-    _prop_attributes    = ['name']
+    _prop_attributes = ['name']
 
     def __init__(self, data=None, index=None, dtype=None, name=None,
                  copy=False, fastpath=False):
@@ -582,7 +588,7 @@ class Series(generic.NDFrame):
 
             if index is None:
                 if not is_list_like(data):
-                    data = [ data ]
+                    data = [data]
                 index = _default_index(len(data))
 
             # create/copy the manager
@@ -597,11 +603,10 @@ class Series(generic.NDFrame):
 
                 data = SingleBlockManager(data, index, fastpath=True)
 
-
         generic.NDFrame.__init__(self, data, fastpath=True)
 
-        object.__setattr__(self,'name',name)
-        self._set_axis(0,index,fastpath=True)
+        object.__setattr__(self, 'name', name)
+        self._set_axis(0, index, fastpath=True)
 
     @classmethod
     def from_array(cls, arr, index=None, name=None, copy=False, fastpath=False):
@@ -624,7 +629,7 @@ class Series(generic.NDFrame):
 
     @property
     def is_time_series(self):
-        return self._subtyp in ['time_series','sparse_time_series']
+        return self._subtyp in ['time_series', 'sparse_time_series']
 
     _index = None
 
@@ -646,15 +651,15 @@ class Series(generic.NDFrame):
                     self._data.set_axis(axis, labels)
         self._set_subtyp(is_all_dates)
 
-        object.__setattr__(self,'_index',labels)
+        object.__setattr__(self, '_index', labels)
         if not fastpath:
             self._data.set_axis(axis, labels)
 
     def _set_subtyp(self, is_all_dates):
         if is_all_dates:
-            object.__setattr__(self,'_subtyp','time_series')
+            object.__setattr__(self, '_subtyp', 'time_series')
         else:
-            object.__setattr__(self,'_subtyp','series')
+            object.__setattr__(self, '_subtyp', 'series')
 
     # ndarray compatibility
     @property
@@ -704,10 +709,10 @@ class Series(generic.NDFrame):
     def size(self):
         return self.__len__()
 
-    def view(self, dtype = None):
-        return self._constructor(self.values.view(dtype),index=self.index,name=self.name)
+    def view(self, dtype=None):
+        return self._constructor(self.values.view(dtype), index=self.index, name=self.name)
 
-    def __array__(self, result = None):
+    def __array__(self, result=None):
         """ the array interface, return my values """
         return self.values
 
@@ -734,12 +739,12 @@ class Series(generic.NDFrame):
 
     # we are preserving name here
     def __getstate__(self):
-        return dict(_data = self._data, name = self.name)
+        return dict(_data=self._data, name=self.name)
 
     def _unpickle_series_compat(self, state):
         if isinstance(state, dict):
             self._data = state['_data']
-            self.name  = state['name']
+            self.name = state['name']
             self.index = self._data.index
 
         elif isinstance(state, tuple):
@@ -749,7 +754,7 @@ class Series(generic.NDFrame):
             nd_state, own_state = state
 
             # recreate the ndarray
-            data = np.empty(nd_state[1],dtype=nd_state[2])
+            data = np.empty(nd_state[1], dtype=nd_state[2])
             np.ndarray.__setstate__(data, nd_state)
 
             # backwards compat
@@ -769,14 +774,14 @@ class Series(generic.NDFrame):
     # indexers
     @property
     def axes(self):
-        return [ self.index ]
+        return [self.index]
 
     def _maybe_box(self, values):
         """ genericically box the values """
 
-        if isinstance(values,self.__class__):
+        if isinstance(values, self.__class__):
             return values
-        elif not hasattr(values,'__iter__'):
+        elif not hasattr(values, '__iter__'):
             v = lib.infer_dtype([values])
             if v == 'datetime':
                 return lib.Timestamp(v)
@@ -786,7 +791,7 @@ class Series(generic.NDFrame):
         if v == 'datetime':
             return lib.map_infer(values, lib.Timestamp)
 
-        if isinstance(values,np.ndarray):
+        if isinstance(values, np.ndarray):
             return self.__class__(values)
 
         return values
@@ -839,7 +844,7 @@ class Series(generic.NDFrame):
             return self.index.get_value(self, key)
         except InvalidIndexError:
             pass
-        except (KeyError,ValueError):
+        except (KeyError, ValueError):
             if isinstance(key, tuple) and isinstance(self.index, MultiIndex):
                 # kludge
                 pass
@@ -883,7 +888,8 @@ class Series(generic.NDFrame):
                             return self._get_values(key)
                     raise
 
-            if not isinstance(key, (list, pa.Array, Series)):  # pragma: no cover
+            # pragma: no cover
+            if not isinstance(key, (list, pa.Array, Series)):
                 key = list(key)
 
             if isinstance(key, Index):
@@ -901,7 +907,7 @@ class Series(generic.NDFrame):
             else:
                 try:
                     # handle the dup indexing case (GH 4246)
-                    if isinstance(key, (list,tuple)):
+                    if isinstance(key, (list, tuple)):
                         return self.ix[key]
 
                     return self.reindex(key)
@@ -928,7 +934,7 @@ class Series(generic.NDFrame):
     def _get_values(self, indexer):
         try:
             return self._constructor(self._data.get_slice(indexer),
-                                     name=self.name,fastpath=True)
+                                     name=self.name, fastpath=True)
         except Exception:
             return self.values[indexer]
 
@@ -936,7 +942,7 @@ class Series(generic.NDFrame):
         try:
             self._set_with_engine(key, value)
             return
-        except (KeyError,ValueError):
+        except (KeyError, ValueError):
             values = self.values
             if (com.is_integer(key)
                     and not self.index.inferred_type == 'integer'):
@@ -969,7 +975,7 @@ class Series(generic.NDFrame):
 
         if _is_bool_indexer(key):
             key = _check_bool_indexer(self.index, key)
-            self.where(~key,value,inplace=True)
+            self.where(~key, value, inplace=True)
         else:
             self._set_with(key, value)
 
@@ -1065,8 +1071,9 @@ class Series(generic.NDFrame):
         """
         See numpy.ndarray.reshape
         """
-        if order not in ['C','F']:
-            raise TypeError("must specify a tuple / singular length to reshape")
+        if order not in ['C', 'F']:
+            raise TypeError(
+                "must specify a tuple / singular length to reshape")
 
         if isinstance(newshape, tuple) and len(newshape) > 1:
             return self.values.reshape(newshape, order=order)
@@ -1178,7 +1185,7 @@ class Series(generic.NDFrame):
                 self.name = name or self.name
             else:
                 return self._constructor(self.values.copy(), index=new_index,
-                              name=self.name)
+                                         name=self.name)
         elif inplace:
             raise TypeError('Cannot reset_index inplace on a Series '
                             'to create a DataFrame')
@@ -1298,8 +1305,9 @@ class Series(generic.NDFrame):
                 with open(buf, 'w') as f:
                     f.write(the_repr)
 
-    def _get_repr(self, name=False, print_header=False, length=True, dtype=True,
-                  na_rep='NaN', float_format=None):
+    def _get_repr(
+        self, name=False, print_header=False, length=True, dtype=True,
+            na_rep='NaN', float_format=None):
         """
 
         Internal function, should always return unicode string
@@ -1339,16 +1347,20 @@ class Series(generic.NDFrame):
     __add__ = _arith_method(operator.add, '__add__')
     __sub__ = _arith_method(operator.sub, '__sub__')
     __mul__ = _arith_method(operator.mul, '__mul__')
-    __truediv__ = _arith_method(operator.truediv, '__truediv__', fill_zeros=np.inf)
-    __floordiv__ = _arith_method(operator.floordiv, '__floordiv__', fill_zeros=np.inf)
+    __truediv__ = _arith_method(
+        operator.truediv, '__truediv__', fill_zeros=np.inf)
+    __floordiv__ = _arith_method(
+        operator.floordiv, '__floordiv__', fill_zeros=np.inf)
     __pow__ = _arith_method(operator.pow, '__pow__')
     __mod__ = _arith_method(operator.mod, '__mod__', fill_zeros=np.nan)
 
     __radd__ = _arith_method(_radd_compat, '__add__')
     __rmul__ = _arith_method(operator.mul, '__mul__')
     __rsub__ = _arith_method(lambda x, y: y - x, '__sub__')
-    __rtruediv__ = _arith_method(lambda x, y: y / x, '__truediv__', fill_zeros=np.inf)
-    __rfloordiv__ = _arith_method(lambda x, y: y // x, '__floordiv__', fill_zeros=np.inf)
+    __rtruediv__ = _arith_method(
+        lambda x, y: y / x, '__truediv__', fill_zeros=np.inf)
+    __rfloordiv__ = _arith_method(
+        lambda x, y: y // x, '__floordiv__', fill_zeros=np.inf)
     __rpow__ = _arith_method(lambda x, y: y ** x, '__pow__')
     __rmod__ = _arith_method(lambda x, y: y % x, '__mod__', fill_zeros=np.nan)
 
@@ -1385,7 +1397,8 @@ class Series(generic.NDFrame):
     # Python 2 division operators
     if not compat.PY3:
         __div__ = _arith_method(operator.div, '__div__', fill_zeros=np.inf)
-        __rdiv__ = _arith_method(lambda x, y: y / x, '__div__', fill_zeros=np.inf)
+        __rdiv__ = _arith_method(
+            lambda x, y: y / x, '__div__', fill_zeros=np.inf)
         __idiv__ = __div__
 
     #----------------------------------------------------------------------
@@ -1884,7 +1897,8 @@ class Series(generic.NDFrame):
         """
         result = _values_from_object(self).round(decimals, out=out)
         if out is None:
-            result = self._constructor(result, index=self.index, name=self.name)
+            result = self._constructor(
+                result, index=self.index, name=self.name)
 
         return result
 
@@ -1970,7 +1984,7 @@ class Series(generic.NDFrame):
                       pretty_name(ub), 'max']
             data += [self.mean(), self.std(), self.min(),
                      self.quantile(
-                     lb), self.median(), self.quantile(ub),
+                         lb), self.median(), self.quantile(ub),
                      self.max()]
 
         return self._constructor(data, index=names)
@@ -2292,7 +2306,7 @@ class Series(generic.NDFrame):
         """
         other = other.reindex_like(self)
         mask = notnull(other)
-        com._maybe_upcast_putmask(self.values,mask,other,change=self.values)
+        com._maybe_upcast_putmask(self.values, mask, other, change=self.values)
 
     #----------------------------------------------------------------------
     # Reindexing, sorting
@@ -2385,13 +2399,15 @@ class Series(generic.NDFrame):
         mask = isnull(values)
 
         if mask.any():
-            result = Series(-1,index=self.index,name=self.name,dtype='int64')
+            result = Series(
+                -1, index=self.index, name=self.name, dtype='int64')
             notmask = -mask
             result[notmask] = np.argsort(values[notmask], kind=kind)
             return self._constructor(result, index=self.index, name=self.name)
         else:
-            return self._constructor(np.argsort(values, kind=kind), index=self.index,
-                          name=self.name,dtype='int64')
+            return self._constructor(
+                np.argsort(values, kind=kind), index=self.index,
+                name=self.name, dtype='int64')
 
     def rank(self, method='average', na_option='keep', ascending=True):
         """
@@ -2470,7 +2486,7 @@ class Series(generic.NDFrame):
             sortedIdx[:n] = idx[bad]
 
         return self._constructor(arr[sortedIdx], index=self.index[sortedIdx],
-                      name=self.name)
+                                 name=self.name)
 
     def sortlevel(self, level=0, ascending=True):
         """
@@ -2712,7 +2728,7 @@ class Series(generic.NDFrame):
 
         def _rep_one(s, to_rep, v):  # replace single value
             mask = com.mask_missing(s.values, to_rep)
-            com._maybe_upcast_putmask(s.values,mask,v,change=change)
+            com._maybe_upcast_putmask(s.values, mask, v, change=change)
 
         def _rep_dict(rs, to_rep):  # replace {[src] -> dest}
 
@@ -2729,13 +2745,11 @@ class Series(generic.NDFrame):
                     masks[d] = com.mask_missing(rs.values, sset)
 
                 for d, m in masks.iteritems():
-                    com._maybe_upcast_putmask(rs.values,m,d,change=change)
+                    com._maybe_upcast_putmask(rs.values, m, d, change=change)
             else:  # if no risk of clobbering then simple
                 for d, sset in dd.iteritems():
                     _rep_one(rs, sset, d)
 
-
-
         if np.isscalar(to_replace):
             to_replace = [to_replace]
 
@@ -2743,7 +2757,8 @@ class Series(generic.NDFrame):
             _rep_dict(result, to_replace)
         elif isinstance(to_replace, (list, pa.Array, Series)):
 
-            if isinstance(value, (list, pa.Array, Series)):  # check same length
+            # check same length
+            if isinstance(value, (list, pa.Array, Series)):
                 vl, rl = len(value), len(to_replace)
                 if vl == rl:
                     _rep_dict(result, dict(zip(to_replace, value)))
@@ -2886,7 +2901,7 @@ class Series(generic.NDFrame):
         """ for compatibility with higher dims """
         if axis != 0:
             raise ValueError("cannot reindex series on non-zero axis!")
-        return self.reindex(index=labels,**kwargs)
+        return self.reindex(index=labels, **kwargs)
 
     def take(self, indices, axis=0, convert=True):
         """
@@ -2904,7 +2919,8 @@ class Series(generic.NDFrame):
         """
         # check/convert indicies here
         if convert:
-            indices = _maybe_convert_indices(indices, len(self._get_axis(axis)))
+            indices = _maybe_convert_indices(
+                indices, len(self._get_axis(axis)))
 
         indices = com._ensure_platform_int(indices)
         new_index = self.index.take(indices)
@@ -3113,15 +3129,16 @@ class Series(generic.NDFrame):
         elif isinstance(self.index, PeriodIndex):
             orig_offset = datetools.to_offset(self.index.freq)
             if orig_offset == offset:
-                return self._constructor(_get_values(), self.index.shift(periods),
-                              name=self.name)
+                return self._constructor(
+                    _get_values(), self.index.shift(periods),
+                    name=self.name)
             msg = ('Given freq %s does not match PeriodIndex freq %s' %
                    (offset.rule_code, orig_offset.rule_code))
             raise ValueError(msg)
         else:
             return self._constructor(_get_values(),
-                          index=self.index.shift(periods, offset),
-                          name=self.name)
+                                     index=self.index.shift(periods, offset),
+                                     name=self.name)
 
     def asof(self, where):
         """
@@ -3256,7 +3273,8 @@ class Series(generic.NDFrame):
         """
         mapper_f = _get_rename_function(mapper)
         result = self if inplace else self.copy()
-        result.index = Index([mapper_f(x) for x in self.index], name=self.index.name)
+        result.index = Index([mapper_f(x)
+                             for x in self.index], name=self.index.name)
 
         if not inplace:
             return result
@@ -3376,7 +3394,8 @@ Series._setup_axes(['index'], info_axis=0)
 _INDEX_TYPES = ndarray, Index, list, tuple
 
 # reinstall the SeriesIndexer
-Series._create_indexer('ix',_SeriesIndexer) # defined in indexing.py; pylint: disable=E0203
+# defined in indexing.py; pylint: disable=E0203
+Series._create_indexer('ix', _SeriesIndexer)
 
 #------------------------------------------------------------------------------
 # Supplementary functions
@@ -3388,6 +3407,7 @@ def remove_na(series):
     """
     return series[notnull(_values_from_object(series))]
 
+
 def _sanitize_array(data, index, dtype=None, copy=False,
                     raise_cast_failure=False):
     if dtype is not None:
@@ -3558,4 +3578,3 @@ import pandas.tools.plotting as _gfx
 
 Series.plot = _gfx.plot_series
 Series.hist = _gfx.hist_series
-
diff --git a/pandas/sparse/api.py b/pandas/sparse/api.py
index a2ff9be81..230ad1593 100644
--- a/pandas/sparse/api.py
+++ b/pandas/sparse/api.py
@@ -5,4 +5,3 @@ from pandas.sparse.list import SparseList
 from pandas.sparse.series import SparseSeries, SparseTimeSeries
 from pandas.sparse.frame import SparseDataFrame
 from pandas.sparse.panel import SparsePanel
-
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index 10adb8245..336dcc004 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -24,6 +24,7 @@ def _sparse_op_wrap(op, name):
     Wrapper function for Series arithmetic operations, to avoid
     code duplication.
     """
+
     def wrapper(self, other):
         if isinstance(other, np.ndarray):
             if not ((len(self) == len(other))):
@@ -87,7 +88,9 @@ def _sparse_fillop(this, other, name):
 
     return result, result_index
 
+
 class SparseArray(PandasObject, np.ndarray):
+
     """Data structure for labeled, sparse floating point data
 
 Parameters
@@ -112,15 +115,16 @@ to sparse
     sp_index = None
     fill_value = None
 
-    def __new__(cls, data, sparse_index=None, index=None, kind='integer', fill_value=None,
-                dtype=np.float64, copy=False):
+    def __new__(
+        cls, data, sparse_index=None, index=None, kind='integer', fill_value=None,
+            dtype=np.float64, copy=False):
 
         if index is not None:
             if data is None:
                 data = np.nan
             if not np.isscalar(data):
                 raise Exception("must only pass scalars with an index ")
-            values = np.empty(len(index),dtype='float64')
+            values = np.empty(len(index), dtype='float64')
             values.fill(data)
             data = values
 
@@ -152,9 +156,8 @@ to sparse
         else:
             subarr = np.asarray(values, dtype=dtype)
 
-
         # if we have a bool type, make sure that we have a bool fill_value
-        if (dtype is not None and issubclass(dtype.type,np.bool_)) or (data is not None and lib.is_bool_array(subarr)):
+        if (dtype is not None and issubclass(dtype.type, np.bool_)) or (data is not None and lib.is_bool_array(subarr)):
             if np.isnan(fill_value) or not fill_value:
                 fill_value = False
             else:
@@ -335,7 +338,8 @@ to sparse
             raise IndexError('out of bounds access')
 
         if self.sp_index.npoints > 0:
-            locs = np.array([self.sp_index.lookup(loc) if loc > -1 else -1 for loc in indices ])
+            locs = np.array(
+                [self.sp_index.lookup(loc) if loc > -1 else -1 for loc in indices])
             result = self.sp_values.take(locs)
             mask = locs == -1
             if mask.any():
@@ -353,11 +357,12 @@ to sparse
         return result
 
     def __setitem__(self, key, value):
-        #if com.is_integer(key):
+        # if com.is_integer(key):
         #    self.values[key] = value
-        #else:
+        # else:
         #    raise Exception("SparseArray does not support seting non-scalars via setitem")
-        raise TypeError("SparseArray does not support item assignment via setitem")
+        raise TypeError(
+            "SparseArray does not support item assignment via setitem")
 
     def __setslice__(self, i, j, value):
         if i < 0:
@@ -366,13 +371,14 @@ to sparse
             j = 0
         slobj = slice(i, j)
 
-        #if not np.isscalar(value):
+        # if not np.isscalar(value):
         #    raise Exception("SparseArray does not support seting non-scalars via slices")
 
         #x = self.values
         #x[slobj] = value
         #self.values = x
-        raise TypeError("SparseArray does not support item assignment via slices")
+        raise TypeError(
+            "SparseArray does not support item assignment via slices")
 
     def astype(self, dtype=None):
         """
@@ -393,7 +399,7 @@ to sparse
         else:
             values = self.sp_values
         return SparseArray(values, sparse_index=self.sp_index,
-                           dtype = self.dtype,
+                           dtype=self.dtype,
                            fill_value=self.fill_value)
 
     def count(self):
@@ -477,17 +483,20 @@ to sparse
 
 def _maybe_to_dense(obj):
     """ try to convert to dense """
-    if hasattr(obj,'to_dense'):
+    if hasattr(obj, 'to_dense'):
         return obj.to_dense()
     return obj
 
+
 def _maybe_to_sparse(array):
     if com.is_sparse_series(array):
-        array = SparseArray(array.values,sparse_index=array.sp_index,fill_value=array.fill_value,copy=True)
+        array = SparseArray(
+            array.values, sparse_index=array.sp_index, fill_value=array.fill_value, copy=True)
     if not isinstance(array, SparseArray):
         array = com._values_from_object(array)
     return array
 
+
 def make_sparse(arr, kind='block', fill_value=nan):
     """
     Convert ndarray to sparse format
@@ -502,11 +511,11 @@ def make_sparse(arr, kind='block', fill_value=nan):
     -------
     (sparse_values, index) : (ndarray, SparseIndex)
     """
-    if hasattr(arr,'values'):
+    if hasattr(arr, 'values'):
         arr = arr.values
     else:
         if np.isscalar(arr):
-            arr = [ arr ]
+            arr = [arr]
         arr = np.asarray(arr)
 
     length = len(arr)
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index e282a89f8..e3968c540 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -23,11 +23,13 @@ import pandas.core.datetools as datetools
 from pandas.core.internals import BlockManager, create_block_manager_from_arrays
 
 from pandas.core.generic import NDFrame
-from pandas.sparse.series import SparseSeries,SparseArray
+from pandas.sparse.series import SparseSeries, SparseArray
 from pandas.util.decorators import Appender
 import pandas.lib as lib
 
+
 class SparseDataFrame(DataFrame):
+
     """
     DataFrame containing sparse floating point data in the form of SparseSeries
     objects
@@ -62,16 +64,16 @@ class SparseDataFrame(DataFrame):
                 default_fill_value = data.default_fill_value
             if default_kind is None:
                 default_kind = data.default_kind
-        elif isinstance(data, (SparseSeries,SparseArray)):
+        elif isinstance(data, (SparseSeries, SparseArray)):
             if index is None:
                 index = data.index
             if default_fill_value is None:
                 default_fill_value = data.fill_value
-            if columns is None and hasattr(data,'name'):
-                columns = [ data.name ]
+            if columns is None and hasattr(data, 'name'):
+                columns = [data.name]
             if columns is None:
                 raise Exception("cannot pass a series w/o a name or columns")
-            data = { columns[0] : data }
+            data = {columns[0]: data}
 
         if default_fill_value is None:
             default_fill_value = np.nan
@@ -90,13 +92,15 @@ class SparseDataFrame(DataFrame):
             if dtype is not None:
                 mgr = mgr.astype(dtype)
         elif isinstance(data, SparseDataFrame):
-            mgr = self._init_mgr(data._data, dict(index=index, columns=columns), dtype=dtype, copy=copy)
+            mgr = self._init_mgr(
+                data._data, dict(index=index, columns=columns), dtype=dtype, copy=copy)
         elif isinstance(data, DataFrame):
             mgr = self._init_dict(data, data.index, data.columns)
             if dtype is not None:
                 mgr = mgr.astype(dtype)
         elif isinstance(data, BlockManager):
-            mgr = self._init_mgr(data, axes = dict(index=index, columns=columns), dtype=dtype, copy=copy)
+            mgr = self._init_mgr(
+                data, axes=dict(index=index, columns=columns), dtype=dtype, copy=copy)
         elif data is None:
             data = {}
 
@@ -129,10 +133,10 @@ class SparseDataFrame(DataFrame):
 
             # fill if requested
             if fill_value is not None and not isnull(fill_value):
-                result.fillna(fill_value,inplace=True)
+                result.fillna(fill_value, inplace=True)
 
             # set the default_fill_value
-            #if default_fill_value is not None:
+            # if default_fill_value is not None:
             #    result._default_fill_value = default_fill_value
             return result
 
@@ -206,11 +210,11 @@ class SparseDataFrame(DataFrame):
 
     def __getstate__(self):
         # pickling
-        return dict(_typ                = self._typ,
-                    _subtyp             = self._subtyp,
-                    _data               = self._data,
-                    _default_fill_value = self._default_fill_value,
-                    _default_kind       = self._default_kind)
+        return dict(_typ=self._typ,
+                    _subtyp=self._subtyp,
+                    _data=self._data,
+                    _default_fill_value=self._default_fill_value,
+                    _default_kind=self._default_kind)
 
     def _unpickle_sparse_frame_compat(self, state):
         """ original pickle format """
@@ -280,12 +284,14 @@ class SparseDataFrame(DataFrame):
 
     def fillna(self, value=None, method=None, axis=0, inplace=False,
                limit=None, downcast=None):
-        new_self = super(SparseDataFrame, self).fillna(value=value, method=method, axis=axis,
-                                                       inplace=inplace, limit=limit, downcast=downcast)
+        new_self = super(
+            SparseDataFrame, self).fillna(value=value, method=method, axis=axis,
+                                          inplace=inplace, limit=limit, downcast=downcast)
         if not inplace:
             self = new_self
 
-        # set the fill value if we are filling as a scalar with nothing special going on
+        # set the fill value if we are filling as a scalar with nothing special
+        # going on
         if value is not None and value == value and method is None and limit is None:
             self._default_fill_value = value
 
@@ -297,12 +303,13 @@ class SparseDataFrame(DataFrame):
 
     def _sanitize_column(self, key, value):
         sp_maker = lambda x, index=None: SparseArray(x,
-                                         index=index,
-                                         fill_value=self._default_fill_value,
-                                         kind=self._default_kind)
+                                                     index=index,
+                                                     fill_value=self._default_fill_value,
+                                                     kind=self._default_kind)
         if isinstance(value, SparseSeries):
-            clean = value.reindex(self.index).as_sparse_array(fill_value=self._default_fill_value,
-                                                              kind=self._default_kind)
+            clean = value.reindex(
+                self.index).as_sparse_array(fill_value=self._default_fill_value,
+                                            kind=self._default_kind)
 
         elif isinstance(value, SparseArray):
             if len(value) != len(self.index):
@@ -323,7 +330,7 @@ class SparseDataFrame(DataFrame):
 
         # Scalar
         else:
-            clean = sp_maker(value,self.index)
+            clean = sp_maker(value, self.index)
 
         # always return a SparseArray!
         return clean
@@ -435,7 +442,7 @@ class SparseDataFrame(DataFrame):
                     new_data[col] = func(this[col], other[col])
 
         # if the fill values are the same use them? or use a valid one
-        other_fill_value = getattr(other,'default_fill_value',np.nan)
+        other_fill_value = getattr(other, 'default_fill_value', np.nan)
         if self.default_fill_value == other_fill_value:
             new_fill_value = self.default_fill_value
         elif np.isnan(self.default_fill_value) and not np.isnan(other_fill_value):
@@ -569,8 +576,8 @@ class SparseDataFrame(DataFrame):
 
     def _reindex_with_indexers(self, reindexers, method=None, copy=False, fill_value=np.nan):
 
-        index,   row_indexer = reindexers.get(0,(None,None))
-        columns, col_indexer = reindexers.get(1,(None, None))
+        index,   row_indexer = reindexers.get(0, (None, None))
+        columns, col_indexer = reindexers.get(1, (None, None))
 
         if columns is None:
             columns = self.columns
@@ -580,8 +587,9 @@ class SparseDataFrame(DataFrame):
             if col not in self:
                 continue
             if row_indexer is not None:
-                new_arrays[col] = com.take_1d(self[col].get_values(), row_indexer,
-                                              fill_value=fill_value)
+                new_arrays[col] = com.take_1d(
+                    self[col].get_values(), row_indexer,
+                    fill_value=fill_value)
             else:
                 new_arrays[col] = self[col]
 
@@ -614,7 +622,7 @@ class SparseDataFrame(DataFrame):
         this, other = this._maybe_rename_join(other, lsuffix, rsuffix)
 
         from pandas import concat
-        return concat([this,other],axis=1,verify_integrity=True)
+        return concat([this, other], axis=1, verify_integrity=True)
 
     def _maybe_rename_join(self, other, lsuffix, rsuffix):
         intersection = self.columns.intersection(other.columns)
@@ -728,7 +736,8 @@ def dict_to_manager(sdict, columns, index):
     # from BlockManager perspective
     axes = [_ensure_index(columns), _ensure_index(index)]
 
-    return create_block_manager_from_arrays([ sdict[c] for c in columns ], columns, axes)
+    return create_block_manager_from_arrays([sdict[c] for c in columns], columns, axes)
+
 
 def stack_sparse_frame(frame):
     """
diff --git a/pandas/sparse/list.py b/pandas/sparse/list.py
index ceb03eae5..bfc4ab9d3 100644
--- a/pandas/sparse/list.py
+++ b/pandas/sparse/list.py
@@ -7,6 +7,7 @@ import pandas._sparse as splib
 
 
 class SparseList(PandasObject):
+
     """
     Data structure for accumulating data to be converted into a
     SparseArray. Has similar API to the standard Python list
@@ -16,6 +17,7 @@ class SparseList(PandasObject):
     data : scalar or array-like
     fill_value : scalar, default NaN
     """
+
     def __init__(self, data=None, fill_value=np.nan):
         self.fill_value = fill_value
         self._chunks = []
diff --git a/pandas/sparse/panel.py b/pandas/sparse/panel.py
index 3d3196b6b..ab946090c 100644
--- a/pandas/sparse/panel.py
+++ b/pandas/sparse/panel.py
@@ -40,6 +40,7 @@ class SparsePanelAxis(object):
 
 
 class SparsePanel(Panel):
+
     """
     Sparse version of Panel
 
@@ -79,7 +80,6 @@ class SparsePanel(Panel):
         if not (isinstance(frames, dict)):
             raise AssertionError()
 
-
         self.default_fill_value = fill_value = default_fill_value
         self.default_kind = kind = default_kind
 
@@ -235,7 +235,6 @@ class SparsePanel(Panel):
         self._minor_axis = _ensure_index(com._unpickle_array(minor))
         self._frames = frames
 
-
     def copy(self):
         """
         Make a (shallow) copy of the sparse panel
@@ -336,7 +335,7 @@ class SparsePanel(Panel):
                     new_frames[item] = self._frames[item]
                 else:
                     raise NotImplementedError('Reindexing with new items not yet '
-                                    'supported')
+                                              'supported')
         else:
             new_frames = self._frames
 
diff --git a/pandas/sparse/series.py b/pandas/sparse/series.py
index f321fcd48..6d7e4994f 100644
--- a/pandas/sparse/series.py
+++ b/pandas/sparse/series.py
@@ -38,6 +38,7 @@ def _sparse_op_wrap(op, name):
     Wrapper function for Series arithmetic operations, to avoid
     code duplication.
     """
+
     def wrapper(self, other):
         if isinstance(other, Series):
             if not isinstance(other, SparseSeries):
@@ -72,7 +73,9 @@ def _sparse_series_op(left, right, op, name):
     result = _sparse_array_op(left, right, op, name)
     return SparseSeries(result, index=new_index, name=new_name)
 
+
 class SparseSeries(Series):
+
     """Data structure for labeled, sparse floating point data
 
     Parameters
@@ -140,7 +143,7 @@ class SparseSeries(Series):
                 # array-like
                 if sparse_index is None:
                     data, sparse_index = make_sparse(data, kind=kind,
-                                                       fill_value=fill_value)
+                                                     fill_value=fill_value)
                 else:
                     assert(len(data) == sparse_index.npoints)
 
@@ -150,7 +153,7 @@ class SparseSeries(Series):
                 if index is None:
                     index = data.index
                 else:
-                    data = data.reindex(index,copy=False)
+                    data = data.reindex(index, copy=False)
 
             else:
 
@@ -187,14 +190,15 @@ class SparseSeries(Series):
 
                 # create a sparse array
                 if not isinstance(data, SparseArray):
-                    data = SparseArray(data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype, copy=copy)
+                    data = SparseArray(
+                        data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype, copy=copy)
 
                 data = SingleBlockManager(data, index)
 
         generic.NDFrame.__init__(self, data)
 
         self.index = index
-        self.name  = name
+        self.name = name
 
     @property
     def values(self):
@@ -299,7 +303,6 @@ class SparseSeries(Series):
         __div__ = _sparse_op_wrap(operator.div, 'div')
         __rdiv__ = _sparse_op_wrap(lambda x, y: y / x, '__rdiv__')
 
-
     def __array_wrap__(self, result):
         """
         Gets called prior to a ufunc (and after)
@@ -320,18 +323,18 @@ class SparseSeries(Series):
 
     def __getstate__(self):
         # pickling
-        return dict(_typ       = self._typ,
-                    _subtyp    = self._subtyp,
-                    _data      = self._data,
-                    fill_value = self.fill_value,
-                    name       = self.name)
+        return dict(_typ=self._typ,
+                    _subtyp=self._subtyp,
+                    _data=self._data,
+                    fill_value=self.fill_value,
+                    name=self.name)
 
     def _unpickle_series_compat(self, state):
 
         nd_state, own_state = state
 
         # recreate the ndarray
-        data = np.empty(nd_state[1],dtype=nd_state[2])
+        data = np.empty(nd_state[1], dtype=nd_state[2])
         np.ndarray.__setstate__(data, nd_state)
 
         index, fill_value, sp_index = own_state[:3]
@@ -341,13 +344,14 @@ class SparseSeries(Series):
 
         # create a sparse array
         if not isinstance(data, SparseArray):
-            data = SparseArray(data, sparse_index=sp_index, fill_value=fill_value, copy=False)
+            data = SparseArray(
+                data, sparse_index=sp_index, fill_value=fill_value, copy=False)
 
         # recreate
         data = SingleBlockManager(data, index, fastpath=True)
         generic.NDFrame.__init__(self, data)
 
-        self._set_axis(0,index)
+        self._set_axis(0, index)
         self.name = name
 
     def __iter__(self):
@@ -356,9 +360,9 @@ class SparseSeries(Series):
 
     def _set_subtyp(self, is_all_dates):
         if is_all_dates:
-            object.__setattr__(self,'_subtyp','sparse_time_series')
+            object.__setattr__(self, '_subtyp', 'sparse_time_series')
         else:
-            object.__setattr__(self,'_subtyp','sparse_series')
+            object.__setattr__(self, '_subtyp', 'sparse_series')
 
     def _get_val_at(self, loc):
         """ forward to the array """
@@ -472,7 +476,8 @@ class SparseSeries(Series):
         if new_values is not None:
             values = new_values
         new_index = values.index
-        values = SparseArray(values, fill_value=self.fill_value, kind=self.kind)
+        values = SparseArray(
+            values, fill_value=self.fill_value, kind=self.kind)
         self._data = SingleBlockManager(values, new_index)
         self._index = new_index
 
@@ -487,7 +492,8 @@ class SparseSeries(Series):
 
         values = self.values.to_dense()
         values[key] = _index.convert_scalar(values, value)
-        values = SparseArray(values, fill_value=self.fill_value, kind=self.kind)
+        values = SparseArray(
+            values, fill_value=self.fill_value, kind=self.kind)
         self._data = SingleBlockManager(values, self.index)
 
     def to_dense(self, sparse_only=False):
@@ -536,7 +542,7 @@ class SparseSeries(Series):
                 return self.copy()
             else:
                 return self
-        return self._constructor(self._data.reindex(new_index,method=method,limit=limit,copy=copy),index=new_index,name=self.name)
+        return self._constructor(self._data.reindex(new_index, method=method, limit=limit, copy=copy), index=new_index, name=self.name)
 
     def sparse_reindex(self, new_index):
         """
@@ -606,7 +612,7 @@ class SparseSeries(Series):
         if isnull(self.fill_value):
             return dense_valid
         else:
-            dense_valid=dense_valid[dense_valid!=self.fill_value]
+            dense_valid = dense_valid[dense_valid != self.fill_value]
             return dense_valid.to_sparse(fill_value=self.fill_value)
 
     def shift(self, periods, freq=None, **kwds):
diff --git a/pandas/sparse/tests/test_array.py b/pandas/sparse/tests/test_array.py
index bd5f99ef7..3d2b67f33 100644
--- a/pandas/sparse/tests/test_array.py
+++ b/pandas/sparse/tests/test_array.py
@@ -33,17 +33,18 @@ class TestSparseArray(unittest.TestCase):
 
     def test_get_item(self):
         errmsg = re.compile("bounds")
-        assertRaisesRegexp(IndexError, errmsg, lambda : self.arr[11])
-        assertRaisesRegexp(IndexError, errmsg, lambda : self.arr[-11])
+        assertRaisesRegexp(IndexError, errmsg, lambda: self.arr[11])
+        assertRaisesRegexp(IndexError, errmsg, lambda: self.arr[-11])
         self.assertEqual(self.arr[-1], self.arr[len(self.arr) - 1])
 
     def test_bad_take(self):
-        assertRaisesRegexp(IndexError, "bounds", lambda : self.arr.take(11))
-        self.assertRaises(IndexError, lambda : self.arr.take(-11))
+        assertRaisesRegexp(IndexError, "bounds", lambda: self.arr.take(11))
+        self.assertRaises(IndexError, lambda: self.arr.take(-11))
 
     def test_set_item(self):
         def setitem():
             self.arr[5] = 3
+
         def setslice():
             self.arr[1:5] = 2
         assertRaisesRegexp(TypeError, "item assignment", setitem)
diff --git a/pandas/sparse/tests/test_sparse.py b/pandas/sparse/tests/test_sparse.py
index 761f7f228..ba002415c 100644
--- a/pandas/sparse/tests/test_sparse.py
+++ b/pandas/sparse/tests/test_sparse.py
@@ -75,6 +75,7 @@ def _test_data2_zero():
     arr[np.isnan(arr)] = 0
     return arr, index
 
+
 def assert_sp_series_equal(a, b, exact_indices=True):
     assert(a.index.equals(b.index))
     assert_sp_array_equal(a, b)
@@ -153,7 +154,7 @@ class TestSparseSeries(TestCase,
                                       fill_value=0)
 
     def test_iteration_and_str(self):
-        [ x for x in self.bseries ]
+        [x for x in self.bseries]
         str(self.bseries)
 
     def test_construct_DataFrame_with_sp_series(self):
@@ -166,20 +167,20 @@ class TestSparseSeries(TestCase,
         df.dtypes
         str(df)
 
-        assert_sp_series_equal(df['col'],self.bseries)
+        assert_sp_series_equal(df['col'], self.bseries)
 
         # blocking
-        expected = Series({ 'col' : 'float64:sparse' })
+        expected = Series({'col': 'float64:sparse'})
         result = df.ftypes
-        assert_series_equal(expected,result)
+        assert_series_equal(expected, result)
 
     def test_series_density(self):
         # GH2803
         ts = Series(np.random.randn(10))
         ts[2:-2] = nan
         sts = ts.to_sparse()
-        density =  sts.density # don't die
-        self.assertEqual(density,4/10.0)
+        density = sts.density  # don't die
+        self.assertEqual(density, 4 / 10.0)
 
     def test_sparse_to_dense(self):
         arr, index = _test_data1()
@@ -226,7 +227,8 @@ class TestSparseSeries(TestCase,
         tm.assert_isinstance(self.iseries.sp_index, IntIndex)
 
         self.assertEquals(self.zbseries.fill_value, 0)
-        assert_equal(self.zbseries.values.values, self.bseries.to_dense().fillna(0).values)
+        assert_equal(self.zbseries.values.values,
+                     self.bseries.to_dense().fillna(0).values)
 
         # pass SparseSeries
         s2 = SparseSeries(self.bseries)
@@ -424,7 +426,8 @@ class TestSparseSeries(TestCase,
 
     def test_setslice(self):
         self.bseries[5:10] = 7.
-        assert_series_equal(self.bseries[5:10].to_dense(),Series(7.,index=range(5,10),name=self.bseries.name))
+        assert_series_equal(self.bseries[5:10].to_dense(), Series(
+            7., index=range(5, 10), name=self.bseries.name))
 
     def test_operators(self):
         def _check_op(a, b, op):
@@ -482,19 +485,20 @@ class TestSparseSeries(TestCase,
 
     def test_binary_operators(self):
 
-        ##### skipping for now #####
+        # skipping for now #####
         raise nose.SkipTest
 
         def _check_inplace_op(iop, op):
             tmp = self.bseries.copy()
 
-            expected = op(tmp,self.bseries)
-            iop(tmp,self.bseries)
-            assert_sp_series_equal(tmp,expected)
+            expected = op(tmp, self.bseries)
+            iop(tmp, self.bseries)
+            assert_sp_series_equal(tmp, expected)
 
         inplace_ops = ['add', 'sub', 'mul', 'truediv', 'floordiv', 'pow']
         for op in inplace_ops:
-            _check_inplace_op(getattr(operator, "i%s" % op), getattr(operator, op))
+            _check_inplace_op(
+                getattr(operator, "i%s" % op), getattr(operator, op))
 
     def test_reindex(self):
         def _compare_with_series(sps, new_index):
@@ -632,7 +636,7 @@ class TestSparseSeries(TestCase,
         sp_valid = sp.valid()
 
         expected = sp.to_dense().valid()
-        expected = expected[expected!=0]
+        expected = expected[expected != 0]
 
         assert_almost_equal(sp_valid.values, expected.values)
         self.assert_(sp_valid.index.equals(expected.index))
@@ -815,7 +819,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
                                columns=self.frame.columns,
                                default_fill_value=self.frame.default_fill_value,
                                default_kind=self.frame.default_kind,
-                               copy = True)
+                               copy=True)
         reindexed = self.frame.reindex(idx)
         assert_sp_frame_equal(cons, reindexed, exact_indices=False)
 
@@ -838,10 +842,12 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
                           level=1)
 
         # wrong length index / columns
-        assertRaisesRegexp(ValueError, "^Index length", SparseDataFrame, self.frame.values,
-                           index=self.frame.index[:-1])
-        assertRaisesRegexp(ValueError, "^Column length", SparseDataFrame, self.frame.values,
-                           columns=self.frame.columns[:-1])
+        assertRaisesRegexp(
+            ValueError, "^Index length", SparseDataFrame, self.frame.values,
+            index=self.frame.index[:-1])
+        assertRaisesRegexp(
+            ValueError, "^Column length", SparseDataFrame, self.frame.values,
+            columns=self.frame.columns[:-1])
 
     def test_constructor_empty(self):
         sp = SparseDataFrame()
@@ -867,8 +873,8 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         df = SparseDataFrame(x)
         tm.assert_isinstance(df,SparseDataFrame)
 
-        x = Series(np.random.randn(10000), name ='a')
-        y = Series(np.random.randn(10000), name ='b')
+        x = Series(np.random.randn(10000), name='a')
+        y = Series(np.random.randn(10000), name='b')
         x2 = x.astype(float)
         x2.ix[:9998] = np.NaN
         x_sparse = x2.to_sparse(fill_value=np.NaN)
@@ -887,7 +893,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         sdf = df.to_sparse()
 
         result = sdf.get_dtype_counts()
-        expected = Series({ 'float64' : 4 })
+        expected = Series({'float64': 4})
         assert_series_equal(result, expected)
 
     def test_str(self):
@@ -1047,7 +1053,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         pass
 
     def test_getitem(self):
-        # #1585 select multiple columns
+        # 1585 select multiple columns
         sdf = SparseDataFrame(index=[0, 1, 2], columns=['a', 'b', 'c'])
 
         result = sdf[['a', 'b']]
@@ -1057,7 +1063,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         self.assertRaises(Exception, sdf.__getitem__, ['a', 'd'])
 
     def test_icol(self):
-        # #2227
+        # 2227
         result = self.frame.icol(0)
         self.assertTrue(isinstance(result, SparseSeries))
         assert_sp_series_equal(result, self.frame['A'])
@@ -1118,8 +1124,9 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
             # insert SparseSeries differently-indexed
             to_insert = frame['A'][::2]
             frame['E'] = to_insert
-            expected = to_insert.to_dense().reindex(frame.index).fillna(to_insert.fill_value)
-            assert_series_equal(frame['E'].to_dense(),expected)
+            expected = to_insert.to_dense().reindex(
+                frame.index).fillna(to_insert.fill_value)
+            assert_series_equal(frame['E'].to_dense(), expected)
 
             # insert Series
             frame['F'] = frame['A'].to_dense()
@@ -1129,9 +1136,9 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
             # insert Series differently-indexed
             to_insert = frame['A'].to_dense()[::2]
             frame['G'] = to_insert
-            expected = to_insert.reindex(frame.index).fillna(frame.default_fill_value)
-            assert_series_equal(frame['G'].to_dense(),expected)
-
+            expected = to_insert.reindex(
+                frame.index).fillna(frame.default_fill_value)
+            assert_series_equal(frame['G'].to_dense(), expected)
 
             # insert ndarray
             frame['H'] = np.random.randn(N)
@@ -1168,7 +1175,8 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
 
         self.frame['F'] = arr[:-1]
         index = self.frame.index[:-1]
-        assert_sp_series_equal(self.frame['E'].reindex(index), self.frame['F'].reindex(index))
+        assert_sp_series_equal(
+            self.frame['E'].reindex(index), self.frame['F'].reindex(index))
 
     def test_delitem(self):
         A = self.frame['A']
@@ -1205,7 +1213,8 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         a = self.frame.ix[:5, :3]
         b = self.frame.ix[5:]
         appended = a.append(b)
-        assert_sp_frame_equal(appended.ix[:, :3], self.frame.ix[:, :3], exact_indices=False)
+        assert_sp_frame_equal(
+            appended.ix[:, :3], self.frame.ix[:, :3], exact_indices=False)
 
     def test_apply(self):
         applied = self.frame.apply(np.sqrt)
@@ -1305,7 +1314,8 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
                                dense_result)
 
             sparse_result2 = sparse_result.reindex(index)
-            dense_result2 = dense_result.reindex(index).fillna(frame.default_fill_value)
+            dense_result2 = dense_result.reindex(
+                index).fillna(frame.default_fill_value)
             assert_frame_equal(sparse_result2.to_dense(), dense_result2)
 
             # propagate CORRECT fill value
@@ -1470,7 +1480,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         assert_frame_equal(xp, rs)
 
     def test_sparse_pow_issue(self):
-        # #2220
+        # 2220
         df = SparseDataFrame({'A': [1.1, 3.3], 'B': [2.5, -3.9]})
 
         # note : no error without nan
