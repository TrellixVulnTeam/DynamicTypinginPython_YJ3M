commit e060616330d63b451fc34e7328080dcbb5e623f2
Author: jreback <jeff@reback.net>
Date:   Mon Jul 7 13:26:54 2014 -0400

    DOC: minor corrections in v0.14.1

diff --git a/doc/source/io.rst b/doc/source/io.rst
index cee4a4e5a..109b7a0a3 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -258,7 +258,7 @@ after a delimiter:
    data = 'a, b, c\n1, 2, 3\n4, 5, 6'
    print(data)
    pd.read_csv(StringIO(data), skipinitialspace=True)
-   
+
 Moreover, ``read_csv`` ignores any completely commented lines:
 
 .. ipython:: python
@@ -2962,7 +2962,7 @@ Notes & Caveats
      ``tables``. The sizes of a string based indexing column
      (e.g. *columns* or *minor_axis*) are determined as the maximum size
      of the elements in that axis or by passing the parameter
-   - Be aware that timezones (e.g., ``pytz.timezone('US/Eastern')``) 
+   - Be aware that timezones (e.g., ``pytz.timezone('US/Eastern')``)
      are not necessarily equal across timezone versions.  So if data is
      localized to a specific timezone in the HDFStore using one version
      of a timezone library and that data is updated with another version, the data
@@ -3409,14 +3409,14 @@ Google BigQuery (Experimental)
 The :mod:`pandas.io.gbq` module provides a wrapper for Google's BigQuery
 analytics web service to simplify retrieving results from BigQuery tables
 using SQL-like queries. Result sets are parsed into a pandas
-DataFrame with a shape and data types derived from the source table. 
-Additionally, DataFrames can be appended to existing BigQuery tables if 
+DataFrame with a shape and data types derived from the source table.
+Additionally, DataFrames can be appended to existing BigQuery tables if
 the destination table is the same shape as the DataFrame.
 
 For specifics on the service itself, see `here <https://developers.google.com/bigquery/>`__
 
-As an example, suppose you want to load all data from an existing BigQuery 
-table : `test_dataset.test_table` into a DataFrame using the :func:`~pandas.io.read_gbq` 
+As an example, suppose you want to load all data from an existing BigQuery
+table : `test_dataset.test_table` into a DataFrame using the :func:`~pandas.io.read_gbq`
 function.
 
 .. code-block:: python
@@ -3447,14 +3447,14 @@ Finally, you can append data to a BigQuery table from a pandas DataFrame
 using the :func:`~pandas.io.to_gbq` function. This function uses the
 Google streaming API which requires that your destination table exists in
 BigQuery. Given the BigQuery table already exists, your DataFrame should
-match the destination table in column order, structure, and data types. 
-DataFrame indexes are not supported. By default, rows are streamed to 
-BigQuery in chunks of 10,000 rows, but you can pass other chuck values 
-via the ``chunksize`` argument. You can also see the progess of your 
-post via the ``verbose`` flag which defaults to ``True``. The http 
-response code of Google BigQuery can be successful (200) even if the 
-append failed. For this reason, if there is a failure to append to the 
-table, the complete error response from BigQuery is returned which 
+match the destination table in column order, structure, and data types.
+DataFrame indexes are not supported. By default, rows are streamed to
+BigQuery in chunks of 10,000 rows, but you can pass other chuck values
+via the ``chunksize`` argument. You can also see the progess of your
+post via the ``verbose`` flag which defaults to ``True``. The http
+response code of Google BigQuery can be successful (200) even if the
+append failed. For this reason, if there is a failure to append to the
+table, the complete error response from BigQuery is returned which
 can be quite long given it provides a status for each row. You may want
 to start with smaller chuncks to test that the size and types of your
 dataframe match your destination table to make debugging simpler.
@@ -3470,9 +3470,9 @@ The BigQuery SQL query language has some oddities, see `here <https://developers
 
 While BigQuery uses SQL-like syntax, it has some important differences
 from traditional databases both in functionality, API limitations (size and
-qunatity of queries or uploads), and how Google charges for use of the service. 
+qunatity of queries or uploads), and how Google charges for use of the service.
 You should refer to Google documentation often as the service seems to
-be changing and evolving. BiqQuery is best for analyzing large sets of 
+be changing and evolving. BiqQuery is best for analyzing large sets of
 data quickly, but it is not a direct replacement for a transactional database.
 
 You can access the management console to determine project id's by:
diff --git a/doc/source/release.rst b/doc/source/release.rst
index fb2c24acf..cb19e2ef4 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -48,13 +48,11 @@ analysis / manipulation tool available in any language.
 pandas 0.14.1
 -------------
 
-**Release date:** (????)
+**Release date:** (July 11, 2014)
 
 This is a minor release from 0.14.0 and includes a number of API changes, several new features, enhancements, and
 performance improvements along with a large number of bug fixes.
 
-Highlights include:
-
 See the :ref:`v0.14.1 Whatsnew <whatsnew_0141>` overview or the issue tracker on GitHub for an extensive list
 of all API changes, enhancements and bugs that have been fixed in 0.14.1.
 
diff --git a/doc/source/v0.14.1.txt b/doc/source/v0.14.1.txt
index 7c6fd09ae..8b64648d2 100644
--- a/doc/source/v0.14.1.txt
+++ b/doc/source/v0.14.1.txt
@@ -151,7 +151,6 @@ Performance
 .. _whatsnew_0141.experimental:
 
 
-- Bug in line plot doesn't set correct ``xlim`` if ``secondary_y=True`` (:issue:`7459`)
 
 
 
@@ -165,7 +164,7 @@ Experimental
   dependency on the Google ``bq.py`` command line client. This submodule
   now uses ``httplib2`` and the Google ``apiclient`` and ``oauth2client`` API client
   libraries which should be more stable and, therefore, reliable than
-  ``bq.py`` (:issue:`6937`).
+  ``bq.py``. See :ref:`the docs <io.bigquery>`. (:issue:`6937`).
 
 .. _whatsnew_0141.bug_fixes:
 
@@ -180,8 +179,8 @@ Bug Fixes
 - Bug in groupby ``.nth`` with a Series and integer-like column name (:issue:`7559`)
 - Bug in ``Series.get`` with a boolean accessor (:issue:`7407`)
 - Bug in ``value_counts`` where ``NaT`` did not qualify as missing (``NaN``) (:issue:`7423`)
-- Bug in ``to_timedelta`` that accepted invalid units and misinterpreted 'm/h' (:issue:`7611`, :issue: `6423`)
-
+- Bug in ``to_timedelta`` that accepted invalid units and misinterpreted 'm/h' (:issue:`7611`, :issue:`6423`)
+- Bug in line plot doesn't set correct ``xlim`` if ``secondary_y=True`` (:issue:`7459`)
 - Bug in grouped ``hist`` and ``scatter`` plots use old ``figsize`` default (:issue:`7394`)
 - Bug in plotting subplots with ``DataFrame.plot``, ``hist`` clears passed ``ax`` even if the number of subplots is one (:issue:`7391`).
 - Bug in plotting subplots with ``DataFrame.boxplot`` with ``by`` kw raises ``ValueError`` if the number of subplots exceeds 1 (:issue:`7391`).
@@ -195,7 +194,7 @@ Bug Fixes
 - Bug in multi-index slicing with datetimelike ranges (strings and Timestamps), (:issue:`7429`)
 - Bug in ``Index.min`` and ``max`` doesn't handle ``nan`` and ``NaT`` properly (:issue:`7261`)
 - Bug in ``PeriodIndex.min/max`` results in ``int`` (:issue:`7609`)
-- Bug in ``resample`` where ``fill_method`` was ignored if you passed ``how`` (:issue:`7261`)
+- Bug in ``resample`` where ``fill_method`` was ignored if you passed ``how`` (:issue:`2073`)
 - Bug in ``TimeGrouper`` doesn't exclude column specified by ``key`` (:issue:`7227`)
 - Bug in ``DataFrame`` and ``Series`` bar and barh plot raises ``TypeError`` when ``bottom``
   and ``left`` keyword is specified (:issue:`7226`)
