commit 9345bbacb25827507bfaecf4b0023afccc212bf5
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Sep 25 22:04:47 2012 -0400

    ENH: more refactoring and sharing of (some still spaghettified) code

diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index def53a0f5..bd42d8543 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -207,6 +207,7 @@ def read_csv(filepath_or_buffer,
              names=None,
              skiprows=None,
              na_values=None,
+             na_filter=True,
              keep_default_na=True,
              thousands=None,
              comment=None,
@@ -227,7 +228,9 @@ def read_csv(filepath_or_buffer,
                 sep=sep, dialect=dialect,
                 header=header, index_col=index_col,
                 names=names, skiprows=skiprows,
-                na_values=na_values, keep_default_na=keep_default_na,
+                na_values=na_values,
+                na_filter=na_filter,
+                keep_default_na=keep_default_na,
                 thousands=thousands,
                 comment=comment, parse_dates=parse_dates,
                 keep_date_col=keep_date_col,
@@ -256,6 +259,7 @@ def read_table(filepath_or_buffer,
                names=None,
                skiprows=None,
                na_values=None,
+               na_filter=True,
                keep_default_na=True,
                thousands=None,
                comment=None,
@@ -276,7 +280,9 @@ def read_table(filepath_or_buffer,
                 sep=sep, dialect=dialect,
                 header=header, index_col=index_col,
                 names=names, skiprows=skiprows,
-                na_values=na_values, keep_default_na=keep_default_na,
+                na_values=na_values,
+                na_filter=na_filter,
+                keep_default_na=keep_default_na,
                 thousands=thousands,
                 comment=comment, parse_dates=parse_dates,
                 keep_date_col=keep_date_col,
@@ -306,6 +312,7 @@ def read_fwf(filepath_or_buffer,
              names=None,
              skiprows=None,
              na_values=None,
+             na_filter=True,
              keep_default_na=True,
              thousands=None,
              comment=None,
@@ -326,7 +333,9 @@ def read_fwf(filepath_or_buffer,
                 colspecs=colspecs, widths=widths,
                 header=header, index_col=index_col,
                 names=names, skiprows=skiprows,
-                na_values=na_values, keep_default_na=keep_default_na,
+                na_values=na_values,
+                na_filter=na_filter,
+                keep_default_na=keep_default_na,
                 thousands=thousands,
                 comment=comment, parse_dates=parse_dates,
                 keep_date_col=keep_date_col,
@@ -551,6 +560,7 @@ class TextFileReader(object):
                       keep_date_col=self.keep_date_col,
                       dayfirst=self.dayfirst,
                       na_values=self.na_values,
+                      na_filter=self.na_filter,
                       verbose=self.verbose)
 
         params.update(self.kwds)
@@ -601,92 +611,215 @@ class TextFileReader(object):
     get_chunk = read
 
 
-class CParserWrapper(object):
-    """
-
-    """
-
-    def __init__(self, src, **kwds):
-        kwds = kwds.copy()
+class ParserBase(object):
 
+    def __init__(self, kwds):
         self.names = kwds.get('names')
+        self.orig_names = None
+
         self.index_col = kwds.pop('index_col', None)
-        self.as_recarray = kwds.get('as_recarray', False)
-        self.kwds = kwds
+        self.index_names = None
 
         self.parse_dates = kwds.pop('parse_dates', False)
         self.date_parser = kwds.pop('date_parser', None)
         self.dayfirst = kwds.pop('dayfirst', False)
         self.keep_date_col = kwds.pop('keep_date_col', False)
 
+        self.na_values = kwds.get('na_values')
+
         self._date_conv = _make_date_converter(date_parser=self.date_parser,
                                                dayfirst=self.dayfirst)
 
-        self._reader = _parser.TextReader(src, **kwds)
+        self._name_processed = False
 
-        self.index_names = None
+    @property
+    def _has_complex_date_col(self):
+        return (isinstance(self.parse_dates, dict) or
+                (isinstance(self.parse_dates, list) and
+                 len(self.parse_dates) > 0 and
+                 isinstance(self.parse_dates[0], list)))
 
-        if self._reader.header is None:
-            self.names = None
+    def _should_parse_dates(self, i):
+        if isinstance(self.parse_dates, bool):
+            return self.parse_dates
         else:
-            self.names = list(self._reader.header)
+            name = self.index_names[i]
+            j = self.index_col[i]
 
-        if self.names is None:
-            self.names = ['X.%d' % (i + 1)
-                          for i in range(self._reader.table_width)]
+            if np.isscalar(self.parse_dates):
+                return (j == self.parse_dates) or (name == self.parse_dates)
+            else:
+                return (j in self.parse_dates) or (name in self.parse_dates)
 
-        if self._reader.leading_cols == 0 and self.index_col is not None:
-            (self.index_names, self.names,
-             self.index_col) = _clean_index_names(self.names, self.index_col)
+    def _make_index(self, data, alldata, columns):
+        if self.index_col is None:
+            index = None
 
-    def read(self, nrows=None):
-        if self.as_recarray:
-            return self._reader.read(nrows)
+        elif not self._has_complex_date_col:
+            index = self._get_simple_index(alldata, columns)
+            index = self._agg_index(index)
 
-        data = self._reader.read(nrows)
-        names = self.names
+        elif self._has_complex_date_col:
+            if not self._name_processed:
+                (self.index_names, _,
+                 self.index_col) = _clean_index_names(list(columns),
+                                                      self.index_col)
+                self._name_processed = True
+            index = self._get_complex_date_index(data, columns)
+            index = self._agg_index(index, try_parse_dates=False)
 
-        index, names, data = self._make_index(names, data)
+        return index
 
-        # rename dict keys
-        data = sorted(data.items())
-        data = dict((k, v) for k, (i, v) in zip(names, data))
+    _implicit_index = False
 
-        names, data = self._do_date_conversions(names, data)
+    def _get_simple_index(self, data, columns):
+        def ix(col):
+            if not isinstance(col, basestring):
+                return col
+            raise ValueError('Index %s invalid' % col)
+        index = None
 
-        return index, names, data
+        to_remove = []
+        index = []
+        for idx in self.index_col:
+            i = ix(idx)
+            to_remove.append(i)
+            index.append(data[i])
+
+        # remove index items from content and columns, don't pop in
+        # loop
+        for i in reversed(sorted(to_remove)):
+            data.pop(i)
+            if not self._implicit_index:
+                columns.pop(i)
+
+        return index
+
+    def _get_complex_date_index(self, data, col_names):
+        def _get_name(icol):
+            if isinstance(icol, basestring):
+                return icol
+
+            if col_names is None:
+                raise ValueError(('Must supply column order to use %s as '
+                                  'index') % str(icol))
+
+            for i, c in enumerate(col_names):
+                if i == icol:
+                    return c
+
+        index = None
+
+        to_remove = []
+        index = []
+        for idx in self.index_col:
+            name = _get_name(idx)
+            to_remove.append(name)
+            index.append(data[name])
+
+        # remove index items from content and columns, don't pop in
+        # loop
+        for c in reversed(sorted(to_remove)):
+            data.pop(c)
+            col_names.remove(c)
+
+        return index
+
+    def _agg_index(self, index, try_parse_dates=True):
+        arrays = []
+        for i, arr in enumerate(index):
+
+            if (try_parse_dates and self._should_parse_dates(i)):
+                arr = self._date_conv(arr)
+
+            col_na_values = self.na_values
+
+            if isinstance(self.na_values, dict):
+                col_name = self.index_names[i]
+                if col_name is not None:
+                    col_na_values = _get_na_values(col_name,
+                                                   self.na_values)
+
+            arr, _ = _convert_types(arr, col_na_values)
+            arrays.append(arr)
+
+        index = MultiIndex.from_arrays(arrays, names=self.index_names)
+
+        return index
 
     def _do_date_conversions(self, names, data):
         # returns data, columns
-        data, names = _process_date_conversion(
-            data, self._date_conv, self.parse_dates, self.index_col,
-            self.index_names, names, keep_date_col=self.keep_date_col)
+        if self.parse_dates is not None:
+            data, names = _process_date_conversion(
+                data, self._date_conv, self.parse_dates, self.index_col,
+                self.index_names, names, keep_date_col=self.keep_date_col)
 
         return names, data
 
-    def _get_index_names(self):
-        names = list(self._reader.header)
-        idx_names = None
+    def _exclude_implicit_index(self, alldata):
 
-        if self._reader.leading_cols == 0 and self.index_col is not None:
-            (idx_names, names,
-             self.index_col) = _clean_index_names(names, self.index_col)
+        if self._implicit_index:
+            excl_indices = self.index_col
 
-        return names, idx_names
+            data = {}
+            offset = 0
+            for i, col in enumerate(self.orig_names):
+                while i + offset in excl_indices:
+                    offset += 1
+                data[col] = alldata[i + offset]
+        else:
+            data = dict((k, v) for k, v in izip(self.orig_names, alldata))
+
+        return data
+
+
+class CParserWrapper(ParserBase):
+    """
+
+    """
+
+    def __init__(self, src, **kwds):
+        self.kwds = kwds
+        kwds = kwds.copy()
+
+        self.as_recarray = kwds.get('as_recarray', False)
+
+        ParserBase.__init__(self, kwds)
+
+        self._reader = _parser.TextReader(src, **kwds)
+
+        if self._reader.header is None:
+            self.names = None
+        else:
+            self.names = list(self._reader.header)
+
+        if self.names is None:
+            self.names = ['X.%d' % (i + 1)
+                          for i in range(self._reader.table_width)]
+
+        self.orig_names = self.names
+
+        if not self._has_complex_date_col:
+            if self._reader.leading_cols == 0 and self.index_col is not None:
+                self._name_processed = True
+                (self.index_names, self.names,
+                 self.index_col) = _clean_index_names(self.names,
+                                                      self.index_col)
 
-    def _make_index(self, names, data, try_parse_dates=True):
-        if names is None:
-            names = self.names
+        self._implicit_index = self._reader.leading_cols > 0
 
-        if names is None: # still None
-            names = range(len(data))
+    def read(self, nrows=None):
+        if self.as_recarray:
+            # what to do if there are leading columns?
+            return self._reader.read(nrows)
+
+        data = self._reader.read(nrows)
+        names = self.names
 
-        def _maybe_parse_dates(values, index):
-            if try_parse_dates and self._should_parse_dates(index):
-                values = self._date_conv(values)
-            return values
+        if self._reader.leading_cols:
+            if self._has_complex_date_col:
+                raise NotImplementedError('file structure not yet supported')
 
-        if self._reader.leading_cols > 0:
             # implicit index, no index names
             arrays = []
 
@@ -696,40 +829,51 @@ class CParserWrapper(object):
                 else:
                     values = data.pop(self.index_col[i])
 
-                values = _maybe_parse_dates(values, i)
+                values = self._maybe_parse_dates(values, i,
+                                                 try_parse_dates=True)
                 arrays.append(values)
 
             index = MultiIndex.from_arrays(arrays)
 
-        elif self.index_col is not None:
-            arrays = []
-            for i, col in enumerate(self.index_col):
-                if col not in data:
-                    raise ValueError('Invalid index column %s' % i)
+            # rename dict keys
+            data = sorted(data.items())
+            data = dict((k, v) for k, (i, v) in zip(names, data))
 
-                values = data.pop(col)
+            names, data = self._do_date_conversions(names, data)
 
-                values = _maybe_parse_dates(values, i)
+        else:
+            # rename dict keys
+            data = sorted(data.items())
 
-                arrays.append(values)
+            # ugh, mutation
+            names = list(self.orig_names)
+
+            # columns as list
+            alldata = [x[1] for x in data]
+
+            data = dict((k, v) for k, (i, v) in zip(self.orig_names, data))
+
+            names, data = self._do_date_conversions(names, data)
+            index = self._make_index(data, alldata, names)
 
-            index = MultiIndex.from_arrays(arrays, names=self.index_names)
-        else:
-            index = None
 
         return index, names, data
 
-    def _should_parse_dates(self, i):
-        if isinstance(self.parse_dates, bool):
-            return self.parse_dates
-        else:
-            name = self.index_names[i]
-            j = self.index_col[i]
+    def _get_index_names(self):
+        names = list(self._reader.header)
+        idx_names = None
+
+        if self._reader.leading_cols == 0 and self.index_col is not None:
+            (idx_names, names,
+             self.index_col) = _clean_index_names(names, self.index_col)
+
+        return names, idx_names
+
+    def _maybe_parse_dates(self, values, index, try_parse_dates=True):
+        if try_parse_dates and self._should_parse_dates(index):
+            values = self._date_conv(values)
+        return values
 
-            if np.isscalar(self.parse_dates):
-                return (j == self.parse_dates) or (name == self.parse_dates)
-            else:
-                return (j in self.parse_dates) or (name in self.parse_dates)
 
 
 def TextParser(*args, **kwds):
@@ -773,11 +917,12 @@ def TextParser(*args, **kwds):
     return TextFileReader(*args, **kwds)
 
 
-class PythonParser(object):
+class PythonParser(ParserBase):
 
     def __init__(self, f, delimiter=None, dialect=None, names=None, header=0,
                  index_col=None,
                  na_values=None,
+                 na_filter=True,
                  thousands=None,
                  quotechar='"',
                  escapechar=None,
@@ -823,7 +968,13 @@ class PythonParser(object):
 
         self.verbose = verbose
         self.converters = converters
+
         self.na_values = na_values
+        self.na_filter = na_filter
+
+        if not na_filter:
+            raise NotImplementedError
+
         self.squeeze = squeeze
 
         self.thousands = thousands
@@ -837,7 +988,7 @@ class PythonParser(object):
         self.columns = self._infer_columns()
 
         # get popped off for index
-        self.orig_columns = list(self.columns)
+        self.orig_names = list(self.columns)
 
         # needs to be cleaned/refactored
         # multiple date column thing turning into a real spaghetti factory
@@ -846,7 +997,7 @@ class PythonParser(object):
         self._name_processed = False
         if not self._has_complex_date_col:
             (self.index_names,
-             self.orig_columns, _) = self._get_index_name(self.columns)
+             self.orig_names, _) = self._get_index_name(self.columns)
             self._name_processed = True
         self._first_chunk = True
 
@@ -918,39 +1069,18 @@ class PythonParser(object):
         # done with first read, next time raise StopIteration
         self._first_chunk = False
 
-        columns = list(self.orig_columns)
+        columns = list(self.orig_names)
         if len(content) == 0: # pragma: no cover
             # DataFrame with the right metadata, even though it's length 0
-            return _get_empty_meta(self.orig_columns,
+            return _get_empty_meta(self.orig_names,
                                    self.index_col,
                                    self.index_names)
 
         alldata = self._rows_to_cols(content)
         data = self._exclude_implicit_index(alldata)
-
         data = self._convert_data(data)
-
-        if self.parse_dates is not None:
-            data, columns = _process_date_conversion(
-                data, self._date_conv, self.parse_dates, self.index_col,
-                self.index_names, self.columns,
-                keep_date_col=self.keep_date_col)
-
-        if self.index_col is None:
-            index = None
-
-        elif not self._has_complex_date_col:
-            index = self._get_simple_index(alldata, columns)
-            index = self._agg_index(index)
-
-        elif self._has_complex_date_col:
-            if not self._name_processed:
-                (self.index_names, _,
-                 self.index_col) = _clean_index_names(list(columns),
-                                                      self.index_col)
-                self._name_processed = True
-            index = self._get_complex_date_index(data, columns)
-            index = self._agg_index(index, try_parse_dates=False)
+        columns, data = self._do_date_conversions(self.columns, data)
+        index = self._make_index(data, alldata, columns)
 
         return index, columns, data
 
@@ -961,8 +1091,8 @@ class PythonParser(object):
         # apply converters
         converted = set()
         for col, f in self.converters.iteritems():
-            if isinstance(col, int) and col not in self.orig_columns:
-                col = self.orig_columns[col]
+            if isinstance(col, int) and col not in self.orig_names:
+                col = self.orig_names[col]
             data[col] = lib.map_infer(data[col], f)
             converted.add(col)
 
@@ -1087,7 +1217,7 @@ class PythonParser(object):
     _implicit_index = False
 
     def _get_index_name(self, columns):
-        orig_columns = list(columns)
+        orig_names = list(columns)
         columns = list(columns)
 
         try:
@@ -1117,7 +1247,7 @@ class PythonParser(object):
                     for c in reversed(line):
                         columns.insert(0, c)
 
-                    return line, columns, orig_columns
+                    return line, columns, orig_names
 
         if implicit_first_cols > 0:
             self._implicit_index = True
@@ -1129,12 +1259,12 @@ class PythonParser(object):
             (index_name, columns,
              self.index_col) = _clean_index_names(columns, self.index_col)
 
-        return index_name, orig_columns, columns
+        return index_name, orig_names, columns
 
     def _rows_to_cols(self, content):
         zipped_content = list(lib.to_object_array(content).T)
 
-        col_len = len(self.orig_columns)
+        col_len = len(self.orig_names)
         zip_len = len(zipped_content)
 
         if self._implicit_index:
@@ -1160,116 +1290,6 @@ class PythonParser(object):
 
         return zipped_content
 
-    def _exclude_implicit_index(self, alldata):
-
-        if self._implicit_index:
-            excl_indices = self.index_col
-
-            data = {}
-            offset = 0
-            for i, col in enumerate(self.orig_columns):
-                while i + offset in excl_indices:
-                    offset += 1
-                data[col] = alldata[i + offset]
-        else:
-            data = dict((k, v) for k, v in izip(self.orig_columns, alldata))
-
-        return data
-
-    @property
-    def _has_complex_date_col(self):
-        return (isinstance(self.parse_dates, dict) or
-                (isinstance(self.parse_dates, list) and
-                 len(self.parse_dates) > 0 and
-                 isinstance(self.parse_dates[0], list)))
-
-    def _get_simple_index(self, data, columns):
-        def ix(col):
-            if not isinstance(col, basestring):
-                return col
-            raise ValueError('Index %s invalid' % col)
-        index = None
-
-        to_remove = []
-        index = []
-        for idx in self.index_col:
-            i = ix(idx)
-            to_remove.append(i)
-            index.append(data[i])
-
-        # remove index items from content and columns, don't pop in
-        # loop
-        for i in reversed(sorted(to_remove)):
-            data.pop(i)
-            if not self._implicit_index:
-                columns.pop(i)
-
-        return index
-
-    def _get_complex_date_index(self, data, col_names):
-        def _get_name(icol):
-            if isinstance(icol, basestring):
-                return icol
-
-            if col_names is None:
-                raise ValueError(('Must supply column order to use %s as '
-                                  'index') % str(icol))
-
-            for i, c in enumerate(col_names):
-                if i == icol:
-                    return c
-
-        index = None
-
-        to_remove = []
-        index = []
-        for idx in self.index_col:
-            name = _get_name(idx)
-            to_remove.append(name)
-            index.append(data[name])
-
-        # remove index items from content and columns, don't pop in
-        # loop
-        for c in reversed(sorted(to_remove)):
-            data.pop(c)
-            col_names.remove(c)
-
-        return index
-
-    def _agg_index(self, index, try_parse_dates=True):
-        arrays = []
-        for i, arr in enumerate(index):
-
-            if (try_parse_dates and self._should_parse_dates(i)):
-                arr = self._date_conv(arr)
-
-            col_na_values = self.na_values
-
-            if isinstance(self.na_values, dict):
-                col_name = self.index_names[i]
-                if col_name is not None:
-                    col_na_values = _get_na_values(col_name,
-                                                   self.na_values)
-
-            arr, _ = _convert_types(arr, col_na_values)
-            arrays.append(arr)
-
-        index = MultiIndex.from_arrays(arrays, names=self.index_names)
-
-        return index
-
-    def _should_parse_dates(self, i):
-        if isinstance(self.parse_dates, bool):
-            return self.parse_dates
-        else:
-            name = self.index_names[i]
-            j = self.index_col[i]
-
-            if np.isscalar(self.parse_dates):
-                return (j == self.parse_dates) or (name == self.parse_dates)
-            else:
-                return (j in self.parse_dates) or (name in self.parse_dates)
-
     def _get_lines(self, rows=None):
         source = self.data
         lines = self.buf
@@ -1350,7 +1370,7 @@ def _process_date_conversion(data_dict, converter, parse_spec,
     new_cols = []
     new_data = {}
 
-    orig_columns = columns
+    orig_names = columns
     columns = list(columns)
 
     date_cols = set()
@@ -1363,13 +1383,13 @@ def _process_date_conversion(data_dict, converter, parse_spec,
         for colspec in parse_spec:
             if np.isscalar(colspec):
                 if isinstance(colspec, int) and colspec not in data_dict:
-                    colspec = orig_columns[colspec]
+                    colspec = orig_names[colspec]
                 if _isindex(colspec):
                     continue
                 data_dict[colspec] = converter(data_dict[colspec])
             else:
                 new_name, col, old_names = _try_convert_dates(
-                    converter, colspec, data_dict, orig_columns)
+                    converter, colspec, data_dict, orig_names)
                 if new_name in data_dict:
                     raise ValueError('New date column already in dict %s' %
                                      new_name)
@@ -1385,7 +1405,7 @@ def _process_date_conversion(data_dict, converter, parse_spec,
                                  new_name)
 
             _, col, old_names = _try_convert_dates(converter, colspec,
-                                                   data_dict, orig_columns)
+                                                   data_dict, orig_names)
 
             new_data[new_name] = col
             new_cols.append(new_name)
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index 5b7ae6e44..c62bdb958 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -53,15 +53,6 @@ baz,12,13,14,15
 qux,12,13,14,15
 foo2,12,13,14,15
 bar2,12,13,14,15
-"""
-    ts_data = """\
-ID,date,nominalTime,actualTime,A,B,C,D,E
-KORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000
-KORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000
-KORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000
-KORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000
-KORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000
-KORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000
 """
 
     def read_csv(self, *args, **kwargs):
@@ -271,6 +262,16 @@ KORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
         df = self.read_csv(StringIO(data), parse_dates={'nominal': [1, 2]})
         self.assert_(not isinstance(df.nominal[0], basestring))
 
+    ts_data = """\
+ID,date,nominalTime,actualTime,A,B,C,D,E
+KORD,19990127, 19:00:00, 18:56:00, 0.8100, 2.8100, 7.2000, 0.0000, 280.0000
+KORD,19990127, 20:00:00, 19:56:00, 0.0100, 2.2100, 7.2000, 0.0000, 260.0000
+KORD,19990127, 21:00:00, 20:56:00, -0.5900, 2.2100, 5.7000, 0.0000, 280.0000
+KORD,19990127, 21:00:00, 21:18:00, -0.9900, 2.0100, 3.6000, 0.0000, 270.0000
+KORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000
+KORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000
+"""
+
     def test_multiple_date_col_name_collision(self):
         self.assertRaises(ValueError, self.read_csv, StringIO(self.ts_data),
                           parse_dates={'ID' : [1, 2]})
@@ -327,8 +328,7 @@ KORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
         #1835
         data = "A;B\n1;2\n3;4"
 
-        rs = self.read_csv(StringIO(data), sep=';', index_col='A',
-                           converters={'A' : lambda x: x})
+        rs = self.read_csv(StringIO(data), sep=';', index_col='A', converters={'A' : lambda x: x})
 
         xp = DataFrame({'B' : [2, 4]}, index=Index([1, 3], name='A'))
         assert_frame_equal(rs, xp)
@@ -1064,59 +1064,6 @@ c,4,5,01/03/2009
         df2 = self.read_csv(StringIO(data), sep=';',converters=converter)
         self.assert_(df2['Number1'].dtype == float)
 
-    def test_regex_separator(self):
-        data = """   A   B   C   D
-a   1   2   3   4
-b   1   2   3   4
-c   1   2   3   4
-"""
-        df = self.read_table(StringIO(data), sep='\s+')
-        expected = self.read_csv(StringIO(re.sub('[ ]+', ',', data)),
-                            index_col=0)
-        self.assert_(expected.index.name is None)
-        assert_frame_equal(df, expected)
-
-    def test_verbose_import(self):
-        text = """a,b,c,d
-one,1,2,3
-one,1,2,3
-,1,2,3
-one,1,2,3
-,1,2,3
-,1,2,3
-one,1,2,3
-two,1,2,3"""
-
-        buf = StringIO()
-        sys.stdout = buf
-
-        try:
-            # it works!
-            df = self.read_csv(StringIO(text), verbose=True)
-            self.assert_(buf.getvalue() == 'Filled 3 NA values in column a\n')
-        finally:
-            sys.stdout = sys.__stdout__
-
-        buf = StringIO()
-        sys.stdout = buf
-
-        text = """a,b,c,d
-one,1,2,3
-two,1,2,3
-three,1,2,3
-four,1,2,3
-five,1,2,3
-,1,2,3
-seven,1,2,3
-eight,1,2,3"""
-
-        try:
-            # it works!
-            df = self.read_csv(StringIO(text), verbose=True, index_col=0)
-            self.assert_(buf.getvalue() == 'Filled 1 NA values in column a\n')
-        finally:
-            sys.stdout = sys.__stdout__
-
     def test_read_table_buglet_4x_multiindex(self):
         text = """                      A       B       C       D        E
 one two three   four
@@ -1251,19 +1198,6 @@ a,b,c,d
         self.assert_(stamp.minute == 39)
         self.assert_(result.index.tz is pytz.utc)
 
-
-class TestPythonParser(ParserTests, unittest.TestCase):
-
-    def read_csv(self, *args, **kwds):
-        kwds = kwds.copy()
-        kwds['engine'] = 'python'
-        return read_csv(*args, **kwds)
-
-    def read_table(self, *args, **kwds):
-        kwds = kwds.copy()
-        kwds['engine'] = 'python'
-        return read_table(*args, **kwds)
-
     def test_multiple_date_cols_index(self):
         data = """\
 ID,date,NominalTime,ActualTime,TDew,TAir,Windspeed,Precip,WindDir
@@ -1286,15 +1220,13 @@ KORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
         assert_frame_equal(df3, df)
 
     def test_multiple_date_cols_chunked(self):
-        df = self.read_csv(StringIO(self.ts_data),
-                           parse_dates={'nominal': [1,2]},
-                           index_col='nominal')
-        reader = self.read_csv(StringIO(self.ts_data),
-                               parse_dates={'nominal': [1,2]},
-                               index_col='nominal', chunksize=2)
+        df = self.read_csv(StringIO(self.ts_data), parse_dates={'nominal': [1,2]}, index_col='nominal')
+        reader = self.read_csv(StringIO(self.ts_data), parse_dates={'nominal': [1,2]}, index_col='nominal', chunksize=2)
 
         chunks = list(reader)
 
+        self.assert_('nominalTime' not in df)
+
         assert_frame_equal(chunks[0], df[:2])
         assert_frame_equal(chunks[1], df[2:4])
         assert_frame_equal(chunks[2], df[4:])
@@ -1319,6 +1251,30 @@ KORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
         assert_frame_equal(xp.set_index(['nominal', 'ID']), df)
 
 
+class TestPythonParser(ParserTests, unittest.TestCase):
+
+    def read_csv(self, *args, **kwds):
+        kwds = kwds.copy()
+        kwds['engine'] = 'python'
+        return read_csv(*args, **kwds)
+
+    def read_table(self, *args, **kwds):
+        kwds = kwds.copy()
+        kwds['engine'] = 'python'
+        return read_table(*args, **kwds)
+
+    def test_regex_separator(self):
+        data = """   A   B   C   D
+a   1   2   3   4
+b   1   2   3   4
+c   1   2   3   4
+"""
+        df = self.read_table(StringIO(data), sep='\s+')
+        expected = self.read_csv(StringIO(re.sub('[ ]+', ',', data)),
+                            index_col=0)
+        self.assert_(expected.index.name is None)
+        assert_frame_equal(df, expected)
+
     def test_comment(self):
         data = """A,B,C
 1,2.,4.#hello world
@@ -1402,6 +1358,47 @@ KORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
                           colspecs=colspecs, widths=[6, 10, 10, 7])
 
 
+    def test_verbose_import(self):
+        text = """a,b,c,d
+one,1,2,3
+one,1,2,3
+,1,2,3
+one,1,2,3
+,1,2,3
+,1,2,3
+one,1,2,3
+two,1,2,3"""
+
+        buf = StringIO()
+        sys.stdout = buf
+
+        try:
+            # it works!
+            df = self.read_csv(StringIO(text), verbose=True)
+            self.assert_(buf.getvalue() == 'Filled 3 NA values in column a\n')
+        finally:
+            sys.stdout = sys.__stdout__
+
+        buf = StringIO()
+        sys.stdout = buf
+
+        text = """a,b,c,d
+one,1,2,3
+two,1,2,3
+three,1,2,3
+four,1,2,3
+five,1,2,3
+,1,2,3
+seven,1,2,3
+eight,1,2,3"""
+
+        try:
+            # it works!
+            df = self.read_csv(StringIO(text), verbose=True, index_col=0)
+            self.assert_(buf.getvalue() == 'Filled 1 NA values in column a\n')
+        finally:
+            sys.stdout = sys.__stdout__
+
     def test_converters_corner_with_nas(self):
         import StringIO
         csv = """id,score,days
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index dbf45c619..4c87053d9 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -856,11 +856,6 @@ cdef _get_na_mask(parser_t *parser, int col, int line_start, int line_end,
     for i in range(lines):
         word = COLITER_NEXT(it)
 
-        # length 0
-        # if word[0] == '\x00':
-        #     result[i] = 1
-        #     continue
-
         k = kh_get_str(na_hashset, word)
         # in the hash table
         if k != na_hashset.n_buckets:
