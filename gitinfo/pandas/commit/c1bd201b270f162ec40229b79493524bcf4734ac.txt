commit c1bd201b270f162ec40229b79493524bcf4734ac
Author: Jeff Reback <jeff@reback.net>
Date:   Thu Feb 9 11:42:15 2017 -0500

    DEPR: remove statsmodels as a dependency
    
    remove pd.ols, pd.fama_macbeth from top-level namespace
    
    xref #11898
    closes https://github.com/pandas-dev/pandas2/issues/26
    
    previously deprecated in 0.18.0
    
    Author: Jeff Reback <jeff@reback.net>
    
    Closes #15353 from jreback/stats and squashes the following commits:
    
    9563740 [Jeff Reback] DEPR: remove statsmodels as a dependency

diff --git a/ci/requirements-2.7.pip b/ci/requirements-2.7.pip
index d7266fe88..d16b932c8 100644
--- a/ci/requirements-2.7.pip
+++ b/ci/requirements-2.7.pip
@@ -1,4 +1,3 @@
-statsmodels
 blosc
 httplib2
 google-api-python-client==1.2
diff --git a/ci/requirements-2.7_COMPAT.run b/ci/requirements-2.7_COMPAT.run
index 32d71beb2..d27b6a72c 100644
--- a/ci/requirements-2.7_COMPAT.run
+++ b/ci/requirements-2.7_COMPAT.run
@@ -4,7 +4,6 @@ pytz=2013b
 scipy=0.11.0
 xlwt=0.7.5
 xlrd=0.9.2
-statsmodels=0.4.3
 bottleneck=0.8.0
 numexpr=2.2.2
 pytables=3.0.0
diff --git a/ci/requirements-2.7_LOCALE.run b/ci/requirements-2.7_LOCALE.run
index 9bb37ee10..1a9b42d83 100644
--- a/ci/requirements-2.7_LOCALE.run
+++ b/ci/requirements-2.7_LOCALE.run
@@ -13,5 +13,4 @@ html5lib=1.0b2
 lxml=3.2.1
 scipy=0.11.0
 beautiful-soup=4.2.1
-statsmodels=0.4.3
 bigquery=2.0.17
diff --git a/ci/requirements-2.7_SLOW.run b/ci/requirements-2.7_SLOW.run
index 630d22636..c2d2a1428 100644
--- a/ci/requirements-2.7_SLOW.run
+++ b/ci/requirements-2.7_SLOW.run
@@ -4,7 +4,6 @@ numpy=1.8.2
 matplotlib=1.3.1
 scipy
 patsy
-statsmodels
 xlwt
 openpyxl
 xlsxwriter
diff --git a/ci/requirements-3.4_SLOW.run b/ci/requirements-3.4_SLOW.run
index 215f84038..39018439a 100644
--- a/ci/requirements-3.4_SLOW.run
+++ b/ci/requirements-3.4_SLOW.run
@@ -17,5 +17,4 @@ sqlalchemy
 bottleneck
 pymysql
 psycopg2
-statsmodels
 jinja2=2.8
diff --git a/doc/source/whatsnew/v0.20.0.txt b/doc/source/whatsnew/v0.20.0.txt
index 9afcf85c9..3fb6f7b0b 100644
--- a/doc/source/whatsnew/v0.20.0.txt
+++ b/doc/source/whatsnew/v0.20.0.txt
@@ -396,7 +396,7 @@ Removal of prior version deprecations/changes
 - The ``pandas.io.ga`` module with a ``google-analytics`` interface is removed (:issue:`11308`).
   Similar functionality can be found in the `Google2Pandas <https://github.com/panalysis/Google2Pandas>`__ package.
 - ``pd.to_datetime`` and ``pd.to_timedelta`` have dropped the ``coerce`` parameter in favor of ``errors`` (:issue:`13602`)
-
+- ``pandas.stats.fama_macbeth``, ``pandas.stats.ols``, ``pandas.stats.plm`` and ``pandas.stats.var``, as well as the top-level ``pandas.fama_macbeth`` and ``pandas.ols`` routines are removed. Similar functionaility can be found in the `statsmodels <shttp://www.statsmodels.org/dev/>`__ package. (:issue:`11898`)
 
 
 
diff --git a/pandas/api/tests/test_api.py b/pandas/api/tests/test_api.py
index 02165d82d..a53f6103b 100644
--- a/pandas/api/tests/test_api.py
+++ b/pandas/api/tests/test_api.py
@@ -42,7 +42,7 @@ class TestPDApi(Base, tm.TestCase):
                      'json', 'lib', 'index', 'parser']
 
     # these are already deprecated; awaiting removal
-    deprecated_modules = ['ols', 'stats', 'datetools']
+    deprecated_modules = ['stats', 'datetools']
 
     # misc
     misc = ['IndexSlice', 'NaT']
@@ -109,7 +109,7 @@ class TestPDApi(Base, tm.TestCase):
                         'expanding_max', 'expanding_mean', 'expanding_median',
                         'expanding_min', 'expanding_quantile',
                         'expanding_skew', 'expanding_std', 'expanding_sum',
-                        'expanding_var', 'fama_macbeth', 'rolling_apply',
+                        'expanding_var', 'rolling_apply',
                         'rolling_corr', 'rolling_count', 'rolling_cov',
                         'rolling_kurt', 'rolling_max', 'rolling_mean',
                         'rolling_median', 'rolling_min', 'rolling_quantile',
diff --git a/pandas/stats/api.py b/pandas/stats/api.py
index fd81b875f..2a11456d4 100644
--- a/pandas/stats/api.py
+++ b/pandas/stats/api.py
@@ -2,10 +2,6 @@
 Common namespace of statistical functions
 """
 
-# pylint: disable-msg=W0611,W0614,W0401
-
 # flake8: noqa
 
 from pandas.stats.moments import *
-from pandas.stats.interface import ols
-from pandas.stats.fama_macbeth import fama_macbeth
diff --git a/pandas/stats/common.py b/pandas/stats/common.py
deleted file mode 100644
index be3b842e9..000000000
--- a/pandas/stats/common.py
+++ /dev/null
@@ -1,45 +0,0 @@
-
-_WINDOW_TYPES = {
-    0: 'full_sample',
-    1: 'rolling',
-    2: 'expanding'
-}
-# also allow 'rolling' as key
-_WINDOW_TYPES.update((v, v) for k, v in list(_WINDOW_TYPES.items()))
-_ADDITIONAL_CLUSTER_TYPES = set(("entity", "time"))
-
-
-def _get_cluster_type(cluster_type):
-    # this was previous behavior
-    if cluster_type is None:
-        return cluster_type
-    try:
-        return _get_window_type(cluster_type)
-    except ValueError:
-        final_type = str(cluster_type).lower().replace("_", " ")
-        if final_type in _ADDITIONAL_CLUSTER_TYPES:
-            return final_type
-        raise ValueError('Unrecognized cluster type: %s' % cluster_type)
-
-
-def _get_window_type(window_type):
-    # e.g., 0, 1, 2
-    final_type = _WINDOW_TYPES.get(window_type)
-    # e.g., 'full_sample'
-    final_type = final_type or _WINDOW_TYPES.get(
-        str(window_type).lower().replace(" ", "_"))
-    if final_type is None:
-        raise ValueError('Unrecognized window type: %s' % window_type)
-    return final_type
-
-
-def banner(text, width=80):
-    """
-
-    """
-    toFill = width - len(text)
-
-    left = toFill // 2
-    right = toFill - left
-
-    return '%s%s%s' % ('-' * left, text, '-' * right)
diff --git a/pandas/stats/fama_macbeth.py b/pandas/stats/fama_macbeth.py
deleted file mode 100644
index d564f9cb6..000000000
--- a/pandas/stats/fama_macbeth.py
+++ /dev/null
@@ -1,241 +0,0 @@
-from pandas.core.base import StringMixin
-from pandas.compat import StringIO, range
-
-import numpy as np
-
-from pandas.core.api import Series, DataFrame
-import pandas.stats.common as common
-from pandas.util.decorators import cache_readonly
-
-# flake8: noqa
-
-
-def fama_macbeth(**kwargs):
-    """Runs Fama-MacBeth regression.
-
-    Parameters
-    ----------
-    Takes the same arguments as a panel OLS, in addition to:
-
-    nw_lags_beta: int
-       Newey-West adjusts the betas by the given lags
-       """
-    window_type = kwargs.get('window_type')
-    if window_type is None:
-        klass = FamaMacBeth
-    else:
-        klass = MovingFamaMacBeth
-
-    return klass(**kwargs)
-
-
-class FamaMacBeth(StringMixin):
-
-    def __init__(self, y, x, intercept=True, nw_lags=None,
-                 nw_lags_beta=None,
-                 entity_effects=False, time_effects=False, x_effects=None,
-                 cluster=None, dropped_dummies=None, verbose=False):
-        import warnings
-        warnings.warn("The pandas.stats.fama_macbeth module is deprecated and will be "
-                      "removed in a future version. We refer to external packages "
-                      "like statsmodels, see here: "
-                      "http://www.statsmodels.org/stable/index.html",
-                      FutureWarning, stacklevel=4)
-
-        if dropped_dummies is None:
-            dropped_dummies = {}
-        self._nw_lags_beta = nw_lags_beta
-
-        from pandas.stats.plm import MovingPanelOLS
-        self._ols_result = MovingPanelOLS(
-            y=y, x=x, window_type='rolling', window=1,
-            intercept=intercept,
-            nw_lags=nw_lags, entity_effects=entity_effects,
-            time_effects=time_effects, x_effects=x_effects, cluster=cluster,
-            dropped_dummies=dropped_dummies, verbose=verbose)
-
-        self._cols = self._ols_result._x.columns
-
-    @cache_readonly
-    def _beta_raw(self):
-        return self._ols_result._beta_raw
-
-    @cache_readonly
-    def _stats(self):
-        return _calc_t_stat(self._beta_raw, self._nw_lags_beta)
-
-    @cache_readonly
-    def _mean_beta_raw(self):
-        return self._stats[0]
-
-    @cache_readonly
-    def _std_beta_raw(self):
-        return self._stats[1]
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        return self._stats[2]
-
-    def _make_result(self, result):
-        return Series(result, index=self._cols)
-
-    @cache_readonly
-    def mean_beta(self):
-        return self._make_result(self._mean_beta_raw)
-
-    @cache_readonly
-    def std_beta(self):
-        return self._make_result(self._std_beta_raw)
-
-    @cache_readonly
-    def t_stat(self):
-        return self._make_result(self._t_stat_raw)
-
-    @cache_readonly
-    def _results(self):
-        return {
-            'mean_beta': self._mean_beta_raw,
-            'std_beta': self._std_beta_raw,
-            't_stat': self._t_stat_raw,
-        }
-
-    @cache_readonly
-    def _coef_table(self):
-        buffer = StringIO()
-        buffer.write('%13s %13s %13s %13s %13s %13s\n' %
-                     ('Variable', 'Beta', 'Std Err', 't-stat', 'CI 2.5%', 'CI 97.5%'))
-        template = '%13s %13.4f %13.4f %13.2f %13.4f %13.4f\n'
-
-        for i, name in enumerate(self._cols):
-            if i and not (i % 5):
-                buffer.write('\n' + common.banner(''))
-
-            mean_beta = self._results['mean_beta'][i]
-            std_beta = self._results['std_beta'][i]
-            t_stat = self._results['t_stat'][i]
-            ci1 = mean_beta - 1.96 * std_beta
-            ci2 = mean_beta + 1.96 * std_beta
-
-            values = '(%s)' % name, mean_beta, std_beta, t_stat, ci1, ci2
-
-            buffer.write(template % values)
-
-        if self._nw_lags_beta is not None:
-            buffer.write('\n')
-            buffer.write('*** The Std Err, t-stat are Newey-West '
-                         'adjusted with Lags %5d\n' % self._nw_lags_beta)
-
-        return buffer.getvalue()
-
-    def __unicode__(self):
-        return self.summary
-
-    @cache_readonly
-    def summary(self):
-        template = """
-----------------------Summary of Fama-MacBeth Analysis-------------------------
-
-Formula: Y ~ %(formulaRHS)s
-# betas : %(nu)3d
-
-----------------------Summary of Estimated Coefficients------------------------
-%(coefTable)s
---------------------------------End of Summary---------------------------------
-"""
-        params = {
-            'formulaRHS': ' + '.join(self._cols),
-            'nu': len(self._beta_raw),
-            'coefTable': self._coef_table,
-        }
-
-        return template % params
-
-
-class MovingFamaMacBeth(FamaMacBeth):
-
-    def __init__(self, y, x, window_type='rolling', window=10,
-                 intercept=True, nw_lags=None, nw_lags_beta=None,
-                 entity_effects=False, time_effects=False, x_effects=None,
-                 cluster=None, dropped_dummies=None, verbose=False):
-        if dropped_dummies is None:
-            dropped_dummies = {}
-        self._window_type = common._get_window_type(window_type)
-        self._window = window
-
-        FamaMacBeth.__init__(
-            self, y=y, x=x, intercept=intercept,
-            nw_lags=nw_lags, nw_lags_beta=nw_lags_beta,
-            entity_effects=entity_effects, time_effects=time_effects,
-            x_effects=x_effects, cluster=cluster,
-            dropped_dummies=dropped_dummies, verbose=verbose)
-
-        self._index = self._ols_result._index
-        self._T = len(self._index)
-
-    @property
-    def _is_rolling(self):
-        return self._window_type == 'rolling'
-
-    def _calc_stats(self):
-        mean_betas = []
-        std_betas = []
-        t_stats = []
-
-        # XXX
-
-        mask = self._ols_result._rolling_ols_call[2]
-        obs_total = mask.astype(int).cumsum()
-
-        start = self._window - 1
-        betas = self._beta_raw
-        for i in range(start, self._T):
-            if self._is_rolling:
-                begin = i - start
-            else:
-                begin = 0
-
-            B = betas[max(obs_total[begin] - 1, 0): obs_total[i]]
-            mean_beta, std_beta, t_stat = _calc_t_stat(B, self._nw_lags_beta)
-            mean_betas.append(mean_beta)
-            std_betas.append(std_beta)
-            t_stats.append(t_stat)
-
-        return np.array([mean_betas, std_betas, t_stats])
-
-    _stats = cache_readonly(_calc_stats)
-
-    def _make_result(self, result):
-        return DataFrame(result, index=self._result_index, columns=self._cols)
-
-    @cache_readonly
-    def _result_index(self):
-        mask = self._ols_result._rolling_ols_call[2]
-        # HACK XXX
-        return self._index[mask.cumsum() >= self._window]
-
-    @cache_readonly
-    def _results(self):
-        return {
-            'mean_beta': self._mean_beta_raw[-1],
-            'std_beta': self._std_beta_raw[-1],
-            't_stat': self._t_stat_raw[-1],
-        }
-
-
-def _calc_t_stat(beta, nw_lags_beta):
-    N = len(beta)
-    B = beta - beta.mean(0)
-    C = np.dot(B.T, B) / N
-
-    if nw_lags_beta is not None:
-        for i in range(nw_lags_beta + 1):
-
-            cov = np.dot(B[i:].T, B[:(N - i)]) / N
-            weight = i / (nw_lags_beta + 1)
-            C += 2 * (1 - weight) * cov
-
-    mean_beta = beta.mean(0)
-    std_beta = np.sqrt(np.diag(C)) / np.sqrt(N)
-    t_stat = mean_beta / std_beta
-
-    return mean_beta, std_beta, t_stat
diff --git a/pandas/stats/interface.py b/pandas/stats/interface.py
deleted file mode 100644
index caf468b4f..000000000
--- a/pandas/stats/interface.py
+++ /dev/null
@@ -1,143 +0,0 @@
-from pandas.core.api import Series, DataFrame, Panel, MultiIndex
-from pandas.stats.ols import OLS, MovingOLS
-from pandas.stats.plm import PanelOLS, MovingPanelOLS, NonPooledPanelOLS
-import pandas.stats.common as common
-
-
-def ols(**kwargs):
-    """Returns the appropriate OLS object depending on whether you need
-    simple or panel OLS, and a full-sample or rolling/expanding OLS.
-
-    Will be a normal linear regression or a (pooled) panel regression depending
-    on the type of the inputs:
-
-    y : Series, x : DataFrame -> OLS
-    y : Series, x : dict of DataFrame -> OLS
-    y : DataFrame, x : DataFrame -> PanelOLS
-    y : DataFrame, x : dict of DataFrame/Panel -> PanelOLS
-    y : Series with MultiIndex, x : Panel/DataFrame + MultiIndex -> PanelOLS
-
-    Parameters
-    ----------
-    y: Series or DataFrame
-        See above for types
-    x: Series, DataFrame, dict of Series, dict of DataFrame, Panel
-    weights : Series or ndarray
-        The weights are presumed to be (proportional to) the inverse of the
-        variance of the observations.  That is, if the variables are to be
-        transformed by 1/sqrt(W) you must supply weights = 1/W
-    intercept: bool
-        True if you want an intercept.  Defaults to True.
-    nw_lags: None or int
-        Number of Newey-West lags.  Defaults to None.
-    nw_overlap: bool
-        Whether there are overlaps in the NW lags.  Defaults to False.
-    window_type: {'full sample', 'rolling', 'expanding'}
-        'full sample' by default
-    window: int
-        size of window (for rolling/expanding OLS). If window passed and no
-        explicit window_type, 'rolling" will be used as the window_type
-
-    Panel OLS options:
-        pool: bool
-            Whether to run pooled panel regression.  Defaults to true.
-        entity_effects: bool
-            Whether to account for entity fixed effects.  Defaults to false.
-        time_effects: bool
-            Whether to account for time fixed effects.  Defaults to false.
-        x_effects: list
-            List of x's to account for fixed effects.  Defaults to none.
-        dropped_dummies: dict
-            Key is the name of the variable for the fixed effect.
-            Value is the value of that variable for which we drop the dummy.
-
-            For entity fixed effects, key equals 'entity'.
-
-            By default, the first dummy is dropped if no dummy is specified.
-        cluster: {'time', 'entity'}
-            cluster variances
-
-    Examples
-    --------
-    # Run simple OLS.
-    result = ols(y=y, x=x)
-
-    # Run rolling simple OLS with window of size 10.
-    result = ols(y=y, x=x, window_type='rolling', window=10)
-    print(result.beta)
-
-    result = ols(y=y, x=x, nw_lags=1)
-
-    # Set up LHS and RHS for data across all items
-    y = A
-    x = {'B' : B, 'C' : C}
-
-    # Run panel OLS.
-    result = ols(y=y, x=x)
-
-    # Run expanding panel OLS with window 10 and entity clustering.
-    result = ols(y=y, x=x, cluster='entity', window_type='expanding',
-                 window=10)
-
-    Returns
-    -------
-    The appropriate OLS object, which allows you to obtain betas and various
-    statistics, such as std err, t-stat, etc.
-    """
-
-    if (kwargs.get('cluster') is not None and
-            kwargs.get('nw_lags') is not None):
-        raise ValueError(
-            'Pandas OLS does not work with Newey-West correction '
-            'and clustering.')
-
-    pool = kwargs.get('pool')
-    if 'pool' in kwargs:
-        del kwargs['pool']
-
-    window_type = kwargs.get('window_type')
-    window = kwargs.get('window')
-
-    if window_type is None:
-        if window is None:
-            window_type = 'full_sample'
-        else:
-            window_type = 'rolling'
-    else:
-        window_type = common._get_window_type(window_type)
-
-    if window_type != 'full_sample':
-        kwargs['window_type'] = common._get_window_type(window_type)
-
-    y = kwargs.get('y')
-    x = kwargs.get('x')
-
-    panel = False
-    if isinstance(y, DataFrame) or (isinstance(y, Series) and
-                                    isinstance(y.index, MultiIndex)):
-        panel = True
-    if isinstance(x, Panel):
-        panel = True
-
-    if window_type == 'full_sample':
-        for rolling_field in ('window_type', 'window', 'min_periods'):
-            if rolling_field in kwargs:
-                del kwargs[rolling_field]
-
-        if panel:
-            if pool is False:
-                klass = NonPooledPanelOLS
-            else:
-                klass = PanelOLS
-        else:
-            klass = OLS
-    else:
-        if panel:
-            if pool is False:
-                klass = NonPooledPanelOLS
-            else:
-                klass = MovingPanelOLS
-        else:
-            klass = MovingOLS
-
-    return klass(**kwargs)
diff --git a/pandas/stats/math.py b/pandas/stats/math.py
deleted file mode 100644
index 505415beb..000000000
--- a/pandas/stats/math.py
+++ /dev/null
@@ -1,130 +0,0 @@
-# pylint: disable-msg=E1103
-# pylint: disable-msg=W0212
-
-from __future__ import division
-
-from pandas.compat import range
-import numpy as np
-import numpy.linalg as linalg
-
-
-def rank(X, cond=1.0e-12):
-    """
-    Return the rank of a matrix X based on its generalized inverse,
-    not the SVD.
-    """
-    X = np.asarray(X)
-    if len(X.shape) == 2:
-        import scipy.linalg as SL
-        D = SL.svdvals(X)
-        result = np.add.reduce(np.greater(D / D.max(), cond))
-        return int(result.astype(np.int32))
-    else:
-        return int(not np.alltrue(np.equal(X, 0.)))
-
-
-def solve(a, b):
-    """Returns the solution of A X = B."""
-    try:
-        return linalg.solve(a, b)
-    except linalg.LinAlgError:
-        return np.dot(linalg.pinv(a), b)
-
-
-def inv(a):
-    """Returns the inverse of A."""
-    try:
-        return np.linalg.inv(a)
-    except linalg.LinAlgError:
-        return np.linalg.pinv(a)
-
-
-def is_psd(m):
-    eigvals = linalg.eigvals(m)
-    return np.isreal(eigvals).all() and (eigvals >= 0).all()
-
-
-def newey_west(m, max_lags, nobs, df, nw_overlap=False):
-    """
-    Compute Newey-West adjusted covariance matrix, taking into account
-    specified number of leads / lags
-
-    Parameters
-    ----------
-    m : (N x K)
-    max_lags : int
-    nobs : int
-        Number of observations in model
-    df : int
-        Degrees of freedom in explanatory variables
-    nw_overlap : boolean, default False
-        Assume data is overlapping
-
-    Returns
-    -------
-    ndarray (K x K)
-
-    Reference
-    ---------
-    Newey, W. K. & West, K. D. (1987) A Simple, Positive
-    Semi-definite, Heteroskedasticity and Autocorrelation Consistent
-    Covariance Matrix, Econometrica, vol. 55(3), 703-708
-    """
-    Xeps = np.dot(m.T, m)
-    for lag in range(1, max_lags + 1):
-        auto_cov = np.dot(m[:-lag].T, m[lag:])
-        weight = lag / (max_lags + 1)
-        if nw_overlap:
-            weight = 0
-        bb = auto_cov + auto_cov.T
-        dd = (1 - weight) * bb
-        Xeps += dd
-
-    Xeps *= nobs / (nobs - df)
-
-    if nw_overlap and not is_psd(Xeps):
-        new_max_lags = int(np.ceil(max_lags * 1.5))
-#         print('nw_overlap is True and newey_west generated a non positive '
-#               'semidefinite matrix, so using newey_west with max_lags of %d.'
-#               % new_max_lags)
-        return newey_west(m, new_max_lags, nobs, df)
-
-    return Xeps
-
-
-def calc_F(R, r, beta, var_beta, nobs, df):
-    """
-    Computes the standard F-test statistic for linear restriction
-    hypothesis testing
-
-    Parameters
-    ----------
-    R: ndarray (N x N)
-        Restriction matrix
-    r: ndarray (N x 1)
-        Restriction vector
-    beta: ndarray (N x 1)
-        Estimated model coefficients
-    var_beta: ndarray (N x N)
-        Variance covariance matrix of regressors
-    nobs: int
-        Number of observations in model
-    df: int
-        Model degrees of freedom
-
-    Returns
-    -------
-    F value, (q, df_resid), p value
-    """
-    from scipy.stats import f
-
-    hyp = np.dot(R, beta.reshape(len(beta), 1)) - r
-    RSR = np.dot(R, np.dot(var_beta, R.T))
-
-    q = len(r)
-
-    F = np.dot(hyp.T, np.dot(inv(RSR), hyp)).squeeze() / q
-
-    p_value = 1 - f.cdf(F, q, nobs - df)
-
-    return F, (q, nobs - df), p_value
diff --git a/pandas/stats/misc.py b/pandas/stats/misc.py
deleted file mode 100644
index 1a077dcb6..000000000
--- a/pandas/stats/misc.py
+++ /dev/null
@@ -1,389 +0,0 @@
-from numpy import NaN
-from pandas import compat
-import numpy as np
-
-from pandas.core.api import Series, DataFrame
-from pandas.core.series import remove_na
-from pandas.compat import zip, lrange
-import pandas.core.common as com
-
-
-def zscore(series):
-    return (series - series.mean()) / np.std(series, ddof=0)
-
-
-def correl_ts(frame1, frame2):
-    """
-    Pairwise correlation of columns of two DataFrame objects
-
-    Parameters
-    ----------
-
-    Returns
-    -------
-    y : Series
-    """
-    results = {}
-    for col, series in compat.iteritems(frame1):
-        if col in frame2:
-            other = frame2[col]
-
-            idx1 = series.valid().index
-            idx2 = other.valid().index
-
-            common_index = idx1.intersection(idx2)
-
-            seriesStand = zscore(series.reindex(common_index))
-            otherStand = zscore(other.reindex(common_index))
-            results[col] = (seriesStand * otherStand).mean()
-
-    return Series(results)
-
-
-def correl_xs(frame1, frame2):
-    return correl_ts(frame1.T, frame2.T)
-
-
-def percentileofscore(a, score, kind='rank'):
-    """The percentile rank of a score relative to a list of scores.
-
-    A `percentileofscore` of, for example, 80% means that 80% of the
-    scores in `a` are below the given score. In the case of gaps or
-    ties, the exact definition depends on the optional keyword, `kind`.
-
-    Parameters
-    ----------
-    a: array like
-        Array of scores to which `score` is compared.
-    score: int or float
-        Score that is compared to the elements in `a`.
-    kind: {'rank', 'weak', 'strict', 'mean'}, optional
-        This optional parameter specifies the interpretation of the
-        resulting score:
-
-        - "rank": Average percentage ranking of score.  In case of
-                  multiple matches, average the percentage rankings of
-                  all matching scores.
-        - "weak": This kind corresponds to the definition of a cumulative
-                  distribution function.  A percentileofscore of 80%
-                  means that 80% of values are less than or equal
-                  to the provided score.
-        - "strict": Similar to "weak", except that only values that are
-                    strictly less than the given score are counted.
-        - "mean": The average of the "weak" and "strict" scores, often used in
-                  testing.  See
-
-                  http://en.wikipedia.org/wiki/Percentile_rank
-
-    Returns
-    -------
-    pcos : float
-        Percentile-position of score (0-100) relative to `a`.
-
-    Examples
-    --------
-    Three-quarters of the given values lie below a given score:
-
-    >>> percentileofscore([1, 2, 3, 4], 3)
-    75.0
-
-    With multiple matches, note how the scores of the two matches, 0.6
-    and 0.8 respectively, are averaged:
-
-    >>> percentileofscore([1, 2, 3, 3, 4], 3)
-    70.0
-
-    Only 2/5 values are strictly less than 3:
-
-    >>> percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')
-    40.0
-
-    But 4/5 values are less than or equal to 3:
-
-    >>> percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')
-    80.0
-
-    The average between the weak and the strict scores is
-
-    >>> percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')
-    60.0
-
-    """
-    a = np.array(a)
-    n = len(a)
-
-    if kind == 'rank':
-        if not(np.any(a == score)):
-            a = np.append(a, score)
-            a_len = np.array(lrange(len(a)))
-        else:
-            a_len = np.array(lrange(len(a))) + 1.0
-
-        a = np.sort(a)
-        idx = [a == score]
-        pct = (np.mean(a_len[idx]) / n) * 100.0
-        return pct
-
-    elif kind == 'strict':
-        return sum(a < score) / float(n) * 100
-    elif kind == 'weak':
-        return sum(a <= score) / float(n) * 100
-    elif kind == 'mean':
-        return (sum(a < score) + sum(a <= score)) * 50 / float(n)
-    else:
-        raise ValueError("kind can only be 'rank', 'strict', 'weak' or 'mean'")
-
-
-def percentileRank(frame, column=None, kind='mean'):
-    """
-    Return score at percentile for each point in time (cross-section)
-
-    Parameters
-    ----------
-    frame: DataFrame
-    column: string or Series, optional
-       Column name or specific Series to compute percentiles for.
-       If not provided, percentiles are computed for all values at each
-       point in time. Note that this can take a LONG time.
-    kind: {'rank', 'weak', 'strict', 'mean'}, optional
-        This optional parameter specifies the interpretation of the
-        resulting score:
-
-        - "rank": Average percentage ranking of score.  In case of
-                  multiple matches, average the percentage rankings of
-                  all matching scores.
-        - "weak": This kind corresponds to the definition of a cumulative
-                  distribution function.  A percentileofscore of 80%
-                  means that 80% of values are less than or equal
-                  to the provided score.
-        - "strict": Similar to "weak", except that only values that are
-                    strictly less than the given score are counted.
-        - "mean": The average of the "weak" and "strict" scores, often used in
-                  testing.  See
-
-                  http://en.wikipedia.org/wiki/Percentile_rank
-
-    Returns
-    -------
-    TimeSeries or DataFrame, depending on input
-    """
-    fun = lambda xs, score: percentileofscore(remove_na(xs),
-                                              score, kind=kind)
-
-    results = {}
-    framet = frame.T
-    if column is not None:
-        if isinstance(column, Series):
-            for date, xs in compat.iteritems(frame.T):
-                results[date] = fun(xs, column.get(date, NaN))
-        else:
-            for date, xs in compat.iteritems(frame.T):
-                results[date] = fun(xs, xs[column])
-        results = Series(results)
-    else:
-        for column in frame.columns:
-            for date, xs in compat.iteritems(framet):
-                results.setdefault(date, {})[column] = fun(xs, xs[column])
-        results = DataFrame(results).T
-    return results
-
-
-def bucket(series, k, by=None):
-    """
-    Produce DataFrame representing quantiles of a Series
-
-    Parameters
-    ----------
-    series : Series
-    k : int
-        number of quantiles
-    by : Series or same-length array
-        bucket by value
-
-    Returns
-    -------
-    DataFrame
-    """
-    if by is None:
-        by = series
-    else:
-        by = by.reindex(series.index)
-
-    split = _split_quantile(by, k)
-    mat = np.empty((len(series), k), dtype=float) * np.NaN
-
-    for i, v in enumerate(split):
-        mat[:, i][v] = series.take(v)
-
-    return DataFrame(mat, index=series.index, columns=np.arange(k) + 1)
-
-
-def _split_quantile(arr, k):
-    arr = np.asarray(arr)
-    mask = np.isfinite(arr)
-    order = arr[mask].argsort()
-    n = len(arr)
-
-    return np.array_split(np.arange(n)[mask].take(order), k)
-
-
-def bucketcat(series, cats):
-    """
-    Produce DataFrame representing quantiles of a Series
-
-    Parameters
-    ----------
-    series : Series
-    cat : Series or same-length array
-        bucket by category; mutually exclusive with 'by'
-
-    Returns
-    -------
-    DataFrame
-    """
-    if not isinstance(series, Series):
-        series = Series(series, index=np.arange(len(series)))
-
-    cats = np.asarray(cats)
-
-    unique_labels = np.unique(cats)
-    unique_labels = unique_labels[com.notnull(unique_labels)]
-
-    # group by
-    data = {}
-
-    for label in unique_labels:
-        data[label] = series[cats == label]
-
-    return DataFrame(data, columns=unique_labels)
-
-
-def bucketpanel(series, bins=None, by=None, cat=None):
-    """
-    Bucket data by two Series to create summary panel
-
-    Parameters
-    ----------
-    series : Series
-    bins : tuple (length-2)
-        e.g. (2, 2)
-    by : tuple of Series
-        bucket by value
-    cat : tuple of Series
-        bucket by category; mutually exclusive with 'by'
-
-    Returns
-    -------
-    DataFrame
-    """
-    use_by = by is not None
-    use_cat = cat is not None
-
-    if use_by and use_cat:
-        raise Exception('must specify by or cat, but not both')
-    elif use_by:
-        if len(by) != 2:
-            raise Exception('must provide two bucketing series')
-
-        xby, yby = by
-        xbins, ybins = bins
-
-        return _bucketpanel_by(series, xby, yby, xbins, ybins)
-
-    elif use_cat:
-        xcat, ycat = cat
-        return _bucketpanel_cat(series, xcat, ycat)
-    else:
-        raise Exception('must specify either values or categories '
-                        'to bucket by')
-
-
-def _bucketpanel_by(series, xby, yby, xbins, ybins):
-    xby = xby.reindex(series.index)
-    yby = yby.reindex(series.index)
-
-    xlabels = _bucket_labels(xby.reindex(series.index), xbins)
-    ylabels = _bucket_labels(yby.reindex(series.index), ybins)
-
-    labels = _uniquify(xlabels, ylabels, xbins, ybins)
-
-    mask = com.isnull(labels)
-    labels[mask] = -1
-
-    unique_labels = np.unique(labels)
-    bucketed = bucketcat(series, labels)
-
-    _ulist = list(labels)
-    index_map = dict((x, _ulist.index(x)) for x in unique_labels)
-
-    def relabel(key):
-        pos = index_map[key]
-
-        xlab = xlabels[pos]
-        ylab = ylabels[pos]
-
-        return '%sx%s' % (int(xlab) if com.notnull(xlab) else 'NULL',
-                          int(ylab) if com.notnull(ylab) else 'NULL')
-
-    return bucketed.rename(columns=relabel)
-
-
-def _bucketpanel_cat(series, xcat, ycat):
-    xlabels, xmapping = _intern(xcat)
-    ylabels, ymapping = _intern(ycat)
-
-    shift = 10 ** (np.ceil(np.log10(ylabels.max())))
-    labels = xlabels * shift + ylabels
-
-    sorter = labels.argsort()
-    sorted_labels = labels.take(sorter)
-    sorted_xlabels = xlabels.take(sorter)
-    sorted_ylabels = ylabels.take(sorter)
-
-    unique_labels = np.unique(labels)
-    unique_labels = unique_labels[com.notnull(unique_labels)]
-
-    locs = sorted_labels.searchsorted(unique_labels)
-    xkeys = sorted_xlabels.take(locs)
-    ykeys = sorted_ylabels.take(locs)
-
-    stringified = ['(%s, %s)' % arg
-                   for arg in zip(xmapping.take(xkeys), ymapping.take(ykeys))]
-
-    result = bucketcat(series, labels)
-    result.columns = stringified
-
-    return result
-
-
-def _intern(values):
-    # assumed no NaN values
-    values = np.asarray(values)
-
-    uniqued = np.unique(values)
-    labels = uniqued.searchsorted(values)
-    return labels, uniqued
-
-
-def _uniquify(xlabels, ylabels, xbins, ybins):
-    # encode the stuff, create unique label
-    shifter = 10 ** max(xbins, ybins)
-    _xpiece = xlabels * shifter
-    _ypiece = ylabels
-
-    return _xpiece + _ypiece
-
-
-def _bucket_labels(series, k):
-    arr = np.asarray(series)
-    mask = np.isfinite(arr)
-    order = arr[mask].argsort()
-    n = len(series)
-
-    split = np.array_split(np.arange(n)[mask].take(order), k)
-
-    mat = np.empty(n, dtype=float) * np.NaN
-    for i, v in enumerate(split):
-        mat[v] = i
-
-    return mat + 1
diff --git a/pandas/stats/ols.py b/pandas/stats/ols.py
deleted file mode 100644
index 96ec70d59..000000000
--- a/pandas/stats/ols.py
+++ /dev/null
@@ -1,1377 +0,0 @@
-"""
-Ordinary least squares regression
-"""
-
-# pylint: disable-msg=W0201
-
-# flake8: noqa
-
-from pandas.compat import zip, range, StringIO
-from itertools import starmap
-from pandas import compat
-import numpy as np
-
-from pandas.core.api import DataFrame, Series, isnull
-from pandas.core.base import StringMixin
-from pandas.types.common import _ensure_float64
-from pandas.core.index import MultiIndex
-from pandas.core.panel import Panel
-from pandas.util.decorators import cache_readonly
-
-import pandas.stats.common as scom
-import pandas.stats.math as math
-import pandas.stats.moments as moments
-
-_FP_ERR = 1e-8
-
-
-class OLS(StringMixin):
-    """
-    Runs a full sample ordinary least squares regression.
-
-    Parameters
-    ----------
-    y : Series
-    x : Series, DataFrame, dict of Series
-    intercept : bool
-        True if you want an intercept.
-    weights : array-like, optional
-        1d array of weights.  If you supply 1/W then the variables are pre-
-        multiplied by 1/sqrt(W).  If no weights are supplied the default value
-        is 1 and WLS reults are the same as OLS.
-    nw_lags : None or int
-        Number of Newey-West lags.
-    nw_overlap : boolean, default False
-        Assume data is overlapping when computing Newey-West estimator
-
-    """
-    _panel_model = False
-
-    def __init__(self, y, x, intercept=True, weights=None, nw_lags=None,
-                 nw_overlap=False):
-        import warnings
-        warnings.warn("The pandas.stats.ols module is deprecated and will be "
-                      "removed in a future version. We refer to external packages "
-                      "like statsmodels, see some examples here: "
-                      "http://www.statsmodels.org/stable/regression.html",
-                      FutureWarning, stacklevel=4)
-
-        try:
-            import statsmodels.api as sm
-        except ImportError:
-            import scikits.statsmodels.api as sm
-
-        self._x_orig = x
-        self._y_orig = y
-        self._weights_orig = weights
-        self._intercept = intercept
-        self._nw_lags = nw_lags
-        self._nw_overlap = nw_overlap
-
-        (self._y, self._x, self._weights, self._x_filtered,
-         self._index, self._time_has_obs) = self._prepare_data()
-
-        if self._weights is not None:
-            self._x_trans = self._x.mul(np.sqrt(self._weights), axis=0)
-            self._y_trans = self._y * np.sqrt(self._weights)
-            self.sm_ols = sm.WLS(self._y.get_values(),
-                                 self._x.get_values(),
-                                 weights=self._weights.values).fit()
-        else:
-            self._x_trans = self._x
-            self._y_trans = self._y
-            self.sm_ols = sm.OLS(self._y.get_values(),
-                                 self._x.get_values()).fit()
-
-    def _prepare_data(self):
-        """
-        Cleans the input for single OLS.
-
-        Parameters
-        ----------
-        lhs: Series
-            Dependent variable in the regression.
-        rhs: dict, whose values are Series, DataFrame, or dict
-            Explanatory variables of the regression.
-
-        Returns
-        -------
-        Series, DataFrame
-            Cleaned lhs and rhs
-        """
-        (filt_lhs, filt_rhs, filt_weights,
-         pre_filt_rhs, index, valid) = _filter_data(self._y_orig, self._x_orig,
-                                                    self._weights_orig)
-        if self._intercept:
-            filt_rhs['intercept'] = 1.
-            pre_filt_rhs['intercept'] = 1.
-
-        if hasattr(filt_weights, 'to_dense'):
-            filt_weights = filt_weights.to_dense()
-
-        return (filt_lhs, filt_rhs, filt_weights,
-                pre_filt_rhs, index, valid)
-
-    @property
-    def nobs(self):
-        return self._nobs
-
-    @property
-    def _nobs(self):
-        return len(self._y)
-
-    @property
-    def nw_lags(self):
-        return self._nw_lags
-
-    @property
-    def x(self):
-        """Returns the filtered x used in the regression."""
-        return self._x
-
-    @property
-    def y(self):
-        """Returns the filtered y used in the regression."""
-        return self._y
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        return self.sm_ols.params
-
-    @cache_readonly
-    def beta(self):
-        """Returns the betas in Series form."""
-        return Series(self._beta_raw, index=self._x.columns)
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        return math.rank(self._x.values)
-
-    @cache_readonly
-    def df(self):
-        """Returns the degrees of freedom.
-
-        This equals the rank of the X matrix.
-        """
-        return self._df_raw
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self.sm_ols.df_model
-
-    @cache_readonly
-    def df_model(self):
-        """Returns the degrees of freedom of the model."""
-        return self._df_model_raw
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self.sm_ols.df_resid
-
-    @cache_readonly
-    def df_resid(self):
-        """Returns the degrees of freedom of the residuals."""
-        return self._df_resid_raw
-
-    @cache_readonly
-    def _f_stat_raw(self):
-        """Returns the raw f-stat value."""
-        from scipy.stats import f
-
-        cols = self._x.columns
-
-        if self._nw_lags is None:
-            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
-
-            q = len(cols)
-            if 'intercept' in cols:
-                q -= 1
-
-            shape = q, self.df_resid
-            p_value = 1 - f.cdf(F, shape[0], shape[1])
-            return F, shape, p_value
-
-        k = len(cols)
-        R = np.eye(k)
-        r = np.zeros((k, 1))
-
-        try:
-            intercept = cols.get_loc('intercept')
-            R = np.concatenate((R[0: intercept], R[intercept + 1:]))
-            r = np.concatenate((r[0: intercept], r[intercept + 1:]))
-        except KeyError:
-            # no intercept
-            pass
-
-        return math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
-                           self._nobs, self.df)
-
-    @cache_readonly
-    def f_stat(self):
-        """Returns the f-stat value."""
-        return f_stat_to_dict(self._f_stat_raw)
-
-    def f_test(self, hypothesis):
-        """Runs the F test, given a joint hypothesis.  The hypothesis is
-        represented by a collection of equations, in the form
-
-        A*x_1+B*x_2=C
-
-        You must provide the coefficients even if they're 1.  No spaces.
-
-        The equations can be passed as either a single string or a
-        list of strings.
-
-        Examples
-        --------
-        o = ols(...)
-        o.f_test('1*x1+2*x2=0,1*x3=0')
-        o.f_test(['1*x1+2*x2=0','1*x3=0'])
-        """
-
-        x_names = self._x.columns
-
-        R = []
-        r = []
-
-        if isinstance(hypothesis, str):
-            eqs = hypothesis.split(',')
-        elif isinstance(hypothesis, list):
-            eqs = hypothesis
-        else:  # pragma: no cover
-            raise Exception('hypothesis must be either string or list')
-        for equation in eqs:
-            row = np.zeros(len(x_names))
-            lhs, rhs = equation.split('=')
-            for s in lhs.split('+'):
-                ss = s.split('*')
-                coeff = float(ss[0])
-                x_name = ss[1]
-
-                if x_name not in x_names:
-                    raise Exception('no coefficient named %s' % x_name)
-                idx = x_names.get_loc(x_name)
-                row[idx] = coeff
-            rhs = float(rhs)
-
-            R.append(row)
-            r.append(rhs)
-
-        R = np.array(R)
-        q = len(r)
-        r = np.array(r).reshape(q, 1)
-
-        result = math.calc_F(R, r, self._beta_raw, self._var_beta_raw,
-                             self._nobs, self.df)
-
-        return f_stat_to_dict(result)
-
-    @cache_readonly
-    def _p_value_raw(self):
-        """Returns the raw p values."""
-        from scipy.stats import t
-
-        return 2 * t.sf(np.fabs(self._t_stat_raw),
-                        self._df_resid_raw)
-
-    @cache_readonly
-    def p_value(self):
-        """Returns the p values."""
-        return Series(self._p_value_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _r2_raw(self):
-        """Returns the raw r-squared values."""
-        if self._use_centered_tss:
-            return 1 - self.sm_ols.ssr / self.sm_ols.centered_tss
-        else:
-            return 1 - self.sm_ols.ssr / self.sm_ols.uncentered_tss
-
-    @property
-    def _use_centered_tss(self):
-        # has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
-        return self._intercept
-
-    @cache_readonly
-    def r2(self):
-        """Returns the r-squared values."""
-        return self._r2_raw
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        return self.sm_ols.rsquared_adj
-
-    @cache_readonly
-    def r2_adj(self):
-        """Returns the r-squared adjusted values."""
-        return self._r2_adj_raw
-
-    @cache_readonly
-    def _resid_raw(self):
-        """Returns the raw residuals."""
-        return self.sm_ols.resid
-
-    @cache_readonly
-    def resid(self):
-        """Returns the residuals."""
-        return Series(self._resid_raw, index=self._x.index)
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        return np.sqrt(self.sm_ols.mse_resid)
-
-    @cache_readonly
-    def rmse(self):
-        """Returns the rmse value."""
-        return self._rmse_raw
-
-    @cache_readonly
-    def _std_err_raw(self):
-        """Returns the raw standard err values."""
-        return np.sqrt(np.diag(self._var_beta_raw))
-
-    @cache_readonly
-    def std_err(self):
-        """Returns the standard err values of the betas."""
-        return Series(self._std_err_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        """Returns the raw t-stat value."""
-        return self._beta_raw / self._std_err_raw
-
-    @cache_readonly
-    def t_stat(self):
-        """Returns the t-stat values of the betas."""
-        return Series(self._t_stat_raw, index=self.beta.index)
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """
-        Returns the raw covariance of beta.
-        """
-        x = self._x.values
-        y = self._y.values
-
-        xx = np.dot(x.T, x)
-
-        if self._nw_lags is None:
-            return math.inv(xx) * (self._rmse_raw ** 2)
-        else:
-            resid = y - np.dot(x, self._beta_raw)
-            m = (x.T * resid).T
-
-            xeps = math.newey_west(m, self._nw_lags, self._nobs, self._df_raw,
-                                   self._nw_overlap)
-
-            xx_inv = math.inv(xx)
-            return np.dot(xx_inv, np.dot(xeps, xx_inv))
-
-    @cache_readonly
-    def var_beta(self):
-        """Returns the variance-covariance matrix of beta."""
-        return DataFrame(self._var_beta_raw, index=self.beta.index,
-                         columns=self.beta.index)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        if self._weights is None:
-            X = self._x_filtered.values
-        else:
-            # XXX
-            return self.sm_ols.fittedvalues
-
-        b = self._beta_raw
-        return np.dot(X, b)
-
-    @cache_readonly
-    def y_fitted(self):
-        """Returns the fitted y values.  This equals BX."""
-        if self._weights is None:
-            index = self._x_filtered.index
-            orig_index = index
-        else:
-            index = self._y.index
-            orig_index = self._y_orig.index
-
-        result = Series(self._y_fitted_raw, index=index)
-        return result.reindex(orig_index)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        return self._y_fitted_raw
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values.
-
-        For in-sample, this is same as y_fitted."""
-        return self.y_fitted
-
-    def predict(self, beta=None, x=None, fill_value=None,
-                fill_method=None, axis=0):
-        """
-        Parameters
-        ----------
-        beta : Series
-        x : Series or DataFrame
-        fill_value : scalar or dict, default None
-        fill_method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None
-        axis : {0, 1}, default 0
-            See DataFrame.fillna for more details
-
-        Notes
-        -----
-        1. If both fill_value and fill_method are None then NaNs are dropped
-        (this is the default behavior)
-        2. An intercept will be automatically added to the new_y_values if
-           the model was fitted using an intercept
-
-        Returns
-        -------
-        Series of predicted values
-        """
-        if beta is None and x is None:
-            return self.y_predict
-
-        if beta is None:
-            beta = self.beta
-        else:
-            beta = beta.reindex(self.beta.index)
-            if isnull(beta).any():
-                raise ValueError('Must supply betas for same variables')
-
-        if x is None:
-            x = self._x
-            orig_x = x
-        else:
-            orig_x = x
-            if fill_value is None and fill_method is None:
-                x = x.dropna(how='any')
-            else:
-                x = x.fillna(value=fill_value, method=fill_method, axis=axis)
-            if isinstance(x, Series):
-                x = DataFrame({'x': x})
-            if self._intercept:
-                x['intercept'] = 1.
-
-            x = x.reindex(columns=self._x.columns)
-
-        rs = np.dot(x.values, beta.values)
-        return Series(rs, x.index).reindex(orig_x.index)
-
-    RESULT_FIELDS = ['r2', 'r2_adj', 'df', 'df_model', 'df_resid', 'rmse',
-                     'f_stat', 'beta', 'std_err', 't_stat', 'p_value', 'nobs']
-
-    @cache_readonly
-    def _results(self):
-        results = {}
-        for result in self.RESULT_FIELDS:
-            results[result] = getattr(self, result)
-
-        return results
-
-    @cache_readonly
-    def _coef_table(self):
-        buf = StringIO()
-
-        buf.write('%14s %10s %10s %10s %10s %10s %10s\n' %
-                  ('Variable', 'Coef', 'Std Err', 't-stat',
-                   'p-value', 'CI 2.5%', 'CI 97.5%'))
-        buf.write(scom.banner(''))
-        coef_template = '\n%14s %10.4f %10.4f %10.2f %10.4f %10.4f %10.4f'
-
-        results = self._results
-
-        beta = results['beta']
-
-        for i, name in enumerate(beta.index):
-            if i and not (i % 5):
-                buf.write('\n' + scom.banner(''))
-
-            std_err = results['std_err'][name]
-            CI1 = beta[name] - 1.96 * std_err
-            CI2 = beta[name] + 1.96 * std_err
-
-            t_stat = results['t_stat'][name]
-            p_value = results['p_value'][name]
-
-            line = coef_template % (name,
-                                    beta[name], std_err, t_stat, p_value, CI1, CI2)
-
-            buf.write(line)
-
-        if self.nw_lags is not None:
-            buf.write('\n')
-            buf.write('*** The calculations are Newey-West '
-                      'adjusted with lags %5d\n' % self.nw_lags)
-
-        return buf.getvalue()
-
-    @cache_readonly
-    def summary_as_matrix(self):
-        """Returns the formatted results of the OLS as a DataFrame."""
-        results = self._results
-        beta = results['beta']
-        data = {'beta': results['beta'],
-                't-stat': results['t_stat'],
-                'p-value': results['p_value'],
-                'std err': results['std_err']}
-        return DataFrame(data, beta.index).T
-
-    @cache_readonly
-    def summary(self):
-        """
-        This returns the formatted result of the OLS computation
-        """
-        template = """
-%(bannerTop)s
-
-Formula: Y ~ %(formula)s
-
-Number of Observations:         %(nobs)d
-Number of Degrees of Freedom:   %(df)d
-
-R-squared:     %(r2)10.4f
-Adj R-squared: %(r2_adj)10.4f
-
-Rmse:          %(rmse)10.4f
-
-F-stat %(f_stat_shape)s: %(f_stat)10.4f, p-value: %(f_stat_p_value)10.4f
-
-Degrees of Freedom: model %(df_model)d, resid %(df_resid)d
-
-%(bannerCoef)s
-%(coef_table)s
-%(bannerEnd)s
-"""
-        coef_table = self._coef_table
-
-        results = self._results
-
-        f_stat = results['f_stat']
-
-        bracketed = ['<%s>' % str(c) for c in results['beta'].index]
-
-        formula = StringIO()
-        formula.write(bracketed[0])
-        tot = len(bracketed[0])
-        line = 1
-        for coef in bracketed[1:]:
-            tot = tot + len(coef) + 3
-
-            if tot // (68 * line):
-                formula.write('\n' + ' ' * 12)
-                line += 1
-
-            formula.write(' + ' + coef)
-
-        params = {
-            'bannerTop': scom.banner('Summary of Regression Analysis'),
-            'bannerCoef': scom.banner('Summary of Estimated Coefficients'),
-            'bannerEnd': scom.banner('End of Summary'),
-            'formula': formula.getvalue(),
-            'r2': results['r2'],
-            'r2_adj': results['r2_adj'],
-            'nobs': results['nobs'],
-            'df': results['df'],
-            'df_model': results['df_model'],
-            'df_resid': results['df_resid'],
-            'coef_table': coef_table,
-            'rmse': results['rmse'],
-            'f_stat': f_stat['f-stat'],
-            'f_stat_shape': '(%d, %d)' % (f_stat['DF X'], f_stat['DF Resid']),
-            'f_stat_p_value': f_stat['p-value'],
-        }
-
-        return template % params
-
-    def __unicode__(self):
-        return self.summary
-
-    @cache_readonly
-    def _time_obs_count(self):
-        # XXX
-        return self._time_has_obs.astype(int)
-
-    @property
-    def _total_times(self):
-        return self._time_has_obs.sum()
-
-
-class MovingOLS(OLS):
-    """
-    Runs a rolling/expanding simple OLS.
-
-    Parameters
-    ----------
-    y : Series
-    x : Series, DataFrame, or dict of Series
-    weights : array-like, optional
-        1d array of weights.  If None, equivalent to an unweighted OLS.
-    window_type : {'full sample', 'rolling', 'expanding'}
-        Default expanding
-    window : int
-        size of window (for rolling/expanding OLS)
-    min_periods : int
-        Threshold of non-null data points to require.
-        If None, defaults to size of window for window_type='rolling' and 1
-        otherwise
-    intercept : bool
-        True if you want an intercept.
-    nw_lags : None or int
-        Number of Newey-West lags.
-    nw_overlap : boolean, default False
-        Assume data is overlapping when computing Newey-West estimator
-
-    """
-
-    def __init__(self, y, x, weights=None, window_type='expanding',
-                 window=None, min_periods=None, intercept=True,
-                 nw_lags=None, nw_overlap=False):
-
-        self._args = dict(intercept=intercept, nw_lags=nw_lags,
-                          nw_overlap=nw_overlap)
-
-        OLS.__init__(self, y=y, x=x, weights=weights, **self._args)
-
-        self._set_window(window_type, window, min_periods)
-
-    def _set_window(self, window_type, window, min_periods):
-        self._window_type = scom._get_window_type(window_type)
-
-        if self._is_rolling:
-            if window is None:
-                raise AssertionError("Must specify window.")
-            if min_periods is None:
-                min_periods = window
-        else:
-            window = len(self._x)
-            if min_periods is None:
-                min_periods = 1
-
-        self._window = int(window)
-        self._min_periods = min_periods
-
-#------------------------------------------------------------------------------
-# "Public" results
-
-    @cache_readonly
-    def beta(self):
-        """Returns the betas in Series/DataFrame form."""
-        return DataFrame(self._beta_raw,
-                         index=self._result_index,
-                         columns=self._x.columns)
-
-    @cache_readonly
-    def rank(self):
-        return Series(self._rank_raw, index=self._result_index)
-
-    @cache_readonly
-    def df(self):
-        """Returns the degrees of freedom."""
-        return Series(self._df_raw, index=self._result_index)
-
-    @cache_readonly
-    def df_model(self):
-        """Returns the model degrees of freedom."""
-        return Series(self._df_model_raw, index=self._result_index)
-
-    @cache_readonly
-    def df_resid(self):
-        """Returns the residual degrees of freedom."""
-        return Series(self._df_resid_raw, index=self._result_index)
-
-    @cache_readonly
-    def f_stat(self):
-        """Returns the f-stat value."""
-        f_stat_dicts = dict((date, f_stat_to_dict(f_stat))
-                            for date, f_stat in zip(self.beta.index,
-                                                    self._f_stat_raw))
-
-        return DataFrame(f_stat_dicts).T
-
-    def f_test(self, hypothesis):
-        raise NotImplementedError('must use full sample')
-
-    @cache_readonly
-    def forecast_mean(self):
-        return Series(self._forecast_mean_raw, index=self._result_index)
-
-    @cache_readonly
-    def forecast_vol(self):
-        return Series(self._forecast_vol_raw, index=self._result_index)
-
-    @cache_readonly
-    def p_value(self):
-        """Returns the p values."""
-        cols = self.beta.columns
-        return DataFrame(self._p_value_raw, columns=cols,
-                         index=self._result_index)
-
-    @cache_readonly
-    def r2(self):
-        """Returns the r-squared values."""
-        return Series(self._r2_raw, index=self._result_index)
-
-    @cache_readonly
-    def resid(self):
-        """Returns the residuals."""
-        return Series(self._resid_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-    @cache_readonly
-    def r2_adj(self):
-        """Returns the r-squared adjusted values."""
-        index = self.r2.index
-
-        return Series(self._r2_adj_raw, index=index)
-
-    @cache_readonly
-    def rmse(self):
-        """Returns the rmse values."""
-        return Series(self._rmse_raw, index=self._result_index)
-
-    @cache_readonly
-    def std_err(self):
-        """Returns the standard err values."""
-        return DataFrame(self._std_err_raw, columns=self.beta.columns,
-                         index=self._result_index)
-
-    @cache_readonly
-    def t_stat(self):
-        """Returns the t-stat value."""
-        return DataFrame(self._t_stat_raw, columns=self.beta.columns,
-                         index=self._result_index)
-
-    @cache_readonly
-    def var_beta(self):
-        """Returns the covariance of beta."""
-        result = {}
-        result_index = self._result_index
-        for i in range(len(self._var_beta_raw)):
-            dm = DataFrame(self._var_beta_raw[i], columns=self.beta.columns,
-                           index=self.beta.columns)
-            result[result_index[i]] = dm
-
-        return Panel.from_dict(result, intersect=False)
-
-    @cache_readonly
-    def y_fitted(self):
-        """Returns the fitted y values."""
-        return Series(self._y_fitted_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values."""
-        return Series(self._y_predict_raw[self._valid_obs_labels],
-                      index=self._result_index)
-
-#------------------------------------------------------------------------------
-# "raw" attributes, calculations
-
-    @property
-    def _is_rolling(self):
-        return self._window_type == 'rolling'
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        beta, indices, mask = self._rolling_ols_call
-
-        return beta[indices]
-
-    @cache_readonly
-    def _result_index(self):
-        return self._index[self._valid_indices]
-
-    @property
-    def _valid_indices(self):
-        return self._rolling_ols_call[1]
-
-    @cache_readonly
-    def _rolling_ols_call(self):
-        return self._calc_betas(self._x_trans, self._y_trans)
-
-    def _calc_betas(self, x, y):
-        N = len(self._index)
-        K = len(self._x.columns)
-
-        betas = np.empty((N, K), dtype=float)
-        betas[:] = np.NaN
-
-        valid = self._time_has_obs
-        enough = self._enough_obs
-        window = self._window
-
-        # Use transformed (demeaned) Y, X variables
-        cum_xx = self._cum_xx(x)
-        cum_xy = self._cum_xy(x, y)
-
-        for i in range(N):
-            if not valid[i] or not enough[i]:
-                continue
-
-            xx = cum_xx[i]
-            xy = cum_xy[i]
-            if self._is_rolling and i >= window:
-                xx = xx - cum_xx[i - window]
-                xy = xy - cum_xy[i - window]
-
-            betas[i] = math.solve(xx, xy)
-
-        mask = ~np.isnan(betas).any(axis=1)
-        have_betas = np.arange(N)[mask]
-
-        return betas, have_betas, mask
-
-    def _rolling_rank(self):
-        dates = self._index
-        window = self._window
-
-        ranks = np.empty(len(dates), dtype=float)
-        ranks[:] = np.NaN
-        for i, date in enumerate(dates):
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = self._x.truncate(before=prior_date, after=date).values
-
-            if len(x_slice) == 0:
-                continue
-
-            ranks[i] = math.rank(x_slice)
-
-        return ranks
-
-    def _cum_xx(self, x):
-        dates = self._index
-        K = len(x.columns)
-        valid = self._time_has_obs
-        cum_xx = []
-
-        slicer = lambda df, dt: df.truncate(dt, dt).values
-        if not self._panel_model:
-            _get_index = x.index.get_loc
-
-            def slicer(df, dt):
-                i = _get_index(dt)
-                return df.values[i:i + 1, :]
-
-        last = np.zeros((K, K))
-
-        for i, date in enumerate(dates):
-            if not valid[i]:
-                cum_xx.append(last)
-                continue
-
-            x_slice = slicer(x, date)
-            xx = last = last + np.dot(x_slice.T, x_slice)
-            cum_xx.append(xx)
-
-        return cum_xx
-
-    def _cum_xy(self, x, y):
-        dates = self._index
-        valid = self._time_has_obs
-        cum_xy = []
-
-        x_slicer = lambda df, dt: df.truncate(dt, dt).values
-        if not self._panel_model:
-            _get_index = x.index.get_loc
-
-            def x_slicer(df, dt):
-                i = _get_index(dt)
-                return df.values[i:i + 1]
-
-        _y_get_index = y.index.get_loc
-        _values = y.values
-        if isinstance(y.index, MultiIndex):
-            def y_slicer(df, dt):
-                loc = _y_get_index(dt)
-                return _values[loc]
-        else:
-            def y_slicer(df, dt):
-                i = _y_get_index(dt)
-                return _values[i:i + 1]
-
-        last = np.zeros(len(x.columns))
-        for i, date in enumerate(dates):
-            if not valid[i]:
-                cum_xy.append(last)
-                continue
-
-            x_slice = x_slicer(x, date)
-            y_slice = y_slicer(y, date)
-
-            xy = last = last + np.dot(x_slice.T, y_slice)
-            cum_xy.append(xy)
-
-        return cum_xy
-
-    @cache_readonly
-    def _rank_raw(self):
-        rank = self._rolling_rank()
-        return rank[self._valid_indices]
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        return self._rank_raw
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self._df_raw - 1
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self._nobs - self._df_raw
-
-    @cache_readonly
-    def _f_stat_raw(self):
-        """Returns the raw f-stat value."""
-        from scipy.stats import f
-
-        items = self.beta.columns
-        nobs = self._nobs
-        df = self._df_raw
-        df_resid = nobs - df
-
-        # var_beta has not been newey-west adjusted
-        if self._nw_lags is None:
-            F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
-
-            q = len(items)
-            if 'intercept' in items:
-                q -= 1
-
-            def get_result_simple(Fst, d):
-                return Fst, (q, d), 1 - f.cdf(Fst, q, d)
-
-            # Compute the P-value for each pair
-            result = starmap(get_result_simple, zip(F, df_resid))
-
-            return list(result)
-
-        K = len(items)
-        R = np.eye(K)
-        r = np.zeros((K, 1))
-
-        try:
-            intercept = items.get_loc('intercept')
-            R = np.concatenate((R[0: intercept], R[intercept + 1:]))
-            r = np.concatenate((r[0: intercept], r[intercept + 1:]))
-        except KeyError:
-            # no intercept
-            pass
-
-        def get_result(beta, vcov, n, d):
-            return math.calc_F(R, r, beta, vcov, n, d)
-
-        results = starmap(get_result,
-                          zip(self._beta_raw, self._var_beta_raw, nobs, df))
-
-        return list(results)
-
-    @cache_readonly
-    def _p_value_raw(self):
-        """Returns the raw p values."""
-        from scipy.stats import t
-
-        result = [2 * t.sf(a, b)
-                  for a, b in zip(np.fabs(self._t_stat_raw),
-                                  self._df_resid_raw)]
-
-        return np.array(result)
-
-    @cache_readonly
-    def _resid_stats(self):
-        uncentered_sst = []
-        sst = []
-        sse = []
-
-        Yreg = self._y
-        Y = self._y_trans
-        X = self._x_trans
-        weights = self._weights
-
-        dates = self._index
-        window = self._window
-        for n, index in enumerate(self._valid_indices):
-            if self._is_rolling and index >= window:
-                prior_date = dates[index - window + 1]
-            else:
-                prior_date = dates[0]
-
-            date = dates[index]
-            beta = self._beta_raw[n]
-
-            X_slice = X.truncate(before=prior_date, after=date).values
-            Y_slice = _y_converter(Y.truncate(before=prior_date, after=date))
-
-            resid = Y_slice - np.dot(X_slice, beta)
-
-            if weights is not None:
-                Y_slice = _y_converter(Yreg.truncate(before=prior_date,
-                                                     after=date))
-                weights_slice = weights.truncate(prior_date, date)
-                demeaned = Y_slice - np.average(Y_slice, weights=weights_slice)
-                SS_total = (weights_slice * demeaned ** 2).sum()
-            else:
-                SS_total = ((Y_slice - Y_slice.mean()) ** 2).sum()
-
-            SS_err = (resid ** 2).sum()
-            SST_uncentered = (Y_slice ** 2).sum()
-
-            sse.append(SS_err)
-            sst.append(SS_total)
-            uncentered_sst.append(SST_uncentered)
-
-        return {
-            'sse': np.array(sse),
-            'centered_tss': np.array(sst),
-            'uncentered_tss': np.array(uncentered_sst),
-        }
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        return np.sqrt(self._resid_stats['sse'] / self._df_resid_raw)
-
-    @cache_readonly
-    def _r2_raw(self):
-        rs = self._resid_stats
-
-        if self._use_centered_tss:
-            return 1 - rs['sse'] / rs['centered_tss']
-        else:
-            return 1 - rs['sse'] / rs['uncentered_tss']
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        nobs = self._nobs
-        factors = (nobs - 1) / (nobs - self._df_raw)
-        return 1 - (1 - self._r2_raw) * factors
-
-    @cache_readonly
-    def _resid_raw(self):
-        """Returns the raw residuals."""
-        return (self._y.values - self._y_fitted_raw)
-
-    @cache_readonly
-    def _std_err_raw(self):
-        """Returns the raw standard err values."""
-        results = []
-        for i in range(len(self._var_beta_raw)):
-            results.append(np.sqrt(np.diag(self._var_beta_raw[i])))
-
-        return np.array(results)
-
-    @cache_readonly
-    def _t_stat_raw(self):
-        """Returns the raw t-stat value."""
-        return self._beta_raw / self._std_err_raw
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """Returns the raw covariance of beta."""
-        x = self._x_trans
-        y = self._y_trans
-        dates = self._index
-        nobs = self._nobs
-        rmse = self._rmse_raw
-        beta = self._beta_raw
-        df = self._df_raw
-        window = self._window
-        cum_xx = self._cum_xx(self._x)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            xx = cum_xx[i]
-            date = dates[i]
-
-            if self._is_rolling and i >= window:
-                xx = xx - cum_xx[i - window]
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = x.truncate(before=prior_date, after=date)
-            y_slice = y.truncate(before=prior_date, after=date)
-            xv = x_slice.values
-            yv = np.asarray(y_slice)
-
-            if self._nw_lags is None:
-                result = math.inv(xx) * (rmse[n] ** 2)
-            else:
-                resid = yv - np.dot(xv, beta[n])
-                m = (xv.T * resid).T
-
-                xeps = math.newey_west(m, self._nw_lags, nobs[n], df[n],
-                                       self._nw_overlap)
-
-                xx_inv = math.inv(xx)
-                result = np.dot(xx_inv, np.dot(xeps, xx_inv))
-
-            results.append(result)
-
-        return np.array(results)
-
-    @cache_readonly
-    def _forecast_mean_raw(self):
-        """Returns the raw covariance of beta."""
-        nobs = self._nobs
-        window = self._window
-
-        # x should be ones
-        dummy = DataFrame(index=self._y.index)
-        dummy['y'] = 1
-
-        cum_xy = self._cum_xy(dummy, self._y)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            sumy = cum_xy[i]
-
-            if self._is_rolling and i >= window:
-                sumy = sumy - cum_xy[i - window]
-
-            results.append(sumy[0] / nobs[n])
-
-        return np.array(results)
-
-    @cache_readonly
-    def _forecast_vol_raw(self):
-        """Returns the raw covariance of beta."""
-        beta = self._beta_raw
-        window = self._window
-        dates = self._index
-        x = self._x
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            date = dates[i]
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            x_slice = x.truncate(prior_date, date).values
-            x_demeaned = x_slice - x_slice.mean(0)
-            x_cov = np.dot(x_demeaned.T, x_demeaned) / (len(x_slice) - 1)
-
-            B = beta[n]
-            result = np.dot(B, np.dot(x_cov, B))
-            results.append(np.sqrt(result))
-
-        return np.array(results)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        return (self._x.values * self._beta_matrix(lag=0)).sum(1)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        return (self._x.values * self._beta_matrix(lag=1)).sum(1)
-
-    @cache_readonly
-    def _results(self):
-        results = {}
-        for result in self.RESULT_FIELDS:
-            value = getattr(self, result)
-            if isinstance(value, Series):
-                value = value[self.beta.index[-1]]
-            elif isinstance(value, DataFrame):
-                value = value.xs(self.beta.index[-1])
-            else:  # pragma: no cover
-                raise Exception('Problem retrieving %s' % result)
-            results[result] = value
-
-        return results
-
-    @cache_readonly
-    def _window_time_obs(self):
-        window_obs = (Series(self._time_obs_count > 0)
-                      .rolling(self._window, min_periods=1)
-                      .sum()
-                      .values
-                      )
-
-        window_obs[np.isnan(window_obs)] = 0
-        return window_obs.astype(int)
-
-    @cache_readonly
-    def _nobs_raw(self):
-        if self._is_rolling:
-            window = self._window
-        else:
-            # expanding case
-            window = len(self._index)
-
-        result = Series(self._time_obs_count).rolling(
-            window, min_periods=1).sum().values
-
-        return result.astype(int)
-
-    def _beta_matrix(self, lag=0):
-        if lag < 0:
-            raise AssertionError("'lag' must be greater than or equal to 0, "
-                                 "input was {0}".format(lag))
-
-        betas = self._beta_raw
-
-        labels = np.arange(len(self._y)) - lag
-        indexer = self._valid_obs_labels.searchsorted(labels, side='left')
-        indexer[indexer == len(betas)] = len(betas) - 1
-
-        beta_matrix = betas[indexer]
-        beta_matrix[labels < self._valid_obs_labels[0]] = np.NaN
-
-        return beta_matrix
-
-    @cache_readonly
-    def _valid_obs_labels(self):
-        dates = self._index[self._valid_indices]
-        return self._y.index.searchsorted(dates)
-
-    @cache_readonly
-    def _nobs(self):
-        return self._nobs_raw[self._valid_indices]
-
-    @property
-    def nobs(self):
-        return Series(self._nobs, index=self._result_index)
-
-    @cache_readonly
-    def _enough_obs(self):
-        # XXX: what's the best way to determine where to start?
-        return self._nobs_raw >= max(self._min_periods,
-                                     len(self._x.columns) + 1)
-
-
-def _safe_update(d, other):
-    """
-    Combine dictionaries with non-overlapping keys
-    """
-    for k, v in compat.iteritems(other):
-        if k in d:
-            raise Exception('Duplicate regressor: %s' % k)
-
-        d[k] = v
-
-
-def _filter_data(lhs, rhs, weights=None):
-    """
-    Cleans the input for single OLS.
-
-    Parameters
-    ----------
-    lhs : Series
-        Dependent variable in the regression.
-    rhs : dict, whose values are Series, DataFrame, or dict
-        Explanatory variables of the regression.
-    weights : array-like, optional
-        1d array of weights.  If None, equivalent to an unweighted OLS.
-
-    Returns
-    -------
-    Series, DataFrame
-        Cleaned lhs and rhs
-    """
-    if not isinstance(lhs, Series):
-        if len(lhs) != len(rhs):
-            raise AssertionError("length of lhs must equal length of rhs")
-        lhs = Series(lhs, index=rhs.index)
-
-    rhs = _combine_rhs(rhs)
-    lhs = DataFrame({'__y__': lhs}, dtype=float)
-    pre_filt_rhs = rhs.dropna(how='any')
-
-    combined = rhs.join(lhs, how='outer')
-    if weights is not None:
-        combined['__weights__'] = weights
-
-    valid = (combined.count(1) == len(combined.columns)).values
-    index = combined.index
-    combined = combined[valid]
-
-    if weights is not None:
-        filt_weights = combined.pop('__weights__')
-    else:
-        filt_weights = None
-
-    filt_lhs = combined.pop('__y__')
-    filt_rhs = combined
-
-    if hasattr(filt_weights, 'to_dense'):
-        filt_weights = filt_weights.to_dense()
-
-    return (filt_lhs.to_dense(), filt_rhs.to_dense(), filt_weights,
-            pre_filt_rhs.to_dense(), index, valid)
-
-
-def _combine_rhs(rhs):
-    """
-    Glue input X variables together while checking for potential
-    duplicates
-    """
-    series = {}
-
-    if isinstance(rhs, Series):
-        series['x'] = rhs
-    elif isinstance(rhs, DataFrame):
-        series = rhs.copy()
-    elif isinstance(rhs, dict):
-        for name, value in compat.iteritems(rhs):
-            if isinstance(value, Series):
-                _safe_update(series, {name: value})
-            elif isinstance(value, (dict, DataFrame)):
-                _safe_update(series, value)
-            else:  # pragma: no cover
-                raise Exception('Invalid RHS data type: %s' % type(value))
-    else:  # pragma: no cover
-        raise Exception('Invalid RHS type: %s' % type(rhs))
-
-    if not isinstance(series, DataFrame):
-        series = DataFrame(series, dtype=float)
-
-    return series
-
-# A little kludge so we can use this method for both
-# MovingOLS and MovingPanelOLS
-
-
-def _y_converter(y):
-    y = y.values.squeeze()
-    if y.ndim == 0:  # pragma: no cover
-        return np.array([y])
-    else:
-        return y
-
-
-def f_stat_to_dict(result):
-    f_stat, shape, p_value = result
-
-    result = {}
-    result['f-stat'] = f_stat
-    result['DF X'] = shape[0]
-    result['DF Resid'] = shape[1]
-    result['p-value'] = p_value
-
-    return result
diff --git a/pandas/stats/plm.py b/pandas/stats/plm.py
deleted file mode 100644
index 806dc289f..000000000
--- a/pandas/stats/plm.py
+++ /dev/null
@@ -1,863 +0,0 @@
-"""
-Linear regression objects for panel data
-"""
-
-# pylint: disable-msg=W0231
-# pylint: disable-msg=E1101,E1103
-
-# flake8: noqa
-
-from __future__ import division
-from pandas.compat import range
-from pandas import compat
-import warnings
-
-import numpy as np
-
-from pandas.core.panel import Panel
-from pandas.core.frame import DataFrame
-from pandas.core.reshape import get_dummies
-from pandas.core.series import Series
-from pandas.stats.ols import OLS, MovingOLS
-import pandas.stats.common as com
-import pandas.stats.math as math
-from pandas.util.decorators import cache_readonly
-
-
-class PanelOLS(OLS):
-    """Implements panel OLS.
-
-    See ols function docs
-    """
-    _panel_model = True
-
-    def __init__(self, y, x, weights=None, intercept=True, nw_lags=None,
-                 entity_effects=False, time_effects=False, x_effects=None,
-                 cluster=None, dropped_dummies=None, verbose=False,
-                 nw_overlap=False):
-        import warnings
-        warnings.warn("The pandas.stats.plm module is deprecated and will be "
-                      "removed in a future version. We refer to external packages "
-                      "like statsmodels, see some examples here: "
-                      "http://www.statsmodels.org/stable/mixed_linear.html",
-                      FutureWarning, stacklevel=4)
-        self._x_orig = x
-        self._y_orig = y
-        self._weights = weights
-
-        self._intercept = intercept
-        self._nw_lags = nw_lags
-        self._nw_overlap = nw_overlap
-        self._entity_effects = entity_effects
-        self._time_effects = time_effects
-        self._x_effects = x_effects
-        self._dropped_dummies = dropped_dummies or {}
-        self._cluster = com._get_cluster_type(cluster)
-        self._verbose = verbose
-
-        (self._x, self._x_trans,
-         self._x_filtered, self._y,
-         self._y_trans) = self._prepare_data()
-
-        self._index = self._x.index.levels[0]
-
-        self._T = len(self._index)
-
-    def log(self, msg):
-        if self._verbose:  # pragma: no cover
-            print(msg)
-
-    def _prepare_data(self):
-        """Cleans and stacks input data into DataFrame objects
-
-        If time effects is True, then we turn off intercepts and omit an item
-        from every (entity and x) fixed effect.
-
-        Otherwise:
-           - If we have an intercept, we omit an item from every fixed effect.
-           - Else, we omit an item from every fixed effect except one of them.
-
-        The categorical variables will get dropped from x.
-        """
-        (x, x_filtered, y, weights, cat_mapping) = self._filter_data()
-
-        self.log('Adding dummies to X variables')
-        x = self._add_dummies(x, cat_mapping)
-
-        self.log('Adding dummies to filtered X variables')
-        x_filtered = self._add_dummies(x_filtered, cat_mapping)
-
-        if self._x_effects:
-            x = x.drop(self._x_effects, axis=1)
-            x_filtered = x_filtered.drop(self._x_effects, axis=1)
-
-        if self._time_effects:
-            x_regressor = x.sub(x.mean(level=0), level=0)
-
-            unstacked_y = y.unstack()
-            y_regressor = unstacked_y.sub(unstacked_y.mean(1), axis=0).stack()
-            y_regressor.index = y.index
-
-        elif self._intercept:
-            # only add intercept when no time effects
-            self.log('Adding intercept')
-            x = x_regressor = add_intercept(x)
-            x_filtered = add_intercept(x_filtered)
-            y_regressor = y
-        else:
-            self.log('No intercept added')
-            x_regressor = x
-            y_regressor = y
-
-        if weights is not None:
-            if not y_regressor.index.equals(weights.index):
-                raise AssertionError("y_regressor and weights must have the "
-                                     "same index")
-            if not x_regressor.index.equals(weights.index):
-                raise AssertionError("x_regressor and weights must have the "
-                                     "same index")
-
-            rt_weights = np.sqrt(weights)
-            y_regressor = y_regressor * rt_weights
-            x_regressor = x_regressor.mul(rt_weights, axis=0)
-
-        return x, x_regressor, x_filtered, y, y_regressor
-
-    def _filter_data(self):
-        """
-
-        """
-        data = self._x_orig
-        cat_mapping = {}
-
-        if isinstance(data, DataFrame):
-            data = data.to_panel()
-        else:
-            if isinstance(data, Panel):
-                data = data.copy()
-
-            data, cat_mapping = self._convert_x(data)
-
-            if not isinstance(data, Panel):
-                data = Panel.from_dict(data, intersect=True)
-
-        x_names = data.items
-
-        if self._weights is not None:
-            data['__weights__'] = self._weights
-
-        # Filter x's without y (so we can make a prediction)
-        filtered = data.to_frame()
-
-        # Filter all data together using to_frame
-
-        # convert to DataFrame
-        y = self._y_orig
-        if isinstance(y, Series):
-            y = y.unstack()
-
-        data['__y__'] = y
-        data_long = data.to_frame()
-
-        x_filt = filtered.filter(x_names)
-        x = data_long.filter(x_names)
-        y = data_long['__y__']
-
-        if self._weights is not None and not self._weights.empty:
-            weights = data_long['__weights__']
-        else:
-            weights = None
-
-        return x, x_filt, y, weights, cat_mapping
-
-    def _convert_x(self, x):
-        # Converts non-numeric data in x to floats. x_converted is the
-        # DataFrame with converted values, and x_conversion is a dict that
-        # provides the reverse mapping.  For example, if 'A' was converted to 0
-        # for x named 'variety', then x_conversion['variety'][0] is 'A'.
-        x_converted = {}
-        cat_mapping = {}
-        # x can be either a dict or a Panel, but in Python 3, dicts don't have
-        # .iteritems
-        iteritems = getattr(x, 'iteritems', x.items)
-        for key, df in iteritems():
-            if not isinstance(df, DataFrame):
-                raise AssertionError("all input items must be DataFrames, "
-                                     "at least one is of "
-                                     "type {0}".format(type(df)))
-
-            if _is_numeric(df):
-                x_converted[key] = df
-            else:
-                try:
-                    df = df.astype(float)
-                except (TypeError, ValueError):
-                    values = df.values
-                    distinct_values = sorted(set(values.flat))
-                    cat_mapping[key] = dict(enumerate(distinct_values))
-                    new_values = np.searchsorted(distinct_values, values)
-                    x_converted[key] = DataFrame(new_values, index=df.index,
-                                                 columns=df.columns)
-
-        if len(cat_mapping) == 0:
-            x_converted = x
-
-        return x_converted, cat_mapping
-
-    def _add_dummies(self, panel, mapping):
-        """
-        Add entity and / or categorical dummies to input X DataFrame
-
-        Returns
-        -------
-        DataFrame
-        """
-        panel = self._add_entity_effects(panel)
-        panel = self._add_categorical_dummies(panel, mapping)
-
-        return panel
-
-    def _add_entity_effects(self, panel):
-        """
-        Add entity dummies to panel
-
-        Returns
-        -------
-        DataFrame
-        """
-        from pandas.core.reshape import make_axis_dummies
-
-        if not self._entity_effects:
-            return panel
-
-        self.log('-- Adding entity fixed effect dummies')
-
-        dummies = make_axis_dummies(panel, 'minor')
-
-        if not self._use_all_dummies:
-            if 'entity' in self._dropped_dummies:
-                to_exclude = str(self._dropped_dummies.get('entity'))
-            else:
-                to_exclude = dummies.columns[0]
-
-            if to_exclude not in dummies.columns:
-                raise Exception('%s not in %s' % (to_exclude,
-                                                  dummies.columns))
-
-            self.log('-- Excluding dummy for entity: %s' % to_exclude)
-
-            dummies = dummies.filter(dummies.columns.difference([to_exclude]))
-
-        dummies = dummies.add_prefix('FE_')
-        panel = panel.join(dummies)
-
-        return panel
-
-    def _add_categorical_dummies(self, panel, cat_mappings):
-        """
-        Add categorical dummies to panel
-
-        Returns
-        -------
-        DataFrame
-        """
-        if not self._x_effects:
-            return panel
-
-        dropped_dummy = (self._entity_effects and not self._use_all_dummies)
-
-        for effect in self._x_effects:
-            self.log('-- Adding fixed effect dummies for %s' % effect)
-
-            dummies = get_dummies(panel[effect])
-
-            val_map = cat_mappings.get(effect)
-            if val_map:
-                val_map = dict((v, k) for k, v in compat.iteritems(val_map))
-
-            if dropped_dummy or not self._use_all_dummies:
-                if effect in self._dropped_dummies:
-                    to_exclude = mapped_name = self._dropped_dummies.get(
-                        effect)
-
-                    if val_map:
-                        mapped_name = val_map[to_exclude]
-                else:
-                    to_exclude = mapped_name = dummies.columns[0]
-
-                if mapped_name not in dummies.columns:  # pragma: no cover
-                    raise Exception('%s not in %s' % (to_exclude,
-                                                      dummies.columns))
-
-                self.log(
-                    '-- Excluding dummy for %s: %s' % (effect, to_exclude))
-
-                dummies = dummies.filter(
-                    dummies.columns.difference([mapped_name]))
-                dropped_dummy = True
-
-            dummies = _convertDummies(dummies, cat_mappings.get(effect))
-            dummies = dummies.add_prefix('%s_' % effect)
-            panel = panel.join(dummies)
-
-        return panel
-
-    @property
-    def _use_all_dummies(self):
-        """
-        In the case of using an intercept or including time fixed
-        effects, completely partitioning the sample would make the X
-        not full rank.
-        """
-        return (not self._intercept and not self._time_effects)
-
-    @cache_readonly
-    def _beta_raw(self):
-        """Runs the regression and returns the beta."""
-        X = self._x_trans.values
-        Y = self._y_trans.values.squeeze()
-
-        beta, _, _, _ = np.linalg.lstsq(X, Y)
-
-        return beta
-
-    @cache_readonly
-    def beta(self):
-        return Series(self._beta_raw, index=self._x.columns)
-
-    @cache_readonly
-    def _df_model_raw(self):
-        """Returns the raw model degrees of freedom."""
-        return self._df_raw - 1
-
-    @cache_readonly
-    def _df_resid_raw(self):
-        """Returns the raw residual degrees of freedom."""
-        return self._nobs - self._df_raw
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        df = math.rank(self._x_trans.values)
-        if self._time_effects:
-            df += self._total_times
-
-        return df
-
-    @cache_readonly
-    def _r2_raw(self):
-        Y = self._y_trans.values.squeeze()
-        X = self._x_trans.values
-
-        resid = Y - np.dot(X, self._beta_raw)
-
-        SSE = (resid ** 2).sum()
-
-        if self._use_centered_tss:
-            SST = ((Y - np.mean(Y)) ** 2).sum()
-        else:
-            SST = (Y ** 2).sum()
-
-        return 1 - SSE / SST
-
-    @property
-    def _use_centered_tss(self):
-        # has_intercept = np.abs(self._resid_raw.sum()) < _FP_ERR
-        return self._intercept or self._entity_effects or self._time_effects
-
-    @cache_readonly
-    def _r2_adj_raw(self):
-        """Returns the raw r-squared adjusted values."""
-        nobs = self._nobs
-        factors = (nobs - 1) / (nobs - self._df_raw)
-        return 1 - (1 - self._r2_raw) * factors
-
-    @cache_readonly
-    def _resid_raw(self):
-        Y = self._y.values.squeeze()
-        X = self._x.values
-        return Y - np.dot(X, self._beta_raw)
-
-    @cache_readonly
-    def resid(self):
-        return self._unstack_vector(self._resid_raw)
-
-    @cache_readonly
-    def _rmse_raw(self):
-        """Returns the raw rmse values."""
-        # X = self._x.values
-        # Y = self._y.values.squeeze()
-
-        X = self._x_trans.values
-        Y = self._y_trans.values.squeeze()
-
-        resid = Y - np.dot(X, self._beta_raw)
-        ss = (resid ** 2).sum()
-        return np.sqrt(ss / (self._nobs - self._df_raw))
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        cluster_axis = None
-        if self._cluster == 'time':
-            cluster_axis = 0
-        elif self._cluster == 'entity':
-            cluster_axis = 1
-
-        x = self._x
-        y = self._y
-
-        if self._time_effects:
-            xx = _xx_time_effects(x, y)
-        else:
-            xx = np.dot(x.values.T, x.values)
-
-        return _var_beta_panel(y, x, self._beta_raw, xx,
-                               self._rmse_raw, cluster_axis, self._nw_lags,
-                               self._nobs, self._df_raw, self._nw_overlap)
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        """Returns the raw fitted y values."""
-        return np.dot(self._x.values, self._beta_raw)
-
-    @cache_readonly
-    def y_fitted(self):
-        return self._unstack_vector(self._y_fitted_raw, index=self._x.index)
-
-    def _unstack_vector(self, vec, index=None):
-        if index is None:
-            index = self._y_trans.index
-        panel = DataFrame(vec, index=index, columns=['dummy'])
-        return panel.to_panel()['dummy']
-
-    def _unstack_y(self, vec):
-        unstacked = self._unstack_vector(vec)
-        return unstacked.reindex(self.beta.index)
-
-    @cache_readonly
-    def _time_obs_count(self):
-        return self._y_trans.count(level=0).values
-
-    @cache_readonly
-    def _time_has_obs(self):
-        return self._time_obs_count > 0
-
-    @property
-    def _nobs(self):
-        return len(self._y)
-
-
-def _convertDummies(dummies, mapping):
-    # cleans up the names of the generated dummies
-    new_items = []
-    for item in dummies.columns:
-        if not mapping:
-            var = str(item)
-            if isinstance(item, float):
-                var = '%g' % item
-
-            new_items.append(var)
-        else:
-            # renames the dummies if a conversion dict is provided
-            new_items.append(mapping[int(item)])
-
-    dummies = DataFrame(dummies.values, index=dummies.index,
-                        columns=new_items)
-
-    return dummies
-
-
-def _is_numeric(df):
-    for col in df:
-        if df[col].dtype.name == 'object':
-            return False
-
-    return True
-
-
-def add_intercept(panel, name='intercept'):
-    """
-    Add column of ones to input panel
-
-    Parameters
-    ----------
-    panel: Panel / DataFrame
-    name: string, default 'intercept']
-
-    Returns
-    -------
-    New object (same type as input)
-    """
-    panel = panel.copy()
-    panel[name] = 1.
-
-    return panel.consolidate()
-
-
-class MovingPanelOLS(MovingOLS, PanelOLS):
-    """Implements rolling/expanding panel OLS.
-
-    See ols function docs
-    """
-    _panel_model = True
-
-    def __init__(self, y, x, weights=None,
-                 window_type='expanding', window=None,
-                 min_periods=None,
-                 min_obs=None,
-                 intercept=True,
-                 nw_lags=None, nw_overlap=False,
-                 entity_effects=False,
-                 time_effects=False,
-                 x_effects=None,
-                 cluster=None,
-                 dropped_dummies=None,
-                 verbose=False):
-
-        self._args = dict(intercept=intercept,
-                          nw_lags=nw_lags,
-                          nw_overlap=nw_overlap,
-                          entity_effects=entity_effects,
-                          time_effects=time_effects,
-                          x_effects=x_effects,
-                          cluster=cluster,
-                          dropped_dummies=dropped_dummies,
-                          verbose=verbose)
-
-        PanelOLS.__init__(self, y=y, x=x, weights=weights,
-                          **self._args)
-
-        self._set_window(window_type, window, min_periods)
-
-        if min_obs is None:
-            min_obs = len(self._x.columns) + 1
-
-        self._min_obs = min_obs
-
-    @cache_readonly
-    def resid(self):
-        return self._unstack_y(self._resid_raw)
-
-    @cache_readonly
-    def y_fitted(self):
-        return self._unstack_y(self._y_fitted_raw)
-
-    @cache_readonly
-    def y_predict(self):
-        """Returns the predicted y values."""
-        return self._unstack_y(self._y_predict_raw)
-
-    def lagged_y_predict(self, lag=1):
-        """
-        Compute forecast Y value lagging coefficient by input number
-        of time periods
-
-        Parameters
-        ----------
-        lag : int
-
-        Returns
-        -------
-        DataFrame
-        """
-        x = self._x.values
-        betas = self._beta_matrix(lag=lag)
-        return self._unstack_y((betas * x).sum(1))
-
-    @cache_readonly
-    def _rolling_ols_call(self):
-        return self._calc_betas(self._x_trans, self._y_trans)
-
-    @cache_readonly
-    def _df_raw(self):
-        """Returns the degrees of freedom."""
-        df = self._rolling_rank()
-
-        if self._time_effects:
-            df += self._window_time_obs
-
-        return df[self._valid_indices]
-
-    @cache_readonly
-    def _var_beta_raw(self):
-        """Returns the raw covariance of beta."""
-        x = self._x
-        y = self._y
-
-        dates = x.index.levels[0]
-
-        cluster_axis = None
-        if self._cluster == 'time':
-            cluster_axis = 0
-        elif self._cluster == 'entity':
-            cluster_axis = 1
-
-        nobs = self._nobs
-        rmse = self._rmse_raw
-        beta = self._beta_raw
-        df = self._df_raw
-        window = self._window
-
-        if not self._time_effects:
-            # Non-transformed X
-            cum_xx = self._cum_xx(x)
-
-        results = []
-        for n, i in enumerate(self._valid_indices):
-            if self._is_rolling and i >= window:
-                prior_date = dates[i - window + 1]
-            else:
-                prior_date = dates[0]
-
-            date = dates[i]
-
-            x_slice = x.truncate(prior_date, date)
-            y_slice = y.truncate(prior_date, date)
-
-            if self._time_effects:
-                xx = _xx_time_effects(x_slice, y_slice)
-            else:
-                xx = cum_xx[i]
-                if self._is_rolling and i >= window:
-                    xx = xx - cum_xx[i - window]
-
-            result = _var_beta_panel(y_slice, x_slice, beta[n], xx, rmse[n],
-                                     cluster_axis, self._nw_lags,
-                                     nobs[n], df[n], self._nw_overlap)
-
-            results.append(result)
-
-        return np.array(results)
-
-    @cache_readonly
-    def _resid_raw(self):
-        beta_matrix = self._beta_matrix(lag=0)
-
-        Y = self._y.values.squeeze()
-        X = self._x.values
-        resid = Y - (X * beta_matrix).sum(1)
-
-        return resid
-
-    @cache_readonly
-    def _y_fitted_raw(self):
-        x = self._x.values
-        betas = self._beta_matrix(lag=0)
-        return (betas * x).sum(1)
-
-    @cache_readonly
-    def _y_predict_raw(self):
-        """Returns the raw predicted y values."""
-        x = self._x.values
-        betas = self._beta_matrix(lag=1)
-        return (betas * x).sum(1)
-
-    def _beta_matrix(self, lag=0):
-        if lag < 0:
-            raise AssertionError("'lag' must be greater than or equal to 0, "
-                                 "input was {0}".format(lag))
-
-        index = self._y_trans.index
-        major_labels = index.labels[0]
-        labels = major_labels - lag
-        indexer = self._valid_indices.searchsorted(labels, side='left')
-
-        beta_matrix = self._beta_raw[indexer]
-        beta_matrix[labels < self._valid_indices[0]] = np.NaN
-
-        return beta_matrix
-
-    @cache_readonly
-    def _enough_obs(self):
-        # XXX: what's the best way to determine where to start?
-        # TODO: write unit tests for this
-
-        rank_threshold = len(self._x.columns) + 1
-        if self._min_obs < rank_threshold:  # pragma: no cover
-            warnings.warn('min_obs is smaller than rank of X matrix')
-
-        enough_observations = self._nobs_raw >= self._min_obs
-        enough_time_periods = self._window_time_obs >= self._min_periods
-        return enough_time_periods & enough_observations
-
-
-def create_ols_dict(attr):
-    def attr_getter(self):
-        d = {}
-        for k, v in compat.iteritems(self.results):
-            result = getattr(v, attr)
-            d[k] = result
-
-        return d
-
-    return attr_getter
-
-
-def create_ols_attr(attr):
-    return property(create_ols_dict(attr))
-
-
-class NonPooledPanelOLS(object):
-    """Implements non-pooled panel OLS.
-
-    Parameters
-    ----------
-    y : DataFrame
-    x : Series, DataFrame, or dict of Series
-    intercept : bool
-        True if you want an intercept.
-    nw_lags : None or int
-        Number of Newey-West lags.
-    window_type : {'full_sample', 'rolling', 'expanding'}
-        'full_sample' by default
-    window : int
-        size of window (for rolling/expanding OLS)
-    """
-
-    ATTRIBUTES = [
-        'beta',
-        'df',
-        'df_model',
-        'df_resid',
-        'f_stat',
-        'p_value',
-        'r2',
-        'r2_adj',
-        'resid',
-        'rmse',
-        'std_err',
-        'summary_as_matrix',
-        't_stat',
-        'var_beta',
-        'x',
-        'y',
-        'y_fitted',
-        'y_predict'
-    ]
-
-    def __init__(self, y, x, window_type='full_sample', window=None,
-                 min_periods=None, intercept=True, nw_lags=None,
-                 nw_overlap=False):
-
-        import warnings
-        warnings.warn("The pandas.stats.plm module is deprecated and will be "
-                      "removed in a future version. We refer to external packages "
-                      "like statsmodels, see some examples here: "
-                      "http://www.statsmodels.org/stable/mixed_linear.html",
-                      FutureWarning, stacklevel=4)
-
-        for attr in self.ATTRIBUTES:
-            setattr(self.__class__, attr, create_ols_attr(attr))
-
-        results = {}
-
-        for entity in y:
-            entity_y = y[entity]
-
-            entity_x = {}
-            for x_var in x:
-                entity_x[x_var] = x[x_var][entity]
-
-            from pandas.stats.interface import ols
-            results[entity] = ols(y=entity_y,
-                                  x=entity_x,
-                                  window_type=window_type,
-                                  window=window,
-                                  min_periods=min_periods,
-                                  intercept=intercept,
-                                  nw_lags=nw_lags,
-                                  nw_overlap=nw_overlap)
-
-        self.results = results
-
-
-def _var_beta_panel(y, x, beta, xx, rmse, cluster_axis,
-                    nw_lags, nobs, df, nw_overlap):
-    xx_inv = math.inv(xx)
-
-    yv = y.values
-
-    if cluster_axis is None:
-        if nw_lags is None:
-            return xx_inv * (rmse ** 2)
-        else:
-            resid = yv - np.dot(x.values, beta)
-            m = (x.values.T * resid).T
-
-            xeps = math.newey_west(m, nw_lags, nobs, df, nw_overlap)
-
-            return np.dot(xx_inv, np.dot(xeps, xx_inv))
-    else:
-        Xb = np.dot(x.values, beta).reshape((len(x.values), 1))
-        resid = DataFrame(yv[:, None] - Xb, index=y.index, columns=['resid'])
-
-        if cluster_axis == 1:
-            x = x.swaplevel(0, 1).sort_index(level=0)
-            resid = resid.swaplevel(0, 1).sort_index(level=0)
-
-        m = _group_agg(x.values * resid.values, x.index._bounds,
-                       lambda x: np.sum(x, axis=0))
-
-        if nw_lags is None:
-            nw_lags = 0
-
-        xox = 0
-        for i in range(len(x.index.levels[0])):
-            xox += math.newey_west(m[i: i + 1], nw_lags,
-                                   nobs, df, nw_overlap)
-
-        return np.dot(xx_inv, np.dot(xox, xx_inv))
-
-
-def _group_agg(values, bounds, f):
-    """
-    R-style aggregator
-
-    Parameters
-    ----------
-    values : N-length or N x K ndarray
-    bounds : B-length ndarray
-    f : ndarray aggregation function
-
-    Returns
-    -------
-    ndarray with same length as bounds array
-    """
-    if values.ndim == 1:
-        N = len(values)
-        result = np.empty(len(bounds), dtype=float)
-    elif values.ndim == 2:
-        N, K = values.shape
-        result = np.empty((len(bounds), K), dtype=float)
-
-    testagg = f(values[:min(1, len(values))])
-    if isinstance(testagg, np.ndarray) and testagg.ndim == 2:
-        raise AssertionError('Function must reduce')
-
-    for i, left_bound in enumerate(bounds):
-        if i == len(bounds) - 1:
-            right_bound = N
-        else:
-            right_bound = bounds[i + 1]
-
-        result[i] = f(values[left_bound:right_bound])
-
-    return result
-
-
-def _xx_time_effects(x, y):
-    """
-    Returns X'X - (X'T) (T'T)^-1 (T'X)
-    """
-    # X'X
-    xx = np.dot(x.values.T, x.values)
-    xt = x.sum(level=0).values
-
-    count = y.unstack().count(1).values
-    selector = count > 0
-
-    # X'X - (T'T)^-1 (T'X)
-    xt = xt[selector]
-    count = count[selector]
-
-    return xx - np.dot(xt.T / count, xt)
diff --git a/pandas/stats/tests/__init__.py b/pandas/stats/tests/__init__.py
deleted file mode 100644
index e69de29bb..000000000
diff --git a/pandas/stats/tests/common.py b/pandas/stats/tests/common.py
deleted file mode 100644
index 0ce4b20a4..000000000
--- a/pandas/stats/tests/common.py
+++ /dev/null
@@ -1,162 +0,0 @@
-# pylint: disable-msg=W0611,W0402
-# flake8: noqa
-
-from datetime import datetime
-import string
-import nose
-
-import numpy as np
-
-from pandas import DataFrame, bdate_range
-from pandas.util.testing import assert_almost_equal  # imported in other tests
-import pandas.util.testing as tm
-
-N = 100
-K = 4
-
-start = datetime(2007, 1, 1)
-DATE_RANGE = bdate_range(start, periods=N)
-
-COLS = ['Col' + c for c in string.ascii_uppercase[:K]]
-
-
-def makeDataFrame():
-    data = DataFrame(np.random.randn(N, K),
-                     columns=COLS,
-                     index=DATE_RANGE)
-
-    return data
-
-
-def getBasicDatasets():
-    A = makeDataFrame()
-    B = makeDataFrame()
-    C = makeDataFrame()
-
-    return A, B, C
-
-
-def check_for_scipy():
-    try:
-        import scipy
-    except ImportError:
-        raise nose.SkipTest('no scipy')
-
-
-def check_for_statsmodels():
-    _have_statsmodels = True
-    try:
-        import statsmodels.api as sm
-    except ImportError:
-        try:
-            import scikits.statsmodels.api as sm
-        except ImportError:
-            raise nose.SkipTest('no statsmodels')
-
-
-class BaseTest(tm.TestCase):
-
-    def setUp(self):
-        check_for_scipy()
-        check_for_statsmodels()
-
-        self.A, self.B, self.C = getBasicDatasets()
-
-        self.createData1()
-        self.createData2()
-        self.createData3()
-
-    def createData1(self):
-        date = datetime(2007, 1, 1)
-        date2 = datetime(2007, 1, 15)
-        date3 = datetime(2007, 1, 22)
-
-        A = self.A.copy()
-        B = self.B.copy()
-        C = self.C.copy()
-
-        A['ColA'][date] = np.NaN
-        B['ColA'][date] = np.NaN
-        C['ColA'][date] = np.NaN
-        C['ColA'][date2] = np.NaN
-
-        # truncate data to save time
-        A = A[:30]
-        B = B[:30]
-        C = C[:30]
-
-        self.panel_y = A
-        self.panel_x = {'B': B, 'C': C}
-
-        self.series_panel_y = A.filter(['ColA'])
-        self.series_panel_x = {'B': B.filter(['ColA']),
-                               'C': C.filter(['ColA'])}
-        self.series_y = A['ColA']
-        self.series_x = {'B': B['ColA'],
-                         'C': C['ColA']}
-
-    def createData2(self):
-        y_data = [[1, np.NaN],
-                  [2, 3],
-                  [4, 5]]
-        y_index = [datetime(2000, 1, 1),
-                   datetime(2000, 1, 2),
-                   datetime(2000, 1, 3)]
-        y_cols = ['A', 'B']
-        self.panel_y2 = DataFrame(np.array(y_data), index=y_index,
-                                  columns=y_cols)
-
-        x1_data = [[6, np.NaN],
-                   [7, 8],
-                   [9, 30],
-                   [11, 12]]
-        x1_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2),
-                    datetime(2000, 1, 3),
-                    datetime(2000, 1, 4)]
-        x1_cols = ['A', 'B']
-        x1 = DataFrame(np.array(x1_data), index=x1_index,
-                       columns=x1_cols)
-
-        x2_data = [[13, 14, np.NaN],
-                   [15, np.NaN, np.NaN],
-                   [16, 17, 48],
-                   [19, 20, 21],
-                   [22, 23, 24]]
-        x2_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2),
-                    datetime(2000, 1, 3),
-                    datetime(2000, 1, 4),
-                    datetime(2000, 1, 5)]
-        x2_cols = ['C', 'A', 'B']
-        x2 = DataFrame(np.array(x2_data), index=x2_index,
-                       columns=x2_cols)
-
-        self.panel_x2 = {'x1': x1, 'x2': x2}
-
-    def createData3(self):
-        y_data = [[1, 2],
-                  [3, 4]]
-        y_index = [datetime(2000, 1, 1),
-                   datetime(2000, 1, 2)]
-        y_cols = ['A', 'B']
-        self.panel_y3 = DataFrame(np.array(y_data), index=y_index,
-                                  columns=y_cols)
-
-        x1_data = [['A', 'B'],
-                   ['C', 'A']]
-        x1_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2)]
-        x1_cols = ['A', 'B']
-        x1 = DataFrame(np.array(x1_data), index=x1_index,
-                       columns=x1_cols)
-
-        x2_data = [['foo', 'bar'],
-                   ['baz', 'foo']]
-        x2_index = [datetime(2000, 1, 1),
-                    datetime(2000, 1, 2)]
-        x2_cols = ['A', 'B']
-        x2 = DataFrame(np.array(x2_data), index=x2_index,
-                       columns=x2_cols)
-
-        self.panel_x3 = {'x1': x1, 'x2': x2}
diff --git a/pandas/stats/tests/test_fama_macbeth.py b/pandas/stats/tests/test_fama_macbeth.py
deleted file mode 100644
index 0c9fcf775..000000000
--- a/pandas/stats/tests/test_fama_macbeth.py
+++ /dev/null
@@ -1,68 +0,0 @@
-# flake8: noqa
-
-from pandas import DataFrame, Panel
-from pandas.stats.api import fama_macbeth
-from .common import assert_almost_equal, BaseTest
-
-from pandas.compat import range
-from pandas import compat
-import pandas.util.testing as tm
-import numpy as np
-
-
-class TestFamaMacBeth(BaseTest):
-
-    def testFamaMacBethRolling(self):
-        # self.checkFamaMacBethExtended('rolling', self.panel_x, self.panel_y,
-        #                               nw_lags_beta=2)
-
-        # df = DataFrame(np.random.randn(50, 10))
-        x = dict((k, DataFrame(np.random.randn(50, 10))) for k in 'abcdefg')
-        x = Panel.from_dict(x)
-        y = (DataFrame(np.random.randn(50, 10)) +
-             DataFrame(0.01 * np.random.randn(50, 10)))
-        self.checkFamaMacBethExtended('rolling', x, y, nw_lags_beta=2)
-        self.checkFamaMacBethExtended('expanding', x, y, nw_lags_beta=2)
-
-    def checkFamaMacBethExtended(self, window_type, x, y, **kwds):
-        window = 25
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = fama_macbeth(y=y, x=x, window_type=window_type, window=window,
-                                  **kwds)
-        self._check_stuff_works(result)
-
-        index = result._index
-        time = len(index)
-
-        for i in range(time - window + 1):
-            if window_type == 'rolling':
-                start = index[i]
-            else:
-                start = index[0]
-
-            end = index[i + window - 1]
-
-            x2 = {}
-            for k, v in x.iteritems():
-                x2[k] = v.truncate(start, end)
-            y2 = y.truncate(start, end)
-
-            with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-                reference = fama_macbeth(y=y2, x=x2, **kwds)
-            # reference._stats is tuple
-            assert_almost_equal(reference._stats, result._stats[:, i],
-                                check_dtype=False)
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            static = fama_macbeth(y=y2, x=x2, **kwds)
-        self._check_stuff_works(static)
-
-    def _check_stuff_works(self, result):
-        # does it work?
-        attrs = ['mean_beta', 'std_beta', 't_stat']
-        for attr in attrs:
-            getattr(result, attr)
-
-        # does it work?
-        result.summary
diff --git a/pandas/stats/tests/test_math.py b/pandas/stats/tests/test_math.py
deleted file mode 100644
index 3f89dbcd2..000000000
--- a/pandas/stats/tests/test_math.py
+++ /dev/null
@@ -1,59 +0,0 @@
-import nose
-
-from datetime import datetime
-from numpy.random import randn
-import numpy as np
-
-from pandas.core.api import Series, DataFrame, date_range
-import pandas.util.testing as tm
-import pandas.stats.math as pmath
-from pandas import ols
-
-N, K = 100, 10
-
-_have_statsmodels = True
-try:
-    import statsmodels.api as sm
-except ImportError:
-    try:
-        import scikits.statsmodels.api as sm  # noqa
-    except ImportError:
-        _have_statsmodels = False
-
-
-class TestMath(tm.TestCase):
-
-    _nan_locs = np.arange(20, 40)
-    _inf_locs = np.array([])
-
-    def setUp(self):
-        arr = randn(N)
-        arr[self._nan_locs] = np.NaN
-
-        self.arr = arr
-        self.rng = date_range(datetime(2009, 1, 1), periods=N)
-
-        self.series = Series(arr.copy(), index=self.rng)
-
-        self.frame = DataFrame(randn(N, K), index=self.rng,
-                               columns=np.arange(K))
-
-    def test_rank_1d(self):
-        self.assertEqual(1, pmath.rank(self.series))
-        self.assertEqual(0, pmath.rank(Series(0, self.series.index)))
-
-    def test_solve_rect(self):
-        if not _have_statsmodels:
-            raise nose.SkipTest("no statsmodels")
-
-        b = Series(np.random.randn(N), self.frame.index)
-        result = pmath.solve(self.frame, b)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            expected = ols(y=b, x=self.frame, intercept=False).beta
-        self.assertTrue(np.allclose(result, expected))
-
-    def test_inv_illformed(self):
-        singular = DataFrame(np.array([[1, 1], [2, 2]]))
-        rs = pmath.inv(singular)
-        expected = np.array([[0.1, 0.2], [0.1, 0.2]])
-        self.assertTrue(np.allclose(rs, expected))
diff --git a/pandas/stats/tests/test_ols.py b/pandas/stats/tests/test_ols.py
deleted file mode 100644
index b90c51366..000000000
--- a/pandas/stats/tests/test_ols.py
+++ /dev/null
@@ -1,968 +0,0 @@
-"""
-Unit test suite for OLS and PanelOLS classes
-"""
-
-# pylint: disable-msg=W0212
-
-# flake8: noqa
-
-from __future__ import division
-
-from datetime import datetime
-from pandas import compat
-from distutils.version import LooseVersion
-import nose
-import numpy as np
-
-from pandas import date_range, bdate_range
-from pandas.core.panel import Panel
-from pandas import DataFrame, Index, Series, notnull, offsets
-from pandas.stats.api import ols
-from pandas.stats.ols import _filter_data
-from pandas.stats.plm import NonPooledPanelOLS, PanelOLS
-from pandas.util.testing import (assert_almost_equal, assert_series_equal,
-                                 assert_frame_equal, assertRaisesRegexp, slow)
-import pandas.util.testing as tm
-import pandas.compat as compat
-from pandas.stats.tests.common import BaseTest
-
-_have_statsmodels = True
-try:
-    import statsmodels.api as sm
-except ImportError:
-    try:
-        import scikits.statsmodels.api as sm
-    except ImportError:
-        _have_statsmodels = False
-
-
-def _check_repr(obj):
-    repr(obj)
-    str(obj)
-
-
-def _compare_ols_results(model1, model2):
-    tm.assertIsInstance(model1, type(model2))
-
-    if hasattr(model1, '_window_type'):
-        _compare_moving_ols(model1, model2)
-    else:
-        _compare_fullsample_ols(model1, model2)
-
-
-def _compare_fullsample_ols(model1, model2):
-    assert_series_equal(model1.beta, model2.beta)
-
-
-def _compare_moving_ols(model1, model2):
-    assert_frame_equal(model1.beta, model2.beta)
-
-
-class TestOLS(BaseTest):
-
-    # TODO: Add tests for OLS y predict
-    # TODO: Right now we just check for consistency between full-sample and
-    # rolling/expanding results of the panel OLS.  We should also cross-check
-    # with trusted implementations of panel OLS (e.g. R).
-    # TODO: Add tests for non pooled OLS.
-
-    @classmethod
-    def setUpClass(cls):
-        super(TestOLS, cls).setUpClass()
-        try:
-            import matplotlib as mpl
-            mpl.use('Agg', warn=False)
-        except ImportError:
-            pass
-
-        if not _have_statsmodels:
-            raise nose.SkipTest("no statsmodels")
-
-    def testOLSWithDatasets_ccard(self):
-        self.checkDataSet(sm.datasets.ccard.load(), skip_moving=True)
-        self.checkDataSet(sm.datasets.cpunish.load(), skip_moving=True)
-        self.checkDataSet(sm.datasets.longley.load(), skip_moving=True)
-        self.checkDataSet(sm.datasets.stackloss.load(), skip_moving=True)
-
-    @slow
-    def testOLSWithDatasets_copper(self):
-        self.checkDataSet(sm.datasets.copper.load())
-
-    @slow
-    def testOLSWithDatasets_scotland(self):
-        self.checkDataSet(sm.datasets.scotland.load())
-
-        # degenerate case fails on some platforms
-        # self.checkDataSet(datasets.ccard.load(), 39, 49) # one col in X all
-        # 0s
-
-    def testWLS(self):
-        # WLS centered SS changed (fixed) in 0.5.0
-        sm_version = sm.version.version
-        if sm_version < LooseVersion('0.5.0'):
-            raise nose.SkipTest("WLS centered SS not fixed in statsmodels"
-                                " version {0}".format(sm_version))
-
-        X = DataFrame(np.random.randn(30, 4), columns=['A', 'B', 'C', 'D'])
-        Y = Series(np.random.randn(30))
-        weights = X.std(1)
-
-        self._check_wls(X, Y, weights)
-
-        weights.loc[[5, 15]] = np.nan
-        Y[[2, 21]] = np.nan
-        self._check_wls(X, Y, weights)
-
-    def _check_wls(self, x, y, weights):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=y, x=x, weights=1 / weights)
-
-        combined = x.copy()
-        combined['__y__'] = y
-        combined['__weights__'] = weights
-        combined = combined.dropna()
-
-        endog = combined.pop('__y__').values
-        aweights = combined.pop('__weights__').values
-        exog = sm.add_constant(combined.values, prepend=False)
-
-        sm_result = sm.WLS(endog, exog, weights=1 / aweights).fit()
-
-        assert_almost_equal(sm_result.params, result._beta_raw)
-        assert_almost_equal(sm_result.resid, result._resid_raw)
-
-        self.checkMovingOLS('rolling', x, y, weights=weights)
-        self.checkMovingOLS('expanding', x, y, weights=weights)
-
-    def checkDataSet(self, dataset, start=None, end=None, skip_moving=False):
-        exog = dataset.exog[start: end]
-        endog = dataset.endog[start: end]
-        x = DataFrame(exog, index=np.arange(exog.shape[0]),
-                      columns=np.arange(exog.shape[1]))
-        y = Series(endog, index=np.arange(len(endog)))
-
-        self.checkOLS(exog, endog, x, y)
-
-        if not skip_moving:
-            self.checkMovingOLS('rolling', x, y)
-            self.checkMovingOLS('rolling', x, y, nw_lags=0)
-            self.checkMovingOLS('expanding', x, y, nw_lags=0)
-            self.checkMovingOLS('rolling', x, y, nw_lags=1)
-            self.checkMovingOLS('expanding', x, y, nw_lags=1)
-            self.checkMovingOLS('expanding', x, y, nw_lags=1, nw_overlap=True)
-
-    def checkOLS(self, exog, endog, x, y):
-        reference = sm.OLS(endog, sm.add_constant(exog, prepend=False)).fit()
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=y, x=x)
-
-        # check that sparse version is the same
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            sparse_result = ols(y=y.to_sparse(), x=x.to_sparse())
-        _compare_ols_results(result, sparse_result)
-
-        assert_almost_equal(reference.params, result._beta_raw)
-        assert_almost_equal(reference.df_model, result._df_model_raw)
-        assert_almost_equal(reference.df_resid, result._df_resid_raw)
-        assert_almost_equal(reference.fvalue, result._f_stat_raw[0])
-        assert_almost_equal(reference.pvalues, result._p_value_raw)
-        assert_almost_equal(reference.rsquared, result._r2_raw)
-        assert_almost_equal(reference.rsquared_adj, result._r2_adj_raw)
-        assert_almost_equal(reference.resid, result._resid_raw)
-        assert_almost_equal(reference.bse, result._std_err_raw)
-        assert_almost_equal(reference.tvalues, result._t_stat_raw)
-        assert_almost_equal(reference.cov_params(), result._var_beta_raw)
-        assert_almost_equal(reference.fittedvalues, result._y_fitted_raw)
-
-        _check_non_raw_results(result)
-
-    def checkMovingOLS(self, window_type, x, y, weights=None, **kwds):
-        window = np.linalg.matrix_rank(x.values) * 2
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            moving = ols(y=y, x=x, weights=weights, window_type=window_type,
-                         window=window, **kwds)
-
-        # check that sparse version is the same
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            sparse_moving = ols(y=y.to_sparse(), x=x.to_sparse(),
-                                weights=weights,
-                                window_type=window_type,
-                                window=window, **kwds)
-        _compare_ols_results(moving, sparse_moving)
-
-        index = moving._index
-
-        for n, i in enumerate(moving._valid_indices):
-            if window_type == 'rolling' and i >= window:
-                prior_date = index[i - window + 1]
-            else:
-                prior_date = index[0]
-
-            date = index[i]
-
-            x_iter = {}
-            for k, v in compat.iteritems(x):
-                x_iter[k] = v.truncate(before=prior_date, after=date)
-            y_iter = y.truncate(before=prior_date, after=date)
-
-            with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-                static = ols(y=y_iter, x=x_iter, weights=weights, **kwds)
-
-            self.compare(static, moving, event_index=i,
-                         result_index=n)
-
-        _check_non_raw_results(moving)
-
-    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat', 'p_value',
-              'r2', 'r2_adj', 'rmse', 'std_err', 't_stat',
-              'var_beta']
-
-    def compare(self, static, moving, event_index=None,
-                result_index=None):
-
-        index = moving._index
-
-        # Check resid if we have a time index specified
-        if event_index is not None:
-            ref = static._resid_raw[-1]
-
-            label = index[event_index]
-
-            res = moving.resid[label]
-
-            assert_almost_equal(ref, res)
-
-            ref = static._y_fitted_raw[-1]
-            res = moving.y_fitted[label]
-
-            assert_almost_equal(ref, res)
-
-        # Check y_fitted
-
-        for field in self.FIELDS:
-            attr = '_%s_raw' % field
-
-            ref = getattr(static, attr)
-            res = getattr(moving, attr)
-
-            if result_index is not None:
-                res = res[result_index]
-
-            assert_almost_equal(ref, res)
-
-    def test_ols_object_dtype(self):
-        df = DataFrame(np.random.randn(20, 2), dtype=object)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=df[0], x=df[1])
-        summary = repr(model)
-
-
-class TestOLSMisc(tm.TestCase):
-
-    """
-    For test coverage with faux data
-    """
-    @classmethod
-    def setUpClass(cls):
-        super(TestOLSMisc, cls).setUpClass()
-        if not _have_statsmodels:
-            raise nose.SkipTest("no statsmodels")
-
-    def test_f_test(self):
-        x = tm.makeTimeDataFrame()
-        y = x.pop('A')
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x)
-
-        hyp = '1*B+1*C+1*D=0'
-        result = model.f_test(hyp)
-
-        hyp = ['1*B=0',
-               '1*C=0',
-               '1*D=0']
-        result = model.f_test(hyp)
-        assert_almost_equal(result['f-stat'], model.f_stat['f-stat'])
-
-        self.assertRaises(Exception, model.f_test, '1*A=0')
-
-    def test_r2_no_intercept(self):
-        y = tm.makeTimeSeries()
-        x = tm.makeTimeDataFrame()
-
-        x_with = x.copy()
-        x_with['intercept'] = 1.
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model1 = ols(y=y, x=x)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model2 = ols(y=y, x=x_with, intercept=False)
-        assert_series_equal(model1.beta, model2.beta)
-
-        # TODO: can we infer whether the intercept is there...
-        self.assertNotEqual(model1.r2, model2.r2)
-
-        # rolling
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model1 = ols(y=y, x=x, window=20)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model2 = ols(y=y, x=x_with, window=20, intercept=False)
-        assert_frame_equal(model1.beta, model2.beta)
-        self.assertTrue((model1.r2 != model2.r2).all())
-
-    def test_summary_many_terms(self):
-        x = DataFrame(np.random.randn(100, 20))
-        y = np.random.randn(100)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x)
-        model.summary
-
-    def test_y_predict(self):
-        y = tm.makeTimeSeries()
-        x = tm.makeTimeDataFrame()
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model1 = ols(y=y, x=x)
-        assert_series_equal(model1.y_predict, model1.y_fitted)
-        assert_almost_equal(model1._y_predict_raw, model1._y_fitted_raw)
-
-    def test_predict(self):
-        y = tm.makeTimeSeries()
-        x = tm.makeTimeDataFrame()
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model1 = ols(y=y, x=x)
-        assert_series_equal(model1.predict(), model1.y_predict)
-        assert_series_equal(model1.predict(x=x), model1.y_predict)
-        assert_series_equal(model1.predict(beta=model1.beta), model1.y_predict)
-
-        exog = x.copy()
-        exog['intercept'] = 1.
-        rs = Series(np.dot(exog.values, model1.beta.values), x.index)
-        assert_series_equal(model1.y_predict, rs)
-
-        x2 = x.reindex(columns=x.columns[::-1])
-        assert_series_equal(model1.predict(x=x2), model1.y_predict)
-
-        x3 = x2 + 10
-        pred3 = model1.predict(x=x3)
-        x3['intercept'] = 1.
-        x3 = x3.reindex(columns=model1.beta.index)
-        expected = Series(np.dot(x3.values, model1.beta.values), x3.index)
-        assert_series_equal(expected, pred3)
-
-        beta = Series(0., model1.beta.index)
-        pred4 = model1.predict(beta=beta)
-        assert_series_equal(Series(0., pred4.index), pred4)
-
-    def test_predict_longer_exog(self):
-        exogenous = {"1998": "4760", "1999": "5904", "2000": "4504",
-                     "2001": "9808", "2002": "4241", "2003": "4086",
-                     "2004": "4687", "2005": "7686", "2006": "3740",
-                     "2007": "3075", "2008": "3753", "2009": "4679",
-                     "2010": "5468", "2011": "7154", "2012": "4292",
-                     "2013": "4283", "2014": "4595", "2015": "9194",
-                     "2016": "4221", "2017": "4520"}
-        endogenous = {"1998": "691", "1999": "1580", "2000": "80",
-                      "2001": "1450", "2002": "555", "2003": "956",
-                      "2004": "877", "2005": "614", "2006": "468",
-                      "2007": "191"}
-
-        endog = Series(endogenous)
-        exog = Series(exogenous)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=endog, x=exog)
-
-        pred = model.y_predict
-        self.assert_index_equal(pred.index, exog.index)
-
-    def test_longpanel_series_combo(self):
-        wp = tm.makePanel()
-        lp = wp.to_frame()
-
-        y = lp.pop('ItemA')
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=lp, entity_effects=True, window=20)
-        self.assertTrue(notnull(model.beta.values).all())
-        tm.assertIsInstance(model, PanelOLS)
-        model.summary
-
-    def test_series_rhs(self):
-        y = tm.makeTimeSeries()
-        x = tm.makeTimeSeries()
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            expected = ols(y=y, x={'x': x})
-        assert_series_equal(model.beta, expected.beta)
-
-        # GH 5233/5250
-        assert_series_equal(model.y_predict, model.predict(x=x))
-
-    def test_various_attributes(self):
-        # just make sure everything "works". test correctness elsewhere
-
-        x = DataFrame(np.random.randn(100, 5))
-        y = np.random.randn(100)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x, window=20)
-
-        series_attrs = ['rank', 'df', 'forecast_mean', 'forecast_vol']
-
-        for attr in series_attrs:
-            value = getattr(model, attr)
-            tm.assertIsInstance(value, Series)
-
-        # works
-        model._results
-
-    def test_catch_regressor_overlap(self):
-        df1 = tm.makeTimeDataFrame().loc[:, ['A', 'B']]
-        df2 = tm.makeTimeDataFrame().loc[:, ['B', 'C', 'D']]
-        y = tm.makeTimeSeries()
-
-        data = {'foo': df1, 'bar': df2}
-
-        def f():
-            with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-                ols(y=y, x=data)
-        self.assertRaises(Exception, f)
-
-    def test_plm_ctor(self):
-        y = tm.makeTimeDataFrame()
-        x = {'a': tm.makeTimeDataFrame(),
-             'b': tm.makeTimeDataFrame()}
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x, intercept=False)
-        model.summary
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=Panel(x))
-        model.summary
-
-    def test_plm_attrs(self):
-        y = tm.makeTimeDataFrame()
-        x = {'a': tm.makeTimeDataFrame(),
-             'b': tm.makeTimeDataFrame()}
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            rmodel = ols(y=y, x=x, window=10)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x)
-        model.resid
-        rmodel.resid
-
-    def test_plm_lagged_y_predict(self):
-        y = tm.makeTimeDataFrame()
-        x = {'a': tm.makeTimeDataFrame(),
-             'b': tm.makeTimeDataFrame()}
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x, window=10)
-        result = model.lagged_y_predict(2)
-
-    def test_plm_f_test(self):
-        y = tm.makeTimeDataFrame()
-        x = {'a': tm.makeTimeDataFrame(),
-             'b': tm.makeTimeDataFrame()}
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=y, x=x)
-
-        hyp = '1*a+1*b=0'
-        result = model.f_test(hyp)
-
-        hyp = ['1*a=0',
-               '1*b=0']
-        result = model.f_test(hyp)
-        assert_almost_equal(result['f-stat'], model.f_stat['f-stat'])
-
-    def test_plm_exclude_dummy_corner(self):
-        y = tm.makeTimeDataFrame()
-        x = {'a': tm.makeTimeDataFrame(),
-             'b': tm.makeTimeDataFrame()}
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(
-                y=y, x=x, entity_effects=True, dropped_dummies={'entity': 'D'})
-        model.summary
-
-        def f():
-            with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-                ols(y=y, x=x, entity_effects=True,
-                    dropped_dummies={'entity': 'E'})
-        self.assertRaises(Exception, f)
-
-    def test_columns_tuples_summary(self):
-        # #1837
-        X = DataFrame(np.random.randn(10, 2), columns=[('a', 'b'), ('c', 'd')])
-        Y = Series(np.random.randn(10))
-
-        # it works!
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            model = ols(y=Y, x=X)
-        model.summary
-
-
-class TestPanelOLS(BaseTest):
-
-    FIELDS = ['beta', 'df', 'df_model', 'df_resid', 'f_stat',
-              'p_value', 'r2', 'r2_adj', 'rmse', 'std_err',
-              't_stat', 'var_beta']
-
-    _other_fields = ['resid', 'y_fitted']
-
-    def testFiltering(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2)
-
-        x = result._x
-        index = x.index.get_level_values(0)
-        index = Index(sorted(set(index)))
-        exp_index = Index([datetime(2000, 1, 1), datetime(2000, 1, 3)])
-        self.assert_index_equal(exp_index, index)
-
-        index = x.index.get_level_values(1)
-        index = Index(sorted(set(index)))
-        exp_index = Index(['A', 'B'])
-        self.assert_index_equal(exp_index, index)
-
-        x = result._x_filtered
-        index = x.index.get_level_values(0)
-        index = Index(sorted(set(index)))
-        exp_index = Index([datetime(2000, 1, 1),
-                           datetime(2000, 1, 3),
-                           datetime(2000, 1, 4)])
-        self.assert_index_equal(exp_index, index)
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 4, 5],
-                            check_dtype=False)
-
-        exp_x = np.array([[6, 14, 1], [9, 17, 1],
-                          [30, 48, 1]], dtype=np.float64)
-        assert_almost_equal(exp_x, result._x.values)
-
-        exp_x_filtered = np.array([[6, 14, 1], [9, 17, 1], [30, 48, 1],
-                                   [11, 20, 1], [12, 21, 1]], dtype=np.float64)
-        assert_almost_equal(exp_x_filtered, result._x_filtered.values)
-
-        self.assert_index_equal(result._x_filtered.index.levels[0],
-                                result.y_fitted.index)
-
-    def test_wls_panel(self):
-        y = tm.makeTimeDataFrame()
-        x = Panel({'x1': tm.makeTimeDataFrame(),
-                   'x2': tm.makeTimeDataFrame()})
-
-        y.iloc[[1, 7], y.columns.get_loc('A')] = np.nan
-        y.iloc[[6, 15], y.columns.get_loc('B')] = np.nan
-        y.iloc[[3, 20], y.columns.get_loc('C')] = np.nan
-        y.iloc[[5, 11], y.columns.get_loc('D')] = np.nan
-
-        stack_y = y.stack()
-        stack_x = DataFrame(dict((k, v.stack())
-                                 for k, v in x.iteritems()))
-
-        weights = x.std('items')
-        stack_weights = weights.stack()
-
-        stack_y.index = stack_y.index._tuple_index
-        stack_x.index = stack_x.index._tuple_index
-        stack_weights.index = stack_weights.index._tuple_index
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=y, x=x, weights=1 / weights)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            expected = ols(y=stack_y, x=stack_x, weights=1 / stack_weights)
-
-        assert_almost_equal(result.beta, expected.beta)
-
-        for attr in ['resid', 'y_fitted']:
-            rvals = getattr(result, attr).stack().values
-            evals = getattr(expected, attr).values
-            assert_almost_equal(rvals, evals)
-
-    def testWithTimeEffects(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2, time_effects=True)
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y_trans.values.flat, [0, -0.5, 0.5],
-                            check_dtype=False)
-
-        exp_x = np.array([[0, 0], [-10.5, -15.5], [10.5, 15.5]])
-        assert_almost_equal(result._x_trans.values, exp_x)
-
-        # _check_non_raw_results(result)
-
-    def testWithEntityEffects(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True)
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 4, 5],
-                            check_dtype=False)
-
-        exp_x = DataFrame([[0., 6., 14., 1.], [0, 9, 17, 1], [1, 30, 48, 1]],
-                          index=result._x.index, columns=['FE_B', 'x1', 'x2',
-                                                          'intercept'],
-                          dtype=float)
-        tm.assert_frame_equal(result._x, exp_x.loc[:, result._x.columns])
-        # _check_non_raw_results(result)
-
-    def testWithEntityEffectsAndDroppedDummies(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2, entity_effects=True,
-                         dropped_dummies={'entity': 'B'})
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 4, 5],
-                            check_dtype=False)
-        exp_x = DataFrame([[1., 6., 14., 1.], [1, 9, 17, 1], [0, 30, 48, 1]],
-                          index=result._x.index, columns=['FE_A', 'x1', 'x2',
-                                                          'intercept'],
-                          dtype=float)
-        tm.assert_frame_equal(result._x, exp_x.loc[:, result._x.columns])
-        # _check_non_raw_results(result)
-
-    def testWithXEffects(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'])
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 4, 5],
-                            check_dtype=False)
-
-        res = result._x
-        exp_x = DataFrame([[0., 0., 14., 1.], [0, 1, 17, 1], [1, 0, 48, 1]],
-                          columns=['x1_30', 'x1_9', 'x2', 'intercept'],
-                          index=res.index, dtype=float)
-        exp_x[['x1_30', 'x1_9']] = exp_x[['x1_30', 'x1_9']].astype(np.uint8)
-        assert_frame_equal(res, exp_x.reindex(columns=res.columns))
-
-    def testWithXEffectsAndDroppedDummies(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y2, x=self.panel_x2, x_effects=['x1'],
-                         dropped_dummies={'x1': 30})
-
-        res = result._x
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 4, 5],
-                            check_dtype=False)
-        exp_x = DataFrame([[1., 0., 14., 1.], [0, 1, 17, 1], [0, 0, 48, 1]],
-                          columns=['x1_6', 'x1_9', 'x2', 'intercept'],
-                          index=res.index, dtype=float)
-        exp_x[['x1_6', 'x1_9']] = exp_x[['x1_6', 'x1_9']].astype(np.uint8)
-
-        assert_frame_equal(res, exp_x.reindex(columns=res.columns))
-
-    def testWithXEffectsAndConversion(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y3, x=self.panel_x3,
-                         x_effects=['x1', 'x2'])
-
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4],
-                            check_dtype=False)
-        exp_x = np.array([[0, 0, 0, 1, 1], [1, 0, 0, 0, 1], [0, 1, 1, 0, 1],
-                          [0, 0, 0, 1, 1]], dtype=np.float64)
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_B', 'x1_C', 'x2_baz', 'x2_foo', 'intercept'])
-        self.assert_index_equal(exp_index, result._x.columns)
-
-        # _check_non_raw_results(result)
-
-    def testWithXEffectsAndConversionAndDroppedDummies(self):
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=self.panel_y3, x=self.panel_x3, x_effects=['x1', 'x2'],
-                         dropped_dummies={'x2': 'foo'})
-        # .flat is flatiter instance
-        assert_almost_equal(result._y.values.flat, [1, 2, 3, 4],
-                            check_dtype=False)
-        exp_x = np.array([[0, 0, 0, 0, 1], [1, 0, 1, 0, 1], [0, 1, 0, 1, 1],
-                          [0, 0, 0, 0, 1]], dtype=np.float64)
-        assert_almost_equal(result._x.values, exp_x)
-
-        exp_index = Index(['x1_B', 'x1_C', 'x2_bar', 'x2_baz', 'intercept'])
-        self.assert_index_equal(exp_index, result._x.columns)
-
-        # _check_non_raw_results(result)
-
-    def testForSeries(self):
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y)
-
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y, nw_lags=0)
-
-        self.checkForSeries(self.series_panel_x, self.series_panel_y,
-                            self.series_x, self.series_y, nw_lags=1,
-                            nw_overlap=True)
-
-    def testRolling(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y)
-
-    def testRollingWithFixedEffects(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            entity_effects=True)
-        self.checkMovingOLS(self.panel_x, self.panel_y, intercept=False,
-                            entity_effects=True)
-
-    def testRollingWithTimeEffects(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            time_effects=True)
-
-    def testRollingWithNeweyWest(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            nw_lags=1)
-
-    def testRollingWithEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            cluster='entity')
-
-    def testUnknownClusterRaisesValueError(self):
-        assertRaisesRegexp(ValueError, "Unrecognized cluster.*ridiculous",
-                           self.checkMovingOLS, self.panel_x, self.panel_y,
-                           cluster='ridiculous')
-
-    def testRollingWithTimeEffectsAndEntityCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            time_effects=True, cluster='entity')
-
-    def testRollingWithTimeCluster(self):
-        self.checkMovingOLS(self.panel_x, self.panel_y,
-                            cluster='time')
-
-    def testRollingWithNeweyWestAndEntityCluster(self):
-        self.assertRaises(ValueError, self.checkMovingOLS,
-                          self.panel_x, self.panel_y,
-                          nw_lags=1, cluster='entity')
-
-    def testRollingWithNeweyWestAndTimeEffectsAndEntityCluster(self):
-        self.assertRaises(ValueError,
-                          self.checkMovingOLS, self.panel_x, self.panel_y,
-                          nw_lags=1, cluster='entity',
-                          time_effects=True)
-
-    def testExpanding(self):
-        self.checkMovingOLS(
-            self.panel_x, self.panel_y, window_type='expanding')
-
-    def testNonPooled(self):
-        self.checkNonPooled(y=self.panel_y, x=self.panel_x)
-        self.checkNonPooled(y=self.panel_y, x=self.panel_x,
-                            window_type='rolling', window=25, min_periods=10)
-
-    def testUnknownWindowType(self):
-        assertRaisesRegexp(ValueError, "window.*ridiculous",
-                           self.checkNonPooled, y=self.panel_y, x=self.panel_x,
-                           window_type='ridiculous', window=25, min_periods=10)
-
-    def checkNonPooled(self, x, y, **kwds):
-        # For now, just check that it doesn't crash
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=y, x=x, pool=False, **kwds)
-
-        _check_repr(result)
-        for attr in NonPooledPanelOLS.ATTRIBUTES:
-            _check_repr(getattr(result, attr))
-
-    def checkMovingOLS(self, x, y, window_type='rolling', **kwds):
-        window = 25  # must be larger than rank of x
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            moving = ols(y=y, x=x, window_type=window_type,
-                         window=window, **kwds)
-
-        index = moving._index
-
-        for n, i in enumerate(moving._valid_indices):
-            if window_type == 'rolling' and i >= window:
-                prior_date = index[i - window + 1]
-            else:
-                prior_date = index[0]
-
-            date = index[i]
-
-            x_iter = {}
-            for k, v in compat.iteritems(x):
-                x_iter[k] = v.truncate(before=prior_date, after=date)
-            y_iter = y.truncate(before=prior_date, after=date)
-
-            with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-                static = ols(y=y_iter, x=x_iter, **kwds)
-
-            self.compare(static, moving, event_index=i,
-                         result_index=n)
-
-        _check_non_raw_results(moving)
-
-    def checkForSeries(self, x, y, series_x, series_y, **kwds):
-        # Consistency check with simple OLS.
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            result = ols(y=y, x=x, **kwds)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            reference = ols(y=series_y, x=series_x, **kwds)
-
-        self.compare(reference, result)
-
-    def compare(self, static, moving, event_index=None,
-                result_index=None):
-
-        # Check resid if we have a time index specified
-        if event_index is not None:
-            staticSlice = _period_slice(static, -1)
-            movingSlice = _period_slice(moving, event_index)
-
-            ref = static._resid_raw[staticSlice]
-            res = moving._resid_raw[movingSlice]
-
-            assert_almost_equal(ref, res)
-
-            ref = static._y_fitted_raw[staticSlice]
-            res = moving._y_fitted_raw[movingSlice]
-
-            assert_almost_equal(ref, res)
-
-        # Check y_fitted
-
-        for field in self.FIELDS:
-            attr = '_%s_raw' % field
-
-            ref = getattr(static, attr)
-            res = getattr(moving, attr)
-
-            if result_index is not None:
-                res = res[result_index]
-
-            assert_almost_equal(ref, res)
-
-    def test_auto_rolling_window_type(self):
-        data = tm.makeTimeDataFrame()
-        y = data.pop('A')
-
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            window_model = ols(y=y, x=data, window=20, min_periods=10)
-        with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
-            rolling_model = ols(y=y, x=data, window=20, min_periods=10,
-                                window_type='rolling')
-
-        assert_frame_equal(window_model.beta, rolling_model.beta)
-
-    def test_group_agg(self):
-        from pandas.stats.plm import _group_agg
-
-        values = np.ones((10, 2)) * np.arange(10).reshape((10, 1))
-        bounds = np.arange(5) * 2
-        f = lambda x: x.mean(axis=0)
-
-        agged = _group_agg(values, bounds, f)
-
-        assert(agged[1][0] == 2.5)
-        assert(agged[2][0] == 4.5)
-
-        # test a function that doesn't aggregate
-        f2 = lambda x: np.zeros((2, 2))
-        self.assertRaises(Exception, _group_agg, values, bounds, f2)
-
-
-def _check_non_raw_results(model):
-    _check_repr(model)
-    _check_repr(model.resid)
-    _check_repr(model.summary_as_matrix)
-    _check_repr(model.y_fitted)
-    _check_repr(model.y_predict)
-
-
-def _period_slice(panelModel, i):
-    index = panelModel._x_trans.index
-    period = index.levels[0][i]
-
-    L, R = index.get_major_bounds(period, period)
-
-    return slice(L, R)
-
-
-class TestOLSFilter(tm.TestCase):
-
-    def setUp(self):
-        date_index = date_range(datetime(2009, 12, 11), periods=3,
-                                freq=offsets.BDay())
-        ts = Series([3, 1, 4], index=date_index)
-        self.TS1 = ts
-
-        date_index = date_range(datetime(2009, 12, 11), periods=5,
-                                freq=offsets.BDay())
-        ts = Series([1, 5, 9, 2, 6], index=date_index)
-        self.TS2 = ts
-
-        date_index = date_range(datetime(2009, 12, 11), periods=3,
-                                freq=offsets.BDay())
-        ts = Series([5, np.nan, 3], index=date_index)
-        self.TS3 = ts
-
-        date_index = date_range(datetime(2009, 12, 11), periods=5,
-                                freq=offsets.BDay())
-        ts = Series([np.nan, 5, 8, 9, 7], index=date_index)
-        self.TS4 = ts
-
-        data = {'x1': self.TS2, 'x2': self.TS4}
-        self.DF1 = DataFrame(data=data)
-
-        data = {'x1': self.TS2, 'x2': self.TS4}
-        self.DICT1 = data
-
-    def testFilterWithSeriesRHS(self):
-        (lhs, rhs, weights, rhs_pre,
-         index, valid) = _filter_data(self.TS1, {'x1': self.TS2}, None)
-        self.tsAssertEqual(self.TS1.astype(np.float64), lhs, check_names=False)
-        self.tsAssertEqual(self.TS2[:3].astype(np.float64), rhs['x1'],
-                           check_names=False)
-        self.tsAssertEqual(self.TS2.astype(np.float64), rhs_pre['x1'],
-                           check_names=False)
-
-    def testFilterWithSeriesRHS2(self):
-        (lhs, rhs, weights, rhs_pre,
-         index, valid) = _filter_data(self.TS2, {'x1': self.TS1}, None)
-        self.tsAssertEqual(self.TS2[:3].astype(np.float64), lhs,
-                           check_names=False)
-        self.tsAssertEqual(self.TS1.astype(np.float64), rhs['x1'],
-                           check_names=False)
-        self.tsAssertEqual(self.TS1.astype(np.float64), rhs_pre['x1'],
-                           check_names=False)
-
-    def testFilterWithSeriesRHS3(self):
-        (lhs, rhs, weights, rhs_pre,
-         index, valid) = _filter_data(self.TS3, {'x1': self.TS4}, None)
-        exp_lhs = self.TS3[2:3]
-        exp_rhs = self.TS4[2:3]
-        exp_rhs_pre = self.TS4[1:]
-        self.tsAssertEqual(exp_lhs, lhs, check_names=False)
-        self.tsAssertEqual(exp_rhs, rhs['x1'], check_names=False)
-        self.tsAssertEqual(exp_rhs_pre, rhs_pre['x1'], check_names=False)
-
-    def testFilterWithDataFrameRHS(self):
-        (lhs, rhs, weights, rhs_pre,
-         index, valid) = _filter_data(self.TS1, self.DF1, None)
-        exp_lhs = self.TS1[1:].astype(np.float64)
-        exp_rhs1 = self.TS2[1:3]
-        exp_rhs2 = self.TS4[1:3].astype(np.float64)
-        self.tsAssertEqual(exp_lhs, lhs, check_names=False)
-        self.tsAssertEqual(exp_rhs1, rhs['x1'], check_names=False)
-        self.tsAssertEqual(exp_rhs2, rhs['x2'], check_names=False)
-
-    def testFilterWithDictRHS(self):
-        (lhs, rhs, weights, rhs_pre,
-         index, valid) = _filter_data(self.TS1, self.DICT1, None)
-        exp_lhs = self.TS1[1:].astype(np.float64)
-        exp_rhs1 = self.TS2[1:3].astype(np.float64)
-        exp_rhs2 = self.TS4[1:3].astype(np.float64)
-        self.tsAssertEqual(exp_lhs, lhs, check_names=False)
-        self.tsAssertEqual(exp_rhs1, rhs['x1'], check_names=False)
-        self.tsAssertEqual(exp_rhs2, rhs['x2'], check_names=False)
-
-    def tsAssertEqual(self, ts1, ts2, **kwargs):
-        self.assert_series_equal(ts1, ts2, **kwargs)
diff --git a/pandas/stats/tests/test_var.py b/pandas/stats/tests/test_var.py
deleted file mode 100644
index 04e2019f0..000000000
--- a/pandas/stats/tests/test_var.py
+++ /dev/null
@@ -1,94 +0,0 @@
-# flake8: noqa
-
-from __future__ import print_function
-
-import pandas.util.testing as tm
-
-from pandas.compat import range
-import nose
-
-raise nose.SkipTest('skipping this for now')
-
-try:
-    import statsmodels.tsa.var as sm_var
-    import statsmodels as sm
-except ImportError:
-    import scikits.statsmodels.tsa.var as sm_var
-    import scikits.statsmodels as sm
-
-
-import pandas.stats.var as _pvar
-reload(_pvar)
-from pandas.stats.var import VAR
-
-DECIMAL_6 = 6
-DECIMAL_5 = 5
-DECIMAL_4 = 4
-DECIMAL_3 = 3
-DECIMAL_2 = 2
-
-
-class CheckVAR(object):
-
-    def test_params(self):
-        tm.assert_almost_equal(self.res1.params, self.res2.params, DECIMAL_3)
-
-    def test_neqs(self):
-        tm.assert_numpy_array_equal(self.res1.neqs, self.res2.neqs)
-
-    def test_nobs(self):
-        tm.assert_numpy_array_equal(self.res1.avobs, self.res2.nobs)
-
-    def test_df_eq(self):
-        tm.assert_numpy_array_equal(self.res1.df_eq, self.res2.df_eq)
-
-    def test_rmse(self):
-        results = self.res1.results
-        for i in range(len(results)):
-            tm.assert_almost_equal(results[i].mse_resid ** .5,
-                                   eval('self.res2.rmse_' + str(i + 1)),
-                                   DECIMAL_6)
-
-    def test_rsquared(self):
-        results = self.res1.results
-        for i in range(len(results)):
-            tm.assert_almost_equal(results[i].rsquared,
-                                   eval('self.res2.rsquared_' + str(i + 1)),
-                                   DECIMAL_3)
-
-    def test_llf(self):
-        results = self.res1.results
-        tm.assert_almost_equal(self.res1.llf, self.res2.llf, DECIMAL_2)
-        for i in range(len(results)):
-            tm.assert_almost_equal(results[i].llf,
-                                   eval('self.res2.llf_' + str(i + 1)),
-                                   DECIMAL_2)
-
-    def test_aic(self):
-        tm.assert_almost_equal(self.res1.aic, self.res2.aic)
-
-    def test_bic(self):
-        tm.assert_almost_equal(self.res1.bic, self.res2.bic)
-
-    def test_hqic(self):
-        tm.assert_almost_equal(self.res1.hqic, self.res2.hqic)
-
-    def test_fpe(self):
-        tm.assert_almost_equal(self.res1.fpe, self.res2.fpe)
-
-    def test_detsig(self):
-        tm.assert_almost_equal(self.res1.detomega, self.res2.detsig)
-
-    def test_bse(self):
-        tm.assert_almost_equal(self.res1.bse, self.res2.bse, DECIMAL_4)
-
-
-class Foo(object):
-
-    def __init__(self):
-        data = sm.datasets.macrodata.load()
-        data = data.data[['realinv', 'realgdp', 'realcons']].view((float, 3))
-        data = diff(log(data), axis=0)
-        self.res1 = VAR2(endog=data).fit(maxlag=2)
-        from results import results_var
-        self.res2 = results_var.MacrodataResults()
diff --git a/pandas/stats/var.py b/pandas/stats/var.py
deleted file mode 100644
index db4028d60..000000000
--- a/pandas/stats/var.py
+++ /dev/null
@@ -1,605 +0,0 @@
-# flake8: noqa
-
-from __future__ import division
-
-from pandas.compat import range, lrange, zip, reduce
-from pandas import compat
-import numpy as np
-from pandas.core.base import StringMixin
-from pandas.util.decorators import cache_readonly
-from pandas.core.frame import DataFrame
-from pandas.core.panel import Panel
-from pandas.core.series import Series
-import pandas.stats.common as common
-from pandas.stats.math import inv
-from pandas.stats.ols import _combine_rhs
-
-
-class VAR(StringMixin):
-    """
-    Estimates VAR(p) regression on multivariate time series data
-    presented in pandas data structures.
-
-    Parameters
-    ----------
-    data : DataFrame or dict of Series
-    p : lags to include
-
-    """
-
-    def __init__(self, data, p=1, intercept=True):
-        import warnings
-        warnings.warn("The pandas.stats.var module is deprecated and will be "
-                      "removed in a future version. We refer to external packages "
-                      "like statsmodels, see some examples here: "
-                      "http://www.statsmodels.org/stable/vector_ar.html#var",
-                      FutureWarning, stacklevel=4)
-
-        try:
-            import statsmodels.tsa.vector_ar.api as sm_var
-        except ImportError:
-            import scikits.statsmodels.tsa.var as sm_var
-
-        self._data = DataFrame(_combine_rhs(data))
-        self._p = p
-
-        self._columns = self._data.columns
-        self._index = self._data.index
-
-        self._intercept = intercept
-
-    @cache_readonly
-    def aic(self):
-        """Returns the Akaike information criterion."""
-        return self._ic['aic']
-
-    @cache_readonly
-    def bic(self):
-        """Returns the Bayesian information criterion."""
-        return self._ic['bic']
-
-    @cache_readonly
-    def beta(self):
-        """
-        Returns a DataFrame, where each column x1 contains the betas
-        calculated by regressing the x1 column of the VAR input with
-        the lagged input.
-
-        Returns
-        -------
-        DataFrame
-        """
-        d = dict([(key, value.beta)
-                  for (key, value) in compat.iteritems(self.ols_results)])
-        return DataFrame(d)
-
-    def forecast(self, h):
-        """
-        Returns a DataFrame containing the forecasts for 1, 2, ..., n time
-        steps.  Each column x1 contains the forecasts of the x1 column.
-
-        Parameters
-        ----------
-        n: int
-            Number of time steps ahead to forecast.
-
-        Returns
-        -------
-        DataFrame
-        """
-        forecast = self._forecast_raw(h)[:, 0, :]
-        return DataFrame(forecast, index=lrange(1, 1 + h),
-                         columns=self._columns)
-
-    def forecast_cov(self, h):
-        """
-        Returns the covariance of the forecast residuals.
-
-        Returns
-        -------
-        DataFrame
-        """
-        return [DataFrame(value, index=self._columns, columns=self._columns)
-                for value in self._forecast_cov_raw(h)]
-
-    def forecast_std_err(self, h):
-        """
-        Returns the standard errors of the forecast residuals.
-
-        Returns
-        -------
-        DataFrame
-        """
-        return DataFrame(self._forecast_std_err_raw(h),
-                         index=lrange(1, 1 + h), columns=self._columns)
-
-    @cache_readonly
-    def granger_causality(self):
-        """Returns the f-stats and p-values from the Granger Causality Test.
-
-        If the data consists of columns x1, x2, x3, then we perform the
-        following regressions:
-
-        x1 ~ L(x2, x3)
-        x1 ~ L(x1, x3)
-        x1 ~ L(x1, x2)
-
-        The f-stats of these results are placed in the 'x1' column of the
-        returned DataFrame.  We then repeat for x2, x3.
-
-        Returns
-        -------
-        Dict, where 'f-stat' returns the DataFrame containing the f-stats,
-        and 'p-value' returns the DataFrame containing the corresponding
-        p-values of the f-stats.
-        """
-        from pandas.stats.api import ols
-        from scipy.stats import f
-
-        d = {}
-        for col in self._columns:
-            d[col] = {}
-            for i in range(1, 1 + self._p):
-                lagged_data = self._lagged_data[i].filter(
-                    self._columns - [col])
-
-                for key, value in compat.iteritems(lagged_data):
-                    d[col][_make_param_name(i, key)] = value
-
-        f_stat_dict = {}
-        p_value_dict = {}
-
-        for col, y in compat.iteritems(self._data):
-            ssr_full = (self.resid[col] ** 2).sum()
-
-            f_stats = []
-            p_values = []
-
-            for col2 in self._columns:
-                result = ols(y=y, x=d[col2])
-
-                resid = result.resid
-                ssr_reduced = (resid ** 2).sum()
-
-                M = self._p
-                N = self._nobs
-                K = self._k * self._p + 1
-                f_stat = ((ssr_reduced - ssr_full) / M) / (ssr_full / (N - K))
-                f_stats.append(f_stat)
-
-                p_value = f.sf(f_stat, M, N - K)
-                p_values.append(p_value)
-
-            f_stat_dict[col] = Series(f_stats, self._columns)
-            p_value_dict[col] = Series(p_values, self._columns)
-
-        f_stat_mat = DataFrame(f_stat_dict)
-        p_value_mat = DataFrame(p_value_dict)
-
-        return {
-            'f-stat': f_stat_mat,
-            'p-value': p_value_mat,
-        }
-
-    @cache_readonly
-    def ols_results(self):
-        """
-        Returns the results of the regressions:
-        x_1 ~ L(X)
-        x_2 ~ L(X)
-        ...
-        x_k ~ L(X)
-
-        where X = [x_1, x_2, ..., x_k]
-        and L(X) represents the columns of X lagged 1, 2, ..., n lags
-        (n is the user-provided number of lags).
-
-        Returns
-        -------
-        dict
-        """
-        from pandas.stats.api import ols
-
-        d = {}
-        for i in range(1, 1 + self._p):
-            for col, series in compat.iteritems(self._lagged_data[i]):
-                d[_make_param_name(i, col)] = series
-
-        result = dict([(col, ols(y=y, x=d, intercept=self._intercept))
-                       for col, y in compat.iteritems(self._data)])
-
-        return result
-
-    @cache_readonly
-    def resid(self):
-        """
-        Returns the DataFrame containing the residuals of the VAR regressions.
-        Each column x1 contains the residuals generated by regressing the x1
-        column of the input against the lagged input.
-
-        Returns
-        -------
-        DataFrame
-        """
-        d = dict([(col, series.resid)
-                  for (col, series) in compat.iteritems(self.ols_results)])
-        return DataFrame(d, index=self._index)
-
-    @cache_readonly
-    def summary(self):
-        template = """
-%(banner_top)s
-
-Number of Observations:         %(nobs)d
-AIC:                            %(aic).3f
-BIC:                            %(bic).3f
-
-%(banner_coef)s
-%(coef_table)s
-%(banner_end)s
-"""
-        params = {
-            'banner_top': common.banner('Summary of VAR'),
-            'banner_coef': common.banner('Summary of Estimated Coefficients'),
-            'banner_end': common.banner('End of Summary'),
-            'coef_table': self.beta,
-            'aic': self.aic,
-            'bic': self.bic,
-            'nobs': self._nobs,
-        }
-
-        return template % params
-
-    @cache_readonly
-    def _alpha(self):
-        """
-        Returns array where the i-th element contains the intercept
-        when regressing the i-th column of self._data with the lagged data.
-        """
-        if self._intercept:
-            return self._beta_raw[-1]
-        else:
-            return np.zeros(self._k)
-
-    @cache_readonly
-    def _beta_raw(self):
-        return np.array([list(self.beta[col].values()) for col in self._columns]).T
-
-    def _trans_B(self, h):
-        """
-        Returns 0, 1, ..., (h-1)-th power of transpose of B as defined in
-        equation (4) on p. 142 of the Stata 11 Time Series reference book.
-        """
-        result = [np.eye(1 + self._k * self._p)]
-
-        row1 = np.zeros((1, 1 + self._k * self._p))
-        row1[0, 0] = 1
-
-        v = self._alpha.reshape((self._k, 1))
-        row2 = np.hstack(tuple([v] + self._lag_betas))
-
-        m = self._k * (self._p - 1)
-        row3 = np.hstack((
-            np.zeros((m, 1)),
-            np.eye(m),
-            np.zeros((m, self._k))
-        ))
-
-        trans_B = np.vstack((row1, row2, row3)).T
-
-        result.append(trans_B)
-
-        for i in range(2, h):
-            result.append(np.dot(trans_B, result[i - 1]))
-
-        return result
-
-    @cache_readonly
-    def _x(self):
-        values = np.array([
-            list(self._lagged_data[i][col].values())
-            for i in range(1, 1 + self._p)
-            for col in self._columns
-        ]).T
-
-        x = np.hstack((np.ones((len(values), 1)), values))[self._p:]
-
-        return x
-
-    @cache_readonly
-    def _cov_beta(self):
-        cov_resid = self._sigma
-
-        x = self._x
-
-        inv_cov_x = inv(np.dot(x.T, x))
-
-        return np.kron(inv_cov_x, cov_resid)
-
-    def _data_xs(self, i):
-        """
-        Returns the cross-section of the data at the given timestep.
-        """
-        return self._data.values[i]
-
-    def _forecast_cov_raw(self, n):
-        resid = self._forecast_cov_resid_raw(n)
-        # beta = self._forecast_cov_beta_raw(n)
-
-        # return [a + b for a, b in zip(resid, beta)]
-        # TODO: ignore the beta forecast std err until it's verified
-
-        return resid
-
-    def _forecast_cov_beta_raw(self, n):
-        """
-        Returns the covariance of the beta errors for the forecast at
-        1, 2, ..., n timesteps.
-        """
-        p = self._p
-
-        values = self._data.values
-        T = len(values) - self._p - 1
-
-        results = []
-
-        for h in range(1, n + 1):
-            psi = self._psi(h)
-            trans_B = self._trans_B(h)
-
-            sum = 0
-
-            cov_beta = self._cov_beta
-
-            for t in range(T + 1):
-                index = t + p
-                y = values.take(lrange(index, index - p, -1), axis=0).ravel()
-                trans_Z = np.hstack(([1], y))
-                trans_Z = trans_Z.reshape(1, len(trans_Z))
-
-                sum2 = 0
-                for i in range(h):
-                    ZB = np.dot(trans_Z, trans_B[h - 1 - i])
-
-                    prod = np.kron(ZB, psi[i])
-                    sum2 = sum2 + prod
-
-                sum = sum + chain_dot(sum2, cov_beta, sum2.T)
-
-            results.append(sum / (T + 1))
-
-        return results
-
-    def _forecast_cov_resid_raw(self, h):
-        """
-        Returns the covariance of the residual errors for the forecast at
-        1, 2, ..., h timesteps.
-        """
-        psi_values = self._psi(h)
-        sum = 0
-        result = []
-        for i in range(h):
-            psi = psi_values[i]
-            sum = sum + chain_dot(psi, self._sigma, psi.T)
-            result.append(sum)
-
-        return result
-
-    def _forecast_raw(self, h):
-        """
-        Returns the forecast at 1, 2, ..., h timesteps in the future.
-        """
-        k = self._k
-        result = []
-        for i in range(h):
-            sum = self._alpha.reshape(1, k)
-            for j in range(self._p):
-                beta = self._lag_betas[j]
-                idx = i - j
-                if idx > 0:
-                    y = result[idx - 1]
-                else:
-                    y = self._data_xs(idx - 1)
-
-                sum = sum + np.dot(beta, y.T).T
-            result.append(sum)
-
-        return np.array(result)
-
-    def _forecast_std_err_raw(self, h):
-        """
-        Returns the standard error of the forecasts
-        at 1, 2, ..., n timesteps.
-        """
-        return np.array([np.sqrt(np.diag(value))
-                         for value in self._forecast_cov_raw(h)])
-
-    @cache_readonly
-    def _ic(self):
-        """
-        Returns the Akaike/Bayesian information criteria.
-        """
-        RSS = self._rss
-        k = self._p * (self._k * self._p + 1)
-        n = self._nobs * self._k
-
-        return {'aic': 2 * k + n * np.log(RSS / n),
-                'bic': n * np.log(RSS / n) + k * np.log(n)}
-
-    @cache_readonly
-    def _k(self):
-        return len(self._columns)
-
-    @cache_readonly
-    def _lag_betas(self):
-        """
-        Returns list of B_i, where B_i represents the (k, k) matrix
-        with the j-th row containing the betas of regressing the j-th
-        column of self._data with self._data lagged i time steps.
-        First element is B_1, second element is B_2, etc.
-        """
-        k = self._k
-        b = self._beta_raw
-        return [b[k * i: k * (i + 1)].T for i in range(self._p)]
-
-    @cache_readonly
-    def _lagged_data(self):
-        return dict([(i, self._data.shift(i))
-                     for i in range(1, 1 + self._p)])
-
-    @cache_readonly
-    def _nobs(self):
-        return len(self._data) - self._p
-
-    def _psi(self, h):
-        """
-        psi value used for calculating standard error.
-
-        Returns [psi_0, psi_1, ..., psi_(h - 1)]
-        """
-        k = self._k
-        result = [np.eye(k)]
-        for i in range(1, h):
-            result.append(sum(
-                [np.dot(result[i - j], self._lag_betas[j - 1])
-                 for j in range(1, 1 + i)
-                 if j <= self._p]))
-
-        return result
-
-    @cache_readonly
-    def _resid_raw(self):
-        resid = np.array([self.ols_results[col]._resid_raw
-                          for col in self._columns])
-        return resid
-
-    @cache_readonly
-    def _rss(self):
-        """Returns the sum of the squares of the residuals."""
-        return (self._resid_raw ** 2).sum()
-
-    @cache_readonly
-    def _sigma(self):
-        """Returns covariance of resids."""
-        k = self._k
-        n = self._nobs
-
-        resid = self._resid_raw
-
-        return np.dot(resid, resid.T) / (n - k)
-
-    def __unicode__(self):
-        return self.summary
-
-
-def lag_select(data, max_lags=5, ic=None):
-    """
-    Select number of lags based on a variety of information criteria
-
-    Parameters
-    ----------
-    data : DataFrame-like
-    max_lags : int
-        Maximum number of lags to evaluate
-    ic : {None, 'aic', 'bic', ...}
-        Choosing None will just display the results
-
-    Returns
-    -------
-    None
-    """
-    pass
-
-
-class PanelVAR(VAR):
-    """
-    Performs Vector Autoregression on panel data.
-
-    Parameters
-    ----------
-    data: Panel or dict of DataFrame
-    lags: int
-    """
-
-    def __init__(self, data, lags, intercept=True):
-        self._data = _prep_panel_data(data)
-        self._p = lags
-        self._intercept = intercept
-
-        self._columns = self._data.items
-
-    @cache_readonly
-    def _nobs(self):
-        """Returns the number of observations."""
-        _, timesteps, entities = self._data.values.shape
-        return (timesteps - self._p) * entities
-
-    @cache_readonly
-    def _rss(self):
-        """Returns the sum of the squares of the residuals."""
-        return (self.resid.values ** 2).sum()
-
-    def forecast(self, h):
-        """
-        Returns the forecasts at 1, 2, ..., n timesteps in the future.
-        """
-        forecast = self._forecast_raw(h).T.swapaxes(1, 2)
-        index = lrange(1, 1 + h)
-        w = Panel(forecast, items=self._data.items, major_axis=index,
-                  minor_axis=self._data.minor_axis)
-        return w
-
-    @cache_readonly
-    def resid(self):
-        """
-        Returns the DataFrame containing the residuals of the VAR regressions.
-        Each column x1 contains the residuals generated by regressing the x1
-        column of the input against the lagged input.
-
-        Returns
-        -------
-        DataFrame
-        """
-        d = dict([(key, value.resid)
-                  for (key, value) in compat.iteritems(self.ols_results)])
-        return Panel.fromDict(d)
-
-    def _data_xs(self, i):
-        return self._data.values[:, i, :].T
-
-    @cache_readonly
-    def _sigma(self):
-        """Returns covariance of resids."""
-        k = self._k
-        resid = _drop_incomplete_rows(self.resid.toLong().values)
-        n = len(resid)
-        return np.dot(resid.T, resid) / (n - k)
-
-
-def _prep_panel_data(data):
-    """Converts the given data into a Panel."""
-    if isinstance(data, Panel):
-        return data
-
-    return Panel.fromDict(data)
-
-
-def _drop_incomplete_rows(array):
-    mask = np.isfinite(array).all(1)
-    indices = np.arange(len(array))[mask]
-    return array.take(indices, 0)
-
-
-def _make_param_name(lag, name):
-    return 'L%d.%s' % (lag, name)
-
-
-def chain_dot(*matrices):
-    """
-    Returns the dot product of the given matrices.
-
-    Parameters
-    ----------
-    matrices: argument list of ndarray
-    """
-    return reduce(lambda x, y: np.dot(y, x), matrices[::-1])
diff --git a/pandas/util/print_versions.py b/pandas/util/print_versions.py
index c7b9f4bde..c3962ad9c 100644
--- a/pandas/util/print_versions.py
+++ b/pandas/util/print_versions.py
@@ -69,7 +69,6 @@ def show_versions(as_json=False):
         ("Cython", lambda mod: mod.__version__),
         ("numpy", lambda mod: mod.version.version),
         ("scipy", lambda mod: mod.version.version),
-        ("statsmodels", lambda mod: mod.__version__),
         ("xarray", lambda mod: mod.__version__),
         ("IPython", lambda mod: mod.__version__),
         ("sphinx", lambda mod: mod.__version__),
diff --git a/setup.py b/setup.py
index 3c2617da1..c3cb56f2d 100755
--- a/setup.py
+++ b/setup.py
@@ -660,7 +660,6 @@ setup(name=DISTNAME,
                 'pandas.io.tests.json',
                 'pandas.io.tests.parser',
                 'pandas.io.tests.sas',
-                'pandas.stats.tests',
                 'pandas.msgpack',
                 'pandas.util.clipboard'
                 ],
