commit 045c73015b8ed20f36e61288efeaa6ed3d2b7984
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Sep 25 18:40:02 2012 -0400

    ENH: more extensive refactoring to get new parser engine integrated / working

diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 3d1815e5a..3ff041419 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -20,12 +20,14 @@ from pandas.core.index import Index, MultiIndex
 from pandas.core.frame import DataFrame
 import datetime
 import pandas.core.common as com
-import pandas.lib as lib
 from pandas.util import py3compat
 from pandas.io.date_converters import generic_parser
 
 from pandas.util.decorators import Appender
 
+import pandas.lib as lib
+import pandas._parser as _parser
+
 class DateConversionError(Exception):
     pass
 
@@ -419,6 +421,7 @@ class TextFileReader(object):
                  skipinitialspace=False,
                  squeeze=False,
                  as_recarray=False,
+                 factorize=True,
                  **kwds):
 
         # Tokenization options
@@ -489,6 +492,8 @@ class TextFileReader(object):
         self.encoding = encoding
         self.engine_kind = engine
 
+        self.factorize = factorize
+
         self.chunksize = chunksize
 
         self._engine = None
@@ -541,6 +546,10 @@ class TextFileReader(object):
                       quotechar=self.quotechar,
                       quoting=self.quoting,
                       converters=self.converters,
+                      parse_dates=self.parse_dates,
+                      date_parser=self.date_parser,
+                      keep_date_col=self.keep_date_col,
+                      dayfirst=self.dayfirst,
                       na_values=self.na_values,
                       verbose=self.verbose)
 
@@ -549,14 +558,12 @@ class TextFileReader(object):
         if kind == 'c':
             params.update(warn_bad_lines=self.warn_bad_lines,
                           error_bad_lines=self.error_bad_lines,
-                          as_recarray=self.as_recarray)
+                          as_recarray=self.as_recarray,
+                          factorize=self.factorize)
+
             self._engine = CParserWrapper(self.f, **params)
         else:
-            params.update(encoding=self.encoding,
-                          parse_dates=self.parse_dates,
-                          date_parser=self.date_parser,
-                          keep_date_col=self.keep_date_col,
-                          dayfirst=self.dayfirst)
+            params.update(encoding=self.encoding)
 
             if kind == 'python':
                 klass = PythonParser
@@ -602,25 +609,35 @@ class CParserWrapper(object):
     def __init__(self, src, **kwds):
         kwds = kwds.copy()
 
-        self.names = kwds.pop('names', None)
+        self.names = kwds.get('names')
         self.index_col = kwds.pop('index_col', None)
-
         self.as_recarray = kwds.get('as_recarray', False)
+        self.kwds = kwds
 
-        if self.names is not None:
-            kwds['header'] = None
+        self.parse_dates = kwds.pop('parse_dates', False)
+        self.date_parser = kwds.pop('date_parser', None)
+        self.dayfirst = kwds.pop('dayfirst', False)
+        self.keep_date_col = kwds.pop('keep_date_col', False)
 
-        self.kwds = kwds
+        self._date_conv = _make_date_converter(date_parser=self.date_parser,
+                                               dayfirst=self.dayfirst)
+
+        self._reader = _parser.TextReader(src, **kwds)
 
-        from pandas._parser import TextReader
-        self._reader = TextReader(src, **kwds)
+        self.names, self.index_names = self._get_index_names()
+
+    def _get_names(self):
+        pass
 
     def read(self, nrows=None):
         if self.as_recarray:
             return self._reader.read(nrows)
 
-        names, data = self._reader.read(nrows)
-        index, names, data = self._get_index(names, data)
+        data = self._reader.read(nrows)
+        names = self.names
+
+        names, data = self._do_date_conversions(names, data)
+        index, names, data = self._make_index(names, data)
 
         # rename dict keys
         data = sorted(data.items())
@@ -628,31 +645,80 @@ class CParserWrapper(object):
 
         return index, names, data
 
-    def _get_index(self, names, data):
+    def _do_date_conversions(self, names, data):
+        # returns data, columns
+        data, names = _process_date_conversion(
+            data, self._date_conv, self.parse_dates, self.index_col,
+            self.index_names, names, keep_date_col=self.keep_date_col)
+
+        return names, data
+
+    def _get_index_names(self):
+        names = list(self._reader.header)
+        idx_names = None
+
+        if self._reader.leading_cols == 0 and self.index_col is not None:
+            (idx_names, names,
+             self.index_col) = _clean_index_names(names, self.index_col)
+
+        return names, idx_names
+
+    def _make_index(self, names, data, try_parse_dates=True):
         if names is None:
             names = self.names
 
         if names is None: # still None
             names = range(len(data))
 
-        if self.index_col is not None:
-            (idx_names, names,
-             self.index_col) = _clean_index_names(names, self.index_col)
+        def _maybe_parse_dates(values, index):
+            if try_parse_dates and self._should_parse_dates(index):
+                values = self._date_conv(values)
+            return values
 
+        if self._reader.leading_cols > 0:
+            # implicit index, no index names
             arrays = []
-            for i in self.index_col:
-                arrays.append(data.pop(i))
-            index = MultiIndex.from_arrays(arrays, names=idx_names)
-        elif len(data) > len(names):
-            # possible implicit index
-            pass
-        elif len(data) < len(names):
-            raise ValueError('Fewer data columns than header names')
+
+            for i in range(self._reader.leading_cols):
+                if self.index_col is None:
+                    values = data.pop(i)
+                else:
+                    values = data.pop(self.index_col[i])
+
+                values = _maybe_parse_dates(values, i)
+                arrays.append(values)
+
+            index = MultiIndex.from_arrays(arrays)
+
+        elif self.index_col is not None:
+            arrays = []
+            for i, col in enumerate(self.index_col):
+                if col not in data:
+                    raise ValueError('Invalid index column %s' % i)
+
+                values = data.pop(col)
+
+                values = _maybe_parse_dates(values, col)
+
+                arrays.append(values)
+
+            index = MultiIndex.from_arrays(arrays, names=self.index_names)
         else:
             index = None
 
         return index, names, data
 
+    def _should_parse_dates(self, i):
+        if isinstance(self.parse_dates, bool):
+            return self.parse_dates
+        else:
+            name = self.index_names[i]
+            j = self.index_col[i]
+
+            if np.isscalar(self.parse_dates):
+                return (j == self.parse_dates) or (name == self.parse_dates)
+            else:
+                return (j in self.parse_dates) or (name in self.parse_dates)
 
 
 def TextParser(*args, **kwds):
@@ -726,9 +792,11 @@ class PythonParser(object):
         self.encoding = encoding
 
         self.parse_dates = parse_dates
-        self.keep_date_col = keep_date_col
         self.date_parser = date_parser
         self.dayfirst = dayfirst
+        self._date_conv = _make_date_converter(date_parser=date_parser,
+                                               dayfirst=dayfirst)
+        self.keep_date_col = keep_date_col
 
         self.skiprows = skiprows
 
@@ -763,10 +831,10 @@ class PythonParser(object):
         # needs to be cleaned/refactored
         # multiple date column thing turning into a real spaghetti factory
 
-        self.index_name = None
+        self.index_names = None
         self._name_processed = False
         if not self._has_complex_date_col:
-            (self.index_name,
+            (self.index_names,
              self.orig_columns, _) = self._get_index_name(self.columns)
             self._name_processed = True
         self._first_chunk = True
@@ -844,7 +912,7 @@ class PythonParser(object):
             # DataFrame with the right metadata, even though it's length 0
             return _get_empty_meta(self.orig_columns,
                                    self.index_col,
-                                   self.index_name)
+                                   self.index_names)
 
         alldata = self._rows_to_cols(content)
         data = self._exclude_implicit_index(alldata)
@@ -852,11 +920,13 @@ class PythonParser(object):
         data = self._convert_data(data)
 
         if self.parse_dates is not None:
-            data, columns = self._process_date_conversion(data)
+            data, columns = _process_date_conversion(
+                data, self._date_conv, self.parse_dates, self.index_col,
+                self.index_names, self.columns,
+                keep_date_col=self.keep_date_col)
 
         if self.index_col is None:
-            numrows = len(content)
-            index = Index(np.arange(numrows))
+            index = None
 
         elif not self._has_complex_date_col:
             index = self._get_simple_index(alldata, columns)
@@ -864,12 +934,12 @@ class PythonParser(object):
 
         elif self._has_complex_date_col:
             if not self._name_processed:
-                (self.index_name, _,
+                (self.index_names, _,
                  self.index_col) = _clean_index_names(list(columns),
                                                       self.index_col)
                 self._name_processed = True
             index = self._get_complex_date_index(data, columns)
-            index = self._agg_index(index, False)
+            index = self._agg_index(index, try_parse_dates=False)
 
         return index, columns, data
 
@@ -878,15 +948,26 @@ class PythonParser(object):
 
     def _convert_data(self, data):
         # apply converters
+        converted = set()
         for col, f in self.converters.iteritems():
             if isinstance(col, int) and col not in self.orig_columns:
                 col = self.orig_columns[col]
             data[col] = lib.map_infer(data[col], f)
+            converted.add(col)
 
         # do type conversions
-        data = _convert_to_ndarrays(data, self.na_values, self.verbose)
+        result = {}
+        for c, values in data.iteritems():
+            if c in converted:
+                result[c] = values
 
-        return data
+            col_na_values = _get_na_values(c, self.na_values)
+            cvals, na_count = _convert_types(values, col_na_values)
+            result[c] = cvals
+            if self.verbose and na_count:
+                print 'Filled %d NA values in column %s' % (na_count, str(c))
+
+        return result
 
     def _infer_columns(self):
         names = self.names
@@ -1149,12 +1230,12 @@ class PythonParser(object):
         for i, arr in enumerate(index):
 
             if (try_parse_dates and self._should_parse_dates(i)):
-                arr = self._conv_date(arr)
+                arr = self._date_conv(arr)
 
             col_na_values = self.na_values
 
             if isinstance(self.na_values, dict):
-                col_name = self.index_name[i]
+                col_name = self.index_names[i]
                 if col_name is not None:
                     col_na_values = _get_na_values(col_name,
                                                    self.na_values)
@@ -1162,7 +1243,7 @@ class PythonParser(object):
             arr, _ = _convert_types(arr, col_na_values)
             arrays.append(arr)
 
-        index = MultiIndex.from_arrays(arrays, names=self.index_name)
+        index = MultiIndex.from_arrays(arrays, names=self.index_names)
 
         return index
 
@@ -1170,7 +1251,7 @@ class PythonParser(object):
         if isinstance(self.parse_dates, bool):
             return self.parse_dates
         else:
-            name = self.index_name[i]
+            name = self.index_names[i]
             j = self.index_col[i]
 
             if np.isscalar(self.parse_dates):
@@ -1178,80 +1259,6 @@ class PythonParser(object):
             else:
                 return (j in self.parse_dates) or (name in self.parse_dates)
 
-    def _conv_date(self, *date_cols):
-        if self.date_parser is None:
-            return lib.try_parse_dates(_concat_date_cols(date_cols),
-                                       dayfirst=self.dayfirst)
-        else:
-            try:
-                return self.date_parser(*date_cols)
-            except Exception, inst:
-                try:
-                    return generic_parser(self.date_parser, *date_cols)
-                except Exception, inst:
-                    return lib.try_parse_dates(_concat_date_cols(date_cols),
-                                               parser=self.date_parser,
-                                               dayfirst=self.dayfirst)
-
-    def _process_date_conversion(self, data_dict):
-        new_cols = []
-        new_data = {}
-        columns = list(self.orig_columns)
-        date_cols = set()
-
-        if self.parse_dates is None or isinstance(self.parse_dates, bool):
-            return data_dict, columns
-
-        if isinstance(self.parse_dates, list):
-            # list of column lists
-            for colspec in self.parse_dates:
-                if np.isscalar(colspec):
-                    if isinstance(colspec, int) and colspec not in data_dict:
-                        colspec = self.orig_columns[colspec]
-                    if self._isindex(colspec):
-                        continue
-                    data_dict[colspec] = self._conv_date(data_dict[colspec])
-                else:
-                    new_name, col, old_names = _try_convert_dates(
-                        self._conv_date, colspec, data_dict, self.orig_columns)
-                    if new_name in data_dict:
-                        raise ValueError('New date column already in dict %s' %
-                                         new_name)
-                    new_data[new_name] = col
-                    new_cols.append(new_name)
-                    date_cols.update(old_names)
-
-        elif isinstance(self.parse_dates, dict):
-            # dict of new name to column list
-            for new_name, colspec in self.parse_dates.iteritems():
-                if new_name in data_dict:
-                    raise ValueError('Date column %s already in dict' %
-                                     new_name)
-
-                _, col, old_names = _try_convert_dates(
-                    self._conv_date, colspec, data_dict, self.orig_columns)
-
-                new_data[new_name] = col
-                new_cols.append(new_name)
-                date_cols.update(old_names)
-
-        data_dict.update(new_data)
-        new_cols.extend(columns)
-
-        if not self.keep_date_col:
-            for c in list(date_cols):
-                data_dict.pop(c)
-                new_cols.remove(c)
-        return data_dict, new_cols
-
-    def _isindex(self, colspec):
-        return (colspec == self.index_col or
-                (isinstance(self.index_col, list) and
-                 colspec in self.index_col) or
-                (colspec == self.index_name or
-                 (isinstance(self.index_name, list) and
-                  colspec in self.index_name)))
-
     def _get_lines(self, rows=None):
         source = self.data
         lines = self.buf
@@ -1304,6 +1311,86 @@ class PythonParser(object):
         return self._check_thousands(lines)
 
 
+def _make_date_converter(date_parser=None, dayfirst=False):
+    def converter(*date_cols):
+        if date_parser is None:
+            return lib.try_parse_dates(_concat_date_cols(date_cols),
+                                       dayfirst=dayfirst)
+        else:
+            try:
+                return date_parser(*date_cols)
+            except Exception:
+                try:
+                    return generic_parser(date_parser, *date_cols)
+                except Exception:
+                    return lib.try_parse_dates(_concat_date_cols(date_cols),
+                                               parser=date_parser,
+                                               dayfirst=dayfirst)
+
+    return converter
+
+
+def _process_date_conversion(data_dict, converter, parse_spec,
+                             index_col, index_names, columns,
+                             keep_date_col=False):
+    def _isindex(colspec):
+        return colspec in index_col or colspec in index_names
+
+    new_cols = []
+    new_data = {}
+
+    orig_columns = columns
+    columns = list(columns)
+
+    date_cols = set()
+
+    if parse_spec is None or isinstance(parse_spec, bool):
+        return data_dict, columns
+
+    if isinstance(parse_spec, list):
+        # list of column lists
+        for colspec in parse_spec:
+            if np.isscalar(colspec):
+                if isinstance(colspec, int) and colspec not in data_dict:
+                    colspec = orig_columns[colspec]
+                if _isindex(colspec):
+                    continue
+                data_dict[colspec] = converter(data_dict[colspec])
+            else:
+                new_name, col, old_names = _try_convert_dates(
+                    converter, colspec, data_dict, orig_columns)
+                if new_name in data_dict:
+                    raise ValueError('New date column already in dict %s' %
+                                     new_name)
+                new_data[new_name] = col
+                new_cols.append(new_name)
+                date_cols.update(old_names)
+
+    elif isinstance(parse_spec, dict):
+        # dict of new name to column list
+        for new_name, colspec in parse_spec.iteritems():
+            if new_name in data_dict:
+                raise ValueError('Date column %s already in dict' %
+                                 new_name)
+
+            _, col, old_names = _try_convert_dates(converter, colspec,
+                                                   data_dict, orig_columns)
+
+            new_data[new_name] = col
+            new_cols.append(new_name)
+            date_cols.update(old_names)
+
+    data_dict.update(new_data)
+    new_cols.extend(columns)
+
+    if not keep_date_col:
+        for c in list(date_cols):
+            data_dict.pop(c)
+            new_cols.remove(c)
+
+    return data_dict, new_cols
+
+
 def _clean_index_names(columns, index_col):
     if index_col is None:
         return None, columns, index_col
@@ -1311,14 +1398,14 @@ def _clean_index_names(columns, index_col):
     columns = list(columns)
 
     cp_cols = list(columns)
-    index_name = []
+    index_names = []
 
     # don't mutate
     index_col = list(index_col)
 
     for i, c in enumerate(index_col):
         if isinstance(c, basestring):
-            index_name.append(c)
+            index_names.append(c)
             for j, name in enumerate(cp_cols):
                 if name == c:
                     index_col[i] = j
@@ -1327,21 +1414,21 @@ def _clean_index_names(columns, index_col):
         else:
             name = cp_cols[c]
             columns.remove(name)
-            index_name.append(name)
+            index_names.append(name)
 
     # hack
-    if index_name[0] is not None and 'Unnamed' in index_name[0]:
-        index_name[0] = None
+    if index_names[0] is not None and 'Unnamed' in index_names[0]:
+        index_names[0] = None
 
-    return index_name, columns, index_col
+    return index_names, columns, index_col
 
 
-def _get_empty_meta(columns, index_col, index_name):
+def _get_empty_meta(columns, index_col, index_names):
     columns = list(columns)
 
     if index_col is not None:
         index = MultiIndex.from_arrays([[]] * len(index_col),
-                                       names=index_name)
+                                       names=index_names)
         for n in index_col:
             columns.pop(n)
     else:
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index d5fd28b90..237d90fd7 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -426,7 +426,7 @@ skip
             it = self.read_table(StringIO(data), sep=',',
                             header=1, comment='#', iterator=True, chunksize=1,
                             skiprows=[2])
-            df = it.get_chunk(5)
+            df = it.read(5)
             self.assert_(False)
         except ValueError, inst:
             self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
@@ -445,8 +445,8 @@ skip
             it = self.read_table(StringIO(data), sep=',',
                             header=1, comment='#', iterator=True, chunksize=1,
                             skiprows=[2])
-            df = it.get_chunk(1)
-            it.get_chunk(2)
+            df = it.read(1)
+            it.read(2)
             self.assert_(False)
         except ValueError, inst:
             self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
@@ -465,8 +465,8 @@ skip
             it = self.read_table(StringIO(data), sep=',',
                             header=1, comment='#', iterator=True, chunksize=1,
                             skiprows=[2])
-            df = it.get_chunk(1)
-            it.get_chunk()
+            df = it.read(1)
+            it.read()
             self.assert_(False)
         except ValueError, inst:
             self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
@@ -706,59 +706,6 @@ baz,7,8,9
         self.assert_(df.ix[:, ['A', 'B', 'C', 'D']].values.dtype == np.float64)
         assert_frame_equal(df, df2)
 
-    def test_excel_stop_iterator(self):
-        _skip_if_no_xlrd()
-
-        excel_data = ExcelFile(os.path.join(self.dirpath, 'test2.xls'))
-        parsed = excel_data.parse('Sheet1')
-        expected = DataFrame([['aaaa','bbbbb']], columns=['Test', 'Test1'])
-        assert_frame_equal(parsed, expected)
-
-    def test_excel_cell_error_na(self):
-        _skip_if_no_xlrd()
-
-        excel_data = ExcelFile(os.path.join(self.dirpath, 'test3.xls'))
-        parsed = excel_data.parse('Sheet1')
-        expected = DataFrame([[np.nan]], columns=['Test'])
-        assert_frame_equal(parsed, expected)
-
-    def test_excel_table(self):
-        _skip_if_no_xlrd()
-
-        pth = os.path.join(self.dirpath, 'test.xls')
-        xls = ExcelFile(pth)
-        df = xls.parse('Sheet1', index_col=0, parse_dates=True)
-        df2 = self.read_csv(self.csv1, index_col=0, parse_dates=True)
-        df3 = xls.parse('Sheet2', skiprows=[1], index_col=0, parse_dates=True)
-        assert_frame_equal(df, df2)
-        assert_frame_equal(df3, df2)
-
-    def test_excel_read_buffer(self):
-        _skip_if_no_xlrd()
-        _skip_if_no_openpyxl()
-
-        pth = os.path.join(self.dirpath, 'test.xls')
-        f = open(pth, 'rb')
-        xls = ExcelFile(f)
-        # it works
-        xls.parse('Sheet1', index_col=0, parse_dates=True)
-
-        pth = os.path.join(self.dirpath, 'test.xlsx')
-        f = open(pth, 'rb')
-        xl = ExcelFile(f)
-        df = xl.parse('Sheet1', index_col=0, parse_dates=True)
-
-    def test_xlsx_table(self):
-        _skip_if_no_openpyxl()
-
-        pth = os.path.join(self.dirpath, 'test.xlsx')
-        xlsx = ExcelFile(pth)
-        df = xlsx.parse('Sheet1', index_col=0, parse_dates=True)
-        df2 = self.read_csv(self.csv1, index_col=0, parse_dates=True)
-        df3 = xlsx.parse('Sheet2', skiprows=[1], index_col=0, parse_dates=True)
-        assert_frame_equal(df, df2)
-        assert_frame_equal(df3, df2)
-
     def test_parse_cols_int(self):
         _skip_if_no_openpyxl()
         _skip_if_no_xlrd()
@@ -944,18 +891,19 @@ baz|7|8|9
         df = self.read_csv(StringIO(data), index_col=0)
 
         parser = TextParser(as_list, index_col=0, chunksize=2)
-        chunk  = parser.get_chunk(None)
+        chunk  = parser.read(None)
 
         assert_frame_equal(chunk, df)
 
     def test_iterator(self):
-        reader = self.read_csv(StringIO(self.data1), index_col=0, iterator=True)
+        reader = self.read_csv(StringIO(self.data1), index_col=0,
+                               iterator=True)
         df = self.read_csv(StringIO(self.data1), index_col=0)
 
-        chunk = reader.get_chunk(3)
+        chunk = reader.read(3)
         assert_frame_equal(chunk, df[:3])
 
-        last_chunk = reader.get_chunk(5)
+        last_chunk = reader.read(5)
         assert_frame_equal(last_chunk, df[3:])
 
         # pass list
@@ -975,9 +923,9 @@ baz|7|8|9
         assert_frame_equal(chunks[0], df[1:3])
 
         # test bad parameter (skip_footer)
-        reader = self.read_csv(StringIO(self.data1), index_col=0, iterator=True,
-                          skip_footer=True)
-        self.assertRaises(ValueError, reader.get_chunk, 3)
+        reader = self.read_csv(StringIO(self.data1), index_col=0,
+                               iterator=True, skip_footer=True)
+        self.assertRaises(ValueError, reader.read, 3)
 
         treader = self.read_table(StringIO(self.data1), sep=',', index_col=0,
                              iterator=True)
@@ -1049,6 +997,7 @@ bar,two,12,13,14,15
         lines = data.split('\n')
         no_header = '\n'.join(lines[1:])
         names = ['A', 'B', 'C', 'D']
+
         df = self.read_csv(StringIO(no_header), index_col=[0, 1], names=names)
         expected = self.read_csv(StringIO(data), index_col=[0, 1])
         assert_frame_equal(df, expected)
@@ -1057,6 +1006,11 @@ bar,two,12,13,14,15
         df2 = self.read_csv(StringIO(data2))
         assert_frame_equal(df2, df)
 
+        # reverse order of index
+        df = self.read_csv(StringIO(no_header), index_col=[1, 0], names=names)
+        expected = self.read_csv(StringIO(data), index_col=[1, 0])
+        assert_frame_equal(df, expected)
+
     def test_multi_index_parse_dates(self):
         data = """index1,index2,A,B,C
 20090101,one,a,1,2
@@ -1148,7 +1102,7 @@ c,4,5,01/03/2009
 1;1521,1541;187101,9543;ABC;poi;4,738797819
 2;121,12;14897,76;DEF;uyt;0,377320872
 3;878,158;108013,434;GHI;rez;2,735694704"""
-        f = lambda x : x.replace(",", ".")
+        f = lambda x : float(x.replace(",", "."))
         converter = {'Number1':f,'Number2':f, 'Number3':f}
         df2 = self.read_csv(StringIO(data), sep=';',converters=converter)
         self.assert_(df2['Number1'].dtype == float)
@@ -1254,59 +1208,6 @@ bar"""
                           parse_dates=True, date_parser=parser,
                           na_values=['NA'])
 
-    def test_converters_corner_with_nas(self):
-        import StringIO
-        csv = """id,score,days
-1,2,12
-2,2-5,
-3,,14+
-4,6-12,2"""
-
-        def convert_days(x):
-           x = x.strip()
-           if not x: return np.nan
-
-           is_plus = x.endswith('+')
-           if is_plus:
-               x = int(x[:-1]) + 1
-           else:
-               x = int(x)
-           return x
-
-        def convert_days_sentinel(x):
-           x = x.strip()
-           if not x: return -1
-
-           is_plus = x.endswith('+')
-           if is_plus:
-               x = int(x[:-1]) + 1
-           else:
-               x = int(x)
-           return x
-
-        def convert_score(x):
-           x = x.strip()
-           if not x: return np.nan
-           if x.find('-')>0:
-               valmin, valmax = map(int, x.split('-'))
-               val = 0.5*(valmin + valmax)
-           else:
-               val = float(x)
-
-           return val
-
-        fh = StringIO.StringIO(csv)
-        result = self.read_csv(fh, converters={'score':convert_score,
-                                                 'days':convert_days},
-                                 na_values=[-1,'',None])
-        self.assert_(isnull(result['days'][1]))
-
-        fh = StringIO.StringIO(csv)
-        result2 = self.read_csv(fh, converters={'score':convert_score,
-                                                  'days':convert_days_sentinel},
-                                  na_values=[-1,'',None])
-        assert_frame_equal(result, result2)
-
     def test_na_value_dict(self):
         data = """A,B,C
 foo,bar,NA
@@ -1489,6 +1390,113 @@ class TestPythonParser(ParserTests, unittest.TestCase):
                           colspecs=colspecs, widths=[6, 10, 10, 7])
 
 
+    def test_converters_corner_with_nas(self):
+        import StringIO
+        csv = """id,score,days
+1,2,12
+2,2-5,
+3,,14+
+4,6-12,2"""
+
+        def convert_days(x):
+           x = x.strip()
+           if not x: return np.nan
+
+           is_plus = x.endswith('+')
+           if is_plus:
+               x = int(x[:-1]) + 1
+           else:
+               x = int(x)
+           return x
+
+        def convert_days_sentinel(x):
+           x = x.strip()
+           if not x: return -1
+
+           is_plus = x.endswith('+')
+           if is_plus:
+               x = int(x[:-1]) + 1
+           else:
+               x = int(x)
+           return x
+
+        def convert_score(x):
+           x = x.strip()
+           if not x: return np.nan
+           if x.find('-')>0:
+               valmin, valmax = map(int, x.split('-'))
+               val = 0.5*(valmin + valmax)
+           else:
+               val = float(x)
+
+           return val
+
+        fh = StringIO.StringIO(csv)
+        result = self.read_csv(fh, converters={'score':convert_score,
+                                                 'days':convert_days},
+                                 na_values=[-1,'',None])
+        self.assert_(isnull(result['days'][1]))
+
+        fh = StringIO.StringIO(csv)
+        result2 = self.read_csv(fh, converters={'score':convert_score,
+                                                  'days':convert_days_sentinel},
+                                  na_values=[-1,'',None])
+        assert_frame_equal(result, result2)
+
+    def test_excel_stop_iterator(self):
+        _skip_if_no_xlrd()
+
+        excel_data = ExcelFile(os.path.join(self.dirpath, 'test2.xls'))
+        parsed = excel_data.parse('Sheet1')
+        expected = DataFrame([['aaaa','bbbbb']], columns=['Test', 'Test1'])
+        assert_frame_equal(parsed, expected)
+
+    def test_excel_cell_error_na(self):
+        _skip_if_no_xlrd()
+
+        excel_data = ExcelFile(os.path.join(self.dirpath, 'test3.xls'))
+        parsed = excel_data.parse('Sheet1')
+        expected = DataFrame([[np.nan]], columns=['Test'])
+        assert_frame_equal(parsed, expected)
+
+    def test_excel_table(self):
+        _skip_if_no_xlrd()
+
+        pth = os.path.join(self.dirpath, 'test.xls')
+        xls = ExcelFile(pth)
+        df = xls.parse('Sheet1', index_col=0, parse_dates=True)
+        df2 = self.read_csv(self.csv1, index_col=0, parse_dates=True)
+        df3 = xls.parse('Sheet2', skiprows=[1], index_col=0, parse_dates=True)
+        assert_frame_equal(df, df2)
+        assert_frame_equal(df3, df2)
+
+    def test_excel_read_buffer(self):
+        _skip_if_no_xlrd()
+        _skip_if_no_openpyxl()
+
+        pth = os.path.join(self.dirpath, 'test.xls')
+        f = open(pth, 'rb')
+        xls = ExcelFile(f)
+        # it works
+        xls.parse('Sheet1', index_col=0, parse_dates=True)
+
+        pth = os.path.join(self.dirpath, 'test.xlsx')
+        f = open(pth, 'rb')
+        xl = ExcelFile(f)
+        df = xl.parse('Sheet1', index_col=0, parse_dates=True)
+
+    def test_xlsx_table(self):
+        _skip_if_no_openpyxl()
+
+        pth = os.path.join(self.dirpath, 'test.xlsx')
+        xlsx = ExcelFile(pth)
+        df = xlsx.parse('Sheet1', index_col=0, parse_dates=True)
+        df2 = self.read_csv(self.csv1, index_col=0, parse_dates=True)
+        df3 = xlsx.parse('Sheet2', skiprows=[1], index_col=0, parse_dates=True)
+        assert_frame_equal(df, df2)
+        assert_frame_equal(df3, df2)
+
+
 class TestCParser(ParserTests, unittest.TestCase):
 
     def read_csv(self, *args, **kwds):
diff --git a/pandas/src/khash.pxd b/pandas/src/khash.pxd
index 43e231f54..a8fd51a62 100644
--- a/pandas/src/khash.pxd
+++ b/pandas/src/khash.pxd
@@ -1,7 +1,7 @@
 from cpython cimport PyObject
 from numpy cimport int64_t, int32_t, uint32_t, float64_t
 
-cdef extern from "khash.h":
+cdef extern from "khash_python.h":
     ctypedef uint32_t khint_t
     ctypedef khint_t khiter_t
 
@@ -9,7 +9,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         PyObject **keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_pymap_t* kh_init_pymap()
     inline void kh_destroy_pymap(kh_pymap_t*)
@@ -25,7 +25,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         PyObject **keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_pyset_t* kh_init_pyset()
     inline void kh_destroy_pyset(kh_pyset_t*)
@@ -43,7 +43,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         kh_cstr_t *keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_str_t* kh_init_str()
     inline void kh_destroy_str(kh_str_t*)
@@ -60,7 +60,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         int64_t *keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_int64_t* kh_init_int64()
     inline void kh_destroy_int64(kh_int64_t*)
@@ -76,7 +76,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         float64_t *keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_float64_t* kh_init_float64()
     inline void kh_destroy_float64(kh_float64_t*)
@@ -92,7 +92,7 @@ cdef extern from "khash.h":
         khint_t n_buckets, size, n_occupied, upper_bound
         uint32_t *flags
         int32_t *keys
-        Py_ssize_t *vals
+        size_t *vals
 
     inline kh_int32_t* kh_init_int32()
     inline void kh_destroy_int32(kh_int32_t*)
diff --git a/pandas/src/khash.h b/pandas/src/klib/khash.h
similarity index 93%
rename from pandas/src/khash.h
rename to pandas/src/klib/khash.h
index 5822187b6..89a086632 100644
--- a/pandas/src/khash.h
+++ b/pandas/src/klib/khash.h
@@ -112,7 +112,6 @@ int main() {
 #include <stdlib.h>
 #include <string.h>
 #include <limits.h>
-#include <Python.h>
 
 
 #if UINT_MAX == 0xffffffffu
@@ -349,11 +348,6 @@ static const double __ac_HASH_UPPER = 0.77;
  */
 #define kh_int64_hash_equal(a, b) ((a) == (b))
 
-// kludge
-
-#define kh_float64_hash_func _Py_HashDouble
-#define kh_float64_hash_equal kh_int64_hash_equal
-
 /*! @function
   @abstract     const char* hash function
   @param  s     Pointer to a null terminated string
@@ -552,8 +546,6 @@ static PANDAS_INLINE khint_t __ac_Wang_hash(khint_t key)
 #define KHASH_MAP_INIT_INT64(name, khval_t)								\
 	KHASH_INIT(name, khint64_t, khval_t, 1, kh_int64_hash_func, kh_int64_hash_equal)
 
-#define KHASH_MAP_INIT_FLOAT64(name, khval_t)								\
-	KHASH_INIT(name, khfloat64_t, khval_t, 1, kh_float64_hash_func, kh_float64_hash_equal)
 
 typedef const char *kh_cstr_t;
 /*! @function
@@ -572,51 +564,14 @@ typedef const char *kh_cstr_t;
 	KHASH_INIT(name, kh_cstr_t, khval_t, 1, kh_str_hash_func, kh_str_hash_equal)
 
 
-#include <Python.h>
-
-int PANDAS_INLINE pyobject_cmp(PyObject* a, PyObject* b) {
-	int result = PyObject_RichCompareBool(a, b, Py_EQ);
-	if (result < 0) {
-		PyErr_Clear();
-		return 0;
-	}
-	return result;
-}
-
-
-#define kh_python_hash_func(key) (PyObject_Hash(key))
-#define kh_python_hash_equal(a, b) (pyobject_cmp(a, b))
-
-
-// Python object
-
-typedef PyObject* kh_pyobject_t;
-
-#define KHASH_MAP_INIT_PYOBJECT(name, khval_t)							\
-	KHASH_INIT(name, kh_pyobject_t, khval_t, 1,						\
-			   kh_python_hash_func, kh_python_hash_equal)
-
-KHASH_MAP_INIT_PYOBJECT(pymap, Py_ssize_t)
-
-#define KHASH_SET_INIT_PYOBJECT(name)                                  \
-	KHASH_INIT(name, kh_pyobject_t, char, 0,     \
-			   kh_python_hash_func, kh_python_hash_equal)
-
-KHASH_SET_INIT_PYOBJECT(pyset)
-
-#define kh_exist_pymap(h, k) (kh_exist(h, k))
-#define kh_exist_pyset(h, k) (kh_exist(h, k))
 #define kh_exist_str(h, k) (kh_exist(h, k))
 #define kh_exist_float64(h, k) (kh_exist(h, k))
 #define kh_exist_int64(h, k) (kh_exist(h, k))
 #define kh_exist_int32(h, k) (kh_exist(h, k))
 
-KHASH_MAP_INIT_STR(str, Py_ssize_t)
-
-KHASH_MAP_INIT_STR(strbox, kh_pyobject_t)
+KHASH_MAP_INIT_STR(str, size_t)
+KHASH_MAP_INIT_INT(int32, size_t)
+KHASH_MAP_INIT_INT64(int64, size_t)
 
-KHASH_MAP_INIT_INT(int32, Py_ssize_t)
-KHASH_MAP_INIT_INT64(int64, Py_ssize_t)
-KHASH_MAP_INIT_FLOAT64(float64, Py_ssize_t)
 
 #endif /* __AC_KHASH_H */
diff --git a/pandas/src/klib/khash_python.h b/pandas/src/klib/khash_python.h
new file mode 100644
index 000000000..d3ef48de0
--- /dev/null
+++ b/pandas/src/klib/khash_python.h
@@ -0,0 +1,49 @@
+#include <Python.h>
+
+#include "khash.h"
+
+// kludge
+
+#define kh_float64_hash_func _Py_HashDouble
+#define kh_float64_hash_equal kh_int64_hash_equal
+
+#define KHASH_MAP_INIT_FLOAT64(name, khval_t)								\
+	KHASH_INIT(name, khfloat64_t, khval_t, 1, kh_float64_hash_func, kh_float64_hash_equal)
+
+KHASH_MAP_INIT_FLOAT64(float64, size_t)
+
+
+int PANDAS_INLINE pyobject_cmp(PyObject* a, PyObject* b) {
+	int result = PyObject_RichCompareBool(a, b, Py_EQ);
+	if (result < 0) {
+		PyErr_Clear();
+		return 0;
+	}
+	return result;
+}
+
+
+#define kh_python_hash_func(key) (PyObject_Hash(key))
+#define kh_python_hash_equal(a, b) (pyobject_cmp(a, b))
+
+
+// Python object
+
+typedef PyObject* kh_pyobject_t;
+
+#define KHASH_MAP_INIT_PYOBJECT(name, khval_t)							\
+	KHASH_INIT(name, kh_pyobject_t, khval_t, 1,						\
+			   kh_python_hash_func, kh_python_hash_equal)
+
+KHASH_MAP_INIT_PYOBJECT(pymap, Py_ssize_t)
+
+#define KHASH_SET_INIT_PYOBJECT(name)                                  \
+	KHASH_INIT(name, kh_pyobject_t, char, 0,     \
+			   kh_python_hash_func, kh_python_hash_equal)
+
+KHASH_SET_INIT_PYOBJECT(pyset)
+
+#define kh_exist_pymap(h, k) (kh_exist(h, k))
+#define kh_exist_pyset(h, k) (kh_exist(h, k))
+
+KHASH_MAP_INIT_STR(strbox, kh_pyobject_t)
diff --git a/pandas/src/ktypes.h b/pandas/src/klib/ktypes.h
similarity index 100%
rename from pandas/src/ktypes.h
rename to pandas/src/klib/ktypes.h
diff --git a/pandas/src/kvec.h b/pandas/src/klib/kvec.h
similarity index 100%
rename from pandas/src/kvec.h
rename to pandas/src/klib/kvec.h
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index ea402c15e..ededfddb3 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -127,7 +127,7 @@ cdef extern from "parser/parser.h":
 
         int header # Boolean: 1: has header, 0: no header
 
-        int skiprows
+        void *skipset
         int skip_footer
 
         table_chunk *chunks
@@ -151,6 +151,7 @@ cdef extern from "parser/parser.h":
 
     int parser_init(parser_t *self) nogil
     void parser_free(parser_t *self) nogil
+    int parser_add_skiprow(parser_t *self, int64_t row)
 
     void parser_set_default_options(parser_t *self)
 
@@ -173,6 +174,14 @@ cdef extern from "parser/parser.h":
 
 DEFAULT_CHUNKSIZE = 1024 * 1024
 
+# common NA values
+# no longer excluding inf representations
+# '1.#INF','-1.#INF', '1.#INF000000',
+_NA_VALUES = set(['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN',
+                 '#N/A N/A', 'NA', '#NA', 'NULL', 'NaN',
+                 'nan', ''])
+
+
 cdef class TextReader:
     '''
 
@@ -185,22 +194,29 @@ cdef class TextReader:
         object file_handle, should_close
         bint factorize, na_filter, verbose
         int parser_start
-        float64_t clk
+        list clocks
 
     cdef public:
+        int leading_cols, table_width
         object delimiter, na_values, converters, delim_whitespace
         object memory_map
         object as_recarray
-        object header
+        object header, names
         object low_memory
+        object skiprows
 
     def __cinit__(self, source,
                   delimiter=b',',
+
                   header=0,
+                  names=None,
+
                   memory_map=False,
                   tokenize_chunksize=DEFAULT_CHUNKSIZE,
                   delim_whitespace=False,
+
                   converters=None,
+
                   factorize=True,
                   as_recarray=False,
 
@@ -228,6 +244,9 @@ cdef class TextReader:
         self.parser = parser_new()
         self.parser.chunksize = tokenize_chunksize
 
+        # For timekeeping
+        self.clocks = []
+
         self._setup_parser_source(source)
         parser_set_default_options(self.parser)
 
@@ -242,15 +261,8 @@ cdef class TextReader:
 
         self.factorize = factorize
 
-        self.header = None
-        # TODO: no header vs. header is not the first row
-        if header is None:
-            # sentinel value
-            self.parser.header = -1
-            self.parser_start = 0
-        else:
-            self.parser.header = header
-            self.parser_start = header + 1
+        #----------------------------------------
+        # parser options
 
         self.parser.skipinitialspace = skipinitialspace
 
@@ -268,10 +280,17 @@ cdef class TextReader:
                 raise ValueError('Only length-1 escapes  supported')
             self.parser.escapechar = ord(escapechar)
 
+        self.parser.quotechar = ord(quotechar)
+        self.parser.quoting = quoting
+
         # error handling of bad lines
         self.parser.error_bad_lines = int(error_bad_lines)
         self.parser.warn_bad_lines = int(warn_bad_lines)
 
+        self.skiprows = skiprows
+        if skiprows is not None:
+            self._make_skiprow_set()
+
         self.should_close = False
 
         self.delimiter = delimiter
@@ -283,11 +302,31 @@ cdef class TextReader:
 
         self.na_filter = na_filter
         self.as_recarray = as_recarray
-        self.header = None
 
         self.verbose = verbose
         self.low_memory = low_memory
 
+        #----------------------------------------
+        # header stuff
+
+        self.names = names
+        self.leading_cols = 0
+        if names is not None:
+            header = None
+
+        # TODO: no header vs. header is not the first row
+        if header is None:
+            # sentinel value
+            self.parser.header = -1
+            self.parser_start = 0
+        else:
+            self.parser.header = header
+            self.parser_start = header + 1
+
+        self.header, self.table_width = self._get_header()
+
+        # print 'Header: %s, width: %d' % (str(self.header), self.table_width)
+
     def __init__(self, *args, **kwards):
         pass
 
@@ -298,6 +337,13 @@ cdef class TextReader:
         if self.should_close:
             self.file_handle.close()
 
+    cdef _make_skiprow_set(self):
+        if isinstance(self.skiprows, (int, np.integer)):
+            self.skiprows = range(self.skiprows)
+
+        for i in self.skiprows:
+            parser_add_skiprow(self.parser, i)
+
     cdef _setup_parser_source(self, source):
         cdef:
             int status
@@ -335,38 +381,66 @@ cdef class TextReader:
                 raise Exception('Initializing parser from file-like '
                                 'object failed')
 
-    def get_header(self):
+    cdef _get_header(self):
+        cdef:
+            size_t i, start, data_line, field_count, passed_count
+            char *word
+
         if self.parser.header >= 0:
-            self.header = self._parse_table_header()
+            # Header is in the file
 
-        return self.header
+            if self.parser.lines < self.parser.header + 1:
+                # print 'tokenizing %d rows' % (self.parser.header + 2)
 
-    cdef _parse_table_header(self):
-        cdef:
-            size_t i, start, fields
-            char *word
-            # ndarray[object] header
+                tokenize_nrows(self.parser, self.parser.header + 2)
+
+            # e.g., if header=3 and file only has 2 lines
+            if self.parser.lines < self.parser.header + 1:
+                raise CParserError('Passed header=%d but only %d lines in file'
+                                   % (self.parser.header, self.parser.lines))
+
+            field_count = self.parser.line_fields[self.parser.header]
+            start = self.parser.line_start[self.parser.header]
 
-        if self.parser.lines < self.parser.header + 1:
-            tokenize_nrows(self.parser, self.parser.header + 1)
+            # TODO: Py3 vs. Py2
+            header = []
+            for i in range(field_count):
+                word = self.parser.words[start + i]
+                header.append(PyString_FromString(word))
 
-        # e.g., if header=3 and file only has 2 lines
-        if self.parser.lines < self.parser.header + 1:
-            raise CParserError('Passed header=%d but only %d lines in file'
-                               % (self.parser.header, self.parser.lines))
+            data_line = 1
 
-        fields = self.parser.line_fields[self.parser.header]
-        start = self.parser.line_start[self.parser.header]
+        elif self.names is not None:
+            # Names passed
+            if self.parser.lines < 1:
+                tokenize_nrows(self.parser, 1)
+
+            header = self.names
+            data_line = 0
+        else:
+            # No header passed nor to be found in the file
+            if self.parser.lines < 1:
+                tokenize_nrows(self.parser, 1)
+
+            return None, self.parser.line_fields[0]
+
+        # Corner case, not enough lines in the file
+        if self.parser.lines < data_line + 1:
+            return None, len(header)
+        else:
+            field_count = self.parser.line_fields[data_line]
+            passed_count = len(header)
 
-        # TODO: Py3 vs. Py2
+            if passed_count > field_count:
+                raise CParserError('Column names have %d fields, data has %d'
+                                   ' fields' % (passed_count, field_count))
 
-        # np.empty(fields, dtype=np.object_)
-        header = []
-        for i in range(fields):
-            word = self.parser.words[start + i]
-            header.append(PyString_FromString(word))
+            self.leading_cols = field_count - passed_count
 
-        return header
+        return header, field_count
+
+    cdef _implicit_index_count(self):
+        pass
 
     def read(self, rows=None):
         """
@@ -377,87 +451,97 @@ cdef class TextReader:
 
         if self.low_memory:
             # Conserve intermediate space
-            names, columns = self._read_low_memory(rows)
+            columns = self._read_low_memory(rows)
         else:
             # Don't care about memory usage
-            names, columns = self._read_high_memory(rows)
+            columns = self._read_high_memory(rows)
 
         if self.as_recarray:
-            # start = time.clock()
-            result = _to_structured_array(columns, names)
-            # end = time.clock()
-            # print 'to_structured_array took %.4f sec' % (end - start)
+            self._start_clock()
+            result = _to_structured_array(columns, self.names)
+            self._end_clock('Conversion to structured array')
 
             return result
         else:
-            return names, columns
+            return columns
 
     cdef _read_low_memory(self, rows):
         pass
 
     cdef _read_high_memory(self, rows):
-        # start = time.clock()
+        cdef int irows
 
         self._start_clock()
+
         if rows is not None:
-            raise NotImplementedError
+            irows = rows
+            with nogil:
+                status = tokenize_nrows(self.parser, irows)
         else:
             with nogil:
                 status = tokenize_all_rows(self.parser)
+
+        if self.parser_start == self.parser.lines:
+            raise StopIteration
+
         self._end_clock('Tokenization')
 
         if status < 0:
             raise_parser_error('Error tokenizing data', self.parser)
 
         self._start_clock()
-        columns, names = self._convert_column_data()
-        self._end_clock('Type conversion')
 
-        header = self.get_header()
-        if header is not None:
-            names = header
+        columns = self._convert_column_data(rows=rows,
+                                            upcast_na=not self.as_recarray)
+
+        self._end_clock('Type conversion')
 
         # debug_print_parser(self.parser)
 
-        return names, columns
+        return columns
 
     cdef _start_clock(self):
-        self.clk = time.clock()
+        self.clocks.append(time.time())
 
     cdef _end_clock(self, what):
         if self.verbose:
-            print '%s took: %.4fms' % (time.clock() - self.clk) * 1000
+            elapsed = time.time() - self.clocks.pop(-1)
+            print '%s took: %.2f ms' % (what, elapsed * 1000)
 
-    def _convert_column_data(self):
+    def _convert_column_data(self, rows=None, upcast_na=False):
         cdef:
             Py_ssize_t i, ncols
             cast_func func
-            kh_str_t *table
+            kh_str_t *na_hashset
             int start, end
 
-        na_values = ['NA', 'nan', 'NaN']
-        table = kset_from_list(na_values)
-
         start = self.parser_start
-        end = self.parser.lines
 
-        ncols = self.parser.line_fields[start]
-
-        names = []
+        if rows is None:
+            end = self.parser.lines
+        else:
+            end = min(start + rows, self.parser.lines)
 
         results = {}
-        for i in range(ncols):
+        for i in range(self.table_width):
+            name = self._get_column_name(i)
+            conv = self._get_converter(i, name)
+
             # XXX
             if self.na_filter:
-                na_mask = _get_na_mask(self.parser, i, start, end, table)
+                na_list = self._get_na_list(i, name)
+                if na_list is None:
+                    na_mask = None # np.zeros(end - start, dtype=np.uint8)
+                else:
+                    na_hashset = kset_from_list(na_list)
+                    na_mask = _get_na_mask(self.parser, i, start,
+                                           end, na_hashset)
+                    self._free_na_set(na_hashset)
             else:
                 na_mask = None
 
-            conv = self._get_converter(i)
-
             if conv:
-                col_res = _apply_converter(conv, self.parser, i, start, end)
-                results[i] = lib.maybe_convert_numeric(col_res)
+                results[i] = _apply_converter(conv, self.parser, i, start, end)
                 continue
 
             col_res = None
@@ -468,22 +552,56 @@ cdef class TextReader:
                     results[i] = col_res
                     break
 
+            if upcast_na and na_count > 0:
+                col_res = _maybe_upcast(col_res, na_mask)
+
             if col_res is None:
                 raise Exception('Unable to parse column %d' % i)
 
-            names.append(i)
             results[i] = col_res
 
-        # XXX: needs to be done elsewhere
-        kh_destroy_str(table)
+        self.parser_start += end - start
 
-        return results, names
+        return results
 
-    def _get_converter(self, col):
+    def _get_converter(self, i, name):
         if self.converters is None:
             return None
 
-        return self.converters.get(col)
+        if name is not None and name in self.converters:
+            return self.converters[name]
+
+        # Converter for position, if any
+        return self.converters.get(i)
+
+    cdef _get_na_list(self, i, name):
+        if self.na_values is None:
+            return None
+
+        if isinstance(self.na_values, dict):
+            values = None
+            if name is not None and name in self.na_values:
+                values = self.na_values[name]
+                if values is not None and not isinstance(values, list):
+                    values = list(values)
+            else:
+                values = self.na_values.get(i)
+
+            return values
+        else:
+            if not isinstance(self.na_values, list):
+                self.na_values = list(self.na_values)
+
+            return self.na_values
+
+    cdef _free_na_set(self, kh_str_t *table):
+        kh_destroy_str(table)
+
+    cdef _get_column_name(self, i):
+        if self.header is not None:
+            return self.header[i - self.leading_cols]
+        else:
+            return None
 
     def _get_col_name(self, col):
         pass
@@ -492,6 +610,17 @@ class CParserError(Exception):
     pass
 
 
+def _maybe_upcast(arr, mask_is_na):
+    """
+
+    """
+    mask_is_na = mask_is_na.view(np.bool_)
+    if issubclass(arr.dtype.type, np.integer):
+        arr = arr.astype(float)
+        np.putmask(arr, mask_is_na, np.nan)
+
+    return arr
+
 # ----------------------------------------------------------------------
 # Type conversions / inference support code
 
@@ -520,7 +649,10 @@ cdef _string_box_factorize(parser_t *parser, int col,
         object NA = na_values[np.object_]
 
     if na_filter:
-        na_mask = _na_mask
+        if _na_mask is None:
+            na_filter = 0
+        else:
+            na_mask = _na_mask
 
     table = kh_init_strbox()
 
@@ -571,7 +703,10 @@ cdef _try_double(parser_t *parser, int col, int line_start, int line_end,
         ndarray[uint8_t, cast=True] na_mask
 
     if na_filter:
-        na_mask = _na_mask
+        if _na_mask is None:
+            na_filter = 0
+        else:
+            na_mask = _na_mask
 
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.float64)
@@ -613,7 +748,10 @@ cdef _try_int64(parser_t *parser, int col, int line_start, int line_end,
         int64_t NA = na_values[np.int64]
 
     if na_filter:
-        na_mask = _na_mask
+        if _na_mask is None:
+            na_filter = 0
+        else:
+            na_mask = _na_mask
 
     lines = line_end - line_start
 
@@ -660,7 +798,10 @@ cdef _try_bool(parser_t *parser, int col, int line_start, int line_end,
         uint8_t NA = na_values[np.bool_]
 
     if na_filter:
-        na_mask = _na_mask
+        if _na_mask is None:
+            na_filter = 0
+        else:
+            na_mask = _na_mask
 
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.uint8)
@@ -695,7 +836,7 @@ cdef _try_bool(parser_t *parser, int col, int line_start, int line_end,
         return result.view(np.bool_), na_count
 
 cdef _get_na_mask(parser_t *parser, int col, int line_start, int line_end,
-                  kh_str_t *na_table):
+                  kh_str_t *na_hashset):
     cdef:
         int error
         Py_ssize_t i
@@ -713,20 +854,20 @@ cdef _get_na_mask(parser_t *parser, int col, int line_start, int line_end,
         word = COLITER_NEXT(it)
 
         # length 0
-        if word[0] == '\x00':
-            result[i] = 1
-            continue
+        # if word[0] == '\x00':
+        #     result[i] = 1
+        #     continue
 
-        k = kh_get_str(na_table, word)
+        k = kh_get_str(na_hashset, word)
         # in the hash table
-        if k != na_table.n_buckets:
+        if k != na_hashset.n_buckets:
             result[i] = 1
         else:
             result[i] = 0
 
     return result
 
-cdef kh_str_t* kset_from_list(list values):
+cdef kh_str_t* kset_from_list(list values) except NULL:
     # caller takes responsibility for freeing the hash table
     cdef:
         Py_ssize_t i
@@ -740,6 +881,8 @@ cdef kh_str_t* kset_from_list(list values):
 
     for i in range(len(values)):
         val = values[i]
+
+        # None creeps in sometimes, which isn't possible here
         if not PyString_Check(val):
             raise TypeError('must be string, was %s' % type(val))
 
@@ -797,7 +940,18 @@ cdef _apply_converter(object f, parser_t *parser, int col,
         val = PyString_FromString(word)
         result[i] = f(val)
 
-    return lib.maybe_convert_objects(result)
+    values = lib.maybe_convert_objects(result)
+
+    if issubclass(values.dtype.type, (np.number, np.bool_)):
+        return values
+
+    # XXX
+    na_values = set([''])
+    try:
+        return lib.maybe_convert_numeric(values, na_values, False)
+    except Exception:
+        na_count = lib.sanitize_objects(values, na_values, False)
+        return result
 
 def _to_structured_array(dict columns, object names):
     cdef:
@@ -820,7 +974,7 @@ def _to_structured_array(dict columns, object names):
     length = len(columns.values()[0])
     stride = dt.itemsize
 
-    # start = time.clock()
+    # start = time.time()
 
     # we own the data
     buf = <char*> malloc(length * stride)
@@ -829,7 +983,7 @@ def _to_structured_array(dict columns, object names):
     assert(recs.flags.owndata)
 
     # buf = <char*> recs.data
-    # end = time.clock()
+    # end = time.time()
     # print 'took %.4f' % (end - start)
 
     for i in range(nfields):
@@ -865,3 +1019,4 @@ cdef _fill_structured_column(char *dst, char* src, int elsize,
             memcpy(dst, src, elsize)
             dst += stride
             src += elsize
+
diff --git a/pandas/src/parser/parser.c b/pandas/src/parser/parser.c
index a20797d71..d9a37b361 100644
--- a/pandas/src/parser/parser.c
+++ b/pandas/src/parser/parser.c
@@ -655,7 +655,7 @@ void parser_set_default_options(parser_t *self) {
     self->commentchar = '#';
     self->thousands = '\0';
 
-    self->skiprows = 0;
+    self->skipset = NULL;
     self->skip_footer = 0;
 }
 
@@ -986,6 +986,7 @@ int inline end_field(parser_t *self) {
 
 int inline end_line(parser_t *self) {
     int fields;
+    khiter_t k;  /* for hash set detection */
     int ex_fields = -1;
 
     fields = self->line_fields[self->lines];
@@ -994,6 +995,23 @@ int inline end_line(parser_t *self) {
         ex_fields = self->line_fields[self->lines - 1];
     }
 
+    if (self->skipset != NULL) {
+        k = kh_get_int64((kh_int64_t*) self->skipset, self->file_lines);
+
+        if (k != ((kh_int64_t*)self->skipset)->n_buckets) {
+            TRACE(("Skipping row %d\n", self->file_lines));
+            // increment file line count
+            self->file_lines++;
+
+            // skip the tokens from this bad line
+            self->line_start[self->lines] += fields;
+
+            // reset field count
+            self->line_fields[self->lines] = 0;
+            return 0;
+        }
+    }
+
     if (!(self->lines <= self->header + 1) && fields != ex_fields) {
         // increment file line count
         self->file_lines++;
@@ -1022,9 +1040,10 @@ int inline end_line(parser_t *self) {
         }
     } else {
         // increment both line counts
-        self->lines++;
         self->file_lines++;
 
+        self->lines++;
+
         // good line, set new start point
         self->line_start[self->lines] = (self->line_start[self->lines - 1] +
                                          fields);
@@ -1068,6 +1087,26 @@ int parser_cleanup(parser_t *self) {
     // XXX where to put this
     free_if_not_null(self->error_msg);
 
+    if (self->skipset != NULL)
+        kh_destroy_int64((kh_int64_t*) self->skipset);
+
+    return 0;
+}
+
+int parser_add_skiprow(parser_t *self, int64_t row) {
+    khiter_t k;
+    kh_int64_t *set;
+    int ret = 0;
+
+    if (self->skipset == NULL) {
+        self->skipset = (void*) kh_init_int64();
+    }
+
+    set = (kh_int64_t*) self->skipset;
+
+    k = kh_put_int64(set, row, &ret);
+    set->keys[k] = row;
+
     return 0;
 }
 
diff --git a/pandas/src/parser/parser.h b/pandas/src/parser/parser.h
index 11ab89f9c..5846fd409 100644
--- a/pandas/src/parser/parser.h
+++ b/pandas/src/parser/parser.h
@@ -9,15 +9,13 @@
 #include <time.h>
 #include <errno.h>
 
-
 #if defined(_MSC_VER)
 #include "ms_stdint.h"
 #else
 #include <stdint.h>
 #endif
 
-// #include "Python.h"
-// #include "structmember.h"
+#include "khash.h"
 
 #define CHUNKSIZE 1024*256
 #define KB 1024
@@ -34,10 +32,11 @@
 #define FALSE 0
 #define TRUE  1
 
-// Maximum number of columns in a file.
+/* Maximum number of columns in a file. */
 #define MAX_NUM_COLUMNS    2000
 
-// Maximum number of characters in single field.
+/* Maximum number of characters in single field. */
+
 #define FIELD_BUFFER_SIZE  2000
 
 
@@ -54,7 +53,7 @@
 #define ERROR_NO_DATA                  23
 
 
-// #define VERBOSE
+/* #define VERBOSE */
 
 #if defined(VERBOSE)
 #define TRACE(X) printf X;
@@ -162,7 +161,7 @@ typedef struct parser_t {
 
     int header; // Boolean: 1: has header, 0: no header
 
-    int skiprows;
+    void *skipset;
     int skip_footer;
 
     table_chunk *chunks;
@@ -207,6 +206,8 @@ int parser_gzip_source_init(parser_t *self, FILE *fp);
 
 int parser_consume_rows(parser_t *self, size_t nrows);
 
+int parser_add_skiprow(parser_t *self, int64_t row);
+
 void parser_free(parser_t *self);
 
 void parser_set_default_options(parser_t *self);
diff --git a/setup.py b/setup.py
index e968386a7..822b0fa7a 100755
--- a/setup.py
+++ b/setup.py
@@ -235,7 +235,10 @@ class CleanCommand(Command):
         self._clean_trees = []
         self._clean_exclude = ['np_datetime.c',
                                'np_datetime_strings.c',
-                               'period.c']
+                               'period.c',
+                               'parser.c',
+                               'conversions.c',
+                               'str_to.c']
 
         for root, dirs, files in list(os.walk('pandas')):
             for f in files:
@@ -383,7 +386,8 @@ lib_ext = Extension('pandas.lib',
                     sources=[srcpath('tseries', suffix=suffix),
                              'pandas/src/datetime/np_datetime.c',
                              'pandas/src/datetime/np_datetime_strings.c'],
-                    include_dirs=[np.get_include()],
+                    include_dirs=[np.get_include(),
+                                  'pandas/src/klib'],
                     # pyrex_gdb=True,
                     # extra_compile_args=['-Wconversion']
                     )
@@ -404,7 +408,8 @@ parser_ext = Extension('pandas._parser',
                                 'pandas/src/parser/conversions.c',
                                 'pandas/src/parser/str_to.c',
                                 ],
-                       include_dirs=[np.get_include()])
+                       include_dirs=[np.get_include(),
+                                     'pandas/src/klib'])
 
 sparse_ext = Extension('pandas._sparse',
                        sources=[srcpath('sparse', suffix=suffix)],
@@ -412,7 +417,8 @@ sparse_ext = Extension('pandas._sparse',
 
 sandbox_ext = Extension('pandas._sandbox',
                         sources=[srcpath('sandbox', suffix=suffix)],
-                        include_dirs=[np.get_include()])
+                        include_dirs=[np.get_include(),
+                                      'pandas/src/klib'])
 
 cppsandbox_ext = Extension('pandas._cppsandbox',
                            language='c++',
@@ -420,7 +426,7 @@ cppsandbox_ext = Extension('pandas._cppsandbox',
                            include_dirs=[np.get_include()])
 
 extensions = [algos_ext,
-              # lib_ext,
+              lib_ext,
               period_ext,
               sparse_ext,
               parser_ext]
