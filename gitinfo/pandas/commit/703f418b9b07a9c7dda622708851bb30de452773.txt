commit 703f418b9b07a9c7dda622708851bb30de452773
Author: Chris Whelan <topherwhelan@gmail.com>
Date:   Sun Jul 26 19:20:49 2015 -0700

    Fixes for vb_suite

diff --git a/vb_suite/binary_ops.py b/vb_suite/binary_ops.py
index db9a6b730..cd8d1ad93 100644
--- a/vb_suite/binary_ops.py
+++ b/vb_suite/binary_ops.py
@@ -88,7 +88,7 @@ frame_float_floor_by_zero = \
     Benchmark("df // 0", setup, name='frame_float_floor_by_zero')
 
 setup = common_setup + """
-df  = DataFrame(np.random.random_integers((1000, 1000)))
+df  = DataFrame(np.random.random_integers(np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(1000, 1000)))
 """
 frame_int_div_by_zero = \
     Benchmark("df / 0", setup, name='frame_int_div_by_zero')
@@ -111,8 +111,8 @@ frame_float_mod = \
     Benchmark("df / df2", setup, name='frame_float_mod')
 
 setup = common_setup + """
-df  = DataFrame(np.random.random_integers((1000, 1000)))
-df2 = DataFrame(np.random.random_integers((1000, 1000)))
+df  = DataFrame(np.random.random_integers(np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(1000, 1000)))
+df2 = DataFrame(np.random.random_integers(np.iinfo(np.int16).min, np.iinfo(np.int16).max, size=(1000, 1000)))
 """
 frame_int_mod = \
     Benchmark("df / df2", setup, name='frame_int_mod')
diff --git a/vb_suite/frame_ctor.py b/vb_suite/frame_ctor.py
index b11dd6c29..8ad63fc55 100644
--- a/vb_suite/frame_ctor.py
+++ b/vb_suite/frame_ctor.py
@@ -50,9 +50,30 @@ frame_ctor_nested_dict_int64 = Benchmark("DataFrame(data)", setup)
 # offset times 1000 can easily go out of Timestamp bounds and raise errors.
 dynamic_benchmarks = {}
 n_steps = [1, 2]
+offset_kwargs = {'WeekOfMonth': {'weekday': 1, 'week': 1},
+                 'LastWeekOfMonth': {'weekday': 1, 'week': 1},
+                 'FY5253': {'startingMonth': 1, 'weekday': 1},
+                 'FY5253Quarter': {'qtr_with_extra_week': 1, 'startingMonth': 1, 'weekday': 1}}
+
+offset_extra_cases = {'FY5253': {'variation': ['nearest', 'last']},
+                      'FY5253Quarter': {'variation': ['nearest', 'last']}}
+
 for offset in offsets.__all__:
     for n in n_steps:
-        setup = common_setup + """
+        kwargs = {}
+        if offset in offset_kwargs:
+            kwargs = offset_kwargs[offset]
+
+        if offset in offset_extra_cases:
+            extras = offset_extra_cases[offset]
+        else:
+            extras = {'': ['']}
+
+        for extra_arg in extras:
+            for extra in extras[extra_arg]:
+                if extra:
+                    kwargs[extra_arg] = extra
+                setup = common_setup + """
 
 def get_period_count(start_date, off):
     ten_offsets_in_days = ((start_date + off * 10) - start_date).days
@@ -69,12 +90,14 @@ def get_index_for_offset(off):
                       periods=min(1000, get_period_count(start_date, off)),
                       freq=off)
 
-idx = get_index_for_offset({}({}))
+idx = get_index_for_offset({}({}, **{}))
 df = DataFrame(np.random.randn(len(idx),10), index=idx)
 d = dict([ (col,df[col]) for col in df.columns ])
-""".format(offset, n)
-        key = 'frame_ctor_dtindex_{}x{}'.format(offset, n)
-        dynamic_benchmarks[key] = Benchmark("DataFrame(d)", setup, name=key)
+""".format(offset, n, kwargs)
+                key = 'frame_ctor_dtindex_{}x{}'.format(offset, n)
+                if extra:
+                    key += '__{}_{}'.format(extra_arg, extra)
+                dynamic_benchmarks[key] = Benchmark("DataFrame(d)", setup, name=key)
 
 # Have to stuff them in globals() so vbench detects them
 globals().update(dynamic_benchmarks)
diff --git a/vb_suite/frame_methods.py b/vb_suite/frame_methods.py
index 1d7c5e0d9..ce5109efe 100644
--- a/vb_suite/frame_methods.py
+++ b/vb_suite/frame_methods.py
@@ -418,8 +418,8 @@ frame_dtypes = Benchmark('df.dtypes', setup,
 #----------------------------------------------------------------------
 # equals
 setup = common_setup + """
-def make_pair(name):
-    df = globals()[name]
+def make_pair(frame):
+    df = frame
     df2 = df.copy()
     df2.ix[-1,-1] = np.nan
     return df, df2
@@ -437,8 +437,8 @@ object_df = DataFrame([['foo']*1000]*1000)
 nonunique_cols = object_df.copy()
 nonunique_cols.columns = ['A']*len(nonunique_cols.columns)
 
-pairs = dict([(name,make_pair(name))
-         for name in ('float_df', 'object_df', 'nonunique_cols')])
+pairs = dict([(name, make_pair(frame))
+         for name, frame in (('float_df', float_df), ('object_df', object_df), ('nonunique_cols', nonunique_cols))])
 """
 frame_float_equal = Benchmark('test_equal("float_df")', setup)
 frame_object_equal = Benchmark('test_equal("object_df")', setup)
diff --git a/vb_suite/gil.py b/vb_suite/gil.py
index 30f41bb3c..d5aec7c3e 100644
--- a/vb_suite/gil.py
+++ b/vb_suite/gil.py
@@ -94,5 +94,5 @@ def take_1d_pg2_float64():
 
 """
 
-nogil_take1d_float64 = Benchmark('take_1d_pg2()_int64', setup, start_date=datetime(2015, 1, 1))
-nogil_take1d_int64 = Benchmark('take_1d_pg2()_float64', setup, start_date=datetime(2015, 1, 1))
+nogil_take1d_float64 = Benchmark('take_1d_pg2_int64()', setup, start_date=datetime(2015, 1, 1))
+nogil_take1d_int64 = Benchmark('take_1d_pg2_float64()', setup, start_date=datetime(2015, 1, 1))
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 73f5f19d6..6795b315f 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -212,7 +212,7 @@ df = DataFrame({'key1': fac1.take(ind1),
 'value3' : np.random.randn(100000)})
 """
 
-stmt = "df.pivot_table(rows='key1', cols=['key2', 'key3'])"
+stmt = "df.pivot_table(index='key1', columns=['key2', 'key3'])"
 groupby_pivot_table = Benchmark(stmt, setup, start_date=datetime(2011, 12, 15))
 
 
@@ -243,13 +243,13 @@ labels = labels.take(np.random.permutation(len(labels)))
 """
 
 groupby_first_float64 = Benchmark('data.groupby(labels).first()', setup,
-                          start_date=datetime(2012, 5, 1))
+                                  start_date=datetime(2012, 5, 1))
 
 groupby_first_float32 = Benchmark('data2.groupby(labels).first()', setup,
                                   start_date=datetime(2013, 1, 1))
 
 groupby_last_float64 = Benchmark('data.groupby(labels).last()', setup,
-                         start_date=datetime(2012, 5, 1))
+                                 start_date=datetime(2012, 5, 1))
 
 groupby_last_float32 = Benchmark('data2.groupby(labels).last()', setup,
                                  start_date=datetime(2013, 1, 1))
@@ -259,7 +259,7 @@ groupby_nth_float64_none = Benchmark('data.groupby(labels).nth(0)', setup,
 groupby_nth_float32_none = Benchmark('data2.groupby(labels).nth(0)', setup,
                                      start_date=datetime(2013, 1, 1))
 groupby_nth_float64_any = Benchmark('data.groupby(labels).nth(0,dropna="all")', setup,
-                                     start_date=datetime(2012, 5, 1))
+                                    start_date=datetime(2012, 5, 1))
 groupby_nth_float32_any = Benchmark('data2.groupby(labels).nth(0,dropna="all")', setup,
                                     start_date=datetime(2013, 1, 1))
 
@@ -269,9 +269,9 @@ df = DataFrame({'a' : date_range('1/1/2011',periods=100000,freq='s'),'b' : range
 """
 
 groupby_first_datetimes = Benchmark('df.groupby("b").first()', setup,
-                                 start_date=datetime(2013, 5, 1))
+                                    start_date=datetime(2013, 5, 1))
 groupby_last_datetimes = Benchmark('df.groupby("b").last()', setup,
-                                 start_date=datetime(2013, 5, 1))
+                                   start_date=datetime(2013, 5, 1))
 groupby_nth_datetimes_none = Benchmark('df.groupby("b").nth(0)', setup,
                                        start_date=datetime(2013, 5, 1))
 groupby_nth_datetimes_any = Benchmark('df.groupby("b").nth(0,dropna="all")', setup,
diff --git a/vb_suite/io_bench.py b/vb_suite/io_bench.py
index a70c543ca..483d61387 100644
--- a/vb_suite/io_bench.py
+++ b/vb_suite/io_bench.py
@@ -2,6 +2,7 @@ from vbench.api import Benchmark
 from datetime import datetime
 
 common_setup = """from pandas_vb_common import *
+from StringIO import StringIO
 """
 
 #----------------------------------------------------------------------
diff --git a/vb_suite/join_merge.py b/vb_suite/join_merge.py
index 02132acb7..244c6abe7 100644
--- a/vb_suite/join_merge.py
+++ b/vb_suite/join_merge.py
@@ -31,15 +31,15 @@ try:
 except:
     pass
 
-df = DataFrame({'data1' : np.random.randn(100000),
+df = pd.DataFrame({'data1' : np.random.randn(100000),
                 'data2' : np.random.randn(100000),
                 'key1' : key1,
                 'key2' : key2})
 
 
-df_key1 = DataFrame(np.random.randn(len(level1), 4), index=level1,
+df_key1 = pd.DataFrame(np.random.randn(len(level1), 4), index=level1,
                     columns=['A', 'B', 'C', 'D'])
-df_key2 = DataFrame(np.random.randn(len(level2), 4), index=level2,
+df_key2 = pd.DataFrame(np.random.randn(len(level2), 4), index=level2,
                     columns=['A', 'B', 'C', 'D'])
 
 df_shuf = df.reindex(df.index[shuf])
@@ -69,10 +69,10 @@ join_dataframe_index_multi = \
 #----------------------------------------------------------------------
 # Joins on integer keys
 setup = common_setup + """
-df = DataFrame({'key1': np.tile(np.arange(500).repeat(10), 2),
+df = pd.DataFrame({'key1': np.tile(np.arange(500).repeat(10), 2),
                 'key2': np.tile(np.arange(250).repeat(10), 4),
                 'value': np.random.randn(10000)})
-df2 = DataFrame({'key1': np.arange(500), 'value2': randn(500)})
+df2 = pd.DataFrame({'key1': np.arange(500), 'value2': randn(500)})
 df3 = df[:5000]
 """
 
@@ -96,9 +96,9 @@ indices2 = tm.makeStringIndex(N).values
 key = np.tile(indices[:8000], 10)
 key2 = np.tile(indices2[:8000], 10)
 
-left = DataFrame({'key' : key, 'key2':key2,
+left = pd.DataFrame({'key' : key, 'key2':key2,
                   'value' : np.random.randn(80000)})
-right = DataFrame({'key': indices[2000:], 'key2':indices2[2000:],
+right = pd.DataFrame({'key': indices[2000:], 'key2':indices2[2000:],
                    'value2' : np.random.randn(8000)})
 """
 
@@ -112,7 +112,7 @@ merge_2intkey_sort = Benchmark('merge(left, right, sort=True)', setup,
 # Appending DataFrames
 
 setup = common_setup + """
-df1 = DataFrame(np.random.randn(10000, 4), columns=['A', 'B', 'C', 'D'])
+df1 = pd.DataFrame(np.random.randn(10000, 4), columns=['A', 'B', 'C', 'D'])
 df2 = df1.copy()
 df2.index = np.arange(10000, 20000)
 mdf1 = df1.copy()
@@ -180,7 +180,7 @@ concat_series_axis1 = Benchmark('concat(pieces, axis=1)', setup,
                                 start_date=datetime(2012, 2, 27))
 
 setup = common_setup + """
-df = DataFrame(randn(5, 4))
+df = pd.DataFrame(randn(5, 4))
 """
 
 concat_small_frames = Benchmark('concat([df] * 1000)', setup,
@@ -191,8 +191,8 @@ concat_small_frames = Benchmark('concat([df] * 1000)', setup,
 # Concat empty
 
 setup = common_setup + """
-df = DataFrame(dict(A = range(10000)),index=date_range('20130101',periods=10000,freq='s'))
-empty = DataFrame()
+df = pd.DataFrame(dict(A = range(10000)),index=date_range('20130101',periods=10000,freq='s'))
+empty = pd.DataFrame()
 """
 
 concat_empty_frames1 = Benchmark('concat([df,empty])', setup,
@@ -207,11 +207,11 @@ concat_empty_frames2 = Benchmark('concat([empty,df])', setup,
 setup = common_setup + """
 groups = tm.makeStringIndex(10).values
 
-left = DataFrame({'group': groups.repeat(5000),
+left = pd.DataFrame({'group': groups.repeat(5000),
                   'key' : np.tile(np.arange(0, 10000, 2), 10),
                   'lvalue': np.random.randn(50000)})
 
-right = DataFrame({'key' : np.arange(10000),
+right = pd.DataFrame({'key' : np.arange(10000),
                    'rvalue' : np.random.randn(10000)})
 
 """
@@ -242,10 +242,10 @@ setup = common_setup + '''
 np.random.seed(2718281)
 n = 50000
 
-left = DataFrame(np.random.randint(1, n/500, (n, 2)),
+left = pd.DataFrame(np.random.randint(1, n/500, (n, 2)),
         columns=['jim', 'joe'])
 
-right = DataFrame(np.random.randint(1, n/500, (n, 2)),
+right = pd.DataFrame(np.random.randint(1, n/500, (n, 2)),
         columns=['jolie', 'jolia']).set_index('jolie')
 '''
 
@@ -255,7 +255,7 @@ left_outer_join_index = Benchmark("left.join(right, on='jim')", setup,
 
 setup = common_setup + """
 low, high, n = -1 << 10, 1 << 10, 1 << 20
-left = DataFrame(np.random.randint(low, high, (n, 7)),
+left = pd.DataFrame(np.random.randint(low, high, (n, 7)),
                     columns=list('ABCDEFG'))
 left['left'] = left.sum(axis=1)
 
diff --git a/vb_suite/packers.py b/vb_suite/packers.py
index 62e0e8fc3..60738a62b 100644
--- a/vb_suite/packers.py
+++ b/vb_suite/packers.py
@@ -92,7 +92,7 @@ packers_write_hdf_store = Benchmark("df2.to_hdf(f,'df')", setup, cleanup="remove
 # hdf table
 
 setup = common_setup + """
-df2.to_hdf(f,'df',table=True)
+df2.to_hdf(f,'df',format='table')
 """
 
 packers_read_hdf_table = Benchmark("pd.read_hdf(f,'df')", setup, start_date=start_date)
diff --git a/vb_suite/pandas_vb_common.py b/vb_suite/pandas_vb_common.py
index a599301bb..128e262d4 100644
--- a/vb_suite/pandas_vb_common.py
+++ b/vb_suite/pandas_vb_common.py
@@ -1,4 +1,5 @@
 from pandas import *
+import pandas as pd
 from datetime import timedelta
 from numpy.random import randn
 from numpy.random import randint
@@ -7,6 +8,7 @@ import pandas.util.testing as tm
 import random
 import numpy as np
 
+np.random.seed(1234)
 try:
     import pandas._tseries as lib
 except:
diff --git a/vb_suite/reindex.py b/vb_suite/reindex.py
index 156382f1f..07f0e0f7e 100644
--- a/vb_suite/reindex.py
+++ b/vb_suite/reindex.py
@@ -49,6 +49,18 @@ reindex_multi = Benchmark(statement, setup,
 #----------------------------------------------------------------------
 # Pad / backfill
 
+def pad(source_series, target_index):
+    try:
+        source_series.reindex(target_index, method='pad')
+    except:
+        source_series.reindex(target_index, fillMethod='pad')
+
+def backfill(source_series, target_index):
+    try:
+        source_series.reindex(target_index, method='backfill')
+    except:
+        source_series.reindex(target_index, fillMethod='backfill')
+
 setup = common_setup + """
 rng = date_range('1/1/2000', periods=100000, freq=datetools.Minute())
 
@@ -57,23 +69,23 @@ ts2 = ts[::2]
 ts3 = ts2.reindex(ts.index)
 ts4 = ts3.astype('float32')
 
-def pad():
+def pad(source_series, target_index):
     try:
-        ts2.reindex(ts.index, method='pad')
+        source_series.reindex(target_index, method='pad')
     except:
-        ts2.reindex(ts.index, fillMethod='pad')
-def backfill():
+        source_series.reindex(target_index, fillMethod='pad')
+def backfill(source_series, target_index):
     try:
-        ts2.reindex(ts.index, method='backfill')
+        source_series.reindex(target_index, method='backfill')
     except:
-        ts2.reindex(ts.index, fillMethod='backfill')
+        source_series.reindex(target_index, fillMethod='backfill')
 """
 
-statement = "pad()"
+statement = "pad(ts2, ts.index)"
 reindex_daterange_pad = Benchmark(statement, setup,
                                   name="reindex_daterange_pad")
 
-statement = "backfill()"
+statement = "backfill(ts2, ts.index)"
 reindex_daterange_backfill = Benchmark(statement, setup,
                                        name="reindex_daterange_backfill")
 
diff --git a/vb_suite/sparse.py b/vb_suite/sparse.py
index e591b197d..5da06451f 100644
--- a/vb_suite/sparse.py
+++ b/vb_suite/sparse.py
@@ -40,7 +40,7 @@ sparse_constructor = Benchmark(stmt, setup, name="sparse_frame_constructor",
 
 
 setup = common_setup + """
-s = pd.Series([nan] * 10000)
+s = pd.Series([np.nan] * 10000)
 s[0] = 3.0
 s[100] = -1.0
 s[999] = 12.1
@@ -59,7 +59,7 @@ import pandas.sparse.series
 A = scipy.sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(100, 100))
 """
 
-stmt = "ss = pandas.sparse.series.from_coo(A)"
+stmt = "ss = pandas.sparse.series.SparseSeries.from_coo(A)"
 
 sparse_series_from_coo = Benchmark(stmt, setup, name="sparse_series_from_coo",
                                start_date=datetime(2015, 1, 3))
diff --git a/vb_suite/timeseries.py b/vb_suite/timeseries.py
index 75147e079..7f5433980 100644
--- a/vb_suite/timeseries.py
+++ b/vb_suite/timeseries.py
@@ -1,16 +1,21 @@
 from vbench.api import Benchmark
 from datetime import datetime
+from pandas import *
 
-common_setup = """from pandas_vb_common import *
-from datetime import timedelta
 N = 100000
-
 try:
-    rng = date_range('1/1/2000', periods=N, freq='min')
+    rng = date_range(start='1/1/2000', periods=N, freq='min')
 except NameError:
-    rng = DatetimeIndex('1/1/2000', periods=N, offset=datetools.Minute())
+    rng = DatetimeIndex(start='1/1/2000', periods=N, freq='T')
     def date_range(start=None, end=None, periods=None, freq=None):
-        return DatetimeIndex(start, end, periods=periods, offset=freq)
+        return DatetimeIndex(start=start, end=end, periods=periods, offset=freq)
+
+
+common_setup = """from pandas_vb_common import *
+from datetime import timedelta
+N = 100000
+
+rng = date_range(start='1/1/2000', periods=N, freq='T')
 
 if hasattr(Series, 'convert'):
     Series.resample = Series.convert
@@ -22,7 +27,7 @@ ts = Series(np.random.randn(N), index=rng)
 # Lookup value in large time series, hash map population
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=1500000, freq='s')
+rng = date_range(start='1/1/2000', periods=1500000, freq='S')
 ts = Series(1, index=rng)
 """
 
@@ -69,7 +74,7 @@ timeseries_add_irregular = Benchmark('left + right', setup)
 
 setup = common_setup + """
 N = 100000
-rng = date_range('1/1/2000', periods=N, freq='s')
+rng = date_range(start='1/1/2000', periods=N, freq='s')
 rng = rng.take(np.random.permutation(N))
 ts = Series(np.random.randn(N), index=rng)
 """
@@ -81,7 +86,7 @@ timeseries_sort_index = Benchmark('ts.sort_index()', setup,
 # Shifting, add offset
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=10000, freq='T')
+rng = date_range(start='1/1/2000', periods=10000, freq='T')
 """
 
 datetimeindex_add_offset = Benchmark('rng + timedelta(minutes=2)', setup,
@@ -89,9 +94,9 @@ datetimeindex_add_offset = Benchmark('rng + timedelta(minutes=2)', setup,
 
 setup = common_setup + """
 N = 10000
-rng = date_range('1/1/1990', periods=N, freq='53s')
+rng = date_range(start='1/1/1990', periods=N, freq='53s')
 ts = Series(np.random.randn(N), index=rng)
-dates = date_range('1/1/1990', periods=N * 10, freq='5s')
+dates = date_range(start='1/1/1990', periods=N * 10, freq='5s')
 """
 timeseries_asof_single = Benchmark('ts.asof(dates[0])', setup,
                                    start_date=datetime(2012, 4, 27))
@@ -108,7 +113,7 @@ timeseries_asof_nan = Benchmark('ts.asof(dates)', setup,
 # Time zone stuff
 
 setup = common_setup + """
-rng = date_range('1/1/2000', '3/1/2000', tz='US/Eastern')
+rng = date_range(start='1/1/2000', end='3/1/2000', tz='US/Eastern')
 """
 
 timeseries_timestamp_tzinfo_cons = \
@@ -118,7 +123,7 @@ timeseries_timestamp_tzinfo_cons = \
 # Resampling period
 
 setup = common_setup + """
-rng = period_range('1/1/2000', '1/1/2001', freq='T')
+rng = period_range(start='1/1/2000', end='1/1/2001', freq='T')
 ts = Series(np.random.randn(len(rng)), index=rng)
 """
 
@@ -127,7 +132,7 @@ timeseries_period_downsample_mean = \
               start_date=datetime(2012, 4, 25))
 
 setup = common_setup + """
-rng = date_range('1/1/2000', '1/1/2001', freq='T')
+rng = date_range(start='1/1/2000', end='1/1/2001', freq='T')
 ts = Series(np.random.randn(len(rng)), index=rng)
 """
 
@@ -149,7 +154,7 @@ timeseries_resample_datetime64 = Benchmark("ts.resample('1S', how='last')", setu
 # to_datetime
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=20000, freq='h')
+rng = date_range(start='1/1/2000', periods=20000, freq='H')
 strings = [x.strftime('%Y-%m-%d %H:%M:%S') for x in rng]
 """
 
@@ -162,7 +167,7 @@ timeseries_to_datetime_iso8601_format = \
               start_date=datetime(2012, 7, 11))
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=10000, freq='D')
+rng = date_range(start='1/1/2000', periods=10000, freq='D')
 strings = Series(rng.year*10000+rng.month*100+rng.day,dtype=np.int64).apply(str)
 """
 
@@ -183,7 +188,7 @@ timeseries_with_format_replace = Benchmark("to_datetime(s.str.replace(':\S+$',''
 
 setup = common_setup + """
 from pandas.tseries.frequencies import infer_freq
-rng = date_range('1/1/1700', freq='D', periods=100000)
+rng = date_range(start='1/1/1700', freq='D', periods=100000)
 a = rng[:50000].append(rng[50002:])
 """
 
@@ -193,7 +198,7 @@ timeseries_infer_freq = \
 # setitem PeriodIndex
 
 setup = common_setup + """
-rng = period_range('1/1/1990', freq='S', periods=20000)
+rng = period_range(start='1/1/1990', freq='S', periods=20000)
 df = DataFrame(index=range(len(rng)))
 """
 
@@ -202,7 +207,7 @@ period_setitem = \
               start_date=datetime(2012, 8, 1))
 
 setup = common_setup + """
-rng = date_range('1/1/2000 9:30', periods=10000, freq='S', tz='US/Eastern')
+rng = date_range(start='1/1/2000 9:30', periods=10000, freq='S', tz='US/Eastern')
 """
 
 datetimeindex_normalize = \
@@ -211,7 +216,7 @@ datetimeindex_normalize = \
 
 setup = common_setup + """
 from pandas.tseries.offsets import Second
-s1 = date_range('1/1/2000', periods=100, freq='S')
+s1 = date_range(start='1/1/2000', periods=100, freq='S')
 curr = s1[-1]
 slst = []
 for i in range(100):
@@ -224,7 +229,7 @@ for i in range(100):
 
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=1000, freq='H')
+rng = date_range(start='1/1/2000', periods=1000, freq='H')
 df = DataFrame(np.random.randn(len(rng), 2), rng)
 """
 
@@ -232,7 +237,7 @@ dti_reset_index = \
     Benchmark('df.reset_index()', setup, start_date=datetime(2012, 9, 1))
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=1000, freq='H',
+rng = date_range(start='1/1/2000', periods=1000, freq='H',
                  tz='US/Eastern')
 df = DataFrame(np.random.randn(len(rng), 2), index=rng)
 """
@@ -241,7 +246,7 @@ dti_reset_index_tz = \
     Benchmark('df.reset_index()', setup, start_date=datetime(2012, 9, 1))
 
 setup = common_setup + """
-rng = date_range('1/1/2000', periods=1000, freq='T')
+rng = date_range(start='1/1/2000', periods=1000, freq='T')
 index = rng.repeat(10)
 """
 
@@ -251,13 +256,13 @@ datetimeindex_unique = Benchmark('index.unique()', setup,
 # tz_localize with infer argument.  This is an attempt to emulate the results
 # of read_csv with duplicated data.  Not passing infer_dst will fail
 setup = common_setup + """
-dst_rng = date_range('10/29/2000 1:00:00',
-                     '10/29/2000 1:59:59', freq='S')
-index = date_range('10/29/2000', '10/29/2000 00:59:59', freq='S')
+dst_rng = date_range(start='10/29/2000 1:00:00',
+                     end='10/29/2000 1:59:59', freq='S')
+index = date_range(start='10/29/2000', end='10/29/2000 00:59:59', freq='S')
 index = index.append(dst_rng)
 index = index.append(dst_rng)
-index = index.append(date_range('10/29/2000 2:00:00',
-                                '10/29/2000 3:00:00', freq='S'))
+index = index.append(date_range(start='10/29/2000 2:00:00',
+                                end='10/29/2000 3:00:00', freq='S'))
 """
 
 datetimeindex_infer_dst = \
@@ -269,7 +274,7 @@ Benchmark('index.tz_localize("US/Eastern", infer_dst=True)',
 # Resampling: fast-path various functions
 
 setup = common_setup + """
-rng = date_range('20130101',periods=100000,freq='50L')
+rng = date_range(start='20130101',periods=100000,freq='50L')
 df = DataFrame(np.random.randn(100000,2),index=rng)
 """
 
@@ -376,7 +381,7 @@ timeseries_custom_bmonthbegin_decr_n = \
 
 setup = common_setup + """
 N = 10000
-rng = date_range('1/1/1', periods=N, freq='B')
+rng = date_range(start='1/1/1', periods=N, freq='B')
 """
 
 timeseries_is_month_start = Benchmark('rng.is_month_start', setup,
