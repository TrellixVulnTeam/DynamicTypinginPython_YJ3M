commit 14ffdaf9d821bbcda6ce9981edf1cb26d0c2c913
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Sep 6 13:42:00 2011 -0400

    DOC: time series docs. bit of LongPanel refactoring

diff --git a/doc/source/api.rst b/doc/source/api.rst
index 7e3368736..16c6951b6 100644
--- a/doc/source/api.rst
+++ b/doc/source/api.rst
@@ -118,6 +118,7 @@ Reshaping, sorting
    Series.argsort
    Series.order
    Series.sort
+   Series.sort_index
    Series.sortlevel
    Series.unstack
 
@@ -286,10 +287,11 @@ Reshaping, sorting, transposing
 .. autosummary::
    :toctree: generated/
 
-   DataFrame.sort
+   DataFrame.sort_index
    DataFrame.delevel
    DataFrame.pivot
    DataFrame.sortlevel
+   DataFrame.swaplevel
    DataFrame.stack
    DataFrame.unstack
    DataFrame.T
@@ -337,8 +339,8 @@ Serialization / IO / Conversion
    DataFrame.load
    DataFrame.info
 
-WidePanel
----------
+Panel
+-----
 
 Input / Output
 --------------
@@ -369,10 +371,6 @@ Standard moving window functions
 
 .. autosummary::
    :toctree: generated/
-Apr 4Apr 11Apr 18Apr 25May 2May 9May 16May 23May 30Jun 6Jun 13Jun 20Jun 27Jul 4Jul 11Jul 18Jul 25Aug 1Aug 8Aug 15Aug 22Aug 29Sep 5
-
-JulAugSepOctNovDec2010FebMarAprMayJunJulAugSepOctNovDec2011FebMarAprMayJunJulAugSepOctNovDec
-
 
    rolling_count
    rolling_sum
diff --git a/doc/source/io.rst b/doc/source/io.rst
index b12263d34..358180518 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -2,9 +2,9 @@
 
 .. currentmodule:: pandas
 
-**************************
-IO Tools (Text, HDF5, ...)
-**************************
+*******************************
+IO Tools (Text, CSV, HDF5, ...)
+*******************************
 
 HDF5 (PyTables)
 ---------------
diff --git a/doc/source/stats.rst b/doc/source/stats.rst
index d0cc5d266..cce96a3bd 100755
--- a/doc/source/stats.rst
+++ b/doc/source/stats.rst
@@ -8,8 +8,172 @@ Built-in statistical functionality
 
 .. currentmodule:: pandas
 
-.. toctree::
-   :maxdepth: 2
+.. currentmodule:: pandas.stats.api
+
+.. _stats.moments:
+
+Moving (rolling) statistics / moments
+-------------------------------------
+
+For working with time series data, a number of functions are provided
+for computing common *moving* or *rolling* statistics. Among these are
+count, sum, mean, median, correlation, variance, covariance, standard
+deviation, skewness, and kurtosis. All of these methods are in the
+:mod:`pandas` namespace, but otherwise they can be found in
+:mod:`pandas.stats.moments`.
+
+Each of these methods observes the same interface (with relevant
+methods accepting two Series arguments instead of one):
+
+.. function:: Unary moment functions
+
+   :Parameters:
+       **arg** : ndarray, Series, DataFrame, etc.
+	       If a DataFrame is passed, function will be applied to **columns**
+
+       **window** : int
+           Number of periods to include in window
+
+       **min_periods** : int or None
+	       Number of periods to require to compute a value (defaults to
+	       **window**)
+
+       **time_rule** : string or DateOffset
+            Frequency to pre-convert data to
+
+.. function:: Binary moment functions
+
+   :Parameters:
+       **arg1** : ndarray, Series
+	       If a DataFrame is passed, function will be applied to **columns**
+
+       **arg2** : ndarray, Series
+	       Must be same type as **arg1**
+
+       **window** : int
+           Number of periods to include in window
+
+       **min_periods** : int or None
+	       Number of periods to require to compute a value (defaults to
+	       **window**)
+
+       **time_rule** : string or DateOffset
+            Frequency to pre-convert data to
+
+::
+
+    >>> ts
+    2000-01-31 00:00:00    -0.550139282247
+    2000-02-01 00:00:00    0.0950636484432
+    2000-02-02 00:00:00    0.0621763420914
+    2000-02-03 00:00:00    0.125698607137
+    2000-02-04 00:00:00    0.222288320816
+    2000-02-07 00:00:00    0.903314747152
+    2000-02-08 00:00:00    -0.391449402196
+    2000-02-09 00:00:00    -0.726137553115
+    2000-02-10 00:00:00    -0.89302167539
+    2000-02-11 00:00:00    0.228509179513
+
+    >>> rolling_sum(ts, 5, min_periods=3)
+    2000-01-31 00:00:00    NaN
+    2000-02-01 00:00:00    NaN
+    2000-02-02 00:00:00    -0.0913037710365
+    2000-02-03 00:00:00    0.798752592168
+    2000-02-04 00:00:00    1.39432346651
+    2000-02-07 00:00:00    2.44074916551
+    2000-02-08 00:00:00    2.77458564938
+    2000-02-09 00:00:00    1.87181399193
+    2000-02-10 00:00:00    2.48549563273
+    2000-02-11 00:00:00    1.81285272663
+
+If passing a DataFrame argument, the statistics will be applied independently to
+the columns:
+
+::
+
+    >>> df
+			   A              B              C              D
+    2000-01-31 00:00:00    NaN            NaN            0.03752        -0.3952
+    2000-02-01 00:00:00    NaN            NaN            -1.511         -0.1126
+    2000-02-02 00:00:00    1.136          NaN            0.777          -0.3502
+    2000-02-03 00:00:00    0.8901         NaN            1.196          0.7456
+    2000-02-04 00:00:00    0.5956         0.7684         0.9042         0.4984
+    2000-02-07 00:00:00    -0.3502        1.015          0.5366         0.6628
+    2000-02-08 00:00:00    0.5036         1.825          0.8682         -1.69
+    2000-02-09 00:00:00    0.2327         -0.3899        0.4493         -0.1267
+    2000-02-10 00:00:00    1.504          0.3904         -0.06148       1.717
+    2000-02-11 00:00:00    -0.07707       0.2286         -1.039         0.1438
+
+    >>> rolling_mean(df, 5, min_periods=3)
+			   A              B              C              D
+    2000-01-31 00:00:00    NaN            NaN            NaN            NaN
+    2000-02-01 00:00:00    NaN            NaN            NaN            NaN
+    2000-02-02 00:00:00    NaN            NaN            -0.2321        -0.286
+    2000-02-03 00:00:00    NaN            NaN            0.125          -0.02811
+    2000-02-04 00:00:00    0.8737         NaN            0.2809         0.07718
+    2000-02-07 00:00:00    0.5677         NaN            0.3807         0.2888
+    2000-02-08 00:00:00    0.5549         1.203          0.8565         -0.0267
+    2000-02-09 00:00:00    0.3744         0.8047         0.7909         0.018
+    2000-02-10 00:00:00    0.4971         0.7219         0.5394         0.2123
+    2000-02-11 00:00:00    0.3626         0.6139         0.1507         0.1414
+
+Each of these methods can optionally accept a **time_rule** argument
+(see :ref:`time rules <datetools.timerules>`) which is provided as a
+convenience when the user wishes to guarantee that the window of the
+statistic.
+
+Here are some plots of the unary moment functions:
+
+.. plot:: plots/stats/moments_rolling.py
+
+And the binary moment functions:
+
+.. plot:: plots/stats/moments_rolling_binary.py
+
+Exponentially weighted moment functions
+---------------------------------------
+
+It's also quite common to want to do non-equally weighted moving statistics,
+such as exponentially weighted (EW) moving average or EW moving standard
+deviation. A number of EW functions are provided using the blending method. For
+example, where :math:`y_t` is the result and :math:`x_t` the input, we compute
+an exponentially weighted moving average as
+
+.. math::
+
+    y_t = (1-\alpha) y_{t-1} + \alpha x_t
+
+One must have :math:`0 < \alpha \leq 1`, but rather than pass :math:`\alpha`
+directly, it's easier to think about either the **span** or **center of mass
+(com)** of an EW moment:
+
+.. math::
+
+   \alpha =
+    \begin{cases}
+	\frac{2}{s + 1}, s = \text{span}\\
+	\frac{1}{c + 1}, c = \text{center of mass}
+    \end{cases}
+
+You can pass one or the other to these functions but not both. **Span**
+corresponds to what is commonly called a "20-day EW moving average" for
+example. **Center of mass** has a more physical interpretation. For example,
+**span** = 20 corresponds to **com** = 9.5.
+
+Here are some examples for a univariate time series:
+
+.. plot:: plots/stats/moments_expw.py
+
+The binary `emwcov` and `ewmcorr` are similar to their equal-weighted
+counterparts above.
+
+.. autofunction:: pandas.stats.moments.ewma
+
+
+.. _stats.ols:
+
+Linear and panel regression
+---------------------------
 
-   stats_moments
-   stats_ols
+.. automodule:: pandas.stats.interface
+   :members:
diff --git a/doc/source/stats_moments.rst b/doc/source/stats_moments.rst
deleted file mode 100644
index 5d204fc8d..000000000
--- a/doc/source/stats_moments.rst
+++ /dev/null
@@ -1,160 +0,0 @@
-.. currentmodule:: pandas.stats.api
-
-.. _stats_moments:
-
-Moving (rolling) statistics / moments
--------------------------------------
-
-For working with time series data, a number of functions are provided
-for computing common *moving* or *rolling* statistics. Among these are
-count, sum, mean, median, correlation, variance, covariance, standard
-deviation, skewness, and kurtosis. All of these methods are in the
-:mod:`pandas` namespace, but otherwise they can be found in
-:mod:`pandas.stats.moments`.
-
-Each of these methods observes the same interface (with relevant
-methods accepting two Series arguments instead of one):
-
-.. function:: Unary moment functions
-
-   :Parameters:
-       **arg** : ndarray, Series, DataFrame, etc.
-	       If a DataFrame is passed, function will be applied to **columns**
-
-       **window** : int
-           Number of periods to include in window
-
-       **min_periods** : int or None
-	       Number of periods to require to compute a value (defaults to
-	       **window**)
-
-       **time_rule** : string or DateOffset
-            Frequency to pre-convert data to
-
-.. function:: Binary moment functions
-
-   :Parameters:
-       **arg1** : ndarray, Series
-	       If a DataFrame is passed, function will be applied to **columns**
-
-       **arg2** : ndarray, Series
-	       Must be same type as **arg1**
-
-       **window** : int
-           Number of periods to include in window
-
-       **min_periods** : int or None
-	       Number of periods to require to compute a value (defaults to
-	       **window**)
-
-       **time_rule** : string or DateOffset
-            Frequency to pre-convert data to
-
-::
-
-    >>> ts
-    2000-01-31 00:00:00    -0.550139282247
-    2000-02-01 00:00:00    0.0950636484432
-    2000-02-02 00:00:00    0.0621763420914
-    2000-02-03 00:00:00    0.125698607137
-    2000-02-04 00:00:00    0.222288320816
-    2000-02-07 00:00:00    0.903314747152
-    2000-02-08 00:00:00    -0.391449402196
-    2000-02-09 00:00:00    -0.726137553115
-    2000-02-10 00:00:00    -0.89302167539
-    2000-02-11 00:00:00    0.228509179513
-
-    >>> rolling_sum(ts, 5, min_periods=3)
-    2000-01-31 00:00:00    NaN
-    2000-02-01 00:00:00    NaN
-    2000-02-02 00:00:00    -0.0913037710365
-    2000-02-03 00:00:00    0.798752592168
-    2000-02-04 00:00:00    1.39432346651
-    2000-02-07 00:00:00    2.44074916551
-    2000-02-08 00:00:00    2.77458564938
-    2000-02-09 00:00:00    1.87181399193
-    2000-02-10 00:00:00    2.48549563273
-    2000-02-11 00:00:00    1.81285272663
-
-If passing a DataFrame argument, the statistics will be applied independently to
-the columns:
-
-::
-
-    >>> df
-			   A              B              C              D
-    2000-01-31 00:00:00    NaN            NaN            0.03752        -0.3952
-    2000-02-01 00:00:00    NaN            NaN            -1.511         -0.1126
-    2000-02-02 00:00:00    1.136          NaN            0.777          -0.3502
-    2000-02-03 00:00:00    0.8901         NaN            1.196          0.7456
-    2000-02-04 00:00:00    0.5956         0.7684         0.9042         0.4984
-    2000-02-07 00:00:00    -0.3502        1.015          0.5366         0.6628
-    2000-02-08 00:00:00    0.5036         1.825          0.8682         -1.69
-    2000-02-09 00:00:00    0.2327         -0.3899        0.4493         -0.1267
-    2000-02-10 00:00:00    1.504          0.3904         -0.06148       1.717
-    2000-02-11 00:00:00    -0.07707       0.2286         -1.039         0.1438
-
-    >>> rolling_mean(df, 5, min_periods=3)
-			   A              B              C              D
-    2000-01-31 00:00:00    NaN            NaN            NaN            NaN
-    2000-02-01 00:00:00    NaN            NaN            NaN            NaN
-    2000-02-02 00:00:00    NaN            NaN            -0.2321        -0.286
-    2000-02-03 00:00:00    NaN            NaN            0.125          -0.02811
-    2000-02-04 00:00:00    0.8737         NaN            0.2809         0.07718
-    2000-02-07 00:00:00    0.5677         NaN            0.3807         0.2888
-    2000-02-08 00:00:00    0.5549         1.203          0.8565         -0.0267
-    2000-02-09 00:00:00    0.3744         0.8047         0.7909         0.018
-    2000-02-10 00:00:00    0.4971         0.7219         0.5394         0.2123
-    2000-02-11 00:00:00    0.3626         0.6139         0.1507         0.1414
-
-Each of these methods can optionally accept a **time_rule** argument
-(see :ref:`time rules <datetools.timerules>`) which is provided as a
-convenience when the user wishes to guarantee that the window of the
-statistic.
-
-Here are some plots of the unary moment functions:
-
-.. plot:: plots/stats/moments_rolling.py
-
-And the binary moment functions:
-
-.. plot:: plots/stats/moments_rolling_binary.py
-
-Exponentially weighted moment functions
----------------------------------------
-
-It's also quite common to want to do non-equally weighted moving statistics,
-such as exponentially weighted (EW) moving average or EW moving standard
-deviation. A number of EW functions are provided using the blending method. For
-example, where :math:`y_t` is the result and :math:`x_t` the input, we compute
-an exponentially weighted moving average as
-
-.. math::
-
-    y_t = (1-\alpha) y_{t-1} + \alpha x_t
-
-One must have :math:`0 < \alpha \leq 1`, but rather than pass :math:`\alpha`
-directly, it's easier to think about either the **span** or **center of mass
-(com)** of an EW moment:
-
-.. math::
-
-   \alpha =
-    \begin{cases}
-	\frac{2}{s + 1}, s = \text{span}\\
-	\frac{1}{c + 1}, c = \text{center of mass}
-    \end{cases}
-
-You can pass one or the other to these functions but not both. **Span**
-corresponds to what is commonly called a "20-day EW moving average" for
-example. **Center of mass** has a more physical interpretation. For example,
-**span** = 20 corresponds to **com** = 9.5.
-
-Here are some examples for a univariate time series:
-
-.. plot:: plots/stats/moments_expw.py
-
-The binary `emwcov` and `ewmcorr` are similar to their equal-weighted
-counterparts above.
-
-.. autofunction:: pandas.stats.moments.ewma
diff --git a/doc/source/stats_ols.rst b/doc/source/stats_ols.rst
deleted file mode 100644
index 4c490cb4b..000000000
--- a/doc/source/stats_ols.rst
+++ /dev/null
@@ -1,9 +0,0 @@
-.. currentmodule:: pandas
-
-.. _ols:
-
-Linear and panel regression
----------------------------
-
-.. automodule:: pandas.stats.interface
-   :members:
diff --git a/doc/source/timeseries.rst b/doc/source/timeseries.rst
index 91b627e9b..ac9581c03 100644
--- a/doc/source/timeseries.rst
+++ b/doc/source/timeseries.rst
@@ -268,40 +268,20 @@ The primary function for changing frequencies is the ``asfreq`` function. This
 is basically just a thin, but convenient wrapper around ``reindex`` which
 generates a ``DateRange`` and calls ``reindex``.
 
-::
-
-    >>> dr = DateRange('1/1/2010', periods=10,
-                       offset=datetools.BMonthEnd())
-    >>> ts = Series(np.arange(10.), index=dr)
-    >>> ts
-    2010-01-29 00:00:00    0.0
-    2010-02-26 00:00:00    1.0
-    2010-03-31 00:00:00    2.0
-    2010-04-30 00:00:00    3.0
-    2010-05-31 00:00:00    4.0
-    2010-06-30 00:00:00    5.0
-    2010-07-30 00:00:00    6.0
-    2010-08-31 00:00:00    7.0
-    2010-09-30 00:00:00    8.0
-    2010-10-29 00:00:00    9.0
-
-    >>> ts.asfreq('WEEKDAY', method='pad')
-    2010-01-29 00:00:00    0.0
-    2010-02-01 00:00:00    0.0
-    2010-02-02 00:00:00    0.0
-    2010-02-03 00:00:00    0.0
-    2010-02-04 00:00:00    0.0
-    <snip>
-    2010-10-22 00:00:00    8.0
-    2010-10-25 00:00:00    8.0
-    2010-10-26 00:00:00    8.0
-    2010-10-27 00:00:00    8.0
-    2010-10-28 00:00:00    8.0
-    2010-10-29 00:00:00    9.0
+.. ipython:: python
+
+   dr = DateRange('1/1/2010', periods=3, offset=3 * datetools.bday)
+   ts = Series(randn(3), index=dr)
+   ts
+   ts.asfreq(BDay())
+   ts.asfreq(BDay(), method='pad')
 
 Filling forward / backward
 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 
+Related to ``asfreq`` and ``reindex`` is the ``fillna`` function documented in
+the :ref:`missing data section <missing_data.fillna>`.
+
 Up- and downsampling
 --------------------
 
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index d2fda30dd..aeff04f6d 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -345,14 +345,15 @@ class DataFrame(NDFrame):
         return dict((k, v.to_dict()) for k, v in self.iteritems())
 
     @classmethod
-    def from_records(cls, data, index=None, indexField=None):
+    def from_records(cls, data, index=None, indexField=None,
+                     exclude=None):
         """
         Convert structured or record ndarray to DataFrame
 
         Parameters
         ----------
         data : NumPy structured array
-        index : string or array-like
+        index : string, list of fields, array-like
             Field of array to use as the index, alternately a specific set of
             input labels to use
 
@@ -360,21 +361,39 @@ class DataFrame(NDFrame):
         -------
         df : DataFrame
         """
-        if not data.dtype.names:
-            raise Exception('Input was not a structured array!')
-
         if indexField is not None:  # pragma: no cover
             warnings.warn("indexField argument is deprecated. Use index "
                           "instead", FutureWarning)
             index = indexField
 
         columns, sdict = _rec_to_dict(data)
+
+        if exclude is None:
+            exclude = set()
+        else:
+            exclude = set(exclude)
+
+        for col in exclude:
+            del sdict[col]
+            columns.remove(col)
+
         if index is not None:
             if isinstance(index, basestring):
                 result_index = sdict.pop(index)
                 columns.remove(index)
             else:
-                result_index = index
+                try:
+                    arrays = []
+                    for field in index:
+                        arrays.append(sdict[field])
+                    for field in index:
+                        del sdict[field]
+                        columns.remove(field)
+                    result_index = MultiIndex.from_arrays(arrays)
+                except Exception:
+                    if len(index) != len(data):
+                        raise
+                    result_index = index
         else:
             result_index = np.arange(len(data))
 
@@ -3008,10 +3027,19 @@ def _prep_ndarray(values, copy=True):
 
 
 def _rec_to_dict(arr):
-    columns = list(arr.dtype.names)
-    sdict = dict((k, arr[k]) for k in columns)
-    return columns, sdict
+    if isinstance(arr, np.ndarray):
+        columns = list(arr.dtype.names)
+        sdict = dict((k, arr[k]) for k in columns)
+    elif isinstance(arr, DataFrame):
+        columns = list(arr.columns)
+        sdict = arr._series
+    elif isinstance(arr, dict):
+        columns = sorted(arr)
+        sdict = arr.copy()
+    else:  # pragma: no cover
+        raise TypeError('%s' % type(arr))
 
+    return columns, sdict
 
 def _homogenize(data, index, columns, dtype=None):
     homogenized = {}
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index df12ce8a7..4b76e901c 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -1027,32 +1027,8 @@ class LongPanel(Panel, DataFrame):
         -------
         LongPanel
         """
-        if isinstance(data, np.ndarray):
-            # Dtype when you have data
-            if not issubclass(data.dtype.type, np.void):
-                raise ValueError('Input was not a structured array!')
-
-            columns = data.dtype.names
-            data = dict((k, data[k]) for k in columns)
-        elif isinstance(data, DataFrame):
-            data = data._series.copy()
-        elif isinstance(data, dict):
-            # otherwise will pop columns out of original
-            data = data.copy()
-
-        if exclude is None:
-            exclude = set()
-        else:
-            exclude = set(exclude)
-
-        for col in exclude:
-            del data[col]
-
-        major = Factor(data.pop(major_field))
-        minor = Factor(data.pop(minor_field))
-        index = MultiIndex(levels=[major.levels, minor.levels],
-                           labels=[major.labels, minor.labels])
-        return LongPanel(data, index=index)
+        return cls.from_records(data, [major_field, minor_field],
+                                exclude=exclude)
 
     def toRecords(self):
         major = np.asarray(self.major_axis).take(self.major_labels)
