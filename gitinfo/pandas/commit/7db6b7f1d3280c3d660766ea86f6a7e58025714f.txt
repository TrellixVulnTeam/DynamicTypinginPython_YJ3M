commit 7db6b7f1d3280c3d660766ea86f6a7e58025714f
Author: jreback <jeff@reback.net>
Date:   Thu Oct 10 08:19:01 2013 -0400

    DOC: whatsnew.rst,missing_data.rst,io.rst updates

diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index 24a2efedc..0e3b66715 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -196,11 +196,11 @@ replace NaN with some other value using ``fillna`` if you wish).
    df + df2
    df.add(df2, fill_value=0)
 
+.. _basics.compare:
+
 Flexible Comparisons
 ~~~~~~~~~~~~~~~~~~~~
 
-.. _basics.compare:
-
 Starting in v0.8, pandas introduced binary comparison methods eq, ne, lt, gt,
 le, and ge to Series and DataFrame whose behavior is analogous to the binary
 arithmetic operations described above:
@@ -214,11 +214,11 @@ These operations produce a pandas object the same type as the left-hand-side inp
 that if of dtype ``bool``. These ``boolean`` objects can be used in indexing operations,
 see :ref:`here<indexing.boolean>`
 
+.. _basics.reductions:
+
 Boolean Reductions
 ~~~~~~~~~~~~~~~~~~
 
-.. _basics.reductions:
-
 Furthermore, you can apply the reductions: ``empty``, ``any()``, ``all()``, and ``bool()`` to provide a
 way to summarize these results.
 
@@ -264,12 +264,12 @@ You can test if a pandas object is empty, via the ``empty`` property.
 
 To evaluate single-element pandas objects in a boolean context, use the method ``.bool()``:
 
-   .. ipython:: python
+.. ipython:: python
 
-      Series([True]).bool()
-      Series([False]).bool()
-      DataFrame([[True]]).bool()
-      DataFrame([[False]]).bool()
+   Series([True]).bool()
+   Series([False]).bool()
+   DataFrame([[True]]).bool()
+   DataFrame([[False]]).bool()
 
 See :ref:`gotchas<gotchas.truth>` for a more detailed discussion.
 
diff --git a/doc/source/gotchas.rst b/doc/source/gotchas.rst
index 6cef1d52a..e8f3367ee 100644
--- a/doc/source/gotchas.rst
+++ b/doc/source/gotchas.rst
@@ -15,11 +15,11 @@
 Caveats and Gotchas
 *******************
 
+.. _gotchas.truth:
+
 Using If/Truth Statements with Pandas
 -------------------------------------
 
-.. _gotchas.truth:
-
 Pandas follows the numpy convention of raising an error when you try to convert something to a ``bool``.
 This happens in a ``if`` or when using the boolean operations, ``and``, ``or``, or ``not``.  It is not clear
 what the result of
@@ -61,12 +61,12 @@ or return if ``any`` value is ``True``.
 
 To evaluate single-element pandas objects in a boolean context, use the method ``.bool()``:
 
-   .. ipython:: python
+.. ipython:: python
 
-       Series([True]).bool()
-       Series([False]).bool()
-       DataFrame([[True]]).bool()
-       DataFrame([[False]]).bool()
+   Series([True]).bool()
+   Series([False]).bool()
+   DataFrame([[True]]).bool()
+   DataFrame([[False]]).bool()
 
 See :ref:`boolean reductions<basics.reductions>` for more examples.
 
diff --git a/doc/source/io.rst b/doc/source/io.rst
index 8e28ea36e..6ed71a1d4 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -2915,19 +2915,21 @@ There are a few other available functions:
    For now, writing your DataFrame into a database works only with
    **SQLite**. Moreover, the **index** will currently be **dropped**.
 
+.. _io.bigquery:
+
 Google BigQuery (Experimental)
 ------------------------------
 
+.. versionadded:: 0.13.0
+
 The :mod:`pandas.io.gbq` module provides a wrapper for Google's BigQuery
 analytics web service to simplify retrieving results from BigQuery tables
 using SQL-like queries. Result sets are parsed into a pandas
 DataFrame with a shape derived from the source table. Additionally,
 DataFrames can be uploaded into BigQuery datasets as tables
-if the source datatypes are compatible with BigQuery ones. The general
-structure of this module and its provided functions are based loosely on those in
- :mod:`pandas.io.sql`.
+if the source datatypes are compatible with BigQuery ones.
 
-For specifics on the service itself, see: <https://developers.google.com/bigquery/>
+For specifics on the service itself, see `here <https://developers.google.com/bigquery/>`__
 
 As an example, suppose you want to load all data from an existing table
 : `test_dataset.test_table`
@@ -2951,7 +2953,9 @@ Additionally, you can define which column to use as an index as well as a prefer
 
 .. code-block:: python
 
-   data_frame = gbq.read_gbq('SELECT * FROM test_dataset.test_table', index_col='index_column_name', col_order='[col1, col2, col3,...]')
+   data_frame = gbq.read_gbq('SELECT * FROM test_dataset.test_table',
+                             index_col='index_column_name',
+                             col_order='[col1, col2, col3,...]')
 
 Finally, if you would like to create a BigQuery table, `my_dataset.my_table`, from the rows of DataFrame, `df`:
 
@@ -2961,7 +2965,8 @@ Finally, if you would like to create a BigQuery table, `my_dataset.my_table`, fr
          'integer_col_name' : [1],
          'boolean_col_name' : [True]})
    schema = ['STRING', 'INTEGER', 'BOOLEAN']
-   data_frame = gbq.to_gbq(df, 'my_dataset.my_table', if_exists='fail', schema = schema)
+   data_frame = gbq.to_gbq(df, 'my_dataset.my_table',
+                           if_exists='fail', schema = schema)
 
 To add more rows to this, simply:
 
@@ -2972,23 +2977,23 @@ To add more rows to this, simply:
          'boolean_col_name' : [False]})
    data_frame = gbq.to_gbq(df2, 'my_dataset.my_table', if_exists='append')
 
-
-
 .. note::
 
-   * There is a hard cap on BigQuery result sets, at 128MB compressed. Also, the BigQuery SQL query language has some oddities,
-   see: <https://developers.google.com/bigquery/query-reference>
+   There is a hard cap on BigQuery result sets, at 128MB compressed. Also, the BigQuery SQL query language has some oddities,
+   see `here <https://developers.google.com/bigquery/query-reference>`__
+
+.. _io.stata:
 
 STATA Format
 ------------
 
-.. _io.stata:
+.. versionadded:: 0.12.0
+
+.. _io.stata_writer:
 
 Writing to STATA format
 ~~~~~~~~~~~~~~~~~~~~~~~
 
-.. _io.stata_writer:
-
 The method :func:`~pandas.core.frame.DataFrame.to_stata` will write a DataFrame
 into a .dta file. The format version of this file is always 115 (Stata 12).
 
@@ -2997,12 +3002,10 @@ into a .dta file. The format version of this file is always 115 (Stata 12).
    df = DataFrame(randn(10, 2), columns=list('AB'))
    df.to_stata('stata.dta')
 
-Reading from STATA format
-~~~~~~~~~~~~~~~~~~~~~~~~~
-
 .. _io.stata_reader:
 
-.. versionadded:: 0.12.0
+Reading from STATA format
+~~~~~~~~~~~~~~~~~~~~~~~~~
 
 The top-level function ``read_stata`` will read a dta format file
 and return a DataFrame:
diff --git a/doc/source/missing_data.rst b/doc/source/missing_data.rst
index 3e2183b07..cee809da6 100644
--- a/doc/source/missing_data.rst
+++ b/doc/source/missing_data.rst
@@ -273,8 +273,8 @@ Interpolation
 
 .. versionadded:: 0.13.0
 
-  DataFrame now has the interpolation method.
-  :meth:`~pandas.Series.interpolate` also gained some additional methods.
+  :meth:`~pandas.DataFrame.interpolate`, and :meth:`~pandas.Series.interpolate` have
+  revamped interpolation methods and functionaility.
 
 Both Series and Dataframe objects have an ``interpolate`` method that, by default,
 performs linear interpolation at missing datapoints.
@@ -291,14 +291,11 @@ performs linear interpolation at missing datapoints.
 
 .. ipython:: python
 
+   ts
    ts.count()
-
-   ts.head()
-
    ts.interpolate().count()
 
-   ts.interpolate().head()
-
+   plt.figure()
    @savefig series_interpolate.png
    ts.interpolate().plot()
 
@@ -307,15 +304,13 @@ Index aware interpolation is available via the ``method`` keyword:
 .. ipython:: python
    :suppress:
 
-   ts = ts[[0, 1, 30, 60, 99]]
+   ts2 = ts[[0, 1, 30, 60, 99]]
 
 .. ipython:: python
 
-   ts
-
-   ts.interpolate()
-
-   ts.interpolate(method='time')
+   ts2
+   ts2.interpolate()
+   ts2.interpolate(method='time')
 
 For a floating-point index, use ``method='values'``:
 
@@ -328,9 +323,7 @@ For a floating-point index, use ``method='values'``:
 .. ipython:: python
 
    ser
-
    ser.interpolate()
-
    ser.interpolate(method='values')
 
 You can also interpolate with a DataFrame:
@@ -339,6 +332,7 @@ You can also interpolate with a DataFrame:
 
    df = DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
                    'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]})
+   df
    df.interpolate()
 
 The ``method`` argument gives access to fancier interpolation methods.
@@ -351,37 +345,38 @@ distribution function, then ``method='pchip'`` should work well.
 
 .. warning::
 
-    These methods require ``scipy``.
+   These methods require ``scipy``.
 
 .. ipython:: python
 
-  df.interpolate(method='barycentric')
+   df.interpolate(method='barycentric')
 
-  df.interpolate(method='pchip')
+   df.interpolate(method='pchip')
 
 When interpolating via a polynomial or spline approximation, you must also specify
 the degree or order of the approximation:
 
 .. ipython:: python
 
-  df.interpolate(method='spline', order=2)
+   df.interpolate(method='spline', order=2)
 
-  df.interpolate(method='polynomial', order=2)
+   df.interpolate(method='polynomial', order=2)
 
 Compare several methods:
 
 .. ipython:: python
 
-  np.random.seed(2)
+   np.random.seed(2)
 
-  ser = Series(np.arange(1, 10.1, .25)**2 + np.random.randn(37))
-  bad = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])
-  ser[bad] = np.nan
-  methods = ['linear', 'quadratic', 'cubic']
+   ser = Series(np.arange(1, 10.1, .25)**2 + np.random.randn(37))
+   bad = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])
+   ser[bad] = np.nan
+   methods = ['linear', 'quadratic', 'cubic']
 
-  df = DataFrame({m: ser.interpolate(method=m) for m in methods})
-  @savefig compare_interpolations.png
-  df.plot()
+   df = DataFrame({m: ser.interpolate(method=m) for m in methods})
+   plt.figure()
+   @savefig compare_interpolations.png
+   df.plot()
 
 Another use case is interpolation at *new* values.
 Suppose you have 100 observations from some distribution. And let's suppose
@@ -391,27 +386,24 @@ at the new values.
 
 .. ipython:: python
 
-  ser = Series(np.sort(np.random.uniform(size=100)))
+   ser = Series(np.sort(np.random.uniform(size=100)))
 
-  # interpolate at new_index
-  new_index = ser.index + Index([49.25, 49.5, 49.75, 50.25, 50.5, 50.75])
-
-  interp_s = ser.reindex(new_index).interpolate(method='pchip')
-
-  interp_s[49:51]
+   # interpolate at new_index
+   new_index = ser.index + Index([49.25, 49.5, 49.75, 50.25, 50.5, 50.75])
+   interp_s = ser.reindex(new_index).interpolate(method='pchip')
+   interp_s[49:51]
 
 .. _scipy: http://www.scipy.org
 .. _documentation: http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation
 .. _guide: http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html
 
-
 Like other pandas fill methods, ``interpolate`` accepts a ``limit`` keyword argument.
-Use this to limit the number of consecutive interpolations, keeping ``NaN``s for interpolations that are too far from the last valid observation:
+Use this to limit the number of consecutive interpolations, keeping ``NaN`` s for interpolations that are too far from the last valid observation:
 
 .. ipython:: python
 
-  ser = Series([1, 3, np.nan, np.nan, np.nan, 11])
-  ser.interpolate(limit=2)
+   ser = Series([1, 3, np.nan, np.nan, np.nan, 11])
+   ser.interpolate(limit=2)
 
 .. _missing_data.replace:
 
diff --git a/doc/source/release.rst b/doc/source/release.rst
index 53db769f3..18f30cf31 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -181,7 +181,7 @@ API Changes
 ~~~~~~~~~~~
 
   - ``DataFrame.reindex()`` and forward/backward filling now raises ValueError
-    if either index is not monotonic (:issue: `4483` , :issue: `4484`).
+    if either index is not monotonic (:issue:`4483`, :issue:`4484`).
   - ``pandas`` now is Python 2/3 compatible without the need for 2to3 thanks to
     @jtratner. As a result, pandas now uses iterators more extensively. This
     also led to the introduction of substantive parts of the Benjamin
@@ -440,7 +440,7 @@ Bug Fixes
     instance rather than the same instance (:issue:`4379`); also adds a test
     for this for the other index types
   - Fixed a bug with all the dtypes being converted to object when using the CSV cparser
-    with the usecols parameter (:issue: `3192`)
+    with the usecols parameter (:issue:`3192`)
   - Fix an issue in merging blocks where the resulting DataFrame had partially
     set _ref_locs (:issue:`4403`)
   - Fixed an issue where hist subplots were being overwritten when they were
diff --git a/doc/source/v0.13.0.txt b/doc/source/v0.13.0.txt
index 66ca0f225..5faaa3f86 100644
--- a/doc/source/v0.13.0.txt
+++ b/doc/source/v0.13.0.txt
@@ -3,89 +3,88 @@
 v0.13.0 (October ??, 2013)
 --------------------------
 
-This is a major release from 0.12.0 and includes several new features and
+This is a major release from 0.12.0 and includes a number of API changes, several new features and
 enhancements along with a large number of bug fixes.
 
+Highlights include support for a new index type ``Float64Index``, support for new methods of interpolation, updated ``timedelta`` operations, and a new string manipulation method ``extract``.
+Several experimental features are added, including new ``eval/query`` methods for expression evaluation, support for ``msgpack`` serialization,
+and an io interface to google's ``BigQuery``.
+
 .. warning::
 
    In 0.13.0 ``Series`` has internally been refactored to no longer sub-class ``ndarray``
-   but instead subclass ``NDFrame``, similarly to the rest of the pandas containers. This should be
+   but instead subclass ``NDFrame``, similar to the rest of the pandas containers. This should be
    a transparent change with only very limited API implications. See :ref:`Internal Refactoring<whatsnew_0130.refactoring>`
 
 API changes
 ~~~~~~~~~~~
 
-  - ``read_excel`` now supports an integer in its ``sheetname`` argument giving
-    the index of the sheet to read in (:issue:`4301`).
-  - Text parser now treats anything that reads like inf ("inf", "Inf", "-Inf",
-    "iNf", etc.) as infinity. (:issue:`4220`, :issue:`4219`), affecting
-    ``read_table``, ``read_csv``, etc.
-  - ``pandas`` now is Python 2/3 compatible without the need for 2to3 thanks to
-    @jtratner. As a result, pandas now uses iterators more extensively. This
-    also led to the introduction of substantive parts of the Benjamin
-    Peterson's ``six`` library into compat. (:issue:`4384`, :issue:`4375`,
-    :issue:`4372`)
-  - ``pandas.util.compat`` and ``pandas.util.py3compat`` have been merged into
-    ``pandas.compat``. ``pandas.compat`` now includes many functions allowing
-    2/3 compatibility. It contains both list and iterator versions of range,
-    filter, map and zip, plus other necessary elements for Python 3
-    compatibility. ``lmap``, ``lzip``, ``lrange`` and ``lfilter`` all produce
-    lists instead of iterators, for compatibility with ``numpy``, subscripting
-    and ``pandas`` constructors.(:issue:`4384`, :issue:`4375`, :issue:`4372`)
-  - deprecated ``iterkv``, which will be removed in a future release (was just
-    an alias of iteritems used to get around ``2to3``'s changes).
-    (:issue:`4384`, :issue:`4375`, :issue:`4372`)
-  - ``Series.get`` with negative indexers now returns the same as ``[]`` (:issue:`4390`)
-  - Changes to how ``Index`` and ``MultiIndex`` handle metadata (``levels``,
-    ``labels``, and ``names``) (:issue:`4039`):
+- ``read_excel`` now supports an integer in its ``sheetname`` argument giving
+  the index of the sheet to read in (:issue:`4301`).
+- Text parser now treats anything that reads like inf ("inf", "Inf", "-Inf",
+  "iNf", etc.) as infinity. (:issue:`4220`, :issue:`4219`), affecting
+  ``read_table``, ``read_csv``, etc.
+- ``pandas`` now is Python 2/3 compatible without the need for 2to3 thanks to
+  @jtratner. As a result, pandas now uses iterators more extensively. This
+  also led to the introduction of substantive parts of the Benjamin
+  Peterson's ``six`` library into compat. (:issue:`4384`, :issue:`4375`,
+  :issue:`4372`)
+- ``pandas.util.compat`` and ``pandas.util.py3compat`` have been merged into
+  ``pandas.compat``. ``pandas.compat`` now includes many functions allowing
+  2/3 compatibility. It contains both list and iterator versions of range,
+  filter, map and zip, plus other necessary elements for Python 3
+  compatibility. ``lmap``, ``lzip``, ``lrange`` and ``lfilter`` all produce
+  lists instead of iterators, for compatibility with ``numpy``, subscripting
+  and ``pandas`` constructors.(:issue:`4384`, :issue:`4375`, :issue:`4372`)
+- ``Series.get`` with negative indexers now returns the same as ``[]`` (:issue:`4390`)
+- Changes to how ``Index`` and ``MultiIndex`` handle metadata (``levels``,
+  ``labels``, and ``names``) (:issue:`4039`):
 
-    .. code-block:: python
+  .. code-block:: python
 
-        # previously, you would have set levels or labels directly
-        index.levels = [[1, 2, 3, 4], [1, 2, 4, 4]]
+      # previously, you would have set levels or labels directly
+      index.levels = [[1, 2, 3, 4], [1, 2, 4, 4]]
 
-        # now, you use the set_levels or set_labels methods
-        index = index.set_levels([[1, 2, 3, 4], [1, 2, 4, 4]])
+      # now, you use the set_levels or set_labels methods
+      index = index.set_levels([[1, 2, 3, 4], [1, 2, 4, 4]])
 
-        # similarly, for names, you can rename the object
-        # but setting names is not deprecated.
-        index = index.set_names(["bob", "cranberry"])
+      # similarly, for names, you can rename the object
+      # but setting names is not deprecated.
+      index = index.set_names(["bob", "cranberry"])
 
-        # and all methods take an inplace kwarg
-        index.set_names(["bob", "cranberry"], inplace=True)
+      # and all methods take an inplace kwarg
+      index.set_names(["bob", "cranberry"], inplace=True)
 
-  - Infer and downcast dtype if ``downcast='infer'`` is passed to ``fillna/ffill/bfill`` (:issue:`4604`)
-  - Remove deprecated ``Factor`` (:issue:`3650`)
-  - Remove deprecated ``set_printoptions/reset_printoptions`` (:issue:``3046``)
-  - Remove deprecated ``_verbose_info`` (:issue:`3215`)
-  - ``__nonzero__`` for all NDFrame objects, will now raise a ``ValueError``, this reverts back to (:issue:`1073`, :issue:`4633`)
-    behavior. Added the ``.bool()`` method to ``NDFrame`` objects to facilitate evaluating of single-element boolean Series
-    See :ref:`gotchas<gotchas.truth>` for a more detailed discussion.
+- Infer and downcast dtype if ``downcast='infer'`` is passed to ``fillna/ffill/bfill`` (:issue:`4604`)
+- ``__nonzero__`` for all NDFrame objects, will now raise a ``ValueError``, this reverts back to (:issue:`1073`, :issue:`4633`)
+  behavior. Added the ``.bool()`` method to ``NDFrame`` objects to facilitate evaluating of single-element boolean Series
+  See :ref:`gotchas<gotchas.truth>` for a more detailed discussion.
 
-    This prevents behaviors like (which will now all raise ``ValueError``)
+  This prevents doing boolean comparision on *entire* pandas objects. These all will raise ``ValueError``.
 
-    .. code-block:: python
+  .. code-block:: python
 
-        if df:
-           ....
+      if df:
+         ....
+      >>> ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
 
-        df1 and df2
-        s1 and s2
+      df1 and df2
+      s1 and s2
 
 
-    To evaluate single-element pandas objects in a boolean context, use the method ``.bool()``:
+  To evaluate single-element pandas objects in a boolean context, use the method ``.bool()``:
 
-    .. ipython:: python
+  .. ipython:: python
 
-       Series([True]).bool()
-       Series([False]).bool()
-       DataFrame([[True]]).bool()
-       DataFrame([[False]]).bool()
+     Series([True]).bool()
+     Series([False]).bool()
+     DataFrame([[True]]).bool()
+     DataFrame([[False]]).bool()
 
-  - All non-Index NDFrames (``Series``, ``DataFrame``, ``Panel``, ``Panel4D``,
-    ``SparsePanel``, etc.), now support the entire set of arithmetic operators
-    and arithmetic flex methods (add, sub, mul, etc.). ``SparsePanel`` does not
-    support ``pow`` or ``mod`` with non-scalars. (:issue:`3765`)
+- All non-Index NDFrames (``Series``, ``DataFrame``, ``Panel``, ``Panel4D``,
+  ``SparsePanel``, etc.), now support the entire set of arithmetic operators
+  and arithmetic flex methods (add, sub, mul, etc.). ``SparsePanel`` does not
+  support ``pow`` or ``mod`` with non-scalars. (:issue:`3765`)
 
 
 Prior Version Deprecations/Changes
@@ -93,494 +92,532 @@ Prior Version Deprecations/Changes
 
 These were announced changes in 0.12 or prior that are taking effect as of 0.13.0
 
-  - Remove deprecated ``Factor`` (:issue:`3650`)
-  - Remove deprecated ``set_printoptions/reset_printoptions`` (:issue:``3046``)
-  - Remove deprecated ``_verbose_info`` (:issue:`3215`)
-  - Remove deprecated ``read_clipboard/to_clipboard/ExcelFile/ExcelWriter`` from ``pandas.io.parsers`` (:issue:`3717`)
-  - default for ``tupleize_cols`` is now ``False`` for both ``to_csv`` and ``read_csv``. Fair warning in 0.12 (:issue:`3604`)
+- Remove deprecated ``Factor`` (:issue:`3650`)
+- Remove deprecated ``set_printoptions/reset_printoptions`` (:issue:``3046``)
+- Remove deprecated ``_verbose_info`` (:issue:`3215`)
+- Remove deprecated ``read_clipboard/to_clipboard/ExcelFile/ExcelWriter`` from ``pandas.io.parsers`` (:issue:`3717`)
+- default for ``tupleize_cols`` is now ``False`` for both ``to_csv`` and ``read_csv``. Fair warning in 0.12 (:issue:`3604`)
+
+Deprecations
+~~~~~~~~~~~~
+
+Deprecated in 0.13.0
+
+- deprecated ``iterkv``, which will be removed in a future release (this was
+  an alias of iteritems used to bypass ``2to3``'s changes).
+  (:issue:`4384`, :issue:`4375`, :issue:`4372`)
 
 Indexing API Changes
 ~~~~~~~~~~~~~~~~~~~~
 
-    Prior to 0.13, it was impossible to use a label indexer (``.loc/.ix``) to set a value that
-    was not contained in the index of a particular axis. (:issue:`2578`). See more :ref:`here<indexing.basics.partial_setting>`
+Prior to 0.13, it was impossible to use a label indexer (``.loc/.ix``) to set a value that
+was not contained in the index of a particular axis. (:issue:`2578`). See more :ref:`here<indexing.basics.partial_setting>`
 
-    In the ``Series`` case this is effectively an appending operation
+In the ``Series`` case this is effectively an appending operation
 
-    .. ipython:: python
+.. ipython:: python
 
-       s = Series([1,2,3])
-       s
-       s[5] = 5.
-       s
+   s = Series([1,2,3])
+   s
+   s[5] = 5.
+   s
 
-    .. ipython:: python
+.. ipython:: python
 
-       dfi = DataFrame(np.arange(6).reshape(3,2),
-                       columns=['A','B'])
-       dfi
+   dfi = DataFrame(np.arange(6).reshape(3,2),
+                   columns=['A','B'])
+   dfi
 
-    This would previously ``KeyError``
+This would previously ``KeyError``
 
-    .. ipython:: python
+.. ipython:: python
 
-       dfi.loc[:,'C'] = dfi.loc[:,'A']
-       dfi
+   dfi.loc[:,'C'] = dfi.loc[:,'A']
+   dfi
 
-    This is like an ``append`` operation.
+This is like an ``append`` operation.
 
-    .. ipython:: python
+.. ipython:: python
 
-       dfi.loc[3] = 5
-       dfi
+   dfi.loc[3] = 5
+   dfi
 
-    A Panel setting operation on an arbitrary axis aligns the input to the Panel
+A Panel setting operation on an arbitrary axis aligns the input to the Panel
 
-    .. ipython:: python
+.. ipython:: python
 
-       p = pd.Panel(np.arange(16).reshape(2,4,2),
-                   items=['Item1','Item2'],
-                   major_axis=pd.date_range('2001/1/12',periods=4),
-                   minor_axis=['A','B'],dtype='float64')
-       p
-       p.loc[:,:,'C'] = Series([30,32],index=p.items)
-       p
-       p.loc[:,:,'C']
+   p = pd.Panel(np.arange(16).reshape(2,4,2),
+               items=['Item1','Item2'],
+               major_axis=pd.date_range('2001/1/12',periods=4),
+               minor_axis=['A','B'],dtype='float64')
+   p
+   p.loc[:,:,'C'] = Series([30,32],index=p.items)
+   p
+   p.loc[:,:,'C']
 
 Float64Index API Change
 ~~~~~~~~~~~~~~~~~~~~~~~
 
-  - Added a new index type, ``Float64Index``. This will be automatically created when passing floating values in index creation.
-    This enables a pure label-based slicing paradigm that makes ``[],ix,loc`` for scalar indexing and slicing work exactly the
-    same. See :ref:`the docs<indexing.float64index>`, (:issue:`263`)
+- Added a new index type, ``Float64Index``. This will be automatically created when passing floating values in index creation.
+  This enables a pure label-based slicing paradigm that makes ``[],ix,loc`` for scalar indexing and slicing work exactly the
+  same. See :ref:`the docs<indexing.float64index>`, (:issue:`263`)
 
-    Construction is by default for floating type values.
+  Construction is by default for floating type values.
 
-    .. ipython:: python
+  .. ipython:: python
 
-       index = Index([1.5, 2, 3, 4.5, 5])
-       index
-       s = Series(range(5),index=index)
-       s
+     index = Index([1.5, 2, 3, 4.5, 5])
+     index
+     s = Series(range(5),index=index)
+     s
 
-    Scalar selection for ``[],.ix,.loc`` will always be label based. An integer will match an equal float index (e.g. ``3`` is equivalent to ``3.0``)
+  Scalar selection for ``[],.ix,.loc`` will always be label based. An integer will match an equal float index (e.g. ``3`` is equivalent to ``3.0``)
 
-    .. ipython:: python
+  .. ipython:: python
 
-       s[3]
-       s.ix[3]
-       s.loc[3]
+     s[3]
+     s.ix[3]
+     s.loc[3]
 
-    The only positional indexing is via ``iloc``
+  The only positional indexing is via ``iloc``
 
-    .. ipython:: python
+  .. ipython:: python
 
-       s.iloc[3]
+     s.iloc[3]
 
-    A scalar index that is not found will raise ``KeyError``
+  A scalar index that is not found will raise ``KeyError``
 
-    Slicing is ALWAYS on the values of the index, for ``[],ix,loc`` and ALWAYS positional with ``iloc``
+  Slicing is ALWAYS on the values of the index, for ``[],ix,loc`` and ALWAYS positional with ``iloc``
 
-    .. ipython:: python
+  .. ipython:: python
 
-       s[2:4]
-       s.ix[2:4]
-       s.loc[2:4]
-       s.iloc[2:4]
+     s[2:4]
+     s.ix[2:4]
+     s.loc[2:4]
+     s.iloc[2:4]
 
-    In float indexes, slicing using floats are allowed
+  In float indexes, slicing using floats are allowed
 
-    .. ipython:: python
+  .. ipython:: python
 
-       s[2.1:4.6]
-       s.loc[2.1:4.6]
+     s[2.1:4.6]
+     s.loc[2.1:4.6]
 
-  - Indexing on other index types are preserved (and positional fallback for ``[],ix``), with the exception, that floating point slicing
-    on indexes on non ``Float64Index`` will now raise a ``TypeError``.
+- Indexing on other index types are preserved (and positional fallback for ``[],ix``), with the exception, that floating point slicing
+  on indexes on non ``Float64Index`` will now raise a ``TypeError``.
 
-    .. code-block:: python
+  .. code-block:: python
 
-       In [1]: Series(range(5))[3.5]
-       TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)
+     In [1]: Series(range(5))[3.5]
+     TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)
 
-       In [1]: Series(range(5))[3.5:4.5]
-       TypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)
+     In [1]: Series(range(5))[3.5:4.5]
+     TypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)
 
-    Using a scalar float indexer will be deprecated in a future version, but is allowed for now.
+  Using a scalar float indexer will be deprecated in a future version, but is allowed for now.
 
-    .. code-block:: python
+  .. code-block:: python
 
-       In [3]: Series(range(5))[3.0]
-       Out[3]: 3
+     In [3]: Series(range(5))[3.0]
+     Out[3]: 3
 
 HDFStore API Changes
 ~~~~~~~~~~~~~~~~~~~~
 
-  - Query Format Changes. A much more string-like query format is now supported. See :ref:`the docs<io.hdf5-query>`.
+- Query Format Changes. A much more string-like query format is now supported. See :ref:`the docs<io.hdf5-query>`.
 
-    .. ipython:: python
-
-       path = 'test.h5'
-       dfq = DataFrame(randn(10,4),
-                columns=list('ABCD'),
-                index=date_range('20130101',periods=10))
-       dfq.to_hdf(path,'dfq',format='table',data_columns=True)
+  .. ipython:: python
 
-    Use boolean expressions, with in-line function evaluation.
+     path = 'test.h5'
+     dfq = DataFrame(randn(10,4),
+              columns=list('ABCD'),
+              index=date_range('20130101',periods=10))
+     dfq.to_hdf(path,'dfq',format='table',data_columns=True)
 
-    .. ipython:: python
+  Use boolean expressions, with in-line function evaluation.
 
-       read_hdf(path,'dfq',
-           where="index>Timestamp('20130104') & columns=['A', 'B']")
+  .. ipython:: python
 
-    Use an inline column reference
+     read_hdf(path,'dfq',
+         where="index>Timestamp('20130104') & columns=['A', 'B']")
 
-    .. ipython:: python
+  Use an inline column reference
 
-       read_hdf(path,'dfq',
-           where="A>0 or C>0")
+  .. ipython:: python
 
-    .. ipython:: python
-       :suppress:
+     read_hdf(path,'dfq',
+         where="A>0 or C>0")
 
-       import os
-       os.remove(path)
+  .. ipython:: python
+     :suppress:
 
-  - the ``format`` keyword now replaces the ``table`` keyword; allowed values are ``fixed(f)`` or ``table(t)``
-    the same defaults as prior < 0.13.0 remain, e.g. ``put`` implies ``fixed`` format
-    and ``append`` imples ``table`` format. This default format can be set as an option by setting ``io.hdf.default_format``.
+     import os
+     os.remove(path)
 
-    .. ipython:: python
+- the ``format`` keyword now replaces the ``table`` keyword; allowed values are ``fixed(f)`` or ``table(t)``
+  the same defaults as prior < 0.13.0 remain, e.g. ``put`` implies ``fixed`` format
+  and ``append`` imples ``table`` format. This default format can be set as an option by setting ``io.hdf.default_format``.
 
-       path = 'test.h5'
-       df = DataFrame(randn(10,2))
-       df.to_hdf(path,'df_table',format='table')
-       df.to_hdf(path,'df_table2',append=True)
-       df.to_hdf(path,'df_fixed')
-       with get_store(path) as store:
-          print(store)
+  .. ipython:: python
 
-    .. ipython:: python
-       :suppress:
+     path = 'test.h5'
+     df = DataFrame(randn(10,2))
+     df.to_hdf(path,'df_table',format='table')
+     df.to_hdf(path,'df_table2',append=True)
+     df.to_hdf(path,'df_fixed')
+     with get_store(path) as store:
+        print(store)
 
-       import os
-       os.remove(path)
+  .. ipython:: python
+     :suppress:
 
-  - Significant table writing performance improvements
-  - handle a passed ``Series`` in table format (:issue:`4330`)
-  - can now serialize a ``timedelta64[ns]`` dtype in a table (:issue:`3577`), See :ref:`here for an example<io.hdf5-timedelta>`.
-  - added an ``is_open`` property to indicate if the underlying file handle is_open;
-    a closed store will now report 'CLOSED' when viewing the store (rather than raising an error)
-    (:issue:`4409`)
-  - a close of a ``HDFStore`` now will close that instance of the ``HDFStore``
-    but will only close the actual file if the ref count (by ``PyTables``) w.r.t. all of the open handles
-    are 0. Essentially you have a local instance of ``HDFStore`` referenced by a variable. Once you
-    close it, it will report closed. Other references (to the same file) will continue to operate
-    until they themselves are closed. Performing an action on a closed file will raise
-    ``ClosedFileError``
+     import os
+     os.remove(path)
+
+- Significant table writing performance improvements
+- handle a passed ``Series`` in table format (:issue:`4330`)
+- can now serialize a ``timedelta64[ns]`` dtype in a table (:issue:`3577`), See :ref:`here for an example<io.hdf5-timedelta>`.
+- added an ``is_open`` property to indicate if the underlying file handle is_open;
+  a closed store will now report 'CLOSED' when viewing the store (rather than raising an error)
+  (:issue:`4409`)
+- a close of a ``HDFStore`` now will close that instance of the ``HDFStore``
+  but will only close the actual file if the ref count (by ``PyTables``) w.r.t. all of the open handles
+  are 0. Essentially you have a local instance of ``HDFStore`` referenced by a variable. Once you
+  close it, it will report closed. Other references (to the same file) will continue to operate
+  until they themselves are closed. Performing an action on a closed file will raise
+  ``ClosedFileError``
 
-    .. ipython:: python
+  .. ipython:: python
 
-       path = 'test.h5'
-       df = DataFrame(randn(10,2))
-       store1 = HDFStore(path)
-       store2 = HDFStore(path)
-       store1.append('df',df)
-       store2.append('df2',df)
+     path = 'test.h5'
+     df = DataFrame(randn(10,2))
+     store1 = HDFStore(path)
+     store2 = HDFStore(path)
+     store1.append('df',df)
+     store2.append('df2',df)
 
-       store1
-       store2
-       store1.close()
-       store2
-       store2.close()
-       store2
+     store1
+     store2
+     store1.close()
+     store2
+     store2.close()
+     store2
 
-    .. ipython:: python
-       :suppress:
+  .. ipython:: python
+     :suppress:
 
-       import os
-       os.remove(path)
+     import os
+     os.remove(path)
 
-  - removed the ``_quiet`` attribute, replace by a ``DuplicateWarning`` if retrieving
-    duplicate rows from a table (:issue:`4367`)
-  - removed the ``warn`` argument from ``open``. Instead a ``PossibleDataLossError`` exception will
-    be raised if you try to use ``mode='w'`` with an OPEN file handle (:issue:`4367`)
-  - allow a passed locations array or mask as a ``where`` condition (:issue:`4467`).
-    See :ref:`here<io.hdf5-where_mask>` for an example.
-  - add the keyword ``dropna=True`` to ``append`` to change whether ALL nan rows are not written
-    to the store (default is ``True``, ALL nan rows are NOT written), also settable
-    via the option ``io.hdf.dropna_table`` (:issue:`4625`)
+- removed the ``_quiet`` attribute, replace by a ``DuplicateWarning`` if retrieving
+  duplicate rows from a table (:issue:`4367`)
+- removed the ``warn`` argument from ``open``. Instead a ``PossibleDataLossError`` exception will
+  be raised if you try to use ``mode='w'`` with an OPEN file handle (:issue:`4367`)
+- allow a passed locations array or mask as a ``where`` condition (:issue:`4467`).
+  See :ref:`here<io.hdf5-where_mask>` for an example.
+- add the keyword ``dropna=True`` to ``append`` to change whether ALL nan rows are not written
+  to the store (default is ``True``, ALL nan rows are NOT written), also settable
+  via the option ``io.hdf.dropna_table`` (:issue:`4625`)
 
 Enhancements
 ~~~~~~~~~~~~
 
-  - ``read_html`` now raises a ``URLError`` instead of catching and raising a
-    ``ValueError`` (:issue:`4303`, :issue:`4305`)
-  - Added a test for ``read_clipboard()`` and ``to_clipboard()`` (:issue:`4282`)
-  - Clipboard functionality now works with PySide (:issue:`4282`)
-  - Added a more informative error message when plot arguments contain
-    overlapping color and style arguments (:issue:`4402`)
-  - ``to_dict`` now takes ``records`` as a possible outtype.  Returns an array
-    of column-keyed dictionaries. (:issue:`4936`)
+- ``read_html`` now raises a ``URLError`` instead of catching and raising a
+  ``ValueError`` (:issue:`4303`, :issue:`4305`)
+- Added a test for ``read_clipboard()`` and ``to_clipboard()`` (:issue:`4282`)
+- Clipboard functionality now works with PySide (:issue:`4282`)
+- Added a more informative error message when plot arguments contain
+  overlapping color and style arguments (:issue:`4402`)
+- ``to_dict`` now takes ``records`` as a possible outtype.  Returns an array
+  of column-keyed dictionaries. (:issue:`4936`)
+
+- ``NaN`` handing in get_dummies (:issue:`4446`) with `dummy_na`
+
+  .. ipython:: python
+
+     # previously, nan was erroneously counted as 2 here
+     # now it is not counted at all
+     get_dummies([1, 2, np.nan])
+
+     # unless requested
+     get_dummies([1, 2, np.nan], dummy_na=True)
 
-  - NaN handing in get_dummies (:issue:`4446`) with `dummy_na`
 
-    .. ipython:: python
+- ``timedelta64[ns]`` operations. See :ref:`here<timeseries.timedeltas_convert>` for the docs.
 
-       # previously, nan was erroneously counted as 2 here
-       # now it is not counted at all
-       get_dummies([1, 2, np.nan])
+  .. warning::
 
-       # unless requested
-       get_dummies([1, 2, np.nan], dummy_na=True)
+     Most of these operations require ``numpy >= 1.7``
 
+  Using the new top-level ``to_timedelta``, you can convert a scalar or array from the standard
+  timedelta format (produced by ``to_csv``) into a timedelta type (``np.timedelta64`` in ``nanoseconds``).
 
-  - ``timedelta64[ns]`` operations. See :ref:`here<timeseries.timedeltas_convert>` for the docs.
+  .. ipython:: python
 
-    .. warning::
+     to_timedelta('1 days 06:05:01.00003')
+     to_timedelta('15.5us')
+     to_timedelta(['1 days 06:05:01.00003','15.5us','nan'])
+     to_timedelta(np.arange(5),unit='s')
+     to_timedelta(np.arange(5),unit='d')
 
-       Most of these operations require ``numpy >= 1.7``
+  A Series of dtype ``timedelta64[ns]`` can now be divided by another
+  ``timedelta64[ns]`` object to yield a ``float64`` dtyped Series. This
+  is frequency conversion. See :ref:`here<timeseries.timedeltas_convert>` for the docs.
 
-    - Using the new top-level ``to_timedelta``, you can convert a scalar or array from the standard
-      timedelta format (produced by ``to_csv``) into a timedelta type (``np.timedelta64`` in ``nanoseconds``).
+  .. ipython:: python
 
-      .. ipython:: python
+     from datetime import timedelta
+     td = Series(date_range('20130101',periods=4))-Series(date_range('20121201',periods=4))
+     td[2] += np.timedelta64(timedelta(minutes=5,seconds=3))
+     td[3] = np.nan
+     td
 
-         to_timedelta('1 days 06:05:01.00003')
-         to_timedelta('15.5us')
-         to_timedelta(['1 days 06:05:01.00003','15.5us','nan'])
-         to_timedelta(np.arange(5),unit='s')
-         to_timedelta(np.arange(5),unit='d')
+     # to days
+     td / np.timedelta64(1,'D')
 
-    - A Series of dtype ``timedelta64[ns]`` can now be divided by another
-      ``timedelta64[ns]`` object to yield a ``float64`` dtyped Series. This
-      is frequency conversion. See :ref:`here<timeseries.timedeltas_convert>` for the docs.
+     # to seconds
+     td / np.timedelta64(1,'s')
 
-      .. ipython:: python
+  Dividing or multiplying a ``timedelta64[ns]`` Series by an integer or integer Series
 
-         from datetime import timedelta
-         td = Series(date_range('20130101',periods=4))-Series(date_range('20121201',periods=4))
-         td[2] += np.timedelta64(timedelta(minutes=5,seconds=3))
-         td[3] = np.nan
-         td
+  .. ipython:: python
 
-         # to days
-         td / np.timedelta64(1,'D')
+     td * -1
+     td * Series([1,2,3,4])
 
-         # to seconds
-         td / np.timedelta64(1,'s')
+  Absolute ``DateOffset`` objects can act equivalenty to ``timedeltas``
 
-    - Dividing or multiplying a ``timedelta64[ns]`` Series by an integer or integer Series
+  .. ipython:: python
 
-      .. ipython:: python
+     from pandas import offsets
+     td + offsets.Minute(5) + offsets.Milli(5)
 
-         td * -1
-         td * Series([1,2,3,4])
+  Fillna is now supported for timedeltas
 
-    - Absolute ``DateOffset`` objects can act equivalenty to ``timedeltas``
+  .. ipython:: python
 
-      .. ipython:: python
+     td.fillna(0)
+     td.fillna(timedelta(days=1,seconds=5))
 
-         from pandas import offsets
-         td + offsets.Minute(5) + offsets.Milli(5)
+  You can do numeric reduction operations on timedeltas. Note that these will return
+  a single-element Series.
 
-    - Fillna is now supported for timedeltas
+  .. ipython:: python
 
-      .. ipython:: python
+     td.mean()
+     td.quantile(.1)
 
-         td.fillna(0)
-         td.fillna(timedelta(days=1,seconds=5))
+- ``plot(kind='kde')`` now accepts the optional parameters ``bw_method`` and
+  ``ind``, passed to scipy.stats.gaussian_kde() (for scipy >= 0.11.0) to set
+  the bandwidth, and to gkde.evaluate() to specify the indicies at which it
+  is evaluated, respecttively. See scipy docs. (:issue:`4298`)
 
-    - You can do numeric reduction operations on timedeltas. Note that these will return
-      a single-element Series.
+- DataFrame constructor now accepts a numpy masked record array (:issue:`3478`)
 
-      .. ipython:: python
+- The new vectorized string method ``extract`` return regular expression
+  matches more conveniently.
 
-         td.mean()
-         td.quantile(.1)
+  .. ipython:: python
 
-  - ``plot(kind='kde')`` now accepts the optional parameters ``bw_method`` and
-    ``ind``, passed to scipy.stats.gaussian_kde() (for scipy >= 0.11.0) to set
-    the bandwidth, and to gkde.evaluate() to specify the indicies at which it
-    is evaluated, respecttively. See scipy docs. (:issue:`4298`)
-  - DataFrame constructor now accepts a numpy masked record array (:issue:`3478`)
-  - The new vectorized string method ``extract`` return regular expression
-    matches more conveniently.
+     Series(['a1', 'b2', 'c3']).str.extract('[ab](\d)')
 
-    .. ipython:: python
+  Elements that do not match return ``NaN``. Extracting a regular expression
+  with more than one group returns a DataFrame with one column per group.
 
-       Series(['a1', 'b2', 'c3']).str.extract('[ab](\d)')
 
-    Elements that do not match return ``NaN``. Extracting a regular expression
-    with more than one group returns a DataFrame with one column per group.
+  .. ipython:: python
 
+     Series(['a1', 'b2', 'c3']).str.extract('([ab])(\d)')
 
-    .. ipython:: python
+  Elements that do not match return a row of ``NaN``.
+  Thus, a Series of messy strings can be *converted* into a
+  like-indexed Series or DataFrame of cleaned-up or more useful strings,
+  without necessitating ``get()`` to access tuples or ``re.match`` objects.
 
-       Series(['a1', 'b2', 'c3']).str.extract('([ab])(\d)')
+  Named groups like
 
-    Elements that do not match return a row of ``NaN``.
-    Thus, a Series of messy strings can be *converted* into a
-    like-indexed Series or DataFrame of cleaned-up or more useful strings,
-    without necessitating ``get()`` to access tuples or ``re.match`` objects.
+  .. ipython:: python
 
-    Named groups like
+     Series(['a1', 'b2', 'c3']).str.extract(
+             '(?P<letter>[ab])(?P<digit>\d)')
 
-    .. ipython:: python
+  and optional groups can also be used.
 
-       Series(['a1', 'b2', 'c3']).str.extract(
-               '(?P<letter>[ab])(?P<digit>\d)')
+  .. ipython:: python
 
-    and optional groups can also be used.
+      Series(['a1', 'b2', '3']).str.extract(
+              '(?P<letter>[ab])?(?P<digit>\d)')
 
-    .. ipython:: python
+- ``read_stata`` now accepts Stata 13 format (:issue:`4291`)
 
-        Series(['a1', 'b2', '3']).str.extract(
-                '(?P<letter>[ab])?(?P<digit>\d)')
+- ``read_fwf`` now infers the column specifications from the first 100 rows of
+  the file if the data has correctly separated and properly aligned columns
+  using the delimiter provided to the function (:issue:`4488`).
 
-  - ``read_stata`` now accepts Stata 13 format (:issue:`4291`)
-  - ``read_fwf`` now infers the column specifications from the first 100 rows of
-    the file if the data has correctly separated and properly aligned columns
-    using the delimiter provided to the function (:issue:`4488`).
+- support for nanosecond times as an offset
 
-  - support for nanosecond times in periods
+  .. warning::
 
-    .. warning::
+     These operations require ``numpy >= 1.7``
 
-       These operations require ``numpy >= 1.7``
+  Period conversions in the range of seconds and below were reworked and extended
+  up to nanoseconds. Periods in the nanosecond range are now available.
 
-    Period conversions in the range of seconds and below were reworked and extended
-    up to nanoseconds. Periods in the nanosecond range are now available.
+  .. ipython:: python
 
-      .. ipython:: python
+     date_range('2013-01-01', periods=5, freq='5N')
 
-        date_range('2013-01-01', periods=5, freq='5N')
+  or with frequency as offset
 
-    or with frequency as offset
+  .. ipython:: python
 
-      .. ipython:: python
+     date_range('2013-01-01', periods=5, freq=pd.offsets.Nano(5))
 
-        date_range('2013-01-01', periods=5, freq=pd.offsets.Nano(5))
+  Timestamps can be modified in the nanosecond range
 
-    Timestamps can be modified in the nanosecond range
+  .. ipython:: python
 
-      .. ipython:: python
+     t = Timestamp('20130101 09:01:02')
+     t + pd.datetools.Nano(123)
 
-        t = Timestamp('20130101 09:01:02')
-        t + pd.datetools.Nano(123)
+- A new method, ``isin`` for DataFrames, plays nicely with boolean indexing. To get the rows where any of the conditions are met:
 
-  - The ``isin`` method plays nicely with boolean indexing. To get the rows where any of the conditions are met:
+  .. ipython:: python
 
-      .. ipython:: python
+     dfi = DataFrame({'A': [1, 2, 3, 4], 'B': ['a', 'b', 'f', 'n']})
+     dfi
+     mask = dfi.isin({'A': [1, 2], 'B': ['e', 'f']})
+     mask
+     dfi[mask.any(1)]
 
-         dfi = DataFrame({'A': [1, 2, 3, 4], 'B': ['a', 'b', 'f', 'n']})
-         dfi
-         mask = dfi.isin({'A': [1, 2], 'B': ['e', 'f']})
-         mask
-         dfi[mask.any(1)]
+    :ref:`See the docs<indexing.basics.indexing_isin>` for more.
 
-      :ref:`See the docs<indexing.basics.indexing_isin>` for more.
-  - All R datasets listed here http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html can now be loaded into Pandas objects
+- All R datasets listed here http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html can now be loaded into Pandas objects
 
-    .. code-block:: python
+  .. code-block:: python
 
-       import pandas.rpy.common as com
-       com.load_data('Titanic')
+     import pandas.rpy.common as com
+     com.load_data('Titanic')
 
-  - ``tz_localize`` can infer a fall daylight savings transition based on the structure
-    of the unlocalized data (:issue:`4230`), see :ref:`here<timeseries.timezone>`
-  - DatetimeIndex is now in the API documentation, see :ref:`here<api.datetimeindex>`
-  - :meth:`~pandas.io.json.json_normalize` is a new method to allow you to create a flat table
-    from semi-structured JSON data. :ref:`See the docs<io.json_normalize>` (:issue:`1067`)
+- ``tz_localize`` can infer a fall daylight savings transition based on the structure
+  of the unlocalized data (:issue:`4230`), see :ref:`here<timeseries.timezone>`
 
-.. _whatsnew_0130.experimental:
+- DatetimeIndex is now in the API documentation, see :ref:`here<api.datetimeindex>`
 
-Experimental
-~~~~~~~~~~~~
+- :meth:`~pandas.io.json.json_normalize` is a new method to allow you to create a flat table
+  from semi-structured JSON data. :ref:`See the docs<io.json_normalize>` (:issue:`1067`)
+
+
+- Added PySide support for the qtpandas DataFrameModel and DataFrameWidget.
+
+- DataFrame has a new ``interpolate`` method, similar to Series (:issue:`4434`, :issue:`1892`)
+
+  .. ipython:: python
+
+      df = DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
+                      'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]})
+      df.interpolate()
 
-- :func:`~pandas.eval`:
+  Additionally, the ``method`` argument to ``interpolate`` has been expanded
+  to include 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
+  'barycentric', 'krogh', 'piecewise_polynomial', 'pchip' or "polynomial" or 'spline'
+  and an integer representing the degree or order of the approximation.  The new methods
+  require scipy_. Consult the Scipy reference guide_ and documentation_ for more information
+  about when the various methods are appropriate.  See :ref:`the docs<missing_data.interpolate>`.
 
-  - The new :func:`~pandas.eval` function implements expression evaluation using
-    ``numexpr`` behind the scenes. This results in large speedups for
-    complicated expressions involving large DataFrames/Series. For example,
+  Interpolate now also accepts a ``limit`` keyword argument.
+  This works similar to ``fillna``'s limit:
 
-      .. ipython:: python
+  .. ipython:: python
 
-         nrows, ncols = 20000, 100
-         df1, df2, df3, df4 = [DataFrame(randn(nrows, ncols))
-                               for _ in xrange(4)]
+    ser = Series([1, 3, np.nan, np.nan, np.nan, 11])
+    ser.interpolate(limit=2)
 
-      .. ipython:: python
+.. _scipy: http://www.scipy.org
+.. _documentation: http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation
+.. _guide: http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html
 
-         # eval with NumExpr backend
-         %timeit pd.eval('df1 + df2 + df3 + df4')
+.. _whatsnew_0130.experimental:
 
-      .. ipython:: python
+Experimental
+~~~~~~~~~~~~
 
-         # pure Python evaluation
-         %timeit df1 + df2 + df3 + df4
+- The new :func:`~pandas.eval` function implements expression evaluation using
+  ``numexpr`` behind the scenes. This results in large speedups for
+  complicated expressions involving large DataFrames/Series. For example,
+
+  .. ipython:: python
+
+     nrows, ncols = 20000, 100
+     df1, df2, df3, df4 = [DataFrame(randn(nrows, ncols))
+                           for _ in xrange(4)]
+
+  .. ipython:: python
 
-    For more details, see the :ref:`enhancing performance documentation on eval
-    <enhancingperf.eval>`
+     # eval with NumExpr backend
+     %timeit pd.eval('df1 + df2 + df3 + df4')
 
-- ``DataFrame.eval``
+  .. ipython:: python
 
-  - Similar to ``pandas.eval``, :class:`~pandas.DataFrame` has a new
-    ``DataFrame.eval`` method that evaluates an expression in the context of
-    the ``DataFrame``. For example,
+     # pure Python evaluation
+     %timeit df1 + df2 + df3 + df4
 
-      .. ipython:: python
-         :suppress:
+  For more details, see the :ref:`enhancing performance documentation on eval
+  <enhancingperf.eval>`
 
-         try:
-            del a
-         except NameError:
-            pass
+- Similar to ``pandas.eval``, :class:`~pandas.DataFrame` has a new
+  ``DataFrame.eval`` method that evaluates an expression in the context of
+  the ``DataFrame``. For example,
 
-         try:
-            del b
-         except NameError:
-            pass
+  .. ipython:: python
+     :suppress:
 
-      .. ipython:: python
+     try:
+        del a
+     except NameError:
+        pass
 
-         df = DataFrame(randn(10, 2), columns=['a', 'b'])
-         df.eval('a + b')
+     try:
+        del b
+     except NameError:
+        pass
 
+  .. ipython:: python
 
-- :meth:`~pandas.DataFrame.query`
+     df = DataFrame(randn(10, 2), columns=['a', 'b'])
+     df.eval('a + b')
 
-  - In 0.13 a :meth:`~pandas.DataFrame.query` method has been added that allows
-    you to select elements of a ``DataFrame`` using a natural query syntax
-    nearly identical to Python syntax. For example,
+- :meth:`~pandas.DataFrame.query` method has been added that allows
+  you to select elements of a ``DataFrame`` using a natural query syntax
+  nearly identical to Python syntax. For example,
 
-      .. ipython:: python
-         :suppress:
+  .. ipython:: python
+     :suppress:
 
-         try:
-            del a
-         except NameError:
-            pass
+     try:
+        del a
+     except NameError:
+        pass
 
-         try:
-            del b
-         except NameError:
-            pass
+     try:
+        del b
+     except NameError:
+        pass
 
-         try:
-            del c
-         except NameError:
-            pass
+     try:
+        del c
+     except NameError:
+        pass
 
-      .. ipython:: python
+  .. ipython:: python
 
-         n = 20
-         df = DataFrame(np.random.randint(n, size=(n, 3)), columns=['a', 'b', 'c'])
-         df.query('a < b < c')
+     n = 20
+     df = DataFrame(np.random.randint(n, size=(n, 3)), columns=['a', 'b', 'c'])
+     df.query('a < b < c')
 
-    selects all the rows of ``df`` where ``a < b < c`` evaluates to ``True``.
-    For more details see the :ref:`indexing documentation on query
-    <indexing.query>`.
+  selects all the rows of ``df`` where ``a < b < c`` evaluates to ``True``.
+  For more details see the :ref:`indexing documentation on query
+  <indexing.query>`.
 
 - ``pd.read_msgpack()`` and ``pd.to_msgpack()`` are now a supported method of serialization
   of arbitrary pandas (and python objects) in a lightweight portable binary format. :ref:`See the docs<io.msgpack>`
@@ -612,36 +649,6 @@ Experimental
 
      os.remove('foo.msg')
 
-- Added PySide support for the qtpandas DataFrameModel and DataFrameWidget.
-
-- DataFrame has a new ``interpolate`` method, similar to Series (:issue:`4434`, :issue:`1892`)
-
-  .. ipython:: python
-
-      df = DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
-                      'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]})
-      df.interpolate()
-
-  Additionally, the ``method`` argument to ``interpolate`` has been expanded
-  to include 'nearest', 'zero', 'slinear', 'quadratic', 'cubic',
-  'barycentric', 'krogh', 'piecewise_polynomial', 'pchip' or "polynomial" or 'spline'
-  and an integer representing the degree or order of the approximation.  The new methods
-  require scipy_. Consult the Scipy reference guide_ and documentation_ for more information
-  about when the various methods are appropriate.  See also the :ref:`pandas interpolation docs<missing_data.interpolate:>`.
-
-  Interpolate now also accepts a ``limit`` keyword argument.
-  This works similar to ``fillna``'s limit:
-
-  .. ipython:: python
-
-    ser = Series([1, 3, np.nan, np.nan, np.nan, 11])
-    ser.interpolate(limit=2)
-
-.. _scipy: http://www.scipy.org
-.. _documentation: http://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation
-.. _guide: http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html
-
-
 .. _whatsnew_0130.refactoring:
 
 Internal Refactoring
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index cdac4939e..3409fdf00 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1966,7 +1966,8 @@ class NDFrame(PandasObject):
 
     def interpolate(self, method='linear', axis=0, limit=None, inplace=False,
                     downcast='infer', **kwargs):
-        """Interpolate values according to different methods.
+        """
+        Interpolate values according to different methods.
 
         Parameters
         ----------
@@ -2013,7 +2014,9 @@ class NDFrame(PandasObject):
         2    2
         3    3
         dtype: float64
+
         """
+
         if self.ndim > 2:
             raise NotImplementedError("Interpolate has not been implemented "
                                       "on Panel and Panel 4D objects.")
