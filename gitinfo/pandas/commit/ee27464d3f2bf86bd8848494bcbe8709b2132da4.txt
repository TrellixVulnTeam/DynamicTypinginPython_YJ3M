commit ee27464d3f2bf86bd8848494bcbe8709b2132da4
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Thu Aug 25 18:44:26 2011 -0400

    DOC: ipython directive from skipper's repo

diff --git a/doc/source/conf.py b/doc/source/conf.py
index e79a34ad3..f6c6c4bd3 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -35,6 +35,8 @@ sys.path.extend([
 extensions = ['sphinx.ext.autodoc',
               'sphinx.ext.doctest',
               'numpydoc',
+              'ipython_directive',
+              'ipython_console_highlighting',
               'sphinx.ext.intersphinx',
               'sphinx.ext.todo',
               'sphinx.ext.coverage',
diff --git a/doc/source/dataframe.rst b/doc/source/dataframe.rst
index 8d6871829..6ed457f54 100644
--- a/doc/source/dataframe.rst
+++ b/doc/source/dataframe.rst
@@ -43,67 +43,29 @@ Basics
 The canonical DataFrame containing time series data takes this form,
 which will be used for many examples to follow:
 
-::
+.. ipython:: python
 
-    >>> from pandas import *; from numpy.random import randn
-
-    >>> index = DateRange('1/1/2009', '12/1/2009', timeRule='EOM')
-    >>> N = len(index)
-    >>> data = {
-	'A' : randn(N),
-	'B' : randn(N),
-	'C' : randn(N),
-    }
-
-    >>> df = DataFrame(data, index=index)
-
-    >>> print df
-
-			   A              B              C
-    2009-01-30 00:00:00    -0.367014      1.29942        1.40773
-    2009-02-27 00:00:00    0.347326       0.651661       0.143376
-    2009-03-31 00:00:00    -0.677813      0.40488        -0.463113
-    2009-04-30 00:00:00    0.125062       1.09505        -1.03278
-    2009-05-29 00:00:00    0.979307       0.149356       0.708128
-    2009-06-30 00:00:00    -1.24432       0.420788       -1.01815
-    2009-07-31 00:00:00    0.536261       -0.276357      -0.227469
-    2009-08-31 00:00:00    0.0603968      -1.42112       -0.271767
-    2009-09-30 00:00:00    0.537841       -0.361833      -0.0488729
-    2009-10-30 00:00:00    -0.335999      2.54742        -0.878263
-    2009-11-30 00:00:00    -0.568216      -0.557347      -1.58623
+    from pandas import *; from numpy.random import randn
+    index = Index(DateRange('1/1/2009', '12/1/2009', timeRule='EOM'))
+    N = len(index)
+    data = {'A' : randn(N), 'B' : randn(N), 'C' : randn(N)}
+    df = DataFrame(data, index=index)
+    df
 
 The **info** method provides a summary of a DataFrame object and will
 be printed by default when the frame is very large:
 
-::
+.. ipython:: python
 
-    >>> df.info()
-    Index: 11 entries, 2009-01-30 00:00:00 to 2009-11-30 00:00:00
-    Columns:
-    A    11  non-null values
-    B    11  non-null values
-    C    11  non-null values
+    df.info()
 
 The DataFrame's index and columns can be accessed by the **index**
-attribute and **cols** method, respectively:
-
-::
-
-    >>> df.index
-    Index([2009-01-30 00:00:00, 2009-02-27 00:00:00, 2009-03-31 00:00:00,
-	   2009-04-30 00:00:00, 2009-05-29 00:00:00, 2009-06-30 00:00:00,
-	   2009-07-31 00:00:00, 2009-08-31 00:00:00, 2009-09-30 00:00:00,
-	   2009-10-30 00:00:00, 2009-11-30 00:00:00], dtype=object)
+attribute and **columns** methods, respectively:
 
-    >>> df.cols()
-    ['A', 'B', 'C']
+.. ipython:: python
 
-
-.. autosummary::
-   :toctree: generated/
-
-   DataFrame.cols
-   DataFrame.info
+    df.index
+    df.columns
 
 .. _dataframe.cons:
 
@@ -123,148 +85,85 @@ There are many ways to create a DataFrame:
    DataFrame.__init__
    DataFrame.fromRecords
 
-Indexing
---------
+Indexing basics
+---------------
 
-.. note::
+.. seealso:: :ref:`Indexing (main documentation) <indexing>`
 
-    I have eschewed "cute" indexing schemes in the interest of being
-    explicit and maintaining DataFrame's status as a "dict of
-    Series". However, it is always desirable to keep the interface
-    simple and intuitive.
+Column access
+~~~~~~~~~~~~~
 
-DataFrame's basic indexing accesses the **columns** by name, producing
-Series:
+DataFrame's basic *__getitem__* (brackets) accesses the **columns** by name,
+result in a Series
 
-::
+.. ipython:: python
 
-    >>> df['A']
-    2009-01-30 00:00:00    -0.367013536107
-    2009-02-27 00:00:00    0.347325830717
-    2009-03-31 00:00:00    -0.677812757268
-    2009-04-30 00:00:00    0.125061634713
-    2009-05-29 00:00:00    0.979307492892
-    2009-06-30 00:00:00    -1.2443243316
-    2009-07-31 00:00:00    0.536260924391
-    2009-08-31 00:00:00    0.060396849998
-    2009-09-30 00:00:00    0.53784064627
-    2009-10-30 00:00:00    -0.335999254912
-    2009-11-30 00:00:00    -0.568216482894
+    df['A']
 
 
 If you add a Series to the frame, it will be automatically conformed
 to the frame's index:
 
-::
+.. ipython:: python
 
-    >>> df['D'] = df['A'][:5]
-    >>> df
-    			   A              B              C              D
-    2009-01-30 00:00:00    -0.367014      1.29942        1.40773        -0.367014
-    2009-02-27 00:00:00    0.347326       0.651661       0.143376       0.347326
-    2009-03-31 00:00:00    -0.677813      0.40488        -0.463113      -0.677813
-    2009-04-30 00:00:00    0.125062       1.09505        -1.03278       0.125062
-    2009-05-29 00:00:00    0.979307       0.149356       0.708128       0.979307
-    2009-06-30 00:00:00    -1.24432       0.420788       -1.01815       nan
-    2009-07-31 00:00:00    0.536261       -0.276357      -0.227469      nan
-    2009-08-31 00:00:00    0.0603968      -1.42112       -0.271767      nan
-    2009-09-30 00:00:00    0.537841       -0.361833      -0.0488729     nan
-    2009-10-30 00:00:00    -0.335999      2.54742        -0.878263      nan
-    2009-11-30 00:00:00    -0.568216      -0.557347      -1.58623       nan
+    df['D'] = df['A'][:5]
+    df
 
 Columns can be deleted or popped as with a dict:
 
-::
-
-    >>> del df['C']
-    >>> B = df.pop('B')
-    >>> df.info()
-    Index: 11 entries, 2009-01-30 00:00:00 to 2009-11-30 00:00:00
-    Columns:
-    A    11  non-null values
-    D    5  non-null values
+.. ipython:: python
 
+    df
+    del df['C']
+    B = df.pop('B')
+    df
 
 New items in the DataFrame do not need to already be Series. They can
 also be an ndarray of the right length or a scalar value:
 
-::
+.. ipython:: python
 
-    >>> df['N'] = np.arange(len(df))
-    >>> df['S'] = 5
-			   A              D              N              S
-    2009-01-30 00:00:00    -0.367014      -0.367014      0              5
-    2009-02-27 00:00:00    0.347326       0.347326       1              5
-    2009-03-31 00:00:00    -0.677813      -0.677813      2              5
-    2009-04-30 00:00:00    0.125062       0.125062       3              5
-    2009-05-29 00:00:00    0.979307       0.979307       4              5
-    2009-06-30 00:00:00    -1.24432       nan            5              5
-    2009-07-31 00:00:00    0.536261       nan            6              5
-    2009-08-31 00:00:00    0.0603968      nan            7              5
-    2009-09-30 00:00:00    0.537841       nan            8              5
-    2009-10-30 00:00:00    -0.335999      nan            9              5
-    2009-11-30 00:00:00    -0.568216      nan            10             5
-
-To be consistent with this dict-like interface, the *__contains__*
-method considers the columns:
+    df['N'] = np.arange(len(df))
+    df['S'] = 5
+    df
 
-::
+To be consistent with this dict-like interface, using **in** checks if the key
+is in the columns:
 
-    >>> 'A' in df
+.. ipython:: python
+
+    'A' in df
     True
 
 .. autosummary::
    :toctree: generated/
 
-   DataFrame.__contains__
-   DataFrame.__getitem__
-   DataFrame.__delitem__
    DataFrame.pop
 
-Retrieving cross sections, transposing
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+Selecting rows (cross-sections)
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-It is often desirable to retrieve all data associated with a
-particular index values (we have been calling this a *cross
-section*). Rather than use *__getitem__* and extra notation to do
-this, DataFrame has the **xs** method:
+Selecting a row can be done using the **xs** method or via fancy indexing (see the
+`main documentation <indexing>` about fancy indexing):
 
-::
 
-    >>> df.xs(datetime(2009, 8, 31))
-    A    0.060396849998
-    D    nan
-    N    7
-    S    5
+.. ipython:: python
 
-If the cross sections in a data set are of the most interest, it is
-also possible to transpose a DataFrame using the same syntax as an
-ndarray:
+    date = datetime(2009, 8, 31)
+    df.xs(date)
+    df.ix[date]
+    df.ix[10]
 
-::
 
-    >>> dftrans = df.T
-    >>> dftrans
-    <class 'pandas.core.frame.DataFrame'>
-    Index: 4 entries, A to S
-    Columns:
-    2009-01-30 00:00:00    4  non-null values
-    2009-02-27 00:00:00    4  non-null values
-    2009-03-31 00:00:00    4  non-null values
-    2009-04-30 00:00:00    4  non-null values
-    2009-05-29 00:00:00    4  non-null values
-    2009-06-30 00:00:00    3  non-null values
-    2009-07-31 00:00:00    3  non-null values
-    2009-08-31 00:00:00    3  non-null values
-    2009-09-30 00:00:00    3  non-null values
-    2009-10-30 00:00:00    3  non-null values
-    2009-11-30 00:00:00    3  non-null values
-
-    >>> dftrans[datetime(2009, 9, 30)]
-    A    0.53784064627
-    D    nan
-    N    8
-    S    5
+Transposing
+~~~~~~~~~~~
+
+To transpose, access the **T** attribute, similar to an ndarray
+
+.. ipython:: python
+
+    df.T
+    df.T[date]
 
 Slicing ranges
 ~~~~~~~~~~~~~~
@@ -272,52 +171,25 @@ Slicing ranges
 Similar to Python lists and ndarrays, for convenience DataFrame
 supports slicing:
 
-::
+.. ipython:: python
+
+    df[:2]
+    df[::-1]
+    df[-3:].T
+
+I do not recommend making heavy use of this functionality but rather using it
+as a convenience for interactive programming (useful for seeing the "head" or
+"tail" of a large DataFrame as in the last example).
 
-    >>> df[:2]
-			   A              D              N              S
-    2009-01-30 00:00:00    -0.367014      -0.367014      0              5
-    2009-02-27 00:00:00    0.347326       0.347326       1              5
-
-    >>> df[::-1]
-			   A              D              N              S
-    2009-11-30 00:00:00    -0.568216      nan            10             5
-    2009-10-30 00:00:00    -0.335999      nan            9              5
-    2009-09-30 00:00:00    0.537841       nan            8              5
-    2009-08-31 00:00:00    0.0603968      nan            7              5
-    2009-07-31 00:00:00    0.536261       nan            6              5
-    2009-06-30 00:00:00    -1.24432       nan            5              5
-    2009-05-29 00:00:00    0.979307       0.979307       4              5
-    2009-04-30 00:00:00    0.125062       0.125062       3              5
-    2009-03-31 00:00:00    -0.677813      -0.677813      2              5
-    2009-02-27 00:00:00    0.347326       0.347326       1              5
-    2009-01-30 00:00:00    -0.367014      -0.367014      0              5
-
-    >>> df[-3:].T
-	 2009-09-30     2009-10-30     2009-11-30
-    A    0.537841       -0.335999      -0.568216
-    D    nan            nan            nan
-    N    8              9              10
-    S    5              5              5
-
-I do not recommend making heavy use of this functionality but rather
-using it as a convenience for interactive programming (useful for
-seeing the "head" or "tail" of a large DataFrame as in the last
-example).
-
-Boolean
-~~~~~~~
+Boolean indexing
+~~~~~~~~~~~~~~~~
 
 As another indexing convenience, it is possible to use boolean
 indexing to select rows of a DataFrame:
 
-::
+.. ipython:: python
 
-    >>> df[df['A'] > 0.5]
-			   A              D              N              S
-    2009-05-29 00:00:00    0.979307       0.979307       4              5
-    2009-07-31 00:00:00    0.536261       nan            6              5
-    2009-09-30 00:00:00    0.537841       nan            8              5
+    df[df['A'] > 0.5]
 
 As we will see later on, the same operation could be accomplished by
 reindexing. However, the syntax would be more verbose; hence, the
@@ -342,60 +214,23 @@ for two values to be combined. If there is no match for a particular
 illustrate, let's return to a similar example from the beginning of
 the tutorial:
 
-::
+.. ipython:: python
 
-    >>> df
-			   A              B              C
-    2009-01-30 00:00:00    -0.173487      -0.330054      2.45767
-    2009-02-27 00:00:00    -1.70517       -1.34422       0.45781
-    2009-03-31 00:00:00    0.517951       0.437294       0.625021
-    2009-04-30 00:00:00    1.13914        0.976763       0.871074
-    2009-05-29 00:00:00    -0.263249      -1.55445       0.386744
-    2009-06-30 00:00:00    0.994217       -0.15012       -0.444482
-    2009-07-31 00:00:00    1.51264        -1.13902       0.846015
-    2009-08-31 00:00:00    0.323804       -0.793455      -1.97154
-    2009-09-30 00:00:00    -0.0450052     0.404083       0.588554
-    2009-10-30 00:00:00    0.268981       -0.20756       -0.328061
-    2009-11-30 00:00:00    0.471714       -0.0450022     -0.280202
-
-    >>> df + df[:7]
-			   A              B              C
-    2009-01-30 00:00:00    -0.346974      -0.660108      4.91534
-    2009-02-27 00:00:00    -3.41035       -2.68845       0.915621
-    2009-03-31 00:00:00    1.0359         0.874588       1.25004
-    2009-04-30 00:00:00    2.27828        1.95353        1.74215
-    2009-05-29 00:00:00    -0.526498      -3.10889       0.773488
-    2009-06-30 00:00:00    1.98843        -0.30024       -0.888965
-    2009-07-31 00:00:00    3.02528        -2.27803       1.69203
-    2009-08-31 00:00:00    nan            nan            nan
-    2009-09-30 00:00:00    nan            nan            nan
-    2009-10-30 00:00:00    nan            nan            nan
-    2009-11-30 00:00:00    nan            nan            nan
+    df = DataFrame(data, index=index)
+    df
+    df + df[:7]
 
 In this first example, we can see that the indices have been combined
 together, and the portion where dates are missing in one of the frames
 has resulted in all NaN values. The resulting columns will also be the
 union of the frames' columns:
 
-::
+.. ipython:: python
 
-    >>> df2 = df.copy()
-    >>> df2['D'] = 5
-    >>> del df2['A']
-
-    >>> df + df2[::2]
-			   A              B              C              D
-    2009-01-30 00:00:00    nan            -0.660108      4.91534        nan
-    2009-02-27 00:00:00    nan            nan            nan            nan
-    2009-03-31 00:00:00    nan            0.874588       1.25004        nan
-    2009-04-30 00:00:00    nan            nan            nan            nan
-    2009-05-29 00:00:00    nan            -3.10889       0.773488       nan
-    2009-06-30 00:00:00    nan            nan            nan            nan
-    2009-07-31 00:00:00    nan            -2.27803       1.69203        nan
-    2009-08-31 00:00:00    nan            nan            nan            nan
-    2009-09-30 00:00:00    nan            0.808167       1.17711        nan
-    2009-10-30 00:00:00    nan            nan            nan            nan
-    2009-11-30 00:00:00    nan            -0.0900043     -0.560404      nan
+    df2 = df.copy()
+    df2['D'] = 5
+    del df2['A']
+    df + df2[::2]
 
 Here, neither **A** nor **D** was in both frames: they appear in the
 result but are all NaN. An argument could be made to exclude these
@@ -414,7 +249,7 @@ index.
 ::
 
     >>> df - df.xs(df.index[5])
-			   A              B              C
+               A              B              C
     2009-01-30 00:00:00    -1.1677        -0.179934      2.90215
     2009-02-27 00:00:00    -2.69939       -1.1941        0.902293
     2009-03-31 00:00:00    -0.476266      0.587414       1.0695
@@ -436,7 +271,7 @@ user intended.
 ::
 
     >>> df - df['A']
-			   A              B              C
+               A              B              C
     2009-01-30 00:00:00    0              -0.156567      2.63116
     2009-02-27 00:00:00    0              0.360951       2.16298
     2009-03-31 00:00:00    0              -0.0806571     0.10707
@@ -466,7 +301,7 @@ Scalar operations work just as expected:
 ::
 
     >>> df * 5 + 2
-			   A              B              C
+               A              B              C
     2009-01-30 00:00:00    1.13256        0.349729       14.2884
     2009-02-27 00:00:00    -6.52587       -4.72111       4.28905
     2009-03-31 00:00:00    4.58976        4.18647        5.12511
@@ -503,7 +338,7 @@ DataFrame. By default the statistic will be computed for each column
 ::
 
     >>> df
-			   A              B              C              D
+               A              B              C              D
     2009-01-30 00:00:00    -0.173487      -0.330054      2.45767        -0.173487
     2009-02-27 00:00:00    -1.70517       -1.34422       0.45781        -1.70517
     2009-03-31 00:00:00    0.517951       0.437294       0.625021       0.517951
@@ -517,23 +352,23 @@ DataFrame. By default the statistic will be computed for each column
     2009-11-30 00:00:00    0.471714       -0.0450022     -0.280202      nan
 
     >>> df.mean()
-    A	0.27650301895
-    B	-0.340521353823
-    C	0.291691664865
-    D	-0.0969634747505
+    A   0.27650301895
+    B   -0.340521353823
+    C   0.291691664865
+    D   -0.0969634747505
 
     >>> df.mean(axis=1)
-    2009-01-30 00:00:00	0.445160910434
-    2009-02-27 00:00:00	-1.07419006997
-    2009-03-31 00:00:00	0.524554413383
-    2009-04-30 00:00:00	1.03152984415
-    2009-05-29 00:00:00	-0.42354987732
-    2009-06-30 00:00:00	0.133204894302
-    2009-07-31 00:00:00	0.406546724596
-    2009-08-31 00:00:00	-0.813729414124
-    2009-09-30 00:00:00	0.315877288659
-    2009-10-30 00:00:00	-0.088879865383
-    2009-11-30 00:00:00	0.048836496438
+    2009-01-30 00:00:00 0.445160910434
+    2009-02-27 00:00:00 -1.07419006997
+    2009-03-31 00:00:00 0.524554413383
+    2009-04-30 00:00:00 1.03152984415
+    2009-05-29 00:00:00 -0.42354987732
+    2009-06-30 00:00:00 0.133204894302
+    2009-07-31 00:00:00 0.406546724596
+    2009-08-31 00:00:00 -0.813729414124
+    2009-09-30 00:00:00 0.315877288659
+    2009-10-30 00:00:00 -0.088879865383
+    2009-11-30 00:00:00 0.048836496438
 
 The other methods listed function similarly. Combining these methods
 with the arithmetic functionality, we can very easily do things like
@@ -542,13 +377,13 @@ computing the cross-sectional or time series z-score:
 ::
 
     >>> (df - df.mean(1)) / df.std(1)    # cross-sectional
-			   A              B              C              D
+               A              B              C              D
     2009-01-30 00:00:00    -0.839662      0.539768       1.13956        -0.839662
     2009-02-27 00:00:00    -0.615664      1.47701        -0.245682      -0.615664
     ...
 
     >>> (df - df.mean()) / df.std()      # time series
-			   A              B              C              D
+               A              B              C              D
     2009-01-30 00:00:00    -1.79314       -0.173077      0.156054       -0.96237
     2009-02-27 00:00:00    -1.16502       1.7612         -1.15894       -0.437405
     ...
@@ -578,7 +413,7 @@ taking care to compute the variances over the intersection of data:
 ::
 
     >>> df.corr()
-	 A              B              C              D
+     A              B              C              D
     A    1              0.423766       -0.0985818     1
     B    0.423766       1              0.134803       0.839771
     C    -0.0985818     0.134803       1              0.129427
@@ -606,7 +441,7 @@ return type of the function, return a Series or DataFrame.
 ::
 
     >>> df.apply(np.log)
-			   A              B              C
+               A              B              C
     2009-01-30 00:00:00    nan            nan            -0.573389
     2009-02-27 00:00:00    nan            -0.0123289     nan
     2009-03-31 00:00:00    0.0324552      -0.982897      -1.85985
@@ -620,9 +455,9 @@ return type of the function, return a Series or DataFrame.
     2009-11-30 00:00:00    -2.08095       nan            nan
 
     >>> df.apply(lambda x: np.sort(x)[-5:].mean())
-    A	0.517625676559
-    B	0.47682898773
-    C	1.2079285542
+    A   0.517625676559
+    B   0.47682898773
+    C   1.2079285542
 
     >>> df.apply(np.sum, axis=1)
     2009-01-30 00:00:00    -1.35501374903
@@ -675,7 +510,7 @@ function. Obviously this will be fairly slow but can be useful:
 ::
 
     >>> df.applymap(lambda x: x if x > 0 else 0)
-			   A              B              C              D
+               A              B              C              D
     2009-01-30 00:00:00    0              0              0.563612       0
     2009-02-27 00:00:00    0              0.987747       0              0
     2009-03-31 00:00:00    1.03299        0.374225       0.155696       1.03299
@@ -780,7 +615,7 @@ arguments and
 ::
 
     >>> df1
-			   A              B
+               A              B
     2000-01-03 00:00:00    -0.1174        -0.941
     2000-01-04 00:00:00    -0.6034        -0.008094
     2000-01-05 00:00:00    -0.3816        -0.9338
@@ -793,7 +628,7 @@ arguments and
     2000-01-14 00:00:00    -1.084         -0.271
 
     >>> df2
-			   C              D
+               C              D
     2000-01-03 00:00:00    0.2833         -0.1937
     2000-01-05 00:00:00    1.868          1.207
     2000-01-07 00:00:00    -0.8586        -0.7367
@@ -802,7 +637,7 @@ arguments and
 
 
     df1.join(df2)
-			   A              B              C              D
+               A              B              C              D
     2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
     2000-01-04 00:00:00    -0.6034        -0.008094      NaN            NaN
     2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
@@ -817,7 +652,7 @@ arguments and
 ::
 
     >>> df1.join(df2, how='inner')
-			   A              B              C              D
+               A              B              C              D
     2000-01-03 00:00:00    -0.1174        -0.941         0.2833         -0.1937
     2000-01-05 00:00:00    -0.3816        -0.9338        1.868          1.207
     2000-01-07 00:00:00    0.9576         0.4652         -0.8586        -0.7367
@@ -832,7 +667,7 @@ mapping.
 ::
 
     >>> df2
-			   C              D              key
+               C              D              key
     2000-01-03 00:00:00    0.2833         -0.1937        0
     2000-01-05 00:00:00    1.868          1.207          1
     2000-01-07 00:00:00    -0.8586        -0.7367        0
@@ -840,12 +675,12 @@ mapping.
     2000-01-13 00:00:00    0.7856         0.9063         0
 
     >>> df3
-	 code
+     code
     0    foo
     1    bar
 
     >>> df2.join(df3, on='key')
-			   C              D              code           key
+               C              D              code           key
     2000-01-03 00:00:00    0.2833         -0.1937        foo            0
     2000-01-05 00:00:00    1.868          1.207          bar            1
     2000-01-07 00:00:00    -0.8586        -0.7367        foo            0
diff --git a/doc/source/hierarchical.rst b/doc/source/hierarchical.rst
deleted file mode 100644
index be15965be..000000000
--- a/doc/source/hierarchical.rst
+++ /dev/null
@@ -1,8 +0,0 @@
-.. _hierarchical:
-
-.. currentmodule:: pandas
-
-*********************
-Hierarchical indexing
-*********************
-
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 22feb773f..2ded7c4d6 100755
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -9,7 +9,7 @@ flexible, and expressive data structures designed to make working with
 "relational" or "labeled" data both easy and intuitive. It aims to be the
 fundamental high-level building block for doing practical data analysis in
 Python. Additionally, it has the broader goal of becoming **the most powerful
-and flexible data analysis and manipulation tool available in any
+and flexible open source data analysis / manipulation tool available in any
 language**. It is already well on its way toward this goal.
 
 pandas is well suited for many different kinds of data:
@@ -30,26 +30,33 @@ engineering. For R users, :class:`DataFrame` provides everything that R's
 <http://www.numpy.org>`__ and is intended to integrate well within a scientific
 computing environment with many other 3rd party libraries.
 
-Here are just a few of the primary features of interest in pandas:
+Here are just a few of the things that pandas does well:
 
-  - Easy handling of missing data (represented as NaN) in floating point as
+  - Easy handling of **missing data** (represented as NaN) in floating point as
     well as non-floating point data
-  - Size mutability: columns can be inserted and deleted from DataFrame and
+  - Size mutability: columns can be **inserted and deleted** from DataFrame and
     higher dimensional objects
-  - Automatic and explicit data alignment: objects can be explicitly aligned to
-    a set of labels, or the user can simply ignore the labels and let `Series`,
-    `DataFrame`, etc. automatically align the data for you in computations
-  - Powerful, flexible "group by" functionality to perform split-apply-combine
-    operations on data sets, for both aggregating and transforming data sets
-  - Make it easy to collect ragged, differently-indexed data in other Python
-    and NumPy data structures into DataFrame objects
-  - Intelligent label-based slicing, (fancy) indexing, and subsetting of large
-    data sets
-  - Intuitive merging and joining of data sets
-  - Flexible reshaping and pivoting of data sets
-  - Hierarchical labeling of axes (possible to have multiple labels per tick)
-  - Robust IO tools for loading data from flat files (CSV and delimited), Excel
-    files, databases, and saving / loading data from the ultrafast HDF5 format
+  - Automatic and explicit **data alignment**: objects can be explicitly
+    aligned to a set of labels, or the user can simply ignore the labels and
+    let `Series`, `DataFrame`, etc. automatically align the data for you in
+    computations
+  - Powerful, flexible **group by** functionality to perform
+    split-apply-combine operations on data sets, for both aggregating and
+    transforming data
+  - Make it **easy to convert** ragged, differently-indexed data in other
+    Python and NumPy data structures into DataFrame objects
+  - Intelligent label-based **slicing**, **fancy indexing**, and **subsetting**
+    of large data sets
+  - Intuitive **merging** and **joining** data sets
+  - Flexible **reshaping** and pivoting of data sets
+  - **Hierarchical** labeling of axes (possible to have multiple labels per
+    tick)
+  - Robust IO tools for loading data from **flat files** (CSV and delimited),
+    Excel files, databases, and saving / loading data from the ultrafast **HDF5
+    format**
+  - **Time series**-specific functionality: date range generation and frequency
+    conversion, moving window statistics, moving window linear regressions,
+    date shifting and lagging, etc.
 
 Many of these principles are here to address the shortcomings frequently
 experienced using other languages / scientific research environments. For data
@@ -60,8 +67,15 @@ is the ideal tool for all of these tasks.
 
 Some other notes
 
- - pandas will soon become a dependency of `statsmodels`, making it a
-   important part of the statistical computing tools available in Python.
+ - pandas is **fast**. Many of the low-level algorithmic bits have been
+   extensively tweaked in `Cython <http://cython.org>`__ code. However, as with
+   anything else generalization usually sacrifices performance. So if you focus
+   on one feature for your application you may be able to create a faster
+   specialized tool.
+
+ - pandas will soon become a dependency of `statsmodels
+   <http://statsmodels.sourceforge.net>`__, making it a important part of the
+   statistical computing ecosystem in Python.
 
  - pandas has been used extensively in production in financial applications.
 
@@ -93,7 +107,7 @@ User manual
 **Code Repository:** http://github.com/wesm/pandas
 
 Library documentation
-~~~~~~~~~~~~~~~~~~~~~
+---------------------
 
 .. toctree::
     :maxdepth: 2
@@ -101,6 +115,7 @@ Library documentation
     install
     overview
     core
+    indexing
     groupby
     datetools
     stats
@@ -108,7 +123,7 @@ Library documentation
     io
 
 Other topics of interest
-~~~~~~~~~~~~~~~~~~~~~~~~
+------------------------
 
 .. toctree::
     :maxdepth: 2
@@ -128,7 +143,7 @@ Indices and tables
 History
 -------
 
-pandas development began at `AQR Capital Management <http://www.aqr.com>` in
+pandas development began at `AQR Capital Management <http://www.aqr.com>`__ in
 April 2008. It was open-sourced at the end of 2009 and continues to be actively
 used and maintained.
 
diff --git a/doc/source/indexing.rst b/doc/source/indexing.rst
new file mode 100644
index 000000000..6d19b39a5
--- /dev/null
+++ b/doc/source/indexing.rst
@@ -0,0 +1,13 @@
+.. _indexing:
+
+.. currentmodule:: pandas
+
+Basics
+------
+
+Fancy ndarray-like indexing with labels
+---------------------------------------
+
+Hierarchical indexing
+---------------------
+
diff --git a/doc/source/install.rst b/doc/source/install.rst
index 08657e1fd..91a2bedd5 100644
--- a/doc/source/install.rst
+++ b/doc/source/install.rst
@@ -6,16 +6,17 @@
 Installation
 ************
 
-You have the option to install an official release or to build the development
-version from the source repository. If you choose to install from source and
-are on Windows, you will have to ensure that you have a compatible C compiler
-(MinGW or Visual Studio) installed. `How-to install MinGW on Windows
+You have the option to install an `official release
+<http://pypi.python.org/pypi/pandas>`__ or to build the `development version
+<http://github.com/wesm/pandas>`__. If you choose to install from source and
+are running Windows, you will have to ensure that you have a compatible C
+compiler (MinGW or Visual Studio) installed. `How-to install MinGW on Windows
 <http://docs.cython.org/src/tutorial/appendix.html>`__
 
 Python version support
 ~~~~~~~~~~~~~~~~~~~~~~
 
-Officially Python 2.5 to 2.7. I will aim for Python 3.x support in a future
+Officially Python 2.5 to 2.7. I will aim for Python 3.x support in the next
 release. Python 2.4 support is being phased out since the userbase has shrunk
 significantly.
 
diff --git a/doc/sphinxext/ipython_console_highlighting.py b/doc/sphinxext/ipython_console_highlighting.py
new file mode 100644
index 000000000..f0a41bebc
--- /dev/null
+++ b/doc/sphinxext/ipython_console_highlighting.py
@@ -0,0 +1,114 @@
+"""reST directive for syntax-highlighting ipython interactive sessions.
+
+XXX - See what improvements can be made based on the new (as of Sept 2009)
+'pycon' lexer for the python console.  At the very least it will give better
+highlighted tracebacks.
+"""
+
+#-----------------------------------------------------------------------------
+# Needed modules
+
+# Standard library
+import re
+
+# Third party
+from pygments.lexer import Lexer, do_insertions
+from pygments.lexers.agile import (PythonConsoleLexer, PythonLexer,
+                                   PythonTracebackLexer)
+from pygments.token import Comment, Generic
+
+from sphinx import highlighting
+
+#-----------------------------------------------------------------------------
+# Global constants
+line_re = re.compile('.*?\n')
+
+#-----------------------------------------------------------------------------
+# Code begins - classes and functions
+
+class IPythonConsoleLexer(Lexer):
+    """
+    For IPython console output or doctests, such as:
+
+    .. sourcecode:: ipython
+
+      In [1]: a = 'foo'
+
+      In [2]: a
+      Out[2]: 'foo'
+
+      In [3]: print a
+      foo
+
+      In [4]: 1 / 0
+
+    Notes:
+
+      - Tracebacks are not currently supported.
+
+      - It assumes the default IPython prompts, not customized ones.
+    """
+
+    name = 'IPython console session'
+    aliases = ['ipython']
+    mimetypes = ['text/x-ipython-console']
+    input_prompt = re.compile("(In \[[0-9]+\]: )|(   \.\.\.+:)")
+    output_prompt = re.compile("(Out\[[0-9]+\]: )|(   \.\.\.+:)")
+    continue_prompt = re.compile("   \.\.\.+:")
+    tb_start = re.compile("\-+")
+
+    def get_tokens_unprocessed(self, text):
+        pylexer = PythonLexer(**self.options)
+        tblexer = PythonTracebackLexer(**self.options)
+
+        curcode = ''
+        insertions = []
+        for match in line_re.finditer(text):
+            line = match.group()
+            input_prompt = self.input_prompt.match(line)
+            continue_prompt = self.continue_prompt.match(line.rstrip())
+            output_prompt = self.output_prompt.match(line)
+            if line.startswith("#"):
+                insertions.append((len(curcode),
+                                   [(0, Comment, line)]))
+            elif input_prompt is not None:
+                insertions.append((len(curcode),
+                                   [(0, Generic.Prompt, input_prompt.group())]))
+                curcode += line[input_prompt.end():]
+            elif continue_prompt is not None:
+                insertions.append((len(curcode),
+                                   [(0, Generic.Prompt, continue_prompt.group())]))
+                curcode += line[continue_prompt.end():]
+            elif output_prompt is not None:
+                # Use the 'error' token for output.  We should probably make
+                # our own token, but error is typicaly in a bright color like
+                # red, so it works fine for our output prompts.
+                insertions.append((len(curcode),
+                                   [(0, Generic.Error, output_prompt.group())]))
+                curcode += line[output_prompt.end():]
+            else:
+                if curcode:
+                    for item in do_insertions(insertions,
+                                              pylexer.get_tokens_unprocessed(curcode)):
+                        yield item
+                        curcode = ''
+                        insertions = []
+                yield match.start(), Generic.Output, line
+        if curcode:
+            for item in do_insertions(insertions,
+                                      pylexer.get_tokens_unprocessed(curcode)):
+                yield item
+
+
+def setup(app):
+    """Setup as a sphinx extension."""
+
+    # This is only a lexer, so adding it below to pygments appears sufficient.
+    # But if somebody knows that the right API usage should be to do that via
+    # sphinx, by all means fix it here.  At least having this setup.py
+    # suppresses the sphinx warning we'd get without it.
+    pass
+
+#-----------------------------------------------------------------------------
+# Register the extension as a valid pygments lexer
+highlighting.lexers['ipython'] = IPythonConsoleLexer()
diff --git a/doc/sphinxext/ipython_directive.py b/doc/sphinxext/ipython_directive.py
new file mode 100644
index 000000000..3fed4c4f8
--- /dev/null
+++ b/doc/sphinxext/ipython_directive.py
@@ -0,0 +1,815 @@
+# -*- coding: utf-8 -*-
+"""Sphinx directive to support embedded IPython code.
+
+This directive allows pasting of entire interactive IPython sessions, prompts
+and all, and their code will actually get re-executed at doc build time, with
+all prompts renumbered sequentially. It also allows you to input code as a pure
+python input by giving the argument python to the directive. The output looks
+like an interactive ipython section.
+
+To enable this directive, simply list it in your Sphinx ``conf.py`` file
+(making sure the directory where you placed it is visible to sphinx, as is
+needed for all Sphinx directives).
+
+By default this directive assumes that your prompts are unchanged IPython ones,
+but this can be customized. The configurable options that can be placed in
+conf.py are
+
+ipython_savefig_dir:
+    The directory in which to save the figures. This is relative to the
+    Sphinx source directory. The default is `html_static_path`.
+ipython_rgxin:
+    The compiled regular expression to denote the start of IPython input
+    lines. The default is re.compile('In \[(\d+)\]:\s?(.*)\s*'). You
+    shouldn't need to change this.
+ipython_rgxout:
+    The compiled regular expression to denote the start of IPython output
+    lines. The default is re.compile('Out\[(\d+)\]:\s?(.*)\s*'). You
+    shouldn't need to change this.
+ipython_promptin:
+    The string to represent the IPython input prompt in the generated ReST.
+    The default is 'In [%d]:'. This expects that the line numbers are used
+    in the prompt.
+ipython_promptout:
+
+    The string to represent the IPython prompt in the generated ReST. The
+    default is 'Out [%d]:'. This expects that the line numbers are used
+    in the prompt.
+
+ToDo
+----
+
+- Turn the ad-hoc test() function into a real test suite.
+- Break up ipython-specific functionality from matplotlib stuff into better
+  separated code.
+
+Authors
+-------
+
+- John D Hunter: orignal author.
+- Fernando Perez: refactoring, documentation, cleanups, port to 0.11.
+- VĂĄclavĹ milauer <eudoxos-AT-arcig.cz>: Prompt generalizations.
+- Skipper Seabold, refactoring, cleanups, pure python addition
+"""
+
+#-----------------------------------------------------------------------------
+# Imports
+#-----------------------------------------------------------------------------
+
+# Stdlib
+import cStringIO
+import os
+import re
+import sys
+import tempfile
+
+# To keep compatibility with various python versions
+try:
+    from hashlib import md5
+except ImportError:
+    from md5 import md5
+
+# Third-party
+import matplotlib
+import sphinx
+from docutils.parsers.rst import directives
+from docutils import nodes
+from sphinx.util.compat import Directive
+
+matplotlib.use('Agg')
+
+# Our own
+from IPython import Config, InteractiveShell
+from IPython.core.profiledir import ProfileDir
+from IPython.utils import io
+
+from pdb import set_trace
+
+#-----------------------------------------------------------------------------
+# Globals
+#-----------------------------------------------------------------------------
+# for tokenizing blocks
+COMMENT, INPUT, OUTPUT =  range(3)
+
+#-----------------------------------------------------------------------------
+# Functions and class declarations
+#-----------------------------------------------------------------------------
+def block_parser(part, rgxin, rgxout, fmtin, fmtout):
+    """
+    part is a string of ipython text, comprised of at most one
+    input, one ouput, comments, and blank lines.  The block parser
+    parses the text into a list of::
+
+      blocks = [ (TOKEN0, data0), (TOKEN1, data1), ...]
+
+    where TOKEN is one of [COMMENT | INPUT | OUTPUT ] and
+    data is, depending on the type of token::
+
+      COMMENT : the comment string
+
+      INPUT: the (DECORATOR, INPUT_LINE, REST) where
+         DECORATOR: the input decorator (or None)
+         INPUT_LINE: the input as string (possibly multi-line)
+         REST : any stdout generated by the input line (not OUTPUT)
+
+
+      OUTPUT: the output string, possibly multi-line
+    """
+
+    block = []
+    lines = part.split('\n')
+    N = len(lines)
+    i = 0
+    decorator = None
+    while 1:
+
+        if i==N:
+            # nothing left to parse -- the last line
+            break
+
+        line = lines[i]
+        i += 1
+        line_stripped = line.strip()
+        if line_stripped.startswith('#'):
+            block.append((COMMENT, line))
+            continue
+
+        if line_stripped.startswith('@'):
+            # we're assuming at most one decorator -- may need to
+            # rethink
+            decorator = line_stripped
+            continue
+
+        # does this look like an input line?
+        matchin = rgxin.match(line)
+        if matchin:
+            lineno, inputline = int(matchin.group(1)), matchin.group(2)
+
+            # the ....: continuation string
+            continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
+            Nc = len(continuation)
+            # input lines can continue on for more than one line, if
+            # we have a '\' line continuation char or a function call
+            # echo line 'print'.  The input line can only be
+            # terminated by the end of the block or an output line, so
+            # we parse out the rest of the input line if it is
+            # multiline as well as any echo text
+
+            rest = []
+            while i<N:
+
+                # look ahead; if the next line is blank, or a comment, or
+                # an output line, we're done
+
+                nextline = lines[i]
+                matchout = rgxout.match(nextline)
+                #print "nextline=%s, continuation=%s, starts=%s"%(nextline, continuation, nextline.startswith(continuation))
+                if matchout or nextline.startswith('#'):
+                    break
+                elif nextline.startswith(continuation):
+                    inputline += '\n' + nextline[Nc:]
+                else:
+                    rest.append(nextline)
+                i+= 1
+
+            block.append((INPUT, (decorator, inputline, '\n'.join(rest))))
+            continue
+
+        # if it looks like an output line grab all the text to the end
+        # of the block
+        matchout = rgxout.match(line)
+        if matchout:
+            lineno, output = int(matchout.group(1)), matchout.group(2)
+            if i<N-1:
+                output = '\n'.join([output] + lines[i:])
+
+            block.append((OUTPUT, output))
+            break
+
+    return block
+
+class EmbeddedSphinxShell(object):
+    """An embedded IPython instance to run inside Sphinx"""
+
+    def __init__(self):
+
+        self.cout = cStringIO.StringIO()
+
+
+        # Create config object for IPython
+        config = Config()
+        config.Global.display_banner = False
+        config.Global.exec_lines = ['import numpy as np',
+                                    'from pylab import *'
+                                    ]
+        config.InteractiveShell.autocall = False
+        config.InteractiveShell.autoindent = False
+        config.InteractiveShell.colors = 'NoColor'
+
+        # create a profile so instance history isn't saved
+        tmp_profile_dir = tempfile.mkdtemp(prefix='profile_')
+        profname = 'auto_profile_sphinx_build'
+        pdir = os.path.join(tmp_profile_dir,profname)
+        profile = ProfileDir.create_profile_dir(pdir)
+
+        # Create and initialize ipython, but don't start its mainloop
+        IP = InteractiveShell.instance(config=config, profile_dir=profile)
+        # io.stdout redirect must be done *after* instantiating InteractiveShell
+        io.stdout = self.cout
+        io.stderr = self.cout
+
+        # For debugging, so we can see normal output, use this:
+        #from IPython.utils.io import Tee
+        #io.stdout = Tee(self.cout, channel='stdout') # dbg
+        #io.stderr = Tee(self.cout, channel='stderr') # dbg
+
+        # Store a few parts of IPython we'll need.
+        self.IP = IP
+        self.user_ns = self.IP.user_ns
+        self.user_global_ns = self.IP.user_global_ns
+
+        self.input = ''
+        self.output = ''
+
+        self.is_verbatim = False
+        self.is_doctest = False
+        self.is_suppress = False
+
+        # on the first call to the savefig decorator, we'll import
+        # pyplot as plt so we can make a call to the plt.gcf().savefig
+        self._pyplot_imported = False
+
+    def clear_cout(self):
+        self.cout.seek(0)
+        self.cout.truncate(0)
+
+    def process_input_line(self, line, store_history=True):
+        """process the input, capturing stdout"""
+        #print "input='%s'"%self.input
+        stdout = sys.stdout
+        splitter = self.IP.input_splitter
+        try:
+            sys.stdout = self.cout
+            splitter.push(line)
+            more = splitter.push_accepts_more()
+            if not more:
+                source_raw = splitter.source_raw_reset()[1]
+                self.IP.run_cell(source_raw, store_history=store_history)
+        finally:
+            sys.stdout = stdout
+
+    def process_image(self, decorator):
+        """
+        # build out an image directive like
+        # .. image:: somefile.png
+        #    :width 4in
+        #
+        # from an input like
+        # savefig somefile.png width=4in
+        """
+        savefig_dir = self.savefig_dir
+        source_dir = self.source_dir
+        saveargs = decorator.split(' ')
+        filename = saveargs[1]
+        # insert relative path to image file in source
+        outfile = os.path.relpath(os.path.join(savefig_dir,filename),
+                    source_dir)
+
+        imagerows = ['.. image:: %s'%outfile]
+
+        for kwarg in saveargs[2:]:
+            arg, val = kwarg.split('=')
+            arg = arg.strip()
+            val = val.strip()
+            imagerows.append('   :%s: %s'%(arg, val))
+
+        image_file = os.path.basename(outfile) # only return file name
+        image_directive = '\n'.join(imagerows)
+        return image_file, image_directive
+
+
+    # Callbacks for each type of token
+    def process_input(self, data, input_prompt, lineno):
+        """Process data block for INPUT token."""
+        decorator, input, rest = data
+        image_file = None
+        image_directive = None
+        #print 'INPUT:', data  # dbg
+        is_verbatim = decorator=='@verbatim' or self.is_verbatim
+        is_doctest = decorator=='@doctest' or self.is_doctest
+        is_suppress = decorator=='@suppress' or self.is_suppress
+        is_savefig = decorator is not None and \
+                     decorator.startswith('@savefig')
+
+        input_lines = input.split('\n')
+
+        continuation = '   %s:'%''.join(['.']*(len(str(lineno))+2))
+        Nc = len(continuation)
+
+        if is_savefig:
+            image_file, image_directive = self.process_image(decorator)
+
+        ret = []
+        is_semicolon = False
+        store_history = True
+
+        for i, line in enumerate(input_lines):
+            if line.endswith(';'):
+                is_semicolon = True
+            if is_semicolon or is_suppress:
+                store_history = False
+
+            if i==0:
+                # process the first input line
+                if is_verbatim:
+                    self.process_input_line('')
+                    self.IP.execution_count += 1 # increment it anyway
+                else:
+                    # only submit the line in non-verbatim mode
+                    self.process_input_line(line, store_history=store_history)
+                formatted_line = '%s %s'%(input_prompt, line)
+            else:
+                # process a continuation line
+                if not is_verbatim:
+                    self.process_input_line(line, store_history=store_history)
+
+                formatted_line = '%s %s'%(continuation, line)
+
+            if not is_suppress:
+                ret.append(formatted_line)
+
+        if not is_suppress:
+            if len(rest.strip()):
+                if is_verbatim:
+                    # the "rest" is the standard output of the
+                    # input, which needs to be added in
+                    # verbatim mode
+                    ret.append(rest)
+
+        self.cout.seek(0)
+        output = self.cout.read()
+        if not is_suppress and not is_semicolon:
+            ret.append(output)
+
+        self.cout.truncate(0)
+        return (ret, input_lines, output, is_doctest, image_file,
+                    image_directive)
+        #print 'OUTPUT', output  # dbg
+
+    def process_output(self, data, output_prompt,
+                       input_lines, output, is_doctest, image_file):
+        """Process data block for OUTPUT token."""
+        if is_doctest:
+            submitted = data.strip()
+            found = output
+            if found is not None:
+                found = found.strip()
+
+                # XXX - fperez: in 0.11, 'output' never comes with the prompt
+                # in it, just the actual output text.  So I think all this code
+                # can be nuked...
+
+                # the above comment does not appear to be accurate... (minrk)
+
+                ind = found.find(output_prompt)
+                if ind<0:
+                    e='output prompt="%s" does not match out line=%s' % \
+                       (output_prompt, found)
+                    raise RuntimeError(e)
+                found = found[len(output_prompt):].strip()
+
+                if found!=submitted:
+                    e = ('doctest failure for input_lines="%s" with '
+                         'found_output="%s" and submitted output="%s"' %
+                         (input_lines, found, submitted) )
+                    raise RuntimeError(e)
+                #print 'doctest PASSED for input_lines="%s" with found_output="%s" and submitted output="%s"'%(input_lines, found, submitted)
+
+    def process_comment(self, data):
+        """Process data fPblock for COMMENT token."""
+        if not self.is_suppress:
+            return [data]
+
+    def save_image(self, image_file):
+        """
+        Saves the image file to disk.
+        """
+        self.ensure_pyplot()
+        command = 'plt.gcf().savefig("%s")'%image_file
+        #print 'SAVEFIG', command  # dbg
+        self.process_input_line('bookmark ipy_thisdir', store_history=False)
+        self.process_input_line('cd -b ipy_savedir', store_history=False)
+        self.process_input_line(command, store_history=False)
+        self.process_input_line('cd -b ipy_thisdir', store_history=False)
+        self.process_input_line('bookmark -d ipy_thisdir', store_history=False)
+        self.clear_cout()
+
+
+    def process_block(self, block):
+        """
+        process block from the block_parser and return a list of processed lines
+        """
+        ret = []
+        output = None
+        input_lines = None
+        lineno = self.IP.execution_count
+
+        input_prompt = self.promptin%lineno
+        output_prompt = self.promptout%lineno
+        image_file = None
+        image_directive = None
+
+        for token, data in block:
+            if token==COMMENT:
+                out_data = self.process_comment(data)
+            elif token==INPUT:
+                (out_data, input_lines, output, is_doctest, image_file,
+                    image_directive) = \
+                          self.process_input(data, input_prompt, lineno)
+            elif token==OUTPUT:
+                out_data = \
+                    self.process_output(data, output_prompt,
+                                        input_lines, output, is_doctest,
+                                        image_file)
+            if out_data:
+                ret.extend(out_data)
+
+        # save the image files
+        if image_file is not None:
+            self.save_image(image_file)
+
+        return ret, image_directive
+
+    def ensure_pyplot(self):
+        if self._pyplot_imported:
+            return
+        self.process_input_line('import matplotlib.pyplot as plt',
+                                store_history=False)
+
+    def process_pure_python(self, content):
+        """
+        content is a list of strings. it is unedited directive conent
+
+        This runs it line by line in the InteractiveShell, prepends
+        prompts as needed capturing stderr and stdout, then returns
+        the content as a list as if it were ipython code
+        """
+        output = []
+        savefig = False # keep up with this to clear figure
+        multiline = False # to handle line continuation
+        fmtin = self.promptin
+
+        for lineno, line in enumerate(content):
+
+            line_stripped = line.strip()
+
+            if not len(line):
+                output.append(line) # preserve empty lines in output
+                continue
+
+            # handle decorators
+            if line_stripped.startswith('@'):
+                output.extend([line])
+                if 'savefig' in line:
+                    savefig = True # and need to clear figure
+                continue
+
+            # handle comments
+            if line_stripped.startswith('#'):
+                output.extend([line])
+                continue
+
+            # deal with multilines
+            if not multiline: # not currently on a multiline
+
+                if line_stripped.endswith('\\'): # now we are
+                    multiline = True
+                    cont_len = len(str(lineno)) + 2
+                    line_to_process = line.strip('\\')
+                    output.extend([u"%s %s" % (fmtin%lineno,line)])
+                    continue
+                else: # no we're still not
+                    line_to_process = line.strip('\\')
+            else: # we are currently on a multiline
+                line_to_process += line.strip('\\')
+                if line_stripped.endswith('\\'): # and we still are
+                    continuation = '.' * cont_len
+                    output.extend([(u'   %s: '+line_stripped) % continuation])
+                    continue
+                # else go ahead and run this multiline then carry on
+
+            # get output of line
+            self.process_input_line(unicode(line_to_process.strip()),
+                                    store_history=False)
+            out_line = self.cout.getvalue()
+            self.clear_cout()
+
+            # clear current figure if plotted
+            if savefig:
+                self.ensure_pyplot()
+                self.process_input_line('plt.clf()', store_history=False)
+                self.clear_cout()
+                savefig = False
+
+            # line numbers don't actually matter, they're replaced later
+            if not multiline:
+                in_line = u"%s %s" % (fmtin%lineno,line)
+
+                output.extend([in_line])
+            else:
+                output.extend([(u'   %s: '+line_stripped) % continuation])
+                multiline = False
+            if len(out_line):
+                output.extend([out_line])
+            output.extend([u''])
+
+        return output
+
+class IpythonDirective(Directive):
+
+    has_content = True
+    required_arguments = 0
+    optional_arguments = 4 # python, suppress, verbatim, doctest
+    final_argumuent_whitespace = True
+    option_spec = { 'python': directives.unchanged,
+                    'suppress' : directives.flag,
+                    'verbatim' : directives.flag,
+                    'doctest' : directives.flag,
+                  }
+
+    shell = EmbeddedSphinxShell()
+
+    def get_config_options(self):
+        # contains sphinx configuration variables
+        config = self.state.document.settings.env.config
+
+        # get config variables to set figure output directory
+        confdir = self.state.document.settings.env.app.confdir
+        savefig_dir = config.ipython_savefig_dir
+        source_dir = os.path.dirname(self.state.document.current_source)
+        if savefig_dir is None:
+            savefig_dir = config.html_static_path
+        if isinstance(savefig_dir, list):
+            savefig_dir = savefig_dir[0] # safe to assume only one path?
+        savefig_dir = os.path.join(confdir, savefig_dir)
+
+        # get regex and prompt stuff
+        rgxin     = config.ipython_rgxin
+        rgxout    = config.ipython_rgxout
+        promptin  = config.ipython_promptin
+        promptout = config.ipython_promptout
+
+        return savefig_dir, source_dir, rgxin, rgxout, promptin, promptout
+
+    def setup(self):
+        # get config values
+        (savefig_dir, source_dir, rgxin,
+                rgxout, promptin, promptout) = self.get_config_options()
+
+        # and attach to shell so we don't have to pass them around
+        self.shell.rgxin = rgxin
+        self.shell.rgxout = rgxout
+        self.shell.promptin = promptin
+        self.shell.promptout = promptout
+        self.shell.savefig_dir = savefig_dir
+        self.shell.source_dir = source_dir
+
+        # setup bookmark for saving figures directory
+
+        self.shell.process_input_line('bookmark ipy_savedir %s'%savefig_dir,
+                                      store_history=False)
+        self.shell.clear_cout()
+
+        return rgxin, rgxout, promptin, promptout
+
+
+    def teardown(self):
+        # delete last bookmark
+        self.shell.process_input_line('bookmark -d ipy_savedir',
+                                      store_history=False)
+        self.shell.clear_cout()
+
+    def run(self):
+        debug = False
+
+        #TODO, any reason block_parser can't be a method of embeddable shell
+        # then we wouldn't have to carry these around
+        rgxin, rgxout, promptin, promptout = self.setup()
+
+        options = self.options
+        self.shell.is_suppress = 'suppress' in options
+        self.shell.is_doctest = 'doctest' in options
+        self.shell.is_verbatim = 'verbatim' in options
+
+
+        # handle pure python code
+        if 'python' in self.arguments:
+            content = self.content
+            self.content = self.shell.process_pure_python(content)
+
+        parts = '\n'.join(self.content).split('\n\n')
+
+        lines = ['.. code-block:: ipython','']
+        figures = []
+
+        for part in parts:
+
+            block = block_parser(part, rgxin, rgxout, promptin, promptout)
+
+            if len(block):
+                rows, figure = self.shell.process_block(block)
+                for row in rows:
+                    lines.extend(['   %s'%line for line in row.split('\n')])
+
+                if figure is not None:
+                    figures.append(figure)
+
+        #text = '\n'.join(lines)
+        #figs = '\n'.join(figures)
+
+        for figure in figures:
+            lines.append('')
+            lines.extend(figure.split('\n'))
+            lines.append('')
+
+        #print lines
+        if len(lines)>2:
+            if debug:
+                print '\n'.join(lines)
+            else: #NOTE: this raises some errors, what's it for?
+                #print 'INSERTING %d lines'%len(lines)
+                self.state_machine.insert_input(
+                    lines, self.state_machine.input_lines.source(0))
+
+        text = '\n'.join(lines)
+        txtnode = nodes.literal_block(text, text)
+        txtnode['language'] = 'ipython'
+        #imgnode = nodes.image(figs)
+
+        # cleanup
+        self.teardown()
+
+        return []#, imgnode]
+
+# Enable as a proper Sphinx directive
+def setup(app):
+    setup.app = app
+
+    app.add_directive('ipython', IpythonDirective)
+    app.add_config_value('ipython_savefig_dir', None, True)
+    app.add_config_value('ipython_rgxin',
+                         re.compile('In \[(\d+)\]:\s?(.*)\s*'), True)
+    app.add_config_value('ipython_rgxout',
+                         re.compile('Out\[(\d+)\]:\s?(.*)\s*'), True)
+    app.add_config_value('ipython_promptin', 'In [%d]:', True)
+    app.add_config_value('ipython_promptout', 'Out[%d]:', True)
+
+
+# Simple smoke test, needs to be converted to a proper automatic test.
+def test():
+
+    examples = [
+        r"""
+In [9]: pwd
+Out[9]: '/home/jdhunter/py4science/book'
+
+In [10]: cd bookdata/
+/home/jdhunter/py4science/book/bookdata
+
+In [2]: from pylab import *
+
+In [2]: ion()
+
+In [3]: im = imread('stinkbug.png')
+
+@savefig mystinkbug.png width=4in
+In [4]: imshow(im)
+Out[4]: <matplotlib.image.AxesImage object at 0x39ea850>
+
+""",
+        r"""
+
+In [1]: x = 'hello world'
+
+# string methods can be
+# used to alter the string
+@doctest
+In [2]: x.upper()
+Out[2]: 'HELLO WORLD'
+
+@verbatim
+In [3]: x.st<TAB>
+x.startswith  x.strip
+""",
+    r"""
+
+In [130]: url = 'http://ichart.finance.yahoo.com/table.csv?s=CROX\
+   .....: &d=9&e=22&f=2009&g=d&a=1&br=8&c=2006&ignore=.csv'
+
+In [131]: print url.split('&')
+['http://ichart.finance.yahoo.com/table.csv?s=CROX', 'd=9', 'e=22', 'f=2009', 'g=d', 'a=1', 'b=8', 'c=2006', 'ignore=.csv']
+
+In [60]: import urllib
+
+""",
+    r"""\
+
+In [133]: import numpy.random
+
+@suppress
+In [134]: numpy.random.seed(2358)
+
+@doctest
+In [135]: numpy.random.rand(10,2)
+Out[135]:
+array([[ 0.64524308,  0.59943846],
+       [ 0.47102322,  0.8715456 ],
+       [ 0.29370834,  0.74776844],
+       [ 0.99539577,  0.1313423 ],
+       [ 0.16250302,  0.21103583],
+       [ 0.81626524,  0.1312433 ],
+       [ 0.67338089,  0.72302393],
+       [ 0.7566368 ,  0.07033696],
+       [ 0.22591016,  0.77731835],
+       [ 0.0072729 ,  0.34273127]])
+
+""",
+
+    r"""
+In [106]: print x
+jdh
+
+In [109]: for i in range(10):
+   .....:     print i
+   .....:
+   .....:
+0
+1
+2
+3
+4
+5
+6
+7
+8
+9
+""",
+
+        r"""
+
+In [144]: from pylab import *
+
+In [145]: ion()
+
+# use a semicolon to suppress the output
+@savefig test_hist.png width=4in
+In [151]: hist(np.random.randn(10000), 100);
+
+
+@savefig test_plot.png width=4in
+In [151]: plot(np.random.randn(10000), 'o');
+   """,
+
+        r"""
+# use a semicolon to suppress the output
+In [151]: plt.clf()
+
+@savefig plot_simple.png width=4in
+In [151]: plot([1,2,3])
+
+@savefig hist_simple.png width=4in
+In [151]: hist(np.random.randn(10000), 100);
+
+""",
+     r"""
+# update the current fig
+In [151]: ylabel('number')
+
+In [152]: title('normal distribution')
+
+
+@savefig hist_with_text.png
+In [153]: grid(True)
+
+        """,
+        ]
+    # skip local-file depending first example:
+    examples = examples[1:]
+
+    #ipython_directive.DEBUG = True  # dbg
+    #options = dict(suppress=True)  # dbg
+    options = dict()
+    for example in examples:
+        content = example.split('\n')
+        ipython_directive('debug', arguments=None, options=options,
+                          content=content, lineno=0,
+                          content_offset=None, block_text=None,
+                          state=None, state_machine=None,
+                          )
+
+# Run test suite as a script
+if __name__=='__main__':
+    if not os.path.isdir('_static'):
+        os.mkdir('_static')
+    test()
+    print 'All OK? Check figures in _static/'
