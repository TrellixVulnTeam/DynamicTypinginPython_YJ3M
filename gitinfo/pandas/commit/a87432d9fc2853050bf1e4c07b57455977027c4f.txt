commit a87432d9fc2853050bf1e4c07b57455977027c4f
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon Mar 12 21:38:57 2012 -0400

    ENH: catch __builtin__.sum in groupby, close #885

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index b4d4aa666..67adc48c6 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -309,6 +309,7 @@ class GroupBy(object):
         return self._wrap_aggregated_output(output)
 
     def _python_agg_general(self, func, *args, **kwargs):
+        func = _intercept_function(func)
         agg_func = lambda x: func(x, *args, **kwargs)
 
         # iterate through "columns" ex exclusions to populate output dict
@@ -330,6 +331,8 @@ class GroupBy(object):
         return self._wrap_aggregated_output(output)
 
     def _python_apply_general(self, func, *args, **kwargs):
+        func = _intercept_function(func)
+
         result_keys = []
         result_values = []
 
@@ -596,6 +599,8 @@ class Grouper(object):
             return self._aggregate_series_pure_python(obj, func)
 
     def _aggregate_series_fast(self, obj, func):
+        func = _intercept_function(func)
+
         if obj.index._has_complex_internals:
             raise TypeError('Incompatible index for Cython grouper')
 
@@ -1616,6 +1621,15 @@ def _reorder_by_uniques(uniques, labels):
 
     return uniques, labels
 
+import __builtin__
+
+_func_table = {
+    __builtin__.sum : np.sum
+}
+
+def _intercept_function(func):
+    return _func_table.get(func, func)
+
 def _groupby_indices(values):
     if values.dtype != np.object_:
         values = values.astype('O')
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 904fb10cb..1f2162d9c 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -1531,6 +1531,17 @@ class TestGroupBy(unittest.TestCase):
         result = grouped.sum()
         _check_groupby(df, result, ['a', 'b'], 'd')
 
+    def test_intercept_builtin_sum(self):
+        import __builtin__
+        s = Series([1., 2., np.nan, 3.])
+        grouped = s.groupby([0, 1, 2, 2])
+
+        result = grouped.agg(__builtin__.sum)
+        result2 = grouped.apply(__builtin__.sum)
+        expected = grouped.sum()
+        assert_series_equal(result, expected)
+        assert_series_equal(result2, expected)
+
 def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):
     tups = map(tuple, df[keys].values)
     tups = com._asarray_tuplesafe(tups)
