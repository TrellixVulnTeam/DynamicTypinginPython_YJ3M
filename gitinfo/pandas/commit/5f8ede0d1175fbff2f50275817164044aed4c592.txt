commit 5f8ede0d1175fbff2f50275817164044aed4c592
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon Jan 9 01:24:27 2012 -0500

    ENH: fix major performance issue in multi-groupby by compressing key space, likely some win32 issues still...

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 80254014b..b3d294f5f 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -187,12 +187,6 @@ class GroupBy(object):
     def primary(self):
         return self.groupings[0]
 
-    @property
-    def _group_index(self):
-        result = get_group_index([ping.labels for ping in self.groupings],
-                                 self._group_shape)
-        return result.astype('i4')
-
     def get_group(self, name, obj=None):
         if obj is None:
             obj = self.obj
@@ -287,12 +281,6 @@ class GroupBy(object):
         """
         return self.aggregate(func, *args, **kwargs)
 
-    def _get_names(self):
-        axes = [ping.group_index for ping in self.groupings]
-        grouping_names = [ping.name for ping in self.groupings]
-        shape = self._group_shape
-        return zip(grouping_names, _ravel_names(axes, shape))
-
     def _iterate_slices(self):
         yield self.name, self.obj
 
@@ -341,12 +329,11 @@ class GroupBy(object):
             return self.aggregate(lambda x: np.sum(x, axis=self.axis))
 
     def _cython_agg_general(self, how):
-        shape = self._group_shape
-
         # TODO: address inefficiencies, like duplicating effort (should
         # aggregate all the columns at once?)
 
         group_index = self._group_index
+        comp_ids, obs_group_ids, max_group = _compress_group_index(group_index)
 
         output = {}
         for name, obj in self._iterate_slices():
@@ -356,38 +343,52 @@ class GroupBy(object):
             if obj.dtype != np.float64:
                 obj = obj.astype('f8')
 
-            result, counts =  cython_aggregate(obj, group_index, shape,
-                                               how=how)
-            result = result.ravel()
-            mask = counts.ravel() > 0
+            result, counts = cython_aggregate(obj, comp_ids,
+                                              max_group, how=how)
+            mask = counts > 0
             output[name] = result[mask]
 
         if len(output) == 0:
             raise GroupByError('No numeric types to aggregate')
 
-        return self._wrap_aggregated_output(output, mask)
+        return self._wrap_aggregated_output(output, mask, obs_group_ids)
+
+    @property
+    def _group_index(self):
+        result = get_group_index([ping.labels for ping in self.groupings],
+                                 self._group_shape)
+
+        if result.dtype != np.int64:  # pragma: no cover
+            result = result.astype('i8')
+        return result
 
-    def _get_multi_index(self, mask):
-        masked = [labels for _, labels in self._get_group_levels(mask)]
+    def _get_multi_index(self, mask, obs_ids):
+        masked = [labels for _, labels in
+                  self._get_group_levels(mask, obs_ids)]
         names = [ping.name for ping in self.groupings]
         return MultiIndex.from_arrays(masked, names=names)
 
-    def _get_group_levels(self, mask):
-        name_list = self._get_names()
-        return [(name, raveled[mask]) for name, raveled in name_list]
+    def _get_group_levels(self, mask, obs_ids):
+        recons_labels = decons_group_index(obs_ids, self._group_shape)
+
+        name_list = []
+        for ping, labels in zip(self.groupings, recons_labels):
+            name_list.append((ping.name, ping.group_index.take(labels)))
+
+        return name_list
 
     def _python_agg_general(self, func, *args, **kwargs):
         agg_func = lambda x: func(x, *args, **kwargs)
 
-        ngroups = np.prod(self._group_shape)
         group_index = self._group_index
+        comp_ids, obs_group_ids, max_group = _compress_group_index(group_index)
 
         # iterate through "columns" ex exclusions to populate output dict
         output = {}
         for name, obj in self._iterate_slices():
             try:
                 result, counts = self._aggregate_series(obj, agg_func,
-                                                        group_index, ngroups)
+                                                        comp_ids, max_group)
                 output[name] = result
             except TypeError:
                 continue
@@ -399,7 +400,7 @@ class GroupBy(object):
         for name, result in output.iteritems():
             output[name] = result[mask]
 
-        return self._wrap_aggregated_output(output, mask)
+        return self._wrap_aggregated_output(output, mask, obs_group_ids)
 
     def _aggregate_series(self, obj, func, group_index, ngroups):
         try:
@@ -766,10 +767,10 @@ class SeriesGroupBy(GroupBy):
 
         return DataFrame(results)
 
-    def _wrap_aggregated_output(self, output, mask):
+    def _wrap_aggregated_output(self, output, mask, comp_ids):
         # sort of a kludge
         output = output[self.name]
-        index = self._get_multi_index(mask)
+        index = self._get_multi_index(mask, comp_ids)
         return Series(output, index=index)
 
     def _wrap_applied_output(self, keys, values, not_indexed_same=False):
@@ -850,19 +851,6 @@ class SeriesGroupBy(GroupBy):
 
         return result
 
-def _ravel_names(axes, shape):
-    """
-    Compute labeling vector for raveled values vector
-    """
-    unrolled = []
-    for i, ax in enumerate(axes):
-        tile_shape = shape[:i] + shape[i+1:] + (1,)
-        tiled = np.tile(ax, tile_shape)
-        tiled = tiled.swapaxes(i, -1)
-        unrolled.append(tiled.ravel())
-
-    return unrolled
-
 class DataFrameGroupBy(GroupBy):
 
     def __getitem__(self, key):
@@ -1020,7 +1008,7 @@ class DataFrameGroupBy(GroupBy):
 
         return DataFrame(result, columns=result_columns)
 
-    def _wrap_aggregated_output(self, output, mask):
+    def _wrap_aggregated_output(self, output, mask, comp_ids):
         agg_axis = 0 if self.axis == 1 else 1
         agg_labels = self._obj_with_exclusions._get_axis(agg_axis)
 
@@ -1039,12 +1027,12 @@ class DataFrameGroupBy(GroupBy):
 
         if not self.as_index:
             result = DataFrame(output, columns=output_keys)
-            group_levels = self._get_group_levels(mask)
+            group_levels = self._get_group_levels(mask, comp_ids)
             for i, (name, labels) in enumerate(group_levels):
                 result.insert(i, name, labels)
             result = result.consolidate()
         else:
-            index = self._get_multi_index(mask)
+            index = self._get_multi_index(mask, comp_ids)
             result = DataFrame(output, index=index, columns=output_keys)
 
         if self.axis == 1:
@@ -1257,6 +1245,38 @@ def get_group_index(label_list, shape):
     np.putmask(group_index, mask, -1)
     return group_index
 
+def decons_group_index(comp_labels, shape):
+    # reconstruct labels
+    label_list = []
+    factor = 1
+    y = 0
+    x = comp_labels
+    for i in reversed(xrange(len(shape))):
+        labels = (x - y) % (factor * shape[i]) // factor
+        np.putmask(labels, comp_labels < 0, -1)
+        label_list.append(labels)
+        y = labels * factor
+        factor *= shape[i]
+    return label_list[::-1]
+
+def test_decons():
+    def testit(label_list, shape):
+        group_index = get_group_index(label_list, shape)
+        label_list2 = decons_group_index(group_index, shape)
+
+        for a, b in zip(label_list, label_list2):
+            assert(np.array_equal(a, b))
+
+    shape = (4, 5, 6)
+    label_list = [np.tile([0, 1, 2, 3, 0, 1, 2, 3], 100),
+                  np.tile([0, 2, 4, 3, 0, 1, 2, 3], 100),
+                  np.tile([5, 1, 0, 2, 3, 0, 5, 4], 100)]
+    testit(label_list, shape)
+
+    shape = (10000, 10000)
+    label_list = [np.tile(np.arange(10000), 5),
+                  np.tile(np.arange(10000), 5)]
+    testit(label_list, shape)
 
 def _aggregate_series_fast(obj, func, group_index, ngroups):
     if obj.index._has_complex_internals:
@@ -1277,19 +1297,16 @@ def _aggregate_series_fast(obj, func, group_index, ngroups):
 # Group aggregations in Cython
 
 
-def cython_aggregate(values, group_index, shape, how='add'):
+def cython_aggregate(values, group_index, ngroups, how='add'):
     agg_func = _cython_functions[how]
     trans_func = _cython_transforms.get(how, lambda x: x)
 
-    result = np.empty(shape, dtype=np.float64)
+    result = np.empty(ngroups, dtype=np.float64)
     result.fill(np.nan)
 
-    counts = np.zeros(shape, dtype=np.int32)
-    agg_func(result.ravel(), counts.ravel(), values,
-             group_index)
-
+    counts = np.zeros(ngroups, dtype=np.int32)
+    agg_func(result, counts, values, group_index)
     result = trans_func(result)
-
     return result, counts
 
 _cython_functions = {
@@ -1306,6 +1323,28 @@ _cython_transforms = {
 #----------------------------------------------------------------------
 # sorting levels...cleverly?
 
+def _compress_group_index(group_index, sort=True):
+    uniques = []
+    table = lib.Int64HashTable(len(group_index))
+    comp_ids = table.get_labels_groupby(group_index, uniques)
+    max_group = len(uniques)
+
+    # these are the ones we observed
+    obs_group_ids = np.array(uniques, dtype='i8')
+
+    if sort and len(obs_group_ids) > 0:
+        sorter = obs_group_ids.argsort()
+        reverse_indexer = np.empty(len(sorter), dtype='i4')
+        reverse_indexer.put(sorter, np.arange(len(sorter)))
+
+        mask = comp_ids < 0
+        comp_ids = reverse_indexer.take(comp_ids)
+        np.putmask(comp_ids, mask, -1)
+
+        obs_group_ids = obs_group_ids.take(sorter)
+
+    return comp_ids, obs_group_ids, max_group
+
 def _groupby_indices(values):
     if values.dtype != np.object_:
         values = values.astype('O')
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
index 28126f068..3d921310d 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/src/hashtable.pyx
@@ -434,6 +434,18 @@ cdef class Int64HashTable:
         else:
             raise KeyError(key)
 
+    def map(self, ndarray[int64_t] keys, ndarray[int64_t] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            int64_t key
+            khiter_t k
+
+        for i in range(n):
+            key = keys[i]
+            k = kh_put_int64(self.table, key, &ret)
+            self.table.vals[k] = <Py_ssize_t> values[i]
+
     def map_locations(self, ndarray[int64_t] values):
         cdef:
             Py_ssize_t i, n = len(values)
@@ -447,7 +459,25 @@ cdef class Int64HashTable:
             # print 'putting %s, %s' % (val, count)
             self.table.vals[k] = i
 
-    def lookup_locations(self, ndarray[int64_t] values):
+    def lookup(self, ndarray[int64_t] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            int64_t val
+            khiter_t k
+            ndarray[int64_t] locs = np.empty(n, dtype='i8')
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_int64(self.table, val)
+            if k != self.table.n_buckets:
+                locs[i] = self.table.vals[k]
+            else:
+                locs[i] = -1
+
+        return locs
+
+    def lookup_i4(self, ndarray[int64_t] values):
         cdef:
             Py_ssize_t i, n = len(values)
             int ret
@@ -501,6 +531,38 @@ cdef class Int64HashTable:
 
         return labels, counts[:count].copy()
 
+    def get_labels_groupby(self, ndarray[int64_t] values, list uniques):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            ndarray[int32_t] labels
+            Py_ssize_t idx, count = 0
+            int ret
+            int64_t val
+            khiter_t k
+
+        labels = np.empty(n, dtype=np.int32)
+
+        for i in range(n):
+            val = values[i]
+
+            # specific for groupby
+            if val < 0:
+                labels[i] = -1
+                continue
+
+            k = kh_get_int64(self.table, val)
+            if k != self.table.n_buckets:
+                idx = self.table.vals[k]
+                labels[i] = idx
+            else:
+                k = kh_put_int64(self.table, val, &ret)
+                self.table.vals[k] = count
+                uniques.append(val)
+                labels[i] = count
+                count += 1
+
+        return labels
+
     def unique(self, ndarray[int64_t] values):
         cdef:
             Py_ssize_t i, n = len(values)
diff --git a/scripts/file_sizes.py b/scripts/file_sizes.py
index 279024e68..139a46555 100644
--- a/scripts/file_sizes.py
+++ b/scripts/file_sizes.py
@@ -1,20 +1,30 @@
-from pandas import DataFrame
-from pandas.util.testing import set_trace
 import os
+import sys
+
 import numpy as np
 import matplotlib.pyplot as plt
 
+from pandas import DataFrame
+from pandas.util.testing import set_trace
+
 dirs = []
 names = []
 lengths = []
 
-walked = os.walk('pandas')
+# if len(sys.argv) > 1:
+#     loc = sys.argv[1]
+# else:
+#     loc = '.'
+
+loc = 'pandas'
+walked = os.walk(loc)
 
 def _should_count_file(path):
     return path.endswith('.py') or path.endswith('.pyx')
 
 def _is_def_line(line):
-    return (line.endswith(':') and
+    """def/cdef/cpdef, but not `cdef class`"""
+    return (line.endswith(':') and not 'class' in line.split() and
             (line.startswith('def ') or
              line.startswith('cdef ') or
              line.startswith('cpdef ') or
@@ -182,9 +192,9 @@ ax = fig.add_subplot(111)
 ax.hist(all_counts, bins=100)
 n = len(all_counts)
 nmore = (all_counts > 50).sum()
-ax.set_title('pandas function lengths, n=%d' % n)
+ax.set_title('%s function lengths, n=%d' % (loc, n))
 ax.set_ylabel('N functions')
 ax.set_xlabel('Function length')
-ax.text(100, 300, '%.3f%% with > 50 lines' % (100 * nmore / float(n)),
+ax.text(60, 200, '%.3f%% with > 50 lines' % ((n - nmore) / float(n)),
         fontsize=18)
 plt.show()
diff --git a/vb_suite/panel_ctor.py b/vb_suite/panel_ctor.py
index f68d8f9b5..262da3beb 100644
--- a/vb_suite/panel_ctor.py
+++ b/vb_suite/panel_ctor.py
@@ -11,7 +11,7 @@ START_DATE = datetime(2011, 6, 1)
 
 setup_same_index = common_setup + """
 # create 1000 dataframes with the same index
-dr = DateRange(datetime(1990,1,1), datetime(2012,1,1)).values
+dr = np.asarray(DateRange(datetime(1990,1,1), datetime(2012,1,1)))
 data_frames = {}
 for x in xrange(1000):
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
@@ -27,7 +27,7 @@ panel_from_dict_same_index = \
 setup_equiv_indexes = common_setup + """
 data_frames = {}
 for x in xrange(1000):
-   dr = DateRange(datetime(1990,1,1), datetime(2012,1,1)).values
+   dr = np.asarray(DateRange(datetime(1990,1,1), datetime(2012,1,1)))
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
@@ -44,7 +44,7 @@ start = datetime(1990,1,1)
 end = datetime(2012,1,1)
 for x in xrange(1000):
    end += timedelta(days=1)
-   dr = DateRange(start, end).values
+   dr = np.asarray(DateRange(start, end))
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
@@ -62,7 +62,7 @@ end = datetime(2012,1,1)
 for x in xrange(1000):
    if x == 500:
        end += timedelta(days=1)
-   dr = DateRange(start, end).values
+   dr = np.asarray(DateRange(start, end))
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
