commit 2686db8a87aee1266f425f9b2ebba519771df866
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Jul 20 15:38:03 2012 -0400

    ENH: hack to not compress single group keys, accelerate single-key and Categorical groupby operations

diff --git a/RELEASE.rst b/RELEASE.rst
index 808950d60..b48efed87 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -45,6 +45,8 @@ pandas 0.8.1
   - Add Cython group median method for >15x speedup (#1358)
   - Drastically improve ``to_datetime`` performance on ISO8601 datetime strings
     (with no time zones) (#1571)
+  - Improve single-key groupby performance on large data sets, accelerate use of
+    groupby with a Categorical variable
   - Add ability to append hierarchical index levels with ``set_index`` and to
     drop single levels with ``reset_index`` (#1569, #1577)
   - Always apply passed functions in ``resample``, even if upsampling (#1596)
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index cb28f11c5..673e3b5d7 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -496,6 +496,7 @@ class Grouper(object):
         self.groupings = groupings
         self.sort = sort
         self.group_keys = group_keys
+        self.compressed = True
 
     @property
     def shape(self):
@@ -604,9 +605,14 @@ class Grouper(object):
         else:
             if len(all_labels) > 1:
                 group_index = get_group_index(all_labels, self.shape)
+                comp_ids, obs_group_ids = _compress_group_index(group_index)
             else:
-                group_index = all_labels[0]
-            comp_ids, obs_group_ids = _compress_group_index(group_index)
+                ping = self.groupings[0]
+                comp_ids = ping.labels
+                obs_group_ids = np.arange(len(ping.group_index))
+                self.compressed = False
+                self._filter_empty_groups = False
+
             return comp_ids, obs_group_ids
 
     @cache_readonly
@@ -624,6 +630,10 @@ class Grouper(object):
 
     def get_group_levels(self):
         obs_ids = self.group_info[1]
+
+        if not self.compressed and len(self.groupings) == 1:
+            return [self.groupings[0].group_index]
+
         if self._overflow_possible:
             recons_labels = [np.array(x) for x in izip(*obs_ids)]
         else:
@@ -960,6 +970,7 @@ class Grouping(object):
 
         # pre-computed
         self._was_factor = False
+        self._should_compress = True
 
         if level is not None:
             if not isinstance(level, int):
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 01a75961f..2786f28b6 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -2028,6 +2028,23 @@ class TestGroupBy(unittest.TestCase):
         exp = df.groupby(labels).agg(nanops.nanmedian)
         assert_frame_equal(result, exp)
 
+    def test_groupby_categorical_no_compress(self):
+        data = Series(np.random.randn(9))
+
+        labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
+        cats = Categorical(labels, [0, 1, 2])
+
+        result = data.groupby(cats).mean()
+        exp = data.groupby(labels).mean()
+        assert_series_equal(result, exp)
+
+        labels = np.array([0, 0, 0, 1, 1, 1, 3, 3, 3])
+        cats = Categorical(labels, [0, 1, 2, 3])
+
+        result = data.groupby(cats).mean()
+        exp = data.groupby(labels).mean().reindex(cats.levels)
+        assert_series_equal(result, exp)
+
 def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):
     tups = map(tuple, df[keys].values)
     tups = com._asarray_tuplesafe(tups)
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 7db790a58..5f6e82b04 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -208,3 +208,15 @@ df = DataFrame(data)
 groupby_frame_median = \
     Benchmark('df.groupby(labels).median()', setup,
               start_date=datetime(2011, 8, 1), logy=True)
+
+
+setup = common_setup + """
+data = np.random.randn(1000000, 2)
+labels = np.random.randint(0, 1000, size=1000000)
+df = DataFrame(data)
+"""
+
+groupby_simple_compress_timing = \
+    Benchmark('df.groupby(labels).mean()', setup,
+              start_date=datetime(2011, 8, 1))
+
