commit 8439d28f07fece62bc873f306cdd248ad22e665b
Author: sinhrks <sinhrks@gmail.com>
Date:   Fri Apr 29 13:15:31 2016 -0400

    BUG: SparseSeries.value_counts ignores fill_value
    
    closes #6749
    
    Author: sinhrks <sinhrks@gmail.com>
    
    Closes #12835 from sinhrks/sparse_valuecounts and squashes the following commits:
    
    2392e7c [sinhrks] Move dtype handling to algorithm
    2f46f73 [sinhrks] BUG: SparseSeries.value_counts ignores fill_value

diff --git a/doc/source/whatsnew/v0.18.1.txt b/doc/source/whatsnew/v0.18.1.txt
index bfc95dea5..17be1ecce 100644
--- a/doc/source/whatsnew/v0.18.1.txt
+++ b/doc/source/whatsnew/v0.18.1.txt
@@ -259,6 +259,7 @@ These changes conform sparse handling to return the correct types and work to ma
 - Bug in ``SparseSeries`` and ``SparseArray`` may have different ``dtype`` from its dense values (:issue:`12908`)
 - Bug in ``SparseSeries.reindex`` incorrectly handle ``fill_value`` (:issue:`12797`)
 - Bug in ``SparseArray.to_frame()`` results in ``DataFrame``, rather than ``SparseDataFrame`` (:issue:`9850`)
+- Bug in ``SparseSeries.value_counts()`` does not count ``fill_value`` (:issue:`6749`)
 - Bug in ``SparseArray.to_dense()`` does not preserve ``dtype`` (:issue:`10648`)
 - Bug in ``SparseArray.to_dense()`` incorrectly handle ``fill_value`` (:issue:`12797`)
 - Bug in ``pd.concat()`` of ``SparseSeries`` results in dense (:issue:`10536`)
@@ -536,6 +537,9 @@ Bug Fixes
 
 
 - Bug in ``value_counts`` when ``normalize=True`` and ``dropna=True`` where nulls still contributed to the normalized count (:issue:`12558`)
+- Bug in ``Series.value_counts()`` loses name if its dtype is category (:issue:`12835`)
+- Bug in ``Series.value_counts()`` loses timezone info (:issue:`12835`)
+- Bug in ``Series.value_counts(normalize=True)`` with ``Categorical`` raises ``UnboundLocalError`` (:issue:`12835`)
 - Bug in ``Panel.fillna()`` ignoring ``inplace=True`` (:issue:`12633`)
 - Bug in ``read_csv`` when specifying ``names``, ``usecols``, and ``parse_dates`` simultaneously with the C engine (:issue:`9755`)
 - Bug in ``read_csv`` when specifying ``delim_whitespace=True`` and ``lineterminator`` simultaneously with the C engine (:issue:`12912`)
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 323cbe8e9..590bf754d 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -10,6 +10,7 @@ from pandas import compat, lib, tslib, _np_version_under1p8
 import pandas.core.common as com
 import pandas.algos as algos
 import pandas.hashtable as htable
+from pandas.types import api as gt
 from pandas.compat import string_types
 from pandas.tslib import iNaT
 
@@ -253,84 +254,101 @@ def value_counts(values, sort=True, ascending=False, normalize=False,
 
     """
     from pandas.core.series import Series
-    from pandas.tools.tile import cut
-    from pandas import Index, PeriodIndex, DatetimeIndex
-
     name = getattr(values, 'name', None)
-    values = Series(values).values
 
     if bins is not None:
         try:
+            from pandas.tools.tile import cut
+            values = Series(values).values
             cat, bins = cut(values, bins, retbins=True)
         except TypeError:
             raise TypeError("bins argument only works with numeric data.")
         values = cat.codes
 
-    if com.is_categorical_dtype(values.dtype):
-        result = values.value_counts(dropna)
-
+    if com.is_extension_type(values) and not com.is_datetimetz(values):
+        # handle Categorical and sparse,
+        # datetime tz can be handeled in ndarray path
+        result = Series(values).values.value_counts(dropna=dropna)
+        result.name = name
+        counts = result.values
     else:
+        # ndarray path. pass original to handle DatetimeTzBlock
+        keys, counts = _value_counts_arraylike(values, dropna=dropna)
 
-        dtype = values.dtype
-        is_period = com.is_period_arraylike(values)
-        is_datetimetz = com.is_datetimetz(values)
+        from pandas import Index, Series
+        if not isinstance(keys, Index):
+            keys = Index(keys)
+        result = Series(counts, index=keys, name=name)
 
-        if com.is_datetime_or_timedelta_dtype(dtype) or is_period or \
-                is_datetimetz:
+    if bins is not None:
+        # TODO: This next line should be more efficient
+        result = result.reindex(np.arange(len(cat.categories)),
+                                fill_value=0)
+        result.index = bins[:-1]
 
-            if is_period:
-                values = PeriodIndex(values)
-            elif is_datetimetz:
-                tz = getattr(values, 'tz', None)
-                values = DatetimeIndex(values).tz_localize(None)
+    if sort:
+        result = result.sort_values(ascending=ascending)
 
-            values = values.view(np.int64)
-            keys, counts = htable.value_count_scalar64(values, dropna)
+    if normalize:
+        result = result / float(counts.sum())
 
-            if dropna:
-                msk = keys != iNaT
-                keys, counts = keys[msk], counts[msk]
+    return result
 
-            # localize to the original tz if necessary
-            if is_datetimetz:
-                keys = DatetimeIndex(keys).tz_localize(tz)
 
-            # convert the keys back to the dtype we came in
-            else:
-                keys = keys.astype(dtype)
+def _value_counts_arraylike(values, dropna=True):
+    is_datetimetz = com.is_datetimetz(values)
+    is_period = (isinstance(values, gt.ABCPeriodIndex) or
+                 com.is_period_arraylike(values))
 
-        elif com.is_integer_dtype(dtype):
-            values = com._ensure_int64(values)
-            keys, counts = htable.value_count_scalar64(values, dropna)
-        elif com.is_float_dtype(dtype):
-            values = com._ensure_float64(values)
-            keys, counts = htable.value_count_scalar64(values, dropna)
+    orig = values
 
-        else:
-            values = com._ensure_object(values)
-            mask = com.isnull(values)
-            keys, counts = htable.value_count_object(values, mask)
-            if not dropna and mask.any():
-                keys = np.insert(keys, 0, np.NaN)
-                counts = np.insert(counts, 0, mask.sum())
+    from pandas.core.series import Series
+    values = Series(values).values
+    dtype = values.dtype
 
-        if not isinstance(keys, Index):
-            keys = Index(keys)
-        result = Series(counts, index=keys, name=name)
+    if com.is_datetime_or_timedelta_dtype(dtype) or is_period:
+        from pandas.tseries.index import DatetimeIndex
+        from pandas.tseries.period import PeriodIndex
 
-        if bins is not None:
-            # TODO: This next line should be more efficient
-            result = result.reindex(np.arange(len(cat.categories)),
-                                    fill_value=0)
-            result.index = bins[:-1]
+        if is_period:
+            values = PeriodIndex(values)
+            freq = values.freq
 
-    if sort:
-        result = result.sort_values(ascending=ascending)
+        values = values.view(np.int64)
+        keys, counts = htable.value_count_scalar64(values, dropna)
 
-    if normalize:
-        result = result / float(counts.sum())
+        if dropna:
+            msk = keys != iNaT
+            keys, counts = keys[msk], counts[msk]
 
-    return result
+        # convert the keys back to the dtype we came in
+        keys = keys.astype(dtype)
+
+        # dtype handling
+        if is_datetimetz:
+            if isinstance(orig, gt.ABCDatetimeIndex):
+                tz = orig.tz
+            else:
+                tz = orig.dt.tz
+            keys = DatetimeIndex._simple_new(keys, tz=tz)
+        if is_period:
+            keys = PeriodIndex._simple_new(keys, freq=freq)
+
+    elif com.is_integer_dtype(dtype):
+        values = com._ensure_int64(values)
+        keys, counts = htable.value_count_scalar64(values, dropna)
+    elif com.is_float_dtype(dtype):
+        values = com._ensure_float64(values)
+        keys, counts = htable.value_count_scalar64(values, dropna)
+    else:
+        values = com._ensure_object(values)
+        mask = com.isnull(values)
+        keys, counts = htable.value_count_object(values, mask)
+        if not dropna and mask.any():
+            keys = np.insert(keys, 0, np.NaN)
+            counts = np.insert(counts, 0, mask.sum())
+
+    return keys, counts
 
 
 def mode(values):
diff --git a/pandas/core/base.py b/pandas/core/base.py
index ba9702f4b..0d2b450f5 100644
--- a/pandas/core/base.py
+++ b/pandas/core/base.py
@@ -10,6 +10,7 @@ import pandas.lib as lib
 from pandas.util.decorators import (Appender, cache_readonly,
                                     deprecate_kwarg, Substitution)
 from pandas.core.common import AbstractMethodError
+from pandas.types import api as gt
 from pandas.formats.printing import pprint_thing
 
 _shared_docs = dict()
@@ -291,15 +292,15 @@ class SelectionMixin(object):
 
     @property
     def _selection_list(self):
-        if not isinstance(self._selection, (list, tuple, com.ABCSeries,
-                                            com.ABCIndex, np.ndarray)):
+        if not isinstance(self._selection, (list, tuple, gt.ABCSeries,
+                                            gt.ABCIndex, np.ndarray)):
             return [self._selection]
         return self._selection
 
     @cache_readonly
     def _selected_obj(self):
 
-        if self._selection is None or isinstance(self.obj, com.ABCSeries):
+        if self._selection is None or isinstance(self.obj, gt.ABCSeries):
             return self.obj
         else:
             return self.obj[self._selection]
@@ -311,7 +312,7 @@ class SelectionMixin(object):
     @cache_readonly
     def _obj_with_exclusions(self):
         if self._selection is not None and isinstance(self.obj,
-                                                      com.ABCDataFrame):
+                                                      gt.ABCDataFrame):
             return self.obj.reindex(columns=self._selection_list)
 
         if len(self.exclusions) > 0:
@@ -323,7 +324,7 @@ class SelectionMixin(object):
         if self._selection is not None:
             raise Exception('Column(s) %s already selected' % self._selection)
 
-        if isinstance(key, (list, tuple, com.ABCSeries, com.ABCIndex,
+        if isinstance(key, (list, tuple, gt.ABCSeries, gt.ABCIndex,
                             np.ndarray)):
             if len(self.obj.columns.intersection(key)) != len(key):
                 bad_keys = list(set(key).difference(self.obj.columns))
@@ -551,7 +552,7 @@ pandas.DataFrame.%(name)s
             if isinstance(result, list):
                 result = concat(result, keys=keys, axis=1)
             elif isinstance(list(compat.itervalues(result))[0],
-                            com.ABCDataFrame):
+                            gt.ABCDataFrame):
                 result = concat([result[k] for k in keys], keys=keys, axis=1)
             else:
                 from pandas import DataFrame
@@ -940,17 +941,8 @@ class IndexOpsMixin(object):
         counts : Series
         """
         from pandas.core.algorithms import value_counts
-        from pandas.tseries.api import DatetimeIndex, PeriodIndex
         result = value_counts(self, sort=sort, ascending=ascending,
                               normalize=normalize, bins=bins, dropna=dropna)
-
-        if isinstance(self, PeriodIndex):
-            # preserve freq
-            result.index = self._simple_new(result.index.values,
-                                            freq=self.freq)
-        elif isinstance(self, DatetimeIndex):
-            result.index = self._simple_new(result.index.values,
-                                            tz=getattr(self, 'tz', None))
         return result
 
     def unique(self):
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index ff7bd8195..793a0c237 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -7,6 +7,7 @@ from __future__ import division
 from numpy import nan, ndarray
 import numpy as np
 
+import pandas as pd
 from pandas.core.base import PandasObject
 import pandas.core.common as com
 
@@ -16,6 +17,7 @@ from pandas.compat import range
 from pandas._sparse import SparseIndex, BlockIndex, IntIndex
 import pandas._sparse as splib
 import pandas.index as _index
+import pandas.core.algorithms as algos
 import pandas.core.ops as ops
 import pandas.formats.printing as printing
 from pandas.util.decorators import Appender
@@ -503,6 +505,42 @@ class SparseArray(PandasObject, np.ndarray):
             nsparse = self.sp_index.ngaps
             return (sp_sum + self.fill_value * nsparse) / (ct + nsparse)
 
+    def value_counts(self, dropna=True):
+        """
+        Returns a Series containing counts of unique values.
+
+        Parameters
+        ----------
+        dropna : boolean, default True
+            Don't include counts of NaN, even if NaN is in sp_values.
+
+        Returns
+        -------
+        counts : Series
+        """
+        keys, counts = algos._value_counts_arraylike(self.sp_values,
+                                                     dropna=dropna)
+        fcounts = self.sp_index.ngaps
+        if fcounts > 0:
+            if self._null_fill_value and dropna:
+                pass
+            else:
+                if self._null_fill_value:
+                    mask = pd.isnull(keys)
+                else:
+                    mask = keys == self.fill_value
+
+                if mask.any():
+                    counts[mask] += fcounts
+                else:
+                    keys = np.insert(keys, 0, self.fill_value)
+                    counts = np.insert(counts, 0, fcounts)
+
+        if not isinstance(keys, pd.Index):
+            keys = pd.Index(keys)
+        result = pd.Series(counts, index=keys)
+        return result
+
 
 def _maybe_to_dense(obj):
     """ try to convert to dense """
diff --git a/pandas/sparse/tests/test_series.py b/pandas/sparse/tests/test_series.py
index f8955e526..4c6c61cea 100644
--- a/pandas/sparse/tests/test_series.py
+++ b/pandas/sparse/tests/test_series.py
@@ -1040,6 +1040,174 @@ class TestSparseSeriesScipyInteraction(tm.TestCase):
         assert_equal(il, il_result)
         assert_equal(jl, jl_result)
 
+    def test_concat(self):
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        for kind in ['integer', 'block']:
+            sparse1 = pd.SparseSeries(val1, name='x', kind=kind)
+            sparse2 = pd.SparseSeries(val2, name='y', kind=kind)
+
+            res = pd.concat([sparse1, sparse2])
+            exp = pd.concat([pd.Series(val1), pd.Series(val2)])
+            exp = pd.SparseSeries(exp, kind=kind)
+            tm.assert_sp_series_equal(res, exp)
+
+            sparse1 = pd.SparseSeries(val1, fill_value=0, name='x', kind=kind)
+            sparse2 = pd.SparseSeries(val2, fill_value=0, name='y', kind=kind)
+
+            res = pd.concat([sparse1, sparse2])
+            exp = pd.concat([pd.Series(val1), pd.Series(val2)])
+            exp = pd.SparseSeries(exp, fill_value=0, kind=kind)
+            tm.assert_sp_series_equal(res, exp)
+
+    def test_concat_axis1(self):
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        sparse1 = pd.SparseSeries(val1, name='x')
+        sparse2 = pd.SparseSeries(val2, name='y')
+
+        res = pd.concat([sparse1, sparse2], axis=1)
+        exp = pd.concat([pd.Series(val1, name='x'),
+                         pd.Series(val2, name='y')], axis=1)
+        exp = pd.SparseDataFrame(exp)
+        tm.assert_sp_frame_equal(res, exp)
+
+    def test_concat_different_fill(self):
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        for kind in ['integer', 'block']:
+            sparse1 = pd.SparseSeries(val1, name='x', kind=kind)
+            sparse2 = pd.SparseSeries(val2, name='y', kind=kind, fill_value=0)
+
+            res = pd.concat([sparse1, sparse2])
+            exp = pd.concat([pd.Series(val1), pd.Series(val2)])
+            exp = pd.SparseSeries(exp, kind=kind)
+            tm.assert_sp_series_equal(res, exp)
+
+            res = pd.concat([sparse2, sparse1])
+            exp = pd.concat([pd.Series(val2), pd.Series(val1)])
+            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)
+            tm.assert_sp_series_equal(res, exp)
+
+    def test_concat_axis1_different_fill(self):
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        sparse1 = pd.SparseSeries(val1, name='x')
+        sparse2 = pd.SparseSeries(val2, name='y', fill_value=0)
+
+        res = pd.concat([sparse1, sparse2], axis=1)
+        exp = pd.concat([pd.Series(val1, name='x'),
+                         pd.Series(val2, name='y')], axis=1)
+        self.assertIsInstance(res, pd.SparseDataFrame)
+        tm.assert_frame_equal(res.to_dense(), exp)
+
+    def test_concat_different_kind(self):
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        sparse1 = pd.SparseSeries(val1, name='x', kind='integer')
+        sparse2 = pd.SparseSeries(val2, name='y', kind='block', fill_value=0)
+
+        res = pd.concat([sparse1, sparse2])
+        exp = pd.concat([pd.Series(val1), pd.Series(val2)])
+        exp = pd.SparseSeries(exp, kind='integer')
+        tm.assert_sp_series_equal(res, exp)
+
+        res = pd.concat([sparse2, sparse1])
+        exp = pd.concat([pd.Series(val2), pd.Series(val1)])
+        exp = pd.SparseSeries(exp, kind='block', fill_value=0)
+        tm.assert_sp_series_equal(res, exp)
+
+    def test_concat_sparse_dense(self):
+        # use first input's fill_value
+        val1 = np.array([1, 2, np.nan, np.nan, 0, np.nan])
+        val2 = np.array([3, np.nan, 4, 0, 0])
+
+        for kind in ['integer', 'block']:
+            sparse = pd.SparseSeries(val1, name='x', kind=kind)
+            dense = pd.Series(val2, name='y')
+
+            res = pd.concat([sparse, dense])
+            exp = pd.concat([pd.Series(val1), dense])
+            exp = pd.SparseSeries(exp, kind=kind)
+            tm.assert_sp_series_equal(res, exp)
+
+            res = pd.concat([dense, sparse, dense])
+            exp = pd.concat([dense, pd.Series(val1), dense])
+            exp = pd.SparseSeries(exp, kind=kind)
+            tm.assert_sp_series_equal(res, exp)
+
+            sparse = pd.SparseSeries(val1, name='x', kind=kind, fill_value=0)
+            dense = pd.Series(val2, name='y')
+
+            res = pd.concat([sparse, dense])
+            exp = pd.concat([pd.Series(val1), dense])
+            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)
+            tm.assert_sp_series_equal(res, exp)
+
+            res = pd.concat([dense, sparse, dense])
+            exp = pd.concat([dense, pd.Series(val1), dense])
+            exp = pd.SparseSeries(exp, kind=kind, fill_value=0)
+            tm.assert_sp_series_equal(res, exp)
+
+    def test_value_counts(self):
+        vals = [1, 2, nan, 0, nan, 1, 2, nan, nan, 1, 2, 0, 1, 1]
+        dense = pd.Series(vals, name='xx')
+
+        sparse = pd.SparseSeries(vals, name='xx')
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
+        sparse = pd.SparseSeries(vals, name='xx', fill_value=0)
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
+    def test_value_counts_dup(self):
+        vals = [1, 2, nan, 0, nan, 1, 2, nan, nan, 1, 2, 0, 1, 1]
+
+        # numeric op may cause sp_values to include the same value as
+        # fill_value
+        dense = pd.Series(vals, name='xx') / 0.
+        sparse = pd.SparseSeries(vals, name='xx') / 0.
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
+        vals = [1, 2, 0, 0, 0, 1, 2, 0, 0, 1, 2, 0, 1, 1]
+
+        dense = pd.Series(vals, name='xx') * 0.
+        sparse = pd.SparseSeries(vals, name='xx') * 0.
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
+    def test_value_counts_int(self):
+        vals = [1, 2, 0, 1, 2, 1, 2, 0, 1, 1]
+        dense = pd.Series(vals, name='xx')
+
+        # fill_value is np.nan, but should not be included in the result
+        sparse = pd.SparseSeries(vals, name='xx')
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
+        sparse = pd.SparseSeries(vals, name='xx', fill_value=0)
+        tm.assert_series_equal(sparse.value_counts(),
+                               dense.value_counts())
+        tm.assert_series_equal(sparse.value_counts(dropna=False),
+                               dense.value_counts(dropna=False))
+
 
 def _dense_series_compare(s, f):
     result = f(s)
diff --git a/pandas/tests/series/test_analytics.py b/pandas/tests/series/test_analytics.py
index af648d346..fabc9306c 100644
--- a/pandas/tests/series/test_analytics.py
+++ b/pandas/tests/series/test_analytics.py
@@ -1669,8 +1669,6 @@ class TestSeriesAnalytics(TestData, tm.TestCase):
         left = ts.unstack()
         right = DataFrame([[nan, 1], [2, nan]], index=[101, 102],
                           columns=[nan, 3.5])
-        print(left)
-        print(right)
         assert_frame_equal(left, right)
 
         idx = pd.MultiIndex.from_arrays([['cat', 'cat', 'cat', 'dog', 'dog'
@@ -1682,3 +1680,112 @@ class TestSeriesAnalytics(TestData, tm.TestCase):
         tpls = [('a', 1), ('a', 2), ('b', nan), ('b', 1)]
         right.index = pd.MultiIndex.from_tuples(tpls)
         assert_frame_equal(ts.unstack(level=0), right)
+
+    def test_value_counts_datetime(self):
+        # most dtypes are tested in test_base.py
+        values = [pd.Timestamp('2011-01-01 09:00'),
+                  pd.Timestamp('2011-01-01 10:00'),
+                  pd.Timestamp('2011-01-01 11:00'),
+                  pd.Timestamp('2011-01-01 09:00'),
+                  pd.Timestamp('2011-01-01 09:00'),
+                  pd.Timestamp('2011-01-01 11:00')]
+
+        exp_idx = pd.DatetimeIndex(['2011-01-01 09:00', '2011-01-01 11:00',
+                                    '2011-01-01 10:00'])
+        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')
+
+        s = pd.Series(values, name='xxx')
+        tm.assert_series_equal(s.value_counts(), exp)
+        # check DatetimeIndex outputs the same result
+        idx = pd.DatetimeIndex(values, name='xxx')
+        tm.assert_series_equal(idx.value_counts(), exp)
+
+        # normalize
+        exp = pd.Series(np.array([3., 2., 1]) / 6.,
+                        index=exp_idx, name='xxx')
+        tm.assert_series_equal(s.value_counts(normalize=True), exp)
+        tm.assert_series_equal(idx.value_counts(normalize=True), exp)
+
+    def test_value_counts_datetime_tz(self):
+        values = [pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),
+                  pd.Timestamp('2011-01-01 10:00', tz='US/Eastern'),
+                  pd.Timestamp('2011-01-01 11:00', tz='US/Eastern'),
+                  pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),
+                  pd.Timestamp('2011-01-01 09:00', tz='US/Eastern'),
+                  pd.Timestamp('2011-01-01 11:00', tz='US/Eastern')]
+
+        exp_idx = pd.DatetimeIndex(['2011-01-01 09:00', '2011-01-01 11:00',
+                                    '2011-01-01 10:00'], tz='US/Eastern')
+        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')
+
+        s = pd.Series(values, name='xxx')
+        tm.assert_series_equal(s.value_counts(), exp)
+        idx = pd.DatetimeIndex(values, name='xxx')
+        tm.assert_series_equal(idx.value_counts(), exp)
+
+        exp = pd.Series(np.array([3., 2., 1]) / 6.,
+                        index=exp_idx, name='xxx')
+        tm.assert_series_equal(s.value_counts(normalize=True), exp)
+        tm.assert_series_equal(idx.value_counts(normalize=True), exp)
+
+    def test_value_counts_period(self):
+        values = [pd.Period('2011-01', freq='M'),
+                  pd.Period('2011-02', freq='M'),
+                  pd.Period('2011-03', freq='M'),
+                  pd.Period('2011-01', freq='M'),
+                  pd.Period('2011-01', freq='M'),
+                  pd.Period('2011-03', freq='M')]
+
+        exp_idx = pd.PeriodIndex(['2011-01', '2011-03', '2011-02'], freq='M')
+        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')
+
+        s = pd.Series(values, name='xxx')
+        tm.assert_series_equal(s.value_counts(), exp)
+        # check DatetimeIndex outputs the same result
+        idx = pd.PeriodIndex(values, name='xxx')
+        tm.assert_series_equal(idx.value_counts(), exp)
+
+        # normalize
+        exp = pd.Series(np.array([3., 2., 1]) / 6.,
+                        index=exp_idx, name='xxx')
+        tm.assert_series_equal(s.value_counts(normalize=True), exp)
+        tm.assert_series_equal(idx.value_counts(normalize=True), exp)
+
+    def test_value_counts_categorical_ordered(self):
+        # most dtypes are tested in test_base.py
+        values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=True)
+
+        exp_idx = pd.CategoricalIndex([1, 3, 2], categories=[1, 2, 3],
+                                      ordered=True)
+        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')
+
+        s = pd.Series(values, name='xxx')
+        tm.assert_series_equal(s.value_counts(), exp)
+        # check CategoricalIndex outputs the same result
+        idx = pd.CategoricalIndex(values, name='xxx')
+        tm.assert_series_equal(idx.value_counts(), exp)
+
+        # normalize
+        exp = pd.Series(np.array([3., 2., 1]) / 6.,
+                        index=exp_idx, name='xxx')
+        tm.assert_series_equal(s.value_counts(normalize=True), exp)
+        tm.assert_series_equal(idx.value_counts(normalize=True), exp)
+
+    def test_value_counts_categorical_not_ordered(self):
+        values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=False)
+
+        exp_idx = pd.CategoricalIndex([1, 3, 2], categories=[1, 2, 3],
+                                      ordered=False)
+        exp = pd.Series([3, 2, 1], index=exp_idx, name='xxx')
+
+        s = pd.Series(values, name='xxx')
+        tm.assert_series_equal(s.value_counts(), exp)
+        # check CategoricalIndex outputs the same result
+        idx = pd.CategoricalIndex(values, name='xxx')
+        tm.assert_series_equal(idx.value_counts(), exp)
+
+        # normalize
+        exp = pd.Series(np.array([3., 2., 1]) / 6.,
+                        index=exp_idx, name='xxx')
+        tm.assert_series_equal(s.value_counts(normalize=True), exp)
+        tm.assert_series_equal(idx.value_counts(normalize=True), exp)
diff --git a/pandas/tests/test_categorical.py b/pandas/tests/test_categorical.py
index a1cc05b0c..ceeb61c5c 100644
--- a/pandas/tests/test_categorical.py
+++ b/pandas/tests/test_categorical.py
@@ -2825,18 +2825,27 @@ Categories (10, timedelta64[ns]): [0 days 01:00:00 < 1 days 01:00:00 < 2 days 01
         tm.assert_series_equal(res, exp)
 
     def test_value_counts(self):
-
-        s = pd.Series(pd.Categorical(
-            ["a", "b", "c", "c", "c", "b"], categories=["c", "a", "b", "d"]))
+        # GH 12835
+        cats = pd.Categorical(["a", "b", "c", "c", "c", "b"],
+                              categories=["c", "a", "b", "d"])
+        s = pd.Series(cats, name='xxx')
         res = s.value_counts(sort=False)
-        exp = Series([3, 1, 2, 0],
+        exp = Series([3, 1, 2, 0], name='xxx',
                      index=pd.CategoricalIndex(["c", "a", "b", "d"]))
         tm.assert_series_equal(res, exp)
+
         res = s.value_counts(sort=True)
-        exp = Series([3, 2, 1, 0],
+        exp = Series([3, 2, 1, 0], name='xxx',
                      index=pd.CategoricalIndex(["c", "b", "a", "d"]))
         tm.assert_series_equal(res, exp)
 
+        # check object dtype handles the Series.name as the same
+        # (tested in test_base.py)
+        s = pd.Series(["a", "b", "c", "c", "c", "b"], name='xxx')
+        res = s.value_counts()
+        exp = Series([3, 2, 1], name='xxx', index=["c", "b", "a"])
+        tm.assert_series_equal(res, exp)
+
     def test_value_counts_with_nan(self):
         # https://github.com/pydata/pandas/issues/9443
 
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index b2672ad5d..0e2dbb7d5 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -2,8 +2,6 @@
 from __future__ import print_function
 import nose
 
-from numpy.testing.decorators import slow
-
 from datetime import datetime
 from numpy import nan
 
@@ -1875,7 +1873,6 @@ class TestGroupBy(tm.TestCase):
             check_nunique(frame, ['jim'])
             check_nunique(frame, ['jim', 'joe'])
 
-    @slow
     def test_series_groupby_value_counts(self):
         from itertools import product
 
@@ -1910,7 +1907,7 @@ class TestGroupBy(tm.TestCase):
 
         days = date_range('2015-08-24', periods=10)
 
-        for n, m in product((100, 10000), (5, 20)):
+        for n, m in product((100, 1000), (5, 20)):
             frame = DataFrame({
                 '1st': np.random.choice(
                     list('abcd'), n),
