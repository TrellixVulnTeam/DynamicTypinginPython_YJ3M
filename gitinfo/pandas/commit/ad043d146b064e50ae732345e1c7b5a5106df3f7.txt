commit ad043d146b064e50ae732345e1c7b5a5106df3f7
Author: luzpaz <luzpaz@users.noreply.github.com>
Date:   Sat Dec 30 11:15:08 2017 -0500

    Misc. Typo fixes (#18966)

diff --git a/appveyor.yml b/appveyor.yml
index 44af73b49..0aaac322c 100644
--- a/appveyor.yml
+++ b/appveyor.yml
@@ -11,7 +11,7 @@ matrix:
 environment:
   global:
     # SDK v7.0 MSVC Express 2008's SetEnv.cmd script will fail if the
-    # /E:ON and /V:ON options are not enabled in the batch script intepreter
+    # /E:ON and /V:ON options are not enabled in the batch script interpreter
     # See: http://stackoverflow.com/a/13751649/163740
     CMD_IN_ENV: "cmd /E:ON /V:ON /C .\\ci\\run_with_env.cmd"
     clone_folder: C:\projects\pandas
diff --git a/asv_bench/benchmarks/frame_ctor.py b/asv_bench/benchmarks/frame_ctor.py
index 391a209cb..21b20cb12 100644
--- a/asv_bench/benchmarks/frame_ctor.py
+++ b/asv_bench/benchmarks/frame_ctor.py
@@ -4,7 +4,7 @@ from pandas import DataFrame, Series, MultiIndex, Timestamp, date_range
 try:
     from pandas.tseries.offsets import Nano, Hour
 except ImportError:
-    # For compatability with older versions
+    # For compatibility with older versions
     from pandas.core.datetools import * # noqa
 
 from .pandas_vb_common import setup # noqa
diff --git a/asv_bench/benchmarks/pandas_vb_common.py b/asv_bench/benchmarks/pandas_vb_common.py
index 4de87ddcb..7b4fec009 100644
--- a/asv_bench/benchmarks/pandas_vb_common.py
+++ b/asv_bench/benchmarks/pandas_vb_common.py
@@ -18,8 +18,8 @@ numeric_dtypes = [np.int64, np.int32, np.uint32, np.uint64, np.float32,
                   np.float64, np.int16, np.int8, np.uint16, np.uint8]
 datetime_dtypes = [np.datetime64, np.timedelta64]
 
-# This function just needs to be imported into each benchmark file in order to 
-# sets up the random seed before each function. 
+# This function just needs to be imported into each benchmark file in order to
+# sets up the random seed before each function.
 # http://asv.readthedocs.io/en/latest/writing_benchmarks.html
 def setup(*args, **kwargs):
     np.random.seed(1234)
@@ -36,14 +36,14 @@ class BaseIO(object):
         try:
             os.remove(f)
         except:
-            # On Windows, attempting to remove a file that is in use 
+            # On Windows, attempting to remove a file that is in use
             # causes an exception to be raised
             pass
 
     def teardown(self, *args, **kwargs):
         self.remove(self.fname)
 
-# Compatability import for lib
+# Compatibility import for lib
 for imp in ['pandas._libs.lib', 'pandas.lib', 'pandas_tseries']:
     try:
         lib = import_module(imp)
diff --git a/doc/source/_static/banklist.html b/doc/source/_static/banklist.html
index 8ec1561f8..cbcce5a2d 100644
--- a/doc/source/_static/banklist.html
+++ b/doc/source/_static/banklist.html
@@ -7,7 +7,7 @@
 <meta charset="UTF-8">
 <!-- Unicode character encoding -->
 <meta http-equiv="X-UA-Compatible" content="IE=edge">
-<!-- Turns off IE Compatiblity Mode -->
+<!-- Turns off IE Compatibility Mode -->
 <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
 <!-- Makes it so phones don't auto zoom out. -->
 <meta name="author" content="DRR">
@@ -4849,7 +4849,7 @@ prepare_responsive_header_nav();
 		<ul>
 			<li><a href="/about/freedom/" title="Freedom of Information Act (FOIA) Service Center">Freedom of Information Act (FOIA) Service Center</a></li>
 			<li><a href="/open/" title="FDIC Open Government Webpage">FDIC Open Government Webpage</a></li>
-			<li><a href="/about/diversity/nofear/" title="No FEAR Act Data">No  FEAR Act Data</a></li>
+			<li><a href="/about/diversity/nofear/" title="No FEAR Act Data">No FEAR Act Data</a></li>
 		</ul>
 	</div>
 	<div id="responsive_footer-small">
diff --git a/doc/source/api.rst b/doc/source/api.rst
index 68721b76e..17f6b8df0 100644
--- a/doc/source/api.rst
+++ b/doc/source/api.rst
@@ -10,7 +10,7 @@ methods. In general, all classes and functions exposed in the top-level
 ``pandas.*`` namespace are regarded as public.
 
 Further some of the subpackages are public, including ``pandas.errors``,
-``pandas.plotting``, and ``pandas.testing``. Certain functions in the the
+``pandas.plotting``, and ``pandas.testing``. Certain functions in the
 ``pandas.io`` and ``pandas.tseries`` submodules are public as well (those
 mentioned in the documentation). Further, the ``pandas.api.types`` subpackage
 holds some public functions related to data types in pandas.
diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index da82f56d3..74b3dbb83 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -947,7 +947,7 @@ Mixed Dtypes
 ++++++++++++
 
 When presented with mixed dtypes that cannot aggregate, ``.agg`` will only take the valid
-aggregations. This is similiar to how groupby ``.agg`` works.
+aggregations. This is similar to how groupby ``.agg`` works.
 
 .. ipython:: python
 
diff --git a/doc/source/computation.rst b/doc/source/computation.rst
index 0994d3599..30071c6c5 100644
--- a/doc/source/computation.rst
+++ b/doc/source/computation.rst
@@ -247,7 +247,7 @@ These are created from methods on ``Series`` and ``DataFrame``.
    r = s.rolling(window=60)
    r
 
-These object provide tab-completion of the avaible methods and properties.
+These object provide tab-completion of the available methods and properties.
 
 .. code-block:: ipython
 
diff --git a/doc/source/conf.py b/doc/source/conf.py
index bcb83d569..c188f83f8 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -380,7 +380,7 @@ ipython_exec_lines = [
     'import pandas as pd',
     # This ensures correct rendering on system with console encoding != utf8
     # (windows). It forces pandas to encode its output reprs using utf8
-    # whereever the docs are built. The docs' target is the browser, not
+    # wherever the docs are built. The docs' target is the browser, not
     # the console, so this is fine.
     'pd.options.display.encoding="utf8"'
     ]
diff --git a/doc/source/contributing.rst b/doc/source/contributing.rst
index 0e5d70135..b25f9779d 100644
--- a/doc/source/contributing.rst
+++ b/doc/source/contributing.rst
@@ -124,16 +124,16 @@ to build the documentation locally before pushing your changes.
 
 .. _contributiong.dev_c:
 
-Installing a C Complier
+Installing a C Compiler
 ~~~~~~~~~~~~~~~~~~~~~~~
 
 Pandas uses C extensions (mostly written using Cython) to speed up certain
 operations. To install pandas from source, you need to compile these C
-extensions, which means you need a C complier. This process depends on which
+extensions, which means you need a C compiler. This process depends on which
 platform you're using. Follow the `CPython contributing guidelines
 <https://docs.python.org/devguide/setup.html#build-dependencies>`_ for getting a
-complier installed. You don't need to do any of the ``./configure`` or ``make``
-steps; you only need to install the complier.
+compiler installed. You don't need to do any of the ``./configure`` or ``make``
+steps; you only need to install the compiler.
 
 For Windows developers, the following links may be helpful.
 
@@ -151,7 +151,7 @@ Let us know if you have any difficulties by opening an issue or reaching out on
 Creating a Python Environment
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-Now that you have a C complier, create an isolated pandas development
+Now that you have a C compiler, create an isolated pandas development
 environment:
 
 - Install either `Anaconda <https://www.anaconda.com/download/>`_ or `miniconda
diff --git a/doc/source/developer.rst b/doc/source/developer.rst
index 9c214020a..b8bb2b2fc 100644
--- a/doc/source/developer.rst
+++ b/doc/source/developer.rst
@@ -40,7 +40,7 @@ where ``KeyValue`` is
    }
 
 So that a ``pandas.DataFrame`` can be faithfully reconstructed, we store a
-``pandas`` metadata key in the ``FileMetaData`` with the the value stored as :
+``pandas`` metadata key in the ``FileMetaData`` with the value stored as :
 
 .. code-block:: text
 
diff --git a/doc/source/dsintro.rst b/doc/source/dsintro.rst
index c8018c8e6..da9d2123b 100644
--- a/doc/source/dsintro.rst
+++ b/doc/source/dsintro.rst
@@ -470,7 +470,7 @@ derived from existing columns.
         .head())
 
 In the example above, we inserted a precomputed value. We can also pass in
-a function of one argument to be evalutated on the DataFrame being assigned to.
+a function of one argument to be evaluated on the DataFrame being assigned to.
 
 .. ipython:: python
 
@@ -957,7 +957,7 @@ pandas to focus on these areas exclusively.
 
 Oftentimes, one can simply use a MultiIndex ``DataFrame`` for easily working with higher dimensional data.
 
-In additon, the ``xarray`` package was built from the ground up, specifically in order to
+In addition, the ``xarray`` package was built from the ground up, specifically in order to
 support the multi-dimensional analysis that is one of ``Panel`` s main usecases.
 `Here is a link to the xarray panel-transition documentation <http://xarray.pydata.org/en/stable/pandas.html#panel-transition>`__.
 
diff --git a/doc/source/enhancingperf.rst b/doc/source/enhancingperf.rst
index d2ca76713..362c99849 100644
--- a/doc/source/enhancingperf.rst
+++ b/doc/source/enhancingperf.rst
@@ -173,7 +173,7 @@ Using ndarray
 
 It's calling series... a lot! It's creating a Series from each row, and get-ting from both
 the index and the series (three times for each row). Function calls are expensive
-in python, so maybe we could minimise these by cythonizing the apply part.
+in python, so maybe we could minimize these by cythonizing the apply part.
 
 .. note::
 
@@ -578,7 +578,7 @@ on the original ``DataFrame`` or return a copy with the new column.
 
 .. warning::
 
-   For backwards compatability, ``inplace`` defaults to ``True`` if not
+   For backwards compatibility, ``inplace`` defaults to ``True`` if not
    specified. This will change in a future version of pandas - if your
    code depends on an inplace assignment you should update to explicitly
    set ``inplace=True``
diff --git a/doc/source/indexing.rst b/doc/source/indexing.rst
index b9223c6ad..355be5039 100644
--- a/doc/source/indexing.rst
+++ b/doc/source/indexing.rst
@@ -651,7 +651,7 @@ Indexing with list with missing labels is Deprecated
 
 In prior versions, using ``.loc[list-of-labels]`` would work as long as *at least 1* of the keys was found (otherwise it
 would raise a ``KeyError``). This behavior is deprecated and will show a warning message pointing to this section. The
-recommeded alternative is to use ``.reindex()``.
+recommended alternative is to use ``.reindex()``.
 
 For example.
 
@@ -724,7 +724,7 @@ Having a duplicated index will raise for a ``.reindex()``:
    In [17]: s.reindex(labels)
    ValueError: cannot reindex from a duplicate axis
 
-Generally, you can interesect the desired labels with the current
+Generally, you can intersect the desired labels with the current
 axis, and then reindex.
 
 .. ipython:: python
diff --git a/doc/source/install.rst b/doc/source/install.rst
index 6133da220..c4e331d64 100644
--- a/doc/source/install.rst
+++ b/doc/source/install.rst
@@ -152,7 +152,7 @@ To install pandas for Python 2 you may need to use the package ``python-pandas``
     Debian, stable, `official Debian repository <http://packages.debian.org/search?keywords=pandas&searchon=names&suite=all&section=all>`__ , ``sudo apt-get install python3-pandas``
     Debian & Ubuntu, unstable (latest packages), `NeuroDebian <http://neuro.debian.net/index.html#how-to-use-this-repository>`__ , ``sudo apt-get install python3-pandas``
     Ubuntu, stable, `official Ubuntu repository <http://packages.ubuntu.com/search?keywords=pandas&searchon=names&suite=all&section=all>`__ , ``sudo apt-get install python3-pandas``
-    OpenSuse, stable, `OpenSuse Repository  <http://software.opensuse.org/package/python-pandas?search_term=pandas>`__ , ``zypper in  python3-pandas``
+    OpenSuse, stable, `OpenSuse Repository  <http://software.opensuse.org/package/python-pandas?search_term=pandas>`__ , ``zypper in python3-pandas``
     Fedora, stable, `official Fedora repository  <https://admin.fedoraproject.org/pkgdb/package/rpms/python-pandas/>`__ , ``dnf install python3-pandas``
     Centos/RHEL, stable, `EPEL repository <https://admin.fedoraproject.org/pkgdb/package/rpms/python-pandas/>`__ , ``yum install python3-pandas``
 
diff --git a/doc/source/io.rst b/doc/source/io.rst
index a5a0a4114..49d742d99 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1048,7 +1048,7 @@ The ``thousands`` keyword allows integers to be parsed correctly
 NA Values
 '''''''''
 
-To control which values are parsed as missing values (which are signified by ``NaN``), specifiy a
+To control which values are parsed as missing values (which are signified by ``NaN``), specify a
 string in ``na_values``. If you specify a list of strings, then all values in
 it are considered to be missing values. If you specify a number (a ``float``, like ``5.0`` or an ``integer`` like ``5``),
 the corresponding equivalent values will also imply a missing value (in this case effectively
@@ -4153,7 +4153,7 @@ Caveats
 
 .. warning::
 
-   ``PyTables`` will show a ``NaturalNameWarning`` if a  column name
+   ``PyTables`` will show a ``NaturalNameWarning`` if a column name
    cannot be used as an attribute selector.
    *Natural* identifiers contain only letters, numbers, and underscores,
    and may not begin with a number.
@@ -4478,7 +4478,7 @@ Several caveats.
 - Non supported types include ``Period`` and actual python object types. These will raise a helpful error message
   on an attempt at serialization.
 
-You can specifiy an ``engine`` to direct the serialization. This can be one of ``pyarrow``, or ``fastparquet``, or ``auto``.
+You can specify an ``engine`` to direct the serialization. This can be one of ``pyarrow``, or ``fastparquet``, or ``auto``.
 If the engine is NOT specified, then the ``pd.options.io.parquet.engine`` option is checked; if this is also ``auto``, then
 then ``pyarrow`` is tried, and falling back to ``fastparquet``.
 
diff --git a/doc/source/options.rst b/doc/source/options.rst
index 5641b2628..cce16a539 100644
--- a/doc/source/options.rst
+++ b/doc/source/options.rst
@@ -164,7 +164,7 @@ lines are replaced by an ellipsis.
    df
    pd.reset_option('max_rows')
 
-``display.expand_frame_repr`` allows for the the representation of
+``display.expand_frame_repr`` allows for the representation of
 dataframes to stretch across pages, wrapped over the full column vs row-wise.
 
 .. ipython:: python
diff --git a/doc/source/overview.rst b/doc/source/overview.rst
index 73e7704b4..4443428ca 100644
--- a/doc/source/overview.rst
+++ b/doc/source/overview.rst
@@ -109,7 +109,7 @@ Wes McKinney is the Benevolent Dictator for Life (BDFL).
 Development Team
 -----------------
 
-The list of the  Core Team members and more detailed information can be found on the `people’s page <https://github.com/pandas-dev/pandas-governance/blob/master/people.md>`__ of the governance repo.
+The list of the Core Team members and more detailed information can be found on the `people’s page <https://github.com/pandas-dev/pandas-governance/blob/master/people.md>`__ of the governance repo.
  
 
 Institutional Partners
diff --git a/doc/source/release.rst b/doc/source/release.rst
index aea6280a4..12932d9fc 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -2043,7 +2043,7 @@ Bug Fixes
 - Fixed missing arg validation in get_options_data (:issue:`6105`)
 - Bug in assignment with duplicate columns in a frame where the locations
   are a slice (e.g. next to each other) (:issue:`6120`)
-- Bug in propogating _ref_locs during construction of a DataFrame with dups
+- Bug in propagating _ref_locs during construction of a DataFrame with dups
   index/columns (:issue:`6121`)
 - Bug in ``DataFrame.apply`` when using mixed datelike reductions (:issue:`6125`)
 - Bug in ``DataFrame.append`` when appending a row with different columns (:issue:`6129`)
@@ -2056,7 +2056,7 @@ Bug Fixes
 - Bug in ``HDFStore`` on appending a dataframe with multi-indexed columns to
   an existing table (:issue:`6167`)
 - Consistency with dtypes in setting an empty DataFrame (:issue:`6171`)
-- Bug in  selecting on a multi-index ``HDFStore`` even in the presence of under
+- Bug in selecting on a multi-index ``HDFStore`` even in the presence of under
   specified column spec (:issue:`6169`)
 - Bug in ``nanops.var`` with ``ddof=1`` and 1 elements would sometimes return ``inf``
   rather than ``nan`` on some platforms (:issue:`6136`)
@@ -2437,7 +2437,7 @@ API Changes
 - The refactoring involving``Series`` deriving from ``NDFrame`` breaks ``rpy2<=2.3.8``. an Issue
   has been opened against rpy2 and a workaround is detailed in :issue:`5698`. Thanks @JanSchulz.
 - ``Series.argmin`` and ``Series.argmax`` are now aliased to ``Series.idxmin`` and ``Series.idxmax``.
-  These return the *index* of the  min or max element respectively. Prior to 0.13.0 these would return
+  These return the *index* of the min or max element respectively. Prior to 0.13.0 these would return
   the position of the min / max element (:issue:`6214`)
 
 Internal Refactoring
@@ -3097,7 +3097,7 @@ Bug Fixes
 - Fixed bug where a time-series was being selected in preference to an actual column name
   in a frame (:issue:`3594`)
 - Make secondary_y work properly for bar plots (:issue:`3598`)
-- Fix modulo and integer division on Series,DataFrames to act similary to ``float`` dtypes to return
+- Fix modulo and integer division on Series,DataFrames to act similarly to ``float`` dtypes to return
   ``np.nan`` or ``np.inf`` as appropriate (:issue:`3590`)
 - Fix incorrect dtype on groupby with ``as_index=False`` (:issue:`3610`)
 - Fix ``read_csv/read_excel`` to correctly encode identical na_values, e.g. ``na_values=[-999.0,-999]``
@@ -3400,11 +3400,11 @@ Bug Fixes
 - Fixed bug in reshape if not passed correct input, now raises TypeError (:issue:`2719`)
 - Fixed a bug where Series ctor did not respect ordering if OrderedDict passed in (:issue:`3282`)
 - Fix NameError issue on RESO_US (:issue:`2787`)
-- Allow selection in an *unordered* timeseries to work similary
+- Allow selection in an *unordered* timeseries to work similarly
   to an *ordered* timeseries (:issue:`2437`).
 - Fix implemented ``.xs`` when called with ``axes=1`` and a level parameter (:issue:`2903`)
 - Timestamp now supports the class method fromordinal similar to datetimes (:issue:`3042`)
-- Fix issue with indexing a series with a boolean key and specifiying a 1-len list on the rhs (:issue:`2745`)
+- Fix issue with indexing a series with a boolean key and specifying a 1-len list on the rhs (:issue:`2745`)
   or a list on the rhs (:issue:`3235`)
 - Fixed bug in groupby apply when kernel generate list of arrays having unequal len (:issue:`1738`)
 - fixed handling of rolling_corr with center=True which could produce corr>1 (:issue:`3155`)
@@ -3555,7 +3555,7 @@ Bug Fixes
 - Upconvert datetime + datetime64 values when concatenating frames (:issue:`2624`)
 - Raise a more helpful error message in merge operations when one DataFrame
   has duplicate columns (:issue:`2649`)
-- Fix partial date parsing issue occuring only when code is run at EOM
+- Fix partial date parsing issue occurring only when code is run at EOM
   (:issue:`2618`)
 - Prevent MemoryError when using counting sort in sortlevel with
   high-cardinality MultiIndex objects (:issue:`2684`)
@@ -3973,7 +3973,7 @@ Bug Fixes
 - Don't lose tzinfo when passing DatetimeIndex as DataFrame column (:issue:`1682`)
 - Fix tz conversion with time zones that haven't had any DST transitions since
   first date in the array (:issue:`1673`)
-- Fix field access with  UTC->local conversion on unsorted arrays (:issue:`1756`)
+- Fix field access with UTC->local conversion on unsorted arrays (:issue:`1756`)
 - Fix isnull handling of array-like (list) inputs (:issue:`1755`)
 - Fix regression in handling of Series in Series constructor (:issue:`1671`)
 - Fix comparison of Int64Index with DatetimeIndex (:issue:`1681`)
@@ -4525,7 +4525,7 @@ Bug Fixes
 - Fix na-filling handling in mixed-type DataFrame (:issue:`910`)
 - Fix to DataFrame.set_value with non-existant row/col (:issue:`911`)
 - Fix malformed block in groupby when excluding nuisance columns (:issue:`916`)
-- Fix inconsistant NA handling in dtype=object arrays (:issue:`925`)
+- Fix inconsistent NA handling in dtype=object arrays (:issue:`925`)
 - Fix missing center-of-mass computation in ewmcov (:issue:`862`)
 - Don't raise exception when opening read-only HDF5 file (:issue:`847`)
 - Fix possible out-of-bounds memory access in 0-length Series (:issue:`917`)
@@ -5395,9 +5395,9 @@ pandas 0.4.3
 
 **Release date:** 10/9/2011
 
-is is largely a bugfix release from 0.4.2 but also includes a handful of new
-d enhanced features. Also, pandas can now be installed and used on Python 3
-hanks Thomas Kluyver!).
+This is largely a bugfix release from 0.4.2 but also includes a handful of new
+and enhanced features. Also, pandas can now be installed and used on Python 3
+(thanks Thomas Kluyver!).
 
 New Features
 ~~~~~~~~~~~~
@@ -5460,9 +5460,9 @@ pandas 0.4.2
 
 **Release date:** 10/3/2011
 
-is is a performance optimization release with several bug fixes. The new
-t64Index and new merging / joining Cython code and related Python
-frastructure are the main new additions
+This is a performance optimization release with several bug fixes. The new
+Int64Index and new merging / joining Cython code and related Python
+infrastructure are the main new additions
 
 New Features
 ~~~~~~~~~~~~
@@ -5537,7 +5537,7 @@ pandas 0.4.1
 
 **Release date:** 9/25/2011
 
-is is primarily a bug fix release but includes some new features and
+This is primarily a bug fix release but includes some new features and
 improvements
 
 New Features
diff --git a/doc/source/style.ipynb b/doc/source/style.ipynb
index 20f7c2a93..152ca9004 100644
--- a/doc/source/style.ipynb
+++ b/doc/source/style.ipynb
@@ -318,7 +318,7 @@
     "Both `Styler.apply`, and `Styler.applymap` accept a `subset` keyword.\n",
     "This allows you to apply styles to specific rows or columns, without having to code that logic into your `style` function.\n",
     "\n",
-    "The value passed to `subset` behaves simlar to slicing a DataFrame.\n",
+    "The value passed to `subset` behaves similar to slicing a DataFrame.\n",
     "\n",
     "- A scalar is treated as a column label\n",
     "- A list (or series or numpy array)\n",
diff --git a/doc/source/timeseries.rst b/doc/source/timeseries.rst
index 26e701d00..201af3c7d 100644
--- a/doc/source/timeseries.rst
+++ b/doc/source/timeseries.rst
@@ -1580,7 +1580,7 @@ We can instead only resample those groups where we have points as follows:
 Aggregation
 ~~~~~~~~~~~
 
-Similar to the :ref:`aggregating API <basics.aggregate>`, :ref:`groupby API <groupby.aggregate>`, and  the :ref:`window functions API <stats.aggregate>`,
+Similar to the :ref:`aggregating API <basics.aggregate>`, :ref:`groupby API <groupby.aggregate>`, and the :ref:`window functions API <stats.aggregate>`,
 a ``Resampler`` can be selectively resampled.
 
 Resampling a ``DataFrame``, the default will be to act on all columns with the same function.
@@ -2108,7 +2108,7 @@ tz-aware data to another time zone:
 
        It is incorrect to pass a timezone directly into the ``datetime.datetime`` constructor (e.g.,
        ``datetime.datetime(2011, 1, 1, tz=timezone('US/Eastern'))``.  Instead, the datetime
-       needs to be localized using the the localize method on the timezone.
+       needs to be localized using the localize method on the timezone.
 
 Under the hood, all timestamps are stored in UTC. Scalar values from a
 ``DatetimeIndex`` with a time zone will have their fields (day, hour, minute)
diff --git a/doc/source/visualization.rst b/doc/source/visualization.rst
index ea720a9ae..2c1d54c27 100644
--- a/doc/source/visualization.rst
+++ b/doc/source/visualization.rst
@@ -140,7 +140,7 @@ You can also create these other plots using the methods ``DataFrame.plot.<kind>`
     df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter
     df.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie
 
-In addition to these ``kind`` s, there are  the :ref:`DataFrame.hist() <visualization.hist>`,
+In addition to these ``kind`` s, there are the :ref:`DataFrame.hist() <visualization.hist>`,
 and :ref:`DataFrame.boxplot() <visualization.box>` methods, which use a separate interface.
 
 Finally, there are several :ref:`plotting functions <visualization.tools>` in ``pandas.plotting``
@@ -716,7 +716,7 @@ You can use the ``labels`` and ``colors`` keywords to specify the labels and col
 
 .. warning::
 
-   Most pandas plots use the the ``label`` and ``color`` arguments (note the lack of "s" on those).
+   Most pandas plots use the ``label`` and ``color`` arguments (note the lack of "s" on those).
    To be consistent with :func:`matplotlib.pyplot.pie` you must use ``labels`` and ``colors``.
 
 If you want to hide wedge labels, specify ``labels=None``.
@@ -1187,7 +1187,7 @@ time-series data. For limited cases where pandas cannot infer the frequency
 information (e.g., in an externally created ``twinx``), you can choose to
 suppress this behavior for alignment purposes.
 
-Here is the default behavior, notice how the x-axis tick labelling is performed:
+Here is the default behavior, notice how the x-axis tick labeling is performed:
 
 .. ipython:: python
 
diff --git a/doc/source/whatsnew/v0.10.0.txt b/doc/source/whatsnew/v0.10.0.txt
index f0db1d822..a0c4a3e00 100644
--- a/doc/source/whatsnew/v0.10.0.txt
+++ b/doc/source/whatsnew/v0.10.0.txt
@@ -369,7 +369,7 @@ Updated PyTables Support
       df1
       df1.get_dtype_counts()
 
-- performance improvments on table writing
+- performance improvements on table writing
 - support for arbitrarily indexed dimensions
 - ``SparseSeries`` now has a ``density`` property (:issue:`2384`)
 - enable ``Series.str.strip/lstrip/rstrip`` methods to take an input argument
diff --git a/doc/source/whatsnew/v0.10.1.txt b/doc/source/whatsnew/v0.10.1.txt
index d5880e44e..2d5843101 100644
--- a/doc/source/whatsnew/v0.10.1.txt
+++ b/doc/source/whatsnew/v0.10.1.txt
@@ -153,7 +153,7 @@ combined result, by using ``where`` on a selector table.
   table
 
 - You can pass ``chunksize=an integer`` to ``append``, to change the writing
-  chunksize (default is 50000). This will signficantly lower your memory usage
+  chunksize (default is 50000). This will significantly lower your memory usage
   on writing.
 
 - You can pass ``expectedrows=an integer`` to the first ``append``, to set the
diff --git a/doc/source/whatsnew/v0.11.0.txt b/doc/source/whatsnew/v0.11.0.txt
index ea149595e..b90a59781 100644
--- a/doc/source/whatsnew/v0.11.0.txt
+++ b/doc/source/whatsnew/v0.11.0.txt
@@ -88,7 +88,7 @@ Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passe
 Dtype Conversion
 ~~~~~~~~~~~~~~~~
 
-This is lower-common-denomicator upcasting, meaning you get the dtype which can accomodate all of the types
+This is lower-common-denominator upcasting, meaning you get the dtype which can accommodate all of the types
 
 .. ipython:: python
 
@@ -193,7 +193,7 @@ Furthermore ``datetime64[ns]`` columns are created by default, when passed datet
    df.loc[df.index[2:4], ['A','timestamp']] = np.nan
    df
 
-Astype conversion on ``datetime64[ns]`` to ``object``, implicity converts ``NaT`` to ``np.nan``
+Astype conversion on ``datetime64[ns]`` to ``object``, implicitly converts ``NaT`` to ``np.nan``
 
 .. ipython:: python
 
diff --git a/doc/source/whatsnew/v0.12.0.txt b/doc/source/whatsnew/v0.12.0.txt
index 27aa47a6b..ad33c4979 100644
--- a/doc/source/whatsnew/v0.12.0.txt
+++ b/doc/source/whatsnew/v0.12.0.txt
@@ -38,7 +38,7 @@ API changes
     * ``to_clipboard``
 
 
-  - Fix modulo and integer division on Series,DataFrames to act similary to ``float`` dtypes to return
+  - Fix modulo and integer division on Series,DataFrames to act similarly to ``float`` dtypes to return
     ``np.nan`` or ``np.inf`` as appropriate (:issue:`3590`). This correct a numpy bug that treats ``integer``
     and ``float`` dtypes differently.
 
@@ -154,7 +154,7 @@ API changes
 
   - The behavior of ``datetime64`` dtypes has changed with respect to certain
     so-called reduction operations (:issue:`3726`). The following operations now
-    raise a ``TypeError`` when perfomed on a ``Series`` and return an *empty*
+    raise a ``TypeError`` when performed on a ``Series`` and return an *empty*
     ``Series`` when performed on a ``DataFrame`` similar to performing these
     operations on, for example, a ``DataFrame`` of ``slice`` objects:
 
@@ -206,11 +206,11 @@ I/O Enhancements
       :ref:`See the installation docs<install.optional_dependencies>`
 
   - Added module for reading and writing Stata files: ``pandas.io.stata`` (:issue:`1512`)
-    accessable via ``read_stata`` top-level function for reading,
+    accessible via ``read_stata`` top-level function for reading,
     and ``to_stata`` DataFrame method for writing, :ref:`See the docs<io.stata>`
 
   - Added module for reading and writing json format files: ``pandas.io.json``
-    accessable via ``read_json`` top-level function for reading,
+    accessible via ``read_json`` top-level function for reading,
     and ``to_json`` DataFrame method for writing, :ref:`See the docs<io.json>`
     various issues (:issue:`1226`, :issue:`3804`, :issue:`3876`, :issue:`3867`, :issue:`1305`)
 
@@ -220,7 +220,7 @@ I/O Enhancements
       list of the rows from which to read the index.
 
     - The option, ``tupleize_cols`` can now be specified in both ``to_csv`` and
-      ``read_csv``, to provide compatiblity for the pre 0.12 behavior of
+      ``read_csv``, to provide compatibility for the pre 0.12 behavior of
       writing and reading ``MultIndex`` columns via a list of tuples. The default in
       0.12 is to write lists of tuples and *not* interpret list of tuples as a
       ``MultiIndex`` column.
diff --git a/doc/source/whatsnew/v0.14.0.txt b/doc/source/whatsnew/v0.14.0.txt
index f1feab4b9..be962ceb1 100644
--- a/doc/source/whatsnew/v0.14.0.txt
+++ b/doc/source/whatsnew/v0.14.0.txt
@@ -83,7 +83,7 @@ API changes
   been removed, instead a header with the column names is returned (:issue:`6062`).
 - ``Series`` and ``Index`` now internall share more common operations, e.g. ``factorize(),nunique(),value_counts()`` are
   now supported on ``Index`` types as well. The ``Series.weekday`` property from is removed
-  from Series for API  consistency. Using a ``DatetimeIndex/PeriodIndex`` method on a Series will now raise a ``TypeError``.
+  from Series for API consistency. Using a ``DatetimeIndex/PeriodIndex`` method on a Series will now raise a ``TypeError``.
   (:issue:`4551`, :issue:`4056`, :issue:`5519`, :issue:`6380`, :issue:`7206`).
 
 - Add ``is_month_start``, ``is_month_end``, ``is_quarter_start``, ``is_quarter_end``, ``is_year_start``, ``is_year_end`` accessors for ``DateTimeIndex`` / ``Timestamp`` which return a boolean array of whether the timestamp(s) are at the start/end of the month/quarter/year defined by the frequency of the ``DateTimeIndex`` / ``Timestamp`` (:issue:`4565`, :issue:`6998`)
@@ -284,7 +284,7 @@ Display Changes
   `large_repr` set to 'info' (:issue:`7105`)
 - The `verbose` keyword in ``DataFrame.info()``, which controls whether to shorten the ``info``
   representation, is now ``None`` by default. This will follow the global setting in
-  ``display.max_info_columns``. The global setting can be overriden with ``verbose=True`` or
+  ``display.max_info_columns``. The global setting can be overridden with ``verbose=True`` or
   ``verbose=False``.
 - Fixed a bug with the `info` repr not honoring the `display.max_info_columns` setting (:issue:`6939`)
 - Offset/freq info now in Timestamp __repr__ (:issue:`4553`)
@@ -446,7 +446,7 @@ Some other enhancements to the sql functions include:
 - support for writing the index. This can be controlled with the ``index``
   keyword (default is True).
 - specify the column label to use when writing the index with ``index_label``.
-- specify string columns to parse as datetimes withh the ``parse_dates``
+- specify string columns to parse as datetimes with the ``parse_dates``
   keyword in :func:`~pandas.read_sql_query` and :func:`~pandas.read_sql_table`.
 
 .. warning::
@@ -596,15 +596,15 @@ Plotting
   - `align`: Specify the bar alignment. Default is `center` (different from matplotlib). In previous versions, pandas passes `align='edge'` to matplotlib and adjust the location to `center` by itself, and it results `align` keyword is not applied as expected. (:issue:`4525`)
   - `position`: Specify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1(right/top-end). Default is 0.5 (center). (:issue:`6604`)
 
-  Because of the default `align` value changes, coordinates of bar plots are now located on integer values (0.0, 1.0, 2.0 ...). This is intended to make bar plot be located on the same coodinates as line plot. However, bar plot may differs unexpectedly when you manually adjust the bar location or drawing area, such as using `set_xlim`, `set_ylim`, etc. In this cases, please modify your script to meet with new coordinates.
+  Because of the default `align` value changes, coordinates of bar plots are now located on integer values (0.0, 1.0, 2.0 ...). This is intended to make bar plot be located on the same coordinates as line plot. However, bar plot may differs unexpectedly when you manually adjust the bar location or drawing area, such as using `set_xlim`, `set_ylim`, etc. In this cases, please modify your script to meet with new coordinates.
 
 - The :func:`parallel_coordinates` function now takes argument ``color``
-  instead of ``colors``. A ``FutureWarning`` is raised  to alert that
+  instead of ``colors``. A ``FutureWarning`` is raised to alert that
   the old ``colors`` argument will not be supported in a future release. (:issue:`6956`)
 
 - The :func:`parallel_coordinates` and :func:`andrews_curves` functions now take
   positional argument ``frame`` instead of ``data``. A ``FutureWarning`` is
-  raised  if the old ``data`` argument is used by name. (:issue:`6956`)
+  raised if the old ``data`` argument is used by name. (:issue:`6956`)
 
 - :meth:`DataFrame.boxplot` now supports ``layout`` keyword (:issue:`6769`)
 - :meth:`DataFrame.boxplot` has a new keyword argument, `return_type`. It accepts ``'dict'``,
@@ -645,17 +645,17 @@ Deprecations
 
 - The :func:`pivot_table`/:meth:`DataFrame.pivot_table` and :func:`crosstab` functions
   now take arguments ``index`` and ``columns`` instead of ``rows`` and ``cols``.  A
-  ``FutureWarning`` is raised  to alert that the old ``rows`` and ``cols`` arguments
+  ``FutureWarning`` is raised to alert that the old ``rows`` and ``cols`` arguments
   will not be supported in a future release (:issue:`5505`)
 
 - The :meth:`DataFrame.drop_duplicates` and :meth:`DataFrame.duplicated` methods
   now take argument ``subset`` instead of ``cols`` to better align with
-  :meth:`DataFrame.dropna`.  A ``FutureWarning`` is raised  to alert that the old
+  :meth:`DataFrame.dropna`.  A ``FutureWarning`` is raised to alert that the old
   ``cols`` arguments will not be supported in a future release (:issue:`6680`)
 
 - The :meth:`DataFrame.to_csv` and :meth:`DataFrame.to_excel` functions
   now takes argument ``columns`` instead of ``cols``.  A
-  ``FutureWarning`` is raised  to alert that the old ``cols`` arguments
+  ``FutureWarning`` is raised to alert that the old ``cols`` arguments
   will not be supported in a future release (:issue:`6645`)
 
 - Indexers will warn ``FutureWarning`` when used with a scalar indexer and
@@ -698,12 +698,12 @@ Deprecations
   ALWAYS return a view. (:issue:`6894`)
 
 - The :func:`parallel_coordinates` function now takes argument ``color``
-  instead of ``colors``. A ``FutureWarning`` is raised  to alert that
+  instead of ``colors``. A ``FutureWarning`` is raised to alert that
   the old ``colors`` argument will not be supported in a future release. (:issue:`6956`)
 
 - The :func:`parallel_coordinates` and :func:`andrews_curves` functions now take
   positional argument ``frame`` instead of ``data``. A ``FutureWarning`` is
-  raised  if the old ``data`` argument is used by name. (:issue:`6956`)
+  raised if the old ``data`` argument is used by name. (:issue:`6956`)
 
 - The support for the 'mysql' flavor when using DBAPI connection objects has been deprecated.
   MySQL will be further supported with SQLAlchemy engines (:issue:`6900`).
@@ -899,7 +899,7 @@ Bug Fixes
 - Raise when trying to align on different levels of a multi-index assignment (:issue:`3738`)
 - Bug in setting complex dtypes via boolean indexing (:issue:`6345`)
 - Bug in TimeGrouper/resample when presented with a non-monotonic DatetimeIndex that would return invalid results. (:issue:`4161`)
-- Bug in index name propogation in TimeGrouper/resample (:issue:`4161`)
+- Bug in index name propagation in TimeGrouper/resample (:issue:`4161`)
 - TimeGrouper has a more compatible API to the rest of the groupers (e.g. ``groups`` was missing) (:issue:`3881`)
 - Bug in multiple grouping with a TimeGrouper depending on target column order (:issue:`6764`)
 - Bug in ``pd.eval`` when parsing strings with possible tokens like ``'&'``
@@ -976,7 +976,7 @@ Bug Fixes
   clean`` (:issue:`6768`)
 - Bug with numpy < 1.7.2 when reading long strings from ``HDFStore`` (:issue:`6166`)
 - Bug in ``DataFrame._reduce`` where non bool-like (0/1) integers were being
-  coverted into bools. (:issue:`6806`)
+  converted into bools. (:issue:`6806`)
 - Regression from 0.13 with ``fillna`` and a Series on datetime-like (:issue:`6344`)
 - Bug in adding ``np.timedelta64`` to ``DatetimeIndex`` with timezone outputs incorrect results (:issue:`6818`)
 - Bug in ``DataFrame.replace()`` where changing a dtype through replacement
diff --git a/doc/source/whatsnew/v0.14.1.txt b/doc/source/whatsnew/v0.14.1.txt
index 239d6c9c6..d8a6dc179 100644
--- a/doc/source/whatsnew/v0.14.1.txt
+++ b/doc/source/whatsnew/v0.14.1.txt
@@ -75,7 +75,7 @@ API changes
 
   Note that for the other offsets the default behaviour did not change.
 
-- Add back ``#N/A N/A`` as a default NA value in text parsing, (regresion from 0.12) (:issue:`5521`)
+- Add back ``#N/A N/A`` as a default NA value in text parsing, (regression from 0.12) (:issue:`5521`)
 - Raise a ``TypeError`` on inplace-setting with a ``.where`` and a non ``np.nan`` value as this is inconsistent
   with a set-item expression like ``df[mask] = None`` (:issue:`7656`)
 
@@ -88,7 +88,7 @@ Enhancements
 - Add ``dropna`` argument to ``value_counts`` and ``nunique`` (:issue:`5569`).
 - Add :meth:`~pandas.DataFrame.select_dtypes` method to allow selection of
   columns based on dtype (:issue:`7316`). See :ref:`the docs <basics.selectdtypes>`.
-- All ``offsets`` suppports the ``normalize`` keyword to specify whether
+- All ``offsets`` supports the ``normalize`` keyword to specify whether
   ``offsets.apply``, ``rollforward`` and ``rollback`` resets the time (hour,
   minute, etc) or not (default ``False``, preserves time) (:issue:`7156`):
 
diff --git a/doc/source/whatsnew/v0.15.0.txt b/doc/source/whatsnew/v0.15.0.txt
index e44bc6e9e..ef17904d5 100644
--- a/doc/source/whatsnew/v0.15.0.txt
+++ b/doc/source/whatsnew/v0.15.0.txt
@@ -22,7 +22,7 @@ users upgrade to this version.
   - ``read_csv`` will now by default ignore blank lines when parsing, see :ref:`here <whatsnew_0150.blanklines>`
   - API change in using Indexes in set operations, see :ref:`here <whatsnew_0150.index_set_ops>`
   - Enhancements in the handling of timezones, see :ref:`here <whatsnew_0150.tz>`
-  - A lot of improvements to the rolling and expanding moment funtions, see :ref:`here <whatsnew_0150.roll>`
+  - A lot of improvements to the rolling and expanding moment functions, see :ref:`here <whatsnew_0150.roll>`
   - Internal refactoring of the ``Index`` class to no longer sub-class ``ndarray``, see :ref:`Internal Refactoring <whatsnew_0150.refactoring>`
   - dropping support for ``PyTables`` less than version 3.0.0, and ``numexpr`` less than version 2.1 (:issue:`7990`)
   - Split indexing documentation into :ref:`Indexing and Selecting Data <indexing>` and :ref:`MultiIndex / Advanced Indexing <advanced>`
@@ -326,7 +326,7 @@ Timezone handling improvements
 
 - ``Timestamp.tz_localize`` and ``Timestamp.tz_convert`` now raise ``TypeError`` in error cases, rather than ``Exception`` (:issue:`8025`)
 
-- a  timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone (rather than being a naive ``datetime64[ns]``) as ``object`` dtype (:issue:`8411`)
+- a timeseries/index localized to UTC when inserted into a Series/DataFrame will preserve the UTC timezone (rather than being a naive ``datetime64[ns]``) as ``object`` dtype (:issue:`8411`)
 
 - ``Timestamp.__repr__`` displays ``dateutil.tz.tzoffset`` info (:issue:`7907`)
 
@@ -837,7 +837,7 @@ Other notable API changes:
      A value is trying to be set on a copy of a slice from a DataFrame.
      Try using .loc[row_indexer,col_indexer] = value instead
 
-     See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
+     See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
 
 - ``merge``, ``DataFrame.merge``, and ``ordered_merge`` now return the same type
   as the ``left`` argument (:issue:`7737`).
@@ -878,7 +878,7 @@ a transparent change with only very limited API implications (:issue:`5080`, :is
 
 - you may need to unpickle pandas version < 0.15.0 pickles using ``pd.read_pickle`` rather than ``pickle.load``. See :ref:`pickle docs <io.pickle>`
 - when plotting with a ``PeriodIndex``, the matplotlib internal axes will now be arrays of ``Period`` rather than a ``PeriodIndex`` (this is similar to how a ``DatetimeIndex`` passes arrays of ``datetimes`` now)
-- MultiIndexes will now raise similary to other pandas objects w.r.t. truth testing, see :ref:`here <gotchas.truth>` (:issue:`7897`).
+- MultiIndexes will now raise similarly to other pandas objects w.r.t. truth testing, see :ref:`here <gotchas.truth>` (:issue:`7897`).
 - When plotting a DatetimeIndex directly with matplotlib's `plot` function,
   the axis labels will no longer be formatted as dates but as integers (the
   internal representation of a ``datetime64``). **UPDATE** This is fixed
@@ -1118,7 +1118,7 @@ Bug Fixes
 - Bug in multi-index slicing with various edge cases (:issue:`8132`)
 - Regression in multi-index indexing with a non-scalar type object (:issue:`7914`)
 - Bug in ``Timestamp`` comparisons with ``==`` and ``int64`` dtype (:issue:`8058`)
-- Bug in pickles contains ``DateOffset`` may raise ``AttributeError`` when ``normalize`` attribute is reffered internally (:issue:`7748`)
+- Bug in pickles contains ``DateOffset`` may raise ``AttributeError`` when ``normalize`` attribute is referred internally (:issue:`7748`)
 - Bug in ``Panel`` when using ``major_xs`` and ``copy=False`` is passed (deprecation warning fails because of missing ``warnings``) (:issue:`8152`).
 - Bug in pickle deserialization that failed for pre-0.14.1 containers with dup items trying to avoid ambiguity
   when matching block and manager items, when there's only one block there's no ambiguity (:issue:`7794`)
diff --git a/doc/source/whatsnew/v0.15.1.txt b/doc/source/whatsnew/v0.15.1.txt
index cd9298c74..f84f25d3e 100644
--- a/doc/source/whatsnew/v0.15.1.txt
+++ b/doc/source/whatsnew/v0.15.1.txt
@@ -274,7 +274,7 @@ Enhancements
 Bug Fixes
 ~~~~~~~~~
 
-- Bug in unpickling  of a ``CustomBusinessDay`` object (:issue:`8591`)
+- Bug in unpickling of a ``CustomBusinessDay`` object (:issue:`8591`)
 - Bug in coercing ``Categorical`` to a records array, e.g. ``df.to_records()`` (:issue:`8626`)
 - Bug in ``Categorical`` not created properly with ``Series.to_frame()`` (:issue:`8626`)
 - Bug in coercing in astype of a ``Categorical`` of a passed ``pd.Categorical`` (this now raises ``TypeError`` correctly), (:issue:`8626`)
diff --git a/doc/source/whatsnew/v0.15.2.txt b/doc/source/whatsnew/v0.15.2.txt
index b908b6033..f1dfab0f5 100644
--- a/doc/source/whatsnew/v0.15.2.txt
+++ b/doc/source/whatsnew/v0.15.2.txt
@@ -215,7 +215,7 @@ Bug Fixes
 - ``io.data.Options`` now raises ``RemoteDataError`` when no expiry dates are available from Yahoo and when it receives no data from Yahoo (:issue:`8761`), (:issue:`8783`).
 - Fix: The font size was only set on x axis if vertical or the y axis if horizontal. (:issue:`8765`)
 - Fixed division by 0 when reading big csv files in python 3 (:issue:`8621`)
-- Bug in outputing a Multindex with ``to_html,index=False`` which would add an extra column (:issue:`8452`)
+- Bug in outputting a Multindex with ``to_html,index=False`` which would add an extra column (:issue:`8452`)
 - Imported categorical variables from Stata files retain the ordinal information in the underlying data (:issue:`8836`).
 - Defined ``.size`` attribute across ``NDFrame`` objects to provide compat with numpy >= 1.9.1; buggy with ``np.array_split`` (:issue:`8846`)
 - Skip testing of histogram plots for matplotlib <= 1.2 (:issue:`8648`).
diff --git a/doc/source/whatsnew/v0.16.0.txt b/doc/source/whatsnew/v0.16.0.txt
index 8238cc32d..48af06d12 100644
--- a/doc/source/whatsnew/v0.16.0.txt
+++ b/doc/source/whatsnew/v0.16.0.txt
@@ -56,7 +56,7 @@ and the entire DataFrame (with all original and new columns) is returned.
    iris.assign(sepal_ratio=iris['SepalWidth'] / iris['SepalLength']).head()
 
 Above was an example of inserting a precomputed value. We can also pass in
-a function to be evalutated.
+a function to be evaluated.
 
 .. ipython :: python
 
@@ -595,7 +595,7 @@ Bug Fixes
 - Bug in ``unstack`` with ``TimedeltaIndex`` or ``DatetimeIndex`` and nulls (:issue:`9491`).
 - Bug in ``rank`` where comparing floats with tolerance will cause inconsistent behaviour (:issue:`8365`).
 - Fixed character encoding bug in ``read_stata`` and ``StataReader`` when loading data from a URL (:issue:`9231`).
-- Bug in adding ``offsets.Nano`` to other offets raises ``TypeError`` (:issue:`9284`)
+- Bug in adding ``offsets.Nano`` to other offsets raises ``TypeError`` (:issue:`9284`)
 - Bug in ``DatetimeIndex`` iteration, related to (:issue:`8890`), fixed in (:issue:`9100`)
 - Bugs in ``resample`` around DST transitions. This required fixing offset classes so they behave correctly on DST transitions. (:issue:`5172`, :issue:`8744`, :issue:`8653`, :issue:`9173`, :issue:`9468`).
 - Bug in binary operator method (eg ``.mul()``) alignment with integer levels (:issue:`9463`).
@@ -611,7 +611,7 @@ Bug Fixes
 - Accessing ``Series.str`` methods on with non-string values now raises ``TypeError`` instead of producing incorrect results (:issue:`9184`)
 - Bug in ``DatetimeIndex.__contains__`` when index has duplicates and is not monotonic increasing (:issue:`9512`)
 - Fixed division by zero error for ``Series.kurt()`` when all values are equal (:issue:`9197`)
-- Fixed issue in the ``xlsxwriter`` engine where it added a default 'General' format to cells if no other format wass applied. This prevented other row or column formatting being applied. (:issue:`9167`)
+- Fixed issue in the ``xlsxwriter`` engine where it added a default 'General' format to cells if no other format was applied. This prevented other row or column formatting being applied. (:issue:`9167`)
 - Fixes issue with ``index_col=False`` when ``usecols`` is also specified in ``read_csv``. (:issue:`9082`)
 - Bug where ``wide_to_long`` would modify the input stubnames list (:issue:`9204`)
 - Bug in ``to_sql`` not storing float64 values using double precision. (:issue:`9009`)
diff --git a/doc/source/whatsnew/v0.17.0.txt b/doc/source/whatsnew/v0.17.0.txt
index a3bbaf73c..239b2ba96 100644
--- a/doc/source/whatsnew/v0.17.0.txt
+++ b/doc/source/whatsnew/v0.17.0.txt
@@ -1157,7 +1157,7 @@ Bug Fixes
 - Bug in ``.var()`` causing roundoff errors for highly similar values (:issue:`10242`)
 - Bug in ``DataFrame.plot(subplots=True)`` with duplicated columns outputs incorrect result (:issue:`10962`)
 - Bug in ``Index`` arithmetic may result in incorrect class (:issue:`10638`)
-- Bug in ``date_range`` results in empty if freq is negative annualy, quarterly and monthly (:issue:`11018`)
+- Bug in ``date_range`` results in empty if freq is negative annually, quarterly and monthly (:issue:`11018`)
 - Bug in ``DatetimeIndex`` cannot infer negative freq (:issue:`11018`)
 - Remove use of some deprecated numpy comparison operations, mainly in tests. (:issue:`10569`)
 - Bug in ``Index`` dtype may not applied properly (:issue:`11017`)
diff --git a/doc/source/whatsnew/v0.17.1.txt b/doc/source/whatsnew/v0.17.1.txt
index 1ad7279ea..d5ed0503d 100644
--- a/doc/source/whatsnew/v0.17.1.txt
+++ b/doc/source/whatsnew/v0.17.1.txt
@@ -157,11 +157,11 @@ Bug Fixes
 - ``Series.sort_index()`` now correctly handles the ``inplace`` option (:issue:`11402`)
 - Incorrectly distributed .c file in the build on ``PyPi`` when reading a csv of floats and passing ``na_values=<a scalar>`` would show an exception (:issue:`11374`)
 - Bug in ``.to_latex()`` output broken when the index has a name (:issue:`10660`)
-- Bug in ``HDFStore.append`` with strings whose encoded length exceded the max unencoded length (:issue:`11234`)
+- Bug in ``HDFStore.append`` with strings whose encoded length exceeded the max unencoded length (:issue:`11234`)
 - Bug in merging ``datetime64[ns, tz]`` dtypes (:issue:`11405`)
 - Bug in ``HDFStore.select`` when comparing with a numpy scalar in a where clause (:issue:`11283`)
 - Bug in using ``DataFrame.ix`` with a multi-index indexer (:issue:`11372`)
-- Bug in ``date_range`` with ambigous endpoints (:issue:`11626`)
+- Bug in ``date_range`` with ambiguous endpoints (:issue:`11626`)
 - Prevent adding new attributes to the accessors ``.str``, ``.dt`` and ``.cat``. Retrieving such
   a value was not possible, so error out on setting it. (:issue:`10673`)
 - Bug in tz-conversions with an ambiguous time and ``.dt`` accessors (:issue:`11295`)
diff --git a/doc/source/whatsnew/v0.18.0.txt b/doc/source/whatsnew/v0.18.0.txt
index 4b27cf706..bfd314639 100644
--- a/doc/source/whatsnew/v0.18.0.txt
+++ b/doc/source/whatsnew/v0.18.0.txt
@@ -217,7 +217,7 @@ It returns a ``DataFrame`` with one column if ``expand=True``.
    pd.Series(['a1', 'b2', 'c3']).str.extract('[ab](\d)', expand=True)
 
 Calling on an ``Index`` with a regex with exactly one capture group
-returns  an ``Index`` if ``expand=False``.
+returns an ``Index`` if ``expand=False``.
 
 .. ipython:: python
 
@@ -944,7 +944,7 @@ assignment should be done inplace or return a copy.
 
 .. warning::
 
-   For backwards compatability, ``inplace`` defaults to ``True`` if not specified.
+   For backwards compatibility, ``inplace`` defaults to ``True`` if not specified.
    This will change in a future version of pandas. If your code depends on an
    inplace assignment you should update to explicitly set ``inplace=True``
 
@@ -1039,7 +1039,7 @@ Deprecations
              2    0.5
              dtype: float64
 
-- The the ``freq`` and ``how`` arguments to the ``.rolling``, ``.expanding``, and ``.ewm`` (new) functions are deprecated, and will be removed in a future version. You can simply resample the input prior to creating a window function. (:issue:`11603`).
+- The ``freq`` and ``how`` arguments to the ``.rolling``, ``.expanding``, and ``.ewm`` (new) functions are deprecated, and will be removed in a future version. You can simply resample the input prior to creating a window function. (:issue:`11603`).
 
   For example, instead of ``s.rolling(window=5,freq='D').max()`` to get the max value on a rolling 5 Day window, one could use ``s.resample('D').mean().rolling(window=5).max()``, which first resamples the data to daily data, then provides a rolling 5 day window.
 
diff --git a/doc/source/whatsnew/v0.18.1.txt b/doc/source/whatsnew/v0.18.1.txt
index ca386da03..de9a5d5d8 100644
--- a/doc/source/whatsnew/v0.18.1.txt
+++ b/doc/source/whatsnew/v0.18.1.txt
@@ -226,7 +226,7 @@ Other Enhancements
 ^^^^^^^^^^^^^^^^^^
 
 - ``pd.read_csv()`` now supports ``delim_whitespace=True`` for the Python engine (:issue:`12958`)
-- ``pd.read_csv()`` now supports opening ZIP files that contains a single CSV, via extension inference or explict ``compression='zip'`` (:issue:`12175`)
+- ``pd.read_csv()`` now supports opening ZIP files that contains a single CSV, via extension inference or explicit ``compression='zip'`` (:issue:`12175`)
 - ``pd.read_csv()`` now supports opening files using xz compression, via extension inference or explicit ``compression='xz'`` is specified; ``xz`` compressions is also supported by ``DataFrame.to_csv`` in the same way (:issue:`11852`)
 - ``pd.read_msgpack()`` now always gives writeable ndarrays even when compression is used (:issue:`12359`).
 - ``pd.read_msgpack()`` now supports serializing and de-serializing categoricals with msgpack (:issue:`12573`)
diff --git a/doc/source/whatsnew/v0.19.0.txt b/doc/source/whatsnew/v0.19.0.txt
index 6093e5302..302105c1e 100644
--- a/doc/source/whatsnew/v0.19.0.txt
+++ b/doc/source/whatsnew/v0.19.0.txt
@@ -1413,7 +1413,7 @@ Performance Improvements
 - Improved performance of ``factorize`` of datetime with timezone (:issue:`13750`)
 - Improved performance of by lazily creating indexing hashtables on larger Indexes (:issue:`14266`)
 - Improved performance of ``groupby.groups`` (:issue:`14293`)
-- Unecessary materializing of a MultiIndex when introspecting for memory usage (:issue:`14308`)
+- Unnecessary materializing of a MultiIndex when introspecting for memory usage (:issue:`14308`)
 
 .. _whatsnew_0190.bug_fixes:
 
@@ -1454,7 +1454,7 @@ Bug Fixes
 - Bug in ``.tz_localize`` with ``dateutil.tz.tzlocal`` may return incorrect result (:issue:`13583`)
 - Bug in ``DatetimeTZDtype`` dtype with ``dateutil.tz.tzlocal`` cannot be regarded as valid dtype (:issue:`13583`)
 - Bug in ``pd.read_hdf()`` where attempting to load an HDF file with a single dataset, that had one or more categorical columns, failed unless the key argument was set to the name of the dataset. (:issue:`13231`)
-- Bug in ``.rolling()`` that allowed a negative integer window in contruction of the ``Rolling()`` object, but would later fail on aggregation (:issue:`13383`)
+- Bug in ``.rolling()`` that allowed a negative integer window in construction of the ``Rolling()`` object, but would later fail on aggregation (:issue:`13383`)
 - Bug in ``Series`` indexing with tuple-valued data and a numeric index (:issue:`13509`)
 - Bug in printing ``pd.DataFrame`` where unusual elements with the ``object`` dtype were causing segfaults (:issue:`13717`)
 - Bug in ranking ``Series`` which could result in segfaults (:issue:`13445`)
diff --git a/doc/source/whatsnew/v0.19.2.txt b/doc/source/whatsnew/v0.19.2.txt
index 722e494c9..171d97b76 100644
--- a/doc/source/whatsnew/v0.19.2.txt
+++ b/doc/source/whatsnew/v0.19.2.txt
@@ -26,7 +26,7 @@ Enhancements
 The ``pd.merge_asof()``, added in 0.19.0, gained some improvements:
 
 - ``pd.merge_asof()`` gained ``left_index``/``right_index`` and ``left_by``/``right_by`` arguments (:issue:`14253`)
-- ``pd.merge_asof()`` can take multiple columns in ``by`` parameter and has specialized dtypes for better performace (:issue:`13936`)
+- ``pd.merge_asof()`` can take multiple columns in ``by`` parameter and has specialized dtypes for better performance (:issue:`13936`)
 
 
 .. _whatsnew_0192.performance:
@@ -62,7 +62,7 @@ Bug Fixes
 - Bug in ``pd.to_numeric`` where a 0 was not unsigned on a ``downcast='unsigned'`` argument (:issue:`14401`)
 - Bug in plotting regular and irregular timeseries using shared axes
   (``sharex=True`` or ``ax.twinx()``) (:issue:`13341`, :issue:`14322`).
-- Bug in not propogating exceptions in parsing invalid datetimes, noted in python 3.6 (:issue:`14561`)
+- Bug in not propagating exceptions in parsing invalid datetimes, noted in python 3.6 (:issue:`14561`)
 - Bug in resampling a ``DatetimeIndex`` in local TZ, covering a DST change, which would raise ``AmbiguousTimeError`` (:issue:`14682`)
 - Bug in indexing that transformed ``RecursionError`` into ``KeyError`` or ``IndexingError`` (:issue:`14554`)
 - Bug in ``HDFStore`` when writing a ``MultiIndex`` when using ``data_columns=True`` (:issue:`14435`)
diff --git a/doc/source/whatsnew/v0.20.0.txt b/doc/source/whatsnew/v0.20.0.txt
index fc869956c..d04a34f7a 100644
--- a/doc/source/whatsnew/v0.20.0.txt
+++ b/doc/source/whatsnew/v0.20.0.txt
@@ -97,7 +97,7 @@ The API also supports a ``.transform()`` function for broadcasting results.
    df.transform(['abs', lambda x: x - x.min()])
 
 When presented with mixed dtypes that cannot be aggregated, ``.agg()`` will only take the valid
-aggregations. This is similiar to how groupby ``.agg()`` works. (:issue:`15015`)
+aggregations. This is similar to how groupby ``.agg()`` works. (:issue:`15015`)
 
 .. ipython:: python
 
@@ -1504,7 +1504,7 @@ Other Deprecations
 - ``TimedeltaIndex.searchsorted()``, ``DatetimeIndex.searchsorted()``, and ``PeriodIndex.searchsorted()`` have deprecated the ``key`` parameter in favor of ``value`` (:issue:`12662`)
 - ``DataFrame.astype()`` has deprecated the ``raise_on_error`` parameter in favor of ``errors`` (:issue:`14878`)
 - ``Series.sortlevel`` and ``DataFrame.sortlevel`` have been deprecated in favor of ``Series.sort_index`` and ``DataFrame.sort_index`` (:issue:`15099`)
-- importing ``concat`` from ``pandas.tools.merge`` has been deprecated in favor of imports from the ``pandas`` namespace. This should only affect explict imports (:issue:`15358`)
+- importing ``concat`` from ``pandas.tools.merge`` has been deprecated in favor of imports from the ``pandas`` namespace. This should only affect explicit imports (:issue:`15358`)
 - ``Series/DataFrame/Panel.consolidate()`` been deprecated as a public method. (:issue:`15483`)
 - The ``as_indexer`` keyword of ``Series.str.match()`` has been deprecated (ignored keyword) (:issue:`15257`).
 - The following top-level pandas functions have been deprecated and will be removed in a future version (:issue:`13790`, :issue:`15940`)
diff --git a/doc/source/whatsnew/v0.21.1.txt b/doc/source/whatsnew/v0.21.1.txt
index a7dde5d6e..67c52dac6 100644
--- a/doc/source/whatsnew/v0.21.1.txt
+++ b/doc/source/whatsnew/v0.21.1.txt
@@ -168,4 +168,4 @@ Categorical
 String
 ^^^^^^
 
-- :meth:`Series.str.split()` will now propogate ``NaN`` values across all expanded columns instead of ``None`` (:issue:`18450`)
+- :meth:`Series.str.split()` will now propagate ``NaN`` values across all expanded columns instead of ``None`` (:issue:`18450`)
diff --git a/doc/source/whatsnew/v0.23.0.txt b/doc/source/whatsnew/v0.23.0.txt
index 0f6660d2f..d6766afdf 100644
--- a/doc/source/whatsnew/v0.23.0.txt
+++ b/doc/source/whatsnew/v0.23.0.txt
@@ -127,7 +127,7 @@ Other Enhancements
 - Better support for :func:`Dataframe.style.to_excel` output with the ``xlsxwriter`` engine. (:issue:`16149`)
 - :func:`pandas.tseries.frequencies.to_offset` now accepts leading '+' signs e.g. '+1h'. (:issue:`18171`)
 - :func:`MultiIndex.unique` now supports the ``level=`` argument, to get unique values from a specific index level (:issue:`17896`)
-- :class:`pandas.io.formats.style.Styler` now has method ``hide_index()`` to determine whether the index will be rendered in ouptut (:issue:`14194`)
+- :class:`pandas.io.formats.style.Styler` now has method ``hide_index()`` to determine whether the index will be rendered in output (:issue:`14194`)
 - :class:`pandas.io.formats.style.Styler` now has method ``hide_columns()`` to determine whether columns will be hidden in output (:issue:`14194`)
 - Improved wording of ``ValueError`` raised in :func:`to_datetime` when ``unit=`` is passed with a non-convertible value (:issue:`14350`)
 - :func:`Series.fillna` now accepts a Series or a dict as a ``value`` for a categorical dtype (:issue:`17033`)
@@ -172,7 +172,7 @@ Build Changes
 ^^^^^^^^^^^^^
 
 - Building pandas for development now requires ``cython >= 0.24`` (:issue:`18613`)
-- Building from source now explicity requires ``setuptools`` in ``setup.py`` (:issue:`18113`)
+- Building from source now explicitly requires ``setuptools`` in ``setup.py`` (:issue:`18113`)
 - Updated conda recipe to be in compliance with conda-build 3.0+ (:issue:`18002`)
 
 .. _whatsnew_0230.api:
@@ -231,7 +231,7 @@ Removal of prior version deprecations/changes
 
 - Warnings against the obsolete usage ``Categorical(codes, categories)``, which were emitted for instance when the first two arguments to ``Categorical()`` had different dtypes, and recommended the use of ``Categorical.from_codes``, have now been removed (:issue:`8074`)
 - The ``levels`` and ``labels`` attributes of a ``MultiIndex`` can no longer be set directly (:issue:`4039`).
-- ``pd.tseries.util.pivot_annual`` has been  removed (deprecated since v0.19). Use ``pivot_table`` instead (:issue:`18370`)
+- ``pd.tseries.util.pivot_annual`` has been removed (deprecated since v0.19). Use ``pivot_table`` instead (:issue:`18370`)
 - ``pd.tseries.util.isleapyear`` has been removed (deprecated since v0.19). Use ``.is_leap_year`` property in Datetime-likes instead (:issue:`18370`)
 - ``pd.ordered_merge`` has been removed (deprecated since v0.19). Use ``pd.merge_ordered`` instead (:issue:`18459`)
 - The ``SparseList`` class has been removed (:issue:`14007`)
@@ -257,7 +257,7 @@ Performance Improvements
 - :class`DateOffset` arithmetic performance is improved (:issue:`18218`)
 - Converting a ``Series`` of ``Timedelta`` objects to days, seconds, etc... sped up through vectorization of underlying methods (:issue:`18092`)
 - Improved performance of ``.map()`` with a ``Series/dict`` input (:issue:`15081`)
-- The overriden ``Timedelta`` properties of days, seconds and microseconds have been removed, leveraging their built-in Python versions instead (:issue:`18242`)
+- The overridden ``Timedelta`` properties of days, seconds and microseconds have been removed, leveraging their built-in Python versions instead (:issue:`18242`)
 - ``Series`` construction will reduce the number of copies made of the input data in certain cases (:issue:`17449`)
 - Improved performance of :func:`Series.dt.date` and :func:`DatetimeIndex.date` (:issue:`18058`)
 - Improved performance of :func:`Series.dt.time` and :func:`DatetimeIndex.time` (:issue:`18461`)
diff --git a/doc/source/whatsnew/v0.8.1.txt b/doc/source/whatsnew/v0.8.1.txt
index 8227bc6bc..add96bec9 100644
--- a/doc/source/whatsnew/v0.8.1.txt
+++ b/doc/source/whatsnew/v0.8.1.txt
@@ -32,5 +32,5 @@ Performance improvements
     strings with ``DatetimeIndex`` or ``to_datetime`` (:issue:`1571`)
   - Improve the performance of GroupBy on single-key aggregations and use with
     Categorical types
-  - Significant datetime parsing performance improvments
+  - Significant datetime parsing performance improvements
 
diff --git a/doc/source/whatsnew/v0.9.1.txt b/doc/source/whatsnew/v0.9.1.txt
index 4faf38219..e2d6d7a27 100644
--- a/doc/source/whatsnew/v0.9.1.txt
+++ b/doc/source/whatsnew/v0.9.1.txt
@@ -80,7 +80,7 @@ New features
 	   df.where(df>0,-df)
 
 	Furthermore, `where` now aligns the input boolean condition (ndarray or DataFrame), such that partial selection
-	with setting is possible. This is analagous to partial setting via `.ix` (but on the contents rather than the axis labels)
+	with setting is possible. This is analogous to partial setting via `.ix` (but on the contents rather than the axis labels)
 
 	.. ipython:: python
 
diff --git a/doc/sphinxext/README.rst b/doc/sphinxext/README.rst
index e39cf8daa..2be5372bc 100644
--- a/doc/sphinxext/README.rst
+++ b/doc/sphinxext/README.rst
@@ -14,4 +14,4 @@ pandas documentation. These copies originate from other projects:
 
     These copies are maintained at the respective projects, so fixes should,
     to the extent possible, be pushed upstream instead of only adapting our
-    local copy to avoid divergence between the the local and upstream version.
+    local copy to avoid divergence between the local and upstream version.
diff --git a/doc/sphinxext/ipython_sphinxext/ipython_console_highlighting.py b/doc/sphinxext/ipython_sphinxext/ipython_console_highlighting.py
index dfb489e49..c5ec26aef 100644
--- a/doc/sphinxext/ipython_sphinxext/ipython_console_highlighting.py
+++ b/doc/sphinxext/ipython_sphinxext/ipython_console_highlighting.py
@@ -83,7 +83,7 @@ class IPythonConsoleLexer(Lexer):
                 curcode += line[continue_prompt.end():]
             elif output_prompt is not None:
                 # Use the 'error' token for output.  We should probably make
-                # our own token, but error is typicaly in a bright color like
+                # our own token, but error is typically in a bright color like
                 # red, so it works fine for our output prompts.
                 insertions.append((len(curcode),
                                    [(0, Generic.Error, output_prompt.group())]))
diff --git a/doc/sphinxext/ipython_sphinxext/ipython_directive.py b/doc/sphinxext/ipython_sphinxext/ipython_directive.py
index 4f7b32840..5616d732e 100644
--- a/doc/sphinxext/ipython_sphinxext/ipython_directive.py
+++ b/doc/sphinxext/ipython_sphinxext/ipython_directive.py
@@ -93,7 +93,7 @@ ToDo
 Authors
 -------
 
-- John D Hunter: orignal author.
+- John D Hunter: original author.
 - Fernando Perez: refactoring, documentation, cleanups, port to 0.11.
 - VáclavŠmilauer <eudoxos-AT-arcig.cz>: Prompt generalizations.
 - Skipper Seabold, refactoring, cleanups, pure python addition
@@ -154,7 +154,7 @@ COMMENT, INPUT, OUTPUT =  range(3)
 def block_parser(part, rgxin, rgxout, fmtin, fmtout):
     """
     part is a string of ipython text, comprised of at most one
-    input, one ouput, comments, and blank lines.  The block parser
+    input, one output, comments, and blank lines.  The block parser
     parses the text into a list of::
 
       blocks = [ (TOKEN0, data0), (TOKEN1, data1), ...]
@@ -268,7 +268,7 @@ class DecodingStringIO(StringIO, object):
                     return super(DecodingStringIO, self).write(data)
                 except :
                     pass
-        # default to brute utf8 if no encoding succeded
+        # default to brute utf8 if no encoding succeeded
             return super(DecodingStringIO, self).write(data.decode('utf8', 'replace'))
 
 
diff --git a/doc/sphinxext/numpydoc/docscrape_sphinx.py b/doc/sphinxext/numpydoc/docscrape_sphinx.py
index 9017480c9..127ed49c1 100755
--- a/doc/sphinxext/numpydoc/docscrape_sphinx.py
+++ b/doc/sphinxext/numpydoc/docscrape_sphinx.py
@@ -115,7 +115,7 @@ class SphinxDocString(NumpyDocString):
                         or inspect.isgetsetdescriptor(param_obj)):
                     param_obj = None
 
-                # pandas HACK - do not exclude attributes wich are None
+                # pandas HACK - do not exclude attributes which are None
                 # if param_obj and (pydoc.getdoc(param_obj) or not desc):
                 #     # Referenced object has a docstring
                 #     autosum += ["   %s%s" % (prefix, param)]
diff --git a/pandas/__init__.py b/pandas/__init__.py
index 861c8e7d6..93c5b6484 100644
--- a/pandas/__init__.py
+++ b/pandas/__init__.py
@@ -106,8 +106,8 @@ Here are just a few of the things that pandas does well:
   - Easy handling of missing data in floating point as well as non-floating
     point data.
   - Size mutability: columns can be inserted and deleted from DataFrame and
-    higher dimensional objects.
-  - Automatic and explicit data alignment: objects can  be explicitly aligned
+    higher dimensional objects
+  - Automatic and explicit data alignment: objects can be explicitly aligned
     to a set of labels, or the user can simply ignore the labels and let
     `Series`, `DataFrame`, etc. automatically align the data for you in
     computations.
diff --git a/pandas/_libs/algos.pyx b/pandas/_libs/algos.pyx
index 3710ddc33..7b61cd22f 100644
--- a/pandas/_libs/algos.pyx
+++ b/pandas/_libs/algos.pyx
@@ -61,7 +61,7 @@ cdef inline are_diff(object left, object right):
 
 
 class Infinity(object):
-    """ provide a positive Infinity comparision method for ranking """
+    """ provide a positive Infinity comparison method for ranking """
 
     __lt__ = lambda self, other: False
     __le__ = lambda self, other: isinstance(other, Infinity)
@@ -73,7 +73,7 @@ class Infinity(object):
 
 
 class NegInfinity(object):
-    """ provide a negative Infinity comparision method for ranking """
+    """ provide a negative Infinity comparison method for ranking """
 
     __lt__ = lambda self, other: (not isinstance(other, NegInfinity) and
                                   not missing.checknull(other))
diff --git a/pandas/_libs/hashing.pyx b/pandas/_libs/hashing.pyx
index aa7aa4b52..c6f182ac5 100644
--- a/pandas/_libs/hashing.pyx
+++ b/pandas/_libs/hashing.pyx
@@ -80,7 +80,7 @@ def hash_object_array(ndarray[object] arr, object key, object encoding='utf8'):
         lens[i] = l
         cdata = data
 
-        # keep the refernce alive thru the end of the
+        # keep the references alive thru the end of the
         # function
         datas.append(data)
         vecs[i] = cdata
diff --git a/pandas/_libs/hashtable.pyx b/pandas/_libs/hashtable.pyx
index 4bbe8c654..72c2834b0 100644
--- a/pandas/_libs/hashtable.pyx
+++ b/pandas/_libs/hashtable.pyx
@@ -148,7 +148,7 @@ cdef class Int64Factorizer:
 def unique_label_indices(ndarray[int64_t, ndim=1] labels):
     """
     indices of the first occurrences of the unique labels
-    *excluding* -1. equivelent to:
+    *excluding* -1. equivalent to:
         np.unique(labels, return_index=True)[1]
     """
     cdef:
diff --git a/pandas/_libs/lib.pyx b/pandas/_libs/lib.pyx
index 3898f7499..788d3c4ac 100644
--- a/pandas/_libs/lib.pyx
+++ b/pandas/_libs/lib.pyx
@@ -929,7 +929,7 @@ def is_lexsorted(list list_of_arrays):
 
 
 # TODO: could do even better if we know something about the data. eg, index has
-# 1-min data, binner has 5-min data, then  bins are just strides in index. This
+# 1-min data, binner has 5-min data, then bins are just strides in index. This
 # is a general, O(max(len(values), len(binner))) method.
 @cython.boundscheck(False)
 @cython.wraparound(False)
diff --git a/pandas/_libs/parsers.pyx b/pandas/_libs/parsers.pyx
index 1f7c359b5..cf63b5083 100644
--- a/pandas/_libs/parsers.pyx
+++ b/pandas/_libs/parsers.pyx
@@ -424,7 +424,7 @@ cdef class TextReader:
 
         if escapechar is not None:
             if len(escapechar) != 1:
-                raise ValueError('Only length-1 escapes  supported')
+                raise ValueError('Only length-1 escapes supported')
             self.parser.escapechar = ord(escapechar)
 
         self._set_quoting(quotechar, quoting)
@@ -523,7 +523,7 @@ cdef class TextReader:
         else:
             if isinstance(header, list):
                 if len(header) > 1:
-                    # need to artifically skip the final line
+                    # need to artificially skip the final line
                     # which is still a header line
                     header = list(header)
                     header.append(header[-1] + 1)
diff --git a/pandas/_libs/src/datetime/np_datetime.c b/pandas/_libs/src/datetime/np_datetime.c
index fd76f3328..89753ccf7 100644
--- a/pandas/_libs/src/datetime/np_datetime.c
+++ b/pandas/_libs/src/datetime/np_datetime.c
@@ -327,7 +327,7 @@ int cmp_pandas_datetimestruct(const pandas_datetimestruct *a,
  * this style of access anyway.
  *
  * Returns -1 on error, 0 on success, and 1 (with no error set)
- * if obj doesn't have the neeeded date or datetime attributes.
+ * if obj doesn't have the needed date or datetime attributes.
  */
 int convert_pydatetime_to_datetimestruct(PyObject *obj,
                                          pandas_datetimestruct *out) {
diff --git a/pandas/_libs/src/numpy.pxd b/pandas/_libs/src/numpy.pxd
index 6fa2bc6af..8ce398ce2 100644
--- a/pandas/_libs/src/numpy.pxd
+++ b/pandas/_libs/src/numpy.pxd
@@ -196,7 +196,7 @@ cdef extern from "numpy/arrayobject.h":
         # -- the details of this may change.
         def __getbuffer__(ndarray self, Py_buffer* info, int flags):
             # This implementation of getbuffer is geared towards Cython
-            # requirements, and does not yet fullfill the PEP.
+            # requirements, and does not yet fulfill the PEP.
             # In particular strided access is always provided regardless
             # of flags
 
diff --git a/pandas/_libs/src/period_helper.c b/pandas/_libs/src/period_helper.c
index 19f810eb5..01fc46481 100644
--- a/pandas/_libs/src/period_helper.c
+++ b/pandas/_libs/src/period_helper.c
@@ -247,7 +247,7 @@ onError:
 
 ///////////////////////////////////////////////
 
-// frequency specifc conversion routines
+// frequency specific conversion routines
 // each function must take an integer fromDate and
 // a char relation ('S' or 'E' for 'START' or 'END')
 ///////////////////////////////////////////////////////////////////////
diff --git a/pandas/_libs/src/ujson/lib/ultrajson.h b/pandas/_libs/src/ujson/lib/ultrajson.h
index 159645b40..0470fef45 100644
--- a/pandas/_libs/src/ujson/lib/ultrajson.h
+++ b/pandas/_libs/src/ujson/lib/ultrajson.h
@@ -140,7 +140,7 @@ typedef int64_t JSLONG;
 #endif
 
 #if !defined(__LITTLE_ENDIAN__) && !defined(__BIG_ENDIAN__)
-#error "Endianess not supported"
+#error "Endianness not supported"
 #endif
 
 enum JSTYPES {
@@ -245,7 +245,7 @@ typedef struct __JSONObjectEncoder {
   int encodeHTMLChars;
 
   /*
-  Set to an error message if error occured */
+  Set to an error message if error occurred */
   const char *errorMsg;
   JSOBJ errorObj;
 
diff --git a/pandas/_libs/src/ujson/python/ujson.c b/pandas/_libs/src/ujson/python/ujson.c
index a0c2146c3..da19afab0 100644
--- a/pandas/_libs/src/ujson/python/ujson.c
+++ b/pandas/_libs/src/ujson/python/ujson.c
@@ -58,12 +58,12 @@ PyObject *JSONFileToObj(PyObject *self, PyObject *args, PyObject *kwargs);
 
 static PyMethodDef ujsonMethods[] = {
     {"encode", (PyCFunction)objToJSON, METH_VARARGS | METH_KEYWORDS,
-     "Converts arbitrary object recursivly into JSON. " ENCODER_HELP_TEXT},
+     "Converts arbitrary object recursively into JSON. " ENCODER_HELP_TEXT},
     {"decode", (PyCFunction)JSONToObj, METH_VARARGS | METH_KEYWORDS,
      "Converts JSON as string to dict object structure. Use precise_float=True "
      "to use high precision float decoder."},
     {"dumps", (PyCFunction)objToJSON, METH_VARARGS | METH_KEYWORDS,
-     "Converts arbitrary object recursivly into JSON. " ENCODER_HELP_TEXT},
+     "Converts arbitrary object recursively into JSON. " ENCODER_HELP_TEXT},
     {"loads", (PyCFunction)JSONToObj, METH_VARARGS | METH_KEYWORDS,
      "Converts JSON as string to dict object structure. Use precise_float=True "
      "to use high precision float decoder."},
diff --git a/pandas/_libs/tslibs/offsets.pyx b/pandas/_libs/tslibs/offsets.pyx
index d3278e42e..585c904a6 100644
--- a/pandas/_libs/tslibs/offsets.pyx
+++ b/pandas/_libs/tslibs/offsets.pyx
@@ -104,7 +104,7 @@ cpdef bint _is_normalized(dt):
 
 def apply_index_wraps(func):
     # Note: normally we would use `@functools.wraps(func)`, but this does
-    # not play nicely wtih cython class methods
+    # not play nicely with cython class methods
     def wrapper(self, other):
         result = func(self, other)
         if self.normalize:
@@ -316,7 +316,7 @@ class EndMixin(object):
 
 class _BaseOffset(object):
     """
-    Base class for DateOffset methods that are not overriden by subclasses
+    Base class for DateOffset methods that are not overridden by subclasses
     and will (after pickle errors are resolved) go into a cdef class.
     """
     _typ = "dateoffset"
@@ -783,7 +783,7 @@ cpdef int get_day_of_month(datetime other, day_opt) except? -1:
     other : datetime or Timestamp
     day_opt : 'start', 'end'
         'start': returns 1
-        'end': returns last day  of the month
+        'end': returns last day of the month
 
     Returns
     -------
@@ -924,7 +924,7 @@ cpdef int roll_yearday(datetime other, int n, int month,
     month : reference month giving the first month of the year
     day_opt : 'start', 'end'
         'start': returns 1
-        'end': returns last day  of the month
+        'end': returns last day of the month
 
     Returns
     -------
diff --git a/pandas/_libs/tslibs/strptime.pyx b/pandas/_libs/tslibs/strptime.pyx
index 65594de58..292129197 100644
--- a/pandas/_libs/tslibs/strptime.pyx
+++ b/pandas/_libs/tslibs/strptime.pyx
@@ -557,7 +557,7 @@ class TimeRE(dict):
         """Convert a list to a regex string for matching a directive.
 
         Want possible matching values to be from longest to shortest.  This
-        prevents the possibility of a match occuring for a value that also
+        prevents the possibility of a match occurring for a value that also
         a substring of a larger value that should have matched (e.g., 'abc'
         matching when 'abcdef' should have been the match).
 
diff --git a/pandas/_libs/tslibs/timestamps.pyx b/pandas/_libs/tslibs/timestamps.pyx
index 1792f852c..c7744bf9d 100644
--- a/pandas/_libs/tslibs/timestamps.pyx
+++ b/pandas/_libs/tslibs/timestamps.pyx
@@ -373,7 +373,7 @@ class Timestamp(_Timestamp):
     """Pandas replacement for datetime.datetime
 
     Timestamp is the pandas equivalent of python's Datetime
-    and is interchangable with it in most cases. It's the type used
+    and is interchangeable with it in most cases. It's the type used
     for the entries that make up a DatetimeIndex, and other timeseries
     oriented data structures in pandas.
 
diff --git a/pandas/core/accessor.py b/pandas/core/accessor.py
index 53ead5e8f..73e01fbf1 100644
--- a/pandas/core/accessor.py
+++ b/pandas/core/accessor.py
@@ -17,7 +17,7 @@ class DirNamesMixin(object):
         return self._accessors | self._deprecations
 
     def _dir_additions(self):
-        """ add addtional __dir__ for this object """
+        """ add additional __dir__ for this object """
         rv = set()
         for accessor in self._accessors:
             try:
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 167f215b6..571db4053 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -68,7 +68,7 @@ def _ensure_data(values, dtype=None):
             return _ensure_object(np.asarray(values)), 'object', 'object'
         if is_bool_dtype(values) or is_bool_dtype(dtype):
             # we are actually coercing to uint64
-            # until our algos suppport uint8 directly (see TODO)
+            # until our algos support uint8 directly (see TODO)
             return np.asarray(values).astype('uint64'), 'bool', 'uint64'
         elif is_signed_integer_dtype(values) or is_signed_integer_dtype(dtype):
             return _ensure_int64(values), 'int64', 'int64'
@@ -120,7 +120,7 @@ def _ensure_data(values, dtype=None):
         dtype = 'category'
 
         # we are actually coercing to int64
-        # until our algos suppport int* directly (not all do)
+        # until our algos support int* directly (not all do)
         values = _ensure_int64(values)
 
         return values, dtype, 'int64'
diff --git a/pandas/core/base.py b/pandas/core/base.py
index 72acd0052..e90794c6c 100644
--- a/pandas/core/base.py
+++ b/pandas/core/base.py
@@ -685,7 +685,7 @@ class GroupByMixin(object):
 
 
 class IndexOpsMixin(object):
-    """ common ops mixin to support a unified inteface / docs for Series /
+    """ common ops mixin to support a unified interface / docs for Series /
     Index
     """
 
diff --git a/pandas/core/categorical.py b/pandas/core/categorical.py
index baf15b3ca..d47cb0762 100644
--- a/pandas/core/categorical.py
+++ b/pandas/core/categorical.py
@@ -648,7 +648,7 @@ class Categorical(PandasObject):
         Parameters
         ----------
         sort : boolean
-            The value of the sort paramter groupby was called with.
+            The value of the sort parameter groupby was called with.
 
         Returns
         -------
@@ -770,7 +770,7 @@ class Categorical(PandasObject):
            If not given, do not change the ordered information.
         rename : boolean (default: False)
            Whether or not the new_categories should be considered as a rename
-           of the old  categories or as reordered categories.
+           of the old categories or as reordered categories.
         inplace : boolean (default: False)
            Whether or not to reorder the categories inplace or return a copy of
            this categorical with reordered categories.
@@ -1139,7 +1139,7 @@ class Categorical(PandasObject):
         shifted : Categorical
         """
         # since categoricals always have ndim == 1, an axis parameter
-        # doesnt make any sense here.
+        # doesn't make any sense here.
         codes = self.codes
         if codes.ndim > 1:
             raise NotImplementedError("Categorical with ndim > 1.")
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 775ecc32b..e606be3cc 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -305,7 +305,7 @@ def split_ranges(mask):
     ranges = [(0, len(mask))]
 
     for pos, val in enumerate(mask):
-        if not val:  # this pos should be ommited, split off the prefix range
+        if not val:  # this pos should be omitted, split off the prefix range
             r = ranges.pop()
             if pos > r[0]:  # yield non-zero range
                 yield (r[0], pos)
diff --git a/pandas/core/computation/expressions.py b/pandas/core/computation/expressions.py
index c74da6379..1dc19d33f 100644
--- a/pandas/core/computation/expressions.py
+++ b/pandas/core/computation/expressions.py
@@ -71,7 +71,7 @@ def _can_use_numexpr(op, op_str, a, b, dtype_check):
         # required min elements (otherwise we are adding overhead)
         if np.prod(a.shape) > _MIN_ELEMENTS:
 
-            # check for dtype compatiblity
+            # check for dtype compatibility
             dtypes = set()
             for o in [a, b]:
                 if hasattr(o, 'get_dtype_counts'):
@@ -224,7 +224,7 @@ def where(cond, a, b, use_numexpr=True):
 
 def set_test_mode(v=True):
     """
-    Keeps track of whether numexpr  was used.  Stores an additional ``True``
+    Keeps track of whether numexpr was used.  Stores an additional ``True``
     for every successful use of evaluate with numexpr since the last
     ``get_test_result``
     """
diff --git a/pandas/core/computation/pytables.py b/pandas/core/computation/pytables.py
index 4b3c608a8..26eefa75b 100644
--- a/pandas/core/computation/pytables.py
+++ b/pandas/core/computation/pytables.py
@@ -439,7 +439,7 @@ class ExprVisitor(BaseExprVisitor):
                 return self.term_type(getattr(resolved, attr), self.env)
             except AttributeError:
 
-                # something like datetime.datetime where scope is overriden
+                # something like datetime.datetime where scope is overridden
                 if isinstance(value, ast.Name) and value.id == attr:
                     return resolved
 
diff --git a/pandas/core/config.py b/pandas/core/config.py
index e71c3b6f5..d10e2d19b 100644
--- a/pandas/core/config.py
+++ b/pandas/core/config.py
@@ -23,7 +23,7 @@ This module supports the following requirements:
 - all options in a certain sub - namespace can be reset at once.
 - the user can set / get / reset or ask for the description of an option.
 - a developer can register and mark an option as deprecated.
-- you can register a callback to be invoked when the the option value
+- you can register a callback to be invoked when the option value
   is set or reset. Changing the stored value is considered misuse, but
   is not verboten.
 
@@ -33,8 +33,8 @@ Implementation
 - Data is stored using nested dictionaries, and should be accessed
   through the provided API.
 
-- "Registered options" and "Deprecated options" have metadata associcated
-  with them, which are stored in auxilary dictionaries keyed on the
+- "Registered options" and "Deprecated options" have metadata associated
+  with them, which are stored in auxiliary dictionaries keyed on the
   fully-qualified key, e.g. "x.y.z.option".
 
 - the config_init module is imported by the package's __init__.py file.
@@ -209,7 +209,7 @@ class DictWrapper(object):
 # in the docstring. For dev convenience we'd like to generate the docstrings
 # dynamically instead of maintaining them by hand. To this, we use the
 # class below which wraps functions inside a callable, and converts
-# __doc__ into a propery function. The doctsrings below are templates
+# __doc__ into a property function. The doctsrings below are templates
 # using the py2.6+ advanced formatting syntax to plug in a concise list
 # of options, and option descriptions.
 
@@ -691,7 +691,7 @@ def pp_options_list(keys, width=80, _print=False):
 
 @contextmanager
 def config_prefix(prefix):
-    """contextmanager for multiple invocations of API  with a common prefix
+    """contextmanager for multiple invocations of API with a common prefix
 
     supported API functions: (register / get / set )__option
 
diff --git a/pandas/core/dtypes/cast.py b/pandas/core/dtypes/cast.py
index 87c6fb69f..5fcb5f09d 100644
--- a/pandas/core/dtypes/cast.py
+++ b/pandas/core/dtypes/cast.py
@@ -520,7 +520,7 @@ def maybe_infer_dtype_type(element):
 
 
 def maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
-    """ provide explict type promotion and coercion
+    """ provide explicit type promotion and coercion
 
     Parameters
     ----------
diff --git a/pandas/core/dtypes/dtypes.py b/pandas/core/dtypes/dtypes.py
index a47f2c0d4..d1637873e 100644
--- a/pandas/core/dtypes/dtypes.py
+++ b/pandas/core/dtypes/dtypes.py
@@ -597,8 +597,8 @@ class PeriodDtype(ExtensionDtype):
         """
 
         if isinstance(dtype, compat.string_types):
-            # PeriodDtype can be instanciated from freq string like "U",
-            # but dosn't regard freq str like "U" as dtype.
+            # PeriodDtype can be instantiated from freq string like "U",
+            # but doesn't regard freq str like "U" as dtype.
             if dtype.startswith('period[') or dtype.startswith('Period['):
                 try:
                     if cls._parse_dtype_strict(dtype) is not None:
diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index ce57b544d..d208c72ff 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -362,7 +362,7 @@ def _infer_fill_value(val):
 
 def _maybe_fill(arr, fill_value=np.nan):
     """
-    if we have a compatiable fill_value and arr dtype, then fill
+    if we have a compatible fill_value and arr dtype, then fill
     """
     if _isna_compat(arr, fill_value):
         arr.fill(fill_value)
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 12a4a7fda..d85f3bf55 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -591,7 +591,7 @@ class DataFrame(NDFrame):
             max_rows = get_option("display.max_rows")
 
         # when auto-detecting, so width=None and not in ipython front end
-        # check whether repr fits horizontal by actualy checking
+        # check whether repr fits horizontal by actually checking
         # the width of the rendered repr
         buf = StringIO()
 
@@ -1578,7 +1578,7 @@ class DataFrame(NDFrame):
             String path of file-like object
         convert_dates : dict
             Dictionary mapping columns containing datetime types to stata
-            internal format to use when wirting the dates. Options are 'tc',
+            internal format to use when writing the dates. Options are 'tc',
             'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer
             or a name. Datetime columns that do not have a conversion type
             specified will be converted to 'tc'. Raises NotImplementedError if
@@ -1606,7 +1606,7 @@ class DataFrame(NDFrame):
             * If datetimes contain timezone information
             * Column dtype is not representable in Stata
         ValueError
-            * Columns listed in convert_dates are noth either datetime64[ns]
+            * Columns listed in convert_dates are neither datetime64[ns]
               or datetime.datetime
             * Column listed in convert_dates is not in DataFrame
             * Categorical label contains more than 32,000 characters
@@ -5736,7 +5736,7 @@ class DataFrame(NDFrame):
         return Series(result, index=self._get_agg_axis(axis))
 
     def _get_agg_axis(self, axis_num):
-        """ let's be explict about this """
+        """ let's be explicit about this """
         if axis_num == 0:
             return self.columns
         elif axis_num == 1:
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index c5359ba2c..004cfce67 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -2341,7 +2341,7 @@ class NDFrame(PandasObject, SelectionMixin):
             if value is None:
                 return
 
-            # see if the copy is not actually refererd; if so, then disolve
+            # see if the copy is not actually referred; if so, then dissolve
             # the copy weakref
             try:
                 gc.collect(2)
@@ -3109,7 +3109,7 @@ class NDFrame(PandasObject, SelectionMixin):
         %(optional_axis)s
         method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}, optional
             method to use for filling holes in reindexed DataFrame.
-            Please note: this is only  applicable to DataFrames/Series with a
+            Please note: this is only applicable to DataFrames/Series with a
             monotonically increasing/decreasing index.
 
             * default: don't fill gaps
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 06b7dbb4e..285a34715 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -2956,7 +2956,7 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True,
 
         return True
 
-    # if the the grouper is obj[name]
+    # if the grouper is obj[name]
     def is_in_obj(gpr):
         try:
             return id(gpr) == id(obj[gpr.name])
diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 04b8ade7e..99ee28c84 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -319,7 +319,7 @@ class Index(IndexOpsMixin, PandasObject):
                     return IntervalIndex.from_intervals(subarr, name=name,
                                                         copy=copy)
                 elif inferred == 'boolean':
-                    # don't support boolean explicity ATM
+                    # don't support boolean explicitly ATM
                     pass
                 elif inferred != 'string':
                     if inferred.startswith('datetime'):
@@ -887,7 +887,7 @@ class Index(IndexOpsMixin, PandasObject):
         # are we a truncated display
         is_truncated = n > max_seq_items
 
-        # adj can optionaly handle unicode eastern asian width
+        # adj can optionally handle unicode eastern asian width
         adj = _get_adjustment()
 
         def _extend_line(s, line, value, display_width, next_line_prefix):
@@ -1788,7 +1788,7 @@ class Index(IndexOpsMixin, PandasObject):
         """
         Concatenate to_concat which has the same class
         """
-        # must be overrided in specific classes
+        # must be overridden in specific classes
         return _concat._concat_index_asobject(to_concat, name)
 
     _index_shared_docs['take'] = """
@@ -3276,7 +3276,7 @@ class Index(IndexOpsMixin, PandasObject):
                 sorter, _ = libalgos.groupsort_indexer(lab, 1 + lab.max())
                 return sorter
 
-            # find indexers of begining of each set of
+            # find indexers of beginning of each set of
             # same-key labels w.r.t all but last level
             tic = labels[0][:-1] != labels[0][1:]
             for lab in labels[1:-1]:
@@ -3573,7 +3573,7 @@ class Index(IndexOpsMixin, PandasObject):
 
     def _get_loc_only_exact_matches(self, key):
         """
-        This is overriden on subclasses (namely, IntervalIndex) to control
+        This is overridden on subclasses (namely, IntervalIndex) to control
         get_slice_bound.
         """
         return self.get_loc(key)
diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py
index 241907a54..ac7cb30fa 100644
--- a/pandas/core/indexes/category.py
+++ b/pandas/core/indexes/category.py
@@ -522,7 +522,7 @@ class CategoricalIndex(Index, accessor.PandasDelegate):
         # we always want to return an Index type here
         # to be consistent with .reindex for other index types (e.g. they don't
         # coerce based on the actual values, only on the dtype)
-        # unless we had an inital Categorical to begin with
+        # unless we had an initial Categorical to begin with
         # in which case we are going to conform to the passed Categorical
         new_target = np.asarray(new_target)
         if is_categorical_dtype(target):
@@ -746,7 +746,7 @@ class CategoricalIndex(Index, accessor.PandasDelegate):
 
                 if isinstance(other, ABCCategorical):
                     if not self.values.is_dtype_equal(other):
-                        raise TypeError("categorical index comparisions must "
+                        raise TypeError("categorical index comparisons must "
                                         "have the same categories and ordered "
                                         "attributes")
 
diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py
index 40c07376d..3fca40562 100644
--- a/pandas/core/indexes/datetimelike.py
+++ b/pandas/core/indexes/datetimelike.py
@@ -132,7 +132,7 @@ class TimelikeOps(object):
 
 
 class DatetimeIndexOpsMixin(object):
-    """ common ops mixin to support a unified inteface datetimelike Index """
+    """ common ops mixin to support a unified interface datetimelike Index """
 
     def equals(self, other):
         """
diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py
index af901440d..b17682b6c 100644
--- a/pandas/core/indexes/datetimes.py
+++ b/pandas/core/indexes/datetimes.py
@@ -355,7 +355,7 @@ class DatetimeIndex(DatelikeOps, TimelikeOps, DatetimeIndexOpsMixin,
             raise ValueError("Must provide freq argument if no data is "
                              "supplied")
 
-        # if dtype has an embeded tz, capture it
+        # if dtype has an embedded tz, capture it
         if dtype is not None:
             try:
                 dtype = DatetimeTZDtype.construct_from_string(dtype)
diff --git a/pandas/core/indexes/interval.py b/pandas/core/indexes/interval.py
index 2a132f683..def9b151f 100644
--- a/pandas/core/indexes/interval.py
+++ b/pandas/core/indexes/interval.py
@@ -683,7 +683,7 @@ class IntervalIndex(IntervalMixin, Index):
 
     @Appender(Index.memory_usage.__doc__)
     def memory_usage(self, deep=False):
-        # we don't use an explict engine
+        # we don't use an explicit engine
         # so return the bytes here
         return (self.left.memory_usage(deep=deep) +
                 self.right.memory_usage(deep=deep))
diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py
index f4c4f91d2..710737867 100644
--- a/pandas/core/indexes/multi.py
+++ b/pandas/core/indexes/multi.py
@@ -799,8 +799,8 @@ class MultiIndex(Index):
 
         *this is internal for use for the cython routines*
 
-        Paramters
-        ---------
+        Parameters
+        ----------
         key : string or tuple
 
         Returns
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index de6713249..fa6614d27 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -1308,7 +1308,7 @@ class _IXIndexer(_NDFrameIndexer):
     ``.ix`` is the most general indexer and will support any of the
     inputs in ``.loc`` and ``.iloc``. ``.ix`` also supports floating
     point label schemes. ``.ix`` is exceptionally useful when dealing
-    with mixed positional and label based hierachical indexes.
+    with mixed positional and label based hierarchical indexes.
 
     However, when an axis is integer based, ONLY label based access
     and not positional access is supported. Thus, in such cases, it's
@@ -1441,8 +1441,8 @@ class _LocIndexer(_LocationIndexer):
         ax = self.obj._get_axis(axis)
 
         # valid for a label where all labels are in the index
-        # slice of lables (where start-end in labels)
-        # slice of integers (only if in the lables)
+        # slice of labels (where start-end in labels)
+        # slice of integers (only if in the labels)
         # boolean
 
         if isinstance(key, slice):
@@ -1929,7 +1929,7 @@ class _iAtIndexer(_ScalarAccessIndexer):
         self._has_valid_positional_setitem_indexer(indexer)
 
     def _convert_key(self, key, is_setter=False):
-        """ require  integer args (and convert to label arguments) """
+        """ require integer args (and convert to label arguments) """
         for a, i in zip(self.obj.axes, key):
             if not is_integer(i):
                 raise ValueError("iAt based indexing can only have integer "
@@ -2118,7 +2118,7 @@ def maybe_convert_ix(*args):
 
 
 def is_nested_tuple(tup, labels):
-    # check for a compatiable nested tuple and multiindexes among the axes
+    # check for a compatible nested tuple and multiindexes among the axes
     if not isinstance(tup, tuple):
         return False
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 3a64a0ef8..ba90503e3 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -170,7 +170,7 @@ class Block(PandasObject):
     def get_values(self, dtype=None):
         """
         return an internal format, currently just the ndarray
-        this is often overriden to handle to_dense like operations
+        this is often overridden to handle to_dense like operations
         """
         if is_object_dtype(dtype):
             return self.values.astype(object)
@@ -954,7 +954,7 @@ class Block(PandasObject):
                 new_values = new_values.T
 
             # If the default repeat behavior in np.putmask would go in the
-            # wrong direction, then explictly repeat and reshape new instead
+            # wrong direction, then explicitly repeat and reshape new instead
             if getattr(new, 'ndim', 0) >= 1:
                 if self.ndim - 1 == new.ndim and axis == 1:
                     new = np.repeat(
@@ -1455,7 +1455,7 @@ class Block(PandasObject):
             cond = cond.values
 
         # If the default broadcasting would go in the wrong direction, then
-        # explictly reshape other instead
+        # explicitly reshape other instead
         if getattr(other, 'ndim', 0) >= 1:
             if values.ndim - 1 == other.ndim and axis == 1:
                 other = other.reshape(tuple(other.shape + (1, )))
@@ -1493,7 +1493,7 @@ class Block(PandasObject):
         except TypeError:
 
             # we cannot coerce, return a compat dtype
-            # we are explicity ignoring errors
+            # we are explicitly ignoring errors
             block = self.coerce_to_target_dtype(other)
             blocks = block.where(orig_other, cond, align=align,
                                  errors=errors,
@@ -4939,7 +4939,7 @@ def _maybe_compare(a, b, op):
     is_a_array = isinstance(a, np.ndarray)
     is_b_array = isinstance(b, np.ndarray)
 
-    # numpy deprecation warning to have i8 vs integer comparisions
+    # numpy deprecation warning to have i8 vs integer comparisons
     if is_datetimelike_v_numeric(a, b):
         result = False
 
diff --git a/pandas/core/missing.py b/pandas/core/missing.py
index c3e72d6c3..74fa21fa4 100644
--- a/pandas/core/missing.py
+++ b/pandas/core/missing.py
@@ -127,7 +127,7 @@ def interpolate_1d(xvalues, yvalues, method='linear', limit=None,
 
     if not valid.any():
         # have to call np.asarray(xvalues) since xvalues could be an Index
-        # which cant be mutated
+        # which can't be mutated
         result = np.empty_like(np.asarray(xvalues), dtype=np.float64)
         result.fill(np.nan)
         return result
diff --git a/pandas/core/ops.py b/pandas/core/ops.py
index 3a7a5e44d..faac8ab31 100644
--- a/pandas/core/ops.py
+++ b/pandas/core/ops.py
@@ -1357,7 +1357,7 @@ def _comp_method_FRAME(func, name, str_rep, masker=False):
             return self._combine_series_infer(other, func, try_cast=False)
         else:
 
-            # straight boolean comparisions we want to allow all columns
+            # straight boolean comparisons we want to allow all columns
             # (regardless of dtype to pass thru) See #4537 for discussion.
             res = self._combine_const(other, func,
                                       errors='ignore',
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index 6d85e5bf7..7ec177b03 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -860,7 +860,7 @@ class Panel(NDFrame):
         xs is only for getting, not setting values.
 
         MultiIndex Slicers is a generic way to get/set values on any level or
-        levels and  is a superset of xs functionality, see
+        levels and is a superset of xs functionality, see
         :ref:`MultiIndex Slicers <advanced.mi_slicers>`
 
         """
diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py
index 9bd5abb2c..aaadf6d3c 100644
--- a/pandas/core/reshape/concat.py
+++ b/pandas/core/reshape/concat.py
@@ -276,7 +276,7 @@ class _Concatenator(object):
             ndims.add(obj.ndim)
 
         # get the sample
-        # want the higest ndim that we have, and must be non-empty
+        # want the highest ndim that we have, and must be non-empty
         # unless all objs are empty
         sample = None
         if len(ndims) > 1:
diff --git a/pandas/core/reshape/melt.py b/pandas/core/reshape/melt.py
index c2804c8f8..b648c426a 100644
--- a/pandas/core/reshape/melt.py
+++ b/pandas/core/reshape/melt.py
@@ -186,7 +186,7 @@ def wide_to_long(df, stubnames, i, j, sep="", suffix=r'\d+'):
         A character indicating the separation of the variable names
         in the wide format, to be stripped from the names in the long format.
         For example, if your column names are A-suffix1, A-suffix2, you
-        can strip the hypen by specifying `sep='-'`
+        can strip the hyphen by specifying `sep='-'`
 
         .. versionadded:: 0.20.0
 
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 360095c38..5d8092fd3 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -345,7 +345,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                               (DatetimeIndex, PeriodIndex, TimedeltaIndex)):
                 try:
                     labels = DatetimeIndex(labels)
-                    # need to set here becuase we changed the index
+                    # need to set here because we changed the index
                     if fastpath:
                         self._data.set_axis(axis, labels)
                 except (libts.OutOfBoundsDatetime, ValueError):
@@ -487,7 +487,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         Return the indices of the elements that are non-zero
 
         This method is equivalent to calling `numpy.nonzero` on the
-        series data. For compatability with NumPy, the return value is
+        series data. For compatibility with NumPy, the return value is
         the same (a tuple with an array of indices for each dimension),
         but it will always be a one-item tuple because series only have
         one dimension.
@@ -2388,7 +2388,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             # expression, e.g.: lambda x: x-x.quantile(0.25)
             # this will fail, so we can try a vectorized evaluation
 
-            # we cannot FIRST try the vectorized evaluation, becuase
+            # we cannot FIRST try the vectorized evaluation, because
             # then .agg and .apply would have different semantics if the
             # operation is actually defined on the Series, e.g. str
             try:
diff --git a/pandas/core/sparse/array.py b/pandas/core/sparse/array.py
index 0424ac870..9b2650359 100644
--- a/pandas/core/sparse/array.py
+++ b/pandas/core/sparse/array.py
@@ -525,7 +525,7 @@ class SparseArray(PandasObject, np.ndarray):
         # if is_integer(key):
         #    self.values[key] = value
         # else:
-        #    raise Exception("SparseArray does not support seting non-scalars
+        #    raise Exception("SparseArray does not support setting non-scalars
         # via setitem")
         raise TypeError(
             "SparseArray does not support item assignment via setitem")
@@ -538,7 +538,7 @@ class SparseArray(PandasObject, np.ndarray):
         slobj = slice(i, j)  # noqa
 
         # if not is_scalar(value):
-        #    raise Exception("SparseArray does not support seting non-scalars
+        #    raise Exception("SparseArray does not support setting non-scalars
         # via slices")
 
         # x = self.values
diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index 99c7563d5..fab4e77ce 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -1414,7 +1414,7 @@ class StringMethods(NoNewAttributesMixin):
 
         elif expand is True and not isinstance(self._orig, Index):
             # required when expand=True is explicitly specified
-            # not needed when infered
+            # not needed when inferred
 
             def cons_row(x):
                 if is_list_like(x):
@@ -1424,7 +1424,7 @@ class StringMethods(NoNewAttributesMixin):
 
             result = [cons_row(x) for x in result]
             if result:
-                # propogate nan values to match longest sequence (GH 18450)
+                # propagate nan values to match longest sequence (GH 18450)
                 max_len = max(len(x) for x in result)
                 result = [x * max_len if x[0] is np.nan else x for x in result]
 
diff --git a/pandas/core/tools/datetimes.py b/pandas/core/tools/datetimes.py
index 6b8edbb14..1de43116d 100644
--- a/pandas/core/tools/datetimes.py
+++ b/pandas/core/tools/datetimes.py
@@ -197,7 +197,8 @@ def to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False,
 
         In case when it is not possible to return designated types (e.g. when
         any element of input is before Timestamp.min or after Timestamp.max)
-        return will have datetime.datetime type (or correspoding array/Series).
+        return will have datetime.datetime type (or corresponding
+        array/Series).
 
     Examples
     --------
@@ -497,7 +498,7 @@ _unit_map = {'year': 'year',
 
 def _assemble_from_unit_mappings(arg, errors):
     """
-    assemble the unit specifed fields from the arg (DataFrame)
+    assemble the unit specified fields from the arg (DataFrame)
     Return a Series for actual parsing
 
     Parameters
diff --git a/pandas/core/window.py b/pandas/core/window.py
index 5ad8d20cc..76ba76b7a 100644
--- a/pandas/core/window.py
+++ b/pandas/core/window.py
@@ -253,8 +253,8 @@ class _Window(PandasObject, SelectionMixin):
         """
         wrap the results
 
-        Paramters
-        ---------
+        Parameters
+        ----------
         results : list of ndarrays
         blocks : list of blocks
         obj : conformed data (may be resampled)
@@ -403,7 +403,7 @@ class Window(_Window):
     3  NaN
     4  NaN
 
-    Same as above, but explicity set the min_periods
+    Same as above, but explicitly set the min_periods
 
     >>> df.rolling(2, min_periods=1).sum()
          B
diff --git a/pandas/errors/__init__.py b/pandas/errors/__init__.py
index 42b3bdd49..b3d1ce31d 100644
--- a/pandas/errors/__init__.py
+++ b/pandas/errors/__init__.py
@@ -38,7 +38,7 @@ class ParserError(ValueError):
 
 class DtypeWarning(Warning):
     """
-    Warning that is raised for a dtype incompatiblity. This
+    Warning that is raised for a dtype incompatibility. This
     can happen whenever `pd.read_csv` encounters non-
     uniform dtypes in a column(s) of a given CSV file.
     """
@@ -56,7 +56,7 @@ class ParserWarning(Warning):
     Warning that is raised in `pd.read_csv` whenever it is necessary
     to change parsers (generally from 'c' to 'python') contrary to the
     one specified by the user due to lack of support or functionality for
-    parsing particular attributes of a CSV file with the requsted engine.
+    parsing particular attributes of a CSV file with the requested engine.
     """
 
 
diff --git a/pandas/io/common.py b/pandas/io/common.py
index 534c1e067..da60698fe 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -312,7 +312,7 @@ def _get_handle(path_or_buf, mode, encoding=None, compression=None,
     f : file-like
         A file-like object
     handles : list of file-like objects
-        A list of file-like object that were openned in this function.
+        A list of file-like object that were opened in this function.
     """
     try:
         from s3fs import S3File
@@ -533,7 +533,7 @@ else:
             # Fetch UTF-8 output from the queue ...
             data = self.queue.getvalue()
             data = data.decode("utf-8")
-            # ... and reencode it into the target encoding
+            # ... and re-encode it into the target encoding
             data = self.encoder.encode(data)
             # write to the target stream
             self.stream.write(data)
@@ -553,7 +553,7 @@ else:
             # Fetch UTF-8 output from the queue ...
             data = self.queue.getvalue()
             data = data.decode("utf-8")
-            # ... and reencode it into the target encoding
+            # ... and re-encode it into the target encoding
             data = self.encoder.encode(data)
             # write to the target stream
             self.stream.write(data)
diff --git a/pandas/io/excel.py b/pandas/io/excel.py
index 4f0655cff..92b29c8da 100644
--- a/pandas/io/excel.py
+++ b/pandas/io/excel.py
@@ -561,7 +561,7 @@ class ExcelFile(object):
                 cell_contents = bool(cell_contents)
             elif convert_float and cell_typ == XL_CELL_NUMBER:
                 # GH5394 - Excel 'numbers' are always floats
-                # it's a minimal perf hit and less suprising
+                # it's a minimal perf hit and less surprising
                 val = int(cell_contents)
                 if val == cell_contents:
                     cell_contents = val
@@ -881,12 +881,12 @@ class ExcelWriter(object):
     def write_cells(self, cells, sheet_name=None, startrow=0, startcol=0,
                     freeze_panes=None):
         """
-        Write given formated cells into Excel an excel sheet
+        Write given formatted cells into Excel an excel sheet
 
         Parameters
         ----------
         cells : generator
-            cell of formated data to save to Excel sheet
+            cell of formatted data to save to Excel sheet
         sheet_name : string, default None
             Name of Excel sheet, if None, then use self.cur_sheet
         startrow: upper left cell row to dump data frame
diff --git a/pandas/io/formats/console.py b/pandas/io/formats/console.py
index bdff59939..36eac8dd5 100644
--- a/pandas/io/formats/console.py
+++ b/pandas/io/formats/console.py
@@ -14,7 +14,7 @@ _initial_defencoding = None
 def detect_console_encoding():
     """
     Try to find the most capable encoding supported by the console.
-    slighly modified from the way IPython handles the same issue.
+    slightly modified from the way IPython handles the same issue.
     """
     global _initial_defencoding
 
diff --git a/pandas/io/formats/excel.py b/pandas/io/formats/excel.py
index a36e82edf..aff3e3586 100644
--- a/pandas/io/formats/excel.py
+++ b/pandas/io/formats/excel.py
@@ -492,7 +492,7 @@ class ExcelFormatter(object):
 
         # output index and index_label?
         if self.index:
-            # chek aliases
+            # check aliases
             # if list only take first as this is not a MultiIndex
             if (self.index_label and
                     isinstance(self.index_label, (list, tuple, np.ndarray,
diff --git a/pandas/io/formats/style.py b/pandas/io/formats/style.py
index 3af9e78a5..2c3d92cea 100644
--- a/pandas/io/formats/style.py
+++ b/pandas/io/formats/style.py
@@ -420,7 +420,7 @@ class Styler(object):
         the rendered HTML in the notebook.
 
         Pandas uses the following keys in render. Arguments passed
-        in ``**kwargs`` take precedence, so think carefuly if you want
+        in ``**kwargs`` take precedence, so think carefully if you want
         to override them:
 
         * head
@@ -1201,7 +1201,7 @@ def _is_visible(idx_row, idx_col, lengths):
 
 def _get_level_lengths(index, hidden_elements=None):
     """
-    Given an index, find the level lenght for each element.
+    Given an index, find the level length for each element.
     Optional argument is a list of index positions which
     should not be visible.
 
@@ -1229,7 +1229,7 @@ def _get_level_lengths(index, hidden_elements=None):
                 lengths[(i, last_label)] = 1
             elif (row != sentinel):
                 # even if its hidden, keep track of it in case
-                # length >1 and later elemens are visible
+                # length >1 and later elements are visible
                 last_label = j
                 lengths[(i, last_label)] = 0
             elif(j not in hidden_elements):
diff --git a/pandas/io/html.py b/pandas/io/html.py
index 67a48198a..e7794864c 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -263,7 +263,7 @@ class _HtmlFrameParser(object):
 
         attrs : dict
             A dictionary of table attributes that can be used to disambiguate
-            mutliple tables on a page.
+            multiple tables on a page.
 
         Raises
         ------
diff --git a/pandas/io/json/json.py b/pandas/io/json/json.py
index bb435c625..72ec5c59c 100644
--- a/pandas/io/json/json.py
+++ b/pandas/io/json/json.py
@@ -162,7 +162,7 @@ class JSONTableWriter(FrameWriter):
     def __init__(self, obj, orient, date_format, double_precision,
                  ensure_ascii, date_unit, index, default_handler=None):
         """
-        Adds a `schema` attribut with the Table Schema, resets
+        Adds a `schema` attribute with the Table Schema, resets
         the index (can't do in caller, because the schema inference needs
         to know what the index is, forces orient to records, and forces
         date_format to 'iso'.
@@ -534,7 +534,7 @@ class JsonReader(BaseIterator):
 
     def close(self):
         """
-        If we opened a  stream earlier, in _get_data_from_filepath, we should
+        If we opened a stream earlier, in _get_data_from_filepath, we should
         close it. If an open stream or file was passed, we leave it open.
         """
         if self.should_close:
diff --git a/pandas/io/msgpack/_unpacker.pyx b/pandas/io/msgpack/_unpacker.pyx
index 05dfaad8b..04bb330e5 100644
--- a/pandas/io/msgpack/_unpacker.pyx
+++ b/pandas/io/msgpack/_unpacker.pyx
@@ -202,7 +202,7 @@ cdef class Unpacker(object):
     :param int max_buffer_size:
         Limits size of data waiting unpacked.  0 means system's
         INT_MAX  (default). Raises `BufferFull` exception when it
-        is insufficient. You shoud set this parameter when unpacking
+        is insufficient. You should set this parameter when unpacking
         data from untrasted source.
 
     :param int max_str_len:
diff --git a/pandas/io/packers.py b/pandas/io/packers.py
index ef65a3275..9289853a1 100644
--- a/pandas/io/packers.py
+++ b/pandas/io/packers.py
@@ -70,7 +70,7 @@ from pandas.util._move import (
     move_into_mutable_buffer as _move_into_mutable_buffer,
 )
 
-# check whcih compression libs we have installed
+# check which compression libs we have installed
 try:
     import zlib
 
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index e053af176..acb7d0028 100755
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -1541,7 +1541,7 @@ class ParserBase(object):
                     values, set(col_na_values) | col_na_fvalues,
                     try_num_bool)
 
-                # type specificed in dtype param
+                # type specified in dtype param
                 if cast_type and not is_dtype_equal(cvals, cast_type):
                     cvals = self._cast_types(cvals, cast_type, c)
 
@@ -2054,7 +2054,7 @@ class PythonParser(ParserBase):
             self.data = f
 
         # Get columns in two steps: infer from data, then
-        # infer column indices from self.usecols if is is specified.
+        # infer column indices from self.usecols if it is specified.
         self._col_indices = None
         self.columns, self.num_original_columns = self._infer_columns()
 
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index c428000d7..efe6ab6c1 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -97,7 +97,7 @@ def _ensure_term(where, scope_level):
     create the terms here with a frame_level=2 (we are 2 levels down)
     """
 
-    # only consider list/tuple here as an ndarray is automaticaly a coordinate
+    # only consider list/tuple here as an ndarray is automatically a coordinate
     # list
     level = scope_level + 1
     if isinstance(where, (list, tuple)):
@@ -301,7 +301,7 @@ def read_hdf(path_or_buf, key=None, mode='r', **kwargs):
             contains a single pandas object.
         mode : string, {'r', 'r+', 'a'}, default 'r'. Mode to use when opening
             the file. Ignored if path_or_buf is a pd.HDFStore.
-        where : list of Term (or convertable) objects, optional
+        where : list of Term (or convertible) objects, optional
         start : optional, integer (defaults to None), row number to start
             selection
         stop  : optional, integer (defaults to None), row number to stop
@@ -498,7 +498,7 @@ class HDFStore(StringMixin):
                              (type(self).__name__, name))
 
     def __contains__(self, key):
-        """ check for existance of this key
+        """ check for existence of this key
               can match the exact pathname or the pathnm w/o the leading '/'
               """
         node = self.get_node(key)
@@ -679,7 +679,7 @@ class HDFStore(StringMixin):
         Parameters
         ----------
         key : object
-        where : list of Term (or convertable) objects, optional
+        where : list of Term (or convertible) objects, optional
         start : integer (defaults to None), row number to start selection
         stop  : integer (defaults to None), row number to stop selection
         columns : a list of columns that if not None, will limit the return
@@ -724,7 +724,7 @@ class HDFStore(StringMixin):
         Parameters
         ----------
         key : object
-        where : list of Term (or convertable) objects, optional
+        where : list of Term (or convertible) objects, optional
         start : integer (defaults to None), row number to start selection
         stop  : integer (defaults to None), row number to stop selection
         """
@@ -873,7 +873,7 @@ class HDFStore(StringMixin):
         ----------
         key : string
             Node to remove or delete rows from
-        where : list of Term (or convertable) objects, optional
+        where : list of Term (or convertible) objects, optional
         start : integer (defaults to None), row number to start selection
         stop  : integer (defaults to None), row number to stop selection
 
@@ -1250,7 +1250,7 @@ class HDFStore(StringMixin):
         # existing node (and must be a table)
         if tt is None:
 
-            # if we are a writer, determin the tt
+            # if we are a writer, determine the tt
             if value is not None:
 
                 if pt == u('series_table'):
@@ -1370,7 +1370,7 @@ class TableIterator(object):
         ----------
 
         store : the reference store
-        s     : the refered storer
+        s     : the referred storer
         func  : the function to execute the query
         where : the where of the query
         nrows : the rows to iterate on
@@ -4653,7 +4653,7 @@ class Selection(object):
     Parameters
     ----------
     table : a Table object
-    where : list of Terms (or convertable to)
+    where : list of Terms (or convertible to)
     start, stop: indicies to start and/or stop selection
 
     """
@@ -4718,7 +4718,7 @@ class Selection(object):
             raise ValueError(
                 "The passed where expression: {0}\n"
                 "            contains an invalid variable reference\n"
-                "            all of the variable refrences must be a "
+                "            all of the variable references must be a "
                 "reference to\n"
                 "            an axis (e.g. 'index' or 'columns'), or a "
                 "data_column\n"
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index c7bbbf994..e2f3033c5 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -1484,7 +1484,7 @@ class SQLiteDatabase(PandasSQL):
             `index` is True, then the index names are used.
             A sequence should be given if the DataFrame uses MultiIndex.
         schema : string, default None
-            Ignored parameter included for compatability with SQLAlchemy
+            Ignored parameter included for compatibility with SQLAlchemy
             version of ``to_sql``.
         chunksize : int, default None
             If not None, then rows will be written in batches of this
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index aafe5f2ce..2b97b4479 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -645,7 +645,7 @@ class StataValueLabel(object):
 
     def _encode(self, s):
         """
-        Python 3 compatability shim
+        Python 3 compatibility shim
         """
         if compat.PY3:
             return s.encode(self._encoding)
@@ -968,7 +968,7 @@ class StataReader(StataParser, BaseIterator):
         self._order_categoricals = order_categoricals
         if encoding is not None:
             if encoding not in VALID_ENCODINGS:
-                raise ValueError('Unknown encoding. Only latin-1 and  ascii '
+                raise ValueError('Unknown encoding. Only latin-1 and ascii '
                                  'supported.')
         self._encoding = encoding
         self._chunksize = chunksize
@@ -1881,7 +1881,7 @@ class StataWriter(StataParser):
         Input to save
     convert_dates : dict
         Dictionary mapping columns containing datetime types to stata internal
-        format to use when wirting the dates. Options are 'tc', 'td', 'tm',
+        format to use when writing the dates. Options are 'tc', 'td', 'tm',
         'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name.
         Datetime columns that do not have a conversion type specified will be
         converted to 'tc'. Raises NotImplementedError if a datetime column has
@@ -1913,7 +1913,7 @@ class StataWriter(StataParser):
     NotImplementedError
         * If datetimes contain timezone information
     ValueError
-        * Columns listed in convert_dates are noth either datetime64[ns]
+        * Columns listed in convert_dates are neither datetime64[ns]
           or datetime.datetime
         * Column dtype is not representable in Stata
         * Column listed in convert_dates is not in DataFrame
diff --git a/pandas/plotting/_core.py b/pandas/plotting/_core.py
index 9d74a308f..3094d7d0a 100644
--- a/pandas/plotting/_core.py
+++ b/pandas/plotting/_core.py
@@ -1951,7 +1951,7 @@ _shared_docs['boxplot'] = """
     return_type : {None, 'axes', 'dict', 'both'}, default None
         The kind of object to return. The default is ``axes``
         'axes' returns the matplotlib axes the boxplot is drawn on;
-        'dict' returns a dictionary  whose values are the matplotlib
+        'dict' returns a dictionary whose values are the matplotlib
         Lines of the boxplot;
         'both' returns a namedtuple with the axes and dict.
 
diff --git a/pandas/plotting/_style.py b/pandas/plotting/_style.py
index 145597e52..887202e22 100644
--- a/pandas/plotting/_style.py
+++ b/pandas/plotting/_style.py
@@ -67,9 +67,9 @@ def _get_standard_colors(num_colors=None, colormap=None, color_type='default',
             except ValueError:
                 return False
 
-        # check whether the string can be convertable to single color
+        # check whether the string can be convertible to single color
         maybe_single_color = _maybe_valid_colors([colors])
-        # check whether each character can be convertable to colors
+        # check whether each character can be convertible to colors
         maybe_color_cycle = _maybe_valid_colors(list(colors))
         if maybe_single_color and maybe_color_cycle and len(colors) > 1:
             # Special case for single str 'CN' match and convert to hex
diff --git a/pandas/tests/frame/test_alter_axes.py b/pandas/tests/frame/test_alter_axes.py
index 6e3b7a059..c824f0026 100644
--- a/pandas/tests/frame/test_alter_axes.py
+++ b/pandas/tests/frame/test_alter_axes.py
@@ -480,7 +480,7 @@ class TestDataFrameAlterAxes(TestData):
         df = DataFrame([(0, 0), (1, 1)], index=index, columns=columns)
 
         #
-        # without specifying level -> accross all levels
+        # without specifying level -> across all levels
 
         renamed = df.rename(index={'foo1': 'foo3', 'bar2': 'bar3'},
                             columns={'fizz1': 'fizz3', 'buzz2': 'buzz3'})
diff --git a/pandas/tests/frame/test_analytics.py b/pandas/tests/frame/test_analytics.py
index 69f1aeddc..b9275fc69 100644
--- a/pandas/tests/frame/test_analytics.py
+++ b/pandas/tests/frame/test_analytics.py
@@ -1907,7 +1907,7 @@ class TestDataFrameAnalytics(TestData):
 
     def test_built_in_round(self):
         if not compat.PY3:
-            pytest.skip("build in round cannot be overriden "
+            pytest.skip("build in round cannot be overridden "
                         "prior to Python 3")
 
         # GH11763
diff --git a/pandas/tests/frame/test_operators.py b/pandas/tests/frame/test_operators.py
index 430562ce7..fd1eb2364 100644
--- a/pandas/tests/frame/test_operators.py
+++ b/pandas/tests/frame/test_operators.py
@@ -865,7 +865,7 @@ class TestDataFrameOperators(TestData):
 
         # 10890
         # we no longer allow auto timeseries broadcasting
-        # and require explict broadcasting
+        # and require explicit broadcasting
         added = self.tsframe.add(ts, axis='index')
 
         for key, col in compat.iteritems(self.tsframe):
diff --git a/pandas/tests/groupby/test_groupby.py b/pandas/tests/groupby/test_groupby.py
index a13d985ab..5172efe25 100644
--- a/pandas/tests/groupby/test_groupby.py
+++ b/pandas/tests/groupby/test_groupby.py
@@ -303,7 +303,7 @@ class TestGroupBy(MixIn):
 
             # assert issubclass(agged.dtype.type, np.integer)
 
-            # explicity return a float from my function
+            # explicitly return a float from my function
             def f(x):
                 return float(len(x))
 
diff --git a/pandas/tests/groupby/test_whitelist.py b/pandas/tests/groupby/test_whitelist.py
index de0deb442..3117525d8 100644
--- a/pandas/tests/groupby/test_whitelist.py
+++ b/pandas/tests/groupby/test_whitelist.py
@@ -184,7 +184,7 @@ def test_regression_whitelist_methods(
         axis, skipna, sort):
     # GH6944
     # GH 17537
-    # explicity test the whitelest methods
+    # explicitly test the whitelest methods
 
     if axis == 0:
         frame = raw_frame
diff --git a/pandas/tests/indexes/datetimes/test_astype.py b/pandas/tests/indexes/datetimes/test_astype.py
index 1d72ca609..4b989eb35 100644
--- a/pandas/tests/indexes/datetimes/test_astype.py
+++ b/pandas/tests/indexes/datetimes/test_astype.py
@@ -71,7 +71,7 @@ class TestDatetimeIndex(object):
 
     def test_astype_str_compat(self):
         # GH 13149, GH 13209
-        # verify that we are returing NaT as a string (and not unicode)
+        # verify that we are returning NaT as a string (and not unicode)
 
         idx = DatetimeIndex(['2016-05-16', 'NaT', NaT, np.NaN])
         result = idx.astype(str)
diff --git a/pandas/tests/indexes/datetimes/test_tools.py b/pandas/tests/indexes/datetimes/test_tools.py
index c89e3ddbf..f94a438fc 100644
--- a/pandas/tests/indexes/datetimes/test_tools.py
+++ b/pandas/tests/indexes/datetimes/test_tools.py
@@ -1464,7 +1464,7 @@ class TestDatetimeParsingWrappers(object):
             actual = tslib._test_parse_iso8601(date_str)
             assert actual == exp
 
-        # seperators must all match - YYYYMM not valid
+        # separators must all match - YYYYMM not valid
         invalid_cases = ['2011-01/02', '2011^11^11',
                          '201401', '201111', '200101',
                          # mixed separated and unseparated
diff --git a/pandas/tests/indexes/interval/test_interval.py b/pandas/tests/indexes/interval/test_interval.py
index 6fc5526e6..3ca4c31b7 100644
--- a/pandas/tests/indexes/interval/test_interval.py
+++ b/pandas/tests/indexes/interval/test_interval.py
@@ -1112,7 +1112,7 @@ class TestIntervalIndex(Base):
         idx = IntervalIndex.from_tuples(tpls[::-1], closed=closed)
         assert idx.is_non_overlapping_monotonic is False
 
-        # Should be False for closed='both', overwise True (GH16560)
+        # Should be False for closed='both', otherwise True (GH16560)
         if closed == 'both':
             idx = IntervalIndex.from_breaks(range(4), closed=closed)
             assert idx.is_non_overlapping_monotonic is False
diff --git a/pandas/tests/indexes/test_base.py b/pandas/tests/indexes/test_base.py
index e33fd1e0f..510954240 100644
--- a/pandas/tests/indexes/test_base.py
+++ b/pandas/tests/indexes/test_base.py
@@ -1067,7 +1067,7 @@ class TestIndex(Base):
         # GH 14626
         # windows has different precision on datetime.datetime.now (it doesn't
         # include us since the default for Timestamp shows these but Index
-        # formating does not we are skipping)
+        # formatting does not we are skipping)
         now = datetime.now()
         if not str(now).endswith("000"):
             index = Index([now])
diff --git a/pandas/tests/indexes/test_numeric.py b/pandas/tests/indexes/test_numeric.py
index cbd819fa9..dcd592345 100644
--- a/pandas/tests/indexes/test_numeric.py
+++ b/pandas/tests/indexes/test_numeric.py
@@ -117,7 +117,7 @@ class Numeric(Base):
     def test_explicit_conversions(self):
 
         # GH 8608
-        # add/sub are overriden explicity for Float/Int Index
+        # add/sub are overridden explicitly for Float/Int Index
         idx = self._holder(np.arange(5, dtype='int64'))
 
         # float conversions
diff --git a/pandas/tests/indexes/test_range.py b/pandas/tests/indexes/test_range.py
index 55c06e885..1ebeef072 100644
--- a/pandas/tests/indexes/test_range.py
+++ b/pandas/tests/indexes/test_range.py
@@ -779,7 +779,7 @@ class TestRangeIndex(Numeric):
     def test_explicit_conversions(self):
 
         # GH 8608
-        # add/sub are overriden explicity for Float/Int Index
+        # add/sub are overridden explicitly for Float/Int Index
         idx = RangeIndex(5)
 
         # float conversions
diff --git a/pandas/tests/indexing/common.py b/pandas/tests/indexing/common.py
index c5fb2580f..ded16224a 100644
--- a/pandas/tests/indexing/common.py
+++ b/pandas/tests/indexing/common.py
@@ -120,8 +120,8 @@ class Base(object):
         if isinstance(key, dict):
             key = key[axis]
 
-        # use an artifical conversion to map the key as integers to the labels
-        # so ix can work for comparisions
+        # use an artificial conversion to map the key as integers to the labels
+        # so ix can work for comparisons
         if method == 'indexer':
             method = 'ix'
             key = obj._get_axis(axis)[key]
@@ -138,7 +138,7 @@ class Base(object):
     def get_value(self, f, i, values=False):
         """ return the value for the location i """
 
-        # check agains values
+        # check against values
         if values:
             return f.values[i]
 
@@ -160,7 +160,7 @@ class Base(object):
         for i in indicies:
             result = getattr(f, func)[i]
 
-            # check agains values
+            # check against values
             if values:
                 expected = f.values[i]
             else:
diff --git a/pandas/tests/indexing/test_floats.py b/pandas/tests/indexing/test_floats.py
index 6c5af84f0..d2692c7dc 100644
--- a/pandas/tests/indexing/test_floats.py
+++ b/pandas/tests/indexing/test_floats.py
@@ -360,7 +360,7 @@ class TestFloatIndexers(object):
 
         # same as above, but for Integer based indexes
         # these coerce to a like integer
-        # oob indiciates if we are out of bounds
+        # oob indicates if we are out of bounds
         # of positional indexing
         for index, oob in [(tm.makeIntIndex(5), False),
                            (tm.makeRangeIndex(5), False),
diff --git a/pandas/tests/indexing/test_indexing.py b/pandas/tests/indexing/test_indexing.py
index 0e66c1576..c66310d10 100644
--- a/pandas/tests/indexing/test_indexing.py
+++ b/pandas/tests/indexing/test_indexing.py
@@ -362,7 +362,7 @@ class TestFancy(Base):
 
     def test_multi_assign(self):
 
-        # GH 3626, an assignement of a sub-df to a df
+        # GH 3626, an assignment of a sub-df to a df
         df = DataFrame({'FC': ['a', 'b', 'a', 'b', 'a', 'b'],
                         'PF': [0, 0, 0, 0, 1, 1],
                         'col1': lrange(6),
diff --git a/pandas/tests/indexing/test_ix.py b/pandas/tests/indexing/test_ix.py
index 568dd7cec..3f71e673a 100644
--- a/pandas/tests/indexing/test_ix.py
+++ b/pandas/tests/indexing/test_ix.py
@@ -235,7 +235,7 @@ class TestIX(object):
         tm.assert_frame_equal(df, expected)
 
         # ok, but chained assignments are dangerous
-        # if we turn off chained assignement it will work
+        # if we turn off chained assignment it will work
         with option_context('chained_assignment', None):
             df = DataFrame({'a': lrange(4)})
             df['b'] = np.nan
diff --git a/pandas/tests/indexing/test_loc.py b/pandas/tests/indexing/test_loc.py
index 6f0d8b1f2..fb5f094f9 100644
--- a/pandas/tests/indexing/test_loc.py
+++ b/pandas/tests/indexing/test_loc.py
@@ -17,7 +17,7 @@ class TestLoc(Base):
 
     def test_loc_getitem_dups(self):
         # GH 5678
-        # repeated gettitems on a dup index returing a ndarray
+        # repeated gettitems on a dup index returning a ndarray
         df = DataFrame(
             np.random.random_sample((20, 5)),
             index=['ABCDE' [x % 5] for x in range(20)])
@@ -385,7 +385,7 @@ class TestLoc(Base):
 
     def test_loc_setitem_consistency(self):
         # GH 6149
-        # coerce similary for setitem and loc when rows have a null-slice
+        # coerce similarly for setitem and loc when rows have a null-slice
         expected = DataFrame({'date': Series(0, index=range(5),
                                              dtype=np.int64),
                               'val': Series(range(5), dtype=np.int64)})
@@ -588,7 +588,7 @@ Region_1,Site_2,3977723089,A,5/20/2015 8:33,5/20/2015 9:09,Yes,No"""
         # non-unique indexer with loc slice
         # https://groups.google.com/forum/?fromgroups#!topic/pydata/zTm2No0crYs
 
-        # these are going to raise becuase the we are non monotonic
+        # these are going to raise because the we are non monotonic
         df = DataFrame({'A': [1, 2, 3, 4, 5, 6],
                         'B': [3, 4, 5, 6, 7, 8]}, index=[0, 1, 0, 1, 2, 3])
         pytest.raises(KeyError, df.loc.__getitem__,
diff --git a/pandas/tests/io/data/banklist.html b/pandas/tests/io/data/banklist.html
index 8ec1561f8..cbcce5a2d 100644
--- a/pandas/tests/io/data/banklist.html
+++ b/pandas/tests/io/data/banklist.html
@@ -7,7 +7,7 @@
 <meta charset="UTF-8">
 <!-- Unicode character encoding -->
 <meta http-equiv="X-UA-Compatible" content="IE=edge">
-<!-- Turns off IE Compatiblity Mode -->
+<!-- Turns off IE Compatibility Mode -->
 <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
 <!-- Makes it so phones don't auto zoom out. -->
 <meta name="author" content="DRR">
@@ -4849,7 +4849,7 @@ prepare_responsive_header_nav();
 		<ul>
 			<li><a href="/about/freedom/" title="Freedom of Information Act (FOIA) Service Center">Freedom of Information Act (FOIA) Service Center</a></li>
 			<li><a href="/open/" title="FDIC Open Government Webpage">FDIC Open Government Webpage</a></li>
-			<li><a href="/about/diversity/nofear/" title="No FEAR Act Data">No  FEAR Act Data</a></li>
+			<li><a href="/about/diversity/nofear/" title="No FEAR Act Data">No FEAR Act Data</a></li>
 		</ul>
 	</div>
 	<div id="responsive_footer-small">
diff --git a/pandas/tests/io/data/macau.html b/pandas/tests/io/data/macau.html
index be62b3221..cfd1a0702 100644
--- a/pandas/tests/io/data/macau.html
+++ b/pandas/tests/io/data/macau.html
@@ -476,7 +476,7 @@ cursor:pointer;
 	toggleclass: ["", "selected"], //Two CSS classes to be applied to the header when it's collapsed and expanded, respectively ["class1", "class2"]
 	togglehtml: ["", "", ""], //Additional HTML added to the header when it's collapsed and expanded, respectively  ["position", "html1", "html2"] (see docs)
 	animatespeed: "normal", //speed of animation: integer in milliseconds (ie: 200), or keywords "fast", "normal", or "slow"
-	oninit:function(headers, expandedindices){ //custom code to run when headers have initalized
+	oninit:function(headers, expandedindices){ //custom code to run when headers have initialized
 		//do nothing
 	},
 	onopenclose:function(header, index, state, isuseractivated){ //custom code to run whenever a header is opened or closed
diff --git a/pandas/tests/io/data/spam.html b/pandas/tests/io/data/spam.html
index 935b39f6d..e4fadab6e 100644
--- a/pandas/tests/io/data/spam.html
+++ b/pandas/tests/io/data/spam.html
@@ -208,7 +208,7 @@ handler: function() {this.cancel();},
 	<table>
                 <thead>
                 
-                <tr><td colspan="6" style="vertical-align:middle;text-align:center;height:2em;" class="buttons"><input type="submit" name="_action_show" value="Apply Changes" class="calc" title="Click to recalculate measures" id="1732" /><a href="/ndb/help/contextHelp/measures" onclick="jQuery.ajax({type:'POST', url:'/ndb/help/contextHelp/measures',success:function(data,textStatus){jQuery('#helpDiv').html(data);},error:function(XMLHttpRequest,textStatus,errorThrown){},complete:function(XMLHttpRequest,textStatus){GRAILSUI.measuresHelpDialog.show();}});return false;" controller="help" action="contextHelp" id="measures"><img  title="Click for more information on calculating household measures" src="/ndb/static/images/skin/help.png" alt="Help" border="0" style="vertical-align:middle"/></a></span></td></tr>
+                <tr><td colspan="6" style="vertical-align:middle;text-align:center;height:2em;" class="buttons"><input type="submit" name="_action_show" value="Apply Changes" class="calc" title="Click to recalculate measures" id="1732" /><a href="/ndb/help/contextHelp/measures" onclick="jQuery.ajax({type:'POST', url:'/ndb/help/contextHelp/measures',success:function(data,textStatus){jQuery('#helpDiv').html(data);},error:function(XMLHttpRequest,textStatus,errorThrown){},complete:function(XMLHttpRequest,textStatus){GRAILSUI.measuresHelpDialog.show();}});return false;" controller="help" action="contextHelp" id="measures"><img title="Click for more information on calculating household measures" src="/ndb/static/images/skin/help.png" alt="Help" border="0" style="vertical-align:middle"/></a></span></td></tr>
                 <th style="vertical-align:middle">Nutrient</th>
 				<th style="vertical-align:middle" >Unit</th>
                 <th style="vertical-align:middle"><input type="text" name="Qv" style="width:30px;text-align:right;border-style:inset;height:15px" maxlength="5" value="1" id="Qv" /><br/>Value per 100.0g</th>
diff --git a/pandas/tests/io/formats/test_format.py b/pandas/tests/io/formats/test_format.py
index 1fefec603..23b42b612 100644
--- a/pandas/tests/io/formats/test_format.py
+++ b/pandas/tests/io/formats/test_format.py
@@ -369,7 +369,7 @@ class TestDataFrameFormatting(object):
 
     def test_auto_detect(self):
         term_width, term_height = get_terminal_size()
-        fac = 1.05  # Arbitrary large factor to exceed term widht
+        fac = 1.05  # Arbitrary large factor to exceed term width
         cols = range(int(term_width * fac))
         index = range(10)
         df = DataFrame(index=index, columns=cols)
diff --git a/pandas/tests/io/formats/test_style.py b/pandas/tests/io/formats/test_style.py
index 4b0ca872d..bedb11d4f 100644
--- a/pandas/tests/io/formats/test_style.py
+++ b/pandas/tests/io/formats/test_style.py
@@ -650,7 +650,7 @@ class TestStyler(object):
                         (0, 0): [''], (1, 0): ['']}
             assert result == expected
 
-        # separate since we cant negate the strs
+        # separate since we can't negate the strs
         df['C'] = ['a', 'b']
         result = df.style.highlight_max()._compute().ctx
         expected = {(1, 1): ['background-color: yellow']}
diff --git a/pandas/tests/io/msgpack/test_extension.py b/pandas/tests/io/msgpack/test_extension.py
index 26a611bea..2ee72c8a5 100644
--- a/pandas/tests/io/msgpack/test_extension.py
+++ b/pandas/tests/io/msgpack/test_extension.py
@@ -46,7 +46,7 @@ def test_extension_type():
             typecode = 123  # application specific typecode
             data = tobytes(obj)
             return ExtType(typecode, data)
-        raise TypeError("Unknwon type object %r" % (obj, ))
+        raise TypeError("Unknown type object %r" % (obj, ))
 
     def ext_hook(code, data):
         print('ext_hook called', code, data)
diff --git a/pandas/tests/io/msgpack/test_seq.py b/pandas/tests/io/msgpack/test_seq.py
index 5f203e899..06e9872a2 100644
--- a/pandas/tests/io/msgpack/test_seq.py
+++ b/pandas/tests/io/msgpack/test_seq.py
@@ -25,7 +25,7 @@ def test_exceeding_unpacker_read_size():
     # double free or corruption (!prev)
 
     # 40 ok for read_size=1024, while 50 introduces errors
-    # 7000 ok for read_size=1024*1024, while 8000 leads to  glibc detected ***
+    # 7000 ok for read_size=1024*1024, while 8000 leads to glibc detected ***
     # python: double free or corruption (!prev):
 
     for idx in range(NUMBER_OF_STRINGS):
diff --git a/pandas/tests/io/parser/parse_dates.py b/pandas/tests/io/parser/parse_dates.py
index 7ff2ac9ff..b7d0dd1a3 100644
--- a/pandas/tests/io/parser/parse_dates.py
+++ b/pandas/tests/io/parser/parse_dates.py
@@ -217,8 +217,8 @@ KORD6,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
             tm.assert_series_equal(expected, result.dtypes)
 
             # test with NaT for the nan_rep
-            # we don't have a method to specif the Datetime na_rep (it defaults
-            # to '')
+            # we don't have a method to specify the Datetime na_rep
+            # (it defaults to '')
             df.to_csv(path)
             result = self.read_csv(path, index_col=0, parse_dates=['B'])
             tm.assert_frame_equal(result, df)
diff --git a/pandas/tests/io/test_excel.py b/pandas/tests/io/test_excel.py
index 168144d78..3263f71de 100644
--- a/pandas/tests/io/test_excel.py
+++ b/pandas/tests/io/test_excel.py
@@ -861,8 +861,8 @@ class XlrdTests(ReadingTestsBase):
                             if (c_idx_levels == 1 and c_idx_names):
                                 continue
 
-                            # empty name case current read in as unamed levels,
-                            # not Nones
+                            # empty name case current read in as unnamed
+                            # levels, not Nones
                             check_names = True
                             if not r_idx_names and r_idx_levels > 1:
                                 check_names = False
diff --git a/pandas/tests/io/test_packers.py b/pandas/tests/io/test_packers.py
index b9d66426c..c343e0105 100644
--- a/pandas/tests/io/test_packers.py
+++ b/pandas/tests/io/test_packers.py
@@ -205,7 +205,7 @@ class TestNumpy(TestPackers):
 
     def test_list_numpy_float_complex(self):
         if not hasattr(np, 'complex128'):
-            pytest.skip('numpy cant handle complex128')
+            pytest.skip('numpy can not handle complex128')
 
         x = [np.float32(np.random.rand()) for i in range(5)] + \
             [np.complex128(np.random.rand() + 1j * np.random.rand())
diff --git a/pandas/tests/io/test_pickle.py b/pandas/tests/io/test_pickle.py
index d5bcf7248..5d2ba8e4f 100644
--- a/pandas/tests/io/test_pickle.py
+++ b/pandas/tests/io/test_pickle.py
@@ -38,7 +38,7 @@ def current_pickle_data():
 
 
 # ---------------------
-# comparision functions
+# comparison functions
 # ---------------------
 def compare_element(result, expected, typ, version=None):
     if isinstance(expected, Index):
diff --git a/pandas/tests/io/test_pytables.py b/pandas/tests/io/test_pytables.py
index 305c1ebce..b40350ada 100644
--- a/pandas/tests/io/test_pytables.py
+++ b/pandas/tests/io/test_pytables.py
@@ -903,7 +903,7 @@ class TestHDFStore(Base):
                     'items', 'major_axis', 'minor_axis'])
                 assert_panel4d_equal(store['p4d'], p4d)
 
-                # test using differnt number of items on each axis
+                # test using different number of items on each axis
                 p4d2 = p4d.copy()
                 p4d2['l4'] = p4d['l1']
                 p4d2['l5'] = p4d['l1']
@@ -1300,11 +1300,11 @@ class TestHDFStore(Base):
             df['int16'] = Series([1] * len(df), dtype='int16')
             store.append('df', df)
 
-            # store additonal fields in different blocks
+            # store additional fields in different blocks
             df['int16_2'] = Series([1] * len(df), dtype='int16')
             pytest.raises(ValueError, store.append, 'df', df)
 
-            # store multile additonal fields in different blocks
+            # store multile additional fields in different blocks
             df['float_3'] = Series([1.] * len(df), dtype='float64')
             pytest.raises(ValueError, store.append, 'df', df)
 
@@ -1330,7 +1330,7 @@ class TestHDFStore(Base):
                 assert_panel4d_equal(store.select('p4d'), p4d)
                 check_indexers('p4d', indexers)
 
-                # same as above, but try to append with differnt axes
+                # same as above, but try to append with different axes
                 _maybe_remove(store, 'p4d')
                 store.append('p4d', p4d.iloc[:, :, :10, :], axes=indexers)
                 store.append('p4d', p4d.iloc[:, :, 10:, :], axes=[
@@ -2083,7 +2083,7 @@ class TestHDFStore(Base):
             assert df.dtypes['invalid'] == np.object_
             pytest.raises(TypeError, store.append, 'df', df)
 
-            # directy ndarray
+            # directly ndarray
             pytest.raises(TypeError, store.append, 'df', np.arange(10))
 
             # series directly
@@ -3066,7 +3066,7 @@ class TestHDFStore(Base):
             expected = df.loc[:, ['A']]
             assert_frame_equal(result, expected)
 
-        # dups accross dtypes
+        # dups across dtypes
         df = concat([DataFrame(np.random.randn(10, 4),
                                columns=['A', 'A', 'B', 'B']),
                      DataFrame(np.random.randint(0, 10, size=20)
@@ -5410,7 +5410,7 @@ class TestTimezones(Base):
                 b_e = b.loc[i, c]
                 if not (a_e == b_e and a_e.tz == b_e.tz):
                     raise AssertionError(
-                        "invalid tz comparsion [%s] [%s]" % (a_e, b_e))
+                        "invalid tz comparison [%s] [%s]" % (a_e, b_e))
 
     def test_append_with_timezones_dateutil(self):
 
diff --git a/pandas/tests/plotting/test_frame.py b/pandas/tests/plotting/test_frame.py
index d61b0a403..3d25b0b51 100644
--- a/pandas/tests/plotting/test_frame.py
+++ b/pandas/tests/plotting/test_frame.py
@@ -675,7 +675,7 @@ class TestDataFramePlots(TestPlotBase):
     def _compare_stacked_y_cood(self, normal_lines, stacked_lines):
         base = np.zeros(len(normal_lines[0].get_data()[1]))
         for nl, sl in zip(normal_lines, stacked_lines):
-            base += nl.get_data()[1]  # get y coodinates
+            base += nl.get_data()[1]  # get y coordinates
             sy = sl.get_data()[1]
             tm.assert_numpy_array_equal(base, sy)
 
diff --git a/pandas/tests/plotting/test_misc.py b/pandas/tests/plotting/test_misc.py
index bb590d523..60ed280bc 100644
--- a/pandas/tests/plotting/test_misc.py
+++ b/pandas/tests/plotting/test_misc.py
@@ -219,7 +219,7 @@ class TestDataFramePlots(TestPlotBase):
         prev_next_tupels = zip([i for i in ordered_color_label_tuples[0:-1]],
                                [i for i in ordered_color_label_tuples[1:]])
         for prev, nxt in prev_next_tupels:
-            # lables and colors are ordered strictly increasing
+            # labels and colors are ordered strictly increasing
             assert prev[1] < nxt[1] and prev[0] < nxt[0]
 
     @pytest.mark.slow
diff --git a/pandas/tests/reshape/test_reshape.py b/pandas/tests/reshape/test_reshape.py
index 0312af12e..22925cceb 100644
--- a/pandas/tests/reshape/test_reshape.py
+++ b/pandas/tests/reshape/test_reshape.py
@@ -341,7 +341,7 @@ class TestGetDummies(object):
         assert_frame_equal(result, expected)
 
     def test_basic_drop_first_NA(self, sparse):
-        # Test NA hadling together with drop_first
+        # Test NA handling together with drop_first
         s_NA = ['a', 'b', np.nan]
         res = get_dummies(s_NA, drop_first=True, sparse=sparse)
         exp = DataFrame({'b': [0, 1, 0]}, dtype=np.uint8)
diff --git a/pandas/tests/series/test_analytics.py b/pandas/tests/series/test_analytics.py
index 14bf194ba..f2b7c20b7 100644
--- a/pandas/tests/series/test_analytics.py
+++ b/pandas/tests/series/test_analytics.py
@@ -704,7 +704,7 @@ class TestSeriesAnalytics(TestData):
     def test_built_in_round(self):
         if not compat.PY3:
             pytest.skip(
-                'build in round cannot be overriden prior to Python 3')
+                'build in round cannot be overridden prior to Python 3')
 
         s = Series([1.123, 2.123, 3.123], index=lrange(3))
         result = round(s)
@@ -1338,7 +1338,7 @@ class TestSeriesAnalytics(TestData):
         with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
             # The deprecation of Series.argmin also causes a deprecation
             # warning when calling np.argmin. This behavior is temporary
-            # until the implemention of Series.argmin is corrected.
+            # until the implementation of Series.argmin is corrected.
             result = np.argmin(s)
 
         assert result == 1
@@ -1408,7 +1408,7 @@ class TestSeriesAnalytics(TestData):
         with tm.assert_produces_warning(FutureWarning, check_stacklevel=False):
             # The deprecation of Series.argmax also causes a deprecation
             # warning when calling np.argmax. This behavior is temporary
-            # until the implemention of Series.argmax is corrected.
+            # until the implementation of Series.argmax is corrected.
             result = np.argmax(s)
         assert result == 10
 
diff --git a/pandas/tests/series/test_api.py b/pandas/tests/series/test_api.py
index a2838f803..8ae7feab4 100644
--- a/pandas/tests/series/test_api.py
+++ b/pandas/tests/series/test_api.py
@@ -216,7 +216,7 @@ class TestSeriesMisc(TestData, SharedWithSparse):
         assert 'dt' not in dir(s)
         assert 'cat' not in dir(s)
 
-        # similiarly for .dt
+        # similarly for .dt
         s = Series(date_range('1/1/2015', periods=5))
         assert 'dt' in dir(s)
         assert 'str' not in dir(s)
diff --git a/pandas/tests/series/test_constructors.py b/pandas/tests/series/test_constructors.py
index 08416fe34..5de5f1f05 100644
--- a/pandas/tests/series/test_constructors.py
+++ b/pandas/tests/series/test_constructors.py
@@ -487,7 +487,7 @@ class TestSeriesConstructors(TestData):
     def test_constructor_datelike_coercion(self):
 
         # GH 9477
-        # incorrectly infering on dateimelike looking when object dtype is
+        # incorrectly inferring on dateimelike looking when object dtype is
         # specified
         s = Series([Timestamp('20130101'), 'NOV'], dtype=object)
         assert s.iloc[0] == Timestamp('20130101')
diff --git a/pandas/tests/series/test_indexing.py b/pandas/tests/series/test_indexing.py
index 00fa980d9..0503a7b30 100644
--- a/pandas/tests/series/test_indexing.py
+++ b/pandas/tests/series/test_indexing.py
@@ -1616,7 +1616,7 @@ class TestSeriesIndexing(TestData):
     def test_setitem_boolean(self):
         mask = self.series > self.series.median()
 
-        # similiar indexed series
+        # similar indexed series
         result = self.series.copy()
         result[mask] = self.series * 2
         expected = self.series * 2
@@ -1668,7 +1668,7 @@ class TestSeriesIndexing(TestData):
         s[::2] = np.nan
         assert_series_equal(s, expected)
 
-        # get's coerced to float, right?
+        # gets coerced to float, right?
         expected = Series([np.nan, 1, np.nan, 0])
         s = Series([True, True, False, False])
         s[::2] = np.nan
@@ -2113,7 +2113,7 @@ class TestSeriesIndexing(TestData):
         result = s.reindex(new_index, method='ffill')
         assert_series_equal(result, expected)
 
-        # inferrence of new dtype
+        # inference of new dtype
         s = Series([True, False, False, True], index=list('abcd'))
         new_index = 'agc'
         result = s.reindex(list(new_index)).ffill()
diff --git a/pandas/tests/series/test_operators.py b/pandas/tests/series/test_operators.py
index 433e3cf44..ce4e388bc 100644
--- a/pandas/tests/series/test_operators.py
+++ b/pandas/tests/series/test_operators.py
@@ -1359,7 +1359,7 @@ class TestSeriesOperators(TestData):
                 expecteds = divmod(series.values, np.asarray(other_np))
 
             for result, expected in zip(results, expecteds):
-                # check the values, name, and index separatly
+                # check the values, name, and index separately
                 assert_almost_equal(np.asarray(result), expected)
 
                 assert result.name == series.name
@@ -1449,7 +1449,7 @@ class TestSeriesOperators(TestData):
                 assert_series_equal(lhs, rhs)
             except:
                 raise AssertionError(
-                    "invalid comparsion [op->{0},d->{1},h->{2},m->{3},"
+                    "invalid comparison [op->{0},d->{1},h->{2},m->{3},"
                     "s->{4},us->{5}]\n{6}\n{7}\n".format(op, d, h, m, s,
                                                          us, lhs, rhs))
 
diff --git a/pandas/tests/test_base.py b/pandas/tests/test_base.py
index df76390d7..cb905d818 100644
--- a/pandas/tests/test_base.py
+++ b/pandas/tests/test_base.py
@@ -116,7 +116,8 @@ class TestPandasDelegate(object):
 
     def test_invalida_delgation(self):
         # these show that in order for the delegation to work
-        # the _delegate_* methods need to be overriden to not raise a TypeError
+        # the _delegate_* methods need to be overridden to not raise
+        # a TypeError
 
         self.Delegate._add_delegate_accessors(
             delegate=self.Delegator,
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index 592b069ef..86d9a9fa9 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -2383,7 +2383,7 @@ Thur,Lunch,Yes,51.51,17"""
 
 
 class TestSorted(Base):
-    """ everthing you wanted to test about sorting """
+    """ everything you wanted to test about sorting """
 
     def test_sort_index_preserve_levels(self):
         result = self.frame.sort_index()
diff --git a/pandas/tests/test_panel.py b/pandas/tests/test_panel.py
index d772dba25..770560134 100644
--- a/pandas/tests/test_panel.py
+++ b/pandas/tests/test_panel.py
@@ -2582,19 +2582,19 @@ class TestLongPanel(object):
             trunced = self.panel.truncate(start, end).to_panel()
             expected = self.panel.to_panel()['ItemA'].truncate(start, end)
 
-            # TODO trucate drops index.names
+            # TODO truncate drops index.names
             assert_frame_equal(trunced['ItemA'], expected, check_names=False)
 
             trunced = self.panel.truncate(before=start).to_panel()
             expected = self.panel.to_panel()['ItemA'].truncate(before=start)
 
-            # TODO trucate drops index.names
+            # TODO truncate drops index.names
             assert_frame_equal(trunced['ItemA'], expected, check_names=False)
 
             trunced = self.panel.truncate(after=end).to_panel()
             expected = self.panel.to_panel()['ItemA'].truncate(after=end)
 
-            # TODO trucate drops index.names
+            # TODO truncate drops index.names
             assert_frame_equal(trunced['ItemA'], expected, check_names=False)
 
             # truncate on dates that aren't in there
diff --git a/pandas/tests/test_sorting.py b/pandas/tests/test_sorting.py
index 57bd5e7b6..d0350ba25 100644
--- a/pandas/tests/test_sorting.py
+++ b/pandas/tests/test_sorting.py
@@ -82,7 +82,7 @@ class TestSorting(object):
         # verify this is testing what it is supposed to test!
         assert is_int64_overflow_possible(gr.grouper.shape)
 
-        # mannually compute groupings
+        # manually compute groupings
         jim, joe = defaultdict(list), defaultdict(list)
         for key, a, b in zip(map(tuple, arr), df['jim'], df['joe']):
             jim[key].append(a)
diff --git a/pandas/tests/test_window.py b/pandas/tests/test_window.py
index ccffc554e..6f9e87252 100644
--- a/pandas/tests/test_window.py
+++ b/pandas/tests/test_window.py
@@ -3417,7 +3417,7 @@ class TestRollingTS(object):
 
         # test as a frame
         # we should be ignoring the 'on' as an aggregation column
-        # note that the expected is setting, computing, and reseting
+        # note that the expected is setting, computing, and resetting
         # so the columns need to be switched compared
         # to the actual result where they are ordered as in the
         # original
@@ -3815,7 +3815,7 @@ class TestRollingTS(object):
 
     def test_all(self):
 
-        # simple comparision of integer vs time-based windowing
+        # simple comparison of integer vs time-based windowing
         df = self.regular * 2
         er = df.rolling(window=1)
         r = df.rolling(window='1s')
@@ -3837,7 +3837,7 @@ class TestRollingTS(object):
 
     def test_all2(self):
 
-        # more sophisticated comparision of integer vs.
+        # more sophisticated comparison of integer vs.
         # time-based windowing
         df = DataFrame({'B': np.arange(50)},
                        index=pd.date_range('20130101',
diff --git a/pandas/tests/tseries/offsets/test_offsets.py b/pandas/tests/tseries/offsets/test_offsets.py
index edabf4a7c..e1a6463e7 100644
--- a/pandas/tests/tseries/offsets/test_offsets.py
+++ b/pandas/tests/tseries/offsets/test_offsets.py
@@ -3016,7 +3016,7 @@ class TestDST(object):
                     t.minute == tstart.minute and
                     t.second == tstart.second)
         elif offset_name in self.valid_date_offsets_singular:
-            # expect the signular offset value to match between tstart and t
+            # expect the singular offset value to match between tstart and t
             datepart_offset = getattr(t, offset_name
                                       if offset_name != 'weekday' else
                                       'dayofweek')
@@ -3063,7 +3063,7 @@ class TestDST(object):
                 expected_utc_offset=hrs_post)
 
     def test_fallback_singular(self):
-        # in the case of signular offsets, we dont neccesarily know which utc
+        # in the case of singular offsets, we don't necessarily know which utc
         # offset the new Timestamp will wind up in (the tz for 1 month may be
         # different from 1 second) so we don't specify an expected_utc_offset
         for tz, utc_offsets in self.timezone_utc_offsets.items():
diff --git a/pandas/tests/tseries/test_timezones.py b/pandas/tests/tseries/test_timezones.py
index 9530cd5ac..b3813d035 100644
--- a/pandas/tests/tseries/test_timezones.py
+++ b/pandas/tests/tseries/test_timezones.py
@@ -577,7 +577,7 @@ class TestTimeZoneSupportPytz(object):
                  '11/06/2011 03:00']
         di_test = DatetimeIndex(times, tz='US/Eastern')
 
-        # left dtype is  datetime64[ns, US/Eastern]
+        # left dtype is datetime64[ns, US/Eastern]
         # right is datetime64[ns, tzfile('/usr/share/zoneinfo/US/Eastern')]
         tm.assert_numpy_array_equal(di_test.values, localized.values)
 
diff --git a/pandas/tseries/holiday.py b/pandas/tseries/holiday.py
index 0e6cbea21..4e874eac9 100644
--- a/pandas/tseries/holiday.py
+++ b/pandas/tseries/holiday.py
@@ -133,7 +133,7 @@ class Holiday(object):
             Name of the holiday , defaults to class name
         offset : array of pandas.tseries.offsets or
                 class from pandas.tseries.offsets
-            computes offset from  date
+            computes offset from date
         observance: function
             computes when holiday is given a pandas Timestamp
         days_of_week:
diff --git a/pandas/util/_validators.py b/pandas/util/_validators.py
index 728db6af5..b30ffc741 100644
--- a/pandas/util/_validators.py
+++ b/pandas/util/_validators.py
@@ -40,7 +40,7 @@ def _check_for_default_values(fname, arg_val_dict, compat_args):
     """
     for key in arg_val_dict:
         # try checking equality directly with '=' operator,
-        # as comparison may have been overriden for the left
+        # as comparison may have been overridden for the left
         # hand object
         try:
             v1 = arg_val_dict[key]
@@ -292,7 +292,7 @@ def validate_axis_style_args(data, args, kwargs, arg_name, method_name):
             out[ax] = v
 
     # All user-provided kwargs have been handled now.
-    # Now we supplement with positional arguments, emmitting warnings
+    # Now we supplement with positional arguments, emitting warnings
     # when there's ambiguity and raising when there's conflicts
 
     if len(args) == 0:
@@ -307,7 +307,7 @@ def validate_axis_style_args(data, args, kwargs, arg_name, method_name):
                    "or 'columns'")
             raise TypeError(msg)
 
-        msg = ("Intepreting call\n\t'.{method_name}(a, b)' as "
+        msg = ("Interpreting call\n\t'.{method_name}(a, b)' as "
                "\n\t'.{method_name}(index=a, columns=b)'.\nUse named "
                "arguments to remove any ambiguity. In the future, using "
                "positional arguments for 'index' or 'columns' will raise "
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 8acf16536..8dc0aa1e8 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -107,7 +107,7 @@ def round_trip_pickle(obj, path=None):
 
 def round_trip_pathlib(writer, reader, path=None):
     """
-    Write an object to file specifed by a pathlib.Path and read it back
+    Write an object to file specified by a pathlib.Path and read it back
 
     Parameters
     ----------
@@ -136,7 +136,7 @@ def round_trip_pathlib(writer, reader, path=None):
 
 def round_trip_localpath(writer, reader, path=None):
     """
-    Write an object to file specifed by a py.path LocalPath and read it back
+    Write an object to file specified by a py.path LocalPath and read it back
 
     Parameters
     ----------
@@ -1784,8 +1784,8 @@ def makeCustomDataframe(nrows, ncols, c_idx_names=True, r_idx_names=True,
     """
    nrows,  ncols - number of data rows/cols
    c_idx_names, idx_names  - False/True/list of strings,  yields No names ,
-        default names or  uses the provided names for the levels of the
-        corresponding  index. You can provide a single string when
+        default names or uses the provided names for the levels of the
+        corresponding index. You can provide a single string when
         c_idx_nlevels ==1.
    c_idx_nlevels - number of levels in columns index. > 1 will yield MultiIndex
    r_idx_nlevels - number of levels in rows index. > 1 will yield MultiIndex
@@ -2081,7 +2081,7 @@ def network(t, url="http://www.google.com",
     _skip_on_messages: iterable of string
         any exception e for which one of the strings is
         a substring of str(e) will be skipped with an appropriate
-        message. Intended to supress errors where an errno isn't available.
+        message. Intended to suppress errors where an errno isn't available.
 
     Notes
     -----
diff --git a/scripts/find_commits_touching_func.py b/scripts/find_commits_touching_func.py
index 4f3b51977..0dd609417 100755
--- a/scripts/find_commits_touching_func.py
+++ b/scripts/find_commits_touching_func.py
@@ -26,21 +26,21 @@ except ImportError:
 import argparse
 
 desc = """
-Find all commits touching a sepcified function across the codebase.
+Find all commits touching a specified function across the codebase.
 """.strip()
 argparser = argparse.ArgumentParser(description=desc)
 argparser.add_argument('funcname', metavar='FUNCNAME',
                    help='Name of function/method to search for changes on.')
 argparser.add_argument('-f', '--file-masks', metavar='f_re(,f_re)*',
                        default=["\.py.?$"],
-                   help='comma seperated list of regexes to match filenames against\n'+
+                   help='comma separated list of regexes to match filenames against\n'+
                    'defaults all .py? files')
 argparser.add_argument('-d', '--dir-masks', metavar='d_re(,d_re)*',
                        default=[],
-                   help='comma seperated list of regexes to match base path against')
+                   help='comma separated list of regexes to match base path against')
 argparser.add_argument('-p', '--path-masks', metavar='p_re(,p_re)*',
                        default=[],
-                   help='comma seperated list of regexes to match full file path against')
+                   help='comma separated list of regexes to match full file path against')
 argparser.add_argument('-y', '--saw-the-warning',
                        action='store_true',default=False,
                    help='must specify this to run, acknowledge you realize this will erase untracked files')
diff --git a/scripts/find_undoc_args.py b/scripts/find_undoc_args.py
index 49273bacc..32b23a67b 100755
--- a/scripts/find_undoc_args.py
+++ b/scripts/find_undoc_args.py
@@ -19,7 +19,7 @@ parser.add_argument('-p', '--path', metavar='PATH', type=str, required=False,
 parser.add_argument('-m', '--module', metavar='MODULE', type=str,required=True,
                    help='name of package to import and examine',action='store')
 parser.add_argument('-G', '--github_repo', metavar='REPO', type=str,required=False,
-                   help='github project where the the code lives, e.g. "pandas-dev/pandas"',
+                   help='github project where the code lives, e.g. "pandas-dev/pandas"',
                    default=None,action='store')
 
 args = parser.parse_args()
diff --git a/setup.py b/setup.py
index 515e1660f..443f3eba6 100755
--- a/setup.py
+++ b/setup.py
@@ -109,7 +109,7 @@ class build_ext(_build_ext):
         # generate template output
         if cython:
             for pxifile in _pxifiles:
-                # build pxifiles first, template extention must be .pxi.in
+                # build pxifiles first, template extension must be .pxi.in
                 assert pxifile.endswith('.pxi.in')
                 outfile = pxifile[:-3]
 
