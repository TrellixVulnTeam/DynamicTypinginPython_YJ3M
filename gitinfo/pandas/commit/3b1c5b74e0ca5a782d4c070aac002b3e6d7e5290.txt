commit 3b1c5b74e0ca5a782d4c070aac002b3e6d7e5290
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Wed Jan 4 20:46:16 2012 -0500

    ENH: extensively refactor BlockJoinOperation to support n > 2, Concatenator class to orchestrate concatenations, #273, #479

diff --git a/RELEASE.rst b/RELEASE.rst
index 184d2bb7c..856837319 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -89,6 +89,7 @@ pandas 0.7.0
     5-10x in most typical use cases (GH #374)
   - Some performance enhancements in constructing a Panel from a dict of
     DataFrame objects
+  - Made ``Index._get_duplicates`` a public method by removing the underscore
 
 **Bug fixes**
 
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index e00d5f1c5..a7ac6545f 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -23,7 +23,7 @@ import numpy as np
 import numpy.ma as ma
 
 from pandas.core.common import (isnull, notnull, PandasError, _try_sort,
-                                _default_index, _stringify, _maybe_upcast)
+                                _default_index, _stringify)
 from pandas.core.daterange import DateRange
 from pandas.core.generic import NDFrame
 from pandas.core.index import Index, MultiIndex, NULL_INDEX, _ensure_index
@@ -1638,7 +1638,8 @@ class DataFrame(NDFrame):
 
     truncate = generic.truncate
 
-    def set_index(self, col_or_cols, drop=True, inplace=False):
+    def set_index(self, col_or_cols, drop=True, inplace=False,
+                  verify_integrity=True):
         """
         Set the DataFrame index (row labels) using one or more existing
         columns. By default yields a new object.
@@ -1650,6 +1651,10 @@ class DataFrame(NDFrame):
             Delete columns to be used as the new index
         inplace : boolean, default False
             Modify the DataFrame in place (do not create a new object)
+        verify_integrity : boolean, default True
+            Check the new index for duplicates. Otherwise defer the check until
+            necessary. Setting to False will improve the performance of this
+            method
 
         Returns
         -------
@@ -1674,8 +1679,8 @@ class DataFrame(NDFrame):
 
         index = MultiIndex.from_arrays(arrays, names=cols)
 
-        if not index._verify_integrity():
-            duplicates = index._get_duplicates()
+        if verify_integrity and not index._verify_integrity():
+            duplicates = index.get_duplicates()
             raise Exception('Index has duplicate keys: %s' % duplicates)
 
         # clear up memory usage
@@ -2738,60 +2743,13 @@ class DataFrame(NDFrame):
         if not self:
             return other.copy()
 
-        if ignore_index:
-            new_index = None
+        from pandas.tools.merge import concat
+        if isinstance(other, list):
+            to_concat = [self] + other
         else:
-            new_index = self.index.append(other.index)
-            assert(new_index._verify_integrity())
-
-        if self.columns.equals(other.columns):
-            return self._append_same_columns(other, new_index)
-        else:
-            return self._append_different_columns(other, new_index)
-
-    def _append_different_columns(self, other, new_index):
-        indexer = self.columns.get_indexer(other.columns)
-
-        if not (indexer == -1).any():
-            new_columns = self.columns
-        else:
-            new_columns = self.columns.union(other.columns)
-
-        new_data = self._append_column_by_column(other)
-        return self._constructor(data=new_data, index=new_index,
-                                 columns=new_columns)
-
-    def _append_same_columns(self, other, new_index):
-        if self._is_mixed_type:
-            new_data = self._append_column_by_column(other)
-        else:
-            new_data = np.concatenate((self.values, other.values), axis=0)
-        return self._constructor(new_data, index=new_index,
-                                 columns=self.columns)
-
-    def _append_column_by_column(self, other):
-        def _concat_missing(values, n):
-            values = _maybe_upcast(values)
-            missing_values = np.empty(n, dtype=values.dtype)
-            missing_values.fill(np.nan)
-            return values, missing_values
-
-        new_data = {}
-        for col in self:
-            values = self._get_raw_column(col)
-            if col in other:
-                other_values = other._get_raw_column(col)
-            else:
-                values, other_values = _concat_missing(values, len(other))
-            new_data[col] = np.concatenate((values, other_values))
-
-        for col in other:
-            values = other._get_raw_column(col)
-            if col not in self:
-                values, missing_values = _concat_missing(values, len(self))
-                new_data[col] = np.concatenate((missing_values, values))
-
-        return new_data
+            to_concat = [self, other]
+        return concat(to_concat, ignore_index=ignore_index,
+                      verify_integrity=True)
 
     def _get_raw_column(self, col):
         return self._data.get(col)
@@ -3618,6 +3576,8 @@ def factor_agg(factor, vec, func):
 
 
 def extract_index(data):
+    from pandas.core.index import _union_indexes
+
     index = None
     if len(data) == 0:
         index = NULL_INDEX
@@ -3663,51 +3623,6 @@ def extract_index(data):
     return _ensure_index(index)
 
 
-def _union_indexes(indexes):
-    if len(indexes) == 0:
-        return Index([])
-
-    if len(indexes) == 1:
-        result = indexes[0]
-        if isinstance(result, list):
-            result = Index(sorted(result))
-        return result
-
-    indexes, kind = _sanitize_and_check(indexes)
-
-    if kind == 'special':
-        result = indexes[0]
-        for other in indexes[1:]:
-            result = result.union(other)
-        return result
-    elif kind == 'array':
-        index = indexes[0]
-        for other in indexes[1:]:
-            if not index.equals(other):
-                return Index(lib.fast_unique_multiple(indexes))
-
-        return index
-    else:
-        return Index(lib.fast_unique_multiple_list(indexes))
-
-
-def _sanitize_and_check(indexes):
-    kinds = list(set([type(index) for index in indexes]))
-
-    if list in kinds:
-        if len(kinds) > 1:
-            indexes = [Index(_try_sort(x)) if not isinstance(x, Index) else x
-                       for x in indexes]
-            kinds.remove(list)
-        else:
-            return indexes, 'list'
-
-
-    if len(kinds) > 1 or Index not in kinds:
-        return indexes, 'special'
-    else:
-        return indexes, 'array'
-
 
 def _check_data_types(data):
     have_raw_arrays = False
diff --git a/pandas/core/index.py b/pandas/core/index.py
index b5807b5f6..abf91261a 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -5,7 +5,7 @@ from itertools import izip
 
 import numpy as np
 
-from pandas.core.common import (adjoin as _adjoin, _stringify,
+from pandas.core.common import (adjoin as _adjoin, _stringify, _try_sort,
                                 _is_bool_indexer, _asarray_tuplesafe)
 from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
@@ -119,6 +119,15 @@ class Index(np.ndarray):
         except TypeError:
             return False
 
+    def get_duplicates(self):
+        from collections import defaultdict
+        counter = defaultdict(lambda: 0)
+        for k in self.values:
+            counter[k] += 1
+        return sorted(k for k, v in counter.iteritems() if v > 1)
+
+    _get_duplicates = get_duplicates
+
     @property
     def indexMap(self):
         "{label -> location}"
@@ -143,13 +152,6 @@ class Index(np.ndarray):
     def _verify_integrity(self):
         return self._engine.has_integrity
 
-    def _get_duplicates(self):
-        from collections import defaultdict
-        counter = defaultdict(lambda: 0)
-        for k in self.values:
-            counter[k] += 1
-        return sorted(k for k, v in counter.iteritems() if v > 1)
-
     _allDates = None
     def is_all_dates(self):
         """
@@ -1261,9 +1263,6 @@ class MultiIndex(Index):
         appended : Index
         """
         if isinstance(other, (list, tuple)):
-            for k in other:
-                assert(isinstance(k, MultiIndex))
-
             to_concat = (self.values,) + tuple(k.values for k in other)
         else:
             to_concat = self.values, other.values
@@ -1871,3 +1870,66 @@ def _ensure_index(index_like):
 def _validate_join_method(method):
     if method not in ['left', 'right', 'inner', 'outer']:
         raise Exception('do not recognize join method %s' % method)
+
+# TODO: handle index names!
+
+def _get_combined_index(indexes, intersect=False):
+    indexes = _get_distinct_indexes(indexes)
+    if len(indexes) == 1:
+        return indexes[0]
+    if intersect:
+        index = indexes[0]
+        for other in indexes[1:]:
+            index = index.intersection(other)
+        return index
+    union =  _union_indexes(indexes)
+    return Index(union)
+
+def _get_distinct_indexes(indexes):
+    return dict((id(x), x) for x in indexes).values()
+
+
+def _union_indexes(indexes):
+    if len(indexes) == 0:
+        return Index([])
+
+    if len(indexes) == 1:
+        result = indexes[0]
+        if isinstance(result, list):
+            result = Index(sorted(result))
+        return result
+
+    indexes, kind = _sanitize_and_check(indexes)
+
+    if kind == 'special':
+        result = indexes[0]
+        for other in indexes[1:]:
+            result = result.union(other)
+        return result
+    elif kind == 'array':
+        index = indexes[0]
+        for other in indexes[1:]:
+            if not index.equals(other):
+                return Index(lib.fast_unique_multiple(indexes))
+
+        return index
+    else:
+        return Index(lib.fast_unique_multiple_list(indexes))
+
+
+def _sanitize_and_check(indexes):
+    kinds = list(set([type(index) for index in indexes]))
+
+    if list in kinds:
+        if len(kinds) > 1:
+            indexes = [Index(_try_sort(x)) if not isinstance(x, Index) else x
+                       for x in indexes]
+            kinds.remove(list)
+        else:
+            return indexes, 'list'
+
+
+    if len(kinds) > 1 or Index not in kinds:
+        return indexes, 'special'
+    else:
+        return indexes, 'array'
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index 7b306d83a..541bea187 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -9,10 +9,11 @@ import numpy as np
 
 from pandas.core.common import (PandasError, _mut_exclusive,
                                 _try_sort, _default_index, _infer_dtype)
-from pandas.core.index import Factor, Index, MultiIndex, _ensure_index
+from pandas.core.index import (Factor, Index, MultiIndex, _ensure_index,
+                               _get_combined_index, _union_indexes)
 from pandas.core.indexing import _NDFrameIndexer
 from pandas.core.internals import BlockManager, make_block, form_blocks
-from pandas.core.frame import DataFrame, _union_indexes
+from pandas.core.frame import DataFrame
 from pandas.core.generic import NDFrame
 from pandas.util import py3compat
 from pandas.util.decorators import deprecate
@@ -1152,8 +1153,11 @@ def _homogenize_dict(frames, intersect=True, dtype=None):
         else:
             adj_frames[k] = v
 
-    index = _get_combined_index(adj_frames, intersect=intersect)
-    columns = _get_combined_columns(adj_frames, intersect=intersect)
+    all_indexes = [df.index for df in adj_frames.values()]
+    all_columns = [df.columns for df in adj_frames.values()]
+
+    index = _get_combined_index(all_indexes, intersect=intersect)
+    columns = _get_combined_index(all_columns, intersect=intersect)
 
     for key, frame in adj_frames.iteritems():
         result[key] = frame.reindex(index=index, columns=columns,
@@ -1161,43 +1165,6 @@ def _homogenize_dict(frames, intersect=True, dtype=None):
 
     return result, index, columns
 
-def _get_combined_columns(frames, intersect=False):
-    columns = None
-
-    if intersect:
-        combine = set.intersection
-    else:
-        combine = set.union
-
-    for _, frame in frames.iteritems():
-        this_cols = set(frame.columns)
-
-        if columns is None:
-            columns = this_cols
-        else:
-            columns = combine(columns, this_cols)
-
-    return Index(sorted(columns))
-
-def _get_combined_index(frames, intersect=False):
-    from pandas.core.frame import _union_indexes
-
-    indexes = _get_distinct_indexes([df.index for df in frames.values()])
-    if len(indexes) == 1:
-        return indexes[0]
-    if intersect:
-        index = indexes[0]
-        for other in indexes[1:]:
-            index = index.intersection(other)
-        return index
-    union =  _union_indexes(indexes)
-    return Index(union)
-
-def _get_distinct_indexes(indexes):
-    from itertools import groupby
-    indexes = sorted(indexes, key=id)
-    return [gp.next() for _, gp in groupby(indexes, id)]
-
 def _monotonic(arr):
     return not (arr[1:] < arr[:-1]).any()
 
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 09222138d..3fb459d2c 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -413,7 +413,7 @@ class TextParser(object):
             index = Index(np.arange(len(content)))
 
         if not index._verify_integrity():
-            dups = index._get_duplicates()
+            dups = index.get_duplicates()
             raise Exception('Index has duplicates: %s' % str(dups))
 
         if len(self.columns) != len(zipped_content):
diff --git a/pandas/sparse/panel.py b/pandas/sparse/panel.py
index f39ef6a45..51b0a67d9 100644
--- a/pandas/sparse/panel.py
+++ b/pandas/sparse/panel.py
@@ -426,7 +426,7 @@ class SparsePanel(Panel):
 SparseWidePanel = SparsePanel
 
 def _convert_frames(frames, index, columns, fill_value=np.nan, kind='block'):
-    from pandas.core.panel import _get_combined_index, _get_combined_columns
+    from pandas.core.panel import _get_combined_index
     output = {}
     for item, df in frames.iteritems():
         if not isinstance(df, SparseDataFrame):
@@ -436,9 +436,11 @@ def _convert_frames(frames, index, columns, fill_value=np.nan, kind='block'):
         output[item] = df
 
     if index is None:
-        index = _get_combined_index(output)
+        all_indexes = [df.index for df in output.values()]
+        index = _get_combined_index(all_indexes)
     if columns is None:
-        columns = _get_combined_columns(output)
+        all_columns = [df.columns for df in output.values()]
+        columns = _get_combined_index(all_columns)
 
     index = _ensure_index(index)
     columns = _ensure_index(columns)
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index ea8004c79..be738cc42 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -2178,63 +2178,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         mixed2 = mixed1.convert_objects()
         assert_frame_equal(mixed1, mixed2)
 
-    def test_append(self):
-        begin_index = self.frame.index[:5]
-        end_index = self.frame.index[5:]
-
-        begin_frame = self.frame.reindex(begin_index)
-        end_frame = self.frame.reindex(end_index)
-
-        appended = begin_frame.append(end_frame)
-        assert_almost_equal(appended['A'], self.frame['A'])
-
-        del end_frame['A']
-        partial_appended = begin_frame.append(end_frame)
-        self.assert_('A' in partial_appended)
-
-        partial_appended = end_frame.append(begin_frame)
-        self.assert_('A' in partial_appended)
-
-        # mixed type handling
-        appended = self.mixed_frame[:5].append(self.mixed_frame[5:])
-        assert_frame_equal(appended, self.mixed_frame)
-
-        # what to test here
-        mixed_appended = self.mixed_frame[:5].append(self.frame[5:])
-        mixed_appended2 = self.frame[:5].append(self.mixed_frame[5:])
-
-        # all equal except 'foo' column
-        assert_frame_equal(mixed_appended.reindex(columns=['A', 'B', 'C', 'D']),
-                           mixed_appended2.reindex(columns=['A', 'B', 'C', 'D']))
-
-        # append empty
-        appended = self.frame.append(self.empty)
-        assert_frame_equal(self.frame, appended)
-        self.assert_(appended is not self.frame)
-
-        appended = self.empty.append(self.frame)
-        assert_frame_equal(self.frame, appended)
-        self.assert_(appended is not self.frame)
-
-        # overlap
-        self.assertRaises(Exception, self.frame.append, self.frame)
-
-    def test_append_records(self):
-        arr1 = np.zeros((2,),dtype=('i4,f4,a10'))
-        arr1[:] = [(1,2.,'Hello'),(2,3.,"World")]
-
-        arr2 = np.zeros((3,),dtype=('i4,f4,a10'))
-        arr2[:] = [(3, 4.,'foo'),
-                   (5, 6.,"bar"),
-                   (7., 8., 'baz')]
-
-        df1 = DataFrame(arr1)
-        df2 = DataFrame(arr2)
-
-        result = df1.append(df2, ignore_index=True)
-        expected = DataFrame(np.concatenate((arr1, arr2)))
-        assert_frame_equal(result, expected)
-
     def test_append_series(self):
         df = DataFrame(np.random.randn(5, 4),
                        columns=['foo', 'bar', 'baz', 'qux'])
@@ -2243,31 +2186,18 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assertRaises(Exception, df.append, series)
 
         result = df.append(series[::-1], ignore_index=True)
-        expected = df.append(DataFrame({0 : series[::-1]}).T,
+        expected = df.append(DataFrame({0 : series[::-1]},
+                                       index=df.columns).T,
                              ignore_index=True)
         assert_frame_equal(result, expected)
 
         result = df.append(series[::-1][:3], ignore_index=True)
         expected = df.append(DataFrame({0 : series[::-1][:3]}).T,
                              ignore_index=True)
-        assert_frame_equal(result, expected)
+        assert_frame_equal(result, expected.ix[:, result.columns])
 
         # can append when name set
 
-
-    def test_append_different_columns(self):
-        df = DataFrame({'bools' : np.random.randn(10) > 0,
-                        'ints' : np.random.randint(0, 10, 10),
-                        'floats' : np.random.randn(10),
-                        'strings' : ['foo', 'bar'] * 5})
-
-        a = df[:5].ix[:, ['bools', 'ints', 'floats']]
-        b = df[5:].ix[:, ['strings', 'ints', 'floats']]
-
-        appended = a.append(b)
-        self.assert_(isnull(appended['strings'][:5]).all())
-        self.assert_(isnull(appended['bools'][5:]).all())
-
     def test_asfreq(self):
         offset_monthly = self.tsframe.asfreq(datetools.bmonthEnd)
         rule_monthly = self.tsframe.asfreq('EOM')
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index 06db05c31..69725d959 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -6,7 +6,7 @@ import numpy as np
 
 from pandas.core.frame import DataFrame, _merge_doc
 from pandas.core.groupby import get_group_index
-from pandas.core.index import Index, MultiIndex
+from pandas.core.index import Index, MultiIndex, _get_combined_index
 from pandas.core.internals import (IntBlock, BoolBlock, BlockManager,
                                    make_block, _consolidate)
 from pandas.util.decorators import cache_readonly
@@ -30,7 +30,8 @@ if __debug__: merge.__doc__ = _merge_doc % '\nleft : DataFrame'
 
 class _MergeOperation(object):
     """
-
+    Perform a database (SQL) merge operation between two DataFrame objects
+    using either columns as keys or their row indexes
     """
 
     def __init__(self, left, right, how='inner', on=None,
@@ -69,10 +70,11 @@ class _MergeOperation(object):
         ldata, rdata = self._get_merge_data(self.join_names)
 
         # TODO: more efficiently handle group keys to avoid extra consolidation!
-        join_op = _BlockJoinOperation(ldata, rdata, join_index,
-                                      left_indexer, right_indexer, axis=1)
+        join_op = _BlockJoinOperation([ldata, rdata], join_index,
+                                      [left_indexer, right_indexer], axis=1,
+                                      copy=self.copy)
 
-        result_data = join_op.get_result(copy=self.copy)
+        result_data = join_op.get_result()
         result = DataFrame(result_data)
 
         self._maybe_add_join_keys(result, left_indexer, right_indexer)
@@ -377,251 +379,384 @@ def _sort_labels(uniques, left, right):
 
 class _BlockJoinOperation(object):
     """
+    BlockJoinOperation made generic for N DataFrames
+
     Object responsible for orchestrating efficient join operation between two
     BlockManager data structures
     """
-    def __init__(self, left, right, join_index, left_indexer, right_indexer,
-                 axis=1):
-        assert(axis > 0)
+    def __init__(self, data_list, join_index, indexers, axis=1, copy=True):
+        if axis <= 0:
+            raise Exception('Only axis >= 1 supported for this operation')
 
-        if not left.is_consolidated():
-            left = left.consolidate()
-        if not right.is_consolidated():
-            right = right.consolidate()
+        assert(len(data_list) == len(indexers))
 
-        self.left = left
-        self.right = right
-        self.axis = axis
+        self.units = []
+        for data, indexer in zip(data_list, indexers):
+            if not data.is_consolidated():
+                data = data.consolidate()
+            self.units.append(_JoinUnit(data.blocks, indexer))
 
         self.join_index = join_index
-        self.lindexer = left_indexer
-        self.rindexer = right_indexer
+        self.axis = axis
+        self.copy = copy
 
         # do NOT sort
-        self.result_items = left.items.append(right.items)
-        self.result_axes = list(left.axes)
+        self.result_items = _concat_indexes([d.items for d in data_list])
+        self.result_axes = list(data_list[0].axes)
         self.result_axes[0] = self.result_items
         self.result_axes[axis] = self.join_index
 
-    def get_result(self, copy=False):
-        """
-        Parameters
-        ----------
-        other
-        lindexer
-        lmask
-        rindexer
-        rmask
+    def _prepare_blocks(self):
+        blockmaps = []
+
+        for unit in self.units:
+            join_blocks = unit.get_upcasted_blocks()
+            type_map = dict((type(blk), blk) for blk in join_blocks)
+            blockmaps.append(type_map)
 
+        return blockmaps
+
+    def get_result(self):
+        """
         Returns
         -------
         merged : BlockManager
         """
-        left_blockmap, right_blockmap = self._prepare_blocks()
+        blockmaps = self._prepare_blocks()
+        kinds = _get_all_block_kinds(blockmaps)
 
         result_blocks = []
 
-        # maybe want to enable flexible copying
-
-        kinds = set(left_blockmap) | set(right_blockmap)
+        # maybe want to enable flexible copying <-- what did I mean?
         for klass in kinds:
-            lblk = left_blockmap.get(klass)
-            rblk = right_blockmap.get(klass)
-
-            if lblk and rblk:
-                # true merge, do not produce intermediate copy
-                res_blk = self._merge_blocks(lblk, rblk)
-            elif lblk:
-                res_blk = self._reindex_block(lblk, side='left')
-            else:
-                res_blk = self._reindex_block(rblk, side='right')
-
+            klass_blocks = [mapping.get(klass) for mapping in blockmaps]
+            res_blk = self._get_merged_block(klass_blocks)
             result_blocks.append(res_blk)
 
         return BlockManager(result_blocks, self.result_axes)
 
-    def _prepare_blocks(self):
-        lblocks = self.left.blocks
-        rblocks = self.right.blocks
+    def _get_merged_block(self, blocks):
 
-        # will short-circuit and not compute lneed_masking
-        if self.lneed_masking:
-            lblocks = self._upcast_blocks(lblocks)
+        to_merge = []
 
-        if self.rneed_masking:
-            rblocks = self._upcast_blocks(rblocks)
+        for unit, block in zip(self.units, blocks):
+            if block is not None:
+                to_merge.append((unit, block))
 
-        left_blockmap = dict((type(blk), blk) for blk in lblocks)
-        right_blockmap = dict((type(blk), blk) for blk in rblocks)
+        if len(to_merge) > 1:
+            return self._merge_blocks(to_merge)
+        else:
+            unit, block = to_merge[0]
+            return unit.reindex_block(block, self.axis,
+                                      self.result_items, copy=self.copy)
 
-        return left_blockmap, right_blockmap
+    def _merge_blocks(self, merge_chunks):
+        """
+        merge_chunks -> [(_JoinUnit, Block)]
+        """
+        funit, fblock = merge_chunks[0]
+        fidx = funit.indexer
 
-    def _reindex_block(self, block, side='left', copy=True):
-        if side == 'left':
-            indexer = self.lindexer
-            mask, need_masking = self.lmask_info
-        else:
-            indexer = self.rindexer
-            mask, need_masking = self.rmask_info
+        out_shape = list(fblock.values.shape)
 
-        # still some inefficiency here for bool/int64 because in the case where
-        # no masking is needed, take_fast will recompute the mask
+        n = len(fidx) if fidx is not None else out_shape[self.axis]
 
-        if indexer is None and copy:
-            result = block.copy()
-        else:
-            result = block.reindex_axis(indexer, mask, need_masking,
-                                        axis=self.axis)
+        out_shape[0] = sum(len(blk) for unit, blk in merge_chunks)
+        out_shape[self.axis] = n
 
-        result.ref_items = self.result_items
-        return result
+        # Should use Fortran order??
+        out = np.empty(out_shape, dtype=fblock.values.dtype)
 
-    @cache_readonly
-    def lmask_info(self):
-        if (self.lindexer is None or
-            not self._may_need_upcasting(self.left.blocks)):
-            lmask = None
-            lneed_masking = False
-        else:
-            lmask = self.lindexer == -1
-            lneed_masking = lmask.any()
+        sofar = 0
+        for unit, blk in merge_chunks:
+            out_chunk = out[sofar : sofar + len(blk)]
+
+            if unit.indexer is None:
+            # is this really faster than assigning to arr.flat?
+                com.take_fast(blk.values, np.arange(n, dtype='i4'),
+                              None, False,
+                              axis=self.axis, out=out_chunk)
+            else:
+                # write out the values to the result array
+                com.take_fast(blk.values, unit.indexer,
+                              None, False,
+                              axis=self.axis, out=out_chunk)
+
+            sofar += len(blk)
+
+        # does not sort
+        new_block_items = _concat_indexes([b.items for _, b in merge_chunks])
+        return make_block(out, new_block_items, self.result_items)
 
-        return lmask, lneed_masking
+
+
+class _JoinUnit(object):
+    """
+    Blocks plus indexer
+    """
+
+    def __init__(self, blocks, indexer):
+        self.blocks = blocks
+        self.indexer = indexer
 
     @cache_readonly
-    def rmask_info(self):
-        if (self.rindexer is None or
-            not self._may_need_upcasting(self.right.blocks)):
-            rmask = None
-            rneed_masking = False
+    def mask_info(self):
+        if self.indexer is None or not _may_need_upcasting(self.blocks):
+            mask = None
+            need_masking = False
         else:
-            rmask = self.rindexer == -1
-            rneed_masking = rmask.any()
+            mask = self.indexer == -1
+            need_masking = mask.any()
 
-        return rmask, rneed_masking
+        return mask, need_masking
 
     @property
-    def lneed_masking(self):
-        return self.lmask_info[1]
+    def need_masking(self):
+        return self.mask_info[1]
 
-    @property
-    def rneed_masking(self):
-        return self.rmask_info[1]
-
-    @staticmethod
-    def _may_need_upcasting(blocks):
-        for block in blocks:
-            if isinstance(block, (IntBlock, BoolBlock)):
-                return True
-        return False
-
-    def _merge_blocks(self, lblk, rblk):
-        lidx = self.lindexer
-        ridx = self.rindexer
-
-        n = lblk.values.shape[self.axis] if lidx is None else len(lidx)
-        lk = len(lblk.items)
-        rk = len(rblk.items)
-
-        out_shape = list(lblk.shape)
-        out_shape[0] = lk + rk
-        out_shape[self.axis] = n
+    def get_upcasted_blocks(self):
+        # will short-circuit and not compute lneed_masking if indexer is None
+        if self.need_masking:
+            return _upcast_blocks(self.blocks)
+        return self.blocks
 
-        out = np.empty(out_shape, dtype=lblk.values.dtype)
+    def reindex_block(self, block, axis, ref_items, copy=True):
+        # still some inefficiency here for bool/int64 because in the case where
+        # no masking is needed, take_fast will recompute the mask
 
-        # is this really faster than assigning to arr.flat?
-        if lidx is None:
-            # out[:lk] = lblk.values
-            com.take_fast(lblk.values, np.arange(n, dtype='i4'),
-                          None, False,
-                          axis=self.axis, out=out[:lk])
-        else:
-            # write out the values to the result array
-            com.take_fast(lblk.values, lidx, None, False,
-                             axis=self.axis, out=out[:lk])
-        if ridx is None:
-            # out[lk:] = lblk.values
-            com.take_fast(rblk.values, np.arange(n, dtype='i4'),
-                          None, False,
-                          axis=self.axis, out=out[lk:])
+        mask, need_masking = self.mask_info
+
+        if self.indexer is None:
+            if copy:
+                result = block.copy()
+            else:
+                result = block
         else:
-            com.take_fast(rblk.values, ridx, None, False,
-                          axis=self.axis, out=out[lk:])
+            result = block.reindex_axis(self.indexer, mask, need_masking,
+                                        axis=axis)
 
-        # does not sort
-        new_items = lblk.items.append(rblk.items)
-        return make_block(out, new_items, self.result_items)
+        result.ref_items = ref_items
+        return result
 
-    @staticmethod
-    def _upcast_blocks(blocks):
-        """
-        Upcast and consolidate if necessary
-        """
-        # if not need_masking:
-        #     return blocks
-
-        new_blocks = []
-        for block in blocks:
-            if isinstance(block, IntBlock):
-                newb = make_block(block.values.astype(float), block.items,
-                                  block.ref_items)
-            elif isinstance(block, BoolBlock):
-                newb = make_block(block.values.astype(object), block.items,
-                                  block.ref_items)
-            else:
-                newb = block
-            new_blocks.append(newb)
+def _may_need_upcasting(blocks):
+    for block in blocks:
+        if isinstance(block, (IntBlock, BoolBlock)):
+            return True
+    return False
 
-        # use any ref_items
-        return _consolidate(new_blocks, newb.ref_items)
 
+def _upcast_blocks(blocks):
+    """
+    Upcast and consolidate if necessary
+    """
+    new_blocks = []
+    for block in blocks:
+        if isinstance(block, IntBlock):
+            newb = make_block(block.values.astype(float), block.items,
+                              block.ref_items)
+        elif isinstance(block, BoolBlock):
+            newb = make_block(block.values.astype(object), block.items,
+                              block.ref_items)
+        else:
+            newb = block
+        new_blocks.append(newb)
+
+    # use any ref_items
+    return _consolidate(new_blocks, newb.ref_items)
+
+def _get_all_block_kinds(blockmaps):
+    kinds = set()
+    for mapping in blockmaps:
+        kinds |= set(mapping)
+    return kinds
 
 #----------------------------------------------------------------------
 # Concatenate DataFrame objects
 
-def concat(frames, axis=0, join='outer', join_index=None):
+def concat(frames, axis=0, join='outer', join_index=None,
+           ignore_index=False, verify_integrity=False):
     """
     Concatenate DataFrame objects row or column wise
 
     Parameters
     ----------
-    frames : list
+    frames : list of DataFrame objects
     axis : {0, 1}, default 0
+        The axis to concatenate along
     join : {'inner', 'outer'}, default 'outer'
         How to handle indexes on other axis
     join_index : index-like
+    verify_integrity : boolean, default False
 
     Returns
     -------
     concatenated : DataFrame
     """
-    return _concat_frames(frames, join_index=join_index, axis=axis)
+    op = Concatenator(frames, axis=axis, join_index=join_index,
+                      ignore_index=ignore_index,
+                      verify_integrity=verify_integrity)
+    return op.get_result()
 
-def _concat_frames(frames, join_index=None, axis=0):
-    if len(frames) == 1:
-        return frames[0]
 
-    if axis == 0:
-        new_index = _concat_indexes([x.index for x in frames])
-        if join_index is None:
-            new_columns = frames[0].columns
+class Concatenator(object):
+    """
+
+    """
+
+    def __init__(self, frames, axis=0, join='outer', join_index=None,
+                 ignore_index=False, verify_integrity=False):
+
+        # consolidate data
+        for frame in frames:
+            frame.consolidate(inplace=True)
+
+        self.frames = frames
+        self.axis = axis
+        self.join = join
+        self.join_index = join_index
+
+        self.ignore_index = ignore_index
+
+        self.verify_integrity = verify_integrity
+
+        self.new_index, self.new_columns = self._get_new_axes()
+
+    def get_result(self):
+        if len(self.frames) == 1:
+            return self.frames[0]
+
+        new_data = self._get_concatenated_data()
+        new_index, new_columns = self._get_new_axes()
+        constructor = self._get_frame_constructor()
+
+        return constructor(new_data, index=new_index, columns=new_columns)
+
+    def _get_concatenated_data(self):
+        try:
+            blockmaps = []
+            for frame in self.frames:
+                type_map = dict((type(blk), blk) for blk in frame._data.blocks)
+                blockmaps.append(type_map)
+            kinds = _get_all_block_kinds(blockmaps)
+
+            new_blocks = []
+            for kind in kinds:
+                klass_blocks = [mapping.get(kind) for mapping in blockmaps]
+                stacked_block = self._concat_blocks(klass_blocks)
+                new_blocks.append(stacked_block)
+            new_axes = [self.new_columns, self.new_index]
+            new_data = BlockManager(new_blocks, new_axes)
+        except Exception:  # EAFP
+            # should not be possible to fail here for the expected reason with
+            # axis=1
+            if self.axis == 1:
+                raise
+
+            new_data = {}
+            for column in self.new_columns:
+                new_data[column] = self._concat_single_column(column)
+
+        return new_data
+
+    def _concat_blocks(self, blocks):
+        cat_axis = 0 if self.axis == 1 else 1
+        concat_values = np.concatenate([b.values for b in blocks],
+                                       axis=cat_axis)
+        if self.axis == 0:
+            # Not safe to remove this check, need to profile
+            if not _all_indexes_same([b.items for b in blocks]):
+                raise Exception('dtypes are not consistent throughout '
+                                'DataFrames')
+            return make_block(concat_values, blocks[0].items, self.new_columns)
         else:
-            new_columns = join_index
-    else:
-        new_columns = _concat_indexes([x.columns for x in frames])
-        new_index = join_index
+            concat_items = _concat_indexes([b.items for b in blocks])
+            # TODO: maybe want to "take" from the new columns?
+            return make_block(concat_values, concat_items, self.new_columns)
+
+    def _concat_single_column(self, col):
+        all_values = []
+        dtypes = set()
+        for frame in self.frames:
+            if len(frame) == 0:
+                continue
+            try:
+                values = frame._get_raw_column(col)
+                dtypes.add(values.dtype)
+                all_values.append(values)
+            except KeyError:
+                all_values.append(None)
+
+        # this stinks
+        have_object = False
+        for dtype in dtypes:
+            if issubclass(dtype.type, (np.object_, np.bool_)):
+                have_object = True
+        if have_object:
+            empty_dtype = np.object_
+        else:
+            empty_dtype = np.float64
+
+        to_concat = []
+        for df, col_values in zip(self.frames, all_values):
+            if col_values is None:
+                missing_arr = np.empty(len(df), dtype=empty_dtype)
+                missing_arr.fill(np.nan)
+                to_concat.append(missing_arr)
+            else:
+                to_concat.append(col_values)
 
-    if frames[0]._is_mixed_type:
-        new_data = {}
-        for col in new_columns:
-            new_data[col] = np.concatenate([x[col].values for x in frames])
-        return DataFrame(new_data, index=new_index, columns=new_columns)
-    else:
-        new_values = np.concatenate([x.values for x in frames], axis=axis)
-        return DataFrame(new_values, index=new_index, columns=new_columns)
+        return np.concatenate(to_concat)
 
-def _concat_indexes(indexes):
-    return indexes[0].append(indexes[1:])
+    def _get_new_axes(self):
+        if self.axis == 0:
+            if self.ignore_index:
+                new_index = None
+            else:
+                new_index = _concat_indexes([x.index for x in self.frames])
+                self._maybe_check_integrity(new_index)
+
+            if self.join_index is None:
+                all_cols = [df.columns for df in self.frames]
+                new_columns = _get_combined_index(all_cols, intersect=False)
+            else:
+                new_columns = self.join_index
+        else:
+            new_columns = _concat_indexes([df.columns for df in self.frames])
+            self._maybe_check_integrity(new_columns)
+
+            if self.verify_integrity:
+                if not new_columns._verify_integrity():
+                    raise Exception('Indexes overlap!')
+
+            if self.join_index is None:
+                all_indexes = [df.index for df in self.frames]
+                new_index = _get_combined_index(all_indexes, intersect=False)
+            else:
+                new_index = self.join_index
+
+        return new_index, new_columns
+
+    def _get_frame_constructor(self):
+        # SparseDataFrame causes us some headache here
+
+        # check that there's only one type present
+        frame_types = set(type(df) for df in self.frames)
+        if len(frame_types) > 1:
+            raise Exception('Can only concatenate like-typed objects, found %s'
+                            % frame_types)
+
+        return self.frames[0]._constructor
+
+    def _maybe_check_integrity(self, concat_index):
+        if self.verify_integrity:
+            if not concat_index._verify_integrity():
+                overlap = concat_index.get_duplicates()
+                raise Exception('Indexes have overlapping values: %s'
+                                % str(overlap))
+
+    @cache_readonly
+    def _all_indexes_same(self):
+        return _all_indexes_same([df.columns for df in self.frames])
 
 def _concat_frames_hierarchical(frames, keys, groupings, axis=0):
     names = [ping.name for ping in groupings]
@@ -646,6 +781,9 @@ def _concat_frames_hierarchical(frames, keys, groupings, axis=0):
         new_values = np.concatenate([x.values for x in frames], axis=axis)
         return DataFrame(new_values, index=new_index, columns=new_columns)
 
+def _concat_indexes(indexes):
+    return indexes[0].append(indexes[1:])
+
 def _make_concat_multiindex(indexes, keys, levels, names):
     single_level = len(levels) == 1
 
@@ -716,3 +854,11 @@ def _all_indexes_same(indexes):
             return False
     return True
 
+
+class _SparseMockBlockManager(object):
+
+    def __init__(self, sp_frame):
+        self.sp_frame = sp_frame
+
+    def get(self, item):
+        return self.sp_frame[item].values
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
index 2375e20da..5362d36f6 100644
--- a/pandas/tools/tests/test_merge.py
+++ b/pandas/tools/tests/test_merge.py
@@ -599,6 +599,87 @@ def _join_by_hand(a, b, how='left'):
         a_re[col] = s
     return a_re.reindex(columns=result_columns)
 
+class TestConcatenate(unittest.TestCase):
+
+    def setUp(self):
+        self.frame = DataFrame(tm.getSeriesData())
+        self.mixed_frame = self.frame.copy()
+        self.mixed_frame['foo'] = 'bar'
+
+    def test_append(self):
+        begin_index = self.frame.index[:5]
+        end_index = self.frame.index[5:]
+
+        begin_frame = self.frame.reindex(begin_index)
+        end_frame = self.frame.reindex(end_index)
+
+        appended = begin_frame.append(end_frame)
+        assert_almost_equal(appended['A'], self.frame['A'])
+
+        del end_frame['A']
+        partial_appended = begin_frame.append(end_frame)
+        self.assert_('A' in partial_appended)
+
+        partial_appended = end_frame.append(begin_frame)
+        self.assert_('A' in partial_appended)
+
+        # mixed type handling
+        appended = self.mixed_frame[:5].append(self.mixed_frame[5:])
+        assert_frame_equal(appended, self.mixed_frame)
+
+        # what to test here
+        mixed_appended = self.mixed_frame[:5].append(self.frame[5:])
+        mixed_appended2 = self.frame[:5].append(self.mixed_frame[5:])
+
+        # all equal except 'foo' column
+        assert_frame_equal(mixed_appended.reindex(columns=['A', 'B', 'C', 'D']),
+                           mixed_appended2.reindex(columns=['A', 'B', 'C', 'D']))
+
+        # append empty
+        empty = DataFrame({})
+
+        appended = self.frame.append(empty)
+        assert_frame_equal(self.frame, appended)
+        self.assert_(appended is not self.frame)
+
+        appended = empty.append(self.frame)
+        assert_frame_equal(self.frame, appended)
+        self.assert_(appended is not self.frame)
+
+        # overlap
+        self.assertRaises(Exception, self.frame.append, self.frame)
+
+    def test_append_records(self):
+        arr1 = np.zeros((2,),dtype=('i4,f4,a10'))
+        arr1[:] = [(1,2.,'Hello'),(2,3.,"World")]
+
+        arr2 = np.zeros((3,),dtype=('i4,f4,a10'))
+        arr2[:] = [(3, 4.,'foo'),
+                   (5, 6.,"bar"),
+                   (7., 8., 'baz')]
+
+        df1 = DataFrame(arr1)
+        df2 = DataFrame(arr2)
+
+        result = df1.append(df2, ignore_index=True)
+        expected = DataFrame(np.concatenate((arr1, arr2)))
+        assert_frame_equal(result, expected)
+
+    def test_append_different_columns(self):
+        df = DataFrame({'bools' : np.random.randn(10) > 0,
+                        'ints' : np.random.randint(0, 10, 10),
+                        'floats' : np.random.randn(10),
+                        'strings' : ['foo', 'bar'] * 5})
+
+        a = df[:5].ix[:, ['bools', 'ints', 'floats']]
+        b = df[5:].ix[:, ['strings', 'ints', 'floats']]
+
+        appended = a.append(b)
+        self.assert_(isnull(appended['strings'][:5]).all())
+        self.assert_(isnull(appended['bools'][5:]).all())
+
+    def test_append_missing_column_proper_upcast(self):
+        pass
 
 if __name__ == '__main__':
     import nose
diff --git a/vb_suite/join_merge.py b/vb_suite/join_merge.py
index f9dc8ff10..ad8629717 100644
--- a/vb_suite/join_merge.py
+++ b/vb_suite/join_merge.py
@@ -67,6 +67,35 @@ join_dataframe_index_multi = \
 #----------------------------------------------------------------------
 # Merges
 
+#----------------------------------------------------------------------
+# Appending DataFrames
+
+setup = common_setup + """
+df1 = DataFrame(np.random.randn(10000, 4), columns=['A', 'B', 'C', 'D'])
+df2 = df1.copy()
+df2.index = np.arange(10000, 20000)
+mdf1 = df1.copy()
+mdf1['obj1'] = 'bar'
+mdf1['obj2'] = 'bar'
+mdf1['int1'] = 5
+try:
+    mdf1.consolidate(inplace=True)
+except:
+    pass
+mdf2 = mdf1.copy()
+mdf2.index = df2.index
+"""
+
+stmt = "df1.append(df2)"
+append_frame_single_homogenous = \
+    Benchmark(stmt, setup, name='append_frame_single_homogenous',
+              ncalls=500, repeat=1)
+
+stmt = "mdf1.append(mdf2)"
+append_frame_single_mixed = Benchmark(stmt, setup,
+                                      name='append_frame_single_mixed',
+                                      ncalls=500, repeat=1)
+
 #----------------------------------------------------------------------
 # data alignment
 
@@ -94,3 +123,4 @@ series_align_left_monotonic = \
     Benchmark(stmt, setup,
               name="series_align_left_monotonic",
               start_date=datetime(2011, 3, 1), logy=True)
+
diff --git a/vb_suite/panel_ctor.py b/vb_suite/panel_ctor.py
index 1a468847c..f68d8f9b5 100644
--- a/vb_suite/panel_ctor.py
+++ b/vb_suite/panel_ctor.py
@@ -11,7 +11,7 @@ START_DATE = datetime(2011, 6, 1)
 
 setup_same_index = common_setup + """
 # create 1000 dataframes with the same index
-dr = DateRange(datetime(1990,1,1), datetime(2012,1,1))
+dr = DateRange(datetime(1990,1,1), datetime(2012,1,1)).values
 data_frames = {}
 for x in xrange(1000):
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
@@ -27,7 +27,7 @@ panel_from_dict_same_index = \
 setup_equiv_indexes = common_setup + """
 data_frames = {}
 for x in xrange(1000):
-   dr = DateRange(datetime(1990,1,1), datetime(2012,1,1))
+   dr = DateRange(datetime(1990,1,1), datetime(2012,1,1)).values
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
@@ -44,7 +44,7 @@ start = datetime(1990,1,1)
 end = datetime(2012,1,1)
 for x in xrange(1000):
    end += timedelta(days=1)
-   dr = DateRange(start, end)
+   dr = DateRange(start, end).values
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
@@ -62,7 +62,7 @@ end = datetime(2012,1,1)
 for x in xrange(1000):
    if x == 500:
        end += timedelta(days=1)
-   dr = DateRange(start, end)
+   dr = DateRange(start, end).values
    df = DataFrame({"a": [0]*len(dr), "b": [1]*len(dr),
                    "c": [2]*len(dr)}, index=dr)
    data_frames[x] = df
