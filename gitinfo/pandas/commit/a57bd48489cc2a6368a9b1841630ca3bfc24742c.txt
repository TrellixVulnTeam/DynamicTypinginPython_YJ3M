commit a57bd48489cc2a6368a9b1841630ca3bfc24742c
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Thu Dec 15 19:27:54 2011 -0500

    ENH: refactored Cython groupby code to not have to sort, GH #93

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 3f4019d8c..3f173b88d 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -7,7 +7,6 @@ from pandas.core.frame import DataFrame
 from pandas.core.generic import NDFrame, PandasObject
 from pandas.core.index import Index, MultiIndex
 from pandas.core.internals import BlockManager
-from pandas.core.reshape import get_group_index
 from pandas.core.series import Series
 from pandas.core.panel import Panel
 from pandas.util.decorators import cache_readonly
@@ -316,6 +315,14 @@ class GroupBy(object):
         """
         return self._cython_agg_general('mean')
 
+    def std(self):
+        """
+        Compute mean of groups, excluding missing values
+
+        For multiple groupings, the result index will be a MultiIndex
+        """
+        return self._cython_agg_general('std')
+
     def size(self):
         """
         Compute group sizes
@@ -356,8 +363,8 @@ class GroupBy(object):
             else:
                 continue
 
-            result, counts =  lib.group_aggregate(obj, label_list,
-                                                  shape, how=how)
+            result, counts =  cython_aggregate(obj, label_list,
+                                               shape, how=how)
             result = result.ravel()
             mask = counts.ravel() > 0
             output[name] = result[mask]
@@ -1315,15 +1322,7 @@ def generate_groups(data, label_list, shape, axis=0, factory=lambda x: x):
     -------
     generator
     """
-    # indexer = np.lexsort(label_list[::-1])
-    group_index = get_group_index(label_list, shape)
-    na_mask = np.zeros(len(label_list[0]), dtype=bool)
-    for arr in label_list:
-        na_mask |= arr == -1
-    group_index[na_mask] = -1
-    indexer = lib.groupsort_indexer(group_index.astype('i4'),
-                                    np.prod(shape))
-
+    indexer = _get_group_sorter(label_list, shape)
     sorted_labels = [labels.take(indexer) for labels in label_list]
 
     if isinstance(data, BlockManager):
@@ -1342,6 +1341,17 @@ def generate_groups(data, label_list, shape, axis=0, factory=lambda x: x):
     for key, group in gen:
         yield key, group
 
+def _get_group_sorter(label_list, shape):
+    group_index = get_group_index(label_list, shape)
+    na_mask = np.zeros(len(label_list[0]), dtype=bool)
+    for arr in label_list:
+        na_mask |= arr == -1
+    group_index[na_mask] = -1
+    indexer = lib.groupsort_indexer(group_index.astype('i4'),
+                                    np.prod(shape))
+
+    return indexer
+
 def _generate_groups(data, labels, shape, start, end, axis=0, which=0,
                      factory=lambda x: x):
     axis_labels = labels[which][start:end]
@@ -1385,6 +1395,50 @@ def _generate_groups(data, labels, shape, start, end, axis=0, which=0,
 
         left = right
 
+def get_group_index(label_list, shape):
+    n = len(label_list[0])
+    group_index = np.zeros(n, dtype=int)
+    mask = np.zeros(n, dtype=bool)
+    for i in xrange(len(shape)):
+        stride = np.prod([x for x in shape[i+1:]], dtype=int)
+        group_index += label_list[i] * stride
+        mask |= label_list[i] < 0
+
+    np.putmask(group_index, mask, -1)
+    return group_index
+
+#----------------------------------------------------------------------
+# Group aggregations in Cython
+
+
+def cython_aggregate(values, label_list, shape, how='add'):
+    agg_func = _cython_functions[how]
+    trans_func = _cython_transforms.get(how, lambda x: x)
+
+    group_index = get_group_index(label_list, shape).astype('i4')
+
+    result = np.empty(shape, dtype=np.float64)
+    result.fill(np.nan)
+
+    counts = np.zeros(shape, dtype=np.int32)
+    agg_func(result.ravel(), counts.ravel(), values,
+             group_index)
+
+    result = trans_func(result)
+
+    return result, counts
+
+_cython_functions = {
+    'add' : lib.group_add,
+    'mean' : lib.group_mean,
+    'var' : lib.group_var,
+    'std' : lib.group_var
+}
+
+_cython_transforms = {
+    'std' : np.sqrt
+}
+
 #----------------------------------------------------------------------
 # sorting levels...cleverly?
 
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 4897c08bd..ab6e2993d 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -9,6 +9,7 @@ from pandas.core.frame import DataFrame
 from pandas.core.series import Series
 
 from pandas.core.common import notnull
+from pandas.core.groupby import get_group_index
 from pandas.core.index import MultiIndex
 
 
@@ -197,13 +198,6 @@ class _Unstacker(object):
 
         return new_index
 
-def get_group_index(label_list, shape):
-    group_index = np.zeros(len(label_list[0]), dtype=int)
-    for i in xrange(len(shape)):
-        stride = np.prod([x for x in shape[i+1:]], dtype=int)
-        group_index += label_list[i] * stride
-    return group_index
-
 def pivot(self, index=None, columns=None, values=None):
     """
     See DataFrame.pivot
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index d33c77d4b..a818f6d0f 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -281,42 +281,6 @@ def get_unique_labels(ndarray[object] values, dict idMap):
 #             uniques.append(val)
 #     return np.asarray(sorted(uniques), dtype=object)
 
-ctypedef double_t (* agg_func)(double_t *out, int32_t *counts, double_t *values,
-                               int32_t *labels, int start, int end,
-                               Py_ssize_t offset)
-
-cdef agg_func get_agg_func(object how):
-    if how == 'add':
-        return _group_add
-    elif how == 'mean':
-        return _group_mean
-
-@cython.boundscheck(False)
-def group_aggregate(ndarray[double_t] values, list label_list,
-                    object shape, how='add'):
-    cdef:
-        list sorted_labels
-        ndarray result, counts
-        agg_func func
-
-    func = get_agg_func(how)
-
-    values, sorted_labels = _group_reorder(values, label_list, shape)
-    result = np.empty(shape, dtype=np.float64)
-    result.fill(nan)
-
-    counts = np.zeros(shape, dtype=np.int32)
-
-    if not values.flags.c_contiguous:
-        values = values.copy()
-
-    _aggregate_group(<double_t*> result.data,
-                     <int32_t*> counts.data,
-                     <double_t*> values.data,
-                     sorted_labels, 0, len(values),
-                     shape, 0, 0, func)
-
-    return result, counts
 
 def _group_reorder(values, label_list, shape):
     # group_index = np.zeros(len(label_list[0]), dtype='i4')
@@ -362,104 +326,139 @@ def groupsort_indexer(ndarray[int32_t] index, Py_ssize_t ngroups):
     return result
 
 
-cdef int _aggregate_group(double_t *out, int32_t *counts, double_t *values,
-                           list labels, int start, int end, tuple shape,
-                           Py_ssize_t which, Py_ssize_t offset,
-                           agg_func func) except -1:
-    cdef:
-        ndarray[int32_t] axis
-        cdef Py_ssize_t stride
-
-    # time to actually aggregate
-    if which == len(labels) - 1:
-        axis = labels[which]
-
-        while start < end and axis[start] == -1:
-            start += 1
-        func(out, counts, values, <int32_t*> axis.data, start, end, offset)
-    else:
-        axis = labels[which][start:end]
-        stride = np.prod(shape[which+1:])
-        # get group counts on axisp
-        edges = axis.searchsorted(np.arange(1, shape[which] + 1), side='left')
-        # print edges, axis
-
-        left = axis.searchsorted(0) # ignore NA values coded as -1
-
-        # aggregate each subgroup
-        for right in edges:
-            _aggregate_group(out, counts, values, labels, start + left,
-                             start + right, shape, which + 1, offset, func)
-            offset += stride
-            left = right
+# cdef int _aggregate_group(float64_t *out, int32_t *counts, float64_t *values,
+#                            list labels, int start, int end, tuple shape,
+#                            Py_ssize_t which, Py_ssize_t offset,
+#                            agg_func func) except -1:
+#     cdef:
+#         ndarray[int32_t] axis
+#         cdef Py_ssize_t stride
+
+#     # time to actually aggregate
+#     if which == len(labels) - 1:
+#         axis = labels[which]
+
+#         while start < end and axis[start] == -1:
+#             start += 1
+#         func(out, counts, values, <int32_t*> axis.data, start, end, offset)
+#     else:
+#         axis = labels[which][start:end]
+#         stride = np.prod(shape[which+1:])
+#         # get group counts on axisp
+#         edges = axis.searchsorted(np.arange(1, shape[which] + 1), side='left')
+#         # print edges, axis
+
+#         left = axis.searchsorted(0) # ignore NA values coded as -1
+
+#         # aggregate each subgroup
+#         for right in edges:
+#             _aggregate_group(out, counts, values, labels, start + left,
+#                              start + right, shape, which + 1, offset, func)
+#             offset += stride
+#             left = right
 
 # TODO: aggregate multiple columns in single pass
 
-cdef double_t _group_add(double_t *out, int32_t *counts, double_t *values,
-                         int32_t *labels, int start, int end,
-                         Py_ssize_t offset):
+@cython.wraparound(False)
+def group_add(ndarray[float64_t] out,
+              ndarray[int32_t] counts,
+              ndarray[float64_t] values,
+              ndarray[int32_t] labels):
     cdef:
-        Py_ssize_t i, it = start
-        int32_t lab
-        int32_t count = 0, tot = 0
-        double_t val, cum = 0
+        Py_ssize_t i, lab
+        float64_t val, count
+        ndarray[float64_t] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
 
-    while it < end:
-        i = labels[it]
-        val = values[it]
-        tot += 1
+    for i in range(len(values)):
+        lab = labels[i]
+        if lab < 0:
+            continue
+
+        counts[lab] += 1
+        val = values[i]
 
         # not nan
         if val == val:
-            count += 1
-            cum += val
-
-        if it == end - 1 or labels[it + 1] > i:
-            if count == 0:
-                out[offset + i] = nan
-            else:
-                out[offset + i] = cum
+            nobs[lab] += 1
+            sumx[lab] += val
 
-            counts[offset + i] = tot
+    for i in range(len(counts)):
+        if nobs[i] == 0:
+            out[i] = nan
+        else:
+            out[i] = sumx[i]
 
-            count = 0
-            cum = 0
-            tot = 0
+@cython.wraparound(False)
+def group_mean(ndarray[float64_t] out,
+               ndarray[int32_t] counts,
+               ndarray[float64_t] values,
+               ndarray[int32_t] labels):
+    cdef:
+        Py_ssize_t i, lab
+        float64_t val, count
+        ndarray[float64_t] sumx, nobs
 
-        it += 1
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
 
-cdef double_t _group_mean(double_t *out, int32_t *counts, double_t *values,
-                          int32_t *labels, int start, int end,
-                          Py_ssize_t offset):
-    cdef:
-        Py_ssize_t i, it = start
-        int32_t lab
-        int32_t count = 0, tot = 0
-        double_t val, cum = 0
+    for i in range(len(values)):
+        lab = labels[i]
+        if lab < 0:
+            continue
 
-    while it < end:
-        i = labels[it]
-        val = values[it]
-        tot += 1
+        val = values[i]
+        counts[lab] += 1
 
         # not nan
         if val == val:
-            count += 1
-            cum += val
+            nobs[lab] += 1
+            sumx[lab] += val
 
-        if it == end - 1 or labels[it + 1] > i:
-            if count == 0:
-                out[offset + i] = nan
-            else:
-                out[offset + i] = cum / count
+    for i in range(len(counts)):
+        count = nobs[i]
+        if count == 0:
+            out[i] = nan
+        else:
+            out[i] = sumx[i] / count
+
+@cython.wraparound(False)
+def group_var(ndarray[float64_t] out,
+              ndarray[int32_t] counts,
+              ndarray[float64_t] values,
+              ndarray[int32_t] labels):
+    cdef:
+        Py_ssize_t i, lab
+        float64_t val, ct
+        ndarray[float64_t] nobs, sumx, sumxx
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+    sumxx = np.zeros_like(out)
 
-            counts[offset + i] = tot
+    for i in range(len(values)):
+        lab = <Py_ssize_t> labels[i]
+        if lab < 0:
+            continue
 
-            count = 0
-            cum = 0
-            tot = 0
+        val = values[i]
+        counts[lab] += 1
 
-        it += 1
+        # not nan
+        if val == val:
+            nobs[lab] += 1
+            sumx[lab] += val
+            sumxx[lab] += val * val
+
+    for i in range(len(counts)):
+        ct = nobs[i]
+        if ct < 2:
+            out[i] = nan
+        else:
+            out[i] = ((ct * sumxx[i] - sumx[i] * sumx[i]) /
+                      (ct * ct - ct))
 
 def _result_shape(label_list):
     # assumed sorted
@@ -470,12 +469,12 @@ def _result_shape(label_list):
 
 def reduce_mean(ndarray[object] indices,
                 ndarray[object] buckets,
-                ndarray[double_t] values,
+                ndarray[float64_t] values,
                 inclusive=False):
     cdef:
         Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t] output
-        double_t the_sum, val, nobs
+        ndarray[float64_t] output
+        float64_t the_sum, val, nobs
 
 
 
@@ -589,13 +588,13 @@ def duplicated(list values, take_last=False):
 
 def ts_upsample_mean(ndarray[object] indices,
                      ndarray[object] buckets,
-                     ndarray[double_t] values,
+                     ndarray[float64_t] values,
                      inclusive=False):
     cdef:
         Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t] output
+        ndarray[float64_t] output
         object next_bound
-        double_t the_sum, val, nobs
+        float64_t the_sum, val, nobs
 
     nbuckets = len(buckets)
     nvalues = len(indices)
@@ -623,9 +622,9 @@ def ts_upsample_mean(ndarray[object] indices,
 
     cdef:
         Py_ssize_t i, j, nbuckets, nvalues
-        ndarray[double_t] output
+        ndarray[float64_t] output
         object next_bound
-        double_t the_sum, val, nobs
+        float64_t the_sum, val, nobs
 
     nbuckets = len(buckets)
     nvalues = len(indices)
@@ -682,7 +681,7 @@ def ts_upsample_mean(ndarray[object] indices,
 
 def ts_upsample_generic(ndarray[object] indices,
                         ndarray[object] buckets,
-                        ndarray[double_t] values,
+                        ndarray[float64_t] values,
                         object aggfunc,
                         inclusive=False):
     '''
@@ -690,9 +689,9 @@ def ts_upsample_generic(ndarray[object] indices,
     '''
     cdef:
         Py_ssize_t i, j, jstart, nbuckets, nvalues
-        ndarray[double_t] output
+        ndarray[float64_t] output
         object next_bound
-        double_t the_sum, val, nobs
+        float64_t the_sum, val, nobs
 
     nbuckets = len(buckets)
     nvalues = len(indices)
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 382607cd1..c815a4c48 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -989,13 +989,13 @@ class TestGroupBy(unittest.TestCase):
         assert_frame_equal(agg_result, expected)
         assert_frame_equal(apply_result, expected)
 
-    def test_cython_na_bug(self):
-        values = np.random.randn(10)
-        shape = (5, 5)
-        label_list = [np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2], dtype=np.int32),
-                      np.array([1, 2, 3, 4, 0, 1, 2, 3, 3, 4], dtype=np.int32)]
+    # def test_cython_na_bug(self):
+    #     values = np.random.randn(10)
+    #     shape = (5, 5)
+    #     label_list = [np.array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2], dtype=np.int32),
+    #                   np.array([1, 2, 3, 4, 0, 1, 2, 3, 3, 4], dtype=np.int32)]
 
-        lib.group_aggregate(values, label_list, shape)
+    #     lib.group_aggregate(values, label_list, shape)
 
     def test_size(self):
         grouped = self.df.groupby(['A', 'B'])
diff --git a/setup.py b/setup.py
index ce9f0fec5..b75a41647 100755
--- a/setup.py
+++ b/setup.py
@@ -294,8 +294,6 @@ if suffix == '.pyx':
 else:
     tseries_depends = []
 
-print tseries_depends
-
 tseries_ext = Extension('pandas._tseries',
                         depends=tseries_depends + ['pandas/src/numpy_helper.h'],
                         sources=[srcpath('tseries', suffix=suffix)],
