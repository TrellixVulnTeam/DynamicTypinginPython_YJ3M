commit 221c8a7a623bfabdeccfb62212e091ed37b4af5e
Author: jbrockmendel <jbrockmendel@gmail.com>
Date:   Sat Nov 2 08:16:17 2019 -0700

    CLN: assorted cleanups (#29314)

diff --git a/pandas/_libs/algos.pyx b/pandas/_libs/algos.pyx
index 30c9af645..e3c7fef6f 100644
--- a/pandas/_libs/algos.pyx
+++ b/pandas/_libs/algos.pyx
@@ -379,13 +379,26 @@ ctypedef fused algos_t:
     uint8_t
 
 
+def _validate_limit(nobs: int, limit=None) -> int:
+    if limit is None:
+        lim = nobs
+    else:
+        if not util.is_integer_object(limit):
+            raise ValueError('Limit must be an integer')
+        if limit < 1:
+            raise ValueError('Limit must be greater than 0')
+        lim = limit
+
+    return lim
+
+
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def pad(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
     cdef:
         Py_ssize_t i, j, nleft, nright
         ndarray[int64_t, ndim=1] indexer
-        algos_t cur, next
+        algos_t cur, next_val
         int lim, fill_count = 0
 
     nleft = len(old)
@@ -393,14 +406,7 @@ def pad(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
     indexer = np.empty(nright, dtype=np.int64)
     indexer[:] = -1
 
-    if limit is None:
-        lim = nright
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(nright, limit)
 
     if nleft == 0 or nright == 0 or new[nright - 1] < old[0]:
         return indexer
@@ -426,9 +432,9 @@ def pad(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
                 j += 1
             break
 
-        next = old[i + 1]
+        next_val = old[i + 1]
 
-        while j < nright and cur <= new[j] < next:
+        while j < nright and cur <= new[j] < next_val:
             if new[j] == cur:
                 indexer[j] = i
             elif fill_count < lim:
@@ -438,16 +444,14 @@ def pad(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
 
         fill_count = 0
         i += 1
-        cur = next
+        cur = next_val
 
     return indexer
 
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
-def pad_inplace(algos_t[:] values,
-                const uint8_t[:] mask,
-                limit=None):
+def pad_inplace(algos_t[:] values, const uint8_t[:] mask, limit=None):
     cdef:
         Py_ssize_t i, N
         algos_t val
@@ -459,14 +463,7 @@ def pad_inplace(algos_t[:] values,
     if N == 0:
         return
 
-    if limit is None:
-        lim = N
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(N, limit)
 
     val = values[0]
     for i in range(N):
@@ -482,9 +479,7 @@ def pad_inplace(algos_t[:] values,
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
-def pad_2d_inplace(algos_t[:, :] values,
-                   const uint8_t[:, :] mask,
-                   limit=None):
+def pad_2d_inplace(algos_t[:, :] values, const uint8_t[:, :] mask, limit=None):
     cdef:
         Py_ssize_t i, j, N, K
         algos_t val
@@ -496,14 +491,7 @@ def pad_2d_inplace(algos_t[:, :] values,
     if N == 0:
         return
 
-    if limit is None:
-        lim = N
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(N, limit)
 
     for j in range(K):
         fill_count = 0
@@ -559,14 +547,7 @@ def backfill(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
     indexer = np.empty(nright, dtype=np.int64)
     indexer[:] = -1
 
-    if limit is None:
-        lim = nright
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(nright, limit)
 
     if nleft == 0 or nright == 0 or new[0] > old[nleft - 1]:
         return indexer
@@ -612,9 +593,7 @@ def backfill(ndarray[algos_t] old, ndarray[algos_t] new, limit=None):
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
-def backfill_inplace(algos_t[:] values,
-                     const uint8_t[:] mask,
-                     limit=None):
+def backfill_inplace(algos_t[:] values, const uint8_t[:] mask, limit=None):
     cdef:
         Py_ssize_t i, N
         algos_t val
@@ -626,14 +605,7 @@ def backfill_inplace(algos_t[:] values,
     if N == 0:
         return
 
-    if limit is None:
-        lim = N
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(N, limit)
 
     val = values[N - 1]
     for i in range(N - 1, -1, -1):
@@ -663,14 +635,7 @@ def backfill_2d_inplace(algos_t[:, :] values,
     if N == 0:
         return
 
-    if limit is None:
-        lim = N
-    else:
-        if not util.is_integer_object(limit):
-            raise ValueError('Limit must be an integer')
-        if limit < 1:
-            raise ValueError('Limit must be greater than 0')
-        lim = limit
+    lim = _validate_limit(N, limit)
 
     for j in range(K):
         fill_count = 0
diff --git a/pandas/_libs/algos_common_helper.pxi.in b/pandas/_libs/algos_common_helper.pxi.in
index c3b0a8406..621173470 100644
--- a/pandas/_libs/algos_common_helper.pxi.in
+++ b/pandas/_libs/algos_common_helper.pxi.in
@@ -23,7 +23,7 @@ def diff_2d(ndarray[diff_t, ndim=2] arr,
             ndarray[out_t, ndim=2] out,
             Py_ssize_t periods, int axis):
     cdef:
-        Py_ssize_t i, j, sx, sy
+        Py_ssize_t i, j, sx, sy, start, stop
 
     # Disable for unsupported dtype combinations,
     #  see https://github.com/cython/cython/issues/2646
diff --git a/pandas/_libs/groupby.pxd b/pandas/_libs/groupby.pxd
deleted file mode 100644
index 70ad8a628..000000000
--- a/pandas/_libs/groupby.pxd
+++ /dev/null
@@ -1,6 +0,0 @@
-cdef enum InterpolationEnumType:
-    INTERPOLATION_LINEAR,
-    INTERPOLATION_LOWER,
-    INTERPOLATION_HIGHER,
-    INTERPOLATION_NEAREST,
-    INTERPOLATION_MIDPOINT
diff --git a/pandas/_libs/groupby.pyx b/pandas/_libs/groupby.pyx
index 49a335218..0e5eaa3b7 100644
--- a/pandas/_libs/groupby.pyx
+++ b/pandas/_libs/groupby.pyx
@@ -27,6 +27,13 @@ _int64_max = np.iinfo(np.int64).max
 
 cdef float64_t NaN = <float64_t>np.NaN
 
+cdef enum InterpolationEnumType:
+    INTERPOLATION_LINEAR,
+    INTERPOLATION_LOWER,
+    INTERPOLATION_HIGHER,
+    INTERPOLATION_NEAREST,
+    INTERPOLATION_MIDPOINT
+
 
 cdef inline float64_t median_linear(float64_t* a, int n) nogil:
     cdef:
diff --git a/pandas/_libs/lib.pyx b/pandas/_libs/lib.pyx
index eddc0beae..328b67b67 100644
--- a/pandas/_libs/lib.pyx
+++ b/pandas/_libs/lib.pyx
@@ -698,8 +698,7 @@ def generate_bins_dt64(ndarray[int64_t] values, const int64_t[:] binner,
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
-def get_level_sorter(const int64_t[:] label,
-                     const int64_t[:] starts):
+def get_level_sorter(const int64_t[:] label, const int64_t[:] starts):
     """
     argsort for a single level of a multi-index, keeping the order of higher
     levels unchanged. `starts` points to starts of same-key indices w.r.t
@@ -1677,6 +1676,7 @@ cpdef bint is_datetime64_array(ndarray values):
     return validator.validate(values)
 
 
+# TODO: only non-here use is in test
 def is_datetime_with_singletz_array(values: ndarray) -> bool:
     """
     Check values have the same tzinfo attribute.
@@ -1720,6 +1720,7 @@ cdef class AnyTimedeltaValidator(TimedeltaValidator):
         return is_timedelta(value)
 
 
+# TODO: only non-here use is in test
 cpdef bint is_timedelta_or_timedelta64_array(ndarray values):
     """ infer with timedeltas and/or nat/none """
     cdef:
diff --git a/pandas/_libs/parsers.pyx b/pandas/_libs/parsers.pyx
index 3f12ec4c1..8b9842ba0 100644
--- a/pandas/_libs/parsers.pyx
+++ b/pandas/_libs/parsers.pyx
@@ -278,7 +278,7 @@ cdef class TextReader:
         object true_values, false_values
         object handle
         bint na_filter, keep_default_na, verbose, has_usecols, has_mi_columns
-        int64_t parser_start
+        uint64_t parser_start
         list clocks
         char *c_encoding
         kh_str_starts_t *false_set
@@ -710,11 +710,11 @@ cdef class TextReader:
         # header is now a list of lists, so field_count should use header[0]
 
         cdef:
-            Py_ssize_t i, start, field_count, passed_count, unnamed_count  # noqa
+            Py_ssize_t i, start, field_count, passed_count, unnamed_count
             char *word
             object name, old_name
             int status
-            int64_t hr, data_line
+            uint64_t hr, data_line
             char *errors = "strict"
             StringPath path = _string_path(self.c_encoding)
 
@@ -1015,12 +1015,14 @@ cdef class TextReader:
         else:
             end = min(start + rows, self.parser.lines)
 
+        # FIXME: dont leave commented-out
         # # skip footer
         # if footer > 0:
         #     end -= footer
 
         num_cols = -1
-        for i in range(self.parser.lines):
+        # Py_ssize_t cast prevents build warning
+        for i in range(<Py_ssize_t>self.parser.lines):
             num_cols = (num_cols < self.parser.line_fields[i]) * \
                 self.parser.line_fields[i] + \
                 (num_cols >= self.parser.line_fields[i]) * num_cols
diff --git a/pandas/_libs/reduction.pyx b/pandas/_libs/reduction.pyx
index f505c0479..45991293b 100644
--- a/pandas/_libs/reduction.pyx
+++ b/pandas/_libs/reduction.pyx
@@ -45,8 +45,7 @@ cdef class Reducer:
         Py_ssize_t increment, chunksize, nresults
         object arr, dummy, f, labels, typ, ityp, index
 
-    def __init__(self, object arr, object f, axis=1, dummy=None,
-                 labels=None):
+    def __init__(self, object arr, object f, axis=1, dummy=None, labels=None):
         n, k = arr.shape
 
         if axis == 0:
@@ -70,8 +69,9 @@ cdef class Reducer:
         self.dummy, self.typ, self.index, self.ityp = self._check_dummy(
             dummy=dummy)
 
-    def _check_dummy(self, dummy=None):
-        cdef object index=None, typ=None, ityp=None
+    cdef _check_dummy(self, dummy=None):
+        cdef:
+            object index = None, typ = None, ityp = None
 
         if dummy is None:
             dummy = np.empty(self.chunksize, dtype=self.arr.dtype)
@@ -92,8 +92,8 @@ cdef class Reducer:
             if dummy.dtype != self.arr.dtype:
                 raise ValueError('Dummy array must be same dtype')
             if len(dummy) != self.chunksize:
-                raise ValueError('Dummy array must be length %d' %
-                                 self.chunksize)
+                raise ValueError('Dummy array must be length {length}'
+                                 .format(length=self.chunksize))
 
         return dummy, typ, index, ityp
 
@@ -190,7 +190,6 @@ cdef class SeriesBinGrouper:
     """
     cdef:
         Py_ssize_t nresults, ngroups
-        bint passed_dummy
 
     cdef public:
         object arr, index, dummy_arr, dummy_index
@@ -199,6 +198,8 @@ cdef class SeriesBinGrouper:
     def __init__(self, object series, object f, object bins, object dummy):
         n = len(series)
 
+        assert dummy is not None  # always obj[:0]
+
         self.bins = bins
         self.f = f
 
@@ -213,7 +214,6 @@ cdef class SeriesBinGrouper:
         self.name = getattr(series, 'name', None)
 
         self.dummy_arr, self.dummy_index = self._check_dummy(dummy)
-        self.passed_dummy = dummy is not None
 
         # kludge for #1688
         if len(bins) > 0 and bins[-1] == len(series):
@@ -221,22 +221,18 @@ cdef class SeriesBinGrouper:
         else:
             self.ngroups = len(bins) + 1
 
-    def _check_dummy(self, dummy=None):
+    cdef _check_dummy(self, dummy):
         # both values and index must be an ndarray!
 
-        if dummy is None:
-            values = np.empty(0, dtype=self.arr.dtype)
-            index = None
-        else:
-            values = dummy.values
-            if values.dtype != self.arr.dtype:
-                raise ValueError('Dummy array must be same dtype')
-            if util.is_array(values) and not values.flags.contiguous:
-                # e.g. Categorical has no `flags` attribute
-                values = values.copy()
-            index = dummy.index.values
-            if not index.flags.contiguous:
-                index = index.copy()
+        values = dummy.values
+        if values.dtype != self.arr.dtype:
+            raise ValueError('Dummy array must be same dtype')
+        if util.is_array(values) and not values.flags.contiguous:
+            # e.g. Categorical has no `flags` attribute
+            values = values.copy()
+        index = dummy.index.values
+        if not index.flags.contiguous:
+            index = index.copy()
 
         return values, index
 
@@ -320,7 +316,6 @@ cdef class SeriesGrouper:
     """
     cdef:
         Py_ssize_t nresults, ngroups
-        bint passed_dummy
 
     cdef public:
         object arr, index, dummy_arr, dummy_index
@@ -330,6 +325,10 @@ cdef class SeriesGrouper:
                  Py_ssize_t ngroups, object dummy):
         n = len(series)
 
+        # in practice we always pass either obj[:0] or the
+        #  safer obj._get_values(slice(None, 0))
+        assert dummy is not None
+
         self.labels = labels
         self.f = f
 
@@ -344,27 +343,22 @@ cdef class SeriesGrouper:
         self.name = getattr(series, 'name', None)
 
         self.dummy_arr, self.dummy_index = self._check_dummy(dummy)
-        self.passed_dummy = dummy is not None
         self.ngroups = ngroups
 
-    def _check_dummy(self, dummy=None):
+    cdef _check_dummy(self, dummy):
         # both values and index must be an ndarray!
 
-        if dummy is None:
-            values = np.empty(0, dtype=self.arr.dtype)
-            index = None
-        else:
-            values = dummy.values
-            # GH 23683: datetimetz types are equivalent to datetime types here
-            if (dummy.dtype != self.arr.dtype
-                    and values.dtype != self.arr.dtype):
-                raise ValueError('Dummy array must be same dtype')
-            if util.is_array(values) and not values.flags.contiguous:
-                # e.g. Categorical has no `flags` attribute
-                values = values.copy()
-            index = dummy.index.values
-            if not index.flags.contiguous:
-                index = index.copy()
+        values = dummy.values
+        # GH 23683: datetimetz types are equivalent to datetime types here
+        if (dummy.dtype != self.arr.dtype
+                and values.dtype != self.arr.dtype):
+            raise ValueError('Dummy array must be same dtype')
+        if util.is_array(values) and not values.flags.contiguous:
+            # e.g. Categorical has no `flags` attribute
+            values = values.copy()
+        index = dummy.index.values
+        if not index.flags.contiguous:
+            index = index.copy()
 
         return values, index
 
@@ -377,7 +371,7 @@ cdef class SeriesGrouper:
             object res
             bint initialized = 0
             Slider vslider, islider
-            object name, cached_typ=None, cached_ityp=None
+            object name, cached_typ = None, cached_ityp = None
 
         labels = self.labels
         counts = np.zeros(self.ngroups, dtype=np.int64)
@@ -489,7 +483,7 @@ cdef class Slider:
         self.buf.data = self.values.data
         self.buf.strides[0] = self.stride
 
-    cpdef advance(self, Py_ssize_t k):
+    cdef advance(self, Py_ssize_t k):
         self.buf.data = <char*>self.buf.data + self.stride * k
 
     cdef move(self, int start, int end):
@@ -499,10 +493,10 @@ cdef class Slider:
         self.buf.data = self.values.data + self.stride * start
         self.buf.shape[0] = end - start
 
-    cpdef set_length(self, Py_ssize_t length):
+    cdef set_length(self, Py_ssize_t length):
         self.buf.shape[0] = length
 
-    cpdef reset(self):
+    cdef reset(self):
 
         self.buf.shape[0] = self.orig_len
         self.buf.data = self.orig_data
@@ -607,10 +601,10 @@ cdef class BlockSlider:
     def __dealloc__(self):
         free(self.base_ptrs)
 
-    cpdef move(self, int start, int end):
+    cdef move(self, int start, int end):
         cdef:
             ndarray arr
-            object index
+            Py_ssize_t i
 
         # move blocks
         for i in range(self.nblocks):
@@ -629,6 +623,7 @@ cdef class BlockSlider:
     cdef reset(self):
         cdef:
             ndarray arr
+            Py_ssize_t i
 
         # reset blocks
         for i in range(self.nblocks):
diff --git a/pandas/_libs/skiplist.pyx b/pandas/_libs/skiplist.pyx
deleted file mode 100644
index eb750a478..000000000
--- a/pandas/_libs/skiplist.pyx
+++ /dev/null
@@ -1,7 +0,0 @@
-# Cython version of IndexableSkiplist, for implementing moving median
-# with O(log n) updates
-# Original author: Raymond Hettinger
-# Original license: MIT
-# Link: http://code.activestate.com/recipes/576930/
-
-# Cython version: Wes McKinney
diff --git a/pandas/_libs/tslibs/ccalendar.pyx b/pandas/_libs/tslibs/ccalendar.pyx
index a82d5e3b5..0588dfe20 100644
--- a/pandas/_libs/tslibs/ccalendar.pyx
+++ b/pandas/_libs/tslibs/ccalendar.pyx
@@ -58,7 +58,8 @@ HOUR_SECONDS = 3600
 @cython.wraparound(False)
 @cython.boundscheck(False)
 cpdef int32_t get_days_in_month(int year, Py_ssize_t month) nogil:
-    """Return the number of days in the given month of the given year.
+    """
+    Return the number of days in the given month of the given year.
 
     Parameters
     ----------
@@ -81,7 +82,8 @@ cpdef int32_t get_days_in_month(int year, Py_ssize_t month) nogil:
 @cython.boundscheck(False)
 @cython.cdivision
 cdef int dayofweek(int y, int m, int d) nogil:
-    """Find the day of week for the date described by the Y/M/D triple y, m, d
+    """
+    Find the day of week for the date described by the Y/M/D triple y, m, d
     using Sakamoto's method, from wikipedia.
 
     0 represents Monday.  See [1]_.
@@ -117,7 +119,8 @@ cdef int dayofweek(int y, int m, int d) nogil:
 
 
 cdef bint is_leapyear(int64_t year) nogil:
-    """Returns 1 if the given year is a leap year, 0 otherwise.
+    """
+    Returns 1 if the given year is a leap year, 0 otherwise.
 
     Parameters
     ----------
@@ -134,7 +137,8 @@ cdef bint is_leapyear(int64_t year) nogil:
 @cython.wraparound(False)
 @cython.boundscheck(False)
 cpdef int32_t get_week_of_year(int year, int month, int day) nogil:
-    """Return the ordinal week-of-year for the given day.
+    """
+    Return the ordinal week-of-year for the given day.
 
     Parameters
     ----------
@@ -178,7 +182,8 @@ cpdef int32_t get_week_of_year(int year, int month, int day) nogil:
 @cython.wraparound(False)
 @cython.boundscheck(False)
 cpdef int32_t get_day_of_year(int year, int month, int day) nogil:
-    """Return the ordinal day-of-year for the given day.
+    """
+    Return the ordinal day-of-year for the given day.
 
     Parameters
     ----------
@@ -207,8 +212,9 @@ cpdef int32_t get_day_of_year(int year, int month, int day) nogil:
     return day_of_year
 
 
-def get_locale_names(name_type: object, locale: object=None):
-    """Returns an array of localized day or month names
+def get_locale_names(name_type: str, locale: object = None):
+    """
+    Returns an array of localized day or month names.
 
     Parameters
     ----------
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index c70e62377..ad4b4bf8b 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -1881,8 +1881,9 @@ def diff(arr, n: int, axis: int = 0):
     out_arr[tuple(na_indexer)] = na
 
     if arr.ndim == 2 and arr.dtype.name in _diff_special:
-        f = algos.diff_2d
-        f(arr, out_arr, n, axis)
+        # TODO: can diff_2d dtype specialization troubles be fixed by defining
+        #  out_arr inside diff_2d?
+        algos.diff_2d(arr, out_arr, n, axis)
     else:
         # To keep mypy happy, _res_indexer is a list while res_indexer is
         #  a tuple, ditto for lag_indexer.
diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index f4c3ac970..996c178bd 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -7,7 +7,6 @@ which here returns a DataFrameGroupBy object.
 """
 from collections import OrderedDict, abc, namedtuple
 import copy
-import functools
 from functools import partial
 from textwrap import dedent
 import typing
@@ -1689,8 +1688,10 @@ class DataFrameGroupBy(GroupBy):
         )
         loc = (blk.mgr_locs for blk in data.blocks)
 
-        counter = partial(lib.count_level_2d, labels=ids, max_bin=ngroups, axis=1)
-        blk = map(make_block, map(counter, val), loc)
+        counted = [
+            lib.count_level_2d(x, labels=ids, max_bin=ngroups, axis=1) for x in val
+        ]
+        blk = map(make_block, counted, loc)
 
         return self._wrap_agged_blocks(data.items, list(blk))
 
@@ -1900,7 +1901,7 @@ def _managle_lambda_list(aggfuncs: Sequence[Any]) -> Sequence[Any]:
     mangled_aggfuncs = []
     for aggfunc in aggfuncs:
         if com.get_callable_name(aggfunc) == "<lambda>":
-            aggfunc = functools.partial(aggfunc)
+            aggfunc = partial(aggfunc)
             aggfunc.__name__ = "<lambda_{}>".format(i)
             i += 1
         mangled_aggfuncs.append(aggfunc)
diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py
index 19cb71cdc..707c0c357 100644
--- a/pandas/core/groupby/ops.py
+++ b/pandas/core/groupby/ops.py
@@ -514,7 +514,6 @@ class BaseGrouper:
             result = result[:, 0]
 
         if how in self._name_functions:
-            # TODO
             names = self._name_functions[how]()
         else:
             names = None
@@ -698,7 +697,7 @@ class BinGrouper(BaseGrouper):
         """
         return self
 
-    def get_iterator(self, data, axis=0):
+    def get_iterator(self, data: NDFrame, axis: int = 0):
         """
         Groupby iterator
 
@@ -707,12 +706,8 @@ class BinGrouper(BaseGrouper):
         Generator yielding sequence of (name, subsetted object)
         for each group
         """
-        if isinstance(data, NDFrame):
-            slicer = lambda start, edge: data._slice(slice(start, edge), axis=axis)
-            length = len(data.axes[axis])
-        else:
-            slicer = lambda start, edge: data[slice(start, edge)]
-            length = len(data)
+        slicer = lambda start, edge: data._slice(slice(start, edge), axis=axis)
+        length = len(data.axes[axis])
 
         start = 0
         for edge, label in zip(self.bins, self.binlabels):
diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py
index c47aaf7c7..21ae820cf 100644
--- a/pandas/core/internals/managers.py
+++ b/pandas/core/internals/managers.py
@@ -1276,7 +1276,6 @@ class BlockManager(PandasObject):
         Returns
         -------
         new_blocks : list of Block
-
         """
 
         allow_fill = fill_tuple is not None
diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index fcbb000ac..f1a67d089 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -123,8 +123,8 @@ def _map(f, arr, na_mask=False, na_value=np.nan, dtype=object):
         arr = np.asarray(arr, dtype=object)
     if na_mask:
         mask = isna(arr)
+        convert = not np.all(mask)
         try:
-            convert = not all(mask)
             result = lib.map_infer_mask(arr, f, mask.view(np.uint8), convert)
         except (TypeError, AttributeError) as e:
             # Reraise the exception if callable `f` got wrong number of args.
@@ -135,6 +135,7 @@ def _map(f, arr, na_mask=False, na_value=np.nan, dtype=object):
             )
 
             if len(e.args) >= 1 and re.search(p_err, e.args[0]):
+                # FIXME: this should be totally avoidable
                 raise e
 
             def g(x):
diff --git a/pandas/core/util/hashing.py b/pandas/core/util/hashing.py
index fddbea8ed..011ea1b8e 100644
--- a/pandas/core/util/hashing.py
+++ b/pandas/core/util/hashing.py
@@ -58,7 +58,7 @@ def hash_pandas_object(
     obj,
     index: bool = True,
     encoding: str = "utf8",
-    hash_key=None,
+    hash_key: str = _default_hash_key,
     categorize: bool = True,
 ):
     """
@@ -70,7 +70,7 @@ def hash_pandas_object(
         include the index in the hash (if Series/DataFrame)
     encoding : str, default 'utf8'
         encoding for data & key when strings
-    hash_key : str, default '_default_hash_key'
+    hash_key : str, default _default_hash_key
         hash_key for string key to encode
     categorize : bool, default True
         Whether to first categorize object arrays before hashing. This is more
@@ -82,9 +82,6 @@ def hash_pandas_object(
     """
     from pandas import Series
 
-    if hash_key is None:
-        hash_key = _default_hash_key
-
     if isinstance(obj, ABCMultiIndex):
         return Series(hash_tuples(obj, encoding, hash_key), dtype="uint64", copy=False)
 
@@ -140,7 +137,7 @@ def hash_pandas_object(
     return h
 
 
-def hash_tuples(vals, encoding="utf8", hash_key=None):
+def hash_tuples(vals, encoding="utf8", hash_key: str = _default_hash_key):
     """
     Hash an MultiIndex / list-of-tuples efficiently
 
@@ -148,7 +145,7 @@ def hash_tuples(vals, encoding="utf8", hash_key=None):
     ----------
     vals : MultiIndex, list-of-tuples, or single tuple
     encoding : str, default 'utf8'
-    hash_key : str, default '_default_hash_key'
+    hash_key : str, default _default_hash_key
 
     Returns
     -------
@@ -183,7 +180,7 @@ def hash_tuples(vals, encoding="utf8", hash_key=None):
     return h
 
 
-def hash_tuple(val, encoding: str = "utf8", hash_key=None):
+def hash_tuple(val, encoding: str = "utf8", hash_key: str = _default_hash_key):
     """
     Hash a single tuple efficiently
 
@@ -191,7 +188,7 @@ def hash_tuple(val, encoding: str = "utf8", hash_key=None):
     ----------
     val : single tuple
     encoding : str, default 'utf8'
-    hash_key : str, default '_default_hash_key'
+    hash_key : str, default _default_hash_key
 
     Returns
     -------
@@ -213,8 +210,8 @@ def _hash_categorical(c, encoding: str, hash_key: str):
     Parameters
     ----------
     c : Categorical
-    encoding : str, default 'utf8'
-    hash_key : str, default '_default_hash_key'
+    encoding : str
+    hash_key : str
 
     Returns
     -------
@@ -243,7 +240,12 @@ def _hash_categorical(c, encoding: str, hash_key: str):
     return result
 
 
-def hash_array(vals, encoding: str = "utf8", hash_key=None, categorize: bool = True):
+def hash_array(
+    vals,
+    encoding: str = "utf8",
+    hash_key: str = _default_hash_key,
+    categorize: bool = True,
+):
     """
     Given a 1d array, return an array of deterministic integers.
 
@@ -252,7 +254,7 @@ def hash_array(vals, encoding: str = "utf8", hash_key=None, categorize: bool = T
     vals : ndarray, Categorical
     encoding : str, default 'utf8'
         encoding for data & key when strings
-    hash_key : str, default '_default_hash_key'
+    hash_key : str, default _default_hash_key
         hash_key for string key to encode
     categorize : bool, default True
         Whether to first categorize object arrays before hashing. This is more
@@ -267,9 +269,6 @@ def hash_array(vals, encoding: str = "utf8", hash_key=None, categorize: bool = T
         raise TypeError("must pass a ndarray-like")
     dtype = vals.dtype
 
-    if hash_key is None:
-        hash_key = _default_hash_key
-
     # For categoricals, we hash the categories, then remap the codes to the
     # hash values. (This check is above the complex check so that we don't ask
     # numpy if categorical is a subdtype of complex, as it will choke).
@@ -320,9 +319,17 @@ def hash_array(vals, encoding: str = "utf8", hash_key=None, categorize: bool = T
     return vals
 
 
-def _hash_scalar(val, encoding: str = "utf8", hash_key=None):
+def _hash_scalar(
+    val, encoding: str = "utf8", hash_key: str = _default_hash_key
+) -> np.ndarray:
     """
-    Hash scalar value
+    Hash scalar value.
+
+    Parameters
+    ----------
+    val : scalar
+    encoding : str, default "utf8"
+    hash_key : str, default _default_hash_key
 
     Returns
     -------
diff --git a/setup.py b/setup.py
index 0dd198008..c75ad5896 100755
--- a/setup.py
+++ b/setup.py
@@ -330,7 +330,6 @@ class CheckSDist(sdist_class):
         "pandas/_libs/missing.pyx",
         "pandas/_libs/reduction.pyx",
         "pandas/_libs/testing.pyx",
-        "pandas/_libs/skiplist.pyx",
         "pandas/_libs/sparse.pyx",
         "pandas/_libs/ops.pyx",
         "pandas/_libs/parsers.pyx",
@@ -604,10 +603,6 @@ ext_data = {
     "_libs.ops": {"pyxfile": "_libs/ops"},
     "_libs.properties": {"pyxfile": "_libs/properties", "include": []},
     "_libs.reshape": {"pyxfile": "_libs/reshape", "depends": []},
-    "_libs.skiplist": {
-        "pyxfile": "_libs/skiplist",
-        "depends": ["pandas/_libs/src/skiplist.h"],
-    },
     "_libs.sparse": {"pyxfile": "_libs/sparse", "depends": _pxi_dep["sparse"]},
     "_libs.tslib": {
         "pyxfile": "_libs/tslib",
