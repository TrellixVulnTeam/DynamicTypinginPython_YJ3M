commit b8382a3ca71f1c06d453b909005024a0ff7cab93
Author: jreback <jeff@reback.net>
Date:   Wed May 1 20:23:43 2013 -0400

    BUG: GH3495 change core/format/CSVFormatter.save to allow generic way of dealing
    
         with columns duplicate or not

diff --git a/RELEASE.rst b/RELEASE.rst
index 38298fde1..1a86ac02b 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -61,16 +61,15 @@ pandas 0.11.1
   - Fix regression in a DataFrame apply with axis=1, objects were not being converted back
     to base dtypes correctly (GH3480_)
   - Fix issue when storing uint dtypes in an HDFStore. (GH3493_)
-  - Fix assigning a new index to a duplicate index in a DataFrame would fail (GH3468_)
-  - ref_locs support to allow duplicative indices across dtypes (GH3468_)
   - Non-unique index support clarified (GH3468_)
 
-    - Fix assigning a new index to a duplicate index in a DataFrame would fail
+    - Fix assigning a new index to a duplicate index in a DataFrame would fail (GH3468_)
     - Fix construction of a DataFrame with a duplicate index
-    - ref_locs support to allow duplicative indices across dtypes
-      (GH2194_)
+    - ref_locs support to allow duplicative indices across dtypes,
+      allows iget support to always find the index (even across dtypes) (GH2194_)
     - applymap on a DataFrame with a non-unique index now works
       (removed warning) (GH2786_), and fix (GH3230_)
+    - Fix to_csv to handle non-unique columns (GH3495_)
 
 .. _GH3164: https://github.com/pydata/pandas/issues/3164
 .. _GH2786: https://github.com/pydata/pandas/issues/2786
@@ -91,6 +90,7 @@ pandas 0.11.1
 .. _GH3468: https://github.com/pydata/pandas/issues/3468
 .. _GH3448: https://github.com/pydata/pandas/issues/3448
 .. _GH3449: https://github.com/pydata/pandas/issues/3449
+.. _GH3495: https://github.com/pydata/pandas/issues/3495
 .. _GH3493: https://github.com/pydata/pandas/issues/3493
 
 
diff --git a/pandas/core/format.py b/pandas/core/format.py
index 5b68b26a4..fa2135bb4 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -820,21 +820,7 @@ class CSVFormatter(object):
         self.blocks = self.obj._data.blocks
         ncols = sum(len(b.items) for b in self.blocks)
         self.data =[None] * ncols
-
-        if self.obj.columns.is_unique:
-            self.colname_map = dict((k,i) for i,k in  enumerate(self.obj.columns))
-        else:
-            ks = [set(x.items) for x in self.blocks]
-            u = len(reduce(lambda a,x: a.union(x),ks,set()))
-            t = sum(map(len,ks))
-            if u != t:
-                if len(set(self.cols)) != len(self.cols):
-                    raise NotImplementedError("duplicate columns with differing dtypes are unsupported")
-            else:
-                # if columns are not unique and we acces this,
-                # we're doing it wrong
-                pass
-
+        self.column_map = self.obj._data.get_items_map()
 
         if chunksize is None:
             chunksize = (100000/ (len(self.cols) or 1)) or 1
@@ -1034,18 +1020,13 @@ class CSVFormatter(object):
 
         # create the data for a chunk
         slicer = slice(start_i,end_i)
-        if self.obj.columns.is_unique:
-            for i in range(len(self.blocks)):
-                b = self.blocks[i]
-                d = b.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)
-                for j, k in enumerate(b.items):
-                    # self.data is a preallocated list
-                    self.data[self.colname_map[k]] = d[j]
-        else:
-            # self.obj should contain a proper view of the dataframes
-            # with the specified ordering of cols if cols was specified
-            for i in range(len(self.obj.columns)):
-                self.data[i] = self.obj.icol(i).values[slicer].tolist()
+        for i in range(len(self.blocks)):
+            b = self.blocks[i]
+            d = b.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)
+            for i, item in enumerate(b.items):
+
+                # self.data is a preallocated list
+                self.data[self.column_map[b][i]] = d[i]
 
         ix = data_index.to_native_types(slicer=slicer, na_rep=self.na_rep, float_format=self.float_format)
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index c874b061d..5c0f9253b 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -65,6 +65,11 @@ class Block(object):
             self._ref_locs = indexer
         return self._ref_locs
 
+    def set_ref_locs(self, placement):
+        """ explicity set the ref_locs indexer, only necessary for duplicate indicies """
+        if placement is not None:
+            self._ref_locs = np.array(placement,dtype='int64')
+
     def set_ref_items(self, ref_items, maybe_rename=True):
         """
         If maybe_rename=True, need to set the items for this guy
@@ -883,7 +888,7 @@ class BlockManager(object):
     -----
     This is *not* a public API class
     """
-    __slots__ = ['axes', 'blocks', '_known_consolidated', '_is_consolidated', '_ref_locs']
+    __slots__ = ['axes', 'blocks', '_known_consolidated', '_is_consolidated', '_ref_locs', '_items_map']
 
     def __init__(self, blocks, axes, do_integrity_check=True):
         self.axes = [_ensure_index(ax) for ax in axes]
@@ -901,6 +906,10 @@ class BlockManager(object):
 
         self._consolidate_check()
 
+        # we have a duplicate items index, setup the block maps
+        if not self.items.is_unique:
+            self._set_ref_locs(do_refs=True)
+
     @classmethod
     def make_empty(self):
         return BlockManager([], [[], []])
@@ -924,76 +933,135 @@ class BlockManager(object):
 
         if axis == 0:
 
-            # we have a non-unique index, so setup the ref_locs
-            if not cur_axis.is_unique:
-                self.set_ref_locs(cur_axis)
+            # set/reset ref_locs based on the current index
+            # and map the new index if needed
+            self._set_ref_locs(labels=cur_axis)
 
             # take via ref_locs
             for block in self.blocks:
                 block.set_ref_items(self.items, maybe_rename=True)
 
-    def set_ref_locs(self, labels = None):
-        # if we have a non-unique index on this axis, set the indexers
-        # we need to set an absolute indexer for the blocks
-        # return the indexer if we are not unique
+            # set/reset ref_locs based on the new index
+            self._set_ref_locs(labels=value, do_refs=True)
+
+    def _set_ref_locs(self, labels=None, do_refs=False):
+        """
+        if we have a non-unique index on this axis, set the indexers
+        we need to set an absolute indexer for the blocks
+        return the indexer if we are not unique
+        
+        labels : the (new) labels for this manager
+        ref    : boolean, whether to set the labels (one a 1-1 mapping)
+
+        """
+
+        im = None
         if labels is None:
             labels = self.items
+        else:
+            _ensure_index(labels)
 
-        if labels.is_unique: 
-            return None
+        # we are unique, and coming from a unique
+        if labels.is_unique and not do_refs:
 
-        #### THIS IS POTENTIALLY VERY SLOW #####
+            # reset our ref locs
+            self._ref_locs = None
+            for b in self.blocks:
+                b._ref_locs = None
 
-        # if we are already computed, then we are done
-        rl = getattr(self,'_ref_locs',None)
-        if rl is not None:
-            return rl
+            return None
 
-        blocks = self.blocks
+        # we are going to a non-unique index
+        # we have ref_locs on the block at this point
+        #   or if ref_locs are not set, then we must assume a block
+        #   ordering
+        if not labels.is_unique and do_refs:
+
+            # create the items map
+            im = getattr(self,'_items_map',None)
+            if im is None:
+
+                im = dict()
+                def maybe_create_block(block):
+                    try:
+                        return d[block]
+                    except:
+                        im[block] = l = [ None ] * len(block.items)
+                    return l
+
+                count_items = 0
+                for block in self.blocks:
+
+                    # if we have a duplicate index but
+                    # _ref_locs have not been set....then
+                    # have to assume ordered blocks are passed
+                    num_items = len(block.items)
+                    try:
+                        rl = block.ref_locs
+                    except:
+                        rl = np.arange(num_items) + count_items
+
+                    m = maybe_create_block(block)
+                    for i, item in enumerate(block.items):
+                        m[i] = rl[i]
+                    count_items += num_items
+
+                self._items_map = im
+
+            # create the _ref_loc map here
+            rl = np.empty(len(labels),dtype=object)
+            for block, items in im.items():
+                for i, loc in enumerate(items):
+                    rl[loc] = (block,i)
+            self._ref_locs = rl
+            return rl
 
-        # initialize
-        blockmap = dict()
-        for b in blocks:
-            arr = np.empty(len(b.items),dtype='int64')
-            arr.fill(-1)
-            b._ref_locs = arr
+        # return our cached _ref_locs (or will compute again 
+        # when we recreate the block manager if needed
+        return getattr(self,'_ref_locs',None)
 
-            # add this block to the blockmap for each
-            # of the items in the block
-            for item in b.items:
-               if item not in blockmap:
-                   blockmap[item] = []
-               blockmap[item].append(b)
+    def get_items_map(self):
+        """ 
+        return an inverted ref_loc map for an item index
+        block -> item (in that block) location -> column location
+        """
 
-        rl = np.empty(len(labels),dtype=object)
-        for i, item in enumerate(labels.values):
+        # cache check
+        im = getattr(self,'_items_map',None)
+        if im is not None:
+            return im
+        
+        im = dict()
+        rl = self._set_ref_locs()
 
+        def maybe_create_block(block):
             try:
-                block = blockmap[item].pop(0)
+                return im[block]
             except:
-                raise Exception("not enough items in set_ref_locs")
+                im[block] = l = [ None ] * len(block.items)
+            return l
 
-            indexer = np.arange(len(block.items))
-            mask = (block.items == item) & (block._ref_locs == -1)
-            if not mask.any():
+        # we have a non-duplicative index
+        if rl is None:
 
-                # this case will catch a comparison of a index of tuples
-                mask = np.empty(len(block.items),dtype=bool)
-                mask.fill(False)
-                for j, (bitem, brl) in enumerate(zip(block.items,block._ref_locs)):
-                    mask[j] = bitem == item and brl == -1
+            axis = self.axes[0]
+            for block in self.blocks:
 
-            indices = indexer[mask]
-            if len(indices):
-                idx = indices[0]
-            else:
-                raise Exception("already set too many items in set_ref_locs")
+                m = maybe_create_block(block)
+                for i, item in enumerate(block.items):
+                    m[i] = axis.get_loc(item)
+
+
+        # use the ref_locs to construct the map
+        else:
 
-            block._ref_locs[idx] = i
-            rl[i] = (block,idx)
-           
-        self._ref_locs = rl
-        return rl
+            for i, (block, idx) in enumerate(rl):
+                
+                m = maybe_create_block(block)
+                m[idx] = i
+
+        self._items_map = im
+        return im
 
     # make items read only for now
     def _get_items(self):
@@ -1259,13 +1327,16 @@ class BlockManager(object):
                                   new_items, 
                                   klass=blk.__class__,
                                   fastpath=True)
+                newb.set_ref_locs(blk._ref_locs)
                 new_blocks = [newb]
             else:
                 return self.reindex_items(new_items)
         else:
             new_blocks = self._slice_blocks(slobj, axis)
 
-        return BlockManager(new_blocks, new_axes, do_integrity_check=False)
+        bm = BlockManager(new_blocks, new_axes, do_integrity_check=False)
+        bm._consolidate_inplace()
+        return bm
 
     def _slice_blocks(self, slobj, axis):
         new_blocks = []
@@ -1280,6 +1351,7 @@ class BlockManager(object):
                               block.ref_items, 
                               klass=block.__class__,
                               fastpath=True)
+            newb.set_ref_locs(block._ref_locs)
             new_blocks.append(newb)
         return new_blocks
 
@@ -1463,9 +1535,9 @@ class BlockManager(object):
             return self.get(item)
 
         # compute the duplicative indexer if needed
-        ref_locs = self.set_ref_locs()
+        ref_locs = self._set_ref_locs()
         b, loc = ref_locs[i]
-        return b.values[loc]
+        return b.iget(loc)
 
     def get_scalar(self, tup):
         """
@@ -1904,54 +1976,55 @@ def form_blocks(arrays, names, axes):
     bool_items = []
     object_items = []
     datetime_items = []
-    for k, v in zip(names, arrays):
+    for i, (k, v) in enumerate(zip(names, arrays)):
         if issubclass(v.dtype.type, np.floating):
-            float_items.append((k, v))
+            float_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.complexfloating):
-            complex_items.append((k, v))
+            complex_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.datetime64):
             if v.dtype != _NS_DTYPE:
                 v = tslib.cast_to_nanoseconds(v)
 
             if hasattr(v, 'tz') and v.tz is not None:
-                object_items.append((k, v))
+                object_items.append((i, k, v))
             else:
-                datetime_items.append((k, v))
+                datetime_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.integer):
             if v.dtype == np.uint64:
                 # HACK #2355 definite overflow
                 if (v > 2 ** 63 - 1).any():
-                    object_items.append((k, v))
+                    object_items.append((i, k, v))
                     continue
-            int_items.append((k, v))
+            int_items.append((i, k, v))
         elif v.dtype == np.bool_:
-            bool_items.append((k, v))
+            bool_items.append((i, k, v))
         else:
-            object_items.append((k, v))
+            object_items.append((i, k, v))
 
+    is_unique = items.is_unique
     blocks = []
     if len(float_items):
-        float_blocks = _multi_blockify(float_items, items)
+        float_blocks = _multi_blockify(float_items, items, is_unique=is_unique)
         blocks.extend(float_blocks)
 
     if len(complex_items):
-        complex_blocks = _simple_blockify(complex_items, items, np.complex128)
+        complex_blocks = _simple_blockify(complex_items, items, np.complex128, is_unique=is_unique)
         blocks.extend(complex_blocks)
 
     if len(int_items):
-        int_blocks = _multi_blockify(int_items, items)
+        int_blocks = _multi_blockify(int_items, items, is_unique=is_unique)
         blocks.extend(int_blocks)
 
     if len(datetime_items):
-        datetime_blocks = _simple_blockify(datetime_items, items, _NS_DTYPE)
+        datetime_blocks = _simple_blockify(datetime_items, items, _NS_DTYPE, is_unique=is_unique)
         blocks.extend(datetime_blocks)
 
     if len(bool_items):
-        bool_blocks = _simple_blockify(bool_items, items, np.bool_)
+        bool_blocks = _simple_blockify(bool_items, items, np.bool_, is_unique=is_unique)
         blocks.extend(bool_blocks)
 
     if len(object_items) > 0:
-        object_blocks = _simple_blockify(object_items, items, np.object_)
+        object_blocks = _simple_blockify(object_items, items, np.object_, is_unique=is_unique)
         blocks.extend(object_blocks)
 
     if len(extra_items):
@@ -1959,7 +2032,6 @@ def form_blocks(arrays, names, axes):
 
         # empty items -> dtype object
         block_values = np.empty(shape, dtype=object)
-
         block_values.fill(nan)
 
         na_block = make_block(block_values, extra_items, items)
@@ -1968,28 +2040,32 @@ def form_blocks(arrays, names, axes):
     return blocks
 
 
-def _simple_blockify(tuples, ref_items, dtype):
+def _simple_blockify(tuples, ref_items, dtype, is_unique=True):
     """ return a single array of a block that has a single dtype; if dtype is not None, coerce to this dtype """
-    block_items, values = _stack_arrays(tuples, ref_items, dtype)
+    block_items, values, placement = _stack_arrays(tuples, ref_items, dtype)
 
     # CHECK DTYPE?
     if dtype is not None and values.dtype != dtype:  # pragma: no cover
         values = values.astype(dtype)
 
-    return [ make_block(values, block_items, ref_items) ]
-
+    block = make_block(values, block_items, ref_items)
+    if not is_unique:
+        block.set_ref_locs(placement)
+    return [ block ]
 
-def _multi_blockify(tuples, ref_items, dtype = None):
+def _multi_blockify(tuples, ref_items, dtype = None, is_unique=True):
     """ return an array of blocks that potentially have different dtypes """
 
     # group by dtype
-    grouper = itertools.groupby(tuples, lambda x: x[1].dtype)
+    grouper = itertools.groupby(tuples, lambda x: x[2].dtype)
 
     new_blocks = []
     for dtype, tup_block in grouper:
 
-        block_items, values = _stack_arrays(list(tup_block), ref_items, dtype)
+        block_items, values, placement = _stack_arrays(list(tup_block), ref_items, dtype)
         block = make_block(values, block_items, ref_items)
+        if not is_unique:
+            block.set_ref_locs(placement)
         new_blocks.append(block)
 
     return new_blocks
@@ -2012,7 +2088,7 @@ def _stack_arrays(tuples, ref_items, dtype):
         else:
             return x.shape
 
-    names, arrays = zip(*tuples)
+    placement, names, arrays = zip(*tuples)
 
     first = arrays[0]
     shape = (len(arrays),) + _shape_compat(first)
@@ -2029,7 +2105,7 @@ def _stack_arrays(tuples, ref_items, dtype):
         if len(items) != len(stacked):
             raise Exception("invalid names passed _stack_arrays")
 
-    return items, stacked
+    return items, stacked, placement
 
 
 def _blocks_to_series_dict(blocks, index=None):
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index cb3799c28..69225c40e 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -4973,17 +4973,33 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         with ensure_clean() as filename:
             df.to_csv(filename) # single dtype, fine
+            result = read_csv(filename,index_col=0)
+            result.columns = df.columns
+            assert_frame_equal(result,df)
 
-        df_float  = DataFrame(np.random.randn(1000, 30),dtype='float64')
-        df_int    = DataFrame(np.random.randn(1000, 30),dtype='int64')
-        df_bool   = DataFrame(True,index=df_float.index,columns=df_float.columns)
-        df_object = DataFrame('foo',index=df_float.index,columns=df_float.columns)
-        df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=df_float.columns)
-        df        = pan.concat([ df_float, df_int, df_bool, df_object, df_dt ], axis=1)
+        df_float  = DataFrame(np.random.randn(1000, 3),dtype='float64')
+        df_int    = DataFrame(np.random.randn(1000, 3),dtype='int64')
+        df_bool   = DataFrame(True,index=df_float.index,columns=range(3))
+        df_object = DataFrame('foo',index=df_float.index,columns=range(3))
+        df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=range(3))
+        df        = pan.concat([ df_float, df_int, df_bool, df_object, df_dt ], axis=1, ignore_index=True)
 
-        #### this raises because we have duplicate column names across dtypes ####
+        cols = []
+        for i in range(5):
+            cols.extend([0,1,2])
+        df.columns = cols
+
+        from pandas import to_datetime
         with ensure_clean() as filename:
-            self.assertRaises(Exception, df.to_csv, filename)
+            df.to_csv(filename)
+            result = read_csv(filename,index_col=0)
+          
+            # date cols
+            for i in ['0.4','1.4','2.4']:
+                 result[i] = to_datetime(result[i])
+
+            result.columns = df.columns
+            assert_frame_equal(result,df)
 
         # GH3457
         from pandas.util.testing import makeCustomDataframe as mkdf
@@ -9246,7 +9262,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=df_float.columns)
         df        = pan.concat([ df_float, df_int, df_bool, df_object, df_dt ], axis=1)
 
-        result = df._data.set_ref_locs()
+        result = df._data._set_ref_locs()
         self.assert_(len(result) == len(df.columns))
 
         # testing iget
diff --git a/pandas/tests/test_indexing.py b/pandas/tests/test_indexing.py
index 8e1ea5699..ae71ec8b3 100644
--- a/pandas/tests/test_indexing.py
+++ b/pandas/tests/test_indexing.py
@@ -774,8 +774,14 @@ class TestIndexing(unittest.TestCase):
 
         # across dtypes
         df = DataFrame([[1,2,1.,2.,3.,'foo','bar']], columns=list('aaaaaaa'))
+        df.head()
+        str(df)
         result = DataFrame([[1,2,1.,2.,3.,'foo','bar']])
         result.columns = list('aaaaaaa')
+
+        df_v  = df.iloc[:,4]
+        res_v = result.iloc[:,4]
+
         assert_frame_equal(df,result)
 
 
