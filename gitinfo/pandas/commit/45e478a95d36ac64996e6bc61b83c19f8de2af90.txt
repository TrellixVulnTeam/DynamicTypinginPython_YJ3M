commit 45e478a95d36ac64996e6bc61b83c19f8de2af90
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Nov 30 16:15:17 2012 -0500

    BLD: pandas imports. lot of stuff still broken though

diff --git a/Makefile b/Makefile
index a4861c147..af44d0223 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 clean:
 	-rm -rf build dist
 
-tseries: pandas/src/tseries.pyx
+tseries: pandas/lib.pyx pandas/tslib.pyx pandas/hashtable.pyx
 	python setup.py build_ext --inplace
 
 sparse: pandas/src/sparse.pyx
diff --git a/pandas/__init__.py b/pandas/__init__.py
index df37b44cc..d97dba65e 100644
--- a/pandas/__init__.py
+++ b/pandas/__init__.py
@@ -7,7 +7,9 @@ from datetime import datetime
 import numpy as np
 
 try:
-    import pandas.lib as lib
+    import hashtable
+    import tslib
+    import lib
 except Exception:  # pragma: no cover
     import sys
     e = sys.exc_info()[1] # Py25 and Py3 current exception syntax conflict
diff --git a/pandas/src/stats.pyx b/pandas/algos.pyx
similarity index 97%
rename from pandas/src/stats.pyx
rename to pandas/algos.pyx
index 7f30606af..a17b923e5 100644
--- a/pandas/src/stats.pyx
+++ b/pandas/algos.pyx
@@ -1,5 +1,6 @@
 from numpy cimport *
 cimport numpy as np
+import numpy as np
 
 cimport cython
 
@@ -1707,6 +1708,83 @@ def roll_generic(ndarray[float64_t, cast=True] input, int win,
 #----------------------------------------------------------------------
 # group operations
 
+@cython.boundscheck(False)
+def groupby_indices(ndarray values):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[int64_t] labels, counts, arr, seen
+        int64_t loc
+        dict ids = {}
+        object val
+        int64_t k
+
+    ids, labels, counts = group_labels(values)
+    seen = np.zeros_like(counts)
+
+    # try not to get in trouble here...
+    cdef int64_t **vecs = <int64_t **> malloc(len(ids) * sizeof(int64_t*))
+    result = {}
+    for i from 0 <= i < len(counts):
+        arr = np.empty(counts[i], dtype=np.int64)
+        result[ids[i]] = arr
+        vecs[i] = <int64_t *> arr.data
+
+    for i from 0 <= i < n:
+        k = labels[i]
+
+        # was NaN
+        if k == -1:
+            continue
+
+        loc = seen[k]
+        vecs[k][loc] = i
+        seen[k] = loc + 1
+
+    free(vecs)
+
+    return result
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def group_labels(ndarray[object] values):
+    '''
+    Compute label vector from input values and associated useful data
+
+    Returns
+    -------
+    '''
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[int64_t] labels = np.empty(n, dtype=np.int64)
+        ndarray[int64_t] counts = np.empty(n, dtype=np.int64)
+        dict ids = {}, reverse = {}
+        int64_t idx
+        object val
+        int64_t count = 0
+
+    for i from 0 <= i < n:
+        val = values[i]
+
+        # is NaN
+        if val != val:
+            labels[i] = -1
+            continue
+
+        # for large number of groups, not doing try: except: makes a big
+        # difference
+        if val in ids:
+            idx = ids[val]
+            labels[i] = idx
+            counts[idx] = counts[idx] + 1
+        else:
+            ids[val] = count
+            reverse[count] = val
+            labels[i] = count
+            counts[count] = 1
+            count += 1
+
+    return reverse, labels, counts[:count].copy()
+
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
@@ -2943,3 +3021,4 @@ def group_var_bin(ndarray[float64_t, ndim=2] out,
                              (ct * ct - ct))
 
 include "join.pyx"
+include "generated.pyx"
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 4afa5953e..2e7d8731a 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -6,8 +6,8 @@ intended for public consumption
 import numpy as np
 
 import pandas.core.common as com
-import pandas.lib as lib
-import pandas._algos as _algos
+import pandas.algos as algos
+import pandas.hashtable as htable
 
 
 def match(to_match, values, na_sentinel=-1):
@@ -70,11 +70,11 @@ def _hashtable_algo(f, dtype):
     f(HashTable, type_caster) -> result
     """
     if com.is_float_dtype(dtype):
-        return f(lib.Float64HashTable, com._ensure_float64)
+        return f(htable.Float64HashTable, com._ensure_float64)
     elif com.is_integer_dtype(dtype):
-        return f(lib.Int64HashTable, com._ensure_int64)
+        return f(htable.Int64HashTable, com._ensure_int64)
     else:
-        return f(lib.PyObjectHashTable, com._ensure_object)
+        return f(htable.PyObjectHashTable, com._ensure_object)
 
 
 def _count_generic(values, table_type, type_caster):
@@ -167,7 +167,7 @@ def value_counts(values, sort=True, ascending=False):
 
     if com.is_integer_dtype(values.dtype):
         values = com._ensure_int64(values)
-        keys, counts = lib.value_count_int64(values)
+        keys, counts = htable.value_count_int64(values)
         result = Series(counts, index=keys)
     else:
         counter = defaultdict(lambda: 0)
@@ -271,7 +271,7 @@ def quantile(x, q, interpolation_method='fraction'):
         return _get_score(q)
     else:
         q = np.asarray(q, np.float64)
-        return _algos.arrmap_float64(q, _get_score)
+        return algos.arrmap_float64(q, _get_score)
 
 
 def _interpolate(a, b, fraction):
@@ -313,19 +313,19 @@ def group_position(*args):
 
 
 _rank1d_functions = {
-    'float64': lib.rank_1d_float64,
-    'int64': lib.rank_1d_int64,
-    'generic': lib.rank_1d_generic
+    'float64': algos.rank_1d_float64,
+    'int64': algos.rank_1d_int64,
+    'generic': algos.rank_1d_generic
 }
 
 _rank2d_functions = {
-    'float64': lib.rank_2d_float64,
-    'int64': lib.rank_2d_int64,
-    'generic': lib.rank_2d_generic
+    'float64': algos.rank_2d_float64,
+    'int64': algos.rank_2d_int64,
+    'generic': algos.rank_2d_generic
 }
 
 _hashtables = {
-    'float64': (lib.Float64HashTable, lib.Float64Vector),
-    'int64': (lib.Int64HashTable, lib.Int64Vector),
-    'generic': (lib.PyObjectHashTable, lib.ObjectVector)
+    'float64': (htable.Float64HashTable, htable.Float64Vector),
+    'int64': (htable.Int64HashTable, htable.Int64Vector),
+    'generic': (htable.PyObjectHashTable, htable.ObjectVector)
 }
diff --git a/pandas/core/common.py b/pandas/core/common.py
index c86ee34f2..3ee78e9d0 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -11,8 +11,10 @@ import itertools
 from numpy.lib.format import read_array, write_array
 import numpy as np
 
-import pandas._algos as _algos
+import pandas.algos as algos
 import pandas.lib as lib
+import pandas.tslib as tslib
+
 from pandas.util import py3compat
 import codecs
 import csv
@@ -84,7 +86,7 @@ def _isnull_ndarraylike(obj):
             result = Series(result, index=obj.index, copy=False)
     elif values.dtype == np.dtype('M8[ns]'):
         # this is the NaT pattern
-        result = values.view('i8') == lib.iNaT
+        result = values.view('i8') == tslib.iNaT
     elif issubclass(values.dtype.type, np.timedelta64):
         result = -np.isfinite(values.view('i8'))
     else:
@@ -168,43 +170,43 @@ def _view_wrapper(f, wrap_dtype, na_override=None):
 
 
 _take1d_dict = {
-    'float64': _algos.take_1d_float64,
-    'int32': _algos.take_1d_int32,
-    'int64': _algos.take_1d_int64,
-    'object': _algos.take_1d_object,
-    'bool': _view_wrapper(_algos.take_1d_bool, np.uint8),
-    'datetime64[ns]': _view_wrapper(_algos.take_1d_int64, np.int64,
-                                    na_override=lib.iNaT),
+    'float64': algos.take_1d_float64,
+    'int32': algos.take_1d_int32,
+    'int64': algos.take_1d_int64,
+    'object': algos.take_1d_object,
+    'bool': _view_wrapper(algos.take_1d_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(algos.take_1d_int64, np.int64,
+                                    na_override=tslib.iNaT),
 }
 
 _take2d_axis0_dict = {
-    'float64': _algos.take_2d_axis0_float64,
-    'int32': _algos.take_2d_axis0_int32,
-    'int64': _algos.take_2d_axis0_int64,
-    'object': _algos.take_2d_axis0_object,
-    'bool': _view_wrapper(_algos.take_2d_axis0_bool, np.uint8),
-    'datetime64[ns]': _view_wrapper(_algos.take_2d_axis0_int64, np.int64,
-                                    na_override=lib.iNaT),
+    'float64': algos.take_2d_axis0_float64,
+    'int32': algos.take_2d_axis0_int32,
+    'int64': algos.take_2d_axis0_int64,
+    'object': algos.take_2d_axis0_object,
+    'bool': _view_wrapper(algos.take_2d_axis0_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(algos.take_2d_axis0_int64, np.int64,
+                                    na_override=tslib.iNaT),
 }
 
 _take2d_axis1_dict = {
-    'float64': _algos.take_2d_axis1_float64,
-    'int32': _algos.take_2d_axis1_int32,
-    'int64': _algos.take_2d_axis1_int64,
-    'object': _algos.take_2d_axis1_object,
-    'bool': _view_wrapper(_algos.take_2d_axis1_bool, np.uint8),
-    'datetime64[ns]': _view_wrapper(_algos.take_2d_axis1_int64, np.int64,
-                                     na_override=lib.iNaT),
+    'float64': algos.take_2d_axis1_float64,
+    'int32': algos.take_2d_axis1_int32,
+    'int64': algos.take_2d_axis1_int64,
+    'object': algos.take_2d_axis1_object,
+    'bool': _view_wrapper(algos.take_2d_axis1_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(algos.take_2d_axis1_int64, np.int64,
+                                     na_override=tslib.iNaT),
 }
 
 _take2d_multi_dict = {
-    'float64': _algos.take_2d_multi_float64,
-    'int32': _algos.take_2d_multi_int32,
-    'int64': _algos.take_2d_multi_int64,
-    'object': _algos.take_2d_multi_object,
-    'bool': _view_wrapper(_algos.take_2d_multi_bool, np.uint8),
-    'datetime64[ns]': _view_wrapper(_algos.take_2d_multi_int64, np.int64,
-                                    na_override=lib.iNaT),
+    'float64': algos.take_2d_multi_float64,
+    'int32': algos.take_2d_multi_int32,
+    'int64': algos.take_2d_multi_int64,
+    'object': algos.take_2d_multi_object,
+    'bool': _view_wrapper(algos.take_2d_multi_bool, np.uint8),
+    'datetime64[ns]': _view_wrapper(algos.take_2d_multi_int64, np.int64,
+                                    na_override=tslib.iNaT),
 }
 
 
@@ -369,9 +371,9 @@ def mask_out_axis(arr, mask, axis, fill_value=np.nan):
     arr[tuple(indexer)] = fill_value
 
 _diff_special = {
-    'float64': lib.diff_2d_float64,
-    'int64': lib.diff_2d_int64,
-    'int32': lib.diff_2d_int32
+    'float64': algos.diff_2d_float64,
+    'int64': algos.diff_2d_int64,
+    'int32': algos.diff_2d_int32
 }
 
 def diff(arr, n, axis=0):
@@ -452,21 +454,21 @@ def _interp_wrapper(f, wrap_dtype, na_override=None):
         f(view, mask, limit=limit)
     return wrapper
 
-_pad_1d_datetime = _interp_wrapper(_algos.pad_inplace_int64, np.int64)
-_pad_2d_datetime = _interp_wrapper(_algos.pad_2d_inplace_int64, np.int64)
-_backfill_1d_datetime = _interp_wrapper(_algos.backfill_inplace_int64,
+_pad_1d_datetime = _interp_wrapper(algos.pad_inplace_int64, np.int64)
+_pad_2d_datetime = _interp_wrapper(algos.pad_2d_inplace_int64, np.int64)
+_backfill_1d_datetime = _interp_wrapper(algos.backfill_inplace_int64,
                                         np.int64)
-_backfill_2d_datetime = _interp_wrapper(_algos.backfill_2d_inplace_int64,
+_backfill_2d_datetime = _interp_wrapper(algos.backfill_2d_inplace_int64,
                                         np.int64)
 
 
 def pad_1d(values, limit=None, mask=None):
     if is_float_dtype(values):
-        _method = _algos.pad_inplace_float64
+        _method = algos.pad_inplace_float64
     elif is_datetime64_dtype(values):
         _method = _pad_1d_datetime
     elif values.dtype == np.object_:
-        _method = _algos.pad_inplace_object
+        _method = algos.pad_inplace_object
     else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
@@ -478,11 +480,11 @@ def pad_1d(values, limit=None, mask=None):
 
 def backfill_1d(values, limit=None, mask=None):
     if is_float_dtype(values):
-        _method = _algos.backfill_inplace_float64
+        _method = algos.backfill_inplace_float64
     elif is_datetime64_dtype(values):
         _method = _backfill_1d_datetime
     elif values.dtype == np.object_:
-        _method = _algos.backfill_inplace_object
+        _method = algos.backfill_inplace_object
     else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
@@ -495,11 +497,11 @@ def backfill_1d(values, limit=None, mask=None):
 
 def pad_2d(values, limit=None, mask=None):
     if is_float_dtype(values):
-        _method = _algos.pad_2d_inplace_float64
+        _method = algos.pad_2d_inplace_float64
     elif is_datetime64_dtype(values):
         _method = _pad_2d_datetime
     elif values.dtype == np.object_:
-        _method = _algos.pad_2d_inplace_object
+        _method = algos.pad_2d_inplace_object
     else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
@@ -516,11 +518,11 @@ def pad_2d(values, limit=None, mask=None):
 
 def backfill_2d(values, limit=None, mask=None):
     if is_float_dtype(values):
-        _method = _algos.backfill_2d_inplace_float64
+        _method = algos.backfill_2d_inplace_float64
     elif is_datetime64_dtype(values):
         _method = _backfill_2d_datetime
     elif values.dtype == np.object_:
-        _method = _algos.backfill_2d_inplace_object
+        _method = algos.backfill_2d_inplace_object
     else:  # pragma: no cover
         raise ValueError('Invalid dtype for padding')
 
@@ -903,11 +905,11 @@ def _is_sequence(x):
     except Exception:
         return False
 
-_ensure_float64 = _algos.ensure_float64
-_ensure_int64 = _algos.ensure_int64
-_ensure_int32 = _algos.ensure_int32
-_ensure_platform_int = _algos.ensure_platform_int
-_ensure_object = _algos.ensure_object
+_ensure_float64 = algos.ensure_float64
+_ensure_int64 = algos.ensure_int64
+_ensure_int32 = algos.ensure_int32
+_ensure_platform_int = algos.ensure_platform_int
+_ensure_object = algos.ensure_object
 
 
 def _astype_nansafe(arr, dtype):
@@ -916,7 +918,7 @@ def _astype_nansafe(arr, dtype):
 
     if issubclass(arr.dtype.type, np.datetime64):
         if dtype == object:
-            return lib.ints_to_pydatetime(arr.view(np.int64))
+            return tslib.ints_to_pydatetime(arr.view(np.int64))
     elif (np.issubdtype(arr.dtype, np.floating) and
         np.issubdtype(dtype, np.integer)):
 
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 3ff663e2e..acc7460db 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -14,7 +14,9 @@ from pandas.util.compat import OrderedDict
 from pandas.util.decorators import Appender
 import pandas.core.algorithms as algos
 import pandas.core.common as com
+
 import pandas.lib as lib
+import pandas.algos as _algos
 
 _agg_doc = """Aggregate using input function or dict of {column -> function}
 
@@ -706,21 +708,21 @@ class Grouper(object):
     # Aggregation functions
 
     _cython_functions = {
-        'add': lib.group_add,
-        'prod': lib.group_prod,
-        'min': lib.group_min,
-        'max': lib.group_max,
-        'mean': lib.group_mean,
-        'median': lib.group_median,
-        'var': lib.group_var,
-        'std': lib.group_var,
-        'first': lambda a, b, c, d: lib.group_nth(a, b, c, d, 1),
-        'last': lib.group_last
+        'add': _algos.group_add,
+        'prod': _algos.group_prod,
+        'min': _algos.group_min,
+        'max': _algos.group_max,
+        'mean': _algos.group_mean,
+        'median': _algos.group_median,
+        'var': _algos.group_var,
+        'std': _algos.group_var,
+        'first': lambda a, b, c, d: _algos.group_nth(a, b, c, d, 1),
+        'last': _algos.group_last
     }
 
     _cython_object_functions = {
-        'first': lambda a, b, c, d: lib.group_nth_object(a, b, c, d, 1),
-        'last': lib.group_last_object
+        'first': lambda a, b, c, d: _algos.group_nth_object(a, b, c, d, 1),
+        'last': _algos.group_last_object
     }
 
     _cython_transforms = {
@@ -830,7 +832,7 @@ class Grouper(object):
 
         # avoids object / Series creation overhead
         dummy = obj[:0].copy()
-        indexer = lib.groupsort_indexer(group_index, ngroups)[0]
+        indexer = _algos.groupsort_indexer(group_index, ngroups)[0]
         obj = obj.take(indexer)
         group_index = com.ndtake(group_index, indexer)
         grouper = lib.SeriesGrouper(obj, func, group_index, ngroups,
@@ -970,21 +972,21 @@ class BinGrouper(Grouper):
     # cython aggregation
 
     _cython_functions = {
-        'add': lib.group_add_bin,
-        'prod': lib.group_prod_bin,
-        'mean': lib.group_mean_bin,
-        'min': lib.group_min_bin,
-        'max': lib.group_max_bin,
-        'var': lib.group_var_bin,
-        'std': lib.group_var_bin,
-        'ohlc': lib.group_ohlc,
-        'first': lambda a, b, c, d: lib.group_nth_bin(a, b, c, d, 1),
-        'last': lib.group_last_bin
+        'add': _algos.group_add_bin,
+        'prod': _algos.group_prod_bin,
+        'mean': _algos.group_mean_bin,
+        'min': _algos.group_min_bin,
+        'max': _algos.group_max_bin,
+        'var': _algos.group_var_bin,
+        'std': _algos.group_var_bin,
+        'ohlc': _algos.group_ohlc,
+        'first': lambda a, b, c, d: _algos.group_nth_bin(a, b, c, d, 1),
+        'last': _algos.group_last_bin
     }
 
     _cython_object_functions = {
-        'first': lambda a, b, c, d: lib.group_nth_bin_object(a, b, c, d, 1),
-        'last': lib.group_last_bin_object
+        'first': lambda a, b, c, d: _algos.group_nth_bin_object(a, b, c, d, 1),
+        'last': _algos.group_last_bin_object
     }
 
     _name_functions = {
@@ -2020,7 +2022,7 @@ def generate_groups(data, group_index, ngroups, axis=0, factory=lambda x: x):
     """
     group_index = com._ensure_int64(group_index)
 
-    indexer = lib.groupsort_indexer(group_index, ngroups)[0]
+    indexer = _algos.groupsort_indexer(group_index, ngroups)[0]
     group_index = com.ndtake(group_index, indexer)
 
     if isinstance(data, BlockManager):
@@ -2119,7 +2121,7 @@ def _indexer_from_factorized(labels, shape, compress=True):
         comp_ids = group_index
         max_group = np.prod(shape)
 
-    indexer, _ = lib.groupsort_indexer(comp_ids.astype(np.int64), max_group)
+    indexer, _ = _algos.groupsort_indexer(comp_ids.astype(np.int64), max_group)
 
     return indexer
 
@@ -2134,7 +2136,7 @@ def _lexsort_indexer(keys, orders=None):
         orders = [True] * len(keys)
 
     for key, order in zip(keys, orders):
-        rizer = lib.Factorizer(len(key))
+        rizer = _hash.Factorizer(len(key))
 
         if not key.dtype == np.object_:
             key = key.astype('O')
@@ -2162,7 +2164,7 @@ class _KeyMapper(object):
         self.comp_ids = comp_ids.astype(np.int64)
 
         self.k = len(labels)
-        self.tables = [lib.Int64HashTable(ngroups) for _ in range(self.k)]
+        self.tables = [_hash.Int64HashTable(ngroups) for _ in range(self.k)]
 
         self._populate_tables()
 
@@ -2179,8 +2181,8 @@ def _get_indices_dict(label_list, keys):
     shape = [len(x) for x in keys]
     group_index = get_group_index(label_list, shape)
 
-    sorter, _ = lib.groupsort_indexer(com._ensure_int64(group_index),
-                                      np.prod(shape))
+    sorter, _ = _algos.groupsort_indexer(com._ensure_int64(group_index),
+                                         np.prod(shape))
 
     sorter_int = com._ensure_platform_int(sorter)
 
@@ -2200,7 +2202,7 @@ def _compress_group_index(group_index, sort=True):
     (comp_ids) into the list of unique labels (obs_group_ids).
     """
 
-    table = lib.Int64HashTable(min(1000000, len(group_index)))
+    table = _hash.Int64HashTable(min(1000000, len(group_index)))
 
     group_index = com._ensure_int64(group_index)
 
@@ -2263,7 +2265,7 @@ def _intercept_cython(func):
 
 
 def _groupby_indices(values):
-    return lib.groupby_indices(com._ensure_object(values))
+    return _algos.groupby_indices(com._ensure_object(values))
 
 
 def numpy_groupby(data, labels, axis=0):
diff --git a/pandas/core/index.py b/pandas/core/index.py
index fb68d1505..f214187b4 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -6,15 +6,19 @@ from itertools import izip
 
 import numpy as np
 
+import pandas.tslib as tslib
+import pandas.lib as lib
+import pandas.algos as _algos
+import pandas.hashtable as _hash
+import pandas.index as _index
+from pandas.lib import Timestamp
+
 from pandas.core.common import ndtake
 from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
-import pandas.lib as lib
-import pandas._algos as _algos
-from pandas.lib import Timestamp
 from pandas.util import py3compat
 from pandas.core.config import get_option
-import pandas._hash as _hash
+
 
 __all__ = ['Index']
 
@@ -41,8 +45,9 @@ _o_dtype = np.dtype(object)
 
 
 def _shouldbe_timestamp(obj):
-    return (lib.is_datetime_array(obj) or lib.is_datetime64_array(obj)
-            or lib.is_timestamp_array(obj))
+    return (tslib.is_datetime_array(obj)
+            or tslib.is_datetime64_array(obj)
+            or tslib.is_timestamp_array(obj))
 
 
 class Index(np.ndarray):
@@ -79,7 +84,7 @@ class Index(np.ndarray):
     name = None
     asi8 = None
 
-    _engine_type = _hash.ObjectEngine
+    _engine_type = _index.ObjectEngine
 
     def __new__(cls, data, dtype=None, copy=False, name=None):
         if isinstance(data, np.ndarray):
@@ -114,7 +119,7 @@ class Index(np.ndarray):
                 return Int64Index(subarr.astype('i8'), name=name)
             elif inferred != 'string':
                 if (inferred.startswith('datetime') or
-                    lib.is_timestamp_array(subarr)):
+                    tslib.is_timestamp_array(subarr)):
                     from pandas.tseries.index import DatetimeIndex
                     return DatetimeIndex(subarr, copy=copy, name=name)
 
@@ -198,7 +203,7 @@ class Index(np.ndarray):
         if self.inferred_type == 'string':
             from dateutil.parser import parse
             parser = lambda x: parse(x, dayfirst=dayfirst)
-            parsed = lib.try_parse_dates(self.values, parser=parser)
+            parsed = tslib.try_parse_dates(self.values, parser=parser)
             return DatetimeIndex(parsed)
         else:
             return DatetimeIndex(self.values)
@@ -1202,7 +1207,7 @@ class Int64Index(Index):
     _inner_indexer = _algos.inner_join_indexer_int64
     _outer_indexer = _algos.outer_join_indexer_int64
 
-    _engine_type = _hash.Int64Engine
+    _engine_type = _index.Int64Engine
 
     def __new__(cls, data, dtype=None, copy=False, name=None):
         if not isinstance(data, np.ndarray):
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 4bb7cb871..fedaf7648 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -29,9 +29,11 @@ import pandas.core.datetools as datetools
 import pandas.core.format as fmt
 import pandas.core.generic as generic
 import pandas.core.nanops as nanops
-import pandas.lib as lib
 from pandas.util.decorators import Appender, Substitution, cache_readonly
 
+import pandas.lib as lib
+import pandas.index as _index
+
 from pandas.compat.scipy import scoreatpercentile as _quantile
 from pandas.core.config import get_option
 
@@ -140,7 +142,7 @@ def _comp_method(op, name):
                           index=self.index, name=self.name)
         else:
             values = self.values
-            other = lib.convert_scalar(values, other)
+            other = _index.convert_scalar(values, other)
 
             if issubclass(values.dtype.type, np.datetime64):
                 values = values.view('i8')
@@ -697,7 +699,7 @@ copy : boolean, default False
 
     def _set_values(self, key, value):
         values = self.values
-        values[key] = lib.convert_scalar(values, value)
+        values[key] = _index.convert_scalar(values, value)
 
     # help out SparseSeries
     _get_val_at = ndarray.__getitem__
diff --git a/pandas/src/hashtable.pxd b/pandas/hashtable.pxd
similarity index 100%
rename from pandas/src/hashtable.pxd
rename to pandas/hashtable.pxd
diff --git a/pandas/src/hashtable.pyx b/pandas/hashtable.pyx
similarity index 99%
rename from pandas/src/hashtable.pyx
rename to pandas/hashtable.pyx
index 2fd366b22..83473d9ae 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/hashtable.pyx
@@ -18,7 +18,7 @@ cnp.import_ufunc()
 
 cdef int64_t iNaT = util.get_nat()
 
-import _algos
+import algos
 
 cdef extern from "datetime.h":
     bint PyDateTime_Check(object o)
diff --git a/pandas/src/engines.pyx b/pandas/index.pyx
similarity index 91%
rename from pandas/src/engines.pyx
rename to pandas/index.pyx
index a21824ff6..b7934882f 100644
--- a/pandas/src/engines.pyx
+++ b/pandas/index.pyx
@@ -13,11 +13,11 @@ cimport util
 
 import numpy as np
 
-import _algos
+import algos
 
-cimport _tseries
-from _tseries import Timestamp
-import _tseries
+cimport tslib
+from tslib import Timestamp
+import tslib
 
 from hashtable cimport *
 import hashtable as _hash
@@ -273,15 +273,15 @@ cdef class Int64Engine(IndexEngine):
         return _hash.Int64HashTable(n)
 
     def _call_monotonic(self, values):
-        return _algos.is_monotonic_int64(values)
+        return algos.is_monotonic_int64(values)
 
     def get_pad_indexer(self, other, limit=None):
-        return _algos.pad_int64(self._get_index_values(), other,
-                                  limit=limit)
+        return algos.pad_int64(self._get_index_values(), other,
+                               limit=limit)
 
     def get_backfill_indexer(self, other, limit=None):
-        return _algos.backfill_int64(self._get_index_values(), other,
-                                       limit=limit)
+        return algos.backfill_int64(self._get_index_values(), other,
+                                    limit=limit)
 
     cdef _check_type(self, object val):
         hash(val)
@@ -329,14 +329,14 @@ cdef class Float64Engine(IndexEngine):
         return _hash.Float64HashTable(n)
 
     def _call_monotonic(self, values):
-        return _algos.is_monotonic_float64(values)
+        return algos.is_monotonic_float64(values)
 
     def get_pad_indexer(self, other, limit=None):
-        return _algos.pad_float64(self._get_index_values(), other,
+        return algos.pad_float64(self._get_index_values(), other,
                                     limit=limit)
 
     def get_backfill_indexer(self, other, limit=None):
-        return _algos.backfill_float64(self._get_index_values(), other,
+        return algos.backfill_float64(self._get_index_values(), other,
                                          limit=limit)
 
 
@@ -366,15 +366,15 @@ cdef Py_ssize_t _bin_search(ndarray values, object val):
         return mid + 1
 
 _pad_functions = {
-    'object' : _algos.pad_object,
-    'int64' : _algos.pad_int64,
-    'float64' : _algos.pad_float64
+    'object' : algos.pad_object,
+    'int64' : algos.pad_int64,
+    'float64' : algos.pad_float64
 }
 
 _backfill_functions = {
-    'object': _algos.backfill_object,
-    'int64': _algos.backfill_int64,
-    'float64': _algos.backfill_float64
+    'object': algos.backfill_object,
+    'int64': algos.backfill_int64,
+    'float64': algos.backfill_float64
 }
 
 cdef class ObjectEngine(IndexEngine):
@@ -383,14 +383,14 @@ cdef class ObjectEngine(IndexEngine):
         return _hash.PyObjectHashTable(n)
 
     def _call_monotonic(self, values):
-        return _algos.is_monotonic_object(values)
+        return algos.is_monotonic_object(values)
 
     def get_pad_indexer(self, other, limit=None):
-        return _algos.pad_object(self._get_index_values(), other,
+        return algos.pad_object(self._get_index_values(), other,
                                    limit=limit)
 
     def get_backfill_indexer(self, other, limit=None):
-        return _algos.backfill_object(self._get_index_values(), other,
+        return algos.backfill_object(self._get_index_values(), other,
                                         limit=limit)
 
 
@@ -412,7 +412,7 @@ cdef class DatetimeEngine(Int64Engine):
         return self.vgetter().view('i8')
 
     def _call_monotonic(self, values):
-        return _algos.is_monotonic_int64(values)
+        return algos.is_monotonic_int64(values)
 
     cpdef get_loc(self, object val):
         if is_definitely_invalid_key(val):
@@ -466,18 +466,18 @@ cdef class DatetimeEngine(Int64Engine):
         if other.dtype != 'M8[ns]':
             return np.repeat(-1, len(other)).astype('i4')
         other = np.asarray(other).view('i8')
-        return _algos.pad_int64(self._get_index_values(), other,
+        return algos.pad_int64(self._get_index_values(), other,
                                 limit=limit)
 
     def get_backfill_indexer(self, other, limit=None):
         if other.dtype != 'M8[ns]':
             return np.repeat(-1, len(other)).astype('i4')
         other = np.asarray(other).view('i8')
-        return _algos.backfill_int64(self._get_index_values(), other,
+        return algos.backfill_int64(self._get_index_values(), other,
                                      limit=limit)
 
 
-cdef convert_scalar(ndarray arr, object value):
+cpdef convert_scalar(ndarray arr, object value):
     if arr.descr.type_num == NPY_DATETIME:
         if isinstance(value, Timestamp):
             return value.value
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index 371d2697c..74a02301e 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -31,8 +31,6 @@ from pandas.tools.merge import concat
 import pandas.lib as lib
 from contextlib import contextmanager
 
-import pandas._pytables as pylib
-
 # reading and writing the full object in one go
 _TYPE_MAP = {
     Series: 'series',
@@ -1444,7 +1442,7 @@ class LegacyTable(Table):
 
         # get our function
         try:
-            func = getattr(pylib,"create_hdf_rows_%sd" % self.ndim)
+            func = getattr(lib,"create_hdf_rows_%sd" % self.ndim)
             args.append(mask)
             args.append(values)
             rows = func(*args)
diff --git a/pandas/src/lib.pyx b/pandas/lib.pyx
similarity index 87%
rename from pandas/src/lib.pyx
rename to pandas/lib.pyx
index 89ab6cf59..b435eefe2 100644
--- a/pandas/src/lib.pyx
+++ b/pandas/lib.pyx
@@ -28,8 +28,9 @@ from datetime import datetime as pydatetime
 # this is our tseries.pxd
 from datetime cimport *
 
-from _tseries cimport convert_to_tsobject
-import _tseries
+from tslib cimport convert_to_tsobject
+import tslib
+from tslib import NaT, Timestamp
 
 cdef int64_t NPY_NAT = util.get_nat()
 
@@ -161,7 +162,6 @@ def time64_to_datetime(ndarray[int64_t, ndim=1] arr):
 cdef double INF = <double> np.inf
 cdef double NEGINF = -INF
 
-from _tseries import NaT
 
 cpdef checknull(object val):
     if util.is_float_object(val) or util.is_complex_object(val):
@@ -685,6 +685,91 @@ def clean_index_list(list obj):
 
     return maybe_convert_objects(converted), 0
 
+from cpython cimport (PyDict_New, PyDict_GetItem, PyDict_SetItem,
+                      PyDict_Contains, PyDict_Keys,
+                      Py_INCREF, PyTuple_SET_ITEM,
+                      PyTuple_SetItem,
+                      PyTuple_New,
+                      PyObject_SetAttrString)
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def create_hdf_rows_2d(ndarray index, ndarray[np.uint8_t, ndim=1] mask,
+                       list values):
+    """ return a list of objects ready to be converted to rec-array format """
+
+    cdef:
+        unsigned int i, b, n_index, n_blocks, tup_size
+        ndarray v
+        list l
+        object tup, val
+
+    n_index   = index.shape[0]
+    n_blocks  = len(values)
+    tup_size  = n_blocks+1
+    l = []
+    for i from 0 <= i < n_index:
+
+        if not mask[i]:
+
+            tup = PyTuple_New(tup_size)
+            val  = index[i]
+            PyTuple_SET_ITEM(tup, 0, val)
+            Py_INCREF(val)
+
+            for b from 0 <= b < n_blocks:
+
+                v   = values[b][:, i]
+                PyTuple_SET_ITEM(tup, b+1, v)
+                Py_INCREF(v)
+
+            l.append(tup)
+
+    return l
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def create_hdf_rows_3d(ndarray index, ndarray columns,
+                       ndarray[np.uint8_t, ndim=2] mask, list values):
+    """ return a list of objects ready to be converted to rec-array format """
+
+    cdef:
+        unsigned int i, j, n_columns, n_index, n_blocks, tup_size
+        ndarray v
+        list l
+        object tup, val
+
+    n_index   = index.shape[0]
+    n_columns = columns.shape[0]
+    n_blocks  = len(values)
+    tup_size  = n_blocks+2
+    l = []
+    for i from 0 <= i < n_index:
+
+        for c from 0 <= c < n_columns:
+
+            if not mask[i, c]:
+
+                tup = PyTuple_New(tup_size)
+
+                val  = columns[c]
+                PyTuple_SET_ITEM(tup, 0, val)
+                Py_INCREF(val)
+
+                val  = index[i]
+                PyTuple_SET_ITEM(tup, 1, val)
+                Py_INCREF(val)
+
+                for b from 0 <= b < n_blocks:
+
+                    v   = values[b][:, i, c]
+                    PyTuple_SET_ITEM(tup, b+2, v)
+                    Py_INCREF(v)
+
+                l.append(tup)
+
+    return l
+
 
 include "groupby.pyx"
 include "reindex.pyx"
diff --git a/pandas/src/generate_code.py b/pandas/src/generate_code.py
index 27b8d8d63..c7897e7de 100644
--- a/pandas/src/generate_code.py
+++ b/pandas/src/generate_code.py
@@ -17,8 +17,6 @@ cimport cpython
 
 import numpy as np
 isnan = np.isnan
-cdef double NaN = <double> np.NaN
-cdef double nan = NaN
 
 from datetime import datetime as pydatetime
 
@@ -27,9 +25,6 @@ from datetime cimport *
 
 from khash cimport *
 
-cdef inline int int_max(int a, int b): return a if a >= b else b
-cdef inline int int_min(int a, int b): return a if a <= b else b
-
 ctypedef unsigned char UChar
 
 cimport util
diff --git a/pandas/src/generated.pyx b/pandas/src/generated.pyx
index 24b3eae41..5ecd8439a 100644
--- a/pandas/src/generated.pyx
+++ b/pandas/src/generated.pyx
@@ -14,8 +14,6 @@ cimport cpython
 
 import numpy as np
 isnan = np.isnan
-cdef double NaN = <double> np.NaN
-cdef double nan = NaN
 
 from datetime import datetime as pydatetime
 
@@ -24,9 +22,6 @@ from datetime cimport *
 
 from khash cimport *
 
-cdef inline int int_max(int a, int b): return a if a >= b else b
-cdef inline int int_min(int a, int b): return a if a <= b else b
-
 ctypedef unsigned char UChar
 
 cimport util
diff --git a/pandas/src/pytables.pyx b/pandas/src/pytables.pyx
deleted file mode 100644
index b4dc4f599..000000000
--- a/pandas/src/pytables.pyx
+++ /dev/null
@@ -1,97 +0,0 @@
-### pytables extensions ###
-
-from numpy cimport ndarray, int32_t, float64_t, int64_t
-cimport numpy as np
-
-cimport cython
-
-import numpy as np
-import operator
-import sys
-
-np.import_array()
-np.import_ufunc()
-
-
-from cpython cimport (PyDict_New, PyDict_GetItem, PyDict_SetItem,
-                      PyDict_Contains, PyDict_Keys,
-                      Py_INCREF, PyTuple_SET_ITEM,
-                      PyTuple_SetItem,
-                      PyTuple_New,
-                      PyObject_SetAttrString)
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def create_hdf_rows_2d(ndarray index, ndarray[np.uint8_t, ndim=1] mask, list values):
-    """ return a list of objects ready to be converted to rec-array format """
-
-    cdef:
-        unsigned int i, b, n_index, n_blocks, tup_size
-        ndarray v
-        list l
-        object tup, val
-
-    n_index   = index.shape[0]
-    n_blocks  = len(values)
-    tup_size  = n_blocks+1
-    l = []
-    for i from 0 <= i < n_index:
-        
-        if not mask[i]:
-
-            tup = PyTuple_New(tup_size)
-            val  = index[i]
-            PyTuple_SET_ITEM(tup, 0, val)
-            Py_INCREF(val)
-
-            for b from 0 <= b < n_blocks:
-
-                v   = values[b][:, i]
-                PyTuple_SET_ITEM(tup, b+1, v)
-                Py_INCREF(v)
-
-            l.append(tup)
-
-    return l
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def create_hdf_rows_3d(ndarray index, ndarray columns, ndarray[np.uint8_t, ndim=2] mask, list values):
-    """ return a list of objects ready to be converted to rec-array format """
-
-    cdef:
-        unsigned int i, j, n_columns, n_index, n_blocks, tup_size
-        ndarray v
-        list l
-        object tup, val
-
-    n_index   = index.shape[0]
-    n_columns = columns.shape[0]
-    n_blocks  = len(values)
-    tup_size  = n_blocks+2
-    l = []
-    for i from 0 <= i < n_index:
-
-        for c from 0 <= c < n_columns:
-
-            if not mask[i, c]:
-
-                tup = PyTuple_New(tup_size)
-
-                val  = columns[c]
-                PyTuple_SET_ITEM(tup, 0, val)
-                Py_INCREF(val)
-
-                val  = index[i]
-                PyTuple_SET_ITEM(tup, 1, val)
-                Py_INCREF(val)
-
-                for b from 0 <= b < n_blocks:
-
-                    v   = values[b][:, i, c]
-                    PyTuple_SET_ITEM(tup, b+2, v)
-                    Py_INCREF(v)
-
-                l.append(tup)
-
-    return l
diff --git a/pandas/stats/moments.py b/pandas/stats/moments.py
index b805a9dca..a76f30c8b 100644
--- a/pandas/stats/moments.py
+++ b/pandas/stats/moments.py
@@ -10,7 +10,7 @@ from numpy import NaN
 import numpy as np
 
 from pandas.core.api import DataFrame, Series, notnull
-import pandas.lib as lib
+import pandas.algos as algos
 
 from pandas.util.decorators import Substitution, Appender
 
@@ -310,7 +310,7 @@ def ewma(arg, com=None, span=None, min_periods=0, freq=None, time_rule=None,
     arg = _conv_timerule(arg, freq, time_rule)
 
     def _ewma(v):
-        result = lib.ewma(v, com, int(adjust))
+        result = algos.ewma(v, com, int(adjust))
         first_index = _first_valid_index(v)
         result[first_index : first_index + min_periods] = NaN
         return result
@@ -459,20 +459,20 @@ def _rolling_func(func, desc, check_minp=_use_window):
 
     return f
 
-rolling_max = _rolling_func(lib.roll_max2, 'Moving maximum')
-rolling_min = _rolling_func(lib.roll_min2, 'Moving minimum')
-rolling_sum = _rolling_func(lib.roll_sum, 'Moving sum')
-rolling_mean = _rolling_func(lib.roll_mean, 'Moving mean')
-rolling_median = _rolling_func(lib.roll_median_cython, 'Moving median')
+rolling_max = _rolling_func(algos.roll_max2, 'Moving maximum')
+rolling_min = _rolling_func(algos.roll_min2, 'Moving minimum')
+rolling_sum = _rolling_func(algos.roll_sum, 'Moving sum')
+rolling_mean = _rolling_func(algos.roll_mean, 'Moving mean')
+rolling_median = _rolling_func(algos.roll_median_cython, 'Moving median')
 
-_ts_std = lambda *a, **kw: _zsqrt(lib.roll_var(*a, **kw))
+_ts_std = lambda *a, **kw: _zsqrt(algos.roll_var(*a, **kw))
 rolling_std = _rolling_func(_ts_std, 'Unbiased moving standard deviation',
                             check_minp=_require_min_periods(1))
-rolling_var = _rolling_func(lib.roll_var, 'Unbiased moving variance',
+rolling_var = _rolling_func(algos.roll_var, 'Unbiased moving variance',
                             check_minp=_require_min_periods(1))
-rolling_skew = _rolling_func(lib.roll_skew, 'Unbiased moving skewness',
+rolling_skew = _rolling_func(algos.roll_skew, 'Unbiased moving skewness',
                              check_minp=_require_min_periods(3))
-rolling_kurt = _rolling_func(lib.roll_kurt, 'Unbiased moving kurtosis',
+rolling_kurt = _rolling_func(algos.roll_kurt, 'Unbiased moving kurtosis',
                              check_minp=_require_min_periods(4))
 
 
@@ -497,7 +497,7 @@ def rolling_quantile(arg, window, quantile, min_periods=None, freq=None,
 
     def call_cython(arg, window, minp):
         minp = _use_window(minp, window)
-        return lib.roll_quantile(arg, window, minp, quantile)
+        return algos.roll_quantile(arg, window, minp, quantile)
     return _rolling_moment(arg, window, call_cython, min_periods,
                            freq=freq, time_rule=time_rule)
 
@@ -523,7 +523,7 @@ def rolling_apply(arg, window, func, min_periods=None, freq=None,
     """
     def call_cython(arg, window, minp):
         minp = _use_window(minp, window)
-        return lib.roll_generic(arg, window, minp, func)
+        return algos.roll_generic(arg, window, minp, func)
     return _rolling_moment(arg, window, call_cython, min_periods,
                            freq=freq, time_rule=time_rule)
 
@@ -543,20 +543,20 @@ def _expanding_func(func, desc, check_minp=_use_window):
 
     return f
 
-expanding_max = _expanding_func(lib.roll_max2, 'Expanding maximum')
-expanding_min = _expanding_func(lib.roll_min2, 'Expanding minimum')
-expanding_sum = _expanding_func(lib.roll_sum, 'Expanding sum')
-expanding_mean = _expanding_func(lib.roll_mean, 'Expanding mean')
-expanding_median = _expanding_func(lib.roll_median_cython, 'Expanding median')
+expanding_max = _expanding_func(algos.roll_max2, 'Expanding maximum')
+expanding_min = _expanding_func(algos.roll_min2, 'Expanding minimum')
+expanding_sum = _expanding_func(algos.roll_sum, 'Expanding sum')
+expanding_mean = _expanding_func(algos.roll_mean, 'Expanding mean')
+expanding_median = _expanding_func(algos.roll_median_cython, 'Expanding median')
 
 expanding_std = _expanding_func(_ts_std,
                                 'Unbiased expanding standard deviation',
                                 check_minp=_require_min_periods(2))
-expanding_var = _expanding_func(lib.roll_var, 'Unbiased expanding variance',
+expanding_var = _expanding_func(algos.roll_var, 'Unbiased expanding variance',
                             check_minp=_require_min_periods(2))
-expanding_skew = _expanding_func(lib.roll_skew, 'Unbiased expanding skewness',
+expanding_skew = _expanding_func(algos.roll_skew, 'Unbiased expanding skewness',
                              check_minp=_require_min_periods(3))
-expanding_kurt = _expanding_func(lib.roll_kurt, 'Unbiased expanding kurtosis',
+expanding_kurt = _expanding_func(algos.roll_kurt, 'Unbiased expanding kurtosis',
                              check_minp=_require_min_periods(4))
 
 
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 9061402bb..ce8de2af4 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -6,7 +6,7 @@ from pandas import Index, isnull
 from pandas.util.testing import assert_almost_equal
 import pandas.util.testing as common
 import pandas.lib as lib
-import pandas._algos as algos
+import pandas.algos as algos
 from datetime import datetime
 
 class TestTseriesUtil(unittest.TestCase):
@@ -78,7 +78,7 @@ def test_left_outer_join_bug():
     right = np.array([3, 1], dtype=np.int64)
     max_groups = 4
 
-    lidx, ridx = lib.left_outer_join(left, right, max_groups, sort=False)
+    lidx, ridx = algos.left_outer_join(left, right, max_groups, sort=False)
 
     exp_lidx = np.arange(len(left))
     exp_ridx = -np.ones(len(left))
@@ -219,7 +219,7 @@ def test_is_lexsorted():
        21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,
         4,  3,  2,  1,  0])]
 
-    assert(not lib.is_lexsorted(failure))
+    assert(not algos.is_lexsorted(failure))
 
 # def test_get_group_index():
 #     a = np.array([0, 1, 2, 0, 2, 1, 0, 0], dtype=np.int64)
@@ -234,7 +234,7 @@ def test_groupsort_indexer():
     a = np.random.randint(0, 1000, 100).astype(np.int64)
     b = np.random.randint(0, 1000, 100).astype(np.int64)
 
-    result = lib.groupsort_indexer(a, 1000)[0]
+    result = algos.groupsort_indexer(a, 1000)[0]
 
     # need to use a stable sort
     expected = np.argsort(a, kind='mergesort')
@@ -242,7 +242,7 @@ def test_groupsort_indexer():
 
     # compare with lexsort
     key = a * 1000 + b
-    result = lib.groupsort_indexer(key, 1000000)[0]
+    result = algos.groupsort_indexer(key, 1000000)[0]
     expected = np.lexsort((b, a))
     assert(np.array_equal(result, expected))
 
@@ -313,7 +313,7 @@ def test_rank():
     def _check(arr):
         mask = -np.isfinite(arr)
         arr = arr.copy()
-        result = lib.rank_1d_float64(arr)
+        result = algos.rank_1d_float64(arr)
         arr[mask] = np.inf
         exp = rankdata(arr)
         exp[mask] = nan
@@ -482,7 +482,7 @@ def test_group_ohlc():
     out  = np.zeros((3, 4), np.float64)
     counts = np.zeros(len(out), dtype=np.int64)
 
-    lib.group_ohlc(out, counts, obj[:, None], bins)
+    algos.group_ohlc(out, counts, obj[:, None], bins)
 
     def _ohlc(group):
         if isnull(group).all():
@@ -496,7 +496,7 @@ def test_group_ohlc():
     assert_almost_equal(counts, [6, 6, 8])
 
     obj[:6] = nan
-    lib.group_ohlc(out, counts, obj[:, None], bins)
+    algos.group_ohlc(out, counts, obj[:, None], bins)
     expected[0] = nan
     assert_almost_equal(out, expected)
 
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index fde0332be..ccfb6ed1d 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -20,6 +20,8 @@ from pandas.sparse.frame import SparseDataFrame
 import pandas.core.common as com
 
 import pandas.lib as lib
+import pandas.algos as algos
+import pandas.hashtable as _hash
 
 
 @Substitution('\nleft : DataFrame')
@@ -464,8 +466,8 @@ class _OrderedMerge(_MergeOperation):
         ldata, rdata = self._get_merge_data()
 
         if self.fill_method == 'ffill':
-            left_join_indexer = lib.ffill_indexer(left_indexer)
-            right_join_indexer = lib.ffill_indexer(right_indexer)
+            left_join_indexer = algos.ffill_indexer(left_indexer)
+            right_join_indexer = algos.ffill_indexer(right_indexer)
         else:
             left_join_indexer = left_indexer
             right_join_indexer = right_indexer
@@ -498,9 +500,9 @@ def _get_multiindex_indexer(join_keys, index, sort=False):
                         sort=False)
 
     left_indexer, right_indexer = \
-        lib.left_outer_join(com._ensure_int64(left_group_key),
-                            com._ensure_int64(right_group_key),
-                            max_groups, sort=False)
+        algos.left_outer_join(com._ensure_int64(left_group_key),
+                              com._ensure_int64(right_group_key),
+                              max_groups, sort=False)
 
     return left_indexer, right_indexer
 
@@ -509,9 +511,9 @@ def _get_single_indexer(join_key, index, sort=False):
     left_key, right_key, count = _factorize_keys(join_key, index, sort=sort)
 
     left_indexer, right_indexer = \
-        lib.left_outer_join(com._ensure_int64(left_key),
-                            com._ensure_int64(right_key),
-                            count, sort=sort)
+        algos.left_outer_join(com._ensure_int64(left_key),
+                              com._ensure_int64(right_key),
+                              count, sort=sort)
 
     return left_indexer, right_indexer
 
@@ -543,24 +545,24 @@ def _left_join_on_index(left_ax, right_ax, join_keys, sort=False):
 
 
 def _right_outer_join(x, y, max_groups):
-    right_indexer, left_indexer = lib.left_outer_join(y, x, max_groups)
+    right_indexer, left_indexer = algos.left_outer_join(y, x, max_groups)
     return left_indexer, right_indexer
 
 _join_functions = {
-    'inner': lib.inner_join,
-    'left': lib.left_outer_join,
+    'inner': algos.inner_join,
+    'left': algos.left_outer_join,
     'right': _right_outer_join,
-    'outer': lib.full_outer_join,
+    'outer': algos.full_outer_join,
 }
 
 
 def _factorize_keys(lk, rk, sort=True):
     if com._is_int_or_datetime_dtype(lk) and com._is_int_or_datetime_dtype(rk):
-        klass = lib.Int64Factorizer
+        klass = _hash.Int64Factorizer
         lk = com._ensure_int64(lk)
         rk = com._ensure_int64(rk)
     else:
-        klass = lib.Factorizer
+        klass = _hash.Factorizer
         lk = com._ensure_object(lk)
         rk = com._ensure_object(rk)
 
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 6572b6c47..06fa4bdb5 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -18,8 +18,9 @@ import pandas.tseries.tools as tools
 
 from pandas.lib import Timestamp
 import pandas.lib as lib
-import pandas._algos as _algos
-import pandas._hash as _hash
+import pandas.tslib as tslib
+import pandas.algos as _algos
+import pandas.index as _index
 
 
 def _utc():
@@ -36,7 +37,7 @@ def _field_accessor(name, field):
             utc = _utc()
             if self.tz is not utc:
                 values = self._local_timestamps()
-        return lib.get_date_field(values, field)
+        return tslib.get_date_field(values, field)
     f.__name__ = name
     return property(f)
 
@@ -135,7 +136,7 @@ class DatetimeIndex(Int64Index):
     # structured array cache for datetime fields
     _sarr_cache = None
 
-    _engine_type = _hash.DatetimeEngine
+    _engine_type = _index.DatetimeEngine
 
     offset = None
 
@@ -225,7 +226,7 @@ class DatetimeIndex(Int64Index):
                     verify_integrity = False
             else:
                 if data.dtype != _NS_DTYPE:
-                    subarr = lib.cast_to_nanoseconds(data)
+                    subarr = tslib.cast_to_nanoseconds(data)
                 else:
                     subarr = data
         elif data.dtype == _INT64_DTYPE:
@@ -257,7 +258,7 @@ class DatetimeIndex(Int64Index):
                     getattr(data, 'tz', None) is None):
                     # Convert tz-naive to UTC
                     ints = subarr.view('i8')
-                    subarr = lib.tz_localize_to_utc(ints, tz)
+                    subarr = tslib.tz_localize_to_utc(ints, tz)
 
                 subarr = subarr.view(_NS_DTYPE)
 
@@ -367,7 +368,7 @@ class DatetimeIndex(Int64Index):
                 index = _generate_regular_range(start, end, periods, offset)
 
             if tz is not None and getattr(index, 'tz', None) is None:
-                index = lib.tz_localize_to_utc(com._ensure_int64(index), tz)
+                index = tslib.tz_localize_to_utc(com._ensure_int64(index), tz)
                 index = index.view(_NS_DTYPE)
 
         index = index.view(cls)
@@ -384,11 +385,11 @@ class DatetimeIndex(Int64Index):
         utc = _utc()
 
         if self.is_monotonic:
-            return lib.tz_convert(self.asi8, utc, self.tz)
+            return tslib.tz_convert(self.asi8, utc, self.tz)
         else:
             values = self.asi8
             indexer = values.argsort()
-            result = lib.tz_convert(values.take(indexer), utc, self.tz)
+            result = tslib.tz_convert(values.take(indexer), utc, self.tz)
 
             n = len(indexer)
             reverse = np.empty(n, dtype=np.int_)
@@ -471,7 +472,7 @@ class DatetimeIndex(Int64Index):
 
     def _mpl_repr(self):
         # how to represent ourselves to matplotlib
-        return lib.ints_to_pydatetime(self.asi8, self.tz)
+        return tslib.ints_to_pydatetime(self.asi8, self.tz)
 
     def __repr__(self):
         from pandas.core.format import _format_datetime64
@@ -658,7 +659,7 @@ class DatetimeIndex(Int64Index):
         values = self.asi8
         if self.tz is not None and self.tz is not utc:
             values = self._local_timestamps()
-        return lib.get_time_micros(values)
+        return tslib.get_time_micros(values)
 
     @property
     def asobject(self):
@@ -689,7 +690,7 @@ class DatetimeIndex(Int64Index):
         -------
         datetimes : ndarray
         """
-        return lib.ints_to_pydatetime(self.asi8, tz=self.tz)
+        return tslib.ints_to_pydatetime(self.asi8, tz=self.tz)
 
     def to_period(self, freq=None):
         """
@@ -1031,12 +1032,12 @@ class DatetimeIndex(Int64Index):
             t1 = Timestamp(datetime(parsed.year, 1, 1))
             t2 = Timestamp(datetime(parsed.year, 12, 31))
         elif reso == 'month':
-            d = lib.monthrange(parsed.year, parsed.month)[1]
+            d = tslib.monthrange(parsed.year, parsed.month)[1]
             t1 = Timestamp(datetime(parsed.year, parsed.month, 1))
             t2 = Timestamp(datetime(parsed.year, parsed.month, d))
         elif reso == 'quarter':
             qe = (((parsed.month - 1) + 2) % 12) + 1  # two months ahead
-            d = lib.monthrange(parsed.year, qe)[1]   # at end of month
+            d = tslib.monthrange(parsed.year, qe)[1]   # at end of month
             t1 = Timestamp(datetime(parsed.year, parsed.month, 1))
             t2 = Timestamp(datetime(parsed.year, qe, d))
         else:
@@ -1208,7 +1209,7 @@ class DatetimeIndex(Int64Index):
         -------
         normalized : DatetimeIndex
         """
-        new_values = lib.date_normalize(self.asi8, self.tz)
+        new_values = tslib.date_normalize(self.asi8, self.tz)
         return DatetimeIndex(new_values, freq='infer', name=self.name,
                              tz=self.tz)
 
@@ -1252,7 +1253,7 @@ class DatetimeIndex(Int64Index):
         """
         Returns True if all of the dates are at midnight ("no time")
         """
-        return lib.dates_normalized(self.asi8, self.tz)
+        return tslib.dates_normalized(self.asi8, self.tz)
 
     def equals(self, other):
         """
@@ -1273,7 +1274,7 @@ class DatetimeIndex(Int64Index):
         if self.tz is not None:
             if other.tz is None:
                 return False
-            same_zone = lib.get_timezone(self.tz) == lib.get_timezone(other.tz)
+            same_zone = tslib.get_timezone(self.tz) == tslib.get_timezone(other.tz)
         else:
             if other.tz is not None:
                 return False
@@ -1340,7 +1341,7 @@ class DatetimeIndex(Int64Index):
         tz = tools._maybe_get_tz(tz)
 
         # Convert to UTC
-        new_dates = lib.tz_localize_to_utc(self.asi8, tz)
+        new_dates = tslib.tz_localize_to_utc(self.asi8, tz)
         new_dates = new_dates.view(_NS_DTYPE)
 
         return self._simple_new(new_dates, self.name, self.offset, tz)
@@ -1566,7 +1567,7 @@ def _to_m8(key):
         # this also converts strings
         key = Timestamp(key)
 
-    return np.int64(lib.pydt_to_i8(key)).view(_NS_DTYPE)
+    return np.int64(tslib.pydt_to_i8(key)).view(_NS_DTYPE)
 
 
 def _str_to_dt_array(arr, offset=None, dayfirst=None, yearfirst=None):
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 3e50ea0f0..c340ac261 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -5,6 +5,7 @@ from pandas.tseries.tools import to_datetime
 # import after tools, dateutil check
 from dateutil.relativedelta import relativedelta
 import pandas.lib as lib
+import pandas.tslib as tslib
 
 __all__ = ['Day', 'BusinessDay', 'BDay',
            'MonthBegin', 'BMonthBegin', 'MonthEnd', 'BMonthEnd',
@@ -363,7 +364,7 @@ class MonthEnd(DateOffset, CacheableOffset):
         other = datetime(other.year, other.month, other.day)
 
         n = self.n
-        _, days_in_month = lib.monthrange(other.year, other.month)
+        _, days_in_month = tslib.monthrange(other.year, other.month)
         if other.day != days_in_month:
             other = other + relativedelta(months=-1, day=31)
             if n <= 0:
@@ -373,7 +374,7 @@ class MonthEnd(DateOffset, CacheableOffset):
 
     @classmethod
     def onOffset(cls, dt):
-        days_in_month = lib.monthrange(dt.year, dt.month)[1]
+        days_in_month = tslib.monthrange(dt.year, dt.month)[1]
         return dt.day == days_in_month
 
     @property
@@ -413,7 +414,7 @@ class BusinessMonthEnd(CacheableOffset, DateOffset):
 
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, other.month)
+        wkday, days_in_month = tslib.monthrange(other.year, other.month)
         lastBDay = days_in_month - max(((wkday + days_in_month - 1) % 7) - 4, 0)
 
         if n > 0 and not other.day >= lastBDay:
@@ -437,7 +438,7 @@ class BusinessMonthBegin(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, _ = lib.monthrange(other.year, other.month)
+        wkday, _ = tslib.monthrange(other.year, other.month)
         first = _get_firstbday(wkday)
 
         if other.day > first and n <= 0:
@@ -448,14 +449,14 @@ class BusinessMonthBegin(DateOffset, CacheableOffset):
             n -= 1
 
         other = other + relativedelta(months=n)
-        wkday, _ = lib.monthrange(other.year, other.month)
+        wkday, _ = tslib.monthrange(other.year, other.month)
         first = _get_firstbday(wkday)
         result = datetime(other.year, other.month, first)
         return result
 
     @classmethod
     def onOffset(cls, dt):
-        first_weekday, _ = lib.monthrange(dt.year, dt.month)
+        first_weekday, _ = tslib.monthrange(dt.year, dt.month)
         if first_weekday == 5:
             return dt.day == 3
         elif first_weekday == 6:
@@ -628,7 +629,7 @@ class BQuarterEnd(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, other.month)
+        wkday, days_in_month = tslib.monthrange(other.year, other.month)
         lastBDay = days_in_month - max(((wkday + days_in_month - 1) % 7) - 4, 0)
 
         monthsToGo = 3 - ((other.month - self.startingMonth) % 3)
@@ -689,7 +690,7 @@ class BQuarterBegin(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, _ = lib.monthrange(other.year, other.month)
+        wkday, _ = tslib.monthrange(other.year, other.month)
 
         first = _get_firstbday(wkday)
 
@@ -707,7 +708,7 @@ class BQuarterBegin(DateOffset, CacheableOffset):
 
         # get the first bday for result
         other = other + relativedelta(months=3 * n - monthsSince)
-        wkday, _ = lib.monthrange(other.year, other.month)
+        wkday, _ = tslib.monthrange(other.year, other.month)
         first = _get_firstbday(wkday)
         result = datetime(other.year, other.month, first,
                           other.hour, other.minute, other.second,
@@ -741,7 +742,7 @@ class QuarterEnd(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, other.month)
+        wkday, days_in_month = tslib.monthrange(other.year, other.month)
 
         monthsToGo = 3 - ((other.month - self.startingMonth) % 3)
         if monthsToGo == 3:
@@ -780,7 +781,7 @@ class QuarterBegin(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, other.month)
+        wkday, days_in_month = tslib.monthrange(other.year, other.month)
 
         monthsSince = (other.month - self.startingMonth) % 3
 
@@ -816,7 +817,7 @@ class BYearEnd(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, self.month)
+        wkday, days_in_month = tslib.monthrange(other.year, self.month)
         lastBDay = (days_in_month -
                     max(((wkday + days_in_month - 1) % 7) - 4, 0))
 
@@ -832,7 +833,7 @@ class BYearEnd(DateOffset, CacheableOffset):
 
         other = other + relativedelta(years=years)
 
-        _, days_in_month = lib.monthrange(other.year, self.month)
+        _, days_in_month = tslib.monthrange(other.year, self.month)
         result = datetime(other.year, self.month, days_in_month,
                           other.hour, other.minute, other.second,
                           other.microsecond)
@@ -863,7 +864,7 @@ class BYearBegin(DateOffset, CacheableOffset):
     def apply(self, other):
         n = self.n
 
-        wkday, days_in_month = lib.monthrange(other.year, self.month)
+        wkday, days_in_month = tslib.monthrange(other.year, self.month)
 
         first = _get_firstbday(wkday)
 
@@ -880,7 +881,7 @@ class BYearBegin(DateOffset, CacheableOffset):
 
         # set first bday for result
         other = other + relativedelta(years=years)
-        wkday, days_in_month = lib.monthrange(other.year, self.month)
+        wkday, days_in_month = tslib.monthrange(other.year, self.month)
         first = _get_firstbday(wkday)
         return datetime(other.year, self.month, first)
 
@@ -904,7 +905,7 @@ class YearEnd(DateOffset, CacheableOffset):
     def apply(self, other):
         def _increment(date):
             if date.month == self.month:
-                _, days_in_month = lib.monthrange(date.year, self.month)
+                _, days_in_month = tslib.monthrange(date.year, self.month)
                 if date.day != days_in_month:
                     year = date.year
                 else:
@@ -913,21 +914,21 @@ class YearEnd(DateOffset, CacheableOffset):
                 year = date.year
             else:
                 year = date.year + 1
-            _, days_in_month = lib.monthrange(year, self.month)
+            _, days_in_month = tslib.monthrange(year, self.month)
             return datetime(year, self.month, days_in_month,
                             date.hour, date.minute, date.second,
                             date.microsecond)
 
         def _decrement(date):
             year = date.year if date.month > self.month else date.year - 1
-            _, days_in_month = lib.monthrange(year, self.month)
+            _, days_in_month = tslib.monthrange(year, self.month)
             return datetime(year, self.month, days_in_month,
                             date.hour, date.minute, date.second,
                             date.microsecond)
 
         def _rollf(date):
             if (date.month != self.month or
-                date.day < lib.monthrange(date.year, date.month)[1]):
+                date.day < tslib.monthrange(date.year, date.month)[1]):
                 date = _increment(date)
             return date
 
@@ -948,7 +949,7 @@ class YearEnd(DateOffset, CacheableOffset):
         return result
 
     def onOffset(self, dt):
-        wkday, days_in_month = lib.monthrange(dt.year, self.month)
+        wkday, days_in_month = tslib.monthrange(dt.year, self.month)
         return self.month == dt.month and dt.day == days_in_month
 
     @property
diff --git a/pandas/tseries/period.py b/pandas/tseries/period.py
index 02748b3c6..865add7c9 100644
--- a/pandas/tseries/period.py
+++ b/pandas/tseries/period.py
@@ -15,7 +15,7 @@ import pandas.core.common as com
 
 from pandas.lib import Timestamp
 import pandas.lib as lib
-import pandas._algos as _algos
+import pandas.algos as _algos
 
 
 #---------------
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index 6af10df19..f6ce72292 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -5,6 +5,7 @@ import sys
 import numpy as np
 
 import pandas.lib as lib
+import pandas.tslib as tslib
 import pandas.core.common as com
 
 try:
@@ -26,7 +27,7 @@ def _infer_tzinfo(start, end):
     def _infer(a, b):
         tz = a.tzinfo
         if b and b.tzinfo:
-            assert(lib.get_timezone(tz) == lib.get_timezone(b.tzinfo))
+            assert(tslib.get_timezone(tz) == tslib.get_timezone(b.tzinfo))
         return tz
     tz = None
     if start is not None:
@@ -70,14 +71,14 @@ def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True):
         arg = com._ensure_object(arg)
 
         try:
-            result = lib.array_to_datetime(arg, raise_=errors == 'raise',
-                                           utc=utc, dayfirst=dayfirst)
+            result = tslib.array_to_datetime(arg, raise_=errors == 'raise',
+                                             utc=utc, dayfirst=dayfirst)
             if com.is_datetime64_dtype(result) and box:
                 result = DatetimeIndex(result, tz='utc' if utc else None)
             return result
         except ValueError, e:
             try:
-                values, tz = lib.datetime_to_datetime64(arg)
+                values, tz = tslib.datetime_to_datetime64(arg)
                 return DatetimeIndex._simple_new(values, None, tz=tz)
             except (ValueError, TypeError):
                 raise e
@@ -99,7 +100,7 @@ def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True):
                     return DatetimeIndex(arg, tz='utc' if utc else None)
                 except ValueError, e:
                     try:
-                        values, tz = lib.datetime_to_datetime64(arg)
+                        values, tz = tslib.datetime_to_datetime64(arg)
                         return DatetimeIndex._simple_new(values, None, tz=tz)
                     except (ValueError, TypeError):
                         raise e
@@ -275,7 +276,7 @@ def _try_parse_monthly(arg):
     return ret
 
 
-normalize_date = lib.normalize_date
+normalize_date = tslib.normalize_date
 
 
 def format(dt):
diff --git a/pandas/tslib.pxd b/pandas/tslib.pxd
new file mode 100644
index 000000000..3e7a6ef61
--- /dev/null
+++ b/pandas/tslib.pxd
@@ -0,0 +1,3 @@
+from numpy cimport ndarray, int64_t
+
+cdef convert_to_tsobject(object, object)
diff --git a/pandas/src/tseries.pyx b/pandas/tslib.pyx
similarity index 99%
rename from pandas/src/tseries.pyx
rename to pandas/tslib.pyx
index 69be22892..55c6f1da0 100644
--- a/pandas/src/tseries.pyx
+++ b/pandas/tslib.pyx
@@ -14,8 +14,6 @@ from datetime cimport *
 from khash cimport *
 cimport cython
 
-import _algos
-
 import numpy as np
 from datetime import timedelta, datetime
 from dateutil.parser import parse as parse_date
diff --git a/setup.py b/setup.py
index a056440c6..67849e154 100755
--- a/setup.py
+++ b/setup.py
@@ -562,13 +562,8 @@ else:
 
 common_include = [np.get_include(), 'pandas/src/klib', 'pandas/src']
 
-algos_ext = Extension('pandas._algos',
-                      sources=[srcpath('generated', suffix=suffix)],
-                      include_dirs=common_include,
-                      )
-
 def pxd(name):
-    return os.path.abspath(pjoin('pandas/src', name+'.pxd'))
+    return os.path.abspath(pjoin('pandas', name+'.pxd'))
 
 
 lib_depends = lib_depends + ['pandas/src/numpy_helper.h',
@@ -589,17 +584,28 @@ ext_data = dict(
          'depends': lib_depends},
     hashtable={'pyxfile': 'hashtable',
                'pxdfiles': ['hashtable']},
-    _stats={'pyxfile': 'stats'}
+    tslib={'pyxfile': 'tslib',
+           'depends': tseries_depends,
+           'sources': ['pandas/src/datetime/np_datetime.c',
+                       'pandas/src/datetime/np_datetime_strings.c',
+                       'pandas/src/period.c']},
+    index={'pyxfile': 'index',
+           'sources': ['pandas/src/datetime/np_datetime.c',
+                       'pandas/src/datetime/np_datetime_strings.c']},
+    algos={'pyxfile': 'algos',
+           'depends': [srcpath('generated', suffix='.pyx')]},
 )
 
 extensions = []
 
 for name, data in ext_data.iteritems():
-    sources = [srcpath(data['pyxfile'], suffix=suffix)]
+    sources = [srcpath(data['pyxfile'], suffix=suffix, subdir='')]
     pxds = [pxd(x) for x in data.get('pxdfiles', [])]
     if suffix == '.pyx' and pxds:
         sources.extend(pxds)
 
+    sources.extend(data.get('sources', []))
+
     include = data.get('include', common_include)
 
     obj = Extension('pandas.%s' % name,
@@ -610,21 +616,6 @@ for name, data in ext_data.iteritems():
     extensions.append(obj)
 
 
-index_ext = Extension('pandas._index',
-                     sources=[srcpath('engines', suffix=suffix),
-                              'pandas/src/datetime/np_datetime.c',
-                              'pandas/src/datetime/np_datetime_strings.c'],
-                     include_dirs=common_include)
-
-
-tseries_ext = Extension('pandas._tseries',
-                        depends=tseries_depends,
-                        sources=[srcpath('tseries', suffix=suffix),
-                                 'pandas/src/datetime/np_datetime.c',
-                                 'pandas/src/datetime/np_datetime_strings.c',
-                                 'pandas/src/period.c'],
-                        include_dirs=common_include)
-
 sparse_ext = Extension('pandas._sparse',
                        sources=[srcpath('sparse', suffix=suffix)],
                        include_dirs=[np.get_include()],
@@ -646,23 +637,12 @@ sandbox_ext = Extension('pandas._sandbox',
                         include_dirs=common_include)
 
 
-pytables_ext = Extension('pandas._pytables',
-                         sources=[srcpath('pytables', suffix=suffix)],
-                         include_dirs=[np.get_include()],
-                         libraries=libraries)
-
-
 cppsandbox_ext = Extension('pandas._cppsandbox',
                            language='c++',
                            sources=[srcpath('cppsandbox', suffix=suffix)],
                            include_dirs=[np.get_include()])
 
-extensions.extend([algos_ext,
-                   tseries_ext,
-                   index_ext,
-                   sparse_ext,
-                   pytables_ext,
-                   parser_ext])
+extensions.extend([sparse_ext, parser_ext])
 
 # if not ISRELEASED:
 #     extensions.extend([sandbox_ext])
