commit 6c304c78d6a0559525abe16f03465d275c516455
Author: Jeffrey Tratner <jeffrey.tratner@gmail.com>
Date:   Fri Jul 26 21:07:57 2013 -0400

    CLN: Newer syntax, unicode, iterator range, zip, etc
    
    Use new syntax ('except as', print as function, new raise syntax, next
    function rather than method, next and __next__ defined throughout,
    switchout xrange, etc.)
    
    Now range is always equivalent to 2.X xrange throughout (but need to
    import range from py3compat to use it). Also remove range fixer from
    setup.py. + compatible long and string types, etc.

diff --git a/bench/alignment.py b/bench/alignment.py
index bf5d5604d..a5ffe9614 100644
--- a/bench/alignment.py
+++ b/bench/alignment.py
@@ -1,4 +1,5 @@
 # Setup
+from pandas.util.py3compat import range
 import numpy as np
 import pandas
 import la
@@ -6,8 +7,8 @@ N = 1000
 K = 50
 arr1 = np.random.randn(N, K)
 arr2 = np.random.randn(N, K)
-idx1 = range(N)
-idx2 = range(K)
+idx1 = list(range(N))
+idx2 = list(range(K))
 
 # pandas
 dma1 = pandas.DataFrame(arr1, idx1, idx2)
diff --git a/bench/bench_get_put_value.py b/bench/bench_get_put_value.py
index 419e8f603..cf1b827e1 100644
--- a/bench/bench_get_put_value.py
+++ b/bench/bench_get_put_value.py
@@ -1,12 +1,13 @@
 from pandas import *
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 
 N = 1000
 K = 50
 
 
 def _random_index(howmany):
-    return Index([rands(10) for _ in xrange(howmany)])
+    return Index([rands(10) for _ in range(howmany)])
 
 df = DataFrame(np.random.randn(N, K), index=_random_index(N),
                columns=_random_index(K))
diff --git a/bench/bench_groupby.py b/bench/bench_groupby.py
index 807d3449e..aa337acf9 100644
--- a/bench/bench_groupby.py
+++ b/bench/bench_groupby.py
@@ -1,5 +1,6 @@
 from pandas import *
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 
 import string
 import random
@@ -7,7 +8,7 @@ import random
 k = 20000
 n = 10
 
-foo = np.tile(np.array([rands(10) for _ in xrange(k)], dtype='O'), n)
+foo = np.tile(np.array([rands(10) for _ in range(k)], dtype='O'), n)
 foo2 = list(foo)
 random.shuffle(foo)
 random.shuffle(foo2)
diff --git a/bench/bench_join_panel.py b/bench/bench_join_panel.py
index 0e484fb49..f3c3f8ba1 100644
--- a/bench/bench_join_panel.py
+++ b/bench/bench_join_panel.py
@@ -35,7 +35,7 @@ def create_panels_append(cls, panels):
         # concatenate values
         try:
                 values = np.concatenate([p.values for p in panels], axis=1)
-        except (Exception), detail:
+        except Exception as detail:
                 raise Exception("cannot append values that dont' match dimensions! -> [%s] %s"
                                 % (','.join(["%s" % p for p in panels]), str(detail)))
         # pm('append - create_panel')
diff --git a/bench/bench_khash_dict.py b/bench/bench_khash_dict.py
index fce3288e3..784704cbb 100644
--- a/bench/bench_khash_dict.py
+++ b/bench/bench_khash_dict.py
@@ -1,12 +1,14 @@
 """
 Some comparisons of khash.h to Python dict
 """
+from __future__ import print_function
 
 import numpy as np
 import os
 
 from vbench.api import Benchmark
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 import pandas._tseries as lib
 import pandas._sandbox as sbx
 import time
@@ -22,7 +24,7 @@ def object_test_data(n):
 
 
 def string_test_data(n):
-    return np.array([rands(10) for _ in xrange(n)], dtype='O')
+    return np.array([rands(10) for _ in range(n)], dtype='O')
 
 
 def int_test_data(n):
@@ -50,7 +52,7 @@ def map_locations_khash_object():
 
 def _timeit(f, iterations=10):
     start = time.time()
-    for _ in xrange(iterations):
+    for _ in range(iterations):
         foo = f()
     elapsed = time.time() - start
     return elapsed
@@ -73,8 +75,8 @@ def lookup_khash(values):
 
 
 def leak(values):
-    for _ in xrange(100):
-        print proc.get_memory_info()
+    for _ in range(100):
+        print(proc.get_memory_info())
         table = lookup_khash(values)
         # table.destroy()
 
diff --git a/bench/bench_merge.py b/bench/bench_merge.py
index 11f8c29a2..7820c7792 100644
--- a/bench/bench_merge.py
+++ b/bench/bench_merge.py
@@ -1,5 +1,6 @@
 from pandas import *
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 import random
 
 N = 10000
@@ -7,7 +8,7 @@ ngroups = 10
 
 
 def get_test_data(ngroups=100, n=N):
-    unique_groups = range(ngroups)
+    unique_groups = list(range(ngroups))
     arr = np.asarray(np.tile(unique_groups, n / ngroups), dtype=object)
 
     if len(arr) < n:
@@ -34,8 +35,8 @@ import time
 from pandas.util.testing import rands
 N = 10000
 
-indices = np.array([rands(10) for _ in xrange(N)], dtype='O')
-indices2 = np.array([rands(10) for _ in xrange(N)], dtype='O')
+indices = np.array([rands(10) for _ in range(N)], dtype='O')
+indices2 = np.array([rands(10) for _ in range(N)], dtype='O')
 key = np.tile(indices[:8000], 10)
 key2 = np.tile(indices2[:8000], 10)
 
@@ -55,7 +56,7 @@ for sort in [False, True]:
         f = lambda: merge(left, right, how=join_method, sort=sort)
         gc.disable()
         start = time.time()
-        for _ in xrange(niter):
+        for _ in range(niter):
             f()
         elapsed = (time.time() - start) / niter
         gc.enable()
@@ -65,7 +66,7 @@ results.columns = ['dont_sort', 'sort']
 
 
 # R results
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
 # many to one
 r_results = read_table(StringIO("""      base::merge   plyr data.table
 inner      0.2475 0.1183     0.1100
@@ -93,7 +94,7 @@ nosort_results['Ratio'] = nosort_results['R'] / nosort_results['pandas']
 
 # many to many
 
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
 # many to one
 r_results = read_table(StringIO("""base::merge   plyr data.table
 inner      0.4610 0.1276     0.1269
diff --git a/bench/bench_merge_sqlite.py b/bench/bench_merge_sqlite.py
index d13b29669..e15a482f3 100644
--- a/bench/bench_merge_sqlite.py
+++ b/bench/bench_merge_sqlite.py
@@ -4,12 +4,14 @@ import gc
 import time
 from pandas import DataFrame
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
+from six.moves import zip
 import random
 
 N = 10000
 
-indices = np.array([rands(10) for _ in xrange(N)], dtype='O')
-indices2 = np.array([rands(10) for _ in xrange(N)], dtype='O')
+indices = np.array([rands(10) for _ in range(N)], dtype='O')
+indices2 = np.array([rands(10) for _ in range(N)], dtype='O')
 key = np.tile(indices[:8000], 10)
 key2 = np.tile(indices2[:8000], 10)
 
@@ -67,7 +69,7 @@ for sort in [False]:
         g = lambda: conn.execute(sql)  # list fetches results
         gc.disable()
         start = time.time()
-        # for _ in xrange(niter):
+        # for _ in range(niter):
         g()
         elapsed = (time.time() - start) / niter
         gc.enable()
diff --git a/bench/bench_sparse.py b/bench/bench_sparse.py
index 600b3d05c..beb3e84c3 100644
--- a/bench/bench_sparse.py
+++ b/bench/bench_sparse.py
@@ -3,6 +3,7 @@ import numpy as np
 
 from pandas import *
 import pandas.core.sparse as spm
+import pandas.util.compat as compat
 reload(spm)
 from pandas.core.sparse import *
 
@@ -41,7 +42,7 @@ sdf = dm.to_sparse()
 
 def new_data_like(sdf):
     new_data = {}
-    for col, series in sdf.iteritems():
+    for col, series in compat.iteritems(sdf):
         new_data[col] = SparseSeries(np.random.randn(len(series.sp_values)),
                                      index=sdf.index,
                                      sparse_index=series.sp_index,
diff --git a/bench/bench_take_indexing.py b/bench/bench_take_indexing.py
index 3ddd647a3..b6a7b04eb 100644
--- a/bench/bench_take_indexing.py
+++ b/bench/bench_take_indexing.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 import numpy as np
 
 from pandas import *
@@ -5,6 +6,7 @@ import pandas._tseries as lib
 
 from pandas import DataFrame
 import timeit
+from six.moves import zip
 
 setup = """
 from pandas import Series
@@ -35,7 +37,7 @@ def _timeit(stmt, size, k=5, iters=1000):
     return timer.timeit(n) / n
 
 for sz, its in zip(sizes, iters):
-    print sz
+    print(sz)
     fancy_2d.append(_timeit('arr[indexer]', sz, iters=its))
     take_2d.append(_timeit('arr.take(indexer, axis=0)', sz, iters=its))
     cython_2d.append(_timeit('lib.take_axis0(arr, indexer)', sz, iters=its))
@@ -44,7 +46,7 @@ df = DataFrame({'fancy': fancy_2d,
                 'take': take_2d,
                 'cython': cython_2d})
 
-print df
+print(df)
 
 from pandas.rpy.common import r
 r('mat <- matrix(rnorm(50000), nrow=10000, ncol=5)')
diff --git a/bench/bench_unique.py b/bench/bench_unique.py
index 392d3b326..8a2463063 100644
--- a/bench/bench_unique.py
+++ b/bench/bench_unique.py
@@ -1,5 +1,8 @@
+from __future__ import print_function
 from pandas import *
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
+from six.moves import zip
 import pandas._tseries as lib
 import numpy as np
 import matplotlib.pyplot as plt
@@ -7,8 +10,8 @@ import matplotlib.pyplot as plt
 N = 50000
 K = 10000
 
-groups = np.array([rands(10) for _ in xrange(K)], dtype='O')
-groups2 = np.array([rands(10) for _ in xrange(K)], dtype='O')
+groups = np.array([rands(10) for _ in range(K)], dtype='O')
+groups2 = np.array([rands(10) for _ in range(K)], dtype='O')
 
 labels = np.tile(groups, N // K)
 labels2 = np.tile(groups2, N // K)
@@ -20,7 +23,7 @@ def timeit(f, niter):
     import time
     gc.disable()
     start = time.time()
-    for _ in xrange(niter):
+    for _ in range(niter):
         f()
     elapsed = (time.time() - start) / niter
     gc.enable()
@@ -75,9 +78,8 @@ data = np.random.uniform(0, 1, 100000)
 
 
 def f():
-    from itertools import izip
     # groupby sum
-    for k, v in izip(x, data):
+    for k, v in zip(x, data):
         try:
             counts[k] += v
         except KeyError:
@@ -128,7 +130,7 @@ def algo4():
 # N = 10000000
 # K = 500000
 
-# groups = np.array([rands(10) for _ in xrange(K)], dtype='O')
+# groups = np.array([rands(10) for _ in range(K)], dtype='O')
 
 # labels = np.tile(groups, N // K)
 data = np.random.randn(N)
@@ -232,11 +234,11 @@ def hash_bench():
     khash_hint = []
     khash_nohint = []
     for K in Ks:
-        print K
-        # groups = np.array([rands(10) for _ in xrange(K)])
+        print(K)
+        # groups = np.array([rands(10) for _ in range(K)])
         # labels = np.tile(groups, N // K).astype('O')
 
-        groups = np.random.randint(0, 100000000000L, size=K)
+        groups = np.random.randint(0, long(100000000000), size=K)
         labels = np.tile(groups, N // K)
         dict_based.append(timeit(lambda: dict_unique(labels, K), 20))
         khash_nohint.append(timeit(lambda: khash_unique_int64(labels, K), 20))
@@ -245,11 +247,11 @@ def hash_bench():
 
         # memory, hard to get
         # dict_based.append(np.mean([dict_unique(labels, K, memory=True)
-        #                            for _ in xrange(10)]))
+        #                            for _ in range(10)]))
         # khash_nohint.append(np.mean([khash_unique(labels, K, memory=True)
-        #                              for _ in xrange(10)]))
+        #                              for _ in range(10)]))
         # khash_hint.append(np.mean([khash_unique(labels, K, size_hint=True, memory=True)
-        #                            for _ in xrange(10)]))
+        #                            for _ in range(10)]))
 
         # dict_based_sort.append(timeit(lambda: dict_unique(labels, K,
         #                                                   sort=True), 10))
diff --git a/bench/better_unique.py b/bench/better_unique.py
index 982dd88e8..f8881ecd7 100644
--- a/bench/better_unique.py
+++ b/bench/better_unique.py
@@ -1,9 +1,13 @@
+from __future__ import print_function
 from pandas import DataFrame
+from pandas.util.py3compat import range
+from six.moves import zip
 import timeit
 
 setup = """
 from pandas import Series
 import pandas._tseries as _tseries
+from pandas.util.py3compat import range
 import random
 import numpy as np
 
@@ -48,11 +52,11 @@ for sz, n in zip(group_sizes, numbers):
     numpy_timer = timeit.Timer(stmt='np.unique(arr)',
                                setup=setup % sz)
 
-    print n
+    print(n)
     numpy_result = numpy_timer.timeit(number=n) / n
     wes_result = wes_timer.timeit(number=n) / n
 
-    print 'Groups: %d, NumPy: %s, Wes: %s' % (sz, numpy_result, wes_result)
+    print('Groups: %d, NumPy: %s, Wes: %s' % (sz, numpy_result, wes_result))
 
     wes.append(wes_result)
     numpy.append(numpy_result)
diff --git a/bench/io_roundtrip.py b/bench/io_roundtrip.py
index a9711dbb8..a033ef0c7 100644
--- a/bench/io_roundtrip.py
+++ b/bench/io_roundtrip.py
@@ -1,16 +1,18 @@
+from __future__ import print_function
 import time
 import os
 import numpy as np
 
 import la
 import pandas
+from pandas.util.py3compat import range
 from pandas import datetools, DateRange
 
 
 def timeit(f, iterations):
     start = time.clock()
 
-    for i in xrange(iterations):
+    for i in range(iterations):
         f()
 
     return time.clock() - start
@@ -54,11 +56,11 @@ def rountrip_archive(N, K=50, iterations=10):
 
     pandas_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
     pandas_time = timeit(pandas_f, iterations) / iterations
-    print 'pandas (HDF5) %7.4f seconds' % pandas_time
+    print('pandas (HDF5) %7.4f seconds' % pandas_time)
 
     pickle_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
     pickle_time = timeit(pickle_f, iterations) / iterations
-    print 'pandas (pickle) %7.4f seconds' % pickle_time
+    print('pandas (pickle) %7.4f seconds' % pickle_time)
 
     # print 'Numpy (npz)   %7.4f seconds' % numpy_time
     # print 'larry (HDF5)  %7.4f seconds' % larry_time
diff --git a/bench/serialize.py b/bench/serialize.py
index 63f885a4e..9c0ba8420 100644
--- a/bench/serialize.py
+++ b/bench/serialize.py
@@ -1,3 +1,5 @@
+from __future__ import print_function
+from pandas.util.py3compat import range
 import time
 import os
 import numpy as np
@@ -9,7 +11,7 @@ import pandas
 def timeit(f, iterations):
     start = time.clock()
 
-    for i in xrange(iterations):
+    for i in range(iterations):
         f()
 
     return time.clock() - start
@@ -20,7 +22,7 @@ def roundtrip_archive(N, iterations=10):
     # Create data
     arr = np.random.randn(N, N)
     lar = la.larry(arr)
-    dma = pandas.DataFrame(arr, range(N), range(N))
+    dma = pandas.DataFrame(arr, list(range(N)), list(range(N)))
 
     # filenames
     filename_numpy = '/Users/wesm/tmp/numpy.npz'
@@ -51,9 +53,9 @@ def roundtrip_archive(N, iterations=10):
     pandas_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
     pandas_time = timeit(pandas_f, iterations) / iterations
 
-    print 'Numpy (npz)   %7.4f seconds' % numpy_time
-    print 'larry (HDF5)  %7.4f seconds' % larry_time
-    print 'pandas (HDF5) %7.4f seconds' % pandas_time
+    print('Numpy (npz)   %7.4f seconds' % numpy_time)
+    print('larry (HDF5)  %7.4f seconds' % larry_time)
+    print('pandas (HDF5) %7.4f seconds' % pandas_time)
 
 
 def numpy_roundtrip(filename, arr1, arr2):
diff --git a/bench/test.py b/bench/test.py
index 2ac91468d..9d47c091b 100644
--- a/bench/test.py
+++ b/bench/test.py
@@ -1,7 +1,9 @@
+from pandas.util.py3compat import range
 import numpy as np
 import itertools
 import collections
 import scipy.ndimage as ndi
+from six.moves import zip
 
 N = 10000
 
diff --git a/doc/make.py b/doc/make.py
index adf34920b..12b60a4f1 100755
--- a/doc/make.py
+++ b/doc/make.py
@@ -14,6 +14,7 @@ Usage
 python make.py clean
 python make.py html
 """
+from __future__ import print_function
 
 import glob
 import os
@@ -60,7 +61,7 @@ def upload_prev(ver, doc_root='./'):
     remote_dir = '/usr/share/nginx/pandas/pandas-docs/version/%s/' % ver
     cmd = 'cd %s; rsync -avz . pandas@pandas.pydata.org:%s -essh'
     cmd = cmd % (local_dir, remote_dir)
-    print cmd
+    print(cmd)
     if os.system(cmd):
         raise SystemExit(
             'Upload to %s from %s failed' % (remote_dir, local_dir))
@@ -154,7 +155,7 @@ def auto_dev_build(debug=False):
         upload_dev_pdf()
         if not debug:
             sendmail(step)
-    except (Exception, SystemExit), inst:
+    except (Exception, SystemExit) as inst:
         msg = str(inst) + '\n'
         sendmail(step, '[ERROR] ' + msg)
 
diff --git a/doc/plots/stats/moment_plots.py b/doc/plots/stats/moment_plots.py
index 9e3a90259..a078651d2 100644
--- a/doc/plots/stats/moment_plots.py
+++ b/doc/plots/stats/moment_plots.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import numpy as np
 
 import matplotlib.pyplot as plt
diff --git a/doc/source/conf.py b/doc/source/conf.py
index 99d1703b9..128e4ade9 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -12,6 +12,7 @@
 
 import sys
 import os
+import six
 
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
@@ -63,8 +64,8 @@ source_suffix = '.rst'
 master_doc = 'index'
 
 # General information about the project.
-project = u'pandas'
-copyright = u'2008-2012, the pandas development team'
+project = six.u('pandas')
+copyright = six.u('2008-2012, the pandas development team')
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -211,8 +212,8 @@ htmlhelp_basename = 'pandas'
 # (source start file, target name, title, author, documentclass [howto/manual]).
 latex_documents = [
     ('index', 'pandas.tex',
-     u'pandas: powerful Python data analysis toolkit',
-     u'Wes McKinney\n\& PyData Development Team', 'manual'),
+     six.u('pandas: powerful Python data analysis toolkit'),
+     six.u('Wes McKinney\n\& PyData Development Team'), 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
diff --git a/doc/source/io.rst b/doc/source/io.rst
index 7290e499c..ee6c35187 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1184,7 +1184,7 @@ You can even pass in an instance of ``StringIO`` if you so desire
 
 .. ipython:: python
 
-   from cStringIO import StringIO
+   from six.moves import cStringIO as StringIO
 
    with open(file_path, 'r') as f:
        sio = StringIO(f.read())
diff --git a/doc/sphinxext/__init__.py b/doc/sphinxext/__init__.py
index ae9073bc4..68dbbb00a 100755
--- a/doc/sphinxext/__init__.py
+++ b/doc/sphinxext/__init__.py
@@ -1 +1 @@
-from numpydoc import setup
+from .numpydoc import setup
diff --git a/doc/sphinxext/comment_eater.py b/doc/sphinxext/comment_eater.py
index e11eea902..3b15bd178 100755
--- a/doc/sphinxext/comment_eater.py
+++ b/doc/sphinxext/comment_eater.py
@@ -1,10 +1,10 @@
-from cStringIO import StringIO
+from six.moves import cStringIO
 import compiler
 import inspect
 import textwrap
 import tokenize
 
-from compiler_unparse import unparse
+from .compiler_unparse import unparse
 
 
 class Comment(object):
@@ -95,7 +95,7 @@ class CommentBlocker(object):
 
     def new_comment(self, string, start, end, line):
         """ Possibly add a new comment.
-        
+
         Only adds a new comment if this comment is the only thing on the line.
         Otherwise, it extends the noncomment block.
         """
diff --git a/doc/sphinxext/compiler_unparse.py b/doc/sphinxext/compiler_unparse.py
index ffcf51b35..240dd1724 100755
--- a/doc/sphinxext/compiler_unparse.py
+++ b/doc/sphinxext/compiler_unparse.py
@@ -12,11 +12,11 @@
 """
 
 import sys
-import cStringIO
+from six.moves import cStringIO as StringIO
 from compiler.ast import Const, Name, Tuple, Div, Mul, Sub, Add
 
 def unparse(ast, single_line_functions=False):
-    s = cStringIO.StringIO()
+    s = StringIO()
     UnparseCompilerAst(ast, s, single_line_functions)
     return s.getvalue().lstrip()
 
@@ -101,13 +101,13 @@ class UnparseCompilerAst:
             if i != len(t.nodes)-1:
                 self._write(") and (")
         self._write(")")
-               
+
     def _AssAttr(self, t):
         """ Handle assigning an attribute of an object
         """
         self._dispatch(t.expr)
         self._write('.'+t.attrname)
- 
+
     def _Assign(self, t):
         """ Expression Assignment such as "a = 1".
 
@@ -145,36 +145,36 @@ class UnparseCompilerAst:
     def _AugAssign(self, t):
         """ +=,-=,*=,/=,**=, etc. operations
         """
-        
+
         self._fill()
         self._dispatch(t.node)
         self._write(' '+t.op+' ')
         self._dispatch(t.expr)
         if not self._do_indent:
             self._write(';')
-            
+
     def _Bitand(self, t):
         """ Bit and operation.
         """
-        
+
         for i, node in enumerate(t.nodes):
             self._write("(")
             self._dispatch(node)
             self._write(")")
             if i != len(t.nodes)-1:
                 self._write(" & ")
-                
+
     def _Bitor(self, t):
         """ Bit or operation
         """
-        
+
         for i, node in enumerate(t.nodes):
             self._write("(")
             self._dispatch(node)
             self._write(")")
             if i != len(t.nodes)-1:
                 self._write(" | ")
-                
+
     def _CallFunc(self, t):
         """ Function call.
         """
@@ -249,7 +249,7 @@ class UnparseCompilerAst:
             self._write(name)
             if asname is not None:
                 self._write(" as "+asname)
-                
+
     def _Function(self, t):
         """ Handle function definitions
         """
@@ -282,12 +282,12 @@ class UnparseCompilerAst:
             self._write(')')
         else:
             self._dispatch(t.expr)
-            
+
         self._write('.'+t.attrname)
-        
+
     def _If(self, t):
         self._fill()
-        
+
         for i, (compare,code) in enumerate(t.tests):
             if i == 0:
                 self._write("if ")
@@ -307,7 +307,7 @@ class UnparseCompilerAst:
             self._dispatch(t.else_)
             self._leave()
             self._write("\n")
-            
+
     def _IfExp(self, t):
         self._dispatch(t.then)
         self._write(" if ")
@@ -322,7 +322,7 @@ class UnparseCompilerAst:
         """ Handle "import xyz.foo".
         """
         self._fill("import ")
-        
+
         for i, (name,asname) in enumerate(t.names):
             if i != 0:
                 self._write(", ")
@@ -336,7 +336,7 @@ class UnparseCompilerAst:
         self._write(t.name)
         self._write("=")
         self._dispatch(t.expr)
-        
+
     def _List(self, t):
         self._write("[")
         for  i,node in enumerate(t.nodes):
@@ -358,12 +358,12 @@ class UnparseCompilerAst:
 
     def _NoneType(self, t):
         self._write("None")
-        
+
     def _Not(self, t):
         self._write('not (')
         self._dispatch(t.expr)
         self._write(')')
-        
+
     def _Or(self, t):
         self._write(" (")
         for i, node in enumerate(t.nodes):
@@ -371,7 +371,7 @@ class UnparseCompilerAst:
             if i != len(t.nodes)-1:
                 self._write(") or (")
         self._write(")")
-                
+
     def _Pass(self, t):
         self._write("pass\n")
 
@@ -452,7 +452,7 @@ class UnparseCompilerAst:
             self._enter()
             self._dispatch(handler[2])
             self._leave()
-            
+
         if t.else_:
             self._fill("else")
             self._enter()
@@ -477,14 +477,14 @@ class UnparseCompilerAst:
             self._dispatch(last_element)
 
             self._write(")")
-            
+
     def _UnaryAdd(self, t):
         self._write("+")
         self._dispatch(t.expr)
-        
+
     def _UnarySub(self, t):
         self._write("-")
-        self._dispatch(t.expr)        
+        self._dispatch(t.expr)
 
     def _With(self, t):
         self._fill('with ')
@@ -496,7 +496,7 @@ class UnparseCompilerAst:
         self._dispatch(t.body)
         self._leave()
         self._write('\n')
-        
+
     def _int(self, t):
         self._write(repr(t))
 
@@ -533,7 +533,7 @@ class UnparseCompilerAst:
 
     def _str(self, t):
         self._write(repr(t))
-        
+
     def _tuple(self, t):
         self._write(str(t))
 
diff --git a/doc/sphinxext/docscrape.py b/doc/sphinxext/docscrape.py
index 63fec42ad..384a6db2c 100755
--- a/doc/sphinxext/docscrape.py
+++ b/doc/sphinxext/docscrape.py
@@ -1,13 +1,15 @@
 """Extract reference documentation from the NumPy source tree.
 
 """
+from __future__ import print_function
 
 import inspect
 import textwrap
 import re
 import pydoc
-from StringIO import StringIO
 from warnings import warn
+from six import StringIO
+import six
 
 class Reader(object):
     """A line-based string reader.
@@ -113,7 +115,7 @@ class NumpyDocString(object):
         return self._parsed_data[key]
 
     def __setitem__(self,key,val):
-        if not self._parsed_data.has_key(key):
+        if key not in self._parsed_data:
             warn("Unknown section %s" % key)
         else:
             self._parsed_data[key] = val
@@ -370,7 +372,7 @@ class NumpyDocString(object):
         idx = self['index']
         out = []
         out += ['.. index:: %s' % idx.get('default','')]
-        for section, references in idx.iteritems():
+        for section, references in six.iteritems(idx):
             if section == 'default':
                 continue
             out += ['   :%s: %s' % (section, ', '.join(references))]
@@ -427,7 +429,7 @@ class FunctionDoc(NumpyDocString):
                 argspec = inspect.formatargspec(*argspec)
                 argspec = argspec.replace('*','\*')
                 signature = '%s%s' % (func_name, argspec)
-            except TypeError, e:
+            except TypeError as e:
                 signature = '%s()' % func_name
             self['Signature'] = signature
 
@@ -449,8 +451,8 @@ class FunctionDoc(NumpyDocString):
                  'meth': 'method'}
 
         if self._role:
-            if not roles.has_key(self._role):
-                print "Warning: invalid role %s" % self._role
+            if self._role not in roles:
+                print("Warning: invalid role %s" % self._role)
             out += '.. %s:: %s\n    \n\n' % (roles.get(self._role,''),
                                              func_name)
 
diff --git a/doc/sphinxext/docscrape_sphinx.py b/doc/sphinxext/docscrape_sphinx.py
index 9f4350d46..a5b53eb09 100755
--- a/doc/sphinxext/docscrape_sphinx.py
+++ b/doc/sphinxext/docscrape_sphinx.py
@@ -1,6 +1,7 @@
 import re, inspect, textwrap, pydoc
 import sphinx
-from docscrape import NumpyDocString, FunctionDoc, ClassDoc
+from .docscrape import NumpyDocString, FunctionDoc, ClassDoc
+import six
 
 class SphinxDocString(NumpyDocString):
     def __init__(self, docstring, config={}):
@@ -127,7 +128,7 @@ class SphinxDocString(NumpyDocString):
             return out
 
         out += ['.. index:: %s' % idx.get('default','')]
-        for section, references in idx.iteritems():
+        for section, references in six.iteritems(idx):
             if section == 'default':
                 continue
             elif section == 'refguide':
diff --git a/doc/sphinxext/ipython_directive.py b/doc/sphinxext/ipython_directive.py
index 0c28e397a..b74808f0e 100644
--- a/doc/sphinxext/ipython_directive.py
+++ b/doc/sphinxext/ipython_directive.py
@@ -51,14 +51,16 @@ Authors
 - VĂĄclavĹ milauer <eudoxos-AT-arcig.cz>: Prompt generalizations.
 - Skipper Seabold, refactoring, cleanups, pure python addition
 """
+from __future__ import print_function
 
 #-----------------------------------------------------------------------------
 # Imports
 #-----------------------------------------------------------------------------
 
 # Stdlib
+from pandas.util.py3compat import range
+from six.moves import map, cStringIO as StringIO
 import ast
-import cStringIO
 import os
 import re
 import sys
@@ -69,6 +71,8 @@ import matplotlib
 from docutils.parsers.rst import directives
 from docutils import nodes
 from sphinx.util.compat import Directive
+import six
+from six.moves import zip
 
 matplotlib.use('Agg')
 
@@ -114,7 +118,7 @@ def block_parser(part, rgxin, rgxout, fmtin, fmtout):
     N = len(lines)
     i = 0
     decorator = None
-    while 1:
+    while True:
 
         if i==N:
             # nothing left to parse -- the last line
@@ -186,7 +190,7 @@ class EmbeddedSphinxShell(object):
 
     def __init__(self):
 
-        self.cout = cStringIO.StringIO()
+        self.cout = StringIO()
 
         # Create config object for IPython
         config = Config()
@@ -299,7 +303,7 @@ class EmbeddedSphinxShell(object):
         def _remove_first_space_if_any(line):
             return line[1:] if line.startswith(' ') else line
 
-        input_lines = map(_remove_first_space_if_any, input.split('\n'))
+        input_lines = list(map(_remove_first_space_if_any, input.split('\n')))
 
         self.datacontent = data
 
@@ -489,7 +493,7 @@ class EmbeddedSphinxShell(object):
                     multiline = True
                     cont_len = len(str(lineno)) + 2
                     line_to_process = line.strip('\\')
-                    output.extend([u"%s %s" % (fmtin%lineno,line)])
+                    output.extend([six.u("%s %s") % (fmtin%lineno,line)])
                     continue
                 else: # no we're still not
                     line_to_process = line.strip('\\')
@@ -497,12 +501,12 @@ class EmbeddedSphinxShell(object):
                 line_to_process += line.strip('\\')
                 if line_stripped.endswith('\\'): # and we still are
                     continuation = '.' * cont_len
-                    output.extend([(u'   %s: '+line_stripped) % continuation])
+                    output.extend([(six.u('   %s: ')+line_stripped) % continuation])
                     continue
                 # else go ahead and run this multiline then carry on
 
             # get output of line
-            self.process_input_line(unicode(line_to_process.strip()),
+            self.process_input_line(six.text_type(line_to_process.strip()),
                                     store_history=False)
             out_line = self.cout.getvalue()
             self.clear_cout()
@@ -516,15 +520,15 @@ class EmbeddedSphinxShell(object):
 
             # line numbers don't actually matter, they're replaced later
             if not multiline:
-                in_line = u"%s %s" % (fmtin%lineno,line)
+                in_line = six.u("%s %s") % (fmtin%lineno,line)
 
                 output.extend([in_line])
             else:
-                output.extend([(u'   %s: '+line_stripped) % continuation])
+                output.extend([(six.u('   %s: ')+line_stripped) % continuation])
                 multiline = False
             if len(out_line):
                 output.extend([out_line])
-            output.extend([u''])
+            output.extend([six.u('')])
 
         return output
 
@@ -566,19 +570,19 @@ class EmbeddedSphinxShell(object):
                 output.extend([line])
                 continue
 
-            continuation  = u'   %s:'% ''.join(['.']*(len(str(ct))+2))
+            continuation  = six.u('   %s:')% ''.join(['.']*(len(str(ct))+2))
             if not multiline:
-                modified = u"%s %s" % (fmtin % ct, line_stripped)
+                modified = six.u("%s %s") % (fmtin % ct, line_stripped)
                 output.append(modified)
                 ct += 1
                 try:
                     ast.parse(line_stripped)
-                    output.append(u'')
+                    output.append(six.u(''))
                 except Exception:
                     multiline = True
                     multiline_start = lineno
             else:
-                modified = u'%s %s' % (continuation, line)
+                modified = six.u('%s %s') % (continuation, line)
                 output.append(modified)
 
                 try:
@@ -590,7 +594,7 @@ class EmbeddedSphinxShell(object):
 
                         continue
 
-                    output.extend([continuation, u''])
+                    output.extend([continuation, six.u('')])
                     multiline = False
                 except Exception:
                     pass
@@ -732,7 +736,7 @@ class IpythonDirective(Directive):
         #print lines
         if len(lines)>2:
             if debug:
-                print '\n'.join(lines)
+                print('\n'.join(lines))
             else: #NOTE: this raises some errors, what's it for?
                 #print 'INSERTING %d lines'%len(lines)
                 self.state_machine.insert_input(
@@ -910,4 +914,4 @@ if __name__=='__main__':
     if not os.path.isdir('_static'):
         os.mkdir('_static')
     test()
-    print 'All OK? Check figures in _static/'
+    print('All OK? Check figures in _static/')
diff --git a/doc/sphinxext/numpydoc.py b/doc/sphinxext/numpydoc.py
index 43c67336b..f32d778b6 100755
--- a/doc/sphinxext/numpydoc.py
+++ b/doc/sphinxext/numpydoc.py
@@ -17,12 +17,13 @@ It will:
 """
 
 import sphinx
+import six
 
 if sphinx.__version__ < '1.0.1':
     raise RuntimeError("Sphinx 1.0.1 or newer is required")
 
 import os, re, pydoc
-from docscrape_sphinx import get_doc_object, SphinxDocString
+from .docscrape_sphinx import get_doc_object, SphinxDocString
 from sphinx.util.compat import Directive
 import inspect
 
@@ -34,28 +35,28 @@ def mangle_docstrings(app, what, name, obj, options, lines,
 
     if what == 'module':
         # Strip top title
-        title_re = re.compile(ur'^\s*[#*=]{4,}\n[a-z0-9 -]+\n[#*=]{4,}\s*',
+        title_re = re.compile(six.u(r'^\s*[#*=]{4,}\n[a-z0-9 -]+\n[#*=]{4,}\s*'),
                               re.I|re.S)
-        lines[:] = title_re.sub(u'', u"\n".join(lines)).split(u"\n")
+        lines[:] = title_re.sub(six.u(''), six.u("\n").join(lines)).split(six.u("\n"))
     else:
-        doc = get_doc_object(obj, what, u"\n".join(lines), config=cfg)
-        lines[:] = unicode(doc).split(u"\n")
+        doc = get_doc_object(obj, what, six.u("\n").join(lines), config=cfg)
+        lines[:] = six.text_type(doc).split(six.u("\n"))
 
     if app.config.numpydoc_edit_link and hasattr(obj, '__name__') and \
            obj.__name__:
         if hasattr(obj, '__module__'):
-            v = dict(full_name=u"%s.%s" % (obj.__module__, obj.__name__))
+            v = dict(full_name=six.u("%s.%s") % (obj.__module__, obj.__name__))
         else:
             v = dict(full_name=obj.__name__)
-        lines += [u'', u'.. htmlonly::', '']
-        lines += [u'    %s' % x for x in
+        lines += [six.u(''), six.u('.. htmlonly::'), '']
+        lines += [six.u('    %s') % x for x in
                   (app.config.numpydoc_edit_link % v).split("\n")]
 
     # replace reference numbers so that there are no duplicates
     references = []
     for line in lines:
         line = line.strip()
-        m = re.match(ur'^.. \[([a-z0-9_.-])\]', line, re.I)
+        m = re.match(six.u(r'^.. \[([a-z0-9_.-])\]'), line, re.I)
         if m:
             references.append(m.group(1))
 
@@ -64,14 +65,14 @@ def mangle_docstrings(app, what, name, obj, options, lines,
     if references:
         for i, line in enumerate(lines):
             for r in references:
-                if re.match(ur'^\d+$', r):
-                    new_r = u"R%d" % (reference_offset[0] + int(r))
+                if re.match(six.u(r'^\d+$'), r):
+                    new_r = six.u("R%d") % (reference_offset[0] + int(r))
                 else:
-                    new_r = u"%s%d" % (r, reference_offset[0])
-                lines[i] = lines[i].replace(u'[%s]_' % r,
-                                            u'[%s]_' % new_r)
-                lines[i] = lines[i].replace(u'.. [%s]' % r,
-                                            u'.. [%s]' % new_r)
+                    new_r = six.u("%s%d") % (r, reference_offset[0])
+                lines[i] = lines[i].replace(six.u('[%s]_') % r,
+                                            six.u('[%s]_') % new_r)
+                lines[i] = lines[i].replace(six.u('.. [%s]') % r,
+                                            six.u('.. [%s]') % new_r)
 
     reference_offset[0] += len(references)
 
@@ -87,8 +88,8 @@ def mangle_signature(app, what, name, obj, options, sig, retann):
 
     doc = SphinxDocString(pydoc.getdoc(obj))
     if doc['Signature']:
-        sig = re.sub(u"^[^(]*", u"", doc['Signature'])
-        return sig, u''
+        sig = re.sub(six.u("^[^(]*"), six.u(""), doc['Signature'])
+        return sig, six.u('')
 
 def setup(app, get_doc_object_=get_doc_object):
     global get_doc_object
diff --git a/doc/sphinxext/phantom_import.py b/doc/sphinxext/phantom_import.py
index c77eeb544..a92eb96e5 100755
--- a/doc/sphinxext/phantom_import.py
+++ b/doc/sphinxext/phantom_import.py
@@ -14,6 +14,7 @@ without needing to rebuild the documented module.
 .. [1] http://code.google.com/p/pydocweb
 
 """
+from __future__ import print_function
 import imp, sys, compiler, types, os, inspect, re
 
 def setup(app):
@@ -23,7 +24,7 @@ def setup(app):
 def initialize(app):
     fn = app.config.phantom_import_file
     if (fn and os.path.isfile(fn)):
-        print "[numpydoc] Phantom importing modules from", fn, "..."
+        print("[numpydoc] Phantom importing modules from", fn, "...")
         import_phantom_module(fn)
 
 #------------------------------------------------------------------------------
@@ -129,7 +130,7 @@ def import_phantom_module(xml_file):
                 doc = "%s%s\n\n%s" % (funcname, argspec, doc)
             obj = lambda: 0
             obj.__argspec_is_invalid_ = True
-            obj.func_name = funcname
+            obj.__name__ = funcname
             obj.__name__ = name
             obj.__doc__ = doc
             if inspect.isclass(object_cache[parent]):
diff --git a/doc/sphinxext/plot_directive.py b/doc/sphinxext/plot_directive.py
index cacd53dbc..795410380 100755
--- a/doc/sphinxext/plot_directive.py
+++ b/doc/sphinxext/plot_directive.py
@@ -75,10 +75,13 @@ TODO
 
 """
 
-import sys, os, glob, shutil, imp, warnings, cStringIO, re, textwrap, traceback
+from pandas.util.py3compat import range
+import sys, os, glob, shutil, imp, warnings, re, textwrap, traceback
+from six.moves import cStringIO as StringIO
 import sphinx
 
 import warnings
+from six.moves import map
 warnings.warn("A plot_directive module is also available under "
               "matplotlib.sphinxext; expect this numpydoc.plot_directive "
               "module to be deprecated after relevant features have been "
@@ -257,7 +260,7 @@ def run(arguments, content, options, state_machine, state, lineno):
 
     # is it in doctest format?
     is_doctest = contains_doctest(code)
-    if options.has_key('format'):
+    if 'format' in options:
         if options['format'] == 'python':
             is_doctest = False
         else:
@@ -291,7 +294,7 @@ def run(arguments, content, options, state_machine, state, lineno):
         results = makefig(code, source_file_name, build_dir, output_base,
                           config)
         errors = []
-    except PlotError, err:
+    except PlotError as err:
         reporter = state.memo.reporter
         sm = reporter.system_message(
             2, "Exception occurred in plotting %s: %s" % (output_base, err),
@@ -448,7 +451,7 @@ def run_code(code, code_path, ns=None):
 
     # Redirect stdout
     stdout = sys.stdout
-    sys.stdout = cStringIO.StringIO()
+    sys.stdout = StringIO()
 
     # Reset sys.argv
     old_sys_argv = sys.argv
@@ -460,9 +463,9 @@ def run_code(code, code_path, ns=None):
             if ns is None:
                 ns = {}
             if not ns:
-                exec setup.config.plot_pre_code in ns
-            exec code in ns
-        except (Exception, SystemExit), err:
+                exec(setup.config.plot_pre_code, ns)
+            exec(code, ns)
+        except (Exception, SystemExit) as err:
             raise PlotError(traceback.format_exc())
     finally:
         os.chdir(pwd)
@@ -524,7 +527,7 @@ def makefig(code, code_path, output_dir, output_base, config):
     all_exists = True
     for i, code_piece in enumerate(code_pieces):
         images = []
-        for j in xrange(1000):
+        for j in range(1000):
             img = ImageFile('%s_%02d_%02d' % (output_base, i, j), output_dir)
             for format, dpi in formats:
                 if out_of_date(code_path, img.filename(format)):
@@ -570,7 +573,7 @@ def makefig(code, code_path, output_dir, output_base, config):
                 try:
                     figman.canvas.figure.savefig(img.filename(format), dpi=dpi,
                                                  bbox_inches='tight')
-                except exceptions.BaseException, err:
+                except exceptions.BaseException as err:
                     raise PlotError(traceback.format_exc())
                 img.formats.append(format)
 
diff --git a/doc/sphinxext/tests/test_docscrape.py b/doc/sphinxext/tests/test_docscrape.py
index 1d775e99e..1abf11b77 100755
--- a/doc/sphinxext/tests/test_docscrape.py
+++ b/doc/sphinxext/tests/test_docscrape.py
@@ -1,6 +1,8 @@
+from __future__ import print_function
 # -*- encoding:utf-8 -*-
 
 import sys, os
+import six
 sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
 
 from docscrape import NumpyDocString, FunctionDoc, ClassDoc
@@ -143,7 +145,7 @@ def test_examples():
 
 def test_index():
     assert_equal(doc['index']['default'], 'random')
-    print doc['index']
+    print(doc['index'])
     assert_equal(len(doc['index']), 2)
     assert_equal(len(doc['index']['refguide']), 2)
 
@@ -491,7 +493,7 @@ def test_unicode():
         äää
 
     """)
-    assert doc['Summary'][0] == u'öäöäöäöäöåååå'.encode('utf-8')
+    assert doc['Summary'][0] == six.u('öäöäöäöäöåååå').encode('utf-8')
 
 def test_plot_examples():
     cfg = dict(use_plots=True)
diff --git a/doc/sphinxext/traitsdoc.py b/doc/sphinxext/traitsdoc.py
index 0fcf2c1cd..952206c44 100755
--- a/doc/sphinxext/traitsdoc.py
+++ b/doc/sphinxext/traitsdoc.py
@@ -18,13 +18,13 @@ import inspect
 import os
 import pydoc
 
-import docscrape
-import docscrape_sphinx
-from docscrape_sphinx import SphinxClassDoc, SphinxFunctionDoc, SphinxDocString
+from . import docscrape
+from . import docscrape_sphinx
+from .docscrape_sphinx import SphinxClassDoc, SphinxFunctionDoc, SphinxDocString
 
-import numpydoc
+from . import numpydoc
 
-import comment_eater
+from . import comment_eater
 
 class SphinxTraitsDoc(SphinxClassDoc):
     def __init__(self, cls, modulename='', func_doc=SphinxFunctionDoc):
diff --git a/examples/finance.py b/examples/finance.py
index 24aa337a8..069f299d5 100644
--- a/examples/finance.py
+++ b/examples/finance.py
@@ -19,7 +19,7 @@ endDate = datetime(2009, 9, 1)
 
 def getQuotes(symbol, start, end):
     quotes = fin.quotes_historical_yahoo(symbol, start, end)
-    dates, open, close, high, low, volume = zip(*quotes)
+    dates, open, close, high, low, volume = list(zip(*quotes))
 
     data = {
         'open': open,
diff --git a/ez_setup.py b/ez_setup.py
index de65d3c1f..6f63b856f 100644
--- a/ez_setup.py
+++ b/ez_setup.py
@@ -13,6 +13,7 @@ the appropriate options to ``use_setuptools()``.
 
 This file can also be run as a script to install or upgrade setuptools.
 """
+from __future__ import print_function
 import sys
 DEFAULT_VERSION = "0.6c11"
 DEFAULT_URL = "http://pypi.python.org/packages/%s/s/setuptools/" % sys.version[
@@ -75,10 +76,10 @@ def _validate_md5(egg_name, data):
     if egg_name in md5_data:
         digest = md5(data).hexdigest()
         if digest != md5_data[egg_name]:
-            print >>sys.stderr, (
+            print((
                 "md5 validation of %s failed!  (Possible download problem?)"
                 % egg_name
-            )
+            ), file=sys.stderr)
             sys.exit(2)
     return data
 
@@ -113,14 +114,14 @@ def use_setuptools(
     try:
         pkg_resources.require("setuptools>=" + version)
         return
-    except pkg_resources.VersionConflict, e:
+    except pkg_resources.VersionConflict as e:
         if was_imported:
-            print >>sys.stderr, (
+            print((
                 "The required version of setuptools (>=%s) is not available, and\n"
                 "can't be installed while this script is running. Please install\n"
                 " a more recent version first, using 'easy_install -U setuptools'."
                 "\n\n(Currently using %r)"
-            ) % (version, e.args[0])
+            ) % (version, e.args[0]), file=sys.stderr)
             sys.exit(2)
         else:
             del pkg_resources, sys.modules['pkg_resources']    # reload ok
@@ -199,10 +200,10 @@ def main(argv, version=DEFAULT_VERSION):
                 os.unlink(egg)
     else:
         if setuptools.__version__ == '0.0.1':
-            print >>sys.stderr, (
+            print((
                 "You have an obsolete version of setuptools installed.  Please\n"
                 "remove it from your system entirely before rerunning this script."
-            )
+            ), file=sys.stderr)
             sys.exit(2)
 
     req = "setuptools>=" + version
@@ -221,8 +222,8 @@ def main(argv, version=DEFAULT_VERSION):
             from setuptools.command.easy_install import main
             main(argv)
         else:
-            print "Setuptools version", version, "or greater has been installed."
-            print '(Run "ez_setup.py -U setuptools" to reinstall or upgrade.)'
+            print("Setuptools version", version, "or greater has been installed.")
+            print('(Run "ez_setup.py -U setuptools" to reinstall or upgrade.)')
 
 
 def update_md5(filenames):
@@ -236,8 +237,7 @@ def update_md5(filenames):
         md5_data[base] = md5(f.read()).hexdigest()
         f.close()
 
-    data = ["    %r: %r,\n" % it for it in md5_data.items()]
-    data.sort()
+    data = sorted(["    %r: %r,\n" % it for it in md5_data.items()])
     repl = "".join(data)
 
     import inspect
@@ -248,7 +248,7 @@ def update_md5(filenames):
 
     match = re.search("\nmd5_data = {\n([^}]+)}", src)
     if not match:
-        print >>sys.stderr, "Internal error!"
+        print("Internal error!", file=sys.stderr)
         sys.exit(2)
 
     src = src[:match.start(1)] + repl + src[match.end(1):]
diff --git a/pandas/compat/scipy.py b/pandas/compat/scipy.py
index 59a9bbdfb..26a70963d 100644
--- a/pandas/compat/scipy.py
+++ b/pandas/compat/scipy.py
@@ -2,6 +2,7 @@
 Shipping functions from SciPy to reduce dependency on having SciPy installed
 """
 
+from pandas.util.py3compat import range
 import numpy as np
 
 
@@ -118,12 +119,12 @@ def rankdata(a):
     sumranks = 0
     dupcount = 0
     newarray = np.zeros(n, float)
-    for i in xrange(n):
+    for i in range(n):
         sumranks += i
         dupcount += 1
         if i == n - 1 or svec[i] != svec[i + 1]:
             averank = sumranks / float(dupcount) + 1
-            for j in xrange(i - dupcount + 1, i + 1):
+            for j in range(i - dupcount + 1, i + 1):
                 newarray[ivec[j]] = averank
             sumranks = 0
             dupcount = 0
@@ -223,9 +224,9 @@ def percentileofscore(a, score, kind='rank'):
     if kind == 'rank':
         if not(np.any(a == score)):
             a = np.append(a, score)
-            a_len = np.array(range(len(a)))
+            a_len = np.array(list(range(len(a))))
         else:
-            a_len = np.array(range(len(a))) + 1.0
+            a_len = np.array(list(range(len(a)))) + 1.0
 
         a = np.sort(a)
         idx = [a == score]
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 4bb990a57..21b6f3289 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -4,6 +4,7 @@ intended for public consumption
 """
 
 import numpy as np
+import six
 
 import pandas.core.common as com
 import pandas.algos as algos
@@ -31,7 +32,7 @@ def match(to_match, values, na_sentinel=-1):
     match : ndarray of integers
     """
     values = com._asarray_tuplesafe(values)
-    if issubclass(values.dtype.type, basestring):
+    if issubclass(values.dtype.type, six.string_types):
         values = np.array(values, dtype='O')
 
     f = lambda htype, caster: _match_generic(to_match, values, htype, caster)
diff --git a/pandas/core/array.py b/pandas/core/array.py
index 0026dfcec..d1d29649d 100644
--- a/pandas/core/array.py
+++ b/pandas/core/array.py
@@ -2,7 +2,9 @@
 Isolate pandas's exposure to NumPy
 """
 
+from pandas.util import compat
 import numpy as np
+import six
 
 Array = np.ndarray
 
@@ -16,7 +18,7 @@ _dtypes = {
 
 _lift_types = []
 
-for _k, _v in _dtypes.iteritems():
+for _k, _v in compat.iteritems(_dtypes):
     for _i in _v:
         _lift_types.append(_k + str(_i))
 
diff --git a/pandas/core/common.py b/pandas/core/common.py
index eba0379a2..25353fe33 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -2,6 +2,7 @@
 Misc tools for implementing data structures
 """
 
+from pandas.util.py3compat import range, long
 import itertools
 import re
 from datetime import datetime
@@ -21,6 +22,8 @@ from pandas.util.py3compat import StringIO, BytesIO
 
 from pandas.core.config import get_option
 from pandas.core import array as pa
+import six
+from six.moves import map
 
 # XXX: HACK for NumPy 1.5.1 to suppress warnings
 try:
@@ -688,7 +691,7 @@ def _infer_dtype_from_scalar(val):
         dtype = val.dtype
         val   = val.item()
 
-    elif isinstance(val, basestring):
+    elif isinstance(val, six.string_types):
 
         # If we create an empty array using a string to infer
         # the dtype, NumPy will only allocate one character per entry
@@ -781,7 +784,7 @@ def _maybe_promote(dtype, fill_value=np.nan):
         dtype = np.object_
 
     # in case we have a string that looked like a number
-    if issubclass(np.dtype(dtype).type, basestring):
+    if issubclass(np.dtype(dtype).type, six.string_types):
         dtype = np.object_
 
     return dtype, fill_value
@@ -1168,7 +1171,7 @@ def _possibly_cast_to_datetime(value, dtype, coerce = False):
     """ try to cast the array/value to a datetimelike dtype, converting float nan to iNaT """
 
     if dtype is not None:
-        if isinstance(dtype, basestring):
+        if isinstance(dtype, six.string_types):
             dtype = np.dtype(dtype)
 
         is_datetime64  = is_datetime64_dtype(dtype)
@@ -1338,7 +1341,7 @@ def _join_unicode(lines, sep=''):
     try:
         return sep.join(lines)
     except UnicodeDecodeError:
-        sep = unicode(sep)
+        sep = six.text_type(sep)
         return sep.join([x.decode('utf-8') if isinstance(x, str) else x
                          for x in lines])
 
@@ -1398,7 +1401,7 @@ def banner(message):
     return '%s\n%s\n%s' % (bar, message, bar)
 
 def _long_prod(vals):
-    result = 1L
+    result = long(1)
     for x in vals:
         result *= x
     return result
@@ -1478,7 +1481,7 @@ def _asarray_tuplesafe(values, dtype=None):
 
     result = np.asarray(values, dtype=dtype)
 
-    if issubclass(result.dtype.type, basestring):
+    if issubclass(result.dtype.type, six.string_types):
         result = np.asarray(values, dtype=object)
 
     if result.ndim == 2:
@@ -1494,7 +1497,7 @@ def _asarray_tuplesafe(values, dtype=None):
 
 
 def _index_labels_to_array(labels):
-    if isinstance(labels, (basestring, tuple)):
+    if isinstance(labels, (six.string_types, tuple)):
         labels = [labels]
 
     if not isinstance(labels, (list, np.ndarray)):
@@ -1609,13 +1612,13 @@ def is_re_compilable(obj):
 
 
 def is_list_like(arg):
-    return hasattr(arg, '__iter__') and not isinstance(arg, basestring)
+    return hasattr(arg, '__iter__') and not isinstance(arg, six.string_types)
 
 def _is_sequence(x):
     try:
         iter(x)
         len(x) # it has a length
-        return not isinstance(x, basestring) and True
+        return not isinstance(x, six.string_types) and True
     except Exception:
         return False
 
@@ -1703,7 +1706,10 @@ class UTF8Recoder:
         return self.reader.readline().encode('utf-8')
 
     def next(self):
-        return self.reader.next().encode("utf-8")
+        return next(self.reader).encode("utf-8")
+
+    # Python 3 iterator
+    __next__ = next
 
 
 def _get_handle(path, mode, encoding=None, compression=None):
@@ -1752,8 +1758,11 @@ else:
             self.reader = csv.reader(f, dialect=dialect, **kwds)
 
         def next(self):
-            row = self.reader.next()
-            return [unicode(s, "utf-8") for s in row]
+            row = next(self.reader)
+            return [six.text_type(s, "utf-8") for s in row]
+
+        # python 3 iterator
+        __next__ = next
 
         def __iter__(self):  # pragma: no cover
             return self
@@ -1951,9 +1960,9 @@ def _pprint_seq(seq, _nest_lvl=0, **kwds):
     bounds length of printed sequence, depending on options
     """
     if isinstance(seq,set):
-        fmt = u"set([%s])"
+        fmt = six.u("set([%s])")
     else:
-        fmt = u"[%s]" if hasattr(seq, '__setitem__') else u"(%s)"
+        fmt = six.u("[%s]") if hasattr(seq, '__setitem__') else six.u("(%s)")
 
     nitems = get_option("max_seq_items") or len(seq)
 
@@ -1976,10 +1985,10 @@ def _pprint_dict(seq, _nest_lvl=0,**kwds):
     internal. pprinter for iterables. you should probably use pprint_thing()
     rather then calling this directly.
     """
-    fmt = u"{%s}"
+    fmt = six.u("{%s}")
     pairs = []
 
-    pfmt = u"%s: %s"
+    pfmt = six.u("%s: %s")
 
     nitems = get_option("max_seq_items") or len(seq)
 
@@ -2025,7 +2034,7 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
         #should deal with it himself.
 
         try:
-            result = unicode(thing)  # we should try this first
+            result = six.text_type(thing)  # we should try this first
         except UnicodeDecodeError:
             # either utf-8 or we replace errors
             result = str(thing).decode('utf-8', "replace")
@@ -2045,11 +2054,11 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
         for c in escape_chars:
             result = result.replace(c, translate[c])
 
-        return unicode(result)
+        return six.text_type(result)
 
     if (py3compat.PY3 and hasattr(thing, '__next__')) or \
             hasattr(thing, 'next'):
-        return unicode(thing)
+        return six.text_type(thing)
     elif (isinstance(thing, dict) and
           _nest_lvl < get_option("display.pprint_nest_depth")):
         result = _pprint_dict(thing, _nest_lvl,quote_strings=True)
@@ -2057,7 +2066,7 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
             get_option("display.pprint_nest_depth"):
         result = _pprint_seq(thing, _nest_lvl, escape_chars=escape_chars,
                              quote_strings=quote_strings)
-    elif isinstance(thing,basestring) and quote_strings:
+    elif isinstance(thing,six.string_types) and quote_strings:
         if py3compat.PY3:
             fmt = "'%s'"
         else:
@@ -2066,7 +2075,7 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
     else:
         result = as_escaped_unicode(thing)
 
-    return unicode(result)  # always unicode
+    return six.text_type(result)  # always unicode
 
 
 def pprint_thing_encoded(object, encoding='utf-8', errors='replace', **kwds):
diff --git a/pandas/core/config.py b/pandas/core/config.py
index ae7c71d08..c66911c12 100644
--- a/pandas/core/config.py
+++ b/pandas/core/config.py
@@ -1,9 +1,7 @@
 """
 The config module holds package-wide configurables and provides
 a uniform API for working with them.
-"""
 
-"""
 Overview
 ========
 
@@ -54,6 +52,8 @@ import re
 
 from collections import namedtuple
 import warnings
+import six
+from six.moves import map
 
 DeprecatedOption = namedtuple('DeprecatedOption', 'key msg rkey removal_ver')
 RegisteredOption = namedtuple(
@@ -149,7 +149,7 @@ def _describe_option(pat='', _print_desc=True):
     if len(keys) == 0:
         raise KeyError('No such keys(s)')
 
-    s = u''
+    s = six.u('')
     for k in keys:  # filter by pat
         s += _build_option_description(k)
 
@@ -588,9 +588,9 @@ def _build_option_description(k):
     o = _get_registered_option(k)
     d = _get_deprecated_option(k)
 
-    s = u'%s: ' % k
+    s = six.u('%s: ') % k
     if o:
-        s += u'[default: %s] [currently: %s]' % (o.defval, _get_option(k, True))
+        s += six.u('[default: %s] [currently: %s]') % (o.defval, _get_option(k, True))
 
     if o.doc:
         s += '\n' + '\n    '.join(o.doc.strip().split('\n'))
@@ -598,9 +598,9 @@ def _build_option_description(k):
         s += 'No description available.\n'
 
     if d:
-        s += u'\n\t(Deprecated'
-        s += (u', use `%s` instead.' % d.rkey if d.rkey else '')
-        s += u')\n'
+        s += six.u('\n\t(Deprecated')
+        s += (six.u(', use `%s` instead.') % d.rkey if d.rkey else '')
+        s += six.u(')\n')
 
     s += '\n'
     return s
@@ -734,7 +734,7 @@ def is_instance_factory(_type):
         if isinstance(_type,(tuple,list)) :
             if not any([isinstance(x,t) for t in _type]):
                 from pandas.core.common import pprint_thing as pp
-                pp_values = map(pp, _type)
+                pp_values = list(map(pp, _type))
                 raise ValueError("Value must be an instance of %s" % pp("|".join(pp_values)))
         elif not isinstance(x, _type):
             raise ValueError("Value must be an instance of '%s'" % str(_type))
@@ -745,7 +745,7 @@ def is_one_of_factory(legal_values):
     def inner(x):
         from pandas.core.common import pprint_thing as pp
         if not x in legal_values:
-            pp_values = map(pp, legal_values)
+            pp_values = list(map(pp, legal_values))
             raise ValueError("Value must be one of %s" % pp("|".join(pp_values)))
 
     return inner
@@ -756,5 +756,5 @@ is_int = is_type_factory(int)
 is_bool = is_type_factory(bool)
 is_float = is_type_factory(float)
 is_str = is_type_factory(str)
-is_unicode = is_type_factory(unicode)
+is_unicode = is_type_factory(six.text_type)
 is_text = is_instance_factory(basestring)
diff --git a/pandas/core/expressions.py b/pandas/core/expressions.py
index abe891b82..27c06e23b 100644
--- a/pandas/core/expressions.py
+++ b/pandas/core/expressions.py
@@ -93,10 +93,10 @@ def _evaluate_numexpr(op, op_str, a, b, raise_on_error = False, **eval_kwargs):
                                  local_dict={ 'a_value' : a_value, 
                                               'b_value' : b_value }, 
                                  casting='safe', **eval_kwargs)
-        except (ValueError), detail:
+        except (ValueError) as detail:
             if 'unknown type object' in str(detail):
                 pass
-        except (Exception), detail:
+        except (Exception) as detail:
             if raise_on_error:
                 raise TypeError(str(detail))
 
@@ -126,10 +126,10 @@ def _where_numexpr(cond, a, b, raise_on_error = False):
                                               'a_value' : a_value, 
                                               'b_value' : b_value }, 
                                  casting='safe')
-        except (ValueError), detail:
+        except (ValueError) as detail:
             if 'unknown type object' in str(detail):
                 pass
-        except (Exception), detail:
+        except (Exception) as detail:
             if raise_on_error:
                 raise TypeError(str(detail))
 
diff --git a/pandas/core/format.py b/pandas/core/format.py
index c9beb729b..1b78b501b 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -1,13 +1,13 @@
+from __future__ import print_function
 # pylint: disable=W0141
 
-from itertools import izip
+from pandas.util.py3compat import range
+from pandas.util import compat
 import sys
+import six
+from six.moves import map, zip, reduce
 
-try:
-    from StringIO import StringIO
-except:
-    from io import StringIO
-
+from pandas.util.py3compat import StringIO
 from pandas.core.common import adjoin, isnull, notnull
 from pandas.core.index import Index, MultiIndex, _ensure_index
 from pandas.util import py3compat
@@ -71,7 +71,7 @@ class SeriesFormatter(object):
     def __init__(self, series, buf=None, header=True, length=True,
                  na_rep='NaN', name=False, float_format=None, dtype=True):
         self.series = series
-        self.buf = buf if buf is not None else StringIO(u"")
+        self.buf = buf if buf is not None else StringIO()
         self.name = name
         self.na_rep = na_rep
         self.length = length
@@ -83,7 +83,7 @@ class SeriesFormatter(object):
         self.dtype  = dtype
 
     def _get_footer(self):
-        footer = u''
+        footer = six.u('')
 
         if self.name:
             if getattr(self.series.index, 'freq', None):
@@ -108,7 +108,7 @@ class SeriesFormatter(object):
                     footer += ', '
                 footer += 'dtype: %s' % com.pprint_thing(self.series.dtype.name)
 
-        return unicode(footer)
+        return six.text_type(footer)
 
     def _get_formatted_index(self):
         index = self.series.index
@@ -131,7 +131,7 @@ class SeriesFormatter(object):
         series = self.series
 
         if len(series) == 0:
-            return u''
+            return six.u('')
 
         fmt_index, have_header = self._get_formatted_index()
         fmt_values = self._get_formatted_values()
@@ -140,7 +140,7 @@ class SeriesFormatter(object):
         pad_space = min(maxlen, 60)
 
         result = ['%s   %s'] * len(fmt_values)
-        for i, (k, v) in enumerate(izip(fmt_index[1:], fmt_values)):
+        for i, (k, v) in enumerate(zip(fmt_index[1:], fmt_values)):
             idx = k.ljust(pad_space)
             result[i] = result[i] % (idx, v)
 
@@ -151,7 +151,7 @@ class SeriesFormatter(object):
         if footer:
             result.append(footer)
 
-        return unicode(u'\n'.join(result))
+        return six.text_type(six.u('\n').join(result))
 
 def _strlen_func():
     if py3compat.PY3:  # pragma: no cover
@@ -285,7 +285,7 @@ class DataFrameFormatter(TableFormatter):
         frame = self.frame
 
         if len(frame.columns) == 0 or len(frame.index) == 0:
-            info_line = (u'Empty %s\nColumns: %s\nIndex: %s'
+            info_line = (six.u('Empty %s\nColumns: %s\nIndex: %s')
                          % (type(self.frame).__name__,
                             com.pprint_thing(frame.columns),
                             com.pprint_thing(frame.index)))
@@ -347,7 +347,7 @@ class DataFrameFormatter(TableFormatter):
         frame = self.frame
 
         if len(frame.columns) == 0 or len(frame.index) == 0:
-            info_line = (u'Empty %s\nColumns: %s\nIndex: %s'
+            info_line = (six.u('Empty %s\nColumns: %s\nIndex: %s')
                          % (type(self.frame).__name__,
                             frame.columns, frame.index))
             strcols = [[info_line]]
@@ -360,7 +360,7 @@ class DataFrameFormatter(TableFormatter):
                 column_format = 'l%s' % ''.join(map(get_col_type, dtypes))
             else:
                 column_format = '%s' % ''.join(map(get_col_type, dtypes))
-        elif not isinstance(column_format, basestring):
+        elif not isinstance(column_format, six.string_types):
             raise AssertionError(('column_format must be str or unicode, not %s'
                                   % type(column_format)))
 
@@ -369,7 +369,7 @@ class DataFrameFormatter(TableFormatter):
             buf.write('\\toprule\n')
 
             nlevels = frame.index.nlevels
-            for i, row in enumerate(izip(*strcols)):
+            for i, row in enumerate(zip(*strcols)):
                 if i == nlevels:
                     buf.write('\\midrule\n')  # End of header
                 crow = [(x.replace('_', '\\_')
@@ -383,7 +383,7 @@ class DataFrameFormatter(TableFormatter):
 
         if hasattr(self.buf, 'write'):
             write(self.buf, frame, column_format, strcols)
-        elif isinstance(self.buf, basestring):
+        elif isinstance(self.buf, six.string_types):
             with open(self.buf, 'w') as f:
                 write(f, frame, column_format, strcols)
         else:
@@ -404,7 +404,7 @@ class DataFrameFormatter(TableFormatter):
         html_renderer = HTMLFormatter(self, classes=classes)
         if hasattr(self.buf, 'write'):
             html_renderer.write_result(self.buf)
-        elif isinstance(self.buf, basestring):
+        elif isinstance(self.buf, six.string_types):
             with open(self.buf, 'w') as f:
                 html_renderer.write_result(f)
         else:
@@ -419,13 +419,13 @@ class DataFrameFormatter(TableFormatter):
 
         if isinstance(self.columns, MultiIndex):
             fmt_columns = self.columns.format(sparsify=False, adjoin=False)
-            fmt_columns = zip(*fmt_columns)
+            fmt_columns = list(zip(*fmt_columns))
             dtypes = self.frame.dtypes.values
             need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))
-            str_columns = zip(*[[' ' + y
+            str_columns = list(zip(*[[' ' + y
                                 if y not in self.formatters and need_leadsp[x]
                                 else y for y in x]
-                                for x in fmt_columns])
+                                for x in fmt_columns]))
             if self.sparsify:
                 str_columns = _sparsify(str_columns)
 
@@ -718,7 +718,7 @@ class HTMLFormatter(TableFormatter):
 
         idx_values = frame.index.format(sparsify=False, adjoin=False,
                                         names=False)
-        idx_values = zip(*idx_values)
+        idx_values = list(zip(*idx_values))
 
         if self.fmt.sparsify:
 
@@ -749,9 +749,9 @@ class HTMLFormatter(TableFormatter):
                               nindex_levels=len(levels) - sparse_offset)
         else:
             for i in range(len(frame)):
-                idx_values = zip(*frame.index.format(sparsify=False,
+                idx_values = list(zip(*frame.index.format(sparsify=False,
                                                      adjoin=False,
-                                                     names=False))
+                                                     names=False)))
                 row = []
                 row.extend(idx_values[i])
                 row.extend(fmt_values[j][i] for j in range(ncols))
@@ -1069,7 +1069,7 @@ class CSVFormatter(object):
         chunksize = self.chunksize
         chunks = int(nrows / chunksize)+1
 
-        for i in xrange(chunks):
+        for i in range(chunks):
             start_i = i * chunksize
             end_i = min((i + 1) * chunksize, nrows)
             if start_i >= end_i:
@@ -1304,7 +1304,7 @@ class ExcelFormatter(object):
                 index_labels = self.index_label
 
             # if index labels are not empty go ahead and dump
-            if (filter(lambda x: x is not None, index_labels)
+            if (any(x is not None for x in index_labels)
                     and self.header is not False):
                 # if isinstance(self.df.columns, MultiIndex):
                 #     self.rowcounter += 1
@@ -1836,9 +1836,9 @@ class EngFormatter(object):
         mant = sign * dnum / (10 ** pow10)
 
         if self.accuracy is None:  # pragma: no cover
-            format_str = u"% g%s"
+            format_str = six.u("% g%s")
         else:
-            format_str = (u"%% .%if%%s" % self.accuracy)
+            format_str = (six.u("%% .%if%%s") % self.accuracy)
 
         formatted = format_str % (mant, prefix)
 
@@ -1864,8 +1864,8 @@ def set_eng_float_format(precision=None, accuracy=3, use_eng_prefix=False):
 
 
 def _put_lines(buf, lines):
-    if any(isinstance(x, unicode) for x in lines):
-        lines = [unicode(x) for x in lines]
+    if any(isinstance(x, six.text_type) for x in lines):
+        lines = [six.text_type(x) for x in lines]
     buf.write('\n'.join(lines))
 
 
@@ -1900,4 +1900,4 @@ if __name__ == '__main__':
                     1134250., 1219550., 855736.85, 1042615.4286,
                     722621.3043, 698167.1818, 803750.])
     fmt = FloatArrayFormatter(arr, digits=7)
-    print (fmt.get_result())
+    print(fmt.get_result())
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 22dc27ff9..94b36ffed 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -12,12 +12,13 @@ labeling information
 # pylint: disable=E1101,E1103
 # pylint: disable=W0212,W0231,W0703,W0622
 
-from itertools import izip
-from StringIO import StringIO
+from six.moves import zip
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
+from pandas.util import compat
 import operator
 import sys
 import collections
-import itertools
 
 from numpy import nan as NA
 import numpy as np
@@ -57,6 +58,8 @@ import pandas.tslib as tslib
 import pandas.algos as _algos
 
 from pandas.core.config import get_option, set_option
+import six
+from six.moves import map
 
 #----------------------------------------------------------------------
 # Docstring templates
@@ -440,7 +443,7 @@ class DataFrame(NDFrame):
                                   'incompatible data and dtype')
 
             if arr.ndim == 0 and index is not None and columns is not None:
-                if isinstance(data, basestring) and dtype is None:
+                if isinstance(data, six.string_types) and dtype is None:
                     dtype = np.object_
                 if dtype is None:
                     dtype, data = _infer_dtype_from_scalar(data)
@@ -656,7 +659,7 @@ class DataFrame(NDFrame):
         Invoked by unicode(df) in py2 only. Yields a Unicode String in both
         py2/py3.
         """
-        buf = StringIO(u"")
+        buf = StringIO(six.u(""))
         fits_vertical = self._repr_fits_vertical_()
         fits_horizontal = False
         if fits_vertical:
@@ -683,7 +686,7 @@ class DataFrame(NDFrame):
                 self.info(buf=buf, verbose=verbose)
 
         value = buf.getvalue()
-        if not  type(value) == unicode:
+        if not isinstance(value, six.text_type):
             raise AssertionError()
 
         return value
@@ -715,7 +718,7 @@ class DataFrame(NDFrame):
                         'max-width:1500px;overflow:auto;">\n' +
                         self.to_html() + '\n</div>')
             else:
-                buf = StringIO(u"")
+                buf = StringIO(six.u(""))
                 max_info_rows = get_option('display.max_info_rows')
                 verbose = (max_info_rows is None or
                            self.shape[0] <= max_info_rows)
@@ -769,7 +772,7 @@ class DataFrame(NDFrame):
             A generator that iterates over the rows of the frame.
         """
         columns = self.columns
-        for k, v in izip(self.index, self.values):
+        for k, v in zip(self.index, self.values):
             s = v.view(Series)
             s.index = columns
             s.name = k
@@ -785,8 +788,8 @@ class DataFrame(NDFrame):
             arrays.append(self.index)
 
         # use integer indexing because of possible duplicate column names
-        arrays.extend(self.iloc[:, k] for k in xrange(len(self.columns)))
-        return izip(*arrays)
+        arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))
+        return zip(*arrays)
 
     iterkv = iteritems
     if py3compat.PY3:  # pragma: no cover
@@ -1031,7 +1034,7 @@ class DataFrame(NDFrame):
                 if py3compat.PY3:
                     first_row = next(data)
                 else:
-                    first_row = data.next()
+                    first_row = next(data)
             except StopIteration:
                 return cls(index=index, columns=columns)
 
@@ -1093,7 +1096,7 @@ class DataFrame(NDFrame):
 
         result_index = None
         if index is not None:
-            if (isinstance(index, basestring) or
+            if (isinstance(index, six.string_types) or
                     not hasattr(index, "__iter__")):
                 i = columns.get_loc(index)
                 exclude.add(index)
@@ -1148,7 +1151,7 @@ class DataFrame(NDFrame):
             else:
                 if isinstance(self.index, MultiIndex):
                     # array of tuples to numpy cols. copy copy copy
-                    ix_vals = map(np.array,zip(*self.index.values))
+                    ix_vals = list(map(np.array,zip(*self.index.values)))
                 else:
                     ix_vals = [self.index.values]
 
@@ -1194,7 +1197,7 @@ class DataFrame(NDFrame):
         -------
         frame : DataFrame
         """
-        keys, values = zip(*items)
+        keys, values = list(zip(*items))
 
         if orient == 'columns':
             if columns is not None:
@@ -1452,7 +1455,7 @@ class DataFrame(NDFrame):
         """
         from pandas.io.excel import ExcelWriter
         need_save = False
-        if isinstance(excel_writer, basestring):
+        if isinstance(excel_writer, six.string_types):
             excel_writer = ExcelWriter(excel_writer)
             need_save = True
 
@@ -2419,8 +2422,6 @@ class DataFrame(NDFrame):
             The found values
 
         """
-        from itertools import izip
-
         n = len(row_labels)
         if n != len(col_labels):
             raise AssertionError('Row labels must have same size as '
@@ -2439,7 +2440,7 @@ class DataFrame(NDFrame):
             result = values.flat[flat_index]
         else:
             result = np.empty(n, dtype='O')
-            for i, (r, c) in enumerate(izip(row_labels, col_labels)):
+            for i, (r, c) in enumerate(zip(row_labels, col_labels)):
                 result[i] = self.get_value(r, c)
 
         if result.dtype == 'O':
@@ -2910,7 +2911,7 @@ class DataFrame(NDFrame):
 
             if not drop:
                 names = self.index.names
-                zipped = zip(self.index.levels, self.index.labels)
+                zipped = list(zip(self.index.levels, self.index.labels))
 
                 multi_col = isinstance(self.columns, MultiIndex)
                 for i, (lev, lab) in reversed(list(enumerate(zipped))):
@@ -3030,7 +3031,7 @@ class DataFrame(NDFrame):
         if items is not None:
             return self.reindex(columns=[r for r in items if r in self])
         elif like:
-            matchf = lambda x: (like in x if isinstance(x, basestring)
+            matchf = lambda x: (like in x if isinstance(x, six.string_types)
                                 else like in str(x))
             return self.select(matchf, axis=1)
         elif regex:
@@ -3152,7 +3153,7 @@ class DataFrame(NDFrame):
         if cols is None:
             values = list(_m8_to_i8(self.values.T))
         else:
-            if np.iterable(cols) and not isinstance(cols, basestring):
+            if np.iterable(cols) and not isinstance(cols, six.string_types):
                 if isinstance(cols, tuple):
                     if cols in self.columns:
                         values = [self[cols]]
@@ -3600,7 +3601,7 @@ class DataFrame(NDFrame):
                 regex = True
 
             items = to_replace.items()
-            keys, values = itertools.izip(*items)
+            keys, values = zip(*items)
 
             are_mappings = [isinstance(v, (dict, Series)) for v in values]
 
@@ -4315,7 +4316,7 @@ class DataFrame(NDFrame):
 
         offset = _resolve_offset(freq, kwds)
 
-        if isinstance(offset, basestring):
+        if isinstance(offset, six.string_types):
             offset = datetools.to_offset(offset)
 
         if offset is None:
@@ -4456,7 +4457,7 @@ class DataFrame(NDFrame):
             values = self.values
             series_gen = (Series.from_array(arr, index=res_columns, name=name)
                           for i, (arr, name) in
-                          enumerate(izip(values, res_index)))
+                          enumerate(zip(values, res_index)))
         else:
             raise ValueError('Axis must be 0 or 1, got %s' % str(axis))
 
@@ -4479,7 +4480,7 @@ class DataFrame(NDFrame):
                 for i, v in enumerate(series_gen):
                     results[i] = func(v)
                     keys.append(v.name)
-            except Exception, e:
+            except Exception as e:
                 try:
                     if hasattr(e, 'args'):
                         k = res_index[i]
@@ -4535,7 +4536,7 @@ class DataFrame(NDFrame):
     def applymap(self, func):
         """
         Apply a function to a DataFrame that is intended to operate
-        elementwise, i.e. like doing map(func, series) for each series in the
+        elementwise, i.e. like doing list(map(func, series)) for each series in the
         DataFrame
 
         Parameters
@@ -4888,7 +4889,7 @@ class DataFrame(NDFrame):
                            series.min(), series.quantile(lb), series.median(),
                            series.quantile(ub), series.max()])
 
-        return self._constructor(map(list, zip(*destat)), index=destat_columns,
+        return self._constructor(list(map(list, zip(*destat))), index=destat_columns,
                                  columns=numdata.columns)
 
     #----------------------------------------------------------------------
@@ -4947,7 +4948,7 @@ class DataFrame(NDFrame):
         # python 2.5
         mask = notnull(frame.values).view(np.uint8)
 
-        if isinstance(level, basestring):
+        if isinstance(level, six.string_types):
             level = self.index._get_level_number(level)
 
         level_index = frame.index.levels[level]
@@ -5849,7 +5850,7 @@ def _to_arrays(data, columns, coerce_float=False, dtype=None):
         return arrays, columns
     else:
         # last ditch effort
-        data = map(tuple, data)
+        data = list(map(tuple, data))
         return _list_to_arrays(data, columns,
                                coerce_float=coerce_float,
                                dtype=dtype)
@@ -5923,7 +5924,7 @@ def _convert_object_array(content, columns, coerce_float=False, dtype=None):
 
 
 def _get_names_from_index(data):
-    index = range(len(data))
+    index = list(range(len(data)))
     has_some_name = any([s.name is not None for s in data])
     if not has_some_name:
         return index
@@ -5996,7 +5997,7 @@ def install_ipython_completers():  # pragma: no cover
     @complete_object.when_type(DataFrame)
     def complete_dataframe(obj, prev_completions):
         return prev_completions + [c for c in obj.columns
-                                   if isinstance(c, basestring) and py3compat.isidentifier(c)]
+                                   if isinstance(c, six.string_types) and py3compat.isidentifier(c)]
 
 
 # Importing IPython brings in about 200 modules, so we want to avoid it unless
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 6be5f456b..2dce7430c 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1,5 +1,6 @@
 # pylint: disable=W0231,E1101
 
+from pandas.util import compat
 import numpy as np
 import pandas.lib as lib
 from pandas.core.base import PandasObject
@@ -9,6 +10,8 @@ import pandas.core.indexing as indexing
 from pandas.core.indexing import _maybe_convert_indices
 from pandas.tseries.index import DatetimeIndex
 import pandas.core.common as com
+import six
+from six.moves import map, zip
 
 
 class PandasError(Exception):
@@ -23,7 +26,7 @@ class PandasContainer(PandasObject):
     }
 
     _AXIS_ALIASES = {}
-    _AXIS_NAMES = dict((v, k) for k, v in _AXIS_NUMBERS.iteritems())
+    _AXIS_NAMES = dict((v, k) for k, v in compat.iteritems(_AXIS_NUMBERS))
 
     def to_pickle(self, path):
         """
@@ -77,7 +80,7 @@ class PandasContainer(PandasObject):
 
     def _get_axis_name(self, axis):
         axis = self._AXIS_ALIASES.get(axis, axis)
-        if isinstance(axis, basestring):
+        if isinstance(axis, six.string_types):
             if axis in self._AXIS_NUMBERS:
                 return axis
         else:
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index cc0a2b758..528d7baca 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -1,6 +1,9 @@
-from itertools import izip
 import types
 import numpy as np
+from pandas.util.py3compat import range, long
+import six
+from six.moves import zip
+from pandas.util import compat
 
 from pandas.core.base import PandasObject
 from pandas.core.categorical import Categorical
@@ -588,7 +591,7 @@ class Grouper(object):
         splitter = self._get_splitter(data, axis=axis,
                                       keep_internal=keep_internal)
         keys = self._get_group_keys()
-        for key, (i, group) in izip(keys, splitter):
+        for key, (i, group) in zip(keys, splitter):
             yield key, group
 
     def _get_splitter(self, data, axis=0, keep_internal=True):
@@ -616,13 +619,13 @@ class Grouper(object):
             try:
                 values, mutated = splitter.fast_apply(f, group_keys)
                 return group_keys, values, mutated
-            except (Exception), detail:
+            except (Exception) as detail:
                 # we detect a mutatation of some kind
                 # so take slow path
                 pass
 
         result_values = []
-        for key, (i, group) in izip(group_keys, splitter):
+        for key, (i, group) in zip(group_keys, splitter):
             object.__setattr__(group, 'name', key)
 
             # group might be modified
@@ -671,7 +674,7 @@ class Grouper(object):
         if len(self.groupings) == 1:
             return self.groupings[0].groups
         else:
-            to_groupby = zip(*(ping.grouper for ping in self.groupings))
+            to_groupby = list(zip(*(ping.grouper for ping in self.groupings)))
             to_groupby = Index(to_groupby)
 
             return self.axis.groupby(to_groupby)
@@ -727,12 +730,12 @@ class Grouper(object):
             return [self.groupings[0].group_index]
 
         if self._overflow_possible:
-            recons_labels = [np.array(x) for x in izip(*obs_ids)]
+            recons_labels = [np.array(x) for x in zip(*obs_ids)]
         else:
             recons_labels = decons_group_index(obs_ids, self.shape)
 
         name_list = []
-        for ping, labels in izip(self.groupings, recons_labels):
+        for ping, labels in zip(self.groupings, recons_labels):
             labels = com._ensure_platform_int(labels)
             name_list.append(ping.group_index.take(labels))
 
@@ -1004,7 +1007,7 @@ class BinGrouper(Grouper):
         """
         if axis == 0:
             start = 0
-            for edge, label in izip(self.bins, self.binlabels):
+            for edge, label in zip(self.bins, self.binlabels):
                 yield label, data[start:edge]
                 start = edge
 
@@ -1012,14 +1015,14 @@ class BinGrouper(Grouper):
                 yield self.binlabels[-1], data[start:]
         else:
             start = 0
-            for edge, label in izip(self.bins, self.binlabels):
-                inds = range(start, edge)
+            for edge, label in zip(self.bins, self.binlabels):
+                inds = list(range(start, edge))
                 yield label, data.take(inds, axis=axis)
                 start = edge
 
             n = len(data.axes[axis])
             if start < n:
-                inds = range(start, n)
+                inds = list(range(start, n))
                 yield self.binlabels[-1], data.take(inds, axis=axis)
 
     def apply(self, f, data, axis=0, keep_internal=False):
@@ -1257,12 +1260,12 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True):
 
     if level is not None:
         if not isinstance(group_axis, MultiIndex):
-            if isinstance(level, basestring):
+            if isinstance(level, six.string_types):
                 if obj.index.name != level:
                     raise ValueError('level name %s is not the name of the index' % level)
             elif level > 0:
                 raise ValueError('level > 0 only valid with MultiIndex')
-            
+
             level = None
             key = group_axis
 
@@ -1305,7 +1308,7 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True):
 
     groupings = []
     exclusions = []
-    for i, (gpr, level) in enumerate(izip(keys, levels)):
+    for i, (gpr, level) in enumerate(zip(keys, levels)):
         name = None
         try:
             obj._data.items.get_loc(gpr)
@@ -1334,7 +1337,7 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True):
 
 
 def _is_label_like(val):
-    return isinstance(val, basestring) or np.isscalar(val)
+    return isinstance(val, six.string_types) or np.isscalar(val)
 
 
 def _convert_grouper(axis, grouper):
@@ -1406,7 +1409,7 @@ class SeriesGroupBy(GroupBy):
         -------
         Series or DataFrame
         """
-        if isinstance(func_or_funcs, basestring):
+        if isinstance(func_or_funcs, six.string_types):
             return getattr(self, func_or_funcs)(*args, **kwargs)
 
         if hasattr(func_or_funcs, '__iter__'):
@@ -1446,11 +1449,11 @@ class SeriesGroupBy(GroupBy):
             # list of functions / function names
             columns = []
             for f in arg:
-                if isinstance(f, basestring):
+                if isinstance(f, six.string_types):
                     columns.append(f)
                 else:
                     columns.append(f.__name__)
-            arg = zip(columns, arg)
+            arg = list(zip(columns, arg))
 
         results = {}
 
@@ -1534,7 +1537,7 @@ class SeriesGroupBy(GroupBy):
             result = result.values
         dtype = result.dtype
 
-        if isinstance(func, basestring):
+        if isinstance(func, six.string_types):
             wrapper = lambda x: getattr(x, func)(*args, **kwargs)
         else:
             wrapper = lambda x: func(x, *args, **kwargs)
@@ -1576,7 +1579,7 @@ class SeriesGroupBy(GroupBy):
         -------
         filtered : Series
         """
-        if isinstance(func, basestring):
+        if isinstance(func, six.string_types):
             wrapper = lambda x: getattr(x, func)(*args, **kwargs)
         else:
             wrapper = lambda x: func(x, *args, **kwargs)
@@ -1690,7 +1693,7 @@ class NDFrameGroupBy(GroupBy):
 
     @Appender(_agg_doc)
     def aggregate(self, arg, *args, **kwargs):
-        if isinstance(arg, basestring):
+        if isinstance(arg, six.string_types):
             return getattr(self, arg)(*args, **kwargs)
 
         result = OrderedDict()
@@ -1905,7 +1908,7 @@ class NDFrameGroupBy(GroupBy):
                     if not all_indexed_same:
                         return self._concat_objects(keys, values,
                                                     not_indexed_same=not_indexed_same)
-                    
+
                 try:
                     if self.axis == 0:
 
@@ -1998,13 +2001,13 @@ class NDFrameGroupBy(GroupBy):
         return concatenated
 
     def _define_paths(self, func, *args, **kwargs):
-        if isinstance(func, basestring):
+        if isinstance(func, six.string_types):
             fast_path = lambda group: getattr(group, func)(*args, **kwargs)
             slow_path = lambda group: group.apply(lambda x: getattr(x, func)(*args, **kwargs), axis=self.axis)
         else:
             fast_path = lambda group: func(group, *args, **kwargs)
             slow_path = lambda group: group.apply(lambda x: func(x, *args, **kwargs), axis=self.axis)
-        return fast_path, slow_path 
+        return fast_path, slow_path
 
     def _choose_path(self, fast_path, slow_path, group):
         path = slow_path
@@ -2249,7 +2252,7 @@ class PanelGroupBy(NDFrameGroupBy):
         -------
         aggregated : Panel
         """
-        if isinstance(arg, basestring):
+        if isinstance(arg, six.string_types):
             return getattr(self, arg)(*args, **kwargs)
 
         return self._aggregate_generic(arg, *args, **kwargs)
@@ -2332,7 +2335,7 @@ class DataSplitter(object):
 
         starts, ends = lib.generate_slices(self.slabels, self.ngroups)
 
-        for i, (start, end) in enumerate(izip(starts, ends)):
+        for i, (start, end) in enumerate(zip(starts, ends)):
             # Since I'm now compressing the group ids, it's now not "possible"
             # to produce empty slices because such groups would not be observed
             # in the data
@@ -2436,7 +2439,7 @@ def get_group_index(label_list, shape):
     n = len(label_list[0])
     group_index = np.zeros(n, dtype=np.int64)
     mask = np.zeros(n, dtype=bool)
-    for i in xrange(len(shape)):
+    for i in range(len(shape)):
         stride = np.prod([x for x in shape[i + 1:]], dtype=np.int64)
         group_index += com._ensure_int64(label_list[i]) * stride
         mask |= label_list[i] < 0
@@ -2448,7 +2451,7 @@ _INT64_MAX = np.iinfo(np.int64).max
 
 
 def _int64_overflow_possible(shape):
-    the_prod = 1L
+    the_prod = long(1)
     for x in shape:
         the_prod *= long(x)
 
@@ -2461,7 +2464,7 @@ def decons_group_index(comp_labels, shape):
     factor = 1
     y = 0
     x = comp_labels
-    for i in reversed(xrange(len(shape))):
+    for i in reversed(range(len(shape))):
         labels = (x - y) % (factor * shape[i]) // factor
         np.putmask(labels, comp_labels < 0, -1)
         label_list.append(labels)
@@ -2503,7 +2506,7 @@ def _lexsort_indexer(keys, orders=None):
     elif orders is None:
         orders = [True] * len(keys)
 
-    for key, order in izip(keys, orders):
+    for key, order in zip(keys, orders):
         rizer = _hash.Factorizer(len(key))
 
         if not key.dtype == np.object_:
@@ -2537,12 +2540,12 @@ class _KeyMapper(object):
         self._populate_tables()
 
     def _populate_tables(self):
-        for labs, table in izip(self.labels, self.tables):
+        for labs, table in zip(self.labels, self.tables):
             table.map(self.comp_ids, labs.astype(np.int64))
 
     def get_key(self, comp_id):
         return tuple(level[table.get_item(comp_id)]
-                     for table, level in izip(self.tables, self.levels))
+                     for table, level in zip(self.tables, self.levels))
 
 
 def _get_indices_dict(label_list, keys):
@@ -2664,7 +2667,7 @@ def install_ipython_completers():  # pragma: no cover
     @complete_object.when_type(DataFrameGroupBy)
     def complete_dataframe(obj, prev_completions):
         return prev_completions + [c for c in obj.obj.columns
-                                   if isinstance(c, basestring) and py3compat.isidentifier(c)]
+                                   if isinstance(c, six.string_types) and py3compat.isidentifier(c)]
 
 
 # Importing IPython brings in about 200 modules, so we want to avoid it unless
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 3eb804d3a..7cff2e51a 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -1,7 +1,9 @@
 # pylint: disable=E1101,E1103,W0232
 
-from itertools import izip
-
+from pandas.util.py3compat import range
+from six.moves import zip
+import six
+from pandas.util import compat
 import numpy as np
 
 import pandas.tslib as tslib
@@ -722,7 +724,7 @@ class Index(PandasObject, np.ndarray):
         """
         try:
             return self._engine.get_value(series, key)
-        except KeyError, e1:
+        except KeyError as e1:
             if len(self) > 0 and self.inferred_type == 'integer':
                 raise
 
@@ -1349,7 +1351,7 @@ class Int64Index(Index):
                 data = list(data)
             data = np.asarray(data)
 
-        if issubclass(data.dtype.type, basestring):
+        if issubclass(data.dtype.type, six.string_types):
             raise TypeError('String dtype not supported, you may need '
                             'to explicitly cast to int')
         elif issubclass(data.dtype.type, np.integer):
@@ -1593,7 +1595,7 @@ class MultiIndex(Index):
         # has duplicates
         shape = [len(lev) for lev in self.levels]
         group_index = np.zeros(len(self), dtype='i8')
-        for i in xrange(len(shape)):
+        for i in range(len(shape)):
             stride = np.prod([x for x in shape[i + 1:]], dtype='i8')
             group_index += self.labels[i] * stride
 
@@ -1610,7 +1612,7 @@ class MultiIndex(Index):
         # Label-based
         try:
             return self._engine.get_value(series, key)
-        except KeyError, e1:
+        except KeyError as e1:
             try:
                 # TODO: what if a level contains tuples??
                 loc = self.get_loc(key)
@@ -1800,7 +1802,7 @@ class MultiIndex(Index):
         elif isinstance(tuples, list):
             arrays = list(lib.to_object_array_tuples(tuples).T)
         else:
-            arrays = zip(*tuples)
+            arrays = list(zip(*tuples))
 
         return MultiIndex.from_arrays(arrays, sortorder=sortorder,
                                       names=names)
@@ -1940,7 +1942,7 @@ class MultiIndex(Index):
             if isinstance(loc, int):
                 inds.append(loc)
             else:
-                inds.extend(range(loc.start, loc.stop))
+                inds.extend(list(range(loc.start, loc.stop)))
 
         return self.delete(inds)
 
@@ -2236,7 +2238,7 @@ class MultiIndex(Index):
 
         n = len(tup)
         start, end = 0, len(self)
-        zipped = izip(tup, self.levels, self.labels)
+        zipped = zip(tup, self.levels, self.labels)
         for k, (lab, lev, labs) in enumerate(zipped):
             section = labs[start:end]
 
@@ -2445,7 +2447,7 @@ class MultiIndex(Index):
         if len(self) != len(other):
             return False
 
-        for i in xrange(self.nlevels):
+        for i in range(self.nlevels):
             svalues = com.take_nd(self.levels[i].values, self.labels[i],
                                   allow_fill=False)
             ovalues = com.take_nd(other.levels[i].values, other.labels[i],
@@ -2463,7 +2465,7 @@ class MultiIndex(Index):
         if self.nlevels != other.nlevels:
             return False
 
-        for i in xrange(self.nlevels):
+        for i in range(self.nlevels):
             if not self.levels[i].equals(other.levels[i]):
                 return False
         return True
@@ -2488,7 +2490,7 @@ class MultiIndex(Index):
         result_names = self.names if self.names == other.names else None
 
         uniq_tuples = lib.fast_unique_multiple([self.values, other.values])
-        return MultiIndex.from_arrays(zip(*uniq_tuples), sortorder=0,
+        return MultiIndex.from_arrays(list(zip(*uniq_tuples)), sortorder=0,
                                       names=result_names)
 
     def intersection(self, other):
@@ -2518,7 +2520,7 @@ class MultiIndex(Index):
                               labels=[[]] * self.nlevels,
                               names=result_names)
         else:
-            return MultiIndex.from_arrays(zip(*uniq_tuples), sortorder=0,
+            return MultiIndex.from_arrays(list(zip(*uniq_tuples)), sortorder=0,
                                           names=result_names)
 
     def diff(self, other):
@@ -2635,7 +2637,7 @@ class MultiIndex(Index):
 # For utility purposes
 
 def _sparsify(label_list, start=0,sentinal=''):
-    pivoted = zip(*label_list)
+    pivoted = list(zip(*label_list))
     k = len(label_list)
 
     result = pivoted[:start + 1]
@@ -2659,7 +2661,7 @@ def _sparsify(label_list, start=0,sentinal=''):
 
         prev = cur
 
-    return zip(*result)
+    return list(zip(*result))
 
 
 def _ensure_index(index_like):
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 0237cfde3..cb841169d 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -3,7 +3,10 @@
 from datetime import datetime
 from pandas.core.common import _asarray_tuplesafe
 from pandas.core.index import Index, MultiIndex, _ensure_index
+from pandas.util.py3compat import range
+from six.moves import zip
 import pandas.core.common as com
+import six
 import pandas.lib as lib
 
 import numpy as np
@@ -340,7 +343,7 @@ class _NDFrameIndexer(object):
             except TypeError:
                 # slices are unhashable
                 pass
-            except Exception, e1:
+            except Exception as e1:
                 if isinstance(tup[0], (slice, Index)):
                     raise IndexingError
 
@@ -707,7 +710,7 @@ class _LocationIndexer(_NDFrameIndexer):
             inds, = key.nonzero()
             try:
                 return self.obj.take(inds, axis=axis, convert=False)
-            except (Exception), detail:
+            except (Exception) as detail:
                 raise self._exception(detail)
     def _get_slice_axis(self, slice_obj, axis=0):
         """ this is pretty simple as we just have to deal with labels """
@@ -920,7 +923,7 @@ def _convert_to_index_sliceable(obj, key):
             indexer = obj.ix._convert_to_indexer(key, axis=0)
         return indexer
 
-    elif isinstance(key, basestring):
+    elif isinstance(key, six.string_types):
 
         # we are an actual column
         if key in obj._data.items:
@@ -1077,7 +1080,7 @@ def _is_label_like(key):
 def _is_list_like(obj):
     # Consider namedtuples to be not list like as they are useful as indices
     return (np.iterable(obj)
-            and not isinstance(obj, basestring)
+            and not isinstance(obj, six.string_types)
             and not (isinstance(obj, tuple) and type(obj) is not tuple))
 
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index f23a89635..0ff462ce2 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -1,5 +1,6 @@
 import itertools
 import re
+import six
 from datetime import datetime
 
 from numpy import nan
@@ -18,6 +19,8 @@ import pandas.core.expressions as expressions
 
 from pandas.tslib import Timestamp
 from pandas.util import py3compat
+from pandas.util.py3compat import range
+from six.moves import map, zip
 
 
 class Block(PandasObject):
@@ -471,7 +474,7 @@ class Block(PandasObject):
         args = [ values, other ]
         try:
             result = self._try_coerce_result(func(*args))
-        except (Exception), detail:
+        except (Exception) as detail:
             if raise_on_error:
                 raise TypeError('Could not operate [%s] with block values [%s]'
                                 % (repr(other),str(detail)))
@@ -546,7 +549,7 @@ class Block(PandasObject):
             v, o = self._try_coerce_args(v, o)
             try:
                 return self._try_coerce_result(expressions.where(c, v, o, raise_on_error=True))
-            except (Exception), detail:
+            except (Exception) as detail:
                 if raise_on_error:
                     raise TypeError('Could not operate [%s] with block values [%s]'
                                     % (repr(o),str(detail)))
@@ -576,7 +579,7 @@ class Block(PandasObject):
         # might need to separate out blocks
         axis = cond.ndim - 1
         cond = cond.swapaxes(axis, 0)
-        mask = np.array([cond[i].all() for i in xrange(cond.shape[0])],
+        mask = np.array([cond[i].all() for i in range(cond.shape[0])],
                         dtype=bool)
 
         result_blocks = []
@@ -686,7 +689,7 @@ class ObjectBlock(Block):
     _can_hold_na = True
 
     def __init__(self, values, items, ref_items, ndim=2, fastpath=False, placement=None):
-        if issubclass(values.dtype.type, basestring):
+        if issubclass(values.dtype.type, six.string_types):
             values = np.array(values, dtype=object)
 
         super(ObjectBlock, self).__init__(values, items, ref_items,
@@ -757,7 +760,7 @@ class ObjectBlock(Block):
                                                    inplace=inplace,
                                                    filter=filter, regex=regex)
         elif both_lists:
-            for to_rep, v in itertools.izip(to_replace, value):
+            for to_rep, v in zip(to_replace, value):
                 blk[0], = blk[0]._replace_single(to_rep, v, inplace=inplace,
                                                  filter=filter, regex=regex)
         elif to_rep_is_list and regex:
@@ -812,7 +815,7 @@ class ObjectBlock(Block):
 
         # deal with replacing values with objects (strings) that match but
         # whose replacement is not a string (numeric, nan, object)
-        if isnull(value) or not isinstance(value, basestring):
+        if isnull(value) or not isinstance(value, six.string_types):
             def re_replacer(s):
                 try:
                     return value if rx.search(s) is not None else s
@@ -830,7 +833,7 @@ class ObjectBlock(Block):
         f = np.vectorize(re_replacer, otypes=[self.dtype])
 
         try:
-            filt = map(self.items.get_loc, filter)
+            filt = list(map(self.items.get_loc, filter))
         except TypeError:
             filt = slice(None)
 
@@ -1922,7 +1925,7 @@ class BlockManager(PandasObject):
 
             # need to shift elements to the right
             if self._ref_locs[loc] is not None:
-                for i in reversed(range(loc+1,len(self._ref_locs))):
+                for i in reversed(list(range(loc+1,len(self._ref_locs)))):
                     self._ref_locs[i] = self._ref_locs[i-1]
 
             self._ref_locs[loc] = (new_block, 0)
@@ -2532,5 +2535,5 @@ def _possibly_convert_to_indexer(loc):
     if com._is_bool_indexer(loc):
         loc = [i for i, v in enumerate(loc) if v]
     elif isinstance(loc,slice):
-        loc = range(loc.start,loc.stop)
+        loc = list(range(loc.start,loc.stop))
     return loc
diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index b2ff366da..72ba4364c 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -1,3 +1,4 @@
+from pandas.util import compat
 import sys
 import itertools
 import functools
@@ -10,6 +11,7 @@ import pandas.lib as lib
 import pandas.algos as algos
 import pandas.hashtable as _hash
 import pandas.tslib as tslib
+import six
 
 try:
     import bottleneck as bn
@@ -30,7 +32,7 @@ class disallow(object):
     def __call__(self, f):
         @functools.wraps(f)
         def _f(*args, **kwargs):
-            obj_iter = itertools.chain(args, kwargs.itervalues())
+            obj_iter = itertools.chain(args, six.itervalues(kwargs))
             if any(self.check(obj) for obj in obj_iter):
                 raise TypeError('reduction operation {0!r} not allowed for '
                                 'this dtype'.format(f.__name__.replace('nan',
@@ -55,7 +57,7 @@ class bottleneck_switch(object):
         @functools.wraps(alt)
         def f(values, axis=None, skipna=True, **kwds):
             if len(self.kwargs) > 0:
-                for k, v in self.kwargs.iteritems():
+                for k, v in compat.iteritems(self.kwargs):
                     if k not in kwds:
                         kwds[k] = v
             try:
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index d33f7144c..739ffc6f3 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -3,6 +3,8 @@ Contains data structures designed for manipulating panel (3-dimensional) data
 """
 # pylint: disable=E1103,W0231,W0212,W0621
 
+from pandas.util.py3compat import range
+from pandas.util import compat
 import operator
 import sys
 import numpy as np
@@ -25,6 +27,8 @@ from pandas.util.decorators import deprecate, Appender, Substitution
 import pandas.core.common as com
 import pandas.core.nanops as nanops
 import pandas.lib as lib
+import six
+from six.moves import map, zip
 
 
 def _ensure_like_indices(time, panels):
@@ -473,17 +477,17 @@ class Panel(NDFrame):
         class_name = str(self.__class__)
 
         shape = self.shape
-        dims = u'Dimensions: %s' % ' x '.join(
+        dims = six.u('Dimensions: %s') % ' x '.join(
             ["%d (%s)" % (s, a) for a, s in zip(self._AXIS_ORDERS, shape)])
 
         def axis_pretty(a):
             v = getattr(self, a)
             if len(v) > 0:
-                return u'%s axis: %s to %s' % (a.capitalize(),
+                return six.u('%s axis: %s to %s') % (a.capitalize(),
                                                com.pprint_thing(v[0]),
                                                com.pprint_thing(v[-1]))
             else:
-                return u'%s axis: None' % a.capitalize()
+                return six.u('%s axis: None') % a.capitalize()
 
         output = '\n'.join(
             [class_name, dims] + [axis_pretty(a) for a in self._AXIS_ORDERS])
@@ -540,7 +544,7 @@ class Panel(NDFrame):
         y : SparseDataFrame
         """
         from pandas.core.sparse import SparsePanel
-        frames = dict(self.iterkv())
+        frames = dict(self.iteritems())
         return SparsePanel(frames, items=self.items,
                            major_axis=self.major_axis,
                            minor_axis=self.minor_axis,
@@ -804,13 +808,13 @@ class Panel(NDFrame):
         new_minor, indexer2 = self.minor_axis.reindex(minor)
 
         if indexer0 is None:
-            indexer0 = range(len(new_items))
+            indexer0 = list(range(len(new_items)))
 
         if indexer1 is None:
-            indexer1 = range(len(new_major))
+            indexer1 = list(range(len(new_major)))
 
         if indexer2 is None:
-            indexer2 = range(len(new_minor))
+            indexer2 = list(range(len(new_minor)))
 
         for i, ind in enumerate(indexer0):
             com.take_2d_multi(values[ind], (indexer1, indexer2),
@@ -976,7 +980,7 @@ class Panel(NDFrame):
             if method is None:
                 raise ValueError('must specify a fill method or value')
             result = {}
-            for col, s in self.iterkv():
+            for col, s in self.iteritems():
                 result[col] = s.fillna(method=method, value=value)
 
             return self._constructor.from_dict(result)
@@ -1137,7 +1141,7 @@ class Panel(NDFrame):
 
         for a in self._AXIS_ORDERS:
             if not a in kwargs:
-                where = map(a.startswith, aliases)
+                where = list(map(a.startswith, aliases))
 
                 if any(where):
                     if sum(where) != 1:
@@ -1483,7 +1487,7 @@ class Panel(NDFrame):
         if not isinstance(values, np.ndarray):
             values = np.asarray(values)
             # NumPy strings are a pain, convert to object
-            if issubclass(values.dtype.type, basestring):
+            if issubclass(values.dtype.type, six.string_types):
                 values = np.array(values, dtype=object, copy=True)
         else:
             if copy:
@@ -1711,7 +1715,7 @@ def install_ipython_completers():  # pragma: no cover
     @complete_object.when_type(Panel)
     def complete_dataframe(obj, prev_completions):
         return prev_completions + [c for c in obj.keys()
-                                   if isinstance(c, basestring)
+                                   if isinstance(c, six.string_types)
                                         and py3compat.isidentifier(c)]
 
 # Importing IPython brings in about 200 modules, so we want to avoid it unless
diff --git a/pandas/core/panelnd.py b/pandas/core/panelnd.py
index 08ff3b70d..3981850d9 100644
--- a/pandas/core/panelnd.py
+++ b/pandas/core/panelnd.py
@@ -1,6 +1,8 @@
 """ Factory methods to create N-D panels """
 
 import pandas.lib as lib
+from six.moves import zip
+import six
 
 
 def create_nd_panel_factory(klass_name, axis_orders, axis_slices, slicer, axis_aliases=None, stat_axis=2,ns=None):
@@ -27,7 +29,7 @@ def create_nd_panel_factory(klass_name, axis_orders, axis_slices, slicer, axis_a
     """
 
     # if slicer is a name, get the object
-    if isinstance(slicer, basestring):
+    if isinstance(slicer, six.string_types):
         import pandas
         try:
             slicer = getattr(pandas, slicer)
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index cb34d0bad..436c22981 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -1,6 +1,10 @@
 # pylint: disable=E1101,E1103
 # pylint: disable=W0703,W0622,W0613,W0201
 
+from pandas.util.py3compat import range
+from pandas.util import compat
+from six.moves import zip
+import six
 import itertools
 
 import numpy as np
@@ -187,7 +191,7 @@ class _Unstacker(object):
         new_mask = np.zeros(result_shape, dtype=bool)
 
         # is there a simpler / faster way of doing this?
-        for i in xrange(values.shape[1]):
+        for i in range(values.shape[1]):
             chunk = new_values[:, i * width: (i + 1) * width]
             mask_chunk = new_mask[:, i * width: (i + 1) * width]
 
@@ -397,7 +401,7 @@ def _slow_pivot(index, columns, values):
     Could benefit from some Cython here.
     """
     tree = {}
-    for i, (idx, col) in enumerate(itertools.izip(index, columns)):
+    for i, (idx, col) in enumerate(zip(index, columns)):
         if col not in tree:
             tree[col] = {}
         branch = tree[col]
@@ -685,11 +689,11 @@ def melt(frame, id_vars=None, value_vars=None,
                 var_name = frame.columns.names
             else:
                 var_name = ['variable_%s' % i for i in
-                            xrange(len(frame.columns.names))]
+                            range(len(frame.columns.names))]
         else:
             var_name = [frame.columns.name if frame.columns.name is not None
                         else 'variable']
-    if isinstance(var_name, basestring):
+    if isinstance(var_name, six.string_types):
         var_name = [var_name]
 
     N, K = frame.shape
@@ -898,7 +902,7 @@ def block2d_to_blocknd(values, items, shape, labels, ref_items=None):
         pvalues.fill(fill_value)
 
     values = values
-    for i in xrange(len(items)):
+    for i in range(len(items)):
         pvalues[i].flat[mask] = values[:, i]
 
     if ref_items is None:
diff --git a/pandas/core/series.py b/pandas/core/series.py
index b77dfbfd9..c8075e223 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -5,7 +5,8 @@ Data structure for 1-dimensional cross-sectional and time series data
 # pylint: disable=E1101,E1103
 # pylint: disable=W0703,W0622,W0613,W0201
 
-from itertools import izip
+from pandas.util import compat
+from six.moves import zip
 import operator
 from distutils.version import LooseVersion
 import types
@@ -43,6 +44,7 @@ import pandas.index as _index
 
 from pandas.compat.scipy import scoreatpercentile as _quantile
 from pandas.core.config import get_option
+import six
 
 __all__ = ['Series', 'TimeSeries']
 
@@ -425,7 +427,7 @@ class Series(generic.PandasContainer, pa.Array):
         'index': 0
     }
 
-    _AXIS_NAMES = dict((v, k) for k, v in _AXIS_NUMBERS.iteritems())
+    _AXIS_NAMES = dict((v, k) for k, v in compat.iteritems(_AXIS_NUMBERS))
 
     def __new__(cls, data=None, index=None, dtype=None, name=None,
                 copy=False):
@@ -829,7 +831,7 @@ class Series(generic.PandasContainer, pa.Array):
                 return
 
             raise KeyError('%s not in this series!' % str(key))
-        except TypeError, e:
+        except TypeError as e:
             # python 3 type errors should be raised
             if 'unorderable' in str(e):  # pragma: no cover
                 raise IndexError(key)
@@ -1116,9 +1118,9 @@ class Series(generic.PandasContainer, pa.Array):
                                     name=True,
                                     dtype=True)
         else:
-            result = u'Series([], dtype: %s)' % self.dtype
+            result = six.u('Series([], dtype: %s)') % self.dtype
 
-        if not ( type(result) == unicode):
+        if not (isinstance(result, six.text_type)):
             raise AssertionError()
         return result
 
@@ -1137,12 +1139,12 @@ class Series(generic.PandasContainer, pa.Array):
         result = head + '\n...\n' + tail
         result = '%s\n%s' % (result, self._repr_footer())
 
-        return unicode(result)
+        return six.text_type(result)
 
     def _repr_footer(self):
-        namestr = u"Name: %s, " % com.pprint_thing(
+        namestr = six.u("Name: %s, ") % com.pprint_thing(
             self.name) if self.name is not None else ""
-        return u'%sLength: %d, dtype: %s' % (namestr, len(self),
+        return six.u('%sLength: %d, dtype: %s') % (namestr, len(self),
                                              str(self.dtype.name))
 
     def to_string(self, buf=None, na_rep='NaN', float_format=None,
@@ -1180,7 +1182,7 @@ class Series(generic.PandasContainer, pa.Array):
                                   length=length, dtype=dtype, name=name)
 
         # catch contract violations
-        if not  type(the_repr) == unicode:
+        if not isinstance(the_repr, six.text_type):
             raise AssertionError("expected unicode string")
 
         if buf is None:
@@ -1203,7 +1205,7 @@ class Series(generic.PandasContainer, pa.Array):
                                         length=length, dtype=dtype, na_rep=na_rep,
                                         float_format=float_format)
         result = formatter.to_string()
-        if not ( type(result) == unicode):
+        if not (isinstance(result, six.text_type)):
             raise AssertionError()
         return result
 
@@ -1217,7 +1219,7 @@ class Series(generic.PandasContainer, pa.Array):
         """
         Lazily iterate over (index, value) tuples
         """
-        return izip(iter(self.index), iter(self))
+        return list(zip(iter(self.index), iter(self)))
 
     iterkv = iteritems
     if py3compat.PY3:  # pragma: no cover
@@ -1333,7 +1335,7 @@ class Series(generic.PandasContainer, pa.Array):
         -------
         value_dict : dict
         """
-        return dict(self.iteritems())
+        return dict(compat.iteritems(self))
 
     def to_sparse(self, kind='block', fill_value=None):
         """
@@ -1384,7 +1386,7 @@ class Series(generic.PandasContainer, pa.Array):
         if level is not None:
             mask = notnull(self.values)
 
-            if isinstance(level, basestring):
+            if isinstance(level, six.string_types):
                 level = self.index._get_level_number(level)
 
             level_index = self.index.levels[level]
@@ -2817,20 +2819,20 @@ class Series(generic.PandasContainer, pa.Array):
 
             all_src = set()
             dd = {}  # group by unique destination value
-            for s, d in to_rep.iteritems():
+            for s, d in compat.iteritems(to_rep):
                 dd.setdefault(d, []).append(s)
                 all_src.add(s)
 
             if any(d in all_src for d in dd.keys()):
                 # don't clobber each other at the cost of temporaries
                 masks = {}
-                for d, sset in dd.iteritems():  # now replace by each dest
+                for d, sset in compat.iteritems(dd):  # now replace by each dest
                     masks[d] = com.mask_missing(rs.values, sset)
 
-                for d, m in masks.iteritems():
+                for d, m in compat.iteritems(masks):
                     com._maybe_upcast_putmask(rs.values,m,d,change=change)
             else:  # if no risk of clobbering then simple
-                for d, sset in dd.iteritems():
+                for d, sset in compat.iteritems(dd):
                     _rep_one(rs, sset, d)
 
         if np.isscalar(to_replace):
@@ -3046,7 +3048,7 @@ class Series(generic.PandasContainer, pa.Array):
 
         offset = _resolve_offset(freq, kwds)
 
-        if isinstance(offset, basestring):
+        if isinstance(offset, six.string_types):
             offset = datetools.to_offset(offset)
 
         def _get_values():
@@ -3099,7 +3101,7 @@ class Series(generic.PandasContainer, pa.Array):
         -------
         value or NaN
         """
-        if isinstance(where, basestring):
+        if isinstance(where, six.string_types):
             where = datetools.to_datetime(where)
 
         values = self.values
@@ -3407,7 +3409,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
 
     # This is to prevent mixed-type Series getting all casted to
     # NumPy string type, e.g. NaN --> '-1#IND'.
-    if issubclass(subarr.dtype.type, basestring):
+    if issubclass(subarr.dtype.type, six.string_types):
         subarr = pa.array(data, dtype=object, copy=copy)
 
     return subarr
@@ -3430,7 +3432,7 @@ def _resolve_offset(freq, kwds):
     if 'timeRule' in kwds or 'offset' in kwds:
         offset = kwds.get('offset', None)
         offset = kwds.get('timeRule', offset)
-        if isinstance(offset, basestring):
+        if isinstance(offset, six.string_types):
             offset = datetools.getOffset(offset)
         warn = True
     else:
diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index 1aa7fe879..e717f5a2b 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -1,6 +1,6 @@
 import numpy as np
 
-from itertools import izip
+from six.moves import zip
 from pandas.core.common import isnull
 from pandas.core.series import Series
 import re
@@ -50,7 +50,7 @@ def str_cat(arr, others=None, sep=None, na_rep=None):
 
             notmask = -na_mask
 
-            tuples = izip(*[x[notmask] for x in arrays])
+            tuples = zip(*[x[notmask] for x in arrays])
             cats = [sep.join(tup) for tup in tuples]
 
             result[notmask] = cats
@@ -284,14 +284,14 @@ def str_repeat(arr, repeats):
             try:
                 return str.__mul__(x, repeats)
             except TypeError:
-                return unicode.__mul__(x, repeats)
+                return six.text_type.__mul__(x, repeats)
         return _na_map(rep, arr)
     else:
         def rep(x, r):
             try:
                 return str.__mul__(x, r)
             except TypeError:
-                return unicode.__mul__(x, r)
+                return six.text_type.__mul__(x, r)
         repeats = np.asarray(repeats, dtype=object)
         result = lib.vec_binop(arr, repeats, rep)
         return result
diff --git a/pandas/io/auth.py b/pandas/io/auth.py
index 6da497687..15e3eb70d 100644
--- a/pandas/io/auth.py
+++ b/pandas/io/auth.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 # see LICENSES directory for copyright and license
 import os
 import sys
@@ -54,8 +55,8 @@ def process_flags(flags=[]):
     # Let the gflags module process the command-line arguments.
     try:
         FLAGS(flags)
-    except gflags.FlagsError, e:
-        print ('%s\nUsage: %s ARGS\n%s' % (e, str(flags), FLAGS))
+    except gflags.FlagsError as e:
+        print('%s\nUsage: %s ARGS\n%s' % (e, str(flags), FLAGS))
         sys.exit(1)
 
     # Set the logging according to the command-line flag.
diff --git a/pandas/io/clipboard.py b/pandas/io/clipboard.py
index 08837474c..fa3e38459 100644
--- a/pandas/io/clipboard.py
+++ b/pandas/io/clipboard.py
@@ -1,5 +1,5 @@
 """ io on the clipboard """
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
 
 def read_clipboard(**kwargs):  # pragma: no cover
     """
diff --git a/pandas/io/common.py b/pandas/io/common.py
index 33958ade2..3ad181c3d 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -5,7 +5,7 @@ import urlparse
 import urllib2
 import zipfile
 from contextlib import contextmanager, closing
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
 
 from pandas.util import py3compat
 
diff --git a/pandas/io/data.py b/pandas/io/data.py
index 1b51ae5ec..74268241d 100644
--- a/pandas/io/data.py
+++ b/pandas/io/data.py
@@ -3,9 +3,9 @@ Module contains tools for collecting data from various remote sources
 
 
 """
+from pandas.util.py3compat import range
 import warnings
 import tempfile
-import itertools
 import datetime as dt
 import urllib
 import time
@@ -20,6 +20,8 @@ from pandas.core.common import PandasError
 from pandas.io.parsers import TextParser
 from pandas.io.common import urlopen, ZipFile
 from pandas.util.testing import _network_error_classes
+import six
+from six.moves import map, zip
 
 
 class SymbolWarning(UserWarning):
@@ -95,7 +97,7 @@ def _in_chunks(seq, size):
     """
     Return sequence in 'chunks' of size defined by size
     """
-    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))
+    return (seq[pos:pos + size] for pos in range(0, len(seq), size))
 
 
 _yahoo_codes = {'symbol': 's', 'last': 'l1', 'change_pct': 'p2', 'PE': 'r',
@@ -107,13 +109,13 @@ def get_quote_yahoo(symbols):
 
     Returns a DataFrame
     """
-    if isinstance(symbols, basestring):
+    if isinstance(symbols, six.string_types):
         sym_list = symbols
     else:
         sym_list = '+'.join(symbols)
 
     # for codes see: http://www.gummy-stuff.org/Yahoo-data.htm
-    request = ''.join(_yahoo_codes.itervalues())  # code request string
+    request = ''.join(six.itervalues(_yahoo_codes))  # code request string
     header = _yahoo_codes.keys()
 
     data = defaultdict(list)
@@ -147,7 +149,7 @@ def get_quote_google(symbols):
 
 
 def _retry_read_url(url, retry_count, pause, name):
-    for _ in xrange(retry_count):
+    for _ in range(retry_count):
         time.sleep(pause)
 
         # kludge to close the socket ASAP
@@ -332,7 +334,7 @@ def _get_data_from(symbols, start, end, retry_count, pause, adjust_price,
     src_fn = _source_functions[source]
 
     # If a single symbol, (e.g., 'GOOG')
-    if isinstance(symbols, (basestring, int)):
+    if isinstance(symbols, (six.string_types, int)):
         hist_data = src_fn(symbols, start, end, retry_count, pause)
     # Or multiple symbols, (e.g., ['GOOG', 'AAPL', 'MSFT'])
     elif isinstance(symbols, DataFrame):
@@ -465,15 +467,15 @@ def get_data_famafrench(name):
         with ZipFile(tmpf, 'r') as zf:
             data = zf.open(name + '.txt').readlines()
 
-    line_lengths = np.array(map(len, data))
+    line_lengths = np.array(list(map(len, data)))
     file_edges = np.where(line_lengths == 2)[0]
 
     datasets = {}
-    edges = itertools.izip(file_edges + 1, file_edges[1:])
+    edges = zip(file_edges + 1, file_edges[1:])
     for i, (left_edge, right_edge) in enumerate(edges):
         dataset = [d.split() for d in data[left_edge:right_edge]]
         if len(dataset) > 10:
-            ncol_raw = np.array(map(len, dataset))
+            ncol_raw = np.array(list(map(len, dataset)))
             ncol = np.median(ncol_raw)
             header_index = np.where(ncol_raw == ncol - 1)[0][-1]
             header = dataset[header_index]
@@ -809,18 +811,18 @@ class Options(object):
         data : dict of str, DataFrame
         """
         warnings.warn("get_forward_data() is deprecated", FutureWarning)
-        in_months = xrange(CUR_MONTH, CUR_MONTH + months + 1)
+        in_months = range(CUR_MONTH, CUR_MONTH + months + 1)
         in_years = [CUR_YEAR] * (months + 1)
 
         # Figure out how many items in in_months go past 12
         to_change = 0
-        for i in xrange(months):
+        for i in range(months):
             if in_months[i] > 12:
                 in_months[i] -= 12
                 to_change += 1
 
         # Change the corresponding items in the in_years list.
-        for i in xrange(1, to_change + 1):
+        for i in range(1, to_change + 1):
             in_years[-i] += 1
 
         to_ret = Series({'calls': call, 'puts': put})
@@ -830,7 +832,7 @@ class Options(object):
         for name in to_ret:
             all_data = DataFrame()
 
-            for mon in xrange(months):
+            for mon in range(months):
                 m2 = in_months[mon]
                 y2 = in_years[mon]
 
diff --git a/pandas/io/date_converters.py b/pandas/io/date_converters.py
index c7a60d13f..c0e9b4da8 100644
--- a/pandas/io/date_converters.py
+++ b/pandas/io/date_converters.py
@@ -1,4 +1,5 @@
 """This module is designed for community supported date conversion functions"""
+from pandas.util.py3compat import range
 import numpy as np
 import pandas.lib as lib
 
@@ -32,7 +33,7 @@ def generic_parser(parse_func, *cols):
     N = _check_columns(cols)
     results = np.empty(N, dtype=object)
 
-    for i in xrange(N):
+    for i in range(N):
         args = [c[i] for c in cols]
         results[i] = parse_func(*args)
 
diff --git a/pandas/io/excel.py b/pandas/io/excel.py
index b3b48382f..65d0b6f01 100644
--- a/pandas/io/excel.py
+++ b/pandas/io/excel.py
@@ -5,13 +5,15 @@ Module parse to/from Excel
 #----------------------------------------------------------------------
 # ExcelFile class
 
+from pandas.util.py3compat import range
 import datetime
-from itertools import izip
 import numpy as np
 
 from pandas.io.parsers import TextParser
 from pandas.tseries.period import Period
 from pandas import json
+from six.moves import map, zip, reduce
+import six
 
 def read_excel(path_or_buf, sheetname, kind=None, **kwds):
     """Read an Excel table into a pandas DataFrame
@@ -73,7 +75,7 @@ class ExcelFile(object):
         self.path_or_buf = path_or_buf
         self.tmpfile = None
 
-        if isinstance(path_or_buf, basestring):
+        if isinstance(path_or_buf, six.string_types):
             self.book = xlrd.open_workbook(path_or_buf)
         else:
             data = path_or_buf.read()
@@ -153,14 +155,14 @@ class ExcelFile(object):
             for rng in areas.split(','):
                 if ':' in rng:
                     rng = rng.split(':')
-                    cols += range(_excel2num(rng[0]), _excel2num(rng[1]) + 1)
+                    cols += list(range(_excel2num(rng[0]), _excel2num(rng[1]) + 1))
                 else:
                     cols.append(_excel2num(rng))
             return cols
 
         if isinstance(parse_cols, int):
             return i <= parse_cols
-        elif isinstance(parse_cols, basestring):
+        elif isinstance(parse_cols, six.string_types):
             return i in _range2cols(parse_cols)
         else:
             return i in parse_cols
@@ -173,16 +175,16 @@ class ExcelFile(object):
                           XL_CELL_ERROR, XL_CELL_BOOLEAN)
 
         datemode = self.book.datemode
-        if isinstance(sheetname, basestring):
+        if isinstance(sheetname, six.string_types):
             sheet = self.book.sheet_by_name(sheetname)
         else:  # assume an integer if not a string
             sheet = self.book.sheet_by_index(sheetname)
 
         data = []
         should_parse = {}
-        for i in xrange(sheet.nrows):
+        for i in range(sheet.nrows):
             row = []
-            for j, (value, typ) in enumerate(izip(sheet.row_values(i),
+            for j, (value, typ) in enumerate(zip(sheet.row_values(i),
                                                   sheet.row_types(i))):
                 if parse_cols is not None and j not in should_parse:
                     should_parse[j] = self._should_parse(j, parse_cols)
diff --git a/pandas/io/ga.py b/pandas/io/ga.py
index 7d6277e2d..d71de9da4 100644
--- a/pandas/io/ga.py
+++ b/pandas/io/ga.py
@@ -5,6 +5,7 @@
 4. Download JSON secret file and move into same directory as this file
 """
 from datetime import datetime
+from pandas.util import compat
 import numpy as np
 from pandas import DataFrame
 import pandas as pd
@@ -16,8 +17,10 @@ from pandas.util.decorators import Appender, Substitution
 
 from apiclient.errors import HttpError
 from oauth2client.client import AccessTokenRefreshError
+import six
+from six.moves import zip
 
-TYPE_MAP = {u'INTEGER': int, u'FLOAT': float, u'TIME': int}
+TYPE_MAP = {six.u('INTEGER'): int, six.u('FLOAT'): float, six.u('TIME'): int}
 
 NO_CALLBACK = auth.OOB_CALLBACK_URN
 DOC_URL = auth.DOC_URL
@@ -261,7 +264,7 @@ class GDataReader(OAuthDataReader):
         profile_id = profile.get('id')
 
         if index_col is None and dimensions is not None:
-            if isinstance(dimensions, basestring):
+            if isinstance(dimensions, six.string_types):
                 dimensions = [dimensions]
             index_col = _clean_index(list(dimensions), parse_dates)
 
@@ -283,7 +286,7 @@ class GDataReader(OAuthDataReader):
                                         dayfirst=dayfirst,
                                         na_values=na_values,
                                         converters=converters, sort=sort)
-            except HttpError, inst:
+            except HttpError as inst:
                 raise ValueError('Google API error %s: %s' % (inst.resp.status,
                                  inst._get_reason()))
 
@@ -312,7 +315,7 @@ class GDataReader(OAuthDataReader):
 
         if isinstance(sort, bool) and sort:
             return df.sort_index()
-        elif isinstance(sort, (basestring, list, tuple, np.ndarray)):
+        elif isinstance(sort, (six.string_types, list, tuple, np.ndarray)):
             return df.sort_index(by=sort)
 
         return df
@@ -330,14 +333,14 @@ class GAnalytics(GDataReader):
                            max_results=max_results, **kwargs)
         try:
             return self.service.data().ga().get(**qry)
-        except TypeError, error:
+        except TypeError as error:
             raise ValueError('Error making query: %s' % error)
 
 
 def format_query(ids, metrics, start_date, end_date=None, dimensions=None,
                  segment=None, filters=None, sort=None, start_index=None,
                  max_results=10000, **kwargs):
-    if isinstance(metrics, basestring):
+    if isinstance(metrics, six.string_types):
         metrics = [metrics]
     met = ','.join(['ga:%s' % x for x in metrics])
 
@@ -356,7 +359,7 @@ def format_query(ids, metrics, start_date, end_date=None, dimensions=None,
     lst = [dimensions, filters, sort]
     [_maybe_add_arg(qry, n, d) for n, d in zip(names, lst)]
 
-    if isinstance(segment, basestring):
+    if isinstance(segment, six.string_types):
         _maybe_add_arg(qry, 'segment', segment, 'dynamic::ga')
     elif isinstance(segment, int):
         _maybe_add_arg(qry, 'segment', segment, 'gaid:')
@@ -374,7 +377,7 @@ def format_query(ids, metrics, start_date, end_date=None, dimensions=None,
 
 def _maybe_add_arg(query, field, data, prefix='ga'):
     if data is not None:
-        if isinstance(data, (basestring, int)):
+        if isinstance(data, (six.string_types, int)):
             data = [data]
         data = ','.join(['%s:%s' % (prefix, x) for x in data])
         query[field] = data
@@ -412,7 +415,7 @@ def _clean_index(index_dims, parse_dates):
                     to_add.append('_'.join(lst))
                 to_remove.extend(lst)
     elif isinstance(parse_dates, dict):
-        for name, lst in parse_dates.iteritems():
+        for name, lst in compat.iteritems(parse_dates):
             if isinstance(lst, (list, tuple, np.ndarray)):
                 if _should_add(lst):
                     to_add.append(name)
@@ -435,12 +438,12 @@ def _get_column_types(header_info):
 
 def _get_dim_names(header_info):
     return [x['name'][3:] for x in header_info
-            if x['columnType'] == u'DIMENSION']
+            if x['columnType'] == six.u('DIMENSION')]
 
 
 def _get_met_names(header_info):
     return [x['name'][3:] for x in header_info
-            if x['columnType'] == u'METRIC']
+            if x['columnType'] == six.u('METRIC')]
 
 
 def _get_data_types(header_info):
diff --git a/pandas/io/html.py b/pandas/io/html.py
index 651a3eb50..bcecc6244 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -3,6 +3,8 @@ HTML IO.
 
 """
 
+from pandas.util.py3compat import range
+from pandas.util import compat
 import os
 import re
 import numbers
@@ -16,6 +18,8 @@ import numpy as np
 
 from pandas import DataFrame, MultiIndex, isnull
 from pandas.io.common import _is_url, urlopen
+import six
+from six.moves import map
 
 
 try:
@@ -91,9 +95,9 @@ def _get_skiprows_iter(skiprows):
         A proper iterator to use to skip rows of a DataFrame.
     """
     if isinstance(skiprows, slice):
-        return range(skiprows.start or 0, skiprows.stop, skiprows.step or 1)
+        return list(range(skiprows.start or 0, skiprows.stop, skiprows.step or 1))
     elif isinstance(skiprows, numbers.Integral):
-        return range(skiprows)
+        return list(range(skiprows))
     elif isinstance(skiprows, collections.Container):
         return skiprows
     else:
@@ -120,7 +124,7 @@ def _read(io):
     elif os.path.isfile(io):
         with open(io) as f:
             raw_text = f.read()
-    elif isinstance(io, basestring):
+    elif isinstance(io, six.string_types):
         raw_text = io
     else:
         raise TypeError("Cannot read object of type "
@@ -343,14 +347,14 @@ class _HtmlFrameParser(object):
         thead = self._parse_thead(table)
         res = []
         if thead:
-            res = map(self._text_getter, self._parse_th(thead[0]))
+            res = list(map(self._text_getter, self._parse_th(thead[0])))
         return np.array(res).squeeze() if res and len(res) == 1 else res
 
     def _parse_raw_tfoot(self, table):
         tfoot = self._parse_tfoot(table)
         res = []
         if tfoot:
-            res = map(self._text_getter, self._parse_td(tfoot[0]))
+            res = list(map(self._text_getter, self._parse_td(tfoot[0])))
         return np.array(res).squeeze() if res and len(res) == 1 else res
 
     def _parse_raw_tbody(self, table):
@@ -450,8 +454,8 @@ def _build_node_xpath_expr(attrs):
     if 'class_' in attrs:
         attrs['class'] = attrs.pop('class_')
 
-    s = (u"@{k}='{v}'".format(k=k, v=v) for k, v in attrs.iteritems())
-    return u'[{0}]'.format(' and '.join(s))
+    s = (six.u("@{k}='{v}'").format(k=k, v=v) for k, v in attrs.iteritems())
+    return six.u('[{0}]').format(' and '.join(s))
 
 
 _re_namespace = {'re': 'http://exslt.org/regular-expressions'}
@@ -492,9 +496,9 @@ class _LxmlFrameParser(_HtmlFrameParser):
         pattern = match.pattern
 
         # check all descendants for the given pattern
-        check_all_expr = u'//*'
+        check_all_expr = six.u('//*')
         if pattern:
-            check_all_expr += u"[re:test(text(), '{0}')]".format(pattern)
+            check_all_expr += six.u("[re:test(text(), '{0}')]").format(pattern)
 
         # go up the tree until we find a table
         check_table_expr = '/ancestor::table'
@@ -733,10 +737,10 @@ def _parser_dispatch(flavor):
 def _validate_parser_flavor(flavor):
     if flavor is None:
         flavor = ['lxml', 'bs4']
-    elif isinstance(flavor, basestring):
+    elif isinstance(flavor, six.string_types):
         flavor = [flavor]
     elif isinstance(flavor, collections.Iterable):
-        if not all(isinstance(flav, basestring) for flav in flavor):
+        if not all(isinstance(flav, six.string_types) for flav in flavor):
             raise TypeError('{0} is not an iterable of strings'.format(flavor))
     else:
         raise TypeError('{0} is not a valid "flavor"'.format(flavor))
diff --git a/pandas/io/json.py b/pandas/io/json.py
index d3bea36b5..ef53d0b9e 100644
--- a/pandas/io/json.py
+++ b/pandas/io/json.py
@@ -1,11 +1,14 @@
 
 # pylint: disable-msg=E1101,W0613,W0603
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
+from pandas.util import compat
+from pandas.util.py3compat import long
 import os
 
 from pandas import Series, DataFrame, to_datetime
 from pandas.io.common import get_filepath_or_buffer
 import pandas.json as _json
+import six
 loads = _json.loads
 dumps = _json.dumps
 
@@ -26,7 +29,7 @@ def to_json(path_or_buf, obj, orient=None, date_format='epoch', double_precision
     else:
         raise NotImplementedError
 
-    if isinstance(path_or_buf, basestring):
+    if isinstance(path_or_buf, six.string_types):
         with open(path_or_buf,'w') as fh:
             fh.write(s)
     elif path_or_buf is None:
@@ -182,7 +185,7 @@ def read_json(path_or_buf=None, orient=None, typ='frame', dtype=True,
     """
 
     filepath_or_buffer,_ = get_filepath_or_buffer(path_or_buf)
-    if isinstance(filepath_or_buffer, basestring):
+    if isinstance(filepath_or_buffer, six.string_types):
         if os.path.exists(filepath_or_buffer):
             with open(filepath_or_buffer,'r') as fh:
                 json = fh.read()
@@ -342,7 +345,7 @@ class Parser(object):
 
         # ignore numbers that are out of range
         if issubclass(new_data.dtype.type,np.number):
-            if not ((new_data == iNaT) | (new_data > 31536000000000000L)).all():
+            if not ((new_data == iNaT) | (new_data > long(31536000000000000))).all():
                 return data, False
 
         try:
@@ -369,9 +372,9 @@ class SeriesParser(Parser):
         orient = self.orient
         if orient == "split":
             decoded = dict((str(k), v)
-                           for k, v in loads(
+                           for k, v in compat.iteritems(loads(
                                json,
-                               precise_float=self.precise_float).iteritems())
+                               precise_float=self.precise_float)))
             self.obj = Series(dtype=None, **decoded)
         else:
             self.obj = Series(
@@ -384,7 +387,7 @@ class SeriesParser(Parser):
         if orient == "split":
             decoded = loads(json, dtype=None, numpy=True,
                             precise_float=self.precise_float)
-            decoded = dict((str(k), v) for k, v in decoded.iteritems())
+            decoded = dict((str(k), v) for k, v in compat.iteritems(decoded))
             self.obj = Series(**decoded)
         elif orient == "columns" or orient == "index":
             self.obj = Series(*loads(json, dtype=None, numpy=True,
@@ -417,7 +420,7 @@ class FrameParser(Parser):
         elif orient == "split":
             decoded = loads(json, dtype=None, numpy=True,
                             precise_float=self.precise_float)
-            decoded = dict((str(k), v) for k, v in decoded.iteritems())
+            decoded = dict((str(k), v) for k, v in compat.iteritems(decoded))
             self.obj = DataFrame(**decoded)
         elif orient == "values":
             self.obj = DataFrame(loads(json, dtype=None, numpy=True,
@@ -436,9 +439,9 @@ class FrameParser(Parser):
                 loads(json, precise_float=self.precise_float), dtype=None)
         elif orient == "split":
             decoded = dict((str(k), v)
-                           for k, v in loads(
+                           for k, v in compat.iteritems(loads(
                                json,
-                               precise_float=self.precise_float).iteritems())
+                               precise_float=self.precise_float)))
             self.obj = DataFrame(dtype=None, **decoded)
         elif orient == "index":
             self.obj = DataFrame(
@@ -467,7 +470,7 @@ class FrameParser(Parser):
 
         def is_ok(col):
             """ return if this col is ok to try for a date parse """
-            if not isinstance(col, basestring): return False
+            if not isinstance(col, six.string_types): return False
 
             if (col.endswith('_at') or
                 col.endswith('_time') or
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 3bcfb66d3..57f1daa62 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -1,9 +1,11 @@
 """
 Module contains tools for processing files into DataFrames or other objects
 """
-from StringIO import StringIO
+from __future__ import print_function
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
+from pandas.util import compat
 import re
-from itertools import izip
 import csv
 from warnings import warn
 
@@ -23,6 +25,8 @@ import pandas.lib as lib
 import pandas.tslib as tslib
 import pandas.parser as _parser
 from pandas.tseries.period import Period
+import six
+from six.moves import zip
 
 _parser_params = """Also supports optionally iterating or breaking of the file
 into chunks.
@@ -558,7 +562,7 @@ class TextFileReader(object):
         na_values, na_fvalues = _clean_na_values(na_values, keep_default_na)
 
         if com.is_integer(skiprows):
-            skiprows = range(skiprows)
+            skiprows = list(range(skiprows))
         skiprows = set() if skiprows is None else set(skiprows)
 
         # put stuff back
@@ -727,7 +731,7 @@ class ParserBase(object):
         field_count = len(header[0])
         def extract(r):
             return tuple([ r[i] for i in range(field_count) if i not in sic ])
-        columns = zip(*[ extract(r) for r in header ])
+        columns = list(zip(*[ extract(r) for r in header ]))
         names = ic + columns
 
         # if we find 'Unnamed' all of a single level, then our header was too long
@@ -784,7 +788,7 @@ class ParserBase(object):
 
     def _get_simple_index(self, data, columns):
         def ix(col):
-            if not isinstance(col, basestring):
+            if not isinstance(col, six.string_types):
                 return col
             raise ValueError('Index %s invalid' % col)
         index = None
@@ -807,7 +811,7 @@ class ParserBase(object):
 
     def _get_complex_date_index(self, data, col_names):
         def _get_name(icol):
-            if isinstance(icol, basestring):
+            if isinstance(icol, six.string_types):
                 return icol
 
             if col_names is None:
@@ -851,7 +855,7 @@ class ParserBase(object):
                     col_na_values, col_na_fvalues = _get_na_values(col_name,
                                                                    self.na_values,
                                                                    self.na_fvalues)
-                    
+
             arr, _ = self._convert_types(arr, col_na_values | col_na_fvalues)
             arrays.append(arr)
 
@@ -874,7 +878,7 @@ class ParserBase(object):
                                                   coerce_type)
             result[c] = cvals
             if verbose and na_count:
-                print ('Filled %d NA values in column %s' % (na_count, str(c)))
+                print('Filled %d NA values in column %s' % (na_count, str(c)))
         return result
 
     def _convert_types(self, values, na_values, try_num_bool=True):
@@ -928,7 +932,7 @@ class ParserBase(object):
                     offset += 1
                 data[col] = alldata[i + offset]
         else:
-            data = dict((k, v) for k, v in izip(self.orig_names, alldata))
+            data = dict((k, v) for k, v in zip(self.orig_names, alldata))
 
         return data
 
@@ -946,7 +950,7 @@ class CParserWrapper(ParserBase):
         ParserBase.__init__(self, kwds)
 
         if 'utf-16' in (kwds.get('encoding') or ''):
-            if isinstance(src, basestring):
+            if isinstance(src, six.string_types):
                 src = open(src, 'rb')
             src = com.UTF8Recoder(src, kwds['encoding'])
             kwds['encoding'] = 'utf-8'
@@ -976,7 +980,7 @@ class CParserWrapper(ParserBase):
                 self.names = ['X%d' % i
                               for i in range(self._reader.table_width)]
             else:
-                self.names = range(self._reader.table_width)
+                self.names = list(range(self._reader.table_width))
 
         # XXX
         self._set_noconvert_columns()
@@ -1227,7 +1231,7 @@ class PythonParser(ParserBase):
         self.comment = kwds['comment']
         self._comment_lines = []
 
-        if isinstance(f, basestring):
+        if isinstance(f, six.string_types):
             f = com._get_handle(f, 'r', encoding=self.encoding,
                                 compression=self.compression)
         elif self.compression:
@@ -1450,7 +1454,7 @@ class PythonParser(ParserBase):
                 if self.prefix:
                     columns = [ ['X%d' % i for i in range(ncols)] ]
                 else:
-                    columns = [ range(ncols) ]
+                    columns = [ list(range(ncols)) ]
             else:
                 columns = [ names ]
 
@@ -1487,7 +1491,7 @@ class PythonParser(ParserBase):
         for l in lines:
             rl = []
             for x in l:
-                if (not isinstance(x, basestring) or
+                if (not isinstance(x, six.string_types) or
                         self.comment not in x):
                     rl.append(x)
                 else:
@@ -1506,7 +1510,7 @@ class PythonParser(ParserBase):
         for l in lines:
             rl = []
             for x in l:
-                if (not isinstance(x, basestring) or
+                if (not isinstance(x, six.string_types) or
                     self.thousands not in x or
                         nonnum.search(x.strip())):
                     rl.append(x)
@@ -1548,7 +1552,7 @@ class PythonParser(ParserBase):
                     # column and index names on diff rows
                     implicit_first_cols = 0
 
-                    self.index_col = range(len(line))
+                    self.index_col = list(range(len(line)))
                     self.buf = self.buf[1:]
 
                     for c in reversed(line):
@@ -1559,7 +1563,7 @@ class PythonParser(ParserBase):
         if implicit_first_cols > 0:
             self._implicit_index = True
             if self.index_col is None:
-                self.index_col = range(implicit_first_cols)
+                self.index_col = list(range(implicit_first_cols))
             index_name = None
 
         else:
@@ -1629,7 +1633,7 @@ class PythonParser(ParserBase):
                 new_rows = []
                 try:
                     if rows is not None:
-                        for _ in xrange(rows):
+                        for _ in range(rows):
                             new_rows.append(next(source))
                         lines.extend(new_rows)
                     else:
@@ -1638,7 +1642,7 @@ class PythonParser(ParserBase):
                             try:
                                 new_rows.append(next(source))
                                 rows += 1
-                            except csv.Error, inst:
+                            except csv.Error as inst:
                                 if 'newline inside string' in str(inst):
                                     row_num = str(self.pos + rows)
                                     msg = ('EOF inside string starting with line '
@@ -1806,7 +1810,7 @@ def _clean_index_names(columns, index_col):
     index_col = list(index_col)
 
     for i, c in enumerate(index_col):
-        if isinstance(c, basestring):
+        if isinstance(c, six.string_types):
             index_names.append(c)
             for j, name in enumerate(cp_cols):
                 if name == c:
@@ -1819,7 +1823,7 @@ def _clean_index_names(columns, index_col):
             index_names.append(name)
 
     # hack
-    if isinstance(index_names[0], basestring) and 'Unnamed' in index_names[0]:
+    if isinstance(index_names[0], six.string_types) and 'Unnamed' in index_names[0]:
         index_names[0] = None
 
     return index_names, columns, index_col
@@ -1901,13 +1905,12 @@ def _get_col_names(colspec, columns):
 def _concat_date_cols(date_cols):
     if len(date_cols) == 1:
         if py3compat.PY3:
-            return np.array([unicode(x) for x in date_cols[0]], dtype=object)
+            return np.array([six.text_type(x) for x in date_cols[0]], dtype=object)
         else:
-            return np.array([str(x) if not isinstance(x, basestring) else x
+            return np.array([str(x) if not isinstance(x, six.string_types) else x
                              for x in date_cols[0]], dtype=object)
 
-    # stripped = [map(str.strip, x) for x in date_cols]
-    rs = np.array([' '.join([unicode(y) for y in x])
+    rs = np.array([' '.join([six.text_type(y) for y in x])
                    for x in zip(*date_cols)], dtype=object)
     return rs
 
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index a5a835556..52cc7dc24 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -2,9 +2,12 @@
 High level interface to PyTables for reading and writing pandas data structures
 to disk
 """
+from __future__ import print_function
 
 # pylint: disable-msg=E1101,W0613,W0603
 from datetime import datetime, date
+from pandas.util.py3compat import range
+from pandas.util import compat
 import time
 import re
 import copy
@@ -35,6 +38,8 @@ import pandas.algos as algos
 import pandas.tslib as tslib
 
 from contextlib import contextmanager
+import six
+from six.moves import map, zip
 
 # versioning attribute
 _version = '0.10.1'
@@ -87,40 +92,40 @@ map directly to c-types [inferred_type->%s,key->%s] [items->%s]
 # map object types
 _TYPE_MAP = {
 
-    Series          : u'series',
-    SparseSeries    : u'sparse_series',
-    TimeSeries      : u'series',
-    DataFrame       : u'frame',
-    SparseDataFrame : u'sparse_frame',
-    Panel           : u'wide',
-    Panel4D         : u'ndim',
-    SparsePanel     : u'sparse_panel'
+    Series: six.u('series'),
+    SparseSeries: six.u('sparse_series'),
+    TimeSeries: six.u('series'),
+    DataFrame: six.u('frame'),
+    SparseDataFrame: six.u('sparse_frame'),
+    Panel: six.u('wide'),
+    Panel4D: six.u('ndim'),
+    SparsePanel: six.u('sparse_panel')
 }
 
 # storer class map
 _STORER_MAP = {
-    u'TimeSeries'    : 'LegacySeriesStorer',
-    u'Series'        : 'LegacySeriesStorer',
-    u'DataFrame'     : 'LegacyFrameStorer',
-    u'DataMatrix'    : 'LegacyFrameStorer',
-    u'series'        : 'SeriesStorer',
-    u'sparse_series' : 'SparseSeriesStorer',
-    u'frame'         : 'FrameStorer',
-    u'sparse_frame'  : 'SparseFrameStorer',
-    u'wide'          : 'PanelStorer',
-    u'sparse_panel'  : 'SparsePanelStorer',
+    six.u('TimeSeries')    : 'LegacySeriesStorer',
+    six.u('Series')        : 'LegacySeriesStorer',
+    six.u('DataFrame')     : 'LegacyFrameStorer',
+    six.u('DataMatrix')    : 'LegacyFrameStorer',
+    six.u('series')        : 'SeriesStorer',
+    six.u('sparse_series') : 'SparseSeriesStorer',
+    six.u('frame')         : 'FrameStorer',
+    six.u('sparse_frame')  : 'SparseFrameStorer',
+    six.u('wide')          : 'PanelStorer',
+    six.u('sparse_panel')  : 'SparsePanelStorer',
 }
 
 # table class map
 _TABLE_MAP = {
-    u'generic_table'    : 'GenericTable',
-    u'appendable_frame'      : 'AppendableFrameTable',
-    u'appendable_multiframe' : 'AppendableMultiFrameTable',
-    u'appendable_panel' : 'AppendablePanelTable',
-    u'appendable_ndim'  : 'AppendableNDimTable',
-    u'worm'             : 'WORMTable',
-    u'legacy_frame'     : 'LegacyFrameTable',
-    u'legacy_panel'     : 'LegacyPanelTable',
+    six.u('generic_table')    : 'GenericTable',
+    six.u('appendable_frame')      : 'AppendableFrameTable',
+    six.u('appendable_multiframe') : 'AppendableMultiFrameTable',
+    six.u('appendable_panel') : 'AppendablePanelTable',
+    six.u('appendable_ndim')  : 'AppendableNDimTable',
+    six.u('worm')             : 'WORMTable',
+    six.u('legacy_frame')     : 'LegacyFrameTable',
+    six.u('legacy_panel')     : 'LegacyPanelTable',
 }
 
 # axes map
@@ -189,7 +194,7 @@ def to_hdf(path_or_buf, key, value, mode=None, complevel=None, complib=None, app
     else:
         f = lambda store: store.put(key, value, **kwargs)
 
-    if isinstance(path_or_buf, basestring):
+    if isinstance(path_or_buf, six.string_types):
         with get_store(path_or_buf, mode=mode, complevel=complevel, complib=complib) as store:
             f(store)
     else:
@@ -199,7 +204,7 @@ def read_hdf(path_or_buf, key, **kwargs):
     """ read from the store, closeit if we opened it """
     f = lambda store, auto_close: store.select(key, auto_close=auto_close, **kwargs)
 
-    if isinstance(path_or_buf, basestring):
+    if isinstance(path_or_buf, six.string_types):
 
         # can't auto open/close if we are using an iterator
         # so delegate to the iterator
@@ -385,9 +390,9 @@ class HDFStore(StringMixin):
 
         try:
             self._handle = h5_open(self._path, self._mode)
-        except IOError, e:  # pragma: no cover
+        except IOError as e:  # pragma: no cover
             if 'can not be written' in str(e):
-                print ('Opening %s in read-only mode' % self._path)
+                print('Opening %s in read-only mode' % self._path)
                 self._handle = h5_open(self._path, 'r')
             else:
                 raise
@@ -513,7 +518,7 @@ class HDFStore(StringMixin):
         # default to single select
         if isinstance(keys, (list, tuple)) and len(keys) == 1:
             keys = keys[0]
-        if isinstance(keys, basestring):
+        if isinstance(keys, six.string_types):
             return self.select(key=keys, where=where, columns=columns, start=start, stop=stop, iterator=iterator, chunksize=chunksize, **kwargs)
 
         if not isinstance(keys, (list, tuple)):
@@ -545,7 +550,7 @@ class HDFStore(StringMixin):
         try:
             c = self.select_as_coordinates(selector, where, start=start, stop=stop)
             nrows = len(c)
-        except (Exception), detail:
+        except (Exception) as detail:
             raise ValueError("invalid selector [%s]" % selector)
 
         def func(_start, _stop):
@@ -744,7 +749,7 @@ class HDFStore(StringMixin):
         """ return a list of all the top-level nodes (that are not themselves a pandas storage object) """
         _tables()
         return [ g for g in self._handle.walkNodes() if getattr(g._v_attrs,'pandas_type',None) or getattr(
-            g,'table',None) or (isinstance(g,_table_mod.table.Table) and g._v_name != u'table') ]
+            g,'table',None) or (isinstance(g,_table_mod.table.Table) and g._v_name != six.u('table')) ]
 
     def get_node(self, key):
         """ return the node with the key or None if it does not exist """
@@ -823,8 +828,8 @@ class HDFStore(StringMixin):
 
                 _tables()
                 if getattr(group,'table',None) or isinstance(group,_table_mod.table.Table):
-                    pt = u'frame_table'
-                    tt = u'generic_table'
+                    pt = six.u('frame_table')
+                    tt = six.u('generic_table')
                 else:
                     raise TypeError("cannot create a storer if the object is not existing nor a value are passed")
             else:
@@ -836,10 +841,10 @@ class HDFStore(StringMixin):
 
                 # we are actually a table
                 if table or append:
-                    pt += u'_table'
+                    pt += six.u('_table')
 
         # a storer node
-        if u'table' not in pt:
+        if six.u('table') not in pt:
             try:
                 return globals()[_STORER_MAP[pt]](self, group, **kwargs)
             except:
@@ -851,26 +856,26 @@ class HDFStore(StringMixin):
             # if we are a writer, determin the tt
             if value is not None:
 
-                if pt == u'frame_table':
+                if pt == six.u('frame_table'):
                     index = getattr(value,'index',None)
                     if index is not None:
                         if index.nlevels == 1:
-                            tt = u'appendable_frame'
+                            tt = six.u('appendable_frame')
                         elif index.nlevels > 1:
-                            tt = u'appendable_multiframe'
-                elif pt == u'wide_table':
-                    tt  = u'appendable_panel'
-                elif pt == u'ndim_table':
-                    tt = u'appendable_ndim'
+                            tt = six.u('appendable_multiframe')
+                elif pt == six.u('wide_table'):
+                    tt  = six.u('appendable_panel')
+                elif pt == six.u('ndim_table'):
+                    tt = six.u('appendable_ndim')
 
             else:
 
                 # distiguish between a frame/table
-                tt = u'legacy_panel'
+                tt = six.u('legacy_panel')
                 try:
                     fields = group.table._v_attrs.fields
-                    if len(fields) == 1 and fields[0] == u'value':
-                        tt = u'legacy_frame'
+                    if len(fields) == 1 and fields[0] == six.u('value'):
+                        tt = six.u('legacy_frame')
                 except:
                     pass
 
@@ -1140,7 +1145,7 @@ class IndexCol(StringMixin):
     def maybe_set_size(self, min_itemsize=None, **kwargs):
         """ maybe set a string col itemsize:
                min_itemsize can be an interger or a dict with this columns name with an integer size """
-        if _ensure_decoded(self.kind) == u'string':
+        if _ensure_decoded(self.kind) == six.u('string'):
 
             if isinstance(min_itemsize, dict):
                 min_itemsize = min_itemsize.get(self.name)
@@ -1160,7 +1165,7 @@ class IndexCol(StringMixin):
 
         # validate this column for string truncation (or reset to the max size)
         dtype = getattr(self, 'dtype', None)
-        if _ensure_decoded(self.kind) == u'string':
+        if _ensure_decoded(self.kind) == six.u('string'):
 
             c = self.col
             if c is not None:
@@ -1290,7 +1295,7 @@ class DataCol(IndexCol):
         super(DataCol, self).__init__(
             values=values, kind=kind, typ=typ, cname=cname, **kwargs)
         self.dtype = None
-        self.dtype_attr = u"%s_dtype" % self.name
+        self.dtype_attr = six.u("%s_dtype") % self.name
         self.set_data(data)
 
     def __unicode__(self):
@@ -1319,15 +1324,15 @@ class DataCol(IndexCol):
         # set my kind if we can
         if self.dtype is not None:
             dtype = _ensure_decoded(self.dtype)
-            if dtype.startswith(u'string') or dtype.startswith(u'bytes'):
+            if dtype.startswith(six.u('string')) or dtype.startswith(six.u('bytes')):
                 self.kind = 'string'
-            elif dtype.startswith(u'float'):
+            elif dtype.startswith(six.u('float')):
                 self.kind = 'float'
-            elif dtype.startswith(u'int') or dtype.startswith(u'uint'):
+            elif dtype.startswith(six.u('int')) or dtype.startswith(six.u('uint')):
                 self.kind = 'integer'
-            elif dtype.startswith(u'date'):
+            elif dtype.startswith(six.u('date')):
                 self.kind = 'datetime'
-            elif dtype.startswith(u'bool'):
+            elif dtype.startswith(six.u('bool')):
                 self.kind = 'bool'
             else:
                 raise AssertionError("cannot interpret dtype of [%s] in [%s]" % (dtype,self))
@@ -1501,7 +1506,7 @@ class DataCol(IndexCol):
             dtype = _ensure_decoded(self.dtype)
 
             # reverse converts
-            if dtype == u'datetime64':
+            if dtype == six.u('datetime64'):
                 # recreate the timezone
                 if self.tz is not None:
 
@@ -1514,10 +1519,10 @@ class DataCol(IndexCol):
                 else:
                     self.data = np.asarray(self.data, dtype='M8[ns]')
 
-            elif dtype == u'date':
+            elif dtype == six.u('date'):
                 self.data = np.array(
                     [date.fromtimestamp(v) for v in self.data], dtype=object)
-            elif dtype == u'datetime':
+            elif dtype == six.u('datetime'):
                 self.data = np.array(
                     [datetime.fromtimestamp(v) for v in self.data],
                     dtype=object)
@@ -1529,7 +1534,7 @@ class DataCol(IndexCol):
                     self.data = self.data.astype('O')
 
         # convert nans / decode
-        if _ensure_decoded(self.kind) == u'string':
+        if _ensure_decoded(self.kind) == six.u('string'):
             self.data = _unconvert_string_array(self.data, nan_rep=nan_rep, encoding=encoding)
 
         return self
@@ -1553,7 +1558,7 @@ class DataIndexableCol(DataCol):
 
     @property
     def is_searchable(self):
-        return _ensure_decoded(self.kind) == u'string'
+        return _ensure_decoded(self.kind) == six.u('string')
 
     def get_atom_string(self, block, itemsize):
         return _tables().StringCol(itemsize=itemsize)
@@ -1790,7 +1795,7 @@ class GenericStorer(Storer):
             else:
                 ret = data
 
-            if dtype == u'datetime64':
+            if dtype == six.u('datetime64'):
                 ret = np.array(ret, dtype='M8[ns]')
 
         if transposed:
@@ -1801,13 +1806,13 @@ class GenericStorer(Storer):
     def read_index(self, key):
         variety = _ensure_decoded(getattr(self.attrs, '%s_variety' % key))
 
-        if variety == u'multi':
+        if variety == six.u('multi'):
             return self.read_multi_index(key)
-        elif variety == u'block':
+        elif variety == six.u('block'):
             return self.read_block_index(key)
-        elif variety == u'sparseint':
+        elif variety == six.u('sparseint'):
             return self.read_sparse_intindex(key)
-        elif variety == u'regular':
+        elif variety == six.u('regular'):
             _, index = self.read_index_node(getattr(self.group, key))
             return index
         else:  # pragma: no cover
@@ -1916,13 +1921,13 @@ class GenericStorer(Storer):
         factory = self._get_index_factory(index_class)
 
         kwargs = {}
-        if u'freq' in node._v_attrs:
+        if six.u('freq') in node._v_attrs:
             kwargs['freq'] = node._v_attrs['freq']
 
-        if u'tz' in node._v_attrs:
+        if six.u('tz') in node._v_attrs:
             kwargs['tz'] = node._v_attrs['tz']
 
-        if kind in (u'date', u'datetime'):
+        if kind in (six.u('date'), six.u('datetime')):
             index = factory(_unconvert_index(data, kind, encoding=self.encoding), dtype=object,
                             **kwargs)
         else:
@@ -2031,7 +2036,7 @@ class LegacyFrameStorer(LegacyStorer):
         return DataFrame(values, index=index, columns=columns)
 
 class SeriesStorer(GenericStorer):
-    pandas_kind = u'series'
+    pandas_kind = six.u('series')
     attributes = ['name']
 
     @property
@@ -2058,7 +2063,7 @@ class SeriesStorer(GenericStorer):
         self.attrs.name = obj.name
 
 class SparseSeriesStorer(GenericStorer):
-    pandas_kind = u'sparse_series'
+    pandas_kind = six.u('sparse_series')
     attributes = ['name','fill_value','kind']
 
     def read(self, **kwargs):
@@ -2067,7 +2072,7 @@ class SparseSeriesStorer(GenericStorer):
         sp_values = self.read_array('sp_values')
         sp_index = self.read_index('sp_index')
         return SparseSeries(sp_values, index=index, sparse_index=sp_index,
-                            kind=self.kind or u'block', fill_value=self.fill_value,
+                            kind=self.kind or six.u('block'), fill_value=self.fill_value,
                             name=self.name)
 
     def write(self, obj, **kwargs):
@@ -2080,7 +2085,7 @@ class SparseSeriesStorer(GenericStorer):
         self.attrs.kind = obj.kind
 
 class SparseFrameStorer(GenericStorer):
-    pandas_kind = u'sparse_frame'
+    pandas_kind = six.u('sparse_frame')
     attributes = ['default_kind','default_fill_value']
 
     def read(self, **kwargs):
@@ -2112,7 +2117,7 @@ class SparseFrameStorer(GenericStorer):
         self.write_index('columns', obj.columns)
 
 class SparsePanelStorer(GenericStorer):
-    pandas_kind = u'sparse_panel'
+    pandas_kind = six.u('sparse_panel')
     attributes = ['default_kind','default_fill_value']
 
     def read(self, **kwargs):
@@ -2135,7 +2140,7 @@ class SparsePanelStorer(GenericStorer):
         self.attrs.default_kind       = obj.default_kind
         self.write_index('items', obj.items)
 
-        for name, sdf in obj.iterkv():
+        for name, sdf in obj.iteritems():
             key = 'sparse_frame_%s' % name
             if key not in self.group._v_children:
                 node = self._handle.createGroup(self.group, key)
@@ -2183,7 +2188,7 @@ class BlockManagerStorer(GenericStorer):
         self.validate_read(kwargs)
 
         axes = []
-        for i in xrange(self.ndim):
+        for i in range(self.ndim):
             ax = self.read_index('axis%d' % i)
             axes.append(ax)
 
@@ -2216,11 +2221,11 @@ class BlockManagerStorer(GenericStorer):
             self.write_index('block%d_items' % i, blk.items)
 
 class FrameStorer(BlockManagerStorer):
-    pandas_kind = u'frame'
+    pandas_kind = six.u('frame')
     obj_type    = DataFrame
 
 class PanelStorer(BlockManagerStorer):
-    pandas_kind = u'wide'
+    pandas_kind = six.u('wide')
     obj_type    = Panel
     is_shape_reversed = True
 
@@ -2245,7 +2250,7 @@ class Table(Storer):
         levels        : the names of levels
 
         """
-    pandas_kind = u'wide_table'
+    pandas_kind = six.u('wide_table')
     table_type  = None
     levels      = 1
     is_table    = True
@@ -2319,7 +2324,7 @@ class Table(Storer):
     @property
     def is_exists(self):
         """ has this table been created """
-        return u'table' in self.group
+        return six.u('table') in self.group
 
     @property
     def storable(self):
@@ -2713,9 +2718,9 @@ class Table(Storer):
                 col.set_pos(j)
 
                 self.values_axes.append(col)
-            except (NotImplementedError, ValueError, TypeError), e:
+            except (NotImplementedError, ValueError, TypeError) as e:
                 raise e
-            except (Exception), detail:
+            except (Exception) as detail:
                 raise Exception("cannot find the correct atom type -> [dtype->%s,items->%s] %s" % (b.dtype.name, b.items, str(detail)))
             j += 1
 
@@ -2838,7 +2843,7 @@ class WORMTable(Table):
          table. writing is a one-time operation the data are stored in a format
          that allows for searching the data on disk
          """
-    table_type = u'worm'
+    table_type = six.u('worm')
 
     def read(self, **kwargs):
         """ read the indicies and the indexing array, calculate offset rows and
@@ -2863,7 +2868,7 @@ class LegacyTable(Table):
                    IndexCol(name='column', axis=2,
                             pos=1, index_kind='columns_kind'),
                    DataCol(name='fields', cname='values', kind_attr='fields', pos=2)]
-    table_type = u'legacy'
+    table_type = six.u('legacy')
     ndim = 3
 
     def write(self, **kwargs):
@@ -2953,8 +2958,8 @@ class LegacyTable(Table):
 
 class LegacyFrameTable(LegacyTable):
     """ support the legacy frame table """
-    pandas_kind = u'frame_table'
-    table_type = u'legacy_frame'
+    pandas_kind = six.u('frame_table')
+    table_type = six.u('legacy_frame')
     obj_type = Panel
 
     def read(self, *args, **kwargs):
@@ -2963,14 +2968,14 @@ class LegacyFrameTable(LegacyTable):
 
 class LegacyPanelTable(LegacyTable):
     """ support the legacy panel table """
-    table_type = u'legacy_panel'
+    table_type = six.u('legacy_panel')
     obj_type = Panel
 
 
 class AppendableTable(LegacyTable):
     """ suppor the new appendable table formats """
     _indexables = None
-    table_type = u'appendable'
+    table_type = six.u('appendable')
 
     def write(self, obj, axes=None, append=False, complib=None,
               complevel=None, fletcher32=None, min_itemsize=None, chunksize=None,
@@ -3043,7 +3048,7 @@ class AppendableTable(LegacyTable):
 
         rows = self.nrows_expected
         chunks = int(rows / chunksize) + 1
-        for i in xrange(chunks):
+        for i in range(chunks):
             start_i = i * chunksize
             end_i = min((i + 1) * chunksize, rows)
             if start_i >= end_i:
@@ -3068,14 +3073,14 @@ class AppendableTable(LegacyTable):
             args = list(indexes)
             args.extend([self.dtype, mask, search, values])
             rows = func(*args)
-        except (Exception), detail:
+        except Exception as detail:
             raise Exception("cannot create row-data -> %s" % str(detail))
 
         try:
             if len(rows):
                 self.table.append(rows)
                 self.table.flush()
-        except (Exception), detail:
+        except Exception as detail:
             raise Exception("tables cannot write this data -> %s" % str(detail))
 
     def delete(self, where=None, **kwargs):
@@ -3120,7 +3125,7 @@ class AppendableTable(LegacyTable):
             # we must remove in reverse order!
             pg = groups.pop()
             for g in reversed(groups):
-                rows = l.take(range(g, pg))
+                rows = l.take(list(range(g, pg)))
                 table.removeRows(start=rows[rows.index[0]
                                             ], stop=rows[rows.index[-1]] + 1)
                 pg = g
@@ -3133,8 +3138,8 @@ class AppendableTable(LegacyTable):
 
 class AppendableFrameTable(AppendableTable):
     """ suppor the new appendable table formats """
-    pandas_kind = u'frame_table'
-    table_type = u'appendable_frame'
+    pandas_kind = six.u('frame_table')
+    table_type = six.u('appendable_frame')
     ndim = 2
     obj_type = DataFrame
 
@@ -3188,8 +3193,8 @@ class AppendableFrameTable(AppendableTable):
 
 class GenericTable(AppendableFrameTable):
     """ a table that read/writes the generic pytables table format """
-    pandas_kind = u'frame_table'
-    table_type = u'generic_table'
+    pandas_kind = six.u('frame_table')
+    table_type = six.u('generic_table')
     ndim = 2
     obj_type = DataFrame
 
@@ -3233,13 +3238,13 @@ class GenericTable(AppendableFrameTable):
 
 class AppendableMultiFrameTable(AppendableFrameTable):
     """ a frame with a multi-index """
-    table_type = u'appendable_multiframe'
+    table_type = six.u('appendable_multiframe')
     obj_type = DataFrame
     ndim = 2
 
     @property
     def table_type_short(self):
-        return u'appendable_multi'
+        return six.u('appendable_multi')
 
     def write(self, obj, data_columns=None, **kwargs):
         if data_columns is None:
@@ -3264,7 +3269,7 @@ class AppendableMultiFrameTable(AppendableFrameTable):
 
 class AppendablePanelTable(AppendableTable):
     """ suppor the new appendable table formats """
-    table_type = u'appendable_panel'
+    table_type = six.u('appendable_panel')
     ndim = 3
     obj_type = Panel
 
@@ -3281,7 +3286,7 @@ class AppendablePanelTable(AppendableTable):
 
 class AppendableNDimTable(AppendablePanelTable):
     """ suppor the new appendable table formats """
-    table_type = u'appendable_ndim'
+    table_type = six.u('appendable_ndim')
     ndim = 4
     obj_type = Panel4D
 
@@ -3349,18 +3354,18 @@ def _convert_index(index, encoding=None):
 
 def _unconvert_index(data, kind, encoding=None):
     kind = _ensure_decoded(kind)
-    if kind == u'datetime64':
+    if kind == six.u('datetime64'):
         index = DatetimeIndex(data)
-    elif kind == u'datetime':
+    elif kind == six.u('datetime'):
         index = np.array([datetime.fromtimestamp(v) for v in data],
                          dtype=object)
-    elif kind == u'date':
+    elif kind == six.u('date'):
         index = np.array([date.fromtimestamp(v) for v in data], dtype=object)
-    elif kind in (u'integer', u'float'):
+    elif kind in (six.u('integer'), six.u('float')):
         index = np.array(data)
-    elif kind in (u'string'):
+    elif kind in (six.u('string')):
         index = _unconvert_string_array(data, nan_rep=None, encoding=encoding)
-    elif kind == u'object':
+    elif kind == six.u('object'):
         index = np.array(data[0])
     else:  # pragma: no cover
         raise ValueError('unrecognized index type %s' % kind)
@@ -3368,11 +3373,11 @@ def _unconvert_index(data, kind, encoding=None):
 
 def _unconvert_index_legacy(data, kind, legacy=False, encoding=None):
     kind = _ensure_decoded(kind)
-    if kind == u'datetime':
+    if kind == six.u('datetime'):
         index = lib.time64_to_datetime(data)
-    elif kind in (u'integer'):
+    elif kind in (six.u('integer')):
         index = np.array(data, dtype=object)
-    elif kind in (u'string'):
+    elif kind in (six.u('string')):
         index = _unconvert_string_array(data, nan_rep=None, encoding=encoding)
     else:  # pragma: no cover
         raise ValueError('unrecognized index type %s' % kind)
@@ -3430,7 +3435,7 @@ def _get_converter(kind, encoding):
 
 def _need_convert(kind):
     kind = _ensure_decoded(kind)
-    if kind in (u'datetime', u'datetime64', u'string'):
+    if kind in (six.u('datetime'), six.u('datetime64'), six.u('string')):
         return True
     return False
 
@@ -3496,7 +3501,7 @@ class Term(StringMixin):
             self.value = field.value
 
         # a string expression (or just the field)
-        elif isinstance(field, basestring):
+        elif isinstance(field, six.string_types):
 
             # is a term is passed
             s = self._search.match(field)
@@ -3509,7 +3514,7 @@ class Term(StringMixin):
                 self.field = field
 
                 # is an op passed?
-                if isinstance(op, basestring) and op in self._ops:
+                if isinstance(op, six.string_types) and op in self._ops:
                     self.op = op
                     self.value = value
                 else:
@@ -3530,7 +3535,7 @@ class Term(StringMixin):
 
         # we have valid conditions
         if self.op in ['>', '>=', '<', '<=']:
-            if hasattr(self.value, '__iter__') and len(self.value) > 1 and not isinstance(self.value,basestring):
+            if hasattr(self.value, '__iter__') and len(self.value) > 1 and not isinstance(self.value,six.string_types):
                 raise ValueError("an inequality condition cannot have multiple values [%s]" % str(self))
 
         if not is_list_like(self.value):
@@ -3540,7 +3545,7 @@ class Term(StringMixin):
             self.eval()
 
     def __unicode__(self):
-        attrs = map(pprint_thing, (self.field, self.op, self.value))
+        attrs = list(map(pprint_thing, (self.field, self.op, self.value)))
         return "field->%s,op->%s,value->%s" % tuple(attrs)
 
     @property
@@ -3620,32 +3625,36 @@ class Term(StringMixin):
             return value
 
         kind = _ensure_decoded(self.kind)
-        if kind == u'datetime64' or kind == u'datetime' :
+        if kind == six.u('datetime64') or kind == six.u('datetime'):
             v = lib.Timestamp(v)
             if v.tz is not None:
                 v = v.tz_convert('UTC')
             return TermValue(v,v.value,kind)
-        elif isinstance(v, datetime) or hasattr(v, 'timetuple') or kind == u'date':
+        elif (isinstance(v, datetime) or hasattr(v, 'timetuple')
+              or kind == six.u('date')):
             v = time.mktime(v.timetuple())
             return TermValue(v,Timestamp(v),kind)
-        elif kind == u'integer':
+        elif kind == six.u('integer'):
             v = int(float(v))
             return TermValue(v,v,kind)
-        elif kind == u'float':
+        elif kind == six.u('float'):
             v = float(v)
             return TermValue(v,v,kind)
-        elif kind == u'bool':
-            if isinstance(v, basestring):
-                v = not v.strip().lower() in [u'false', u'f', u'no', u'n', u'none', u'0', u'[]', u'{}', u'']
+        elif kind == six.u('bool'):
+            if isinstance(v, six.string_types):
+                poss_vals = [six.u('false'), six.u('f'), six.u('no'),
+                             six.u('n'), six.u('none'), six.u('0'),
+                             six.u('[]'), six.u('{}'), six.u('')]
+                v = not v.strip().lower() in poss_vals
             else:
                 v = bool(v)
             return TermValue(v,v,kind)
-        elif not isinstance(v, basestring):
+        elif not isinstance(v, six.string_types):
             v = stringify(v)
-            return TermValue(v,stringify(v),u'string')
+            return TermValue(v,stringify(v),six.u('string'))
 
         # string quoting
-        return TermValue(v,stringify(v),u'string')
+        return TermValue(v,stringify(v),six.u('string'))
 
 class TermValue(object):
     """ hold a term value the we use to construct a condition/filter """
@@ -3658,7 +3667,7 @@ class TermValue(object):
     def tostring(self, encoding):
         """ quote the string if not encoded
             else encode and return """
-        if self.kind == u'string':
+        if self.kind == six.u('string'):
             if encoding is not None:
                 return self.converted
             return '"%s"' % self.converted
@@ -3733,7 +3742,7 @@ class Selection(object):
             # operands inside any terms
             if not any([isinstance(w, (list, tuple, Term)) for w in where]):
 
-                if not any([isinstance(w, basestring) and Term._search.match(w) for w in where]):
+                if not any([isinstance(w, six.string_types) and Term._search.match(w) for w in where]):
                     where = [where]
 
         queryables = self.table.queryables()
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index 11b139b62..16ccafcd1 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -2,13 +2,17 @@
 Collection of query wrappers / abstractions to both facilitate data
 retrieval and to reduce dependency on DB-specific API.
 """
+from __future__ import print_function
 from datetime import datetime, date
 
+from pandas.util.py3compat import range
 import numpy as np
 import traceback
 
 from pandas.core.datetools import format as date_format
 from pandas.core.api import DataFrame, isnull
+from six.moves import map, zip
+import six
 
 #------------------------------------------------------------------------------
 # Helper execution function
@@ -51,7 +55,7 @@ def execute(sql, con, retry=True, cur=None, params=None):
         except Exception:  # pragma: no cover
             pass
 
-        print ('Error on sql %s' % sql)
+        print('Error on sql %s' % sql)
         raise
 
 
@@ -61,7 +65,7 @@ def _safe_fetch(cur):
         if not isinstance(result, list):
             result = list(result)
         return result
-    except Exception, e:  # pragma: no cover
+    except Exception as e:  # pragma: no cover
         excName = e.__class__.__name__
         if excName == 'OperationalError':
             return []
@@ -91,7 +95,7 @@ def tquery(sql, con=None, cur=None, retry=True):
         try:
             cur.close()
             con.commit()
-        except Exception, e:
+        except Exception as e:
             excName = e.__class__.__name__
             if excName == 'OperationalError':  # pragma: no cover
                 print ('Failed to commit, may need to restart interpreter')
@@ -121,7 +125,7 @@ def uquery(sql, con=None, cur=None, retry=True, params=None):
     result = cur.rowcount
     try:
         con.commit()
-    except Exception, e:
+    except Exception as e:
         excName = e.__class__.__name__
         if excName != 'OperationalError':
             raise
@@ -198,7 +202,7 @@ def write_frame(frame, name, con, flavor='sqlite', if_exists='fail', **kwargs):
             if_exists='fail'
     exists = table_exists(name, con, flavor)
     if if_exists == 'fail' and exists:
-        raise ValueError, "Table '%s' already exists." % name
+        raise ValueError("Table '%s' already exists." % name)
 
     #create or drop-recreate if necessary
     create = None
@@ -289,7 +293,7 @@ def get_schema(frame, name, flavor, keys=None):
     lookup_type = lambda dtype: get_sqltype(dtype.type, flavor)
     # Replace spaces in DataFrame column names with _.
     safe_columns = [s.replace(' ', '_').strip() for s in frame.dtypes.index]
-    column_types = zip(safe_columns, map(lookup_type, frame.dtypes))
+    column_types = list(zip(safe_columns, map(lookup_type, frame.dtypes)))
     if flavor == 'sqlite':
         columns = ',\n  '.join('[%s] %s' % x for x in column_types)
     else:
@@ -297,7 +301,7 @@ def get_schema(frame, name, flavor, keys=None):
 
     keystr = ''
     if keys is not None:
-        if isinstance(keys, basestring):
+        if isinstance(keys, six.string_types):
             keys = (keys,)
         keystr = ', PRIMARY KEY (%s)' % ','.join(keys)
     template = """CREATE TABLE %(name)s (
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index 9257338cd..50b3d63cb 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -9,8 +9,9 @@ improved version.
 You can find more information on http://presbrey.mit.edu/PyDTA and
 http://statsmodels.sourceforge.net/devel/
 """
-
+# TODO: Fix this module so it can use cross-compatible zip, map, and range
 from StringIO import StringIO
+from pandas.util import compat
 import numpy as np
 
 import sys
@@ -21,6 +22,7 @@ from pandas.core.series import Series
 from pandas.core.categorical import Categorical
 import datetime
 from pandas.util import py3compat
+from pandas.util.py3compat import long
 from pandas import isnull
 from pandas.io.parsers import _parser_params, Appender
 from pandas.io.common import get_filepath_or_buffer
@@ -225,7 +227,7 @@ class StataParser(object):
         # we're going to drop the label and cast to int
         self.DTYPE_MAP = \
             dict(
-                zip(range(1, 245), ['a' + str(i) for i in range(1, 245)]) +
+                list(zip(range(1, 245), ['a' + str(i) for i in range(1, 245)])) +
                 [
                     (251, np.int16),
                     (252, np.int32),
@@ -234,7 +236,7 @@ class StataParser(object):
                     (255, np.float64)
                 ]
             )
-        self.TYPE_MAP = range(251) + list('bhlfd')
+        self.TYPE_MAP = list(range(251)) + list('bhlfd')
         #NOTE: technically, some of these are wrong. there are more numbers
         # that can be represented. it's the 27 ABOVE and BELOW the max listed
         # numeric data type in [U] 12.2.2 of the 11.2 manual
@@ -384,7 +386,7 @@ class StataReader(StataParser):
     def _col_size(self, k=None):
         """Calculate size of a data record."""
         if len(self.col_sizes) == 0:
-            self.col_sizes = map(lambda x: self._calcsize(x), self.typlist)
+            self.col_sizes = list(map(lambda x: self._calcsize(x), self.typlist))
         if k is None:
             return self.col_sizes
         else:
@@ -427,9 +429,9 @@ class StataReader(StataParser):
                     data[i] = self._unpack(typlist[i], self.path_or_buf.read(self._col_size(i)))
             return data
         else:
-            return map(lambda i: self._unpack(typlist[i],
+            return list(map(lambda i: self._unpack(typlist[i],
                                               self.path_or_buf.read(self._col_size(i))),
-                       range(self.nvar))
+                       range(self.nvar)))
 
     def _dataset(self):
         """
diff --git a/pandas/io/tests/generate_legacy_pickles.py b/pandas/io/tests/generate_legacy_pickles.py
index 1838e0907..49a7b90b2 100644
--- a/pandas/io/tests/generate_legacy_pickles.py
+++ b/pandas/io/tests/generate_legacy_pickles.py
@@ -1,4 +1,6 @@
 """ self-contained to write legacy pickle files """
+from __future__ import print_function
+from six.moves import zip
 
 def _create_sp_series():
 
@@ -28,13 +30,13 @@ def _create_sp_frame():
             'B': [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],
             'C': np.arange(10),
             'D': [0, 1, 2, 3, 4, 5, nan, nan, nan, nan]}
-    
+
     dates = bdate_range('1/1/2011', periods=10)
     return SparseDataFrame(data, index=dates)
 
 def create_data():
     """ create the pickle data """
-    
+
     import numpy as np
     import pandas
     from pandas import (Series,DataFrame,Panel,
@@ -50,29 +52,29 @@ def create_data():
         'D': date_range('1/1/2009', periods=5),
         'E' : [0., 1, Timestamp('20100101'),'foo',2.],
         }
-    
-    index  = dict(int   = Index(np.arange(10)),
-                  date  = date_range('20130101',periods=10))
-    mi     = dict(reg   = MultiIndex.from_tuples(zip([['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
-                                                      ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]),
+
+    index = dict(int = Index(np.arange(10)),
+                  date = date_range('20130101',periods=10))
+    mi = dict(reg = MultiIndex.from_tuples(list(zip([['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],
+                                                      ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])),
                                                  names=['first', 'second']))
     series = dict(float = Series(data['A']),
-                  int   = Series(data['B']),
+                  int = Series(data['B']),
                   mixed = Series(data['E']))
-    frame  = dict(float = DataFrame(dict(A = series['float'], B = series['float'] + 1)),
-                  int   = DataFrame(dict(A = series['int']  , B = series['int']   + 1)),
+    frame = dict(float = DataFrame(dict(A = series['float'], B = series['float'] + 1)),
+                  int = DataFrame(dict(A = series['int']  , B = series['int']   + 1)),
                   mixed = DataFrame(dict([ (k,data[k]) for k in ['A','B','C','D']])))
-    panel  = dict(float = Panel(dict(ItemA = frame['float'], ItemB = frame['float']+1)))
+    panel = dict(float = Panel(dict(ItemA = frame['float'], ItemB = frame['float']+1)))
+
 
- 
 
-    return dict( series = series, 
-                 frame  = frame, 
-                 panel  = panel,
-                 index  = index,
-                 mi     = mi,
+    return dict( series = series,
+                 frame = frame,
+                 panel = panel,
+                 index = index,
+                 mi = mi,
                  sp_series = dict(float = _create_sp_series()),
-                 sp_frame  = dict(float = _create_sp_frame())
+                 sp_frame = dict(float = _create_sp_frame())
                  )
 
 def write_legacy_pickles():
@@ -92,9 +94,9 @@ def write_legacy_pickles():
 
     base_dir, _ = os.path.split(os.path.abspath(__file__))
     base_dir = os.path.join(base_dir,'data/legacy_pickle')
-    
+
     # could make this a parameter?
-    version  = None
+    version = None
 
 
     if version is None:
@@ -108,11 +110,11 @@ def write_legacy_pickles():
     # construct a reasonable platform name
     f = '_'.join([ str(pl.machine()), str(pl.system().lower()), str(pl.python_version()) ])
     pth = os.path.abspath(os.path.join(pth,'%s.pickle' % f))
-    
+
     fh = open(pth,'wb')
     pickle.dump(create_data(),fh,pickle.HIGHEST_PROTOCOL)
     fh.close()
-    
+
     print("created pickle file: %s" % pth)
 
 if __name__ == '__main__':
diff --git a/pandas/io/tests/test_cparser.py b/pandas/io/tests/test_cparser.py
index 7fa8d06f4..2063b34c9 100644
--- a/pandas/io/tests/test_cparser.py
+++ b/pandas/io/tests/test_cparser.py
@@ -4,6 +4,7 @@ C/Cython ascii file parser tests
 
 from pandas.util.py3compat import StringIO, BytesIO
 from datetime import datetime
+from pandas.util import compat
 import csv
 import os
 import sys
@@ -29,6 +30,8 @@ import pandas.util.testing as tm
 
 from pandas.parser import TextReader
 import pandas.parser as parser
+import six
+from six.moves import map
 
 
 class TestCParser(unittest.TestCase):
@@ -325,7 +328,7 @@ a,b,c
 
 
 def assert_array_dicts_equal(left, right):
-    for k, v in left.iteritems():
+    for k, v in compat.iteritems(left):
         assert(np.array_equal(v, right[k]))
 
 if __name__ == '__main__':
diff --git a/pandas/io/tests/test_data.py b/pandas/io/tests/test_data.py
index e760ddff5..a6ccc56fb 100644
--- a/pandas/io/tests/test_data.py
+++ b/pandas/io/tests/test_data.py
@@ -1,3 +1,5 @@
+from __future__ import print_function
+from pandas.util import compat
 import unittest
 import warnings
 import nose
@@ -12,11 +14,12 @@ from pandas.io.data import DataReader, SymbolWarning
 from pandas.util.testing import (assert_series_equal, assert_produces_warning,
                                  network, assert_frame_equal)
 from numpy.testing import assert_array_equal
+import six
 
 
 def assert_n_failed_equals_n_null_columns(wngs, obj, cls=SymbolWarning):
     all_nan_cols = pd.Series(dict((k, pd.isnull(v).all()) for k, v in
-                                  obj.iteritems()))
+                                  compat.iteritems(obj)))
     n_all_nan_cols = all_nan_cols.sum()
     valid_warnings = pd.Series([wng for wng in wngs if isinstance(wng, cls)])
     assert_equal(len(valid_warnings), n_all_nan_cols)
@@ -33,7 +36,7 @@ class TestGoogle(unittest.TestCase):
         # an exception when DataReader can't get a 200 response from
         # google
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
 
         self.assertEquals(
             web.DataReader("F", 'google', start, end)['Close'][-1],
@@ -97,7 +100,7 @@ class TestYahoo(unittest.TestCase):
         # an exception when DataReader can't get a 200 response from
         # yahoo
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
 
         self.assertEquals( web.DataReader("F", 'yahoo', start,
                                           end)['Close'][-1], 13.68)
@@ -105,7 +108,7 @@ class TestYahoo(unittest.TestCase):
     @network
     def test_yahoo_fails(self):
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
         self.assertRaises(Exception, web.DataReader, "NON EXISTENT TICKER",
                           'yahoo', start, end)
 
@@ -363,7 +366,7 @@ class TestFred(unittest.TestCase):
         FRED.
         """
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
 
         self.assertEquals(
             web.DataReader("GDP", "fred", start, end)['GDP'].tail(1),
@@ -375,14 +378,14 @@ class TestFred(unittest.TestCase):
     @network
     def test_fred_nan(self):
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
         df = web.DataReader("DFII5", "fred", start, end)
         assert pd.isnull(df.ix['2010-01-01'])
 
     @network
     def test_fred_parts(self):
         start = datetime(2010, 1, 1)
-        end = datetime(2013, 01, 27)
+        end = datetime(2013, 1, 27)
         df = web.get_data_fred("CPIAUCSL", start, end)
         self.assertEqual(df.ix['2010-05-01'], 217.23)
 
diff --git a/pandas/io/tests/test_excel.py b/pandas/io/tests/test_excel.py
index ebbb7292c..251a32cc3 100644
--- a/pandas/io/tests/test_excel.py
+++ b/pandas/io/tests/test_excel.py
@@ -3,6 +3,7 @@
 from pandas.util.py3compat import StringIO, BytesIO, PY3
 from datetime import datetime
 from os.path import split as psplit
+from pandas.util.py3compat import range
 import csv
 import os
 import sys
@@ -35,6 +36,8 @@ import pandas.tseries.tools as tools
 from numpy.testing.decorators import slow
 
 from pandas.parser import OverflowError
+import six
+from six.moves import map
 
 def _skip_if_no_xlrd():
     try:
@@ -707,7 +710,7 @@ class ExcelTests(unittest.TestCase):
         _skip_if_no_excelsuite()
 
         for ext in ['xls', 'xlsx']:
-            filename = u'\u0192u.' + ext
+            filename = six.u('\u0192u.') + ext
 
             try:
                 f = open(filename, 'wb')
@@ -769,7 +772,7 @@ class ExcelTests(unittest.TestCase):
     # def test_to_excel_header_styling_xls(self):
 
     #     import StringIO
-    #     s = StringIO.StringIO(
+    #     s = StringIO(
     #     """Date,ticker,type,value
     #     2001-01-01,x,close,12.2
     #     2001-01-01,x,open ,12.1
@@ -816,7 +819,7 @@ class ExcelTests(unittest.TestCase):
     #     os.remove(filename)
     # def test_to_excel_header_styling_xlsx(self):
     #     import StringIO
-    #     s = StringIO.StringIO(
+    #     s = StringIO(
     #     """Date,ticker,type,value
     #     2001-01-01,x,close,12.2
     #     2001-01-01,x,open ,12.1
diff --git a/pandas/io/tests/test_ga.py b/pandas/io/tests/test_ga.py
index d2061a6d0..e33b75c56 100644
--- a/pandas/io/tests/test_ga.py
+++ b/pandas/io/tests/test_ga.py
@@ -82,8 +82,8 @@ class TestGoogle(unittest.TestCase):
                 dimensions='date',
                 max_results=10, chunksize=5)
 
-            df1 = it.next()
-            df2 = it.next()
+            df1 = next(it)
+            df2 = next(it)
 
             for df in [df1, df2]:
                 assert isinstance(df, DataFrame)
diff --git a/pandas/io/tests/test_html.py b/pandas/io/tests/test_html.py
index 1d0c2a133..2f7c6092d 100644
--- a/pandas/io/tests/test_html.py
+++ b/pandas/io/tests/test_html.py
@@ -1,8 +1,10 @@
+from __future__ import print_function
 import os
 import re
-from cStringIO import StringIO
+from pandas.util.py3compat import StringIO
 from unittest import TestCase
 import warnings
+import six
 from distutils.version import LooseVersion
 import urllib2
 
@@ -12,6 +14,7 @@ from nose.tools import assert_raises
 import numpy as np
 from numpy.random import rand
 from numpy.testing.decorators import slow
+from six.moves import map, zip
 
 try:
     from importlib import import_module
@@ -42,7 +45,7 @@ def _skip_if_no(module_name):
 
 
 def _skip_if_none_of(module_names):
-    if isinstance(module_names, basestring):
+    if isinstance(module_names, six.string_types):
         _skip_if_no(module_names)
         if module_names == 'bs4':
             import bs4
@@ -112,8 +115,8 @@ class TestReadHtmlBase(TestCase):
         out = df.to_html()
         res = self.run_read_html(out, attrs={'class': 'dataframe'},
                                  index_col=0)[0]
-        print (df.dtypes)
-        print (res.dtypes)
+        print(df.dtypes)
+        print(res.dtypes)
         assert_frame_equal(res, df)
 
     @network
@@ -149,7 +152,7 @@ class TestReadHtmlBase(TestCase):
         df2 = self.run_read_html(self.spam_data, 'Unit', infer_types=False)
 
         assert_framelist_equal(df1, df2)
-        print (df1[0])
+        print(df1[0])
 
         self.assertEqual(df1[0].ix[0, 0], 'Proximates')
         self.assertEqual(df1[0].columns[0], 'Nutrient')
@@ -178,7 +181,7 @@ class TestReadHtmlBase(TestCase):
 
     def test_skiprows_xrange(self):
         df1 = [self.run_read_html(self.spam_data, '.*Water.*').pop()[2:]]
-        df2 = self.run_read_html(self.spam_data, 'Unit', skiprows=xrange(2))
+        df2 = self.run_read_html(self.spam_data, 'Unit', skiprows=range(2))
 
         assert_framelist_equal(df1, df2)
 
diff --git a/pandas/io/tests/test_json/test_pandas.py b/pandas/io/tests/test_json/test_pandas.py
index 21fae9a50..2aaffe404 100644
--- a/pandas/io/tests/test_json/test_pandas.py
+++ b/pandas/io/tests/test_json/test_pandas.py
@@ -2,7 +2,9 @@
 # pylint: disable-msg=W0612,E1101
 from copy import deepcopy
 from datetime import datetime, timedelta
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
+from pandas.util import compat
 import cPickle as pickle
 import operator
 import os
@@ -91,7 +93,7 @@ class TestPandasContainer(unittest.TestCase):
             try:
                 unser = read_json(dfjson, orient=orient, dtype=dtype,
                                   numpy=numpy, convert_axes=convert_axes)
-            except (Exception), detail:
+            except (Exception) as detail:
                 if raise_ok is not None:
                     if type(detail) == raise_ok:
                         return
@@ -320,7 +322,7 @@ class TestPandasContainer(unittest.TestCase):
         _check_all_orients(self.ts)
 
         # dtype
-        s = Series(range(6), index=['a','b','c','d','e','f'])
+        s = Series(list(range(6)), index=['a','b','c','d','e','f'])
         _check_all_orients(Series(s, dtype=np.float64), dtype=np.float64)
         _check_all_orients(Series(s, dtype=np.int), dtype=np.int)
 
@@ -340,7 +342,7 @@ class TestPandasContainer(unittest.TestCase):
 
     def test_typ(self):
 
-        s = Series(range(6), index=['a','b','c','d','e','f'], dtype='int64')
+        s = Series(list(range(6)), index=['a','b','c','d','e','f'], dtype='int64')
         result = read_json(s.to_json(),typ=None)
         assert_series_equal(result,s)
 
@@ -439,7 +441,7 @@ class TestPandasContainer(unittest.TestCase):
     def test_doc_example(self):
         dfj2 = DataFrame(np.random.randn(5, 2), columns=list('AB'))
         dfj2['date'] = Timestamp('20130101')
-        dfj2['ints'] = range(5)
+        dfj2['ints'] = list(range(5))
         dfj2['bools'] = True
         dfj2.index = pd.date_range('20130101',periods=5)
 
diff --git a/pandas/io/tests/test_json/test_ujson.py b/pandas/io/tests/test_json/test_ujson.py
index 86aeecf16..a8f6ddffe 100644
--- a/pandas/io/tests/test_json/test_ujson.py
+++ b/pandas/io/tests/test_json/test_ujson.py
@@ -1,7 +1,6 @@
 ﻿import unittest
 from unittest import TestCase
 
-import pandas.json as ujson
 try:
     import json
 except ImportError:
@@ -13,11 +12,15 @@ import sys
 import time
 import datetime
 import calendar
-import StringIO
 import re
 import random
 import decimal
 from functools import partial
+from pandas.util.py3compat import range, StringIO
+from pandas.util import compat
+import pandas.json as ujson
+import six
+from six.moves import zip
 import pandas.util.py3compat as py3compat
 
 import numpy as np
@@ -69,7 +72,7 @@ class UltraJSONTests(TestCase):
         helper(html_encoded, ensure_ascii=False, encode_html_chars=True)
 
     def test_doubleLongIssue(self):
-        sut = {u'a': -4342969734183514}
+        sut = {six.u('a'): -4342969734183514}
         encoded = json.dumps(sut)
         decoded = json.loads(encoded)
         self.assertEqual(sut, decoded)
@@ -78,7 +81,7 @@ class UltraJSONTests(TestCase):
         self.assertEqual(sut, decoded)
 
     def test_doubleLongDecimalIssue(self):
-        sut = {u'a': -12345678901234.56789012}
+        sut = {six.u('a'): -12345678901234.56789012}
         encoded = json.dumps(sut)
         decoded = json.loads(encoded)
         self.assertEqual(sut, decoded)
@@ -88,12 +91,12 @@ class UltraJSONTests(TestCase):
 
 
     def test_encodeDecodeLongDecimal(self):
-        sut = {u'a': -528656961.4399388}
+        sut = {six.u('a'): -528656961.4399388}
         encoded = ujson.dumps(sut, double_precision=15)
         ujson.decode(encoded)
 
     def test_decimalDecodeTestPrecise(self):
-        sut = {u'a': 4.56}
+        sut = {six.u('a'): 4.56}
         encoded = ujson.encode(sut)
         decoded = ujson.decode(encoded, precise_float=True)
         self.assertEqual(sut, decoded)
@@ -109,10 +112,16 @@ class UltraJSONTests(TestCase):
         self.assert_(np.allclose(num, ujson.decode(ujson.encode(num))))
 
     def test_encodeDictWithUnicodeKeys(self):
-        input = { u"key1": u"value1", u"key1": u"value1", u"key1": u"value1", u"key1": u"value1", u"key1": u"value1", u"key1": u"value1" }
+        input = {six.u("key1"): six.u("value1"), six.u("key1"):
+                six.u("value1"), six.u("key1"): six.u("value1"),
+                six.u("key1"): six.u("value1"), six.u("key1"):
+                six.u("value1"), six.u("key1"): six.u("value1")}
         output = ujson.encode(input)
 
-        input = { u"بن": u"value1", u"بن": u"value1", u"بن": u"value1", u"بن": u"value1", u"بن": u"value1", u"بن": u"value1", u"بن": u"value1" }
+        input = {six.u("بن"): six.u("value1"), six.u("بن"): six.u("value1"),
+                six.u("بن"): six.u("value1"), six.u("بن"): six.u("value1"),
+                six.u("بن"): six.u("value1"), six.u("بن"): six.u("value1"),
+                six.u("بن"): six.u("value1")}
         output = ujson.encode(input)
 
         pass
@@ -361,7 +370,7 @@ class UltraJSONTests(TestCase):
         self.assertEquals(dec, json.loads(enc))
 
     def test_decodeFromUnicode(self):
-        input = u"{\"obj\": 31337}"
+        input = six.u("{\"obj\": 31337}")
         dec1 = ujson.decode(input)
         dec2 = ujson.decode(str(input))
         self.assertEquals(dec1, dec2)
@@ -520,18 +529,18 @@ class UltraJSONTests(TestCase):
 
     def test_decodeBrokenDictKeyTypeLeakTest(self):
         input = '{{1337:""}}'
-        for x in xrange(1000):
+        for x in range(1000):
             try:
                 ujson.decode(input)
                 assert False, "Expected exception!"
-            except(ValueError),e:
+            except ValueError as e:
                 continue
 
             assert False, "Wrong exception"
 
     def test_decodeBrokenDictLeakTest(self):
         input = '{{"key":"}'
-        for x in xrange(1000):
+        for x in range(1000):
             try:
                 ujson.decode(input)
                 assert False, "Expected exception!"
@@ -542,7 +551,7 @@ class UltraJSONTests(TestCase):
 
     def test_decodeBrokenListLeakTest(self):
         input = '[[[true'
-        for x in xrange(1000):
+        for x in range(1000):
             try:
                 ujson.decode(input)
                 assert False, "Expected exception!"
@@ -611,7 +620,7 @@ class UltraJSONTests(TestCase):
         self.assertEquals(output, json.dumps(input))
         self.assertEquals(input, ujson.decode(output))
 
-        self.assertEquals('"  \\u0000\\r\\n "', ujson.dumps(u"  \u0000\r\n "))
+        self.assertEquals('"  \\u0000\\r\\n "', ujson.dumps(six.u("  \u0000\r\n ")))
         pass
 
     def test_decodeNullCharacter(self):
@@ -678,7 +687,7 @@ class UltraJSONTests(TestCase):
         self.assertAlmostEqual(output, json.loads(input))
 
     def test_dumpToFile(self):
-        f = StringIO.StringIO()
+        f = StringIO()
         ujson.dump([1, 2, 3], f)
         self.assertEquals("[1,2,3]", f.getvalue())
 
@@ -701,9 +710,9 @@ class UltraJSONTests(TestCase):
             assert False, 'expected TypeError'
 
     def test_loadFile(self):
-        f = StringIO.StringIO("[1,2,3,4]")
+        f = StringIO("[1,2,3,4]")
         self.assertEquals([1, 2, 3, 4], ujson.load(f))
-        f = StringIO.StringIO("[1,2,3,4]")
+        f = StringIO("[1,2,3,4]")
         assert_array_equal(np.array([1, 2, 3, 4]), ujson.load(f, numpy=True))
 
     def test_loadFileLikeObject(self):
@@ -740,7 +749,7 @@ class UltraJSONTests(TestCase):
             assert False, "expected OverflowError"
 
     def test_encodeNumericOverflowNested(self):
-        for n in xrange(0, 100):
+        for n in range(0, 100):
             class Nested:
                 x = 12839128391289382193812939
 
@@ -769,7 +778,7 @@ class UltraJSONTests(TestCase):
             self.assertEqual(ujson.decode(doc)['id'], result)
 
     def test_encodeBigEscape(self):
-        for x in xrange(10):
+        for x in range(10):
             if py3compat.PY3:
                 base = '\u00e5'.encode('utf-8')
             else:
@@ -778,7 +787,7 @@ class UltraJSONTests(TestCase):
             output = ujson.encode(input)
 
     def test_decodeBigEscape(self):
-        for x in xrange(10):
+        for x in range(10):
             if py3compat.PY3:
                 base = '\u00e5'.encode('utf-8')
             else:
@@ -788,7 +797,7 @@ class UltraJSONTests(TestCase):
             output = ujson.decode(input)
 
     def test_toDict(self):
-        d = {u"key": 31337}
+        d = {six.u("key"): 31337}
 
         class DictTest:
             def toDict(self):
@@ -1034,7 +1043,7 @@ class NumpyJSONTests(TestCase):
         output = ujson.loads(ujson.dumps(input), numpy=True, labelled=True)
         self.assertTrue((np.array([42]) == output[0]).all())
         self.assertTrue(output[1] is None)
-        self.assertTrue((np.array([u'a']) == output[2]).all())
+        self.assertTrue((np.array([six.u('a')]) == output[2]).all())
 
         # py3 is non-determinstic on the ordering......
         if not py3compat.PY3:
@@ -1043,7 +1052,7 @@ class NumpyJSONTests(TestCase):
             expectedvals = np.array([42, 31, 24, 99, 2.4, 78], dtype=int).reshape((3,2))
             self.assertTrue((expectedvals == output[0]).all())
             self.assertTrue(output[1] is None)
-            self.assertTrue((np.array([u'a', 'b']) == output[2]).all())
+            self.assertTrue((np.array([six.u('a'), 'b']) == output[2]).all())
 
 
             input = {1: {'a': 42, 'b':31}, 2: {'a': 24, 'c': 99}, 3: {'a': 2.4, 'b': 78}}
@@ -1331,7 +1340,7 @@ class PandasJSONTests(TestCase):
         try:
             input = "9223372036854775808"
             ujson.decode(input)
-        except ValueError, e:
+        except ValueError as e:
             pass
         else:
             assert False, "expected ValueError"
@@ -1340,7 +1349,7 @@ class PandasJSONTests(TestCase):
         try:
             input = "-90223372036854775809"
             ujson.decode(input)
-        except ValueError,e:
+        except ValueError as e:
             pass
         else:
             assert False, "expected ValueError"
@@ -1418,7 +1427,7 @@ class PandasJSONTests(TestCase):
 
     def test_encodeBigSet(self):
         s = set()
-        for x in xrange(0, 100000):
+        for x in range(0, 100000):
             s.add(x)
         ujson.encode(s)
 
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index b88b1ab77..198de5d0f 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -2,6 +2,7 @@
 
 from pandas.util.py3compat import StringIO, BytesIO, PY3
 from datetime import datetime
+from pandas.util.py3compat import range, long
 import csv
 import os
 import sys
@@ -36,6 +37,8 @@ import pandas.tseries.tools as tools
 from numpy.testing.decorators import slow
 
 from pandas.parser import OverflowError
+import six
+from six.moves import map
 
 
 class ParserTests(object):
@@ -110,10 +113,10 @@ g,7,seven
     def test_read_csv(self):
         if not py3compat.PY3:
             if 'win' in sys.platform:
-                prefix = u"file:///"
+                prefix = six.u("file:///")
             else:
-                prefix = u"file://"
-            fname = prefix + unicode(self.csv1)
+                prefix = six.u("file://")
+            fname = prefix + six.text_type(self.csv1)
             # it works!
             df1 = read_csv(fname, index_col=0, parse_dates=True)
 
@@ -181,7 +184,6 @@ j,-inF"""
         df = read_csv(StringIO(data), index_col=0)
         assert_almost_equal(df['A'].values, expected.values)
         df = read_csv(StringIO(data), index_col=0, na_filter=False)
-        print df['A'].values
         assert_almost_equal(df['A'].values, expected.values)
 
     def test_multiple_date_col(self):
@@ -316,7 +318,7 @@ KORD,19990127, 22:00:00, 21:56:00, -0.5900, 1.7100, 5.1000, 0.0000, 290.0000
 KORD,19990127, 23:00:00, 22:56:00, -0.5900, 1.7100, 4.6000, 0.0000, 280.0000"""
 
         df = self.read_csv(StringIO(data), parse_dates={'nominal': [1, 2]})
-        self.assert_(not isinstance(df.nominal[0], basestring))
+        self.assert_(not isinstance(df.nominal[0], six.string_types))
 
     ts_data = """\
 ID,date,nominalTime,actualTime,A,B,C,D,E
@@ -423,7 +425,7 @@ A,B,C
             df = self.read_table(
                 StringIO(data), sep=',', header=1, comment='#')
             self.assert_(False)
-        except Exception, inst:
+        except Exception as inst:
             self.assert_('Expected 3 fields in line 4, saw 5' in str(inst))
 
         # skip_footer
@@ -440,7 +442,7 @@ footer
                 StringIO(data), sep=',', header=1, comment='#',
                 skip_footer=1)
             self.assert_(False)
-        except Exception, inst:
+        except Exception as inst:
             self.assert_('Expected 3 fields in line 4, saw 5' in str(inst))
 
         # first chunk
@@ -458,7 +460,7 @@ skip
                                  skiprows=[2])
             df = it.read(5)
             self.assert_(False)
-        except Exception, inst:
+        except Exception as inst:
             self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
         # middle chunk
@@ -477,7 +479,7 @@ skip
             df = it.read(1)
             it.read(2)
             self.assert_(False)
-        except Exception, inst:
+        except Exception as inst:
             self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
         # last chunk
@@ -496,7 +498,7 @@ skip
             df = it.read(1)
             it.read()
             self.assert_(False)
-        except Exception, inst:
+        except Exception as inst:
             self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
     def test_passing_dtype(self):
@@ -610,7 +612,7 @@ ignore,this,row
 
         # GH 3062
         df = DataFrame(dict({
-                    'A' : np.asarray(range(10),dtype='float64'),
+                    'A' : np.asarray(list(range(10)),dtype='float64'),
                     'B' : pd.Timestamp('20010101') }))
         df.iloc[3:6,:] = np.nan
 
@@ -640,7 +642,7 @@ ignore,this,row
 1/2/2000,4,5,6
 1/3/2000,7,8,9
 """
-        data = self.read_csv(StringIO(text), skiprows=range(6), header=None,
+        data = self.read_csv(StringIO(text), skiprows=list(range(6)), header=None,
                              index_col=0, parse_dates=True)
 
         data2 = self.read_csv(StringIO(text), skiprows=6, header=None,
@@ -793,20 +795,20 @@ c,4,5
 15/01/2010;P;P;50;1;14/1/2011
 01/05/2010;P;P;50;1;15/1/2011'''
 
-        expected = self.read_csv(StringIO(data), sep=";", index_col=range(4))
+        expected = self.read_csv(StringIO(data), sep=";", index_col=list(range(4)))
 
         lev = expected.index.levels[0]
         expected.index.levels[0] = lev.to_datetime(dayfirst=True)
         expected['aux_date'] = to_datetime(expected['aux_date'],
                                            dayfirst=True)
-        expected['aux_date'] = map(Timestamp, expected['aux_date'])
+        expected['aux_date'] = list(map(Timestamp, expected['aux_date']))
         self.assert_(isinstance(expected['aux_date'][0], datetime))
 
-        df = self.read_csv(StringIO(data), sep=";", index_col=range(4),
+        df = self.read_csv(StringIO(data), sep=";", index_col=list(range(4)),
                            parse_dates=[0, 5], dayfirst=True)
         tm.assert_frame_equal(df, expected)
 
-        df = self.read_csv(StringIO(data), sep=";", index_col=range(4),
+        df = self.read_csv(StringIO(data), sep=";", index_col=list(range(4)),
                            parse_dates=['date', 'aux_date'], dayfirst=True)
         tm.assert_frame_equal(df, expected)
 
@@ -829,7 +831,7 @@ c,4,5
 
         self.assert_(np.array_equal(df_pref.columns,
                                     ['X0', 'X1', 'X2', 'X3', 'X4']))
-        self.assert_(np.array_equal(df.columns, range(5)))
+        self.assert_(np.array_equal(df.columns, list(range(5))))
 
         self.assert_(np.array_equal(df2.columns, names))
 
@@ -870,7 +872,7 @@ baz,7,8,9
         tm.assert_frame_equal(df, df2)
 
     def test_read_table_unicode(self):
-        fin = BytesIO(u'\u0141aski, Jan;1'.encode('utf-8'))
+        fin = BytesIO(six.u('\u0141aski, Jan;1').encode('utf-8'))
         df1 = read_table(fin, sep=";", encoding="utf-8", header=None)
         self.assert_(isinstance(df1[0].values[0], unicode))
 
@@ -1553,23 +1555,23 @@ False,NA,True"""
 
         sfile = StringIO(s)
         # it's 33 columns
-        result = self.read_csv(sfile, names=range(33), na_values=['-9999.0'],
+        result = self.read_csv(sfile, names=list(range(33)), na_values=['-9999.0'],
                                header=None, skipinitialspace=True)
         self.assertTrue(pd.isnull(result.ix[0, 29]))
 
     def test_utf16_bom_skiprows(self):
         # #2298
-        data = u"""skip this
+        data = six.u("""skip this
 skip this too
 A\tB\tC
 1\t2\t3
-4\t5\t6"""
+4\t5\t6""")
 
-        data2 = u"""skip this
+        data2 = six.u("""skip this
 skip this too
 A,B,C
 1,2,3
-4,5,6"""
+4,5,6""")
 
         path = '__%s__.csv' % tm.rands(10)
 
@@ -1610,7 +1612,7 @@ A,B,C
         if hash(np.int64(-1)) != -2:
             raise nose.SkipTest
 
-        import StringIO
+        from pandas.util.py3compat import StringIO
         csv = """id,score,days
 1,2,12
 2,2-5,
@@ -1646,20 +1648,20 @@ A,B,C
             if not x:
                 return np.nan
             if x.find('-') > 0:
-                valmin, valmax = map(int, x.split('-'))
+                valmin, valmax = list(map(int, x.split('-')))
                 val = 0.5 * (valmin + valmax)
             else:
                 val = float(x)
 
             return val
 
-        fh = StringIO.StringIO(csv)
+        fh = StringIO(csv)
         result = self.read_csv(fh, converters={'score': convert_score,
                                                'days': convert_days},
                                na_values=['', None])
         self.assert_(pd.isnull(result['days'][1]))
 
-        fh = StringIO.StringIO(csv)
+        fh = StringIO(csv)
         result2 = self.read_csv(fh, converters={'score': convert_score,
                                                 'days': convert_days_sentinel},
                                 na_values=['', None])
@@ -1672,7 +1674,7 @@ A,B,C
         result = result.set_index(0)
 
         got = result[1][1632]
-        expected = u'\xc1 k\xf6ldum klaka (Cold Fever) (1994)'
+        expected = six.u('\xc1 k\xf6ldum klaka (Cold Fever) (1994)')
 
         self.assertEquals(got, expected)
 
@@ -1800,13 +1802,13 @@ baz|7|8|9
                               sep=None, skiprows=2)
         tm.assert_frame_equal(data, data3)
 
-        text = u"""ignore this
+        text = six.u("""ignore this
 ignore this too
 index|A|B|C
 foo|1|2|3
 bar|4|5|6
 baz|7|8|9
-""".encode('utf-8')
+""").encode('utf-8')
 
         s = BytesIO(text)
         if py3compat.PY3:
@@ -2325,9 +2327,9 @@ No,No,No"""
         data = "1,2\n3,4,5"
 
         result = self.read_csv(StringIO(data), header=None,
-                               names=range(50))
+                               names=list(range(50)))
         expected = self.read_csv(StringIO(data), header=None,
-                                 names=range(3)).reindex(columns=range(50))
+                                 names=list(range(3))).reindex(columns=list(range(50)))
 
         tm.assert_frame_equal(result, expected)
 
@@ -2374,9 +2376,11 @@ class TestParseSQL(unittest.TestCase):
         assert_same_values_and_dtype(result, expected)
 
     def test_convert_sql_column_unicode(self):
-        arr = np.array([u'1.5', None, u'3', u'4.2'], dtype=object)
+        arr = np.array([six.u('1.5'), None, six.u('3'), six.u('4.2')],
+                       dtype=object)
         result = lib.convert_sql_column(arr)
-        expected = np.array([u'1.5', np.nan, u'3', u'4.2'], dtype=object)
+        expected = np.array([six.u('1.5'), np.nan, six.u('3'), six.u('4.2')],
+                            dtype=object)
         assert_same_values_and_dtype(result, expected)
 
     def test_convert_sql_column_ints(self):
@@ -2394,12 +2398,12 @@ class TestParseSQL(unittest.TestCase):
         assert_same_values_and_dtype(result, expected)
 
     def test_convert_sql_column_longs(self):
-        arr = np.array([1L, 2L, 3L, 4L], dtype='O')
+        arr = np.array([long(1), long(2), long(3), long(4)], dtype='O')
         result = lib.convert_sql_column(arr)
         expected = np.array([1, 2, 3, 4], dtype='i8')
         assert_same_values_and_dtype(result, expected)
 
-        arr = np.array([1L, 2L, 3L, None, 4L], dtype='O')
+        arr = np.array([long(1), long(2), long(3), None, long(4)], dtype='O')
         result = lib.convert_sql_column(arr)
         expected = np.array([1, 2, 3, np.nan, 4], dtype='f8')
         assert_same_values_and_dtype(result, expected)
diff --git a/pandas/io/tests/test_pickle.py b/pandas/io/tests/test_pickle.py
index 5c79c57c1..69a52f448 100644
--- a/pandas/io/tests/test_pickle.py
+++ b/pandas/io/tests/test_pickle.py
@@ -27,7 +27,7 @@ class TestPickle(unittest.TestCase):
     def compare(self, vf):
 
         # py3 compat when reading py2 pickle
-        
+
         try:
             with open(vf,'rb') as fh:
                 data = pickle.load(fh)
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index 6518f9cb6..aff43cc91 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -1,3 +1,5 @@
+from __future__ import print_function
+from pandas.util.py3compat import range
 import nose
 import unittest
 import os
@@ -20,6 +22,7 @@ from pandas import concat, Timestamp
 from pandas.util import py3compat
 
 from numpy.testing.decorators import slow
+import six
 
 try:
     import tables
@@ -127,7 +130,7 @@ class TestHDFStore(unittest.TestCase):
             tm.assert_panel_equal(o, roundtrip('panel',o))
 
             # table
-            df = DataFrame(dict(A=range(5), B=range(5)))
+            df = DataFrame(dict(A=list(range(5)), B=list(range(5))))
             df.to_hdf(self.path,'table',append=True)
             result = read_hdf(self.path, 'table', where = ['index>2'])
             assert_frame_equal(df[df.index>2],result)
@@ -481,7 +484,7 @@ class TestHDFStore(unittest.TestCase):
             raise nose.SkipTest('system byteorder is not little, skipping test_encoding!')
 
         with ensure_clean(self.path) as store:
-            df = DataFrame(dict(A='foo',B='bar'),index=range(5))
+            df = DataFrame(dict(A='foo',B='bar'),index=list(range(5)))
             df.loc[2,'A'] = np.nan
             df.loc[3,'B'] = np.nan
             _maybe_remove(store, 'df')
@@ -604,7 +607,7 @@ class TestHDFStore(unittest.TestCase):
             for i in range(10):
 
                 df = DataFrame(np.random.randn(10,2),columns=list('AB'))
-                df['index'] = range(10)
+                df['index'] = list(range(10))
                 df['index'] += i*10
                 df['int64'] = Series([1]*len(df),dtype='int64')
                 df['int16'] = Series([1]*len(df),dtype='int16')
@@ -780,7 +783,7 @@ class TestHDFStore(unittest.TestCase):
             def check_col(key,name,size):
                 self.assert_(getattr(store.get_storer(key).table.description,name).itemsize == size)
 
-            df = DataFrame(dict(A = 'foo', B = 'bar'),index=range(10))
+            df = DataFrame(dict(A = 'foo', B = 'bar'),index=list(range(10)))
 
             # a min_itemsize that creates a data_column
             _maybe_remove(store, 'df')
@@ -1015,8 +1018,8 @@ class TestHDFStore(unittest.TestCase):
         raise nose.SkipTest('no big table frame')
 
         # create and write a big table
-        df = DataFrame(np.random.randn(2000 * 100, 100), index=range(
-            2000 * 100), columns=['E%03d' % i for i in xrange(100)])
+        df = DataFrame(np.random.randn(2000 * 100, 100), index=list(range(
+            2000 * 100)), columns=['E%03d' % i for i in range(100)])
         for x in range(20):
             df['String%03d' % x] = 'string%03d' % x
 
@@ -1027,7 +1030,7 @@ class TestHDFStore(unittest.TestCase):
             rows = store.root.df.table.nrows
             recons = store.select('df')
 
-        print ("\nbig_table frame [%s] -> %5.2f" % (rows, time.time() - x))
+        print("\nbig_table frame [%s] -> %5.2f" % (rows, time.time() - x))
 
     def test_big_table2_frame(self):
         # this is a really big table: 1m rows x 60 float columns, 20 string, 20 datetime
@@ -1038,14 +1041,14 @@ class TestHDFStore(unittest.TestCase):
         print ("\nbig_table2 start")
         import time
         start_time = time.time()
-        df = DataFrame(np.random.randn(1000 * 1000, 60), index=xrange(int(
-            1000 * 1000)), columns=['E%03d' % i for i in xrange(60)])
-        for x in xrange(20):
+        df = DataFrame(np.random.randn(1000 * 1000, 60), index=range(int(
+            1000 * 1000)), columns=['E%03d' % i for i in range(60)])
+        for x in range(20):
             df['String%03d' % x] = 'string%03d' % x
-        for x in xrange(20):
+        for x in range(20):
             df['datetime%03d' % x] = datetime.datetime(2001, 1, 2, 0, 0)
 
-        print ("\nbig_table2 frame (creation of df) [rows->%s] -> %5.2f"
+        print("\nbig_table2 frame (creation of df) [rows->%s] -> %5.2f"
                     % (len(df.index), time.time() - start_time))
 
         def f(chunksize):
@@ -1056,9 +1059,9 @@ class TestHDFStore(unittest.TestCase):
 
         for c in [10000, 50000, 250000]:
             start_time = time.time()
-            print ("big_table2 frame [chunk->%s]" % c)
+            print("big_table2 frame [chunk->%s]" % c)
             rows = f(c)
-            print ("big_table2 frame [rows->%s,chunk->%s] -> %5.2f"
+            print("big_table2 frame [rows->%s,chunk->%s] -> %5.2f"
                     % (rows, c, time.time() - start_time))
 
     def test_big_put_frame(self):
@@ -1067,14 +1070,14 @@ class TestHDFStore(unittest.TestCase):
         print ("\nbig_put start")
         import time
         start_time = time.time()
-        df = DataFrame(np.random.randn(1000 * 1000, 60), index=xrange(int(
-            1000 * 1000)), columns=['E%03d' % i for i in xrange(60)])
-        for x in xrange(20):
+        df = DataFrame(np.random.randn(1000 * 1000, 60), index=range(int(
+            1000 * 1000)), columns=['E%03d' % i for i in range(60)])
+        for x in range(20):
             df['String%03d' % x] = 'string%03d' % x
-        for x in xrange(20):
+        for x in range(20):
             df['datetime%03d' % x] = datetime.datetime(2001, 1, 2, 0, 0)
 
-        print ("\nbig_put frame (creation of df) [rows->%s] -> %5.2f"
+        print("\nbig_put frame (creation of df) [rows->%s] -> %5.2f"
                 % (len(df.index), time.time() - start_time))
 
         with ensure_clean(self.path, mode='w') as store:
@@ -1082,8 +1085,8 @@ class TestHDFStore(unittest.TestCase):
             store = HDFStore(fn, mode='w')
             store.put('df', df)
 
-            print (df.get_dtype_counts())
-            print ("big_put frame [shape->%s] -> %5.2f"
+            print(df.get_dtype_counts())
+            print("big_put frame [shape->%s] -> %5.2f"
                     % (df.shape, time.time() - start_time))
 
     def test_big_table_panel(self):
@@ -1091,8 +1094,8 @@ class TestHDFStore(unittest.TestCase):
 
         # create and write a big table
         wp = Panel(
-            np.random.randn(20, 1000, 1000), items=['Item%03d' % i for i in xrange(20)],
-            major_axis=date_range('1/1/2000', periods=1000), minor_axis=['E%03d' % i for i in xrange(1000)])
+            np.random.randn(20, 1000, 1000), items=['Item%03d' % i for i in range(20)],
+            major_axis=date_range('1/1/2000', periods=1000), minor_axis=['E%03d' % i for i in range(1000)])
 
         wp.ix[:, 100:200, 300:400] = np.nan
 
@@ -1108,7 +1111,7 @@ class TestHDFStore(unittest.TestCase):
             rows = store.root.wp.table.nrows
             recons = store.select('wp')
 
-        print ("\nbig_table panel [%s] -> %5.2f" % (rows, time.time() - x))
+        print("\nbig_table panel [%s] -> %5.2f" % (rows, time.time() - x))
 
     def test_append_diff_item_order(self):
 
@@ -1328,7 +1331,7 @@ class TestHDFStore(unittest.TestCase):
 
             # py3 ok for unicode
             if not py3compat.PY3:
-                l.append(('unicode', u'\u03c3'))
+                l.append(('unicode', six.u('\u03c3')))
 
             ### currently not supported dtypes ####
             for n, f in l:
@@ -1377,14 +1380,14 @@ class TestHDFStore(unittest.TestCase):
             compare(store.select('df_tz',where=Term('A','>=',df.A[3])),df[df.A>=df.A[3]])
 
             _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130103',tz='US/Eastern')),index=range(5))
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130103',tz='US/Eastern')),index=list(range(5)))
             store.append('df_tz',df)
             result = store['df_tz']
             compare(result,df)
             assert_frame_equal(result,df)
 
             _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='EET')),index=range(5))
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='EET')),index=list(range(5)))
             self.assertRaises(TypeError, store.append, 'df_tz', df)
 
             # this is ok
@@ -1395,14 +1398,14 @@ class TestHDFStore(unittest.TestCase):
             assert_frame_equal(result,df)
 
             # can't append with diff timezone
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='CET')),index=range(5))
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='CET')),index=list(range(5)))
             self.assertRaises(ValueError, store.append, 'df_tz', df)
 
         # as index
         with ensure_clean(self.path) as store:
 
             # GH 4098 example
-            df = DataFrame(dict(A = Series(xrange(3), index=date_range('2000-1-1',periods=3,freq='H', tz='US/Eastern'))))
+            df = DataFrame(dict(A = Series(range(3), index=date_range('2000-1-1',periods=3,freq='H', tz='US/Eastern'))))
 
             _maybe_remove(store, 'df')
             store.put('df',df)
@@ -1989,12 +1992,12 @@ class TestHDFStore(unittest.TestCase):
 
             # selection on the non-indexable with a large number of columns
             wp = Panel(
-                np.random.randn(100, 100, 100), items=['Item%03d' % i for i in xrange(100)],
-                major_axis=date_range('1/1/2000', periods=100), minor_axis=['E%03d' % i for i in xrange(100)])
+                np.random.randn(100, 100, 100), items=['Item%03d' % i for i in range(100)],
+                major_axis=date_range('1/1/2000', periods=100), minor_axis=['E%03d' % i for i in range(100)])
 
             _maybe_remove(store, 'wp')
             store.append('wp', wp)
-            items = ['Item%03d' % i for i in xrange(80)]
+            items = ['Item%03d' % i for i in range(80)]
             result = store.select('wp', Term('items', items))
             expected = wp.reindex(items=items)
             tm.assert_panel_equal(expected, result)
@@ -2092,7 +2095,7 @@ class TestHDFStore(unittest.TestCase):
 
             df = DataFrame(dict(ts=bdate_range('2012-01-01', periods=300),
                                 A=np.random.randn(300),
-                                B=range(300),
+                                B=list(range(300)),
                                 users = ['a']*50 + ['b']*50 + ['c']*100 + ['a%03d' % i for i in range(100)]))
             _maybe_remove(store, 'df')
             store.append('df', df, data_columns=['ts', 'A', 'B', 'users'])
@@ -2108,12 +2111,12 @@ class TestHDFStore(unittest.TestCase):
             tm.assert_frame_equal(expected, result)
 
             # big selector along the columns
-            selector = [ 'a','b','c' ] + [ 'a%03d' % i for i in xrange(60) ]
+            selector = [ 'a','b','c' ] + [ 'a%03d' % i for i in range(60) ]
             result = store.select('df', [Term('ts', '>=', Timestamp('2012-02-01')),Term('users',selector)])
             expected = df[ (df.ts >= Timestamp('2012-02-01')) & df.users.isin(selector) ]
             tm.assert_frame_equal(expected, result)
 
-            selector = range(100,200)
+            selector = list(range(100,200))
             result = store.select('df', [Term('B', selector)])
             expected = df[ df.B.isin(selector) ]
             tm.assert_frame_equal(expected, result)
@@ -2211,7 +2214,7 @@ class TestHDFStore(unittest.TestCase):
     def test_retain_index_attributes(self):
 
         # GH 3499, losing frequency info on index recreation
-        df = DataFrame(dict(A = Series(xrange(3),
+        df = DataFrame(dict(A = Series(range(3),
                                        index=date_range('2000-1-1',periods=3,freq='H'))))
 
         with ensure_clean(self.path) as store:
@@ -2228,7 +2231,7 @@ class TestHDFStore(unittest.TestCase):
 
             # try to append a table with a different frequency
             warnings.filterwarnings('ignore', category=AttributeConflictWarning)
-            df2 = DataFrame(dict(A = Series(xrange(3),
+            df2 = DataFrame(dict(A = Series(range(3),
                                             index=date_range('2002-1-1',periods=3,freq='D'))))
             store.append('data',df2)
             warnings.filterwarnings('always', category=AttributeConflictWarning)
@@ -2237,10 +2240,10 @@ class TestHDFStore(unittest.TestCase):
 
             # this is ok
             _maybe_remove(store,'df2')
-            df2 = DataFrame(dict(A = Series(xrange(3),
+            df2 = DataFrame(dict(A = Series(range(3),
                                             index=[Timestamp('20010101'),Timestamp('20010102'),Timestamp('20020101')])))
             store.append('df2',df2)
-            df3 = DataFrame(dict(A = Series(xrange(3),index=date_range('2002-1-1',periods=3,freq='D'))))
+            df3 = DataFrame(dict(A = Series(range(3),index=date_range('2002-1-1',periods=3,freq='D'))))
             store.append('df2',df3)
 
     def test_retain_index_attributes2(self):
@@ -2249,20 +2252,20 @@ class TestHDFStore(unittest.TestCase):
 
             warnings.filterwarnings('ignore', category=AttributeConflictWarning)
 
-            df  = DataFrame(dict(A = Series(xrange(3), index=date_range('2000-1-1',periods=3,freq='H'))))
+            df  = DataFrame(dict(A = Series(range(3), index=date_range('2000-1-1',periods=3,freq='H'))))
             df.to_hdf(path,'data',mode='w',append=True)
-            df2 = DataFrame(dict(A = Series(xrange(3), index=date_range('2002-1-1',periods=3,freq='D'))))
+            df2 = DataFrame(dict(A = Series(range(3), index=date_range('2002-1-1',periods=3,freq='D'))))
             df2.to_hdf(path,'data',append=True)
 
             idx = date_range('2000-1-1',periods=3,freq='H')
             idx.name = 'foo'
-            df  = DataFrame(dict(A = Series(xrange(3), index=idx)))
+            df  = DataFrame(dict(A = Series(range(3), index=idx)))
             df.to_hdf(path,'data',mode='w',append=True)
             self.assert_(read_hdf(path,'data').index.name == 'foo')
 
             idx2 = date_range('2001-1-1',periods=3,freq='H')
             idx2.name = 'bar'
-            df2 = DataFrame(dict(A = Series(xrange(3), index=idx2)))
+            df2 = DataFrame(dict(A = Series(range(3), index=idx2)))
             df2.to_hdf(path,'data',append=True)
             self.assert_(read_hdf(path,'data').index.name is None)
 
@@ -2422,7 +2425,7 @@ class TestHDFStore(unittest.TestCase):
             # get coordinates back & test vs frame
             _maybe_remove(store, 'df')
 
-            df = DataFrame(dict(A=range(5), B=range(5)))
+            df = DataFrame(dict(A=list(range(5)), B=list(range(5))))
             store.append('df', df)
             c = store.select_as_coordinates('df', ['index<3'])
             assert((c.values == np.arange(3)).all() == True)
@@ -2527,11 +2530,11 @@ class TestHDFStore(unittest.TestCase):
                 expected = concat([df1, df2], axis=1)
                 expected = expected[5:]
                 tm.assert_frame_equal(result, expected)
-            except (Exception), detail:
-                print ("error in select_as_multiple %s" % str(detail))
-                print ("store: %s" % store)
-                print ("df1: %s" % df1)
-                print ("df2: %s" % df2)
+            except (Exception) as detail:
+                print("error in select_as_multiple %s" % str(detail))
+                print("store: %s" % store)
+                print("df1: %s" % df1)
+                print("df2: %s" % df2)
 
 
             # test excpection for diff rows
@@ -2751,7 +2754,7 @@ class TestHDFStore(unittest.TestCase):
                        columns=['A', 'B', 'C'])
         store.append('mi', df)
 
-        df = DataFrame(dict(A = 'foo', B = 'bar'),index=range(10))
+        df = DataFrame(dict(A = 'foo', B = 'bar'),index=list(range(10)))
         store.append('df', df, data_columns = ['B'], min_itemsize={'A' : 200 })
 
         store.close()
@@ -2808,7 +2811,7 @@ class TestHDFStore(unittest.TestCase):
 
     def test_unicode_index(self):
 
-        unicode_values = [u'\u03c3', u'\u03c3\u03c3']
+        unicode_values = [six.u('\u03c3'), six.u('\u03c3\u03c3')]
         warnings.filterwarnings('ignore', category=PerformanceWarning)
         s = Series(np.random.randn(len(unicode_values)), unicode_values)
         self._check_roundtrip(s, tm.assert_series_equal)
diff --git a/pandas/io/tests/test_sql.py b/pandas/io/tests/test_sql.py
index 5b23bf173..614b401ce 100644
--- a/pandas/io/tests/test_sql.py
+++ b/pandas/io/tests/test_sql.py
@@ -1,5 +1,7 @@
+from __future__ import print_function
 from __future__ import with_statement
 from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
 import unittest
 import sqlite3
 import sys
@@ -171,15 +173,15 @@ class TestSQLite(unittest.TestCase):
 
         frame['txt'] = ['a'] * len(frame)
         frame2 = frame.copy()
-        frame2['Idx'] = Index(range(len(frame2))) + 10
+        frame2['Idx'] = Index(list(range(len(frame2)))) + 10
         sql.write_frame(frame2, name='test_table2', con=self.db)
         result = sql.read_frame("select * from test_table2", self.db,
                                 index_col='Idx')
         expected = frame.copy()
-        expected.index = Index(range(len(frame2))) + 10
+        expected.index = Index(list(range(len(frame2)))) + 10
         expected.index.name = 'Idx'
-        print expected.index.names
-        print result.index.names
+        print(expected.index.names)
+        print(result.index.names)
         tm.assert_frame_equal(expected, result)
 
     def test_tquery(self):
@@ -257,12 +259,12 @@ class TestMySQL(unittest.TestCase):
             return
         try:
             self.db = MySQLdb.connect(read_default_group='pandas')
-        except MySQLdb.ProgrammingError, e:
+        except MySQLdb.ProgrammingError as e:
             raise nose.SkipTest(
                 "Create a group of connection parameters under the heading "
                 "[pandas] in your system's mysql default file, "
                 "typically located at ~/.my.cnf or /etc/.my.cnf. ")
-        except MySQLdb.Error, e:
+        except MySQLdb.Error as e:
             raise nose.SkipTest(
                 "Cannot connect to database. "
                 "Create a group of connection parameters under the heading "
@@ -408,7 +410,7 @@ class TestMySQL(unittest.TestCase):
 
         frame['txt'] = ['a'] * len(frame)
         frame2 = frame.copy()
-        index = Index(range(len(frame2))) + 10
+        index = Index(list(range(len(frame2)))) + 10
         frame2['Idx'] = index
         drop_sql = "DROP TABLE IF EXISTS test_table2"
         cur = self.db.cursor()
diff --git a/pandas/io/tests/test_wb.py b/pandas/io/tests/test_wb.py
index 46eeabaf1..e1492c13c 100644
--- a/pandas/io/tests/test_wb.py
+++ b/pandas/io/tests/test_wb.py
@@ -5,20 +5,23 @@ from pandas.util.testing import network
 from pandas.util.testing import assert_frame_equal
 from numpy.testing.decorators import slow
 from pandas.io.wb import search, download
+import six
 
 
 @slow
 @network
 def test_wdi_search():
     raise nose.SkipTest
-    expected = {u'id': {2634: u'GDPPCKD',
-                        4649: u'NY.GDP.PCAP.KD',
-                        4651: u'NY.GDP.PCAP.KN',
-                        4653: u'NY.GDP.PCAP.PP.KD'},
-                u'name': {2634: u'GDP per Capita, constant US$, millions',
-                          4649: u'GDP per capita (constant 2000 US$)',
-                          4651: u'GDP per capita (constant LCU)',
-                          4653: u'GDP per capita, PPP (constant 2005 international $)'}}
+    expected = {six.u('id'): {2634: six.u('GDPPCKD'),
+                        4649: six.u('NY.GDP.PCAP.KD'),
+                        4651: six.u('NY.GDP.PCAP.KN'),
+                        4653: six.u('NY.GDP.PCAP.PP.KD')},
+                six.u('name'): {2634: six.u('GDP per Capita, constant US$, '
+                                             'millions'),
+                          4649: six.u('GDP per capita (constant 2000 US$)'),
+                          4651: six.u('GDP per capita (constant LCU)'),
+                          4653: six.u('GDP per capita, PPP (constant 2005 '
+                                      'international $)')}}
     result = search('gdp.*capita.*constant').ix[:, :2]
     expected = pandas.DataFrame(expected)
     expected.index = result.index
@@ -29,7 +32,7 @@ def test_wdi_search():
 @network
 def test_wdi_download():
     raise nose.SkipTest
-    expected = {'GDPPCKN': {(u'United States', u'2003'): u'40800.0735367688', (u'Canada', u'2004'): u'37857.1261134552', (u'United States', u'2005'): u'42714.8594790102', (u'Canada', u'2003'): u'37081.4575704003', (u'United States', u'2004'): u'41826.1728310667', (u'Mexico', u'2003'): u'72720.0691255285', (u'Mexico', u'2004'): u'74751.6003347038', (u'Mexico', u'2005'): u'76200.2154469437', (u'Canada', u'2005'): u'38617.4563629611'}, 'GDPPCKD': {(u'United States', u'2003'): u'40800.0735367688', (u'Canada', u'2004'): u'34397.055116118', (u'United States', u'2005'): u'42714.8594790102', (u'Canada', u'2003'): u'33692.2812368928', (u'United States', u'2004'): u'41826.1728310667', (u'Mexico', u'2003'): u'7608.43848670658', (u'Mexico', u'2004'): u'7820.99026814334', (u'Mexico', u'2005'): u'7972.55364129367', (u'Canada', u'2005'): u'35087.8925933298'}}
+    expected = {'GDPPCKN': {(six.u('United States'), six.u('2003')): six.u('40800.0735367688'), (six.u('Canada'), six.u('2004')): six.u('37857.1261134552'), (six.u('United States'), six.u('2005')): six.u('42714.8594790102'), (six.u('Canada'), six.u('2003')): six.u('37081.4575704003'), (six.u('United States'), six.u('2004')): six.u('41826.1728310667'), (six.u('Mexico'), six.u('2003')): six.u('72720.0691255285'), (six.u('Mexico'), six.u('2004')): six.u('74751.6003347038'), (six.u('Mexico'), six.u('2005')): six.u('76200.2154469437'), (six.u('Canada'), six.u('2005')): six.u('38617.4563629611')}, 'GDPPCKD': {(six.u('United States'), six.u('2003')): six.u('40800.0735367688'), (six.u('Canada'), six.u('2004')): six.u('34397.055116118'), (six.u('United States'), six.u('2005')): six.u('42714.8594790102'), (six.u('Canada'), six.u('2003')): six.u('33692.2812368928'), (six.u('United States'), six.u('2004')): six.u('41826.1728310667'), (six.u('Mexico'), six.u('2003')): six.u('7608.43848670658'), (six.u('Mexico'), six.u('2004')): six.u('7820.99026814334'), (six.u('Mexico'), six.u('2005')): six.u('7972.55364129367'), (six.u('Canada'), six.u('2005')): six.u('35087.8925933298')}}
     expected = pandas.DataFrame(expected)
     result = download(country=['CA', 'MX', 'US', 'junk'], indicator=['GDPPCKD',
                                                                      'GDPPCKN', 'junk'], start=2003, end=2005)
diff --git a/pandas/io/wb.py b/pandas/io/wb.py
index f83ed296e..5048551cf 100644
--- a/pandas/io/wb.py
+++ b/pandas/io/wb.py
@@ -1,8 +1,11 @@
+from __future__ import print_function
 from urllib2 import urlopen
+from pandas.util.py3compat import range
 import json
 from contextlib import closing
 import pandas
 import numpy as np
+from six.moves import map, reduce
 
 
 def download(country=['MX', 'CA', 'US'], indicator=['GDPPCKD', 'GDPPCKN'],
@@ -65,10 +68,10 @@ def download(country=['MX', 'CA', 'US'], indicator=['GDPPCKD', 'GDPPCKN'],
             bad_indicators.append(ind)
     # Warn
     if len(bad_indicators) > 0:
-        print ('Failed to obtain indicator(s): %s' % '; '.join(bad_indicators))
+        print('Failed to obtain indicator(s): %s' % '; '.join(bad_indicators))
         print ('The data may still be available for download at http://data.worldbank.org')
     if len(bad_countries) > 0:
-        print ('Invalid ISO-2 codes: %s' % ' '.join(bad_countries))
+        print('Invalid ISO-2 codes: %s' % ' '.join(bad_countries))
     # Merge WDI series
     if len(data) > 0:
         out = reduce(lambda x, y: x.merge(y, how='outer'), data)
@@ -90,10 +93,10 @@ def _get_data(indicator="NY.GNS.ICTR.GN.ZS", country='US',
         data = response.read()
     # Parse JSON file
     data = json.loads(data)[1]
-    country = map(lambda x: x['country']['value'], data)
-    iso2c = map(lambda x: x['country']['id'], data)
-    year = map(lambda x: x['date'], data)
-    value = map(lambda x: x['value'], data)
+    country = list(map(lambda x: x['country']['value'], data))
+    iso2c = list(map(lambda x: x['country']['id'], data))
+    year = list(map(lambda x: x['date'], data))
+    value = list(map(lambda x: x['value'], data))
     # Prepare output
     out = pandas.DataFrame([country, iso2c, year, value]).T
     return out
@@ -107,10 +110,10 @@ def get_countries():
         data = response.read()
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
-    data.adminregion = map(lambda x: x['value'], data.adminregion)
-    data.incomeLevel = map(lambda x: x['value'], data.incomeLevel)
-    data.lendingType = map(lambda x: x['value'], data.lendingType)
-    data.region = map(lambda x: x['value'], data.region)
+    data.adminregion = list(map(lambda x: x['value'], data.adminregion))
+    data.incomeLevel = list(map(lambda x: x['value'], data.incomeLevel))
+    data.lendingType = list(map(lambda x: x['value'], data.lendingType))
+    data.region = list(map(lambda x: x['value'], data.region))
     data = data.rename(columns={'id': 'iso3c', 'iso2Code': 'iso2c'})
     return data
 
@@ -124,7 +127,7 @@ def get_indicators():
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
     # Clean fields
-    data.source = map(lambda x: x['value'], data.source)
+    data.source = list(map(lambda x: x['value'], data.source))
     fun = lambda x: x.encode('ascii', 'ignore')
     data.sourceOrganization = data.sourceOrganization.apply(fun)
     # Clean topic field
@@ -134,12 +137,12 @@ def get_indicators():
             return x['value']
         except:
             return ''
-    fun = lambda x: map(lambda y: get_value(y), x)
+    fun = lambda x: list(map(lambda y: get_value(y), x))
     data.topics = data.topics.apply(fun)
     data.topics = data.topics.apply(lambda x: ' ; '.join(x))
     # Clean outpu
     data = data.sort(columns='id')
-    data.index = pandas.Index(range(data.shape[0]))
+    data.index = pandas.Index(list(range(data.shape[0])))
     return data
 
 
diff --git a/pandas/rpy/__init__.py b/pandas/rpy/__init__.py
index 3e77a0b0b..d5cf8a420 100644
--- a/pandas/rpy/__init__.py
+++ b/pandas/rpy/__init__.py
@@ -1,4 +1,4 @@
 try:
-    from common import importr, r, load_data
+    from .common import importr, r, load_data
 except ImportError:
     pass
diff --git a/pandas/rpy/common.py b/pandas/rpy/common.py
index 92adee5bd..75065a19d 100644
--- a/pandas/rpy/common.py
+++ b/pandas/rpy/common.py
@@ -2,7 +2,10 @@
 Utilities for making working with rpy2 more user- and
 developer-friendly.
 """
+from __future__ import print_function
 
+from six.moves import zip
+from pandas.util.py3compat import range
 import numpy as np
 
 import pandas as pd
@@ -73,7 +76,7 @@ def _convert_array(obj):
                             major_axis=name_list[0],
                             minor_axis=name_list[1])
         else:
-            print ('Cannot handle dim=%d' % len(dim))
+            print('Cannot handle dim=%d' % len(dim))
     else:
         return arr
 
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index f5e57efdc..7bc6f818c 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -6,6 +6,8 @@ with float64 data
 # pylint: disable=E1101,E1103,W0231,E0202
 
 from numpy import nan
+from pandas.util.py3compat import range
+from pandas.util import compat
 import numpy as np
 
 from pandas.core.common import _pickle_array, _unpickle_array, _try_sort
@@ -21,6 +23,7 @@ import pandas.core.datetools as datetools
 from pandas.sparse.series import SparseSeries
 from pandas.util.decorators import Appender
 import pandas.lib as lib
+from six.moves import map
 
 
 class _SparseMockBlockManager(object):
@@ -259,7 +262,7 @@ class SparseDataFrame(DataFrame):
         for k, v in self.iteritems():
             d[v.dtype.name] += 1
         return Series(d)
-     
+
     def astype(self, dtype):
         raise NotImplementedError
 
@@ -649,7 +652,7 @@ class SparseDataFrame(DataFrame):
 
     def _rename_index_inplace(self, mapper):
         self.index = [mapper(x) for x in self.index]
- 
+
     def _rename_columns_inplace(self, mapper):
         new_series = {}
         new_columns = []
@@ -850,7 +853,7 @@ class SparseDataFrame(DataFrame):
     def applymap(self, func):
         """
         Apply a function to a DataFrame that is intended to operate
-        elementwise, i.e. like doing map(func, series) for each series in the
+        elementwise, i.e. like doing list(map(func, series)) for each series in the
         DataFrame
 
         Parameters
@@ -862,12 +865,12 @@ class SparseDataFrame(DataFrame):
         -------
         applied : DataFrame
         """
-        return self.apply(lambda x: map(func, x))
+        return self.apply(lambda x: list(map(func, x)))
 
     @Appender(DataFrame.fillna.__doc__)
     def fillna(self, value=None, method=None, inplace=False, limit=None):
         new_series = {}
-        for k, v in self.iterkv():
+        for k, v in self.iteritems():
             new_series[k] = v.fillna(value=value, method=method, limit=limit)
 
         if inplace:
diff --git a/pandas/sparse/panel.py b/pandas/sparse/panel.py
index 246e6fa93..746b91a89 100644
--- a/pandas/sparse/panel.py
+++ b/pandas/sparse/panel.py
@@ -5,6 +5,9 @@ with float64 data
 
 # pylint: disable=E1101,E1103,W0231
 
+from pandas.util.py3compat import range
+from six.moves import zip
+from pandas.util import compat
 import numpy as np
 
 from pandas.core.index import Index, MultiIndex, _ensure_index
@@ -205,7 +208,7 @@ class SparsePanel(Panel):
 
     def __delitem__(self, key):
         loc = self.items.get_loc(key)
-        indices = range(loc) + range(loc + 1, len(self.items))
+        indices = list(range(loc)) + list(range(loc + 1, len(self.items)))
         del self._frames[key]
         self._items = self._items.take(indices)
 
@@ -346,7 +349,7 @@ class SparsePanel(Panel):
             return self._combinePanel(other, func)
         elif np.isscalar(other):
             new_frames = dict((k, func(v, other))
-                              for k, v in self.iterkv())
+                              for k, v in self.iteritems())
             return self._new_like(new_frames)
 
     def _combineFrame(self, other, func, axis=0):
@@ -423,7 +426,7 @@ class SparsePanel(Panel):
         y : DataFrame
             index -> minor axis, columns -> items
         """
-        slices = dict((k, v.xs(key)) for k, v in self.iterkv())
+        slices = dict((k, v.xs(key)) for k, v in self.iteritems())
         return DataFrame(slices, index=self.minor_axis, columns=self.items)
 
     def minor_xs(self, key):
@@ -440,7 +443,7 @@ class SparsePanel(Panel):
         y : SparseDataFrame
             index -> major axis, columns -> items
         """
-        slices = dict((k, v[key]) for k, v in self.iterkv())
+        slices = dict((k, v[key]) for k, v in self.iteritems())
         return SparseDataFrame(slices, index=self.major_axis,
                                columns=self.items,
                                default_fill_value=self.default_fill_value,
diff --git a/pandas/sparse/tests/test_array.py b/pandas/sparse/tests/test_array.py
index a92170621..96edc71d1 100644
--- a/pandas/sparse/tests/test_array.py
+++ b/pandas/sparse/tests/test_array.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import re
 from numpy import nan, ndarray
 import numpy as np
diff --git a/pandas/sparse/tests/test_list.py b/pandas/sparse/tests/test_list.py
index a69385dd9..47ad7b0c1 100644
--- a/pandas/sparse/tests/test_list.py
+++ b/pandas/sparse/tests/test_list.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import unittest
 
 from numpy import nan
@@ -6,7 +7,7 @@ import numpy as np
 from pandas.sparse.api import SparseList, SparseArray
 from pandas.util.testing import assert_almost_equal
 
-from test_sparse import assert_sp_array_equal
+from .test_sparse import assert_sp_array_equal
 
 
 def assert_sp_list_equal(left, right):
diff --git a/pandas/sparse/tests/test_sparse.py b/pandas/sparse/tests/test_sparse.py
index 1382a6a64..75d58f483 100644
--- a/pandas/sparse/tests/test_sparse.py
+++ b/pandas/sparse/tests/test_sparse.py
@@ -1,6 +1,8 @@
 # pylint: disable-msg=E1101,W0612
 
 from unittest import TestCase
+from pandas.util.py3compat import range
+from pandas.util import compat
 import cPickle as pickle
 import operator
 from datetime import datetime
@@ -36,7 +38,7 @@ import pandas.tests.test_panel as test_panel
 import pandas.tests.test_series as test_series
 from pandas.util.py3compat import StringIO
 
-from test_array import assert_sp_array_equal
+from .test_array import assert_sp_array_equal
 
 import warnings
 warnings.filterwarnings(action='ignore', category=FutureWarning)
@@ -105,7 +107,7 @@ def assert_sp_frame_equal(left, right, exact_indices=True):
 
 
 def assert_sp_panel_equal(left, right, exact_indices=True):
-    for item, frame in left.iterkv():
+    for item, frame in left.iteritems():
         assert(item in right)
         # trade-off?
         assert_sp_frame_equal(frame, right[item], exact_indices=exact_indices)
@@ -315,7 +317,7 @@ class TestSparseSeries(TestCase,
             for idx, val in dense.iteritems():
                 assert_almost_equal(val, sp[idx])
 
-            for i in xrange(len(dense)):
+            for i in range(len(dense)):
                 assert_almost_equal(sp[i], dense[i])
                 # j = np.float64(i)
                 # assert_almost_equal(sp[j], dense[j])
@@ -826,7 +828,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
 
     def test_constructor_convert_index_once(self):
         arr = np.array([1.5, 2.5, 3.5])
-        sdf = SparseDataFrame(columns=range(4), index=arr)
+        sdf = SparseDataFrame(columns=list(range(4)), index=arr)
         self.assertTrue(sdf[0].index is sdf[1].index)
 
     def test_constructor_from_series(self):
@@ -843,7 +845,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         x2 = x.astype(float)
         x2.ix[:9998] = np.NaN
         x_sparse = x2.to_sparse(fill_value=np.NaN)
-        
+
         # Currently fails too with weird ufunc error
         # df1 = SparseDataFrame([x_sparse, y])
 
@@ -867,7 +869,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         sdf = df.to_sparse()
 
         str(sdf)
-        
+
     def test_array_interface(self):
         res = np.sqrt(self.frame)
         dres = np.sqrt(self.frame.to_dense())
@@ -1217,7 +1219,7 @@ class TestSparseDataFrame(TestCase, test_frame.SafeForSparse):
         self.assertRaises(Exception, self.frame.astype, np.int64)
 
     def test_fillna(self):
-        df = self.zframe.reindex(range(5))
+        df = self.zframe.reindex(list(range(5)))
         result = df.fillna(0)
         expected = df.to_dense().fillna(0).to_sparse(fill_value=0)
         assert_sp_frame_equal(result, expected)
diff --git a/pandas/src/generate_code.py b/pandas/src/generate_code.py
index 2d5873393..040e12922 100644
--- a/pandas/src/generate_code.py
+++ b/pandas/src/generate_code.py
@@ -1,5 +1,7 @@
+from __future__ import print_function
+from pandas.util.py3compat import range
 import os
-from cStringIO import StringIO
+from pandas.util.py3compat import cStringIO as StringIO
 
 header = """
 cimport numpy as np
@@ -2290,21 +2292,21 @@ take_templates = [take_1d_template,
 
 def generate_take_cython_file(path='generated.pyx'):
     with open(path, 'w') as f:
-        print >> f, header
+        print(header, file=f)
 
-        print >> f, generate_ensure_dtypes()
+        print(generate_ensure_dtypes(), file=f)
 
         for template in templates_1d:
-            print >> f, generate_from_template(template)
+            print(generate_from_template(template), file=f)
 
         for template in take_templates:
-            print >> f, generate_take_template(template)
+            print(generate_take_template(template), file=f)
 
         for template in put_2d:
-            print >> f, generate_put_template(template)
+            print(generate_put_template(template), file=f)
 
         for template in groupbys:
-            print >> f, generate_put_template(template, use_ints = False)
+            print(generate_put_template(template, use_ints = False), file=f)
 
         # for template in templates_1d_datetime:
         #     print >> f, generate_from_template_datetime(template)
@@ -2313,7 +2315,7 @@ def generate_take_cython_file(path='generated.pyx'):
         #     print >> f, generate_from_template_datetime(template, ndim=2)
 
         for template in nobool_1d_templates:
-            print >> f, generate_from_template(template, exclude=['bool'])
+            print(generate_from_template(template, exclude=['bool']), file=f)
 
 if __name__ == '__main__':
     generate_take_cython_file()
diff --git a/pandas/src/offsets.pyx b/pandas/src/offsets.pyx
index 1823edeb0..096198c8a 100644
--- a/pandas/src/offsets.pyx
+++ b/pandas/src/offsets.pyx
@@ -85,6 +85,10 @@ cdef class _Offset:
     cpdef next(self):
         pass
 
+    cpdef __next__(self):
+        """wrapper around next"""
+        return self.next()
+
     cpdef prev(self):
         pass
 
diff --git a/pandas/stats/fama_macbeth.py b/pandas/stats/fama_macbeth.py
index 967199c0b..9e4e62a07 100644
--- a/pandas/stats/fama_macbeth.py
+++ b/pandas/stats/fama_macbeth.py
@@ -1,6 +1,7 @@
 from pandas.core.base import StringMixin
 from pandas.util.py3compat import StringIO
 
+from pandas.util.py3compat import range
 import numpy as np
 
 from pandas.core.api import Series, DataFrame
@@ -173,7 +174,7 @@ class MovingFamaMacBeth(FamaMacBeth):
 
         start = self._window - 1
         betas = self._beta_raw
-        for i in xrange(start, self._T):
+        for i in range(start, self._T):
             if self._is_rolling:
                 begin = i - start
             else:
@@ -213,7 +214,7 @@ def _calc_t_stat(beta, nw_lags_beta):
     C = np.dot(B.T, B) / N
 
     if nw_lags_beta is not None:
-        for i in xrange(nw_lags_beta + 1):
+        for i in range(nw_lags_beta + 1):
 
             cov = np.dot(B[i:].T, B[:(N - i)]) / N
             weight = i / (nw_lags_beta + 1)
diff --git a/pandas/stats/math.py b/pandas/stats/math.py
index 579d49edb..7a36654a4 100644
--- a/pandas/stats/math.py
+++ b/pandas/stats/math.py
@@ -3,6 +3,7 @@
 
 from __future__ import division
 
+from pandas.util.py3compat import range
 import numpy as np
 import numpy.linalg as linalg
 
@@ -70,7 +71,7 @@ def newey_west(m, max_lags, nobs, df, nw_overlap=False):
     Covariance Matrix, Econometrica, vol. 55(3), 703-708
     """
     Xeps = np.dot(m.T, m)
-    for lag in xrange(1, max_lags + 1):
+    for lag in range(1, max_lags + 1):
         auto_cov = np.dot(m[:-lag].T, m[lag:])
         weight = lag / (max_lags + 1)
         if nw_overlap:
diff --git a/pandas/stats/misc.py b/pandas/stats/misc.py
index e81319cb7..3e5db98d8 100644
--- a/pandas/stats/misc.py
+++ b/pandas/stats/misc.py
@@ -1,8 +1,11 @@
 from numpy import NaN
+from pandas.util import compat
 import numpy as np
 
 from pandas.core.api import Series, DataFrame, isnull, notnull
 from pandas.core.series import remove_na
+import six
+from six.moves import zip
 
 
 def zscore(series):
@@ -21,7 +24,7 @@ def correl_ts(frame1, frame2):
     y : Series
     """
     results = {}
-    for col, series in frame1.iteritems():
+    for col, series in compat.iteritems(frame1):
         if col in frame2:
             other = frame2[col]
 
@@ -82,15 +85,15 @@ def percentileRank(frame, column=None, kind='mean'):
     framet = frame.T
     if column is not None:
         if isinstance(column, Series):
-            for date, xs in frame.T.iteritems():
+            for date, xs in compat.iteritems(frame.T):
                 results[date] = fun(xs, column.get(date, NaN))
         else:
-            for date, xs in frame.T.iteritems():
+            for date, xs in compat.iteritems(frame.T):
                 results[date] = fun(xs, xs[column])
         results = Series(results)
     else:
         for column in frame.columns:
-            for date, xs in framet.iteritems():
+            for date, xs in compat.iteritems(framet):
                 results.setdefault(date, {})[column] = fun(xs, xs[column])
         results = DataFrame(results).T
     return results
diff --git a/pandas/stats/ols.py b/pandas/stats/ols.py
index 742d832a9..e9563dcd1 100644
--- a/pandas/stats/ols.py
+++ b/pandas/stats/ols.py
@@ -4,9 +4,12 @@ Ordinary least squares regression
 
 # pylint: disable-msg=W0201
 
-from itertools import izip, starmap
-from StringIO import StringIO
+from six.moves import zip
+from itertools import starmap
+from pandas.util.py3compat import StringIO
 
+from pandas.util.py3compat import range
+from pandas.util import compat
 import numpy as np
 
 from pandas.core.api import DataFrame, Series, isnull
@@ -41,7 +44,7 @@ class OLS(StringMixin):
         Number of Newey-West lags.
     nw_overlap : boolean, default False
         Assume data is overlapping when computing Newey-West estimator
-    
+
     """
     _panel_model = False
 
@@ -610,15 +613,15 @@ class MovingOLS(OLS):
     window : int
         size of window (for rolling/expanding OLS)
     min_periods : int
-        Threshold of non-null data points to require. 
-        If None, defaults to size of window. 
+        Threshold of non-null data points to require.
+        If None, defaults to size of window.
     intercept : bool
         True if you want an intercept.
     nw_lags : None or int
         Number of Newey-West lags.
     nw_overlap : boolean, default False
         Assume data is overlapping when computing Newey-West estimator
-    
+
     """
     def __init__(self, y, x, weights=None, window_type='expanding',
                  window=None, min_periods=None, intercept=True,
@@ -743,7 +746,7 @@ class MovingOLS(OLS):
         """Returns the covariance of beta."""
         result = {}
         result_index = self._result_index
-        for i in xrange(len(self._var_beta_raw)):
+        for i in range(len(self._var_beta_raw)):
             dm = DataFrame(self._var_beta_raw[i], columns=self.beta.columns,
                            index=self.beta.columns)
             result[result_index[i]] = dm
@@ -803,7 +806,7 @@ class MovingOLS(OLS):
         cum_xx = self._cum_xx(x)
         cum_xy = self._cum_xy(x, y)
 
-        for i in xrange(N):
+        for i in range(N):
             if not valid[i] or not enough[i]:
                 continue
 
@@ -948,7 +951,7 @@ class MovingOLS(OLS):
                 return Fst, (q, d), 1 - f.cdf(Fst, q, d)
 
             # Compute the P-value for each pair
-            result = starmap(get_result_simple, izip(F, df_resid))
+            result = starmap(get_result_simple, zip(F, df_resid))
 
             return list(result)
 
@@ -968,7 +971,7 @@ class MovingOLS(OLS):
             return math.calc_F(R, r, beta, vcov, n, d)
 
         results = starmap(get_result,
-                          izip(self._beta_raw, self._var_beta_raw, nobs, df))
+                          zip(self._beta_raw, self._var_beta_raw, nobs, df))
 
         return list(results)
 
@@ -978,7 +981,7 @@ class MovingOLS(OLS):
         from scipy.stats import t
 
         result = [2 * t.sf(a, b)
-                  for a, b in izip(np.fabs(self._t_stat_raw),
+                  for a, b in zip(np.fabs(self._t_stat_raw),
                                    self._df_resid_raw)]
 
         return np.array(result)
@@ -1062,7 +1065,7 @@ class MovingOLS(OLS):
     def _std_err_raw(self):
         """Returns the raw standard err values."""
         results = []
-        for i in xrange(len(self._var_beta_raw)):
+        for i in range(len(self._var_beta_raw)):
             results.append(np.sqrt(np.diag(self._var_beta_raw[i])))
 
         return np.array(results)
diff --git a/pandas/stats/plm.py b/pandas/stats/plm.py
index e8c413ec4..44f0dcf2b 100644
--- a/pandas/stats/plm.py
+++ b/pandas/stats/plm.py
@@ -6,6 +6,8 @@ Linear regression objects for panel data
 # pylint: disable-msg=E1101,E1103
 
 from __future__ import division
+from pandas.util.py3compat import range
+from pandas.util import compat
 import warnings
 
 import numpy as np
diff --git a/pandas/stats/tests/test_fama_macbeth.py b/pandas/stats/tests/test_fama_macbeth.py
index ef262cfaf..593d6ab5e 100644
--- a/pandas/stats/tests/test_fama_macbeth.py
+++ b/pandas/stats/tests/test_fama_macbeth.py
@@ -1,7 +1,8 @@
 from pandas import DataFrame, Panel
 from pandas.stats.api import fama_macbeth
-from common import assert_almost_equal, BaseTest
+from .common import assert_almost_equal, BaseTest
 
+from pandas.util.py3compat import range
 import numpy as np
 
 
@@ -28,7 +29,7 @@ class TestFamaMacBeth(BaseTest):
         index = result._index
         time = len(index)
 
-        for i in xrange(time - window + 1):
+        for i in range(time - window + 1):
             if window_type == 'rolling':
                 start = index[i]
             else:
@@ -37,7 +38,7 @@ class TestFamaMacBeth(BaseTest):
             end = index[i + window - 1]
 
             x2 = {}
-            for k, v in x.iterkv():
+            for k, v in x.iteritems():
                 x2[k] = v.truncate(start, end)
             y2 = y.truncate(start, end)
 
diff --git a/pandas/stats/tests/test_moments.py b/pandas/stats/tests/test_moments.py
index 6312a2859..c948d2aba 100644
--- a/pandas/stats/tests/test_moments.py
+++ b/pandas/stats/tests/test_moments.py
@@ -1,3 +1,5 @@
+from pandas.util.py3compat import range
+from six.moves import zip
 import unittest
 import nose
 import sys
@@ -487,7 +489,7 @@ class TestMoments(unittest.TestCase):
             assert_frame_equal(frame_xp, frame_rs)
 
     def test_legacy_time_rule_arg(self):
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         # suppress deprecation warnings
         sys.stderr = StringIO()
 
diff --git a/pandas/stats/tests/test_ols.py b/pandas/stats/tests/test_ols.py
index 88f9224e8..cbfbc0ad1 100644
--- a/pandas/stats/tests/test_ols.py
+++ b/pandas/stats/tests/test_ols.py
@@ -7,6 +7,7 @@ Unit test suite for OLS and PanelOLS classes
 from __future__ import division
 
 from datetime import datetime
+from pandas.util import compat
 import unittest
 import nose
 import numpy as np
@@ -23,6 +24,7 @@ from pandas.util.testing import (assert_almost_equal, assert_series_equal,
 import pandas.util.testing as tm
 
 from common import BaseTest
+import six
 
 _have_statsmodels = True
 try:
@@ -196,7 +198,7 @@ class TestOLS(BaseTest):
             date = index[i]
 
             x_iter = {}
-            for k, v in x.iteritems():
+            for k, v in compat.iteritems(x):
                 x_iter[k] = v.truncate(before=prior_date, after=date)
             y_iter = y.truncate(before=prior_date, after=date)
 
@@ -529,7 +531,7 @@ class TestPanelOLS(BaseTest):
 
         stack_y = y.stack()
         stack_x = DataFrame(dict((k, v.stack())
-                                 for k, v in x.iterkv()))
+                                 for k, v in x.iteritems()))
 
         weights = x.std('items')
         stack_weights = weights.stack()
@@ -722,7 +724,7 @@ class TestPanelOLS(BaseTest):
             date = index[i]
 
             x_iter = {}
-            for k, v in x.iteritems():
+            for k, v in compat.iteritems(x):
                 x_iter[k] = v.truncate(before=prior_date, after=date)
             y_iter = y.truncate(before=prior_date, after=date)
 
diff --git a/pandas/stats/tests/test_var.py b/pandas/stats/tests/test_var.py
index cbaacd0e8..99ee9f3bf 100644
--- a/pandas/stats/tests/test_var.py
+++ b/pandas/stats/tests/test_var.py
@@ -1,7 +1,9 @@
+from __future__ import print_function
 from numpy.testing import run_module_suite, assert_equal, TestCase
 
 from pandas.util.testing import assert_almost_equal
 
+from pandas.util.py3compat import range
 import nose
 import unittest
 
@@ -124,10 +126,10 @@ class RVAR(object):
         return rpy.convert_robj(r.coef(self._estimate))
 
     def summary(self, equation=None):
-        print (r.summary(self._estimate, equation=equation))
+        print(r.summary(self._estimate, equation=equation))
 
     def output(self):
-        print (self._estimate)
+        print(self._estimate)
 
     def estimate(self):
         self._estimate = r.VAR(self.rdata, p=self.p, type=self.type)
@@ -144,7 +146,7 @@ class RVAR(object):
         return test
 
     def data_summary(self):
-        print (r.summary(self.rdata))
+        print(r.summary(self.rdata))
 
 
 class TestVAR(TestCase):
diff --git a/pandas/stats/var.py b/pandas/stats/var.py
index 8953f7bad..b10d6b9fa 100644
--- a/pandas/stats/var.py
+++ b/pandas/stats/var.py
@@ -1,5 +1,8 @@
 from __future__ import division
 
+from pandas.util.py3compat import range
+from six.moves import zip, reduce
+from pandas.util import compat
 import numpy as np
 from pandas.core.base import StringMixin
 from pandas.util.decorators import cache_readonly
@@ -77,7 +80,7 @@ class VAR(StringMixin):
         DataFrame
         """
         forecast = self._forecast_raw(h)[:, 0, :]
-        return DataFrame(forecast, index=xrange(1, 1 + h),
+        return DataFrame(forecast, index=range(1, 1 + h),
                          columns=self._columns)
 
     def forecast_cov(self, h):
@@ -100,7 +103,7 @@ class VAR(StringMixin):
         DataFrame
         """
         return DataFrame(self._forecast_std_err_raw(h),
-                         index=xrange(1, 1 + h), columns=self._columns)
+                         index=range(1, 1 + h), columns=self._columns)
 
     @cache_readonly
     def granger_causality(self):
@@ -128,7 +131,7 @@ class VAR(StringMixin):
         d = {}
         for col in self._columns:
             d[col] = {}
-            for i in xrange(1, 1 + self._p):
+            for i in range(1, 1 + self._p):
                 lagged_data = self._lagged_data[i].filter(
                     self._columns - [col])
 
@@ -190,7 +193,7 @@ class VAR(StringMixin):
         from pandas.stats.api import ols
 
         d = {}
-        for i in xrange(1, 1 + self._p):
+        for i in range(1, 1 + self._p):
             for col, series in self._lagged_data[i].iteritems():
                 d[_make_param_name(i, col)] = series
 
@@ -278,7 +281,7 @@ BIC:                            %(bic).3f
 
         result.append(trans_B)
 
-        for i in xrange(2, h):
+        for i in range(2, h):
             result.append(np.dot(trans_B, result[i - 1]))
 
         return result
@@ -287,7 +290,7 @@ BIC:                            %(bic).3f
     def _x(self):
         values = np.array([
             self._lagged_data[i][col].values()
-            for i in xrange(1, 1 + self._p)
+            for i in range(1, 1 + self._p)
             for col in self._columns
         ]).T
 
@@ -315,7 +318,7 @@ BIC:                            %(bic).3f
         resid = self._forecast_cov_resid_raw(n)
         # beta = self._forecast_cov_beta_raw(n)
 
-        # return [a + b for a, b in izip(resid, beta)]
+        # return [a + b for a, b in zip(resid, beta)]
         # TODO: ignore the beta forecast std err until it's verified
 
         return resid
@@ -332,7 +335,7 @@ BIC:                            %(bic).3f
 
         results = []
 
-        for h in xrange(1, n + 1):
+        for h in range(1, n + 1):
             psi = self._psi(h)
             trans_B = self._trans_B(h)
 
@@ -340,14 +343,14 @@ BIC:                            %(bic).3f
 
             cov_beta = self._cov_beta
 
-            for t in xrange(T + 1):
+            for t in range(T + 1):
                 index = t + p
-                y = values.take(xrange(index, index - p, -1), axis=0).ravel()
+                y = values.take(range(index, index - p, -1), axis=0).ravel()
                 trans_Z = np.hstack(([1], y))
                 trans_Z = trans_Z.reshape(1, len(trans_Z))
 
                 sum2 = 0
-                for i in xrange(h):
+                for i in range(h):
                     ZB = np.dot(trans_Z, trans_B[h - 1 - i])
 
                     prod = np.kron(ZB, psi[i])
@@ -367,7 +370,7 @@ BIC:                            %(bic).3f
         psi_values = self._psi(h)
         sum = 0
         result = []
-        for i in xrange(h):
+        for i in range(h):
             psi = psi_values[i]
             sum = sum + chain_dot(psi, self._sigma, psi.T)
             result.append(sum)
@@ -380,9 +383,9 @@ BIC:                            %(bic).3f
         """
         k = self._k
         result = []
-        for i in xrange(h):
+        for i in range(h):
             sum = self._alpha.reshape(1, k)
-            for j in xrange(self._p):
+            for j in range(self._p):
                 beta = self._lag_betas[j]
                 idx = i - j
                 if idx > 0:
@@ -429,12 +432,12 @@ BIC:                            %(bic).3f
         """
         k = self._k
         b = self._beta_raw
-        return [b[k * i: k * (i + 1)].T for i in xrange(self._p)]
+        return [b[k * i: k * (i + 1)].T for i in range(self._p)]
 
     @cache_readonly
     def _lagged_data(self):
         return dict([(i, self._data.shift(i))
-                     for i in xrange(1, 1 + self._p)])
+                     for i in range(1, 1 + self._p)])
 
     @cache_readonly
     def _nobs(self):
@@ -448,10 +451,10 @@ BIC:                            %(bic).3f
         """
         k = self._k
         result = [np.eye(k)]
-        for i in xrange(1, h):
+        for i in range(1, h):
             result.append(sum(
                 [np.dot(result[i - j], self._lag_betas[j - 1])
-                 for j in xrange(1, 1 + i)
+                 for j in range(1, 1 + i)
                  if j <= self._p]))
 
         return result
@@ -532,7 +535,7 @@ class PanelVAR(VAR):
         Returns the forecasts at 1, 2, ..., n timesteps in the future.
         """
         forecast = self._forecast_raw(h).T.swapaxes(1, 2)
-        index = xrange(1, 1 + h)
+        index = range(1, 1 + h)
         w = Panel(forecast, items=self._data.items, major_axis=index,
                   minor_axis=self._data.minor_axis)
         return w
diff --git a/pandas/tests/test_algos.py b/pandas/tests/test_algos.py
index 8706bb9cf..1e04403b3 100644
--- a/pandas/tests/test_algos.py
+++ b/pandas/tests/test_algos.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import unittest
 
 import numpy as np
@@ -46,7 +47,7 @@ class TestUnique(unittest.TestCase):
 
     def test_object_refcount_bug(self):
         lst = ['A', 'B', 'C', 'D', 'E']
-        for i in xrange(1000):
+        for i in range(1000):
             len(algos.unique(lst))
 
     def test_on_index_object(self):
diff --git a/pandas/tests/test_categorical.py b/pandas/tests/test_categorical.py
index 48db7afa2..2b70fbcff 100644
--- a/pandas/tests/test_categorical.py
+++ b/pandas/tests/test_categorical.py
@@ -1,6 +1,7 @@
 # pylint: disable=E1101,E1103,W0232
 
 from datetime import datetime
+from pandas.util.py3compat import range
 import unittest
 import nose
 
@@ -103,7 +104,7 @@ class TestCategorical(unittest.TestCase):
     def test_na_flags_int_levels(self):
         # #1457
 
-        levels = range(10)
+        levels = list(range(10))
         labels = np.random.randint(0, 10, 20)
         labels[::5] = -1
 
diff --git a/pandas/tests/test_common.py b/pandas/tests/test_common.py
index 321210556..048b4c6f1 100644
--- a/pandas/tests/test_common.py
+++ b/pandas/tests/test_common.py
@@ -1,4 +1,5 @@
 from datetime import datetime
+from pandas.util.py3compat import range, long
 import sys
 import re
 
@@ -15,6 +16,8 @@ import numpy as np
 
 from pandas.tslib import iNaT
 from pandas.util import py3compat
+import six
+from six.moves import map
 
 _multiprocess_can_split_ = True
 
@@ -24,7 +27,7 @@ def test_is_sequence():
     assert(is_seq((1, 2)))
     assert(is_seq([1, 2]))
     assert(not is_seq("abcd"))
-    assert(not is_seq(u"abcd"))
+    assert(not is_seq(six.u("abcd")))
     assert(not is_seq(np.int64))
 
     class A(object):
@@ -94,7 +97,7 @@ def test_isnull_lists():
     result = isnull(['foo', 'bar'])
     assert(not result.any())
 
-    result = isnull([u'foo', u'bar'])
+    result = isnull([six.u('foo'), six.u('bar')])
     assert(not result.any())
 
 
@@ -120,7 +123,7 @@ def test_datetimeindex_from_empty_datetime64_array():
 def test_nan_to_nat_conversions():
 
     df = DataFrame(dict({
-        'A' : np.asarray(range(10),dtype='float64'),
+        'A' : np.asarray(list(range(10)),dtype='float64'),
         'B' : Timestamp('20010101') }))
     df.iloc[3:6,:] = np.nan
     result = df.loc[4,'B'].value
@@ -176,7 +179,7 @@ def test_iterpairs():
 def test_split_ranges():
     def _bin(x, width):
         "return int(x) as a base2 string of given width"
-        return ''.join(str((x >> i) & 1) for i in xrange(width - 1, -1, -1))
+        return ''.join(str((x >> i) & 1) for i in range(width - 1, -1, -1))
 
     def test_locs(mask):
         nfalse = sum(np.array(mask) == 0)
@@ -193,7 +196,7 @@ def test_split_ranges():
     # exhaustively test all possible mask sequences of length 8
     ncols = 8
     for i in range(2 ** ncols):
-        cols = map(int, list(_bin(i, ncols)))  # count up in base2
+        cols = list(map(int, list(_bin(i, ncols))))  # count up in base2
         mask = [cols[i] == 1 for i in range(len(cols))]
         test_locs(mask)
 
@@ -332,8 +335,8 @@ def test_is_re():
 
 
 def test_is_recompilable():
-    passes = (r'a', u'x', r'asdf', re.compile('adsf'), ur'\u2233\s*',
-              re.compile(r''))
+    passes = (r'a', six.u('x'), r'asdf', re.compile('adsf'),
+              six.u(r'\u2233\s*'), re.compile(r''))
     fails = 1, [], object()
 
     for p in passes:
@@ -720,7 +723,7 @@ class TestTake(unittest.TestCase):
 
     def test_2d_datetime64(self):
         # 2005/01/01 - 2006/01/01
-        arr = np.random.randint(11045376L, 11360736L, (5,3))*100000000000
+        arr = np.random.randint(long(11045376), long(11360736), (5,3))*100000000000
         arr = arr.view(dtype='datetime64[ns]')
         indexer = [0, 2, -1, 1, -1]
 
diff --git a/pandas/tests/test_expressions.py b/pandas/tests/test_expressions.py
index ba0a9926d..018440dd0 100644
--- a/pandas/tests/test_expressions.py
+++ b/pandas/tests/test_expressions.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 # pylint: disable-msg=W0612,E1101
 
 import unittest
diff --git a/pandas/tests/test_format.py b/pandas/tests/test_format.py
index bca38ba55..4b7ca2c70 100644
--- a/pandas/tests/test_format.py
+++ b/pandas/tests/test_format.py
@@ -1,10 +1,13 @@
+from __future__ import print_function
 # -*- coding: utf-8 -*-
 
 try:
-    from StringIO import StringIO
+    from pandas.util.py3compat import StringIO
 except:
     from io import StringIO
 
+from pandas.util.py3compat import range
+from six.moves import zip
 import os
 import sys
 import unittest
@@ -16,7 +19,7 @@ from numpy.random import randn
 import numpy as np
 
 from pandas import DataFrame, Series, Index
-from pandas.util.py3compat import lzip, PY3
+from pandas.util.py3compat import PY3
 
 import pandas.core.format as fmt
 import pandas.util.testing as tm
@@ -25,6 +28,7 @@ import pandas
 import pandas as pd
 from pandas.core.config import (set_option, get_option,
                                 option_context, reset_option)
+import six
 
 _frame = DataFrame(tm.getSeriesData())
 
@@ -86,7 +90,7 @@ class TestDataFrameFormatting(unittest.TestCase):
     def test_repr_tuples(self):
         buf = StringIO()
 
-        df = DataFrame({'tups': zip(range(10), range(10))})
+        df = DataFrame({'tups': list(zip(range(10), range(10)))})
         repr(df)
         df.to_string(col_space=10, buf=buf)
 
@@ -101,7 +105,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
             _strlen = fmt._strlen_func()
 
-            for line, value in zip(r.split('\n'), df['B']):
+            for line, value in list(zip(r.split('\n'), df['B'])):
                 if _strlen(value) + 1 > max_len:
                     self.assert_('...' in line)
                 else:
@@ -132,10 +136,10 @@ class TestDataFrameFormatting(unittest.TestCase):
 
         #unlimited
         reset_option("display.max_seq_items")
-        self.assertTrue(len(com.pprint_thing(range(1000)))> 2000)
+        self.assertTrue(len(com.pprint_thing(list(range(1000))))> 2000)
 
         with option_context("display.max_seq_items",5):
-            self.assertTrue(len(com.pprint_thing(range(1000)))< 100)
+            self.assertTrue(len(com.pprint_thing(list(range(1000))))< 100)
 
     def test_repr_is_valid_construction_code(self):
         import pandas as pd
@@ -154,8 +158,9 @@ class TestDataFrameFormatting(unittest.TestCase):
 
 
         data = [8, 5, 3, 5]
-        index1 = [u"\u03c3", u"\u03c4", u"\u03c5", u"\u03c6"]
-        cols = [u"\u03c8"]
+        index1 = [six.u("\u03c3"), six.u("\u03c4"), six.u("\u03c5"),
+                  six.u("\u03c6")]
+        cols = [six.u("\u03c8")]
         df = DataFrame(data, columns=cols, index=index1)
         self.assertTrue(type(df.__repr__() == str))  # both py2 / 3
 
@@ -166,8 +171,8 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_expand_frame_repr(self):
         df_small = DataFrame('hello', [0], [0])
-        df_wide = DataFrame('hello', [0], range(10))
-        df_tall = DataFrame('hello', range(30), range(5))
+        df_wide = DataFrame('hello', [0], list(range(10)))
+        df_tall = DataFrame('hello', list(range(30)), list(range(5)))
 
         with option_context('mode.sim_interactive', True):
             with option_context('display.max_columns', 10,
@@ -192,7 +197,7 @@ class TestDataFrameFormatting(unittest.TestCase):
     def test_repr_non_interactive(self):
         # in non interactive mode, there can be no dependency on the
         # result of terminal auto size detection
-        df = DataFrame('hello', range(1000), range(5))
+        df = DataFrame('hello', list(range(1000)), list(range(5)))
 
         with option_context('mode.sim_interactive', False,
                             'display.width', 0,
@@ -247,7 +252,7 @@ class TestDataFrameFormatting(unittest.TestCase):
     def test_to_string_repr_unicode(self):
         buf = StringIO()
 
-        unicode_values = [u'\u03c3'] * 10
+        unicode_values = [six.u('\u03c3')] * 10
         unicode_values = np.array(unicode_values, dtype=object)
         df = DataFrame({'unicode': unicode_values})
         df.to_string(col_space=10, buf=buf)
@@ -255,7 +260,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         # it works!
         repr(df)
 
-        idx = Index(['abc', u'\u03c3a', 'aegdvg'])
+        idx = Index(['abc', six.u('\u03c3a'), 'aegdvg'])
         ser = Series(np.random.randn(len(idx)), idx)
         rs = repr(ser).split('\n')
         line_len = len(rs[0])
@@ -276,7 +281,7 @@ class TestDataFrameFormatting(unittest.TestCase):
             sys.stdin = _stdin
 
     def test_to_string_unicode_columns(self):
-        df = DataFrame({u'\u03c3': np.arange(10.)})
+        df = DataFrame({six.u('\u03c3'): np.arange(10.)})
 
         buf = StringIO()
         df.to_string(buf=buf)
@@ -290,14 +295,14 @@ class TestDataFrameFormatting(unittest.TestCase):
         self.assert_(isinstance(result, unicode))
 
     def test_to_string_utf8_columns(self):
-        n = u"\u05d0".encode('utf-8')
+        n = six.u("\u05d0").encode('utf-8')
 
         with option_context('display.max_rows', 1):
             df = pd.DataFrame([1, 2], columns=[n])
             repr(df)
 
     def test_to_string_unicode_two(self):
-        dm = DataFrame({u'c/\u03c3': []})
+        dm = DataFrame({six.u('c/\u03c3'): []})
         buf = StringIO()
         dm.to_string(buf)
 
@@ -316,7 +321,7 @@ class TestDataFrameFormatting(unittest.TestCase):
                       ('float', lambda x: '[% 4.1f]' % x),
                       ('object', lambda x: '-%s-' % str(x))]
         result = df.to_string(formatters=dict(formatters))
-        result2 = df.to_string(formatters=lzip(*formatters)[1])
+        result2 = df.to_string(formatters=list(zip(*formatters))[1])
         self.assertEqual(result, ('  int  float    object\n'
                                   '0 0x1 [ 1.0]  -(1, 2)-\n'
                                   '1 0x2 [ 2.0]    -True-\n'
@@ -324,21 +329,20 @@ class TestDataFrameFormatting(unittest.TestCase):
         self.assertEqual(result, result2)
 
     def test_to_string_with_formatters_unicode(self):
-        df = DataFrame({u'c/\u03c3': [1, 2, 3]})
-        result = df.to_string(formatters={u'c/\u03c3': lambda x: '%s' % x})
-        self.assertEqual(result, (u'  c/\u03c3\n'
-                                  '0   1\n'
-                                  '1   2\n'
-                                  '2   3'))
+        df = DataFrame({six.u('c/\u03c3'): [1, 2, 3]})
+        result = df.to_string(formatters={six.u('c/\u03c3'):
+                              lambda x: '%s' % x})
+        self.assertEqual(result, six.u('  c/\u03c3\n') +
+                                 '0   1\n1   2\n2   3')
 
     def test_to_string_buffer_all_unicode(self):
         buf = StringIO()
 
-        empty = DataFrame({u'c/\u03c3': Series()})
-        nonempty = DataFrame({u'c/\u03c3': Series([1, 2, 3])})
+        empty = DataFrame({six.u('c/\u03c3'): Series()})
+        nonempty = DataFrame({six.u('c/\u03c3'): Series([1, 2, 3])})
 
-        print >>buf, empty
-        print >>buf, nonempty
+        print(empty, file=buf)
+        print(nonempty, file=buf)
 
         # this should work
         buf.getvalue()
@@ -376,9 +380,9 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_to_html_unicode(self):
         # it works!
-        df = DataFrame({u'\u03c3': np.arange(10.)})
+        df = DataFrame({six.u('\u03c3'): np.arange(10.)})
         df.to_html()
-        df = DataFrame({'A': [u'\u03c3']})
+        df = DataFrame({'A': [six.u('\u03c3')]})
         df.to_html()
 
     def test_to_html_escaped(self):
@@ -657,7 +661,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_to_html_index_formatter(self):
         df = DataFrame([[0, 1], [2, 3], [4, 5], [6, 7]],
-                       columns=['foo', None], index=range(4))
+                       columns=['foo', None], index=list(range(4)))
 
         f = lambda x: 'abcd'[x]
         result = df.to_html(formatters={'__index__': f})
@@ -702,8 +706,8 @@ class TestDataFrameFormatting(unittest.TestCase):
         self.assert_(len(lines[1]) == len(lines[2]))
 
     def test_unicode_problem_decoding_as_ascii(self):
-        dm = DataFrame({u'c/\u03c3': Series({'test': np.NaN})})
-        unicode(dm.to_string())
+        dm = DataFrame({six.u('c/\u03c3'): Series({'test': np.NaN})})
+        six.text_type(dm.to_string())
 
     def test_string_repr_encoding(self):
         filepath = tm.get_data_path('unicode_series.csv')
@@ -771,17 +775,24 @@ class TestDataFrameFormatting(unittest.TestCase):
         if PY3:
             raise nose.SkipTest()
 
-        self.assertEquals(pp_t('a') , u'a')
-        self.assertEquals(pp_t(u'a') , u'a')
+        self.assertEquals(pp_t('a') , six.u('a'))
+        self.assertEquals(pp_t(six.u('a')) , six.u('a'))
         self.assertEquals(pp_t(None) , 'None')
-        self.assertEquals(pp_t(u'\u05d0',quote_strings=True) , u"u'\u05d0'")
-        self.assertEquals(pp_t(u'\u05d0',quote_strings=False) , u'\u05d0')
-        self.assertEquals(pp_t((u'\u05d0', u'\u05d1'),quote_strings=True) ,
-                          u"(u'\u05d0', u'\u05d1')")
-        self.assertEquals(pp_t((u'\u05d0', (u'\u05d1', u'\u05d2')),quote_strings=True) ,
-               u"(u'\u05d0', (u'\u05d1', u'\u05d2'))")
-        self.assertEquals(pp_t(('foo', u'\u05d0', (u'\u05d0', u'\u05d0')),quote_strings=True)
-                          , u"(u'foo', u'\u05d0', (u'\u05d0', u'\u05d0'))")
+        self.assertEquals(pp_t(six.u('\u05d0'), quote_strings=True),
+                          six.u("u'\u05d0'"))
+        self.assertEquals(pp_t(six.u('\u05d0'), quote_strings=False),
+                          six.u('\u05d0'))
+        self.assertEquals(pp_t((six.u('\u05d0'),
+                                six.u('\u05d1')), quote_strings=True),
+                          six.u("(u'\u05d0', u'\u05d1')"))
+        self.assertEquals(pp_t((six.u('\u05d0'), (six.u('\u05d1'),
+                                                  six.u('\u05d2'))),
+                               quote_strings=True),
+                          six.u("(u'\u05d0', (u'\u05d1', u'\u05d2'))"))
+        self.assertEquals(pp_t(('foo', six.u('\u05d0'), (six.u('\u05d0'),
+                                                         six.u('\u05d0'))),
+                               quote_strings=True),
+                          six.u("(u'foo', u'\u05d0', (u'\u05d0', u'\u05d0'))"))
 
         # escape embedded tabs in string
         # GH #2038
@@ -789,7 +800,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_wide_repr(self):
         with option_context('mode.sim_interactive', True):
-            col = lambda l, k: [tm.rands(k) for _ in xrange(l)]
+            col = lambda l, k: [tm.rands(k) for _ in range(l)]
             max_cols = get_option('display.max_columns')
             df = DataFrame([col(max_cols-1, 25) for _ in range(10)])
             set_option('display.expand_frame_repr', False)
@@ -813,7 +824,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_wide_repr_named(self):
         with option_context('mode.sim_interactive', True):
-            col = lambda l, k: [tm.rands(k) for _ in xrange(l)]
+            col = lambda l, k: [tm.rands(k) for _ in range(l)]
             max_cols = get_option('display.max_columns')
             df = DataFrame([col(max_cols-1, 25) for _ in range(10)])
             df.index.name = 'DataFrame Index'
@@ -835,7 +846,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_wide_repr_multiindex(self):
         with option_context('mode.sim_interactive', True):
-            col = lambda l, k: [tm.rands(k) for _ in xrange(l)]
+            col = lambda l, k: [tm.rands(k) for _ in range(l)]
             midx = pandas.MultiIndex.from_arrays([np.array(col(10, 5)),
                                                   np.array(col(10, 5))])
             max_cols = get_option('display.max_columns')
@@ -860,7 +871,7 @@ class TestDataFrameFormatting(unittest.TestCase):
     def test_wide_repr_multiindex_cols(self):
         with option_context('mode.sim_interactive', True):
             max_cols = get_option('display.max_columns')
-            col = lambda l, k: [tm.rands(k) for _ in xrange(l)]
+            col = lambda l, k: [tm.rands(k) for _ in range(l)]
             midx = pandas.MultiIndex.from_arrays([np.array(col(10, 5)),
                                                   np.array(col(10, 5))])
             mcols = pandas.MultiIndex.from_arrays([np.array(col(max_cols-1, 3)),
@@ -882,7 +893,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_wide_repr_unicode(self):
         with option_context('mode.sim_interactive', True):
-            col = lambda l, k: [tm.randu(k) for _ in xrange(l)]
+            col = lambda l, k: [tm.randu(k) for _ in range(l)]
             max_cols = get_option('display.max_columns')
             df = DataFrame([col(max_cols-1, 25) for _ in range(10)])
             set_option('display.expand_frame_repr', False)
@@ -908,7 +919,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_long_series(self):
         n = 1000
-        s = Series(np.random.randint(-50,50,n),index=['s%04d' % x for x in xrange(n)], dtype='int64')
+        s = Series(np.random.randint(-50,50,n),index=['s%04d' % x for x in range(n)], dtype='int64')
 
         import re
         str_rep = str(s)
@@ -923,13 +934,13 @@ class TestDataFrameFormatting(unittest.TestCase):
         # multi-index
         y = df.set_index(['id1', 'id2', 'id3'])
         result = y.to_string()
-        expected = u'             value\nid1 id2 id3       \n1a3 NaN 78d    123\n9h4 d67 79d     64'
+        expected = six.u('             value\nid1 id2 id3       \n1a3 NaN 78d    123\n9h4 d67 79d     64')
         self.assert_(result == expected)
 
         # index
         y = df.set_index('id2')
         result = y.to_string()
-        expected = u'     id1  id3  value\nid2                 \nNaN  1a3  78d    123\nd67  9h4  79d     64'
+        expected = six.u('     id1  id3  value\nid2                 \nNaN  1a3  78d    123\nd67  9h4  79d     64')
         self.assert_(result == expected)
 
         # all-nan in mi
@@ -937,7 +948,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         df2.ix[:,'id2'] = np.nan
         y = df2.set_index('id2')
         result = y.to_string()
-        expected = u'     id1  id3  value\nid2                 \nNaN  1a3  78d    123\nNaN  9h4  79d     64'
+        expected = six.u('     id1  id3  value\nid2                 \nNaN  1a3  78d    123\nNaN  9h4  79d     64')
         self.assert_(result == expected)
 
         # partial nan in mi
@@ -945,7 +956,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         df2.ix[:,'id2'] = np.nan
         y = df2.set_index(['id2','id3'])
         result = y.to_string()
-        expected = u'         id1  value\nid2 id3            \nNaN 78d  1a3    123\n    79d  9h4     64'
+        expected = six.u('         id1  value\nid2 id3            \nNaN 78d  1a3    123\n    79d  9h4     64')
         self.assert_(result == expected)
 
         df = DataFrame({'id1': {0: np.nan, 1: '9h4'}, 'id2': {0: np.nan, 1: 'd67'},
@@ -953,7 +964,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
         y = df.set_index(['id1','id2','id3'])
         result = y.to_string()
-        expected = u'             value\nid1 id2 id3       \nNaN NaN NaN    123\n9h4 d67 79d     64'
+        expected = six.u('             value\nid1 id2 id3       \nNaN NaN NaN    123\n9h4 d67 79d     64')
         self.assert_(result == expected)
 
     def test_to_string(self):
@@ -963,7 +974,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         # big mixed
         biggie = DataFrame({'A': randn(200),
                             'B': tm.makeStringIndex(200)},
-                           index=range(200))
+                           index=list(range(200)))
 
         biggie['A'][:20] = nan
         biggie['B'][:20] = nan
@@ -974,7 +985,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         self.assert_(retval is None)
         self.assertEqual(buf.getvalue(), s)
 
-        self.assert_(isinstance(s, basestring))
+        tm.assert_isinstance(s, six.string_types)
 
         # print in right order
         result = biggie.to_string(columns=['B', 'A'], col_space=17,
@@ -1101,7 +1112,7 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_to_string_float_index(self):
         index = Index([1.5, 2, 3, 4, 5])
-        df = DataFrame(range(5), index=index)
+        df = DataFrame(list(range(5)), index=index)
 
         result = df.to_string()
         expected = ('     0\n'
@@ -1114,8 +1125,8 @@ class TestDataFrameFormatting(unittest.TestCase):
 
     def test_to_string_ascii_error(self):
         data = [('0  ',
-                 u'                        .gitignore ',
-                 u'     5 ',
+                 six.u('                        .gitignore '),
+                 six.u('     5 '),
                  ' \xe2\x80\xa2\xe2\x80\xa2\xe2\x80'
                  '\xa2\xe2\x80\xa2\xe2\x80\xa2')]
         df = DataFrame(data)
@@ -1136,7 +1147,7 @@ class TestDataFrameFormatting(unittest.TestCase):
         self.assertEqual(output, expected)
 
     def test_to_string_index_formatter(self):
-        df = DataFrame([range(5), range(5, 10), range(10, 15)])
+        df = DataFrame([list(range(5)), list(range(5, 10)), list(range(10, 15))])
 
         rs = df.to_string(formatters={'__index__': lambda x: 'abc'[x]})
 
@@ -1184,7 +1195,7 @@ c  10  11  12  13  14\
         self.assertEqual(result, expected)
 
     def test_to_string_line_width(self):
-        df = pd.DataFrame(123, range(10, 15), range(30))
+        df = pd.DataFrame(123, list(range(10, 15)), list(range(30)))
         s = df.to_string(line_width=80)
         self.assertEqual(max(len(l) for l in s.split('\n')), 80)
 
@@ -1192,7 +1203,7 @@ c  10  11  12  13  14\
         # big mixed
         biggie = DataFrame({'A': randn(200),
                             'B': tm.makeStringIndex(200)},
-                           index=range(200))
+                           index=list(range(200)))
 
         biggie['A'][:20] = nan
         biggie['B'][:20] = nan
@@ -1203,7 +1214,7 @@ c  10  11  12  13  14\
         self.assert_(retval is None)
         self.assertEqual(buf.getvalue(), s)
 
-        self.assert_(isinstance(s, basestring))
+        tm.assert_isinstance(s, six.string_types)
 
         biggie.to_html(columns=['B', 'A'], col_space=17)
         biggie.to_html(columns=['B', 'A'],
@@ -1219,7 +1230,7 @@ c  10  11  12  13  14\
     def test_to_html_filename(self):
         biggie = DataFrame({'A': randn(200),
                             'B': tm.makeStringIndex(200)},
-                           index=range(200))
+                           index=list(range(200)))
 
         biggie['A'][:20] = nan
         biggie['B'][:20] = nan
@@ -1246,8 +1257,8 @@ c  10  11  12  13  14\
         self.assert_('<th>B</th>' not in result)
 
     def test_to_html_multiindex(self):
-        columns = pandas.MultiIndex.from_tuples(zip(np.arange(2).repeat(2),
-                                                    np.mod(range(4), 2)),
+        columns = pandas.MultiIndex.from_tuples(list(zip(np.arange(2).repeat(2),
+                                                    np.mod(list(range(4)), 2))),
                                                 names=['CL0', 'CL1'])
         df = pandas.DataFrame([list('abcd'), list('efgh')], columns=columns)
         result = df.to_html(justify='left')
@@ -1286,8 +1297,8 @@ c  10  11  12  13  14\
 
         self.assertEqual(result, expected)
 
-        columns = pandas.MultiIndex.from_tuples(zip(range(4),
-                                                    np.mod(range(4), 2)))
+        columns = pandas.MultiIndex.from_tuples(list(zip(range(4),
+                                                    np.mod(list(range(4)), 2))))
         df = pandas.DataFrame([list('abcd'), list('efgh')], columns=columns)
 
         result = df.to_html(justify='right')
@@ -1538,10 +1549,10 @@ class TestSeriesFormatting(unittest.TestCase):
         self.ts = tm.makeTimeSeries()
 
     def test_repr_unicode(self):
-        s = Series([u'\u03c3'] * 10)
+        s = Series([six.u('\u03c3')] * 10)
         repr(s)
 
-        a = Series([u"\u05d0"] * 1000)
+        a = Series([six.u("\u05d0")] * 1000)
         a.name = 'title1'
         repr(a)
 
@@ -1585,26 +1596,26 @@ class TestSeriesFormatting(unittest.TestCase):
     def test_to_string_mixed(self):
         s = Series(['foo', np.nan, -1.23, 4.56])
         result = s.to_string()
-        expected = (u'0     foo\n'
-                    u'1     NaN\n'
-                    u'2   -1.23\n'
-                    u'3    4.56')
+        expected = (six.u('0     foo\n') +
+                    six.u('1     NaN\n') +
+                    six.u('2   -1.23\n') +
+                    six.u('3    4.56'))
         self.assertEqual(result, expected)
 
         # but don't count NAs as floats
         s = Series(['foo', np.nan, 'bar', 'baz'])
         result = s.to_string()
-        expected = (u'0    foo\n'
-                    '1    NaN\n'
-                    '2    bar\n'
+        expected = (six.u('0    foo\n') +
+                    '1    NaN\n' +
+                    '2    bar\n' +
                     '3    baz')
         self.assertEqual(result, expected)
 
         s = Series(['foo', 5, 'bar', 'baz'])
         result = s.to_string()
-        expected = (u'0    foo\n'
-                    '1      5\n'
-                    '2    bar\n'
+        expected = (six.u('0    foo\n') +
+                    '1      5\n' +
+                    '2    bar\n' +
                     '3    baz')
         self.assertEqual(result, expected)
 
@@ -1613,16 +1624,16 @@ class TestSeriesFormatting(unittest.TestCase):
         s[::2] = np.nan
 
         result = s.to_string()
-        expected = (u'0       NaN\n'
-                    '1    1.5678\n'
-                    '2       NaN\n'
-                    '3   -3.0000\n'
+        expected = (six.u('0       NaN\n') +
+                    '1    1.5678\n' +
+                    '2       NaN\n' +
+                    '3   -3.0000\n' +
                     '4       NaN')
         self.assertEqual(result, expected)
 
     def test_unicode_name_in_footer(self):
-        s = Series([1, 2], name=u'\u05e2\u05d1\u05e8\u05d9\u05ea')
-        sf = fmt.SeriesFormatter(s, name=u'\u05e2\u05d1\u05e8\u05d9\u05ea')
+        s = Series([1, 2], name=six.u('\u05e2\u05d1\u05e8\u05d9\u05ea'))
+        sf = fmt.SeriesFormatter(s, name=six.u('\u05e2\u05d1\u05e8\u05d9\u05ea'))
         sf._get_footer()  # should not raise exception
 
     def test_float_trim_zeros(self):
@@ -1916,7 +1927,7 @@ class TestEngFormatter(unittest.TestCase):
 
         formatter = fmt.EngFormatter(accuracy=3, use_eng_prefix=True)
         result = formatter(0)
-        self.assertEqual(result, u' 0.000')
+        self.assertEqual(result, six.u(' 0.000'))
 
 
 def _three_digit_exp():
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index 577cbfe9d..a3a799279 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -1,7 +1,10 @@
+from __future__ import print_function
 # pylint: disable-msg=W0612,E1101
 from copy import deepcopy
 from datetime import datetime, timedelta, time
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range, long
+from pandas.util import compat
 import cPickle as pickle
 import operator
 import re
@@ -39,6 +42,9 @@ import pandas.util.testing as tm
 import pandas.lib as lib
 
 from numpy.testing.decorators import slow
+import six
+from six.moves import map
+from six.moves import zip
 
 def _skip_if_no_scipy():
     try:
@@ -58,7 +64,7 @@ def _check_mixed_float(df, dtype = None):
 
     # float16 are most likely to be upcasted to float32
     dtypes = dict(A = 'float32', B = 'float32', C = 'float16', D = 'float64')
-    if isinstance(dtype, basestring):
+    if isinstance(dtype, six.string_types):
         dtypes = dict([ (k,dtype) for k, v in dtypes.items() ])
     elif isinstance(dtype, dict):
         dtypes.update(dtype)
@@ -73,7 +79,7 @@ def _check_mixed_float(df, dtype = None):
 
 def _check_mixed_int(df, dtype = None):
     dtypes = dict(A = 'int32', B = 'uint64', C = 'uint8', D = 'int64')
-    if isinstance(dtype, basestring):
+    if isinstance(dtype, six.string_types):
         dtypes = dict([ (k,dtype) for k, v in dtypes.items() ])
     elif isinstance(dtype, dict):
         dtypes.update(dtype)
@@ -172,7 +178,7 @@ class CheckIndexing(object):
         assert_series_equal(self.frame['B'], data['A'])
         assert_series_equal(self.frame['A'], data['B'])
 
-        df = DataFrame(0, range(3), ['tt1', 'tt2'], dtype=np.int_)
+        df = DataFrame(0, list(range(3)), ['tt1', 'tt2'], dtype=np.int_)
         df.ix[1, ['tt1', 'tt2']] = [1, 2]
 
         result = df.ix[1, ['tt1', 'tt2']]
@@ -191,7 +197,7 @@ class CheckIndexing(object):
         assert_almost_equal(self.frame[['A', 'B']].values, data)
 
     def test_setitem_list_of_tuples(self):
-        tuples = zip(self.frame['A'], self.frame['B'])
+        tuples = list(zip(self.frame['A'], self.frame['B']))
         self.frame['tuples'] = tuples
 
         result = self.frame['tuples']
@@ -357,7 +363,7 @@ class CheckIndexing(object):
                           'NONEXISTENT_NAME')
 
     def test_setattr_column(self):
-        df = DataFrame({'foobar': 1}, index=range(10))
+        df = DataFrame({'foobar': 1}, index=list(range(10)))
 
         df.foobar = 5
         self.assert_((df.foobar == 5).all())
@@ -561,11 +567,11 @@ class CheckIndexing(object):
         from decimal import Decimal
 
         # created as float type
-        dm = DataFrame(index=range(3), columns=range(3))
+        dm = DataFrame(index=list(range(3)), columns=list(range(3)))
 
         coercable_series = Series([Decimal(1) for _ in range(3)],
-                                  index=range(3))
-        uncoercable_series = Series(['foo', 'bzr', 'baz'], index=range(3))
+                                  index=list(range(3)))
+        uncoercable_series = Series(['foo', 'bzr', 'baz'], index=list(range(3)))
 
         dm[0] = np.ones(3)
         self.assertEqual(len(dm.columns), 3)
@@ -663,7 +669,7 @@ class CheckIndexing(object):
         self.assert_(isnull(df.ix[:8:2]).values.all())
 
     def test_getitem_setitem_integer_slice_keyerrors(self):
-        df = DataFrame(np.random.randn(10, 5), index=range(0, 20, 2))
+        df = DataFrame(np.random.randn(10, 5), index=list(range(0, 20, 2)))
 
         # this is OK
         cp = df.copy()
@@ -776,11 +782,12 @@ class CheckIndexing(object):
         assert_frame_equal(frame, expected)
 
         # new corner case of boolean slicing / setting
-        frame = DataFrame(zip([2, 3, 9, 6, 7], [np.nan] * 5),
+        frame = DataFrame(list(zip([2, 3, 9, 6, 7], [np.nan] * 5)),
                           columns=['a', 'b'])
         lst = [100]
         lst.extend([np.nan] * 4)
-        expected = DataFrame(zip([100, 3, 9, 6, 7], lst), columns=['a', 'b'])
+        expected = DataFrame(list(zip([100, 3, 9, 6, 7], lst)),
+                             columns=['a', 'b'])
         frame[frame['a'] == 2] = 100
         assert_frame_equal(frame, expected)
 
@@ -1486,7 +1493,7 @@ class CheckIndexing(object):
         self.assertRaises(ValueError, res3.set_value, 'foobar', 'baz', 'sam')
 
     def test_set_value_with_index_dtype_change(self):
-        df = DataFrame(randn(3, 3), index=range(3), columns=list('ABC'))
+        df = DataFrame(randn(3, 3), index=list(range(3)), columns=list('ABC'))
         res = df.set_value('C', 2, 1.0)
         self.assert_(list(res.index) == list(df.index) + ['C'])
         self.assert_(list(res.columns) == list(df.columns) + [2])
@@ -1494,7 +1501,7 @@ class CheckIndexing(object):
     def test_get_set_value_no_partial_indexing(self):
         # partial w/ MultiIndex raise exception
         index = MultiIndex.from_tuples([(0, 1), (0, 2), (1, 1), (1, 2)])
-        df = DataFrame(index=index, columns=range(4))
+        df = DataFrame(index=index, columns=list(range(4)))
         self.assertRaises(KeyError, df.get_value, 0, 1)
         # self.assertRaises(KeyError, df.set_value, 0, 1, 0)
 
@@ -1507,7 +1514,7 @@ class CheckIndexing(object):
         self.assert_(com.is_integer(result))
 
     def test_irow(self):
-        df = DataFrame(np.random.randn(10, 4), index=range(0, 20, 2))
+        df = DataFrame(np.random.randn(10, 4), index=list(range(0, 20, 2)))
 
         result = df.irow(1)
         exp = df.ix[2]
@@ -1534,7 +1541,7 @@ class CheckIndexing(object):
         assert_frame_equal(result, expected)
 
     def test_icol(self):
-        df = DataFrame(np.random.randn(4, 10), columns=range(0, 20, 2))
+        df = DataFrame(np.random.randn(4, 10), columns=list(range(0, 20, 2)))
 
         result = df.icol(1)
         exp = df.ix[:, 2]
@@ -1621,7 +1628,7 @@ class CheckIndexing(object):
 
         try:
             repr(df)
-        except Exception, e:
+        except Exception as e:
             self.assertNotEqual(type(e), UnboundLocalError)
 
 _seriesd = tm.getSeriesData()
@@ -2066,7 +2073,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         result = DataFrame([DataFrame([])])
         self.assert_(result.shape == (1,0))
 
-        result = DataFrame([DataFrame(dict(A = range(5)))])
+        result = DataFrame([DataFrame(dict(A = list(range(5))))])
         self.assert_(type(result.iloc[0,0]) == DataFrame)
 
     def test_constructor_mixed_dtypes(self):
@@ -2080,7 +2087,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                 dtypes = MIXED_FLOAT_DTYPES
                 arrays = [ np.array(np.random.randint(10, size=10), dtype = d) for d in dtypes ]
 
-            zipper = zip(dtypes,arrays)
+            zipper = list(zip(dtypes,arrays))
             for d,a in zipper:
                 assert(a.dtype == d)
             if ad is None:
@@ -2141,8 +2148,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         # #2355
         data_scores = [(6311132704823138710, 273), (2685045978526272070, 23),
-                       (8921811264899370420, 45), (17019687244989530680L, 270),
-                       (9930107427299601010L, 273)]
+                       (8921811264899370420, 45), (long(17019687244989530680), 270),
+                       (long(9930107427299601010), 273)]
         dtype = [('uid', 'u8'), ('score', 'u8')]
         data = np.zeros((len(data_scores),), dtype=dtype)
         data[:] = data_scores
@@ -2156,7 +2163,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_constructor_ordereddict(self):
         import random
         nitems = 100
-        nums = range(nitems)
+        nums = list(range(nitems))
         random.shuffle(nums)
         expected = ['A%d' % i for i in nums]
         df = DataFrame(OrderedDict(zip(expected, [[0]] * nitems)))
@@ -2251,8 +2258,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_constructor_subclass_dict(self):
         # Test for passing dict subclass to constructor
-        data = {'col1': tm.TestSubDict((x, 10.0 * x) for x in xrange(10)),
-                'col2': tm.TestSubDict((x, 20.0 * x) for x in xrange(10))}
+        data = {'col1': tm.TestSubDict((x, 10.0 * x) for x in range(10)),
+                'col2': tm.TestSubDict((x, 20.0 * x) for x in range(10))}
         df = DataFrame(data)
         refdf = DataFrame(dict((col, dict(val.iteritems()))
                                for col, val in data.iteritems()))
@@ -2266,7 +2273,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         from collections import defaultdict
         data = {}
         self.frame['B'][:10] = np.nan
-        for k, v in self.frame.iterkv():
+        for k, v in self.frame.iteritems():
             dct = defaultdict(dict)
             dct.update(v.to_dict())
             data[k] = dct
@@ -2356,14 +2363,14 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         # automatic labeling
         frame = DataFrame(mat)
-        self.assert_(np.array_equal(frame.index, range(2)))
-        self.assert_(np.array_equal(frame.columns, range(3)))
+        self.assert_(np.array_equal(frame.index, list(range(2))))
+        self.assert_(np.array_equal(frame.columns, list(range(3))))
 
         frame = DataFrame(mat, index=[1, 2])
-        self.assert_(np.array_equal(frame.columns, range(3)))
+        self.assert_(np.array_equal(frame.columns, list(range(3))))
 
         frame = DataFrame(mat, columns=['A', 'B', 'C'])
-        self.assert_(np.array_equal(frame.index, range(2)))
+        self.assert_(np.array_equal(frame.index, list(range(2))))
 
         # 0-length axis
         frame = DataFrame(np.empty((0, 3)))
@@ -2414,14 +2421,14 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         # automatic labeling
         frame = DataFrame(mat)
-        self.assert_(np.array_equal(frame.index, range(2)))
-        self.assert_(np.array_equal(frame.columns, range(3)))
+        self.assert_(np.array_equal(frame.index, list(range(2))))
+        self.assert_(np.array_equal(frame.columns, list(range(3))))
 
         frame = DataFrame(mat, index=[1, 2])
-        self.assert_(np.array_equal(frame.columns, range(3)))
+        self.assert_(np.array_equal(frame.columns, list(range(3))))
 
         frame = DataFrame(mat, columns=['A', 'B', 'C'])
-        self.assert_(np.array_equal(frame.index, range(2)))
+        self.assert_(np.array_equal(frame.index, list(range(2))))
 
         # 0-length axis
         frame = DataFrame(ma.masked_all((0, 3)))
@@ -2502,11 +2509,11 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assertEqual(df.values.shape, (0, 0))
 
         # empty but with specified dtype
-        df = DataFrame(index=range(10), columns=['a', 'b'], dtype=object)
+        df = DataFrame(index=list(range(10)), columns=['a', 'b'], dtype=object)
         self.assert_(df.values.dtype == np.object_)
 
         # does not error but ends up float
-        df = DataFrame(index=range(10), columns=['a', 'b'], dtype=int)
+        df = DataFrame(index=list(range(10)), columns=['a', 'b'], dtype=int)
         self.assert_(df.values.dtype == np.object_)
 
         # #1783 empty dtype object
@@ -2680,7 +2687,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assertRaises(Exception, DataFrame, data)
 
     def test_constructor_scalar(self):
-        idx = Index(range(3))
+        idx = Index(list(range(3)))
         df = DataFrame({"a": 0}, index=idx)
         expected = DataFrame({"a": [0, 0, 0]}, index=idx)
         assert_frame_equal(df, expected, check_dtype=False)
@@ -2849,7 +2856,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         # assignment
         # GH 3687
         arr = np.random.randn(3, 2)
-        idx = range(2)
+        idx = list(range(2))
         df = DataFrame(arr, columns=['A', 'A'])
         df.columns = idx
         expected = DataFrame(arr,columns=idx)
@@ -2950,11 +2957,11 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         # from the vb_suite/frame_methods/frame_insert_columns
         N = 10
         K = 5
-        df = DataFrame(index=range(N))
+        df = DataFrame(index=list(range(N)))
         new_col = np.random.randn(N)
         for i in range(K):
             df[i] = new_col
-        expected = DataFrame(np.repeat(new_col,K).reshape(N,K),index=range(N))
+        expected = DataFrame(np.repeat(new_col,K).reshape(N,K),index=list(range(N)))
         assert_frame_equal(df,expected)
 
     def test_constructor_single_value(self):
@@ -3090,12 +3097,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         expected = Series({'float64' : 1})
         assert_series_equal(result, expected)
 
-        df = DataFrame({'a' : 1 }, index=range(3))
+        df = DataFrame({'a' : 1 }, index=list(range(3)))
         result = df.get_dtype_counts()
         expected = Series({'int64': 1})
         assert_series_equal(result, expected)
 
-        df = DataFrame({'a' : 1. }, index=range(3))
+        df = DataFrame({'a' : 1. }, index=list(range(3)))
         result = df.get_dtype_counts()
         expected = Series({'float64': 1 })
         assert_series_equal(result, expected)
@@ -3200,7 +3207,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test__slice_consolidate_invalidate_item_cache(self):
         # #3970
-        df = DataFrame({ "aa":range(5), "bb":[2.2]*5})
+        df = DataFrame({ "aa":list(range(5)), "bb":[2.2]*5})
 
         # Creates a second float block
         df["cc"] = 0.0
@@ -3573,7 +3580,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         str_dates = ['20120209', '20120222']
         dt_dates = [datetime(2012, 2, 9), datetime(2012, 2, 22)]
 
-        A = DataFrame(str_dates, index=range(2), columns=['aa'])
+        A = DataFrame(str_dates, index=list(range(2)), columns=['aa'])
         C = DataFrame([[1, 2], [3, 4]], index=str_dates, columns=dt_dates)
 
         tst = A.join(C, on='aa')
@@ -3598,7 +3605,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         for dtype, b in blocks.iteritems():
             columns.extend(b.columns)
             dtypes.extend([ (c,np.dtype(dtype).descr[0][1]) for c in b.columns ])
-        for i in xrange(len(df.index)):
+        for i in range(len(df.index)):
             tup = []
             for _, b in blocks.iteritems():
                 tup.extend(b.irow(i).values)
@@ -3625,12 +3632,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         # tuples is in the order of the columns
         result = DataFrame.from_records(tuples)
-        self.assert_(np.array_equal(result.columns, range(8)))
+        self.assert_(np.array_equal(result.columns, list(range(8))))
 
         # test exclude parameter & we are casting the results here (as we don't have dtype info to recover)
         columns_to_test = [ columns.index('C'), columns.index('E1') ]
 
-        exclude = list(set(xrange(8))-set(columns_to_test))
+        exclude = list(set(range(8))-set(columns_to_test))
         result = DataFrame.from_records(tuples, exclude=exclude)
         result.columns = [ columns[i] for i in sorted(columns_to_test) ]
         assert_series_equal(result['C'], df['C'])
@@ -3708,7 +3715,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                 return iter(self.args)
 
         recs = [Record(1, 2, 3), Record(4, 5, 6), Record(7, 8, 9)]
-        tups = map(tuple, recs)
+        tups = list(map(tuple, recs))
 
         result = DataFrame.from_records(recs)
         expected = DataFrame.from_records(tups)
@@ -3767,7 +3774,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         # big mixed
         biggie = DataFrame({'A': randn(200),
                             'B': tm.makeStringIndex(200)},
-                           index=range(200))
+                           index=list(range(200)))
         biggie['A'][:20] = nan
         biggie['B'][:20] = nan
 
@@ -3803,8 +3810,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         buf = StringIO()
 
         # big one
-        biggie = DataFrame(np.zeros((200, 4)), columns=range(4),
-                           index=range(200))
+        biggie = DataFrame(np.zeros((200, 4)), columns=list(range(4)),
+                           index=list(range(200)))
         foo = repr(biggie)
 
     def test_repr_unsortable(self):
@@ -3837,7 +3844,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         warnings.filters = warn_filters
 
     def test_repr_unicode(self):
-        uval = u'\u03c3\u03c3\u03c3\u03c3'
+        uval = six.u('\u03c3\u03c3\u03c3\u03c3')
         bval = uval.encode('utf-8')
         df = DataFrame({'A': [uval, uval]})
 
@@ -3850,15 +3857,15 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assertEqual(result.split('\n')[0].rstrip(), ex_top)
 
     def test_unicode_string_with_unicode(self):
-        df = DataFrame({'A': [u"\u05d0"]})
+        df = DataFrame({'A': [six.u("\u05d0")]})
 
         if py3compat.PY3:
             str(df)
         else:
-            unicode(df)
+            six.text_type(df)
 
     def test_bytestring_with_unicode(self):
-        df = DataFrame({'A': [u"\u05d0"]})
+        df = DataFrame({'A': [six.u("\u05d0")]})
         if py3compat.PY3:
             bytes(df)
         else:
@@ -3866,7 +3873,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_very_wide_info_repr(self):
         df = DataFrame(np.random.randn(10, 20),
-                       columns=[tm.rands(10) for _ in xrange(20)])
+                       columns=[tm.rands(10) for _ in range(20)])
         repr(df)
 
     def test_repr_column_name_unicode_truncation_bug(self):
@@ -3971,7 +3978,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
             assert_series_equal(s, expected)
 
         df = DataFrame({'floats': np.random.randn(5),
-                        'ints': range(5)}, columns=['floats', 'ints'])
+                        'ints': list(range(5))}, columns=['floats', 'ints'])
 
         for tup in df.itertuples(index=False):
             self.assert_(isinstance(tup[1], np.integer))
@@ -4636,7 +4643,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(df[-mask_b], df.ix[1:1, :])
 
     def test_float_none_comparison(self):
-        df = DataFrame(np.random.randn(8, 3), index=range(8),
+        df = DataFrame(np.random.randn(8, 3), index=list(range(8)),
                        columns=['A', 'B', 'C'])
 
         self.assertRaises(TypeError, df.__eq__, None)
@@ -4679,8 +4686,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
              assert_almost_equal(self.tsframe.values, recons.values)
 
              # corner case
-             dm = DataFrame({'s1': Series(range(3), range(3)),
-                             's2': Series(range(2), range(2))})
+             dm = DataFrame({'s1': Series(list(range(3)), list(range(3))),
+                             's2': Series(list(range(2)), list(range(2)))})
              dm.to_csv(path)
              recons = DataFrame.from_csv(path)
              assert_frame_equal(dm, recons)
@@ -4723,8 +4730,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
             df2.to_csv(path,mode='a',header=False)
             xp = pd.concat([df1,df2])
             rs = pd.read_csv(path,index_col=0)
-            rs.columns = map(int,rs.columns)
-            xp.columns = map(int,xp.columns)
+            rs.columns = list(map(int,rs.columns))
+            xp.columns = list(map(int,xp.columns))
             assert_frame_equal(xp,rs)
 
     def test_to_csv_cols_reordering(self):
@@ -4807,10 +4814,10 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                      dupe_col=False):
 
                if cnlvl:
-                   header = range(cnlvl)
+                   header = list(range(cnlvl))
                    with ensure_clean(path) as path:
                         df.to_csv(path,encoding='utf8',chunksize=chunksize,tupleize_cols=False)
-                        recons = DataFrame.from_csv(path,header=range(cnlvl),tupleize_cols=False,parse_dates=False)
+                        recons = DataFrame.from_csv(path,header=list(range(cnlvl)),tupleize_cols=False,parse_dates=False)
                else:
                    with ensure_clean(path) as path:
                        df.to_csv(path,encoding='utf8',chunksize=chunksize)
@@ -4834,19 +4841,22 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                if r_dtype:
                     if r_dtype == 'u': # unicode
                         r_dtype='O'
-                        recons.index = np.array(map(_to_uni,recons.index),
-                                                dtype=r_dtype )
-                        df.index = np.array(map(_to_uni,df.index),dtype=r_dtype )
+                        recons.index = np.array(list(map(_to_uni,recons.index)),
+                                                dtype=r_dtype)
+                        df.index = np.array(list(map(_to_uni,df.index)),dtype=r_dtype)
                     if r_dtype == 'dt': # unicode
                         r_dtype='O'
-                        recons.index = np.array(map(Timestamp,recons.index),
-                                                dtype=r_dtype )
-                        df.index = np.array(map(Timestamp,df.index),dtype=r_dtype )
+                        recons.index = np.array(list(map(Timestamp,recons.index)),
+                                                dtype=r_dtype)
+                        df.index = np.array(list(map(Timestamp,df.index)),dtype=r_dtype)
                     elif r_dtype == 'p':
                         r_dtype='O'
-                        recons.index = np.array(map(Timestamp,recons.index.to_datetime()),
-                                                dtype=r_dtype )
-                        df.index = np.array(map(Timestamp,df.index.to_datetime()),dtype=r_dtype )
+                        recons.index = np.array(list(map(Timestamp,
+                                                         recons.index.to_datetime())),
+                                                dtype=r_dtype)
+                        df.index = np.array(list(map(Timestamp,
+                                                     df.index.to_datetime())),
+                                            dtype=r_dtype)
                     else:
                         r_dtype= type_map.get(r_dtype)
                         recons.index = np.array(recons.index,dtype=r_dtype )
@@ -4854,19 +4864,19 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                if c_dtype:
                     if c_dtype == 'u':
                         c_dtype='O'
-                        recons.columns = np.array(map(_to_uni,recons.columns),
-                                                dtype=c_dtype )
-                        df.columns = np.array(map(_to_uni,df.columns),dtype=c_dtype )
+                        recons.columns = np.array(list(map(_to_uni,recons.columns)),
+                                                dtype=c_dtype)
+                        df.columns = np.array(list(map(_to_uni,df.columns)),dtype=c_dtype )
                     elif c_dtype == 'dt':
                         c_dtype='O'
-                        recons.columns = np.array(map(Timestamp,recons.columns),
+                        recons.columns = np.array(list(map(Timestamp,recons.columns)),
                                                 dtype=c_dtype )
-                        df.columns = np.array(map(Timestamp,df.columns),dtype=c_dtype )
+                        df.columns = np.array(list(map(Timestamp,df.columns)),dtype=c_dtype)
                     elif c_dtype == 'p':
                         c_dtype='O'
-                        recons.columns = np.array(map(Timestamp,recons.columns.to_datetime()),
-                                                dtype=c_dtype )
-                        df.columns = np.array(map(Timestamp,df.columns.to_datetime()),dtype=c_dtype )
+                        recons.columns = np.array(list(map(Timestamp,recons.columns.to_datetime())),
+                                                dtype=c_dtype)
+                        df.columns = np.array(list(map(Timestamp,df.columns.to_datetime())),dtype=c_dtype )
                     else:
                         c_dtype= type_map.get(c_dtype)
                         recons.columns = np.array(recons.columns,dtype=c_dtype )
@@ -4947,7 +4957,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
             _do_test(df,path,dupe_col=True)
 
 
-        _do_test(DataFrame(index=range(10)),path)
+        _do_test(DataFrame(index=list(range(10))),path)
         _do_test(mkdf(chunksize//2+1, 2,r_idx_nlevels=2),path,rnlvl=2)
         for ncols in [2,3,4]:
             base = int(chunksize//ncols)
@@ -5123,15 +5133,15 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
             # catch invalid headers
             def testit():
-                read_csv(path,tupleize_cols=False,header=range(3),index_col=0)
+                read_csv(path,tupleize_cols=False,header=list(range(3)),index_col=0)
             assertRaisesRegexp(CParserError, 'Passed header=\[0,1,2\] are too many rows for this multi_index of columns', testit)
 
             def testit():
-                read_csv(path,tupleize_cols=False,header=range(7),index_col=0)
+                read_csv(path,tupleize_cols=False,header=list(range(7)),index_col=0)
             assertRaisesRegexp(CParserError, 'Passed header=\[0,1,2,3,4,5,6\], len of 7, but only 6 lines in file', testit)
 
             for i in [3,4,5,6,7]:
-                 self.assertRaises(Exception, read_csv, path, tupleize_cols=False, header=range(i), index_col=0)
+                 self.assertRaises(Exception, read_csv, path, tupleize_cols=False, header=list(range(i)), index_col=0)
             self.assertRaises(Exception, read_csv, path, tupleize_cols=False, header=[0,2], index_col=0)
 
             # write with cols
@@ -5171,7 +5181,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_to_csv_mixed(self):
 
         def create_cols(name):
-            return [ "%s%03d" % (name,i) for i in xrange(5) ]
+            return [ "%s%03d" % (name,i) for i in range(5) ]
 
         df_float  = DataFrame(np.random.randn(100, 5),dtype='float64',columns=create_cols('float'))
         df_int    = DataFrame(np.random.randn(100, 5),dtype='int64',columns=create_cols('int'))
@@ -5200,7 +5210,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_to_csv_dups_cols(self):
 
-        df        = DataFrame(np.random.randn(1000, 30),columns=range(15)+range(15),dtype='float64')
+        df        = DataFrame(np.random.randn(1000, 30),columns=list(range(15))+list(range(15)),dtype='float64')
 
         with ensure_clean() as filename:
             df.to_csv(filename) # single dtype, fine
@@ -5210,9 +5220,9 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         df_float  = DataFrame(np.random.randn(1000, 3),dtype='float64')
         df_int    = DataFrame(np.random.randn(1000, 3),dtype='int64')
-        df_bool   = DataFrame(True,index=df_float.index,columns=range(3))
-        df_object = DataFrame('foo',index=df_float.index,columns=range(3))
-        df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=range(3))
+        df_bool   = DataFrame(True,index=df_float.index,columns=list(range(3)))
+        df_object = DataFrame('foo',index=df_float.index,columns=list(range(3)))
+        df_dt     = DataFrame(Timestamp('20010101'),index=df_float.index,columns=list(range(3)))
         df        = pan.concat([ df_float, df_int, df_bool, df_object, df_dt ], axis=1, ignore_index=True)
 
         cols = []
@@ -5249,7 +5259,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_to_csv_chunking(self):
 
-        aa=DataFrame({'A':range(100000)})
+        aa=DataFrame({'A':list(range(100000))})
         aa['B'] = aa.A + 1.0
         aa['C'] = aa.A + 2.0
         aa['D'] = aa.A + 3.0
@@ -5273,7 +5283,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_to_csv_unicode(self):
 
-        df = DataFrame({u'c/\u03c3': [1, 2, 3]})
+        df = DataFrame({six.u('c/\u03c3'): [1, 2, 3]})
         with ensure_clean() as path:
 
             df.to_csv(path, encoding='UTF-8')
@@ -5287,10 +5297,10 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_to_csv_unicode_index_col(self):
         buf = StringIO('')
         df = DataFrame(
-            [[u"\u05d0", "d2", "d3", "d4"], ["a1", "a2", "a3", "a4"]],
-            columns=[u"\u05d0",
-                     u"\u05d1", u"\u05d2", u"\u05d3"],
-            index=[u"\u05d0", u"\u05d1"])
+            [[six.u("\u05d0"), "d2", "d3", "d4"], ["a1", "a2", "a3", "a4"]],
+            columns=[six.u("\u05d0"),
+                     six.u("\u05d1"), six.u("\u05d2"), six.u("\u05d3")],
+            index=[six.u("\u05d0"), six.u("\u05d1")])
 
         df.to_csv(buf, encoding='UTF-8')
         buf.seek(0)
@@ -5586,7 +5596,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_asfreq_datetimeindex(self):
         df = DataFrame({'A': [1, 2, 3]},
-                       index=[datetime(2011, 11, 01), datetime(2011, 11, 2),
+                       index=[datetime(2011, 11, 1), datetime(2011, 11, 2),
                               datetime(2011, 11, 3)])
         df = df.asfreq('B')
         self.assert_(isinstance(df.index, DatetimeIndex))
@@ -5929,7 +5939,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(dropped, expected)
 
         dropped = df.dropna(axis=0)
-        expected = df.ix[range(2, 6)]
+        expected = df.ix[list(range(2, 6))]
         assert_frame_equal(dropped, expected)
 
         # threshold
@@ -5938,7 +5948,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(dropped, expected)
 
         dropped = df.dropna(axis=0, thresh=4)
-        expected = df.ix[range(2, 6)]
+        expected = df.ix[list(range(2, 6))]
         assert_frame_equal(dropped, expected)
 
         dropped = df.dropna(axis=1, thresh=4)
@@ -5984,7 +5994,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                         'B': ['one', 'one', 'two', 'two',
                               'two', 'two', 'one', 'two'],
                         'C': [1, 1, 2, 2, 2, 2, 1, 2],
-                        'D': range(8)})
+                        'D': list(range(8))})
 
         # single column
         result = df.drop_duplicates('AAA')
@@ -6024,7 +6034,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                         'B': ['one', 'one', 'two', 'two',
                               'two', 'two', 'one', 'two'],
                         'C': [1, 1, 2, 2, 2, 2, 1, 2],
-                        'D': range(8)})
+                        'D': list(range(8))})
 
         # single column
         result = df.drop_duplicates(('AA', 'AB'))
@@ -6047,7 +6057,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                         'B': ['one', 'one', 'two', 'two',
                               'two', 'two', 'one', 'two'],
                         'C': [1.0, np.nan, np.nan, np.nan, 1., 1., 1, 1.],
-                        'D': range(8)})
+                        'D': list(range(8))})
 
         # single column
         result = df.drop_duplicates('A')
@@ -6073,7 +6083,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                         'B': ['one', 'one', 'two', 'two',
                               'two', 'two', 'one', 'two'],
                         'C': [1.0, np.nan, np.nan, np.nan, 1., 1., 1, 1.],
-                        'D': range(8)})
+                        'D': list(range(8))})
 
         # single column
         result = df.drop_duplicates('C')
@@ -6099,7 +6109,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                           'B': ['one', 'one', 'two', 'two',
                                 'two', 'two', 'one', 'two'],
                           'C': [1, 1, 2, 2, 2, 2, 1, 2],
-                          'D': range(8)})
+                          'D': list(range(8))})
 
         # single column
         df = orig.copy()
@@ -6148,7 +6158,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                   ['', '', '', 'OD'],
                   ['', '', '', 'wx']]
 
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         tuples.sort()
         index = MultiIndex.from_tuples(tuples)
 
@@ -6271,7 +6281,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_fillna_invalid_method(self):
         try:
             self.frame.fillna(method='ffil')
-        except ValueError, inst:
+        except ValueError as inst:
             self.assert_('ffil' in str(inst))
 
     def test_fillna_invalid_value(self):
@@ -6305,7 +6315,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_regex_replace_scalar(self):
         obj = {'a': list('ab..'), 'b': list('efgh')}
         dfobj = DataFrame(obj)
-        mix = {'a': range(4), 'b': list('ab..')}
+        mix = {'a': list(range(4)), 'b': list('ab..')}
         dfmix = DataFrame(mix)
 
         ### simplest cases
@@ -6371,7 +6381,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_regex_replace_scalar_inplace(self):
         obj = {'a': list('ab..'), 'b': list('efgh')}
         dfobj = DataFrame(obj)
-        mix = {'a': range(4), 'b': list('ab..')}
+        mix = {'a': list(range(4)), 'b': list('ab..')}
         dfmix = DataFrame(mix)
 
         ### simplest cases
@@ -6579,14 +6589,14 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_regex_replace_list_mixed(self):
         ## mixed frame to make sure this doesn't break things
-        mix = {'a': range(4), 'b': list('ab..')}
+        mix = {'a': list(range(4)), 'b': list('ab..')}
         dfmix = DataFrame(mix)
 
         ## lists of regexes and values
         # list of [re1, re2, ..., reN] -> [v1, v2, ..., vN]
         to_replace_res = [r'\s*\.\s*', r'a']
         values = [nan, 'crap']
-        mix2 = {'a': range(4), 'b': list('ab..'), 'c': list('halo')}
+        mix2 = {'a': list(range(4)), 'b': list('ab..'), 'c': list('halo')}
         dfmix2 = DataFrame(mix2)
         res = dfmix2.replace(to_replace_res, values, regex=True)
         expec = DataFrame({'a': mix2['a'], 'b': ['crap', 'b', nan, nan],
@@ -6617,7 +6627,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(res, expec)
 
     def test_regex_replace_list_mixed_inplace(self):
-        mix = {'a': range(4), 'b': list('ab..')}
+        mix = {'a': list(range(4)), 'b': list('ab..')}
         dfmix = DataFrame(mix)
         # the same inplace
         ## lists of regexes and values
@@ -6656,7 +6666,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(res, expec)
 
     def test_regex_replace_dict_mixed(self):
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         dfmix = DataFrame(mix)
 
         ## dicts
@@ -6713,7 +6723,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_regex_replace_dict_nested(self):
         # nested dicts will not work until this is implemented for Series
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         dfmix = DataFrame(mix)
         res = dfmix.replace({'b': {r'\s*\.\s*': nan}}, regex=True)
         res2 = dfmix.copy()
@@ -6734,7 +6744,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(df.replace({'Type': {'Q':0,'T':1}}), expected)
 
     def test_regex_replace_list_to_scalar(self):
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         df = DataFrame(mix)
         res = df.replace([r'\s*\.\s*', 'a|b'], nan, regex=True)
         res2 = df.copy()
@@ -6749,7 +6759,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_regex_replace_str_to_numeric(self):
         # what happens when you try to replace a numeric value with a regex?
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         df = DataFrame(mix)
         res = df.replace(r'\s*\.\s*', 0, regex=True)
         res2 = df.copy()
@@ -6763,7 +6773,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(res3, expec)
 
     def test_regex_replace_regex_list_to_numeric(self):
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         df = DataFrame(mix)
         res = df.replace([r'\s*\.\s*', 'b'], 0, regex=True)
         res2 = df.copy()
@@ -6778,7 +6788,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(res3, expec)
 
     def test_regex_replace_series_of_regexes(self):
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         df = DataFrame(mix)
         s1 = Series({'b': r'\s*\.\s*'})
         s2 = Series({'b': nan})
@@ -6794,7 +6804,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(res3, expec)
 
     def test_regex_replace_numeric_to_object_conversion(self):
-        mix = {'a': range(4), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
+        mix = {'a': list(range(4)), 'b': list('ab..'), 'c': ['a', 'b', nan, 'd']}
         df = DataFrame(mix)
         res = df.replace(0, 'a')
         expec = DataFrame({'a': ['a', 1, 2, 3], 'b': mix['b'], 'c': mix['c']})
@@ -7335,42 +7345,42 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         df = DataFrame(np.random.randn(10, 4))
 
         # axis=0
-        result = df.reindex(range(15))
+        result = df.reindex(list(range(15)))
         self.assert_(np.isnan(result.values[-5:]).all())
 
-        result = df.reindex(range(15), fill_value=0)
-        expected = df.reindex(range(15)).fillna(0)
+        result = df.reindex(list(range(15)), fill_value=0)
+        expected = df.reindex(list(range(15))).fillna(0)
         assert_frame_equal(result, expected)
 
         # axis=1
-        result = df.reindex(columns=range(5), fill_value=0.)
+        result = df.reindex(columns=list(range(5)), fill_value=0.)
         expected = df.copy()
         expected[4] = 0.
         assert_frame_equal(result, expected)
 
-        result = df.reindex(columns=range(5), fill_value=0)
+        result = df.reindex(columns=list(range(5)), fill_value=0)
         expected = df.copy()
         expected[4] = 0
         assert_frame_equal(result, expected)
 
-        result = df.reindex(columns=range(5), fill_value='foo')
+        result = df.reindex(columns=list(range(5)), fill_value='foo')
         expected = df.copy()
         expected[4] = 'foo'
         assert_frame_equal(result, expected)
 
         # reindex_axis
-        result = df.reindex_axis(range(15), fill_value=0., axis=0)
-        expected = df.reindex(range(15)).fillna(0)
+        result = df.reindex_axis(list(range(15)), fill_value=0., axis=0)
+        expected = df.reindex(list(range(15))).fillna(0)
         assert_frame_equal(result, expected)
 
-        result = df.reindex_axis(range(5), fill_value=0., axis=1)
-        expected = df.reindex(columns=range(5)).fillna(0)
+        result = df.reindex_axis(list(range(5)), fill_value=0., axis=1)
+        expected = df.reindex(columns=list(range(5))).fillna(0)
         assert_frame_equal(result, expected)
 
         # other dtypes
         df['foo'] = 'foo'
-        result = df.reindex(range(15), fill_value=0)
-        expected = df.reindex(range(15)).fillna(0)
+        result = df.reindex(list(range(15)), fill_value=0)
+        expected = df.reindex(list(range(15))).fillna(0)
         assert_frame_equal(result, expected)
 
     def test_align(self):
@@ -8186,7 +8196,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         try:
             transformed = data.apply(transform, axis=1)
-        except Exception, e:
+        except Exception as e:
             self.assertEqual(len(e.args), 2)
             self.assertEqual(e.args[1], 'occurred at index 4')
 
@@ -8303,7 +8313,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assert_('foo' in filtered)
 
         # unicode columns, won't ascii-encode
-        df = self.frame.rename(columns={'B': u'\u2202'})
+        df = self.frame.rename(columns={'B': six.u('\u2202')})
         filtered = df.filter(like='C')
         self.assertTrue('C' in filtered)
 
@@ -8505,12 +8515,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         try:
             df.sort_index(by='a')
-        except Exception, e:
+        except Exception as e:
             self.assertTrue('duplicate' in str(e))
 
         try:
             df.sort_index(by=['a'])
-        except Exception, e:
+        except Exception as e:
             self.assertTrue('duplicate' in str(e))
 
     def test_sort_datetimes(self):
@@ -8956,12 +8966,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assert_(isinstance(ct2, Series))
 
         # GH #423
-        df = DataFrame(index=range(10))
+        df = DataFrame(index=list(range(10)))
         result = df.count(1)
         expected = Series(0, index=df.index)
         assert_series_equal(result, expected)
 
-        df = DataFrame(columns=range(10))
+        df = DataFrame(columns=list(range(10)))
         result = df.count(0)
         expected = Series(0, index=df.columns)
         assert_series_equal(result, expected)
@@ -9144,7 +9154,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
             print (df)
             self.assertFalse(len(_f()))
 
-            df['a'] = range(len(df))
+            df['a'] = list(range(len(df)))
             self.assert_(len(getattr(df, name)()))
 
         if has_skipna:
@@ -9523,12 +9533,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_series_equal(result, expected)
 
     def test_combine_first_mixed(self):
-        a = Series(['a', 'b'], index=range(2))
-        b = Series(range(2), index=range(2))
+        a = Series(['a', 'b'], index=list(range(2)))
+        b = Series(list(range(2)), index=list(range(2)))
         f = DataFrame({'A': a, 'B': b})
 
-        a = Series(['a', 'b'], index=range(5, 7))
-        b = Series(range(2), index=range(5, 7))
+        a = Series(['a', 'b'], index=list(range(5, 7)))
+        b = Series(list(range(2)), index=list(range(5, 7)))
         g = DataFrame({'A': a, 'B': b})
 
         combined = f.combine_first(g)
@@ -9546,7 +9556,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assert_(reindexed.values.dtype == np.object_)
         self.assert_(isnull(reindexed[0][1]))
 
-        reindexed = frame.reindex(columns=range(3))
+        reindexed = frame.reindex(columns=list(range(3)))
         self.assert_(reindexed.values.dtype == np.object_)
         self.assert_(isnull(reindexed[1]).all())
 
@@ -9606,22 +9616,22 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_reindex_multi(self):
         df = DataFrame(np.random.randn(3, 3))
 
-        result = df.reindex(range(4), range(4))
-        expected = df.reindex(range(4)).reindex(columns=range(4))
+        result = df.reindex(list(range(4)), list(range(4)))
+        expected = df.reindex(list(range(4))).reindex(columns=list(range(4)))
 
         assert_frame_equal(result, expected)
 
         df = DataFrame(np.random.randint(0, 10, (3, 3)))
 
-        result = df.reindex(range(4), range(4))
-        expected = df.reindex(range(4)).reindex(columns=range(4))
+        result = df.reindex(list(range(4)), list(range(4)))
+        expected = df.reindex(list(range(4))).reindex(columns=list(range(4)))
 
         assert_frame_equal(result, expected)
 
         df = DataFrame(np.random.randint(0, 10, (3, 3)))
 
-        result = df.reindex(range(2), range(2))
-        expected = df.reindex(range(2)).reindex(columns=range(2))
+        result = df.reindex(list(range(2)), list(range(2)))
+        expected = df.reindex(list(range(2))).reindex(columns=list(range(2)))
 
         assert_frame_equal(result, expected)
 
@@ -9657,7 +9667,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_cumsum_corner(self):
         dm = DataFrame(np.arange(20).reshape(4, 5),
-                       index=range(4), columns=range(5))
+                       index=list(range(4)), columns=list(range(5)))
         result = dm.cumsum()
 
     #----------------------------------------------------------------------
@@ -9711,7 +9721,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         # check composability of unstack
         old_data = data.copy()
-        for _ in xrange(4):
+        for _ in range(4):
             data = data.unstack()
         assert_frame_equal(old_data, data)
 
@@ -9867,13 +9877,13 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(rs, xp)
 
         rs = df.reset_index('a', col_fill=None)
-        xp = DataFrame(full, Index(range(3), name='d'),
+        xp = DataFrame(full, Index(list(range(3)), name='d'),
                        columns=[['a', 'b', 'b', 'c'],
                                 ['a', 'mean', 'median', 'mean']])
         assert_frame_equal(rs, xp)
 
         rs = df.reset_index('a', col_fill='blah', col_level=1)
-        xp = DataFrame(full, Index(range(3), name='d'),
+        xp = DataFrame(full, Index(list(range(3)), name='d'),
                        columns=[['blah', 'b', 'b', 'c'],
                                 ['a', 'mean', 'median', 'mean']])
         assert_frame_equal(rs, xp)
@@ -10148,7 +10158,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_xs_view(self):
         dm = DataFrame(np.arange(20.).reshape(4, 5),
-                       index=range(4), columns=range(5))
+                       index=list(range(4)), columns=list(range(5)))
 
         dm.xs(2, copy=False)[:] = 5
         self.assert_((dm.xs(2) == 5).all())
@@ -10166,7 +10176,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assert_((dm.xs(3) == 10).all())
 
     def test_boolean_indexing(self):
-        idx = range(3)
+        idx = list(range(3))
         cols = ['A','B','C']
         df1 = DataFrame(index=idx, columns=cols,
                         data=np.array([[0.0, 0.5, 1.0],
@@ -10186,15 +10196,15 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
     def test_boolean_indexing_mixed(self):
         df = DataFrame(
-            {0L: {35: np.nan, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
-             1L: {35: np.nan,
+            {long(0): {35: np.nan, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
+             long(1): {35: np.nan,
                   40: 0.32632316859446198,
                   43: np.nan,
                   49: 0.32632316859446198,
                   50: 0.39114724480578139},
-             2L: {35: np.nan, 40: np.nan, 43: 0.29012581014105987, 49: np.nan, 50: np.nan},
-             3L: {35: np.nan, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
-             4L: {35: 0.34215328467153283, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
+             long(2): {35: np.nan, 40: np.nan, 43: 0.29012581014105987, 49: np.nan, 50: np.nan},
+             long(3): {35: np.nan, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
+             long(4): {35: 0.34215328467153283, 40: np.nan, 43: np.nan, 49: np.nan, 50: np.nan},
              'y': {35: 0, 40: 0, 43: 0, 49: 0, 50: 1}})
 
         # mixed int/float ok
@@ -10212,15 +10222,15 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         self.assertRaises(ValueError, df.__setitem__, df>0.3, 1)
 
     def test_sum_bools(self):
-        df = DataFrame(index=range(1), columns=range(10))
+        df = DataFrame(index=list(range(1)), columns=list(range(10)))
         bools = isnull(df)
         self.assert_(bools.sum(axis=1)[0] == 10)
 
     def test_fillna_col_reordering(self):
-        idx = range(20)
+        idx = list(range(20))
         cols = ["COL." + str(i) for i in range(5, 0, -1)]
         data = np.random.rand(20, 5)
-        df = DataFrame(index=range(20), columns=cols, data=data)
+        df = DataFrame(index=list(range(20)), columns=cols, data=data)
         filled = df.fillna(method='ffill')
         self.assert_(df.columns.tolist() == filled.columns.tolist())
 
@@ -10300,7 +10310,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
             assert_frame_equal(result, expected)
 
     def test_iterkv_names(self):
-        for k, v in self.mixed_frame.iterkv():
+        for k, v in self.mixed_frame.iteritems():
             self.assertEqual(v.name, k)
 
     def test_series_put_names(self):
@@ -10347,8 +10357,8 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         result = A.dot(b)
 
         # unaligned
-        df = DataFrame(randn(3, 4), index=[1, 2, 3], columns=range(4))
-        df2 = DataFrame(randn(5, 3), index=range(5), columns=[1, 2, 3])
+        df = DataFrame(randn(3, 4), index=[1, 2, 3], columns=list(range(4)))
+        df2 = DataFrame(randn(5, 3), index=list(range(5)), columns=[1, 2, 3])
 
         self.assertRaises(ValueError, df.dot, df2)
 
@@ -10554,7 +10564,7 @@ starting,ending,measure
         #    df[col] = nan
 
         for i, dt in enumerate(df.index):
-            for col in xrange(100, 200):
+            for col in range(100, 200):
                 if not col in wasCol:
                     wasCol[col] = 1
                     df[col] = nan
@@ -10675,12 +10685,12 @@ starting,ending,measure
 
         # without using iloc
         result = df.isin(d)
-        assert_frame_equal(result, expected)        
+        assert_frame_equal(result, expected)
 
         # using iloc
         result = df.isin(d, iloc=True)
         expected.iloc[0, 0] = True
-        assert_frame_equal(result, expected)        
+        assert_frame_equal(result, expected)
 
 
 if __name__ == '__main__':
diff --git a/pandas/tests/test_graphics.py b/pandas/tests/test_graphics.py
index 08b42d7cf..53c169a7a 100644
--- a/pandas/tests/test_graphics.py
+++ b/pandas/tests/test_graphics.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import nose
 import os
 import string
@@ -17,6 +18,9 @@ from numpy import random
 from numpy.testing import assert_array_equal
 from numpy.testing.decorators import slow
 import pandas.tools.plotting as plotting
+import six
+from six.moves import map
+from six.moves import zip
 
 
 def _skip_if_no_scipy():
@@ -115,7 +119,7 @@ class TestSeriesPlots(unittest.TestCase):
 
         rects = ax.patches
 
-        rgba_colors = map(cm.jet, np.linspace(0, 1, 5))
+        rgba_colors = list(map(cm.jet, np.linspace(0, 1, 5)))
         for i, rect in enumerate(rects[::5]):
             xp = rgba_colors[i]
             rs = rect.get_facecolor()
@@ -128,7 +132,7 @@ class TestSeriesPlots(unittest.TestCase):
 
         rects = ax.patches
 
-        rgba_colors = map(cm.jet, np.linspace(0, 1, 5))
+        rgba_colors = list(map(cm.jet, np.linspace(0, 1, 5)))
         for i, rect in enumerate(rects[::5]):
             xp = rgba_colors[i]
             rs = rect.get_facecolor()
@@ -271,7 +275,7 @@ class TestSeriesPlots(unittest.TestCase):
 
     @slow
     def test_valid_object_plot(self):
-        s = Series(range(10), dtype=object)
+        s = Series(list(range(10)), dtype=object)
         kinds = 'line', 'bar', 'barh', 'kde', 'density'
 
         for kind in kinds:
@@ -327,27 +331,27 @@ class TestDataFramePlots(unittest.TestCase):
         _check_plot_works(df.plot, subplots=True, title='blah')
         _check_plot_works(df.plot, title='blah')
 
-        tuples = zip(list(string.ascii_letters[:10]), range(10))
+        tuples = list(zip(string.ascii_letters[:10], range(10)))
         df = DataFrame(np.random.rand(10, 3),
                        index=MultiIndex.from_tuples(tuples))
         _check_plot_works(df.plot, use_index=True)
 
         # unicode
-        index = MultiIndex.from_tuples([(u'\u03b1', 0),
-                                        (u'\u03b1', 1),
-                                        (u'\u03b2', 2),
-                                        (u'\u03b2', 3),
-                                        (u'\u03b3', 4),
-                                        (u'\u03b3', 5),
-                                        (u'\u03b4', 6),
-                                        (u'\u03b4', 7)], names=['i0', 'i1'])
-        columns = MultiIndex.from_tuples([('bar', u'\u0394'),
-                                        ('bar', u'\u0395')], names=['c0',
+        index = MultiIndex.from_tuples([(six.u('\u03b1'), 0),
+                                        (six.u('\u03b1'), 1),
+                                        (six.u('\u03b2'), 2),
+                                        (six.u('\u03b2'), 3),
+                                        (six.u('\u03b3'), 4),
+                                        (six.u('\u03b3'), 5),
+                                        (six.u('\u03b4'), 6),
+                                        (six.u('\u03b4'), 7)], names=['i0', 'i1'])
+        columns = MultiIndex.from_tuples([('bar', six.u('\u0394')),
+                                        ('bar', six.u('\u0395'))], names=['c0',
                                                                     'c1'])
         df = DataFrame(np.random.randint(0, 10, (8, 2)),
                        columns=columns,
                        index=index)
-        _check_plot_works(df.plot, title=u'\u03A3')
+        _check_plot_works(df.plot, title=six.u('\u03A3'))
 
     def test_nonnumeric_exclude(self):
         import matplotlib.pyplot as plt
@@ -384,7 +388,7 @@ class TestDataFramePlots(unittest.TestCase):
         self._check_data(df.plot(y='B'), df.B.plot())
 
         # columns.inferred_type == 'integer'
-        df.columns = range(1, len(df.columns) + 1)
+        df.columns = list(range(1, len(df.columns) + 1))
         self._check_data(df.plot(x=1, y=2),
                          df.set_index(1)[2].plot())
         self._check_data(df.plot(x=1), df.set_index(1).plot())
@@ -497,7 +501,7 @@ class TestDataFramePlots(unittest.TestCase):
 
         df = DataFrame(np.random.randn(10, 15),
                        index=list(string.ascii_letters[:10]),
-                       columns=range(15))
+                       columns=list(range(15)))
         _check_plot_works(df.plot, kind='bar')
 
         df = DataFrame({'a': [0, 1], 'b': [1, 0]})
@@ -505,13 +509,13 @@ class TestDataFramePlots(unittest.TestCase):
 
     def test_bar_stacked_center(self):
         # GH2157
-        df = DataFrame({'A': [3] * 5, 'B': range(5)}, index=range(5))
+        df = DataFrame({'A': [3] * 5, 'B': list(range(5))}, index=list(range(5)))
         ax = df.plot(kind='bar', stacked='True', grid=True)
         self.assertEqual(ax.xaxis.get_ticklocs()[0],
                          ax.patches[0].get_x() + ax.patches[0].get_width() / 2)
 
     def test_bar_center(self):
-        df = DataFrame({'A': [3] * 5, 'B': range(5)}, index=range(5))
+        df = DataFrame({'A': [3] * 5, 'B': list(range(5))}, index=list(range(5)))
         ax = df.plot(kind='bar', grid=True)
         self.assertEqual(ax.xaxis.get_ticklocs()[0],
                          ax.patches[0].get_x() + ax.patches[0].get_width())
@@ -521,7 +525,7 @@ class TestDataFramePlots(unittest.TestCase):
         # GH3254, GH3298 matplotlib/matplotlib#1882, #1892
         # regressions in 1.2.1
 
-        df = DataFrame({'A': [3] * 5, 'B': range(1, 6)}, index=range(5))
+        df = DataFrame({'A': [3] * 5, 'B': list(range(1, 6))}, index=list(range(5)))
         ax = df.plot(kind='bar', grid=True, log=True)
         self.assertEqual(ax.yaxis.get_ticklocs()[0], 1.0)
 
@@ -765,7 +769,7 @@ class TestDataFramePlots(unittest.TestCase):
     def test_line_colors(self):
         import matplotlib.pyplot as plt
         import sys
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         from matplotlib import cm
 
         custom_colors = 'rgcby'
@@ -796,7 +800,7 @@ class TestDataFramePlots(unittest.TestCase):
 
         ax = df.plot(colormap='jet')
 
-        rgba_colors = map(cm.jet, np.linspace(0, 1, len(df)))
+        rgba_colors = list(map(cm.jet, np.linspace(0, 1, len(df))))
 
         lines = ax.get_lines()
         for i, l in enumerate(lines):
@@ -808,7 +812,7 @@ class TestDataFramePlots(unittest.TestCase):
 
         ax = df.plot(colormap=cm.jet)
 
-        rgba_colors = map(cm.jet, np.linspace(0, 1, len(df)))
+        rgba_colors = list(map(cm.jet, np.linspace(0, 1, len(df))))
 
         lines = ax.get_lines()
         for i, l in enumerate(lines):
@@ -887,7 +891,7 @@ class TestDataFrameGroupByPlots(unittest.TestCase):
         _check_plot_works(grouped.boxplot)
         _check_plot_works(grouped.boxplot, subplots=False)
 
-        tuples = zip(list(string.ascii_letters[:10]), range(10))
+        tuples = list(zip(string.ascii_letters[:10], range(10)))
         df = DataFrame(np.random.rand(10, 3),
                        index=MultiIndex.from_tuples(tuples))
         grouped = df.groupby(level=1)
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 6af287b77..28756a1c0 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -1,3 +1,6 @@
+from __future__ import print_function
+from pandas.util.py3compat import range, long
+from pandas.util import compat
 import nose
 import unittest
 
@@ -23,11 +26,13 @@ from numpy.testing import assert_equal
 import pandas.core.nanops as nanops
 
 import pandas.util.testing as tm
+from six.moves import map
+from six.moves import zip
 
 
 def commonSetUp(self):
     self.dateRange = bdate_range('1/1/2005', periods=250)
-    self.stringIndex = Index([rands(8).upper() for x in xrange(250)])
+    self.stringIndex = Index([rands(8).upper() for x in range(250)])
 
     self.groupId = Series([x[0] for x in self.stringIndex],
                           index=self.stringIndex)
@@ -189,9 +194,9 @@ class TestGroupBy(unittest.TestCase):
         assert_frame_equal(nth, expected, check_names=False)
 
         # GH 2763, first/last shifting dtypes
-        idx = range(10)
+        idx = list(range(10))
         idx.append(9)
-        s = Series(data=range(11), index=idx, name='IntCol')
+        s = Series(data=list(range(11)), index=idx, name='IntCol')
         self.assert_(s.dtype == 'int64')
         f = s.groupby(level=0).first()
         self.assert_(f.dtype == 'int64')
@@ -263,7 +268,7 @@ class TestGroupBy(unittest.TestCase):
 
         # GH 3911, mixed frame non-conversion
         df = self.df_mixed_floats.copy()
-        df['value'] = range(len(df))
+        df['value'] = list(range(len(df)))
 
         def max_value(group):
             return group.ix[group['value'].idxmax()]
@@ -278,7 +283,7 @@ class TestGroupBy(unittest.TestCase):
     def test_groupby_return_type(self):
 
         # GH2893, return a reduced type
-        df1 = DataFrame([{"val1": 1, "val2" : 20}, {"val1":1, "val2": 19}, 
+        df1 = DataFrame([{"val1": 1, "val2" : 20}, {"val1":1, "val2": 19},
                          {"val1":2, "val2": 27}, {"val1":2, "val2": 12}])
 
         def func(dataf):
@@ -287,7 +292,7 @@ class TestGroupBy(unittest.TestCase):
         result = df1.groupby("val1", squeeze=True).apply(func)
         self.assert_(isinstance(result,Series))
 
-        df2 = DataFrame([{"val1": 1, "val2" : 20}, {"val1":1, "val2": 19}, 
+        df2 = DataFrame([{"val1": 1, "val2" : 20}, {"val1":1, "val2": 19},
                          {"val1":1, "val2": 27}, {"val1":1, "val2": 12}])
         def func(dataf):
             return dataf["val2"]  - dataf["val2"].mean()
@@ -500,7 +505,7 @@ class TestGroupBy(unittest.TestCase):
 
         def raiseException(df):
           print ('----------------------------------------')
-          print (df.to_string())
+          print(df.to_string())
           raise TypeError
 
         self.assertRaises(TypeError, df.groupby(0).agg,
@@ -508,11 +513,11 @@ class TestGroupBy(unittest.TestCase):
 
     def test_basic_regression(self):
         # regression
-        T = [1.0 * x for x in range(1, 10) * 10][:1095]
-        result = Series(T, range(0, len(T)))
+        T = [1.0 * x for x in list(range(1, 10)) * 10][:1095]
+        result = Series(T, list(range(0, len(T))))
 
         groupings = np.random.random((1100,))
-        groupings = Series(groupings, range(0, len(groupings))) * 10.
+        groupings = Series(groupings, list(range(0, len(groupings)))) * 10.
 
         grouped = result.groupby(groupings)
         grouped.mean()
@@ -707,12 +712,12 @@ class TestGroupBy(unittest.TestCase):
                 return y
 
         df = DataFrame({'a':[1,2,2,2],
-                        'b':range(4),
-                        'c':range(5,9)})
+                        'b':list(range(4)),
+                        'c':list(range(5,9))})
 
         df2 = DataFrame({'a':[3,2,2,2],
-                         'b':range(4),
-                         'c':range(5,9)})
+                         'b':list(range(4)),
+                         'c':list(range(5,9))})
 
 
         # correct result
@@ -1153,7 +1158,7 @@ class TestGroupBy(unittest.TestCase):
         result = grouped.mean()
         expected = data.groupby(['A', 'B']).mean()
 
-        arrays = zip(*expected.index._tuple_index)
+        arrays = list(zip(*expected.index._tuple_index))
         expected.insert(0, 'A', arrays[0])
         expected.insert(1, 'B', arrays[1])
         expected.index = np.arange(len(expected))
@@ -1416,7 +1421,7 @@ class TestGroupBy(unittest.TestCase):
 
     def test_groupby_level_index_names(self):
         ## GH4014 this used to raise ValueError since 'exp'>1 (in py2)
-        df = DataFrame({'exp' : ['A']*3 + ['B']*3, 'var1' : range(6),}).set_index('exp')
+        df = DataFrame({'exp' : ['A']*3 + ['B']*3, 'var1' : list(range(6)),}).set_index('exp')
         df.groupby(level='exp')
         self.assertRaises(ValueError, df.groupby, level='foo')
 
@@ -1565,7 +1570,7 @@ class TestGroupBy(unittest.TestCase):
         mydf = DataFrame({
                 'cat1' : ['a'] * 8 + ['b'] * 6,
                 'cat2' : ['c'] * 2 + ['d'] * 2 + ['e'] * 2 + ['f'] * 2 + ['c'] * 2 + ['d'] * 2 + ['e'] * 2,
-                'cat3' : map(lambda x: 'g%s' % x, range(1,15)),
+                'cat3' : list(map(lambda x: 'g%s' % x, list(range(1,15)))),
                 'val' : np.random.randint(100, size=14),
                 })
 
@@ -1585,7 +1590,7 @@ class TestGroupBy(unittest.TestCase):
     def test_apply_chunk_view(self):
         # Low level tinkering could be unsafe, make sure not
         df = DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3],
-                        'value': range(9)})
+                        'value': list(range(9))})
 
         # return view
         f = lambda x: x[:2]
@@ -1597,7 +1602,7 @@ class TestGroupBy(unittest.TestCase):
     def test_apply_no_name_column_conflict(self):
         df = DataFrame({'name': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2],
                         'name2': [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],
-                        'value': range(10)[::-1]})
+                        'value': list(range(10))[::-1]})
 
         # it works! #2605
         grouped = df.groupby(['name', 'name2'])
@@ -1615,10 +1620,10 @@ class TestGroupBy(unittest.TestCase):
         assert_series_equal(agged, exp)
 
     def test_groupby_with_hier_columns(self):
-        tuples = zip(*[['bar', 'bar', 'baz', 'baz',
+        tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',
                         'foo', 'foo', 'qux', 'qux'],
                        ['one', 'two', 'one', 'two',
-                        'one', 'two', 'one', 'two']])
+                        'one', 'two', 'one', 'two']]))
         index = MultiIndex.from_tuples(tuples)
         columns = MultiIndex.from_tuples([('A', 'cat'), ('B', 'dog'),
                                           ('B', 'cat'), ('A', 'dog')])
@@ -1849,14 +1854,14 @@ class TestGroupBy(unittest.TestCase):
     def test_cython_grouper_series_bug_noncontig(self):
         arr = np.empty((100, 100))
         arr.fill(np.nan)
-        obj = Series(arr[:, 0], index=range(100))
-        inds = np.tile(range(10), 10)
+        obj = Series(arr[:, 0], index=list(range(100)))
+        inds = np.tile(list(range(10)), 10)
 
         result = obj.groupby(inds).agg(Series.median)
         self.assert_(result.isnull().all())
 
     def test_series_grouper_noncontig_index(self):
-        index = Index([tm.rands(10) for _ in xrange(100)])
+        index = Index([tm.rands(10) for _ in range(100)])
 
         values = Series(np.random.randn(50), index=index[::2])
         labels = np.random.randint(0, 5, 50)
@@ -1872,7 +1877,7 @@ class TestGroupBy(unittest.TestCase):
 
         from decimal import Decimal
 
-        s = Series(range(5))
+        s = Series(list(range(5)))
         labels = np.array(['a', 'b', 'c', 'd', 'e'], dtype='O')
 
         def convert_fast(x):
@@ -1987,7 +1992,7 @@ class TestGroupBy(unittest.TestCase):
         assert_almost_equal(result, expected)
 
     def test_groupby_2d_malformed(self):
-        d = DataFrame(index=range(2))
+        d = DataFrame(index=list(range(2)))
         d['group'] = ['g1', 'g2']
         d['zeros'] = [0, 0]
         d['ones'] = [1, 1]
@@ -2031,8 +2036,8 @@ class TestGroupBy(unittest.TestCase):
         exp_index, _ = right.index.sortlevel(0)
         self.assert_(right.index.equals(exp_index))
 
-        tups = map(tuple, df[['A', 'B', 'C', 'D',
-                              'E', 'F', 'G', 'H']].values)
+        tups = list(map(tuple, df[['A', 'B', 'C', 'D',
+                              'E', 'F', 'G', 'H']].values))
         tups = com._asarray_tuplesafe(tups)
         expected = df.groupby(tups).sum()['values']
 
@@ -2046,18 +2051,18 @@ class TestGroupBy(unittest.TestCase):
                         'c': [0, 1, 2],
                         'd': np.random.randn(3)})
 
-        tups = map(tuple, df[['a', 'b', 'c']].values)
+        tups = list(map(tuple, df[['a', 'b', 'c']].values))
         tups = com._asarray_tuplesafe(tups)
         result = df.groupby(['a', 'b', 'c'], sort=True).sum()
         self.assert_(np.array_equal(result.index.values,
                                     tups[[1, 2, 0]]))
 
-        tups = map(tuple, df[['c', 'a', 'b']].values)
+        tups = list(map(tuple, df[['c', 'a', 'b']].values))
         tups = com._asarray_tuplesafe(tups)
         result = df.groupby(['c', 'a', 'b'], sort=True).sum()
         self.assert_(np.array_equal(result.index.values, tups))
 
-        tups = map(tuple, df[['b', 'c', 'a']].values)
+        tups = list(map(tuple, df[['b', 'c', 'a']].values))
         tups = com._asarray_tuplesafe(tups)
         result = df.groupby(['b', 'c', 'a'], sort=True).sum()
         self.assert_(np.array_equal(result.index.values,
@@ -2092,8 +2097,8 @@ class TestGroupBy(unittest.TestCase):
         assert_frame_equal(result, expected)
 
     def test_rank_apply(self):
-        lev1 = np.array([rands(10) for _ in xrange(100)], dtype=object)
-        lev2 = np.array([rands(10) for _ in xrange(130)], dtype=object)
+        lev1 = np.array([rands(10) for _ in range(100)], dtype=object)
+        lev2 = np.array([rands(10) for _ in range(130)], dtype=object)
         lab1 = np.random.randint(0, 100, size=500)
         lab2 = np.random.randint(0, 130, size=500)
 
@@ -2410,7 +2415,7 @@ class TestGroupBy(unittest.TestCase):
         l = [['count', 'values'], ['to filter', '']]
         midx = MultiIndex.from_tuples(l)
 
-        df = DataFrame([[1L, 'A']], columns=midx)
+        df = DataFrame([[long(1), 'A']], columns=midx)
 
         grouped = df.groupby('to filter').groups
         self.assert_(np.array_equal(grouped['A'], [0]))
@@ -2418,13 +2423,13 @@ class TestGroupBy(unittest.TestCase):
         grouped = df.groupby([('to filter', '')]).groups
         self.assert_(np.array_equal(grouped['A'], [0]))
 
-        df = DataFrame([[1L, 'A'], [2L, 'B']], columns=midx)
+        df = DataFrame([[long(1), 'A'], [long(2), 'B']], columns=midx)
 
         expected = df.groupby('to filter').groups
         result = df.groupby([('to filter', '')]).groups
         self.assertEquals(result, expected)
 
-        df = DataFrame([[1L, 'A'], [2L, 'A']], columns=midx)
+        df = DataFrame([[long(1), 'A'], [long(2), 'A']], columns=midx)
 
         expected = df.groupby('to filter').groups
         result = df.groupby([('to filter', '')]).groups
@@ -2553,7 +2558,7 @@ class TestGroupBy(unittest.TestCase):
             grouped.filter(lambda x: x.mean() < 10, dropna=False),
                            expected_odd.reindex(df.index))
         assert_frame_equal(
-            grouped.filter(lambda x: x.mean() > 10, dropna=False), 
+            grouped.filter(lambda x: x.mean() > 10, dropna=False),
                            expected_even.reindex(df.index))
 
     def test_filter_multi_column_df(self):
@@ -2570,7 +2575,7 @@ class TestGroupBy(unittest.TestCase):
         df = pd.DataFrame({'A': [1, 12, 12, 1], 'B': 'a b c d'.split()})
         grouper = df['A'].apply(lambda x: x % 2)
         grouped = df.groupby(grouper)
-        expected = pd.DataFrame({'A': [12, 12], 'B': ['b', 'c']}, 
+        expected = pd.DataFrame({'A': [12, 12], 'B': ['b', 'c']},
                                 index=[1, 2])
         assert_frame_equal(
             grouped.filter(lambda x: x['A'].sum() > 10), expected)
@@ -2613,7 +2618,7 @@ class TestGroupBy(unittest.TestCase):
         s = pd.Series([-1,0,1,2])
         grouper = s.apply(lambda x: x % 2)
         grouped = s.groupby(grouper)
-        self.assertRaises(ValueError, 
+        self.assertRaises(ValueError,
                           lambda: grouped.filter(raise_if_sum_is_zero))
 
     def test_filter_against_workaround(self):
@@ -2673,7 +2678,7 @@ def assert_fp_equal(a, b):
 
 
 def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):
-    tups = map(tuple, df[keys].values)
+    tups = list(map(tuple, df[keys].values))
     tups = com._asarray_tuplesafe(tups)
     expected = f(df.groupby(tups)[field])
     for k, v in expected.iteritems():
diff --git a/pandas/tests/test_index.py b/pandas/tests/test_index.py
index 250728dc5..2141a6fc9 100644
--- a/pandas/tests/test_index.py
+++ b/pandas/tests/test_index.py
@@ -1,6 +1,7 @@
 # pylint: disable=E1101,E1103,W0232
 
 from datetime import datetime, timedelta
+from pandas.util.py3compat import range
 import operator
 import pickle
 import unittest
@@ -22,6 +23,8 @@ import pandas.tseries.offsets as offsets
 
 import pandas as pd
 from pandas.lib import Timestamp
+import six
+from six.moves import zip
 
 
 class TestIndex(unittest.TestCase):
@@ -34,7 +37,7 @@ class TestIndex(unittest.TestCase):
         self.intIndex = tm.makeIntIndex(100)
         self.floatIndex = tm.makeFloatIndex(100)
         self.empty = Index([])
-        self.tuples = Index(zip(['foo', 'bar', 'baz'], [1, 2, 3]))
+        self.tuples = Index(list(zip(['foo', 'bar', 'baz'], [1, 2, 3])))
 
     def test_hash_error(self):
         self.assertRaises(TypeError, hash, self.strIndex)
@@ -368,13 +371,13 @@ class TestIndex(unittest.TestCase):
         # 2845
         index = Index([1, 2.0+3.0j, np.nan])
         formatted = index.format()
-        expected = [str(index[0]), str(index[1]), u'NaN']
+        expected = [str(index[0]), str(index[1]), six.u('NaN')]
         self.assertEquals(formatted, expected)
 
         # is this really allowed?
         index = Index([1, 2.0+3.0j, None])
         formatted = index.format()
-        expected = [str(index[0]), str(index[1]), u'NaN']
+        expected = [str(index[0]), str(index[1]), six.u('NaN')]
         self.assertEquals(formatted, expected)
 
         self.strIndex[:0].format()
@@ -467,8 +470,8 @@ class TestIndex(unittest.TestCase):
     def test_drop(self):
         n = len(self.strIndex)
 
-        dropped = self.strIndex.drop(self.strIndex[range(5, 10)])
-        expected = self.strIndex[range(5) + range(10, n)]
+        dropped = self.strIndex.drop(self.strIndex[list(range(5, 10))])
+        expected = self.strIndex[list(range(5)) + list(range(10, n))]
         self.assert_(dropped.equals(expected))
 
         self.assertRaises(ValueError, self.strIndex.drop, ['foo', 'bar'])
@@ -857,7 +860,7 @@ class TestInt64Index(unittest.TestCase):
         from datetime import datetime, timedelta
         # corner case, non-Int64Index
         now = datetime.now()
-        other = Index([now + timedelta(i) for i in xrange(4)], dtype=object)
+        other = Index([now + timedelta(i) for i in range(4)], dtype=object)
         result = self.index.union(other)
         expected = np.concatenate((self.index, other))
         self.assert_(np.array_equal(result, expected))
@@ -890,14 +893,14 @@ class TestInt64Index(unittest.TestCase):
     def test_int_name_format(self):
         from pandas import Series, DataFrame
         index = Index(['a', 'b', 'c'], name=0)
-        s = Series(range(3), index)
-        df = DataFrame(range(3), index=index)
+        s = Series(list(range(3)), index)
+        df = DataFrame(list(range(3)), index=index)
         repr(s)
         repr(df)
 
     def test_print_unicode_columns(self):
         df = pd.DataFrame(
-            {u"\u05d0": [1, 2, 3], "\u05d1": [4, 5, 6], "c": [7, 8, 9]})
+            {six.u("\u05d0"): [1, 2, 3], "\u05d1": [4, 5, 6], "c": [7, 8, 9]})
         repr(df.columns)  # should not raise UnicodeDecodeError
 
     def test_repr_summary(self):
@@ -907,15 +910,15 @@ class TestInt64Index(unittest.TestCase):
             self.assertTrue("..." in r)
 
     def test_unicode_string_with_unicode(self):
-        idx = Index(range(1000))
+        idx = Index(list(range(1000)))
 
         if py3compat.PY3:
             str(idx)
         else:
-            unicode(idx)
+            six.text_type(idx)
 
     def test_bytestring_with_unicode(self):
-        idx = Index(range(1000))
+        idx = Index(list(range(1000)))
         if py3compat.PY3:
             bytes(idx)
         else:
@@ -1151,9 +1154,9 @@ class TestMultiIndex(unittest.TestCase):
         self.assertRaises(KeyError, self.index.get_loc, 'quux')
 
         # 3 levels
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])])
@@ -1173,9 +1176,9 @@ class TestMultiIndex(unittest.TestCase):
         assert(rs == xp)
 
     def test_get_loc_level(self):
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])])
@@ -1193,7 +1196,7 @@ class TestMultiIndex(unittest.TestCase):
 
         self.assertRaises(KeyError, index.get_loc_level, (2, 2))
 
-        index = MultiIndex(levels=[[2000], range(4)],
+        index = MultiIndex(levels=[[2000], list(range(4))],
                            labels=[np.array([0, 0, 0, 0]),
                                    np.array([0, 1, 2, 3])])
         result, new_index = index.get_loc_level((2000, slice(None, None)))
@@ -1219,9 +1222,9 @@ class TestMultiIndex(unittest.TestCase):
         tm.assert_almost_equal(sliced.values, expected.values)
 
     def test_slice_locs_not_sorted(self):
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])])
@@ -1276,11 +1279,11 @@ class TestMultiIndex(unittest.TestCase):
 
     def test_consistency(self):
         # need to construct an overflow
-        major_axis = range(70000)
-        minor_axis = range(10)
+        major_axis = list(range(70000))
+        minor_axis = list(range(10))
 
         major_labels = np.arange(70000)
-        minor_labels = np.repeat(range(10), 7000)
+        minor_labels = np.repeat(list(range(10)), 7000)
 
         # the fact that is works means it's consistent
         index = MultiIndex(levels=[major_axis, minor_axis],
@@ -1295,8 +1298,8 @@ class TestMultiIndex(unittest.TestCase):
         self.assert_(not index.is_unique)
 
     def test_truncate(self):
-        major_axis = Index(range(4))
-        minor_axis = Index(range(2))
+        major_axis = Index(list(range(4)))
+        minor_axis = Index(list(range(2)))
 
         major_labels = np.array([0, 0, 1, 2, 3, 3])
         minor_labels = np.array([0, 1, 0, 1, 0, 1])
@@ -1319,8 +1322,8 @@ class TestMultiIndex(unittest.TestCase):
         self.assertRaises(ValueError, index.truncate, 3, 1)
 
     def test_get_indexer(self):
-        major_axis = Index(range(4))
-        minor_axis = Index(range(2))
+        major_axis = Index(list(range(4)))
+        minor_axis = Index(list(range(2)))
 
         major_labels = np.array([0, 0, 1, 2, 2, 3, 3])
         minor_labels = np.array([0, 1, 0, 0, 1, 0, 1])
@@ -1353,8 +1356,6 @@ class TestMultiIndex(unittest.TestCase):
         r1 = idx1.get_indexer([1, 2, 3])
         self.assert_((r1 == [-1, -1, -1]).all())
 
-        # self.assertRaises(Exception, idx1.get_indexer,
-        #                   list(list(zip(*idx2._tuple_index))[0]))
 
     def test_format(self):
         self.index.format()
@@ -1404,9 +1405,9 @@ class TestMultiIndex(unittest.TestCase):
         self.assert_(self.index.equals(self.index._tuple_index))
 
         # different number of levels
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])])
@@ -1417,8 +1418,8 @@ class TestMultiIndex(unittest.TestCase):
         self.assert_(not index.equal_levels(index2))
 
         # levels are different
-        major_axis = Index(range(4))
-        minor_axis = Index(range(2))
+        major_axis = Index(list(range(4)))
+        minor_axis = Index(list(range(2)))
 
         major_labels = np.array([0, 0, 1, 2, 2, 3])
         minor_labels = np.array([0, 1, 0, 0, 1, 0])
@@ -1637,9 +1638,9 @@ class TestMultiIndex(unittest.TestCase):
         dropped = index.droplevel(0)
         self.assertEqual(dropped.name, 'second')
 
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])],
@@ -1652,9 +1653,9 @@ class TestMultiIndex(unittest.TestCase):
         self.assert_(dropped.equals(expected))
 
     def test_droplevel_multiple(self):
-        index = MultiIndex(levels=[Index(range(4)),
-                                   Index(range(4)),
-                                   Index(range(4))],
+        index = MultiIndex(levels=[Index(list(range(4))),
+                                   Index(list(range(4))),
+                                   Index(list(range(4)))],
                            labels=[np.array([0, 0, 1, 2, 2, 2, 3, 3]),
                                    np.array([0, 1, 0, 0, 0, 1, 0, 1]),
                                    np.array([1, 0, 1, 1, 0, 0, 1, 0])],
@@ -1774,21 +1775,21 @@ class TestMultiIndex(unittest.TestCase):
         self.assertEqual(result, exp)
 
     def test_repr_with_unicode_data(self):
-        d = {"a": [u"\u05d0", 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
+        d = {"a": [six.u("\u05d0"), 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
         index = pd.DataFrame(d).set_index(["a", "b"]).index
         self.assertFalse("\\u" in repr(index))  # we don't want unicode-escaped
 
     def test_unicode_string_with_unicode(self):
-        d = {"a": [u"\u05d0", 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
+        d = {"a": [six.u("\u05d0"), 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
         idx = pd.DataFrame(d).set_index(["a", "b"]).index
 
         if py3compat.PY3:
             str(idx)
         else:
-            unicode(idx)
+            six.text_type(idx)
 
     def test_bytestring_with_unicode(self):
-        d = {"a": [u"\u05d0", 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
+        d = {"a": [six.u("\u05d0"), 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]}
         idx = pd.DataFrame(d).set_index(["a", "b"]).index
 
         if py3compat.PY3:
diff --git a/pandas/tests/test_indexing.py b/pandas/tests/test_indexing.py
index f0ace52f2..b72b1f387 100644
--- a/pandas/tests/test_indexing.py
+++ b/pandas/tests/test_indexing.py
@@ -1,8 +1,9 @@
 # pylint: disable-msg=W0612,E1101
+from pandas.util.py3compat import range
 import unittest
 import nose
 import itertools
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
 
 from numpy import random, nan
 from numpy.random import randn
@@ -21,6 +22,7 @@ import pandas.util.testing as tm
 import pandas.lib as lib
 from pandas import date_range
 from numpy.testing.decorators import slow
+from six.moves import map
 
 _verbose = False
 
@@ -36,7 +38,7 @@ def _generate_indices(f, values=False):
 
     axes = f.axes
     if values:
-        axes = [ range(len(a)) for a in axes ]
+        axes = [ list(range(len(a))) for a in axes ]
 
     return itertools.product(*axes)
 
@@ -94,9 +96,9 @@ class TestIndexing(unittest.TestCase):
         import warnings
         warnings.filterwarnings(action='ignore', category=FutureWarning)
 
-        self.series_ints   = Series(np.random.rand(4), index=range(0,8,2))
-        self.frame_ints    = DataFrame(np.random.randn(4, 4), index=range(0, 8, 2), columns=range(0,12,3))
-        self.panel_ints    = Panel(np.random.rand(4,4,4), items=range(0,8,2),major_axis=range(0,12,3),minor_axis=range(0,16,4))
+        self.series_ints   = Series(np.random.rand(4), index=list(range(0,8,2)))
+        self.frame_ints    = DataFrame(np.random.randn(4, 4), index=list(range(0, 8, 2)), columns=list(range(0,12,3)))
+        self.panel_ints    = Panel(np.random.rand(4,4,4), items=list(range(0,8,2)),major_axis=list(range(0,12,3)),minor_axis=list(range(0,16,4)))
 
         self.series_labels = Series(np.random.randn(4), index=list('abcd'))
         self.frame_labels  = DataFrame(np.random.randn(4, 4), index=list('abcd'), columns=list('ABCD'))
@@ -201,11 +203,11 @@ class TestIndexing(unittest.TestCase):
 
                 _print(result)
 
-            except (AssertionError):
+            except AssertionError:
                 raise
-            except (TypeError):
+            except TypeError:
                 raise AssertionError(_print('type error'))
-            except (Exception), detail:
+            except Exception as detail:
 
                 # if we are in fails, the ok, otherwise raise it
                 if fails is not None:
@@ -342,7 +344,7 @@ class TestIndexing(unittest.TestCase):
     def test_iloc_getitem_array(self):
 
         # array like
-        s = Series(index=range(1,4))
+        s = Series(index=list(range(1,4)))
         self.check_result('array like', 'iloc', s.index, 'ix', { 0 : [2,4,6], 1 : [3,6,9], 2: [4,8,12] }, typs = ['ints'])
 
     def test_iloc_getitem_bool(self):
@@ -547,7 +549,7 @@ class TestIndexing(unittest.TestCase):
 
     def test_iloc_getitem_frame(self):
         """ originally from test_frame.py"""
-        df = DataFrame(np.random.randn(10, 4), index=range(0, 20, 2), columns=range(0,8,2))
+        df = DataFrame(np.random.randn(10, 4), index=list(range(0, 20, 2)), columns=list(range(0,8,2)))
 
         result = df.iloc[2]
         exp = df.ix[4]
@@ -586,7 +588,7 @@ class TestIndexing(unittest.TestCase):
         assert_frame_equal(result, expected)
 
         # with index-like
-        s = Series(index=range(1,5))
+        s = Series(index=list(range(1,5)))
         result = df.iloc[s.index]
         expected = df.ix[[2,4,6,8]]
         assert_frame_equal(result, expected)
@@ -633,7 +635,7 @@ class TestIndexing(unittest.TestCase):
         assert_frame_equal(result, expected)
 
     def test_iloc_setitem_series(self):
-        s = Series(np.random.randn(10), index=range(0,20,2))
+        s = Series(np.random.randn(10), index=list(range(0,20,2)))
 
         s.iloc[1] = 1
         result = s.iloc[1]
@@ -796,7 +798,7 @@ class TestIndexing(unittest.TestCase):
 
         # GH 3561, dups not in selected order
         ind = ['A', 'A', 'B', 'C']
-        df = DataFrame({'test':range(len(ind))}, index=ind)
+        df = DataFrame({'test':list(range(len(ind)))}, index=ind)
         rows = ['C', 'B']
         res = df.ix[rows]
         self.assert_(rows == list(res.index))
@@ -878,8 +880,8 @@ class TestIndexing(unittest.TestCase):
         # GH 3626, an assignement of a sub-df to a df
         df = DataFrame({'FC':['a','b','a','b','a','b'],
                         'PF':[0,0,0,0,1,1],
-                        'col1':range(6),
-                        'col2':range(6,12)})
+                        'col1':list(range(6)),
+                        'col2':list(range(6,12))})
         df.ix[1,0]=np.nan
         df2 = df.copy()
 
@@ -918,7 +920,7 @@ class TestIndexing(unittest.TestCase):
         assert_series_equal(df.B, orig + 1)
 
         # GH 3668, mixed frame with series value
-        df = DataFrame({'x':range(10), 'y':range(10,20),'z' : 'bar'})
+        df = DataFrame({'x':list(range(10)), 'y':list(range(10,20)),'z' : 'bar'})
         expected = df.copy()
         expected.ix[0, 'y'] = 1000
         expected.ix[2, 'y'] = 1200
@@ -932,10 +934,10 @@ class TestIndexing(unittest.TestCase):
     def test_iloc_mask(self):
 
         # GH 3631, iloc with a mask (of a series) should raise
-        df = DataFrame(range(5), list('ABCDE'), columns=['a'])
+        df = DataFrame(list(range(5)), list('ABCDE'), columns=['a'])
         mask = (df.a%2 == 0)
         self.assertRaises(ValueError, df.iloc.__getitem__, tuple([mask]))
-        mask.index = range(len(mask))
+        mask.index = list(range(len(mask)))
         self.assertRaises(NotImplementedError, df.iloc.__getitem__, tuple([mask]))
 
         # ndarray ok
@@ -945,7 +947,7 @@ class TestIndexing(unittest.TestCase):
         # the possibilities
         locs = np.arange(4)
         nums = 2**locs
-        reps = map(bin, nums)
+        reps = list(map(bin, nums))
         df = DataFrame({'locs':locs, 'nums':nums}, reps)
 
         expected = {
@@ -974,7 +976,7 @@ class TestIndexing(unittest.TestCase):
                     else:
                         accessor = df
                     ans = str(bin(accessor[mask]['nums'].sum()))
-                except Exception, e:
+                except Exception as e:
                     ans = str(e)
 
                 key = tuple([idx,method])
@@ -1042,7 +1044,7 @@ class TestIndexing(unittest.TestCase):
 
         #GH 4017, non-unique indexing (on the axis)
         df = DataFrame({'A' : [0.1] * 3000, 'B' : [1] * 3000})
-        idx = np.array(range(30)) * 99
+        idx = np.array(list(range(30))) * 99
         expected = df.iloc[idx]
 
         df3 = pd.concat([df, 2*df, 3*df])
@@ -1109,7 +1111,7 @@ class TestIndexing(unittest.TestCase):
 
         columns = list('ABCDEFG')
         def gen_test(l,l2):
-            return pd.concat([ DataFrame(randn(l,len(columns)),index=range(l),columns=columns),
+            return pd.concat([ DataFrame(randn(l,len(columns)),index=list(range(l)),columns=columns),
                                DataFrame(np.ones((l2,len(columns))),index=[0]*l2,columns=columns) ])
 
 
diff --git a/pandas/tests/test_internals.py b/pandas/tests/test_internals.py
index 0f3b8c163..b1d29a97b 100644
--- a/pandas/tests/test_internals.py
+++ b/pandas/tests/test_internals.py
@@ -11,6 +11,8 @@ import pandas.util.testing as tm
 
 from pandas.util.testing import (
     assert_almost_equal, assert_frame_equal, randn)
+import six
+from six.moves import zip
 
 
 def assert_block_equal(left, right):
@@ -199,7 +201,7 @@ class TestBlock(unittest.TestCase):
         mat = np.empty((N, 2), dtype=object)
         mat[:, 0] = 'foo'
         mat[:, 1] = 'bar'
-        cols = ['b', u"\u05d0"]
+        cols = ['b', six.u("\u05d0")]
         str_repr = repr(make_block(mat.T, cols, TEST_COLS))
 
     def test_get(self):
@@ -385,7 +387,7 @@ class TestBlockManager(unittest.TestCase):
             self.assert_(tmgr.as_matrix().dtype == np.dtype(t))
 
     def test_convert(self):
-        
+
         def _compare(old_mgr, new_mgr):
             """ compare the blocks, numeric compare ==, object don't """
             old_blocks = set(old_mgr.blocks)
@@ -440,7 +442,7 @@ class TestBlockManager(unittest.TestCase):
         _check(new_mgr,FloatBlock,['b','g'])
         _check(new_mgr,IntBlock,['a','f'])
 
-        mgr = create_blockmanager([b, get_int_ex(['f'],np.int32), get_bool_ex(['bool']), get_dt_ex(['dt']), 
+        mgr = create_blockmanager([b, get_int_ex(['f'],np.int32), get_bool_ex(['bool']), get_dt_ex(['dt']),
                                    get_int_ex(['i'],np.int64), get_float_ex(['g'],np.float64), get_float_ex(['h'],np.float16)])
         new_mgr = mgr.convert(convert_numeric = True)
 
@@ -535,7 +537,7 @@ class TestBlockManager(unittest.TestCase):
     def test_missing_unicode_key(self):
         df = DataFrame({"a": [1]})
         try:
-            df.ix[:, u"\u05d0"]  # should not raise UnicodeEncodeError
+            df.ix[:, six.u("\u05d0")]  # should not raise UnicodeEncodeError
         except KeyError:
             pass  # this is the expected exception
 
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index d852bad21..d152e6ed1 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -1,5 +1,6 @@
 # pylint: disable-msg=W0612,E1101,W0141
 from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
 import nose
 import unittest
 
@@ -18,6 +19,8 @@ from pandas.util.compat import product as cart_product
 import pandas as pd
 
 import pandas.index as _index
+import six
+from six.moves import zip
 
 
 class TestMultiLevel(unittest.TestCase):
@@ -43,7 +46,7 @@ class TestMultiLevel(unittest.TestCase):
         # create test series object
         arrays = [['bar', 'bar', 'baz', 'baz', 'qux', 'qux', 'foo', 'foo'],
                   ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         index = MultiIndex.from_tuples(tuples)
         s = Series(randn(8), index=index)
         s[3] = np.NaN
@@ -89,7 +92,7 @@ class TestMultiLevel(unittest.TestCase):
                                   ['x', 'y', 'x', 'y']])
         self.assert_(isinstance(multi.index, MultiIndex))
 
-        multi = Series(range(4), index=[['a', 'a', 'b', 'b'],
+        multi = Series(list(range(4)), index=[['a', 'a', 'b', 'b'],
                                         ['x', 'y', 'x', 'y']])
         self.assert_(isinstance(multi.index, MultiIndex))
 
@@ -349,8 +352,8 @@ class TestMultiLevel(unittest.TestCase):
 
     def test_getitem_tuple_plus_slice(self):
         # GH #671
-        df = DataFrame({'a': range(10),
-                        'b': range(10),
+        df = DataFrame({'a': list(range(10)),
+                        'b': list(range(10)),
                         'c': np.random.randn(10),
                         'd': np.random.randn(10)})
 
@@ -429,7 +432,7 @@ class TestMultiLevel(unittest.TestCase):
 
     def test_xs_level_multiple(self):
         from pandas import read_table
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         text = """                      A       B       C       D        E
 one two three   four
 a   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640
@@ -443,7 +446,7 @@ x   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838"""
         assert_frame_equal(result, expected)
 
         # GH2107
-        dates = range(20111201, 20111205)
+        dates = list(range(20111201, 20111205))
         ids = 'abcde'
         idx = MultiIndex.from_tuples([x for x in cart_product(dates, ids)])
         idx.names = ['date', 'secid']
@@ -454,7 +457,7 @@ x   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838"""
 
     def test_xs_level0(self):
         from pandas import read_table
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         text = """                      A       B       C       D        E
 one two three   four
 a   b   10.0032 5    -0.5109 -2.3358 -0.4645  0.05076  0.3640
@@ -588,7 +591,7 @@ x   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838"""
 
         # with integer labels
         df = self.frame.copy()
-        df.columns = range(3)
+        df.columns = list(range(3))
         df.ix[('bar', 'two'), 1] = 7
         self.assertEquals(df.ix[('bar', 'two'), 1], 7)
 
@@ -950,8 +953,8 @@ Thur,Lunch,Yes,51.51,17"""
 
     def test_stack_dropna(self):
         # GH #3997
-        df = pd.DataFrame({'A': ['a1', 'a2'], 
-                           'B': ['b1', 'b2'], 
+        df = pd.DataFrame({'A': ['a1', 'a2'],
+                           'B': ['b1', 'b2'],
                            'C': [1, 1]})
         df = df.set_index(['A', 'B'])
 
@@ -1167,7 +1170,7 @@ Thur,Lunch,Yes,51.51,17"""
     def test_series_getitem_not_sorted(self):
         arrays = [['bar', 'bar', 'baz', 'baz', 'qux', 'qux', 'foo', 'foo'],
                  ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         index = MultiIndex.from_tuples(tuples)
         s = Series(randn(8), index=index)
 
@@ -1211,7 +1214,7 @@ Thur,Lunch,Yes,51.51,17"""
 
     def test_series_group_min_max(self):
         for op, level, skipna in cart_product(self.AGG_FUNCTIONS,
-                                              range(2),
+                                              list(range(2)),
                                               [False, True]):
             grouped = self.series.groupby(level=level)
             aggf = lambda x: getattr(x, op)(skipna=skipna)
@@ -1225,7 +1228,7 @@ Thur,Lunch,Yes,51.51,17"""
         self.frame.ix[7, [0, 1]] = np.nan
 
         for op, level, axis, skipna in cart_product(self.AGG_FUNCTIONS,
-                                                    range(2), range(2),
+                                                    list(range(2)), list(range(2)),
                                                     [False, True]):
             if axis == 0:
                 frame = self.frame
@@ -1496,7 +1499,7 @@ Thur,Lunch,Yes,51.51,17"""
                   ['', 'OD', 'OD', 'result1', 'result2', 'result1'],
                   ['', 'wx', 'wy', '', '', '']]
 
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         tuples.sort()
         index = MultiIndex.from_tuples(tuples)
         df = DataFrame(randn(4, 6), columns=index)
@@ -1516,7 +1519,7 @@ Thur,Lunch,Yes,51.51,17"""
                   ['', 'OD', 'OD', 'result1', 'result2', 'result1'],
                   ['', 'wx', 'wy', '', '', '']]
 
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         tuples.sort()
         index = MultiIndex.from_tuples(tuples)
         df = DataFrame(randn(4, 6), columns=index)
@@ -1532,7 +1535,7 @@ Thur,Lunch,Yes,51.51,17"""
                   ['', 'OD', 'OD', 'result1', 'result2', 'result1'],
                   ['', 'wx', 'wy', '', '', '']]
 
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         tuples.sort()
         index = MultiIndex.from_tuples(tuples)
         df = DataFrame(randn(4, 6), columns=index)
@@ -1584,7 +1587,7 @@ Thur,Lunch,Yes,51.51,17"""
                   ['', 'OD', 'OD', 'result1', 'result2', 'result1'],
                   ['', 'wx', 'wy', '', '', '']]
 
-        tuples = zip(*arrays)
+        tuples = list(zip(*arrays))
         tuples.sort()
         index = MultiIndex.from_tuples(tuples)
         df = DataFrame(randn(4, 6), columns=index)
@@ -1677,7 +1680,7 @@ Thur,Lunch,Yes,51.51,17"""
         self.assert_(result.index.names == ['one', 'two'])
 
     def test_unicode_repr_issues(self):
-        levels = [Index([u'a/\u03c3', u'b/\u03c3', u'c/\u03c3']),
+        levels = [Index([six.u('a/\u03c3'), six.u('b/\u03c3'), six.u('c/\u03c3')]),
                   Index([0, 1])]
         labels = [np.arange(3).repeat(2), np.tile(np.arange(2), 3)]
         index = MultiIndex(levels=levels, labels=labels)
@@ -1689,9 +1692,9 @@ Thur,Lunch,Yes,51.51,17"""
 
     def test_unicode_repr_level_names(self):
         index = MultiIndex.from_tuples([(0, 0), (1, 1)],
-                                       names=[u'\u0394', 'i1'])
+                                       names=[six.u('\u0394'), 'i1'])
 
-        s = Series(range(2), index=index)
+        s = Series(list(range(2)), index=index)
         df = DataFrame(np.random.randn(2, 4), index=index)
         repr(s)
         repr(df)
diff --git a/pandas/tests/test_panel.py b/pandas/tests/test_panel.py
index 5d1053289..d8c45ed65 100644
--- a/pandas/tests/test_panel.py
+++ b/pandas/tests/test_panel.py
@@ -1,6 +1,8 @@
 # pylint: disable=W0612,E1101
 
 from datetime import datetime
+from pandas.util.py3compat import range
+from pandas.util import compat
 import operator
 import unittest
 import nose
@@ -269,12 +271,12 @@ class SafeForSparse(object):
         tm.equalContents(self.panel.keys(), self.panel.items)
 
     def test_iteritems(self):
-        # Test panel.iteritems(), aka panel.iterkv()
+        # Test panel.iteritems(), aka panel.iteritems()
         # just test that it works
-        for k, v in self.panel.iterkv():
+        for k, v in self.panel.iteritems():
             pass
 
-        self.assertEqual(len(list(self.panel.iterkv())),
+        self.assertEqual(len(list(self.panel.iteritems())),
                          len(self.panel.items))
 
     def test_combineFrame(self):
@@ -390,7 +392,7 @@ class CheckIndexing(object):
         values[1] = 1
         values[2] = 2
 
-        panel = Panel(values, range(3), range(3), range(3))
+        panel = Panel(values, list(range(3)), list(range(3)), list(range(3)))
 
         # did we delete the right row?
 
@@ -811,8 +813,8 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
 
     def test_constructor_observe_dtype(self):
         # GH #411
-        panel = Panel(items=range(3), major_axis=range(3),
-                      minor_axis=range(3), dtype='O')
+        panel = Panel(items=list(range(3)), major_axis=list(range(3)),
+                      minor_axis=list(range(3)), dtype='O')
         self.assert_(panel.values.dtype == np.object_)
 
     def test_constructor_dtypes(self):
@@ -824,19 +826,19 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
 
         # only nan holding types allowed here
         for dtype in ['float64','float32','object']:
-            panel = Panel(items=range(2),major_axis=range(10),minor_axis=range(5),dtype=dtype)
+            panel = Panel(items=list(range(2)),major_axis=list(range(10)),minor_axis=list(range(5)),dtype=dtype)
             _check_dtype(panel,dtype)
 
         for dtype in ['float64','float32','int64','int32','object']:
-            panel = Panel(np.array(np.random.randn(2,10,5),dtype=dtype),items=range(2),major_axis=range(10),minor_axis=range(5),dtype=dtype)
+            panel = Panel(np.array(np.random.randn(2,10,5),dtype=dtype),items=list(range(2)),major_axis=list(range(10)),minor_axis=list(range(5)),dtype=dtype)
             _check_dtype(panel,dtype)
 
         for dtype in ['float64','float32','int64','int32','object']:
-            panel = Panel(np.array(np.random.randn(2,10,5),dtype='O'),items=range(2),major_axis=range(10),minor_axis=range(5),dtype=dtype)
+            panel = Panel(np.array(np.random.randn(2,10,5),dtype='O'),items=list(range(2)),major_axis=list(range(10)),minor_axis=list(range(5)),dtype=dtype)
             _check_dtype(panel,dtype)
 
         for dtype in ['float64','float32','int64','int32','object']:
-            panel = Panel(np.random.randn(2,10,5),items=range(2),major_axis=range(10),minor_axis=range(5),dtype=dtype)
+            panel = Panel(np.random.randn(2,10,5),items=list(range(2)),major_axis=list(range(10)),minor_axis=list(range(5)),dtype=dtype)
             _check_dtype(panel,dtype)
 
     def test_consolidate(self):
@@ -892,7 +894,7 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
         assert_panel_equal(result, expected)
 
     def test_constructor_dict_mixed(self):
-        data = dict((k, v.values) for k, v in self.panel.iterkv())
+        data = dict((k, v.values) for k, v in self.panel.iteritems())
         result = Panel(data)
         exp_major = Index(np.arange(len(self.panel.major_axis)))
         self.assert_(result.major_axis.equals(exp_major))
@@ -961,15 +963,15 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
     def test_constructor_error_msgs(self):
 
         def testit():
-            Panel(np.random.randn(3,4,5), range(4), range(5), range(5))
+            Panel(np.random.randn(3,4,5), list(range(4)), list(range(5)), list(range(5)))
         assertRaisesRegexp(ValueError, "Shape of passed values is \(3, 4, 5\), indices imply \(4, 5, 5\)", testit)
 
         def testit():
-            Panel(np.random.randn(3,4,5), range(5), range(4), range(5))
+            Panel(np.random.randn(3,4,5), list(range(5)), list(range(4)), list(range(5)))
         assertRaisesRegexp(ValueError, "Shape of passed values is \(3, 4, 5\), indices imply \(5, 4, 5\)", testit)
 
         def testit():
-            Panel(np.random.randn(3,4,5), range(5), range(5), range(4))
+            Panel(np.random.randn(3,4,5), list(range(5)), list(range(5)), list(range(4)))
         assertRaisesRegexp(ValueError, "Shape of passed values is \(3, 4, 5\), indices imply \(5, 5, 4\)", testit)
 
     def test_conform(self):
@@ -1282,7 +1284,7 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
         # negative numbers, #2164
         result = self.panel.shift(-1)
         expected = Panel(dict((i, f.shift(-1)[:-1])
-                              for i, f in self.panel.iterkv()))
+                              for i, f in self.panel.iteritems()))
         assert_panel_equal(result, expected)
 
     def test_multiindex_get(self):
@@ -1381,7 +1383,7 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
                 except ImportError:
                     raise nose.SkipTest
 
-                for item, df in self.panel.iterkv():
+                for item, df in self.panel.iteritems():
                     recdf = reader.parse(str(item), index_col=0)
                     assert_frame_equal(df, recdf)
 
diff --git a/pandas/tests/test_panel4d.py b/pandas/tests/test_panel4d.py
index 9c3a66c32..4119d2b5a 100644
--- a/pandas/tests/test_panel4d.py
+++ b/pandas/tests/test_panel4d.py
@@ -1,4 +1,5 @@
 from datetime import datetime
+from pandas.util.py3compat import range
 import os
 import operator
 import unittest
@@ -218,12 +219,9 @@ class SafeForSparse(object):
         tm.equalContents(self.panel4d.keys(), self.panel4d.labels)
 
     def test_iteritems(self):
-        """Test panel4d.iteritems(), aka panel4d.iterkv()"""
-        # just test that it works
-        for k, v in self.panel4d.iterkv():
-            pass
+        """Test panel4d.iteritems()"""
 
-        self.assertEqual(len(list(self.panel4d.iterkv())),
+        self.assertEqual(len(list(self.panel4d.iteritems())),
                          len(self.panel4d.labels))
 
     def test_combinePanel4d(self):
@@ -308,7 +306,7 @@ class CheckIndexing(object):
         values[2] = 2
         values[3] = 3
 
-        panel4d = Panel4D(values, range(4), range(4), range(4), range(4))
+        panel4d = Panel4D(values, list(range(4)), list(range(4)), list(range(4)), list(range(4)))
 
         # did we delete the right row?
 
@@ -610,8 +608,8 @@ class TestPanel4d(unittest.TestCase, CheckIndexing, SafeForSparse,
 
     def test_constructor_observe_dtype(self):
         # GH #411
-        panel = Panel(items=range(3), major_axis=range(3),
-                      minor_axis=range(3), dtype='O')
+        panel = Panel(items=list(range(3)), major_axis=list(range(3)),
+                      minor_axis=list(range(3)), dtype='O')
         self.assert_(panel.values.dtype == np.object_)
 
     def test_consolidate(self):
@@ -658,7 +656,7 @@ class TestPanel4d(unittest.TestCase, CheckIndexing, SafeForSparse,
         # assert_panel_equal(result, expected)
 
     def test_constructor_dict_mixed(self):
-        data = dict((k, v.values) for k, v in self.panel4d.iterkv())
+        data = dict((k, v.values) for k, v in self.panel4d.iteritems())
         result = Panel4D(data)
         exp_major = Index(np.arange(len(self.panel4d.major_axis)))
         self.assert_(result.major_axis.equals(exp_major))
@@ -721,7 +719,7 @@ class TestPanel4d(unittest.TestCase, CheckIndexing, SafeForSparse,
 
     def test_values(self):
         self.assertRaises(Exception, Panel, np.random.randn(5, 5, 5),
-                          range(5), range(5), range(4))
+                          list(range(5)), list(range(5)), list(range(4)))
 
     def test_conform(self):
         p = self.panel4d['l1'].filter(items=['ItemA', 'ItemB'])
diff --git a/pandas/tests/test_reshape.py b/pandas/tests/test_reshape.py
index b24e09723..1228e1605 100644
--- a/pandas/tests/test_reshape.py
+++ b/pandas/tests/test_reshape.py
@@ -1,7 +1,8 @@
 # pylint: disable-msg=W0612,E1101
 from copy import deepcopy
 from datetime import datetime, timedelta
-from StringIO import StringIO
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
 import cPickle as pickle
 import operator
 import os
diff --git a/pandas/tests/test_rplot.py b/pandas/tests/test_rplot.py
index 0f429bf71..18f0c76b4 100644
--- a/pandas/tests/test_rplot.py
+++ b/pandas/tests/test_rplot.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import unittest
 import pandas.tools.rplot as rplot
 from pandas import read_csv
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index cbf7fb070..f53b62474 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -1,6 +1,9 @@
 # pylint: disable-msg=E1101,W0612
 
 from datetime import datetime, timedelta, date
+from pandas.util.py3compat import range
+from six.moves import zip
+from pandas.util import compat
 import os
 import operator
 import unittest
@@ -29,6 +32,7 @@ from pandas.util.testing import (assert_series_equal,
                                  assert_almost_equal,
                                  ensure_clean)
 import pandas.util.testing as tm
+import six
 
 
 def _skip_if_no_scipy():
@@ -128,8 +132,8 @@ class CheckNameIntegration(object):
         self.assert_((result == 5).all())
 
     def test_getitem_negative_out_of_bounds(self):
-        s = Series([tm.rands(5) for _ in xrange(10)],
-                   index=[tm.rands(10) for _ in xrange(10)])
+        s = Series([tm.rands(5) for _ in range(10)],
+                   index=[tm.rands(10) for _ in range(10)])
 
         self.assertRaises(IndexError, s.__getitem__, -11)
         self.assertRaises(IndexError, s.__setitem__, -11, 'foo')
@@ -140,7 +144,7 @@ class CheckNameIntegration(object):
                            labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],
                                    [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],
                            names=['first', 'second'])
-        s = Series(range(0, len(index)), index=index, name='sth')
+        s = Series(list(range(0, len(index))), index=index, name='sth')
         expected = ["first  second",
                     "foo    one       0",
                     "       two       1",
@@ -177,7 +181,7 @@ class CheckNameIntegration(object):
         s.name = None
         self.assert_(not "Name:" in repr(s))
         # test big series (diff code path)
-        s = Series(range(0, 1000))
+        s = Series(list(range(0, 1000)))
         s.name = "test"
         self.assert_("Name: test" in repr(s))
         s.name = None
@@ -231,7 +235,7 @@ class TestNanops(unittest.TestCase):
 
     def test_none_comparison(self):
         # bug brought up by #1079
-        s = Series(np.random.randn(10), index=range(0, 20, 2))
+        s = Series(np.random.randn(10), index=list(range(0, 20, 2)))
         self.assertRaises(TypeError, s.__eq__, None)
 
     def test_sum_zero(self):
@@ -320,8 +324,8 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         empty2 = Series([])
         assert_series_equal(empty, empty2)
 
-        empty = Series(index=range(10))
-        empty2 = Series(np.nan, index=range(10))
+        empty = Series(index=list(range(10)))
+        empty2 = Series(np.nan, index=list(range(10)))
         assert_series_equal(empty, empty2)
 
     def test_constructor_series(self):
@@ -336,12 +340,12 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         gen = (i for i in range(10))
 
         result = Series(gen)
-        exp = Series(range(10))
+        exp = Series(list(range(10)))
         assert_series_equal(result, exp)
 
         gen = (i for i in range(10))
-        result = Series(gen, index=range(10, 20))
-        exp.index = range(10, 20)
+        result = Series(gen, index=list(range(10, 20)))
+        exp.index = list(range(10, 20))
         assert_series_equal(result, exp)
 
     def test_constructor_maskedarray(self):
@@ -434,10 +438,10 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertEquals(s.dtype, np.dtype('f8'))
 
     def test_constructor_pass_none(self):
-        s = Series(None, index=range(5))
+        s = Series(None, index=list(range(5)))
         self.assert_(s.dtype == np.float64)
 
-        s = Series(None, index=range(5), dtype=object)
+        s = Series(None, index=list(range(5)), dtype=object)
         self.assert_(s.dtype == np.object_)
 
     def test_constructor_cast(self):
@@ -455,15 +459,15 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
     def test_constructor_dtype_datetime64(self):
         import pandas.tslib as tslib
 
-        s = Series(tslib.iNaT, dtype='M8[ns]', index=range(5))
+        s = Series(tslib.iNaT, dtype='M8[ns]', index=list(range(5)))
         self.assert_(isnull(s).all() == True)
 
         #### in theory this should be all nulls, but since
         #### we are not specifying a dtype is ambiguous
-        s = Series(tslib.iNaT, index=range(5))
+        s = Series(tslib.iNaT, index=list(range(5)))
         self.assert_(isnull(s).all() == False)
 
-        s = Series(nan, dtype='M8[ns]', index=range(5))
+        s = Series(nan, dtype='M8[ns]', index=list(range(5)))
         self.assert_(isnull(s).all() == True)
 
         s = Series([datetime(2001, 1, 2, 0, 0), tslib.iNaT], dtype='M8[ns]')
@@ -510,7 +514,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(result, expected)
 
     def test_constructor_subclass_dict(self):
-        data = tm.TestSubDict((x, 10.0 * x) for x in xrange(10))
+        data = tm.TestSubDict((x, 10.0 * x) for x in range(10))
         series = Series(data)
         refseries = Series(dict(data.iteritems()))
         assert_series_equal(refseries, series)
@@ -639,7 +643,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertRaises(KeyError, self.ts.__getitem__, d)
 
     def test_iget(self):
-        s = Series(np.random.randn(10), index=range(0, 20, 2))
+        s = Series(np.random.randn(10), index=list(range(0, 20, 2)))
         for i in range(len(s)):
             result = s.iget(i)
             exp = s[s.index[i]]
@@ -664,12 +668,12 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertEqual(s.iget(2), 2)
 
     def test_getitem_regression(self):
-        s = Series(range(5), index=range(5))
-        result = s[range(5)]
+        s = Series(list(range(5)), index=list(range(5)))
+        result = s[list(range(5))]
         assert_series_equal(result, s)
 
     def test_getitem_setitem_slice_bug(self):
-        s = Series(range(10), range(10))
+        s = Series(list(range(10)), list(range(10)))
         result = s[-12:]
         assert_series_equal(result, s)
 
@@ -679,7 +683,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         result = s[:-12]
         assert_series_equal(result, s[:0])
 
-        s = Series(range(10), range(10))
+        s = Series(list(range(10)), list(range(10)))
         s[-12:] = 0
         self.assert_((s == 0).all())
 
@@ -779,12 +783,12 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assert_(isinstance(value, np.float64))
 
     def test_getitem_ambiguous_keyerror(self):
-        s = Series(range(10), index=range(0, 20, 2))
+        s = Series(list(range(10)), index=list(range(0, 20, 2)))
         self.assertRaises(KeyError, s.__getitem__, 1)
         self.assertRaises(KeyError, s.ix.__getitem__, 1)
 
     def test_getitem_unordered_dup(self):
-        obj = Series(range(5), index=['c', 'a', 'a', 'b', 'b'])
+        obj = Series(list(range(5)), index=['c', 'a', 'a', 'b', 'b'])
         self.assert_(np.isscalar(obj['c']))
         self.assert_(obj['c'] == 0)
 
@@ -798,7 +802,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(result,expected)
 
     def test_setitem_ambiguous_keyerror(self):
-        s = Series(range(10), index=range(0, 20, 2))
+        s = Series(list(range(10)), index=list(range(0, 20, 2)))
         self.assertRaises(KeyError, s.__setitem__, 1, 5)
         self.assertRaises(KeyError, s.ix.__setitem__, 1, 5)
 
@@ -971,7 +975,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(result, expected)
 
         # integer indexes, be careful
-        s = Series(np.random.randn(10), index=range(0, 20, 2))
+        s = Series(np.random.randn(10), index=list(range(0, 20, 2)))
         inds = [0, 2, 5, 7, 8]
         arr_inds = np.array([0, 2, 5, 7, 8])
         result = s[inds]
@@ -998,7 +1002,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(cp, exp)
 
         # integer indexes, be careful
-        s = Series(np.random.randn(10), index=range(0, 20, 2))
+        s = Series(np.random.randn(10), index=list(range(0, 20, 2)))
         inds = [0, 4, 6]
         arr_inds = np.array([0, 4, 6])
 
@@ -1047,7 +1051,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertRaises(KeyError, ts2.ix.__setitem__, slice(d1, d2), 0)
 
     def test_ix_getitem_setitem_integer_slice_keyerrors(self):
-        s = Series(np.random.randn(10), index=range(0, 20, 2))
+        s = Series(np.random.randn(10), index=list(range(0, 20, 2)))
 
         # this is OK
         cp = s.copy()
@@ -1111,8 +1115,8 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         for dtype in [ np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64 ]:
             s = Series(np.arange(10), dtype=dtype)
             mask = s < 5
-            s[mask] = range(2,7)
-            expected = Series(range(2,7) + range(5,10), dtype=dtype)
+            s[mask] = list(range(2,7))
+            expected = Series(list(range(2,7)) + list(range(5,10)), dtype=dtype)
             assert_series_equal(s, expected)
             self.assertEquals(s.dtype, expected.dtype)
 
@@ -1122,7 +1126,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
             mask = s < 5
             values = [2.5,3.5,4.5,5.5,6.5]
             s[mask] = values
-            expected = Series(values + range(5,10), dtype='float64')
+            expected = Series(values + list(range(5,10)), dtype='float64')
             assert_series_equal(s, expected)
             self.assertEquals(s.dtype, expected.dtype)
 
@@ -1136,8 +1140,8 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         # GH3235
         s = Series(np.arange(10),dtype='int64')
         mask = s < 5
-        s[mask] = range(2,7)
-        expected = Series(range(2,7) + range(5,10),dtype='int64')
+        s[mask] = list(range(2,7))
+        expected = Series(list(range(2,7)) + list(range(5,10)),dtype='int64')
         assert_series_equal(s, expected)
         self.assertEquals(s.dtype, expected.dtype)
 
@@ -1286,13 +1290,13 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         repr(ots)
 
         # various names
-        for name in ['', 1, 1.2, 'foo', u'\u03B1\u03B2\u03B3',
+        for name in ['', 1, 1.2, 'foo', six.u('\u03B1\u03B2\u03B3'),
                      'loooooooooooooooooooooooooooooooooooooooooooooooooooong',
                      ('foo', 'bar', 'baz'),
                      (1, 2),
                      ('foo', 1, 2.3),
-                     (u'\u03B1', u'\u03B2', u'\u03B3'),
-                     (u'\u03B1', 'bar')]:
+                     (six.u('\u03B1'), six.u('\u03B2'), six.u('\u03B3')),
+                     (six.u('\u03B1'), 'bar')]:
             self.series.name = name
             repr(self.series)
 
@@ -1316,7 +1320,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertFalse("a\n" in repr(ser))
 
     def test_tidy_repr(self):
-        a = Series([u"\u05d0"] * 1000)
+        a = Series([six.u("\u05d0")] * 1000)
         a.name = 'title1'
         repr(a)         # should not raise exception
 
@@ -1341,7 +1345,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         # it works!
         repr(s)
 
-        s.name = (u"\u05d0",) * 2
+        s.name = (six.u("\u05d0"),) * 2
         repr(s)
 
     def test_repr_should_return_str(self):
@@ -1354,19 +1358,19 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
 
         """
         data = [8, 5, 3, 5]
-        index1 = [u"\u03c3", u"\u03c4", u"\u03c5", u"\u03c6"]
+        index1 = [six.u("\u03c3"), six.u("\u03c4"), six.u("\u03c5"), six.u("\u03c6")]
         df = Series(data, index=index1)
         self.assertTrue(type(df.__repr__() == str))  # both py2 / 3
 
     def test_unicode_string_with_unicode(self):
-        df = Series([u"\u05d0"], name=u"\u05d1")
+        df = Series([six.u("\u05d0")], name=six.u("\u05d1"))
         if py3compat.PY3:
             str(df)
         else:
-            unicode(df)
+            six.text_type(df)
 
     def test_bytestring_with_unicode(self):
-        df = Series([u"\u05d0"], name=u"\u05d1")
+        df = Series([six.u("\u05d0")], name=six.u("\u05d1"))
         if py3compat.PY3:
             bytes(df)
         else:
@@ -1447,7 +1451,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self._check_stat_op('median', np.median)
 
         # test with integers, test failure
-        int_ts = TimeSeries(np.ones(10, dtype=int), index=range(10))
+        int_ts = TimeSeries(np.ones(10, dtype=int), index=list(range(10)))
         self.assertAlmostEqual(np.median(int_ts), int_ts.median())
 
     def test_prod(self):
@@ -1508,11 +1512,11 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assert_(isnull(shifted[4]) == True)
 
         result = s.argsort()
-        expected = Series(range(5),dtype='int64')
+        expected = Series(list(range(5)),dtype='int64')
         assert_series_equal(result,expected)
 
         result = shifted.argsort()
-        expected = Series(range(4) + [-1],dtype='int64')
+        expected = Series(list(range(4)) + [-1],dtype='int64')
         assert_series_equal(result,expected)
 
     def test_argsort_stable(self):
@@ -1634,7 +1638,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assertEqual(result.name, self.ts.name)
 
     def test_prod_numpy16_bug(self):
-        s = Series([1., 1., 1.], index=range(3))
+        s = Series([1., 1., 1.], index=list(range(3)))
         result = s.prod()
         self.assert_(not isinstance(result, Series))
 
@@ -2343,7 +2347,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         import operator
 
         # GH 353
-        vals = Series([rands(5) for _ in xrange(10)])
+        vals = Series([rands(5) for _ in range(10)])
         result = 'foo_' + vals
         expected = vals.map(lambda x: 'foo_' + x)
         assert_series_equal(result, expected)
@@ -2620,9 +2624,9 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         assert_series_equal(hist, expected)
 
         # GH 3002, datetime64[ns]
-        import StringIO
+        from pandas.util.py3compat import StringIO
         import pandas as pd
-        f = StringIO.StringIO("xxyyzz20100101PIE\nxxyyzz20100101GUM\nxxyyww20090101EGG\nfoofoo20080909PIE")
+        f = StringIO("xxyyzz20100101PIE\nxxyyzz20100101GUM\nxxyyww20090101EGG\nfoofoo20080909PIE")
         df = pd.read_fwf(f, widths=[6,8,3], names=["person_id", "dt", "food"], parse_dates=["dt"])
         s = df.dt.copy()
         result = s.value_counts()
@@ -2671,7 +2675,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assert_(np.array_equal(result, expected))
 
         # test string arrays for coverage
-        strings = np.tile(np.array([tm.rands(10) for _ in xrange(10)]), 10)
+        strings = np.tile(np.array([tm.rands(10) for _ in range(10)]), 10)
         result = np.sort(nanops.unique1d(strings))
         expected = np.unique(strings)
         self.assert_(np.array_equal(result, expected))
@@ -2819,7 +2823,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
 
     def test_to_csv_unicode_index(self):
         buf = StringIO()
-        s = Series([u"\u05d0", "d2"], index=[u"\u05d0", u"\u05d1"])
+        s = Series([six.u("\u05d0"), "d2"], index=[six.u("\u05d0"), six.u("\u05d1")])
 
         s.to_csv(buf, encoding='UTF-8')
         buf.seek(0)
@@ -3343,7 +3347,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
     def test_astype_datetimes(self):
         import pandas.tslib as tslib
 
-        s = Series(tslib.iNaT, dtype='M8[ns]', index=range(5))
+        s = Series(tslib.iNaT, dtype='M8[ns]', index=list(range(5)))
         s = s.astype('O')
         self.assert_(s.dtype == np.object_)
 
@@ -3391,7 +3395,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         self.assert_(not isnull(merged['c']))
 
     def test_map_type_inference(self):
-        s = Series(range(3))
+        s = Series(list(range(3)))
         s2 = s.map(lambda x: np.where(x == 0, 0, 1))
         self.assert_(issubclass(s2.dtype.type, np.integer))
 
@@ -3938,7 +3942,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
     def test_fillna_invalid_method(self):
         try:
             self.ts.fillna(method='ffil')
-        except ValueError, inst:
+        except ValueError as inst:
             self.assert_('ffil' in str(inst))
 
     def test_ffill(self):
@@ -4024,7 +4028,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
 
         # malformed
         self.assertRaises(ValueError, ser.replace, [1, 2, 3], [np.nan, 0])
-        self.assertRaises(ValueError, ser.replace, xrange(1, 3), [np.nan, 0])
+        self.assertRaises(ValueError, ser.replace, range(1, 3), [np.nan, 0])
 
         ser = Series([0, 1, 2, 3, 4])
         result = ser.replace([0, 1, 2, 3, 4], [4, 3, 2, 1, 0])
@@ -4302,7 +4306,7 @@ class TestSeriesNonUnique(unittest.TestCase):
     def test_set_index_makes_timeseries(self):
         idx = tm.makeDateIndex(10)
 
-        s = Series(range(10))
+        s = Series(list(range(10)))
         s.index = idx
 
         self.assertTrue(isinstance(s, TimeSeries))
diff --git a/pandas/tests/test_stats.py b/pandas/tests/test_stats.py
index 0432d11aa..8dc582342 100644
--- a/pandas/tests/test_stats.py
+++ b/pandas/tests/test_stats.py
@@ -1,3 +1,4 @@
+from pandas.util import compat
 import nose
 import unittest
 
@@ -10,6 +11,7 @@ from pandas.util.compat import product
 from pandas.util.testing import (assert_frame_equal,
                                  assert_series_equal,
                                  assert_almost_equal)
+import six
 
 
 class TestRank(unittest.TestCase):
@@ -106,7 +108,7 @@ class TestRank(unittest.TestCase):
     def test_rank_int(self):
         s = self.s.dropna().astype('i8')
 
-        for method, res in self.results.iteritems():
+        for method, res in compat.iteritems(self.results):
             result = s.rank(method=method)
             expected = Series(res).dropna()
             expected.index = result.index
diff --git a/pandas/tests/test_strings.py b/pandas/tests/test_strings.py
index d057dc530..9a1d3bc71 100644
--- a/pandas/tests/test_strings.py
+++ b/pandas/tests/test_strings.py
@@ -1,6 +1,7 @@
 # pylint: disable-msg=E1101,W0612
 
 from datetime import datetime, timedelta, date
+from pandas.util.py3compat import range
 import os
 import operator
 import re
@@ -21,6 +22,7 @@ from pandas.util.testing import assert_series_equal, assert_almost_equal
 import pandas.util.testing as tm
 
 import pandas.core.strings as strings
+import six
 
 
 class TestStringMethods(unittest.TestCase):
@@ -41,8 +43,8 @@ class TestStringMethods(unittest.TestCase):
             assert_array_equal(s.index, ds.index)
 
             for el in s:
-                # each element of the series is either a basestring or nan
-                self.assert_(isinstance(el, basestring) or isnull(el))
+                # each element of the series is either a six.string_types or nan
+                self.assert_(isinstance(el, six.string_types) or isnull(el))
 
         # desired behavior is to iterate until everything would be nan on the
         # next iter so make sure the last element of the iterator was 'l' in
@@ -73,7 +75,7 @@ class TestStringMethods(unittest.TestCase):
 
     def test_iter_numeric_try_string(self):
         # behavior identical to empty series
-        dsi = Series(range(4))
+        dsi = Series(list(range(4)))
 
         i, s = 100, 'h'
 
@@ -93,7 +95,7 @@ class TestStringMethods(unittest.TestCase):
 
     def test_iter_object_try_string(self):
         ds = Series([slice(None, randint(10), randint(10, 20))
-                     for _ in xrange(4)])
+                     for _ in range(4)])
 
         i, s = 100, 'h'
 
@@ -154,7 +156,7 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = [u'foo', u'foofoo', NA, u'foooofooofommmfoo']
+        values = [six.u('foo'), six.u('foofoo'), NA, six.u('foooofooofommmfoo')]
 
         result = strings.str_count(values, 'f[o]+')
         exp = [1, 2, NA, 4]
@@ -189,7 +191,7 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = [u'foo', NA, u'fooommm__foo', u'mmm_']
+        values = [six.u('foo'), NA, six.u('fooommm__foo'), six.u('mmm_')]
         pat = 'mmm[_]+'
 
         result = strings.str_contains(values, pat)
@@ -229,8 +231,8 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'om', NA, u'foo_nom', u'nom', u'bar_foo', NA,
-                         u'foo'])
+        values = Series([six.u('om'), NA, six.u('foo_nom'), six.u('nom'), six.u('bar_foo'), NA,
+                         six.u('foo')])
 
         result = values.str.startswith('foo')
         exp = Series([False, NA, True, False, False, NA, True])
@@ -257,8 +259,8 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'om', NA, u'foo_nom', u'nom', u'bar_foo', NA,
-                         u'foo'])
+        values = Series([six.u('om'), NA, six.u('foo_nom'), six.u('nom'), six.u('bar_foo'), NA,
+                         six.u('foo')])
 
         result = values.str.endswith('foo')
         exp = Series([False, NA, False, False, True, NA, True])
@@ -282,10 +284,10 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(mixed, exp)
 
         # unicode
-        values = Series([u"FOO", NA, u"bar", u"Blurg"])
+        values = Series([six.u("FOO"), NA, six.u("bar"), six.u("Blurg")])
 
         results = values.str.title()
-        exp = Series([u"Foo", NA, u"Bar", u"Blurg"])
+        exp = Series([six.u("Foo"), NA, six.u("Bar"), six.u("Blurg")])
 
         tm.assert_series_equal(results, exp)
 
@@ -309,10 +311,10 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'om', NA, u'nom', u'nom'])
+        values = Series([six.u('om'), NA, six.u('nom'), six.u('nom')])
 
         result = values.str.upper()
-        exp = Series([u'OM', NA, u'NOM', u'NOM'])
+        exp = Series([six.u('OM'), NA, six.u('NOM'), six.u('NOM')])
         tm.assert_series_equal(result, exp)
 
         result = result.str.lower()
@@ -339,14 +341,14 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'fooBAD__barBAD', NA])
+        values = Series([six.u('fooBAD__barBAD'), NA])
 
         result = values.str.replace('BAD[_]*', '')
-        exp = Series([u'foobar', NA])
+        exp = Series([six.u('foobar'), NA])
         tm.assert_series_equal(result, exp)
 
         result = values.str.replace('BAD[_]*', '', n=1)
-        exp = Series([u'foobarBAD', NA])
+        exp = Series([six.u('foobarBAD'), NA])
         tm.assert_series_equal(result, exp)
 
         #flags + unicode
@@ -377,14 +379,17 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a', u'b', NA, u'c', NA, u'd'])
+        values = Series([six.u('a'), six.u('b'), NA, six.u('c'), NA,
+                         six.u('d')])
 
         result = values.str.repeat(3)
-        exp = Series([u'aaa', u'bbb', NA, u'ccc', NA, u'ddd'])
+        exp = Series([six.u('aaa'), six.u('bbb'), NA, six.u('ccc'), NA,
+                      six.u('ddd')])
         tm.assert_series_equal(result, exp)
 
         result = values.str.repeat([1, 2, 3, 4, 5, 6])
-        exp = Series([u'a', u'bb', NA, u'cccc', NA, u'dddddd'])
+        exp = Series([six.u('a'), six.u('bb'), NA, six.u('cccc'), NA,
+                      six.u('dddddd')])
         tm.assert_series_equal(result, exp)
 
     def test_match(self):
@@ -404,10 +409,10 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'fooBAD__barBAD', NA, u'foo'])
+        values = Series([six.u('fooBAD__barBAD'), NA, six.u('foo')])
 
         result = values.str.match('.*(BAD[_]+).*(BAD)')
-        exp = Series([(u'BAD__', u'BAD'), NA, []])
+        exp = Series([(six.u('BAD__'), six.u('BAD')), NA, []])
         tm.assert_series_equal(result, exp)
 
     def test_join(self):
@@ -426,7 +431,8 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a_b_c', u'c_d_e', np.nan, u'f_g_h'])
+        values = Series([six.u('a_b_c'), six.u('c_d_e'), np.nan,
+                         six.u('f_g_h')])
         result = values.str.split('_').str.join('_')
         tm.assert_series_equal(values, result)
 
@@ -448,7 +454,8 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'foo', u'fooo', u'fooooo', np.nan, u'fooooooo'])
+        values = Series([six.u('foo'), six.u('fooo'), six.u('fooooo'), np.nan,
+                         six.u('fooooooo')])
 
         result = values.str.len()
         exp = values.map(lambda x: len(x) if com.notnull(x) else NA)
@@ -472,10 +479,11 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'fooBAD__barBAD', NA, u'foo', u'BAD'])
+        values = Series([six.u('fooBAD__barBAD'), NA, six.u('foo'),
+                         six.u('BAD')])
 
         result = values.str.findall('BAD[_]*')
-        exp = Series([[u'BAD__', u'BAD'], NA, [], [u'BAD']])
+        exp = Series([[six.u('BAD__'), six.u('BAD')], NA, [], [six.u('BAD')]])
         tm.assert_almost_equal(result, exp)
 
     def test_pad(self):
@@ -522,18 +530,22 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a', u'b', NA, u'c', NA, u'eeeeee'])
+        values = Series([six.u('a'), six.u('b'), NA, six.u('c'), NA,
+                         six.u('eeeeee')])
 
         result = values.str.pad(5, side='left')
-        exp = Series([u'    a', u'    b', NA, u'    c', NA, u'eeeeee'])
+        exp = Series([six.u('    a'), six.u('    b'), NA, six.u('    c'), NA,
+                      six.u('eeeeee')])
         tm.assert_almost_equal(result, exp)
 
         result = values.str.pad(5, side='right')
-        exp = Series([u'a    ', u'b    ', NA, u'c    ', NA, u'eeeeee'])
+        exp = Series([six.u('a    '), six.u('b    '), NA, six.u('c    '), NA,
+                      six.u('eeeeee')])
         tm.assert_almost_equal(result, exp)
 
         result = values.str.pad(5, side='both')
-        exp = Series([u'  a  ', u'  b  ', NA, u'  c  ', NA, u'eeeeee'])
+        exp = Series([six.u('  a  '), six.u('  b  '), NA, six.u('  c  '), NA,
+                      six.u('eeeeee')])
         tm.assert_almost_equal(result, exp)
 
     def test_center(self):
@@ -555,10 +567,12 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a', u'b', NA, u'c', NA, u'eeeeee'])
+        values = Series([six.u('a'), six.u('b'), NA, six.u('c'), NA,
+                         six.u('eeeeee')])
 
         result = values.str.center(5)
-        exp = Series([u'  a  ', u'  b  ', NA, u'  c  ', NA, u'eeeeee'])
+        exp = Series([six.u('  a  '), six.u('  b  '), NA, six.u('  c  '), NA,
+                      six.u('eeeeee')])
         tm.assert_almost_equal(result, exp)
 
     def test_split(self):
@@ -585,11 +599,12 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a_b_c', u'c_d_e', NA, u'f_g_h'])
+        values = Series([six.u('a_b_c'), six.u('c_d_e'), NA, six.u('f_g_h')])
 
         result = values.str.split('_')
-        exp = Series([[u'a', u'b', u'c'], [u'c', u'd', u'e'], NA,
-                      [u'f', u'g', u'h']])
+        exp = Series([[six.u('a'), six.u('b'), six.u('c')],
+                      [six.u('c'), six.u('d'), six.u('e')], NA,
+                      [six.u('f'), six.u('g'), six.u('h')]])
         tm.assert_series_equal(result, exp)
 
     def test_split_noargs(self):
@@ -650,10 +665,11 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'aafootwo', u'aabartwo', NA, u'aabazqux'])
+        values = Series([six.u('aafootwo'), six.u('aabartwo'), NA,
+                         six.u('aabazqux')])
 
         result = values.str.slice(2, 5)
-        exp = Series([u'foo', u'bar', NA, u'baz'])
+        exp = Series([six.u('foo'), six.u('bar'), NA, six.u('baz')])
         tm.assert_series_equal(result, exp)
 
     def test_slice_replace(self):
@@ -702,18 +718,19 @@ class TestStringMethods(unittest.TestCase):
 
     def test_strip_lstrip_rstrip_unicode(self):
         # unicode
-        values = Series([u'  aa   ', u' bb \n', NA, u'cc  '])
+        values = Series([six.u('  aa   '), six.u(' bb \n'), NA,
+                         six.u('cc  ')])
 
         result = values.str.strip()
-        exp = Series([u'aa', u'bb', NA, u'cc'])
+        exp = Series([six.u('aa'), six.u('bb'), NA, six.u('cc')])
         tm.assert_series_equal(result, exp)
 
         result = values.str.lstrip()
-        exp = Series([u'aa   ', u'bb \n', NA, u'cc  '])
+        exp = Series([six.u('aa   '), six.u('bb \n'), NA, six.u('cc  ')])
         tm.assert_series_equal(result, exp)
 
         result = values.str.rstrip()
-        exp = Series([u'  aa', u' bb', NA, u'cc'])
+        exp = Series([six.u('  aa'), six.u(' bb'), NA, six.u('cc')])
         tm.assert_series_equal(result, exp)
 
     def test_strip_lstrip_rstrip_args(self):
@@ -732,17 +749,18 @@ class TestStringMethods(unittest.TestCase):
         assert_series_equal(rs, xp)
 
     def test_strip_lstrip_rstrip_args_unicode(self):
-        values = Series([u'xxABCxx', u'xx BNSD', u'LDFJH xx'])
+        values = Series([six.u('xxABCxx'), six.u('xx BNSD'),
+                         six.u('LDFJH xx')])
 
-        rs = values.str.strip(u'x')
+        rs = values.str.strip(six.u('x'))
         xp = Series(['ABC', ' BNSD', 'LDFJH '])
         assert_series_equal(rs, xp)
 
-        rs = values.str.lstrip(u'x')
+        rs = values.str.lstrip(six.u('x'))
         xp = Series(['ABCxx', ' BNSD', 'LDFJH xx'])
         assert_series_equal(rs, xp)
 
-        rs = values.str.rstrip(u'x')
+        rs = values.str.rstrip(six.u('x'))
         xp = Series(['xxABC', 'xx BNSD', 'LDFJH '])
         assert_series_equal(rs, xp)
 
@@ -768,10 +786,11 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_almost_equal(rs, xp)
 
         # unicode
-        values = Series([u'a_b_c', u'c_d_e', np.nan, u'f_g_h'])
+        values = Series([six.u('a_b_c'), six.u('c_d_e'), np.nan,
+                         six.u('f_g_h')])
 
         result = values.str.split('_').str.get(1)
-        expected = Series([u'b', u'd', np.nan, u'g'])
+        expected = Series([six.u('b'), six.u('d'), np.nan, six.u('g')])
         tm.assert_series_equal(result, expected)
 
     def test_more_contains(self):
@@ -872,7 +891,7 @@ class TestStringMethods(unittest.TestCase):
         self.assertEquals(result[0], True)
 
     def test_encode_decode(self):
-        base = Series([u'a', u'b', u'a\xe4'])
+        base = Series([six.u('a'), six.u('b'), six.u('a\xe4')])
         series = base.str.encode('utf-8')
 
         f = lambda x: x.decode('utf-8')
@@ -882,7 +901,7 @@ class TestStringMethods(unittest.TestCase):
         tm.assert_series_equal(result, exp)
 
     def test_encode_decode_errors(self):
-        encodeBase = Series([u'a', u'b', u'a\x9d'])
+        encodeBase = Series([six.u('a'), six.u('b'), six.u('a\x9d')])
 
         self.assertRaises(UnicodeEncodeError,
                           encodeBase.str.encode, 'cp1252')
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 54c00e798..22679d36b 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -1,3 +1,5 @@
+from pandas.util.py3compat import range
+from six.moves import zip
 import unittest
 
 from numpy import nan
@@ -30,7 +32,7 @@ class TestTseriesUtil(unittest.TestCase):
 
     def test_backfill(self):
         old = Index([1, 5, 10])
-        new = Index(range(12))
+        new = Index(list(range(12)))
 
         filler = algos.backfill_int64(old, new)
 
@@ -39,7 +41,7 @@ class TestTseriesUtil(unittest.TestCase):
 
         # corner case
         old = Index([1, 4])
-        new = Index(range(5, 10))
+        new = Index(list(range(5, 10)))
         filler = algos.backfill_int64(old, new)
 
         expect_filler = [-1, -1, -1, -1, -1]
@@ -47,7 +49,7 @@ class TestTseriesUtil(unittest.TestCase):
 
     def test_pad(self):
         old = Index([1, 5, 10])
-        new = Index(range(12))
+        new = Index(list(range(12)))
 
         filler = algos.pad_int64(old, new)
 
@@ -56,7 +58,7 @@ class TestTseriesUtil(unittest.TestCase):
 
         # corner case
         old = Index([5, 10])
-        new = Index(range(5))
+        new = Index(list(range(5)))
         filler = algos.pad_int64(old, new)
         expect_filler = [-1, -1, -1, -1, -1]
         self.assert_(np.array_equal(filler, expect_filler))
@@ -526,7 +528,7 @@ def test_group_ohlc():
         bins = np.array([6, 12], dtype=np.int64)
         out = np.zeros((3, 4), dtype)
         counts = np.zeros(len(out), dtype=np.int64)
-        
+
         func = getattr(algos,'group_ohlc_%s' % dtype)
         func(out, counts, obj[:, None], bins)
 
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index f96f3b98a..63d78ddde 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -2,7 +2,9 @@
 SQL-style merge routines
 """
 
-import itertools
+from pandas.util.py3compat import range, long
+from six.moves import zip
+import six
 import numpy as np
 import types
 from pandas.core.categorical import Categorical
@@ -441,7 +443,7 @@ def _get_join_indexers(left_keys, right_keys, sort=False, how='inner'):
         right_labels.append(rlab)
         group_sizes.append(count)
 
-    max_groups = 1L
+    max_groups = long(1)
     for x in group_sizes:
         max_groups *= long(x)
 
@@ -892,7 +894,7 @@ class _Concatenator(object):
             raise AssertionError('first argument must be a list-like of pandas '
                                  'objects, you passed an object of type '
                                  '"{0}"'.format(type(objs).__name__))
-        
+
         if join == 'outer':
             self.intersect = False
         elif join == 'inner':
@@ -959,7 +961,7 @@ class _Concatenator(object):
             name = com._consensus_name_attr(self.objs)
             return Series(new_data, index=self.new_axes[0], name=name)
         elif self._is_series:
-            data = dict(itertools.izip(xrange(len(self.objs)), self.objs))
+            data = dict(zip(range(len(self.objs)), self.objs))
             index, columns = self.new_axes
             tmpdf = DataFrame(data, index=index)
             if columns is not None:
@@ -1057,7 +1059,7 @@ class _Concatenator(object):
                 concat_items = indexer
             else:
                 concat_items = self.new_axes[0].take(indexer)
-                
+
             if self.ignore_index:
                 ref_items = self._get_fresh_axis()
                 return make_block(concat_values, concat_items, ref_items)
@@ -1134,7 +1136,7 @@ class _Concatenator(object):
                 raise AssertionError()
 
             # ufff...
-            indices = range(ndim)
+            indices = list(range(ndim))
             indices.remove(self.axis)
 
             for i, ax in zip(indices, self.join_axes):
@@ -1199,7 +1201,7 @@ def _concat_indexes(indexes):
 def _make_concat_multiindex(indexes, keys, levels=None, names=None):
     if ((levels is None and isinstance(keys[0], tuple)) or
             (levels is not None and len(levels) > 1)):
-        zipped = zip(*keys)
+        zipped = list(zip(*keys))
         if names is None:
             names = [None] * len(zipped)
 
@@ -1297,7 +1299,7 @@ def _make_concat_multiindex(indexes, keys, levels=None, names=None):
 
 
 def _should_fill(lname, rname):
-    if not isinstance(lname, basestring) or not isinstance(rname, basestring):
+    if not isinstance(lname, six.string_types) or not isinstance(rname, six.string_types):
         return True
     return lname == rname
 
diff --git a/pandas/tools/pivot.py b/pandas/tools/pivot.py
index 945f7fb4a..bc1ebd375 100644
--- a/pandas/tools/pivot.py
+++ b/pandas/tools/pivot.py
@@ -5,7 +5,11 @@ from pandas.core.index import MultiIndex
 from pandas.core.reshape import _unstack_multiple
 from pandas.tools.merge import concat
 from pandas.tools.util import cartesian_product
+from pandas.util.py3compat import range
+from pandas.util import compat
+import six
 import pandas.core.common as com
+from six.moves import zip
 import numpy as np
 
 
@@ -151,7 +155,7 @@ def _add_margins(table, data, values, rows=None, cols=None, aggfunc=np.mean):
     grand_margin = {}
     for k, v in data[values].iteritems():
         try:
-            if isinstance(aggfunc, basestring):
+            if isinstance(aggfunc, six.string_types):
                 grand_margin[k] = getattr(v, aggfunc)()
             else:
                 grand_margin[k] = aggfunc(v)
@@ -196,7 +200,7 @@ def _add_margins(table, data, values, rows=None, cols=None, aggfunc=np.mean):
         row_margin = row_margin.stack()
 
         # slight hack
-        new_order = [len(cols)] + range(len(cols))
+        new_order = [len(cols)] + list(range(len(cols)))
         row_margin.index = row_margin.index.reorder_levels(new_order)
     else:
         row_margin = Series(np.nan, index=result.columns)
diff --git a/pandas/tools/plotting.py b/pandas/tools/plotting.py
index 1ffdf83b0..483d989e9 100644
--- a/pandas/tools/plotting.py
+++ b/pandas/tools/plotting.py
@@ -1,5 +1,6 @@
 # being a bit too dynamic
 # pylint: disable=E1101
+import six
 import datetime
 import warnings
 import re
@@ -15,6 +16,8 @@ from pandas.tseries.index import DatetimeIndex
 from pandas.tseries.period import PeriodIndex, Period
 from pandas.tseries.frequencies import get_period_alias, get_base_alias
 from pandas.tseries.offsets import DateOffset
+from pandas.util.py3compat import range
+from six.moves import map, zip
 
 try:  # mpl optional
     import pandas.tseries.converter as conv
@@ -96,13 +99,13 @@ def _get_standard_colors(num_colors=None, colormap=None, color_type='default',
     import matplotlib.pyplot as plt
 
     if color is None and colormap is not None:
-        if isinstance(colormap, basestring):
+        if isinstance(colormap, six.string_types):
             import matplotlib.cm as cm
             cmap = colormap
             colormap = cm.get_cmap(colormap)
             if colormap is None:
                 raise ValueError("Colormap {0} is not recognized".format(cmap))
-        colors = map(colormap, np.linspace(0, 1, num=num_colors))
+        colors = list(map(colormap, np.linspace(0, 1, num=num_colors)))
     elif color is not None:
         if colormap is not None:
             warnings.warn("'color' and 'colormap' cannot be used "
@@ -111,7 +114,7 @@ def _get_standard_colors(num_colors=None, colormap=None, color_type='default',
     else:
         if color_type == 'default':
             colors = plt.rcParams.get('axes.color_cycle', list('bgrcmyk'))
-            if isinstance(colors, basestring):
+            if isinstance(colors, six.string_types):
                 colors = list(colors)
         elif color_type == 'random':
             import random
@@ -119,7 +122,7 @@ def _get_standard_colors(num_colors=None, colormap=None, color_type='default',
                 random.seed(column)
                 return [random.random() for _ in range(3)]
 
-            colors = map(random_color, range(num_colors))
+            colors = list(map(random_color, list(range(num_colors))))
         else:
             raise NotImplementedError
 
@@ -240,8 +243,8 @@ def scatter_matrix(frame, alpha=0.5, figsize=None, ax=None, grid=False,
 
     marker = _get_marker_compat(marker)
 
-    for i, a in zip(range(n), df.columns):
-        for j, b in zip(range(n), df.columns):
+    for i, a in zip(list(range(n)), df.columns):
+        for j, b in zip(list(range(n)), df.columns):
             ax = axes[i, j]
 
             if i == j:
@@ -500,7 +503,7 @@ def bootstrap_plot(series, fig=None, size=50, samples=500, **kwds):
                           for sampling in samplings])
     if fig is None:
         fig = plt.figure()
-    x = range(samples)
+    x = list(range(samples))
     axes = []
     ax1 = fig.add_subplot(2, 3, 1)
     ax1.set_xlabel("Sample")
@@ -598,7 +601,7 @@ def parallel_coordinates(data, class_column, cols=None, ax=None, colors=None,
             raise ValueError('Length of xticks must match number of columns')
         x = xticks
     else:
-        x = range(ncols)
+        x = list(range(ncols))
 
     if ax is None:
         ax = plt.gca()
@@ -681,7 +684,7 @@ def autocorrelation_plot(series, ax=None):
     def r(h):
         return ((data[:n - h] - mean) * (data[h:] - mean)).sum() / float(n) / c0
     x = np.arange(n) + 1
-    y = map(r, x)
+    y = list(map(r, x))
     z95 = 1.959963984540054
     z99 = 2.5758293035489004
     ax.axhline(y=z99 / np.sqrt(n), linestyle='--', color='grey')
@@ -984,7 +987,7 @@ class MPLPlot(object):
 
         if self._need_to_set_index:
             labels = [com.pprint_thing(key) for key in self.data.index]
-            labels = dict(zip(range(len(self.data.index)), labels))
+            labels = dict(zip(list(range(len(self.data.index))), labels))
 
             for ax_ in self.axes:
                 # ax_.set_xticks(self.xticks)
@@ -1035,9 +1038,9 @@ class MPLPlot(object):
                 x = self.data.index._mpl_repr()
             else:
                 self._need_to_set_index = True
-                x = range(len(index))
+                x = list(range(len(index)))
         else:
-            x = range(len(index))
+            x = list(range(len(index)))
 
         return x
 
@@ -1711,7 +1714,7 @@ def plot_series(series, label=None, kind='line', use_index=True, rot=None,
         if ax.get_yaxis().get_ticks_position().strip().lower() == 'right':
             fig = _gcf()
             axes = fig.get_axes()
-            for i in range(len(axes))[::-1]:
+            for i in reversed(range(len(axes))):
                 ax = axes[i]
                 ypos = ax.get_yaxis().get_ticks_position().strip().lower()
                 if ypos == 'left':
diff --git a/pandas/tools/rplot.py b/pandas/tools/rplot.py
index 43cbb9344..747d7bfb0 100644
--- a/pandas/tools/rplot.py
+++ b/pandas/tools/rplot.py
@@ -1,3 +1,5 @@
+from pandas.util.py3compat import range
+from six.moves import zip
 import numpy as np
 import random
 from copy import deepcopy
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
index b0261077f..0e6235438 100644
--- a/pandas/tools/tests/test_merge.py
+++ b/pandas/tools/tests/test_merge.py
@@ -1,5 +1,8 @@
 # pylint: disable=E1103
 
+from pandas.util.py3compat import range
+from six.moves import zip
+from pandas.util import compat
 import nose
 import unittest
 
@@ -26,7 +29,7 @@ JOIN_TYPES = ['inner', 'outer', 'left', 'right']
 
 
 def get_test_data(ngroups=NGROUPS, n=N):
-    unique_groups = range(ngroups)
+    unique_groups = list(range(ngroups))
     arr = np.asarray(np.tile(unique_groups, n // ngroups))
 
     if len(arr) < n:
@@ -555,8 +558,8 @@ class TestMerge(unittest.TestCase):
         assert_almost_equal(merged['value_y'], [6, np.nan, 5, 8, 5, 8, 7])
 
     def test_merge_nocopy(self):
-        left = DataFrame({'a': 0, 'b': 1}, index=range(10))
-        right = DataFrame({'c': 'foo', 'd': 'bar'}, index=range(10))
+        left = DataFrame({'a': 0, 'b': 1}, index=list(range(10)))
+        right = DataFrame({'c': 'foo', 'd': 'bar'}, index=list(range(10)))
 
         merged = merge(left, right, left_index=True,
                        right_index=True, copy=False)
@@ -582,15 +585,15 @@ class TestMerge(unittest.TestCase):
 
         # smoke test
         joined = left.join(right, on='key', sort=False)
-        self.assert_(np.array_equal(joined.index, range(4)))
+        self.assert_(np.array_equal(joined.index, list(range(4))))
 
     def test_intelligently_handle_join_key(self):
         # #733, be a bit more 1337 about not returning unconsolidated DataFrame
 
         left = DataFrame({'key': [1, 1, 2, 2, 3],
-                          'value': range(5)}, columns=['value', 'key'])
+                          'value': list(range(5))}, columns=['value', 'key'])
         right = DataFrame({'key': [1, 1, 2, 3, 4, 5],
-                           'rvalue': range(6)})
+                           'rvalue': list(range(6))})
 
         joined = merge(left, right, on='key', how='outer')
         expected = DataFrame({'key': [1, 1, 1, 1, 2, 2, 3, 4, 5.],
@@ -604,8 +607,8 @@ class TestMerge(unittest.TestCase):
 
     def test_handle_join_key_pass_array(self):
         left = DataFrame({'key': [1, 1, 2, 2, 3],
-                          'value': range(5)}, columns=['value', 'key'])
-        right = DataFrame({'rvalue': range(6)})
+                          'value': list(range(5))}, columns=['value', 'key'])
+        right = DataFrame({'rvalue': list(range(6))})
         key = np.array([1, 1, 2, 3, 4, 5])
 
         merged = merge(left, right, left_on='key', right_on=key, how='outer')
@@ -615,8 +618,8 @@ class TestMerge(unittest.TestCase):
         self.assert_(merged['key'].notnull().all())
         self.assert_(merged2['key'].notnull().all())
 
-        left = DataFrame({'value': range(5)}, columns=['value'])
-        right = DataFrame({'rvalue': range(6)})
+        left = DataFrame({'value': list(range(5))}, columns=['value'])
+        right = DataFrame({'rvalue': list(range(6))})
         lkey = np.array([1, 1, 2, 2, 3])
         rkey = np.array([1, 1, 2, 3, 4, 5])
 
@@ -624,8 +627,8 @@ class TestMerge(unittest.TestCase):
         self.assert_(np.array_equal(merged['key_0'],
                                     np.array([1, 1, 1, 1, 2, 2, 3, 4, 5])))
 
-        left = DataFrame({'value': range(3)})
-        right = DataFrame({'rvalue': range(6)})
+        left = DataFrame({'value': list(range(3))})
+        right = DataFrame({'rvalue': list(range(6))})
 
         key = np.array([0, 1, 1, 2, 2, 3])
         merged = merge(left, right, left_index=True, right_on=key, how='outer')
@@ -787,7 +790,7 @@ class TestMergeMulti(unittest.TestCase):
     def test_merge_on_multikey(self):
         joined = self.data.join(self.to_join, on=['key1', 'key2'])
 
-        join_key = Index(zip(self.data['key1'], self.data['key2']))
+        join_key = Index(list(zip(self.data['key1'], self.data['key2'])))
         indexer = self.to_join.index.get_indexer(join_key)
         ex_values = self.to_join.values.take(indexer, axis=0)
         ex_values[indexer == -1] = np.nan
@@ -809,7 +812,7 @@ class TestMergeMulti(unittest.TestCase):
     def test_compress_group_combinations(self):
 
         # ~ 40000000 possible unique groups
-        key1 = np.array([rands(10) for _ in xrange(10000)], dtype='O')
+        key1 = np.array([rands(10) for _ in range(10000)], dtype='O')
         key1 = np.tile(key1, 2)
         key2 = key1[::-1]
 
@@ -1469,7 +1472,7 @@ class TestConcatenate(unittest.TestCase):
 
         data_dict = {}
         for p in panels:
-            data_dict.update(p.iterkv())
+            data_dict.update(p.iteritems())
 
         joined = panels[0].join(panels[1:], how='inner')
         expected = Panel.from_dict(data_dict, intersect=True)
@@ -1613,7 +1616,7 @@ class TestConcatenate(unittest.TestCase):
 
         s2.name = None
         result = concat([s, s2], axis=1)
-        self.assertTrue(np.array_equal(result.columns, range(2)))
+        self.assertTrue(np.array_equal(result.columns, list(range(2))))
 
         # must reindex, #2603
         s = Series(randn(3), index=['c', 'a', 'b'], name='A')
diff --git a/pandas/tools/tests/test_pivot.py b/pandas/tools/tests/test_pivot.py
index a603118c2..11a9fef9a 100644
--- a/pandas/tools/tests/test_pivot.py
+++ b/pandas/tools/tests/test_pivot.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import unittest
 
 import numpy as np
@@ -7,6 +8,7 @@ from pandas import DataFrame, Series, Index, MultiIndex
 from pandas.tools.merge import concat
 from pandas.tools.pivot import pivot_table, crosstab
 import pandas.util.testing as tm
+import six
 
 
 class TestPivotTable(unittest.TestCase):
@@ -72,9 +74,18 @@ class TestPivotTable(unittest.TestCase):
         pv_col = df.pivot_table('quantity', 'month', ['customer', 'product'], dropna=False)
         pv_ind = df.pivot_table('quantity', ['customer', 'product'], 'month', dropna=False)
 
-        m = MultiIndex.from_tuples([(u'A', u'a'), (u'A', u'b'), (u'A', u'c'), (u'A', u'd'), 
-                                   (u'B', u'a'), (u'B', u'b'), (u'B', u'c'), (u'B', u'd'),
-                                   (u'C', u'a'), (u'C', u'b'), (u'C', u'c'), (u'C', u'd')])
+        m = MultiIndex.from_tuples([(six.u('A'), six.u('a')),
+                                    (six.u('A'), six.u('b')),
+                                    (six.u('A'), six.u('c')),
+                                    (six.u('A'), six.u('d')),
+                                    (six.u('B'), six.u('a')),
+                                    (six.u('B'), six.u('b')),
+                                    (six.u('B'), six.u('c')),
+                                    (six.u('B'), six.u('d')),
+                                    (six.u('C'), six.u('a')),
+                                    (six.u('C'), six.u('b')),
+                                    (six.u('C'), six.u('c')),
+                                    (six.u('C'), six.u('d'))])
 
         assert_equal(pv_col.columns.values, m.values)
         assert_equal(pv_ind.index.values, m.values)
@@ -212,7 +223,7 @@ class TestPivotTable(unittest.TestCase):
 
         d = datetime.date.min
         data = list(product(['foo', 'bar'], ['A', 'B', 'C'], ['x1', 'x2'],
-                            [d + datetime.timedelta(i) for i in xrange(20)], [1.0]))
+                            [d + datetime.timedelta(i) for i in range(20)], [1.0]))
         df = pandas.DataFrame(data)
         table = df.pivot_table(values=4, rows=[0, 1, 3], cols=[2])
 
diff --git a/pandas/tools/tests/test_tile.py b/pandas/tools/tests/test_tile.py
index 7da9a3bb5..54b8f05b6 100644
--- a/pandas/tools/tests/test_tile.py
+++ b/pandas/tools/tests/test_tile.py
@@ -3,6 +3,7 @@ import nose
 import unittest
 
 import numpy as np
+from six.moves import zip
 
 from pandas import DataFrame, Series, unique
 import pandas.util.testing as tm
diff --git a/pandas/tools/tile.py b/pandas/tools/tile.py
index ffed6cafc..31db8ed70 100644
--- a/pandas/tools/tile.py
+++ b/pandas/tools/tile.py
@@ -8,6 +8,7 @@ from pandas.core.index import _ensure_index
 import pandas.core.algorithms as algos
 import pandas.core.common as com
 import pandas.core.nanops as nanops
+from six.moves import zip
 
 import numpy as np
 
diff --git a/pandas/tseries/converter.py b/pandas/tseries/converter.py
index d0ec942ce..efbd80350 100644
--- a/pandas/tseries/converter.py
+++ b/pandas/tseries/converter.py
@@ -1,4 +1,6 @@
 from datetime import datetime, timedelta
+from pandas.util.py3compat import range
+import six
 import datetime as pydt
 import numpy as np
 
@@ -36,7 +38,7 @@ def _to_ordinalf(tm):
 
 
 def time2num(d):
-    if isinstance(d, basestring):
+    if isinstance(d, six.string_types):
         parsed = tools.to_datetime(d)
         if not isinstance(parsed, datetime):
             raise ValueError('Could not parse time %s' % d)
@@ -161,7 +163,7 @@ class DatetimeConverter(dates.DateConverter):
             return dates.date2num(values)
         elif (com.is_integer(values) or com.is_float(values)):
             return values
-        elif isinstance(values, basestring):
+        elif isinstance(values, six.string_types):
             return try_parse(values)
         elif isinstance(values, (list, tuple, np.ndarray)):
             if not isinstance(values, np.ndarray):
@@ -330,7 +332,7 @@ class MilliSecondLocator(dates.DateLocator):
             if len(all_dates) > 0:
                 locs = self.raise_if_exceeds(dates.date2num(all_dates))
                 return locs
-        except Exception, e:  # pragma: no cover
+        except Exception as e:  # pragma: no cover
             pass
 
         lims = dates.date2num([dmin, dmax])
@@ -808,7 +810,7 @@ def _annual_finder(vmin, vmax, freq):
 
 
 def get_finder(freq):
-    if isinstance(freq, basestring):
+    if isinstance(freq, six.string_types):
         freq = frequencies.get_freq(freq)
     fgroup = frequencies.get_freq_group(freq)
 
@@ -845,7 +847,7 @@ class TimeSeries_DateLocator(Locator):
 
     def __init__(self, freq, minor_locator=False, dynamic_mode=True,
                  base=1, quarter=1, month=1, day=1, plot_obj=None):
-        if isinstance(freq, basestring):
+        if isinstance(freq, six.string_types):
             freq = frequencies.get_freq(freq)
         self.freq = freq
         self.base = base
@@ -884,7 +886,7 @@ class TimeSeries_DateLocator(Locator):
             base = self.base
             (d, m) = divmod(vmin, base)
             vmin = (d + 1) * base
-            locs = range(vmin, vmax + 1, base)
+            locs = list(range(vmin, vmax + 1, base))
         return locs
 
     def autoscale(self):
@@ -924,7 +926,7 @@ class TimeSeries_DateFormatter(Formatter):
 
     def __init__(self, freq, minor_locator=False, dynamic_mode=True,
                  plot_obj=None):
-        if isinstance(freq, basestring):
+        if isinstance(freq, six.string_types):
             freq = frequencies.get_freq(freq)
         self.format = None
         self.freq = freq
diff --git a/pandas/tseries/frequencies.py b/pandas/tseries/frequencies.py
index 51b8e5d04..20caf150c 100644
--- a/pandas/tseries/frequencies.py
+++ b/pandas/tseries/frequencies.py
@@ -1,4 +1,8 @@
 from datetime import datetime
+from pandas.util.py3compat import range, long
+from pandas.util import compat
+from six.moves import zip
+import six
 import re
 
 import numpy as np
@@ -54,14 +58,14 @@ def get_to_timestamp_base(base):
 
 
 def get_freq_group(freq):
-    if isinstance(freq, basestring):
+    if isinstance(freq, six.string_types):
         base, mult = get_freq_code(freq)
         freq = base
     return (freq // 1000) * 1000
 
 
 def get_freq(freq):
-    if isinstance(freq, basestring):
+    if isinstance(freq, six.string_types):
         base, mult = get_freq_code(freq)
         freq = base
     return freq
@@ -364,7 +368,7 @@ _rule_aliases = {
 }
 
 for _i, _weekday in enumerate(['MON', 'TUE', 'WED', 'THU', 'FRI']):
-    for _iweek in xrange(4):
+    for _iweek in range(4):
         _name = 'WOM-%d%s' % (_iweek + 1, _weekday)
         _offset_map[_name] = offsets.WeekOfMonth(week=_iweek, weekday=_i)
         _rule_aliases[_name.replace('-', '@')] = _name
@@ -416,7 +420,7 @@ def to_offset(freqstr):
     if isinstance(freqstr, tuple):
         name = freqstr[0]
         stride = freqstr[1]
-        if isinstance(stride, basestring):
+        if isinstance(stride, six.string_types):
             name, stride = stride, name
         name, _ = _base_and_stride(name)
         delta = get_offset(name) * stride
@@ -770,7 +774,7 @@ def infer_freq(index, warn=True):
     inferer = _FrequencyInferer(index, warn=warn)
     return inferer.get_freq()
 
-_ONE_MICRO = 1000L
+_ONE_MICRO = long(1000)
 _ONE_MILLI = _ONE_MICRO * 1000
 _ONE_SECOND = _ONE_MILLI * 1000
 _ONE_MINUTE = 60 * _ONE_SECOND
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 9983f12bb..2bff7c0e4 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -23,6 +23,7 @@ import pandas.lib as lib
 import pandas.tslib as tslib
 import pandas.algos as _algos
 import pandas.index as _index
+import six
 
 
 def _utc():
@@ -70,7 +71,7 @@ def _dt_index_cmp(opname):
             other = _to_m8(other, tz=self.tz)
         elif isinstance(other, list):
             other = DatetimeIndex(other)
-        elif isinstance(other, basestring):
+        elif isinstance(other, six.string_types):
             other = _to_m8(other, tz=self.tz)
         elif not isinstance(other, np.ndarray):
             other = _ensure_datetime64(other)
@@ -207,7 +208,7 @@ class DatetimeIndex(Int64Index):
 
                     return data
 
-        if issubclass(data.dtype.type, basestring):
+        if issubclass(data.dtype.type, six.string_types):
             data = _str_to_dt_array(data, offset, dayfirst=dayfirst,
                                       yearfirst=yearfirst)
 
@@ -581,21 +582,23 @@ class DatetimeIndex(Int64Index):
     def _format_with_header(self, header, **kwargs):
         return header + self._format_native_types(**kwargs)
 
-    def _format_native_types(self, na_rep=u'NaT', **kwargs):
+    def _format_native_types(self, na_rep=six.u('NaT'), **kwargs):
         data = list(self)
 
         # tz formatter or time formatter
         zero_time = time(0, 0)
         for d in data:
             if d.time() != zero_time or d.tzinfo is not None:
-                return [u'%s' % x for x in data ]
+                return [six.u('%s') % x for x in data]
 
         values = np.array(data,dtype=object)
         mask = isnull(self.values)
         values[mask] = na_rep
 
         imask = -mask
-        values[imask] = np.array([ u'%d-%.2d-%.2d' % (dt.year, dt.month, dt.day) for dt in values[imask] ])
+        values[imask] = np.array([six.u('%d-%.2d-%.2d') % (
+                                  dt.year, dt.month, dt.day)
+                                  for dt in values[imask] ])
         return values.tolist()
 
     def isin(self, values):
@@ -766,7 +769,7 @@ class DatetimeIndex(Int64Index):
         shifted : DatetimeIndex
         """
         if freq is not None and freq != self.offset:
-            if isinstance(freq, basestring):
+            if isinstance(freq, six.string_types):
                 freq = to_offset(freq)
             result = Index.shift(self, n, freq)
             result.tz = self.tz
@@ -1230,7 +1233,7 @@ class DatetimeIndex(Int64Index):
         """
         Index.slice_locs, customized to handle partial ISO-8601 string slicing
         """
-        if isinstance(start, basestring) or isinstance(end, basestring):
+        if isinstance(start, six.string_types) or isinstance(end, six.string_types):
 
             if self.is_monotonic:
                 try:
@@ -1543,7 +1546,7 @@ class DatetimeIndex(Int64Index):
         if asof:
             raise NotImplementedError
 
-        if isinstance(time, basestring):
+        if isinstance(time, six.string_types):
             time = parse(time).time()
 
         if time.tzinfo:
@@ -1573,10 +1576,10 @@ class DatetimeIndex(Int64Index):
         """
         from dateutil.parser import parse
 
-        if isinstance(start_time, basestring):
+        if isinstance(start_time, six.string_types):
             start_time = parse(start_time).time()
 
-        if isinstance(end_time, basestring):
+        if isinstance(end_time, six.string_types):
             end_time = parse(end_time).time()
 
         if start_time.tzinfo or end_time.tzinfo:
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index fc57f9623..ce63fa7db 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -1,4 +1,7 @@
 from datetime import date, datetime, timedelta
+from pandas.util.py3compat import range
+from pandas.util import compat
+import six
 import numpy as np
 
 from pandas.tseries.tools import to_datetime
@@ -80,10 +83,10 @@ class DateOffset(object):
     def apply(self, other):
         if len(self.kwds) > 0:
             if self.n > 0:
-                for i in xrange(self.n):
+                for i in range(self.n):
                     other = other + self._offset
             else:
-                for i in xrange(-self.n):
+                for i in range(-self.n):
                     other = other - self._offset
             return other
         else:
@@ -137,7 +140,7 @@ class DateOffset(object):
         if other is None:
             return False
 
-        if isinstance(other, basestring):
+        if isinstance(other, six.string_types):
             from pandas.tseries.frequencies import to_offset
             other = to_offset(other)
 
@@ -428,7 +431,7 @@ class CustomBusinessDay(BusinessDay):
 
     @staticmethod
     def _to_dt64(dt, dtype='datetime64'):
-        if isinstance(dt, (datetime, basestring)):
+        if isinstance(dt, (datetime, six.string_types)):
             dt = np.datetime64(dt, dtype=dtype)
         if isinstance(dt, np.datetime64):
             dt = dt.astype(dtype)
@@ -622,14 +625,14 @@ class Week(DateOffset, CacheableOffset):
             if otherDay != self.weekday:
                 other = other + timedelta((self.weekday - otherDay) % 7)
                 k = k - 1
-            for i in xrange(k):
+            for i in range(k):
                 other = other + self._inc
         else:
             k = self.n
             otherDay = other.weekday()
             if otherDay != self.weekday:
                 other = other + timedelta((self.weekday - otherDay) % 7)
-            for i in xrange(-k):
+            for i in range(-k):
                 other = other - self._inc
         return other
 
@@ -713,7 +716,7 @@ class WeekOfMonth(DateOffset, CacheableOffset):
 
         d = w.rollforward(d)
 
-        for i in xrange(self.week):
+        for i in range(self.week):
             d = w.apply(d)
 
         return d
@@ -1166,7 +1169,7 @@ class Tick(DateOffset):
         return self.apply(other)
 
     def __eq__(self, other):
-        if isinstance(other, basestring):
+        if isinstance(other, six.string_types):
             from pandas.tseries.frequencies import to_offset
             other = to_offset(other)
 
@@ -1181,7 +1184,7 @@ class Tick(DateOffset):
         return hash(self._params())
 
     def __ne__(self, other):
-        if isinstance(other, basestring):
+        if isinstance(other, six.string_types):
             from pandas.tseries.frequencies import to_offset
             other = to_offset(other)
 
@@ -1315,7 +1318,7 @@ def generate_range(start=None, end=None, periods=None,
     end : datetime (default None)
     periods : int, optional
     time_rule : (legacy) name of DateOffset object to be used, optional
-        Corresponds with names expected by tseries.frequencies.get_offset        
+        Corresponds with names expected by tseries.frequencies.get_offset
 
     Note
     ----
diff --git a/pandas/tseries/period.py b/pandas/tseries/period.py
index 4fec590dd..0a7b57387 100644
--- a/pandas/tseries/period.py
+++ b/pandas/tseries/period.py
@@ -20,6 +20,8 @@ from pandas.lib import Timestamp
 import pandas.lib as lib
 import pandas.tslib as tslib
 import pandas.algos as _algos
+import six
+from six.moves import map, zip
 
 
 #---------------
@@ -47,7 +49,7 @@ class Period(PandasObject):
 
     Parameters
     ----------
-    value : Period or basestring, default None
+    value : Period or six.string_types, default None
         The time period represented (e.g., '4Q2005')
     freq : str, default None
         e.g., 'B' for businessday, ('T', 5) or '5T' for 5 minutes
@@ -99,7 +101,7 @@ class Period(PandasObject):
                 converted = other.asfreq(freq)
                 self.ordinal = converted.ordinal
 
-        elif isinstance(value, basestring) or com.is_integer(value):
+        elif isinstance(value, six.string_types) or com.is_integer(value):
             if com.is_integer(value):
                 value = str(value)
 
@@ -666,7 +668,7 @@ class PeriodIndex(Int64Index):
 
     def __contains__(self, key):
         if not isinstance(key, Period) or key.freq != self.freq:
-            if isinstance(key, basestring):
+            if isinstance(key, six.string_types):
                 try:
                     self.get_loc(key)
                     return True
@@ -946,7 +948,7 @@ class PeriodIndex(Int64Index):
         """
         Index.slice_locs, customized to handle partial ISO-8601 string slicing
         """
-        if isinstance(start, basestring) or isinstance(end, basestring):
+        if isinstance(start, six.string_types) or isinstance(end, six.string_types):
             try:
                 if start:
                     start_loc = self._get_string_slice(start).start
@@ -1057,14 +1059,14 @@ class PeriodIndex(Int64Index):
     def _format_with_header(self, header, **kwargs):
         return header + self._format_native_types(**kwargs)
 
-    def _format_native_types(self, na_rep=u'NaT', **kwargs):
+    def _format_native_types(self, na_rep=six.u('NaT'), **kwargs):
 
         values = np.array(list(self),dtype=object)
         mask = isnull(self.values)
         values[mask] = na_rep
 
         imask = -mask
-        values[imask] = np.array([ u'%s' % dt for dt in values[imask] ])
+        values[imask] = np.array([six.u('%s') % dt for dt in values[imask]])
         return values.tolist()
 
     def __array_finalize__(self, obj):
@@ -1084,7 +1086,7 @@ class PeriodIndex(Int64Index):
 
     def __unicode__(self):
         output = self.__class__.__name__
-        output += u'('
+        output += six.u('(')
         prefix = '' if py3compat.PY3 else 'u'
         mapper = "{0}'{{0}}'".format(prefix)
         output += '[{0}]'.format(', '.join(map(mapper.format, self)))
diff --git a/pandas/tseries/resample.py b/pandas/tseries/resample.py
index 9c22ad66d..253abcbd8 100644
--- a/pandas/tseries/resample.py
+++ b/pandas/tseries/resample.py
@@ -1,5 +1,6 @@
 from datetime import timedelta
 
+import six
 import numpy as np
 
 from pandas.core.groupby import BinGrouper, CustomGrouper
@@ -230,7 +231,7 @@ class TimeGrouper(CustomGrouper):
                                        limit=self.limit)
 
         loffset = self.loffset
-        if isinstance(loffset, basestring):
+        if isinstance(loffset, six.string_types):
             loffset = to_offset(self.loffset)
 
         if isinstance(loffset, (DateOffset, timedelta)):
@@ -291,7 +292,7 @@ def _take_new_index(obj, indexer, new_index, axis=0):
 
 
 def _get_range_edges(axis, offset, closed='left', base=0):
-    if isinstance(offset, basestring):
+    if isinstance(offset, six.string_types):
         offset = to_offset(offset)
 
     if isinstance(offset, Tick):
diff --git a/pandas/tseries/tests/test_converter.py b/pandas/tseries/tests/test_converter.py
index dc5d5cf67..aca714080 100644
--- a/pandas/tseries/tests/test_converter.py
+++ b/pandas/tseries/tests/test_converter.py
@@ -6,6 +6,7 @@ import unittest
 import nose
 
 import numpy as np
+import six
 
 try:
     import pandas.tseries.converter as converter
@@ -14,7 +15,7 @@ except ImportError:
 
 
 def test_timtetonum_accepts_unicode():
-    assert(converter.time2num("00:01") == converter.time2num(u"00:01"))
+    assert(converter.time2num("00:01") == converter.time2num(six.u("00:01")))
 
 
 class TestDateTimeConverter(unittest.TestCase):
@@ -25,7 +26,7 @@ class TestDateTimeConverter(unittest.TestCase):
 
     def test_convert_accepts_unicode(self):
         r1 = self.dtc.convert("12:22", None, None)
-        r2 = self.dtc.convert(u"12:22", None, None)
+        r2 = self.dtc.convert(six.u("12:22"), None, None)
         assert(r1 == r2), "DatetimeConverter.convert should accept unicode"
 
     def test_conversion(self):
diff --git a/pandas/tseries/tests/test_cursor.py b/pandas/tseries/tests/test_cursor.py
index ffada1876..fc02a83cb 100644
--- a/pandas/tseries/tests/test_cursor.py
+++ b/pandas/tseries/tests/test_cursor.py
@@ -11,7 +11,7 @@ class TestNewOffsets(unittest.TestCase):
             self.assert_(t.day == 1)
             self.assert_(t.month == 1)
             self.assert_(t.year == 2002 + i)
-            off.next()
+            next(off)
 
         for i in range(499, -1, -1):
             off.prev()
@@ -27,7 +27,7 @@ class TestNewOffsets(unittest.TestCase):
             self.assert_(t.month == 12)
             self.assert_(t.day == 31)
             self.assert_(t.year == 2001 + i)
-            off.next()
+            next(off)
 
         for i in range(499, -1, -1):
             off.prev()
@@ -47,7 +47,7 @@ class TestNewOffsets(unittest.TestCase):
             self.assert_(t.day == 31 or t.day == 30 or t.day == 29)
             self.assert_(t.year == 2001 + i)
             self.assert_(t.weekday() < 5)
-            off.next()
+            next(off)
 
         for i in range(499, -1, -1):
             off.prev()
@@ -66,7 +66,7 @@ class TestNewOffsets(unittest.TestCase):
             self.assert_(t.day == 1)
             self.assert_(t.month == 1 + i)
             self.assert_(t.year == 2002)
-            off.next()
+            next(off)
 
         for i in range(11, -1, -1):
             off.prev()
@@ -82,7 +82,7 @@ class TestNewOffsets(unittest.TestCase):
             self.assert_(t.day >= 28)
             self.assert_(t.month == (12 if i == 0 else i))
             self.assert_(t.year == 2001 + (i != 0))
-            off.next()
+            next(off)
 
         for i in range(11, -1, -1):
             off.prev()
@@ -103,7 +103,7 @@ class TestNewOffsets(unittest.TestCase):
             else:
                 self.assert_(t.day >= 26)
             self.assert_(t.weekday() < 5)
-            off.next()
+            next(off)
 
         for i in range(499, -1, -1):
             off.prev()
@@ -124,8 +124,8 @@ class TestNewOffsets(unittest.TestCase):
 
                 for k in range(500):
                     self.assert_(off1.ts == off2.ts)
-                    off1.next()
-                    off2.next()
+                    next(off1)
+                    next(off2)
 
                 for k in range(500):
                     self.assert_(off1.ts == off2.ts)
@@ -139,7 +139,7 @@ class TestNewOffsets(unittest.TestCase):
 
         t0 = lib.Timestamp(off.ts)
         for i in range(500):
-            off.next()
+            next(off)
             t1 = lib.Timestamp(off.ts)
             self.assert_(t1.value - t0.value == us_in_day)
             t0 = t1
@@ -155,7 +155,7 @@ class TestNewOffsets(unittest.TestCase):
 
         t0 = lib.Timestamp(off.ts)
         for i in range(500):
-            off.next()
+            next(off)
             t1 = lib.Timestamp(off.ts)
             self.assert_(t1.weekday() < 5)
             self.assert_(t1.value - t0.value == us_in_day or
@@ -184,7 +184,7 @@ class TestNewOffsets(unittest.TestCase):
                     t = lib.Timestamp(off.ts)
                     stack.append(t)
                     self.assert_(t.weekday() == day)
-                    off.next()
+                    next(off)
 
                 for i in range(499, -1, -1):
                     off.prev()
diff --git a/pandas/tseries/tests/test_daterange.py b/pandas/tseries/tests/test_daterange.py
index 4c46dcccb..4f4df38af 100644
--- a/pandas/tseries/tests/test_daterange.py
+++ b/pandas/tseries/tests/test_daterange.py
@@ -1,4 +1,5 @@
 from datetime import datetime
+from pandas.util.py3compat import range
 import pickle
 import unittest
 import nose
diff --git a/pandas/tseries/tests/test_frequencies.py b/pandas/tseries/tests/test_frequencies.py
index aad831ae4..bcaba1fee 100644
--- a/pandas/tseries/tests/test_frequencies.py
+++ b/pandas/tseries/tests/test_frequencies.py
@@ -1,4 +1,5 @@
 from datetime import datetime, time, timedelta
+from pandas.util.py3compat import range
 import sys
 import os
 import unittest
diff --git a/pandas/tseries/tests/test_offsets.py b/pandas/tseries/tests/test_offsets.py
index 487a3091f..9cc7383ed 100644
--- a/pandas/tseries/tests/test_offsets.py
+++ b/pandas/tseries/tests/test_offsets.py
@@ -1,4 +1,6 @@
 from datetime import date, datetime, timedelta
+from pandas.util.py3compat import range
+from pandas.util import compat
 import unittest
 import nose
 from nose.tools import assert_raises
@@ -1651,7 +1653,7 @@ def test_compare_ticks():
         three = kls(3)
         four = kls(4)
 
-        for _ in xrange(10):
+        for _ in range(10):
             assert(three < kls(4))
             assert(kls(3) < four)
             assert(four > kls(3))
diff --git a/pandas/tseries/tests/test_period.py b/pandas/tseries/tests/test_period.py
index 9fd5e6bf5..8058d1202 100644
--- a/pandas/tseries/tests/test_period.py
+++ b/pandas/tseries/tests/test_period.py
@@ -22,6 +22,9 @@ import pandas.tseries.period as pmod
 import pandas.core.datetools as datetools
 import pandas as pd
 import numpy as np
+import six
+from pandas.util.py3compat import range
+from six.moves import map, zip
 randn = np.random.randn
 
 from pandas import Series, TimeSeries, DataFrame
@@ -209,8 +212,8 @@ class TestPeriodProperties(TestCase):
     def test_strftime(self):
         p = Period('2000-1-1 12:34:12', freq='S')
         res = p.strftime('%Y-%m-%d %H:%M:%S')
-        self.assert_( res ==  '2000-01-01 12:34:12')
-        self.assert_( isinstance(res,unicode)) # GH3363
+        self.assertEqual(res,  '2000-01-01 12:34:12')
+        tm.assert_isinstance(res, six.text_type) # GH3363
 
     def test_sub_delta(self):
         left, right = Period('2011', freq='A'), Period('2007', freq='A')
@@ -1115,7 +1118,7 @@ class TestPeriodIndex(TestCase):
 
     def test_constructor_arrays_negative_year(self):
         years = np.arange(1960, 2000).repeat(4)
-        quarters = np.tile(range(1, 5), 40)
+        quarters = np.tile(list(range(1, 5)), 40)
 
         pindex = PeriodIndex(year=years, quarter=quarters)
 
@@ -1123,8 +1126,8 @@ class TestPeriodIndex(TestCase):
         self.assert_(np.array_equal(pindex.quarter, quarters))
 
     def test_constructor_invalid_quarters(self):
-        self.assertRaises(ValueError, PeriodIndex, year=range(2000, 2004),
-                          quarter=range(4), freq='Q-DEC')
+        self.assertRaises(ValueError, PeriodIndex, year=list(range(2000, 2004)),
+                          quarter=list(range(4)), freq='Q-DEC')
 
     def test_constructor_corner(self):
         self.assertRaises(ValueError, PeriodIndex, periods=10, freq='A')
@@ -1213,7 +1216,7 @@ class TestPeriodIndex(TestCase):
 
     def test_getitem_datetime(self):
         rng = period_range(start='2012-01-01', periods=10, freq='W-MON')
-        ts = Series(range(len(rng)), index=rng)
+        ts = Series(list(range(len(rng))), index=rng)
 
         dt1 = datetime(2011, 10, 2)
         dt4 = datetime(2012, 4, 20)
@@ -1285,7 +1288,7 @@ class TestPeriodIndex(TestCase):
 
     def test_to_timestamp_quarterly_bug(self):
         years = np.arange(1960, 2000).repeat(4)
-        quarters = np.tile(range(1, 5), 40)
+        quarters = np.tile(list(range(1, 5)), 40)
 
         pindex = PeriodIndex(year=years, quarter=quarters)
 
@@ -1622,45 +1625,45 @@ class TestPeriodIndex(TestCase):
     def test_period_index_unicode(self):
         pi = PeriodIndex(freq='A', start='1/1/2001', end='12/1/2009')
         assert_equal(len(pi), 9)
-        assert_equal(pi, eval(unicode(pi)))
+        assert_equal(pi, eval(six.text_type(pi)))
 
         pi = PeriodIndex(freq='Q', start='1/1/2001', end='12/1/2009')
         assert_equal(len(pi), 4 * 9)
-        assert_equal(pi, eval(unicode(pi)))
+        assert_equal(pi, eval(six.text_type(pi)))
 
         pi = PeriodIndex(freq='M', start='1/1/2001', end='12/1/2009')
         assert_equal(len(pi), 12 * 9)
-        assert_equal(pi, eval(unicode(pi)))
+        assert_equal(pi, eval(six.text_type(pi)))
 
         start = Period('02-Apr-2005', 'B')
         i1 = PeriodIndex(start=start, periods=20)
         assert_equal(len(i1), 20)
         assert_equal(i1.freq, start.freq)
         assert_equal(i1[0], start)
-        assert_equal(i1, eval(unicode(i1)))
+        assert_equal(i1, eval(six.text_type(i1)))
 
         end_intv = Period('2006-12-31', 'W')
         i1 = PeriodIndex(end=end_intv, periods=10)
         assert_equal(len(i1), 10)
         assert_equal(i1.freq, end_intv.freq)
         assert_equal(i1[-1], end_intv)
-        assert_equal(i1, eval(unicode(i1)))
+        assert_equal(i1, eval(six.text_type(i1)))
 
         end_intv = Period('2006-12-31', '1w')
         i2 = PeriodIndex(end=end_intv, periods=10)
         assert_equal(len(i1), len(i2))
         self.assert_((i1 == i2).all())
         assert_equal(i1.freq, i2.freq)
-        assert_equal(i1, eval(unicode(i1)))
-        assert_equal(i2, eval(unicode(i2)))
+        assert_equal(i1, eval(six.text_type(i1)))
+        assert_equal(i2, eval(six.text_type(i2)))
 
         end_intv = Period('2006-12-31', ('w', 1))
         i2 = PeriodIndex(end=end_intv, periods=10)
         assert_equal(len(i1), len(i2))
         self.assert_((i1 == i2).all())
         assert_equal(i1.freq, i2.freq)
-        assert_equal(i1, eval(unicode(i1)))
-        assert_equal(i2, eval(unicode(i2)))
+        assert_equal(i1, eval(six.text_type(i1)))
+        assert_equal(i2, eval(six.text_type(i2)))
 
         try:
             PeriodIndex(start=start, end=end_intv)
@@ -1670,7 +1673,7 @@ class TestPeriodIndex(TestCase):
 
         end_intv = Period('2005-05-01', 'B')
         i1 = PeriodIndex(start=start, end=end_intv)
-        assert_equal(i1, eval(unicode(i1)))
+        assert_equal(i1, eval(six.text_type(i1)))
 
         try:
             PeriodIndex(start=start)
@@ -1683,12 +1686,12 @@ class TestPeriodIndex(TestCase):
         i2 = PeriodIndex([end_intv, Period('2005-05-05', 'B')])
         assert_equal(len(i2), 2)
         assert_equal(i2[0], end_intv)
-        assert_equal(i2, eval(unicode(i2)))
+        assert_equal(i2, eval(six.text_type(i2)))
 
         i2 = PeriodIndex(np.array([end_intv, Period('2005-05-05', 'B')]))
         assert_equal(len(i2), 2)
         assert_equal(i2[0], end_intv)
-        assert_equal(i2, eval(unicode(i2)))
+        assert_equal(i2, eval(six.text_type(i2)))
 
         # Mixed freq should fail
         vals = [end_intv, Period('2006-12-31', 'w')]
@@ -2001,7 +2004,7 @@ class TestPeriodIndex(TestCase):
             types += unicode,
 
         for t in types:
-            expected = np.array(map(t, raw), dtype=object)
+            expected = np.array(list(map(t, raw)), dtype=object)
             res = index.map(t)
 
             # should return an array
diff --git a/pandas/tseries/tests/test_plotting.py b/pandas/tseries/tests/test_plotting.py
index f1602bbd3..2bb70e6ef 100644
--- a/pandas/tseries/tests/test_plotting.py
+++ b/pandas/tseries/tests/test_plotting.py
@@ -3,6 +3,8 @@ from datetime import datetime, timedelta, date, time
 
 import unittest
 import nose
+from pandas.util.py3compat import range
+from six.moves import zip
 
 import numpy as np
 from numpy.testing.decorators import slow
@@ -186,7 +188,7 @@ class TestTSPlot(unittest.TestCase):
         plt.clf()
         fig.add_subplot(111)
         rng = date_range('2001-1-1', '2001-1-10')
-        ts = Series(range(len(rng)), rng)
+        ts = Series(list(range(len(rng))), rng)
         ts = ts[:3].append(ts[5:])
         ax = ts.plot()
         self.assert_(not hasattr(ax, 'freq'))
@@ -942,7 +944,7 @@ class TestTSPlot(unittest.TestCase):
     def test_ax_plot(self):
         x = DatetimeIndex(start='2012-01-02', periods=10,
                           freq='D')
-        y = range(len(x))
+        y = list(range(len(x)))
         import matplotlib.pyplot as plt
         fig = plt.figure()
         ax = fig.add_subplot(111)
diff --git a/pandas/tseries/tests/test_resample.py b/pandas/tseries/tests/test_resample.py
index 02a3030f6..22e103e54 100644
--- a/pandas/tseries/tests/test_resample.py
+++ b/pandas/tseries/tests/test_resample.py
@@ -2,6 +2,8 @@
 
 from datetime import datetime, timedelta
 
+from pandas.util.py3compat import range
+from six.moves import zip
 import numpy as np
 
 from pandas import Series, TimeSeries, DataFrame, Panel, isnull, notnull, Timestamp
@@ -860,7 +862,7 @@ class TestResamplePeriodIndex(unittest.TestCase):
 
     def test_resample_tz_localized(self):
         dr = date_range(start='2012-4-13', end='2012-5-1')
-        ts = Series(range(len(dr)), dr)
+        ts = Series(list(range(len(dr))), dr)
 
         ts_utc = ts.tz_localize('UTC')
         ts_local = ts_utc.tz_convert('America/Los_Angeles')
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index f41d31d2a..4b87dd295 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -1,5 +1,4 @@
 # pylint: disable-msg=E1101,W0612
-import pandas.util.compat as itertools
 from datetime import datetime, time, timedelta
 import sys
 import os
@@ -8,6 +7,9 @@ import unittest
 import nose
 
 import numpy as np
+from pandas.util.py3compat import range, long, StringIO
+from pandas.util.compat import product
+from six.moves import map, zip
 randn = np.random.randn
 
 from pandas import (Index, Series, TimeSeries, DataFrame,
@@ -23,8 +25,6 @@ import pandas as pd
 from pandas.util.testing import assert_series_equal, assert_almost_equal
 import pandas.util.testing as tm
 
-from pandas.util.py3compat import StringIO
-
 from pandas.tslib import NaT, iNaT
 import pandas.lib as lib
 import pandas.tslib as tslib
@@ -239,17 +239,17 @@ class TestTimeSeriesDuplicates(unittest.TestCase):
 
         # GH3546 (not including times on the last day)
         idx = date_range(start='2013-05-31 00:00', end='2013-05-31 23:00', freq='H')
-        ts  = Series(range(len(idx)), index=idx)
+        ts  = Series(list(range(len(idx))), index=idx)
         expected = ts['2013-05']
         assert_series_equal(expected,ts)
 
         idx = date_range(start='2013-05-31 00:00', end='2013-05-31 23:59', freq='S')
-        ts  = Series(range(len(idx)), index=idx)
+        ts  = Series(list(range(len(idx))), index=idx)
         expected = ts['2013-05']
         assert_series_equal(expected,ts)
 
         idx = [ Timestamp('2013-05-31 00:00'), Timestamp(datetime(2013,5,31,23,59,59,999999))]
-        ts  = Series(range(len(idx)), index=idx)
+        ts  = Series(list(range(len(idx))), index=idx)
         expected = ts['2013']
         assert_series_equal(expected,ts)
 
@@ -453,7 +453,7 @@ class TestTimeSeries(unittest.TestCase):
         # 2155
         columns = DatetimeIndex(start='1/1/2012', end='2/1/2012',
                                 freq=datetools.bday)
-        index = range(10)
+        index = list(range(10))
         data = DataFrame(columns=columns, index=index)
         t = datetime(2012, 11, 1)
         ts = Timestamp(t)
@@ -664,7 +664,7 @@ class TestTimeSeries(unittest.TestCase):
         rng = date_range('1/1/2000 00:00:00', periods=10, freq='10s')
         series = Series(rng)
 
-        result = series.reindex(range(15))
+        result = series.reindex(list(range(15)))
         self.assert_(np.issubdtype(result.dtype, np.dtype('M8[ns]')))
 
         mask = result.isnull()
@@ -675,7 +675,7 @@ class TestTimeSeries(unittest.TestCase):
         rng = date_range('1/1/2000 00:00:00', periods=10, freq='10s')
         df = DataFrame({'A': np.random.randn(len(rng)), 'B': rng})
 
-        result = df.reindex(range(15))
+        result = df.reindex(list(range(15)))
         self.assert_(np.issubdtype(result['B'].dtype, np.dtype('M8[ns]')))
 
         mask = com.isnull(result)['B']
@@ -890,7 +890,7 @@ class TestTimeSeries(unittest.TestCase):
         ### array = ['2012','20120101','20120101 12:01:01']
         array = ['20120101','20120101 12:01:01']
         expected = list(to_datetime(array))
-        result = map(Timestamp,array)
+        result = list(map(Timestamp,array))
         tm.assert_almost_equal(result,expected)
 
         ### currently fails ###
@@ -954,7 +954,7 @@ class TestTimeSeries(unittest.TestCase):
         index = DatetimeIndex(['1/3/2000'])
         try:
             index.get_loc('1/1/2000')
-        except KeyError, e:
+        except KeyError as e:
             self.assert_('2000' in str(e))
 
     def test_reindex_with_datetimes(self):
@@ -1153,7 +1153,7 @@ class TestTimeSeries(unittest.TestCase):
         stime = time(0, 0)
         etime = time(1, 0)
 
-        close_open = itertools.product([True, False], [True, False])
+        close_open = product([True, False], [True, False])
         for inc_start, inc_end in close_open:
             filtered = ts.between_time(stime, etime, inc_start, inc_end)
             exp_len = 13 * 4 + 1
@@ -1185,7 +1185,7 @@ class TestTimeSeries(unittest.TestCase):
         stime = time(22, 0)
         etime = time(9, 0)
 
-        close_open = itertools.product([True, False], [True, False])
+        close_open = product([True, False], [True, False])
         for inc_start, inc_end in close_open:
             filtered = ts.between_time(stime, etime, inc_start, inc_end)
             exp_len = (12 * 11 + 1) * 4 + 1
@@ -1213,7 +1213,7 @@ class TestTimeSeries(unittest.TestCase):
         stime = time(0, 0)
         etime = time(1, 0)
 
-        close_open = itertools.product([True, False], [True, False])
+        close_open = product([True, False], [True, False])
         for inc_start, inc_end in close_open:
             filtered = ts.between_time(stime, etime, inc_start, inc_end)
             exp_len = 13 * 4 + 1
@@ -1245,7 +1245,7 @@ class TestTimeSeries(unittest.TestCase):
         stime = time(22, 0)
         etime = time(9, 0)
 
-        close_open = itertools.product([True, False], [True, False])
+        close_open = product([True, False], [True, False])
         for inc_start, inc_end in close_open:
             filtered = ts.between_time(stime, etime, inc_start, inc_end)
             exp_len = (12 * 11 + 1) * 4 + 1
@@ -1513,11 +1513,11 @@ class TestTimeSeries(unittest.TestCase):
         dr = date_range(start='1/1/2012', freq='5min', periods=10)
 
         # BAD Example, datetimes first
-        s = Series(np.arange(10), index=[dr, range(10)])
+        s = Series(np.arange(10), index=[dr, list(range(10))])
         grouped = s.groupby(lambda x: x[1] % 2 == 0)
         result = grouped.count()
 
-        s = Series(np.arange(10), index=[range(10), dr])
+        s = Series(np.arange(10), index=[list(range(10)), dr])
         grouped = s.groupby(lambda x: x[0] % 2 == 0)
         expected = grouped.count()
 
@@ -1668,7 +1668,7 @@ class TestTimeSeries(unittest.TestCase):
         df2_obj = DataFrame.from_records(rows, columns=['date', 'test'])
 
         ind = date_range(start="2000/1/1", freq="D", periods=10)
-        df1 = DataFrame({'date': ind, 'test':range(10)})
+        df1 = DataFrame({'date': ind, 'test':list(range(10))})
 
         # it works!
         pd.concat([df1, df2_obj])
@@ -1687,7 +1687,7 @@ class TestDatetimeIndex(unittest.TestCase):
         import datetime
         start=datetime.datetime.now()
         idx=DatetimeIndex(start=start,freq="1d",periods=10)
-        df=DataFrame(range(10),index=idx)
+        df=DataFrame(list(range(10)),index=idx)
         df["2013-01-14 23:44:34.437768-05:00":] # no exception here
 
     def test_append_join_nondatetimeindex(self):
@@ -1981,7 +1981,7 @@ class TestLegacySupport(unittest.TestCase):
             cls.series = pickle.load(f)
 
     def test_pass_offset_warn(self):
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         buf = StringIO()
 
         sys.stderr = buf
@@ -2402,7 +2402,7 @@ class TestLegacySupport(unittest.TestCase):
 class TestLegacyCompat(unittest.TestCase):
 
     def setUp(self):
-        from StringIO import StringIO
+        from pandas.util.py3compat import StringIO
         # suppress deprecation warnings
         sys.stderr = StringIO()
 
@@ -2650,7 +2650,7 @@ class TestDatetime64(unittest.TestCase):
     def test_slice_locs_indexerror(self):
         times = [datetime(2000, 1, 1) + timedelta(minutes=i * 10)
                  for i in range(100000)]
-        s = Series(range(100000), times)
+        s = Series(list(range(100000)), times)
         s.ix[datetime(1900, 1, 1):datetime(2100, 1, 1)]
 
 
@@ -2813,26 +2813,26 @@ class TestTimestamp(unittest.TestCase):
         days = (ts - Timestamp('1970-01-01')).days
 
         check(val)
-        check(val/1000L,unit='us')
-        check(val/1000000L,unit='ms')
-        check(val/1000000000L,unit='s')
+        check(val/long(1000),unit='us')
+        check(val/long(1000000),unit='ms')
+        check(val/long(1000000000),unit='s')
         check(days,unit='D',h=0)
 
         # using truediv, so these are like floats
         if py3compat.PY3:
-            check((val+500000)/1000000000L,unit='s',us=500)
-            check((val+500000000)/1000000000L,unit='s',us=500000)
-            check((val+500000)/1000000L,unit='ms',us=500)
+            check((val+500000)/long(1000000000),unit='s',us=500)
+            check((val+500000000)/long(1000000000),unit='s',us=500000)
+            check((val+500000)/long(1000000),unit='ms',us=500)
 
         # get chopped in py2
         else:
-            check((val+500000)/1000000000L,unit='s')
-            check((val+500000000)/1000000000L,unit='s')
-            check((val+500000)/1000000L,unit='ms')
+            check((val+500000)/long(1000000000),unit='s')
+            check((val+500000000)/long(1000000000),unit='s')
+            check((val+500000)/long(1000000),unit='ms')
 
         # ok
-        check((val+500000)/1000L,unit='us',us=500)
-        check((val+500000000)/1000000L,unit='ms',us=500000)
+        check((val+500000)/long(1000),unit='us',us=500)
+        check((val+500000000)/long(1000000),unit='ms',us=500000)
 
         # floats
         check(val/1000.0 + 5,unit='us',us=5)
@@ -2857,7 +2857,7 @@ class TestTimestamp(unittest.TestCase):
 
     def test_comparison(self):
         # 5-18-2012 00:00:00.000
-        stamp = 1337299200000000000L
+        stamp = long(1337299200000000000)
 
         val = Timestamp(stamp)
 
@@ -2908,7 +2908,7 @@ class TestTimestamp(unittest.TestCase):
             self.assertFalse(a.to_pydatetime() == b)
 
     def test_delta_preserve_nanos(self):
-        val = Timestamp(1337299200000000123L)
+        val = Timestamp(long(1337299200000000123))
         result = val + timedelta(1)
         self.assert_(result.nanosecond == val.nanosecond)
 
diff --git a/pandas/tseries/tests/test_timezones.py b/pandas/tseries/tests/test_timezones.py
index 09224d013..bf441a970 100644
--- a/pandas/tseries/tests/test_timezones.py
+++ b/pandas/tseries/tests/test_timezones.py
@@ -5,6 +5,8 @@ import os
 import unittest
 import nose
 
+from pandas.util.py3compat import range
+from six.moves import zip
 import numpy as np
 import pytz
 
@@ -393,7 +395,7 @@ class TestTimeZoneSupport(unittest.TestCase):
         _skip_if_no_pytz()
         rng = date_range('1/1/2000', periods=20, tz='US/Eastern')
 
-        result = rng.take(range(5))
+        result = rng.take(list(range(5)))
         self.assert_(result.tz == rng.tz)
         self.assert_(result.freq == rng.freq)
 
@@ -620,7 +622,7 @@ class TestTimeZoneSupport(unittest.TestCase):
                            tz='Europe/Berlin')
         ts = Series(index=index, data=index.hour)
         time_pandas = Timestamp('2012-12-24 17:00', tz='Europe/Berlin')
-        time_datetime = datetime(2012, 12, 24, 17, 00,
+        time_datetime = datetime(2012, 12, 24, 17, 0,
                                  tzinfo=pytz.timezone('Europe/Berlin'))
         self.assertEqual(ts[time_pandas], ts[time_datetime])
 
@@ -635,14 +637,14 @@ class TestTimeZoneSupport(unittest.TestCase):
         """ Test different DatetimeIndex constructions with timezone
         Follow-up of #4229
         """
-        
+
         arr = ['11/10/2005 08:00:00', '11/10/2005 09:00:00']
-        
+
         idx1 = to_datetime(arr).tz_localize('US/Eastern')
         idx2 = DatetimeIndex(start="2005-11-10 08:00:00", freq='H', periods=2, tz='US/Eastern')
         idx3 = DatetimeIndex(arr, tz='US/Eastern')
         idx4 = DatetimeIndex(np.array(arr), tz='US/Eastern')
-        
+
         for other in [idx2, idx3, idx4]:
             self.assert_(idx1.equals(other))
 
@@ -746,7 +748,7 @@ class TestTimeZones(unittest.TestCase):
         test2 = DataFrame(np.zeros((3, 3)),
                           index=date_range("2012-11-15 00:00:00", periods=3,
                                            freq="250L", tz="US/Central"),
-                          columns=range(3, 6))
+                          columns=list(range(3, 6)))
 
         result = test1.join(test2, how='outer')
         ex_index = test1.index.union(test2.index)
@@ -815,7 +817,7 @@ class TestTimeZones(unittest.TestCase):
         # mixed
 
         rng1 = date_range('1/1/2011 01:00', periods=1, freq='H')
-        rng2 = range(100)
+        rng2 = list(range(100))
         ts1 = Series(np.random.randn(len(rng1)), index=rng1)
         ts2 = Series(np.random.randn(len(rng2)), index=rng2)
         ts_result = ts1.append(ts2)
diff --git a/pandas/tseries/tests/test_util.py b/pandas/tseries/tests/test_util.py
index 09dad264b..5bfdbba56 100644
--- a/pandas/tseries/tests/test_util.py
+++ b/pandas/tseries/tests/test_util.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import nose
 import unittest
 
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index d914a8fa5..c56fa192b 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -2,6 +2,7 @@ from datetime import datetime, timedelta
 import re
 import sys
 
+import six
 import numpy as np
 
 import pandas.lib as lib
@@ -40,7 +41,7 @@ def _infer_tzinfo(start, end):
 
 
 def _maybe_get_tz(tz):
-    if isinstance(tz, basestring):
+    if isinstance(tz, six.string_types):
         import pytz
         tz = pytz.timezone(tz)
     if com.is_integer(tz):
@@ -91,7 +92,7 @@ def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True,
             if box and not isinstance(arg, DatetimeIndex):
                 try:
                     return DatetimeIndex(arg, tz='utc' if utc else None)
-                except ValueError, e:
+                except ValueError as e:
                     values, tz = tslib.datetime_to_datetime64(arg)
                     return DatetimeIndex._simple_new(values, None, tz=tz)
 
@@ -109,7 +110,7 @@ def to_datetime(arg, errors='ignore', dayfirst=False, utc=None, box=True,
                 result = DatetimeIndex(result, tz='utc' if utc else None)
             return result
 
-        except ValueError, e:
+        except ValueError as e:
             try:
                 values, tz = tslib.datetime_to_datetime64(arg)
                 return DatetimeIndex._simple_new(values, None, tz=tz)
@@ -148,7 +149,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
 
     Parameters
     ----------
-    arg : basestring
+    arg : six.string_types
     freq : str or DateOffset, default None
         Helps with interpreting time string if supplied
     dayfirst : bool, default None
@@ -165,7 +166,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
     from pandas.tseries.frequencies import (_get_rule_month, _month_numbers,
                                             _get_freq_str)
 
-    if not isinstance(arg, basestring):
+    if not isinstance(arg, six.string_types):
         return arg
 
     arg = arg.upper()
@@ -236,7 +237,7 @@ def parse_time_string(arg, freq=None, dayfirst=None, yearfirst=None):
     try:
         parsed, reso = dateutil_parse(arg, default, dayfirst=dayfirst,
                                       yearfirst=yearfirst)
-    except Exception, e:
+    except Exception as e:
         raise DateParseError(e)
 
     if parsed is None:
@@ -278,7 +279,7 @@ def dateutil_parse(timestr, default,
                 tzdata = tzinfos.get(res.tzname)
             if isinstance(tzdata, datetime.tzinfo):
                 tzinfo = tzdata
-            elif isinstance(tzdata, basestring):
+            elif isinstance(tzdata, six.string_types):
                 tzinfo = tz.tzstr(tzdata)
             elif isinstance(tzdata, int):
                 tzinfo = tz.tzoffset(res.tzname, tzdata)
diff --git a/pandas/tseries/util.py b/pandas/tseries/util.py
index eb80746cf..92ec7d2be 100644
--- a/pandas/tseries/util.py
+++ b/pandas/tseries/util.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import numpy as np
 
 import pandas as pd
@@ -53,12 +54,12 @@ def pivot_annual(series, freq=None):
         # adjust for leap year
         offset[(-isleapyear(year)) & (offset >= 59)] += 1
 
-        columns = range(1, 367)
+        columns = list(range(1, 367))
         # todo: strings like 1/1, 1/25, etc.?
     elif freq in ('M', 'BM'):
         width = 12
         offset = index.month - 1
-        columns = range(1, 13)
+        columns = list(range(1, 13))
     elif freq == 'H':
         width = 8784
         grouped = series.groupby(series.index.year)
@@ -66,7 +67,7 @@ def pivot_annual(series, freq=None):
         defaulted.index = defaulted.index.droplevel(0)
         offset = np.asarray(defaulted.index)
         offset[-isleapyear(year) & (offset >= 1416)] += 24
-        columns = range(1, 8785)
+        columns = list(range(1, 8785))
     else:
         raise NotImplementedError(freq)
 
diff --git a/pandas/util/compat.py b/pandas/util/compat.py
index c18044fc6..a42b9218a 100644
--- a/pandas/util/compat.py
+++ b/pandas/util/compat.py
@@ -1,12 +1,15 @@
 # itertools.product not in Python 2.5
 
+import sys
+import six
+from six.moves import map
 try:
     from itertools import product
 except ImportError:  # python 2.5
     def product(*args, **kwds):
         # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy
         # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111
-        pools = map(tuple, args) * kwds.get('repeat', 1)
+        pools = list(map(tuple, args) * kwds.get('repeat', 1))
         result = [[]]
         for pool in pools:
             result = [x + [y] for x in result for y in pool]
@@ -17,7 +20,6 @@ except ImportError:  # python 2.5
 # OrderedDict Shim from  Raymond Hettinger, python core dev
 # http://code.activestate.com/recipes/576693-ordered-dictionary-for-py24/
 # here to support versions before 2.6
-import sys
 try:
     from thread import get_ident as _get_ident
 except ImportError:
@@ -29,6 +31,14 @@ except ImportError:
     pass
 
 
+def iteritems(obj):
+    """replacement for six's iteritems to use iteritems on PandasObjects"""
+    if hasattr(obj, "iteritems"):
+        return obj.iteritems()
+    else:
+        return obj.items()
+
+
 class _OrderedDict(dict):
     'Dictionary that remembers insertion order'
     # An inherited dict maps keys to values.
@@ -98,7 +108,7 @@ class _OrderedDict(dict):
     def clear(self):
         'od.clear() -> None.  Remove all items from od.'
         try:
-            for node in self.__map.itervalues():
+            for node in six.itervalues(self.__map):
                 del node[:]
             root = self.__root
             root[:] = [root, root, None]
@@ -323,8 +333,8 @@ class _Counter(dict):
 
         '''
         if n is None:
-            return sorted(self.iteritems(), key=itemgetter(1), reverse=True)
-        return nlargest(n, self.iteritems(), key=itemgetter(1))
+            return sorted(iteritems(self), key=itemgetter(1), reverse=True)
+        return nlargest(n, iteritems(self), key=itemgetter(1))
 
     def elements(self):
         '''Iterator over elements repeating each as many times as its count.
@@ -337,7 +347,7 @@ class _Counter(dict):
         elements() will ignore it.
 
         '''
-        for elem, count in self.iteritems():
+        for elem, count in iteritems(self):
             for _ in repeat(None, count):
                 yield elem
 
@@ -491,7 +501,7 @@ class OrderedDefaultdict(OrderedDict):
         self.default_factory = newdefault
         super(self.__class__, self).__init__(*newargs, **kwargs)
 
-    def __missing__ (self, key):
+    def __missing__(self, key):
         if self.default_factory is None:
             raise KeyError(key)
         self[key] = value = self.default_factory()
diff --git a/pandas/util/counter.py b/pandas/util/counter.py
index 29e8906fd..90e71d3b8 100644
--- a/pandas/util/counter.py
+++ b/pandas/util/counter.py
@@ -1,9 +1,12 @@
 # This is copied from collections in Python 2.7, for compatibility with older
 # versions of Python. It can be dropped when we depend on Python 2.7/3.1
 
+from pandas.util import compat
 import heapq as _heapq
 from itertools import repeat as _repeat, chain as _chain, starmap as _starmap
 from operator import itemgetter as _itemgetter
+import six
+from six.moves import map
 
 try:
     from collections import Mapping
@@ -92,8 +95,8 @@ class Counter(dict):
         '''
         # Emulate Bag.sortedByCount from Smalltalk
         if n is None:
-            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)
-        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))
+            return sorted(compat.iteritems(self), key=_itemgetter(1), reverse=True)
+        return _heapq.nlargest(n, compat.iteritems(self), key=_itemgetter(1))
 
     def elements(self):
         '''Iterator over elements repeating each as many times as its count.
@@ -115,7 +118,7 @@ class Counter(dict):
 
         '''
         # Emulate Bag.do from Smalltalk and Multiset.begin from C++.
-        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))
+        return _chain.from_iterable(_starmap(_repeat, compat.iteritems(self)))
 
     # Override dict methods where necessary
 
@@ -150,7 +153,7 @@ class Counter(dict):
             if isinstance(iterable, Mapping):
                 if self:
                     self_get = self.get
-                    for elem, count in iterable.iteritems():
+                    for elem, count in compat.iteritems(iterable):
                         self[elem] = self_get(elem, 0) + count
                 else:
                     # fast path when counter is empty
diff --git a/pandas/util/decorators.py b/pandas/util/decorators.py
index 97b2ee335..4a8762dcb 100644
--- a/pandas/util/decorators.py
+++ b/pandas/util/decorators.py
@@ -5,7 +5,7 @@ import warnings
 
 
 def deprecate(name, alternative):
-    alt_name = alternative.func_name
+    alt_name = alternative.__name__
 
     def wrapper(*args, **kwargs):
         warnings.warn("%s is deprecated. Use %s instead" % (name, alt_name),
@@ -107,7 +107,7 @@ class Appender(object):
 
 
 def indent(text, indents=1):
-    if not text or type(text) != str:
+    if not text or not isinstance(text, str):
         return ''
     jointext = ''.join(['\n'] + ['    '] * indents)
     return jointext.join(text.split('\n'))
diff --git a/pandas/util/py3compat.py b/pandas/util/py3compat.py
index dcc877b09..240f8c0fc 100644
--- a/pandas/util/py3compat.py
+++ b/pandas/util/py3compat.py
@@ -12,7 +12,8 @@ if PY3:
     def bytes_to_str(b, encoding='utf-8'):
         return b.decode(encoding)
 
-    lzip = lambda *args: list(zip(*args))
+    range = range
+    long = int
 else:
     # Python 2
     import re
@@ -27,12 +28,18 @@ else:
     def bytes_to_str(b, encoding='ascii'):
         return b
 
-    lzip = zip
+    range = xrange
+    long = long
 
 try:
-    from cStringIO import StringIO
+    # not writeable if instantiated with string, not good with unicode
+    from cStringIO import StringIO as cStringIO
+    # writeable and handles unicode
+    from StringIO import StringIO
 except:
+    # no more StringIO
     from io import StringIO
+    cStringIO = StringIO
 
 try:
     from io import BytesIO
diff --git a/pandas/util/terminal.py b/pandas/util/terminal.py
index 3b5f893d1..fc985855d 100644
--- a/pandas/util/terminal.py
+++ b/pandas/util/terminal.py
@@ -11,6 +11,7 @@ Harco Kuppens (http://stackoverflow.com/users/825214/harco-kuppens)
 It is mentioned in the stackoverflow response that this code works
 on linux, os x, windows and cygwin (windows).
 """
+from __future__ import print_function
 
 import os
 
@@ -117,4 +118,4 @@ def _get_terminal_size_linux():
 
 if __name__ == "__main__":
     sizex, sizey = get_terminal_size()
-    print ('width = %s height = %s' % (sizex, sizey))
+    print('width = %s height = %s' % (sizex, sizey))
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 7b2960ef4..6b710b442 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -2,6 +2,8 @@ from __future__ import division
 
 # pylint: disable-msg=W0402
 
+from pandas.util.py3compat import range
+from six.moves import zip
 import random
 import string
 import sys
@@ -26,11 +28,16 @@ import pandas.core.series as series
 import pandas.core.frame as frame
 import pandas.core.panel as panel
 import pandas.core.panel4d as panel4d
+import pandas.util.compat as compat
 
 from pandas import bdate_range
 from pandas.tseries.index import DatetimeIndex
 from pandas.tseries.period import PeriodIndex
 
+from pandas.io.common import urlopen
+import six
+from six.moves import map
+
 Index = index.Index
 MultiIndex = index.MultiIndex
 Series = series.Series
@@ -45,12 +52,13 @@ _RAISE_NETWORK_ERROR_DEFAULT = False
 
 def rands(n):
     choices = string.ascii_letters + string.digits
-    return ''.join(random.choice(choices) for _ in xrange(n))
+    return ''.join(random.choice(choices) for _ in range(n))
 
 
 def randu(n):
-    choices = u"".join(map(unichr, range(1488, 1488 + 26))) + string.digits
-    return ''.join([random.choice(choices) for _ in xrange(n)])
+    choices = six.u("").join(map(unichr, list(range(1488, 1488 + 26))))
+    choices += string.digits
+    return ''.join([random.choice(choices) for _ in range(n)])
 
 #------------------------------------------------------------------------------
 # Console debugging tools
@@ -123,8 +131,8 @@ def assert_almost_equal(a, b, check_less_precise = False):
     if isinstance(a, dict) or isinstance(b, dict):
         return assert_dict_equal(a, b)
 
-    if isinstance(a, basestring):
-        assert a == b, "%r != %r" % (a, b)
+    if isinstance(a, six.string_types):
+        assert a == b, "%s != %s" % (a, b)
         return True
 
     if isiterable(a):
@@ -135,7 +143,7 @@ def assert_almost_equal(a, b, check_less_precise = False):
         if np.array_equal(a, b):
             return True
         else:
-            for i in xrange(na):
+            for i in range(na):
                 assert_almost_equal(a[i], b[i], check_less_precise)
         return True
 
@@ -258,7 +266,7 @@ def assert_panel_equal(left, right,
     assert(left.major_axis.equals(right.major_axis))
     assert(left.minor_axis.equals(right.minor_axis))
 
-    for col, series in left.iterkv():
+    for col, series in compat.iteritems(left):
         assert(col in right)
         assert_frame_equal(series, right[col], check_less_precise=check_less_precise, check_names=False)  # TODO strangely check_names fails in py3 ?
 
@@ -273,7 +281,7 @@ def assert_panel4d_equal(left, right,
     assert(left.major_axis.equals(right.major_axis))
     assert(left.minor_axis.equals(right.minor_axis))
 
-    for col, series in left.iterkv():
+    for col, series in compat.iteritems(left):
         assert(col in right)
         assert_panel_equal(series, right[col], check_less_precise=check_less_precise)
 
@@ -291,15 +299,15 @@ def getCols(k):
 
 
 def makeStringIndex(k):
-    return Index([rands(10) for _ in xrange(k)])
+    return Index([rands(10) for _ in range(k)])
 
 
 def makeUnicodeIndex(k):
-    return Index([randu(10) for _ in xrange(k)])
+    return Index([randu(10) for _ in range(k)])
 
 
 def makeIntIndex(k):
-    return Index(range(k))
+    return Index(list(range(k)))
 
 
 def makeFloatIndex(k):
@@ -444,7 +452,7 @@ def makeCustomIndex(nentries, nlevels, prefix='#', names=False, ndupe_l=None,
         names = None
 
     # make singelton case uniform
-    if isinstance(names, basestring) and nlevels == 1:
+    if isinstance(names, six.string_types) and nlevels == 1:
         names = [names]
 
     # specific 1D index type requested?
@@ -471,7 +479,7 @@ def makeCustomIndex(nentries, nlevels, prefix='#', names=False, ndupe_l=None,
         def keyfunc(x):
             import re
             numeric_tuple = re.sub("[^\d_]_?","",x).split("_")
-            return map(int,numeric_tuple)
+            return list(map(int,numeric_tuple))
 
         # build a list of lists to create the index from
         div_factor = nentries // ndupe_l[i] + 1
@@ -483,7 +491,7 @@ def makeCustomIndex(nentries, nlevels, prefix='#', names=False, ndupe_l=None,
         result = list(sorted(cnt.elements(), key=keyfunc))[:nentries]
         tuples.append(result)
 
-    tuples = zip(*tuples)
+    tuples = list(zip(*tuples))
 
     # convert tuples to index
     if nentries == 1:
diff --git a/scripts/bench_join.py b/scripts/bench_join.py
index be24dac81..758a4fedd 100644
--- a/scripts/bench_join.py
+++ b/scripts/bench_join.py
@@ -1,3 +1,4 @@
+from pandas.util.py3compat import range
 import numpy as np
 import pandas.lib as lib
 from pandas import *
@@ -27,8 +28,8 @@ bvf = np.random.randn(n, K)
 a_series = Series(av, index=a)
 b_series = Series(bv, index=b)
 
-a_frame = DataFrame(avf, index=a, columns=range(K))
-b_frame = DataFrame(bvf, index=b, columns=range(K, 2 * K))
+a_frame = DataFrame(avf, index=a, columns=list(range(K)))
+b_frame = DataFrame(bvf, index=b, columns=list(range(K, 2 * K)))
 
 
 def do_left_join(a, b, av, bv):
@@ -77,7 +78,7 @@ def do_left_join_python(a, b, av, bv):
 def _take_multi(data, indexer, out):
     if not data.flags.c_contiguous:
         data = data.copy()
-    for i in xrange(data.shape[0]):
+    for i in range(data.shape[0]):
         data[i].take(indexer, out=out[i])
 
 
@@ -162,8 +163,8 @@ def bench_python(n=100000, pct_overlap=0.20, K=1):
         avf = np.random.randn(n, K)
         bvf = np.random.randn(n, K)
 
-        a_frame = DataFrame(avf, index=a, columns=range(K))
-        b_frame = DataFrame(bvf, index=b, columns=range(K, 2 * K))
+        a_frame = DataFrame(avf, index=a, columns=list(range(K)))
+        b_frame = DataFrame(bvf, index=b, columns=list(range(K, 2 * K)))
 
         all_results[logn] = result = {}
 
diff --git a/scripts/bench_join_multi.py b/scripts/bench_join_multi.py
index cdac37f28..0683fbb67 100644
--- a/scripts/bench_join_multi.py
+++ b/scripts/bench_join_multi.py
@@ -1,26 +1,27 @@
 from pandas import *
 
 import numpy as np
-from itertools import izip
+from six.moves import zip
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 import pandas.lib as lib
 
 N = 100000
 
-key1 = [rands(10) for _ in xrange(N)]
-key2 = [rands(10) for _ in xrange(N)]
+key1 = [rands(10) for _ in range(N)]
+key2 = [rands(10) for _ in range(N)]
 
-zipped = izip(key1, key2)
+zipped = list(zip(key1, key2))
 
 
 def _zip(*args):
     arr = np.empty(N, dtype=object)
-    arr[:] = zip(*args)
+    arr[:] = list(zip(*args))
     return arr
 
 
 def _zip2(*args):
-    return lib.list_to_object_array(zip(*args))
+    return lib.list_to_object_array(list(zip(*args)))
 
 index = MultiIndex.from_arrays([key1, key2])
 to_join = DataFrame({'j1': np.random.randn(100000)}, index=index)
diff --git a/scripts/bench_refactor.py b/scripts/bench_refactor.py
index 3d0c7e40c..812c42b0e 100644
--- a/scripts/bench_refactor.py
+++ b/scripts/bench_refactor.py
@@ -1,4 +1,5 @@
 from pandas import *
+from pandas.util.py3compat import range
 try:
     import pandas.core.internals as internals
     reload(internals)
@@ -17,7 +18,7 @@ def horribly_unconsolidated():
 
     df = DataMatrix(index=index)
 
-    for i in xrange(K):
+    for i in range(K):
         df[i] = float(K)
 
     return df
@@ -25,13 +26,13 @@ def horribly_unconsolidated():
 
 def bench_reindex_index(df, it=100):
     new_idx = np.arange(0, N, 2)
-    for i in xrange(it):
+    for i in range(it):
         df.reindex(new_idx)
 
 
 def bench_reindex_columns(df, it=100):
     new_cols = np.arange(0, K, 2)
-    for i in xrange(it):
+    for i in range(it):
         df.reindex(columns=new_cols)
 
 
@@ -39,7 +40,7 @@ def bench_join_index(df, it=10):
     left = df.reindex(index=np.arange(0, N, 2),
                       columns=np.arange(K // 2))
     right = df.reindex(columns=np.arange(K // 2 + 1, K))
-    for i in xrange(it):
+    for i in range(it):
         joined = left.join(right)
 
 if __name__ == '__main__':
diff --git a/scripts/file_sizes.py b/scripts/file_sizes.py
index 8720730d2..12cd12c25 100644
--- a/scripts/file_sizes.py
+++ b/scripts/file_sizes.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 import os
 import sys
 
@@ -6,6 +7,7 @@ import matplotlib.pyplot as plt
 
 from pandas import DataFrame
 from pandas.util.testing import set_trace
+from pandas.util import compat
 
 dirs = []
 names = []
@@ -154,13 +156,13 @@ assert(result == expected)
 
 def doit():
     for directory, _, files in walked:
-        print directory
+        print(directory)
         for path in files:
             if not _should_count_file(path):
                 continue
 
             full_path = os.path.join(directory, path)
-            print full_path
+            print(full_path)
             lines = len(open(full_path).readlines())
 
             dirs.append(directory)
@@ -174,7 +176,7 @@ def doit():
 def doit2():
     counts = {}
     for directory, _, files in walked:
-        print directory
+        print(directory)
         for path in files:
             if not _should_count_file(path) or path.startswith('test_'):
                 continue
@@ -189,7 +191,7 @@ counts = doit2()
 # counts = _get_file_function_lengths('pandas/tests/test_series.py')
 
 all_counts = []
-for k, v in counts.iteritems():
+for k, v in compat.iteritems(counts):
     all_counts.extend(v)
 all_counts = np.array(all_counts)
 
diff --git a/scripts/find_commits_touching_func.py b/scripts/find_commits_touching_func.py
index d23889ec8..925d40d0f 100755
--- a/scripts/find_commits_touching_func.py
+++ b/scripts/find_commits_touching_func.py
@@ -4,6 +4,9 @@
 # copryright 2013, y-p @ github
 
 from __future__ import print_function
+from pandas.util.py3compat import range
+import six
+from six.moves import map
 
 """Search the git history for all commits touching a named method
 
@@ -93,7 +96,7 @@ def get_hits(defname,files=()):
 
 def get_commit_info(c,fmt,sep='\t'):
     r=sh.git('log', "--format={}".format(fmt), '{}^..{}'.format(c,c),"-n","1",_tty_out=False)
-    return unicode(r).split(sep)
+    return six.text_type(r).split(sep)
 
 def get_commit_vitals(c,hlen=HASH_LEN):
     h,s,d= get_commit_info(c,'%H\t%s\t%ci',"\t")
@@ -159,7 +162,7 @@ def pprint_hits(hits):
 
     print("\nThese commits touched the %s method in these files on these dates:\n" \
           % args.funcname)
-    for i in sorted(range(len(hits)),key=sorter):
+    for i in sorted(list(range(len(hits))),key=sorter):
         hit = hits[i]
         h,s,d=get_commit_vitals(hit.commit)
         p=hit.path.split(os.path.realpath(os.curdir)+os.path.sep)[-1]
@@ -182,11 +185,11 @@ You must specify the -y argument to ignore this warning.
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 """)
         return
-    if isinstance(args.file_masks,basestring):
+    if isinstance(args.file_masks,six.string_types):
         args.file_masks = args.file_masks.split(',')
-    if isinstance(args.path_masks,basestring):
+    if isinstance(args.path_masks,six.string_types):
         args.path_masks = args.path_masks.split(',')
-    if isinstance(args.dir_masks,basestring):
+    if isinstance(args.dir_masks,six.string_types):
         args.dir_masks = args.dir_masks.split(',')
 
     logger.setLevel(getattr(logging,args.debug_level))
diff --git a/scripts/find_undoc_args.py b/scripts/find_undoc_args.py
index 4a4099afc..f6bcd4318 100755
--- a/scripts/find_undoc_args.py
+++ b/scripts/find_undoc_args.py
@@ -41,18 +41,18 @@ def entry_gen(root_ns,module_name):
                 seen.add(cand.__name__)
                 q.insert(0,cand)
             elif (isinstance(cand,(types.MethodType,types.FunctionType)) and
-                  cand not in seen and cand.func_doc):
+                  cand not in seen and cand.__doc__):
                 seen.add(cand)
                 yield cand
 
 def cmp_docstring_sig(f):
     def build_loc(f):
-        path=f.func_code.co_filename.split(args.path,1)[-1][1:]
-        return dict(path=path,lnum=f.func_code.co_firstlineno)
+        path=f.__code__.co_filename.split(args.path,1)[-1][1:]
+        return dict(path=path,lnum=f.__code__.co_firstlineno)
 
     import inspect
     sig_names=set(inspect.getargspec(f).args)
-    doc = f.func_doc.lower()
+    doc = f.__doc__.lower()
     doc = re.split("^\s*parameters\s*",doc,1,re.M)[-1]
     doc = re.split("^\s*returns*",doc,1,re.M)[0]
     doc_names={x.split(":")[0].strip() for x in doc.split("\n")
diff --git a/scripts/gen_release_notes.py b/scripts/gen_release_notes.py
index c64b33d71..905240fcf 100644
--- a/scripts/gen_release_notes.py
+++ b/scripts/gen_release_notes.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 import sys
 import urllib2
 import json
@@ -93,4 +94,4 @@ def release_notes(milestone):
 if __name__ == '__main__':
 
     rs = release_notes(sys.argv[1])
-    print rs
+    print(rs)
diff --git a/scripts/groupby_sample.py b/scripts/groupby_sample.py
index 8685b2bbe..af422bd4b 100644
--- a/scripts/groupby_sample.py
+++ b/scripts/groupby_sample.py
@@ -1,6 +1,8 @@
 from pandas import *
 import numpy as np
 import string
+import six
+import pandas.util.compat as compat
 
 g1 = np.array(list(string.letters))[:-1]
 g2 = np.arange(510)
@@ -30,7 +32,7 @@ def random_sample_v2():
     grouped = df.groupby(['group1', 'group2'])['value']
     from random import choice
     choose = lambda group: choice(group.index)
-    indices = [choice(v) for k, v in grouped.groups.iteritems()]
+    indices = [choice(v) for k, v in compat.iteritems(grouped.groups)]
     return df.reindex(indices)
 
 
@@ -43,7 +45,7 @@ def do_shuffle(arr):
 
 def shuffle_uri(df, grouped):
     perm = np.r_[tuple([np.random.permutation(
-        idxs) for idxs in grouped.groups.itervalues()])]
+        idxs) for idxs in six.itervalues(grouped.groups)])]
     df['state_permuted'] = np.asarray(df.ix[perm]['value'])
 
 df2 = df.copy()
diff --git a/scripts/groupby_speed.py b/scripts/groupby_speed.py
index a25b00206..4e60c3455 100644
--- a/scripts/groupby_speed.py
+++ b/scripts/groupby_speed.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 from pandas import *
 
 rng = DateRange('1/3/2011', '11/30/2011', offset=datetools.Minute())
@@ -23,12 +24,12 @@ def get2(dt):
 def f():
     for i, date in enumerate(df.index):
         if i % 10000 == 0:
-            print i
+            print(i)
         get1(date)
 
 
 def g():
     for i, date in enumerate(df.index):
         if i % 10000 == 0:
-            print i
+            print(i)
         get2(date)
diff --git a/scripts/groupby_test.py b/scripts/groupby_test.py
index 76c9cb0cb..6dbf1b073 100644
--- a/scripts/groupby_test.py
+++ b/scripts/groupby_test.py
@@ -8,6 +8,7 @@ from pandas import *
 import pandas.lib as tseries
 import pandas.core.groupby as gp
 import pandas.util.testing as tm
+from pandas.util.py3compat import range
 reload(gp)
 
 """
diff --git a/scripts/hdfstore_panel_perf.py b/scripts/hdfstore_panel_perf.py
index d344fc809..18668d729 100644
--- a/scripts/hdfstore_panel_perf.py
+++ b/scripts/hdfstore_panel_perf.py
@@ -1,13 +1,14 @@
 from pandas import *
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 
 i, j, k = 7, 771, 5532
 
 panel = Panel(np.random.randn(i, j, k),
-              items=[rands(10) for _ in xrange(i)],
+              items=[rands(10) for _ in range(i)],
               major_axis=DateRange('1/1/2000', periods=j,
                                    offset=datetools.Minute()),
-              minor_axis=[rands(10) for _ in xrange(k)])
+              minor_axis=[rands(10) for _ in range(k)])
 
 
 store = HDFStore('test.h5')
diff --git a/scripts/json_manip.py b/scripts/json_manip.py
index e76a99cca..29c2d88aa 100644
--- a/scripts/json_manip.py
+++ b/scripts/json_manip.py
@@ -65,6 +65,7 @@ Simplifying
     themselves.
 
 """
+from __future__ import print_function
 
 from collections import Counter, namedtuple
 import csv
@@ -73,7 +74,9 @@ from itertools import product
 from operator import attrgetter as aget, itemgetter as iget
 import operator
 import sys
-
+import six
+from six.moves import map
+import pandas.util.compat as compat
 
 
 ##  note 'url' appears multiple places and not all extensions have same struct
@@ -89,77 +92,77 @@ ex1 = {
 }
 
 ## much longer example
-ex2 = {u'metadata': {u'accessibilities': [{u'name': u'accessibility.tabfocus',
-    u'value': 7},
-   {u'name': u'accessibility.mouse_focuses_formcontrol', u'value': False},
-   {u'name': u'accessibility.browsewithcaret', u'value': False},
-   {u'name': u'accessibility.win32.force_disabled', u'value': False},
-   {u'name': u'accessibility.typeaheadfind.startlinksonly', u'value': False},
-   {u'name': u'accessibility.usebrailledisplay', u'value': u''},
-   {u'name': u'accessibility.typeaheadfind.timeout', u'value': 5000},
-   {u'name': u'accessibility.typeaheadfind.enabletimeout', u'value': True},
-   {u'name': u'accessibility.tabfocus_applies_to_xul', u'value': False},
-   {u'name': u'accessibility.typeaheadfind.flashBar', u'value': 1},
-   {u'name': u'accessibility.typeaheadfind.autostart', u'value': True},
-   {u'name': u'accessibility.blockautorefresh', u'value': False},
-   {u'name': u'accessibility.browsewithcaret_shortcut.enabled',
-    u'value': True},
-   {u'name': u'accessibility.typeaheadfind.enablesound', u'value': True},
-   {u'name': u'accessibility.typeaheadfind.prefillwithselection',
-    u'value': True},
-   {u'name': u'accessibility.typeaheadfind.soundURL', u'value': u'beep'},
-   {u'name': u'accessibility.typeaheadfind', u'value': False},
-   {u'name': u'accessibility.typeaheadfind.casesensitive', u'value': 0},
-   {u'name': u'accessibility.warn_on_browsewithcaret', u'value': True},
-   {u'name': u'accessibility.usetexttospeech', u'value': u''},
-   {u'name': u'accessibility.accesskeycausesactivation', u'value': True},
-   {u'name': u'accessibility.typeaheadfind.linksonly', u'value': False},
-   {u'name': u'isInstantiated', u'value': True}],
-  u'extensions': [{u'id': u'216ee7f7f4a5b8175374cd62150664efe2433a31',
-    u'isEnabled': True},
-   {u'id': u'1aa53d3b720800c43c4ced5740a6e82bb0b3813e', u'isEnabled': False},
-   {u'id': u'01ecfac5a7bd8c9e27b7c5499e71c2d285084b37', u'isEnabled': True},
-   {u'id': u'1c01f5b22371b70b312ace94785f7b0b87c3dfb2', u'isEnabled': True},
-   {u'id': u'fb723781a2385055f7d024788b75e959ad8ea8c3', u'isEnabled': True}],
-  u'fxVersion': u'9.0',
-  u'location': u'zh-CN',
-  u'operatingSystem': u'WINNT Windows NT 5.1',
-  u'surveyAnswers': u'',
-  u'task_guid': u'd69fbd15-2517-45b5-8a17-bb7354122a75',
-  u'tpVersion': u'1.2',
-  u'updateChannel': u'beta'},
- u'survey_data': {
-  u'extensions': [{u'appDisabled': False,
-    u'id': u'testpilot?labs.mozilla.com',
-    u'isCompatible': True,
-    u'isEnabled': True,
-    u'isPlatformCompatible': True,
-    u'name': u'Test Pilot'},
-   {u'appDisabled': True,
-    u'id': u'dict?www.youdao.com',
-    u'isCompatible': False,
-    u'isEnabled': False,
-    u'isPlatformCompatible': True,
-    u'name': u'Youdao Word Capturer'},
-   {u'appDisabled': False,
-    u'id': u'jqs?sun.com',
-    u'isCompatible': True,
-    u'isEnabled': True,
-    u'isPlatformCompatible': True,
-    u'name': u'Java Quick Starter'},
-   {u'appDisabled': False,
-    u'id': u'?20a82645-c095-46ed-80e3-08825760534b?',
-    u'isCompatible': True,
-    u'isEnabled': True,
-    u'isPlatformCompatible': True,
-    u'name': u'Microsoft .NET Framework Assistant'},
-   {u'appDisabled': False,
-    u'id': u'?a0d7ccb3-214d-498b-b4aa-0e8fda9a7bf7?',
-    u'isCompatible': True,
-    u'isEnabled': True,
-    u'isPlatformCompatible': True,
-    u'name': u'WOT'}],
-  u'version_number': 1}}
+ex2 = {six.u('metadata'): {six.u('accessibilities'): [{six.u('name'): six.u('accessibility.tabfocus'),
+    six.u('value'): 7},
+   {six.u('name'): six.u('accessibility.mouse_focuses_formcontrol'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.browsewithcaret'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.win32.force_disabled'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.typeaheadfind.startlinksonly'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.usebrailledisplay'), six.u('value'): six.u('')},
+   {six.u('name'): six.u('accessibility.typeaheadfind.timeout'), six.u('value'): 5000},
+   {six.u('name'): six.u('accessibility.typeaheadfind.enabletimeout'), six.u('value'): True},
+   {six.u('name'): six.u('accessibility.tabfocus_applies_to_xul'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.typeaheadfind.flashBar'), six.u('value'): 1},
+   {six.u('name'): six.u('accessibility.typeaheadfind.autostart'), six.u('value'): True},
+   {six.u('name'): six.u('accessibility.blockautorefresh'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.browsewithcaret_shortcut.enabled'),
+    six.u('value'): True},
+   {six.u('name'): six.u('accessibility.typeaheadfind.enablesound'), six.u('value'): True},
+   {six.u('name'): six.u('accessibility.typeaheadfind.prefillwithselection'),
+    six.u('value'): True},
+   {six.u('name'): six.u('accessibility.typeaheadfind.soundURL'), six.u('value'): six.u('beep')},
+   {six.u('name'): six.u('accessibility.typeaheadfind'), six.u('value'): False},
+   {six.u('name'): six.u('accessibility.typeaheadfind.casesensitive'), six.u('value'): 0},
+   {six.u('name'): six.u('accessibility.warn_on_browsewithcaret'), six.u('value'): True},
+   {six.u('name'): six.u('accessibility.usetexttospeech'), six.u('value'): six.u('')},
+   {six.u('name'): six.u('accessibility.accesskeycausesactivation'), six.u('value'): True},
+   {six.u('name'): six.u('accessibility.typeaheadfind.linksonly'), six.u('value'): False},
+   {six.u('name'): six.u('isInstantiated'), six.u('value'): True}],
+  six.u('extensions'): [{six.u('id'): six.u('216ee7f7f4a5b8175374cd62150664efe2433a31'),
+    six.u('isEnabled'): True},
+   {six.u('id'): six.u('1aa53d3b720800c43c4ced5740a6e82bb0b3813e'), six.u('isEnabled'): False},
+   {six.u('id'): six.u('01ecfac5a7bd8c9e27b7c5499e71c2d285084b37'), six.u('isEnabled'): True},
+   {six.u('id'): six.u('1c01f5b22371b70b312ace94785f7b0b87c3dfb2'), six.u('isEnabled'): True},
+   {six.u('id'): six.u('fb723781a2385055f7d024788b75e959ad8ea8c3'), six.u('isEnabled'): True}],
+  six.u('fxVersion'): six.u('9.0'),
+  six.u('location'): six.u('zh-CN'),
+  six.u('operatingSystem'): six.u('WINNT Windows NT 5.1'),
+  six.u('surveyAnswers'): six.u(''),
+  six.u('task_guid'): six.u('d69fbd15-2517-45b5-8a17-bb7354122a75'),
+  six.u('tpVersion'): six.u('1.2'),
+  six.u('updateChannel'): six.u('beta')},
+ six.u('survey_data'): {
+  six.u('extensions'): [{six.u('appDisabled'): False,
+    six.u('id'): six.u('testpilot?labs.mozilla.com'),
+    six.u('isCompatible'): True,
+    six.u('isEnabled'): True,
+    six.u('isPlatformCompatible'): True,
+    six.u('name'): six.u('Test Pilot')},
+   {six.u('appDisabled'): True,
+    six.u('id'): six.u('dict?www.youdao.com'),
+    six.u('isCompatible'): False,
+    six.u('isEnabled'): False,
+    six.u('isPlatformCompatible'): True,
+    six.u('name'): six.u('Youdao Word Capturer')},
+   {six.u('appDisabled'): False,
+    six.u('id'): six.u('jqs?sun.com'),
+    six.u('isCompatible'): True,
+    six.u('isEnabled'): True,
+    six.u('isPlatformCompatible'): True,
+    six.u('name'): six.u('Java Quick Starter')},
+   {six.u('appDisabled'): False,
+    six.u('id'): six.u('?20a82645-c095-46ed-80e3-08825760534b?'),
+    six.u('isCompatible'): True,
+    six.u('isEnabled'): True,
+    six.u('isPlatformCompatible'): True,
+    six.u('name'): six.u('Microsoft .NET Framework Assistant')},
+   {six.u('appDisabled'): False,
+    six.u('id'): six.u('?a0d7ccb3-214d-498b-b4aa-0e8fda9a7bf7?'),
+    six.u('isCompatible'): True,
+    six.u('isEnabled'): True,
+    six.u('isPlatformCompatible'): True,
+    six.u('name'): six.u('WOT')}],
+  six.u('version_number'): 1}}
 
 # class SurveyResult(object):
 
@@ -267,7 +270,7 @@ def flatten(*stack):
     """
     stack = list(stack)
     while stack:
-        try: x = stack[0].next()
+        try: x = next(stack[0])
         except StopIteration:
             stack.pop(0)
             continue
@@ -282,7 +285,7 @@ def _Q(filter_, thing):
     """ underlying machinery for Q function recursion """
     T = type(thing)
     if T is type({}):
-        for k,v in thing.iteritems():
+        for k,v in compat.iteritems(thing):
             #print k,v
             if filter_ == k:
                 if type(v) is type([]):
@@ -386,34 +389,34 @@ def printout(queries,things,default=None, f=sys.stdout, **kwargs):
 
 
 def test_run():
-    print "\n>>> print list(Q('url',ex1))"
-    print list(Q('url',ex1))
+    print("\n>>> print list(Q('url',ex1))")
+    print(list(Q('url',ex1)))
     assert  list(Q('url',ex1)) == ['url1','url2','url3']
     assert Ql('url',ex1) == ['url1','url2','url3']
 
-    print "\n>>>  print list(Q(['name','id'],ex1))"
-    print list(Q(['name','id'],ex1))
+    print("\n>>>  print list(Q(['name','id'],ex1))")
+    print(list(Q(['name','id'],ex1)))
     assert Ql(['name','id'],ex1) == ['Gregg','hello','gbye']
 
 
-    print "\n>>> print Ql('more url',ex1)"
-    print Ql('more url',ex1)
+    print("\n>>> print Ql('more url',ex1)")
+    print(Ql('more url',ex1))
 
 
-    print "\n>>> list(Q('extensions',ex1))"
-    print list(Q('extensions',ex1))
+    print("\n>>> list(Q('extensions',ex1))")
+    print(list(Q('extensions',ex1)))
 
-    print "\n>>> print Ql('extensions',ex1)"
-    print Ql('extensions',ex1)
+    print("\n>>> print Ql('extensions',ex1)")
+    print(Ql('extensions',ex1))
 
-    print "\n>>> printout(['name','extensions'],[ex1,], extrasaction='ignore')"
+    print("\n>>> printout(['name','extensions'],[ex1,], extrasaction='ignore')")
     printout(['name','extensions'],[ex1,], extrasaction='ignore')
 
-    print "\n\n"
+    print("\n\n")
 
     from pprint import pprint as pp
 
-    print "-- note that the extension fields are also flattened!  (and N/A) -- "
+    print("-- note that the extension fields are also flattened!  (and N/A) -- ")
     pp(denorm(['location','fxVersion','notthere','survey_data extensions'],[ex2,], default="N/A")[:2])
 
 
diff --git a/scripts/leak.py b/scripts/leak.py
index 3d704af4f..5b81a3dfc 100644
--- a/scripts/leak.py
+++ b/scripts/leak.py
@@ -1,4 +1,5 @@
 from pandas import *
+from pandas.util.py3compat import range
 import numpy as np
 import pandas.util.testing as tm
 import os
diff --git a/scripts/parser_magic.py b/scripts/parser_magic.py
index c35611350..17bdba165 100644
--- a/scripts/parser_magic.py
+++ b/scripts/parser_magic.py
@@ -1,5 +1,6 @@
 from pandas.util.testing import set_trace
 import pandas.util.testing as tm
+import pandas.util.compat as compat
 
 from pandas import *
 import ast
@@ -45,7 +46,7 @@ def _format_call(call):
     if args:
         content += ', '.join(args)
     if kwds:
-        fmt_kwds = ['%s=%s' % item for item in kwds.iteritems()]
+        fmt_kwds = ['%s=%s' % item for item in compat.iteritems(kwds)]
         joined_kwds = ', '.join(fmt_kwds)
         if args:
             content = content + ', ' + joined_kwds
diff --git a/scripts/roll_median_leak.py b/scripts/roll_median_leak.py
index 6441a69f3..6dbb1a74d 100644
--- a/scripts/roll_median_leak.py
+++ b/scripts/roll_median_leak.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 from pandas import *
 
 import numpy as np
@@ -5,6 +6,7 @@ import os
 
 from vbench.api import Benchmark
 from pandas.util.testing import rands
+from pandas.util.py3compat import range
 import pandas.lib as lib
 import pandas._sandbox as sbx
 import time
@@ -18,7 +20,7 @@ lst = SparseList()
 lst.append([5] * 10000)
 lst.append(np.repeat(np.nan, 1000000))
 
-for _ in xrange(10000):
-    print proc.get_memory_info()
+for _ in range(10000):
+    print(proc.get_memory_info())
     sdf = SparseDataFrame({'A': lst.to_array()})
     chunk = sdf[sdf['A'] == 5]
diff --git a/scripts/runtests.py b/scripts/runtests.py
index b995db65a..e14752b43 100644
--- a/scripts/runtests.py
+++ b/scripts/runtests.py
@@ -1,4 +1,5 @@
+from __future__ import print_function
 import os
-print os.getpid()
+print(os.getpid())
 import nose
 nose.main('pandas.core')
diff --git a/scripts/testmed.py b/scripts/testmed.py
index ed0f76cd2..c90734912 100644
--- a/scripts/testmed.py
+++ b/scripts/testmed.py
@@ -2,6 +2,9 @@
 
 from random import random
 from math import log, ceil
+from pandas.util.py3compat import range
+from numpy.random import randn
+from pandas.lib.skiplist import rolling_median
 
 
 class Node(object):
@@ -138,8 +141,6 @@ def test():
 
     _test(arr, K)
 
-from numpy.random import randn
-from pandas.lib.skiplist import rolling_median
 
 
 def test2():
diff --git a/setup.py b/setup.py
index d66ac345a..a99ba8832 100755
--- a/setup.py
+++ b/setup.py
@@ -40,14 +40,12 @@ if sys.version_info[0] >= 3:
     if sys.version_info[1] >= 3:  # 3.3 needs numpy 1.7+
         min_numpy_ver = "1.7.0b2"
 
-    setuptools_kwargs = {'use_2to3': True,
+    setuptools_kwargs = {
                          'zip_safe': False,
                          'install_requires': ['python-dateutil >= 2',
                                               'pytz >= 2011k',
                                               'numpy >= %s' % min_numpy_ver],
                          'setup_requires': ['numpy >= %s' % min_numpy_ver],
-                         'use_2to3_exclude_fixers': ['lib2to3.fixes.fix_next',
-                                                     ],
                          }
     if not _have_setuptools:
         sys.exit("need setuptools/distribute for Py3k"
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index f38f42c89..665a33f92 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -1,5 +1,6 @@
 from vbench.api import Benchmark
 from datetime import datetime
+from six.moves import map
 
 common_setup = """from pandas_vb_common import *
 """
@@ -284,12 +285,12 @@ n_columns = 3
 share_na = 0.1
 
 dates = date_range('1997-12-31', periods=n_dates, freq='B')
-dates = Index(map(lambda x: x.year * 10000 + x.month * 100 + x.day, dates))
+dates = Index(list(map(lambda x: x.year * 10000 + x.month * 100 + x.day, dates)))
 
 secid_min = int('10000000', 16)
 secid_max = int('F0000000', 16)
 step = (secid_max - secid_min) // (n_securities - 1)
-security_ids = map(lambda x: hex(x)[2:10].upper(), range(secid_min, secid_max + 1, step))
+security_ids = list(map(lambda x: hex(x)[2:10].upper(), range(secid_min, secid_max + 1, step)))
 
 data_index = MultiIndex(levels=[dates.values, security_ids],
     labels=[[i for i in xrange(n_dates) for _ in xrange(n_securities)], range(n_securities) * n_dates],
diff --git a/vb_suite/indexing.py b/vb_suite/indexing.py
index 1264ae053..8a56ef8ff 100644
--- a/vb_suite/indexing.py
+++ b/vb_suite/indexing.py
@@ -106,6 +106,7 @@ indexing_dataframe_boolean = \
               start_date=datetime(2012, 1, 1))
 
 setup = common_setup + """
+from pandas.util.py3compat import range
 import pandas.core.expressions as expr
 df  = DataFrame(np.random.randn(50000, 100))
 df2 = DataFrame(np.random.randn(50000, 100))
diff --git a/vb_suite/make.py b/vb_suite/make.py
index 5a8a8215d..74a0818fb 100755
--- a/vb_suite/make.py
+++ b/vb_suite/make.py
@@ -71,7 +71,7 @@ def auto_update():
         html()
         upload()
         sendmail()
-    except (Exception, SystemExit), inst:
+    except (Exception, SystemExit) as inst:
         msg += str(inst) + '\n'
         sendmail(msg)
 
diff --git a/vb_suite/measure_memory_consumption.py b/vb_suite/measure_memory_consumption.py
index bb73cf5da..8d15b7806 100755
--- a/vb_suite/measure_memory_consumption.py
+++ b/vb_suite/measure_memory_consumption.py
@@ -45,7 +45,7 @@ def main():
 
         s = Series(results)
         s.sort()
-        print((s))
+        print(s)
 
     finally:
         shutil.rmtree(TMP_DIR)
diff --git a/vb_suite/pandas_vb_common.py b/vb_suite/pandas_vb_common.py
index 77d0e2e27..37775557f 100644
--- a/vb_suite/pandas_vb_common.py
+++ b/vb_suite/pandas_vb_common.py
@@ -4,6 +4,7 @@ from datetime import timedelta
 from numpy.random import randn
 from numpy.random import randint
 from numpy.random import permutation
+import pandas.util.compat as compat
 import pandas.util.testing as tm
 import random
 import numpy as np
@@ -23,3 +24,9 @@ try:
     from pandas.core.index import MultiIndex
 except ImportError:
     pass
+try:
+    # if no range in py3compat, then don't import zip or map either
+    from pandas.util.py3compat import range
+    from six.moves import zip, map
+except ImportError:
+    pass
diff --git a/vb_suite/parser.py b/vb_suite/parser.py
index 50d37f377..8bcba2b20 100644
--- a/vb_suite/parser.py
+++ b/vb_suite/parser.py
@@ -44,7 +44,7 @@ read_csv_comment2 = Benchmark(stmt, setup,
                               start_date=datetime(2011, 11, 1))
 
 setup = common_setup + """
-from cStringIO import StringIO
+from six.moves import cStringIO as StringIO
 import os
 N = 10000
 K = 8
@@ -63,7 +63,7 @@ sdate = datetime(2012, 5, 7)
 read_table_multiple_date = Benchmark(cmd, setup, start_date=sdate)
 
 setup = common_setup + """
-from cStringIO import StringIO
+from six.moves import cStringIO as StringIO
 import os
 N = 10000
 K = 8
diff --git a/vb_suite/perf_HEAD.py b/vb_suite/perf_HEAD.py
index c14a1795f..0f2adf41d 100755
--- a/vb_suite/perf_HEAD.py
+++ b/vb_suite/perf_HEAD.py
@@ -13,6 +13,7 @@ from urllib2 import urlopen
 import json
 
 import pandas as pd
+import pandas.util.compat as compat
 
 WEB_TIMEOUT = 10
 
@@ -114,7 +115,7 @@ def main():
                 if d['succeeded']:
                     print("\nException:\n%s\n" % str(e))
                 else:
-                    for k, v in sorted(d.iteritems()):
+                    for k, v in sorted(compat.iteritems(d)):
                         print("{k}: {v}".format(k=k, v=v))
 
             print("------->\n")
@@ -238,6 +239,6 @@ def get_all_results_joined(repo_id=53976):
     dfs = get_all_results(repo_id)
     for k in dfs:
         dfs[k] = mk_unique(dfs[k])
-    ss = [pd.Series(v.timing, name=k) for k, v in dfs.iteritems()]
+    ss = [pd.Series(v.timing, name=k) for k, v in compat.iteritems(dfs)]
     results = pd.concat(reversed(ss), 1)
     return results
diff --git a/vb_suite/source/conf.py b/vb_suite/source/conf.py
index d83448fd9..2b5753a03 100644
--- a/vb_suite/source/conf.py
+++ b/vb_suite/source/conf.py
@@ -12,6 +12,7 @@
 
 import sys
 import os
+import six
 
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
@@ -49,8 +50,8 @@ source_suffix = '.rst'
 master_doc = 'index'
 
 # General information about the project.
-project = u'pandas'
-copyright = u'2008-2011, the pandas development team'
+project = six.u('pandas')
+copyright = six.u('2008-2011, the pandas development team')
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -197,8 +198,8 @@ htmlhelp_basename = 'performance'
 # (source start file, target name, title, author, documentclass [howto/manual]).
 latex_documents = [
     ('index', 'performance.tex',
-     u'pandas vbench Performance Benchmarks',
-     u'Wes McKinney', 'manual'),
+     six.u('pandas vbench Performance Benchmarks'),
+     six.u('Wes McKinney'), 'manual'),
 ]
 
 # The name of an image file (relative to this directory) to place at the top of
diff --git a/vb_suite/suite.py b/vb_suite/suite.py
index 905c43718..76fafb87b 100644
--- a/vb_suite/suite.py
+++ b/vb_suite/suite.py
@@ -1,3 +1,4 @@
+from __future__ import print_function
 from vbench.api import Benchmark, GitRepo
 from datetime import datetime
 
@@ -90,15 +91,15 @@ def generate_rst_files(benchmarks):
     fig_base_path = os.path.join(vb_path, 'figures')
 
     if not os.path.exists(vb_path):
-        print 'creating %s' % vb_path
+        print('creating %s' % vb_path)
         os.makedirs(vb_path)
 
     if not os.path.exists(fig_base_path):
-        print 'creating %s' % fig_base_path
+        print('creating %s' % fig_base_path)
         os.makedirs(fig_base_path)
 
     for bmk in benchmarks:
-        print 'Generating rst file for %s' % bmk.name
+        print('Generating rst file for %s' % bmk.name)
         rst_path = os.path.join(RST_BASE, 'vbench/%s.txt' % bmk.name)
 
         fig_full_path = os.path.join(fig_base_path, '%s.png' % bmk.name)
@@ -120,7 +121,7 @@ def generate_rst_files(benchmarks):
             f.write(rst_text)
 
     with open(os.path.join(RST_BASE, 'index.rst'), 'w') as f:
-        print >> f, """
+        print("""
 Performance Benchmarks
 ======================
 
@@ -141,15 +142,15 @@ Produced on a machine with
 .. toctree::
     :hidden:
     :maxdepth: 3
-"""
+""", file=f)
         for modname, mod_bmks in sorted(by_module.items()):
-            print >> f, '    vb_%s' % modname
+            print('    vb_%s' % modname, file=f)
             modpath = os.path.join(RST_BASE, 'vb_%s.rst' % modname)
             with open(modpath, 'w') as mh:
                 header = '%s\n%s\n\n' % (modname, '=' * len(modname))
-                print >> mh, header
+                print(header, file=mh)
 
                 for bmk in mod_bmks:
-                    print >> mh, bmk.name
-                    print >> mh, '-' * len(bmk.name)
-                    print >> mh, '.. include:: vbench/%s.txt\n' % bmk.name
+                    print(bmk.name, file=mh)
+                    print('-' * len(bmk.name), file=mh)
+                    print('.. include:: vbench/%s.txt\n' % bmk.name, file=mh)
diff --git a/vb_suite/test_perf.py b/vb_suite/test_perf.py
index ca98b94e4..7428bbb07 100755
--- a/vb_suite/test_perf.py
+++ b/vb_suite/test_perf.py
@@ -25,7 +25,10 @@ everything and calculate a ration for the timing information.
 5) print the results to the log file and to stdout.
 
 """
+from __future__ import print_function
 
+from pandas.util.py3compat import range
+from six.moves import map
 import shutil
 import os
 import sys
@@ -275,7 +278,8 @@ def profile_head_single(benchmark):
                         err = str(e)
                 except:
                     pass
-                print("%s died with:\n%s\nSkipping...\n" % (benchmark.name, err))
+                print("%s died with:\n%s\nSkipping...\n" % (benchmark.name,
+                                                            err))
 
             results.append(d.get('timing',np.nan))
             gc.enable()
@@ -296,7 +300,8 @@ def profile_head_single(benchmark):
     # return df.set_index("name")[HEAD_COL]
 
 def profile_head(benchmarks):
-    print( "Performing %d benchmarks (%d runs each)" % ( len(benchmarks), args.hrepeats))
+    print("Performing %d benchmarks (%d runs each)" % (len(benchmarks),
+                                                       args.hrepeats))
 
     ss= [profile_head_single(b) for b in benchmarks]
     print("\n")
