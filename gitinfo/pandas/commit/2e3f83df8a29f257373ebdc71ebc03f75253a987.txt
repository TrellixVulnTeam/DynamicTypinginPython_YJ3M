commit 2e3f83df8a29f257373ebdc71ebc03f75253a987
Author: Jonathan Chambers <jonathan.chambers@ben-energy.com>
Date:   Fri Jan 24 16:05:10 2014 +0000

    ENH #4163 Tweaks to docs, avoid mutable default args, mysql tests

diff --git a/doc/source/io.rst b/doc/source/io.rst
index 174c35432..58265b4ca 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -3105,8 +3105,14 @@ below and the SQLAlchemy `documentation <http://docs.sqlalchemy.org/en/rel_0_9/c
    # Create your connection.
    engine = create_engine('sqlite:///:memory:')
 
+Writing DataFrames
+~~~~~~~~~~~~~~~~~~
 
+<<<<<<< HEAD
 Assuming the following data is in a DataFrame "data", we can insert it into
+=======
+Assuming the following data is in a DataFrame ``data``, we can insert it into
+>>>>>>> 6314e6f... ENH #4163 Tweaks to docs, avoid mutable default args, mysql tests
 the database using :func:`~pandas.io.sql.to_sql`.
 
 
@@ -3138,17 +3144,16 @@ the database using :func:`~pandas.io.sql.to_sql`.
 
    data  = DataFrame(d, columns=c)
 
-.. ipython:: python
-
-   sql.to_sql(data, 'data', engine)
+Reading Tables
+~~~~~~~~~~~~~~
 
-You can read from the database simply by specifying a table
-name using the :func:`~pandas.io.sql.read_table` function.
+:func:`~pandas.io.sql.read_table` will read a databse table given the
+table name and optionally a subset of columns to read.
 
 .. note::
 
-    In order to use read_table, you MUST have the SQLAlchemy optional
-    dependency installed.
+    In order to use :func:`~pandas.io.sql.read_table`, you **must** have the
+    SQLAlchemy optional dependency installed.
 
 .. ipython:: python
 
@@ -3176,11 +3181,17 @@ to pass to :func:`pandas.tseries.tools.to_datetime`.
    sql.read_table('data', engine, parse_dates={'Date': '%Y-%m-%d'})
    sql.read_table('data', engine, parse_dates={'Date': {'format': '%Y-%m-%d %H:%M:%S'}})
 
+
+You can check if a table exists using :func:`~pandas.io.sql.has_table`
+
+In addition, the class :class:`~pandas.io.sql.PandasSQLWithEngine` can be
+instantiated directly for more manual control over the SQL interaction.
+
 Querying
 ~~~~~~~~
 
-You can also query using raw SQL in the :func:`~pandas.io.sql.read_sql` function.
-In this case you must use valid SQL for your database.
+You can query using raw SQL in the :func:`~pandas.io.sql.read_sql` function.
+In this case you must use the SQL variant appropriate for your database.
 When using SQLAlchemy, you can also pass SQLAlchemy Expression language constructs,
 which are database-agnostic.
 
@@ -3195,10 +3206,17 @@ Of course, you can specify a more "complex" query.
    sql.read_frame("SELECT id, Col_1, Col_2 FROM data WHERE id = 42;", engine)
 
 
-There are a few other available functions:
+You can also run a plain query without creating a dataframe with
+:func:`~pandas.io.sql.execute`. This is useful for queries that don't return values,
+such as INSERT. This is functionally equivalent to calling ``execute`` on the
+SQLAlchemy engine or db connection object. Again, ou must use the SQL syntax
+variant appropriate for your database.
+
+.. code-block:: python
 
-:func:`~pandas.io.sql.has_table` checks if a given table exists.
+   sql.execute('SELECT * FROM table_name', engine)
 
+<<<<<<< HEAD
 <<<<<<< HEAD
 :func:`~pandas.io.sql.tquery` returns a list of tuples corresponding to each row.
 
@@ -3209,6 +3227,10 @@ returning results it returns the number of related rows.
 
 In addition, the class :class:`~pandas.io.sql.PandasSQLWithEngine` can be
 instantiated directly for more manual control over the SQL interaction.
+=======
+   sql.execute('INSERT INTO table_name VALUES(?, ?, ?)', engine, params=[('id', 1, 12.2, True)])
+
+>>>>>>> 6314e6f... ENH #4163 Tweaks to docs, avoid mutable default args, mysql tests
 
 Engine connection examples
 ~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index 1a7f0665e..25e005513 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -14,6 +14,7 @@ from pandas.core.api import DataFrame
 from pandas.core.base import PandasObject
 from pandas.tseries.tools import to_datetime
 
+
 class SQLAlchemyRequired(ImportError):
     pass
 
@@ -33,7 +34,7 @@ def _convert_params(sql, params):
     return args
 
 
-def execute(sql, con, cur=None, params=[], flavor='sqlite'):
+def execute(sql, con, cur=None, params=None, flavor='sqlite'):
     """
     Execute the given SQL query using the provided connection object.
 
@@ -60,7 +61,7 @@ def execute(sql, con, cur=None, params=[], flavor='sqlite'):
     return pandas_sql.execute(*args)
 
 
-def tquery(sql, con, cur=None, params=[], flavor='sqlite'):
+def tquery(sql, con, cur=None, params=None, flavor='sqlite'):
     """
     Returns list of tuples corresponding to each row in given sql
     query.
@@ -80,19 +81,22 @@ def tquery(sql, con, cur=None, params=[], flavor='sqlite'):
         List of parameters to pass to execute method.
     flavor : string "sqlite", "mysql"
         Specifies the flavor of SQL to use.
-        Ignored when using SQLAlchemy engine. Required when using DBAPI2 connection.
+        Ignored when using SQLAlchemy engine. Required when using DBAPI2
+        connection.
     Returns
     -------
     Results Iterable
     """
-    warnings.warn("tquery is depreciated, and will be removed in future versions", DeprecationWarning)
+    warnings.warn(
+        "tquery is depreciated, and will be removed in future versions",
+        DeprecationWarning)
 
     pandas_sql = pandasSQL_builder(con, flavor=flavor)
     args = _convert_params(sql, params)
     return pandas_sql.tquery(*args)
 
 
-def uquery(sql, con, cur=None, params=[], engine=None, flavor='sqlite'):
+def uquery(sql, con, cur=None, params=None, engine=None, flavor='sqlite'):
     """
     Does the same thing as tquery, but instead of returning results, it
     returns the number of rows affected.  Good for update queries.
@@ -110,12 +114,15 @@ def uquery(sql, con, cur=None, params=[], engine=None, flavor='sqlite'):
         List of parameters to pass to execute method.
     flavor : string "sqlite", "mysql"
         Specifies the flavor of SQL to use.
-        Ignored when using SQLAlchemy engine. Required when using DBAPI2 connection.
+        Ignored when using SQLAlchemy engine. Required when using DBAPI2
+        connection.
     Returns
     -------
     Number of affected rows
     """
-    warnings.warn("uquery is depreciated, and will be removed in future versions", DeprecationWarning)
+    warnings.warn(
+        "uquery is depreciated, and will be removed in future versions",
+        DeprecationWarning)
     pandas_sql = pandasSQL_builder(con, flavor=flavor)
     args = _convert_params(sql, params)
     return pandas_sql.uquery(*args)
@@ -125,7 +132,8 @@ def uquery(sql, con, cur=None, params=[], engine=None, flavor='sqlite'):
 # Read and write to DataFrames
 
 
-def read_sql(sql, con, index_col=None, flavor='sqlite', coerce_float=True, params=[], parse_dates=[]):
+def read_sql(sql, con, index_col=None, flavor='sqlite', coerce_float=True,
+             params=None, parse_dates=None):
     """
     Returns a DataFrame corresponding to the result set of the query
     string.
@@ -160,7 +168,8 @@ def read_sql(sql, con, index_col=None, flavor='sqlite', coerce_float=True, param
         Or
         Dict of {column_name: arg dict}, where the arg dict corresponds
         to the keyword arguments of :func:`pandas.tseries.tools.to_datetime`
-        Especially useful with databases without native Datetime support, such as SQLite
+        Especially useful with databases without native Datetime support,
+        such as SQLite
     Returns
     -------
     DataFrame
@@ -216,7 +225,8 @@ def has_table(table_name, con, meta=None, flavor='sqlite'):
     return pandas_sql.has_table(table_name)
 
 
-def read_table(table_name, con, meta=None, index_col=None, coerce_float=True, parse_dates=[], columns=[]):
+def read_table(table_name, con, meta=None, index_col=None, coerce_float=True,
+               parse_dates=None, columns=None):
     """Given a table name and SQLAlchemy engine, return a DataFrame.
     Type convertions will be done automatically
 
@@ -238,7 +248,8 @@ def read_table(table_name, con, meta=None, index_col=None, coerce_float=True, pa
         Or
         Dict of {column_name: arg dict}, where the arg dict corresponds
         to the keyword arguments of :func:`pandas.tseries.tools.to_datetime`
-        Especially useful with databases without native Datetime support, such as SQLite
+        Especially useful with databases without native Datetime support,
+        such as SQLite
     columns: list
         List of column names to select from sql table
     Returns
@@ -268,7 +279,8 @@ def pandasSQL_builder(con, flavor=None, meta=None):
         if isinstance(con, sqlalchemy.engine.Engine):
             return PandasSQLAlchemy(con, meta=meta)
         else:
-            warnings.warn("Not an SQLAlchemy engine, attempting to use as legacy DBAPI connection")
+            warnings.warn(
+                "Not an SQLAlchemy engine, attempting to use as legacy DBAPI connection")
             if flavor is None:
                 raise ValueError("""PandasSQL must be created with an SQLAlchemy engine
                     or a DBAPI2 connection and SQL flavour""")
@@ -284,31 +296,41 @@ def pandasSQL_builder(con, flavor=None, meta=None):
 
 
 class PandasSQL(PandasObject):
+
     """
     Subclasses Should define read_sql and to_sql
     """
+
     def read_sql(self, *args, **kwargs):
-        raise ValueError("PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
+        raise ValueError(
+            "PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
 
     def to_sql(self, *args, **kwargs):
-        raise ValueError("PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
+        raise ValueError(
+            "PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
 
     def _create_sql_schema(self, frame, name, keys):
-        raise ValueError("PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
+        raise ValueError(
+            "PandasSQL must be created with an SQLAlchemy engine or connection+sql flavor")
 
-    def _frame_from_data_and_columns(self, data, columns, index_col=None, coerce_float=True):
-        df = DataFrame.from_records(data, columns=columns, coerce_float=coerce_float)
+    def _frame_from_data_and_columns(self, data, columns, index_col=None,
+                                     coerce_float=True):
+        df = DataFrame.from_records(
+            data, columns=columns, coerce_float=coerce_float)
         if index_col is not None:
             df.set_index(index_col, inplace=True)
         return df
 
     def _safe_col_names(self, col_names):
-        return [s.replace(' ', '_').strip() for s in col_names]  # may not be safe enough...
+        # may not be safe enough...
+        return [s.replace(' ', '_').strip() for s in col_names]
 
     def _parse_date_columns(self, data_frame, parse_dates):
+        """ Force non-datetime columns to be read as such.
+            Supports both string formatted and integer timestamp columns
         """
-        """
-        if parse_dates is True:
+        # handle non-list entries for parse_dates gracefully
+        if parse_dates is True or parse_dates is None or parse_dates is False:
             parse_dates = []
 
         if not hasattr(parse_dates, '__iter__'):
@@ -331,7 +353,7 @@ class PandasSQL(PandasObject):
                 if format in ['D', 's', 'ms', 'us', 'ns']:
                     return to_datetime(col, coerce=True, unit=format)
                 elif issubclass(col.dtype.type, np.floating) or issubclass(col.dtype.type, np.integer):
-                    #parse dates as timestamp
+                    # parse dates as timestamp
                     format = 's' if format is None else format
                     return to_datetime(col, coerce=True, unit=format)
                 else:
@@ -339,10 +361,12 @@ class PandasSQL(PandasObject):
 
 
 class PandasSQLAlchemy(PandasSQL):
+
     """
     This class enables convertion between DataFrame and SQL databases
     using SQLAlchemy to handle DataBase abstraction
     """
+
     def __init__(self, engine, meta=None):
         self.engine = engine
         if not meta:
@@ -366,7 +390,7 @@ class PandasSQLAlchemy(PandasSQL):
         result = self.execute(*args, **kwargs)
         return result.rowcount
 
-    def read_sql(self, sql, index_col=None, coerce_float=True, parse_dates=[], params=[]):
+    def read_sql(self, sql, index_col=None, coerce_float=True, parse_dates=None, params=None):
         args = _convert_params(sql, params)
         result = self.execute(*args)
         data = result.fetchall()
@@ -383,7 +407,8 @@ class PandasSQLAlchemy(PandasSQL):
             if if_exists == 'fail':
                 raise ValueError("Table '%s' already exists." % name)
             elif if_exists == 'replace':
-                #TODO: this triggers a full refresh of metadata, could probably avoid this.
+                # TODO: this triggers a full refresh of metadata, could
+                # probably avoid this.
                 self._drop_table(name)
                 self._create_table(frame, name)
             elif if_exists == 'append':
@@ -415,7 +440,8 @@ class PandasSQLAlchemy(PandasSQL):
         else:
             return None
 
-    def read_table(self, table_name, index_col=None, coerce_float=True, parse_dates=[], columns=[]):
+    def read_table(self, table_name, index_col=None, coerce_float=True,
+                   parse_dates=None, columns=None):
         table = self.get_table(table_name)
 
         if table is not None:
@@ -434,7 +460,8 @@ class PandasSQLAlchemy(PandasSQL):
                                                            index_col=index_col,
                                                            coerce_float=coerce_float)
 
-            data_frame = self._harmonize_columns(data_frame, table, parse_dates)
+            data_frame = self._harmonize_columns(
+                data_frame, table, parse_dates)
             return data_frame
         else:
             return None
@@ -464,7 +491,8 @@ class PandasSQLAlchemy(PandasSQL):
         columns = [(col_name, col_sqltype, col_name in keys)
                    for col_name, col_sqltype in zip(safe_columns, column_types)]
 
-        columns = [Column(name, typ, primary_key=pk) for name, typ, pk in columns]
+        columns = [Column(name, typ, primary_key=pk)
+                   for name, typ, pk in columns]
 
         return Table(table_name, self.meta, *columns)
 
@@ -504,7 +532,7 @@ class PandasSQLAlchemy(PandasSQL):
             return bool
         return object
 
-    def _harmonize_columns(self, data_frame, sql_table, parse_dates=[]):
+    def _harmonize_columns(self, data_frame, sql_table, parse_dates=None):
         """ Make a data_frame's column type align with an sql_table column types
             Need to work around limited NA value support.
             Floats are always fine, ints must always
@@ -520,11 +548,13 @@ class PandasSQLAlchemy(PandasSQL):
             col_name = sql_col.name
             try:
                 df_col = data_frame[col_name]
-                col_type = self._lookup_np_type(sql_col.type)  # the type the dataframe column should have
+                # the type the dataframe column should have
+                col_type = self._lookup_np_type(sql_col.type)
 
                 if col_type is datetime or col_type is date:
                     if not issubclass(df_col.dtype.type, np.datetime64):
-                        data_frame[col_name] = self._parse_date_col(df_col, col_type)
+                        data_frame[col_name] = self._parse_date_col(
+                            df_col, col_type)
 
                 elif col_type is float:
                     # floats support NA, can always convert!
@@ -542,42 +572,34 @@ class PandasSQLAlchemy(PandasSQL):
         return data_frame
 
 
-
-
 # ---- SQL without SQLAlchemy ---
-# Flavour specific sql strings and handler class for access to DBs without SQLAlchemy installed
-
+# Flavour specific sql strings and handler class for access to DBs without
+# SQLAlchemy installed
 # SQL type convertions for each DB
 _SQL_TYPES = {
     'text': {
         'mysql': 'VARCHAR (63)',
         'sqlite': 'TEXT',
-        'postgres': 'text'
     },
     'float': {
         'mysql': 'FLOAT',
         'sqlite': 'REAL',
-        'postgres': 'real'
     },
     'int': {
         'mysql': 'BIGINT',
         'sqlite': 'INTEGER',
-        'postgres': 'integer'
     },
     'datetime': {
         'mysql': 'DATETIME',
         'sqlite': 'TIMESTAMP',
-        'postgres': 'timestamp'
     },
     'date': {
         'mysql': 'DATE',
         'sqlite': 'TIMESTAMP',
-        'postgres': 'date'
     },
     'bool': {
         'mysql': 'BOOLEAN',
         'sqlite': 'INTEGER',
-        'postgres': 'boolean'
     }
 }
 
@@ -592,19 +614,15 @@ _SQL_SYMB = {
         'br_l': '[',
         'br_r': ']',
         'wld': '?'
-    },
-    'postgres': {
-        'br_l': '',
-        'br_r': '',
-        'wld': '?'
     }
 }
 
 
 class PandasSQLLegacy(PandasSQL):
+
     def __init__(self, con, flavor):
         self.con = con
-        if flavor not in ['sqlite', 'mysql', 'postgres']:
+        if flavor not in ['sqlite', 'mysql']:
             raise NotImplementedError
         else:
             self.flavor = flavor
@@ -648,7 +666,8 @@ class PandasSQLLegacy(PandasSQL):
         cur = self.execute(*args)
         return cur.rowcount
 
-    def read_sql(self, sql, index_col=None, coerce_float=True, params=[], flavor='sqlite', parse_dates=[]):
+    def read_sql(self, sql, index_col=None, coerce_float=True, params=None,
+                 parse_dates=None):
         args = _convert_params(sql, params)
         cursor = self.execute(*args)
         columns = [col_desc[0] for col_desc in cursor.description]
@@ -795,14 +814,17 @@ def get_schema(frame, name, con, flavor='sqlite'):
     flavor: {'sqlite', 'mysql', 'postgres'}, default 'sqlite'
 
     """
+    warnings.warn(
+        "get_schema is depreciated", DeprecationWarning)
     pandas_sql = pandasSQL_builder(con=con, flavor=flavor)
-    return pandas_sql._create_sql_schema()
+    return pandas_sql._create_sql_schema(frame, name)
 
 
 def read_frame(*args, **kwargs):
     """DEPRECIATED - use read_sql
     """
-    warnings.warn("read_frame is depreciated, use read_sql", DeprecationWarning)
+    warnings.warn(
+        "read_frame is depreciated, use read_sql", DeprecationWarning)
     return read_sql(*args, **kwargs)
 
 
@@ -813,7 +835,6 @@ def write_frame(*args, **kwargs):
     return to_sql(*args, **kwargs)
 
 
-#Append wrapped function docstrings
+# Append wrapped function docstrings
 read_frame.__doc__ += read_sql.__doc__
 write_frame.__doc__ += to_sql.__doc__
-
diff --git a/pandas/io/tests/test_sql.py b/pandas/io/tests/test_sql.py
index 222bc12c5..c225a0636 100644
--- a/pandas/io/tests/test_sql.py
+++ b/pandas/io/tests/test_sql.py
@@ -21,39 +21,97 @@ try:
 except ImportError:
     SQLALCHEMY_INSTALLED = False
 
+SQL_STRINGS = {
+    'create_iris': {
+        'sqlite': """CREATE TABLE iris (
+                `SepalLength` REAL,
+                `SepalWidth` REAL,
+                `PetalLength` REAL,
+                `PetalWidth` REAL,
+                `Name` TEXT
+            )""",
+        'mysql': """CREATE TABLE iris (
+                `SepalLength` DOUBLE,
+                `SepalWidth` DOUBLE,
+                `PetalLength` DOUBLE,
+                `PetalWidth` DOUBLE,
+                `Name` VARCHAR(200)
+            )"""
+    },
+    'insert_iris': {
+        'sqlite': """INSERT INTO iris VALUES(?, ?, ?, ?, ?)""",
+        'mysql': """INSERT INTO iris VALUES(%s, %s, %s, %s, "%s");"""
+    },
+    'create_test_types': {
+        'sqlite': """CREATE TABLE types_test_data (
+                    `TextCol` TEXT,
+                    `DateCol` TEXT,
+                    `IntDateCol` INTEGER,
+                    `FloatCol` REAL,
+                    `IntCol` INTEGER,
+                    `BoolCol` INTEGER,
+                    `IntColWithNull` INTEGER,
+                    `BoolColWithNull` INTEGER
+                )""",
+        'mysql': """CREATE TABLE types_test_data (
+                    `TextCol` TEXT,
+                    `DateCol` DATETIME,
+                    `IntDateCol` INTEGER,
+                    `FloatCol` DOUBLE,
+                    `IntCol` INTEGER,
+                    `BoolCol` BOOLEAN,
+                    `IntColWithNull` INTEGER,
+                    `BoolColWithNull` BOOLEAN
+                )"""
+    },
+    'insert_test_types': {
+        'sqlite': """
+                INSERT INTO types_test_data
+                VALUES(?, ?, ?, ?, ?, ?, ?, ?)
+                """,
+        'mysql': """
+                INSERT INTO types_test_data
+                VALUES("%s", %s, %s, %s, %s, %s, %s, %s)
+                """
+    }
+}
+
 
 class PandasSQLTest(unittest.TestCase):
+
     """Base class with common private methods for
     SQLAlchemy and fallback cases.
     """
 
-    def _load_iris_data(self, conn):
+    def drop_table(self, table_name):
+        self._get_exec().execute("DROP TABLE IF EXISTS %s" % table_name)
+
+    def _get_exec(self):
+        if hasattr(self.conn, 'execute'):
+            return self.conn
+        else:
+            return self.conn.cursor()
+
+    def _load_iris_data(self):
         iris_csv_file = os.path.join(tm.get_data_path(), 'iris.csv')
 
-        # Raw SQLite
-        conn.execute("""CREATE TABLE iris (
-                `SepalLength` REAL,
-                `SepalWidth` REAL,
-                `PetalLength` REAL,
-                `PetalWidth` REAL,
-                `Name` TEXT
-            )""")
+        self.drop_table('iris')
+        self._get_exec().execute(SQL_STRINGS['create_iris'][self.flavor])
 
         with open(iris_csv_file, 'rU') as iris_csv:
             r = csv.reader(iris_csv)
             next(r)  # skip header row
-            ins = """
-                INSERT INTO iris
-                VALUES(?, ?, ?, ?, ?)
-                """
+            ins = SQL_STRINGS['insert_iris'][self.flavor]
+
             for row in r:
-                conn.execute(ins, row)
+                self._get_exec().execute(ins, row)
 
     def _check_iris_loaded_frame(self, iris_frame):
         pytype = iris_frame.dtypes[0].type
         row = iris_frame.iloc[0]
 
-        self.assertTrue(issubclass(pytype, np.floating), 'Loaded frame has incorrect type')
+        self.assertTrue(
+            issubclass(pytype, np.floating), 'Loaded frame has incorrect type')
         tm.equalContents(row.values, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])
 
     def _load_test1_data(self):
@@ -63,30 +121,20 @@ class PandasSQLTest(unittest.TestCase):
             dr = csv.DictReader(test1_csv)
             self.test_frame1 = DataFrame(list(dr))
 
-    def _load_raw_sql(self, conn):
-        # Raw SQLite
-        conn.execute("""CREATE TABLE types_test_data (
-                `TextCol` TEXT,
-                `DateCol` TEXT,
-                `IntDateCol` INTEGER,
-                `FloatCol` REAL,
-                `IntCol` INTEGER,
-                `BoolCol` INTEGER,
-                `IntColWithNull` INTEGER,
-                `BoolColWithNull` INTEGER
-            )""")
-
-        ins = """
-                INSERT INTO types_test_data
-                VALUES(?, ?, ?, ?, ?, ?, ?, ?)
-                """
-        data = [('first', '2000-01-03 00:00:00', 535852800, 10.10, 1, False, 1, False),
-                ('first', '2000-01-04 00:00:00', 1356998400, 10.10, 1, False, None, None)]
+    def _load_raw_sql(self):
+        self.drop_table('types_test_data')
+        self._get_exec().execute(SQL_STRINGS['create_test_types'][self.flavor])
+        ins = SQL_STRINGS['insert_test_types'][self.flavor]
+
+        data = [(
+            'first', '2000-01-03 00:00:00', 535852800, 10.10, 1, False, 1, False),
+            ('first', '2000-01-04 00:00:00', 1356998400, 10.10, 1, False, None, None)]
         for d in data:
-            conn.execute(ins, d)
+            self._get_exec().execute(ins, d)
 
-    def _count_rows(self, table_name, con):
-        result = con.execute("SELECT count(*) AS count_1 FROM %s" % table_name).fetchone()
+    def _count_rows(self, table_name):
+        result = self._get_exec().execute(
+            "SELECT count(*) AS count_1 FROM %s" % table_name).fetchone()
         return result[0]
 
     def _read_sql_iris(self):
@@ -94,63 +142,70 @@ class PandasSQLTest(unittest.TestCase):
         self._check_iris_loaded_frame(iris_frame)
 
     def _to_sql(self):
-        # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
         self.pandasSQL.to_sql(self.test_frame1, 'test_frame1')
-        self.assertTrue(self.pandasSQL.has_table('test_frame1'), 'Table not written to DB')
+        self.assertTrue(self.pandasSQL.has_table(
+            'test_frame1'), 'Table not written to DB')
 
         # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
     def _to_sql_fail(self):
-        # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
-        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1', if_exists='fail')
-        self.assertTrue(self.pandasSQL.has_table('test_frame1'), 'Table not written to DB')
+        self.pandasSQL.to_sql(
+            self.test_frame1, 'test_frame1', if_exists='fail')
+        self.assertTrue(self.pandasSQL.has_table(
+            'test_frame1'), 'Table not written to DB')
 
-        self.assertRaises(ValueError, self.pandasSQL.to_sql, self.test_frame1, 'test_frame1', if_exists='fail')
+        self.assertRaises(ValueError, self.pandasSQL.to_sql,
+                          self.test_frame1, 'test_frame1', if_exists='fail')
 
-        # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
     def _to_sql_replace(self):
-        # Nuke table just in case
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
-        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1', if_exists='fail')
+        self.pandasSQL.to_sql(
+            self.test_frame1, 'test_frame1', if_exists='fail')
         # Add to table again
-        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1', if_exists='replace')
-        self.assertTrue(self.pandasSQL.has_table('test_frame1'), 'Table not written to DB')
+        self.pandasSQL.to_sql(
+            self.test_frame1, 'test_frame1', if_exists='replace')
+        self.assertTrue(self.pandasSQL.has_table(
+            'test_frame1'), 'Table not written to DB')
 
         num_entries = len(self.test_frame1)
-        num_rows = self._count_rows('test_frame1', self.conn)
+        num_rows = self._count_rows('test_frame1')
 
-        self.assertEqual(num_rows, num_entries, "not the same number of rows as entries")
+        self.assertEqual(
+            num_rows, num_entries, "not the same number of rows as entries")
 
-        # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
     def _to_sql_append(self):
         # Nuke table just in case
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
-        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1', if_exists='fail')
+        self.pandasSQL.to_sql(
+            self.test_frame1, 'test_frame1', if_exists='fail')
 
         # Add to table again
-        self.pandasSQL.to_sql(self.test_frame1, 'test_frame1', if_exists='append')
-        self.assertTrue(self.pandasSQL.has_table('test_frame1'), 'Table not written to DB')
+        self.pandasSQL.to_sql(
+            self.test_frame1, 'test_frame1', if_exists='append')
+        self.assertTrue(self.pandasSQL.has_table(
+            'test_frame1'), 'Table not written to DB')
 
-        num_entries = 2*len(self.test_frame1)
-        num_rows = self._count_rows('test_frame1', self.conn)
+        num_entries = 2 * len(self.test_frame1)
+        num_rows = self._count_rows('test_frame1')
 
-        self.assertEqual(num_rows, num_entries, "not the same number of rows as entries")
+        self.assertEqual(
+            num_rows, num_entries, "not the same number of rows as entries")
 
-        # Nuke table
-        self.drop_table('test_frame1', self.conn)
+        self.drop_table('test_frame1')
 
     def _roundtrip(self):
+        self.drop_table('test_frame_roundtrip')
         self.pandasSQL.to_sql(self.test_frame1, 'test_frame_roundtrip')
         result = self.pandasSQL.read_sql('SELECT * FROM test_frame_roundtrip')
 
@@ -172,6 +227,7 @@ class PandasSQLTest(unittest.TestCase):
 
 
 class TestSQLApi(PandasSQLTest):
+
     """Test the public API as it would be used
     directly, including legacy names
 
@@ -182,6 +238,8 @@ class TestSQLApi(PandasSQLTest):
     we don't use drop_table because that isn't part of the public api
 
     """
+    flavor = 'sqlite'
+
     def connect(self):
         if SQLALCHEMY_INSTALLED:
             return sqlalchemy.create_engine('sqlite:///:memory:')
@@ -190,61 +248,79 @@ class TestSQLApi(PandasSQLTest):
 
     def setUp(self):
         self.conn = self.connect()
-        self._load_iris_data(self.conn)
+        self._load_iris_data()
         self._load_test1_data()
-        self._load_raw_sql(self.conn)
+        self._load_raw_sql()
 
     def test_read_sql_iris(self):
-        iris_frame = sql.read_sql("SELECT * FROM iris", self.conn, flavor='sqlite')
+        iris_frame = sql.read_sql(
+            "SELECT * FROM iris", self.conn, flavor='sqlite')
         self._check_iris_loaded_frame(iris_frame)
 
     def test_legacy_read_frame(self):
         """Test legacy name read_frame"""
-        iris_frame = sql.read_frame("SELECT * FROM iris", self.conn, flavor='sqlite')
+        iris_frame = sql.read_frame(
+            "SELECT * FROM iris", self.conn, flavor='sqlite')
         self._check_iris_loaded_frame(iris_frame)
 
     def test_to_sql(self):
         sql.to_sql(self.test_frame1, 'test_frame1', self.conn, flavor='sqlite')
-        self.assertTrue(sql.has_table('test_frame1', self.conn, flavor='sqlite'), 'Table not written to DB')
+        self.assertTrue(
+            sql.has_table('test_frame1', self.conn, flavor='sqlite'), 'Table not written to DB')
 
     def test_to_sql_fail(self):
-        sql.to_sql(self.test_frame1, 'test_frame2', self.conn, flavor='sqlite', if_exists='fail')
-        self.assertTrue(sql.has_table('test_frame2', self.conn, flavor='sqlite'), 'Table not written to DB')
+        sql.to_sql(self.test_frame1, 'test_frame2',
+                   self.conn, flavor='sqlite', if_exists='fail')
+        self.assertTrue(
+            sql.has_table('test_frame2', self.conn, flavor='sqlite'), 'Table not written to DB')
 
-        self.assertRaises(ValueError, sql.to_sql, self.test_frame1, 'test_frame2', self.conn, flavor='sqlite', if_exists='fail')
+        self.assertRaises(ValueError, sql.to_sql, self.test_frame1,
+                          'test_frame2', self.conn, flavor='sqlite', if_exists='fail')
 
     def test_to_sql_replace(self):
-        sql.to_sql(self.test_frame1, 'test_frame3', self.conn, flavor='sqlite', if_exists='fail')
+        sql.to_sql(self.test_frame1, 'test_frame3',
+                   self.conn, flavor='sqlite', if_exists='fail')
         # Add to table again
-        sql.to_sql(self.test_frame1, 'test_frame3', self.conn, flavor='sqlite', if_exists='replace')
-        self.assertTrue(sql.has_table('test_frame3', self.conn, flavor='sqlite'), 'Table not written to DB')
+        sql.to_sql(self.test_frame1, 'test_frame3',
+                   self.conn, flavor='sqlite', if_exists='replace')
+        self.assertTrue(
+            sql.has_table('test_frame3', self.conn, flavor='sqlite'), 'Table not written to DB')
 
         num_entries = len(self.test_frame1)
-        num_rows = self._count_rows('test_frame3', self.conn)
+        num_rows = self._count_rows('test_frame3')
 
-        self.assertEqual(num_rows, num_entries, "not the same number of rows as entries")
+        self.assertEqual(
+            num_rows, num_entries, "not the same number of rows as entries")
 
     def test_to_sql_append(self):
-        sql.to_sql(self.test_frame1, 'test_frame4', self.conn, flavor='sqlite', if_exists='fail')
+        sql.to_sql(self.test_frame1, 'test_frame4',
+                   self.conn, flavor='sqlite', if_exists='fail')
 
         # Add to table again
-        sql.to_sql(self.test_frame1, 'test_frame4', self.conn, flavor='sqlite', if_exists='append')
-        self.assertTrue(sql.has_table('test_frame4', self.conn, flavor='sqlite'), 'Table not written to DB')
+        sql.to_sql(self.test_frame1, 'test_frame4',
+                   self.conn, flavor='sqlite', if_exists='append')
+        self.assertTrue(
+            sql.has_table('test_frame4', self.conn, flavor='sqlite'), 'Table not written to DB')
 
-        num_entries = 2*len(self.test_frame1)
-        num_rows = self._count_rows('test_frame4', self.conn)
+        num_entries = 2 * len(self.test_frame1)
+        num_rows = self._count_rows('test_frame4')
 
-        self.assertEqual(num_rows, num_entries, "not the same number of rows as entries")
+        self.assertEqual(
+            num_rows, num_entries, "not the same number of rows as entries")
 
     def test_legacy_write_frame(self):
         """Test legacy write frame name.
         Assume that functionality is already tested above so just do quick check that it basically works"""
-        sql.write_frame(self.test_frame1, 'test_frame_legacy', self.conn, flavor='sqlite')
-        self.assertTrue(sql.has_table('test_frame_legacy', self.conn, flavor='sqlite'), 'Table not written to DB')
+        sql.write_frame(
+            self.test_frame1, 'test_frame_legacy', self.conn, flavor='sqlite')
+        self.assertTrue(
+            sql.has_table('test_frame_legacy', self.conn, flavor='sqlite'), 'Table not written to DB')
 
     def test_roundtrip(self):
-        sql.to_sql(self.test_frame1, 'test_frame_roundtrip', con=self.conn, flavor='sqlite')
-        result = sql.read_sql('SELECT * FROM test_frame_roundtrip', con=self.conn, flavor='sqlite')
+        sql.to_sql(self.test_frame1, 'test_frame_roundtrip',
+                   con=self.conn, flavor='sqlite')
+        result = sql.read_sql(
+            'SELECT * FROM test_frame_roundtrip', con=self.conn, flavor='sqlite')
 
         # HACK!
         result.index = self.test_frame1.index
@@ -253,45 +329,57 @@ class TestSQLApi(PandasSQLTest):
 
     def test_execute_sql(self):
         # drop_sql = "DROP TABLE IF EXISTS test"  # should already be done
-        iris_results = sql.execute("SELECT * FROM iris", con=self.conn, flavor='sqlite')
+        iris_results = sql.execute(
+            "SELECT * FROM iris", con=self.conn, flavor='sqlite')
         row = iris_results.fetchone()
         tm.equalContents(row, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])
 
     def test_tquery(self):
-        iris_results = sql.tquery("SELECT * FROM iris", con=self.conn, flavor='sqlite')
+        iris_results = sql.tquery(
+            "SELECT * FROM iris", con=self.conn, flavor='sqlite')
         row = iris_results[0]
         tm.equalContents(row, [5.1, 3.5, 1.4, 0.2, 'Iris-setosa'])
 
     def test_date_parsing(self):
         """ Test date parsing in read_sql """
         # No Parsing
-        df = sql.read_sql("SELECT * FROM types_test_data", self.conn, flavor='sqlite')
-        self.assertFalse(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
+        df = sql.read_sql(
+            "SELECT * FROM types_test_data", self.conn, flavor='sqlite')
+        self.assertFalse(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_sql("SELECT * FROM types_test_data", self.conn, flavor='sqlite', parse_dates=['DateCol'])
-        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
+        df = sql.read_sql("SELECT * FROM types_test_data",
+                          self.conn, flavor='sqlite', parse_dates=['DateCol'])
+        self.assertTrue(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_sql("SELECT * FROM types_test_data", self.conn, flavor='sqlite', parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})
-        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
+        df = sql.read_sql("SELECT * FROM types_test_data", self.conn,
+                          flavor='sqlite', parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})
+        self.assertTrue(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_sql("SELECT * FROM types_test_data", self.conn, flavor='sqlite', parse_dates=['IntDateCol'])
-        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_sql("SELECT * FROM types_test_data",
+                          self.conn, flavor='sqlite', parse_dates=['IntDateCol'])
+        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
-        df = sql.read_sql("SELECT * FROM types_test_data", self.conn, flavor='sqlite', parse_dates={'IntDateCol': 's'})
-        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_sql("SELECT * FROM types_test_data",
+                          self.conn, flavor='sqlite', parse_dates={'IntDateCol': 's'})
+        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
 
 class TestSQLAlchemy(PandasSQLTest):
+
     '''
     Test the sqlalchemy backend against an in-memory sqlite database.
     Assume that sqlalchemy takes case of the DB specifics
     '''
+    flavor = 'sqlite'
+
     def connect(self):
         return sqlalchemy.create_engine('sqlite:///:memory:')
 
-    def drop_table(self, table_name, conn):
-        conn.execute("DROP TABLE IF EXISTS %s" % table_name)
-
     def setUp(self):
         # Skip this test if SQLAlchemy not available
         if not SQLALCHEMY_INSTALLED:
@@ -300,8 +388,8 @@ class TestSQLAlchemy(PandasSQLTest):
         self.conn = self.connect()
         self.pandasSQL = sql.PandasSQLAlchemy(self.conn)
 
-        self._load_iris_data(self.conn)
-        self._load_raw_sql(self.conn)
+        self._load_iris_data()
+        self._load_raw_sql()
 
         self._load_test1_data()
 
@@ -322,26 +410,31 @@ class TestSQLAlchemy(PandasSQLTest):
 
     def test_create_table(self):
         temp_conn = self.connect()
-        temp_frame = DataFrame({'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
+        temp_frame = DataFrame(
+            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
 
         pandasSQL = sql.PandasSQLAlchemy(temp_conn)
         pandasSQL._create_table(temp_frame, 'temp_frame')
 
-        self.assertTrue(temp_conn.has_table('temp_frame'), 'Table not written to DB')
+        self.assertTrue(
+            temp_conn.has_table('temp_frame'), 'Table not written to DB')
 
     def test_drop_table(self):
         temp_conn = self.connect()
 
-        temp_frame = DataFrame({'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
+        temp_frame = DataFrame(
+            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
 
         pandasSQL = sql.PandasSQLAlchemy(temp_conn)
         pandasSQL._create_table(temp_frame, 'temp_frame')
 
-        self.assertTrue(temp_conn.has_table('temp_frame'), 'Table not written to DB')
+        self.assertTrue(
+            temp_conn.has_table('temp_frame'), 'Table not written to DB')
 
         pandasSQL._drop_table('temp_frame')
 
-        self.assertFalse(temp_conn.has_table('temp_frame'), 'Table not deleted from DB')
+        self.assertFalse(
+            temp_conn.has_table('temp_frame'), 'Table not deleted from DB')
 
     def test_roundtrip(self):
         self._roundtrip()
@@ -354,59 +447,88 @@ class TestSQLAlchemy(PandasSQLTest):
         self._check_iris_loaded_frame(iris_frame)
 
     def test_read_table_columns(self):
-        iris_frame = sql.read_table("iris", con=self.conn, columns=['SepalLength', 'SepalLength'])
-        tm.equalContents(iris_frame.columns.values, ['SepalLength', 'SepalLength'])
+        iris_frame = sql.read_table(
+            "iris", con=self.conn, columns=['SepalLength', 'SepalLength'])
+        tm.equalContents(
+            iris_frame.columns.values, ['SepalLength', 'SepalLength'])
 
     def test_read_table_absent(self):
-        self.assertRaises(ValueError, sql.read_table, "this_doesnt_exist", con=self.conn)
+        self.assertRaises(
+            ValueError, sql.read_table, "this_doesnt_exist", con=self.conn)
 
     def test_default_type_convertion(self):
         """ Test default type conversion"""
         df = sql.read_table("types_test_data", self.conn)
-        self.assertTrue(issubclass(df.FloatCol.dtype.type, np.floating), "FloatCol loaded with incorrect type")
-        self.assertTrue(issubclass(df.IntCol.dtype.type, np.integer), "IntCol loaded with incorrect type")
-        self.assertTrue(issubclass(df.BoolCol.dtype.type, np.integer), "BoolCol loaded with incorrect type")
+        self.assertTrue(
+            issubclass(df.FloatCol.dtype.type, np.floating), "FloatCol loaded with incorrect type")
+        self.assertTrue(
+            issubclass(df.IntCol.dtype.type, np.integer), "IntCol loaded with incorrect type")
+        self.assertTrue(
+            issubclass(df.BoolCol.dtype.type, np.integer), "BoolCol loaded with incorrect type")
 
         # Int column with NA values stays as float
-        self.assertTrue(issubclass(df.IntColWithNull.dtype.type, np.floating), "IntColWithNull loaded with incorrect type")
+        self.assertTrue(issubclass(df.IntColWithNull.dtype.type, np.floating),
+                        "IntColWithNull loaded with incorrect type")
         # Non-native Bool column with NA values stays as float
-        self.assertTrue(issubclass(df.BoolColWithNull.dtype.type, np.floating), "BoolCol loaded with incorrect type")
+        self.assertTrue(
+            issubclass(df.BoolColWithNull.dtype.type, np.floating), "BoolCol loaded with incorrect type")
+
+    def test_default_date_load(self):
+        df = sql.read_table("types_test_data", self.conn)
+
+        # IMPORTANT - sqlite has no native date type, so shouldn't parse, but MySQL SHOULD be converted.
+        self.assertFalse(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
     def test_date_parsing(self):
         """ Test date parsing """
         # No Parsing
         df = sql.read_table("types_test_data", self.conn)
-        self.assertFalse(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates=['DateCol'])
-        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
+        df = sql.read_table(
+            "types_test_data", self.conn, parse_dates=['DateCol'])
+        self.assertTrue(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})
-        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
+        df = sql.read_table(
+            "types_test_data", self.conn, parse_dates={'DateCol': '%Y-%m-%d %H:%M:%S'})
+        self.assertTrue(
+            issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates={'DateCol': {'format': '%Y-%m-%d %H:%M:%S'}})
-        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_table("types_test_data", self.conn, parse_dates={
+                            'DateCol': {'format': '%Y-%m-%d %H:%M:%S'}})
+        self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates=['IntDateCol'])
-        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_table(
+            "types_test_data", self.conn, parse_dates=['IntDateCol'])
+        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates={'IntDateCol': 's'})
-        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_table(
+            "types_test_data", self.conn, parse_dates={'IntDateCol': 's'})
+        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
-        df = sql.read_table("types_test_data", self.conn, parse_dates={'IntDateCol': {'unit': 's'}})
-        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64), "IntDateCol loaded with incorrect type")
+        df = sql.read_table(
+            "types_test_data", self.conn, parse_dates={'IntDateCol': {'unit': 's'}})
+        self.assertTrue(issubclass(df.IntDateCol.dtype.type, np.datetime64),
+                        "IntDateCol loaded with incorrect type")
 
 
 # --- Test SQLITE fallback
 class TestSQLite(PandasSQLTest):
+
     '''
     Test the sqlalchemy backend against an in-memory sqlite database.
     Assume that sqlalchemy takes case of the DB specifics
     '''
+    flavor = 'sqlite'
+
     def connect(self):
         return sqlite3.connect(':memory:')
 
-    def drop_table(self, table_name, conn):
+    def drop_table(self, table_name):
         cur = self.conn.cursor()
         cur.execute("DROP TABLE IF EXISTS %s" % table_name)
         self.conn.commit()
@@ -415,12 +537,13 @@ class TestSQLite(PandasSQLTest):
         self.conn = self.connect()
         self.pandasSQL = sql.PandasSQLLegacy(self.conn, 'sqlite')
 
-        self._load_iris_data(self.conn)
+        self._load_iris_data()
 
         self._load_test1_data()
 
     def test_invalid_flavor(self):
-        self.assertRaises(NotImplementedError, sql.PandasSQLLegacy, self.conn, 'oracle')
+        self.assertRaises(
+            NotImplementedError, sql.PandasSQLLegacy, self.conn, 'oracle')
 
     def test_read_sql(self):
         self._read_sql_iris()
@@ -437,28 +560,19 @@ class TestSQLite(PandasSQLTest):
     def test_to_sql_append(self):
         self._to_sql_append()
 
-    def test_create_table(self):
-        temp_conn = self.connect()
-        temp_frame = DataFrame({'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
-
-        pandasSQL = sql.PandasSQLLegacy(temp_conn, 'sqlite')
-        pandasSQL._create_table(temp_frame, 'temp_frame')
-
-        self.assertTrue(pandasSQL.has_table('temp_frame'), 'Table not written to DB')
+    def test_create_and_drop_table(self):
+        temp_frame = DataFrame(
+            {'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
 
-    def test_drop_table(self):
-        temp_conn = self.connect()
-
-        temp_frame = DataFrame({'one': [1., 2., 3., 4.], 'two': [4., 3., 2., 1.]})
-
-        pandasSQL = sql.PandasSQLLegacy(temp_conn, 'sqlite')
-        pandasSQL._create_table(temp_frame, 'temp_frame')
+        self.pandasSQL._create_table(temp_frame, 'drop_test_frame')
 
-        self.assertTrue(pandasSQL.has_table('temp_frame'), 'Table not written to DB')
+        self.assertTrue(self.pandasSQL.has_table(
+            'drop_test_frame'), 'Table not written to DB')
 
-        pandasSQL._drop_table('temp_frame')
+        self.pandasSQL._drop_table('drop_test_frame')
 
-        self.assertFalse(pandasSQL.has_table('temp_frame'), 'Table not deleted from DB')
+        self.assertFalse(self.pandasSQL.has_table(
+            'drop_test_frame'), 'Table not deleted from DB')
 
     def test_roundtrip(self):
         self._roundtrip()
@@ -470,141 +584,85 @@ class TestSQLite(PandasSQLTest):
         self._tquery()
 
 
-"""
-class TestSQLA_pymysql(TestSQLAlchemy):
-    def setUp(self):
-        raise nose.SkipTest("MySQLdb was not installed")
+class TestMySQL(TestSQLite):
+    flavor = 'mysql'
+
+    def drop_table(self, table_name):
+        cur = self.conn.cursor()
+        cur.execute("DROP TABLE IF EXISTS %s" % table_name)
+        self.conn.commit()
+
+    def _count_rows(self, table_name):
+        cur = self._get_exec()
+        cur.execute(
+            "SELECT count(*) AS count_1 FROM %s" % table_name)
+        rows = cur.fetchall()
+        return rows[0][0]
 
-    def set_flavor_engine(self):
-        # if can't import should skip all tests
+    def connect(self):
+        return self.driver.connect(host='127.0.0.1', user='root', passwd='', db='pandas_nosetest')
+
+    def setUp(self):
         try:
             import pymysql
+            self.driver = pymysql
+
         except ImportError:
-            raise nose.SkipTest("pymysql was not installed")
+            raise nose.SkipTest
 
-        try:
-            self.engine = sqlalchemy.create_engine("mysql+pymysql://root:@localhost/pandas_nosetest")
-        except pymysql.Error as e:
-            raise nose.SkipTest(
-                "Cannot connect to database. "
-                "Create a group of conn parameters under the heading "
-                "[pandas] in your system's mysql default file, "
-                "typically located at ~/.my.cnf or /etc/.my.cnf. ")
-        except pymysql.ProgrammingError as e:
-            raise nose.SkipTest(
-                "Create a group of connection parameters under the heading "
-                "[pandas] in your system's mysql default file, "
-                "typically located at ~/.my.cnf or /etc/.my.cnf. ")
-
-
-class TestSQLA_MySQLdb(TestSQLAlchemy):
-    def setUp(self):
-        raise nose.SkipTest("MySQLdb was not installed")
+        self.conn = self.connect()
+        self.pandasSQL = sql.PandasSQLLegacy(self.conn, 'mysql')
 
-    def set_flavor_engine(self):
-        # if can't import should skip all tests
-        try:
-            import MySQLdb
-        except ImportError:
-            raise nose.SkipTest("MySQLdb was not installed")
+        self._load_iris_data()
+        self._load_test1_data()
 
-        try:
-<<<<<<< HEAD
-            sys.stdout = StringIO()
-
-            self.assertRaises(MySQLdb.ProgrammingError, sql.tquery,
-                              'insert into blah values (1)', con=self.db)
-
-            self.assertRaises(MySQLdb.ProgrammingError, sql.tquery,
-                              'insert into blah values (1)', con=self.db,
-                              retry=True)
-        finally:
-            sys.stdout = sys.__stdout__
-
-    def test_keyword_as_column_names(self):
-        '''
-        '''
-        _skip_if_no_MySQLdb()
-        df = DataFrame({'From':np.ones(5)})
-        sql.write_frame(df, con = self.db, name = 'testkeywords',
-                        if_exists='replace', flavor='mysql')
-
-    def test_if_exists(self):
-        _skip_if_no_MySQLdb()
-        df_if_exists_1 = DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})
-        df_if_exists_2 = DataFrame({'col1': [3, 4, 5], 'col2': ['C', 'D', 'E']})
-        table_name = 'table_if_exists'
-        sql_select = "SELECT * FROM %s" % table_name
-
-        def clean_up(test_table_to_drop):
-            """
-            Drops tables created from individual tests
-            so no dependencies arise from sequential tests
-            """
-            if sql.table_exists(test_table_to_drop, self.db, flavor='mysql'):
-                cur = self.db.cursor()
-                cur.execute("DROP TABLE %s" % test_table_to_drop)
-                cur.close()
-
-        # test if invalid value for if_exists raises appropriate error
-        self.assertRaises(ValueError,
-                          sql.write_frame,
-                          frame=df_if_exists_1,
-                          con=self.db,
-                          name=table_name,
-                          flavor='mysql',
-                          if_exists='notvalidvalue')
-        clean_up(table_name)
-
-        # test if_exists='fail'
-        sql.write_frame(frame=df_if_exists_1, con=self.db, name=table_name,
-                        flavor='mysql', if_exists='fail')
-        self.assertRaises(ValueError,
-                          sql.write_frame,
-                          frame=df_if_exists_1,
-                          con=self.db,
-                          name=table_name,
-                          flavor='mysql',
-                          if_exists='fail')
-
-        # test if_exists='replace'
-        sql.write_frame(frame=df_if_exists_1, con=self.db, name=table_name,
-                        flavor='mysql', if_exists='replace')
-        self.assertEqual(sql.tquery(sql_select, con=self.db),
-                         [(1, 'A'), (2, 'B')])
-        sql.write_frame(frame=df_if_exists_2, con=self.db, name=table_name,
-                        flavor='mysql', if_exists='replace')
-        self.assertEqual(sql.tquery(sql_select, con=self.db),
-                         [(3, 'C'), (4, 'D'), (5, 'E')])
-        clean_up(table_name)
-
-        # test if_exists='append'
-        sql.write_frame(frame=df_if_exists_1, con=self.db, name=table_name,
-                        flavor='mysql', if_exists='fail')
-        self.assertEqual(sql.tquery(sql_select, con=self.db),
-                         [(1, 'A'), (2, 'B')])
-        sql.write_frame(frame=df_if_exists_2, con=self.db, name=table_name,
-                        flavor='mysql', if_exists='append')
-        self.assertEqual(sql.tquery(sql_select, con=self.db),
-                         [(1, 'A'), (2, 'B'), (3, 'C'), (4, 'D'), (5, 'E')])
-        clean_up(table_name)
+    def tearDown(self):
+        c = self.conn.cursor()
+        c.execute('SHOW TABLES')
+        for table in c.fetchall():
+            c.execute('DROP TABLE %s' % table[0])
+        self.conn.commit()
+        self.conn.close()
+
+
+class TestMySQLAlchemy(TestSQLAlchemy):
+        flavor = 'mysql'
+
+        def connect(self):
+            return sqlalchemy.create_engine(
+                'mysql+{driver}://root@localhost/pandas_nosetest'.format(driver=self.driver))
+
+        def setUp(self):
+            if not SQLALCHEMY_INSTALLED:
+                raise nose.SkipTest('SQLAlchemy not installed')
+
+            try:
+                import pymysql
+                self.driver = 'pymysql'
+
+            except ImportError:
+                raise nose.SkipTest
+
+            self.conn = self.connect()
+            self.pandasSQL = sql.PandasSQLAlchemy(self.conn)
+
+            self._load_iris_data()
+            self._load_raw_sql()
+
+            self._load_test1_data()
+
+        def tearDown(self):
+            c = self.conn.execute('SHOW TABLES')
+            for table in c.fetchall():
+                self.conn.execute('DROP TABLE %s' % table[0])
+
+        def test_default_date_load(self):
+            df = sql.read_table("types_test_data", self.conn)
 
+            # IMPORTANT - sqlite has no native date type, so shouldn't parse, but MySQL SHOULD be converted.
+            self.assertTrue(
+                issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
 
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
                    exit=False)
-=======
-            self.engine = sqlalchemy.create_engine("mysql+mysqldb://root:@localhost/pandas_nosetest")
-        except MySQLdb.Error:
-            raise nose.SkipTest(
-                "Cannot connect to database. "
-                "Create a group of connection parameters under the heading "
-                "[pandas] in your system's mysql default file, "
-                "typically located at ~/.my.cnf or /etc/.my.cnf. ")
-        except MySQLdb.ProgrammingError:
-            raise nose.SkipTest(
-                "Create a group of connection parameters under the heading "
-                "[pandas] in your system's mysql default file, "
-                "typically located at ~/.my.cnf or /etc/.my.cnf. ")
-"""
->>>>>>> 1259dca... ENH #4163 Use SQLAlchemy for DB abstraction
