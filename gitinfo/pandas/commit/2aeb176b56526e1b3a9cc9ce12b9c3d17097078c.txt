commit 2aeb176b56526e1b3a9cc9ce12b9c3d17097078c
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Sun Sep 19 23:40:55 2010 +0000

    misc documentation, some work on rpy2 interface. near git migration
    
    git-svn-id: http://pandas.googlecode.com/svn/trunk@202 d5231056-7de3-11de-ac95-d976489f1ece

diff --git a/.coveragerc b/.coveragerc
new file mode 100644
index 000000000..5b264a626
--- /dev/null
+++ b/.coveragerc
@@ -0,0 +1,26 @@
+# .coveragerc to control coverage.py
+[run]
+branch = False
+
+[report]
+# Regexes for lines to exclude from consideration
+exclude_lines =
+    # Have to re-enable the standard pragma
+    pragma: no cover
+
+    # Don't complain about missing debug-only code:
+    def __repr__
+    if self\.debug
+
+    # Don't complain if tests don't hit defensive assertion code:
+    raise AssertionError
+    raise NotImplementedError
+
+    # Don't complain if non-runnable code isn't run:
+    if 0:
+    if __name__ == .__main__.:
+
+ignore_errors = False
+
+[html]
+directory = coverage_html_report
\ No newline at end of file
diff --git a/LICENSE.txt b/LICENSE
similarity index 100%
rename from LICENSE.txt
rename to LICENSE
diff --git a/README.txt b/README.rst
similarity index 100%
rename from README.txt
rename to README.rst
diff --git a/RELEASE.rst b/RELEASE.rst
new file mode 100644
index 000000000..ec49331af
--- /dev/null
+++ b/RELEASE.rst
@@ -0,0 +1,17 @@
+=============
+Release Notes
+=============
+
+pandas 0.3.0
+============
+
+**Release date:**
+
+**New features / modules**
+
+**Improvements**
+
+**API Changes**
+
+**Bug fixes**
+
diff --git a/TODO.txt b/TODO.rst
similarity index 100%
rename from TODO.txt
rename to TODO.rst
diff --git a/bench/alignment.py b/bench/alignment.py
new file mode 100644
index 000000000..461e1a1d1
--- /dev/null
+++ b/bench/alignment.py
@@ -0,0 +1,21 @@
+# Setup
+import numpy as np
+import pandas
+import la
+N = 1000
+K = 50
+arr1 = np.random.randn(N, K)
+arr2 = np.random.randn(N, K)
+idx1 = range(N)
+idx2 = range(K)
+
+# pandas
+dma1 = pandas.DataMatrix(arr1, idx1, idx2)
+dma2 = pandas.DataMatrix(arr2, idx1[::-1], idx2[::-1])
+
+# larry
+lar1 = la.larry(arr1, [idx1, idx2])
+lar2 = la.larry(arr2, [idx1[::-1], idx2[::-1]])
+
+for i in range(100):
+    result = lar1 + lar2
diff --git a/bench/serialize.py b/bench/serialize.py
new file mode 100644
index 000000000..2f5a5560e
--- /dev/null
+++ b/bench/serialize.py
@@ -0,0 +1,80 @@
+import time, os
+import numpy as np
+
+import la
+import pandas
+
+def timeit(f, iterations):
+    start = time.clock()
+
+    for i in xrange(iterations):
+        f()
+
+    return time.clock() - start
+
+def roundtrip_archive(N, iterations=10):
+
+    # Create data
+    arr = np.random.randn(N, N)
+    lar = la.larry(arr)
+    dma = pandas.DataMatrix(arr, range(N), range(N))
+
+    # filenames
+    filename_numpy = '/Users/wesm/tmp/numpy.npz'
+    filename_larry = '/Users/wesm/tmp/archive.hdf5'
+    filename_pandas = '/Users/wesm/tmp/pandas_tmp'
+
+    # Delete old files
+    try:
+        os.unlink(filename_numpy)
+    except:
+        pass
+    try:
+        os.unlink(filename_larry)
+    except:
+        pass
+    try:
+        os.unlink(filename_pandas)
+    except:
+        pass
+
+    # Time a round trip save and load
+    numpy_f = lambda: numpy_roundtrip(filename_numpy, arr, arr)
+    numpy_time = timeit(numpy_f, iterations) / iterations
+
+    larry_f = lambda: larry_roundtrip(filename_larry, lar, lar)
+    larry_time = timeit(larry_f, iterations) / iterations
+
+    pandas_f = lambda: pandas_roundtrip(filename_pandas, dma, dma)
+    pandas_time = timeit(pandas_f, iterations) / iterations
+
+    print 'Numpy (npz)   %7.4f seconds' % numpy_time
+    print 'larry (HDF5)  %7.4f seconds' % larry_time
+    print 'pandas (HDF5) %7.4f seconds' % pandas_time
+
+def numpy_roundtrip(filename, arr1, arr2):
+    np.savez(filename, arr1=arr1, arr2=arr2)
+    npz = np.load(filename)
+    arr1 = npz['arr1']
+    arr2 = npz['arr2']
+
+def larry_roundtrip(filename, lar1, lar2):
+    io = la.IO(filename)
+    io['lar1'] = lar1
+    io['lar2'] = lar2
+    lar1 = io['lar1']
+    lar2 = io['lar2']
+
+def pandas_roundtrip(filename, dma1, dma2):
+    from pandas.io.pytables import HDFStore
+    store = HDFStore(filename)
+    store['dma1'] = dma1
+    store['dma2'] = dma2
+    dma1 = store['dma1']
+    dma2 = store['dma2']
+
+def pandas_roundtrip_pickle(filename, dma1, dma2):
+    dma1.save(filename)
+    dma1 = pandas.DataMatrix.load(filename)
+    dma2.save(filename)
+    dma2 = pandas.DataMatrix.load(filename)
diff --git a/bench/test.py b/bench/test.py
new file mode 100644
index 000000000..7fdf94fd1
--- /dev/null
+++ b/bench/test.py
@@ -0,0 +1,65 @@
+import numpy as np
+import itertools
+import collections
+import scipy.ndimage as ndi
+
+N = 10000
+
+lat = np.random.randint(0, 360, N)
+lon = np.random.randint(0, 360, N)
+data = np.random.randn(N)
+
+def groupby1(lat, lon, data):
+    indexer = np.lexsort((lon, lat))
+    lat = lat.take(indexer)
+    lon = lon.take(indexer)
+    sorted_data = data.take(indexer)
+
+    keys = 1000. * lat + lon
+    unique_keys = np.unique(keys)
+    bounds = keys.searchsorted(unique_keys)
+
+    result = group_agg(sorted_data, bounds, lambda x: x.mean())
+
+    decoder = keys.searchsorted(unique_keys)
+
+    return dict(zip(zip(lat.take(decoder), lon.take(decoder)), result))
+
+def group_mean(lat, lon, data):
+    indexer = np.lexsort((lon, lat))
+    lat = lat.take(indexer)
+    lon = lon.take(indexer)
+    sorted_data = data.take(indexer)
+
+    keys = 1000 * lat + lon
+    unique_keys = np.unique(keys)
+
+    result = ndi.mean(sorted_data, labels=keys, index=unique_keys)
+    decoder = keys.searchsorted(unique_keys)
+
+    return dict(zip(zip(lat.take(decoder), lon.take(decoder)), result))
+
+def group_mean_naive(lat, lon, data):
+    grouped = collections.defaultdict(list)
+    for lt, ln, da in zip(lat, lon, data):
+      grouped[(lt, ln)].append(da)
+
+    averaged = dict((ltln, np.mean(da)) for ltln, da in grouped.items())
+
+    return averaged
+
+def group_agg(values, bounds, f):
+    N = len(values)
+    result = np.empty(len(bounds), dtype=float)
+    for i, left_bound in enumerate(bounds):
+        if i == len(bounds) - 1:
+            right_bound = N
+        else:
+            right_bound = bounds[i + 1]
+
+        result[i] = f(values[left_bound : right_bound])
+
+    return result
+
+# for i in range(10):
+#     groupby1(lat, lon, data)
diff --git a/doc/source/r_guide.rst b/doc/source/r_guide.rst
new file mode 100644
index 000000000..2b4c5da58
--- /dev/null
+++ b/doc/source/r_guide.rst
@@ -0,0 +1,6 @@
+.. currentmodule:: pandas
+
+.. r_guide:
+
+pandas for R users
+------------------
\ No newline at end of file
diff --git a/pandas/core/tests/test_common.py b/pandas/core/tests/test_common.py
new file mode 100644
index 000000000..536375fe5
--- /dev/null
+++ b/pandas/core/tests/test_common.py
@@ -0,0 +1,19 @@
+from pandas.core.common import notnull, isnull
+import pandas.core.common as common
+
+import numpy as np
+
+def test_notnull():
+    assert notnull(1.)
+    assert not notnull(None)
+    assert not notnull(np.NaN)
+    assert not notnull(np.inf)
+    assert not notnull(-np.inf)
+
+def test_isnull():
+    assert not isnull(1.)
+    assert isnull(None)
+    assert isnull(np.NaN)
+    assert isnull(np.inf)
+    assert isnull(-np.inf)
+
diff --git a/pandas/lib/src/moments.pyx b/pandas/lib/src/moments.pyx
index 6d31edd23..28adc005d 100644
--- a/pandas/lib/src/moments.pyx
+++ b/pandas/lib/src/moments.pyx
@@ -1,5 +1,13 @@
 # Cython implementations of rolling sum, mean, variance, skewness,
 # other statistical moment functions
+#
+# Misc implementation notes
+# -------------------------
+#
+# - In Cython x * x is faster than x ** 2 for C types, this should be
+#   periodically revisited to see if it's still true.
+#
+# -
 
 # original C implementation by N. Devillard.
 # This code in public domain.
@@ -32,18 +40,15 @@ def kth_smallest(ndarray[double_t, ndim=1] a, int k):
         j = m
 
         while 1:
-            while a[i] < x:
-                i += 1
-            while x < a[j]:
-                j -= 1
+            while a[i] < x: i += 1
+            while x < a[j]: j -= 1
             if i <= j:
                 t = a[i]
                 a[i] = a[j]
                 a[j] = t
-                i += 1
-                j -= 1
-            if i > j:
-                break
+                i += 1; j -= 1
+
+            if i > j: break
 
         if j < k: l = i
         if k < i: m = j
@@ -158,8 +163,20 @@ def roll_mean(ndarray[double_t, ndim=1] input,
 #-------------------------------------------------------------------------------
 # Exponentially weighted moving average
 
-def ewma(ndarray[double_t, ndim=1] input,
-         int com):
+def ewma(ndarray[double_t, ndim=1] input, double_t com):
+    '''
+    Compute exponentially-weighted moving average using center-of-mass.
+
+    Parameters
+    ----------
+    input : ndarray (float64 type)
+    com : float64
+
+    Returns
+    -------
+    y : ndarray
+    '''
+
     cdef double cur, prev, neww, oldw, adj
     cdef int i
     cdef int N = len(input)
@@ -423,12 +440,21 @@ cdef _roll_skiplist_op(ndarray arg, int win, int minp, skiplist_f op):
     return output
 
 def roll_median(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
     return _roll_skiplist_op(input, win, minp, _get_median)
 
 def roll_max(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
     return _roll_skiplist_op(input, win, minp, _get_max)
 
 def roll_min(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
     return _roll_skiplist_op(input, win, minp, _get_min)
 
 cdef double_t _get_median(IndexableSkiplist skiplist, int nobs,
diff --git a/pandas/rpy/common.py b/pandas/rpy/common.py
index e82eca8ed..ab65fa2c9 100644
--- a/pandas/rpy/common.py
+++ b/pandas/rpy/common.py
@@ -1,37 +1,126 @@
+"""
+Utilities for making working with rpy2 more user- and
+developer-friendly.
+"""
+
 import numpy as np
 
 from pandas import DataFrame, DataMatrix
 
 from rpy2.robjects.packages import importr
-# import rpy2.robjects as robjects
+from rpy2.robjects import r
+import rpy2.robjects as robj
 
-# r = robjects.r
+__all__ = ['convert_robj', 'load_data']
 
 def load_data(name, package=None):
     if package:
-        importr(package)
+        pack = importr(package)
 
     r.data(name)
-    return _convert_robj(r[name])
+    return convert_robj(r[name])
+
+def _rclass(obj):
+    """
+    Return R class name for input object
+    """
+    return r['class'](obj)[0]
+
+def _is_null(obj):
+    return _rclass(obj) == 'NULL'
+
+def _convert_list(obj):
+    pass
 
-def _convert_robj(robj):
-    if isinstance(robj, robjects.DataFrame):
-        return _from_DataFrame(robj)
-    elif isinstance(robj, robjects.Matrix):
-        return _from_Matrix(robj)
+def _convert_named_list(obj):
+    pass
 
-def _from_DataFrame(rdf):
+def _convert_DataFrame(rdf):
     columns = list(rdf.colnames)
+    rows = np.array(rdf.rownames)
 
     data = {}
     for i, col in enumerate(columns):
         vec = rdf.rx2(i + 1)
         data[col] = list(vec)
 
-    return DataFrame(data)
+    return DataFrame(data, index=rows)
+
+def _convert_Matrix(mat):
+    columns = mat.colnames
+    rows = mat.rownames
+
+    columns = None if _is_null(columns) else list(columns)
+    index = None if _is_null(index) else list(index)
+
+    return DataMatrix(np.array(mat), index=index, columns=columns)
+
+def _check_int(vec):
+    try:
+        # R observation numbers come through as strings
+        vec = vec.astype(int)
+    except Exception:
+        pass
+
+    return vec
+
+_converters = [
+    (robj.DataFrame , _convert_DataFrame),
+    (robj.Matrix , _convert_Matrix),
+]
+
+def convert_robj(obj):
+    """
+    Convert rpy2 object to a pandas-friendly form
+
+    Parameters
+    ----------
+    obj : rpy2 object
+
+    Returns
+    -------
+    Non-rpy data structure, mix of NumPy and pandas objects
+    """
+    if not isinstance(obj, orbj.RObjectMixin):
+        return obj
+
+    for rpy_type, converter in _converters:
+        if isinstance(obj, rpy_type):
+            return converter(obj)
+
+    raise Exception('Do not know what to do with %s object' % klass)
+
+
+import pandas.util.testing as _test
+
+def test_convert_list():
+    obj = r('list(a=1, b=2, c=3)')
+    converted = convert_robj(obj)
+
+    _test.assert_dict_equal
+
+def test_convert_frame():
+    # built-in dataset
+    df = r['faithful']
+
+    converted = convert_robj(obj)
+
+def _named_matrix():
+    r('mat <- matrix(rnorm(9), ncol=3)')
+    r('colnames(mat) <- c("one", "two", "three")')
+    r('rownames(mat) <- c("a", "b", "c")')
+
+    return r['mat']
+
+def test_convert_matrix():
+    mat = _named_matrix()
+
+    converted = convert_robj(mat)
+
+    assert np.array_equal(converted.index, ['a', 'b', 'c'])
+    assert np.array_equal(converted.columns, ['one', 'two', 'three'])
 
-def _from_Matrix(mat):
-    columns = list(mat.colnames)
+def test_convert_nested():
+    pass
 
-    return DataMatrix(np.array(mat), columns=columns)
 
diff --git a/pandas/stats/moments.py b/pandas/stats/moments.py
index cc844f440..793ba3e73 100644
--- a/pandas/stats/moments.py
+++ b/pandas/stats/moments.py
@@ -9,6 +9,7 @@ import numpy as np
 
 from pandas.core.api import (DataFrame, DataMatrix, Series, notnull)
 import pandas.lib.tseries as tseries
+import pandas.util.misc as misc_util
 
 __all__ = ['rolling_count', 'rolling_max', 'rolling_min',
            'rolling_sum', 'rolling_mean', 'rolling_std', 'rolling_cov',
@@ -270,26 +271,52 @@ def _ewmoment(values, func, min_periods=None, biasCorrection=None):
 
     return output
 
-def ewma(arg, com, minCom=0):
+def ewma(arg, com=None, span=None, minCom=0):
     """
     Calculates the rolling exponentially weighted moving average of a series.
 
     Parameters
     ----------
     arg : Series, DataFrame, or DataMatrix
-    com : integer
-        Center of Mass for exponentially weighted moving average
-        decay = com / (1 + com) maps center of mass to decay parameter
-
+    com : float. optional
+        Center of mass: \alpha = com / (1 + com),
+    span : float, optional
+        Specify decay in terms of span, \alpha = 2 / (span + 1)
     minCom : int, default 0
         Optionally require that at least a certain number of periods as
         a multiple of the Center of Mass be included in the sample.
+
+    Returns
+    -------
+    y : type of input argument
+
+    Note
+    ----
+    Either center of mass or span must be specified
+
+    EWMA is sometimes specified using a "span" parameter s, we have
+    have that the decay parameter \alpha is related to the span in the
+    following way:
+
+    \alpha = 1 - 2 / (span + 1) = c / (1 + c)
+
+    where c is the center of mass. Given a span, the
+    associated center of mass is:
+
+    c = (s - 1) / 2
+
+    So a "20-day EWMA" would have center 9.5
     """
+    if com is None and span is None:
+        raise Exception("Must pass either com or span")
+    elif com is not None and span is not None:
+        raise Exception("com and span are mutually exclusive")
+
     def ewmaFunc(series):
         series[np.isinf(series)] = NaN
         result = _ewma(series, com)
 
-        firstIndex = np.arange(len(series))[notnull(series)][0]
+        firstIndex = _first_valid_index(series)
 
         result[firstIndex : firstIndex + minCom*com] = NaN
         result = Series(result, index=arg.index)
@@ -301,11 +328,15 @@ def ewma(arg, com, minCom=0):
         output = arg.apply(ewmaFunc)
     else:
         output = _ewma(arg, com)
-        firstIndex = np.arange(len(arg))[notnull(arg)][0]
+        firstIndex = _first_valid_index(arg)
         output[firstIndex : firstIndex + minCom * com] = NaN
 
     return output
 
+def _first_valid_index(arr):
+    # argmax scans from left
+    return notnull(arr).argmax()
+
 def ewmvar(arg, com, minCom = 0, correctBias = True):
     """
     Calculates the rolling exponentially weighted moving variance of a series.
diff --git a/pandas/stats/tests/test_moments.py b/pandas/stats/tests/test_moments.py
new file mode 100644
index 000000000..ec83fca6b
--- /dev/null
+++ b/pandas/stats/tests/test_moments.py
@@ -0,0 +1,18 @@
+import numpy as np
+import pandas.stats.moments as moments
+
+def test_rolling_median():
+    arr = np.random.randn(100)
+    arr[20:40] = np.NaN
+
+    result = moments.rolling_median(arr, 50)
+
+    assert(np.isnan(result[20]))
+
+    assert(result[-1] == np.median(arr[-50:]))
+
+    result = moments.rolling_median(arr, 49)
+
+    assert(np.isnan(result[20]))
+
+    assert(result[-1] == np.median(arr[-49:]))
diff --git a/pandas/stats/tests/test_var.py b/pandas/stats/tests/test_var.py
new file mode 100644
index 000000000..c6930de68
--- /dev/null
+++ b/pandas/stats/tests/test_var.py
@@ -0,0 +1,37 @@
+from rpy2.robjects import r
+from rpy2.robjects.packages import importr
+
+vars = importr('vars')
+urca = importr('urca')
+
+class RVAR(object):
+    pass
+
+if __name__ == '__main__':
+    r.data("Canada")
+    can = r['Canada']
+    p1ct = r('p1ct <- VAR(Canada, p=1, type="both")')
+    coefs = r('coefs <- coef(p1ct)')
+
+    ecoef = coefs.rx2('e')
+
+    # summary(Canada)
+
+    # plot(Canada, nc=2, xlab="")
+
+    # adf1 <- summary(ur.df(Canada[, "prod"], type = "trend", lags = 2))
+    # adf1
+
+    # adf2 <- summary(ur.df(diff(Canada[, "prod"]), type = "drift", lags = 1))
+    # adf2
+
+    # VARselect(Canada, lag.max = 8, type = "both")
+
+    # Canada <- Canada[, c("prod", "e", "U", "rw")]
+
+    # p1ct <- VAR(Canada, p = 1, type = "both")
+    # p1ct
+
+    # coefs <- coef(p1ct)
+    # class(coefs)
+
diff --git a/pandas/util/misc.py b/pandas/util/misc.py
new file mode 100644
index 000000000..25edfb745
--- /dev/null
+++ b/pandas/util/misc.py
@@ -0,0 +1,4 @@
+def exclusive(*args):
+    count = sum([arg is not None for arg in args])
+    return count == 1
+
diff --git a/scripts/runtests.py b/scripts/runtests.py
new file mode 100644
index 000000000..7816ac25d
--- /dev/null
+++ b/scripts/runtests.py
@@ -0,0 +1,3 @@
+import os; print os.getpid()
+import nose
+nose.main('pandas.core')
diff --git a/scripts/testmed.py b/scripts/testmed.py
new file mode 100644
index 000000000..1184fee82
--- /dev/null
+++ b/scripts/testmed.py
@@ -0,0 +1,161 @@
+## {{{ Recipe 576930 (r10): Efficient Running Median using an Indexable Skiplist
+
+from random import random
+from math import log, ceil
+
+class Node(object):
+    __slots__ = 'value', 'next', 'width'
+    def __init__(self, value, next, width):
+        self.value, self.next, self.width = value, next, width
+
+class End(object):
+    'Sentinel object that always compares greater than another object'
+    def __cmp__(self, other):
+        return 1
+
+NIL = Node(End(), [], [])               # Singleton terminator node
+
+class IndexableSkiplist:
+    'Sorted collection supporting O(lg n) insertion, removal, and lookup by rank.'
+
+    def __init__(self, expected_size=100):
+        self.size = 0
+        self.maxlevels = int(1 + log(expected_size, 2))
+        self.head = Node('HEAD', [NIL]*self.maxlevels, [1]*self.maxlevels)
+
+    def __len__(self):
+        return self.size
+
+    def __getitem__(self, i):
+        node = self.head
+        i += 1
+        for level in reversed(range(self.maxlevels)):
+            while node.width[level] <= i:
+                i -= node.width[level]
+                node = node.next[level]
+        return node.value
+
+    def insert(self, value):
+        # find first node on each level where node.next[levels].value > value
+        chain = [None] * self.maxlevels
+        steps_at_level = [0] * self.maxlevels
+        node = self.head
+        for level in reversed(range(self.maxlevels)):
+            while node.next[level].value <= value:
+                steps_at_level[level] += node.width[level]
+                node = node.next[level]
+            chain[level] = node
+
+        # insert a link to the newnode at each level
+        d = min(self.maxlevels, 1 - int(log(random(), 2.0)))
+        newnode = Node(value, [None]*d, [None]*d)
+        steps = 0
+        for level in range(d):
+            prevnode = chain[level]
+            newnode.next[level] = prevnode.next[level]
+            prevnode.next[level] = newnode
+            newnode.width[level] = prevnode.width[level] - steps
+            prevnode.width[level] = steps + 1
+            steps += steps_at_level[level]
+        for level in range(d, self.maxlevels):
+            chain[level].width[level] += 1
+        self.size += 1
+
+    def remove(self, value):
+        # find first node on each level where node.next[levels].value >= value
+        chain = [None] * self.maxlevels
+        node = self.head
+        for level in reversed(range(self.maxlevels)):
+            while node.next[level].value < value:
+                node = node.next[level]
+            chain[level] = node
+        if value != chain[0].next[0].value:
+            raise KeyError('Not Found')
+
+        # remove one link at each level
+        d = len(chain[0].next[0].next)
+        for level in range(d):
+            prevnode = chain[level]
+            prevnode.width[level] += prevnode.next[level].width[level] - 1
+            prevnode.next[level] = prevnode.next[level].next[level]
+        for level in range(d, self.maxlevels):
+            chain[level].width[level] -= 1
+        self.size -= 1
+
+    def __iter__(self):
+        'Iterate over values in sorted order'
+        node = self.head.next[0]
+        while node is not NIL:
+            yield node.value
+            node = node.next[0]
+
+from collections import deque
+from itertools import islice
+
+class RunningMedian:
+    'Fast running median with O(lg n) updates where n is the window size'
+
+    def __init__(self, n, iterable):
+        from pandas.lib.skiplist import IndexableSkiplist as skiplist
+
+        self.it = iter(iterable)
+        self.queue = deque(islice(self.it, n))
+        self.skiplist = IndexableSkiplist(n)
+        for elem in self.queue:
+            self.skiplist.insert(elem)
+
+    def __iter__(self):
+        queue = self.queue
+        skiplist = self.skiplist
+        midpoint = len(queue) // 2
+        yield skiplist[midpoint]
+        for newelem in self.it:
+            oldelem = queue.popleft()
+            skiplist.remove(oldelem)
+            queue.append(newelem)
+            skiplist.insert(newelem)
+            yield skiplist[midpoint]
+
+N = 100000
+K = 10000
+
+import time
+
+def test():
+    from numpy.random import randn
+
+    arr = randn(N)
+
+    def _test(arr, k):
+        meds = RunningMedian(k, arr)
+        return list(meds)
+
+    _test(arr, K)
+
+from numpy.random import randn
+from pandas.lib.skiplist import rolling_median
+
+def test2():
+
+    arr = randn(N)
+
+    return rolling_median(arr, K)
+
+def runmany(f, arr, arglist):
+    timings = []
+
+    for arg in arglist:
+        tot = 0
+        for i in range(5):
+            tot += _time(f, arr, arg)
+        timings.append(tot / 5)
+
+    return timings
+
+def _time(f, *args):
+    _start = time.clock()
+    result = f(*args)
+    return time.clock() - _start
+
+if __name__ == '__main__':
+    test2()
diff --git a/test.sh b/test.sh
new file mode 100755
index 000000000..a771f8750
--- /dev/null
+++ b/test.sh
@@ -0,0 +1,5 @@
+#!/bin/sh
+#nosetests -w pandas/core --with-coverage --cover-package=pandas.core --pdb-failure
+coverage erase
+nosetests -w pandas/core --with-coverage --cover-package=pandas.core
+# coverage run runtests.py
\ No newline at end of file
