commit 8ff2ebd9b2cfbe4a3e1eb7893dc4343c01ad55b4
Author: MomIsBestFriend <50263213+MomIsBestFriend@users.noreply.github.com>
Date:   Tue Jan 14 02:47:47 2020 +0200

    STY: concat strings (#30991)

diff --git a/pandas/compat/__init__.py b/pandas/compat/__init__.py
index 60cfecd58..3547a33ea 100644
--- a/pandas/compat/__init__.py
+++ b/pandas/compat/__init__.py
@@ -110,8 +110,7 @@ def _import_lzma():
         return lzma
     except ImportError:
         msg = (
-            "Could not import the lzma module. "
-            "Your installed Python is incomplete. "
+            "Could not import the lzma module. Your installed Python is incomplete. "
             "Attempting to use lzma compression will result in a RuntimeError."
         )
         warnings.warn(msg)
diff --git a/pandas/compat/numpy/__init__.py b/pandas/compat/numpy/__init__.py
index 27f1c3205..6c9ac5944 100644
--- a/pandas/compat/numpy/__init__.py
+++ b/pandas/compat/numpy/__init__.py
@@ -18,11 +18,9 @@ _is_numpy_dev = ".dev" in str(_nlv)
 
 if _nlv < "1.13.3":
     raise ImportError(
-        f"this version of pandas is incompatible with "
-        f"numpy < 1.13.3\n"
+        "this version of pandas is incompatible with numpy < 1.13.3\n"
         f"your numpy version is {_np_version}.\n"
-        f"Please upgrade numpy to >= 1.13.3 to use "
-        f"this pandas version"
+        "Please upgrade numpy to >= 1.13.3 to use this pandas version"
     )
 
 
diff --git a/pandas/compat/numpy/function.py b/pandas/compat/numpy/function.py
index 50f234cbf..05ecccc67 100644
--- a/pandas/compat/numpy/function.py
+++ b/pandas/compat/numpy/function.py
@@ -389,9 +389,8 @@ def validate_resampler_func(method: str, args, kwargs) -> None:
     if len(args) + len(kwargs) > 0:
         if method in RESAMPLER_NUMPY_OPS:
             raise UnsupportedFunctionCall(
-                f"numpy operations are not "
-                f"valid with resample. Use "
-                f".resample(...).{method}() instead"
+                "numpy operations are not valid with resample. "
+                f"Use .resample(...).{method}() instead"
             )
         else:
             raise TypeError("too many arguments passed in")
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 05066ac0e..ada26b55a 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1701,8 +1701,7 @@ class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
                 multi_message = (
                     "\n"
                     "For a multi-index, the label must be a "
-                    "tuple with elements corresponding to "
-                    "each level."
+                    "tuple with elements corresponding to each level."
                 )
             else:
                 multi_message = ""
diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py
index 502b8d194..449f70b2b 100644
--- a/pandas/core/reshape/concat.py
+++ b/pandas/core/reshape/concat.py
@@ -305,8 +305,7 @@ class _Concatenator:
         if isinstance(objs, (NDFrame, str)):
             raise TypeError(
                 "first argument must be an iterable of pandas "
-                "objects, you passed an object of type "
-                '"{name}"'.format(name=type(objs).__name__)
+                f'objects, you passed an object of type "{type(objs).__name__}"'
             )
 
         if join == "outer":
@@ -577,10 +576,7 @@ class _Concatenator:
         if self.verify_integrity:
             if not concat_index.is_unique:
                 overlap = concat_index[concat_index.duplicated()].unique()
-                raise ValueError(
-                    "Indexes have overlapping values: "
-                    "{overlap!s}".format(overlap=overlap)
-                )
+                raise ValueError(f"Indexes have overlapping values: {overlap}")
 
 
 def _concat_indexes(indexes) -> Index:
@@ -648,8 +644,7 @@ def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde
             # make sure that all of the passed indices have the same nlevels
             if not len({idx.nlevels for idx in indexes}) == 1:
                 raise AssertionError(
-                    "Cannot concat indices that do "
-                    "not have the same number of levels"
+                    "Cannot concat indices that do not have the same number of levels"
                 )
 
             # also copies
diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py
index acb53ff6c..ceee2f66d 100644
--- a/pandas/core/reshape/merge.py
+++ b/pandas/core/reshape/merge.py
@@ -1071,9 +1071,8 @@ class _MergeOperation:
                 continue
 
             msg = (
-                "You are trying to merge on {lk_dtype} and "
-                "{rk_dtype} columns. If you wish to proceed "
-                "you should use pd.concat".format(lk_dtype=lk.dtype, rk_dtype=rk.dtype)
+                f"You are trying to merge on {lk.dtype} and "
+                f"{rk.dtype} columns. If you wish to proceed you should use pd.concat"
             )
 
             # if we are numeric, then allow differing
diff --git a/pandas/io/clipboards.py b/pandas/io/clipboards.py
index 34e8e03d8..97178261b 100644
--- a/pandas/io/clipboards.py
+++ b/pandas/io/clipboards.py
@@ -69,8 +69,7 @@ def read_clipboard(sep=r"\s+", **kwargs):  # pragma: no cover
         kwargs["engine"] = "python"
     elif len(sep) > 1 and kwargs.get("engine") == "c":
         warnings.warn(
-            "read_clipboard with regex separator does not work "
-            "properly with c engine"
+            "read_clipboard with regex separator does not work properly with c engine"
         )
 
     return read_csv(StringIO(text), sep=sep, **kwargs)
diff --git a/pandas/io/common.py b/pandas/io/common.py
index 771a302d6..6a764ff25 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -78,8 +78,7 @@ def _expand_user(
 def validate_header_arg(header) -> None:
     if isinstance(header, bool):
         raise TypeError(
-            "Passing a bool to header is invalid. "
-            "Use header=None for no header or "
+            "Passing a bool to header is invalid. Use header=None for no header or "
             "header=int or list-like of ints to specify "
             "the row(s) making up the column names"
         )
diff --git a/pandas/io/date_converters.py b/pandas/io/date_converters.py
index 7fdca2d65..07919dbda 100644
--- a/pandas/io/date_converters.py
+++ b/pandas/io/date_converters.py
@@ -57,8 +57,7 @@ def _check_columns(cols):
     for i, n in enumerate(map(len, tail)):
         if n != N:
             raise AssertionError(
-                f"All columns must have the same length: {N}; "
-                f"column {i} has length {n}"
+                f"All columns must have the same length: {N}; column {i} has length {n}"
             )
 
     return N
diff --git a/pandas/io/feather_format.py b/pandas/io/feather_format.py
index eb05004d9..5d4925620 100644
--- a/pandas/io/feather_format.py
+++ b/pandas/io/feather_format.py
@@ -37,16 +37,13 @@ def to_feather(df: DataFrame, path):
         typ = type(df.index)
         raise ValueError(
             f"feather does not support serializing {typ} "
-            "for the index; you can .reset_index() "
-            "to make the index into column(s)"
+            "for the index; you can .reset_index() to make the index into column(s)"
         )
 
     if not df.index.equals(RangeIndex.from_range(range(len(df)))):
         raise ValueError(
-            "feather does not support serializing a "
-            "non-default index for the index; you "
-            "can .reset_index() to make the index "
-            "into column(s)"
+            "feather does not support serializing a non-default index for the index; "
+            "you can .reset_index() to make the index into column(s)"
         )
 
     if df.index.name is not None:
diff --git a/pandas/io/html.py b/pandas/io/html.py
index eafcca0e8..809ce77ee 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -899,8 +899,7 @@ def _parse(flavor, io, match, attrs, encoding, displayed_only, **kwargs):
                     f"The flavor {flav} failed to parse your input. "
                     "Since you passed a non-rewindable file "
                     "object, we can't rewind it to try "
-                    "another parser. Try read_html() with a "
-                    "different flavor."
+                    "another parser. Try read_html() with a different flavor."
                 )
 
             retained = caught
diff --git a/pandas/io/json/_normalize.py b/pandas/io/json/_normalize.py
index c0596c984..cf292a13f 100644
--- a/pandas/io/json/_normalize.py
+++ b/pandas/io/json/_normalize.py
@@ -317,8 +317,7 @@ def _json_normalize(
                                 meta_val = np.nan
                             else:
                                 raise KeyError(
-                                    "Try running with "
-                                    "errors='ignore' as key "
+                                    "Try running with errors='ignore' as key "
                                     f"{e} is not always present"
                                 )
                     meta_vals[key].append(meta_val)
diff --git a/pandas/io/parquet.py b/pandas/io/parquet.py
index 3a686a1a3..4be62b886 100644
--- a/pandas/io/parquet.py
+++ b/pandas/io/parquet.py
@@ -32,8 +32,7 @@ def get_engine(engine: str) -> "BaseImpl":
         raise ImportError(
             "Unable to find a usable engine; "
             "tried using: 'pyarrow', 'fastparquet'.\n"
-            "pyarrow or fastparquet is required for parquet "
-            "support"
+            "pyarrow or fastparquet is required for parquet support"
         )
 
     if engine == "pyarrow":
@@ -156,8 +155,7 @@ class FastParquetImpl(BaseImpl):
         if "partition_on" in kwargs and partition_cols is not None:
             raise ValueError(
                 "Cannot use both partition_on and "
-                "partition_cols. Use partition_cols for "
-                "partitioning data"
+                "partition_cols. Use partition_cols for partitioning data"
             )
         elif "partition_on" in kwargs:
             partition_cols = kwargs.pop("partition_on")
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index b4eb2fb14..62b82f174 100755
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -612,8 +612,7 @@ def _make_parser_function(name, default_sep=","):
         if delim_whitespace and delimiter != default_sep:
             raise ValueError(
                 "Specified a delimiter with both sep and "
-                "delim_whitespace=True; you can only "
-                "specify one."
+                "delim_whitespace=True; you can only specify one."
             )
 
         if engine is not None:
@@ -968,8 +967,7 @@ class TextFileReader(abc.Iterator):
                 fallback_reason = (
                     "the 'c' engine does not support "
                     "regex separators (separators > 1 char and "
-                    r"different from '\s+' are "
-                    "interpreted as regex)"
+                    r"different from '\s+' are interpreted as regex)"
                 )
                 engine = "python"
         elif delim_whitespace:
@@ -1000,8 +998,7 @@ class TextFileReader(abc.Iterator):
                 fallback_reason = (
                     "ord(quotechar) > 127, meaning the "
                     "quotechar is larger than one byte, "
-                    "and the 'c' engine does not support "
-                    "such quotechars"
+                    "and the 'c' engine does not support such quotechars"
                 )
                 engine = "python"
 
@@ -1119,9 +1116,8 @@ class TextFileReader(abc.Iterator):
                 klass = FixedWidthFieldParser
             else:
                 raise ValueError(
-                    f"Unknown engine: {engine} (valid options are "
-                    '"c", "python", or '
-                    '"python-fwf")'
+                    f"Unknown engine: {engine} (valid options "
+                    'are "c", "python", or "python-fwf")'
                 )
             self._engine = klass(self.f, **self.options)
 
@@ -1230,8 +1226,7 @@ def _validate_usecols_names(usecols, names):
     missing = [c for c in usecols if c not in names]
     if len(missing) > 0:
         raise ValueError(
-            "Usecols do not match columns, "
-            f"columns expected but not found: {missing}"
+            f"Usecols do not match columns, columns expected but not found: {missing}"
         )
 
     return usecols
@@ -1325,8 +1320,7 @@ def _validate_parse_dates_arg(parse_dates):
     that is the case.
     """
     msg = (
-        "Only booleans, lists, and "
-        "dictionaries are accepted "
+        "Only booleans, lists, and dictionaries are accepted "
         "for the 'parse_dates' parameter"
     )
 
@@ -1680,8 +1674,7 @@ class ParserBase:
                     warnings.warn(
                         (
                             "Both a converter and dtype were specified "
-                            f"for column {c} - only the converter will "
-                            "be used"
+                            f"for column {c} - only the converter will be used"
                         ),
                         ParserWarning,
                         stacklevel=7,
@@ -1826,8 +1819,7 @@ class ParserBase:
             except NotImplementedError:
                 raise NotImplementedError(
                     f"Extension Array: {array_type} must implement "
-                    "_from_sequence_of_strings in order "
-                    "to be used in parser methods"
+                    "_from_sequence_of_strings in order to be used in parser methods"
                 )
 
         else:
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index d61d1cf7f..9e8d8a2e8 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -413,8 +413,8 @@ def read_hdf(
             for group_to_check in groups[1:]:
                 if not _is_metadata_of(group_to_check, candidate_only_group):
                     raise ValueError(
-                        "key must be provided when HDF5 file "
-                        "contains multiple datasets."
+                        "key must be provided when HDF5 "
+                        "file contains multiple datasets."
                     )
             key = candidate_only_group._v_pathname
         return store.select(
@@ -1240,8 +1240,7 @@ class HDFStore:
             if v is None:
                 if remain_key is not None:
                     raise ValueError(
-                        "append_to_multiple can only have one value in d that "
-                        "is None"
+                        "append_to_multiple can only have one value in d that is None"
                     )
                 remain_key = k
             else:
@@ -2313,8 +2312,7 @@ class DataCol(IndexCol):
             existing_dtype = getattr(self.attrs, self.dtype_attr, None)
             if existing_dtype is not None and existing_dtype != self.dtype:
                 raise ValueError(
-                    "appended items dtype do not match existing "
-                    "items dtype in table!"
+                    "appended items dtype do not match existing items dtype in table!"
                 )
 
     def convert(self, values: np.ndarray, nan_rep, encoding: str, errors: str):
@@ -2680,14 +2678,12 @@ class GenericFixed(Fixed):
         if columns is not None:
             raise TypeError(
                 "cannot pass a column specification when reading "
-                "a Fixed format store. this store must be "
-                "selected in its entirety"
+                "a Fixed format store. this store must be selected in its entirety"
             )
         if where is not None:
             raise TypeError(
                 "cannot pass a where specification when reading "
-                "from a Fixed format store. this store must be "
-                "selected in its entirety"
+                "from a Fixed format store. this store must be selected in its entirety"
             )
 
     @property
@@ -2908,8 +2904,7 @@ class GenericFixed(Fixed):
 
         if is_categorical_dtype(value):
             raise NotImplementedError(
-                "Cannot store a category dtype in "
-                "a HDF5 dataset that uses format="
+                "Cannot store a category dtype in a HDF5 dataset that uses format="
                 '"fixed". Use format="table".'
             )
         if not empty_array:
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index f4527994d..58fed0d18 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -977,8 +977,7 @@ class SQLTable(PandasObject):
         if col_type == "timedelta64":
             warnings.warn(
                 "the 'timedelta' type is not supported, and will be "
-                "written as integer values (ns frequency) to the "
-                "database.",
+                "written as integer values (ns frequency) to the database.",
                 UserWarning,
                 stacklevel=8,
             )
@@ -1413,8 +1412,7 @@ def _get_valid_sqlite_name(name):
 
 _SAFE_NAMES_WARNING = (
     "The spaces in these column names will not be changed. "
-    "In pandas versions < 0.14, spaces were converted to "
-    "underscores."
+    "In pandas versions < 0.14, spaces were converted to underscores."
 )
 
 
@@ -1528,8 +1526,7 @@ class SQLiteTable(SQLTable):
         if col_type == "timedelta64":
             warnings.warn(
                 "the 'timedelta' type is not supported, and will be "
-                "written as integer values (ns frequency) to the "
-                "database.",
+                "written as integer values (ns frequency) to the database.",
                 UserWarning,
                 stacklevel=8,
             )
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index b216ee80c..2c1222aad 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -639,8 +639,7 @@ class StataValueLabel:
         if self.text_len > 32000:
             raise ValueError(
                 "Stata value labels for a single variable must "
-                "have a combined length less than 32,000 "
-                "characters."
+                "have a combined length less than 32,000 characters."
             )
 
         # Ensure int32
@@ -1729,9 +1728,10 @@ the string values returned are correct."""
                 raise ValueError("columns contains duplicate entries")
             unmatched = column_set.difference(data.columns)
             if unmatched:
+                joined = ", ".join(list(unmatched))
                 raise ValueError(
-                    "The following columns were not found in the "
-                    "Stata data set: " + ", ".join(list(unmatched))
+                    "The following columns were not "
+                    f"found in the Stata data set: {joined}"
                 )
             # Copy information for retained columns for later processing
             dtyplist = []
diff --git a/pandas/plotting/_matplotlib/converter.py b/pandas/plotting/_matplotlib/converter.py
index 5b37ebb42..a1035fd08 100644
--- a/pandas/plotting/_matplotlib/converter.py
+++ b/pandas/plotting/_matplotlib/converter.py
@@ -421,8 +421,7 @@ class MilliSecondLocator(dates.DateLocator):
         if estimate > self.MAXTICKS * 2:
             raise RuntimeError(
                 "MillisecondLocator estimated to generate "
-                f"{estimate:d} ticks from {dmin} to {dmax}: "
-                "exceeds Locator.MAXTICKS"
+                f"{estimate:d} ticks from {dmin} to {dmax}: exceeds Locator.MAXTICKS"
                 f"* 2 ({self.MAXTICKS * 2:d}) "
             )
 
diff --git a/pandas/plotting/_matplotlib/hist.py b/pandas/plotting/_matplotlib/hist.py
index f8b2c7ab1..d54fc73b4 100644
--- a/pandas/plotting/_matplotlib/hist.py
+++ b/pandas/plotting/_matplotlib/hist.py
@@ -318,8 +318,7 @@ def hist_series(
         if "figure" in kwds:
             raise ValueError(
                 "Cannot pass 'figure' when using the "
-                "'by' argument, since a new 'Figure' instance "
-                "will be created"
+                "'by' argument, since a new 'Figure' instance will be created"
             )
         axes = _grouped_hist(
             self,
diff --git a/pandas/plotting/_matplotlib/tools.py b/pandas/plotting/_matplotlib/tools.py
index dd4034a97..d7732c869 100644
--- a/pandas/plotting/_matplotlib/tools.py
+++ b/pandas/plotting/_matplotlib/tools.py
@@ -190,8 +190,7 @@ def _subplots(
             if sharex or sharey:
                 warnings.warn(
                     "When passing multiple axes, sharex and sharey "
-                    "are ignored. These settings must be specified "
-                    "when creating axes",
+                    "are ignored. These settings must be specified when creating axes",
                     UserWarning,
                     stacklevel=4,
                 )
