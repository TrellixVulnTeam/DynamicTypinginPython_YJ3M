commit e18b546b8185e6b1881d11132484aecd7af9f509
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Jul 26 22:36:40 2011 -0400

    ENH: proper masking using counts in groupby. unit test and release notes

diff --git a/RELEASE.rst b/RELEASE.rst
index cd7c4046b..adfd9c3ab 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -35,10 +35,6 @@ Release notes
   will result in significant performance boosts, and smaller memory
   footprint. Added `to_sparse` methods to `Series`, `DataFrame`, and
   `WidePanel`. See online documentation for more on these
-* `Index` objects (labels for axes) are now capable of holding tuples
-* `Series.describe`, `DataFrame.describe`: produces an R-like table of summary
-  statistics about each data column
-* `DataFrame.quantile`, `Series.quantile`
 * Fancy indexing operator on Series / DataFrame, e.g. via .ix operator. Both
   getting and setting of values is supported; however, setting values will only
   currently work on homogeneously-typed DataFrame objects
@@ -53,6 +49,11 @@ Release notes
   * Added automatic "dispatching to Series / DataFrame methods to more easily
     invoke methods on groups. e.g. s.groupby(crit).std() will work even though
     `std` is not implemented on the `GroupBy` class
+* `Index` objects (labels for axes) are now capable of holding tuples
+* `Series.describe`, `DataFrame.describe`: produces an R-like table of summary
+  statistics about each data column
+* `DataFrame.quantile`, `Series.quantile` for computing sample quantiles of data
+  across requested axis
 * `Series` arithmetic methods with optional fill_value for missing data,
   e.g. a.add(b, fill_value=0). If a location is missing for both it will still
   be missing in the result though.
@@ -98,8 +99,11 @@ Release notes
   `DataFrame`. In prior releases, there was inconsistent column ordering in
   `DataMatrix`
 * Improved console / string formatting of DataMatrix with negative numbers
-* Added `skiprows` and `na_values` arguments to `pandas.io.parsers` functions
-  for more flexible IO
+* Improved tabular data parsing functions, `read_table` and `read_csv`:
+  * Added `skiprows` and `na_values` arguments to `pandas.io.parsers` functions
+    for more flexible IO
+  * `parseCSV` / `read_csv` functions and others in `pandas.io.parsers` now can
+    take a list of custom NA values, and also a list of rows to skip
 * Can slice `DataFrame` and get a view of the data (when homogeneously typed),
   e.g. frame.xs(idx, copy=False) or frame.ix[idx]
 * Many speed optimizations throughout `Series` and `DataFrame`
@@ -108,8 +112,6 @@ Release notes
   sometime later on when the groups are needed
 * `datetools.WeekOfMonth` offset can be parameterized with `n` different than 1
   or -1.
-* `parseCSV` / `read_csv` functions and others in `pandas.io.parsers` now can
-  take a list of custom NA values, and also a list of rows to skip
 * Statistical methods on DataFrame like `mean`, `std`, `var`, `skew` will now
   ignore non-numerical data. Before a not very useful error message was
   generated. A flag `numeric_only` has been added to `DataFrame.sum` and
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index fc604d3cd..3aa05bb9d 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -185,13 +185,10 @@ class GroupBy(object):
                 cannot_agg.append(name)
                 continue
 
-            result =  _tseries.group_aggregate(obj, label_list,
-                                               shape, how=how)
+            result, counts =  _tseries.group_aggregate(obj, label_list,
+                                                       shape, how=how)
             result = result.ravel()
-
-            # better to get the counts!
-            mask = -np.isnan(result)
-
+            mask = counts.ravel() > 0
             output[name] = result[mask]
 
         # do I want a warning message or silently exclude?
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index 4aa7d6c1a..c406dd9ff 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -147,8 +147,9 @@ def group_labels(ndarray[object] values):
 
     return reverse, labels, counts[:count].copy()
 
-ctypedef double_t (* agg_func)(double_t *out, double_t *values, int32_t *labels,
-                               int start, int end, Py_ssize_t offset)
+ctypedef double_t (* agg_func)(double_t *out, int32_t *counts, double_t *values,
+                               int32_t *labels, int start, int end,
+                               Py_ssize_t offset)
 
 cdef agg_func get_agg_func(object how):
     if how == 'add':
@@ -161,7 +162,7 @@ def group_aggregate(ndarray[double_t] values, list label_list,
                     object shape, how='add'):
     cdef:
         list sorted_labels
-        ndarray result
+        ndarray result, counts
         agg_func func
 
     func = get_agg_func(how)
@@ -170,15 +171,20 @@ def group_aggregate(ndarray[double_t] values, list label_list,
     result = np.empty(shape, dtype=np.float64)
     result.fill(nan)
 
+    counts = np.zeros(shape, dtype=np.int32)
+
     if not values.flags.c_contiguous:
         values = values.copy()
 
-    _aggregate_group(<double_t*> result.data, <double_t*> values.data,
-                     sorted_labels, 0, len(values), shape, 0, 0, func)
+    _aggregate_group(<double_t*> result.data,
+                     <int32_t*> counts.data,
+                     <double_t*> values.data,
+                     sorted_labels, 0, len(values),
+                     shape, 0, 0, func)
 
-    return result
+    return result, counts
 
-cdef void _aggregate_group(double_t *out, double_t *values,
+cdef void _aggregate_group(double_t *out, int32_t *counts, double_t *values,
                            list labels, int start, int end, tuple shape,
                            Py_ssize_t which, Py_ssize_t offset,
                            agg_func func):
@@ -190,7 +196,7 @@ cdef void _aggregate_group(double_t *out, double_t *values,
     if which == len(labels) - 1:
         # print axis, start, end
         axis = labels[which]
-        func(out, values, <int32_t*> axis.data, start, end, offset)
+        func(out, counts, values, <int32_t*> axis.data, start, end, offset)
     else:
         axis = labels[which][start:end]
         stride = np.prod(shape[which+1:])
@@ -200,21 +206,26 @@ cdef void _aggregate_group(double_t *out, double_t *values,
         left = 0
         # aggregate each subgroup
         for right in edges:
-            _aggregate_group(out, values, labels, start + left, start + right,
-                             shape, which + 1, offset, func)
+            _aggregate_group(out, counts, values, labels, start + left,
+                             start + right, shape, which + 1, offset, func)
             offset += stride
             left = right
 
-cdef double_t _group_add(double_t *out, double_t *values, int32_t *labels,
-                         int start, int end, Py_ssize_t offset):
+# TODO: aggregate multiple columns in single pass
+
+cdef double_t _group_add(double_t *out, int32_t *counts, double_t *values,
+                         int32_t *labels, int start, int end,
+                         Py_ssize_t offset):
     cdef:
         Py_ssize_t i = 0, it = start
         int32_t lab
-        int32_t count = 0
+        int32_t count = 0, tot = 0
         double_t val, cum = 0
 
     while it < end:
         val = values[it]
+        tot += 1
+
         # not nan
         if val == val:
             count += 1
@@ -225,23 +236,30 @@ cdef double_t _group_add(double_t *out, double_t *values, int32_t *labels,
                 out[offset + i] = nan
             else:
                 out[offset + i] = cum
+
+            counts[offset + i] = tot
+
             count = 0
             cum = 0
+            tot = 0
 
             i += 1
 
         it += 1
 
-cdef double_t _group_mean(double_t *out, double_t *values, int32_t *labels,
-                          int start, int end, Py_ssize_t offset):
+cdef double_t _group_mean(double_t *out, int32_t *counts, double_t *values,
+                          int32_t *labels, int start, int end,
+                          Py_ssize_t offset):
     cdef:
         Py_ssize_t i = 0, it = start
         int32_t lab
-        int32_t count = 0
+        int32_t count = 0, tot = 0
         double_t val, cum = 0
 
     while it < end:
         val = values[it]
+        tot += 1
+
         # not nan
         if val == val:
             count += 1
@@ -252,8 +270,12 @@ cdef double_t _group_mean(double_t *out, double_t *values, int32_t *labels,
                 out[offset + i] = nan
             else:
                 out[offset + i] = cum / count
+
+            counts[offset + i] = tot
+
             count = 0
             cum = 0
+            tot = 0
 
             i += 1
 
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 6428592e7..622fa0746 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -315,6 +315,17 @@ class TestGroupBy(unittest.TestCase):
         # hierarchical index
         assert_series_equal(result['result'], expected)
 
+    def test_groupby_multi_corner(self):
+        # test that having an all-NA column doesn't mess you up
+        df = self.df.copy()
+        df['bad'] = np.nan
+        agged = df.groupby(['A', 'B']).mean()
+
+        expected = self.df.groupby(['A', 'B']).mean()
+        expected['bad'] = np.nan
+
+        assert_frame_equal(agged, expected)
+
     def test_omit_nuisance(self):
         grouped = self.df.groupby('A')
         result = grouped.mean()
