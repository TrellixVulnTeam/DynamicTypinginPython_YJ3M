commit 0a9d4962dc7b813e9ffcf93a9550a9a55fd3aff8
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon May 7 19:40:07 2012 -0400

    ENH: groupby refactoring to enable reasonably fast resampling of panel data, unit test for user-specified function fails #1149

diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 590061012..ce657eb16 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -1259,6 +1259,23 @@ class DataFrame(NDFrame):
 
     T = property(transpose)
 
+    def swapaxes(self, i, j):
+        """
+        Like ndarray.swapaxes, equivalent to transpose
+
+        Returns
+        -------
+        swapped : DataFrame
+            View on original data (no copy)
+        """
+        if i in (0, 1) and j in (0, 1):
+            if i == j:
+                return self
+            return self._constructor(data=self.values.T, index=self.columns,
+                                     columns=self.index, copy=False)
+        else:  # pragma: no cover
+            raise ValueError('Axis numbers must be in (0, 1)')
+
     #----------------------------------------------------------------------
     # Picklability
 
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 089802695..2fb20d7b9 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -18,6 +18,19 @@ class GroupByError(Exception):
     pass
 
 
+def _groupby_function(name, alias, npfunc):
+    def f(self):
+        try:
+            return self._cython_agg_general(alias)
+        except Exception:
+            return self.aggregate(lambda x: npfunc(x, axis=self.axis))
+
+    f.__doc__ = "Compute %s of group values" % name
+    f.__name__ = name
+
+    return f
+
+
 class GroupBy(object):
     """
     Class for grouping and aggregating relational data. See aggregate,
@@ -297,49 +310,10 @@ class GroupBy(object):
         """
         return self.grouper.size()
 
-    def sum(self):
-        """
-        Compute sum of values, excluding missing values
-
-        For multiple groupings, the result index will be a MultiIndex
-        """
-        try:
-            return self._cython_agg_general('add')
-        except Exception:
-            return self.aggregate(lambda x: np.sum(x, axis=self.axis))
-
-    def prod(self):
-        """
-        Compute product of values, excluding missing values
-
-        For multiple groupings, the result index will be a MultiIndex
-        """
-        try:
-            return self._cython_agg_general('prod')
-        except Exception:
-            return self.aggregate(lambda x: np.prod(x, axis=self.axis))
-
-    def min(self):
-        """
-        Compute minimum of values, excluding missing values
-
-        For multiple groupings, the result index will be a MultiIndex
-        """
-        try:
-            return self._cython_agg_general('min')
-        except Exception:
-            return self.aggregate(lambda x: np.min(x, axis=self.axis))
-
-    def max(self):
-        """
-        Compute maximum of values, excluding missing values
-
-        For multiple groupings, the result index will be a MultiIndex
-        """
-        try:
-            return self._cython_agg_general('max')
-        except Exception:
-            return self.aggregate(lambda x: np.max(x, axis=self.axis))
+    sum = _groupby_function('sum', 'add', np.sum)
+    prod = _groupby_function('prod', 'prod', np.prod)
+    min = _groupby_function('min', 'min', np.min)
+    max = _groupby_function('max', 'max', np.max)
 
     def ohlc(self):
         """
@@ -654,39 +628,40 @@ class Grouper(object):
         'std' : np.sqrt
     }
 
+    _cython_arity = {
+        'ohlc' : 4, # OHLC
+    }
+
     _name_functions = {}
 
     _filter_empty_groups = True
 
-    def aggregate(self, values, how):
+    def aggregate(self, values, how, axis=0):
         values = com._ensure_float64(values)
-
-        comp_ids, _, ngroups = self.group_info
-        agg_func = self._cython_functions[how]
+        arity = self._cython_arity.get(how, 1)
 
         vdim = values.ndim
-
         if vdim == 1:
             values = values[:, None]
-            out_shape = (ngroups, 1)
+            out_shape = (self.ngroups, arity)
         else:
-            out_shape = (ngroups, values.shape[1])
-
-        trans_func = self._cython_transforms.get(how, lambda x: x)
+            if axis > 0:
+                values = values.swapaxes(0, axis)
+            if arity > 1:
+                raise NotImplementedError
+            out_shape = (self.ngroups,) + values.shape[1:]
 
         # will be filled in Cython function
         result = np.empty(out_shape, dtype=np.float64)
-        counts = np.zeros(ngroups, dtype=np.int32)
+        counts = np.zeros(self.ngroups, dtype=np.int32)
 
-        agg_func(result, counts, values, comp_ids)
-        result = trans_func(result)
+        result = self._aggregate(result, counts, values, how)
 
         if self._filter_empty_groups:
-            # python 2.5
             result = lib.row_bool_subset(result,
                                          (counts > 0).view(np.uint8))
 
-        if vdim == 1:
+        if vdim == 1 and arity == 1:
             result = result[:, 0]
 
         if how in self._name_functions:
@@ -695,8 +670,27 @@ class Grouper(object):
         else:
             names = None
 
+        if axis > 0:
+            result = result.swapaxes(0, axis)
+
         return result, names
 
+    def _aggregate(self, result, counts, values, how):
+        agg_func = self._cython_functions[how]
+        trans_func = self._cython_transforms.get(how, lambda x: x)
+
+        comp_ids, _, ngroups = self.group_info
+        if values.ndim > 3:
+            # punting for now
+            raise NotImplementedError
+        elif values.ndim > 2:
+            for i, chunk in enumerate(values.transpose(2, 0, 1)):
+                agg_func(result[:, :, i], counts, chunk, comp_ids)
+        else:
+            agg_func(result, counts, values, comp_ids)
+
+        return trans_func(result)
+
     def agg_series(self, obj, func):
         try:
             return self._aggregate_series_fast(obj, func)
@@ -867,52 +861,26 @@ class BinGrouper(Grouper):
         'ohlc' : lib.group_ohlc
     }
 
-    _cython_arity = {
-        'ohlc' : 4, # OHLC
-    }
-
     _name_functions = {
         'ohlc' : lambda *args: ['open', 'high', 'low', 'close']
     }
 
     _filter_empty_groups = True
 
-    def aggregate(self, values, how):
-        values = com._ensure_float64(values)
-
+    def _aggregate(self, result, counts, values, how):
         agg_func = self._cython_functions[how]
-        arity = self._cython_arity.get(how, 1)
-
-        vdim = values.ndim
-        if vdim == 1:
-            values = values[:, None]
-            out_shape = (self.ngroups, arity)
-        else:
-            out_shape = (self.ngroups, values.shape[1] * arity)
-
         trans_func = self._cython_transforms.get(how, lambda x: x)
 
-        # will be filled in Cython function
-        result = np.empty(out_shape, dtype=np.float64)
-        counts = np.zeros(self.ngroups, dtype=np.int32)
-
-        agg_func(result, counts, values, self.bins)
-        result = trans_func(result)
-
-        if self._filter_empty_groups:
-            result = lib.row_bool_subset(result,
-                                         (counts > 0).view(np.uint8))
-
-        if vdim == 1 and arity == 1:
-            result = result[:, 0]
-
-        if how in self._name_functions:
-            # TODO
-            names = self._name_functions[how]()
+        if values.ndim > 3:
+            # punting for now
+            raise NotImplementedError
+        elif values.ndim > 2:
+            for i, chunk in enumerate(values.transpose(2, 0, 1)):
+                agg_func(result[:, :, i], counts, chunk, self.bins)
         else:
-            names = None
+            agg_func(result, counts, values, self.bins)
 
-        return result, names
+        return trans_func(result)
 
     def agg_series(self, obj, func):
         dummy = obj[:0]
@@ -1294,25 +1262,7 @@ class SeriesGroupBy(GroupBy):
 
         return result
 
-class DataFrameGroupBy(GroupBy):
-
-    def __getitem__(self, key):
-        if self._column is not None:
-            raise Exception('Column %s already selected' % self._column)
-
-        if key not in self.obj:  # pragma: no cover
-            raise KeyError(str(key))
-
-        # kind of a kludge
-        if self.as_index:
-            return SeriesGroupBy(self.obj[key], column=key,
-                                 grouper=self.grouper,
-                                 exclusions=self.exclusions)
-        else:
-            return DataFrameGroupBy(self.obj, self.grouper, column=key,
-                                    grouper=self.grouper,
-                                    exclusions=self.exclusions,
-                                    as_index=self.as_index)
+class NDFrameGroupBy(GroupBy):
 
     def _iterate_slices(self):
         if self.axis == 0:
@@ -1333,59 +1283,54 @@ class DataFrameGroupBy(GroupBy):
             yield val, slicer(val)
 
     def _cython_agg_general(self, how):
+        new_blocks = self._cython_agg_blocks(how)
+        return self._wrap_agged_blocks(new_blocks)
+
+    def _wrap_agged_blocks(self, blocks):
         obj = self._obj_with_exclusions
-        if self.axis == 1:
-            obj = obj.T
+
+        new_axes = list(obj._data.axes)
+        new_axes[self.axis] = self.grouper.result_index
+        mgr = BlockManager(blocks, new_axes)
+
+        new_obj = type(obj)(mgr)
+
+        return self._post_process_cython_aggregate(new_obj)
+
+    _block_agg_axis = 0
+
+    def _cython_agg_blocks(self, how):
+        data, agg_axis = self._get_data_to_aggregate()
 
         new_blocks = []
 
-        for block in obj._data.blocks:
-            values = block.values.T
+        for block in data.blocks:
+            values = block.values
             if not issubclass(values.dtype.type, (np.number, np.bool_)):
                 continue
 
             values = com._ensure_float64(values)
-            result, names = self.grouper.aggregate(values, how)
-            newb = make_block(result.T, block.items, block.ref_items)
+            result, names = self.grouper.aggregate(values, how, axis=agg_axis)
+            newb = make_block(result, block.items, block.ref_items)
             new_blocks.append(newb)
 
         if len(new_blocks) == 0:
             raise GroupByError('No numeric types to aggregate')
 
-        agg_axis = 0 if self.axis == 1 else 1
-        agg_labels = self._obj_with_exclusions._get_axis(agg_axis)
-
-        if sum(len(x.items) for x in new_blocks) == len(agg_labels):
-            output_keys = agg_labels
-        else:
-            all_items = []
-            for b in new_blocks:
-                all_items.extend(b.items)
-            output_keys = agg_labels[agg_labels.isin(all_items)]
-
-            for blk in new_blocks:
-                blk.set_ref_items(output_keys, maybe_rename=False)
-
-        if not self.as_index:
-            index = np.arange(new_blocks[0].values.shape[1])
-            mgr = BlockManager(new_blocks, [output_keys, index])
-            result = DataFrame(mgr)
-
-            group_levels = self.grouper.get_group_levels()
-            zipped = zip(self.grouper.names, group_levels)
+        return new_blocks
 
-            for i, (name, labels) in enumerate(zipped):
-                result.insert(i, name, labels)
-            result = result.consolidate()
+    def _get_data_to_aggregate(self):
+        obj = self._obj_with_exclusions
+        if self.axis == 0:
+            return obj.swapaxes(0, 1)._data, 1
         else:
-            index = self.grouper.result_index
-            mgr = BlockManager(new_blocks, [output_keys, index])
-            result = DataFrame(mgr)
+            return obj._data, self.axis
 
-        if self.axis == 1:
-            result = result.T
-
-        return result
+    def _post_process_cython_aggregate(self, obj):
+        # undoing kludge from below
+        if self.axis == 0:
+            obj = obj.swapaxes(0, 1)
+        return obj
 
     @cache_readonly
     def _obj_with_exclusions(self):
@@ -1413,7 +1358,7 @@ class DataFrameGroupBy(GroupBy):
         aggregated : DataFrame
         """
         if isinstance(arg, basestring):
-            return getattr(self, arg)()
+            return getattr(self, arg)(*args, **kwargs)
 
         result = {}
         if isinstance(arg, dict):
@@ -1671,9 +1616,104 @@ class DataFrameGroupBy(GroupBy):
                               axis=self.axis, verify_integrity=False)
         return concatenated.reindex_like(obj)
 
-class PanelGroupBy(GroupBy):
 
-    def aggregate(self, func, *args, **kwargs):
+
+class DataFrameGroupBy(NDFrameGroupBy):
+
+    _block_agg_axis = 1
+
+    def __getitem__(self, key):
+        if self._column is not None:
+            raise Exception('Column %s already selected' % self._column)
+
+        if key not in self.obj:  # pragma: no cover
+            raise KeyError(str(key))
+
+        # kind of a kludge
+        if self.as_index:
+            return SeriesGroupBy(self.obj[key], column=key,
+                                 grouper=self.grouper,
+                                 exclusions=self.exclusions)
+        else:
+            return DataFrameGroupBy(self.obj, self.grouper, column=key,
+                                    grouper=self.grouper,
+                                    exclusions=self.exclusions,
+                                    as_index=self.as_index)
+
+    def _get_data_to_aggregate(self):
+        obj = self._obj_with_exclusions
+        if self.axis == 1:
+            return obj.T._data, 1
+        else:
+            return obj._data, 1
+
+    def _post_process_cython_aggregate(self, obj):
+        # undoing kludge from below
+        if self.axis == 0:
+            obj = obj.T
+        return obj
+
+    def _wrap_agged_blocks(self, blocks):
+        obj = self._obj_with_exclusions
+
+        if self.axis == 0:
+            agg_labels = obj.columns
+        else:
+            agg_labels = obj.index
+
+        if sum(len(x.items) for x in blocks) == len(agg_labels):
+            output_keys = agg_labels
+        else:
+            all_items = []
+            for b in blocks:
+                all_items.extend(b.items)
+            output_keys = agg_labels[agg_labels.isin(all_items)]
+
+            for blk in blocks:
+                blk.set_ref_items(output_keys, maybe_rename=False)
+
+        if not self.as_index:
+            index = np.arange(blocks[0].values.shape[1])
+            mgr = BlockManager(blocks, [output_keys, index])
+            result = DataFrame(mgr)
+
+            group_levels = self.grouper.get_group_levels()
+            zipped = zip(self.grouper.names, group_levels)
+
+            for i, (name, labels) in enumerate(zipped):
+                result.insert(i, name, labels)
+            result = result.consolidate()
+        else:
+            index = self.grouper.result_index
+            mgr = BlockManager(blocks, [output_keys, index])
+            result = DataFrame(mgr)
+
+        if self.axis == 1:
+            result = result.T
+
+        return result
+
+class PanelGroupBy(NDFrameGroupBy):
+
+    def _iterate_slices(self):
+        if self.axis == 0:
+            # kludge
+            if self._column is None:
+                slice_axis = self.obj.items
+            else:
+                slice_axis = [self._column]
+            slicer = lambda x: self.obj[x]
+        elif foo:
+            slice_axis = self.obj.index
+            slicer = lambda x: self.obj.xs(x, axis=self.axis)
+
+        for val in slice_axis:
+            if val in self.exclusions:
+                continue
+
+            yield val, slicer(val)
+
+    def aggregate(self, arg, *args, **kwargs):
         """
         Aggregate using input function or dict of {column -> function}
 
@@ -1688,7 +1728,10 @@ class PanelGroupBy(GroupBy):
         -------
         aggregated : Panel
         """
-        return self._aggregate_generic(func, *args, **kwargs)
+        if isinstance(arg, basestring):
+            return getattr(self, arg)(*args, **kwargs)
+
+        return self._aggregate_generic(arg, *args, **kwargs)
 
     def _aggregate_generic(self, func, *args, **kwargs):
         result = {}
@@ -1705,7 +1748,7 @@ class PanelGroupBy(GroupBy):
                 wrapper = lambda x: func(x, *args, **kwargs)
                 result[name] = data.apply(wrapper, axis=axis)
 
-        result = Panel.fromDict(result, intersect=False)
+        result = Panel(result)
 
         if axis > 0:
             result = result.swapaxes(0, axis)
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index 8b1953cbd..6bb21e0d2 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -924,8 +924,8 @@ def group_ohlc(ndarray[float64_t, ndim=2] out,
             out[b, 3] = vclose
 
 
-@cython.boundscheck(False)
-@cython.wraparound(False)
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
 def group_mean_bin(ndarray[float64_t, ndim=2] out,
                    ndarray[int32_t] counts,
                    ndarray[float64_t, ndim=2] values,
diff --git a/pandas/tseries/resample.py b/pandas/tseries/resample.py
index 4af2bc800..d175d3667 100644
--- a/pandas/tseries/resample.py
+++ b/pandas/tseries/resample.py
@@ -52,9 +52,6 @@ class TimeGrouper(CustomGrouper):
         self.fill_method = fill_method
         self.limit = limit
 
-        if axis != 0:
-            raise NotImplementedError
-
     def resample(self, obj):
         axis = obj._get_axis(self.axis)
         if isinstance(axis, DatetimeIndex):
diff --git a/pandas/tseries/tests/test_resample.py b/pandas/tseries/tests/test_resample.py
index 1915e6d73..9cb02f6f6 100644
--- a/pandas/tseries/tests/test_resample.py
+++ b/pandas/tseries/tests/test_resample.py
@@ -2,7 +2,7 @@ from datetime import datetime, timedelta
 
 import numpy as np
 
-from pandas import Series, TimeSeries, DataFrame, isnull, notnull
+from pandas import Series, TimeSeries, DataFrame, Panel, isnull, notnull
 
 from pandas.tseries.index import date_range
 from pandas.tseries.offsets import Minute, bday
@@ -47,6 +47,7 @@ class TestResample(unittest.TestCase):
         for f in funcs:
             g._cython_agg_general(f)
 
+
         self.assertEquals(g.ngroups, 2593)
         self.assert_(notnull(g.mean()).all())
 
@@ -299,6 +300,52 @@ class TestResample(unittest.TestCase):
 
         self.assertRaises(Exception, ts.asfreq, 'B')
 
+    def test_resample_axis1(self):
+        rng = date_range('1/1/2000', '2/29/2000')
+        df = DataFrame(np.random.randn(3, len(rng)), columns=rng,
+                       index=['a', 'b', 'c'])
+
+        result = df.resample('M', axis=1)
+        expected = df.T.resample('M').T
+        tm.assert_frame_equal(result, expected)
+
+    def test_resample_panel(self):
+        rng = date_range('1/1/2000', '6/30/2000')
+        n = len(rng)
+
+        panel = Panel(np.random.randn(3, n, 5),
+                      items=['one', 'two', 'three'],
+                      major_axis=rng,
+                      minor_axis=['a', 'b', 'c', 'd', 'e'])
+
+        result = panel.resample('M', axis=1)
+
+        def apply(panel, f):
+            result = {}
+            for item in panel.items:
+                result[item] = f(panel[item])
+            return Panel(result, items=panel.items)
+
+        expected = apply(panel, lambda x: x.resample('M'))
+        tm.assert_panel_equal(result, expected)
+
+        panel2 = panel.swapaxes(1, 2)
+        result = panel2.resample('M', axis=2)
+        expected = apply(panel2, lambda x: x.resample('M', axis=1))
+        tm.assert_panel_equal(result, expected)
+
+    def test_resample_panel_numpy(self):
+        rng = date_range('1/1/2000', '6/30/2000')
+        n = len(rng)
+
+        panel = Panel(np.random.randn(3, n, 5),
+                      items=['one', 'two', 'three'],
+                      major_axis=rng,
+                      minor_axis=['a', 'b', 'c', 'd', 'e'])
+
+        result = panel.resample('M', how=lambda x: x.mean(), axis=1)
+        expected = panel.resample('M', how='mean', axis=1)
+        tm.assert_panel_equal(result, expected)
 
 def _simple_ts(start, end, freq='D'):
     rng = date_range(start, end, freq=freq)
