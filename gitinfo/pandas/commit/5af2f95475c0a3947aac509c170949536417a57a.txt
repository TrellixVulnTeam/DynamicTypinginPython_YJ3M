commit 5af2f95475c0a3947aac509c170949536417a57a
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Nov 23 14:10:43 2012 -0500

    ENH: accelerate label compression. accept >= 10 micros in test_perf

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 6705df681..7cd02f5eb 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -2206,16 +2206,12 @@ def _compress_group_index(group_index, sort=True):
     (comp_ids) into the list of unique labels (obs_group_ids).
     """
 
-    uniques = []
     table = lib.Int64HashTable(min(1000000, len(group_index)))
 
     group_index = com._ensure_int64(group_index)
 
     # note, group labels come out ascending (ie, 1,2,3 etc)
-    comp_ids = table.get_labels_groupby(group_index, uniques)
-
-    # these are the unique ones we observed, in the order we observed them
-    obs_group_ids = np.array(uniques, dtype=np.int64)
+    comp_ids, obs_group_ids = table.get_labels_groupby(group_index)
 
     if sort and len(obs_group_ids) > 0:
         obs_group_ids, comp_ids = _reorder_by_uniques(obs_group_ids, comp_ids)
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
index 97abbff21..06745ca3f 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/src/hashtable.pyx
@@ -502,7 +502,7 @@ cdef class Int64HashTable(HashTable):
 
         return labels, counts[:count].copy()
 
-    def get_labels_groupby(self, ndarray[int64_t] values, list uniques):
+    def get_labels_groupby(self, ndarray[int64_t] values):
         cdef:
             Py_ssize_t i, n = len(values)
             ndarray[int64_t] labels
@@ -510,6 +510,7 @@ cdef class Int64HashTable(HashTable):
             int ret = 0
             int64_t val
             khiter_t k
+            Int64Vector uniques = Int64Vector()
 
         labels = np.empty(n, dtype=np.int64)
 
@@ -532,7 +533,9 @@ cdef class Int64HashTable(HashTable):
                 labels[i] = count
                 count += 1
 
-        return labels
+        arr_uniques = uniques.to_array(xfer_data=True)
+
+        return labels, arr_uniques
 
     def unique(self, ndarray[int64_t] values):
         cdef:
diff --git a/vb_suite/reshape.py b/vb_suite/reshape.py
index 46094e5e4..e0ff758fb 100644
--- a/vb_suite/reshape.py
+++ b/vb_suite/reshape.py
@@ -37,8 +37,8 @@ reshape_pivot_time_series = Benchmark('f()', setup,
 
 setup = common_setup + """
 NUM_ROWS = 1000
-df = DataFrame({'A' : np.random.randint(50, size=NUM_ROWS),
-                'B' : np.random.randint(50, size=NUM_ROWS),
+df = DataFrame({'A' : np.random.randint(25, size=NUM_ROWS),
+                'B' : np.random.randint(25, size=NUM_ROWS),
                 'C' : np.random.randint(0,10, size=NUM_ROWS),
                 'D' : np.random.randint(0,10, size=NUM_ROWS),
                 'E' : np.random.randint(10, size=NUM_ROWS),
diff --git a/vb_suite/test_perf.py b/vb_suite/test_perf.py
index 1d9ee6942..a6534e2d8 100755
--- a/vb_suite/test_perf.py
+++ b/vb_suite/test_perf.py
@@ -108,10 +108,10 @@ def main():
                                 t_baseline=baseline_res['timing'],
                                 ratio=ratio,
                                 name=baseline_res.name),columns=["t_head","t_baseline","ratio","name"])
-        totals = totals.ix[totals.t_head > 1.0] # ignore sub 1ms
+        totals = totals.ix[totals.t_head > 0.010] # ignore sub 10micros
         totals = totals.dropna().sort("ratio").set_index('name') # sort in ascending order
 
-        s = "\n\nResults:\n" + totals.to_string(float_format=lambda x: "%0.2f" %x) + "\n\n"
+        s = "\n\nResults:\n" + totals.to_string(float_format=lambda x: "%0.4f" %x) + "\n\n"
         s += "Columns: test_name | head_time [ms] | baseline_time [ms] | ratio\n\n"
         s += "- a Ratio of 1.30 means HEAD is 30% slower then the Baseline.\n\n"
 
