commit 0802d1c1a4cb17287015094b6c3202fbaa775135
Author: Jeffrey Tratner <jeffrey.tratner@gmail.com>
Date:   Sat Jul 27 16:49:54 2013 -0400

    CLN: Py2/3-compatible dict keys/items/values.
    
    + many other fixups

diff --git a/doc/make.py b/doc/make.py
index 12b60a4f1..dbce5aaa7 100755
--- a/doc/make.py
+++ b/doc/make.py
@@ -259,7 +259,7 @@ elif len(sys.argv) > 1:
         func = funcd.get(arg)
         if func is None:
             raise SystemExit('Do not know how to handle %s; valid args are %s' % (
-                arg, funcd.keys()))
+                arg, list(funcd.keys())))
         func()
 else:
     small_docs = False
diff --git a/pandas/core/array.py b/pandas/core/array.py
index d1d29649d..842bbdbf1 100644
--- a/pandas/core/array.py
+++ b/pandas/core/array.py
@@ -2,7 +2,6 @@
 Isolate pandas's exposure to NumPy
 """
 
-from pandas.util import compat
 import numpy as np
 import six
 
@@ -18,7 +17,7 @@ _dtypes = {
 
 _lift_types = []
 
-for _k, _v in compat.iteritems(_dtypes):
+for _k, _v in _dtypes.items():
     for _i in _v:
         _lift_types.append(_k + str(_i))
 
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 25353fe33..f4ae9e9c8 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -2,10 +2,10 @@
 Misc tools for implementing data structures
 """
 
-from pandas.util.py3compat import range, long
-import itertools
 import re
 from datetime import datetime
+import codecs
+import csv
 
 from numpy.lib.format import read_array, write_array
 import numpy as np
@@ -15,15 +15,13 @@ import pandas.lib as lib
 import pandas.tslib as tslib
 
 from pandas.util import py3compat
-import codecs
-import csv
+from pandas.util.py3compat import StringIO, BytesIO, range, long
+from six.moves import zip, map
+import six
 
-from pandas.util.py3compat import StringIO, BytesIO
 
 from pandas.core.config import get_option
 from pandas.core import array as pa
-import six
-from six.moves import map
 
 # XXX: HACK for NumPy 1.5.1 to suppress warnings
 try:
@@ -1366,7 +1364,7 @@ def iterpairs(seq):
     seq_it_next = iter(seq)
     next(seq_it_next)
 
-    return itertools.izip(seq_it, seq_it_next)
+    return zip(seq_it, seq_it_next)
 
 
 def split_ranges(mask):
@@ -1992,7 +1990,7 @@ def _pprint_dict(seq, _nest_lvl=0,**kwds):
 
     nitems = get_option("max_seq_items") or len(seq)
 
-    for k, v in seq.items()[:nitems]:
+    for k, v in list(seq.items())[:nitems]:
         pairs.append(pfmt % (pprint_thing(k,_nest_lvl+1,**kwds),
                              pprint_thing(v,_nest_lvl+1,**kwds)))
 
@@ -2048,7 +2046,7 @@ def pprint_thing(thing, _nest_lvl=0, escape_chars=None, default_escapes=False,
                 translate.update(escape_chars)
             else:
                 translate = escape_chars
-            escape_chars = escape_chars.keys()
+            escape_chars = list(escape_chars.keys())
         else:
             escape_chars = escape_chars or tuple()
         for c in escape_chars:
diff --git a/pandas/core/config.py b/pandas/core/config.py
index 725f86958..34dd2b744 100644
--- a/pandas/core/config.py
+++ b/pandas/core/config.py
@@ -128,8 +128,8 @@ def _set_option(*args, **kwargs):
 
     # if 1 kwarg then it must be silent=True or silent=False
     if nkwargs:
-        k, = kwargs.keys()
-        v, = kwargs.values()
+        k, = list(kwargs.keys())
+        v, = list(kwargs.values())
 
         if k != 'silent':
             raise ValueError("the only allowed keyword argument is 'silent', "
@@ -209,7 +209,7 @@ class DictWrapper(object):
             return _get_option(prefix)
 
     def __dir__(self):
-        return self.d.keys()
+        return list(self.d.keys())
 
 # For user convenience,  we'd like to have the available options described
 # in the docstring. For dev convenience we'd like to generate the docstrings
@@ -232,7 +232,7 @@ class CallableDynamicDoc(object):
     @property
     def __doc__(self):
         opts_desc = _describe_option('all', _print_desc=False)
-        opts_list = pp_options_list(_registered_options.keys())
+        opts_list = pp_options_list(list(_registered_options.keys()))
         return self.__doc_tmpl__.format(opts_desc=opts_desc,
                                         opts_list=opts_list)
 
@@ -351,7 +351,7 @@ class option_context(object):
            errmsg =  "Need to invoke as option_context(pat,val,[(pat,val),..))."
            raise AssertionError(errmsg)
 
-        ops = zip(args[::2], args[1::2])
+        ops = list(zip(args[::2], args[1::2]))
         undo = []
         for pat, val in ops:
             undo.append((pat, _get_option(pat, silent=True)))
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 07def64b2..1c2ff4130 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -496,7 +496,7 @@ class DataFrame(NDFrame):
             data = dict((k, v) for k, v in compat.iteritems(data) if k in columns)
 
             if index is None:
-                index = extract_index(data.values())
+                index = extract_index(list(data.values()))
             else:
                 index = _ensure_index(index)
 
@@ -521,9 +521,9 @@ class DataFrame(NDFrame):
                 data_names.append(k)
                 arrays.append(v)
         else:
-            keys = data.keys()
+            keys = list(data.keys())
             if not isinstance(data, OrderedDict):
-                keys = _try_sort(data.keys())
+                keys = _try_sort(list(data.keys()))
             columns = data_names = Index(keys)
             arrays = [data[k] for k in columns]
 
@@ -954,10 +954,10 @@ class DataFrame(NDFrame):
         if orient == 'index':
             if len(data) > 0:
                 # TODO speed up Series case
-                if isinstance(data.values()[0], (Series, dict)):
+                if isinstance(list(data.values())[0], (Series, dict)):
                     data = _from_nested_dict(data)
                 else:
-                    data, index = data.values(), data.keys()
+                    data, index = list(data.values()), list(data.keys())
         elif orient != 'columns':  # pragma: no cover
             raise ValueError('only recognize index or columns for orient')
 
@@ -3600,7 +3600,7 @@ class DataFrame(NDFrame):
                 to_replace = regex
                 regex = True
 
-            items = to_replace.items()
+            items = list(to_replace.items())
             keys, values = zip(*items)
 
             are_mappings = [isinstance(v, (dict, Series)) for v in values]
@@ -3615,8 +3615,8 @@ class DataFrame(NDFrame):
                 value_dict = {}
 
                 for k, v in items:
-                    to_rep_dict[k] = v.keys()
-                    value_dict[k] = v.values()
+                    to_rep_dict[k] = list(v.keys())
+                    value_dict[k] = list(v.values())
 
                 to_replace, value = to_rep_dict, value_dict
             else:
@@ -5735,7 +5735,7 @@ def extract_index(data):
                 indexes.append(v.index)
             elif isinstance(v, dict):
                 have_dicts = True
-                indexes.append(v.keys())
+                indexes.append(list(v.keys()))
             elif isinstance(v, (list, tuple, np.ndarray)):
                 have_raw_arrays = True
                 raw_lengths.append(len(v))
@@ -5895,7 +5895,7 @@ def _list_of_series_to_arrays(data, columns, coerce_float=False, dtype=None):
 
 def _list_of_dict_to_arrays(data, columns, coerce_float=False, dtype=None):
     if columns is None:
-        gen = (x.keys() for x in data)
+        gen = (list(x.keys()) for x in data)
         columns = lib.fast_unique_multiple_list_gen(gen)
 
     # assure that they are of the base dict class and not of derived
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index ccbec5e9f..d465ce8b1 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -1438,8 +1438,8 @@ class SeriesGroupBy(GroupBy):
 
     def _aggregate_multiple_funcs(self, arg):
         if isinstance(arg, dict):
-            columns = arg.keys()
-            arg = arg.items()
+            columns = list(arg.keys())
+            arg = list(arg.items())
         elif any(isinstance(x, (tuple, list)) for x in arg):
             arg = [(x, x) if not isinstance(x, (tuple, list)) else x
                    for x in arg]
@@ -1731,7 +1731,7 @@ class NDFrameGroupBy(GroupBy):
                     result[col] = colg.aggregate(agg_how)
                     keys.append(col)
 
-            if isinstance(result.values()[0], DataFrame):
+            if isinstance(list(result.values())[0], DataFrame):
                 from pandas.tools.merge import concat
                 result = concat([result[k] for k in keys], keys=keys, axis=1)
             else:
diff --git a/pandas/core/index.py b/pandas/core/index.py
index c46e61271..8b2f420d8 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -2704,7 +2704,7 @@ def _get_combined_index(indexes, intersect=False):
 
 
 def _get_distinct_indexes(indexes):
-    return dict((id(x), x) for x in indexes).values()
+    return list(dict((id(x), x) for x in indexes).values())
 
 
 def _union_indexes(indexes):
diff --git a/pandas/core/panel.py b/pandas/core/panel.py
index f2fb213f8..63c554883 100644
--- a/pandas/core/panel.py
+++ b/pandas/core/panel.py
@@ -284,7 +284,7 @@ class Panel(NDFrame):
             data = OrderedDict((k, v) for k, v
                                in compat.iteritems(data) if k in haxis)
         else:
-            ks = data.keys()
+            ks = list(data.keys())
             if not isinstance(data,OrderedDict):
                 ks = _try_sort(ks)
             haxis = Index(ks)
@@ -360,7 +360,7 @@ class Panel(NDFrame):
             raise ValueError('Orientation must be one of {items, minor}.')
 
         d = cls._homogenize_dict(cls, data, intersect=intersect, dtype=dtype)
-        ks = d['data'].keys()
+        ks = list(d['data'].keys())
         if not isinstance(d['data'],OrderedDict):
             ks = list(sorted(ks))
         d[cls._info_axis] = Index(ks)
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 3e6e4ea36..a89f5f270 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -543,9 +543,9 @@ def _stack_multi_columns(frame, level=-1, dropna=True):
 
     # tuple list excluding level for grouping columns
     if len(frame.columns.levels) > 2:
-        tuples = zip(*[lev.values.take(lab)
+        tuples = list(zip(*[lev.values.take(lab)
                        for lev, lab in zip(this.columns.levels[:-1],
-                                           this.columns.labels[:-1])])
+                                           this.columns.labels[:-1])]))
         unique_groups = [key for key, _ in itertools.groupby(tuples)]
         new_names = this.columns.names[:-1]
         new_columns = MultiIndex.from_tuples(unique_groups, names=new_names)
@@ -747,8 +747,8 @@ def lreshape(data, groups, dropna=True, label=None):
     reshaped : DataFrame
     """
     if isinstance(groups, dict):
-        keys = groups.keys()
-        values = groups.values()
+        keys = list(groups.keys())
+        values = list(groups.values())
     else:
         keys, values = zip(*groups)
 
diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index e717f5a2b..dbf50aef5 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -1,6 +1,7 @@
 import numpy as np
 
 from six.moves import zip
+import six
 from pandas.core.common import isnull
 from pandas.core.series import Series
 import re
@@ -282,16 +283,18 @@ def str_repeat(arr, repeats):
     if np.isscalar(repeats):
         def rep(x):
             try:
-                return str.__mul__(x, repeats)
+                return six.binary_type.__mul__(x, repeats)
             except TypeError:
                 return six.text_type.__mul__(x, repeats)
+
         return _na_map(rep, arr)
     else:
         def rep(x, r):
             try:
-                return str.__mul__(x, r)
+                return six.binary_type.__mul__(x, r)
             except TypeError:
                 return six.text_type.__mul__(x, r)
+
         repeats = np.asarray(repeats, dtype=object)
         result = lib.vec_binop(arr, repeats, rep)
         return result
diff --git a/pandas/io/common.py b/pandas/io/common.py
index 786153183..93f4f0b5d 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -10,13 +10,15 @@ from pandas.util import py3compat
 
 if py3compat.PY3:
     from urllib.request import urlopen
+    _urlopen = urlopen
     from urllib.parse import urlparse as parse_url
     import urllib.parse as compat_parse
-    from urllib.parse import uses_relative, uses_netloc, uses_params
+    from urllib.parse import uses_relative, uses_netloc, uses_params, urlencode
     from urllib.error import URLError
     from http.client import HTTPException
 else:
     from urllib2 import urlopen as _urlopen
+    from urllib import urlencode
     from urlparse import urlparse as parse_url
     from urlparse import uses_relative, uses_netloc, uses_params
     from urllib2 import URLError
@@ -24,7 +26,7 @@ else:
     from contextlib import contextmanager, closing
     from functools import wraps
 
-    @wraps(_urlopen)
+    # @wraps(_urlopen)
     @contextmanager
     def urlopen(*args, **kwargs):
         with closing(_urlopen(*args, **kwargs)) as f:
@@ -80,8 +82,7 @@ def get_filepath_or_buffer(filepath_or_buffer, encoding=None):
     """
 
     if _is_url(filepath_or_buffer):
-        from urllib2 import urlopen
-        filepath_or_buffer = urlopen(filepath_or_buffer)
+        req = _urlopen(filepath_or_buffer)
         if py3compat.PY3:  # pragma: no cover
             if encoding:
                 errors = 'strict'
@@ -101,7 +102,7 @@ def get_filepath_or_buffer(filepath_or_buffer, encoding=None):
             raise ImportError("boto is required to handle s3 files")
         # Assuming AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
         # are environment variables
-        parsed_url = urlparse.urlparse(filepath_or_buffer)
+        parsed_url = parse_url(filepath_or_buffer)
         conn = boto.connect_s3()
         b = conn.get_bucket(parsed_url.netloc)
         k = boto.s3.key.Key(b)
diff --git a/pandas/io/data.py b/pandas/io/data.py
index febf5c049..90e046998 100644
--- a/pandas/io/data.py
+++ b/pandas/io/data.py
@@ -6,7 +6,6 @@ Module contains tools for collecting data from various remote sources
 import warnings
 import tempfile
 import datetime as dt
-import urllib
 import time
 
 from collections import defaultdict
@@ -17,7 +16,7 @@ from pandas.util.py3compat import StringIO, bytes_to_str, range
 from pandas import Panel, DataFrame, Series, read_csv, concat
 from pandas.core.common import PandasError
 from pandas.io.parsers import TextParser
-from pandas.io.common import urlopen, ZipFile
+from pandas.io.common import urlopen, ZipFile, urlencode
 from pandas.util.testing import _network_error_classes
 import six
 from six.moves import map, zip
@@ -115,7 +114,7 @@ def get_quote_yahoo(symbols):
 
     # for codes see: http://www.gummy-stuff.org/Yahoo-data.htm
     request = ''.join(six.itervalues(_yahoo_codes))  # code request string
-    header = _yahoo_codes.keys()
+    header = list(_yahoo_codes.keys())
 
     data = defaultdict(list)
 
@@ -202,7 +201,7 @@ def _get_hist_google(sym, start, end, retry_count, pause):
     google_URL = 'http://www.google.com/finance/historical?'
 
     # www.google.com/finance/historical?q=GOOG&startdate=Jun+9%2C+2011&enddate=Jun+8%2C+2013&output=csv
-    url = google_URL + urllib.urlencode({"q": sym,
+    url = google_URL + urlencode({"q": sym,
                                          "startdate": start.strftime('%b %d, '
                                                                      '%Y'),
                                          "enddate": end.strftime('%b %d, %Y'),
diff --git a/pandas/io/excel.py b/pandas/io/excel.py
index 65d0b6f01..132b1549e 100644
--- a/pandas/io/excel.py
+++ b/pandas/io/excel.py
@@ -5,7 +5,6 @@ Module parse to/from Excel
 #----------------------------------------------------------------------
 # ExcelFile class
 
-from pandas.util.py3compat import range
 import datetime
 import numpy as np
 
@@ -13,6 +12,7 @@ from pandas.io.parsers import TextParser
 from pandas.tseries.period import Period
 from pandas import json
 from six.moves import map, zip, reduce
+from pandas.util.py3compat import range
 import six
 
 def read_excel(path_or_buf, sheetname, kind=None, **kwds):
diff --git a/pandas/io/ga.py b/pandas/io/ga.py
index d71de9da4..b0db040b0 100644
--- a/pandas/io/ga.py
+++ b/pandas/io/ga.py
@@ -385,8 +385,8 @@ def _maybe_add_arg(query, field, data, prefix='ga'):
 def _get_match(obj_store, name, id, **kwargs):
     key, val = None, None
     if len(kwargs) > 0:
-        key = kwargs.keys()[0]
-        val = kwargs.values()[0]
+        key = list(kwargs.keys())[0]
+        val = list(kwargs.values())[0]
 
     if name is None and id is None and key is None:
         return obj_store.get('items')[0]
diff --git a/pandas/io/html.py b/pandas/io/html.py
index dd58e8068..9805c194d 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -708,7 +708,7 @@ def _parser_dispatch(flavor):
     ImportError
         * If you do not have the requested `flavor`
     """
-    valid_parsers = _valid_parsers.keys()
+    valid_parsers = list(_valid_parsers.keys())
     if flavor not in valid_parsers:
         raise AssertionError('"{0!r}" is not a valid flavor, valid flavors are'
                              ' {1}'.format(flavor, valid_parsers))
@@ -744,7 +744,7 @@ def _validate_parser_flavor(flavor):
         raise TypeError('{0} is not a valid "flavor"'.format(flavor))
 
     flavor = list(flavor)
-    valid_flavors = _valid_parsers.keys()
+    valid_flavors = list(_valid_parsers.keys())
 
     if not set(flavor) & set(valid_flavors):
         raise ValueError('{0} is not a valid set of flavors, valid flavors are'
diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py
index a81c77d87..b1dee20e6 100644
--- a/pandas/io/pickle.py
+++ b/pandas/io/pickle.py
@@ -1,8 +1,4 @@
-try:
-    import cPickle as pkl
-except ImportError:
-    import pickle as pkl
-
+from six.moves import cPickle as pkl
 
 def to_pickle(obj, path):
     """
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index e18cd2d8c..f03928366 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -324,7 +324,7 @@ class HDFStore(StringMixin):
     def __unicode__(self):
         output = '%s\nFile path: %s\n' % (type(self), pprint_thing(self._path))
 
-        if len(self.keys()):
+        if len(list(self.keys())):
             keys   = []
             values = []
 
@@ -372,6 +372,8 @@ class HDFStore(StringMixin):
         self._mode = mode
         if warn and mode == 'w':  # pragma: no cover
             while True:
+                if py3compat.PY3:
+                    raw_input = input
                 response = raw_input("Re-opening as mode='w' will delete the "
                                      "current file. Continue (y/n)?")
                 if response == 'y':
@@ -787,7 +789,7 @@ class HDFStore(StringMixin):
         """
         new_store = HDFStore(file, mode = mode, complib = complib, complevel = complevel, fletcher32 = fletcher32)
         if keys is None:
-            keys = self.keys()
+            keys = list(self.keys())
         if not isinstance(keys, (tuple,list)):
             keys = [ keys ]
         for k in keys:
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index 033f0cf0e..f76a6f154 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -10,8 +10,6 @@ You can find more information on http://presbrey.mit.edu/PyDTA and
 http://statsmodels.sourceforge.net/devel/
 """
 # TODO: Fix this module so it can use cross-compatible zip, map, and range
-from StringIO import StringIO
-from pandas.util import compat
 import numpy as np
 
 import sys
@@ -23,7 +21,7 @@ from pandas.core.categorical import Categorical
 import datetime
 from pandas.util import py3compat
 from pandas.util import compat
-from pandas.util.py3compat import long
+from pandas.util.py3compat import StringIO, long
 from pandas import isnull
 from pandas.io.parsers import _parser_params, Appender
 from pandas.io.common import get_filepath_or_buffer
@@ -541,13 +539,13 @@ class StataReader(StataParser):
                     data[col] = Series(data[col], data[col].index, self.dtyplist[i])
 
         if convert_dates:
-            cols = np.where(map(lambda x: x in _date_formats, self.fmtlist))[0]
+            cols = np.where(list(map(lambda x: x in _date_formats, self.fmtlist)))[0]
             for i in cols:
                 col = data.columns[i]
                 data[col] = data[col].apply(_stata_elapsed_date_to_datetime, args=(self.fmtlist[i],))
 
         if convert_categoricals:
-            cols = np.where(map(lambda x: x in six.iterkeys(self.value_label_dict), self.lbllist))[0]
+            cols = np.where(list(map(lambda x: x in six.iterkeys(self.value_label_dict), self.lbllist)))[0]
             for i in cols:
                 col = data.columns[i]
                 labeled_data = np.copy(data[col])
diff --git a/pandas/io/tests/generate_legacy_pickles.py b/pandas/io/tests/generate_legacy_pickles.py
index 49a7b90b2..7659b22e4 100644
--- a/pandas/io/tests/generate_legacy_pickles.py
+++ b/pandas/io/tests/generate_legacy_pickles.py
@@ -1,6 +1,7 @@
 """ self-contained to write legacy pickle files """
 from __future__ import print_function
-from six.moves import zip
+
+from six.moves import zip, cPickle as pickle
 
 def _create_sp_series():
 
@@ -88,7 +89,6 @@ def write_legacy_pickles():
     import pandas
     import pandas.util.testing as tm
     import platform as pl
-    import cPickle as pickle
 
     print("This script generates a pickle file for the current arch, system, and python version")
 
diff --git a/pandas/io/tests/test_clipboard.py b/pandas/io/tests/test_clipboard.py
index 9eadd16c2..12c696f70 100644
--- a/pandas/io/tests/test_clipboard.py
+++ b/pandas/io/tests/test_clipboard.py
@@ -33,7 +33,7 @@ class TestClipboard(unittest.TestCase):
         cls.data['mixed'] = DataFrame({'a': np.arange(1.0, 6.0) + 0.01,
                                        'b': np.arange(1, 6),
                                        'c': list('abcde')})
-        cls.data_types = cls.data.keys()
+        cls.data_types = list(cls.data.keys())
 
     @classmethod
     def tearDownClass(cls):
diff --git a/pandas/io/tests/test_json/test_pandas.py b/pandas/io/tests/test_json/test_pandas.py
index 4bb73dc76..36bf0306d 100644
--- a/pandas/io/tests/test_json/test_pandas.py
+++ b/pandas/io/tests/test_json/test_pandas.py
@@ -6,7 +6,7 @@ from pandas.util.py3compat import StringIO
 from pandas.util.py3compat import range
 from pandas.util import compat
 from pandas.io.common import URLError
-import cPickle as pickle
+from six.moves import cPickle as pickle
 import operator
 import os
 import unittest
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index ba5886ba1..bbfe7e931 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -2700,7 +2700,7 @@ class TestHDFStore(unittest.TestCase):
 
                 # check keys
                 if keys is None:
-                    keys = store.keys()
+                    keys = list(store.keys())
                 self.assert_(set(keys) == set(tstore.keys()))
 
                 # check indicies & nrows
diff --git a/pandas/io/tests/test_sql.py b/pandas/io/tests/test_sql.py
index 614b401ce..28703975f 100644
--- a/pandas/io/tests/test_sql.py
+++ b/pandas/io/tests/test_sql.py
@@ -1,7 +1,4 @@
 from __future__ import print_function
-from __future__ import with_statement
-from pandas.util.py3compat import StringIO
-from pandas.util.py3compat import range
 import unittest
 import sqlite3
 import sys
@@ -14,6 +11,8 @@ import numpy as np
 
 from pandas.core.datetools import format as date_format
 from pandas.core.api import DataFrame, isnull
+from pandas.util.py3compat import StringIO, range
+import six
 
 import pandas.io.sql as sql
 import pandas.util.testing as tm
@@ -24,7 +23,8 @@ _formatters = {
     datetime: lambda dt: "'%s'" % date_format(dt),
     str: lambda x: "'%s'" % x,
     np.str_: lambda x: "'%s'" % x,
-    unicode: lambda x: "'%s'" % x,
+    six.text_type: lambda x: "'%s'" % x,
+    six.binary_type: lambda x: "'%s'" % x,
     float: lambda x: "%.8f" % x,
     int: lambda x: "%s" % x,
     type(None): lambda x: "NULL",
diff --git a/pandas/io/wb.py b/pandas/io/wb.py
index 59e3c211a..65a666228 100644
--- a/pandas/io/wb.py
+++ b/pandas/io/wb.py
@@ -1,10 +1,11 @@
 from __future__ import print_function
+
+from six.moves import map, reduce
 from pandas.util.py3compat import range
 from pandas.io.common import urlopen
 from pandas.io import json
 import pandas
 import numpy as np
-from six.moves import map, reduce
 
 
 def download(country=['MX', 'CA', 'US'], indicator=['GDPPCKD', 'GDPPCKN'],
@@ -92,10 +93,10 @@ def _get_data(indicator="NY.GNS.ICTR.GN.ZS", country='US',
         data = response.read()
     # Parse JSON file
     data = json.loads(data)[1]
-    country = list(map(lambda x: x['country']['value'], data))
-    iso2c = list(map(lambda x: x['country']['id'], data))
-    year = list(map(lambda x: x['date'], data))
-    value = list(map(lambda x: x['value'], data))
+    country = [x['country']['value'] for x in data]
+    iso2c = [x['country']['id'] for x in data]
+    year = [x['date'] for x in data]
+    value = [x['value'] for x in data]
     # Prepare output
     out = pandas.DataFrame([country, iso2c, year, value]).T
     return out
@@ -109,10 +110,10 @@ def get_countries():
         data = response.read()
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
-    data.adminregion = list(map(lambda x: x['value'], data.adminregion))
-    data.incomeLevel = list(map(lambda x: x['value'], data.incomeLevel))
-    data.lendingType = list(map(lambda x: x['value'], data.lendingType))
-    data.region = list(map(lambda x: x['value'], data.region))
+    data.adminregion = [x['value'] for x in data.adminregion]
+    data.incomeLevel = [x['value'] for x in data.incomeLevel]
+    data.lendingType = [x['value'] for x in data.lendingType]
+    data.region = [x['value'] for x in data.region]
     data = data.rename(columns={'id': 'iso3c', 'iso2Code': 'iso2c'})
     return data
 
@@ -126,7 +127,7 @@ def get_indicators():
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
     # Clean fields
-    data.source = list(map(lambda x: x['value'], data.source))
+    data.source = [x['value'] for x in data.source]
     fun = lambda x: x.encode('ascii', 'ignore')
     data.sourceOrganization = data.sourceOrganization.apply(fun)
     # Clean topic field
@@ -136,7 +137,7 @@ def get_indicators():
             return x['value']
         except:
             return ''
-    fun = lambda x: list(map(lambda y: get_value(y), x))
+    fun = lambda x: [get_value(y) for y in x]
     data.topics = data.topics.apply(fun)
     data.topics = data.topics.apply(lambda x: ' ; '.join(x))
     # Clean outpu
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index 09e8bdb57..26c0a151a 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -153,10 +153,10 @@ class SparseDataFrame(DataFrame):
             columns = _ensure_index(columns)
             data = dict((k, v) for k, v in compat.iteritems(data) if k in columns)
         else:
-            columns = Index(_try_sort(data.keys()))
+            columns = Index(_try_sort(list(data.keys())))
 
         if index is None:
-            index = extract_index(data.values())
+            index = extract_index(list(data.values()))
 
         sp_maker = lambda x: SparseSeries(x, index=index,
                                           kind=self.default_kind,
diff --git a/pandas/sparse/tests/test_sparse.py b/pandas/sparse/tests/test_sparse.py
index 5be3703dd..ff9d57bed 100644
--- a/pandas/sparse/tests/test_sparse.py
+++ b/pandas/sparse/tests/test_sparse.py
@@ -1,9 +1,6 @@
 # pylint: disable-msg=E1101,W0612
 
 from unittest import TestCase
-from pandas.util.py3compat import range
-from pandas.util import compat
-import cPickle as pickle
 import operator
 from datetime import datetime
 
@@ -25,6 +22,9 @@ from pandas.tseries.index import DatetimeIndex
 import pandas.core.datetools as datetools
 from pandas.core.common import isnull
 import pandas.util.testing as tm
+from pandas.util.py3compat import range
+from pandas.util import compat
+from six.moves import cPickle as pickle
 
 import pandas.sparse.frame as spf
 
diff --git a/pandas/stats/var.py b/pandas/stats/var.py
index 0aa7a50d8..5f4a4ec13 100644
--- a/pandas/stats/var.py
+++ b/pandas/stats/var.py
@@ -255,7 +255,7 @@ BIC:                            %(bic).3f
 
     @cache_readonly
     def _beta_raw(self):
-        return np.array([self.beta[col].values() for col in self._columns]).T
+        return np.array([list(self.beta[col].values()) for col in self._columns]).T
 
     def _trans_B(self, h):
         """
@@ -289,7 +289,7 @@ BIC:                            %(bic).3f
     @cache_readonly
     def _x(self):
         values = np.array([
-            self._lagged_data[i][col].values()
+            list(self._lagged_data[i][col].values())
             for i in range(1, 1 + self._p)
             for col in self._columns
         ]).T
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index cd894ce2e..aadfea05c 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -2729,7 +2729,7 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         a = {'hi': [32, 3, 3],
              'there': [3, 5, 3]}
         rs = DataFrame.from_dict(a, orient='index')
-        xp = DataFrame.from_dict(a).T.reindex(a.keys())
+        xp = DataFrame.from_dict(a).T.reindex(list(a.keys()))
         assert_frame_equal(rs, xp)
 
     def test_constructor_Series_named(self):
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index a0ec25ab1..9fe98e27c 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -2376,7 +2376,7 @@ class TestGroupBy(unittest.TestCase):
 
         # it works!
         groups = grouped.groups
-        tm.assert_isinstance(groups.keys()[0], datetime)
+        tm.assert_isinstance(list(groups.keys())[0], datetime)
 
     def test_groupby_reindex_inside_function(self):
         from pandas.tseries.api import DatetimeIndex
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index 5d3171365..82f1fa824 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -1,6 +1,4 @@
 # pylint: disable-msg=W0612,E1101,W0141
-from pandas.util.py3compat import StringIO
-from pandas.util.py3compat import range
 import nose
 import unittest
 
@@ -15,12 +13,14 @@ from pandas.util.testing import (assert_almost_equal,
                                  assert_frame_equal)
 import pandas.core.common as com
 import pandas.util.testing as tm
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
 from pandas.util.compat import product as cart_product
 import pandas as pd
 
 import pandas.index as _index
 import six
-from six.moves import zip
+from six.moves import zip, cPickle
 
 
 class TestMultiLevel(unittest.TestCase):
@@ -139,7 +139,6 @@ class TestMultiLevel(unittest.TestCase):
         _check_op('div')
 
     def test_pickle(self):
-        import cPickle
 
         def _test_roundtrip(frame):
             pickled = cPickle.dumps(frame)
diff --git a/pandas/tests/test_panel.py b/pandas/tests/test_panel.py
index 69fae70bd..1d9707858 100644
--- a/pandas/tests/test_panel.py
+++ b/pandas/tests/test_panel.py
@@ -16,6 +16,7 @@ from pandas.core.panel import Panel
 from pandas.core.series import remove_na
 import pandas.core.common as com
 from pandas.util import py3compat
+from six.moves import cPickle
 
 from pandas.util.testing import (assert_panel_equal,
                                  assert_frame_equal,
@@ -40,7 +41,6 @@ class PanelTests(object):
     panel = None
 
     def test_pickle(self):
-        import cPickle
         pickled = cPickle.dumps(self.panel)
         unpickled = cPickle.loads(pickled)
         assert_frame_equal(unpickled['ItemA'], self.panel['ItemA'])
@@ -268,7 +268,7 @@ class SafeForSparse(object):
         assert_frame_equal(result['ItemA'], op(panel['ItemA'], 1))
 
     def test_keys(self):
-        tm.equalContents(self.panel.keys(), self.panel.items)
+        tm.equalContents(list(self.panel.keys()), self.panel.items)
 
     def test_iteritems(self):
         # Test panel.iteritems(), aka panel.iteritems()
diff --git a/pandas/tests/test_panel4d.py b/pandas/tests/test_panel4d.py
index 31f5bc64a..a1566bf30 100644
--- a/pandas/tests/test_panel4d.py
+++ b/pandas/tests/test_panel4d.py
@@ -217,7 +217,7 @@ class SafeForSparse(object):
         assert_panel_equal(result['l1'], op(panel4d['l1'], 1))
 
     def test_keys(self):
-        tm.equalContents(self.panel4d.keys(), self.panel4d.labels)
+        tm.equalContents(list(self.panel4d.keys()), self.panel4d.labels)
 
     def test_iteritems(self):
         """Test panel4d.iteritems()"""
diff --git a/pandas/tests/test_reshape.py b/pandas/tests/test_reshape.py
index 1228e1605..3b34934f1 100644
--- a/pandas/tests/test_reshape.py
+++ b/pandas/tests/test_reshape.py
@@ -1,9 +1,6 @@
 # pylint: disable-msg=W0612,E1101
 from copy import deepcopy
 from datetime import datetime, timedelta
-from pandas.util.py3compat import StringIO
-from pandas.util.py3compat import range
-import cPickle as pickle
 import operator
 import os
 import unittest
@@ -18,6 +15,9 @@ import numpy as np
 
 from pandas.core.reshape import melt, convert_dummies, lreshape
 import pandas.util.testing as tm
+from pandas.util.py3compat import StringIO
+from pandas.util.py3compat import range
+from six.moves import cPickle
 
 _multiprocess_can_split_ = True
 
@@ -57,9 +57,9 @@ class TestMelt(unittest.TestCase):
                                'id2': self.df['id2'].tolist() * 2,
                                'variable': ['A']*10 + ['B']*10,
                                'value': self.df['A'].tolist() + self.df['B'].tolist()},
-                              columns=['id1', 'id2', 'variable', 'value'])                  
+                              columns=['id1', 'id2', 'variable', 'value'])
         tm.assert_frame_equal(result4, expected4)
-      
+
     def test_custom_var_name(self):
         result5 = melt(self.df, var_name=self.var_name)
         self.assertEqual(result5.columns.tolist(), ['var', 'value'])
@@ -80,7 +80,7 @@ class TestMelt(unittest.TestCase):
                                'id2': self.df['id2'].tolist() * 2,
                                self.var_name: ['A']*10 + ['B']*10,
                                'value': self.df['A'].tolist() + self.df['B'].tolist()},
-                              columns=['id1', 'id2', self.var_name, 'value'])                  
+                              columns=['id1', 'id2', self.var_name, 'value'])
         tm.assert_frame_equal(result9, expected9)
 
     def test_custom_value_name(self):
@@ -98,12 +98,12 @@ class TestMelt(unittest.TestCase):
         self.assertEqual(result13.columns.tolist(), ['id1', 'id2', 'variable', 'val'])
 
         result14 = melt(self.df, id_vars=['id1', 'id2'],
-                        value_vars=['A', 'B'], value_name=self.value_name)         
+                        value_vars=['A', 'B'], value_name=self.value_name)
         expected14 = DataFrame({'id1': self.df['id1'].tolist() * 2,
                                 'id2': self.df['id2'].tolist() * 2,
                                 'variable': ['A']*10 + ['B']*10,
                                 self.value_name: self.df['A'].tolist() + self.df['B'].tolist()},
-                               columns=['id1', 'id2', 'variable', self.value_name])                  
+                               columns=['id1', 'id2', 'variable', self.value_name])
         tm.assert_frame_equal(result14, expected14)
 
     def test_custom_var_and_value_name(self):
@@ -123,12 +123,12 @@ class TestMelt(unittest.TestCase):
         self.assertEqual(result18.columns.tolist(), ['id1', 'id2', 'var', 'val'])
 
         result19 = melt(self.df, id_vars=['id1', 'id2'],
-                        value_vars=['A', 'B'], var_name=self.var_name, value_name=self.value_name)                        
+                        value_vars=['A', 'B'], var_name=self.var_name, value_name=self.value_name)
         expected19 = DataFrame({'id1': self.df['id1'].tolist() * 2,
                                 'id2': self.df['id2'].tolist() * 2,
                                 var_name: ['A']*10 + ['B']*10,
                                 value_name: self.df['A'].tolist() + self.df['B'].tolist()},
-                               columns=['id1', 'id2', self.var_name, self.value_name])                  
+                               columns=['id1', 'id2', self.var_name, self.value_name])
         tm.assert_frame_equal(result19, expected19)
 
     def test_custom_var_and_value_name(self):
diff --git a/pandas/tests/test_rplot.py b/pandas/tests/test_rplot.py
index 0dfae47dd..95ef66eb8 100644
--- a/pandas/tests/test_rplot.py
+++ b/pandas/tests/test_rplot.py
@@ -69,7 +69,7 @@ class TestUtilityFunctions(unittest.TestCase):
         dict2 = {1 : 1, 2 : 2, 4 : 4}
         union = rplot.dictionary_union(dict1, dict2)
         self.assertEqual(len(union), 4)
-        keys = union.keys()
+        keys = list(union.keys())
         self.assertTrue(1 in keys)
         self.assertTrue(2 in keys)
         self.assertTrue(3 in keys)
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index 84fbc4397..3b7f693c8 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -525,7 +525,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         import pandas, random
         data = OrderedDict([('col%s' % i, random.random()) for i in range(12)])
         s = pandas.Series(data)
-        self.assertTrue(all(s.values == data.values()))
+        self.assertTrue(all(s.values == list(data.values())))
 
     def test_orderedDict_subclass_ctor(self):
         # GH3283
@@ -535,7 +535,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
             pass
         data = A([('col%s' % i, random.random()) for i in range(12)])
         s = pandas.Series(data)
-        self.assertTrue(all(s.values == data.values()))
+        self.assertTrue(all(s.values == list(data.values())))
 
     def test_constructor_list_of_tuples(self):
         data = [(1, 1), (2, 2), (2, 3)]
diff --git a/pandas/tools/plotting.py b/pandas/tools/plotting.py
index 483d989e9..e356e9e9f 100644
--- a/pandas/tools/plotting.py
+++ b/pandas/tools/plotting.py
@@ -987,7 +987,7 @@ class MPLPlot(object):
 
         if self._need_to_set_index:
             labels = [com.pprint_thing(key) for key in self.data.index]
-            labels = dict(zip(list(range(len(self.data.index))), labels))
+            labels = dict(zip(range(len(self.data.index)), labels))
 
             for ax_ in self.axes:
                 # ax_.set_xticks(self.xticks)
diff --git a/pandas/tools/rplot.py b/pandas/tools/rplot.py
index 747d7bfb0..f2d2b1fd6 100644
--- a/pandas/tools/rplot.py
+++ b/pandas/tools/rplot.py
@@ -602,7 +602,7 @@ class TrellisGrid(Layer):
                 grouped = data.groupby(self.by[0])
             else:
                 grouped = data.groupby(self.by)
-            groups = grouped.groups.keys()
+            groups = list(grouped.groups.keys())
             if self.by[0] == '.' or self.by[1] == '.':
                 shingle1 = set([g for g in groups])
             else:
@@ -646,8 +646,8 @@ def dictionary_union(dict1, dict2):
     A union of the dictionaries. It assumes that values
     with the same keys are identical.
     """
-    keys1 = dict1.keys()
-    keys2 = dict2.keys()
+    keys1 = list(dict1.keys())
+    keys2 = list(dict2.keys())
     result = {}
     for key1 in keys1:
         result[key1] = dict1[key1]
@@ -773,13 +773,13 @@ def adjust_subplots(fig, axes, trellis, layers):
         legend = dictionary_union(legend, layer.legend)
     patches = []
     labels = []
-    if len(legend.keys()) == 0:
+    if len(list(legend.keys())) == 0:
         key_function = lambda tup: tup
-    elif len(legend.keys()[0]) == 2:
+    elif len(list(legend.keys())[0]) == 2:
         key_function = lambda tup: (tup[1])
     else:
         key_function = lambda tup: (tup[1], tup[3])
-    for key in sorted(legend.keys(), key=key_function):
+    for key in sorted(list(legend.keys()), key=key_function):
         value = legend[key]
         patches.append(value)
         if len(key) == 2:
@@ -846,13 +846,13 @@ class RPlot:
                 legend = dictionary_union(legend, layer.legend)
             patches = []
             labels = []
-            if len(legend.keys()) == 0:
+            if len(list(legend.keys())) == 0:
                 key_function = lambda tup: tup
-            elif len(legend.keys()[0]) == 2:
+            elif len(list(legend.keys())[0]) == 2:
                 key_function = lambda tup: (tup[1])
             else:
                 key_function = lambda tup: (tup[1], tup[3])
-            for key in sorted(legend.keys(), key=key_function):
+            for key in sorted(list(legend.keys()), key=key_function):
                 value = legend[key]
                 patches.append(value)
                 if len(key) == 2:
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 3bcf93464..303a11929 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -105,7 +105,7 @@ class DateOffset(object):
         attrs = [(k, v) for k, v in compat.iteritems(vars(self))
                  if k not in ['kwds', '_offset', 'name', 'normalize',
                  'busdaycalendar']]
-        attrs.extend(self.kwds.items())
+        attrs.extend(list(self.kwds.items()))
         attrs = sorted(set(attrs))
 
         params = tuple([str(self.__class__)] + attrs)
diff --git a/pandas/tseries/tests/test_period.py b/pandas/tseries/tests/test_period.py
index 053ff8af2..2057a418f 100644
--- a/pandas/tseries/tests/test_period.py
+++ b/pandas/tseries/tests/test_period.py
@@ -2000,8 +2000,10 @@ class TestPeriodIndex(TestCase):
         raw = [2005, 2007, 2009]
         index = PeriodIndex(raw, freq='A')
         types = str,
-        if not py3compat.PY3:
-            types += unicode,
+
+        if py3compat.PY3:
+            # unicode
+            types += six.text_type,
 
         for t in types:
             expected = np.array(list(map(t, raw)), dtype=object)
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index 0336f659f..68ea73a66 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -7,9 +7,6 @@ import unittest
 import nose
 
 import numpy as np
-from pandas.util.py3compat import range, long, StringIO
-from pandas.util.compat import product
-from six.moves import map, zip
 randn = np.random.randn
 
 from pandas import (Index, Series, TimeSeries, DataFrame,
@@ -31,7 +28,9 @@ import pandas.tslib as tslib
 
 import pandas.index as _index
 
-import cPickle as pickle
+from pandas.util.py3compat import range, long, StringIO
+from pandas.util.compat import product
+from six.moves import map, zip, cPickle as pickle
 from pandas import read_pickle
 import pandas.core.datetools as dt
 from numpy.random import rand
@@ -1790,7 +1789,7 @@ class TestDatetimeIndex(unittest.TestCase):
     def test_misc_coverage(self):
         rng = date_range('1/1/2000', periods=5)
         result = rng.groupby(rng.day)
-        tm.assert_isinstance(result.values()[0][0], Timestamp)
+        tm.assert_isinstance(list(result.values())[0][0], Timestamp)
 
         idx = DatetimeIndex(['2000-01-03', '2000-01-01', '2000-01-02'])
         self.assert_(idx.equals(list(idx)))
diff --git a/pandas/tseries/tests/test_timezones.py b/pandas/tseries/tests/test_timezones.py
index 1f3e80dc0..1c7607c63 100644
--- a/pandas/tseries/tests/test_timezones.py
+++ b/pandas/tseries/tests/test_timezones.py
@@ -5,8 +5,6 @@ import os
 import unittest
 import nose
 
-from pandas.util.py3compat import range
-from six.moves import zip
 import numpy as np
 import pytz
 
@@ -26,11 +24,12 @@ from pandas.util.testing import assert_series_equal, assert_almost_equal, assert
 import pandas.util.testing as tm
 
 import pandas.lib as lib
-import cPickle as pickle
 import pandas.core.datetools as dt
 from numpy.random import rand
 from pandas.util.testing import assert_frame_equal
 import pandas.util.py3compat as py3compat
+from pandas.util.py3compat import range
+from six.moves import zip, cPickle as pickle
 from pandas.core.datetools import BDay
 import pandas.core.common as com
 
diff --git a/pandas/util/compat.py b/pandas/util/compat.py
index 8b9148456..413cc0a9d 100644
--- a/pandas/util/compat.py
+++ b/pandas/util/compat.py
@@ -223,7 +223,7 @@ class _OrderedDict(dict):
         try:
             if not self:
                 return '%s()' % (self.__class__.__name__,)
-            return '%s(%r)' % (self.__class__.__name__, self.items())
+            return '%s(%r)' % (self.__class__.__name__, list(self.items()))
         finally:
             del _repr_running[call_key]
 
@@ -258,7 +258,7 @@ class _OrderedDict(dict):
 
         '''
         if isinstance(other, OrderedDict):
-            return len(self) == len(other) and self.items() == other.items()
+            return len(self) == len(other) and list(self.items()) == list(other.items())
         return dict.__eq__(self, other)
 
     def __ne__(self, other):
@@ -499,4 +499,4 @@ class OrderedDefaultdict(OrderedDict):
 
     def __reduce__(self):  # optional, for pickle support
         args = self.default_factory if self.default_factory else tuple()
-        return type(self), args, None, None, self.items()
+        return type(self), args, None, None, list(self.items())
diff --git a/pandas/util/py3compat.py b/pandas/util/py3compat.py
index f4d262e45..969ba94fd 100644
--- a/pandas/util/py3compat.py
+++ b/pandas/util/py3compat.py
@@ -39,12 +39,12 @@ try:
     from cStringIO import StringIO as cStringIO
     # writeable and handles unicode
     from StringIO import StringIO
-except:
+except ImportError:
     # no more StringIO
     from io import StringIO
     cStringIO = StringIO
 
 try:
     from io import BytesIO
-except:
+except ImportError:
     from cStringIO import StringIO as BytesIO
diff --git a/scripts/json_manip.py b/scripts/json_manip.py
index ccdf02b3f..4733df68c 100644
--- a/scripts/json_manip.py
+++ b/scripts/json_manip.py
@@ -321,7 +321,7 @@ def Q(filter_,thing):
     if isinstance(filter_, type([])):
         return flatten(*[_Q(x,thing) for x in filter_])
     elif isinstance(filter_, type({})):
-        d = dict.fromkeys(filter_.keys())
+        d = dict.fromkeys(list(filter_.keys()))
         #print d
         for k in d:
             #print flatten(Q(k,thing))
diff --git a/scripts/pypistats.py b/scripts/pypistats.py
index e64be6355..41343f6d3 100644
--- a/scripts/pypistats.py
+++ b/scripts/pypistats.py
@@ -93,7 +93,7 @@ if __name__ == '__main__':
     result = pd.DataFrame({'downloads': totals,
                            'release_date': first_upload})
     result = result.sort('release_date')
-    result = result.drop(to_omit + rollup.keys())
+    result = result.drop(to_omit + list(rollup.keys()))
     result.index.name = 'release'
 
     by_date = result.reset_index().set_index('release_date').downloads
diff --git a/vb_suite/make.py b/vb_suite/make.py
index 74a0818fb..1bea9ae1a 100755
--- a/vb_suite/make.py
+++ b/vb_suite/make.py
@@ -159,7 +159,7 @@ if len(sys.argv) > 1:
         func = funcd.get(arg)
         if func is None:
             raise SystemExit('Do not know how to handle %s; valid args are %s' % (
-                arg, funcd.keys()))
+                arg, list(funcd.keys())))
         func()
 else:
     small_docs = False
diff --git a/vb_suite/perf_HEAD.py b/vb_suite/perf_HEAD.py
index 4ecaa3b9c..b9f859942 100755
--- a/vb_suite/perf_HEAD.py
+++ b/vb_suite/perf_HEAD.py
@@ -71,7 +71,7 @@ def dump_as_gist(data, desc="The Commit", njobs=None):
                 print("\n\n" + "-" * 80)
 
                 gist = json.loads(r.read())
-                file_raw_url = gist['files'].items()[0][1]['raw_url']
+                file_raw_url = list(gist['files'].items())[0][1]['raw_url']
                 print("[vbench-gist-raw_url] %s" % file_raw_url)
                 print("[vbench-html-url] %s" % gist['html_url'])
                 print("[vbench-api-url] %s" % gist['url'])
diff --git a/vb_suite/test_perf.py b/vb_suite/test_perf.py
index 7428bbb07..d7a5b9d3e 100755
--- a/vb_suite/test_perf.py
+++ b/vb_suite/test_perf.py
@@ -144,7 +144,7 @@ def get_results_df(db, rev):
 
     # Sinch vbench.db._reg_rev_results returns an unlabeled dict,
     # we have to break encapsulation a bit.
-    results.columns = db._results.c.keys()
+    results.columns = list(db._results.c.keys())
     results = results.join(bench['name'], on='checksum').set_index("checksum")
     return results
 
