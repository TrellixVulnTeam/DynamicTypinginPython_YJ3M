commit 5c78ecb34cbcc4e3a6b79206d2090c299dcb20c3
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Nov 30 16:52:48 2012 -0500

    TST: test suite passes\!

diff --git a/pandas/algos.pyx b/pandas/algos.pyx
index a17b923e5..a2125d953 100644
--- a/pandas/algos.pyx
+++ b/pandas/algos.pyx
@@ -1708,6 +1708,41 @@ def roll_generic(ndarray[float64_t, cast=True] input, int win,
 #----------------------------------------------------------------------
 # group operations
 
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def is_lexsorted(list list_of_arrays):
+    cdef:
+        int i
+        Py_ssize_t n, nlevels
+        int64_t k, cur, pre
+        ndarray arr
+
+    nlevels = len(list_of_arrays)
+    n = len(list_of_arrays[0])
+
+    cdef int64_t **vecs = <int64_t**> malloc(nlevels * sizeof(int64_t*))
+    for i from 0 <= i < nlevels:
+        # vecs[i] = <int64_t *> (<ndarray> list_of_arrays[i]).data
+
+        arr = list_of_arrays[i]
+        vecs[i] = <int64_t *> arr.data
+    # assume uniqueness??
+
+    for i from 1 <= i < n:
+        for k from 0 <= k < nlevels:
+            cur = vecs[k][i]
+            pre = vecs[k][i-1]
+            if cur == pre:
+                continue
+            elif cur > pre:
+                break
+            else:
+                return False
+    free(vecs)
+    return True
+
+
 @cython.boundscheck(False)
 def groupby_indices(ndarray values):
     cdef:
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 58528578c..87ae1ba67 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -42,7 +42,10 @@ import pandas.core.common as com
 import pandas.core.format as fmt
 import pandas.core.generic as generic
 import pandas.core.nanops as nanops
+
 import pandas.lib as lib
+import pandas.tslib as tslib
+import pandas.algos as _algos
 
 from pandas.core.config import get_option
 
@@ -380,7 +383,7 @@ class DataFrame(NDFrame):
             mask = ma.getmaskarray(data)
             datacopy = ma.copy(data)
             if issubclass(data.dtype.type, np.datetime64):
-                datacopy[mask] = lib.iNaT
+                datacopy[mask] = tslib.iNaT
             else:
                 datacopy = com._maybe_upcast(datacopy)
                 datacopy[mask] = NA
@@ -4306,7 +4309,8 @@ class DataFrame(NDFrame):
         mat = numeric_df.values
 
         if method == 'pearson':
-            correl = lib.nancorr(com._ensure_float64(mat), minp=min_periods)
+            correl = _algos.nancorr(com._ensure_float64(mat),
+                                    minp=min_periods)
         else:
             if min_periods is None:
                 min_periods = 1
@@ -4357,8 +4361,8 @@ class DataFrame(NDFrame):
             else:
                 baseCov = np.cov(mat.T)
         else:
-            baseCov = lib.nancorr(com._ensure_float64(mat), cov=True,
-                                  minp=min_periods)
+            baseCov = _algos.nancorr(com._ensure_float64(mat), cov=True,
+                                     minp=min_periods)
 
         return self._constructor(baseCov, index=cols, columns=cols)
 
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index acc7460db..735616f72 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -17,6 +17,7 @@ import pandas.core.common as com
 
 import pandas.lib as lib
 import pandas.algos as _algos
+import pandas.hashtable as _hash
 
 _agg_doc = """Aggregate using input function or dict of {column -> function}
 
diff --git a/pandas/core/index.py b/pandas/core/index.py
index f214187b4..ba7817046 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -203,7 +203,7 @@ class Index(np.ndarray):
         if self.inferred_type == 'string':
             from dateutil.parser import parse
             parser = lambda x: parse(x, dayfirst=dayfirst)
-            parsed = tslib.try_parse_dates(self.values, parser=parser)
+            parsed = lib.try_parse_dates(self.values, parser=parser)
             return DatetimeIndex(parsed)
         else:
             return DatetimeIndex(self.values)
@@ -727,7 +727,7 @@ class Index(np.ndarray):
                 raise
 
             try:
-                return lib.get_value_box(series, key)
+                return tslib.get_value_box(series, key)
             except IndexError:
                 raise
             except TypeError:
@@ -1526,7 +1526,7 @@ class MultiIndex(Index):
                 pass
 
             try:
-                return lib.get_value_at(series, key)
+                return _index.get_value_at(series, key)
             except IndexError:
                 raise
             except TypeError:
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index fef873a15..79a123be1 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -7,13 +7,14 @@ import numpy as np
 from pandas.core.index import Index, _ensure_index, _handle_legacy_indexes
 import pandas.core.common as com
 import pandas.lib as lib
+import pandas.tslib as tslib
 
 from pandas.util import py3compat
 
 class Block(object):
     """
-    Canonical n-dimensional unit of homogeneous dtype contained in a pandas data
-    structure
+    Canonical n-dimensional unit of homogeneous dtype contained in a pandas
+    data structure
 
     Index-ignorant; let the container take care of that
     """
@@ -399,7 +400,7 @@ class DatetimeBlock(Block):
 
     def __init__(self, values, items, ref_items, ndim=2):
         if values.dtype != _NS_DTYPE:
-            values = lib.cast_to_nanoseconds(values)
+            values = tslib.cast_to_nanoseconds(values)
 
         Block.__init__(self, values, items, ref_items, ndim=ndim)
 
@@ -429,14 +430,14 @@ class DatetimeBlock(Block):
         loc = self.items.get_loc(item)
 
         if value.dtype != _NS_DTYPE:
-            value = lib.cast_to_nanoseconds(value)
+            value = tslib.cast_to_nanoseconds(value)
 
         self.values[loc] = value
 
     def get_values(self, dtype):
         if dtype == object:
             flat_i8 = self.values.ravel().view(np.int64)
-            res = lib.ints_to_pydatetime(flat_i8)
+            res = tslib.ints_to_pydatetime(flat_i8)
             return res.reshape(self.values.shape)
         return self.values
 
@@ -1300,7 +1301,7 @@ def form_blocks(arrays, names, axes):
             complex_items.append((k, v))
         elif issubclass(v.dtype.type, np.datetime64):
             if v.dtype != _NS_DTYPE:
-                v = lib.cast_to_nanoseconds(v)
+                v = tslib.cast_to_nanoseconds(v)
 
             if hasattr(v, 'tz') and v.tz is not None:
                 object_items.append((k, v))
diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index 073a58f8f..9e11184cc 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -5,6 +5,8 @@ import numpy as np
 from pandas.core.common import isnull, notnull
 import pandas.core.common as com
 import pandas.lib as lib
+import pandas.algos as algos
+import pandas.hashtable as _hash
 
 try:
     import bottleneck as bn
@@ -121,7 +123,7 @@ def _nanmedian(values, axis=None, skipna=True):
         mask = notnull(x)
         if not skipna and not mask.all():
             return np.nan
-        return lib.median(x[mask])
+        return algos.median(x[mask])
 
     if values.dtype != np.float64:
         values = values.astype('f8')
@@ -494,17 +496,17 @@ def unique1d(values):
     Hash table-based unique
     """
     if np.issubdtype(values.dtype, np.floating):
-        table = lib.Float64HashTable(len(values))
+        table = _hash.Float64HashTable(len(values))
         uniques = np.array(table.unique(com._ensure_float64(values)),
                            dtype=np.float64)
     elif np.issubdtype(values.dtype, np.datetime64):
-        table = lib.Int64HashTable(len(values))
+        table = _hash.Int64HashTable(len(values))
         uniques = table.unique(com._ensure_int64(values))
         uniques = uniques.view('M8[ns]')
     elif np.issubdtype(values.dtype, np.integer):
-        table = lib.Int64HashTable(len(values))
+        table = _hash.Int64HashTable(len(values))
         uniques = table.unique(com._ensure_int64(values))
     else:
-        table = lib.PyObjectHashTable(len(values))
+        table = _hash.PyObjectHashTable(len(values))
         uniques = table.unique(com._ensure_object(values))
     return uniques
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 3a4cdf2eb..5b82c13d9 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -14,6 +14,7 @@ from pandas.core.groupby import (get_group_index, _compress_group_index,
                                  decons_group_index)
 import pandas.core.common as com
 import pandas.lib as lib
+import pandas.algos as algos
 
 
 from pandas.core.index import MultiIndex
@@ -87,7 +88,7 @@ class _Unstacker(object):
         comp_index, obs_ids = _compress_group_index(group_index)
         ngroups = len(obs_ids)
 
-        indexer = lib.groupsort_indexer(comp_index, ngroups)[0]
+        indexer = algos.groupsort_indexer(comp_index, ngroups)[0]
         indexer = _ensure_platform_int(indexer)
 
         self.sorted_values = com.take_2d(self.values, indexer, axis=0)
diff --git a/pandas/core/series.py b/pandas/core/series.py
index fedaf7648..ae21014c2 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -32,6 +32,7 @@ import pandas.core.nanops as nanops
 from pandas.util.decorators import Appender, Substitution, cache_readonly
 
 import pandas.lib as lib
+import pandas.tslib as tslib
 import pandas.index as _index
 
 from pandas.compat.scipy import scoreatpercentile as _quantile
@@ -808,7 +809,7 @@ copy : boolean, default False
         value : scalar (int) or Series (slice, sequence)
         """
         try:
-            return lib.get_value_at(self, i)
+            return _index.get_value_at(self, i)
         except IndexError:
             raise
         except:
@@ -819,7 +820,7 @@ copy : boolean, default False
                 if isinstance(label, Index):
                     return self.reindex(label)
                 else:
-                    return lib.get_value_at(self, i)
+                    return _index.get_value_at(self, i)
 
     iget = iget_value
     irow = iget_value
@@ -2986,7 +2987,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                     not com.is_datetime64_dtype(dtype)):
                     if dtype == object:
                         ints = np.asarray(data).view('i8')
-                        subarr = lib.ints_to_pydatetime(ints)
+                        subarr = tslib.ints_to_pydatetime(ints)
                     elif raise_cast_failure:
                         raise TypeError('Cannot cast datetime64 to %s' % dtype)
                 else:
diff --git a/pandas/hashtable.pyx b/pandas/hashtable.pyx
index 83473d9ae..3aad4b287 100644
--- a/pandas/hashtable.pyx
+++ b/pandas/hashtable.pyx
@@ -565,7 +565,7 @@ cdef class Float64HashTable(HashTable):
         labels = self.get_labels(values, uniques, 0, -1)
         return uniques.to_array(), labels
 
-    def get_lables(self, ndarray[float64_t] values,
+    def get_labels(self, ndarray[float64_t] values,
                      Float64Vector uniques,
                      Py_ssize_t count_prior, int64_t na_sentinel):
         cdef:
@@ -779,7 +779,7 @@ cdef class PyObjectHashTable(HashTable):
 
         return result
 
-    def get_lables(self, ndarray[object] values, ObjectVector uniques,
+    def get_labels(self, ndarray[object] values, ObjectVector uniques,
                      Py_ssize_t count_prior, int64_t na_sentinel):
         cdef:
             Py_ssize_t i, n = len(values)
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index 74a02301e..5a5d9d294 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -29,6 +29,9 @@ import pandas.core.common as com
 from pandas.tools.merge import concat
 
 import pandas.lib as lib
+import pandas.algos as algos
+import pandas.tslib as tslib
+
 from contextlib import contextmanager
 
 # reading and writing the full object in one go
@@ -609,9 +612,9 @@ class HDFStore(object):
                 node._v_attrs.freq = index.freq
 
             if hasattr(index, 'tz') and index.tz is not None:
-                zone = lib.get_timezone(index.tz)
+                zone = tslib.get_timezone(index.tz)
                 if zone is None:
-                    zone = lib.tot_seconds(index.tz.utcoffset())
+                    zone = tslib.tot_seconds(index.tz.utcoffset())
                 node._v_attrs.tz = zone
 
     def _read_index(self, group, key):
@@ -1276,26 +1279,27 @@ class Table(object):
         raise NotImplementedError("cannot delete on an abstract table")
 
 class WORMTable(Table):
-    """ a write-once read-many table:
-         this format DOES NOT ALLOW appending to a table. writing is a one-time operation
-         the data are stored in a format that allows for searching the data on disk
+    """ a write-once read-many table: this format DOES NOT ALLOW appending to a
+         table. writing is a one-time operation the data are stored in a format
+         that allows for searching the data on disk
          """
     table_type = 'worm'
 
     def read(self, **kwargs):
-        """ read the indicies and the indexing array, calculate offset rows and return """
+        """ read the indicies and the indexing array, calculate offset rows and
+        return """
         raise NotImplementedError("WORMTable needs to implement read")
 
     def write(self, **kwargs):
-        """ write in a format that we can search later on (but cannot append to):
-               write out the indicies and the values using _write_array (e.g. a CArray)
-               create an indexing table so that we can search """
+        """ write in a format that we can search later on (but cannot append
+               to): write out the indicies and the values using _write_array
+               (e.g. a CArray) create an indexing table so that we can search"""
         raise NotImplementedError("WORKTable needs to implement write")
 
 class LegacyTable(Table):
-    """ an appendable table:
-          allow append/query/delete operations to a (possibily) already existing appendable table
-          this table ALLOWS append (but doesn't require them), and stores the data in a format
+    """ an appendable table: allow append/query/delete operations to a
+          (possibily) already existing appendable table this table ALLOWS
+          append (but doesn't require them), and stores the data in a format
           that can be easily searched
 
         """
@@ -1318,7 +1322,7 @@ class LegacyTable(Table):
 
         panels = []
         if len(unique(key)) == len(key):
-            sorter, _ = lib.groupsort_indexer(com._ensure_int64(key), J * K)
+            sorter, _ = algos.groupsort_indexer(com._ensure_int64(key), J * K)
             sorter = com._ensure_platform_int(sorter)
 
             # create the panels
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index c7e783dee..777a58a0f 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -15,6 +15,7 @@ from pandas.util import py3compat
 from pandas._sparse import BlockIndex, IntIndex
 import pandas._sparse as splib
 import pandas.lib as lib
+import pandas.index as _index
 
 
 def _sparse_op_wrap(op, name):
@@ -264,7 +265,7 @@ to sparse
         if sp_loc == -1:
             return self.fill_value
         else:
-            return lib.get_value_at(self, sp_loc)
+            return _index.get_value_at(self, sp_loc)
 
     def take(self, indices, axis=0):
         """
diff --git a/pandas/src/inference.pyx b/pandas/src/inference.pyx
index 7327e5586..7c055d1bb 100644
--- a/pandas/src/inference.pyx
+++ b/pandas/src/inference.pyx
@@ -294,20 +294,6 @@ def is_period_array(ndarray[object] values):
             return False
     return True
 
-def extract_ordinals(ndarray[object] values, freq):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[int64_t] ordinals = np.empty(n, dtype=np.int64)
-        object p
-
-    for i in range(n):
-        p = values[i]
-        ordinals[i] = p.ordinal
-        if p.freq != freq:
-            raise ValueError("%s is wrong freq" % p)
-
-    return ordinals
-
 
 cdef extern from "parse_helper.h":
     inline int floatify(object, double *result) except -1
diff --git a/pandas/tests/test_common.py b/pandas/tests/test_common.py
index dd93666cb..b9d495af1 100644
--- a/pandas/tests/test_common.py
+++ b/pandas/tests/test_common.py
@@ -11,6 +11,7 @@ import pandas.util.testing as tm
 
 import numpy as np
 
+from pandas.tslib import iNaT
 from pandas.util import py3compat
 
 def test_is_sequence():
@@ -76,9 +77,8 @@ def test_isnull_datetime():
     idx = date_range('1/1/1990', periods=20)
     assert(notnull(idx).all())
 
-    import pandas.lib as lib
     idx = np.asarray(idx)
-    idx[0] = lib.iNaT
+    idx[0] = iNaT
     idx = DatetimeIndex(idx)
     mask = isnull(idx)
     assert(mask[0])
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index 9bef76a68..f3c07a4be 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -17,6 +17,8 @@ import pandas.util.testing as tm
 from pandas.util.compat import product as cart_product
 import pandas as pd
 
+import pandas.index as _index
+
 class TestMultiLevel(unittest.TestCase):
 
     def setUp(self):
@@ -1591,9 +1593,8 @@ Thur,Lunch,Yes,51.51,17"""
     def test_indexing_over_hashtable_size_cutoff(self):
         n = 10000
 
-        import pandas.lib as lib
-        old_cutoff = lib._SIZE_CUTOFF
-        lib._SIZE_CUTOFF = 20000
+        old_cutoff = _index._SIZE_CUTOFF
+        _index._SIZE_CUTOFF = 20000
 
         s = Series(np.arange(n),
                    MultiIndex.from_arrays((["a"] * n, np.arange(n))))
@@ -1603,7 +1604,7 @@ Thur,Lunch,Yes,51.51,17"""
         self.assertEquals(s[("a", 6)], 6)
         self.assertEquals(s[("a", 7)], 7)
 
-        lib._SIZE_CUTOFF = old_cutoff
+        _index._SIZE_CUTOFF = old_cutoff
 
     def test_xs_mixed_no_copy(self):
         index = MultiIndex.from_arrays([['a','a', 'b', 'b'], [1,2,1,2]],
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index ce8de2af4..69941e9f6 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -435,8 +435,8 @@ class TestBinGroupers(unittest.TestCase):
         }
 
         for fname in funcs:
-            args = [getattr(lib, 'group_%s' % fname),
-                    getattr(lib, 'group_%s_bin' % fname),
+            args = [getattr(algos, 'group_%s' % fname),
+                    getattr(algos, 'group_%s_bin' % fname),
                     np_funcs[fname]]
             self._check_versions(*args)
 
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
index 2e333317d..3964a04d2 100644
--- a/pandas/tools/tests/test_merge.py
+++ b/pandas/tools/tests/test_merge.py
@@ -14,7 +14,7 @@ from pandas.tseries.index import DatetimeIndex
 from pandas.tools.merge import merge, concat, ordered_merge, MergeError
 from pandas.util.testing import (assert_frame_equal, assert_series_equal,
                                  assert_almost_equal, rands)
-import pandas.lib as lib
+import pandas.algos as algos
 import pandas.util.testing as tm
 
 a_ = np.array
@@ -68,7 +68,7 @@ class TestMerge(unittest.TestCase):
         right = a_([1, 1, 0, 4, 2, 2, 1], dtype=np.int64)
         max_group = 5
 
-        ls, rs = lib.left_outer_join(left, right, max_group)
+        ls, rs = algos.left_outer_join(left, right, max_group)
 
         exp_ls = left.argsort(kind='mergesort')
         exp_rs = right.argsort(kind='mergesort')
@@ -92,7 +92,7 @@ class TestMerge(unittest.TestCase):
         right = a_([1, 1, 0, 4, 2, 2, 1], dtype=np.int64)
         max_group = 5
 
-        rs, ls  = lib.left_outer_join(right, left, max_group)
+        rs, ls  = algos.left_outer_join(right, left, max_group)
 
         exp_ls = left.argsort(kind='mergesort')
         exp_rs = right.argsort(kind='mergesort')
@@ -118,7 +118,7 @@ class TestMerge(unittest.TestCase):
         right = a_([1, 1, 0, 4, 2, 2, 1, 4], dtype=np.int64)
         max_group = 5
 
-        ls, rs = lib.inner_join(left, right, max_group)
+        ls, rs = algos.inner_join(left, right, max_group)
 
         exp_ls = left.argsort(kind='mergesort')
         exp_rs = right.argsort(kind='mergesort')
diff --git a/pandas/tseries/frequencies.py b/pandas/tseries/frequencies.py
index bc1770d58..4b7867e5f 100644
--- a/pandas/tseries/frequencies.py
+++ b/pandas/tseries/frequencies.py
@@ -9,6 +9,7 @@ from pandas.util.decorators import cache_readonly
 import pandas.tseries.offsets as offsets
 import pandas.core.common as com
 import pandas.lib as lib
+import pandas.tslib as tslib
 
 
 class FreqGroup(object):
@@ -763,7 +764,7 @@ class _FrequencyInferer(object):
 
     @cache_readonly
     def deltas(self):
-        return lib.unique_deltas(self.values)
+        return tslib.unique_deltas(self.values)
 
     @cache_readonly
     def is_unique(self):
@@ -805,7 +806,7 @@ class _FrequencyInferer(object):
 
     @cache_readonly
     def fields(self):
-        return lib.build_field_sarray(self.values)
+        return tslib.build_field_sarray(self.values)
 
     @cache_readonly
     def rep_stamp(self):
@@ -856,11 +857,11 @@ class _FrequencyInferer(object):
     @cache_readonly
     def mdiffs(self):
         nmonths = self.fields['Y'] * 12 + self.fields['M']
-        return lib.unique_deltas(nmonths.astype('i8'))
+        return tslib.unique_deltas(nmonths.astype('i8'))
 
     @cache_readonly
     def ydiffs(self):
-        return lib.unique_deltas(self.fields['Y'].astype('i8'))
+        return tslib.unique_deltas(self.fields['Y'].astype('i8'))
 
     def _infer_daily_rule(self):
         annual_rule = self._get_annual_rule()
diff --git a/pandas/tseries/period.py b/pandas/tseries/period.py
index 865add7c9..602dabe54 100644
--- a/pandas/tseries/period.py
+++ b/pandas/tseries/period.py
@@ -15,6 +15,7 @@ import pandas.core.common as com
 
 from pandas.lib import Timestamp
 import pandas.lib as lib
+import pandas.tslib as tslib
 import pandas.algos as _algos
 
 
@@ -24,7 +25,7 @@ import pandas.algos as _algos
 def _period_field_accessor(name, alias):
     def f(self):
         base, mult = _gfc(self.freq)
-        return lib.get_period_field(alias, self.ordinal, base)
+        return tslib.get_period_field(alias, self.ordinal, base)
     f.__name__ = name
     return property(f)
 
@@ -32,7 +33,7 @@ def _period_field_accessor(name, alias):
 def _field_accessor(name, alias):
     def f(self):
         base, mult = _gfc(self.freq)
-        return lib.get_period_field_arr(alias, self.values, base)
+        return tslib.get_period_field_arr(alias, self.values, base)
     f.__name__ = name
     return property(f)
 
@@ -119,9 +120,9 @@ class Period(object):
             raise ValueError('Only mult == 1 supported')
 
         if self.ordinal is None:
-            self.ordinal = lib.period_ordinal(dt.year, dt.month, dt.day,
-                                               dt.hour, dt.minute, dt.second,
-                                               base)
+            self.ordinal = tslib.period_ordinal(dt.year, dt.month, dt.day,
+                                                dt.hour, dt.minute, dt.second,
+                                                base)
 
         self.freq = _freq_mod._get_freq_str(base)
 
@@ -174,7 +175,7 @@ class Period(object):
             raise ValueError('Only mult == 1 supported')
 
         end = how == 'E'
-        new_ordinal = lib.period_asfreq(self.ordinal, base1, base2, end)
+        new_ordinal = tslib.period_asfreq(self.ordinal, base1, base2, end)
 
         return Period(ordinal=new_ordinal, freq=base2)
 
@@ -214,7 +215,7 @@ class Period(object):
         base, mult = _gfc(freq)
         val = self.asfreq(freq, how)
 
-        dt64 = lib.period_ordinal_to_dt64(val.ordinal, base)
+        dt64 = tslib.period_ordinal_to_dt64(val.ordinal, base)
         return Timestamp(dt64)
 
     year = _period_field_accessor('year', 0)
@@ -237,13 +238,13 @@ class Period(object):
 
     def __repr__(self):
         base, mult = _gfc(self.freq)
-        formatted = lib.period_format(self.ordinal, base)
+        formatted = tslib.period_format(self.ordinal, base)
         freqstr = _freq_mod._reverse_period_code_map[base]
         return "Period('%s', '%s')" % (formatted, freqstr)
 
     def __str__(self):
         base, mult = _gfc(self.freq)
-        formatted = lib.period_format(self.ordinal, base)
+        formatted = tslib.period_format(self.ordinal, base)
         return ("%s" % formatted)
 
     def strftime(self, fmt):
@@ -384,7 +385,7 @@ class Period(object):
             'Jan. 01, 2001 was a Monday'
         """
         base, mult = _gfc(self.freq)
-        return lib.period_format(self.ordinal, base, fmt)
+        return tslib.period_format(self.ordinal, base, fmt)
 
 
 def _get_date_and_freq(value, freq):
@@ -415,7 +416,7 @@ def _get_date_and_freq(value, freq):
 def _get_ordinals(data, freq):
     f = lambda x: Period(x, freq=freq).ordinal
     if isinstance(data[0], Period):
-        return lib.extract_ordinals(data, freq)
+        return tslib.extract_ordinals(data, freq)
     else:
         return lib.map_infer(data, f)
 
@@ -425,7 +426,7 @@ def dt64arr_to_periodarr(data, freq, tz):
         raise ValueError('Wrong dtype: %s' % data.dtype)
 
     base, mult = _gfc(freq)
-    return lib.dt64arr_to_periodarr(data.view('i8'), base, tz)
+    return tslib.dt64arr_to_periodarr(data.view('i8'), base, tz)
 
 # --- Period index sketch
 def _period_index_cmp(opname):
@@ -600,7 +601,8 @@ class PeriodIndex(Int64Index):
                 else:
                     base1, _ = _gfc(data.freq)
                     base2, _ = _gfc(freq)
-                    data = lib.period_asfreq_arr(data.values, base1, base2, 1)
+                    data = tslib.period_asfreq_arr(data.values, base1,
+                                                   base2, 1)
             else:
                 if freq is None and len(data) > 0:
                     freq = getattr(data[0], 'freq', None)
@@ -721,7 +723,7 @@ class PeriodIndex(Int64Index):
             raise ValueError('Only mult == 1 supported')
 
         end = how == 'E'
-        new_data = lib.period_asfreq_arr(self.values, base1, base2, end)
+        new_data = tslib.period_asfreq_arr(self.values, base1, base2, end)
 
         result = new_data.view(PeriodIndex)
         result.name = self.name
@@ -788,7 +790,7 @@ class PeriodIndex(Int64Index):
         base, mult = _gfc(freq)
         new_data = self.asfreq(freq, how)
 
-        new_data = lib.periodarr_to_dt64arr(new_data.values, base)
+        new_data = tslib.periodarr_to_dt64arr(new_data.values, base)
         return DatetimeIndex(new_data, freq='infer', name=self.name)
 
     def shift(self, n):
@@ -1130,7 +1132,7 @@ def _range_from_fields(year=None, month=None, quarter=None, day=None,
         year, quarter = _make_field_arrays(year, quarter)
         for y, q in zip(year, quarter):
             y, m = _quarter_to_myear(y, q, freq)
-            val = lib.period_ordinal(y, m, 1, 1, 1, 1, base)
+            val = tslib.period_ordinal(y, m, 1, 1, 1, 1, base)
             ordinals.append(val)
     else:
         base, mult = _gfc(freq)
@@ -1139,7 +1141,7 @@ def _range_from_fields(year=None, month=None, quarter=None, day=None,
 
         arrays = _make_field_arrays(year, month, day, hour, minute, second)
         for y, mth, d, h, mn, s in zip(*arrays):
-            ordinals.append(lib.period_ordinal(y, mth, d, h, mn, s, base))
+            ordinals.append(tslib.period_ordinal(y, mth, d, h, mn, s, base))
 
     return np.array(ordinals, dtype=np.int64), freq
 
@@ -1168,7 +1170,7 @@ def _ordinal_from_fields(year, month, quarter, day, hour, minute,
     if quarter is not None:
         year, month = _quarter_to_myear(year, quarter, freq)
 
-    return lib.period_ordinal(year, month, day, hour, minute, second, base)
+    return tslib.period_ordinal(year, month, day, hour, minute, second, base)
 
 
 def _quarter_to_myear(year, quarter, freq):
diff --git a/pandas/tseries/tests/test_offsets.py b/pandas/tseries/tests/test_offsets.py
index 45de93f00..6506e7ab4 100644
--- a/pandas/tseries/tests/test_offsets.py
+++ b/pandas/tseries/tests/test_offsets.py
@@ -18,14 +18,14 @@ import pandas.tseries.offsets as offsets
 
 from nose.tools import assert_raises
 
-import pandas.lib as lib
+from pandas.tslib import monthrange
 from pandas.lib import Timestamp
 
 def test_monthrange():
     import calendar
     for y in range(2000,2013):
         for m in range(1,13):
-            assert lib.monthrange(y,m) == calendar.monthrange(y,m)
+            assert monthrange(y,m) == calendar.monthrange(y,m)
 
 ####
 ## Misc function tests
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index 84ff68fa2..4a07034eb 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -24,8 +24,12 @@ import pandas.util.testing as tm
 
 from pandas.util.py3compat import StringIO
 
-from pandas.lib import NaT, iNaT
+from pandas.tslib import NaT, iNaT
 import pandas.lib as lib
+import pandas.tslib as tslib
+
+import pandas.index as _index
+
 import cPickle as pickle
 import pandas.core.datetools as dt
 from numpy.random import rand
@@ -110,9 +114,9 @@ class TestTimeSeriesDuplicates(unittest.TestCase):
         import datetime
         # #1821
 
-        old_cutoff = lib._SIZE_CUTOFF
+        old_cutoff = _index._SIZE_CUTOFF
         try:
-            lib._SIZE_CUTOFF = 1000
+            _index._SIZE_CUTOFF = 1000
 
             # create large list of non periodic datetime
             dates = []
@@ -144,7 +148,7 @@ class TestTimeSeriesDuplicates(unittest.TestCase):
             df.ix[timestamp]
             self.assert_(len(df.ix[[timestamp]]) > 0)
         finally:
-            lib._SIZE_CUTOFF = old_cutoff
+            _index._SIZE_CUTOFF = old_cutoff
 
 def assert_range_equal(left, right):
     assert(left.equals(right))
@@ -500,7 +504,7 @@ class TestTimeSeries(unittest.TestCase):
 
         idx = Index(arr)
 
-        self.assert_((idx.values == lib.cast_to_nanoseconds(arr)).all())
+        self.assert_((idx.values == tslib.cast_to_nanoseconds(arr)).all())
 
     def test_index_astype_datetime64(self):
         idx = Index([datetime(2012, 1, 1)], dtype=object)
@@ -598,7 +602,7 @@ class TestTimeSeries(unittest.TestCase):
             else:
                 expected[i] = parse(val)
 
-        result = lib.array_to_datetime(strings)
+        result = tslib.array_to_datetime(strings)
         assert_almost_equal(result, expected)
 
         result2 = to_datetime(strings)
@@ -2291,16 +2295,16 @@ class TestSeriesDatetime64(unittest.TestCase):
 
     def test_set_none_nan(self):
         self.series[3] = None
-        self.assert_(self.series[3] is lib.NaT)
+        self.assert_(self.series[3] is NaT)
 
         self.series[3:5] = None
-        self.assert_(self.series[4] is lib.NaT)
+        self.assert_(self.series[4] is NaT)
 
         self.series[5] = np.nan
-        self.assert_(self.series[5] is lib.NaT)
+        self.assert_(self.series[5] is NaT)
 
         self.series[5:7] = np.nan
-        self.assert_(self.series[6] is lib.NaT)
+        self.assert_(self.series[6] is NaT)
 
     def test_intercept_astype_object(self):
         # Work around NumPy 1.6 bugs
diff --git a/pandas/tslib.pyx b/pandas/tslib.pyx
index 55c6f1da0..42aa2f9f7 100644
--- a/pandas/tslib.pyx
+++ b/pandas/tslib.pyx
@@ -1967,3 +1967,17 @@ cdef accessor _get_accessor_func(int code):
     else:
         raise ValueError('Unrecognized code: %s' % code)
 
+
+def extract_ordinals(ndarray[object] values, freq):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[int64_t] ordinals = np.empty(n, dtype=np.int64)
+        object p
+
+    for i in range(n):
+        p = values[i]
+        ordinals[i] = p.ordinal
+        if p.freq != freq:
+            raise ValueError("%s is wrong freq" % p)
+
+    return ordinals
