commit 0b1b2fc98c4998f286911f0a5e1d5252281bff27
Author: jreback <jeff@reback.net>
Date:   Sat Jul 6 15:52:29 2013 -0400

    ENH: add Expr based terms for pytables

diff --git a/doc/source/io.rst b/doc/source/io.rst
index c29af29d2..a20d2a7aa 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1962,7 +1962,7 @@ storing/selecting from homogeneous index DataFrames.
         store.select('df_mi')
 
         # the levels are automatically included as data columns
-        store.select('df_mi', Term('foo=bar'))
+        store.select('df_mi', 'foo=bar')
 
 
 .. _io.hdf5-query:
@@ -1970,49 +1970,80 @@ storing/selecting from homogeneous index DataFrames.
 Querying a Table
 ~~~~~~~~~~~~~~~~
 
+.. warning::
+
+   This query capabilities have changed substantially starting in ``0.13.0``.
+   Queries from prior version are accepted (with a ``DeprecationWarning``) printed
+   if its not string-like.
+
 ``select`` and ``delete`` operations have an optional criterion that can
 be specified to select/delete only a subset of the data. This allows one
 to have a very large on-disk table and retrieve only a portion of the
 data.
 
-A query is specified using the ``Term`` class under the hood.
+A query is specified using the ``Term`` class under the hood, as a boolean expression.
 
-   - 'index' and 'columns' are supported indexers of a DataFrame
-   - 'major_axis', 'minor_axis', and 'items' are supported indexers of
+   - ``index`` and ``columns`` are supported indexers of a DataFrame
+   - ``major_axis``, ``minor_axis``, and ``items`` are supported indexers of
      the Panel
+   - if ``data_columns`` are specified, these can be used as additional indexers
+
+Valid comparison operators are:
+
+   - ``=, ==, !=, >, >=, <, <=``
+
+Valid boolean expressions are combined with:
+
+   - ``|`` : or
+   - ``&`` : and
+   - ``(`` and ``)`` : for grouping
+
+These rules are similar to how boolean expressions are used in pandas for indexing.
+
+.. note::
+
+   - ``=`` will be automatically expanded to the comparison operator ``==``
+   - ``~`` is the not operator, but can only be used in very limited circumstances
+   - If a list/tuple of expressions are passed they will be combined via ``&``.
+
+The following are valid expressions:
+
+   - ``'index>=date'``
+   - ``"columns=['A', 'D']"``
+   - ``'columns=A'``
+   - ``'columns==A'``
+   - ``"~(columns=['A','B'])"``
+   - ``'index>df.index[3] & string="bar"'``
+   - ``'(index>df.index[3] & index<=df.index[6]) | string="bar"'``
+   - ``"ts>=Timestamp('2012-02-01')"``
+   - ``"major_axis>=20130101"``
+
+The ``indexers`` are on the left-hand side of the sub-expression:
 
-Valid terms can be created from ``dict, list, tuple, or
-string``. Objects can be embeded as values. Allowed operations are: ``<,
-<=, >, >=, =, !=``. ``=`` will be inferred as an implicit set operation
-(e.g. if 2 or more values are provided). The following are all valid
-terms.
+   - ``columns``, ``major_axis``, ``ts``
 
-       - ``dict(field = 'index', op = '>', value = '20121114')``
-       - ``('index', '>', '20121114')``
-       - ``'index > 20121114'``
-       - ``('index', '>', datetime(2012, 11, 14))``
-       - ``('index', ['20121114', '20121115'])``
-       - ``('major_axis', '=', Timestamp('2012/11/14'))``
-       - ``('minor_axis', ['A', 'B'])``
+The right-hand side of the sub-expression (after a comparsion operator), can be:
 
-Queries are built up using a list of ``Terms`` (currently only
-**anding** of terms is supported). An example query for a panel might be
-specified as follows.  ``['major_axis>20000102', ('minor_axis', '=',
-['A', 'B']) ]``. This is roughly translated to: `major_axis must be
-greater than the date 20000102 and the minor_axis must be A or B`
+   - functions that will be evaluated, e.g. ``Timestamp('2012-02-01')``
+   - strings, e.g. ``"bar"``
+   - date-like, e.g. ``20130101``, or ``"20130101"``
+   - lists, e.g. ``"['A','B']"``
+   - variables that are defined in the local names space, e.g. ``date``
+
+Here is an example:
 
 .. ipython:: python
 
    store.append('wp',wp)
    store
-   store.select('wp', [ Term('major_axis>20000102'), Term('minor_axis', '=', ['A', 'B']) ])
+   store.select('wp', "major_axis>Timestamp('20000102') & minor_axis=['A', 'B']")
 
 The ``columns`` keyword can be supplied to select a list of columns to be returned,
-this is equivalent to passing a ``Term('columns', list_of_columns_to_filter)``:
+this is equivalent to passing a ``'columns=list_of_columns_to_filter'``:
 
 .. ipython:: python
 
-   store.select('df', columns=['A', 'B'])
+   store.select('df', "columns=['A', 'B']")
 
 ``start`` and ``stop`` parameters can be specified to limit the total search
 space. These are in terms of the total number of rows in a table.
@@ -2023,8 +2054,7 @@ space. These are in terms of the total number of rows in a table.
    wp.to_frame()
 
    # limiting the search
-   store.select('wp',[ Term('major_axis>20000102'),
-                       Term('minor_axis', '=', ['A','B']) ],
+   store.select('wp',"major_axis>20000102 & minor_axis=['A','B']",
                 start=0, stop=10)
 
 .. _io.hdf5-timedelta:
@@ -2057,10 +2087,13 @@ You can create/modify an index for a table with ``create_table_index``
 after data is already in the table (after and ``append/put``
 operation). Creating a table index is **highly** encouraged. This will
 speed your queries a great deal when you use a ``select`` with the
-indexed dimension as the ``where``. **Indexes are automagically created
-(starting 0.10.1)** on the indexables and any data columns you
-specify. This behavior can be turned off by passing ``index=False`` to
-``append``.
+indexed dimension as the ``where``.
+
+.. note::
+
+   Indexes are automagically created (starting ``0.10.1``) on the indexables
+   and any data columns you specify. This behavior can be turned off by passing
+   ``index=False`` to ``append``.
 
 .. ipython:: python
 
@@ -2117,7 +2150,7 @@ create a new table!)
 Iterator
 ~~~~~~~~
 
-Starting in 0.11, you can pass, ``iterator=True`` or ``chunksize=number_in_a_chunk``
+Starting in ``0.11.0``, you can pass, ``iterator=True`` or ``chunksize=number_in_a_chunk``
 to ``select`` and ``select_as_multiple`` to return an iterator on the results.
 The default is 50,000 rows returned in a chunk.
 
@@ -2151,7 +2184,7 @@ Advanced Queries
 To retrieve a single indexable or data column, use the
 method ``select_column``. This will, for example, enable you to get the index
 very quickly. These return a ``Series`` of the result, indexed by the row number.
-These do not currently accept the ``where`` selector (coming soon)
+These do not currently accept the ``where`` selector.
 
 .. ipython:: python
 
diff --git a/doc/source/v0.10.0.txt b/doc/source/v0.10.0.txt
index d0c0ecc14..476760e4b 100644
--- a/doc/source/v0.10.0.txt
+++ b/doc/source/v0.10.0.txt
@@ -258,11 +258,10 @@ Updated PyTables Support
    store.append('wp',wp)
 
    # selecting via A QUERY
-   store.select('wp',
-     [ Term('major_axis>20000102'), Term('minor_axis', '=', ['A','B']) ])
+   store.select('wp', "major_axis>20000102 & minor_axis=['A','B']")
 
    # removing data from tables
-   store.remove('wp', [ 'major_axis', '>', wp.major_axis[3] ])
+   store.remove('wp', 'major_axis>wp.major_axis[3]')
    store.select('wp')
 
    # deleting a store
diff --git a/pandas/computation/align.py b/pandas/computation/align.py
index 529fe84fd..65840bb68 100644
--- a/pandas/computation/align.py
+++ b/pandas/computation/align.py
@@ -77,18 +77,20 @@ def _filter_special_cases(f):
         if len(terms) == 1:
             return _align_core_single_unary_op(terms[0])
 
+        term_values = (term.value for term in terms)
         # only scalars
-        elif all(term.isscalar for term in terms):
-            return np.result_type(*(term.value for term in terms)), None
+        if all(isinstance(term.value, pd.Index) or term.isscalar for term in
+               terms):
+            return np.result_type(*term_values), None
 
         # single element ndarrays
         all_has_size = all(hasattr(term.value, 'size') for term in terms)
-        if (all_has_size and all(term.value.size == 1 for term in terms)):
-            return np.result_type(*(term.value for term in terms)), None
+        if all_has_size and all(term.value.size == 1 for term in terms):
+            return np.result_type(*term_values), None
 
         # no pandas so just punt to the evaluator
         if not _any_pandas_objects(terms):
-            return np.result_type(*(term.value for term in terms)), None
+            return np.result_type(*term_values), None
 
         return f(terms)
     return wrapper
@@ -162,17 +164,11 @@ def _filter_terms(flat):
     return names, literals
 
 
-def _align(terms, env):
-
-    # flatten the parse tree (a nested list)
+def _align(terms):
+    """Align a set of terms"""
+    # flatten the parse tree (a nested list, really)
     terms = list(flatten(terms))
 
-    # separate names and literals
-    names, literals = _filter_terms(terms)
-
-    if not names:  # only literals so just promote to a common type
-        return np.result_type(*literals).type, None
-
     # if all resolved variables are numeric scalars
     if all(term.isscalar for term in terms):
         return np.result_type(*(term.value for term in terms)).type, None
diff --git a/pandas/computation/engines.py b/pandas/computation/engines.py
index 7f500dccb..ea296ad0e 100644
--- a/pandas/computation/engines.py
+++ b/pandas/computation/engines.py
@@ -21,8 +21,7 @@ class AbstractEngine(object):
 
     def evaluate(self):
         if not self._is_aligned:
-            self.result_type, self.aligned_axes = _align(self.expr.terms,
-                                                         self.expr.env)
+            self.result_type, self.aligned_axes = _align(self.expr.terms)
 
         res = self._evaluate(self.expr.env)
         return _reconstruct_object(self.result_type, res, self.aligned_axes,
@@ -77,4 +76,4 @@ class PythonEngine(AbstractEngine):
         pass
 
 
-_engines = {'numexpr': NumExprEngine, 'python': PythonEngine}
+_engines = {'numexpr': NumExprEngine, 'python': PythonEngine }
diff --git a/pandas/computation/eval.py b/pandas/computation/eval.py
index 1a681e37d..04e17e3e4 100644
--- a/pandas/computation/eval.py
+++ b/pandas/computation/eval.py
@@ -11,7 +11,7 @@ from pandas.computation.engines import _engines
 
 
 def eval(expr, engine='numexpr', truediv=True, local_dict=None,
-         global_dict=None):
+         global_dict=None, resolvers=None):
     """Evaluate a Python expression as a string using various backends.
 
     The following arithmetic operations are supported: +, -, *, /, **, %, //
@@ -24,7 +24,7 @@ def eval(expr, engine='numexpr', truediv=True, local_dict=None,
     expr : string or Expr object
         The expression to evaluate. This can be either a string or an ``Expr``
         object.
-    engine : string, optional, default 'numexpr', {'python', 'numexpr', 'pytables'}
+    engine : string, optional, default 'numexpr', {'python', 'numexpr' }
         The engine used to evaluate the expression. Supported engines are
 
         - 'numexpr': This default engine evaluates pandas objects using numexpr
@@ -32,8 +32,6 @@ def eval(expr, engine='numexpr', truediv=True, local_dict=None,
                      frames.
         - 'python': Performs operations as if you had eval'd in top level
                     python
-        - 'pytables': Engine used for evaluating expressions for selection of
-                      objects from PyTables HDF5 tables.
 
     truediv : bool, optional, default True
         Whether to use true division, like in Python >= 3
@@ -61,9 +59,9 @@ def eval(expr, engine='numexpr', truediv=True, local_dict=None,
     eng = _engines[engine]
 
     if isinstance(expr, six.string_types):
-        # need to go 2 up in the call stack from the constructor since we want
-        # the calling scope's variables
-        env = Scope(global_dict, local_dict, frame_level=2)
+        # need to go 2 up in the call stack from the constructor
+        env = Scope(global_dict, local_dict, frame_level=2,
+                    resolvers=resolvers)
         parsed_expr = Expr(expr, engine, env, truediv)
     elif isinstance(expr, Expr):
         parsed_expr = expr
@@ -77,7 +75,6 @@ def eval(expr, engine='numexpr', truediv=True, local_dict=None,
 
     # sanity check for a number
     # TODO: eventually take out
-    # TODO: pytables engine will probably need a string check
     if np.isscalar(ret):
         if not isinstance(ret, (np.number, np.bool_, numbers.Number)):
             raise TypeError('scalar result must be numeric or bool, passed '
diff --git a/pandas/computation/expr.py b/pandas/computation/expr.py
index 9a9cd2262..cb7b26948 100644
--- a/pandas/computation/expr.py
+++ b/pandas/computation/expr.py
@@ -1,23 +1,27 @@
 import ast
+import operator
 import sys
+import inspect
 import itertools
 import tokenize
-import re
 from cStringIO import StringIO
 from functools import partial
 
-
 from pandas.core.base import StringMixin
 from pandas.computation.ops import BinOp, UnaryOp, _reductions, _mathops
 from pandas.computation.ops import _cmp_ops_syms, _bool_ops_syms
 from pandas.computation.ops import _arith_ops_syms, _unary_ops_syms
 from pandas.computation.ops import Term, Constant
 
+import pandas.lib as lib
+import datetime
+
 
 class Scope(object):
-    __slots__ = 'globals', 'locals'
+    __slots__ = ('globals', 'locals', 'resolvers', '_global_resolvers',
+                 'resolver_keys', '_resolver')
 
-    def __init__(self, gbls=None, lcls=None, frame_level=1):
+    def __init__(self, gbls=None, lcls=None, frame_level=1, resolvers=None):
         frame = sys._getframe(frame_level)
 
         try:
@@ -26,9 +30,53 @@ class Scope(object):
         finally:
             del frame
 
-
-class ExprParserError(Exception):
-    pass
+        # add some useful defaults
+        self.globals['Timestamp'] = lib.Timestamp
+        self.globals['datetime'] = datetime
+
+        self.resolvers = resolvers or []
+        self.resolver_keys = set(reduce(operator.add, (list(o.keys()) for o in
+                                                       self.resolvers), []))
+        self._global_resolvers = self.resolvers + [self.locals, self.globals]
+        self._resolver = None
+
+    @property
+    def resolver(self):
+        if self._resolver is None:
+            def resolve_key(key):
+                for resolver in self._global_resolvers:
+                    try:
+                        return resolver[key]
+                    except KeyError:
+                        pass
+            self._resolver = resolve_key
+
+        return self._resolver
+
+    def update(self, scope_level=None):
+
+        # we are always 2 levels below the caller
+        # plus the caller maybe below the env level
+        # in which case we need addtl levels
+        sl = 2
+        if scope_level is not None:
+            sl += scope_level
+
+        # add sl frames to the scope starting with the
+        # most distant and overwritting with more current
+        # makes sure that we can capture variable scope
+        frame = inspect.currentframe()
+        try:
+            frames = []
+            while sl >= 0:
+                frame = frame.f_back
+                sl -= 1
+                frames.append(frame)
+            for f in frames[::-1]:
+                self.locals.update(f.f_locals)
+        finally:
+            del frame
+            del frames
 
 
 def _rewrite_assign(source):
@@ -52,110 +100,279 @@ def _parenthesize_booleans(source, ops='|&'):
     return res
 
 
-def preparse(source):
+def _preparse(source):
     return _parenthesize_booleans(_rewrite_assign(source))
 
 
-class ExprVisitor(ast.NodeVisitor):
+
+# partition all AST nodes
+_all_nodes = frozenset(filter(lambda x: isinstance(x, type) and
+                              issubclass(x, ast.AST),
+                              (getattr(ast, node) for node in dir(ast))))
+
+
+def _filter_nodes(superclass, all_nodes=_all_nodes):
+    node_names = (node.__name__ for node in all_nodes
+                  if issubclass(node, superclass))
+    return frozenset(node_names)
+
+
+_all_node_names = frozenset(map(lambda x: x.__name__, _all_nodes))
+_mod_nodes = _filter_nodes(ast.mod)
+_stmt_nodes = _filter_nodes(ast.stmt)
+_expr_nodes = _filter_nodes(ast.expr)
+_expr_context_nodes = _filter_nodes(ast.expr_context)
+_slice_nodes = _filter_nodes(ast.slice)
+_boolop_nodes = _filter_nodes(ast.boolop)
+_operator_nodes = _filter_nodes(ast.operator)
+_unary_op_nodes = _filter_nodes(ast.unaryop)
+_cmp_op_nodes = _filter_nodes(ast.cmpop)
+_comprehension_nodes = _filter_nodes(ast.comprehension)
+_handler_nodes = _filter_nodes(ast.excepthandler)
+_arguments_nodes = _filter_nodes(ast.arguments)
+_keyword_nodes = _filter_nodes(ast.keyword)
+_alias_nodes = _filter_nodes(ast.alias)
+
+
+# nodes that we don't support directly but are needed for parsing
+_hacked_nodes = frozenset(['Assign', 'Module', 'Expr'])
+
+
+# these nodes are low priority or won't ever be supported (e.g., AST)
+_unsupported_nodes = ((_stmt_nodes | _mod_nodes | _handler_nodes |
+                       _arguments_nodes | _keyword_nodes | _alias_nodes |
+                       _expr_context_nodes | frozenset(['Yield',
+                                                        'GeneratorExp',
+                                                        'IfExp', 'DictComp',
+                                                        'SetComp', 'Repr',
+                                                        'Lambda', 'Set', 'In',
+                                                        'NotIn', 'AST',
+                                                        'Is', 'IsNot'])) -
+                      _hacked_nodes)
+
+# we're adding a different assignment in some cases to be equality comparison
+# and we don't want `stmt` and friends in their so get only the class whose
+# names are capitalized
+_base_supported_nodes = (_all_node_names - _unsupported_nodes) | _hacked_nodes
+_msg = 'cannot both support and not support {0}'.format(_unsupported_nodes &
+                                                        _base_supported_nodes)
+assert not _unsupported_nodes & _base_supported_nodes, _msg
+
+
+def _node_not_implemented(node_name, cls):
+    def f(self, *args, **kwargs):
+        raise NotImplementedError("{0!r} nodes are not "
+                                  "implemented".format(node_name))
+    return f
+
+
+def disallow(nodes):
+    def disallowed(cls):
+        cls.unsupported_nodes = ()
+        for node in nodes:
+            new_method =  _node_not_implemented(node, cls)
+            name = 'visit_{0}'.format(node)
+            cls.unsupported_nodes += (name,)
+            setattr(cls, name, new_method)
+        return cls
+    return disallowed
+
+
+def _op_maker(op_class, op_symbol):
+    def f(self, node, *args, **kwargs):
+        return partial(op_class, op_symbol, *args, **kwargs)
+    return f
+
+
+_op_classes = {'binary': BinOp, 'unary': UnaryOp}
+
+def add_ops(op_classes):
+    def f(cls):
+        for op_attr_name, op_class in op_classes.iteritems():
+            ops = getattr(cls, '{0}_ops'.format(op_attr_name))
+            ops_map = getattr(cls, '{0}_op_nodes_map'.format(op_attr_name))
+            for op in ops:
+                setattr(cls, 'visit_{0}'.format(ops_map[op]),
+                        _op_maker(op_class, op))
+        return cls
+    return f
+
+
+@disallow(_unsupported_nodes)
+@add_ops(_op_classes)
+class BaseExprVisitor(ast.NodeVisitor):
+
     """Custom ast walker
     """
-    bin_ops = _cmp_ops_syms + _bool_ops_syms + _arith_ops_syms
-    bin_op_nodes = ('Gt', 'Lt', 'GtE', 'LtE', 'Eq', 'NotEq', None,
-                    'BitAnd', 'BitOr', 'Add', 'Sub', 'Mult', 'Div', 'Pow',
-                    'FloorDiv', 'Mod')
-    bin_op_nodes_map = dict(zip(bin_ops, bin_op_nodes))
+    binary_ops = _cmp_ops_syms + _bool_ops_syms + _arith_ops_syms
+    binary_op_nodes = ('Gt', 'Lt', 'GtE', 'LtE', 'Eq', 'NotEq', 'BitAnd',
+                       'BitOr', 'Add', 'Sub', 'Mult', 'Div', 'Pow', 'FloorDiv',
+                       'Mod')
+    binary_op_nodes_map = dict(itertools.izip(binary_ops, binary_op_nodes))
 
     unary_ops = _unary_ops_syms
     unary_op_nodes = 'UAdd', 'USub', 'Invert'
-    unary_op_nodes_map = dict(zip(unary_ops, unary_op_nodes))
+    unary_op_nodes_map = dict(itertools.izip(unary_ops, unary_op_nodes))
 
-    def __init__(self, env):
-        for bin_op in itertools.ifilter(lambda x: x is not None, self.bin_ops):
-            setattr(self, 'visit_{0}'.format(self.bin_op_nodes_map[bin_op]),
-                    lambda node, bin_op=bin_op: partial(BinOp, bin_op))
-
-        for unary_op in self.unary_ops:
-            setattr(self,
-                    'visit_{0}'.format(self.unary_op_nodes_map[unary_op]),
-                    lambda node, unary_op=unary_op: partial(UnaryOp, unary_op))
+    def __init__(self, env, preparser=_preparse):
         self.env = env
+        self.preparser = preparser
 
-    def visit(self, node):
-        if not (isinstance(node, ast.AST) or isinstance(node, basestring)):
-            raise TypeError('"node" must be an AST node or a string, you'
-                            ' passed a(n) {0}'.format(node.__class__))
+    def visit(self, node, **kwargs):
         if isinstance(node, basestring):
-            node = ast.fix_missing_locations(ast.parse(preparse(node)))
-        return super(ExprVisitor, self).visit(node)
+            node = ast.fix_missing_locations(ast.parse(self.preparser(node)))
 
-    def visit_Module(self, node):
-        if len(node.body) != 1:
-            raise ExprParserError('only a single expression is allowed')
+        method = 'visit_' + node.__class__.__name__
+        visitor = getattr(self, method, None)
+        return visitor(node, **kwargs)
 
+    def visit_Module(self, node, **kwargs):
+        if len(node.body) != 1:
+            raise SyntaxError('only a single expression is allowed')
         expr = node.body[0]
-        if not isinstance(expr, (ast.Expr, ast.Assign)):
-            raise SyntaxError('only expressions are allowed')
+        return self.visit(expr, **kwargs)
 
-        return self.visit(expr)
-
-    def visit_Expr(self, node):
-        return self.visit(node.value)
+    def visit_Expr(self, node, **kwargs):
+        return self.visit(node.value, **kwargs)
 
-    def visit_BinOp(self, node):
+    def visit_BinOp(self, node, **kwargs):
         op = self.visit(node.op)
-        left = self.visit(node.left)
-        right = self.visit(node.right)
+        left = self.visit(node.left, side='left')
+        right = self.visit(node.right, side='right')
         return op(left, right)
 
-    def visit_UnaryOp(self, node):
-        if isinstance(node.op, ast.Not):
-            raise NotImplementedError("not operator not yet supported")
+    def visit_UnaryOp(self, node, **kwargs):
         op = self.visit(node.op)
         return op(self.visit(node.operand))
 
-    def visit_Name(self, node):
+    def visit_Name(self, node, **kwargs):
         return Term(node.id, self.env)
 
-    def visit_Num(self, node):
+    def visit_Num(self, node, **kwargs):
         return Constant(node.n, self.env)
 
-    def visit_Compare(self, node):
-        ops = node.ops
-        comps = node.comparators
-        if len(ops) != 1:
-            raise ExprParserError('chained comparisons not supported')
-        return self.visit(ops[0])(self.visit(node.left), self.visit(comps[0]))
-
-    def visit_Assign(self, node):
-        cmpr = ast.copy_location(ast.Compare(ops=[ast.Eq()],
-                                             left=node.targets[0],
-                                             comparators=[node.value]), node)
+    def visit_Str(self, node, **kwargs):
+        return Constant(node.s, self.env)
+
+    def visit_List(self, node, **kwargs):
+        return Constant([self.visit(e).value for e in node.elts], self.env)
+
+    visit_Tuple = visit_List
+
+    def visit_Index(self, node, **kwargs):
+        """ df.index[4] """
+        return self.visit(node.value)
+
+
+    def visit_Subscript(self, node, **kwargs):
+        """ df.index[4:6] """
+        value = self.visit(node.value)
+        slobj = self.visit(node.slice)
+
+        try:
+            return Constant(value[slobj], self.env)
+        except TypeError:
+            raise ValueError("cannot subscript [{0}] with "
+                             "[{1}]".format(value, slobj))
+
+    def visit_Slice(self, node, **kwargs):
+        """ df.index[slice(4,6)] """
+        lower = node.lower
+        if lower is not None:
+            lower = self.visit(lower).value
+        upper = node.upper
+        if upper is not None:
+            upper = self.visit(upper).value
+        step = node.step
+        if step is not None:
+            step = self.visit(step).value
+
+        return slice(lower, upper, step)
+
+    def visit_Assign(self, node, **kwargs):
+        cmpr = ast.Compare(ops=[ast.Eq()], left=node.targets[0],
+                           comparators=[node.value])
         return self.visit(cmpr)
 
-    def visit_Call(self, node):
-        if not isinstance(node.func, ast.Name):
+    def visit_Attribute(self, node, **kwargs):
+        attr = node.attr
+        value = node.value
+
+        ctx = node.ctx.__class__
+        if ctx == ast.Load:
+            # resolve the value
+            return getattr(self.visit(value).value, attr)
+        raise ValueError("Invalid Attribute context {0}".format(ctx.__name__))
+
+    def visit_Call(self, node, **kwargs):
+
+        # this can happen with: datetime.datetime
+        if isinstance(node.func, ast.Attribute):
+            res = self.visit_Attribute(node.func)
+        elif not isinstance(node.func, ast.Name):
             raise TypeError("Only named functions are supported")
+        else:
+            res = self.visit(node.func)
+
+        if res is None:
+            raise ValueError("Invalid function call {0}".format(node.func.id))
+        if hasattr(res, 'value'):
+            res = res.value
+
+        args = [self.visit(targ).value for targ in node.args]
+        if node.starargs is not None:
+            args = args + self.visit(node.starargs).value
+
+        keywords = {}
+        for key in node.keywords:
+            if not isinstance(key, ast.keyword):
+                raise ValueError(
+                    "keyword error in function call '{0}'".format(node.func.id))
+            keywords[key.arg] = self.visit(key.value).value
+        if node.kwargs is not None:
+            keywords.update(self.visit(node.kwargs).value)
+
+        return Constant(res(*args, **keywords), self.env)
+
+    def visit_Compare(self, node, **kwargs):
+        ops = node.ops
+        comps = node.comparators
+        for op, comp in itertools.izip(ops, comps):
+            vop = self.visit(op)
+            node = vop(self.visit(node.left, side='left'),
+                       self.visit(comp, side='right'))
+        return node
 
-        valid_ops = _reductions + _mathops
 
-        if node.func.id not in valid_ops:
-            raise ValueError("Only {0} are supported".format(valid_ops))
+_numexpr_not_supported = frozenset(['Assign', 'BoolOp', 'Not', 'Str', 'Slice',
+                                    'Index', 'Subscript', 'Tuple', 'List',
+                                    'Dict', 'Call'])
+_numexpr_supported_calls = frozenset(_reductions + _mathops)
 
-        raise NotImplementedError("function calls not yet supported")
+@disallow(_unsupported_nodes | _numexpr_not_supported)
+class NumExprVisitor(BaseExprVisitor):
+    def __init__(self, env, preparser=None):
+        if preparser is not None:
+            raise ValueError("only strict numexpr syntax is supported")
+        preparser = lambda x: x
+        super(NumExprVisitor, self).__init__(env, preparser)
 
-    def visit_Attribute(self, node):
-        raise NotImplementedError("attribute access is not yet supported")
 
-    def visit_BoolOp(self, node):
-        raise NotImplementedError("boolean operators are not yet supported")
+_python_not_supported = _numexpr_not_supported
+
+@disallow(_unsupported_nodes | _python_not_supported)
+class PythonExprVisitor(BaseExprVisitor):
+    pass
 
 
 class Expr(StringMixin):
+
     """Expr object"""
+
     def __init__(self, expr, engine='numexpr', env=None, truediv=True):
         self.expr = expr
         self.env = env or Scope(frame_level=2)
-        self._visitor = ExprVisitor(self.env)
+        self._visitor = _visitors[engine](self.env)
         self.terms = self.parse()
         self.engine = engine
         self.truediv = truediv
@@ -167,6 +384,9 @@ class Expr(StringMixin):
     def __unicode__(self):
         return unicode(self.terms)
 
+    def __len__(self):
+        return len(self.expr)
+
     def parse(self):
         """return a Termset"""
         return self._visitor.visit(self.expr)
@@ -185,3 +405,6 @@ def isexpr(s, check_names=True):
         return not check_names
     else:
         return True
+
+
+_visitors = {'python': PythonExprVisitor, 'numexpr': NumExprVisitor}
diff --git a/pandas/computation/ops.py b/pandas/computation/ops.py
index ca5f6d487..b2dd638da 100644
--- a/pandas/computation/ops.py
+++ b/pandas/computation/ops.py
@@ -1,6 +1,8 @@
 import operator as op
 
 import numpy as np
+
+import pandas as pd
 from pandas.util.py3compat import PY3
 import pandas.core.common as com
 from pandas.core.base import StringMixin
@@ -25,36 +27,12 @@ class BinaryOperatorError(OperatorError):
     pass
 
 
-def _resolve_name(env, key):
-    res = env.locals.get(key, env.globals.get(key))
-
-    if res is None:
-        if not isinstance(key, basestring):
-            return key
-
-        raise NameError('name {0!r} is not defined'.format(key))
-
-    return res
-
-
-def _update_name(env, key, value):
-    if isinstance(key, basestring):
-        try:
-            del env.locals[key]
-            env.locals[key] = value
-        except KeyError:
-            try:
-                del env.globals[key]
-                env.globals[key] = value
-            except KeyError:
-                raise NameError('name {0!r} is not defined'.format(key))
-
-
 class Term(StringMixin):
-    def __init__(self, name, env):
+    def __init__(self, name, env, side=None):
         self.name = name
         self.env = env
-        self.value = _resolve_name(self.env, self.name)
+        self.side = side
+        self.value = self._resolve_name()
 
         try:
             # ndframe potentially very slow for large, mixed dtype frames
@@ -70,8 +48,39 @@ class Term(StringMixin):
     def __unicode__(self):
         return com.pprint_thing(self.name)
 
+    def _resolve_name(self):
+        env = self.env
+        key = self.name
+        res = env.resolver(key)
+        self.update(res)
+
+        if res is None:
+            if not isinstance(key, basestring):
+                return key
+            raise NameError('name {0!r} is not defined'.format(key))
+
+        if isinstance(res, pd.Panel):
+            raise NotImplementedError("Panel objects are not supported with "
+                                      "eval")
+        return res
+
     def update(self, value):
-        _update_name(self.env, self.name, value)
+        env = self.env
+        key = self.name
+        if isinstance(key, basestring):
+            try:
+                del env.locals[key]
+                env.locals[key] = value
+            except KeyError:
+                if key in env.resolver_keys:
+                    env.locals[key] = value
+                else:
+                    try:
+                        del env.globals[key]
+                        env.globals[key] = value
+                    except KeyError:
+                        raise NameError('{0!r} is undefined'.format(key))
+
         self.value = value
 
     @property
@@ -83,6 +92,9 @@ class Constant(Term):
     def __init__(self, value, env):
         super(Constant, self).__init__(value, env)
 
+    def _resolve_name(self):
+        return self.name
+
 
 def _print_operand(opr):
     return opr.name if is_term(opr) else unicode(opr)
@@ -91,7 +103,7 @@ def _print_operand(opr):
 class Op(StringMixin):
     """Hold an operator of unknown arity
     """
-    def __init__(self, op, operands):
+    def __init__(self, op, operands, *args, **kwargs):
         self.op = op
         self.operands = operands
 
@@ -114,8 +126,8 @@ class Op(StringMixin):
         return np.result_type(*(term.type for term in flatten(self)))
 
 
-_cmp_ops_syms = '>', '<', '>=', '<=', '==', '!=', '='
-_cmp_ops_funcs = op.gt, op.lt, op.ge, op.le, op.eq, op.ne, op.eq
+_cmp_ops_syms = '>', '<', '>=', '<=', '==', '!='
+_cmp_ops_funcs = op.gt, op.lt, op.ge, op.le, op.eq, op.ne
 _cmp_ops_dict = dict(zip(_cmp_ops_syms, _cmp_ops_funcs))
 
 _bool_ops_syms = '&', '|'
@@ -165,7 +177,7 @@ class BinOp(Op):
     left : str or Op
     right : str or Op
     """
-    def __init__(self, op, lhs, rhs):
+    def __init__(self, op, lhs, rhs, **kwargs):
         super(BinOp, self).__init__(op, (lhs, rhs))
         self.lhs = lhs
         self.rhs = rhs
@@ -208,8 +220,8 @@ class BinOp(Op):
 
 
 class Mod(BinOp):
-    def __init__(self, lhs, rhs):
-        super(Mod, self).__init__('%', lhs, rhs)
+    def __init__(self, lhs, rhs, *args, **kwargs):
+        super(Mod, self).__init__('%', lhs, rhs, *args, **kwargs)
         _cast_inplace(self.operands, np.float_)
 
 
diff --git a/pandas/computation/pytables.py b/pandas/computation/pytables.py
new file mode 100644
index 000000000..64a1036bb
--- /dev/null
+++ b/pandas/computation/pytables.py
@@ -0,0 +1,505 @@
+""" manage PyTables query interface via Expressions """
+
+import ast
+import time
+import warnings
+from functools import partial
+from datetime import datetime
+
+import numpy as np
+
+import pandas.core.common as com
+import pandas.lib as lib
+from pandas.computation import expr, ops
+from pandas.computation.ops import is_term, Constant
+from pandas.computation.expr import BaseExprVisitor
+from pandas import Index
+from pandas.core.common import is_list_like
+
+
+def _ensure_decoded(s):
+    """ if we have bytes, decode them to unicode """
+    if isinstance(s, (np.bytes_, bytes)):
+        s = s.decode('UTF-8')
+    return s
+
+
+class Scope(expr.Scope):
+    __slots__ = 'globals', 'locals', 'queryables'
+
+    def __init__(self, gbls=None, lcls=None, queryables=None, frame_level=1):
+        super(
+            Scope,
+            self).__init__(gbls=gbls,
+                           lcls=lcls,
+                           frame_level=frame_level)
+        self.queryables = queryables or dict()
+
+
+class Term(ops.Term):
+
+    def __init__(self, name, env, side=None):
+        super(Term, self).__init__(name, env, side=side)
+
+    def _resolve_name(self):
+
+        # must be a queryables
+        if self.side == 'left':
+            if self.name not in self.env.queryables:
+                raise NameError('name {0!r} is not defined'.format(self.name))
+            return self.name
+
+        # resolve the rhs (and allow to be None)
+        return self.env.locals.get(self.name,
+                                   self.env.globals.get(self.name, self.name))
+
+
+class BinOp(ops.BinOp):
+
+    _max_selectors = 31
+
+    def __init__(self, op, lhs, rhs, queryables, encoding):
+        super(BinOp, self).__init__(op, lhs, rhs)
+        self.queryables = queryables
+        self.encoding = encoding
+        self.filter = None
+        self.condition = None
+
+    def prune(self, klass):
+
+        def pr(left, right):
+            """ create and return a new specilized BinOp from myself """
+
+            if left is None:
+                return right
+            elif right is None:
+                return left
+
+            k = klass
+            if isinstance(left, ConditionBinOp):
+                if (isinstance(left, ConditionBinOp) and
+                    isinstance(right, ConditionBinOp)):
+                    k = JointConditionBinOp
+                elif isinstance(left, k):
+                    return left
+                elif isinstance(right, k):
+                    return right
+
+            elif isinstance(left, FilterBinOp):
+                if (isinstance(left, FilterBinOp) and
+                    isinstance(right, FilterBinOp)):
+                    k = JointFilterBinOp
+                elif isinstance(left, k):
+                    return left
+                elif isinstance(right, k):
+                    return right
+
+            return k(self.op, left, right, queryables=self.queryables,
+                     encoding=self.encoding).evaluate()
+
+        left, right = self.lhs, self.rhs
+
+        if is_term(left) and is_term(right):
+            res = pr(left.value, right.value)
+        elif not is_term(left) and is_term(right):
+            res = pr(left.prune(klass), right.value)
+        elif is_term(left) and not is_term(right):
+            res = pr(left.value, right.prune(klass))
+        elif not (is_term(left) or is_term(right)):
+            res = pr(left.prune(klass), right.prune(klass))
+
+        return res
+
+    def conform(self, rhs):
+        """ inplace conform rhs """
+        if not is_list_like(rhs):
+            rhs = [rhs]
+        if hasattr(self.rhs, 'ravel'):
+            rhs = rhs.ravel()
+        return rhs
+
+    @property
+    def is_valid(self):
+        """ return True if this is a valid field """
+        return self.lhs in self.queryables
+
+    @property
+    def is_in_table(self):
+        """ return True if this is a valid column name for generation (e.g. an
+        actual column in the table) """
+        return self.queryables.get(self.lhs) is not None
+
+    @property
+    def kind(self):
+        """ the kind of my field """
+        return self.queryables.get(self.lhs)
+
+    def generate(self, v):
+        """ create and return the op string for this TermValue """
+        val = v.tostring(self.encoding)
+        return "(%s %s %s)" % (self.lhs, self.op, val)
+
+    def convert_value(self, v):
+        """ convert the expression that is in the term to something that is
+        accepted by pytables """
+
+        def stringify(value):
+            value = str(value)
+            if self.encoding is not None:
+                value = value.encode(self.encoding)
+            return value
+
+        kind = _ensure_decoded(self.kind)
+        if kind == u'datetime64' or kind == u'datetime':
+
+            if isinstance(v, (int, float)):
+                v = stringify(v)
+            v = _ensure_decoded(v)
+            v = lib.Timestamp(v)
+            if v.tz is not None:
+                v = v.tz_convert('UTC')
+            return TermValue(v, v.value, kind)
+        elif isinstance(v, datetime) or hasattr(v, 'timetuple') or kind == u'date':
+            v = time.mktime(v.timetuple())
+            return TermValue(v, lib.Timestamp(v), kind)
+        elif kind == u'integer':
+            v = int(float(v))
+            return TermValue(v, v, kind)
+        elif kind == u'float':
+            v = float(v)
+            return TermValue(v, v, kind)
+        elif kind == u'bool':
+            if isinstance(v, basestring):
+                v = not v.strip().lower() in [u'false', u'f', u'no', u'n',
+                                              u'none', u'0', u'[]', u'{}', u'']
+            else:
+                v = bool(v)
+            return TermValue(v, v, kind)
+        elif not isinstance(v, basestring):
+            v = stringify(v)
+            return TermValue(v, stringify(v), u'string')
+
+        # string quoting
+        return TermValue(v, stringify(v), u'string')
+
+
+class FilterBinOp(BinOp):
+
+    def __unicode__(self):
+        return com.pprint_thing("[Filter : [{0}] -> "
+                                "[{1}]".format(self.filter[0], self.filter[1]))
+
+    def invert(self):
+        """ invert the filter """
+        if self.filter is not None:
+            f = list(self.filter)
+            f[1] = self.generate_filter_op(invert=True)
+            self.filter = tuple(f)
+        return self
+
+    def format(self):
+        """ return the actual filter format """
+        return [self.filter]
+
+    def evaluate(self):
+
+        if not isinstance(self.lhs, basestring):
+            return self
+
+        if not self.is_valid:
+            raise ValueError("query term is not valid [%s]" % self)
+
+        rhs = self.conform(self.rhs)
+        values = [TermValue(v, v, self.kind) for v in rhs]
+
+        if self.is_in_table:
+
+            # if too many values to create the expression, use a filter instead
+            if self.op in ['==', '!='] and len(values) > self._max_selectors:
+
+                filter_op = self.generate_filter_op()
+                self.filter = (
+                    self.lhs,
+                    filter_op,
+                    Index([v.value for v in values]))
+
+                return self
+            return None
+
+        # equality conditions
+        if self.op in ['==', '!=']:
+
+            filter_op = self.generate_filter_op()
+            self.filter = (
+                self.lhs,
+                filter_op,
+                Index([v.value for v in values]))
+
+        else:
+            raise TypeError(
+                "passing a filterable condition to a non-table indexer [%s]" %
+                self)
+
+        return self
+
+    def generate_filter_op(self, invert=False):
+        if (self.op == '!=' and not invert) or (self.op == '==' and invert):
+            return lambda axis, vals: ~axis.isin(vals)
+        else:
+            return lambda axis, vals: axis.isin(vals)
+
+
+class JointFilterBinOp(FilterBinOp):
+
+    def format(self):
+        raise NotImplementedError("unable to collapse Joint Filters")
+
+    def evaluate(self):
+        return self
+
+
+class ConditionBinOp(BinOp):
+
+    def __unicode__(self):
+        return com.pprint_thing("[Condition : [{0}]]".format(self.condition))
+
+    def invert(self):
+        """ invert the condition """
+        #if self.condition is not None:
+        #    self.condition = "~(%s)" % self.condition
+        #return self
+        raise NotImplementedError("cannot use an invert condition when passing to numexpr")
+
+    def format(self):
+        """ return the actual ne format """
+        return self.condition
+
+    def evaluate(self):
+
+        if not isinstance(self.lhs, basestring):
+            return self
+
+        if not self.is_valid:
+            raise ValueError("query term is not valid [%s]" % self)
+
+        # convert values if we are in the table
+        if not self.is_in_table:
+            return None
+
+        rhs = self.conform(self.rhs)
+        values = [self.convert_value(v) for v in rhs]
+
+        # equality conditions
+        if self.op in ['==', '!=']:
+
+            # too many values to create the expression?
+            if len(values) <= self._max_selectors:
+                vs = [self.generate(v) for v in values]
+                self.condition = "(%s)" % ' | '.join(vs)
+
+            # use a filter after reading
+            else:
+                return None
+        else:
+            self.condition = self.generate(values[0])
+
+        return self
+
+
+class JointConditionBinOp(ConditionBinOp):
+
+    def evaluate(self):
+        self.condition = "(%s %s %s)" % (
+            self.lhs.condition,
+            self.op,
+            self.rhs.condition)
+        return self
+
+
+class UnaryOp(ops.UnaryOp):
+
+    def prune(self, klass):
+
+        if self.op != '~':
+            raise NotImplementedError("UnaryOp only support invert type ops")
+
+        operand = self.operand
+        operand = operand.prune(klass)
+
+        if operand is not None:
+            if issubclass(klass,ConditionBinOp):
+                if operand.condition is not None:
+                    return operand.invert()
+            elif issubclass(klass,FilterBinOp):
+                if operand.filter is not None:
+                    return operand.invert()
+
+        return None
+
+
+
+_op_classes = {'unary': UnaryOp}
+
+
+class ExprVisitor(BaseExprVisitor):
+    def __init__(self, env, **kwargs):
+        super(ExprVisitor, self).__init__(env)
+        for bin_op in self.binary_ops:
+            setattr(self, 'visit_{0}'.format(self.binary_op_nodes_map[bin_op]),
+                    lambda node, bin_op=bin_op: partial(BinOp, bin_op,
+                                                        **kwargs))
+
+    def visit_Name(self, node, side=None, **kwargs):
+        return Term(node.id, self.env, side=side, **kwargs)
+
+    def visit_UnaryOp(self, node, **kwargs):
+        if isinstance(node.op, (ast.Not, ast.Invert)):
+            return UnaryOp('~', self.visit(node.operand))
+        elif isinstance(node.op, ast.USub):
+            return Constant(-self.visit(node.operand).value, self.env)
+        elif isinstance(node.op, ast.UAdd):
+            raise NotImplementedError('Unary addition not supported')
+
+    def visit_USub(self, node, **kwargs):
+        return Constant(-self.visit(node.operand).value, self.env)
+
+    def visit_Index(self, node, **kwargs):
+        return self.visit(node.value).value
+
+class Expr(expr.Expr):
+
+    """ hold a pytables like expression, comprised of possibly multiple 'terms'
+
+    Parameters
+    ----------
+    where : string term expression, Expr, or list-like of Exprs
+    queryables : a kinds map (dict of column name -> kind), or None i column is non-indexable
+    encoding : an encoding that will encode the query terms
+
+    Returns
+    -------
+    an Expr object
+
+    Examples
+    --------
+
+    'index>=date'
+    "columns=['A', 'D']"
+    'columns=A'
+    'columns==A'
+    "~(columns=['A','B'])"
+    'index>df.index[3] & string="bar"'
+    '(index>df.index[3] & index<=df.index[6]) | string="bar"'
+    "ts>=Timestamp('2012-02-01')"
+    "major_axis>=20130101"
+    """
+
+    def __init__(self, where, op=None, value=None, queryables=None,
+                 encoding=None, scope_level=None):
+
+        # try to be back compat
+        where = self.parse_back_compat(where, op, value)
+
+        self.encoding = encoding
+        self.condition = None
+        self.filter = None
+        self.terms = None
+        self._visitor = None
+
+        # capture the environement if needed
+        lcls = dict()
+        if isinstance(where, Expr):
+
+            lcls.update(where.env.locals)
+            where = str(where)
+
+        elif isinstance(where, (list, tuple)):
+
+            for w in where:
+                if isinstance(w, Expr):
+                    lcls.update(w.env.locals)
+                else:
+                    w = self.parse_back_compat(w)
+
+            where = ' & ' .join(["(%s)" % w for w in where])
+
+        self.expr = where
+        self.env = Scope(lcls=lcls)
+        self.env.update(scope_level)
+
+        if queryables is not None:
+            self.env.queryables.update(queryables)
+            self._visitor = ExprVisitor(self.env, queryables=queryables,
+                                        encoding=encoding)
+            self.terms = self.parse()
+
+    def parse_back_compat(self, w, op=None, value=None):
+        """ allow backward compatibility for passed arguments """
+
+        if isinstance(w, dict):
+            w, op, value = w.get('field'), w.get('op'), w.get('value')
+            if not isinstance(w, basestring):
+                raise TypeError(
+                    "where must be passed as a string if op/value are passed")
+            warnings.warn("passing a dict to Expr is deprecated, "
+                          "pass the where as a single string",
+                          DeprecationWarning)
+
+        if op is not None:
+            if not isinstance(w, basestring):
+                raise TypeError(
+                    "where must be passed as a string if op/value are passed")
+
+            if isinstance(op, Expr):
+                raise TypeError("invalid op passed, must be a string")
+            w = "{0}{1}".format(w, op)
+            if value is not None:
+                if isinstance(value, Expr):
+                    raise TypeError("invalid value passed, must be a string")
+                w = "{0}{1}".format(w, value)
+
+            warnings.warn("passing multiple values to Expr is deprecated, "
+                          "pass the where as a single string",
+                          DeprecationWarning)
+
+        return w
+
+    def __unicode__(self):
+        if self.terms is not None:
+            return unicode(self.terms)
+        return self.expr
+
+    def evaluate(self):
+        """ create and return the numexpr condition and filter """
+
+        try:
+            self.condition = self.terms.prune(ConditionBinOp)
+        except AttributeError:
+            raise ValueError(
+                "cannot process expression [{0}], [{1}] is not a valid condition".format(self.expr,self))
+        try:
+            self.filter = self.terms.prune(FilterBinOp)
+        except AttributeError:
+            raise ValueError(
+                "cannot process expression [{0}], [{1}] is not a valid filter".format(self.expr,self))
+
+        return self.condition, self.filter
+
+
+class TermValue(object):
+
+    """ hold a term value the we use to construct a condition/filter """
+
+    def __init__(self, value, converted, kind):
+        self.value = value
+        self.converted = converted
+        self.kind = kind
+
+    def tostring(self, encoding):
+        """ quote the string if not encoded
+            else encode and return """
+        if self.kind == u'string':
+            if encoding is not None:
+                return self.converted
+            return '"%s"' % self.converted
+        return self.converted
diff --git a/pandas/computation/tests/test_eval.py b/pandas/computation/tests/test_eval.py
old mode 100644
new mode 100755
index 6ec630b80..fa96342ec
--- a/pandas/computation/tests/test_eval.py
+++ b/pandas/computation/tests/test_eval.py
@@ -3,10 +3,11 @@
 import unittest
 import itertools
 from itertools import product
+import ast
 
 import nose
 from nose.tools import assert_raises, assert_tuple_equal
-from nose.tools import assert_true, assert_false
+from nose.tools import assert_true, assert_false, assert_equal
 
 from numpy.random import randn, rand
 import numpy as np
@@ -15,12 +16,14 @@ from numpy.testing.decorators import slow
 
 import pandas as pd
 from pandas.core import common as com
-from pandas import DataFrame, Series
+from pandas import DataFrame, Series, Panel
 from pandas.util.testing import makeCustomDataframe as mkdf
 from pandas.computation.engines import _engines, _reconstruct_object
 from pandas.computation.align import _align_core
+from pandas.computation.expr import NumExprVisitor, PythonExprVisitor
 from pandas.computation.ops import _binary_ops_dict, _unary_ops_dict, Term
 import pandas.computation.expr as expr
+from pandas.computation import pytables
 from pandas.computation.expressions import _USE_NUMEXPR
 from pandas.computation.eval import Scope
 from pandas.util.testing import assert_frame_equal, randbool
@@ -96,7 +99,6 @@ def _series_and_2d_ndarray(lhs, rhs):
             > 1)
 
 
-# Smoke testing
 class TestBasicEval(unittest.TestCase):
 
     @classmethod
@@ -645,6 +647,86 @@ def test_or_fails():
         check_or_fails(engine)
 
 
+_visitors = {'numexpr': NumExprVisitor, 'python': PythonExprVisitor,
+             'pytables': pytables.ExprVisitor}
+
+
+def check_disallowed_nodes(engine):
+    """make sure the disallowed decorator works"""
+    VisitorClass = _visitors[engine]
+    uns_ops = VisitorClass.unsupported_nodes
+    inst = VisitorClass('x + 1')
+    for ops in uns_ops:
+        assert_raises(NotImplementedError, getattr(inst, ops), inst, ast.AST())
+
+
+def test_disallowed_nodes():
+    for engine in ('pytables', 'numexpr', 'python'):
+        check_disallowed_nodes(engine)
+
+
+def check_simple_ops(engine):
+    ops = '+', '*', '/', '-', '%', '**'
+
+    for op in ops:
+        expec = _eval_single_bin(1, op, 1, engine_has_neg_frac(engine))
+        x = pd.eval('1 {0} 1'.format(op), engine=engine)
+        assert_equal(x, expec)
+
+        expec = _eval_single_bin(x, op, 1, engine_has_neg_frac(engine))
+        y = pd.eval('x {0} 1'.format(op), engine=engine)
+        assert_equal(y, expec)
+
+        expec = _eval_single_bin(1, op, x + 1, engine_has_neg_frac(engine))
+        y = pd.eval('1 {0} (x + 1)'.format(op), engine=engine)
+        assert_equal(y, expec)
+
+
+def test_simple_ops():
+    for engine in _engines:
+        check_simple_ops(engine)
+
+
+def check_no_new_locals(engine):
+    x = 1
+    lcls = locals().copy()
+    pd.eval('x + 1', local_dict=lcls)
+    lcls2 = locals().copy()
+    lcls2.pop('lcls')
+    assert_equal(lcls, lcls2)
+
+
+def test_no_new_locals():
+    for engine in _engines:
+        check_no_new_locals(engine)
+
+
+def check_no_new_globals(engine):
+    x = 1
+    gbls = globals().copy()
+    pd.eval('x + 1')
+    gbls2 = globals().copy()
+    assert_equal(gbls, gbls2)
+
+
+def test_no_new_globals():
+    for engine in _engines:
+        check_no_new_globals(engine)
+
+
+def check_panel_fails(engine):
+    x = Panel(randn(3, 4, 5))
+    y = Series(randn(10))
+    assert_raises(NotImplementedError, pd.eval, 'x + y', local_dict={'x': x,
+                                                                     'y': y},
+                  engine=engine)
+
+
+def test_panel_fails():
+    for engine in _engines:
+        check_panel_fails(engine)
+
+
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
                    exit=False)
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index c957ec9d3..59145cd54 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -28,15 +28,16 @@ from pandas.core.common import (isnull, notnull, PandasError, _try_sort,
                                 _coerce_to_dtypes, _DATELIKE_DTYPES, is_list_like)
 from pandas.core.generic import NDFrame
 from pandas.core.index import Index, MultiIndex, _ensure_index
-from pandas.core.indexing import (_NDFrameIndexer, _maybe_droplevels,
-                                  _convert_to_index_sliceable, _check_bool_indexer,
-                                  _maybe_convert_indices)
+from pandas.core.indexing import (_maybe_droplevels,
+                                  _convert_to_index_sliceable,
+                                  _check_bool_indexer, _maybe_convert_indices)
 from pandas.core.internals import (BlockManager,
                                    create_block_manager_from_arrays,
                                    create_block_manager_from_blocks)
 from pandas.core.series import Series, _radd_compat
 from pandas.sparse.array import SparseArray
 import pandas.computation.expressions as expressions
+from pandas.computation.eval import eval as _eval
 from pandas.compat.scipy import scoreatpercentile as _quantile
 from pandas.compat import(range, zip, lrange, lmap, lzip, StringIO, u,
                           OrderedDict, raise_with_traceback)
@@ -55,7 +56,6 @@ import pandas.core.generic as generic
 import pandas.core.nanops as nanops
 
 import pandas.lib as lib
-import pandas.tslib as tslib
 import pandas.algos as _algos
 
 from pandas.core.config import get_option, set_option
@@ -1898,6 +1898,18 @@ class DataFrame(NDFrame):
             raise ValueError('Must pass DataFrame with boolean values only')
         return self.where(key)
 
+    def query(self, expr, **kwargs):
+        resolvers = kwargs.get('resolvers', None)
+        if resolvers is None:
+            index_resolvers = {}
+            if self.index.name is not None:
+                index_resolvers[self.index.name] = self.index
+            index_resolvers.update({'index': self.index,
+                                    'columns': self.columns})
+            resolvers = [self, index_resolvers]
+            kwargs.update({'resolvers': resolvers})
+        return self[_eval(expr, **kwargs)]
+
     def _slice(self, slobj, axis=0, raise_on_error=False):
         axis = self._get_block_manager_axis(axis)
         new_data = self._data.get_slice(
@@ -4599,6 +4611,7 @@ class DataFrame(NDFrame):
 DataFrame._setup_axes(
     ['index', 'columns'], info_axis=1, stat_axis=0, axes_are_reversed=True)
 
+
 _EMPTY_SERIES = Series([])
 
 
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index 6e7f72195..3695a994b 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -34,6 +34,7 @@ import pandas.core.common as com
 from pandas.tools.merge import concat
 from pandas.io.common import PerformanceWarning
 from pandas.core.config import get_option
+from pandas.computation.pytables import Expr
 
 import pandas.lib as lib
 import pandas.algos as algos
@@ -62,6 +63,21 @@ def _ensure_encoding(encoding):
             encoding = _default_encoding
     return encoding
 
+Term = Expr
+
+def _ensure_term(where):
+    """ ensure that the where is a Term or a list of Term
+        this makes sure that we are capturing the scope of variables
+        that are passed """
+
+    # create the terms here with a frame_level=2 (we are 2 levels down)
+    if isinstance(where, (list, tuple)):
+        where = [ w if isinstance(w, Term) else Term(w, scope_level=2) for w in where if w is not None ]
+    elif where is None or isinstance(where, Coordinates):
+        pass
+    elif not isinstance(where, Term):
+        where = Term(where, scope_level=2)
+    return where
 
 class PossibleDataLossError(Exception):
     pass
@@ -556,6 +572,7 @@ class HDFStore(StringMixin):
             raise KeyError('No object named %s in the file' % key)
 
         # create the storer and axes
+        where = _ensure_term(where)
         s = self._create_storer(group)
         s.infer_axes()
 
@@ -587,6 +604,7 @@ class HDFStore(StringMixin):
         start : integer (defaults to None), row number to start selection
         stop  : integer (defaults to None), row number to stop selection
         """
+        where = _ensure_term(where)
         return self.get_storer(key).read_coordinates(where=where, start=start, stop=stop, **kwargs)
 
     def unique(self, key, column, **kwargs):
@@ -632,6 +650,7 @@ class HDFStore(StringMixin):
         """
 
         # default to single select
+        where = _ensure_term(where)
         if isinstance(keys, (list, tuple)) and len(keys) == 1:
             keys = keys[0]
         if isinstance(keys, compat.string_types):
@@ -735,6 +754,7 @@ class HDFStore(StringMixin):
         raises KeyError if key is not a valid store
 
         """
+        where = _ensure_term(where)
         try:
             s = self.get_storer(key)
         except:
@@ -3070,8 +3090,8 @@ class Table(Fixed):
             obj = _reindex_axis(obj, axis, labels, columns)
 
         # apply the selection filters (but keep in the same order)
-        if self.selection.filter:
-            for field, op, filt in self.selection.filter:
+        if self.selection.filter is not None:
+            for field, op, filt in self.selection.filter.format():
 
                 def process_filter(field, filt):
 
@@ -4211,15 +4231,8 @@ class Selection(object):
             self.terms = self.generate(where)
 
             # create the numexpr & the filter
-            if self.terms:
-                terms = [t for t in self.terms if t.condition is not None]
-                if len(terms):
-                    self.condition = "(%s)" % ' & '.join(
-                        [t.condition for t in terms])
-                self.filter = []
-                for t in self.terms:
-                    if t.filter is not None:
-                        self.filter.append(t.filter)
+            if self.terms is not None:
+                self.condition, self.filter = self.terms.evaluate()
 
     def generate(self, where):
         """ where can be a : dict,list,tuple,string """
@@ -4245,7 +4258,7 @@ class Selection(object):
         generate the selection
         """
         if self.condition is not None:
-            return self.table.table.readWhere(self.condition, start=self.start, stop=self.stop)
+            return self.table.table.readWhere(self.condition.format(), start=self.start, stop=self.stop)
         elif self.coordinates is not None:
             return self.table.table.readCoordinates(self.coordinates)
         return self.table.table.read(start=self.start, stop=self.stop)
@@ -4257,7 +4270,7 @@ class Selection(object):
         if self.condition is None:
             return np.arange(self.table.nrows)
 
-        return self.table.table.getWhereList(self.condition, start=self.start, stop=self.stop, sort=True)
+        return self.table.table.getWhereList(self.condition.format(), start=self.start, stop=self.stop, sort=True)
 
 
 # utilities ###
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index 6a325db8a..88173c001 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -81,6 +81,7 @@ def _maybe_remove(store, key):
     except:
         pass
 
+
 def compat_assert_produces_warning(w,f):
     """ don't produce a warning under PY3 """
     if compat.PY3:
@@ -89,6 +90,7 @@ def compat_assert_produces_warning(w,f):
         with tm.assert_produces_warning(expected_warning=w):
             f()
 
+
 class TestHDFStore(unittest.TestCase):
 
     def setUp(self):
@@ -328,8 +330,8 @@ class TestHDFStore(unittest.TestCase):
             self.assert_('bar' not in store)
 
             # GH 2694
-            with tm.assert_produces_warning(expected_warning=tables.NaturalNameWarning):
-                store['node())'] = tm.makeDataFrame()
+            warnings.filterwarnings('ignore', category=tables.NaturalNameWarning)
+            store['node())'] = tm.makeDataFrame()
             self.assert_('node())' in store)
 
     def test_versioning(self):
@@ -886,16 +888,16 @@ class TestHDFStore(unittest.TestCase):
             expected = df.reindex(columns=['A'])
             tm.assert_frame_equal(expected, result)
 
-            # this isn't supported
-            self.assertRaises(TypeError, store.select, 'df1', (
-                    'columns=A', Term('index', '>', df.index[4])))
-
             # selection on the non-indexable
             result = store.select(
-                'df1', ('columns=A', Term('index', '=', df.index[0:4])))
+                'df1', ('columns=A', Term('index=df.index[0:4]')))
             expected = df.reindex(columns=['A'], index=df.index[0:4])
             tm.assert_frame_equal(expected, result)
 
+            # this isn't supported
+            self.assertRaises(TypeError, store.select, 'df1', (
+                    'columns=A', Term('index>df.index[4]')))
+
     def test_append_with_different_block_ordering(self):
 
         #GH 4096; using same frames, but different block orderings
@@ -1133,7 +1135,7 @@ class TestHDFStore(unittest.TestCase):
 
             # data column searching (with an indexable and a data_columns)
             result = store.select(
-                'df', [Term('B>0'), Term('index', '>', df.index[3])])
+                'df', [Term('B>0'), Term('index>df.index[3]')])
             df_new = df.reindex(index=df.index[4:])
             expected = df_new[df_new.B > 0]
             tm.assert_frame_equal(result, expected)
@@ -1145,7 +1147,7 @@ class TestHDFStore(unittest.TestCase):
             df_new['string'][5:6] = 'bar'
             _maybe_remove(store, 'df')
             store.append('df', df_new, data_columns=['string'])
-            result = store.select('df', [Term('string', '=', 'foo')])
+            result = store.select('df', [Term('string=foo')])
             expected = df_new[df_new.string == 'foo']
             tm.assert_frame_equal(result, expected)
 
@@ -1191,14 +1193,14 @@ class TestHDFStore(unittest.TestCase):
             _maybe_remove(store, 'df')
             store.append(
                 'df', df_new, data_columns=['A', 'B', 'string', 'string2'])
-            result = store.select('df', [Term('string', '=', 'foo'), Term(
+            result = store.select('df', [Term('string=foo'), Term(
                         'string2=foo'), Term('A>0'), Term('B<0')])
             expected = df_new[(df_new.string == 'foo') & (
                     df_new.string2 == 'foo') & (df_new.A > 0) & (df_new.B < 0)]
             tm.assert_frame_equal(result, expected)
 
             # yield an empty frame
-            result = store.select('df', [Term('string', '=', 'foo'), Term(
+            result = store.select('df', [Term('string=foo'), Term(
                         'string2=cool')])
             expected = df_new[(df_new.string == 'foo') & (
                     df_new.string2 == 'cool')]
@@ -1759,7 +1761,7 @@ class TestHDFStore(unittest.TestCase):
             assert_frame_equal(result,df)
 
             # select with tz aware
-            compare(store.select('df_tz',where=Term('A','>=',df.A[3])),df[df.A>=df.A[3]])
+            compare(store.select('df_tz',where=Term('A>=df.A[3]')),df[df.A>=df.A[3]])
 
             _maybe_remove(store, 'df_tz')
             df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130103',tz='US/Eastern')),index=lrange(5))
@@ -1927,14 +1929,14 @@ class TestHDFStore(unittest.TestCase):
         with ensure_clean(self.path) as store:
 
             # non-existance
-            crit1 = Term('index', '>', 'foo')
+            crit1 = Term('index>foo')
             self.assertRaises(KeyError, store.remove, 'a', [crit1])
 
             # try to remove non-table (with crit)
             # non-table ok (where = None)
             wp = tm.makePanel()
-            store.put('wp', wp, format='t')
-            store.remove('wp', [('minor_axis', ['A', 'D'])])
+            store.put('wp', wp, fmt='t')
+            store.remove('wp', [("minor_axis=['A', 'D']")])
             rs = store.select('wp')
             expected = wp.reindex(minor_axis=['B', 'C'])
             assert_panel_equal(rs, expected)
@@ -1966,8 +1968,8 @@ class TestHDFStore(unittest.TestCase):
 
             # group row removal
             date4 = wp.major_axis.take([0, 1, 2, 4, 5, 6, 8, 9, 10])
-            crit4 = Term('major_axis', date4)
-            store.put('wp3', wp, format='table')
+            crit4 = Term('major_axis=date4')
+            store.put('wp3', wp, format='t')
             n = store.remove('wp3', where=[crit4])
             assert(n == 36)
             result = store.select('wp3')
@@ -1978,8 +1980,8 @@ class TestHDFStore(unittest.TestCase):
             store.put('wp', wp, format='table')
             date = wp.major_axis[len(wp.major_axis) // 2]
 
-            crit1 = Term('major_axis', '>', date)
-            crit2 = Term('minor_axis', ['A', 'D'])
+            crit1 = Term('major_axis>date')
+            crit2 = Term("minor_axis=['A', 'D']")
             n = store.remove('wp', where=[crit1])
 
             assert(n == 56)
@@ -1995,14 +1997,14 @@ class TestHDFStore(unittest.TestCase):
             store.put('wp2', wp, format='table')
 
             date1 = wp.major_axis[1:3]
-            crit1 = Term('major_axis', date1)
+            crit1 = Term('major_axis=date1')
             store.remove('wp2', where=[crit1])
             result = store.select('wp2')
             expected = wp.reindex(major_axis=wp.major_axis - date1)
             assert_panel_equal(result, expected)
 
             date2 = wp.major_axis[5]
-            crit2 = Term('major_axis', date2)
+            crit2 = Term('major_axis=date2')
             store.remove('wp2', where=[crit2])
             result = store['wp2']
             expected = wp.reindex(
@@ -2010,7 +2012,7 @@ class TestHDFStore(unittest.TestCase):
             assert_panel_equal(result, expected)
 
             date3 = [wp.major_axis[7], wp.major_axis[9]]
-            crit3 = Term('major_axis', date3)
+            crit3 = Term('major_axis=date3')
             store.remove('wp2', where=[crit3])
             result = store['wp2']
             expected = wp.reindex(
@@ -2020,62 +2022,94 @@ class TestHDFStore(unittest.TestCase):
             # corners
             store.put('wp4', wp, format='table')
             n = store.remove(
-                'wp4', where=[Term('major_axis', '>', wp.major_axis[-1])])
+                'wp4', where=[Term('major_axis>wp.major_axis[-1]')])
             result = store.select('wp4')
             assert_panel_equal(result, wp)
 
-    def test_terms(self):
+    def test_invalid_terms(self):
 
         with ensure_clean(self.path) as store:
 
+            df = tm.makeTimeDataFrame()
+            df['string'] = 'foo'
+            df.ix[0:4,'string'] = 'bar'
             wp = tm.makePanel()
             p4d = tm.makePanel4D()
             store.put('wp', wp, format='table')
             store.put('p4d', p4d, format='table')
 
             # some invalid terms
-            terms = [
-                ['minor', ['A', 'B']],
-                ['index', ['20121114']],
-                ['index', ['20121114', '20121114']],
-                ]
-            for t in terms:
-                self.assertRaises(Exception, store.select, 'wp', t)
+            self.assertRaises(NameError, store.select, 'wp', "minor=['A', 'B']")
+            self.assertRaises(NameError, store.select, 'wp', ["index=['20121114']"])
+            self.assertRaises(NameError, store.select, 'wp', ["index=['20121114', '20121114']"])
+
+            # deprecations
+            with tm.assert_produces_warning(expected_warning=DeprecationWarning):
+                Term('index','==')
 
-            self.assertRaises(Exception, Term.__init__)
-            self.assertRaises(Exception, Term.__init__, 'blah')
-            self.assertRaises(Exception, Term.__init__, 'index')
-            self.assertRaises(Exception, Term.__init__, 'index', '==')
-            self.assertRaises(Exception, Term.__init__, 'index', '>', 5)
+            with tm.assert_produces_warning(expected_warning=DeprecationWarning):
+                Term('index', '>', 5)
+
+            self.assertRaises(TypeError, Term)
+
+            # more invalid
+            self.assertRaises(ValueError,  store.select, 'df','df.index[3]')
+            self.assertRaises(SyntaxError, store.select, 'df','index>')
+            self.assertRaises(ValueError,  store.select, 'wp', "major_axis<'20000108' & minor_axis['A', 'B']")
+
+    def test_terms(self):
+
+        with ensure_clean(self.path) as store:
+
+            wp = tm.makePanel()
+            p4d = tm.makePanel4D()
+            store.put('wp', wp, table=True)
+            store.put('p4d', p4d, table=True)
 
             # panel
             result = store.select('wp', [Term(
-                        'major_axis<20000108'), Term('minor_axis', '=', ['A', 'B'])])
+                        'major_axis<"20000108"'), Term("minor_axis=['A', 'B']")])
             expected = wp.truncate(after='20000108').reindex(minor=['A', 'B'])
             assert_panel_equal(result, expected)
 
+            # with deprecation
+            with tm.assert_produces_warning(expected_warning=DeprecationWarning):
+                result = store.select('wp', [Term(
+                    'major_axis','<',"20000108"), Term("minor_axis=['A', 'B']")])
+                expected = wp.truncate(after='20000108').reindex(minor=['A', 'B'])
+                tm.assert_panel_equal(result, expected)
+
             # p4d
-            result = store.select('p4d', [Term('major_axis<20000108'),
-                                          Term('minor_axis', '=', ['A', 'B']),
-                                          Term('items', '=', ['ItemA', 'ItemB'])])
+            result = store.select('p4d', [Term('major_axis<"20000108"'),
+                                          Term("minor_axis=['A', 'B']"),
+                                          Term("items=['ItemA', 'ItemB']")])
             expected = p4d.truncate(after='20000108').reindex(
                 minor=['A', 'B'], items=['ItemA', 'ItemB'])
             assert_panel4d_equal(result, expected)
 
-            # valid terms
+            # back compat invalid terms
             terms = [
                 dict(field='major_axis', op='>', value='20121114'),
-                ('major_axis', '20121114'),
-                ('major_axis', '>', '20121114'),
-                (('major_axis', ['20121114', '20121114']),),
-                ('major_axis', datetime.datetime(2012, 11, 14)),
+                [ dict(field='major_axis', op='>', value='20121114') ],
+                [ "minor_axis=['A','B']", dict(field='major_axis', op='>', value='20121114') ]
+                ]
+            for t in terms:
+                with tm.assert_produces_warning(expected_warning=DeprecationWarning):
+                    Term(t)
+
+            # valid terms
+            terms = [
+                ('major_axis=20121114'),
+                ('major_axis>20121114'),
+                (("major_axis=['20121114', '20121114']"),),
+                ('major_axis=datetime.datetime(2012, 11, 14)'),
                 'major_axis> 20121114',
                 'major_axis >20121114',
                 'major_axis > 20121114',
-                (('minor_axis', ['A', 'B']),),
-                (('minor_axis', ['A', 'B']),),
-                ((('minor_axis', ['A', 'B']),),),
-                (('items', ['ItemA', 'ItemB']),),
+                (("minor_axis=['A', 'B']"),),
+                (("minor_axis=['A', 'B']"),),
+                ((("minor_axis==['A', 'B']"),),),
+                (("items=['ItemA', 'ItemB']"),),
                 ('items=ItemA'),
                 ]
 
@@ -2085,8 +2119,8 @@ class TestHDFStore(unittest.TestCase):
 
             # valid for p4d only
             terms = [
-                (('labels', '=', ['l1', 'l2']),),
-                Term('labels', '=', ['l1', 'l2']),
+                (("labels=['l1', 'l2']"),),
+                Term("labels=['l1', 'l2']"),
                 ]
 
             for t in terms:
@@ -2211,7 +2245,7 @@ class TestHDFStore(unittest.TestCase):
         self._check_roundtrip(ser, func)
 
         ser = Series(values, [datetime.datetime(
-                    2012, 1, 1), datetime.datetime(2012, 1, 2)])
+            2012, 1, 1), datetime.datetime(2012, 1, 2)])
         self._check_roundtrip(ser, func)
 
     def test_timeseries_preepoch(self):
@@ -2525,7 +2559,7 @@ class TestHDFStore(unittest.TestCase):
             _maybe_remove(store, 'wp')
             store.append('wp', wp)
             items = ['Item%03d' % i for i in range(80)]
-            result = store.select('wp', Term('items', items))
+            result = store.select('wp', Term('items=items'))
             expected = wp.reindex(items=items)
             assert_panel_equal(expected, result)
 
@@ -2542,7 +2576,7 @@ class TestHDFStore(unittest.TestCase):
             tm.assert_frame_equal(expected, result)
 
             # equivalentsly
-            result = store.select('df', [('columns', ['A', 'B'])])
+            result = store.select('df', [("columns=['A', 'B']")])
             expected = df.reindex(columns=['A', 'B'])
             tm.assert_frame_equal(expected, result)
 
@@ -2575,7 +2609,8 @@ class TestHDFStore(unittest.TestCase):
             df = DataFrame(dict(ts=bdate_range('2012-01-01', periods=300), A=np.random.randn(300)))
             _maybe_remove(store, 'df')
             store.append('df', df, data_columns=['ts', 'A'])
-            result = store.select('df', [Term('ts', '>=', Timestamp('2012-02-01'))])
+
+            result = store.select('df', [Term("ts>=Timestamp('2012-02-01')")])
             expected = df[df.ts >= Timestamp('2012-02-01')]
             tm.assert_frame_equal(expected, result)
 
@@ -2602,7 +2637,7 @@ class TestHDFStore(unittest.TestCase):
             _maybe_remove(store, 'df_int')
             store.append('df_int', df)
             result = store.select(
-                'df_int', [Term("index<10"), Term("columns", "=", ["A"])])
+                'df_int', [Term("index<10"), Term("columns=['A']")])
             expected = df.reindex(index=list(df.index)[0:10],columns=['A'])
             tm.assert_frame_equal(expected, result)
 
@@ -2612,7 +2647,7 @@ class TestHDFStore(unittest.TestCase):
             _maybe_remove(store, 'df_float')
             store.append('df_float', df)
             result = store.select(
-                'df_float', [Term("index<10.0"), Term("columns", "=", ["A"])])
+                'df_float', [Term("index<10.0"), Term("columns=['A']")])
             expected = df.reindex(index=list(df.index)[0:10],columns=['A'])
             tm.assert_frame_equal(expected, result)
 
@@ -2628,30 +2663,30 @@ class TestHDFStore(unittest.TestCase):
             store.append('df', df, data_columns=['ts', 'A', 'B', 'users'])
 
             # regular select
-            result = store.select('df', [Term('ts', '>=', Timestamp('2012-02-01'))])
+            result = store.select('df', [Term("ts>=Timestamp('2012-02-01')")])
             expected = df[df.ts >= Timestamp('2012-02-01')]
             tm.assert_frame_equal(expected, result)
 
             # small selector
-            result = store.select('df', [Term('ts', '>=', Timestamp('2012-02-01')),Term('users',['a','b','c'])])
+            result = store.select('df', [Term("ts>=Timestamp('2012-02-01') & users=['a','b','c']")])
             expected = df[ (df.ts >= Timestamp('2012-02-01')) & df.users.isin(['a','b','c']) ]
             tm.assert_frame_equal(expected, result)
 
             # big selector along the columns
             selector = [ 'a','b','c' ] + [ 'a%03d' % i for i in range(60) ]
-            result = store.select('df', [Term('ts', '>=', Timestamp('2012-02-01')),Term('users',selector)])
+            result = store.select('df', [Term("ts>=Timestamp('2012-02-01')"),Term('users=selector')])
             expected = df[ (df.ts >= Timestamp('2012-02-01')) & df.users.isin(selector) ]
             tm.assert_frame_equal(expected, result)
 
             selector = lrange(100,200)
-            result = store.select('df', [Term('B', selector)])
+            result = store.select('df', [Term('B=selector')])
             expected = df[ df.B.isin(selector) ]
             tm.assert_frame_equal(expected, result)
             self.assert_(len(result) == 100)
 
             # big selector along the index
             selector = Index(df.ts[0:100].values)
-            result  = store.select('df', [Term('ts', selector)])
+            result  = store.select('df', [Term('ts=selector')])
             expected = df[ df.ts.isin(selector.values) ]
             tm.assert_frame_equal(expected, result)
             self.assert_(len(result) == 100)
@@ -2807,15 +2842,15 @@ class TestHDFStore(unittest.TestCase):
             store.put('wp', wp, format='table')
             date = wp.major_axis[len(wp.major_axis) // 2]
 
-            crit1 = ('major_axis', '>=', date)
-            crit2 = ('minor_axis', '=', ['A', 'D'])
+            crit1 = ('major_axis>=date')
+            crit2 = ("minor_axis=['A', 'D']")
 
             result = store.select('wp', [crit1, crit2])
             expected = wp.truncate(before=date).reindex(minor=['A', 'D'])
             assert_panel_equal(result, expected)
 
             result = store.select(
-                'wp', ['major_axis>=20000124', ('minor_axis', '=', ['A', 'B'])])
+                'wp', ['major_axis>="20000124"', ("minor_axis=['A', 'B']")])
             expected = wp.truncate(before='20000124').reindex(minor=['A', 'B'])
             assert_panel_equal(result, expected)
 
@@ -2827,9 +2862,9 @@ class TestHDFStore(unittest.TestCase):
             store.put('frame', df,format='table')
             date = df.index[len(df) // 2]
 
-            crit1 = ('index', '>=', date)
-            crit2 = ('columns', ['A', 'D'])
-            crit3 = ('columns', 'A')
+            crit1 = Term('index>=date')
+            crit2 = ("columns=['A', 'D']")
+            crit3 = ('columns=A')
 
             result = store.select('frame', [crit1, crit2])
             expected = df.ix[date:, ['A', 'D']]
@@ -2850,6 +2885,62 @@ class TestHDFStore(unittest.TestCase):
             # self.assertRaises(ValueError, store.select,
             #                  'frame', [crit1, crit2])
 
+    def test_frame_select_complex(self):
+        """ select via complex criteria """
+
+        df = tm.makeTimeDataFrame()
+        df['string'] = 'foo'
+        df.loc[df.index[0:4],'string'] = 'bar'
+
+        with ensure_clean(self.path) as store:
+            store.put('df', df, table=True, data_columns=['string'])
+
+            # empty
+            result = store.select('df', 'index>df.index[3] & string="bar"')
+            expected = df.loc[(df.index>df.index[3]) & (df.string=='bar')]
+            tm.assert_frame_equal(result, expected)
+
+            result = store.select('df', 'index>df.index[3] & string="foo"')
+            expected = df.loc[(df.index>df.index[3]) & (df.string=='foo')]
+            tm.assert_frame_equal(result, expected)
+
+            # or
+            result = store.select('df', 'index>df.index[3] | string="bar"')
+            expected = df.loc[(df.index>df.index[3]) | (df.string=='bar')]
+            tm.assert_frame_equal(result, expected)
+
+            result = store.select('df', '(index>df.index[3] & index<=df.index[6]) | string="bar"')
+            expected = df.loc[((df.index>df.index[3]) & (df.index<=df.index[6])) | (df.string=='bar')]
+            tm.assert_frame_equal(result, expected)
+
+            # invert
+            result = store.select('df', 'string!="bar"')
+            expected = df.loc[df.string!='bar']
+            tm.assert_frame_equal(result, expected)
+
+            # invert not implemented in numexpr :(
+            self.assertRaises(NotImplementedError, store.select, 'df', '~(string="bar")')
+
+            # invert ok for filters
+            result = store.select('df', "~(columns=['A','B'])")
+            expected = df.loc[:,df.columns-['A','B']]
+            tm.assert_frame_equal(result, expected)
+
+    def test_invalid_filtering(self):
+
+        # can't use more than one filter (atm)
+
+        df = tm.makeTimeDataFrame()
+
+        with ensure_clean(self.path) as store:
+            store.put('df', df, table=True)
+
+            # not implemented
+            self.assertRaises(NotImplementedError, store.select, 'df', "columns=['A'] | columns=['B']")
+
+            # in theory we could deal with this
+            self.assertRaises(NotImplementedError, store.select, 'df', "columns=['A','B'] & columns=['C']")
+
     def test_string_select(self):
 
         # GH 2973
@@ -3121,12 +3212,17 @@ class TestHDFStore(unittest.TestCase):
                 expected = concat([df1, df2], axis=1)
                 expected = expected[5:]
                 tm.assert_frame_equal(result, expected)
-            except (Exception) as detail:
-                print("error in select_as_multiple %s" % str(detail))
-                print("store: %s" % store)
-                print("df1: %s" % df1)
-                print("df2: %s" % df2)
-
+            except (Exception), detail:
+                print ("error in select_as_multiple %s" % str(detail))
+                print ("store: %s" % store)
+                print ("df1: %s" % df1)
+                print ("df2: %s" % df2)
+
+            result = store.select_as_multiple(['df1', 'df2'], where=[Term(
+                'index>df2.index[4]')], selector='df2')
+            expected = concat([df1, df2], axis=1)
+            expected = expected[5:]
+            tm.assert_frame_equal(result, expected)
 
             # test excpection for diff rows
             store.append('df3', tm.makeTimeDataFrame(nper=50))
@@ -3141,13 +3237,13 @@ class TestHDFStore(unittest.TestCase):
             store.append('df', df)
 
             result = store.select(
-                'df', [Term("columns", "=", ["A"])], start=0, stop=5)
+                'df', [Term("columns=['A']")], start=0, stop=5)
             expected = df.ix[0:4, ['A']]
             tm.assert_frame_equal(result, expected)
 
             # out of range
             result = store.select(
-                'df', [Term("columns", "=", ["A"])], start=30, stop=40)
+                'df', [Term("columns=['A']")], start=30, stop=40)
             assert(len(result) == 0)
             tm.assert_isinstance(result, DataFrame)
 
@@ -3160,7 +3256,7 @@ class TestHDFStore(unittest.TestCase):
         with ensure_clean(self.path) as store:
             store.put('frame', df, format='table')
 
-            crit = Term('columns', df.columns[:75])
+            crit = Term('columns=df.columns[:75]')
             result = store.select('frame', [crit])
             tm.assert_frame_equal(result, df.ix[:, df.columns[:75]])
 
@@ -3330,11 +3426,12 @@ class TestHDFStore(unittest.TestCase):
             # old version warning
             with tm.assert_produces_warning(expected_warning=IncompatibilityWarning):
                 self.assertRaises(
-                    Exception, store.select, 'wp1', Term('minor_axis', '=', 'B'))
+                    Exception, store.select, 'wp1', Term('minor_axis=B'))
 
-            with tm.assert_produces_warning(expected_warning=IncompatibilityWarning):
                 df2 = store.select('df2')
-                store.select('df2', Term('index', '>', df2.index[2]))
+                result = store.select('df2', Term('index>df2.index[2]'))
+                expected = df2[df2.index > df2.index[2]]
+                assert_frame_equal(expected, result)
 
         finally:
             safe_close(store)
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index a5c1941a7..66a71d720 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -30,7 +30,7 @@ import pandas.core.common as com
 import pandas.core.format as fmt
 import pandas.core.datetools as datetools
 from pandas.core.api import (DataFrame, Index, Series, notnull, isnull,
-                             MultiIndex, DatetimeIndex, Timestamp, Period)
+                             MultiIndex, DatetimeIndex, Timestamp)
 from pandas import date_range
 import pandas as pd
 from pandas.io.parsers import read_csv
@@ -44,6 +44,7 @@ from pandas.util.testing import (assert_almost_equal,
                                  ensure_clean)
 from pandas.core.indexing import IndexingError
 from pandas.core.common import PandasError
+from pandas.util.compat import OrderedDict
 
 import pandas.util.testing as tm
 import pandas.lib as lib
@@ -2119,7 +2120,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         # this is ok
         df['foo2'] = np.ones((4,2)).tolist()
 
-
     def test_constructor_dtype_nocast_view(self):
         df = DataFrame([[1, 2]])
         should_be_view = DataFrame(df, dtype=df[0].dtype)
@@ -3166,7 +3166,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         with tm.assertRaisesRegexp(TypeError, 'incompatible data and dtype'):
             DataFrame('a', [1, 2], ['a', 'c'], float)
 
-
     def test_constructor_with_datetimes(self):
         intname = np.dtype(np.int_).name
         floatname = np.dtype(np.float_).name
@@ -5238,8 +5237,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
                 _do_test(mkdf(nrows, ncols,r_idx_nlevels=2,c_idx_nlevels=2),
                          path,rnlvl=2,cnlvl=2)
 
-
-
     def test_to_csv_from_csv_w_some_infs(self):
 
         # test roundtrip with inf, -inf, nan, as full columns and mix
@@ -8098,6 +8095,45 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         expec = DataFrame([[nan, 2]])
         assert_frame_equal(res, expec)
 
+    def test_query(self):
+        try:
+            import numexpr as ne
+        except ImportError:
+            raise nose.SkipTest
+        # comparison
+        df = DataFrame(np.random.randn(10, 3), columns=['a', 'b', 'c'])
+        assert_frame_equal(df.query('a < b'), df[df.a < df.b])
+
+        # arith ops
+        assert_frame_equal(df.query('a + b > b * c'),
+                           df[df.a + df.b > df.b * df.c])
+
+        local_dict = dict(df.iteritems())
+        local_dict.update({'df': df})
+        self.assertRaises(NameError, df.query, 'a < d & b < f',
+                          local_dict=local_dict)
+
+        # make sure that it's not just because we didn't pass the locals in
+        self.assertRaises(AssertionError, self.assertRaises, NameError,
+                          df.query, 'a < b', local_dict=local_dict)
+
+    def test_query_index(self):
+        try:
+            import numexpr as ne
+        except ImportError:
+            raise nose.SkipTest
+
+        df = DataFrame(np.random.randn(10, 3), index=Index(range(10),
+                                                           name='blob'),
+                       columns=['a', 'b', 'c'])
+        assert_frame_equal(df.query('index < b'), df[df.index < df.b])
+        assert_frame_equal(df.query('index < 5'), df[df.index < 5])
+        assert_frame_equal(df.query('(blob < 5) & (a < b)'), df[(df.index < 5)
+                                                                & (df.a <
+                                                                   df.b)])
+        assert_frame_equal(df.query('blob < b'), df[df.index < df.b])
+
+
     #----------------------------------------------------------------------
     # Transposing
     def test_transpose(self):
@@ -8228,7 +8264,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_series_equal(the_diff['A'],
                             tf['A'] - tf['A'].shift(1))
 
-
     def test_diff_mixed_dtype(self):
         df = DataFrame(np.random.randn(5, 3))
         df['A'] = np.array([1, 2, 3, 4, 5], dtype=object)
@@ -10137,7 +10172,6 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         expected = Series({'float64' : 2, 'object' : 2})
         assert_series_equal(result, expected)
 
-
     def test_reset_index(self):
         stacked = self.frame.stack()[::2]
         stacked = DataFrame({'foo': stacked, 'bar': stacked})
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index eeb5ca436..a070fa7ca 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -13,12 +13,14 @@ import os
 from datetime import datetime
 from functools import wraps, partial
 from contextlib import contextmanager
+from httplib import HTTPException
+from urllib2 import urlopen
 from distutils.version import LooseVersion
 
 from numpy.random import randn, rand
 import numpy as np
 
-from pandas.core.common import isnull, _is_sequence
+from pandas.core.common import isnull, _is_sequence, is_list_like
 import pandas.core.index as index
 import pandas.core.series as series
 import pandas.core.frame as frame
@@ -1136,7 +1138,7 @@ class _AssertRaisesContextmanager(object):
 
 
 @contextmanager
-def assert_produces_warning(expected_warning=Warning, filter_level="always"):
+def assert_produces_warning(expected_warning=None, filter_level="always"):
     """
     Context manager for running code that expects to raise (or not raise)
     warnings.  Checks that code raises the expected warning and only the
@@ -1162,19 +1164,25 @@ def assert_produces_warning(expected_warning=Warning, filter_level="always"):
 
     ..warn:: This is *not* thread-safe.
     """
+    if expected_warning is None:
+        expected_warning = [Warning]
+    elif not is_list_like(expected_warning):
+        expected_warning = [expected_warning]
     with warnings.catch_warnings(record=True) as w:
         saw_warning = False
         warnings.simplefilter(filter_level)
         yield w
         extra_warnings = []
         for actual_warning in w:
-            if (expected_warning and issubclass(actual_warning.category,
-                                                expected_warning)):
+            if (expected_warning and any(issubclass(actual_warning.category,
+                                                    ew) for ew in
+                                         expected_warning)):
                 saw_warning = True
             else:
                 extra_warnings.append(actual_warning.category.__name__)
         if expected_warning:
-            assert saw_warning, ("Did not see expected warning of class %r."
-                                 % expected_warning.__name__)
+            msg = ', '.join(ew.__name__ for ew in expected_warning)
+            assert saw_warning, ("Did not see expected warning(s) of "
+                                 "class(es): %s." % msg)
         assert not extra_warnings, ("Caused unexpected warning(s): %r."
                                     % extra_warnings)
