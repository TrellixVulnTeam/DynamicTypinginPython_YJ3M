commit 3618568b8792315d2f1e8b367ff56bb8f84c3b41
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Sun Dec 11 17:40:22 2011 -0500

    TST: more unit tests, add r* operators to sparse Cython code, #463

diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index 2ba2cbb1d..79fd56608 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -193,12 +193,12 @@ to sparse
     __pow__ = _sparse_op_wrap(operator.pow, 'pow')
 
     # reverse operators
-    __radd__ = _sparse_op_wrap(operator.add, '__radd__')
-    __rsub__ = _sparse_op_wrap(lambda x, y: y - x, '__rsub__')
-    __rmul__ = _sparse_op_wrap(operator.mul, '__rmul__')
-    __rtruediv__ = _sparse_op_wrap(lambda x, y: y / x, '__rtruediv__')
-    __rfloordiv__ = _sparse_op_wrap(lambda x, y: y // x, 'floordiv')
-    __rpow__ = _sparse_op_wrap(lambda x, y: y ** x, '__rpow__')
+    __radd__ = _sparse_op_wrap(operator.add, 'add')
+    __rsub__ = _sparse_op_wrap(lambda x, y: y - x, 'rsub')
+    __rmul__ = _sparse_op_wrap(operator.mul, 'mul')
+    __rtruediv__ = _sparse_op_wrap(lambda x, y: y / x, 'rtruediv')
+    __rfloordiv__ = _sparse_op_wrap(lambda x, y: y // x, 'rfloordiv')
+    __rpow__ = _sparse_op_wrap(lambda x, y: y ** x, 'rpow')
 
     def disable(self, other):
         raise NotImplementedError('inplace binary ops not supported')
diff --git a/pandas/sparse/tests/test_array.py b/pandas/sparse/tests/test_array.py
index b2e30e67d..0c2864078 100644
--- a/pandas/sparse/tests/test_array.py
+++ b/pandas/sparse/tests/test_array.py
@@ -23,6 +23,9 @@ class TestSparseArray(unittest.TestCase):
         self.arr = SparseArray(self.arr_data)
         self.zarr = SparseArray([0, 0, 1, 2, 3, 0, 4, 5, 0, 6], fill_value=0)
 
+    def test_constructor(self):
+        pass
+
     def test_values_asarray(self):
         assert_almost_equal(self.arr.values, self.arr_data)
         assert_almost_equal(self.arr.sp_values, np.asarray(self.arr))
@@ -41,12 +44,33 @@ class TestSparseArray(unittest.TestCase):
         arr1 = SparseArray(data1)
         arr2 = SparseArray(data2)
 
-        def _check_op(op):
-            res = op(arr1, arr2)
-            exp = SparseArray(op(arr1.values, arr2.values))
+        data1[::2] = 3
+        data2[::3] = 3
+        farr1 = SparseArray(data1, fill_value=3)
+        farr2 = SparseArray(data2, fill_value=3)
+
+        def _check_op(op, first, second):
+            res = op(first, second)
+            exp = SparseArray(op(first.values, second.values),
+                              fill_value=first.fill_value)
             self.assert_(isinstance(res, SparseArray))
             assert_almost_equal(res.values, exp.values)
 
+            res2 = op(first, second.values)
+            self.assert_(isinstance(res2, SparseArray))
+            assert_sp_array_equal(res, res2)
+
+            res3 = op(first.values, second)
+            self.assert_(isinstance(res3, SparseArray))
+            assert_sp_array_equal(res, res3)
+
+            res4 = op(first, 4)
+            self.assert_(isinstance(res4, SparseArray))
+            exp = op(first.values, 4)
+            exp_fv = op(first.fill_value, 4)
+            assert_almost_equal(res4.fill_value, exp_fv)
+            assert_almost_equal(res4.values, exp)
+
         def _check_inplace_op(op):
             tmp = arr1.copy()
             self.assertRaises(NotImplementedError, op, tmp, arr2)
@@ -54,7 +78,8 @@ class TestSparseArray(unittest.TestCase):
         bin_ops = [operator.add, operator.sub, operator.mul, operator.truediv,
                    operator.floordiv, operator.pow]
         for op in bin_ops:
-            _check_op(op)
+            _check_op(op, arr1, arr2)
+            _check_op(op, farr1, farr2)
 
         inplace_ops = ['iadd', 'isub', 'imul', 'itruediv', 'ifloordiv', 'ipow']
         for op in inplace_ops:
diff --git a/pandas/src/sparse.pyx b/pandas/src/sparse.pyx
index a08e58694..ad8c198ca 100644
--- a/pandas/src/sparse.pyx
+++ b/pandas/src/sparse.pyx
@@ -16,47 +16,6 @@ cdef float64_t INF = <float64_t> np.inf
 cdef inline int int_max(int a, int b): return a if a >= b else b
 cdef inline int int_min(int a, int b): return a if a <= b else b
 
-cdef inline float64_t __add(float64_t a, float64_t b):
-    return a + b
-cdef inline float64_t __sub(float64_t a, float64_t b):
-    return a - b
-cdef inline float64_t __div(float64_t a, float64_t b):
-    if b == 0:
-        if a >= 0:
-            return INF
-        else:
-            return -INF
-    else:
-        return a / b
-
-cdef inline float64_t __floordiv(float64_t a, float64_t b):
-    if b == 0:
-        if a >= 0:
-            return INF
-        else:
-            return -INF
-    else:
-        return a // b
-
-cdef inline float64_t __mul(float64_t a, float64_t b):
-    return a * b
-cdef inline float64_t __eq(float64_t a, float64_t b):
-    return a == b
-cdef inline float64_t __ne(float64_t a, float64_t b):
-    return a != b
-cdef inline float64_t __lt(float64_t a, float64_t b):
-    return a < b
-cdef inline float64_t __gt(float64_t a, float64_t b):
-    return a > b
-
-cdef inline float64_t __pow(float64_t a, float64_t b):
-    # NaN
-    if a != a or b != b:
-        return NaN
-    return a ** b
-
-ctypedef float64_t (* double_func)(float64_t a, float64_t b)
-
 #-------------------------------------------------------------------------------
 
 
@@ -715,34 +674,7 @@ cdef class BlockUnion(BlockMerge):
 #-------------------------------------------------------------------------------
 # Sparse arithmetic
 
-# This probably needs to be "templated" to achieve maximum performance.
-# TODO: quantify performance boost to "templating"
-
-cpdef sparse_nanadd(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __add)
-
-cpdef sparse_nansub(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __sub)
-
-cpdef sparse_nanmul(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __mul)
-
-cpdef sparse_nandiv(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __div)
-
-sparse_nantruediv = sparse_nandiv
-
-cpdef sparse_nanfloordiv(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __floordiv)
-
-cpdef sparse_nanpow(ndarray x, SparseIndex xindex,
-                    ndarray y, SparseIndex yindex):
-    return sparse_nancombine(x, xindex, y, yindex, __pow)
+ctypedef float64_t (* double_func)(float64_t a, float64_t b)
 
 cdef inline tuple sparse_nancombine(ndarray x, SparseIndex xindex,
                                     ndarray y, SparseIndex yindex,
@@ -758,36 +690,6 @@ cdef inline tuple sparse_nancombine(ndarray x, SparseIndex xindex,
     #     return int_nanop(x, xindex.to_int_index(),
     #                      y, yindex.to_int_index(), op)
 
-cpdef sparse_add(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __add)
-
-cpdef sparse_sub(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __sub)
-
-cpdef sparse_mul(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __mul)
-
-cpdef sparse_div(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __div)
-sparse_truediv = sparse_div
-
-cpdef sparse_floordiv(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __floordiv)
-
-cpdef sparse_pow(ndarray x, SparseIndex xindex, float64_t xfill,
-                 ndarray y, SparseIndex yindex, float64_t yfill):
-    return sparse_combine(x, xindex, xfill,
-                             y, yindex, yfill, __pow)
 
 cdef inline tuple sparse_combine(ndarray x, SparseIndex xindex, float64_t xfill,
                                  ndarray y, SparseIndex yindex, float64_t yfill,
@@ -1063,6 +965,160 @@ cdef inline tuple int_op(ndarray x_, IntIndex xindex, float64_t xfill,
 
     return out, out_index
 
+cdef inline float64_t __add(float64_t a, float64_t b):
+    return a + b
+
+cdef inline float64_t __sub(float64_t a, float64_t b):
+    return a - b
+
+cdef inline float64_t __rsub(float64_t a, float64_t b):
+    return b - a
+
+cdef inline float64_t __div(float64_t a, float64_t b):
+    if b == 0:
+        if a >= 0:
+            return INF
+        else:
+            return -INF
+    else:
+        return a / b
+
+cdef inline float64_t __rdiv(float64_t a, float64_t b):
+    return __div(b, a)
+
+cdef inline float64_t __floordiv(float64_t a, float64_t b):
+    if b == 0:
+        if a >= 0:
+            return INF
+        else:
+            return -INF
+    else:
+        return a // b
+
+cdef inline float64_t __rfloordiv(float64_t a, float64_t b):
+    return __floordiv(b, a)
+
+cdef inline float64_t __mul(float64_t a, float64_t b):
+    return a * b
+cdef inline float64_t __eq(float64_t a, float64_t b):
+    return a == b
+cdef inline float64_t __ne(float64_t a, float64_t b):
+    return a != b
+cdef inline float64_t __lt(float64_t a, float64_t b):
+    return a < b
+cdef inline float64_t __gt(float64_t a, float64_t b):
+    return a > b
+
+cdef inline float64_t __pow(float64_t a, float64_t b):
+    # NaN
+    if a != a or b != b:
+        return NaN
+    return a ** b
+
+cdef inline float64_t __rpow(float64_t a, float64_t b):
+    return __pow(b, a)
+
+
+# This probably needs to be "templated" to achieve maximum performance.
+# TODO: quantify performance boost to "templating"
+
+cpdef sparse_nanadd(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __add)
+
+cpdef sparse_nansub(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __sub)
+
+cpdef sparse_nanrsub(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __rsub)
+
+cpdef sparse_nanmul(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __mul)
+
+cpdef sparse_nandiv(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __div)
+
+cpdef sparse_nanrdiv(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __rdiv)
+
+sparse_nantruediv = sparse_nandiv
+sparse_nanrtruediv = sparse_nanrdiv
+
+cpdef sparse_nanfloordiv(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __floordiv)
+
+cpdef sparse_nanrfloordiv(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __rfloordiv)
+
+cpdef sparse_nanpow(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __pow)
+
+cpdef sparse_nanrpow(ndarray x, SparseIndex xindex,
+                    ndarray y, SparseIndex yindex):
+    return sparse_nancombine(x, xindex, y, yindex, __rpow)
+
+cpdef sparse_add(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __add)
+
+cpdef sparse_sub(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __sub)
+
+cpdef sparse_rsub(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __rsub)
+
+cpdef sparse_mul(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __mul)
+
+cpdef sparse_div(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __div)
+
+cpdef sparse_rdiv(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __rdiv)
+
+sparse_truediv = sparse_div
+sparse_rtruediv = sparse_rdiv
+
+cpdef sparse_floordiv(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __floordiv)
+
+cpdef sparse_rfloordiv(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __rfloordiv)
+
+cpdef sparse_pow(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __pow)
+
+cpdef sparse_rpow(ndarray x, SparseIndex xindex, float64_t xfill,
+                 ndarray y, SparseIndex yindex, float64_t yfill):
+    return sparse_combine(x, xindex, xfill,
+                             y, yindex, yfill, __rpow)
+
+
 #-------------------------------------------------------------------------------
 # Indexing operations
 
