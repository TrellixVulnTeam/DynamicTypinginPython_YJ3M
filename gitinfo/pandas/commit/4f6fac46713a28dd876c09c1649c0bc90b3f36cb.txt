commit 4f6fac46713a28dd876c09c1649c0bc90b3f36cb
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Wed Oct 3 00:16:25 2012 -0400

    ENH: tokenize no more lines than requested; enable stopping mid-chunk. test suite passing

diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 51ffb48ae..aa6253a85 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -281,7 +281,7 @@ def _make_parser_function(name, sep=','):
                  na_filter=True,
                  compact_ints=False,
                  use_unsigned=False,
-                 low_memory=False,
+                 low_memory=_c_parser_defaults['low_memory'],
                  buffer_lines=2**14,
                  warn_bad_lines=True,
                  error_bad_lines=True,
@@ -569,13 +569,23 @@ class TextFileReader(object):
         raise NotImplementedError
 
     def read(self, nrows=None):
-        if nrows is not None and self.options.get('skip_footer'):
-            raise ValueError('skip_footer not supported for iteration')
+        suppressed_warnings = False
+        if nrows is not None:
+            if self.options.get('skip_footer'):
+                raise ValueError('skip_footer not supported for iteration')
+
+            # # XXX hack
+            # if isinstance(self._engine, CParserWrapper):
+            #     suppressed_warnings = True
+            #     self._engine.set_error_bad_lines(False)
 
         # index = None
 
         ret = self._engine.read(nrows)
 
+        # if suppressed_warnings:
+        #     self._engine.set_error_bad_lines(True)
+
         if self.options.get('as_recarray'):
             return ret
 
@@ -794,12 +804,22 @@ class CParserWrapper(ParserBase):
 
         self._implicit_index = self._reader.leading_cols > 0
 
+    def set_error_bad_lines(self, status):
+        self._reader.set_error_bad_lines(int(status))
+
     def read(self, nrows=None):
         if self.as_recarray:
             # what to do if there are leading columns?
             return self._reader.read(nrows)
 
-        data = self._reader.read(nrows)
+        try:
+            data = self._reader.read(nrows)
+        except StopIteration:
+            if nrows is None:
+                return None, self.names, {}
+            else:
+                raise
+
         names = self.names
 
         if self._reader.leading_cols:
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index 1f9e97893..8b639f6e6 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -407,9 +407,7 @@ skip
 2,3,4
 """
         try:
-            it = self.read_table(StringIO(data), sep=',',
-                            header=1, comment='#', iterator=True, chunksize=1,
-                            skiprows=[2])
+            it = self.read_table(StringIO(data), sep=',', header=1, comment='#', iterator=True, chunksize=1, skiprows=[2])
             df = it.read(1)
             it.read(2)
             self.assert_(False)
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index d6e581fb1..bf88faf16 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -238,7 +238,7 @@ cdef class TextReader:
 
                   skipinitialspace=False,
                   escapechar=None,
-                  doublequote=None,
+                  doublequote=True,
                   quotechar=b'"',
                   quoting=0,
 
@@ -282,6 +282,7 @@ cdef class TextReader:
         #----------------------------------------
         # parser options
 
+        self.parser.doublequote = doublequote
         self.parser.skipinitialspace = skipinitialspace
 
         if len(decimal) != 1:
@@ -366,6 +367,9 @@ cdef class TextReader:
         if self.should_close:
             self.file_handle.close()
 
+    def set_error_bad_lines(self, int status):
+        self.parser.error_bad_lines = status
+
     cdef _make_skiprow_set(self):
         if isinstance(self.skiprows, (int, np.integer)):
             self.skiprows = range(self.skiprows)
@@ -417,6 +421,8 @@ cdef class TextReader:
             object name
             int status
 
+        header = []
+
         if self.parser.header >= 0:
             # Header is in the file
 
@@ -432,7 +438,6 @@ cdef class TextReader:
             start = self.parser.line_start[self.parser.header]
 
             # TODO: Py3 vs. Py2
-            header = []
             counts = {}
             for i in range(field_count):
                 word = self.parser.words[start + i]
@@ -466,7 +471,7 @@ cdef class TextReader:
 
         # Corner case, not enough lines in the file
         if self.parser.lines < data_line + 1:
-            return None, len(header)
+            field_count = len(header)
         else:
             field_count = self.parser.line_fields[data_line]
             passed_count = len(header)
diff --git a/pandas/src/parser/parser.c b/pandas/src/parser/parser.c
index 6925a8056..53f60b48a 100644
--- a/pandas/src/parser/parser.c
+++ b/pandas/src/parser/parser.c
@@ -647,7 +647,7 @@ void parser_set_default_options(parser_t *self) {
     self->skipinitialspace = 0;
     self->quoting = QUOTE_MINIMAL;
     self->allow_embedded_newline = 1;
-    self->strict = 1;
+    self->strict = 0;
 
     self->error_bad_lines = 0;
     self->warn_bad_lines = 0;
@@ -847,6 +847,10 @@ int parser_init(parser_t *self) {
         return PARSER_OUT_OF_MEMORY;
     }
 
+    /* amount of bytes buffered */
+    self->datalen = 0;
+    self->datapos = 0;
+
     self->line_start[0] = 0;
     self->line_fields[0] = 0;
 
@@ -1121,6 +1125,8 @@ int parser_buffer_bytes(parser_t *self, size_t nbytes) {
 
     status = 0;
 
+    self->datapos = 0;
+
     switch(self->sourcetype) {
         case 'F': // basic FILE*
 
@@ -1238,27 +1244,40 @@ int parser_cleanup_filebuffers(parser_t *self) {
     stream = self->stream + self->stream_len;  \
     slen = self->stream_len;
 
-#define END_LINE()                             \
-    self->stream_len = slen;                   \
-    if (end_line(self) < 0) {                  \
-        goto parsingerror;                     \
-    }                                          \
-    stream = self->stream + self->stream_len;  \
+#define END_LINE()                                                      \
+    self->stream_len = slen;                                            \
+    if (end_line(self) < 0) {                                           \
+        goto parsingerror;                                              \
+    }                                                                   \
+    self->state = START_RECORD;                                         \
+    if (line_limit > 0 && self->lines == start_lines + line_limit) {    \
+        goto linelimit;                                                 \
+                                                                        \
+    }                                                                   \
+    stream = self->stream + self->stream_len;                           \
     slen = self->stream_len;
 
 #define IS_WHITESPACE(c) ((c == ' ' || c == '\t'))
 
-typedef int (*parser_op)(parser_t *self);
+typedef int (*parser_op)(parser_t *self, size_t line_limit);
+
+#define _TOKEN_CLEANUP()                                                \
+    self->stream_len = slen;                                            \
+    self->datapos = i;                                                  \
+    TRACE(("datapos: %d, datalen: %d\n", self->datapos, self->datalen));
 
 
-int tokenize_delimited(parser_t *self)
+
+int tokenize_delimited(parser_t *self, size_t line_limit)
 {
-    int i, slen;
+    int i, slen, start_lines;
     char c;
     char *stream;
-    char *buf = self->data;
+    char *buf = self->data + self->datapos;
+
+    start_lines = self->lines;
 
-    if (make_stream_space(self, self->datalen) < 0) {
+    if (make_stream_space(self, self->datalen - self->datapos) < 0) {
         self->error_msg = "out of memory";
         return -1;
     }
@@ -1268,13 +1287,14 @@ int tokenize_delimited(parser_t *self)
 
     TRACE(("%s\n", buf));
 
-    for (i = 0; i < self->datalen; ++i)
+    for (i = self->datapos; i < self->datalen; ++i)
     {
         // Next character in file
         c = *buf++;
 
-        /* TRACE(("Iter: %d Char: %c Line %d field_count %d\n", */
-        /*        i, c, self->file_lines + 1, self->line_fields[self->file_lines])); */
+        TRACE(("Iter: %d Char: %c Line %d field_count %d, state %d\n",
+               i, c, self->file_lines + 1, self->line_fields[self->lines],
+               self->state));
 
         switch(self->state) {
         case START_RECORD:
@@ -1297,7 +1317,7 @@ int tokenize_delimited(parser_t *self)
             if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else if (c == '\r') {
                 END_FIELD();
                 self->state = EAT_CRNL;
@@ -1342,7 +1362,7 @@ int tokenize_delimited(parser_t *self)
             if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else if (c == '\r') {
                 END_FIELD();
                 self->state = EAT_CRNL;
@@ -1410,7 +1430,7 @@ int tokenize_delimited(parser_t *self)
             else if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             }
             else if (c == '\r') {
                 END_FIELD();
@@ -1431,7 +1451,7 @@ int tokenize_delimited(parser_t *self)
         case EAT_CRNL:
             if (c == '\n') {
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else {
                 /* self->error_msg = ("new-line character seen in" */
                 /*                 " unquoted field - do you need" */
@@ -1445,27 +1465,35 @@ int tokenize_delimited(parser_t *self)
 
     }
 
-    self->stream_len = slen;
+    _TOKEN_CLEANUP();
 
     TRACE(("Finished tokenizing input\n"))
 
     return 0;
 
 parsingerror:
-
-    self->stream_len = slen;
+    i++;
+    _TOKEN_CLEANUP();
 
     return -1;
+
+linelimit:
+    i++;
+    _TOKEN_CLEANUP();
+
+    return 0;
 }
 
-int tokenize_whitespace(parser_t *self)
+int tokenize_whitespace(parser_t *self, size_t line_limit)
 {
-    int i, slen;
+    int i, slen, start_lines;
     char c;
     char *stream;
-    char *buf = self->data;
+    char *buf = self->data + self->datapos;
 
-    if (make_stream_space(self, self->datalen) < 0) {
+    start_lines = self->lines;
+
+    if (make_stream_space(self, self->datalen - self->datapos) < 0) {
         self->error_msg = "out of memory";
         return -1;
     }
@@ -1475,13 +1503,13 @@ int tokenize_whitespace(parser_t *self)
 
     TRACE(("%s\n", buf));
 
-    for (i = 0; i < self->datalen; ++i)
+    for (i = self->datapos; i < self->datalen; ++i)
     {
         // Next character in file
         c = *buf++;
 
         TRACE(("Iter: %d Char: %c Line %d field_count %d\n",
-               i, c, self->file_lines + 1, self->line_fields[self->file_lines]));
+               i, c, self->file_lines + 1, self->line_fields[self->lines]));
 
         switch(self->state) {
 
@@ -1514,7 +1542,7 @@ int tokenize_whitespace(parser_t *self)
             if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else if (c == '\r') {
                 END_FIELD();
                 self->state = EAT_CRNL;
@@ -1558,7 +1586,7 @@ int tokenize_whitespace(parser_t *self)
             if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else if (c == '\r') {
                 END_FIELD();
                 self->state = EAT_CRNL;
@@ -1617,16 +1645,16 @@ int tokenize_whitespace(parser_t *self)
                 PUSH_CHAR(c);
                 self->state = IN_QUOTED_FIELD;
             }
-            else if (c == self->delimiter) {
+            else if (IS_WHITESPACE(c)) {
                 // End of field. End of line not reached yet
 
                 END_FIELD();
-                self->state = START_FIELD;
+                self->state = EAT_WHITESPACE;
             }
             else if (c == '\n') {
                 END_FIELD();
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             }
             else if (c == '\r') {
                 END_FIELD();
@@ -1647,7 +1675,7 @@ int tokenize_whitespace(parser_t *self)
         case EAT_CRNL:
             if (c == '\n') {
                 END_LINE();
-                self->state = START_RECORD;
+                /* self->state = START_RECORD; */
             } else {
                 /* self->error_msg = ("new-line character seen in" */
                 /*                 " unquoted field - do you need" */
@@ -1661,17 +1689,23 @@ int tokenize_whitespace(parser_t *self)
 
     }
 
-    self->stream_len = slen;
+    _TOKEN_CLEANUP();
 
     TRACE(("Finished tokenizing input\n"))
 
     return 0;
 
 parsingerror:
-
-    self->stream_len = slen;
+    i++;
+    _TOKEN_CLEANUP();
 
     return -1;
+
+linelimit:
+    i++;
+    _TOKEN_CLEANUP();
+
+    return 0;
 }
 
 
@@ -1681,9 +1715,18 @@ int parser_handle_eof(parser_t *self) {
         // test cases needed here
         // TODO: empty field at end of line
         TRACE(("handling eof\n"));
+
         if (self->state == IN_FIELD) {
             if (end_field(self) < 0)
                 return -1;
+        } else if (self->state == QUOTE_IN_QUOTED_FIELD) {
+            if (end_field(self) < 0)
+                return -1;
+        } else if (self->state == IN_QUOTED_FIELD) {
+            self->error_msg = (char*) malloc(100);
+            sprintf(self->error_msg, "EOF inside string starting at line %d",
+                    self->file_lines);
+            return -1;
         }
 
         if (end_line(self) < 0)
@@ -1733,13 +1776,14 @@ int parser_consume_rows(parser_t *self, size_t nrows) {
     self->word_start -= char_count;
 
     /* move line metadata */
-    for (i = 0; i < nrows; ++i)
+    for (i = 0; i < self->lines - nrows; ++i)
     {
         offset = i + nrows;
         self->line_start[i] = self->line_start[offset] - word_deletions;
         self->line_fields[i] = self->line_fields[offset];
     }
     self->lines -= nrows;
+    self->line_fields[self->lines] = 0;
 
     return 0;
 }
@@ -1836,20 +1880,21 @@ int _tokenize_helper(parser_t *self, size_t nrows, int all) {
         if (!all && self->lines - start_lines >= nrows)
             break;
 
-        TRACE(("Trying to process %d bytes\n", self->chunksize));
+        if (self->datapos == self->datalen) {
+            status = parser_buffer_bytes(self, self->chunksize);
 
-        status = parser_buffer_bytes(self, self->chunksize);
+            if (status == REACHED_EOF) {
+                // close out last line
+                status = parser_handle_eof(self);
+                self->state = FINISHED;
+                break;
+            }
+        }
 
+        TRACE(("Trying to process %d bytes\n", self->datalen - self->datapos));
         TRACE(("sourcetype: %c, status: %d\n", self->sourcetype, status));
 
-        if (status == REACHED_EOF) {
-            // close out last line
-            status = parser_handle_eof(self);
-            self->state = FINISHED;
-            break;
-        }
-
-        status = tokenize_bytes(self);
+        status = tokenize_bytes(self, nrows);
 
         if (status < 0) {
             // XXX
diff --git a/pandas/src/parser/parser.h b/pandas/src/parser/parser.h
index 0ed53d715..4d88959b9 100644
--- a/pandas/src/parser/parser.h
+++ b/pandas/src/parser/parser.h
@@ -109,6 +109,7 @@ typedef struct parser_t {
     int chunksize;  // Number of bytes to prepare for each chunk
     char *data;     // pointer to data to be processed
     int datalen;    // amount of data available
+    int datapos;
 
     // where to write out tokenized data
     char *stream;
