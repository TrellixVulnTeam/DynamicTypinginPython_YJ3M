commit 8d2c2a8cc3e7b0c96be00722b497089a27b1af13
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Dec 30 16:12:15 2011 -0500

    BUG: groupby level should preserve level order, perf enhancement. docs fix. moved join op code from internals to merge.py

diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index 175fb10aa..818d6b0db 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -692,10 +692,11 @@ produces the "keys" of the objects, namely:
 
 Thus, for example:
 
-.. ipython:: python
+.. ipython::
 
-   for col in df:
-       print col
+   In [0]: for col in df:
+      ...:     print col
+      ...:
 
 iteritems
 ~~~~~~~~~
@@ -709,13 +710,12 @@ key-value pairs:
 
 For example:
 
-.. ipython:: python
-
-   for item, frame in wp.iteritems():
-       print item
-       print frame
+.. ipython::
 
-.. _basics.sorting:
+   In [0]: for item, frame in wp.iteritems():
+      ...:     print item
+      ...:     print frame
+      ...:
 
 Sorting by index and value
 --------------------------
diff --git a/doc/source/io.rst b/doc/source/io.rst
index d8bbe7db1..c5d7caa29 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -15,7 +15,7 @@
    np.set_printoptions(precision=4, suppress=True)
    import matplotlib.pyplot as plt
    plt.close('all')
-   clipdf = DataFrame({'A':[1,2,3],'B':[4,5,6],'C':['p','q','r']}, 
+   clipdf = DataFrame({'A':[1,2,3],'B':[4,5,6],'C':['p','q','r']},
                       index=['x','y','z'])
 
 *******************************
@@ -295,7 +295,7 @@ which, if set to ``True``, will additionally output the length of the Series.
 
 
 Writing to HTML format
-~~~~~~~~~~~~~~~~~~~~~
+~~~~~~~~~~~~~~~~~~~~~~
 
 .. _io.html:
 
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 5a6bdc7f2..30e583b6e 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -2730,7 +2730,7 @@ class DataFrame(NDFrame):
     def _get_raw_column(self, col):
         return self._data.get(col)
 
-    def join(self, other, on=None, how=None, lsuffix='', rsuffix=''):
+    def join(self, other, on=None, how='left', lsuffix='', rsuffix=''):
         """
         Join columns with other DataFrame either on index or on a key
         column.
@@ -2762,8 +2762,24 @@ class DataFrame(NDFrame):
         -------
         joined : DataFrame
         """
-        if how is None:
-            how = 'left'
+        # For SparseDataFrame's benefit
+        return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,
+                                 rsuffix=rsuffix)
+
+    def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix=''):
+        # from pandas.tools.merge import merge
+
+        # if isinstance(other, Series):
+        #     assert(other.name is not None)
+        #     other = DataFrame({other.name : other})
+
+        # return merge(self, other, left_on=on, left_index=on is None,
+        #              right_index=True, suffixes=(lsuffix, rsuffix))
+
+        if isinstance(other, Series):
+            assert(other.name is not None)
+            other = DataFrame({other.name : other})
+
         if on is not None:
             return self._join_on(other, on, how, lsuffix, rsuffix)
         else:
@@ -2773,10 +2789,6 @@ class DataFrame(NDFrame):
         if how not in ('left', 'inner'):  # pragma: no cover
             raise Exception('Only inner / left joins currently supported')
 
-        if isinstance(other, Series):
-            assert(other.name is not None)
-            other = DataFrame({other.name : other})
-
         if isinstance(on, (list, tuple)):
             if len(on) == 1:
                 join_key = self[on[0]].values
@@ -2792,11 +2804,7 @@ class DataFrame(NDFrame):
         return self._constructor(new_data)
 
     def _join_index(self, other, how, lsuffix, rsuffix):
-        from pandas.core.internals import join_managers
-
-        if isinstance(other, Series):
-            assert(other.name is not None)
-            other = DataFrame({other.name : other})
+        from pandas.tools.merge import join_managers
 
         thisdata, otherdata = self._data._maybe_rename_join(
             other._data, lsuffix, rsuffix, copydata=False)
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 4a5c81fb6..da8851597 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -149,7 +149,7 @@ class GroupBy(object):
 
     @property
     def _group_shape(self):
-        return tuple(len(ping.ids) for ping in self.groupings)
+        return tuple(len(ping.counts) for ping in self.groupings)
 
     @property
     def _agg_stride_shape(self):
@@ -535,24 +535,35 @@ class Grouping(object):
         if isinstance(grouper, Series) and name is None:
             self.name = grouper.name
 
+        # pre-computed
+        self._was_factor = False
+
         if level is not None:
             if not isinstance(level, int):
                 assert(level in index.names)
                 level = index.names.index(level)
 
             inds = index.labels[level]
-            labels = index.levels[level].take(inds)
+            level_index = index.levels[level]
+
             if self.name is None:
                 self.name = index.names[level]
 
+            # XXX complete hack
+
+            level_values = index.levels[level].take(inds)
             if grouper is not None:
-                self.grouper = labels.map(self.grouper)
+                self.grouper = level_values.map(self.grouper)
             else:
-                self.grouper = labels
-
-        # no level passed
-        if not isinstance(self.grouper, np.ndarray):
-            self.grouper = self.index.map(self.grouper)
+                self._was_factor = True
+                self._labels = inds
+                self._group_index = level_index
+                self._counts = lib.group_count(inds, len(level_index))
+                self.grouper = level_values
+        else:
+            # no level passed
+            if not isinstance(self.grouper, np.ndarray):
+                self.grouper = self.index.map(self.grouper)
 
     def __repr__(self):
         return 'Grouping(%s)' % self.name
@@ -563,6 +574,7 @@ class Grouping(object):
     _labels = None
     _ids = None
     _counts = None
+    _group_index = None
 
     @cache_readonly
     def indices(self):
@@ -577,7 +589,11 @@ class Grouping(object):
     @property
     def ids(self):
         if self._ids is None:
-            self._make_labels()
+            if self._was_factor:
+                index = self._group_index
+                self._ids = dict(zip(range(len(index)), index))
+            else:
+                self._make_labels()
         return self._ids
 
     @cache_readonly
@@ -590,13 +606,21 @@ class Grouping(object):
             self._make_labels()
         return self._counts
 
-    @cache_readonly
+    @property
     def group_index(self):
-        return Index([self.ids[i] for i in range(len(self.ids))])
+        if self._group_index is None:
+            ids = self.ids
+            values = np.arange(len(self.ids), dtype='O')
+            self._group_index = Index(lib.lookup_values(values, ids))
+        return self._group_index
 
     def _make_labels(self):
-        ids, labels, counts  = _group_labels(self.grouper)
-        sids, slabels, scounts = sort_group_labels(ids, labels, counts)
+        if self._was_factor:  # pragma: no cover
+            raise Exception('Should not call this method grouping by level')
+        else:
+            ids, labels, counts  = _group_labels(self.grouper)
+            sids, slabels, scounts = sort_group_labels(ids, labels, counts)
+
         self._labels = slabels
         self._ids = sids
         self._counts = scounts
@@ -768,7 +792,12 @@ class SeriesGroupBy(GroupBy):
             if len(self.groupings) > 1:
                 index = MultiIndex.from_tuples(keys, names=key_names)
             else:
-                index = Index(keys, name=key_names[0])
+                ping = self.groupings[0]
+                if len(keys) == len(ping.counts):
+                    index = ping.group_index
+                    index.name = key_names[0]
+                else:
+                    index = Index(keys, name=key_names[0])
             return index
 
         if isinstance(values[0], Series):
@@ -981,7 +1010,10 @@ class DataFrameGroupBy(GroupBy):
 
         index_name = (self.groupings[0].name
                       if len(self.groupings) == 1 else None)
-        result_index = Index(sorted(result), name=index_name)
+
+        result_index = self.groupings[0].group_index
+
+        # result_index = Index(sorted(result), name=index_name)
 
         if result:
             if axis == 0:
@@ -1062,25 +1094,36 @@ class DataFrameGroupBy(GroupBy):
                                      not_indexed_same=not_indexed_same)
         else:
             if len(self.groupings) > 1:
-                keys = MultiIndex.from_tuples(keys, names=key_names)
+                key_index = MultiIndex.from_tuples(keys, names=key_names)
             else:
-                keys = Index(keys, name=key_names[0])
+                ping = self.groupings[0]
+                if len(keys) == len(ping.counts):
+                    key_index = ping.group_index
+                    key_index.name = key_names[0]
+
+                    key_lookup = Index(keys)
+                    indexer = key_lookup.get_indexer(key_index)
+
+                    # reorder the values
+                    values = [values[i] for i in indexer]
+                else:
+                    key_index = Index(keys, name=key_names[0])
 
             if isinstance(values[0], np.ndarray):
                 if self.axis == 0:
                     stacked_values = np.vstack([np.asarray(x)
                                                 for x in values])
                     columns = values[0].index
-                    index = keys
+                    index = key_index
                 else:
                     stacked_values = np.vstack([np.asarray(x)
                                                 for x in values]).T
                     index = values[0].index
-                    columns = keys
+                    columns = key_index
                 return DataFrame(stacked_values, index=index,
                                  columns=columns)
             else:
-                return Series(values, index=keys)
+                return Series(values, index=key_index)
 
     def transform(self, func, *args, **kwargs):
         """
@@ -1417,6 +1460,11 @@ def _group_labels(values):
 
 def sort_group_labels(ids, labels, counts):
     n = len(ids)
+
+    # corner all NA case
+    if n == 0:
+        return ids, labels, counts
+
     rng = np.arange(n)
     values = Series(ids, index=rng, dtype=object).values
     indexer = values.argsort()
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 3ba95288d..2d4533df2 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -1,11 +1,9 @@
-from collections import defaultdict
 import itertools
 
 from numpy import nan
 import numpy as np
 
 from pandas.core.index import Index, _ensure_index
-from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
 import pandas._tseries as lib
 
@@ -1082,210 +1080,3 @@ def _union_items_slow(all_items):
         else:
             seen = seen.union(items)
     return seen
-
-def join_managers(left, right, axis=1, how='left', copy=True):
-    join_index, left_indexer, right_indexer = \
-        left.axes[axis].join(right.axes[axis], how=how, return_indexers=True)
-    op = _JoinOperation(left, right, join_index, left_indexer,
-                        right_indexer, axis=axis)
-    return op.get_result(copy=copy)
-
-class _JoinOperation(object):
-    """
-    Object responsible for orchestrating efficient join operation between two
-    BlockManager data structures
-    """
-    def __init__(self, left, right, join_index, left_indexer, right_indexer,
-                 axis=1):
-        assert(axis > 0)
-
-        if not left.is_consolidated():
-            left = left.consolidate()
-        if not right.is_consolidated():
-            right = right.consolidate()
-
-        self.left = left
-        self.right = right
-        self.axis = axis
-
-        self.join_index = join_index
-        self.lindexer = left_indexer
-        self.rindexer = right_indexer
-
-        # do NOT sort
-        self.result_items = left.items.append(right.items)
-        self.result_axes = list(left.axes)
-        self.result_axes[0] = self.result_items
-        self.result_axes[axis] = self.join_index
-
-    def get_result(self, copy=False):
-        """
-        Parameters
-        ----------
-        other
-        lindexer
-        lmask
-        rindexer
-        rmask
-
-        Returns
-        -------
-        merged : BlockManager
-        """
-        left_blockmap, right_blockmap = self._prepare_blocks()
-
-        result_blocks = []
-
-        # maybe want to enable flexible copying
-
-        kinds = set(left_blockmap) | set(right_blockmap)
-        for klass in kinds:
-            lblk = left_blockmap.get(klass)
-            rblk = right_blockmap.get(klass)
-
-            if lblk and rblk:
-                # true merge, do not produce intermediate copy
-                res_blk = self._merge_blocks(lblk, rblk)
-            elif lblk:
-                res_blk = self._reindex_block(lblk, side='left')
-            else:
-                res_blk = self._reindex_block(rblk, side='right')
-
-            result_blocks.append(res_blk)
-
-        return BlockManager(result_blocks, self.result_axes)
-
-    def _prepare_blocks(self):
-        lblocks = self.left.blocks
-        rblocks = self.right.blocks
-
-        # will short-circuit and not compute lneed_masking
-        if self.lneed_masking:
-            lblocks = self._upcast_blocks(lblocks)
-
-        if self.rneed_masking:
-            rblocks = self._upcast_blocks(rblocks)
-
-        left_blockmap = dict((type(blk), blk) for blk in lblocks)
-        right_blockmap = dict((type(blk), blk) for blk in rblocks)
-
-        return left_blockmap, right_blockmap
-
-    def _reindex_block(self, block, side='left', copy=True):
-        if side == 'left':
-            indexer = self.lindexer
-            mask, need_masking = self.lmask_info
-        else:
-            indexer = self.rindexer
-            mask, need_masking = self.rmask_info
-
-        # still some inefficiency here for bool/int64 because in the case where
-        # no masking is needed, take_fast will recompute the mask
-
-        if indexer is None and copy:
-            result = block.copy()
-        else:
-            result = block.reindex_axis(indexer, mask, need_masking,
-                                        axis=self.axis)
-
-        result.ref_items = self.result_items
-        return result
-
-    @cache_readonly
-    def lmask_info(self):
-        if (self.lindexer is None or
-            not self._may_need_upcasting(self.left.blocks)):
-            lmask = None
-            lneed_masking = False
-        else:
-            lmask = self.lindexer == -1
-            lneed_masking = lmask.any()
-
-        return lmask, lneed_masking
-
-    @cache_readonly
-    def rmask_info(self):
-        if (self.rindexer is None or
-            not self._may_need_upcasting(self.right.blocks)):
-            rmask = None
-            rneed_masking = False
-        else:
-            rmask = self.rindexer == -1
-            rneed_masking = rmask.any()
-
-        return rmask, rneed_masking
-
-    @property
-    def lneed_masking(self):
-        return self.lmask_info[1]
-
-    @property
-    def rneed_masking(self):
-        return self.rmask_info[1]
-
-    @staticmethod
-    def _may_need_upcasting(blocks):
-        for block in blocks:
-            if isinstance(block, (IntBlock, BoolBlock)):
-                return True
-        return False
-
-    def _merge_blocks(self, lblk, rblk):
-        lidx = self.lindexer
-        ridx = self.rindexer
-
-        n = lblk.values.shape[self.axis] if lidx is None else len(lidx)
-        lk = len(lblk.items)
-        rk = len(rblk.items)
-
-        out_shape = list(lblk.shape)
-        out_shape[0] = lk + rk
-        out_shape[self.axis] = n
-
-        out = np.empty(out_shape, dtype=lblk.values.dtype)
-
-        # is this really faster than assigning to arr.flat?
-        if lidx is None:
-            # out[:lk] = lblk.values
-            com.take_fast(lblk.values, np.arange(n, dtype='i4'),
-                          None, False,
-                          axis=self.axis, out=out[:lk])
-        else:
-            # write out the values to the result array
-            com.take_fast(lblk.values, lidx, None, False,
-                             axis=self.axis, out=out[:lk])
-        if ridx is None:
-            # out[lk:] = lblk.values
-            com.take_fast(rblk.values, np.arange(n, dtype='i4'),
-                          None, False,
-                          axis=self.axis, out=out[lk:])
-        else:
-            com.take_fast(rblk.values, ridx, None, False,
-                          axis=self.axis, out=out[lk:])
-
-        # does not sort
-        new_items = lblk.items.append(rblk.items)
-        return make_block(out, new_items, self.result_items)
-
-    @staticmethod
-    def _upcast_blocks(blocks):
-        """
-        Upcast and consolidate if necessary
-        """
-        # if not need_masking:
-        #     return blocks
-
-        new_blocks = []
-        for block in blocks:
-            if isinstance(block, IntBlock):
-                newb = make_block(block.values.astype(float), block.items,
-                                  block.ref_items)
-            elif isinstance(block, BoolBlock):
-                newb = make_block(block.values.astype(object), block.items,
-                                  block.ref_items)
-            else:
-                newb = block
-            new_blocks.append(newb)
-
-        # use any ref_items
-        return _consolidate(new_blocks, newb.ref_items)
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index c4544eb14..974dd44df 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -523,6 +523,12 @@ class SparseDataFrame(DataFrame):
         f = ('%s' + ('%s' % suffix)).__mod__
         return self.rename(columns=f)
 
+    def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix=''):
+        if on is not None:
+            return self._join_on(other, on, how, lsuffix, rsuffix)
+        else:
+            return self._join_index(other, how, lsuffix, rsuffix)
+
     def _join_on(self, other, on, how, lsuffix, rsuffix):
         # need to implement?
         raise NotImplementedError
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index 5dc7aa087..3b30ca32a 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -395,6 +395,25 @@ def group_var(ndarray[float64_t] out,
             out[i] = ((ct * sumxx[i] - sumx[i] * sumx[i]) /
                       (ct * ct - ct))
 
+def group_count(ndarray[int32_t] values, Py_ssize_t size):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[int32_t] counts
+
+    counts = np.zeros(size, dtype='i4')
+    for i in range(n):
+        counts[values[i]] += 1
+    return counts
+
+def lookup_values(ndarray[object] values, dict mapping):
+    cdef:
+        Py_ssize_t i, n = len(values)
+
+    result = np.empty(n, dtype='O')
+    for i in range(n):
+        result[i] = mapping[values[i]]
+    return maybe_convert_objects(result)
+
 def reduce_mean(ndarray[object] indices,
                 ndarray[object] buckets,
                 ndarray[float64_t] values,
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index baeb140a7..8d6c68371 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -148,7 +148,7 @@ class TestGroupBy(unittest.TestCase):
 
         # DataFrame
         grouped = self.tsframe.groupby(self.tsframe['A'] * np.nan)
-        assert_frame_equal(grouped.sum(), DataFrame({}))
+        assert_frame_equal(grouped.sum(), DataFrame(columns=self.tsframe.columns))
         assert_frame_equal(grouped.agg(np.sum), DataFrame({}))
         assert_frame_equal(grouped.apply(np.sum), DataFrame({}))
 
@@ -791,7 +791,9 @@ class TestGroupBy(unittest.TestCase):
 
         for i, ping in enumerate(grouped.groupings):
             the_counts = self.mframe.groupby(level=i).count()['A']
-            assert_almost_equal(ping.counts, the_counts)
+            other_counts = Series(ping.counts, ping.group_index)
+            assert_almost_equal(the_counts,
+                                other_counts.reindex(the_counts.index))
 
     def test_groupby_level(self):
         frame = self.mframe
@@ -803,6 +805,9 @@ class TestGroupBy(unittest.TestCase):
         expected0 = frame.groupby(deleveled['first'].values).sum()
         expected1 = frame.groupby(deleveled['second'].values).sum()
 
+        expected0 = expected0.reindex(frame.index.levels[0])
+        expected1 = expected1.reindex(frame.index.levels[1])
+
         self.assert_(result0.index.name == 'first')
         self.assert_(result1.index.name == 'second')
 
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index 340fae8c9..ef99d7fbd 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -733,6 +733,13 @@ class TestMultiLevel(unittest.TestCase):
             leftside = grouped.agg(aggf)
             rightside = getattr(frame, op)(level=level, axis=axis,
                                            skipna=skipna)
+
+            # for good measure, groupby detail
+            level_index = frame._get_axis(axis).levels[level]
+
+            self.assert_(leftside._get_axis(axis).equals(level_index))
+            self.assert_(rightside._get_axis(axis).equals(level_index))
+
             assert_frame_equal(leftside, rightside)
 
     def test_frame_series_agg_multiple_levels(self):
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index c40ff9d0a..ba2b6c968 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -6,7 +6,9 @@ import numpy as np
 
 from pandas.core.frame import DataFrame
 from pandas.core.index import Index
-from pandas.core.internals import _JoinOperation
+from pandas.core.internals import (IntBlock, BoolBlock, BlockManager,
+                                   make_block, _consolidate)
+from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
 
 import pandas._tseries as lib
@@ -22,9 +24,7 @@ def merge(left, right, how='left', on=None, left_on=None, right_on=None,
     ----------
     left : DataFrame
     right : DataFrame
-    how : {'left', 'right', 'outer', 'inner'}
-        How to handle indexes of the two objects. Default: 'left'
-        for joining on index, None otherwise
+    how : {'left', 'right', 'outer', 'inner'}, default 'left'
         * left: use only keys from left frame (SQL: left outer join)
         * right: use only keys from right frame (SQL: right outer join)
         * outer: use union of keys from both frames (SQL: full outer join)
@@ -64,6 +64,13 @@ def merge(left, right, how='left', on=None, left_on=None, right_on=None,
 # TODO: transformations??
 # TODO: only copy DataFrames when modification necessary
 
+def join_managers(left, right, axis=1, how='left', copy=True):
+    join_index, left_indexer, right_indexer = \
+        left.axes[axis].join(right.axes[axis], how=how, return_indexers=True)
+    op = _JoinOperation(left, right, join_index, left_indexer,
+                        right_indexer, axis=axis)
+    return op.get_result(copy=copy)
+
 class _MergeOperation(object):
 
     def __init__(self, left, right, how='inner', on=None,
@@ -87,43 +94,59 @@ class _MergeOperation(object):
         self.left_index = left_index
         self.right_index = right_index
 
-    def get_result(self):
         # note this function has side effects
-        left_join_keys, right_join_keys, join_names = self._get_merge_keys()
-
-        # this is a bit kludgy
-        ldata, rdata = self._get_merge_data(join_names)
+        (self.left_join_keys,
+         self.right_join_keys,
+         self.join_names) = self._get_merge_keys()
 
-        # max groups = largest possible number of distinct groups
-        left_key, right_key, max_groups = \
-            _get_group_keys(left_join_keys, right_join_keys, sort=self.sort)
-
-        join_func = _join_functions[self.how]
-        left_indexer, right_indexer = join_func(left_key.astype('i4'),
-                                                right_key.astype('i4'),
-                                                max_groups)
+    def get_result(self):
+        left_indexer, right_indexer = self._get_join_indexers()
+        new_axis = self._get_new_axis(left_indexer)
 
-        new_axis = Index(np.arange(len(left_indexer)))
+        # this is a bit kludgy
+        ldata, rdata = self._get_merge_data(self.join_names)
 
         # TODO: more efficiently handle group keys to avoid extra consolidation!
-
         join_op = _JoinOperation(ldata, rdata, new_axis,
                                  left_indexer, right_indexer, axis=1)
 
         result_data = join_op.get_result(copy=self.copy)
         result = DataFrame(result_data)
 
+        self._maybe_add_join_keys(result, left_indexer, right_indexer)
+
+        return result
+
+    def _maybe_add_join_keys(self, result, left_indexer, right_indexer):
         # insert group keys
-        for i, name in enumerate(join_names):
+        for i, name in enumerate(self.join_names):
             # a faster way?
-            key_col = com.take_1d(left_join_keys[i], left_indexer)
+            key_col = com.take_1d(self.left_join_keys[i], left_indexer)
             na_indexer = (left_indexer == -1).nonzero()[0]
             right_na_indexer = right_indexer.take(na_indexer)
-            key_col.put(na_indexer, com.take_1d(right_join_keys[i],
+            key_col.put(na_indexer, com.take_1d(self.right_join_keys[i],
                                                 right_na_indexer))
             result.insert(i, name, key_col)
 
-        return result
+    def _get_join_indexers(self):
+        # max groups = largest possible number of distinct groups
+        left_key, right_key, max_groups = \
+            _get_group_keys(self.left_join_keys, self.right_join_keys,
+                            sort=self.sort)
+
+        join_func = _join_functions[self.how]
+        left_indexer, right_indexer = join_func(left_key.astype('i4'),
+                                                right_key.astype('i4'),
+                                                max_groups)
+
+        return left_indexer, right_indexer
+
+    def _get_new_axis(self, left_indexer):
+        if left_indexer is None:
+            new_axis = self.left.index
+        else:
+            new_axis = Index(np.arange(len(left_indexer)))
+        return new_axis
 
     def _get_merge_data(self, join_names):
         """
@@ -301,3 +324,204 @@ def _sort_labels(uniques, left, right):
     reverse_indexer = np.empty(len(sorter), dtype=np.int32)
     reverse_indexer.put(sorter, np.arange(len(sorter)))
     return reverse_indexer.take(left), reverse_indexer.take(right)
+
+
+class _JoinOperation(object):
+    """
+    Object responsible for orchestrating efficient join operation between two
+    BlockManager data structures
+    """
+    def __init__(self, left, right, join_index, left_indexer, right_indexer,
+                 axis=1):
+        assert(axis > 0)
+
+        if not left.is_consolidated():
+            left = left.consolidate()
+        if not right.is_consolidated():
+            right = right.consolidate()
+
+        self.left = left
+        self.right = right
+        self.axis = axis
+
+        self.join_index = join_index
+        self.lindexer = left_indexer
+        self.rindexer = right_indexer
+
+        # do NOT sort
+        self.result_items = left.items.append(right.items)
+        self.result_axes = list(left.axes)
+        self.result_axes[0] = self.result_items
+        self.result_axes[axis] = self.join_index
+
+    def get_result(self, copy=False):
+        """
+        Parameters
+        ----------
+        other
+        lindexer
+        lmask
+        rindexer
+        rmask
+
+        Returns
+        -------
+        merged : BlockManager
+        """
+        left_blockmap, right_blockmap = self._prepare_blocks()
+
+        result_blocks = []
+
+        # maybe want to enable flexible copying
+
+        kinds = set(left_blockmap) | set(right_blockmap)
+        for klass in kinds:
+            lblk = left_blockmap.get(klass)
+            rblk = right_blockmap.get(klass)
+
+            if lblk and rblk:
+                # true merge, do not produce intermediate copy
+                res_blk = self._merge_blocks(lblk, rblk)
+            elif lblk:
+                res_blk = self._reindex_block(lblk, side='left')
+            else:
+                res_blk = self._reindex_block(rblk, side='right')
+
+            result_blocks.append(res_blk)
+
+        return BlockManager(result_blocks, self.result_axes)
+
+    def _prepare_blocks(self):
+        lblocks = self.left.blocks
+        rblocks = self.right.blocks
+
+        # will short-circuit and not compute lneed_masking
+        if self.lneed_masking:
+            lblocks = self._upcast_blocks(lblocks)
+
+        if self.rneed_masking:
+            rblocks = self._upcast_blocks(rblocks)
+
+        left_blockmap = dict((type(blk), blk) for blk in lblocks)
+        right_blockmap = dict((type(blk), blk) for blk in rblocks)
+
+        return left_blockmap, right_blockmap
+
+    def _reindex_block(self, block, side='left', copy=True):
+        if side == 'left':
+            indexer = self.lindexer
+            mask, need_masking = self.lmask_info
+        else:
+            indexer = self.rindexer
+            mask, need_masking = self.rmask_info
+
+        # still some inefficiency here for bool/int64 because in the case where
+        # no masking is needed, take_fast will recompute the mask
+
+        if indexer is None and copy:
+            result = block.copy()
+        else:
+            result = block.reindex_axis(indexer, mask, need_masking,
+                                        axis=self.axis)
+
+        result.ref_items = self.result_items
+        return result
+
+    @cache_readonly
+    def lmask_info(self):
+        if (self.lindexer is None or
+            not self._may_need_upcasting(self.left.blocks)):
+            lmask = None
+            lneed_masking = False
+        else:
+            lmask = self.lindexer == -1
+            lneed_masking = lmask.any()
+
+        return lmask, lneed_masking
+
+    @cache_readonly
+    def rmask_info(self):
+        if (self.rindexer is None or
+            not self._may_need_upcasting(self.right.blocks)):
+            rmask = None
+            rneed_masking = False
+        else:
+            rmask = self.rindexer == -1
+            rneed_masking = rmask.any()
+
+        return rmask, rneed_masking
+
+    @property
+    def lneed_masking(self):
+        return self.lmask_info[1]
+
+    @property
+    def rneed_masking(self):
+        return self.rmask_info[1]
+
+    @staticmethod
+    def _may_need_upcasting(blocks):
+        for block in blocks:
+            if isinstance(block, (IntBlock, BoolBlock)):
+                return True
+        return False
+
+    def _merge_blocks(self, lblk, rblk):
+        lidx = self.lindexer
+        ridx = self.rindexer
+
+        n = lblk.values.shape[self.axis] if lidx is None else len(lidx)
+        lk = len(lblk.items)
+        rk = len(rblk.items)
+
+        out_shape = list(lblk.shape)
+        out_shape[0] = lk + rk
+        out_shape[self.axis] = n
+
+        out = np.empty(out_shape, dtype=lblk.values.dtype)
+
+        # is this really faster than assigning to arr.flat?
+        if lidx is None:
+            # out[:lk] = lblk.values
+            com.take_fast(lblk.values, np.arange(n, dtype='i4'),
+                          None, False,
+                          axis=self.axis, out=out[:lk])
+        else:
+            # write out the values to the result array
+            com.take_fast(lblk.values, lidx, None, False,
+                             axis=self.axis, out=out[:lk])
+        if ridx is None:
+            # out[lk:] = lblk.values
+            com.take_fast(rblk.values, np.arange(n, dtype='i4'),
+                          None, False,
+                          axis=self.axis, out=out[lk:])
+        else:
+            com.take_fast(rblk.values, ridx, None, False,
+                          axis=self.axis, out=out[lk:])
+
+        # does not sort
+        new_items = lblk.items.append(rblk.items)
+        return make_block(out, new_items, self.result_items)
+
+    @staticmethod
+    def _upcast_blocks(blocks):
+        """
+        Upcast and consolidate if necessary
+        """
+        # if not need_masking:
+        #     return blocks
+
+        new_blocks = []
+        for block in blocks:
+            if isinstance(block, IntBlock):
+                newb = make_block(block.values.astype(float), block.items,
+                                  block.ref_items)
+            elif isinstance(block, BoolBlock):
+                newb = make_block(block.values.astype(object), block.items,
+                                  block.ref_items)
+            else:
+                newb = block
+            new_blocks.append(newb)
+
+        # use any ref_items
+        return _consolidate(new_blocks, newb.ref_items)
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 033a3c53c..e9b90d9b8 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -28,7 +28,7 @@ def f():
     df.groupby(['key1', 'key2']).agg(lambda x: x.values.sum())
 """
 
-stmt1 = "df.groupby(['key1', 'key2'])['data'].agg(lambda x: x.values.sum())"
+stmt1 = "df.groupby(['key1', 'key2'])['data1'].agg(lambda x: x.values.sum())"
 groupby_multi_python = Benchmark(stmt1, setup,
                                  name="groupby_multi_python",
                                  start_date=datetime(2011, 7, 1))
diff --git a/vb_suite/join_merge.py b/vb_suite/join_merge.py
new file mode 100644
index 000000000..759ac758b
--- /dev/null
+++ b/vb_suite/join_merge.py
@@ -0,0 +1,69 @@
+from vbench.benchmark import Benchmark
+from datetime import datetime
+
+common_setup = """from pandas_vb_common import *
+"""
+
+setup = common_setup + """
+level1 = np.array([rands(10) for _ in xrange(10)], dtype='O')
+level2 = np.array([rands(10) for _ in xrange(1000)], dtype='O')
+label1 = np.arange(10).repeat(1000)
+label2 = np.tile(np.arange(1000), 10)
+
+key1 = np.tile(level1.take(label1), 10)
+key2 = np.tile(level2.take(label2), 10)
+
+shuf = np.arange(100000)
+random.shuffle(shuf)
+try:
+    index2 = MultiIndex(levels=[level1, level2], labels=[label1, label2])
+    index3 = MultiIndex(levels=[np.arange(10), np.arange(100), np.arange(100)],
+                        labels=[np.arange(10).repeat(10000),
+                                np.tile(np.arange(100).repeat(100), 10),
+                                np.tile(np.tile(np.arange(100), 100), 10)])
+    df_multi = DataFrame(np.random.randn(len(index2), 4), index=index2,
+                         columns=['A', 'B', 'C', 'D'])
+except:  # pre-MultiIndex
+    pass
+
+try:
+    DataFrame = DataMatrix
+except:
+    pass
+
+df = DataFrame({'data1' : np.random.randn(100000),
+                'data2' : np.random.randn(100000),
+                'key1' : key1,
+                'key2' : key2})
+
+
+df_key1 = DataFrame(np.random.randn(len(level1), 4), index=level1,
+                    columns=['A', 'B', 'C', 'D'])
+df_key2 = DataFrame(np.random.randn(len(level2), 4), index=level2,
+                    columns=['A', 'B', 'C', 'D'])
+"""
+
+#----------------------------------------------------------------------
+# DataFrame joins on key
+
+join_dataframe_index_single_key_small = \
+    Benchmark("df.join(df_key1, on='key1')", setup,
+              name='join_dataframe_index_single_key_small')
+
+join_dataframe_index_single_key_bigger = \
+    Benchmark("df.join(df_key2, on='key2')", setup,
+              name='join_dataframe_index_single_key_bigger')
+
+join_dataframe_index_multi = \
+    Benchmark("df.join(df_multi, on=['key1', 'key2'])", setup,
+              name='join_dataframe_index_multi',
+              start_date=datetime(2011, 9, 1))
+
+#----------------------------------------------------------------------
+# DataFrame joins on index
+
+
+
+#----------------------------------------------------------------------
+# Merges
+
diff --git a/vb_suite/pandas_vb_common.py b/vb_suite/pandas_vb_common.py
index ee3eb4865..c7c826d0a 100644
--- a/vb_suite/pandas_vb_common.py
+++ b/vb_suite/pandas_vb_common.py
@@ -1,4 +1,5 @@
 from pandas import *
+from pandas.util.testing import rands
 import pandas.util.testing as tm
 import random
 import numpy as np
diff --git a/vb_suite/suite.py b/vb_suite/suite.py
index d017ab1f3..20ff798b0 100644
--- a/vb_suite/suite.py
+++ b/vb_suite/suite.py
@@ -5,7 +5,7 @@ import os
 
 modules = ['groupby', 'indexing', 'reindex', 'binary_ops',
            'sparse', 'index_object', 'miscellaneous',
-           'stat_ops']
+           'stat_ops', 'join_merge']
 
 by_module = {}
 benchmarks = []
