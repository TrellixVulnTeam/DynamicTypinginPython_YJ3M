commit 7097368a20fdbec7505092e723d359a871f16c10
Author: Phillip Cloud <cpcloud@gmail.com>
Date:   Tue Jun 4 17:48:32 2013 -0400

    ENH: allow fallback when lxml fails to parse
    
    DOC: add release notes for new list convention
    
    ENH: add list of parsers
    
    CLN: remove raise_on_error
    
    BUG: fix format string
    
    BUG: fix different raise type
    
    BLD: travis python 3.2 is strange
    
    BUG: bring in urlparse again since 2to3 shows proper conversion
    
    TST: fix for python26

diff --git a/RELEASE.rst b/RELEASE.rst
index 982715680..6a0c22c77 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -70,7 +70,6 @@ pandas 0.11.1
   - ``melt`` now accepts the optional parameters ``var_name`` and ``value_name``
     to specify custom column names of the returned DataFrame (GH3649_),
     thanks @hoechenberger
-  - ``read_html`` no longer performs hard date conversion
   - Plotting functions now raise a ``TypeError`` before trying to plot anything
     if the associated objects have have a dtype of ``object`` (GH1818_,
     GH3572_). This happens before any drawing takes place which elimnates any
@@ -133,6 +132,9 @@ pandas 0.11.1
     as an int, maxing with ``int64``, to avoid precision issues (GH3733_)
   - ``na_values`` in a list provided to ``read_csv/read_excel`` will match string and numeric versions
     e.g. ``na_values=['99']`` will match 99 whether the column ends up being int, float, or string (GH3611_)
+  - ``read_html`` now defaults to ``None`` when reading, and falls back on
+    ``bs4`` + ``html5lib`` when lxml fails to parse. a list of parsers to try
+    until success is also valid
 
 **Bug Fixes**
 
diff --git a/doc/source/io.rst b/doc/source/io.rst
index 1c615ca27..a65b7c902 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1054,6 +1054,21 @@ Read in pandas ``to_html`` output (with some loss of floating point precision)
    dfin[0].columns
    np.allclose(df, dfin[0])
 
+``lxml`` will raise an error on a failed parse if that is the only parser you
+provide
+
+.. ipython:: python
+
+   dfs = read_html(url, match='Metcalf Bank', index_col=0, flavor=['lxml'])
+
+However, if you have bs4 and html5lib installed and pass ``None`` or ``['lxml',
+'bs4']`` then the parse will most likely succeed. Note that *as soon as a parse
+succeeds, the function will return*.
+
+.. ipython:: python
+
+   dfs = read_html(url, match='Metcalf Bank', index_col=0, flavor=['lxml', 'bs4'])
+
 
 Writing to HTML files
 ~~~~~~~~~~~~~~~~~~~~~~
diff --git a/doc/source/v0.11.1.txt b/doc/source/v0.11.1.txt
index 982b2f9f2..ee2c15d42 100644
--- a/doc/source/v0.11.1.txt
+++ b/doc/source/v0.11.1.txt
@@ -139,6 +139,10 @@ API changes
 
     - sum, prod, mean, std, var, skew, kurt, corr, and cov
 
+  - ``read_html`` now defaults to ``None`` when reading, and falls back on
+    ``bs4`` + ``html5lib`` when lxml fails to parse. a list of parsers to try
+    until success is also valid
+
 Enhancements
 ~~~~~~~~~~~~
 
diff --git a/pandas/io/html.py b/pandas/io/html.py
index 9b2f292d3..5b83c7c18 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -7,9 +7,10 @@ import os
 import re
 import numbers
 import urllib2
+import urlparse
 import contextlib
 import collections
-import urlparse
+
 
 try:
     from importlib import import_module
@@ -18,10 +19,34 @@ except ImportError:
 
 import numpy as np
 
-from pandas import DataFrame, MultiIndex, Index, Series, isnull
+from pandas import DataFrame, MultiIndex, isnull
 from pandas.io.parsers import _is_url
 
 
+try:
+    import_module('bs4')
+except ImportError:
+    _HAS_BS4 = False
+else:
+    _HAS_BS4 = True
+
+
+try:
+    import_module('lxml')
+except ImportError:
+    _HAS_LXML = False
+else:
+    _HAS_LXML = True
+
+
+try:
+    import_module('html5lib')
+except ImportError:
+    _HAS_HTML5LIB = False
+else:
+    _HAS_HTML5LIB = True
+
+
 #############
 # READ HTML #
 #############
@@ -345,7 +370,7 @@ class _HtmlFrameParser(object):
         return self._parse_raw_data(res)
 
 
-class _BeautifulSoupLxmlFrameParser(_HtmlFrameParser):
+class _BeautifulSoupHtml5LibFrameParser(_HtmlFrameParser):
     """HTML to DataFrame parser that uses BeautifulSoup under the hood.
 
     See Also
@@ -359,7 +384,8 @@ class _BeautifulSoupLxmlFrameParser(_HtmlFrameParser):
     :class:`pandas.io.html._HtmlFrameParser`.
     """
     def __init__(self, *args, **kwargs):
-        super(_BeautifulSoupLxmlFrameParser, self).__init__(*args, **kwargs)
+        super(_BeautifulSoupHtml5LibFrameParser, self).__init__(*args,
+                                                                **kwargs)
         from bs4 import SoupStrainer
         self._strainer = SoupStrainer('table')
 
@@ -406,17 +432,6 @@ class _BeautifulSoupLxmlFrameParser(_HtmlFrameParser):
             raise AssertionError('No text parsed from document')
         return raw_text
 
-    def _build_doc(self):
-        from bs4 import BeautifulSoup
-        return BeautifulSoup(self._setup_build_doc(), features='lxml',
-                             parse_only=self._strainer)
-
-
-class _BeautifulSoupHtml5LibFrameParser(_BeautifulSoupLxmlFrameParser):
-    def __init__(self, *args, **kwargs):
-        super(_BeautifulSoupHtml5LibFrameParser, self).__init__(*args,
-                                                                **kwargs)
-
     def _build_doc(self):
         from bs4 import BeautifulSoup
         return BeautifulSoup(self._setup_build_doc(), features='html5lib')
@@ -516,16 +531,27 @@ class _LxmlFrameParser(_HtmlFrameParser):
         --------
         pandas.io.html._HtmlFrameParser._build_doc
         """
-        from lxml.html import parse, fromstring
-        from lxml.html.clean import clean_html
+        from lxml.html import parse, fromstring, HTMLParser
+        from lxml.etree import XMLSyntaxError
+        parser = HTMLParser(recover=False)
 
         try:
             # try to parse the input in the simplest way
-            r = parse(self.io)
-        except (UnicodeDecodeError, IOError) as e:
+            r = parse(self.io, parser=parser)
+
+            try:
+                r = r.getroot()
+            except AttributeError:
+                pass
+        except (UnicodeDecodeError, IOError):
             # if the input is a blob of html goop
             if not _is_url(self.io):
-                r = fromstring(self.io)
+                r = fromstring(self.io, parser=parser)
+
+                try:
+                    r = r.getroot()
+                except AttributeError:
+                    pass
             else:
                 # not a url
                 scheme = urlparse.urlparse(self.io).scheme
@@ -536,8 +562,11 @@ class _LxmlFrameParser(_HtmlFrameParser):
                     raise ValueError(msg)
                 else:
                     # something else happened: maybe a faulty connection
-                    raise e
-        return clean_html(r)
+                    raise
+        else:
+            if not hasattr(r, 'text_content'):
+                raise XMLSyntaxError("no text parsed from document", 0, 0, 0)
+        return r
 
     def _parse_tbody(self, table):
         return table.xpath('.//tbody')
@@ -559,17 +588,6 @@ class _LxmlFrameParser(_HtmlFrameParser):
                 table.xpath(expr)]
 
 
-def _maybe_convert_index_type(index):
-    try:
-        index = index.astype(int)
-    except (TypeError, ValueError):
-        if not isinstance(index, MultiIndex):
-            s = Series(index, name=index.name)
-            index = Index(s.convert_objects(convert_numeric=True),
-                          name=index.name)
-    return index
-
-
 def _data_to_frame(data, header, index_col, infer_types, skiprows):
     """Parse a BeautifulSoup table into a DataFrame.
 
@@ -665,18 +683,12 @@ def _data_to_frame(data, header, index_col, infer_types, skiprows):
             names = [name or None for name in df.index.names]
             df.index = MultiIndex.from_tuples(df.index.values, names=names)
 
-    if infer_types:
-        df.index = _maybe_convert_index_type(df.index)
-        df.columns = _maybe_convert_index_type(df.columns)
-
     return df
 
 
-_invalid_parsers = {'lxml': _LxmlFrameParser,
-                    'bs4': _BeautifulSoupLxmlFrameParser}
-_valid_parsers = {'html5lib': _BeautifulSoupHtml5LibFrameParser}
-_all_parsers = _valid_parsers.copy()
-_all_parsers.update(_invalid_parsers)
+_valid_parsers = {'lxml': _LxmlFrameParser, None: _LxmlFrameParser,
+                  'html5lib': _BeautifulSoupHtml5LibFrameParser,
+                  'bs4': _BeautifulSoupHtml5LibFrameParser}
 
 
 def _parser_dispatch(flavor):
@@ -696,46 +708,74 @@ def _parser_dispatch(flavor):
     ------
     AssertionError
         * If `flavor` is not a valid backend.
+    ImportError
+        * If you do not have the requested `flavor`
     """
     valid_parsers = _valid_parsers.keys()
     if flavor not in valid_parsers:
-        raise AssertionError('"{0}" is not a valid flavor'.format(flavor))
+        raise AssertionError('"{0!r}" is not a valid flavor, valid flavors are'
+                             ' {1}'.format(flavor, valid_parsers))
 
-    if flavor == 'bs4':
-        try:
-            import_module('lxml')
-            parser_t = _BeautifulSoupLxmlFrameParser
-        except ImportError:
-            try:
-                import_module('html5lib')
-                parser_t = _BeautifulSoupHtml5LibFrameParser
-            except ImportError:
-                raise ImportError("read_html does not support the native "
-                                  "Python 'html.parser' backend for bs4, "
-                                  "please install either 'lxml' or 'html5lib'")
-    elif flavor == 'html5lib':
-        try:
-            # much better than python's builtin
-            import_module('html5lib')
-            parser_t = _BeautifulSoupHtml5LibFrameParser
-        except ImportError:
+    if flavor in ('bs4', 'html5lib'):
+        if not _HAS_HTML5LIB:
             raise ImportError("html5lib not found please install it")
+        if not _HAS_BS4:
+            raise ImportError("bs4 not found please install it")
     else:
-        parser_t = _LxmlFrameParser
-    return parser_t
+        if not _HAS_LXML:
+            raise ImportError("lxml not found please install it")
+    return _valid_parsers[flavor]
+
+
+def _validate_parser_flavor(flavor):
+    if flavor is None:
+        flavor = ['lxml', 'bs4']
+    elif isinstance(flavor, basestring):
+        flavor = [flavor]
+    elif isinstance(flavor, collections.Iterable):
+        if not all(isinstance(flav, basestring) for flav in flavor):
+            raise TypeError('{0} is not an iterable of strings'.format(flavor))
+    else:
+        raise TypeError('{0} is not a valid "flavor"'.format(flavor))
+
+    flavor = list(flavor)
+    valid_flavors = _valid_parsers.keys()
+
+    if not set(flavor) & set(valid_flavors):
+        raise ValueError('{0} is not a valid set of flavors, valid flavors are'
+                         ' {1}'.format(flavor, valid_flavors))
+    return flavor
 
 
-def _parse(parser, io, match, flavor, header, index_col, skiprows, infer_types,
-           attrs):
+def _parse(flavor, io, match, header, index_col, skiprows, infer_types, attrs):
     # bonus: re.compile is idempotent under function iteration so you can pass
     # a compiled regex to it and it will return itself
-    p = parser(io, re.compile(match), attrs)
-    tables = p.parse_tables()
+    flavor = _validate_parser_flavor(flavor)
+    compiled_match = re.compile(match)
+
+    succeeded = False
+    while not succeeded and flavor:
+        flav = flavor.pop()
+        parser = _parser_dispatch(flav)
+        p = parser(io, compiled_match, attrs)
+
+        try:
+            tables = p.parse_tables()
+        except Exception as e:
+            pass
+        else:
+            succeeded = True
+
+    if not succeeded:
+        # all flavors have been tried
+        assert not flavor
+        raise e
+
     return [_data_to_frame(table, header, index_col, infer_types, skiprows)
             for table in tables]
 
 
-def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
+def read_html(io, match='.+', flavor=None, header=None, index_col=None,
               skiprows=None, infer_types=True, attrs=None):
     r"""Read an HTML table into a DataFrame.
 
@@ -747,7 +787,7 @@ def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
         the http, ftp and file url protocols. If you have a URI that starts
         with ``'https'`` you might removing the ``'s'``.
 
-    match : str or regex, optional
+    match : str or regex, optional, default '.+'
         The set of tables containing text matching this regex or string will be
         returned. Unless the HTML is extremely simple you will probably need to
         pass a non-empty string here. Defaults to '.+' (match any non-empty
@@ -755,23 +795,24 @@ def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
         This value is converted to a regular expression so that there is
         consistent behavior between Beautiful Soup and lxml.
 
-    flavor : str, {'html5lib'}
-        The parsing engine to use under the hood. Right now only ``html5lib``
-        is supported because it returns correct output whereas ``lxml`` does
-        not.
+    flavor : str, container of strings, default ``None``
+        The parsing engine to use under the hood. 'bs4' and 'html5lib' are
+        synonymous with each other, they are both there for backwards
+        compatibility. The default of ``None`` tries to use ``lxml`` to parse
+        and if that fails it falls back on ``bs4`` + ``html5lib``.
 
-    header : int or array-like or None, optional
+    header : int or array-like or None, optional, default ``None``
         The row (or rows for a MultiIndex) to use to make the columns headers.
-        Note that this row will be removed from the data. Defaults to None.
+        Note that this row will be removed from the data.
 
-    index_col : int or array-like or None, optional
+    index_col : int or array-like or None, optional, default ``None``
         The column to use to make the index. Note that this column will be
-        removed from the data. Defaults to None.
+        removed from the data.
 
-    skiprows : int or collections.Container or slice or None, optional
+    skiprows : int or collections.Container or slice or None, optional, default ``None``
         If an integer is given then skip this many rows after parsing the
         column header. If a sequence of integers is given skip those specific
-        rows (0-based). Defaults to None, i.e., no rows are skipped. Note that
+        rows (0-based). Note that
 
         .. code-block:: python
 
@@ -787,16 +828,15 @@ def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
         it is treated as "skip :math:`n` rows", *not* as "skip the
         :math:`n^\textrm{th}` row".
 
-    infer_types : bool, optional
+    infer_types : bool, optional, default ``True``
         Whether to convert numeric types and date-appearing strings to numbers
-        and dates, respectively. Defaults to True.
+        and dates, respectively.
 
-    attrs : dict or None, optional
+    attrs : dict or None, optional, default ``None``
         This is a dictionary of attributes that you can pass to use to identify
         the table in the HTML. These are not checked for validity before being
         passed to lxml or Beautiful Soup. However, these attributes must be
-        valid HTML table attributes to work correctly. Defaults to None. For
-        example,
+        valid HTML table attributes to work correctly. For example,
 
         .. code-block:: python
 
@@ -826,6 +866,9 @@ def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
 
     Notes
     -----
+    Before using this function you should probably read the :ref:`gotchas about
+    the parser libraries that this function uses <html-gotchas>`.
+
     There's as little cleaning of the data as possible due to the heterogeneity
     and general disorder of HTML on the web.
 
@@ -848,37 +891,13 @@ def read_html(io, match='.+', flavor='html5lib', header=None, index_col=None,
 
     Examples
     --------
-    Parse a table from a list of failed banks from the FDIC:
-
-    >>> from pandas import read_html, DataFrame
-    >>> url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
-    >>> dfs = read_html(url, match='Florida', attrs={'id': 'table'})
-    >>> assert dfs  # will not be empty if the call to read_html doesn't fail
-    >>> assert isinstance(dfs, list)  # read_html returns a list of DataFrames
-    >>> assert all(map(lambda x: isinstance(x, DataFrame), dfs))
-
-    Parse some spam infomation from the USDA:
-
-    >>> from pandas import read_html, DataFrame
-    >>> url = ('http://ndb.nal.usda.gov/ndb/foods/show/1732?fg=&man=&'
-    ...        'lfacet=&format=&count=&max=25&offset=&sort=&qlookup=spam')
-    >>> dfs = read_html(url, match='Water', header=0)
-    >>> assert dfs
-    >>> assert isinstance(dfs, list)
-    >>> assert all(map(lambda x: isinstance(x, DataFrame), dfs))
-
-    You can pass nothing to the `match` argument:
-
-    >>> from pandas import read_html, DataFrame
-    >>> url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
-    >>> dfs = read_html(url)
-    >>> print(len(dfs))  # this will most likely be greater than 1
+    See the :ref:`read_html documentation in the IO section of the docs
+    <io.read_html>` for many examples of reading HTML.
     """
     # Type check here. We don't want to parse only to fail because of an
     # invalid value of an integer skiprows.
     if isinstance(skiprows, numbers.Integral) and skiprows < 0:
         raise AssertionError('cannot skip rows starting from the end of the '
                              'data (you passed a negative value)')
-    parser = _parser_dispatch(flavor)
-    return _parse(parser, io, match, flavor, header, index_col, skiprows,
-                  infer_types, attrs)
+    return _parse(flavor, io, match, header, index_col, skiprows, infer_types,
+                  attrs)
diff --git a/pandas/io/tests/data/valid_markup.html b/pandas/io/tests/data/valid_markup.html
new file mode 100644
index 000000000..5db90da3b
--- /dev/null
+++ b/pandas/io/tests/data/valid_markup.html
@@ -0,0 +1,71 @@
+<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
+<html>
+    <head>
+        <meta name="generator" content=
+        "HTML Tidy for Linux (vers 25 March 2009), see www.w3.org">
+        <title></title>
+    </head>
+    <body>
+        <table border="1" class="dataframe">
+            <thead>
+                <tr style="text-align: right;">
+                    <th></th>
+                    <th>a</th>
+                    <th>b</th>
+                </tr>
+            </thead>
+            <tbody>
+                <tr>
+                    <th>0</th>
+                    <td>6</td>
+                    <td>7</td>
+                </tr>
+                <tr>
+                    <th>1</th>
+                    <td>4</td>
+                    <td>0</td>
+                </tr>
+                <tr>
+                    <th>2</th>
+                    <td>9</td>
+                    <td>4</td>
+                </tr>
+                <tr>
+                    <th>3</th>
+                    <td>7</td>
+                    <td>0</td>
+                </tr>
+                <tr>
+                    <th>4</th>
+                    <td>4</td>
+                    <td>3</td>
+                </tr>
+                <tr>
+                    <th>5</th>
+                    <td>5</td>
+                    <td>4</td>
+                </tr>
+                <tr>
+                    <th>6</th>
+                    <td>4</td>
+                    <td>5</td>
+                </tr>
+                <tr>
+                    <th>7</th>
+                    <td>1</td>
+                    <td>4</td>
+                </tr>
+                <tr>
+                    <th>8</th>
+                    <td>6</td>
+                    <td>7</td>
+                </tr>
+                <tr>
+                    <th>9</th>
+                    <td>8</td>
+                    <td>5</td>
+                </tr>
+            </tbody>
+        </table>
+    </body>
+</html>
diff --git a/pandas/io/tests/test_html.py b/pandas/io/tests/test_html.py
index ea3c0520d..93f8023f5 100644
--- a/pandas/io/tests/test_html.py
+++ b/pandas/io/tests/test_html.py
@@ -2,7 +2,6 @@ import os
 import re
 from cStringIO import StringIO
 from unittest import TestCase
-import numbers
 from urllib2 import urlopen
 from contextlib import closing
 import warnings
@@ -13,13 +12,11 @@ import numpy as np
 from numpy.random import rand
 from numpy.testing.decorators import slow
 
-from pandas.io.html import read_html, import_module, _parse, _LxmlFrameParser
-from pandas.io.html import _BeautifulSoupHtml5LibFrameParser
-from pandas.io.html import _BeautifulSoupLxmlFrameParser, _remove_whitespace
+from pandas.io.html import read_html, import_module
+from pandas.io.html import _remove_whitespace
 from pandas import DataFrame, MultiIndex, read_csv, Timestamp
 from pandas.util.testing import (assert_frame_equal, network,
                                  get_data_path)
-from numpy.testing.decorators import slow
 
 from pandas.util.testing import makeCustomDataframe as mkdf
 
@@ -37,7 +34,7 @@ def _skip_if_no(module_name):
         raise nose.SkipTest
 
 
-def _skip_if_none(module_names):
+def _skip_if_none_of(module_names):
     if isinstance(module_names, basestring):
         _skip_if_no(module_names)
     else:
@@ -47,12 +44,11 @@ def _skip_if_none(module_names):
 
 DATA_PATH = get_data_path()
 
-
 def isframe(x):
     return isinstance(x, DataFrame)
 
 
-def assert_framelist_equal(list1, list2):
+def assert_framelist_equal(list1, list2, *args, **kwargs):
     assert len(list1) == len(list2), ('lists are not of equal size '
                                       'len(list1) == {0}, '
                                       'len(list2) == {1}'.format(len(list1),
@@ -60,24 +56,33 @@ def assert_framelist_equal(list1, list2):
     assert all(map(lambda x, y: isframe(x) and isframe(y), list1, list2)), \
         'not all list elements are DataFrames'
     for frame_i, frame_j in zip(list1, list2):
-        assert_frame_equal(frame_i, frame_j)
+        assert_frame_equal(frame_i, frame_j, *args, **kwargs)
         assert not frame_i.empty, 'frames are both empty'
 
 
-def _run_read_html(parser, io, match='.+', flavor='bs4', header=None,
-                   index_col=None, skiprows=None, infer_types=False,
-                   attrs=None):
-    if isinstance(skiprows, numbers.Integral) and skiprows < 0:
-        raise AssertionError('cannot skip rows starting from the end of the '
-                             'data (you passed a negative value)')
-    return _parse(parser, io, match, flavor, header, index_col, skiprows,
-                  infer_types, attrs)
+class TestReadHtmlBase(TestCase):
+    def run_read_html(self, *args, **kwargs):
+        self.try_skip()
+        kwargs['flavor'] = kwargs.get('flavor', self.flavor)
+        return read_html(*args, **kwargs)
+
+    def try_skip(self):
+        _skip_if_none_of(('bs4', 'html5lib'))
+
+    def setup_data(self):
+        self.spam_data = os.path.join(DATA_PATH, 'spam.html')
+        self.banklist_data = os.path.join(DATA_PATH, 'banklist.html')
 
+    def setup_flavor(self):
+        self.flavor = 'bs4'
+
+    def setUp(self):
+        self.setup_data()
+        self.setup_flavor()
 
-class TestLxmlReadHtml(TestCase):
     def test_to_html_compat(self):
         df = mkdf(4, 3, data_gen_f=lambda *args: rand(), c_idx_names=False,
-                  r_idx_names=False).applymap('{0:.3f}'.format)
+                  r_idx_names=False).applymap('{0:.3f}'.format).astype(float)
         out = df.to_html()
         res = self.run_read_html(out, attrs={'class': 'dataframe'},
                                  index_col=0)[0]
@@ -85,16 +90,6 @@ class TestLxmlReadHtml(TestCase):
         print res.dtypes
         assert_frame_equal(res, df)
 
-    def setUp(self):
-        self.spam_data = os.path.join(DATA_PATH, 'spam.html')
-        self.banklist_data = os.path.join(DATA_PATH, 'banklist.html')
-
-    def run_read_html(self, *args, **kwargs):
-        kwargs['flavor'] = 'lxml'
-        _skip_if_no('lxml')
-        parser = _LxmlFrameParser
-        return _run_read_html(parser, *args, **kwargs)
-
     @network
     @slow
     def test_banklist_url(self):
@@ -124,34 +119,6 @@ class TestLxmlReadHtml(TestCase):
 
         assert_framelist_equal(df1, df2)
 
-    @slow
-    def test_banklist_header(self):
-        def try_remove_ws(x):
-            try:
-                return _remove_whitespace(x)
-            except AttributeError:
-                return x
-
-        df = self.run_read_html(self.banklist_data, 'Metcalf',
-                                attrs={'id': 'table'}, infer_types=False)[0]
-        ground_truth = read_csv(os.path.join(DATA_PATH, 'banklist.csv'),
-                                converters={'Closing Date': Timestamp,
-                                            'Updated Date': Timestamp})
-        self.assertNotEqual(df.shape, ground_truth.shape)
-        self.assertRaises(AssertionError, assert_frame_equal, df,
-                          ground_truth.applymap(try_remove_ws))
-
-    @slow
-    def test_gold_canyon(self):
-        gc = 'Gold Canyon'
-        with open(self.banklist_data, 'r') as f:
-            raw_text = f.read()
-
-        self.assertIn(gc, raw_text)
-        df = self.run_read_html(self.banklist_data, 'Gold Canyon',
-                                attrs={'id': 'table'}, infer_types=False)[0]
-        self.assertNotIn(gc, df.to_string())
-
     def test_spam(self):
         df1 = self.run_read_html(self.spam_data, '.*Water.*',
                                  infer_types=False)
@@ -241,7 +208,14 @@ class TestLxmlReadHtml(TestCase):
         df2 = self.run_read_html(self.spam_data, 'Unit', index_col=0)
         assert_framelist_equal(df1, df2)
 
-    def test_header_and_index(self):
+    def test_header_and_index_no_types(self):
+        df1 = self.run_read_html(self.spam_data, '.*Water.*', header=1,
+                                 index_col=0, infer_types=False)
+        df2 = self.run_read_html(self.spam_data, 'Unit', header=1, index_col=0,
+                                 infer_types=False)
+        assert_framelist_equal(df1, df2)
+
+    def test_header_and_index_with_types(self):
         df1 = self.run_read_html(self.spam_data, '.*Water.*', header=1,
                                  index_col=0)
         df2 = self.run_read_html(self.spam_data, 'Unit', header=1, index_col=0)
@@ -374,36 +348,6 @@ class TestLxmlReadHtml(TestCase):
         zz = [df.iloc[0, 0] for df in dfs]
         self.assertListEqual(sorted(zz), sorted(['Python', 'SciTE']))
 
-
-def test_invalid_flavor():
-    url = 'google.com'
-    nose.tools.assert_raises(AssertionError, read_html, url, 'google',
-                             flavor='not a* valid**++ flaver')
-
-
-@slow
-class TestBs4LxmlParser(TestLxmlReadHtml):
-    def test(self):
-        pass
-
-    def run_read_html(self, *args, **kwargs):
-        kwargs['flavor'] = 'bs4'
-        _skip_if_none(('lxml', 'bs4'))
-        parser = _BeautifulSoupLxmlFrameParser
-        return _run_read_html(parser, *args, **kwargs)
-
-
-@slow
-class TestBs4Html5LibParser(TestBs4LxmlParser):
-    def test(self):
-        pass
-
-    def run_read_html(self, *args, **kwargs):
-        kwargs['flavor'] = 'bs4'
-        _skip_if_none(('html5lib', 'bs4'))
-        parser = _BeautifulSoupHtml5LibFrameParser
-        return _run_read_html(parser, *args, **kwargs)
-
     @slow
     def test_banklist_header(self):
         def try_remove_ws(x):
@@ -445,19 +389,61 @@ class TestBs4Html5LibParser(TestBs4LxmlParser):
         with open(self.banklist_data, 'r') as f:
             raw_text = f.read()
 
-        self.assertIn(gc, raw_text)
+        self.assert_(gc in raw_text)
         df = self.run_read_html(self.banklist_data, 'Gold Canyon',
                                 attrs={'id': 'table'}, infer_types=False)[0]
         self.assertIn(gc, df.to_string())
 
 
-def get_elements_from_url(url, flavor, element='table'):
-    _skip_if_no('bs4')
-    _skip_if_no(flavor)
+class TestReadHtmlLxml(TestCase):
+    def run_read_html(self, *args, **kwargs):
+        self.flavor = 'lxml'
+        self.try_skip()
+        kwargs['flavor'] = kwargs.get('flavor', self.flavor)
+        return read_html(*args, **kwargs)
+
+    def try_skip(self):
+        _skip_if_no('lxml')
+
+    def test_spam_data_fail(self):
+        from lxml.etree import XMLSyntaxError
+        spam_data = os.path.join(DATA_PATH, 'spam.html')
+        self.assertRaises(XMLSyntaxError, self.run_read_html, spam_data, flavor=['lxml'])
+
+    def test_banklist_data_fail(self):
+        from lxml.etree import XMLSyntaxError
+        banklist_data = os.path.join(DATA_PATH, 'banklist.html')
+        self.assertRaises(XMLSyntaxError, self.run_read_html, banklist_data, flavor=['lxml'])
+
+    def test_works_on_valid_markup(self):
+        filename = os.path.join(DATA_PATH, 'valid_markup.html')
+        dfs = self.run_read_html(filename, index_col=0, flavor=['lxml'])
+        self.assertIsInstance(dfs, list)
+        self.assertIsInstance(dfs[0], DataFrame)
+
+    def setUp(self):
+        self.try_skip()
+
+    @slow
+    def test_fallback_success(self):
+        _skip_if_none_of(('bs4', 'html5lib'))
+        banklist_data = os.path.join(DATA_PATH, 'banklist.html')
+        self.run_read_html(banklist_data, '.*Water.*', flavor=['lxml',
+                                                               'html5lib'])
+
+
+def test_invalid_flavor():
+    url = 'google.com'
+    nose.tools.assert_raises(ValueError, read_html, url, 'google',
+                             flavor='not a* valid**++ flaver')
+
+
+def get_elements_from_url(url, element='table'):
+    _skip_if_none_of(('bs4', 'html5lib'))
     from bs4 import BeautifulSoup, SoupStrainer
     strainer = SoupStrainer(element)
     with closing(urlopen(url)) as f:
-        soup = BeautifulSoup(f, features=flavor, parse_only=strainer)
+        soup = BeautifulSoup(f, features='html5lib', parse_only=strainer)
     return soup.find_all(element)
 
 
@@ -465,16 +451,12 @@ def get_elements_from_url(url, flavor, element='table'):
 def test_bs4_finds_tables():
     url = ('http://ndb.nal.usda.gov/ndb/foods/show/1732?fg=&man=&'
            'lfacet=&format=&count=&max=25&offset=&sort=&qlookup=spam')
-    flavors = 'lxml', 'html5lib'
     with warnings.catch_warnings():
         warnings.filterwarnings('ignore')
-
-        for flavor in flavors:
-            assert get_elements_from_url(url, flavor, 'table')
+        assert get_elements_from_url(url, 'table')
 
 
 def get_lxml_elements(url, element):
-
     _skip_if_no('lxml')
     from lxml.html import parse
     doc = parse(url)
