commit aafe311d48ce0a00cd7c24c74186827aa36138e7
Author: jreback <jeff@reback.net>
Date:   Sun Dec 23 13:15:29 2012 -0500

    DOC: minor doc updates/typos

diff --git a/RELEASE.rst b/RELEASE.rst
index c43d242ec..fd958ebb2 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -34,7 +34,7 @@ pandas 0.10.1
   - ``HDFStore``
     - enables storing of multi-index dataframes (closes GH1277_)
     - support data column indexing and selection, via ``data_columns`` keyword in append
-    - support write chunking to reduce memory footprint, via ``chunksize`` keywork to append
+    - support write chunking to reduce memory footprint, via ``chunksize`` keyword to append
     - support automagic indexing via ``index`` keywork to append
     - support ``expectedrows`` keyword in append to inform ``PyTables`` about the expected tablesize
     - support ``start`` and ``stop`` keywords in select to limit the row selection space
diff --git a/doc/source/io.rst b/doc/source/io.rst
index e27e4af82..36ae14aaa 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1114,6 +1114,7 @@ Passing ``min_itemsize = { `values` : size }`` as a parameter to append will set
     df_mixed['string']   = 'string'
     df_mixed['int']      = 1
     df_mixed['bool']     = True
+    df_mixed.ix[3:4,['A','B','string']] = np.nan
 
     store.append('df_mixed', df_mixed, min_itemsize = { 'values' : 50 })
     df_mixed1 = store.select('df_mixed')
@@ -1126,7 +1127,7 @@ Passing ``min_itemsize = { `values` : size }`` as a parameter to append will set
 Storing Multi-Index DataFrames
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-Storing multi-index dataframes is very similar to storing/selecting from homogenous index DataFrames.
+Storing multi-index dataframes as tables is very similar to storing/selecting from homogenous index DataFrames.
 
 .. ipython:: python
 
@@ -1194,7 +1195,7 @@ Start and Stop parameters can be specified to limit the total search space. Thes
 
 Indexing
 ~~~~~~~~
-You can create/modify an index for a table with ``create_table_index`` after data is already in the table (after and ``append/put`` operation). Creating a table index is **highly** encouraged. This will speed your queries a great deal when you use a ``select`` with the indexed dimension as the ``where``. **Indexes are automagically created** on the indexables and any data columns you specify. This behavior can be turned off by passing ``index=False`` to ``append``.
+You can create/modify an index for a table with ``create_table_index`` after data is already in the table (after and ``append/put`` operation). Creating a table index is **highly** encouraged. This will speed your queries a great deal when you use a ``select`` with the indexed dimension as the ``where``. **Indexes are automagically created (starting 0.10.1)** on the indexables and any data columns you specify. This behavior can be turned off by passing ``index=False`` to ``append``.
 
 .. ipython:: python
 
@@ -1210,7 +1211,7 @@ You can create/modify an index for a table with ``create_table_index`` after dat
 
 Query via Data Columns
 ~~~~~~~~~~~~~~~~~~~~~~
-You can designate (and index) certain columns that you want to be able to perform queries (other than the `indexable` columns, which you can always query). For instance say you want to perform this this common operation, on-disk, and return just the frame that matches this query.
+You can designate (and index) certain columns that you want to be able to perform queries (other than the `indexable` columns, which you can always query). For instance say you want to perform this common operation, on-disk, and return just the frame that matches this query.
 
 .. ipython:: python
 
@@ -1231,7 +1232,7 @@ You can designate (and index) certain columns that you want to be able to perfor
    # this is in-memory version of this type of selection
    df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == 'foo')]
 
-   # we have automagically created this index and that the B/string columns are stored separately as ``PyTables`` columns
+   # we have automagically created this index and that the B/C/string/string2 columns are stored separately as ``PyTables`` columns
    store.root.df_dc.table
 
 There is some performance degredation by making lots of columns into `data columns`, so it is up to the user to designate these. In addition, you cannot change data columns (nor indexables) after the first append/put operation (Of course you can simply read in the data and create a new table!)
@@ -1257,14 +1258,14 @@ If you want to inspect the table object, retrieve via ``get_table``. You could u
 Multiple Table Queries
 ~~~~~~~~~~~~~~~~~~~~~~
 
-New in 0.10.1 are the methods ``append_to_multple`` and ``select_as_multiple``, that can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables that you are indexed the same the selector table. You can then perform a very fast query on the selector table, yet get lots of data back. This method works similar to having a very wide-table, but is more efficient in terms of queries.
+New in 0.10.1 are the methods ``append_to_multple`` and ``select_as_multiple``, that can perform appending/selecting from multiple tables at once. The idea is to have one table (call it the selector table) that you index most/all of the columns, and perform your queries. The other table(s) are data tables that are indexed the same the selector table. You can then perform a very fast query on the selector table, yet get lots of data back. This method works similar to having a very wide-table, but is more efficient in terms of queries.
 
 Note, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**. This means, append to the tables in the same order; ``append_to_multiple`` splits a single object to multiple tables, given a specification (as a dictionary). This dictionary is a mapping of the table names to the 'columns' you want included in that table. Pass a `None` for a single table (optional) to let it have the remaining columns. The argument ``selector`` defines which table is the selector table.
 
 .. ipython:: python
 
-   index = date_range('1/1/2000', periods=8)
-   df_mt = DataFrame(randn(8, 6), index=index, columns=['A', 'B', 'C', 'D', 'E', 'F'])
+   df_mt = DataFrame(randn(8, 6), index=date_range('1/1/2000', periods=8), 
+                                  columns=['A', 'B', 'C', 'D', 'E', 'F'])
    df_mt['foo'] = 'bar'
 
    # you can also create the tables individually
@@ -1276,7 +1277,7 @@ Note, **THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES**. This means, appe
    store.select('df2_mt')
    
    # as a multiple
-   store.select_as_multiple(['df1_mt','df2_mt'], where = [ 'A>0','B>0' ], axis = 1, selector = 'df1_mt')
+   store.select_as_multiple(['df1_mt','df2_mt'], where = [ 'A>0','B>0' ], selector = 'df1_mt')
   
 
 Delete from a Table
@@ -1303,11 +1304,13 @@ It should be clear that a delete operation on the ``major_axis`` will be fairly
    store.remove('wp', 'major_axis>20000102' )
    store.select('wp')
 
+Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically. Thus, repeatedly deleting (or removing nodes) and adding again **WILL TEND TO INCREASE THE FILE SIZE**. To *clean* the file, use ``ptrepack`` (see below).
+
 Compression
 ~~~~~~~~~~~
 ``PyTables`` allows the stored data to be compressed (this applies to all kinds of stores, not just tables). You can pass ``complevel=int`` for a compression level (1-9, with 0 being no compression, and the default), ``complib=lib`` where lib is any of ``zlib, bzip2, lzo, blosc`` for whichever compression library you prefer. ``blosc`` offers very fast compression (its level defaults to 9), and is my most used. 
 
-``PyTables`` offer better write performance when compressed after writing them, as opposed to turning on compression at the very beginning. You can use the supplied ``PyTables`` utility ``ptrepack``. ``ptrepack`` also can change compression levels after the fact.
+``PyTables`` offer better write performance when compressed after writing them, as opposed to turning on compression at the very beginning. You can use the supplied ``PyTables`` utility ``ptrepack``. In addition, ``ptrepack`` can change compression levels after the fact.
 
    - ``ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5``
 
@@ -1315,7 +1318,7 @@ Or on-the-fly compression
 
    - ``store_compressed = HDFStore('store_compressed.h5', complevel=9, complib='blosc')``
 
-
+Furthermore ``ptrepack in.h5 out.h5`` will *repack* the file to allow you to reuse previously deleted space (alternatively, one can simply remove the file and write again).
 
 Notes & Caveats
 ~~~~~~~~~~~~~~~
@@ -1351,12 +1354,7 @@ Performance
      Write times are generally longer as compared with regular stores. Query times can be quite fast, especially on an indexed axis.
    - You can pass ``chunksize=an integer`` to ``append``, to change the writing chunksize (default is 50000). This will signficantly lower your memory usage on writing.
    - You can pass ``expectedrows=an integer`` to the first ``append``, to set the TOTAL number of expectedrows that ``PyTables`` will expected. This will optimize read/write performance.
-   - ``Tables`` can be expressed as different types.
-
-     - ``AppendableTable`` which is a similiar table to past versions (this is the default).
-     - ``WORMTable`` (pending implementation) - is available to faciliate very fast writing of tables that are also queryable (but CANNOT support appends)
-
-   - Duplicate rows can be written, but are filtered out in selection (with the last items being selected; thus a table is unique on major, minor pairs)
+   - Duplicate rows can be written to tables, but are filtered out in selection (with the last items being selected; thus a table is unique on major, minor pairs)
 
 Experimental
 ~~~~~~~~~~~~
diff --git a/doc/source/v0.10.1.txt b/doc/source/v0.10.1.txt
index 56d38ee9c..855fd619b 100644
--- a/doc/source/v0.10.1.txt
+++ b/doc/source/v0.10.1.txt
@@ -38,13 +38,10 @@ You can designate (and index) certain columns that you want to be able to perfor
 
    # on-disk operations
    store.append('df', df, data_columns = ['B','C','string','string2'])
-   store.select('df',[ Term('B>0') ])
-
-   # getting creative
-   store.select('df',[ Term('B>0'), Term('C>0'), Term('string=foo') ])
+   store.select('df',[ 'B > 0', 'string == foo' ])
 
    # this is in-memory version of this type of selection
-   df[(df.B > 0) & (df.C > 0) & (df.string == 'foo')]
+   df[(df.B > 0) & (df.string == 'foo')]
 
 
 You can pass ``columns`` keyword to select to filter a list of the return columns, this is equivalent to passing a ``Term('columns',list_of_columns_to_filter)``
@@ -53,7 +50,7 @@ You can pass ``columns`` keyword to select to filter a list of the return column
 
    store.select('df',columns = ['A','B'])
 
-``HDFStore`` now serializes multi-index dataframes.
+``HDFStore`` now serializes multi-index dataframes when appending tables.
 
 .. ipython:: python
 
@@ -76,8 +73,8 @@ Multi-table creation via ``append_to_multiple`` and selection via ``select_multi
 
 .. ipython:: python
 
-   index = date_range('1/1/2000', periods=8)
-   df_mt = DataFrame(randn(8, 6), index=index, columns=['A', 'B', 'C', 'D', 'E', 'F'])
+   df_mt = DataFrame(randn(8, 6), index=date_range('1/1/2000', periods=8), 
+                                  columns=['A', 'B', 'C', 'D', 'E', 'F'])
    df_mt['foo'] = 'bar'
 
    # you can also create the tables individually
@@ -89,7 +86,7 @@ Multi-table creation via ``append_to_multiple`` and selection via ``select_multi
    store.select('df2_mt')
    
    # as a multiple
-   store.select_as_multiple(['df1_mt','df2_mt'], where = [ 'A>0','B>0' ], axis = 1, selector = 'df1_mt')
+   store.select_as_multiple(['df1_mt','df2_mt'], where = [ 'A>0','B>0' ], selector = 'df1_mt')
 
 .. ipython:: python
    :suppress:
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index ad31f0f53..2c065d894 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -379,7 +379,7 @@ class HDFStore(object):
         """
         return self.get_table(key).read_coordinates(where = where, **kwargs)
 
-    def select_as_multiple(self, keys, where=None, selector=None, columns=None, axis=1, **kwargs):
+    def select_as_multiple(self, keys, where=None, selector=None, columns=None, **kwargs):
         """ Retrieve pandas objects from multiple tables
 
         Parameters
@@ -387,7 +387,6 @@ class HDFStore(object):
         keys : a list of the tables
         selector : the table to apply the where criteria (defaults to keys[0] if not supplied)
         columns : the columns I want back
-        axis : the concentation axis (defaults to 1)
 
         Exceptions
         ----------
@@ -424,6 +423,9 @@ class HDFStore(object):
         # collect the returns objs
         objs = [ t.read(where = c, columns = columns) for t in tbls ]
 
+        # axis is the concentation axes
+        axis = list(set([ t.non_index_axes[0][0] for t in tbls ]))[0]
+
         # concat and return
         return concat(objs, axis = axis, verify_integrity = True)
 
@@ -2463,7 +2465,7 @@ class Term(object):
     """
 
     _ops     = ['<=','<','>=','>','!=','==','=']
-    _search  = re.compile("^(?P<field>\w+)\s*(?P<op>%s)\s*(?P<value>.+)$" % '|'.join(_ops))
+    _search  = re.compile("^\s*(?P<field>\w+)\s*(?P<op>%s)\s*(?P<value>.+)\s*$" % '|'.join(_ops))
 
     def __init__(self, field, op = None, value = None, queryables = None):
         self.field = None
