commit 3fe28fcb6f4292e7d6c01ea78d6a2a21b0bac16a
Author: Soyoun(Rose) Kim <makeajourney@gmail.com>
Date:   Fri Jan 11 21:51:28 2019 +0900

    CLN: replace %s syntax with .format in pandas.io (#24719)
    
    progress towards #16130

diff --git a/pandas/io/packers.py b/pandas/io/packers.py
index b83eab7d0..efe4e3a91 100644
--- a/pandas/io/packers.py
+++ b/pandas/io/packers.py
@@ -519,7 +519,8 @@ def encode(obj):
         elif isinstance(obj, date):
             return {u'typ': u'date',
                     u'data': u(obj.isoformat())}
-        raise Exception("cannot encode this datetimelike object: %s" % obj)
+        raise Exception(
+            "cannot encode this datetimelike object: {obj}".format(obj=obj))
     elif isinstance(obj, Period):
         return {u'typ': u'period',
                 u'ordinal': obj.ordinal,
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index b115529f6..4f410a34f 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -363,7 +363,7 @@ def read_hdf(path_or_buf, key=None, mode='r', **kwargs):
 
         if not exists:
             raise compat.FileNotFoundError(
-                'File %s does not exist' % path_or_buf)
+                'File {path} does not exist'.format(path=path_or_buf))
 
         store = HDFStore(path_or_buf, mode=mode, **kwargs)
         # can't auto open/close if we are using an iterator
@@ -465,8 +465,8 @@ class HDFStore(StringMixin):
         try:
             import tables  # noqa
         except ImportError as ex:  # pragma: no cover
-            raise ImportError('HDFStore requires PyTables, "{ex}" problem '
-                              'importing'.format(ex=str(ex)))
+            raise ImportError('HDFStore requires PyTables, "{ex!s}" problem '
+                              'importing'.format(ex=ex))
 
         if complib is not None and complib not in tables.filters.all_complibs:
             raise ValueError(
@@ -515,8 +515,9 @@ class HDFStore(StringMixin):
             return self.get(name)
         except (KeyError, ClosedFileError):
             pass
-        raise AttributeError("'%s' object has no attribute '%s'" %
-                             (type(self).__name__, name))
+        raise AttributeError(
+            "'{object}' object has no attribute '{name}'".format(
+                object=type(self).__name__, name=name))
 
     def __contains__(self, key):
         """ check for existence of this key
@@ -533,7 +534,8 @@ class HDFStore(StringMixin):
         return len(self.groups())
 
     def __unicode__(self):
-        return '%s\nFile path: %s\n' % (type(self), pprint_thing(self._path))
+        return '{type}\nFile path: {path}\n'.format(
+            type=type(self), path=pprint_thing(self._path))
 
     def __enter__(self):
         return self
@@ -601,7 +603,8 @@ class HDFStore(StringMixin):
             self._handle = tables.open_file(self._path, self._mode, **kwargs)
         except (IOError) as e:  # pragma: no cover
             if 'can not be written' in str(e):
-                print('Opening %s in read-only mode' % self._path)
+                print(
+                    'Opening {path} in read-only mode'.format(path=self._path))
                 self._handle = tables.open_file(self._path, 'r', **kwargs)
             else:
                 raise
@@ -688,7 +691,7 @@ class HDFStore(StringMixin):
         """
         group = self.get_node(key)
         if group is None:
-            raise KeyError('No object named %s in the file' % key)
+            raise KeyError('No object named {key} in the file'.format(key=key))
         return self._read_group(group)
 
     def select(self, key, where=None, start=None, stop=None, columns=None,
@@ -716,7 +719,7 @@ class HDFStore(StringMixin):
         """
         group = self.get_node(key)
         if group is None:
-            raise KeyError('No object named %s in the file' % key)
+            raise KeyError('No object named {key} in the file'.format(key=key))
 
         # create the storer and axes
         where = _ensure_term(where, scope_level=1)
@@ -821,11 +824,11 @@ class HDFStore(StringMixin):
         nrows = None
         for t, k in itertools.chain([(s, selector)], zip(tbls, keys)):
             if t is None:
-                raise KeyError("Invalid table [%s]" % k)
+                raise KeyError("Invalid table [{key}]".format(key=k))
             if not t.is_table:
                 raise TypeError(
-                    "object [%s] is not a table, and cannot be used in all "
-                    "select as multiple" % t.pathname
+                    "object [{obj}] is not a table, and cannot be used in all "
+                    "select as multiple".format(obj=t.pathname)
                 )
 
             if nrows is None:
@@ -1162,7 +1165,7 @@ class HDFStore(StringMixin):
         """ return the storer object for a key, raise if not in the file """
         group = self.get_node(key)
         if group is None:
-            raise KeyError('No object named {} in the file'.format(key))
+            raise KeyError('No object named {key} in the file'.format(key=key))
 
         s = self._create_storer(group)
         s.infer_axes()
@@ -1225,7 +1228,8 @@ class HDFStore(StringMixin):
 
         .. versionadded:: 0.21.0
         """
-        output = '%s\nFile path: %s\n' % (type(self), pprint_thing(self._path))
+        output = '{type}\nFile path: {path}\n'.format(
+            type=type(self), path=pprint_thing(self._path))
         if self.is_open:
             lkeys = sorted(list(self.keys()))
             if len(lkeys):
@@ -1241,8 +1245,9 @@ class HDFStore(StringMixin):
                                 pprint_thing(s or 'invalid_HDFStore node'))
                     except Exception as detail:
                         keys.append(k)
-                        values.append("[invalid_HDFStore node: %s]"
-                                      % pprint_thing(detail))
+                        values.append(
+                            "[invalid_HDFStore node: {detail}]".format(
+                                detail=pprint_thing(detail)))
 
                 output += adjoin(12, keys, values)
             else:
@@ -1276,10 +1281,11 @@ class HDFStore(StringMixin):
 
         def error(t):
             raise TypeError(
-                "cannot properly create the storer for: [%s] [group->%s,"
-                "value->%s,format->%s,append->%s,kwargs->%s]"
-                % (t, group, type(value), format, append, kwargs)
-            )
+                "cannot properly create the storer for: [{t}] [group->"
+                "{group},value->{value},format->{format},append->{append},"
+                "kwargs->{kwargs}]".format(t=t, group=group,
+                                           value=type(value), format=format,
+                                           append=append, kwargs=kwargs))
 
         pt = _ensure_decoded(getattr(group._v_attrs, 'pandas_type', None))
         tt = _ensure_decoded(getattr(group._v_attrs, 'table_type', None))
@@ -1559,7 +1565,7 @@ class IndexCol(StringMixin):
     def set_name(self, name, kind_attr=None):
         """ set the name of this indexer """
         self.name = name
-        self.kind_attr = kind_attr or "%s_kind" % name
+        self.kind_attr = kind_attr or "{name}_kind".format(name=name)
         if self.cname is None:
             self.cname = name
 
@@ -1590,7 +1596,9 @@ class IndexCol(StringMixin):
                      self.axis,
                      self.pos,
                      self.kind)))
-        return "name->%s,cname->%s,axis->%s,pos->%s,kind->%s" % temp
+        return ','.join(("{key}->{value}".format(key=key, value=value)
+                         for key, value in zip(
+            ['name', 'cname', 'axis', 'pos', 'kind'], temp)))
 
     def __eq__(self, other):
         """ compare 2 col items """
@@ -1714,10 +1722,12 @@ class IndexCol(StringMixin):
                     itemsize = self.itemsize
                 if c.itemsize < itemsize:
                     raise ValueError(
-                        "Trying to store a string with len [%s] in [%s] "
-                        "column but\nthis column has a limit of [%s]!\n"
-                        "Consider using min_itemsize to preset the sizes on "
-                        "these columns" % (itemsize, self.cname, c.itemsize))
+                        "Trying to store a string with len [{itemsize}] in "
+                        "[{cname}] column but\nthis column has a limit of "
+                        "[{c_itemsize}]!\nConsider using min_itemsize to "
+                        "preset the sizes on these columns".format(
+                            itemsize=itemsize, cname=self.cname,
+                            c_itemsize=c.itemsize))
                 return c.itemsize
 
         return None
@@ -1727,8 +1737,10 @@ class IndexCol(StringMixin):
         if append:
             existing_kind = getattr(self.attrs, self.kind_attr, None)
             if existing_kind is not None and existing_kind != self.kind:
-                raise TypeError("incompatible kind in col [%s - %s]" %
-                                (existing_kind, self.kind))
+                raise TypeError(
+                    "incompatible kind in col [{existing} - "
+                    "{self_kind}]".format(
+                        existing=existing_kind, self_kind=self.kind))
 
     def update_info(self, info):
         """ set/update the info for this indexable with the key/value
@@ -1753,9 +1765,11 @@ class IndexCol(StringMixin):
 
                 else:
                     raise ValueError(
-                        "invalid info for [%s] for [%s], existing_value [%s] "
-                        "conflicts with new value [%s]"
-                        % (self.name, key, existing_value, value))
+                        "invalid info for [{name}] for [{key}], "
+                        "existing_value [{existing_value}] conflicts with "
+                        "new value [{value}]".format(
+                            name=self.name, key=key,
+                            existing_value=existing_value, value=value))
             else:
                 if value is not None or existing_value is not None:
                     idx[key] = value
@@ -1840,7 +1854,7 @@ class DataCol(IndexCol):
         """ return a new datacol with the block i """
 
         if cname is None:
-            cname = name or 'values_block_%d' % i
+            cname = name or 'values_block_{idx}'.format(idx=i)
         if name is None:
             name = cname
 
@@ -1850,7 +1864,7 @@ class DataCol(IndexCol):
             if version[0] == 0 and version[1] <= 10 and version[2] == 0:
                 m = re.search(r"values_block_(\d+)", name)
                 if m:
-                    name = "values_%s" % m.groups()[0]
+                    name = "values_{group}".format(group=m.groups()[0])
         except IndexError:
             pass
 
@@ -1862,9 +1876,9 @@ class DataCol(IndexCol):
         super(DataCol, self).__init__(values=values, kind=kind, typ=typ,
                                       cname=cname, **kwargs)
         self.dtype = None
-        self.dtype_attr = u'{}_dtype'.format(self.name)
+        self.dtype_attr = u'{name}_dtype'.format(name=self.name)
         self.meta = meta
-        self.meta_attr = u'{}_meta'.format(self.name)
+        self.meta_attr = u'{name}_meta'.format(name=self.name)
         self.set_data(data)
         self.set_metadata(metadata)
 
@@ -1876,7 +1890,9 @@ class DataCol(IndexCol):
                      self.dtype,
                      self.kind,
                      self.shape)))
-        return "name->%s,cname->%s,dtype->%s,kind->%s,shape->%s" % temp
+        return ','.join(("{key}->{value}".format(key=key, value=value)
+                         for key, value in zip(
+            ['name', 'cname', 'dtype', 'kind', 'shape'], temp)))
 
     def __eq__(self, other):
         """ compare 2 col items """
@@ -1926,7 +1942,8 @@ class DataCol(IndexCol):
                 self.kind = 'bool'
             else:
                 raise AssertionError(
-                    "cannot interpret dtype of [%s] in [%s]" % (dtype, self))
+                    "cannot interpret dtype of [{dtype}] in [{obj}]".format(
+                        dtype=dtype, obj=self))
 
             # set my typ if we need
             if self.typ is None:
@@ -2009,9 +2026,9 @@ class DataCol(IndexCol):
                 inferred_type = lib.infer_dtype(col.ravel(), skipna=False)
                 if inferred_type != 'string':
                     raise TypeError(
-                        "Cannot serialize the column [%s] because\n"
-                        "its data contents are [%s] object dtype"
-                        % (item, inferred_type)
+                        "Cannot serialize the column [{item}] because\n"
+                        "its data contents are [{type}] object dtype".format(
+                            item=item, type=inferred_type)
                     )
 
         # itemsize is the maximum length of a string (along any dimension)
@@ -2033,16 +2050,17 @@ class DataCol(IndexCol):
         self.itemsize = itemsize
         self.kind = 'string'
         self.typ = self.get_atom_string(block, itemsize)
-        self.set_data(data_converted.astype('|S%d' % itemsize, copy=False))
+        self.set_data(data_converted.astype(
+            '|S{size}'.format(size=itemsize), copy=False))
 
     def get_atom_coltype(self, kind=None):
         """ return the PyTables column class for this column """
         if kind is None:
             kind = self.kind
         if self.kind.startswith('uint'):
-            col_name = "UInt%sCol" % kind[4:]
+            col_name = "UInt{name}Col".format(name=kind[4:])
         else:
-            col_name = "%sCol" % kind.capitalize()
+            col_name = "{name}Col".format(name=kind.capitalize())
 
         return getattr(_tables(), col_name)
 
@@ -2325,8 +2343,10 @@ class Fixed(StringMixin):
         s = self.shape
         if s is not None:
             if isinstance(s, (list, tuple)):
-                s = "[%s]" % ','.join(pprint_thing(x) for x in s)
-            return "%-12.12s (shape->%s)" % (self.pandas_type, s)
+                s = "[{shape}]".format(
+                    shape=','.join(pprint_thing(x) for x in s))
+            return "{type:12.12} (shape->{shape})".format(
+                type=self.pandas_type, shape=s)
         return self.pandas_type
 
     def set_object_info(self):
@@ -2542,7 +2562,8 @@ class GenericFixed(Fixed):
             return ret
 
     def read_index(self, key, **kwargs):
-        variety = _ensure_decoded(getattr(self.attrs, '%s_variety' % key))
+        variety = _ensure_decoded(
+            getattr(self.attrs, '{key}_variety'.format(key=key)))
 
         if variety == u'multi':
             return self.read_multi_index(key, **kwargs)
@@ -2554,20 +2575,22 @@ class GenericFixed(Fixed):
             _, index = self.read_index_node(getattr(self.group, key), **kwargs)
             return index
         else:  # pragma: no cover
-            raise TypeError('unrecognized index variety: %s' % variety)
+            raise TypeError(
+                'unrecognized index variety: {variety}'.format(
+                    variety=variety))
 
     def write_index(self, key, index):
         if isinstance(index, MultiIndex):
-            setattr(self.attrs, '%s_variety' % key, 'multi')
+            setattr(self.attrs, '{key}_variety'.format(key=key), 'multi')
             self.write_multi_index(key, index)
         elif isinstance(index, BlockIndex):
-            setattr(self.attrs, '%s_variety' % key, 'block')
+            setattr(self.attrs, '{key}_variety'.format(key=key), 'block')
             self.write_block_index(key, index)
         elif isinstance(index, IntIndex):
-            setattr(self.attrs, '%s_variety' % key, 'sparseint')
+            setattr(self.attrs, '{key}_variety'.format(key=key), 'sparseint')
             self.write_sparse_intindex(key, index)
         else:
-            setattr(self.attrs, '%s_variety' % key, 'regular')
+            setattr(self.attrs, '{key}_variety'.format(key=key), 'regular')
             converted = _convert_index(index, self.encoding, self.errors,
                                        self.format_type).set_name('index')
 
@@ -2587,33 +2610,33 @@ class GenericFixed(Fixed):
                 node._v_attrs.tz = _get_tz(index.tz)
 
     def write_block_index(self, key, index):
-        self.write_array('%s_blocs' % key, index.blocs)
-        self.write_array('%s_blengths' % key, index.blengths)
-        setattr(self.attrs, '%s_length' % key, index.length)
+        self.write_array('{key}_blocs'.format(key=key), index.blocs)
+        self.write_array('{key}_blengths'.format(key=key), index.blengths)
+        setattr(self.attrs, '{key}_length'.format(key=key), index.length)
 
     def read_block_index(self, key, **kwargs):
-        length = getattr(self.attrs, '%s_length' % key)
-        blocs = self.read_array('%s_blocs' % key, **kwargs)
-        blengths = self.read_array('%s_blengths' % key, **kwargs)
+        length = getattr(self.attrs, '{key}_length'.format(key=key))
+        blocs = self.read_array('{key}_blocs'.format(key=key), **kwargs)
+        blengths = self.read_array('{key}_blengths'.format(key=key), **kwargs)
         return BlockIndex(length, blocs, blengths)
 
     def write_sparse_intindex(self, key, index):
-        self.write_array('%s_indices' % key, index.indices)
-        setattr(self.attrs, '%s_length' % key, index.length)
+        self.write_array('{key}_indices'.format(key=key), index.indices)
+        setattr(self.attrs, '{key}_length'.format(key=key), index.length)
 
     def read_sparse_intindex(self, key, **kwargs):
-        length = getattr(self.attrs, '%s_length' % key)
-        indices = self.read_array('%s_indices' % key, **kwargs)
+        length = getattr(self.attrs, '{key}_length'.format(key=key))
+        indices = self.read_array('{key}_indices'.format(key=key), **kwargs)
         return IntIndex(length, indices)
 
     def write_multi_index(self, key, index):
-        setattr(self.attrs, '%s_nlevels' % key, index.nlevels)
+        setattr(self.attrs, '{key}_nlevels'.format(key=key), index.nlevels)
 
         for i, (lev, level_codes, name) in enumerate(zip(index.levels,
                                                          index.codes,
                                                          index.names)):
             # write the level
-            level_key = '%s_level%d' % (key, i)
+            level_key = '{key}_level{idx}'.format(key=key, idx=i)
             conv_level = _convert_index(lev, self.encoding, self.errors,
                                         self.format_type).set_name(level_key)
             self.write_array(level_key, conv_level.values)
@@ -2622,26 +2645,27 @@ class GenericFixed(Fixed):
             node._v_attrs.name = name
 
             # write the name
-            setattr(node._v_attrs, '%s_name%d' % (key, i), name)
+            setattr(node._v_attrs, '{key}_name{name}'.format(
+                key=key, name=name), name)
 
             # write the labels
-            label_key = '%s_label%d' % (key, i)
+            label_key = '{key}_label{idx}'.format(key=key, idx=i)
             self.write_array(label_key, level_codes)
 
     def read_multi_index(self, key, **kwargs):
-        nlevels = getattr(self.attrs, '%s_nlevels' % key)
+        nlevels = getattr(self.attrs, '{key}_nlevels'.format(key=key))
 
         levels = []
         codes = []
         names = []
         for i in range(nlevels):
-            level_key = '%s_level%d' % (key, i)
+            level_key = '{key}_level{idx}'.format(key=key, idx=i)
             name, lev = self.read_index_node(getattr(self.group, level_key),
                                              **kwargs)
             levels.append(lev)
             names.append(name)
 
-            label_key = '%s_label%d' % (key, i)
+            label_key = '{key}_label{idx}'.format(key=key, idx=i)
             level_codes = self.read_array(label_key, **kwargs)
             codes.append(level_codes)
 
@@ -2889,7 +2913,7 @@ class SparseFrameFixed(SparseFixed):
         columns = self.read_index('columns')
         sdict = {}
         for c in columns:
-            key = 'sparse_series_%s' % c
+            key = 'sparse_series_{columns}'.format(columns=c)
             s = SparseSeriesFixed(self.parent, getattr(self.group, key))
             s.infer_axes()
             sdict[c] = s.read()
@@ -2901,7 +2925,7 @@ class SparseFrameFixed(SparseFixed):
         """ write it as a collection of individual sparse series """
         super(SparseFrameFixed, self).write(obj, **kwargs)
         for name, ss in compat.iteritems(obj):
-            key = 'sparse_series_%s' % name
+            key = 'sparse_series_{name}'.format(name=name)
             if key not in self.group._v_children:
                 node = self._handle.create_group(self.group, key)
             else:
@@ -2925,7 +2949,7 @@ class BlockManagerFixed(GenericFixed):
             # items
             items = 0
             for i in range(self.nblocks):
-                node = getattr(self.group, 'block%d_items' % i)
+                node = getattr(self.group, 'block{idx}_items'.format(idx=i))
                 shape = getattr(node, 'shape', None)
                 if shape is not None:
                     items += shape[0]
@@ -2958,15 +2982,16 @@ class BlockManagerFixed(GenericFixed):
         for i in range(self.ndim):
 
             _start, _stop = (start, stop) if i == select_axis else (None, None)
-            ax = self.read_index('axis%d' % i, start=_start, stop=_stop)
+            ax = self.read_index('axis{idx}'.format(
+                idx=i), start=_start, stop=_stop)
             axes.append(ax)
 
         items = axes[0]
         blocks = []
         for i in range(self.nblocks):
 
-            blk_items = self.read_index('block%d_items' % i)
-            values = self.read_array('block%d_values' % i,
+            blk_items = self.read_index('block{idx}_items'.format(idx=i))
+            values = self.read_array('block{idx}_values'.format(idx=i),
                                      start=_start, stop=_stop)
             blk = make_block(values,
                              placement=items.get_indexer(blk_items))
@@ -2986,15 +3011,16 @@ class BlockManagerFixed(GenericFixed):
                 if not ax.is_unique:
                     raise ValueError(
                         "Columns index has to be unique for fixed format")
-            self.write_index('axis%d' % i, ax)
+            self.write_index('axis{idx}'.format(idx=i), ax)
 
         # Supporting mixed-type DataFrame objects...nontrivial
         self.attrs.nblocks = len(data.blocks)
         for i, blk in enumerate(data.blocks):
             # I have no idea why, but writing values before items fixed #2299
             blk_items = data.items.take(blk.mgr_locs)
-            self.write_array('block%d_values' % i, blk.values, items=blk_items)
-            self.write_index('block%d_items' % i, blk_items)
+            self.write_array('block{idx}_values'.format(idx=i),
+                             blk.values, items=blk_items)
+            self.write_index('block{idx}_items'.format(idx=i), blk_items)
 
 
 class FrameFixed(BlockManagerFixed):
@@ -3065,17 +3091,22 @@ class Table(Fixed):
     def __unicode__(self):
         """ return a pretty representatgion of myself """
         self.infer_axes()
-        dc = ",dc->[%s]" % ','.join(
-            self.data_columns) if len(self.data_columns) else ''
+        dc = ",dc->[{columns}]".format(columns=(','.join(
+            self.data_columns) if len(self.data_columns) else ''))
 
         ver = ''
         if self.is_old_version:
-            ver = "[%s]" % '.'.join(str(x) for x in self.version)
-
-        return "%-12.12s%s (typ->%s,nrows->%s,ncols->%s,indexers->[%s]%s)" % (
-            self.pandas_type, ver, self.table_type_short, self.nrows,
-            self.ncols, ','.join(a.name for a in self.index_axes), dc
-        )
+            ver = "[{version}]".format(
+                version='.'.join(str(x) for x in self.version))
+
+        return (
+            "{pandas_type:12.12}{ver} (typ->{table_type},nrows->{nrows},"
+            "ncols->{ncols},indexers->[{index_axes}]{dc})".format(
+                pandas_type=self.pandas_type, ver=ver,
+                table_type=self.table_type_short, nrows=self.nrows,
+                ncols=self.ncols,
+                index_axes=(','.join(a.name for a in self.index_axes)), dc=dc
+            ))
 
     def __getitem__(self, c):
         """ return the axis for c """
@@ -3090,8 +3121,10 @@ class Table(Fixed):
             return
 
         if other.table_type != self.table_type:
-            raise TypeError("incompatible table_type with existing [%s - %s]" %
-                            (other.table_type, self.table_type))
+            raise TypeError(
+                "incompatible table_type with existing "
+                "[{other} - {self}]".format(
+                    other=other.table_type, self=self.table_type))
 
         for c in ['index_axes', 'non_index_axes', 'values_axes']:
             sv = getattr(self, c, None)
@@ -3103,13 +3136,14 @@ class Table(Fixed):
                     oax = ov[i]
                     if sax != oax:
                         raise ValueError(
-                            "invalid combinate of [%s] on appending data [%s] "
-                            "vs current table [%s]" % (c, sax, oax))
+                            "invalid combinate of [{c}] on appending data "
+                            "[{sax}] vs current table [{oax}]".format(
+                                c=c, sax=sax, oax=oax))
 
                 # should never get here
                 raise Exception(
-                    "invalid combinate of [%s] on appending data [%s] vs "
-                    "current table [%s]" % (c, sv, ov))
+                    "invalid combinate of [{c}] on appending data [{sv}] vs "
+                    "current table [{ov}]".format(c=c, sv=sv, ov=ov))
 
     @property
     def is_multi_index(self):
@@ -3292,8 +3326,8 @@ class Table(Fixed):
                 continue
             if k not in q:
                 raise ValueError(
-                    "min_itemsize has the key [%s] which is not an axis or "
-                    "data_column" % k)
+                    "min_itemsize has the key [{key}] which is not an axis or "
+                    "data_column".format(key=k))
 
     @property
     def indexables(self):
@@ -3480,9 +3514,10 @@ class Table(Fixed):
             try:
                 axes = _AXES_MAP[type(obj)]
             except KeyError:
-                raise TypeError("cannot properly create the storer for: "
-                                "[group->%s,value->%s]"
-                                % (self.group._v_name, type(obj)))
+                raise TypeError(
+                    "cannot properly create the storer for: [group->{group},"
+                    "value->{value}]".format(
+                        group=self.group._v_name, value=type(obj)))
 
         # map axes to numbers
         axes = [obj._get_axis_number(a) for a in axes]
@@ -3600,9 +3635,10 @@ class Table(Fixed):
                     new_blk_items.append(b_items)
                 except (IndexError, KeyError):
                     raise ValueError(
-                        "cannot match existing table structure for [%s] on "
-                        "appending data" % ','.join(pprint_thing(item) for
-                                                    item in items))
+                        "cannot match existing table structure for [{items}] "
+                        "on appending data".format(
+                            items=(','.join(pprint_thing(item) for
+                                            item in items))))
             blocks = new_blocks
             blk_items = new_blk_items
 
@@ -3627,9 +3663,11 @@ class Table(Fixed):
                 try:
                     existing_col = existing_table.values_axes[i]
                 except (IndexError, KeyError):
-                    raise ValueError("Incompatible appended table [%s] with "
-                                     "existing table [%s]"
-                                     % (blocks, existing_table.values_axes))
+                    raise ValueError(
+                        "Incompatible appended table [{blocks}]"
+                        "with existing table [{table}]".format(
+                            blocks=blocks,
+                            table=existing_table.values_axes))
             else:
                 existing_col = None
 
@@ -3651,9 +3689,8 @@ class Table(Fixed):
             except Exception as detail:
                 raise Exception(
                     "cannot find the correct atom type -> "
-                    "[dtype->%s,items->%s] %s"
-                    % (b.dtype.name, b_items, str(detail))
-                )
+                    "[dtype->{name},items->{items}] {detail!s}".format(
+                        name=b.dtype.name, items=b_items, detail=detail))
             j += 1
 
         # validate our min_itemsize
@@ -3719,8 +3756,8 @@ class Table(Fixed):
                             return obj.loc._getitem_axis(takers,
                                                          axis=axis_number)
 
-                    raise ValueError(
-                        "cannot find the field [%s] for filtering!" % field)
+                    raise ValueError("cannot find the field [{field}] for "
+                                     "filtering!".format(field=field))
 
                 obj = process_filter(field, filt)
 
@@ -3798,8 +3835,8 @@ class Table(Fixed):
 
                 if not a.is_data_indexable:
                     raise ValueError(
-                        "column [%s] can not be extracted individually; it is "
-                        "not data indexable" % column)
+                        "column [{column}] can not be extracted individually; "
+                        "it is not data indexable".format(column=column))
 
                 # column must be an indexable or a data column
                 c = getattr(self.table.cols, column)
@@ -3811,7 +3848,8 @@ class Table(Fixed):
                                                 ).take_data(),
                                       a.tz, True), name=column)
 
-        raise KeyError("column [%s] not found in the table" % column)
+        raise KeyError(
+            "column [{column}] not found in the table".format(column=column))
 
 
 class WORMTable(Table):
@@ -4120,14 +4158,17 @@ class AppendableTable(LegacyTable):
                     rows = rows[m]
 
         except Exception as detail:
-            raise Exception("cannot create row-data -> %s" % detail)
+            raise Exception(
+                "cannot create row-data -> {detail}".format(detail=detail))
 
         try:
             if len(rows):
                 self.table.append(rows)
                 self.table.flush()
         except Exception as detail:
-            raise TypeError("tables cannot write this data -> %s" % detail)
+            raise TypeError(
+                "tables cannot write this data -> {detail}".format(
+                    detail=detail))
 
     def delete(self, where=None, start=None, stop=None, **kwargs):
 
@@ -4597,7 +4638,7 @@ def _unconvert_index(data, kind, encoding=None, errors='strict'):
     elif kind == u'object':
         index = np.asarray(data[0])
     else:  # pragma: no cover
-        raise ValueError('unrecognized index type %s' % kind)
+        raise ValueError('unrecognized index type {kind}'.format(kind=kind))
     return index
 
 
@@ -4612,7 +4653,7 @@ def _unconvert_index_legacy(data, kind, legacy=False, encoding=None,
         index = _unconvert_string_array(data, nan_rep=None, encoding=encoding,
                                         errors=errors)
     else:  # pragma: no cover
-        raise ValueError('unrecognized index type %s' % kind)
+        raise ValueError('unrecognized index type {kind}'.format(kind=kind))
     return index
 
 
@@ -4643,7 +4684,7 @@ def _convert_string_array(data, encoding, errors, itemsize=None):
         ensured = ensure_object(data.ravel())
         itemsize = max(1, libwriters.max_len_string_array(ensured))
 
-    data = np.asarray(data, dtype="S%d" % itemsize)
+    data = np.asarray(data, dtype="S{size}".format(size=itemsize))
     return data
 
 
@@ -4708,7 +4749,7 @@ def _get_converter(kind, encoding, errors):
         return lambda x: _unconvert_string_array(x, encoding=encoding,
                                                  errors=errors)
     else:  # pragma: no cover
-        raise ValueError('invalid kind %s' % kind)
+        raise ValueError('invalid kind {kind}'.format(kind=kind))
 
 
 def _need_convert(kind):
diff --git a/pandas/io/sas/sas7bdat.py b/pandas/io/sas/sas7bdat.py
index d634b5ec4..eb77f79d3 100644
--- a/pandas/io/sas/sas7bdat.py
+++ b/pandas/io/sas/sas7bdat.py
@@ -163,7 +163,7 @@ class SAS7BDATReader(BaseIterator):
         if buf in const.encoding_names:
             self.file_encoding = const.encoding_names[buf]
         else:
-            self.file_encoding = "unknown (code=%s)" % str(buf)
+            self.file_encoding = "unknown (code={name!s})".format(name=buf)
 
         # Get platform information
         buf = self._read_bytes(const.platform_offset, const.platform_length)
@@ -435,8 +435,11 @@ class SAS7BDATReader(BaseIterator):
         self.column_count = self._read_int(offset, int_len)
         if (self.col_count_p1 + self.col_count_p2 !=
                 self.column_count):
-            print("Warning: column count mismatch (%d + %d != %d)\n",
-                  self.col_count_p1, self.col_count_p2, self.column_count)
+            print(
+                "Warning: column count mismatch ({p1} + {p2} != "
+                "{column_count})\n".format(
+                    p1=self.col_count_p1, p2=self.col_count_p2,
+                    column_count=self.column_count))
 
     # Unknown purpose
     def _process_subheader_counts(self, offset, length):
@@ -694,7 +697,7 @@ class SAS7BDATReader(BaseIterator):
                 js += 1
             else:
                 self.close()
-                raise ValueError("unknown column type %s" %
-                                 self._column_types[j])
+                raise ValueError("unknown column type {type}".format(
+                    type=self._column_types[j]))
 
         return rslt
diff --git a/pandas/io/sql.py b/pandas/io/sql.py
index 2f4093e15..5d1163b3e 100644
--- a/pandas/io/sql.py
+++ b/pandas/io/sql.py
@@ -240,7 +240,7 @@ def read_sql_table(table_name, con, schema=None, index_col=None,
     try:
         meta.reflect(only=[table_name], views=True)
     except sqlalchemy.exc.InvalidRequestError:
-        raise ValueError("Table %s not found" % table_name)
+        raise ValueError("Table {name} not found".format(name=table_name))
 
     pandas_sql = SQLDatabase(con, meta=meta)
     table = pandas_sql.read_table(
@@ -250,7 +250,7 @@ def read_sql_table(table_name, con, schema=None, index_col=None,
     if table is not None:
         return table
     else:
-        raise ValueError("Table %s not found" % table_name, con)
+        raise ValueError("Table {name} not found".format(name=table_name), con)
 
 
 def read_sql_query(sql, con, index_col=None, coerce_float=True, params=None,
@@ -552,7 +552,8 @@ class SQLTable(PandasObject):
             self.table = self.pd_sql.get_table(self.name, self.schema)
 
         if self.table is None:
-            raise ValueError("Could not init table '%s'" % name)
+            raise ValueError(
+                "Could not init table '{name}'".format(name=name))
 
     def exists(self):
         return self.pd_sql.has_table(self.name, self.schema)
@@ -569,7 +570,8 @@ class SQLTable(PandasObject):
     def create(self):
         if self.exists():
             if self.if_exists == 'fail':
-                raise ValueError("Table '%s' already exists." % self.name)
+                raise ValueError(
+                    "Table '{name}' already exists.".format(name=self.name))
             elif self.if_exists == 'replace':
                 self.pd_sql.drop_table(self.name, self.schema)
                 self._execute_create()
@@ -1161,8 +1163,8 @@ class SQLDatabase(PandasSQL):
             from sqlalchemy.types import to_instance, TypeEngine
             for col, my_type in dtype.items():
                 if not isinstance(to_instance(my_type), TypeEngine):
-                    raise ValueError('The type of %s is not a SQLAlchemy '
-                                     'type ' % col)
+                    raise ValueError('The type of {column} is not a '
+                                     'SQLAlchemy type '.format(column=col))
 
         table = SQLTable(name, self, frame=frame, index=index,
                          if_exists=if_exists, index_label=index_label,
@@ -1244,7 +1246,8 @@ def _get_unicode_name(name):
     try:
         uname = text_type(name).encode("utf-8", "strict").decode("utf-8")
     except UnicodeError:
-        raise ValueError("Cannot convert identifier to UTF-8: '%s'" % name)
+        raise ValueError(
+            "Cannot convert identifier to UTF-8: '{name}'".format(name=name))
     return uname
 
 
@@ -1305,8 +1308,9 @@ class SQLiteTable(SQLTable):
         bracketed_names = [escape(column) for column in names]
         col_names = ','.join(bracketed_names)
         wildcards = ','.join([wld] * len(names))
-        insert_statement = 'INSERT INTO %s (%s) VALUES (%s)' % (
-            escape(self.name), col_names, wildcards)
+        insert_statement = \
+            u'INSERT INTO {table} ({columns}) VALUES ({wld})'.format(
+                table=escape(self.name), columns=col_names, wld=wildcards)
         return insert_statement
 
     def _execute_insert(self, conn, keys, data_iter):
@@ -1429,12 +1433,14 @@ class SQLiteDatabase(PandasSQL):
             try:
                 self.con.rollback()
             except Exception:  # pragma: no cover
-                ex = DatabaseError("Execution failed on sql: %s\n%s\nunable"
-                                   " to rollback" % (args[0], exc))
+                ex = DatabaseError(
+                    "Execution failed on sql: {sql}\n{exc}\nunable "
+                    "to rollback".format(sql=args[0], exc=exc))
                 raise_with_traceback(ex)
 
             ex = DatabaseError(
-                "Execution failed on sql '%s': %s" % (args[0], exc))
+                "Execution failed on sql '{sql}': {exc}".format(
+                    sql=args[0], exc=exc))
             raise_with_traceback(ex)
 
     @staticmethod
@@ -1530,8 +1536,8 @@ class SQLiteDatabase(PandasSQL):
         if dtype is not None:
             for col, my_type in dtype.items():
                 if not isinstance(my_type, str):
-                    raise ValueError('%s (%s) not a string' % (
-                        col, str(my_type)))
+                    raise ValueError('{column} ({type!s}) not a string'.format(
+                        column=col, type=my_type))
 
         table = SQLiteTable(name, self, frame=frame, index=index,
                             if_exists=if_exists, index_label=index_label,
@@ -1546,7 +1552,7 @@ class SQLiteDatabase(PandasSQL):
 
         wld = '?'
         query = ("SELECT name FROM sqlite_master "
-                 "WHERE type='table' AND name=%s;") % wld
+                 "WHERE type='table' AND name={wld};").format(wld=wld)
 
         return len(self.execute(query, [name, ]).fetchall()) > 0
 
@@ -1554,7 +1560,8 @@ class SQLiteDatabase(PandasSQL):
         return None  # not supported in fallback mode
 
     def drop_table(self, name, schema=None):
-        drop_sql = "DROP TABLE %s" % _get_valid_sqlite_name(name)
+        drop_sql = "DROP TABLE {name}".format(
+            name=_get_valid_sqlite_name(name))
         self.execute(drop_sql)
 
     def _create_sql_schema(self, frame, table_name, keys=None, dtype=None):
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index b5e7eb244..1b0660171 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -357,7 +357,7 @@ def _stata_elapsed_date_to_datetime_vec(dates, fmt):
         month = np.ones_like(dates)
         conv_dates = convert_year_month_safe(year, month)
     else:
-        raise ValueError("Date fmt %s not understood" % fmt)
+        raise ValueError("Date fmt {fmt} not understood".format(fmt=fmt))
 
     if has_bad_values:  # Restore NaT for bad values
         conv_dates[bad_locs] = NaT
@@ -452,7 +452,8 @@ def _datetime_to_stata_elapsed_vec(dates, fmt):
         d = parse_dates_safe(dates, year=True)
         conv_dates = d.year
     else:
-        raise ValueError("Format %s is not a known Stata date format" % fmt)
+        raise ValueError(
+            "Format {fmt} is not a known Stata date format".format(fmt=fmt))
 
     conv_dates = Series(conv_dates, dtype=np.float64)
     missing_value = struct.unpack('<d', b'\x00\x00\x00\x00\x00\x00\xe0\x7f')[0]
@@ -813,7 +814,7 @@ class StataMissingValue(StringMixin):
 
     def __repr__(self):
         # not perfect :-/
-        return "%s(%s)" % (self.__class__, self)
+        return "{cls}({obj})".format(cls=self.__class__, obj=self)
 
     def __eq__(self, other):
         return (isinstance(other, self.__class__) and
@@ -1771,7 +1772,8 @@ def _set_endianness(endianness):
     elif endianness.lower() in [">", "big"]:
         return ">"
     else:  # pragma : no cover
-        raise ValueError("Endianness %s not understood" % endianness)
+        raise ValueError(
+            "Endianness {endian} not understood".format(endian=endianness))
 
 
 def _pad_bytes(name, length):
@@ -1789,7 +1791,8 @@ def _convert_datetime_to_stata_type(fmt):
                "%tq", "th", "%th", "ty", "%ty"]:
         return np.float64  # Stata expects doubles for SIFs
     else:
-        raise NotImplementedError("Format %s not implemented" % fmt)
+        raise NotImplementedError(
+            "Format {fmt} not implemented".format(fmt=fmt))
 
 
 def _maybe_convert_to_int_keys(convert_dates, varlist):
@@ -1840,7 +1843,8 @@ def _dtype_to_stata_type(dtype, column):
     elif dtype == np.int8:
         return 251
     else:  # pragma : no cover
-        raise NotImplementedError("Data type %s not supported." % dtype)
+        raise NotImplementedError(
+            "Data type {dtype} not supported.".format(dtype=dtype))
 
 
 def _dtype_to_default_stata_fmt(dtype, column, dta_version=114,
@@ -1895,7 +1899,8 @@ def _dtype_to_default_stata_fmt(dtype, column, dta_version=114,
     elif dtype == np.int8 or dtype == np.int16:
         return "%8.0g"
     else:  # pragma : no cover
-        raise NotImplementedError("Data type %s not supported." % dtype)
+        raise NotImplementedError(
+            "Data type {dtype} not supported.".format(dtype=dtype))
 
 
 class StataWriter(StataParser):
@@ -2389,7 +2394,7 @@ class StataWriter(StataParser):
             if typ <= self._max_string_length:
                 has_strings = True
                 data[col] = data[col].fillna('').apply(_pad_bytes, args=(typ,))
-                stype = 'S%d' % typ
+                stype = 'S{type}'.format(type=typ)
                 dtypes.append(('c' + str(i), stype))
                 string = data[col].str.encode(self._encoding)
                 data_cols.append(string.values.astype(stype))
