commit 4192599d934cfca653e29e14c9943b715f5e7bd4
Author: y-p <yoval@gmx.com>
Date:   Mon Apr 8 11:10:10 2013 +0300

    DOC: remove OLS docs, it's in statsmodels now

diff --git a/doc/source/computation.rst b/doc/source/computation.rst
index 59e7334d5..f58d5647a 100644
--- a/doc/source/computation.rst
+++ b/doc/source/computation.rst
@@ -494,192 +494,3 @@ Here are an example for a univariate time series:
    The EW functions perform a standard adjustment to the initial observations
    whereby if there are fewer observations than called for in the span, those
    observations are reweighted accordingly.
-
-.. _stats.ols:
-
-Linear and panel regression
----------------------------
-
-.. note::
-
-   We plan to move this functionality to `statsmodels
-   <http://statsmodels.sourceforge.net>`__ for the next release. Some of the
-   result attributes may change names in order to foster naming consistency
-   with the rest of statsmodels. We will provide every effort to provide
-   compatibility with older versions of pandas, however.
-
-We have implemented a very fast set of *moving-window linear regression*
-classes in pandas. Two different types of regressions are supported:
-
-  - Standard ordinary least squares (OLS) multiple regression
-  - Multiple regression (OLS-based) on `panel data
-    <http://en.wikipedia.org/wiki/Panel_data>`__ including with fixed-effects
-    (also known as entity or individual effects) or time-effects.
-
-Both kinds of linear models are accessed through the ``ols`` function in the
-pandas namespace. They all take the following arguments to specify either a
-static (full sample) or dynamic (moving window) regression:
-
-  - ``window_type``: ``'full sample'`` (default), ``'expanding'``, or
-    ``rolling``
-  - ``window``: size of the moving window in the ``window_type='rolling'``
-    case. If ``window`` is specified, ``window_type`` will be automatically set
-    to ``'rolling'``
-  - ``min_periods``: minimum number of time periods to require to compute the
-    regression coefficients
-
-Generally speaking, the ``ols`` works by being given a ``y`` (response) object
-and an ``x`` (predictors) object. These can take many forms:
-
-  - ``y``: a Series, ndarray, or DataFrame (panel model)
-  - ``x``: Series, DataFrame, dict of Series, dict of DataFrame or Panel
-
-Based on the types of ``y`` and ``x``, the model will be inferred to either a
-panel model or a regular linear model. If the ``y`` variable is a DataFrame,
-the result will be a panel model. In this case, the ``x`` variable must either
-be a Panel, or a dict of DataFrame (which will be coerced into a Panel).
-
-Standard OLS regression
-~~~~~~~~~~~~~~~~~~~~~~~
-
-Let's pull in some sample data:
-
-.. ipython:: python
-
-   from pandas.io.data import DataReader
-   symbols = ['MSFT', 'GOOG', 'AAPL']
-   data = dict((sym, DataReader(sym, "yahoo"))
-               for sym in symbols)
-   panel = Panel(data).swapaxes('items', 'minor')
-   close_px = panel['Close']
-
-   # convert closing prices to returns
-   rets = close_px / close_px.shift(1) - 1
-   rets.info()
-
-Let's do a static regression of ``AAPL`` returns on ``GOOG`` returns:
-
-.. ipython:: python
-
-   model = ols(y=rets['AAPL'], x=rets.ix[:, ['GOOG']])
-   model
-   model.beta
-
-If we had passed a Series instead of a DataFrame with the single ``GOOG``
-column, the model would have assigned the generic name ``x`` to the sole
-right-hand side variable.
-
-We can do a moving window regression to see how the relationship changes over
-time:
-
-.. ipython:: python
-   :suppress:
-
-   plt.close('all')
-
-.. ipython:: python
-
-   model = ols(y=rets['AAPL'], x=rets.ix[:, ['GOOG']],
-               window=250)
-
-   # just plot the coefficient for GOOG
-   @savefig moving_lm_ex.png width=5in
-   model.beta['GOOG'].plot()
-
-It looks like there are some outliers rolling in and out of the window in the
-above regression, influencing the results. We could perform a simple
-`winsorization <http://en.wikipedia.org/wiki/Winsorising>`__ at the 3 STD level
-to trim the impact of outliers:
-
-.. ipython:: python
-   :suppress:
-
-   plt.close('all')
-
-.. ipython:: python
-
-   winz = rets.copy()
-   std_1year = rolling_std(rets, 250, min_periods=20)
-
-   # cap at 3 * 1 year standard deviation
-   cap_level = 3 * np.sign(winz) * std_1year
-   winz[np.abs(winz) > 3 * std_1year] = cap_level
-
-   winz_model = ols(y=winz['AAPL'], x=winz.ix[:, ['GOOG']],
-               window=250)
-
-   model.beta['GOOG'].plot(label="With outliers")
-
-   @savefig moving_lm_winz.png width=5in
-   winz_model.beta['GOOG'].plot(label="Winsorized"); plt.legend(loc='best')
-
-So in this simple example we see the impact of winsorization is actually quite
-significant. Note the correlation after winsorization remains high:
-
-.. ipython:: python
-
-   winz.corrwith(rets)
-
-Multiple regressions can be run by passing a DataFrame with multiple columns
-for the predictors ``x``:
-
-.. ipython:: python
-
-   ols(y=winz['AAPL'], x=winz.drop(['AAPL'], axis=1))
-
-Panel regression
-~~~~~~~~~~~~~~~~
-
-We've implemented moving window panel regression on potentially unbalanced
-panel data (see `this article <http://en.wikipedia.org/wiki/Panel_data>`__ if
-this means nothing to you). Suppose we wanted to model the relationship between
-the magnitude of the daily return and trading volume among a group of stocks,
-and we want to pool all the data together to run one big regression. This is
-actually quite easy:
-
-.. ipython:: python
-
-   # make the units somewhat comparable
-   volume = panel['Volume'] / 1e8
-   model = ols(y=volume, x={'return' : np.abs(rets)})
-   model
-
-In a panel model, we can insert dummy (0-1) variables for the "entities"
-involved (here, each of the stocks) to account the a entity-specific effect
-(intercept):
-
-.. ipython:: python
-
-   fe_model = ols(y=volume, x={'return' : np.abs(rets)},
-                  entity_effects=True)
-   fe_model
-
-Because we ran the regression with an intercept, one of the dummy variables
-must be dropped or the design matrix will not be full rank. If we do not use an
-intercept, all of the dummy variables will be included:
-
-.. ipython:: python
-
-   fe_model = ols(y=volume, x={'return' : np.abs(rets)},
-                  entity_effects=True, intercept=False)
-   fe_model
-
-We can also include *time effects*, which demeans the data cross-sectionally at
-each point in time (equivalent to including dummy variables for each
-date). More mathematical care must be taken to properly compute the standard
-errors in this case:
-
-.. ipython:: python
-
-   te_model = ols(y=volume, x={'return' : np.abs(rets)},
-                  time_effects=True, entity_effects=True)
-   te_model
-
-Here the intercept (the mean term) is dropped by default because it will be 0
-according to the model assumptions, having subtracted off the group means.
-
-Result fields and tests
-~~~~~~~~~~~~~~~~~~~~~~~
-
-We'll leave it to the user to explore the docstrings and source, especially as
-we'll be moving this code into statsmodels in the near future.
