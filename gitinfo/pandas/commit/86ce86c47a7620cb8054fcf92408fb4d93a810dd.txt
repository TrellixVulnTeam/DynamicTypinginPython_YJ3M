commit 86ce86c47a7620cb8054fcf92408fb4d93a810dd
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Oct 12 10:43:20 2010 -0400

    added matplotlib plot_directive extension to config, few moment plots etc.

diff --git a/doc/plots/stats/moments_ewma.py b/doc/plots/stats/moments_ewma.py
new file mode 100644
index 000000000..2919b05fa
--- /dev/null
+++ b/doc/plots/stats/moments_ewma.py
@@ -0,0 +1,14 @@
+import matplotlib.pyplot as plt
+import pandas.util.testing as t
+import pandas.stats.moments as m
+
+t.N = 200
+s = t.makeTimeSeries().cumsum()
+
+plt.figure(figsize=(10, 5))
+plt.plot(s.index, s.values)
+plt.plot(s.index, m.ewma(s, 20, min_periods=1).values)
+f = plt.gcf()
+f.autofmt_xdate()
+
+plt.show()
diff --git a/doc/plots/stats/moments_ewmvol.py b/doc/plots/stats/moments_ewmvol.py
new file mode 100644
index 000000000..e1ba12289
--- /dev/null
+++ b/doc/plots/stats/moments_ewmvol.py
@@ -0,0 +1,22 @@
+import matplotlib.pyplot as plt
+import pandas.util.testing as t
+import pandas.stats.moments as m
+
+t.N = 500
+ts = t.makeTimeSeries()
+ts[::100] = 20
+
+s = ts.cumsum()
+
+
+plt.figure(figsize=(10, 5))
+plt.plot(s.index, m.ewmvol(s, span=50, min_periods=1).values, color='b')
+plt.plot(s.index, m.rolling_std(s, 50, min_periods=1).values, color='r')
+
+plt.title('Exp-weighted std with shocks')
+plt.legend(('Exp-weighted', 'Equal-weighted'))
+
+f = plt.gcf()
+f.autofmt_xdate()
+
+plt.show()
diff --git a/doc/source/conf.py b/doc/source/conf.py
index 52b51832d..f121e23a4 100644
--- a/doc/source/conf.py
+++ b/doc/source/conf.py
@@ -40,7 +40,10 @@ extensions = ['sphinx.ext.autodoc',
               'sphinx.ext.coverage',
               'sphinx.ext.pngmath',
               'sphinx.ext.ifconfig',
-              'sphinx.ext.autosummary']
+              'sphinx.ext.autosummary',
+              'matplotlib.sphinxext.only_directives',
+              'matplotlib.sphinxext.plot_directive',
+              ]
 
 # Add any paths that contain templates here, relative to this directory.
 templates_path = ['_templates', '_templates/autosummary']
diff --git a/doc/source/index.rst b/doc/source/index.rst
index ba95a3b8a..90ae05f8b 100755
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -52,10 +52,7 @@ User manual
 
 **Requirements:** python 2.4 to 2.6, NumPy, and dateutil
 
-**Code Repository:** http://pandas.googlecode.com
-
-**Pardon the dust, documentation will be updated frequently in the
-coming weeks and months but will be incomplete in many places.**
+**Code Repository:** http://github.com/wesm/pandas
 
 Library documentation
 ~~~~~~~~~~~~~~~~~~~~~
@@ -78,6 +75,7 @@ Other topics of interest
     :maxdepth: 2
 
     examples
+    frame_vs_matrix
     r_guide
     missing_data
     related
diff --git a/doc/source/overview.rst b/doc/source/overview.rst
index 63b670914..3d77c5305 100644
--- a/doc/source/overview.rst
+++ b/doc/source/overview.rst
@@ -46,7 +46,7 @@ Data structures at a glance
 Why more than 1 data structure?
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
-The best way to think about the pandas data tructures is as flexible
+The best way to think about the pandas data structures is as flexible
 containers for lower dimensional data. For example, DataFrame /
 DataMatrix are containers for Series, and WidePanel is a container for
 DataFrame / DataMatrix objects. We would like to be able to insert and
diff --git a/doc/source/panel.rst b/doc/source/panel.rst
index fef4dc880..6409132f9 100644
--- a/doc/source/panel.rst
+++ b/doc/source/panel.rst
@@ -1,3 +1,4 @@
+.. currentmodule:: pandas
 
 .. _panel:
 
diff --git a/doc/source/stats.rst b/doc/source/stats.rst
index 8f916365e..a0ad21b35 100755
--- a/doc/source/stats.rst
+++ b/doc/source/stats.rst
@@ -11,5 +11,5 @@ Built-in statistical functionality
 .. toctree::
    :maxdepth: 2
 
-   moments
-   ols
+   stats_moments
+   stats_ols
diff --git a/doc/source/stats/var.rst b/doc/source/stats/var.rst
new file mode 100644
index 000000000..8b1378917
--- /dev/null
+++ b/doc/source/stats/var.rst
@@ -0,0 +1 @@
+
diff --git a/doc/source/moments.rst b/doc/source/stats_moments.rst
similarity index 88%
rename from doc/source/moments.rst
rename to doc/source/stats_moments.rst
index 92318e236..27c28cfdd 100644
--- a/doc/source/moments.rst
+++ b/doc/source/stats_moments.rst
@@ -1,11 +1,11 @@
 .. currentmodule:: pandas.stats.api
 
-.. _moments:
+.. _stats_moments:
 
-Moving statistical statistics / moments
----------------------------------------
+Moving (rolling) statistics / moments
+-------------------------------------
 
-For TimeSeries-oriented operations, a number of functions are provided
+For working with time series data, a number of functions are provided
 for computing common *moving* or *rolling* statistics. Among these are
 count, sum, mean, median, correlation, variance, covariance, standard
 deviation, skewness, and kurtosis. All of these methods are in the
@@ -94,5 +94,23 @@ Method summary
    rolling_skew
    rolling_kurt
 
-Exponentially weighted moving average
--------------------------------------
+Exponentially weighted moment functions
+---------------------------------------
+
+.. autofunction:: pandas.stats.moments.ewma
+
+.. plot:: plots/stats/moments_ewma.py
+
+There are similar functions for other basic moments, like standard
+deviation (a.k.a. volatility):
+
+.. plot:: plots/stats/moments_ewmvol.py
+
+.. autosummary::
+   :toctree: generated/
+
+   ewma
+   ewmvol
+   ewmvar
+   ewmcorr
+   ewmcov
diff --git a/doc/source/ols.rst b/doc/source/stats_ols.rst
similarity index 100%
rename from doc/source/ols.rst
rename to doc/source/stats_ols.rst
diff --git a/pandas/finance/__init__.py b/pandas/finance/__init__.py
new file mode 100644
index 000000000..8062daa4a
--- /dev/null
+++ b/pandas/finance/__init__.py
@@ -0,0 +1,3 @@
+"""
+
+"""
diff --git a/pandas/finance/portstats.py b/pandas/finance/portstats.py
new file mode 100644
index 000000000..cdf93f31c
--- /dev/null
+++ b/pandas/finance/portstats.py
@@ -0,0 +1,76 @@
+"""
+Compute common portfolio statistics
+"""
+
+import pandas.lib.tseries as _tseries
+import pandas.stats.moments as moments
+
+def leverage(weights):
+    """
+    Parameters
+    ----------
+
+    Returns
+    -------
+    y : TimeSeries
+    """
+    pass
+
+def turnover(weights):
+    """
+
+    Returns
+    -------
+    y : TimeSeries
+    """
+    pass
+
+def max_drawdown(returns):
+    """
+    Parameters
+    ----------
+    returns : TimeSeries or DataFrame
+
+    """
+    pass
+
+def sharpe_ratio(returns):
+    """
+
+    Parameters
+    ----------
+    returns : TimeSeries or DataFrame
+
+    Returns
+    -------
+    y : Series
+    """
+    pass
+
+def rolling_sharpe_ratio(returns, window, min_periods=None):
+    """
+
+    Parameters
+    ----------
+    returns : TimeSeries or DataFrame
+
+
+    Returns
+    -------
+    y : TimeSeries or DataFrame
+    """
+    pass
+
+def beta(returns, market_returns):
+    """
+
+    Parameters
+    ----------
+    returns : TimeSeries or DataFrame
+
+
+    Returns
+    -------
+    y : TimeSeries or DataFrame
+    """
+    pass
diff --git a/pandas/stats/misc.py b/pandas/stats/misc.py
new file mode 100644
index 000000000..22c86a0ba
--- /dev/null
+++ b/pandas/stats/misc.py
@@ -0,0 +1,369 @@
+from numpy import NaN
+import numpy as np
+
+from pandas.core.api import Series, DataMatrix, isnull, notnull
+
+__all__ = ['bucket', 'bucketpanel']
+
+def zscore(series):
+    return (series - series.mean()) / np.std(series, ddof = 0)
+
+def correl_ts(frame1, frame2):
+    """
+    Pairwise correlation of columns of two DataFrame objects
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+    y : Series
+    """
+    results = {}
+    for col, series in frame1.iteritems():
+        if col in frame2:
+            other = frame2[col]
+
+            idx1 = series.valid().index
+            idx2 = other.valid().index
+
+            common_index = idx1.intersection(idx2)
+
+            seriesStand = zscore(series.reindex(common_index))
+            otherStand = zscore(other.reindex(common_index))
+            results[col] = (seriesStand * otherStand).mean()
+
+    return Series(results)
+
+def correl_xs(frame1, frame2):
+    return correl_ts(frame1.T, frame2.T)
+
+#-------------------------------------------------------------------------------
+# Quantilization functions
+
+def bucket(series, k, by=None):
+    """
+    Produce DataMatrix representing quantiles of a Series
+
+    Parameters
+    ----------
+    series : Series
+    k : int
+        number of quantiles
+    by : Series or same-length array
+        bucket by value
+
+    Returns
+    -------
+    DataMatrix
+    """
+    if by is None:
+        by = series
+    else:
+        by = by.reindex(series.index)
+
+    split = _split_quantile(by, k)
+    mat = np.empty((len(series), k), dtype=float) * np.NaN
+
+    for i, v in enumerate(split):
+        mat[:, i][v] = series.take(v)
+
+    return DataMatrix(mat, index=series.index, columns=np.arange(k) + 1)
+
+def _split_quantile(arr, k):
+    arr = np.asarray(arr)
+    mask = np.isfinite(arr)
+    order = arr[mask].argsort()
+    n = len(arr)
+
+    return np.array_split(np.arange(n)[mask].take(order), k)
+
+def bucketcat(series, cats):
+    """
+    Produce DataMatrix representing quantiles of a Series
+
+    Parameters
+    ----------
+    series : Series
+    cat : Series or same-length array
+        bucket by category; mutually exxlusive with 'by'
+
+    Returns
+    -------
+    DataMatrix
+    """
+    if not isinstance(series, Series):
+        series = Series(series, index=np.arange(len(series)))
+
+    cats = np.asarray(cats)
+
+    unique_labels = np.unique(cats)
+    unique_labels = unique_labels[notnull(unique_labels)]
+
+    # group by
+    data = {}
+
+    for i, label in enumerate(unique_labels):
+        data[label] = series[cats == label]
+
+    return DataMatrix(data, columns=unique_labels)
+
+def bucketpanel(series, bins=None, by=None, cat=None):
+    """
+    Bucket data by two Series to create summary panel
+
+    Parameters
+    ----------
+    series : Series
+    bins : tuple (length-2)
+        e.g. (2, 2)
+    by : tuple of Series
+        bucket by value
+    cat : tuple of Series
+        bucket by category; mutually exxlusive with 'by'
+
+    Returns
+    -------
+    DataMatrix
+    """
+    use_by = by is not None
+    use_cat = cat is not None
+
+    if use_by and use_cat:
+        raise Exception('must specify by or cat, but not both')
+    elif use_by:
+        if len(by) != 2:
+            raise Exception('must provide two bucketing series')
+
+        xby, yby = by
+        xbins, ybins = bins
+
+        return _bucketpanel_by(series, xby, yby, xbins, ybins)
+
+    elif use_cat:
+        xcat, ycat = cat
+        return _bucketpanel_cat(series, xcat, ycat)
+    else:
+        raise Exception('must specify either values or categories to bucket by')
+
+def _bucketpanel_by(series, xby, yby, xbins, ybins):
+    xby = xby.reindex(series.index)
+    yby = yby.reindex(series.index)
+
+    n = len(series)
+    # indices = np.arange(n)
+
+    xlabels = _bucket_labels(xby.reindex(series.index), xbins)
+    ylabels = _bucket_labels(yby.reindex(series.index), ybins)
+
+    labels = _uniquify(xlabels, ylabels, xbins, ybins)
+
+    mask = isnull(labels)
+    labels[mask] = -1
+
+    unique_labels = np.unique(labels)
+    bucketed = bucketcat(series, labels)
+
+    _ulist = list(labels)
+    index_map = dict((x, _ulist.index(x)) for x in unique_labels)
+
+    def relabel(key):
+        pos = index_map[key]
+
+        xlab = xlabels[pos]
+        ylab = ylabels[pos]
+
+        return '%sx%s' % (int(xlab) if notnull(xlab) else 'NULL',
+                          int(ylab) if notnull(ylab) else 'NULL')
+
+    return bucketed.rename(columns=relabel)
+
+def _bucketpanel_cat(series, xcat, ycat):
+    xlabels, xmapping = _intern(xcat)
+    ylabels, ymapping = _intern(ycat)
+
+    shift = 10 ** (np.ceil(np.log10(ylabels.max())))
+    labels = xlabels * shift + ylabels
+
+    sorter = labels.argsort()
+    sorted_labels = labels.take(sorter)
+    sorted_xlabels = xlabels.take(sorter)
+    sorted_ylabels = ylabels.take(sorter)
+
+    unique_labels = np.unique(labels)
+    unique_labels = unique_labels[notnull(unique_labels)]
+
+    locs = sorted_labels.searchsorted(unique_labels)
+    xkeys = sorted_xlabels.take(locs)
+    ykeys = sorted_ylabels.take(locs)
+
+    stringified = ['(%s, %s)' % arg
+                   for arg in zip(xmapping.take(xkeys), ymapping.take(ykeys))]
+
+    result = bucketcat(series, labels)
+    result.columns = stringified
+
+    return result
+
+def _intern(values):
+    # assumed no NaN values
+    values = np.asarray(values)
+
+    uniqued = np.unique(values)
+    labels = uniqued.searchsorted(values)
+    return labels, uniqued
+
+def _intern_fast(values):
+    pass
+
+def _uniquify(xlabels, ylabels, xbins, ybins):
+    # encode the stuff, create unique label
+    shifter = 10 ** max(xbins, ybins)
+    _xpiece = xlabels * shifter
+    _ypiece = ylabels
+
+    return _xpiece + _ypiece
+
+def _cat_labels(labels):
+    # group by
+    data = {}
+
+    unique_labels = np.unique(labels)
+    unique_labels = unique_labels[notnull(unique_labels)]
+
+    for label in unique_labels:
+        mask = labels == label
+        data[stringified] = series[mask]
+
+    return DataMatrix(data, index=series.index)
+
+def _bucket_labels(series, k):
+    arr = np.asarray(series)
+    mask = np.isfinite(arr)
+    order = arr[mask].argsort()
+    n = len(series)
+
+    split = np.array_split(np.arange(n)[mask].take(order), k)
+
+    bucketsize = n / k
+
+    mat = np.empty(n, dtype=float) * np.NaN
+    for i, v in enumerate(split):
+        mat[v] = i
+
+    return mat + 1
+
+def makeQuantiles(series, n):
+    """
+    Compute quantiles of input series.
+
+    Parameters
+    ----------
+    series: Series
+        Must have 'order' method and index
+    n: int
+        Number of quantile buckets
+
+    Returns
+    -------
+    (edges, quantiles)
+       edges: ith bucket --> (left edge, right edge)
+       quantiles: ith bucket --> set of values
+    """
+    series = remove_na(series).copy()
+    series = series.order()
+    quantiles = {}
+    edges = {}
+    T = float(len(series))
+    inc = T / n
+    for i in range(n):
+        theSlice = series[inc*i:(i+1)*inc]
+        quantiles[i+1] = theSlice
+        edges[i+1] = theSlice[0], theSlice[-1]
+    return edges, quantiles
+
+def quantileTS(frame, percentile):
+    """
+    Return score at percentile for each point in time (cross-section)
+
+    Parameters
+    ----------
+    frame: DataFrame / DataMatrix
+    percentile: int
+       nth percentile
+
+    See also
+    --------
+    scipy.stats.scoreatpercentile
+
+    Returns
+    -------
+    Series (or TimeSeries)
+    """
+    from scipy.stats import scoreatpercentile
+
+    def func(x):
+        x = np.asarray(x.valid())
+        if x.any():
+            return scoreatpercentile(x, percentile)
+        else:
+            return NaN
+    return frame.apply(func, axis=1)
+
+def percentileRank(frame, column=None, kind='mean'):
+    """
+    Return score at percentile for each point in time (cross-section)
+
+    Parameters
+    ----------
+    frame: DataFrame / DataMatrix
+    column: string or Series, optional
+       Column name or specific Series to compute percentiles for.
+       If not provided, percentiles are computed for all values at each
+       point in time. Note that this can take a LONG time.
+    kind: {'rank', 'weak', 'strict', 'mean'}, optional
+        This optional parameter specifies the interpretation of the
+        resulting score:
+
+        - "rank": Average percentage ranking of score.  In case of
+                  multiple matches, average the percentage rankings of
+                  all matching scores.
+        - "weak": This kind corresponds to the definition of a cumulative
+                  distribution function.  A percentileofscore of 80%
+                  means that 80% of values are less than or equal
+                  to the provided score.
+        - "strict": Similar to "weak", except that only values that are
+                    strictly less than the given score are counted.
+        - "mean": The average of the "weak" and "strict" scores, often used in
+                  testing.  See
+
+                  http://en.wikipedia.org/wiki/Percentile_rank
+
+    See also
+    --------
+    scipy.stats.percentileofscore
+
+    Returns
+    -------
+    TimeSeries or DataFrame, depending on input
+    """
+    from scipy.stats import percentileofscore
+    fun = lambda xs, score: percentileofscore(remove_na(xs),
+                                              score, kind=kind)
+
+    results = {}
+    framet = frame.T
+    if column is not None:
+        if isinstance(column, Series):
+            for date, xs in frame.T.iteritems():
+                results[date] = fun(xs, column.get(date, NaN))
+        else:
+            for date, xs in frame.T.iteritems():
+                results[date] = fun(xs, xs[column])
+        results = Series(results)
+    else:
+        for column in frame.columns:
+            for date, xs in framet.iteritems():
+                results.setdefault(date, {})[column] = fun(xs, xs[column])
+        results = frame.__class__(results).T
+    return results
