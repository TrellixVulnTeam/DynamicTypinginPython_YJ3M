commit 9bfdc3c037e926143e708463d76fcdae3fb709f2
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Wed Jun 20 19:00:40 2012 -0400

    BUG: test coverage, groupby bug fixes

diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 302b1e03f..b071bca8c 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -240,7 +240,7 @@ def _flex_comp_method(op, name, default_axis='columns'):
                 mask = notnull(xrav)
                 result[mask] = op(np.array(list(xrav[mask])), y)
 
-            if op == operator.ne:
+            if op == operator.ne:  # pragma: no cover
                 np.putmask(result, -mask, True)
             else:
                 np.putmask(result, -mask, False)
@@ -1869,12 +1869,12 @@ class DataFrame(NDFrame):
         if np.isscalar(loc):
             new_values = self._data.fast_2d_xs(loc, copy=copy)
             return Series(new_values, index=self.columns, name=key)
-        elif isinstance(loc, slice) or loc.dtype == np.bool_:
+        else: # isinstance(loc, slice) or loc.dtype == np.bool_:
             result = self[loc]
             result.index = new_index
             return result
-        else:
-            return self.take(loc)
+        # else:
+        #     return self.take(loc)
 
     def lookup(self, row_labels, col_labels):
         """
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 765c0f628..cd3f1743f 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -345,12 +345,11 @@ class GroupBy(object):
 
     def nth(self, n):
         def picker(arr):
-            if arr is not None:
-                n_ok_pos = n >= 0 and len(arr) > n
-                n_ok_neg = n < 0 and len(arr) >= n
-                if n_ok_pos or n_ok_neg:
-                    return arr.iget(n)
-            return np.nan
+            arr = arr[com.notnull(arr)]
+            if len(arr) >= n + 1:
+                return arr.iget(n)
+            else:
+                return np.nan
         return self.agg(picker)
 
     def _cython_agg_general(self, how):
@@ -656,11 +655,13 @@ class Grouper(object):
         arity = self._cython_arity.get(how, 1)
 
         vdim = values.ndim
+        swapped = False
         if vdim == 1:
             values = values[:, None]
             out_shape = (self.ngroups, arity)
         else:
             if axis > 0:
+                swapped = True
                 values = values.swapaxes(0, axis)
             if arity > 1:
                 raise NotImplementedError
@@ -673,8 +674,11 @@ class Grouper(object):
         result = self._aggregate(result, counts, values, how)
 
         if self._filter_empty_groups:
-            result = lib.row_bool_subset(result,
-                                         (counts > 0).view(np.uint8))
+            if result.ndim == 2:
+                result = lib.row_bool_subset(result,
+                                             (counts > 0).view(np.uint8))
+            else:
+                result = result[counts > 0]
 
         if vdim == 1 and arity == 1:
             result = result[:, 0]
@@ -685,7 +689,7 @@ class Grouper(object):
         else:
             names = None
 
-        if axis > 0:
+        if swapped:
             result = result.swapaxes(0, axis)
 
         return result, names
@@ -700,7 +704,8 @@ class Grouper(object):
             raise NotImplementedError
         elif values.ndim > 2:
             for i, chunk in enumerate(values.transpose(2, 0, 1)):
-                agg_func(result[:, :, i], counts, chunk, comp_ids)
+                agg_func(result[:, :, i], counts, chunk.squeeze(),
+                         comp_ids)
         else:
             agg_func(result, counts, values, comp_ids)
 
@@ -942,10 +947,6 @@ class Grouping(object):
         # pre-computed
         self._was_factor = False
 
-        # did we pass a custom grouper object? Do nothing
-        if isinstance(grouper, Grouper):
-            return
-
         if level is not None:
             if not isinstance(level, int):
                 assert(level in index.names)
@@ -1349,7 +1350,13 @@ class NDFrameGroupBy(GroupBy):
         obj = self._obj_with_exclusions
 
         new_axes = list(obj._data.axes)
-        new_axes[self.axis] = self.grouper.result_index
+
+        # more kludge
+        if self.axis == 0:
+            new_axes[0], new_axes[1] = new_axes[1], self.grouper.result_index
+        else:
+            new_axes[self.axis] = self.grouper.result_index
+
         mgr = BlockManager(blocks, new_axes)
 
         new_obj = type(obj)(mgr)
@@ -1693,7 +1700,7 @@ class NDFrameGroupBy(GroupBy):
             except Exception:
                 pass
 
-        if len(output) == 0:
+        if len(output) == 0:  # pragma: no cover
             raise TypeError('Transform function invalid for data types')
 
         columns = obj.columns
@@ -1769,12 +1776,6 @@ class DataFrameGroupBy(NDFrameGroupBy):
 
         return result
 
-    def _post_process_cython_aggregate(self, obj):
-        # undoing kludge from below
-        if self.axis == 0:
-            obj = obj.T
-        return obj
-
     def _wrap_agged_blocks(self, blocks):
         obj = self._obj_with_exclusions
 
@@ -1827,8 +1828,6 @@ class PanelGroupBy(NDFrameGroupBy):
             slicer = lambda x: self.obj[x]
         else:
             raise NotImplementedError
-            # slice_axis = self.obj.index
-            # slicer = lambda x: self.obj.xs(x, axis=self.axis)
 
         for val in slice_axis:
             if val in self.exclusions:
@@ -1857,7 +1856,6 @@ class PanelGroupBy(NDFrameGroupBy):
         return self._aggregate_generic(arg, *args, **kwargs)
 
     def _wrap_generic_output(self, result, obj):
-
         new_axes = list(obj.axes)
         new_axes[self.axis] = self.grouper.result_index
 
@@ -1882,8 +1880,6 @@ class PanelGroupBy(NDFrameGroupBy):
                     result[item] = itemg.aggregate(func, *args, **kwargs)
                 except (ValueError, TypeError):
                     raise
-                    # cannot_agg.append(item)
-                    # continue
             new_axes = list(obj.axes)
             new_axes[self.axis] = self.grouper.result_index
             return Panel._from_axes(result, new_axes)
@@ -1892,16 +1888,6 @@ class PanelGroupBy(NDFrameGroupBy):
 
     def _wrap_aggregated_output(self, output, names=None):
         raise NotImplementedError
-        new_axes = list(self._obj_with_exclusions.axes)
-        new_axes[self.axis] = self.grouper.result_index
-
-        result = Panel(output, index=self.grouper.result_index,
-                       columns=output_keys)
-
-        if self.axis > 0:
-            result = result.swapaxes(0, self.axis)
-
-        return result
 
 
 class NDArrayGroupBy(GroupBy):
diff --git a/pandas/core/series.py b/pandas/core/series.py
index b858bd326..6bed506d3 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -2736,7 +2736,7 @@ def _resolve_offset(freq, kwds):
         offset = freq
         warn = False
 
-    if warn and _SHOW_WARNINGS:
+    if warn and _SHOW_WARNINGS:  # pragma: no cover
         import warnings
         warnings.warn("'timeRule' and 'offset' parameters are deprecated,"
                       " please use 'freq' instead",
diff --git a/pandas/stats/misc.py b/pandas/stats/misc.py
index e445a8d8e..7e5419b79 100644
--- a/pandas/stats/misc.py
+++ b/pandas/stats/misc.py
@@ -4,9 +4,7 @@ import numpy as np
 from pandas.core.api import Series, DataFrame, isnull, notnull
 from pandas.core.series import remove_na
 
-from pandas.tools.tile import (bucket, bucketcat, bucketpanel,
-                               quantileTS)
-
+from pandas.tools.tile import quantileTS
 
 def zscore(series):
     return (series - series.mean()) / np.std(series, ddof = 0)
@@ -96,3 +94,196 @@ def percentileRank(frame, column=None, kind='mean'):
                 results.setdefault(date, {})[column] = fun(xs, xs[column])
         results = DataFrame(results).T
     return results
+
+
+def bucket(series, k, by=None):
+    """
+    Produce DataFrame representing quantiles of a Series
+
+    Parameters
+    ----------
+    series : Series
+    k : int
+        number of quantiles
+    by : Series or same-length array
+        bucket by value
+
+    Returns
+    -------
+    DataFrame
+    """
+    if by is None:
+        by = series
+    else:
+        by = by.reindex(series.index)
+
+    split = _split_quantile(by, k)
+    mat = np.empty((len(series), k), dtype=float) * np.NaN
+
+    for i, v in enumerate(split):
+        mat[:, i][v] = series.take(v)
+
+    return DataFrame(mat, index=series.index, columns=np.arange(k) + 1)
+
+def _split_quantile(arr, k):
+    arr = np.asarray(arr)
+    mask = np.isfinite(arr)
+    order = arr[mask].argsort()
+    n = len(arr)
+
+    return np.array_split(np.arange(n)[mask].take(order), k)
+
+def bucketcat(series, cats):
+    """
+    Produce DataFrame representing quantiles of a Series
+
+    Parameters
+    ----------
+    series : Series
+    cat : Series or same-length array
+        bucket by category; mutually exxlusive with 'by'
+
+    Returns
+    -------
+    DataFrame
+    """
+    if not isinstance(series, Series):
+        series = Series(series, index=np.arange(len(series)))
+
+    cats = np.asarray(cats)
+
+    unique_labels = np.unique(cats)
+    unique_labels = unique_labels[com.notnull(unique_labels)]
+
+    # group by
+    data = {}
+
+    for label in unique_labels:
+        data[label] = series[cats == label]
+
+    return DataFrame(data, columns=unique_labels)
+
+def bucketpanel(series, bins=None, by=None, cat=None):
+    """
+    Bucket data by two Series to create summary panel
+
+    Parameters
+    ----------
+    series : Series
+    bins : tuple (length-2)
+        e.g. (2, 2)
+    by : tuple of Series
+        bucket by value
+    cat : tuple of Series
+        bucket by category; mutually exxlusive with 'by'
+
+    Returns
+    -------
+    DataFrame
+    """
+    use_by = by is not None
+    use_cat = cat is not None
+
+    if use_by and use_cat:
+        raise Exception('must specify by or cat, but not both')
+    elif use_by:
+        if len(by) != 2:
+            raise Exception('must provide two bucketing series')
+
+        xby, yby = by
+        xbins, ybins = bins
+
+        return _bucketpanel_by(series, xby, yby, xbins, ybins)
+
+    elif use_cat:
+        xcat, ycat = cat
+        return _bucketpanel_cat(series, xcat, ycat)
+    else:
+        raise Exception('must specify either values or categories to bucket by')
+
+def _bucketpanel_by(series, xby, yby, xbins, ybins):
+    xby = xby.reindex(series.index)
+    yby = yby.reindex(series.index)
+
+    xlabels = _bucket_labels(xby.reindex(series.index), xbins)
+    ylabels = _bucket_labels(yby.reindex(series.index), ybins)
+
+    labels = _uniquify(xlabels, ylabels, xbins, ybins)
+
+    mask = com.isnull(labels)
+    labels[mask] = -1
+
+    unique_labels = np.unique(labels)
+    bucketed = bucketcat(series, labels)
+
+    _ulist = list(labels)
+    index_map = dict((x, _ulist.index(x)) for x in unique_labels)
+
+    def relabel(key):
+        pos = index_map[key]
+
+        xlab = xlabels[pos]
+        ylab = ylabels[pos]
+
+        return '%sx%s' % (int(xlab) if com.notnull(xlab) else 'NULL',
+                          int(ylab) if com.notnull(ylab) else 'NULL')
+
+    return bucketed.rename(columns=relabel)
+
+def _bucketpanel_cat(series, xcat, ycat):
+    xlabels, xmapping = _intern(xcat)
+    ylabels, ymapping = _intern(ycat)
+
+    shift = 10 ** (np.ceil(np.log10(ylabels.max())))
+    labels = xlabels * shift + ylabels
+
+    sorter = labels.argsort()
+    sorted_labels = labels.take(sorter)
+    sorted_xlabels = xlabels.take(sorter)
+    sorted_ylabels = ylabels.take(sorter)
+
+    unique_labels = np.unique(labels)
+    unique_labels = unique_labels[com.notnull(unique_labels)]
+
+    locs = sorted_labels.searchsorted(unique_labels)
+    xkeys = sorted_xlabels.take(locs)
+    ykeys = sorted_ylabels.take(locs)
+
+    stringified = ['(%s, %s)' % arg
+                   for arg in zip(xmapping.take(xkeys), ymapping.take(ykeys))]
+
+    result = bucketcat(series, labels)
+    result.columns = stringified
+
+    return result
+
+def _intern(values):
+    # assumed no NaN values
+    values = np.asarray(values)
+
+    uniqued = np.unique(values)
+    labels = uniqued.searchsorted(values)
+    return labels, uniqued
+
+
+def _uniquify(xlabels, ylabels, xbins, ybins):
+    # encode the stuff, create unique label
+    shifter = 10 ** max(xbins, ybins)
+    _xpiece = xlabels * shifter
+    _ypiece = ylabels
+
+    return _xpiece + _ypiece
+
+def _bucket_labels(series, k):
+    arr = np.asarray(series)
+    mask = np.isfinite(arr)
+    order = arr[mask].argsort()
+    n = len(series)
+
+    split = np.array_split(np.arange(n)[mask].take(order), k)
+
+    mat = np.empty(n, dtype=float) * np.NaN
+    for i, v in enumerate(split):
+        mat[v] = i
+
+    return mat + 1
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index cb3ee628c..6c3cc89f1 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -138,6 +138,9 @@ class CheckIndexing(object):
         subframe_obj = self.tsframe[indexer_obj]
         assert_frame_equal(subframe_obj, subframe)
 
+        self.assertRaises(ValueError, self.tsframe.__getitem__, self.tsframe)
+
+
     def test_getitem_boolean_list(self):
         df = DataFrame(np.arange(12).reshape(3,4))
         def _checkit(lst):
@@ -1434,6 +1437,10 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
 
         self.assertEqual(self.mixed_frame['foo'].dtype, np.object_)
 
+    def test_constructor_cast_failure(self):
+        foo = DataFrame({'a': ['a', 'b', 'c']}, dtype=np.float64)
+        self.assert_(foo['a'].dtype == object)
+
     def test_constructor_rec(self):
         rec = self.frame.to_records(index=False)
 
@@ -2819,6 +2826,13 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         rs = df3.gt(2j)
         self.assert_(not rs.values.any())
 
+        # corner, dtype=object
+        df1 = DataFrame({'col' : ['foo', np.nan, 'bar']})
+        df2 = DataFrame({'col' : ['foo', datetime.now(), 'bar']})
+        result = df1.ne(df2)
+        exp = DataFrame({'col' : [False, True, False]})
+        assert_frame_equal(result, exp)
+
     def test_arith_flex_series(self):
         df = self.simple
 
@@ -6358,6 +6372,13 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         getattr(mixed, name)(axis=0)
         getattr(mixed, name)(axis=1)
 
+        class NonzeroFail:
+
+            def __nonzero__(self):
+                raise ValueError
+
+        mixed['_nonzero_fail_'] = NonzeroFail()
+
         if has_bool_only:
             getattr(mixed, name)(axis=0, bool_only=True)
             getattr(mixed, name)(axis=1, bool_only=True)
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index e340460f6..eb4fc3ff0 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -144,10 +144,25 @@ class TestGroupBy(unittest.TestCase):
         grouped['B'].last()
         grouped['B'].nth(0)
 
+        self.df['B'][self.df['A'] == 'foo'] = np.nan
+        self.assert_(com.isnull(grouped['B'].first()['foo']))
+        self.assert_(com.isnull(grouped['B'].last()['foo']))
+        self.assert_(com.isnull(grouped['B'].nth(0)['foo']))
+
+    def test_grouper_iter(self):
+        self.assertEqual(sorted(self.df.groupby('A').grouper), ['bar', 'foo'])
+
     def test_empty_groups(self):
         # GH # 1048
         self.assertRaises(ValueError, self.df.groupby, [])
 
+    def test_groupby_grouper(self):
+        grouped = self.df.groupby('A')
+
+        result = self.df.groupby(grouped.grouper).mean()
+        expected = grouped.mean()
+        assert_frame_equal(result, expected)
+
     def test_groupby_dict_mapping(self):
         # GH #679
         from pandas import Series
@@ -1209,6 +1224,10 @@ class TestGroupBy(unittest.TestCase):
         self.assert_(isinstance(result, DataFrame))
         self.assert_(result.index.equals(ts.index))
 
+    def test_apply_series_yield_constant(self):
+        result = self.df.groupby(['A', 'B'])['C'].apply(len)
+        self.assertEquals(result.index.names[:2], ['A', 'B'])
+
     def test_apply_frame_to_series(self):
         grouped = self.df.groupby(['A', 'B'])
         result = grouped.apply(len)
@@ -1541,17 +1560,21 @@ class TestGroupBy(unittest.TestCase):
         tm.add_nans(self.panel)
         grouped = self.panel.groupby({'ItemA' : 0, 'ItemB' : 0, 'ItemC' : 1},
                                      axis='items')
-        agged = grouped.agg(np.mean)
+        agged = grouped.mean()
+        agged2 = grouped.agg(lambda x: x.mean('items'))
+
+        tm.assert_panel_equal(agged, agged2)
+
         self.assert_(np.array_equal(agged.items, [0, 1]))
 
         grouped = self.panel.groupby(lambda x: x.month, axis='major')
-        agged = grouped.agg(np.mean)
+        agged = grouped.mean()
 
         self.assert_(np.array_equal(agged.major_axis, [1, 2]))
 
         grouped = self.panel.groupby({'A' : 0, 'B' : 0, 'C' : 1, 'D' : 1},
                                      axis='minor')
-        agged = grouped.agg(np.mean)
+        agged = grouped.mean()
         self.assert_(np.array_equal(agged.minor_axis, [0, 1]))
 
     def test_numpy_groupby(self):
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index d7db52953..e6698ff52 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -1002,6 +1002,19 @@ x   q   30      3    -0.6662 -0.5243 -0.3580  0.89145  2.5838"""
 
             assert_frame_equal(leftside, rightside)
 
+    def test_frame_any_all_group(self):
+        df = DataFrame({'data': [False, False, True, False, True, False, True]},
+                       index=[['one', 'one', 'two', 'one', 'two', 'two', 'two'],
+                              [0, 1, 0, 2, 1, 2, 3]])
+
+        result = df.any(level=0)
+        ex = DataFrame({'data': [False, True]}, index=['one', 'two'])
+        assert_frame_equal(result, ex)
+
+        result = df.all(level=0)
+        ex = DataFrame({'data': [False, False]}, index=['one', 'two'])
+        assert_frame_equal(result, ex)
+
     def test_std_var_pass_ddof(self):
         index = MultiIndex.from_arrays([np.arange(5).repeat(10),
                                         np.tile(np.arange(10), 5)])
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index 930881038..81381a50c 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -479,7 +479,7 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         result = s[range(5)]
         assert_series_equal(result, s)
 
-    def test_getitem_slice_bug(self):
+    def test_getitem_setitem_slice_bug(self):
         s = Series(range(10), range(10))
         result = s[-12:]
         assert_series_equal(result, s)
@@ -490,6 +490,13 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         result = s[:-12]
         assert_series_equal(result, s[:0])
 
+        s = Series(range(10), range(10))
+        s[-12:] = 0
+        self.assert_((s == 0).all())
+
+        s[:-12] = 5
+        self.assert_((s == 0).all())
+
     def test_getitem_int64(self):
         idx = np.int64(5)
         self.assertEqual(self.ts[idx], self.ts[5])
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 383063d07..9061402bb 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -386,25 +386,6 @@ def test_series_bin_grouper():
     exp_counts = np.array([3, 3, 4], dtype=np.int64)
     assert_almost_equal(counts, exp_counts)
 
-def test_generate_bins():
-    from pandas.core.groupby import generate_bins_generic
-    values = np.array([1,2,3,4,5,6], dtype=np.int64)
-    binner = np.array([0,3,6,9], dtype=np.int64)
-
-    for func in [lib.generate_bins_dt64, generate_bins_generic]:
-        bins = func(values, binner, closed='left')
-        assert((bins == np.array([2, 5, 6])).all())
-
-        bins = func(values, binner, closed='right')
-        assert((bins == np.array([3, 6, 6])).all())
-
-    for func in [lib.generate_bins_dt64, generate_bins_generic]:
-        values = np.array([1,2,3,4,5,6], dtype=np.int64)
-        binner = np.array([0,3,6], dtype=np.int64)
-
-        bins = func(values, binner, closed='right')
-        assert((bins == np.array([3, 6])).all())
-
 class TestBinGroupers(unittest.TestCase):
 
     def setUp(self):
@@ -412,6 +393,35 @@ class TestBinGroupers(unittest.TestCase):
         self.labels = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2], dtype=np.int64)
         self.bins = np.array([3, 6], dtype=np.int64)
 
+    def test_generate_bins(self):
+        from pandas.core.groupby import generate_bins_generic
+        values = np.array([1,2,3,4,5,6], dtype=np.int64)
+        binner = np.array([0,3,6,9], dtype=np.int64)
+
+        for func in [lib.generate_bins_dt64, generate_bins_generic]:
+            bins = func(values, binner, closed='left')
+            assert((bins == np.array([2, 5, 6])).all())
+
+            bins = func(values, binner, closed='right')
+            assert((bins == np.array([3, 6, 6])).all())
+
+        for func in [lib.generate_bins_dt64, generate_bins_generic]:
+            values = np.array([1,2,3,4,5,6], dtype=np.int64)
+            binner = np.array([0,3,6], dtype=np.int64)
+
+            bins = func(values, binner, closed='right')
+            assert((bins == np.array([3, 6])).all())
+
+        self.assertRaises(ValueError, generate_bins_generic, values, [],
+                          'right')
+        self.assertRaises(ValueError, generate_bins_generic, values[:0],
+                          binner, 'right')
+
+        self.assertRaises(ValueError, generate_bins_generic,
+                          values, [4], 'right')
+        self.assertRaises(ValueError, generate_bins_generic,
+                          values, [-3, -1], 'right')
+
     def test_group_bin_functions(self):
         funcs = ['add', 'mean', 'prod', 'min', 'max', 'var']
 
diff --git a/pandas/tools/tests/test_tile.py b/pandas/tools/tests/test_tile.py
index 51e905588..d11759870 100644
--- a/pandas/tools/tests/test_tile.py
+++ b/pandas/tools/tests/test_tile.py
@@ -9,6 +9,7 @@ import pandas.core.common as com
 
 from pandas.core.algorithms import quantile
 from pandas.tools.tile import cut, qcut
+import pandas.tools.tile as tmod
 
 from numpy.testing import assert_equal, assert_almost_equal
 
@@ -48,6 +49,18 @@ class TestCut(unittest.TestCase):
         data = [.2, 1.4, 2.5, 6.2, 9.7, 2.1]
         self.assertRaises(ValueError, cut, data, [0.1, 1.5, 1, 10])
 
+    def test_wrong_num_labels(self):
+        data = [.2, 1.4, 2.5, 6.2, 9.7, 2.1]
+        self.assertRaises(ValueError, cut, data, [0, 1, 10],
+                          labels=['foo', 'bar', 'baz'])
+
+    def test_cut_corner(self):
+        # h3h
+        self.assertRaises(ValueError, cut, [], 2)
+
+        self.assertRaises(ValueError, cut, [1, 2, 3], 0.5)
+
+
     def test_labels(self):
         arr = np.tile(np.arange(0, 1.01, 0.1), 4)
 
@@ -153,6 +166,14 @@ class TestCut(unittest.TestCase):
         result = qcut(arr, 4)
         self.assert_(com.isnull(result[:20]).all())
 
+    def test_label_formatting(self):
+        self.assertEquals(tmod._trim_zeros('1.000'), '1')
+
+        # it works
+        result = cut(np.arange(11.), 2)
+
+        result = cut(np.arange(11.) / 1e10, 2)
+
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
                    exit=False)
diff --git a/pandas/tools/tile.py b/pandas/tools/tile.py
index f0691db36..d4c9b2736 100644
--- a/pandas/tools/tile.py
+++ b/pandas/tools/tile.py
@@ -72,8 +72,9 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,
             x = np.asarray(x)
             sz = x.size
         if sz == 0:
+            raise ValueError('Cannot cut empty array')
             # handle empty arrays. Can't determine range, so use 0-1.
-            rng = (0, 1)
+            # rng = (0, 1)
         else:
             rng = (nanops.nanmin(x), nanops.nanmax(x))
         mn, mx = [mi + 0.0 for mi in rng]
@@ -202,7 +203,7 @@ def _format_label(x, precision=3):
                 val = _trim_zeros(val)
                 if '.' in val:
                     return sgn + '.'.join(('%d' % whole, val.split('.')[1]))
-                else:
+                else:  # pragma: no cover
                     return sgn + '.'.join(('%d' % whole, val))
         else:
             return sgn + '%d' % whole
@@ -215,219 +216,3 @@ def _trim_zeros(x):
     if len(x) > 1 and x[-1] == '.':
         x = x[:-1]
     return x
-
-def bucket(series, k, by=None):
-    """
-    Produce DataFrame representing quantiles of a Series
-
-    Parameters
-    ----------
-    series : Series
-    k : int
-        number of quantiles
-    by : Series or same-length array
-        bucket by value
-
-    Returns
-    -------
-    DataFrame
-    """
-    if by is None:
-        by = series
-    else:
-        by = by.reindex(series.index)
-
-    split = _split_quantile(by, k)
-    mat = np.empty((len(series), k), dtype=float) * np.NaN
-
-    for i, v in enumerate(split):
-        mat[:, i][v] = series.take(v)
-
-    return DataFrame(mat, index=series.index, columns=np.arange(k) + 1)
-
-def _split_quantile(arr, k):
-    arr = np.asarray(arr)
-    mask = np.isfinite(arr)
-    order = arr[mask].argsort()
-    n = len(arr)
-
-    return np.array_split(np.arange(n)[mask].take(order), k)
-
-def bucketcat(series, cats):
-    """
-    Produce DataFrame representing quantiles of a Series
-
-    Parameters
-    ----------
-    series : Series
-    cat : Series or same-length array
-        bucket by category; mutually exxlusive with 'by'
-
-    Returns
-    -------
-    DataFrame
-    """
-    if not isinstance(series, Series):
-        series = Series(series, index=np.arange(len(series)))
-
-    cats = np.asarray(cats)
-
-    unique_labels = np.unique(cats)
-    unique_labels = unique_labels[com.notnull(unique_labels)]
-
-    # group by
-    data = {}
-
-    for label in unique_labels:
-        data[label] = series[cats == label]
-
-    return DataFrame(data, columns=unique_labels)
-
-def bucketpanel(series, bins=None, by=None, cat=None):
-    """
-    Bucket data by two Series to create summary panel
-
-    Parameters
-    ----------
-    series : Series
-    bins : tuple (length-2)
-        e.g. (2, 2)
-    by : tuple of Series
-        bucket by value
-    cat : tuple of Series
-        bucket by category; mutually exxlusive with 'by'
-
-    Returns
-    -------
-    DataFrame
-    """
-    use_by = by is not None
-    use_cat = cat is not None
-
-    if use_by and use_cat:
-        raise Exception('must specify by or cat, but not both')
-    elif use_by:
-        if len(by) != 2:
-            raise Exception('must provide two bucketing series')
-
-        xby, yby = by
-        xbins, ybins = bins
-
-        return _bucketpanel_by(series, xby, yby, xbins, ybins)
-
-    elif use_cat:
-        xcat, ycat = cat
-        return _bucketpanel_cat(series, xcat, ycat)
-    else:
-        raise Exception('must specify either values or categories to bucket by')
-
-def _bucketpanel_by(series, xby, yby, xbins, ybins):
-    xby = xby.reindex(series.index)
-    yby = yby.reindex(series.index)
-
-    xlabels = _bucket_labels(xby.reindex(series.index), xbins)
-    ylabels = _bucket_labels(yby.reindex(series.index), ybins)
-
-    labels = _uniquify(xlabels, ylabels, xbins, ybins)
-
-    mask = com.isnull(labels)
-    labels[mask] = -1
-
-    unique_labels = np.unique(labels)
-    bucketed = bucketcat(series, labels)
-
-    _ulist = list(labels)
-    index_map = dict((x, _ulist.index(x)) for x in unique_labels)
-
-    def relabel(key):
-        pos = index_map[key]
-
-        xlab = xlabels[pos]
-        ylab = ylabels[pos]
-
-        return '%sx%s' % (int(xlab) if com.notnull(xlab) else 'NULL',
-                          int(ylab) if com.notnull(ylab) else 'NULL')
-
-    return bucketed.rename(columns=relabel)
-
-def _bucketpanel_cat(series, xcat, ycat):
-    xlabels, xmapping = _intern(xcat)
-    ylabels, ymapping = _intern(ycat)
-
-    shift = 10 ** (np.ceil(np.log10(ylabels.max())))
-    labels = xlabels * shift + ylabels
-
-    sorter = labels.argsort()
-    sorted_labels = labels.take(sorter)
-    sorted_xlabels = xlabels.take(sorter)
-    sorted_ylabels = ylabels.take(sorter)
-
-    unique_labels = np.unique(labels)
-    unique_labels = unique_labels[com.notnull(unique_labels)]
-
-    locs = sorted_labels.searchsorted(unique_labels)
-    xkeys = sorted_xlabels.take(locs)
-    ykeys = sorted_ylabels.take(locs)
-
-    stringified = ['(%s, %s)' % arg
-                   for arg in zip(xmapping.take(xkeys), ymapping.take(ykeys))]
-
-    result = bucketcat(series, labels)
-    result.columns = stringified
-
-    return result
-
-def _intern(values):
-    # assumed no NaN values
-    values = np.asarray(values)
-
-    uniqued = np.unique(values)
-    labels = uniqued.searchsorted(values)
-    return labels, uniqued
-
-
-def _uniquify(xlabels, ylabels, xbins, ybins):
-    # encode the stuff, create unique label
-    shifter = 10 ** max(xbins, ybins)
-    _xpiece = xlabels * shifter
-    _ypiece = ylabels
-
-    return _xpiece + _ypiece
-
-def _bucket_labels(series, k):
-    arr = np.asarray(series)
-    mask = np.isfinite(arr)
-    order = arr[mask].argsort()
-    n = len(series)
-
-    split = np.array_split(np.arange(n)[mask].take(order), k)
-
-    mat = np.empty(n, dtype=float) * np.NaN
-    for i, v in enumerate(split):
-        mat[v] = i
-
-    return mat + 1
-
-def quantileTS(frame, percentile):
-    """
-    Return score at percentile for each point in time (cross-section)
-
-    Parameters
-    ----------
-    frame: DataFrame
-    percentile: int
-       nth percentile
-
-    Returns
-    -------
-    Series (or TimeSeries)
-    """
-    from pandas.compat.scipy import scoreatpercentile
-
-    def func(x):
-        x = np.asarray(x.valid())
-        if x.any():
-            return scoreatpercentile(x, percentile)
-        else:
-            return np.nan
-    return frame.apply(func, axis=1)
diff --git a/pandas/tseries/plotting.py b/pandas/tseries/plotting.py
index 4fae7107c..5f87b8d42 100644
--- a/pandas/tseries/plotting.py
+++ b/pandas/tseries/plotting.py
@@ -870,70 +870,6 @@ def format_dateaxis(subplot, freq):
     subplot.xaxis.set_minor_formatter(minformatter)
     pylab.draw_if_interactive()
 
-def add_yaxis(fsp=None, position='right', yscale=None, basey=10, subsy=None):
-    """
-    Adds a second y-axis to a :class:`Subplot`.
-    This function can also be used as a method.
-
-    Parameters
-    ----------
-    fsp : {None, Subplot}
-        Subplot to which the secondary y-axis is added.
-        If None, the current subplot is selected: in that case, it should be
-        a valid :class:`Subplot`.
-        When used as a :class:`Subplot` method, this parameter points
-        automatically to the calling subplot.
-    position : {string}
-        Position of the new axis, as either ``'left'`` or ``'right'``.
-    yscale : {string}
-        Scale of the new axis, as either ``'log'``, ``'linear'`` or ``None``.
-        If None, uses the same scale as the first y axis.
-    basey : {integer}
-        Base of the logarithm for the new axis (if needed).
-    subsy : {sequence}
-        Sequence of the location of the minor ticks;
-        None defaults to autosubs, which depend on the number of decades in
-        the plot.
-        Eg for base 10, ``subsy=(1,2,5)`` will  put minor ticks on 1, 2, 5, 11,
-        12,15, 21, ....
-        To turn off minor ticking, set ``subsy=[]``.
-
-    Raises
-    ------
-    TypeError
-        If the selected subplot is not a valid :class:`Subplot` object.
-
-    """
-    if fsp is None:
-        fsp = pylab.gca()
-
-    fig = fsp.figure
-    axisini = fsp.axis()
-    fsp_alt_args = (fsp._rows, fsp._cols, fsp._num + 1)
-    fsp_alt = fig.add_subplot(frameon=False, position=fsp.get_position(),
-                              sharex=fsp, *fsp_alt_args)
-    # Set position ....................
-    if position.lower() == 'right':
-        (inipos, newpos) = ('left', 'right')
-    else:
-        (inipos, newpos) = ('right', 'left')
-    # Force scales tics to one side ...
-    fsp.yaxis.set_ticks_position(inipos)
-    fsp.yaxis.set_label_position(inipos)
-    # Force 2nd ticks to the other side..
-    fsp_alt.yaxis.set_ticks_position(newpos)
-    fsp_alt.yaxis.set_label_position(newpos)
-    # Force period axis scale..........
-    if yscale is None:
-        yscale = fsp.get_yscale()
-        try:
-            basey = fsp.yaxis.get_major_locator()._base
-        except AttributeError:
-            basey = 10.
-    fsp_alt.set_yscale(yscale, basey=basey, subsy=subsy)
-
-    pylab.draw_if_interactive()
-    return fsp_alt
 
 class DateConverter(object):
 
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index 59e3b2d0a..06fc71c2c 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -1680,6 +1680,10 @@ class TestSeriesDatetime64(unittest.TestCase):
         series = Series(list(date_range('1/1/2000', periods=10)))
         self.assert_(series.dtype == object)
 
+    def test_constructor_cant_cast_datetime64(self):
+        self.assertRaises(TypeError, Series,
+                          date_range('1/1/2000', periods=10), dtype=float)
+
     def test_series_comparison_scalars(self):
         val = datetime(2000, 1, 4)
         result = self.series > val
