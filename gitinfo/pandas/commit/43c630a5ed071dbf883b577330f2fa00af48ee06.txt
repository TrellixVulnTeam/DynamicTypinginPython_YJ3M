commit 43c630a5ed071dbf883b577330f2fa00af48ee06
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Nov 1 22:23:56 2011 -0400

    ENH: some tinkering with multi-key groupby, no changes yet

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 78d2c9f4e..a5c150a52 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -1232,16 +1232,22 @@ def generate_groups(data, label_list, shape, axis=0, factory=lambda x: x):
     -------
     generator
     """
-    sorted_data, sorted_labels = _group_reorder(data, label_list, axis=axis)
+    indexer = np.lexsort(label_list[::-1])
 
-    gen = _generate_groups(sorted_data, sorted_labels, shape,
-                           0, len(label_list[0]), axis=axis, which=0,
-                           factory=factory)
-    for key, group in gen:
-        yield key, group
+    from pandas.core.reshape import get_group_index
+    group_index = get_group_index(label_list, shape)
+
+    na_mask = np.zeros(len(label_list[0]), dtype=bool)
+    for arr in label_list:
+        na_mask |= arr == -1
+    group_index[na_mask] = -1
+
+    indexer2 = lib.groupsort_indexer(group_index.astype('i4'),
+                                     np.prod(shape))
+    assert((indexer == indexer2).all())
+
+    indexer = indexer2
 
-def _group_reorder(data, label_list, axis=0):
-    indexer = np.lexsort(label_list[::-1])
     sorted_labels = [labels.take(indexer) for labels in label_list]
 
     if isinstance(data, BlockManager):
@@ -1254,7 +1260,11 @@ def _group_reorder(data, label_list, axis=0):
     elif isinstance(data, DataFrame):
         sorted_data = data.take(indexer, axis=axis)
 
-    return sorted_data, sorted_labels
+    gen = _generate_groups(sorted_data, sorted_labels, shape,
+                           0, len(label_list[0]), axis=axis, which=0,
+                           factory=factory)
+    for key, group in gen:
+        yield key, group
 
 def _generate_groups(data, labels, shape, start, end, axis=0, which=0,
                      factory=lambda x: x):
diff --git a/pandas/core/reshape.py b/pandas/core/reshape.py
index 4e13737a7..e37539d87 100644
--- a/pandas/core/reshape.py
+++ b/pandas/core/reshape.py
@@ -85,12 +85,8 @@ class _Unstacker(object):
         new_levels = self.new_index_levels
 
         # make the mask
-        group_index = np.zeros(len(self.index), dtype=int)
-
-        for i in xrange(len(new_levels)):
-            stride = np.prod([len(x) for x in new_levels[i+1:]],
-                             dtype=int)
-            group_index += self.sorted_labels[i] * stride
+        group_index = get_group_index(self.sorted_labels,
+                                      [len(x) for x in new_levels])
 
         group_mask = np.zeros(self.full_shape[0], dtype=bool)
         group_mask.put(group_index, True)
@@ -194,6 +190,13 @@ class _Unstacker(object):
 
         return new_index
 
+def get_group_index(label_list, shape):
+    group_index = np.zeros(len(label_list[0]), dtype=int)
+    for i in xrange(len(shape)):
+        stride = np.prod([x for x in shape[i+1:]], dtype=int)
+        group_index += label_list[i] * stride
+    return group_index
+
 def pivot(self, index=None, columns=None, values=None):
     """
     See DataFrame.pivot
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index b89a18e0f..ada8c1eb2 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -104,6 +104,41 @@ def groupby_indices(ndarray values):
 
     return result
 
+# def get_group_index(list list_of_arrays, tuple tshape):
+#     '''
+#     Should just use NumPy, this is slower
+#     '''
+#     cdef:
+#         int32_t **vecs
+#         int32_t *strides
+#         int32_t cur, stride
+#         ndarray[int32_t] result
+#         Py_ssize_t i, j, n, nlevels, ngroups
+
+#     nlevels = len(list_of_arrays)
+#     n = len(list_of_arrays[0])
+
+#     strides = <int32_t*> malloc(nlevels * sizeof(int32_t))
+#     vecs = to_ptr_array(list_of_arrays)
+
+#     result = np.empty(n, dtype='i4')
+
+#     ngroups = 1
+#     for j from 0 <= j < nlevels:
+#         strides[j] = tshape[j]
+#         ngroups *= strides[j]
+
+#     for i from 0 <= i < n:
+#         cur = 0
+#         stride = ngroups
+#         for j from 0 <= j < nlevels:
+#             stride /= strides[j]
+#             cur += vecs[j][i] * stride
+#         result[i] = cur
+
+#     free(strides)
+#     free(vecs)
+#     return result
 
 @cython.wraparound(False)
 @cython.boundscheck(False)
@@ -116,10 +151,9 @@ def is_lexsorted(list list_of_arrays):
     nlevels = len(list_of_arrays)
     n = len(list_of_arrays[0])
 
-    cdef int32_t **vecs = <int32_t **> malloc(nlevels * sizeof(int32_t*))
+    cdef int32_t **vecs = <int32_t**> malloc(nlevels * sizeof(int32_t*))
     for i from 0 <= i < nlevels:
         vecs[i] = <int32_t *> (<ndarray> list_of_arrays[i]).data
-
     # assume uniqueness??
 
     for i from 1 <= i < n:
@@ -267,7 +301,7 @@ def group_aggregate(ndarray[double_t] values, list label_list,
 
     func = get_agg_func(how)
 
-    values, sorted_labels = _group_reorder(values, label_list)
+    values, sorted_labels = _group_reorder(values, label_list, shape)
     result = np.empty(shape, dtype=np.float64)
     result.fill(nan)
 
@@ -284,12 +318,50 @@ def group_aggregate(ndarray[double_t] values, list label_list,
 
     return result, counts
 
-def _group_reorder(values, label_list):
+def _group_reorder(values, label_list, shape):
+    # group_index = np.zeros(len(label_list[0]), dtype='i4')
+    # for i in xrange(len(shape)):
+    #     stride = np.prod([x for x in shape[i+1:]], dtype='i4')
+    #     group_index += label_list[i] * stride
+    # na_mask = np.zeros(len(label_list[0]), dtype=bool)
+    # for arr in label_list:
+    #     na_mask |= arr == -1
+    # group_index[na_mask] = -1
+
+    # indexer = groupsort_indexer(group_index, np.prod(shape))
+
     indexer = np.lexsort(label_list[::-1])
     sorted_labels = [labels.take(indexer) for labels in label_list]
     sorted_values = values.take(indexer)
     return sorted_values, sorted_labels
 
+@cython.wraparound(False)
+def groupsort_indexer(ndarray[int32_t] index, Py_ssize_t ngroups):
+    cdef:
+        Py_ssize_t i, loc, label, n
+        ndarray[int32_t] counts, where, result
+
+    # count group sizes, location 0 for NA
+    counts = np.zeros(ngroups + 1, dtype='i4')
+    n = len(index)
+    for i from 0 <= i < n:
+        counts[index[i] + 1] += 1
+
+    # mark the start of each contiguous group of like-indexed data
+    where = np.zeros(ngroups + 1, dtype='i4')
+    for i from 1 <= i < ngroups + 1:
+        where[i] = where[i - 1] + counts[i - 1]
+
+    # this is our indexer
+    result = np.zeros(n, dtype='i4')
+    for i from 0 <= i < n:
+        label = index[i] + 1
+        result[where[label]] = i
+        where[label] += 1
+
+    return result
+
+
 cdef int _aggregate_group(double_t *out, int32_t *counts, double_t *values,
                            list labels, int start, int end, tuple shape,
                            Py_ssize_t which, Py_ssize_t offset,
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index e10b263c9..0675a60d8 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -122,6 +122,31 @@ def test_is_lexsorted():
 
     assert(not lib.is_lexsorted(failure))
 
+# def test_get_group_index():
+#     a = np.array([0, 1, 2, 0, 2, 1, 0, 0], dtype='i4')
+#     b = np.array([1, 0, 3, 2, 0, 2, 3, 0], dtype='i4')
+#     expected = np.array([1, 4, 11, 2, 8, 6, 3, 0], dtype='i4')
+
+#     result = lib.get_group_index([a, b], (3, 4))
+
+#     assert(np.array_equal(result, expected))
+
+def test_groupsort_indexer():
+    a = np.random.randint(0, 1000, 100).astype('i4')
+    b = np.random.randint(0, 1000, 100).astype('i4')
+
+    result = lib.groupsort_indexer(a, 1000)
+
+    # need to use a stable sort
+    expected = np.argsort(a, kind='mergesort')
+    assert(np.array_equal(result, expected))
+
+    # compare with lexsort
+    key = a * 1000 + b
+    result = lib.groupsort_indexer(key, 1000000)
+    expected = np.lexsort((b, a))
+    assert(np.array_equal(result, expected))
+
 class TestMoments(unittest.TestCase):
     pass
 
diff --git a/scripts/groupby_test.py b/scripts/groupby_test.py
index e2a0f107c..effe0671e 100644
--- a/scripts/groupby_test.py
+++ b/scripts/groupby_test.py
@@ -86,30 +86,45 @@ grouped = df.groupby(['key1', 'key2'])
 # print 'got'
 # print result
 
-tm.N = 10000
+# tm.N = 10000
 
-mapping = {'A': 0, 'C': 1, 'B': 0, 'D': 1}
-tf = lambda x: x - x.mean()
+# mapping = {'A': 0, 'C': 1, 'B': 0, 'D': 1}
+# tf = lambda x: x - x.mean()
 
-df = tm.makeTimeDataFrame()
-ts = df['A']
+# df = tm.makeTimeDataFrame()
+# ts = df['A']
 
-# grouped = df.groupby(lambda x: x.strftime('%m/%y'))
-grouped = df.groupby(mapping, axis=1)
-groupedT = df.T.groupby(mapping, axis=0)
+# # grouped = df.groupby(lambda x: x.strftime('%m/%y'))
+# grouped = df.groupby(mapping, axis=1)
+# groupedT = df.T.groupby(mapping, axis=0)
 
-r1 = groupedT.transform(tf).T
-r2 = grouped.transform(tf)
+# r1 = groupedT.transform(tf).T
+# r2 = grouped.transform(tf)
 
-fillit = lambda x: x.fillna(method='pad')
+# fillit = lambda x: x.fillna(method='pad')
 
-f = lambda x: x
+# f = lambda x: x
 
-transformed = df.groupby(lambda x: x.strftime('%m/%y')).transform(lambda x: x)
+# transformed = df.groupby(lambda x: x.strftime('%m/%y')).transform(lambda x: x)
 
-def ohlc(group):
-    return Series([group[0], group.max(), group.min(), group[-1]],
-                  index=['open', 'high', 'low', 'close'])
-grouper = [lambda x: x.year, lambda x: x.month]
-dr = DateRange('1/1/2000', '1/1/2002')
-ts = Series(np.random.randn(len(dr)), index=dr)
+# def ohlc(group):
+#     return Series([group[0], group.max(), group.min(), group[-1]],
+#                   index=['open', 'high', 'low', 'close'])
+# grouper = [lambda x: x.year, lambda x: x.month]
+# dr = DateRange('1/1/2000', '1/1/2002')
+# ts = Series(np.random.randn(len(dr)), index=dr)
+
+import string
+
+k = 20
+n = 1000
+
+keys = list(string.letters[:k])
+
+df = DataFrame({'A' : np.tile(keys, n),
+                'B' : np.repeat(keys[:k/2], n * 2),
+                'C' : np.random.randn(k * n)})
+
+def f():
+    for x in df.groupby(['A', 'B']):
+        pass
