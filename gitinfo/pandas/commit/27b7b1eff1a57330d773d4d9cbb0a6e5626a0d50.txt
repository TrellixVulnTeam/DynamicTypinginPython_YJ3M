commit 27b7b1eff1a57330d773d4d9cbb0a6e5626a0d50
Author: Jeff Reback <jeff@reback.net>
Date:   Mon Aug 10 19:21:02 2015 -0400

    API: add DatetimeBlockTZ #8260
    
    fix scalar comparisons vs None generally
    
    fix NaT formattting in Series
    
    TST: skip postgresql test with tz's
    
    update for msgpack
    
    Conflicts:
            pandas/core/base.py
            pandas/core/categorical.py
            pandas/core/format.py
            pandas/tests/test_base.py
            pandas/util/testing.py
    
    full interop for tz-aware Series & timedeltas #10763

diff --git a/asv_bench/asv.conf.json b/asv_bench/asv.conf.json
index 239f9aa19..dcea59545 100644
--- a/asv_bench/asv.conf.json
+++ b/asv_bench/asv.conf.json
@@ -18,7 +18,7 @@
     // If missing or the empty string, the tool will be automatically
     // determined by looking for tools on the PATH environment
     // variable.
-    "environment_type": "",
+    "environment_type": "conda",
 
     // the base URL to show a commit for the project.
     "show_commit_url": "https://github.com/pydata/pandas/commit/",
@@ -26,7 +26,7 @@
     // The Pythons you'd like to test against.  If not provided, defaults
     // to the current version of Python used to run `asv`.
     // "pythons": ["2.7", "3.4"],
-    "pythons": ["2.7", "3.4"],
+    "pythons": ["2.7"],
 
     // The matrix of dependencies to test.  Each key is the name of a
     // package (in PyPI) and the values are version numbers.  An empty
@@ -41,7 +41,7 @@
         "sqlalchemy": [],
         "scipy": [],
         "numexpr": [],
-        "tables": [],
+        "pytables": [],
         "openpyxl": [],
         "xlrd": [],
         "xlwt": []
diff --git a/asv_bench/benchmarks/binary_ops.py b/asv_bench/benchmarks/binary_ops.py
index 187101b1f..d22d01f26 100644
--- a/asv_bench/benchmarks/binary_ops.py
+++ b/asv_bench/benchmarks/binary_ops.py
@@ -203,34 +203,59 @@ class series_timestamp_compare(object):
 
 class timestamp_ops_diff1(object):
     goal_time = 0.2
+    N = 1000000
 
     def setup(self):
-        self.N = 1000000
-        self.s = Series(date_range('20010101', periods=self.N, freq='s'))
+        self.s = self.create()
+
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='s'))
 
     def time_timestamp_ops_diff1(self):
         self.s.diff()
 
+class timestamp_tz_ops_diff1(timestamp_ops_diff1):
+    N = 10000
+
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='s', tz='US/Eastern'))
 
 class timestamp_ops_diff2(object):
     goal_time = 0.2
+    N = 1000000
 
     def setup(self):
-        self.N = 1000000
-        self.s = Series(date_range('20010101', periods=self.N, freq='s'))
+        self.s = self.create()
+
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='s'))
 
     def time_timestamp_ops_diff2(self):
         (self.s - self.s.shift())
 
+class timestamp_tz_ops_diff2(timestamp_ops_diff2):
+    N = 10000
+
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='s', tz='US/Eastern'))
 
 class timestamp_series_compare(object):
     goal_time = 0.2
+    N = 1000000
 
     def setup(self):
-        self.N = 1000000
         self.halfway = ((self.N // 2) - 1)
-        self.s = Series(date_range('20010101', periods=self.N, freq='T'))
+        self.s = self.create()
         self.ts = self.s[self.halfway]
 
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='T'))
+
     def time_timestamp_series_compare(self):
-        (self.ts >= self.s)
\ No newline at end of file
+        (self.ts >= self.s)
+
+class timestamp_tz_series_compare(timestamp_series_compare):
+    N = 10000
+
+    def create(self):
+        return Series(date_range('20010101', periods=self.N, freq='T', tz='US/Eastern'))
diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index 3ea90447d..bc4b463e5 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -1590,9 +1590,10 @@ dtypes
 ------
 
 The main types stored in pandas objects are ``float``, ``int``, ``bool``,
-``datetime64[ns]``, ``timedelta[ns]`` and ``object``. In addition these dtypes
-have item sizes, e.g. ``int64`` and ``int32``. A convenient :attr:`~DataFrame.dtypes``
-attribute for DataFrames returns a Series with the data type of each column.
+``datetime64[ns]`` and ``datetime64[ns, tz]`` (in >= 0.17.0), ``timedelta[ns]``, ``category`` (in >= 0.15.0), and ``object``. In addition these dtypes
+have item sizes, e.g. ``int64`` and ``int32``. See :ref:`Series with TZ <timeseries.timezone_series>` for more detail on ``datetime64[ns, tz]`` dtypes.
+
+A convenient :attr:`~DataFrame.dtypes` attribute for DataFrames returns a Series with the data type of each column.
 
 .. ipython:: python
 
@@ -1814,8 +1815,14 @@ dtypes:
    df['tdeltas'] = df.dates.diff()
    df['uint64'] = np.arange(3, 6).astype('u8')
    df['other_dates'] = pd.date_range('20130101', periods=3).values
+   df['tz_aware_dates'] = pd.date_range('20130101', periods=3, tz='US/Eastern')
    df
 
+And the dtypes
+
+.. ipython:: python
+
+   df.dtypes
 
 :meth:`~DataFrame.select_dtypes` has two parameters ``include`` and ``exclude`` that allow you to
 say "give me the columns WITH these dtypes" (``include``) and/or "give the
@@ -1868,7 +1875,7 @@ All numpy dtypes are subclasses of ``numpy.generic``:
 
 .. note::
 
-    Pandas also defines an additional ``category`` dtype, which is not integrated into the normal
+    Pandas also defines the types ``category``, and ``datetime64[ns, tz]``, which are not integrated into the normal
     numpy hierarchy and wont show up with the above function.
 
 .. note::
diff --git a/doc/source/release.rst b/doc/source/release.rst
index 9580f90c2..23f8ad368 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -50,6 +50,7 @@ Highlights include:
 
 - Release the Global Interpreter Lock (GIL) on some cython operations, see :ref:`here <whatsnew_0170.gil>`
 - The sorting API has been revamped to remove some long-time inconsistencies, see :ref:`here <whatsnew_0170.api_breaking.sorting>`
+- Support for a ``datetime64[ns]`` with timezones as a first-class dtype, see :ref:`here <whatsnew_0170.tz>`
 - The default for ``to_datetime`` will now be to ``raise`` when presented with unparseable formats,
   previously this would return the original input, see :ref:`here <whatsnew_0170.api_breaking.to_datetime>`
 - The default for ``dropna`` in ``HDFStore`` has changed to ``False``, to store by default all rows even
diff --git a/doc/source/timeseries.rst b/doc/source/timeseries.rst
index 9795c082d..dd13e8fab 100644
--- a/doc/source/timeseries.rst
+++ b/doc/source/timeseries.rst
@@ -1745,3 +1745,30 @@ constructor as well as ``tz_localize``.
 
    # tz_convert(None) is identical with tz_convert('UTC').tz_localize(None)
    didx.tz_convert('UCT').tz_localize(None)
+
+.. _timeseries.timezone_series:
+
+TZ aware Dtypes
+~~~~~~~~~~~~~~~
+
+.. versionadded:: 0.17.0
+
+``Series/DatetimeIndex`` with a timezone naive value are represented with a dtype of ``datetime64[ns]``.
+
+.. ipython:: python
+
+   dr = pd.date_range('20130101',periods=3)
+   dr
+   s = Series(dr)
+   s
+
+``Series/DatetimeIndex`` with a timezone aware value are represented with a dtype of ``datetime64[ns, tz]``.
+
+.. ipython:: python
+
+   dr = pd.date_range('20130101',periods=3,tz='US/Eastern')
+   dr
+   s = Series(dr)
+   s
+
+Both of these ``Series`` can be manipulated via the ``.dt`` accessor, see the :ref:`docs <basics.dt_accessors>` as well.
diff --git a/doc/source/whatsnew/v0.17.0.txt b/doc/source/whatsnew/v0.17.0.txt
index e2c3bfd17..3eed23784 100644
--- a/doc/source/whatsnew/v0.17.0.txt
+++ b/doc/source/whatsnew/v0.17.0.txt
@@ -30,6 +30,7 @@ Highlights include:
 
 - Release the Global Interpreter Lock (GIL) on some cython operations, see :ref:`here <whatsnew_0170.gil>`
 - The sorting API has been revamped to remove some long-time inconsistencies, see :ref:`here <whatsnew_0170.api_breaking.sorting>`
+- Support for a ``datetime64[ns]`` with timezones as a first-class dtype, see :ref:`here <whatsnew_0170.tz>`
 - The default for ``to_datetime`` will now be to ``raise`` when presented with unparseable formats,
   previously this would return the original input, see :ref:`here <whatsnew_0170.api_breaking.to_datetime>`
 - The default for ``dropna`` in ``HDFStore`` has changed to ``False``, to store by default all rows even
@@ -417,6 +418,58 @@ To keep the previous behaviour, you can use ``errors='ignore'``:
 Furthermore, ``pd.to_timedelta`` has gained a similar API, of ``errors='raise'|'ignore'|'coerce'``, and the ``coerce`` keyword
 has been deprecated in favor of ``errors='coerce'``.
 
+.. _whatsnew_0170.tz:
+
+Datetime with TZ
+~~~~~~~~~~~~~~~~
+
+We are adding an implementation that natively supports datetime with timezones. A ``Series`` or a ``DataFrame`` column previously
+*could* be assigned a datetime with timezones, and would work as an ``object`` dtype. This had performance issues with a large
+number rows. (:issue:`8260`, :issue:`10763`)
+
+The new implementation allows for having a single-timezone across all rows, and operating on it in a performant manner.
+
+.. ipython:: python
+
+   df = DataFrame({'A' : date_range('20130101',periods=3),
+                   'B' : date_range('20130101',periods=3,tz='US/Eastern'),
+                   'C' : date_range('20130101',periods=3,tz='CET')})
+   df
+   df.dtypes
+
+.. ipython:: python
+
+   df.B
+   df.B.dt.tz_localize(None)
+
+This uses a new-dtype representation as well, that is very similar in look-and-feel to its numpy cousin ``datetime64[ns]``
+
+.. ipython:: python
+
+   df['B'].dtype
+   type(df['B']).dtype
+
+.. note::
+
+   There is a slightly different string repr for the underlying ``DatetimeIndex`` as a result of the dtype changes, but
+   functionally these are the same.
+
+   .. code-block:: python
+
+      In [1]: pd.date_range('20130101',periods=3,tz='US/Eastern')
+      Out[1]: DatetimeIndex(['2013-01-01 00:00:00-05:00', '2013-01-02 00:00:00-05:00',
+                             '2013-01-03 00:00:00-05:00'],
+                            dtype='datetime64[ns]', freq='D', tz='US/Eastern')
+
+      In [2]: pd.date_range('20130101',periods=3,tz='US/Eastern').dtype
+      Out[2]: dtype('<M8[ns]')
+
+   .. ipython:: python
+
+      pd.date_range('20130101',periods=3,tz='US/Eastern')
+      pd.date_range('20130101',periods=3,tz='US/Eastern').dtype
+
+
 .. _whatsnew_0170.api_breaking.convert_objects:
 
 Changes to convert_objects
@@ -824,6 +877,9 @@ Bug Fixes
 - Bug in incorrection computation of ``.mean()`` on ``timedelta64[ns]`` because of overflow (:issue:`9442`)
 - Bug in ``DataFrame.to_html(index=False)`` renders unnecessary ``name`` row (:issue:`10344`)
 - Bug in ``DataFrame.to_latex()`` the ``column_format`` argument could not be passed (:issue:`9402`)
+- Bug in ``DatetimeIndex`` when localizing with ``NaT`` (:issue:`10477`)
+- Bug in ``Series.dt`` ops in preserving meta-data (:issue:`10477`)
+- Bug in preserving ``NaT`` when passed in an otherwise invalid ``to_datetime`` construction (:issue:`10477`)
 - Bug in ``DataFrame.apply`` when function returns categorical series. (:issue:`9573`)
 - Bug in ``to_datetime`` with invalid dates and formats supplied (:issue:`10154`)
 - Bug in ``Index.drop_duplicates`` dropping name(s) (:issue:`10115`)
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 8f1dab4f8..34bf173d6 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -206,7 +206,7 @@ def value_counts(values, sort=True, ascending=False, normalize=False,
     """
     from pandas.core.series import Series
     from pandas.tools.tile import cut
-    from pandas.tseries.period import PeriodIndex
+    from pandas import Index, PeriodIndex, DatetimeIndex
 
     name = getattr(values, 'name', None)
     values = Series(values).values
@@ -225,11 +225,15 @@ def value_counts(values, sort=True, ascending=False, normalize=False,
 
         dtype = values.dtype
         is_period = com.is_period_arraylike(values)
+        is_datetimetz = com.is_datetimetz(values)
 
-        if com.is_datetime_or_timedelta_dtype(dtype) or is_period:
+        if com.is_datetime_or_timedelta_dtype(dtype) or is_period or is_datetimetz:
 
             if is_period:
-                values = PeriodIndex(values, name=name)
+                values = PeriodIndex(values)
+            elif is_datetimetz:
+                tz = getattr(values, 'tz', None)
+                values = DatetimeIndex(values).tz_localize(None)
 
             values = values.view(np.int64)
             keys, counts = htable.value_count_scalar64(values, dropna)
@@ -239,8 +243,14 @@ def value_counts(values, sort=True, ascending=False, normalize=False,
                 msk = keys != iNaT
                 keys, counts = keys[msk], counts[msk]
 
+            # localize to the original tz if necessary
+            if is_datetimetz:
+                keys = DatetimeIndex(keys).tz_localize(tz)
+
             # convert the keys back to the dtype we came in
-            keys = keys.astype(dtype)
+            else:
+                keys = keys.astype(dtype)
+
 
         elif com.is_integer_dtype(dtype):
             values = com._ensure_int64(values)
@@ -257,7 +267,9 @@ def value_counts(values, sort=True, ascending=False, normalize=False,
                 keys = np.insert(keys, 0, np.NaN)
                 counts = np.insert(counts, 0, mask.sum())
 
-        result = Series(counts, index=com._values_from_object(keys), name=name)
+        if not isinstance(keys, Index):
+            keys = Index(keys)
+        result = Series(counts, index=keys, name=name)
 
         if bins is not None:
             # TODO: This next line should be more efficient
diff --git a/pandas/core/base.py b/pandas/core/base.py
index fe9bac7f4..d3850be13 100644
--- a/pandas/core/base.py
+++ b/pandas/core/base.py
@@ -364,6 +364,11 @@ class IndexOpsMixin(object):
         """ return the base object if the memory of the underlying data is shared """
         return self.values.base
 
+    @property
+    def _values(self):
+        """ the internal implementation """
+        return self.values
+
     def max(self):
         """ The maximum value of the object """
         return nanops.nanmax(self.values)
@@ -397,6 +402,14 @@ class IndexOpsMixin(object):
         """ return if I have any nans; enables various perf speedups """
         return com.isnull(self).any()
 
+    def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,
+                filter_type=None, **kwds):
+        """ perform the reduction type operation if we can """
+        func = getattr(self,name,None)
+        if func is None:
+            raise TypeError("{klass} cannot perform the operation {op}".format(klass=self.__class__.__name__,op=name))
+        return func(**kwds)
+
     def value_counts(self, normalize=False, sort=True, ascending=False,
                      bins=None, dropna=True):
         """
@@ -586,7 +599,7 @@ class IndexOpsMixin(object):
     @deprecate_kwarg('take_last', 'keep', mapping={True: 'last', False: 'first'})
     @Appender(_shared_docs['duplicated'] % _indexops_doc_kwargs)
     def duplicated(self, keep='first'):
-        keys = com._ensure_object(self.values)
+        keys = com._values_from_object(com._ensure_object(self.values))
         duplicated = lib.duplicated(keys, keep=keep)
         try:
             return self._constructor(duplicated,
diff --git a/pandas/core/categorical.py b/pandas/core/categorical.py
index 2f465ded1..9decd5e21 100644
--- a/pandas/core/categorical.py
+++ b/pandas/core/categorical.py
@@ -12,13 +12,14 @@ from pandas.core.base import PandasObject, PandasDelegate
 import pandas.core.common as com
 from pandas.util.decorators import cache_readonly, deprecate_kwarg
 
-from pandas.core.common import (CategoricalDtype, ABCSeries, ABCIndexClass, ABCCategoricalIndex,
+from pandas.core.common import (ABCSeries, ABCIndexClass, ABCPeriodIndex, ABCCategoricalIndex,
                                 isnull, notnull, is_dtype_equal,
                                 is_categorical_dtype, is_integer_dtype, is_object_dtype,
                                 _possibly_infer_to_datetimelike, get_dtype_kinds,
                                 is_list_like, is_sequence, is_null_slice, is_bool,
                                 _ensure_platform_int, _ensure_object, _ensure_int64,
                                 _coerce_indexer_dtype, take_1d)
+from pandas.core.dtypes import CategoricalDtype
 from pandas.util.terminal import get_terminal_size
 from pandas.core.config import get_option
 
@@ -85,7 +86,7 @@ def _cat_compare_op(op):
 def maybe_to_categorical(array):
     """ coerce to a categorical if a series is given """
     if isinstance(array, (ABCSeries, ABCCategoricalIndex)):
-        return array.values
+        return array._values
     return array
 
 _codes_doc = """The category codes of this categorical.
@@ -231,7 +232,7 @@ class Categorical(PandasObject):
 
             # we are either a Series or a CategoricalIndex
             if isinstance(values, (ABCSeries, ABCCategoricalIndex)):
-                values = values.values
+                values = values._values
 
             if ordered is None:
                 ordered = values.ordered
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 77536fb39..8ffffae6b 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -20,7 +20,7 @@ import pandas.lib as lib
 import pandas.tslib as tslib
 from pandas import compat
 from pandas.compat import StringIO, BytesIO, range, long, u, zip, map, string_types, iteritems
-
+from pandas.core.dtypes import CategoricalDtype, CategoricalDtypeType, DatetimeTZDtype, DatetimeTZDtypeType
 from pandas.core.config import get_option
 
 class PandasError(Exception):
@@ -114,77 +114,29 @@ class _ABCGeneric(type):
 ABCGeneric = _ABCGeneric("ABCGeneric", tuple(), {})
 
 
-class CategoricalDtypeType(type):
-    """
-    the type of CategoricalDtype, this metaclass determines subclass ability
-    """
-    def __init__(cls, name, bases, attrs):
-        pass
-
-class CategoricalDtype(object):
-    __meta__ = CategoricalDtypeType
-    """
-    A np.dtype duck-typed class, suitable for holding a custom categorical dtype.
-
-    THIS IS NOT A REAL NUMPY DTYPE, but essentially a sub-class of np.object
-    """
-    name = 'category'
-    names = None
-    type = CategoricalDtypeType
-    subdtype = None
-    kind = 'O'
-    str = '|O08'
-    num = 100
-    shape = tuple()
-    itemsize = 8
-    base = np.dtype('O')
-    isbuiltin = 0
-    isnative = 0
-
-    def __unicode__(self):
-        return self.name
-
-    def __str__(self):
-        """
-        Return a string representation for a particular Object
-
-        Invoked by str(df) in both py2/py3.
-        Yields Bytestring in Py2, Unicode String in py3.
-        """
-
-        if compat.PY3:
-            return self.__unicode__()
-        return self.__bytes__()
-
-    def __bytes__(self):
-        """
-        Return a string representation for a particular object.
-
-        Invoked by bytes(obj) in py3 only.
-        Yields a bytestring in both py2/py3.
-        """
-        from pandas.core.config import get_option
-
-        encoding = get_option("display.encoding")
-        return self.__unicode__().encode(encoding, 'replace')
-
-    def __repr__(self):
-        """
-        Return a string representation for a particular object.
+def bind_method(cls, name, func):
+    """Bind a method to class, python 2 and python 3 compatible.
 
-        Yields Bytestring in Py2, Unicode String in py3.
-        """
-        return str(self)
+    Parameters
+    ----------
 
-    def __hash__(self):
-        # make myself hashable
-        return hash(str(self))
+    cls : type
+        class to receive bound method
+    name : basestring
+        name of method on class instance
+    func : function
+        function to be bound as method
 
-    def __eq__(self, other):
-        if isinstance(other, compat.string_types):
-            return other == self.name
 
-        return isinstance(other, CategoricalDtype)
+    Returns
+    -------
+    None
+    """
+    # only python 2 has bound/unbound method issue
+    if not compat.PY3:
+        setattr(cls, name, types.MethodType(func, None, cls))
+    else:
+        setattr(cls, name, func)
 
 def isnull(obj):
     """Detect missing values (NaN in numeric arrays, None/NaN in object arrays)
@@ -764,9 +716,12 @@ def take_nd(arr, indexer, axis=0, out=None, fill_value=np.nan,
         undefined if allow_fill == False and -1 is present in indexer.
     """
 
+    # dispatch to internal type takes
     if is_categorical(arr):
         return arr.take_nd(indexer, fill_value=fill_value,
                            allow_fill=allow_fill)
+    elif is_datetimetz(arr):
+        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
 
     if indexer is None:
         indexer = np.arange(arr.shape[axis], dtype=np.int64)
@@ -1247,13 +1202,18 @@ def _maybe_upcast(values, fill_value=np.nan, dtype=None, copy=False):
     copy : if True always make a copy even if no upcast is required
     """
 
-    if dtype is None:
-        dtype = values.dtype
-    new_dtype, fill_value = _maybe_promote(dtype, fill_value)
-    if new_dtype != values.dtype:
-        values = values.astype(new_dtype)
-    elif copy:
-        values = values.copy()
+    if is_internal_type(values):
+        if copy:
+            values = values.copy()
+    else:
+        if dtype is None:
+            dtype = values.dtype
+        new_dtype, fill_value = _maybe_promote(dtype, fill_value)
+        if new_dtype != values.dtype:
+            values = values.astype(new_dtype)
+        elif copy:
+            values = values.copy()
+
     return values, fill_value
 
 
@@ -1724,7 +1684,7 @@ def _interpolate_scipy_wrapper(x, y, new_x, method, fill_value=None,
 
     if getattr(x, 'is_all_dates', False):
         # GH 5975, scipy.interp1d can't hande datetime64s
-        x, new_x = x.values.astype('i8'), new_x.astype('i8')
+        x, new_x = x._values.astype('i8'), new_x.astype('i8')
 
     try:
         alt_methods['pchip'] = interpolate.pchip_interpolate
@@ -1831,7 +1791,8 @@ def _invalidate_string_dtypes(dtype_set):
 
 
 def _get_dtype_from_object(dtype):
-    """Get a numpy dtype.type-style object.
+    """Get a numpy dtype.type-style object. This handles the
+       datetime64[ns] and datetime64[ns, TZ] compat
 
     Notes
     -----
@@ -1840,6 +1801,10 @@ def _get_dtype_from_object(dtype):
     # type object from a dtype
     if isinstance(dtype, type) and issubclass(dtype, np.generic):
         return dtype
+    elif is_categorical(dtype):
+        return CategoricalDtype().type
+    elif is_datetimetz(dtype):
+        return DatetimeTZDtype(dtype).type
     elif isinstance(dtype, np.dtype):  # dtype object
         try:
             _validate_date_like_dtype(dtype)
@@ -1850,15 +1815,16 @@ def _get_dtype_from_object(dtype):
     elif isinstance(dtype, compat.string_types):
         if dtype == 'datetime' or dtype == 'timedelta':
             dtype += '64'
-        elif dtype == 'category':
-            return CategoricalDtypeType
+
         try:
-            return _get_dtype_from_object(getattr(np, dtype))
+            return _get_dtype_from_object(getattr(np,dtype))
         except AttributeError:
             # handles cases like _get_dtype(int)
             # i.e., python objects that are valid dtypes (unlike user-defined
             # types, in general)
+            # further handle internal types
             pass
+
     return _get_dtype_from_object(np.dtype(dtype))
 
 
@@ -1978,8 +1944,8 @@ def _possibly_convert_platform(values):
     if isinstance(values, (list, tuple)):
         values = lib.list_to_object_array(values)
     if getattr(values, 'dtype', None) == np.object_:
-        if hasattr(values, 'values'):
-            values = values.values
+        if hasattr(values, '_values'):
+            values = values._values
         values = lib.maybe_convert_objects(values)
 
     return values
@@ -1997,18 +1963,21 @@ def _possibly_cast_to_datetime(value, dtype, errors='raise'):
             dtype = np.dtype(dtype)
 
         is_datetime64 = is_datetime64_dtype(dtype)
+        is_datetime64tz = is_datetime64tz_dtype(dtype)
         is_timedelta64 = is_timedelta64_dtype(dtype)
 
-        if is_datetime64 or is_timedelta64:
+        if is_datetime64 or is_datetime64tz or is_timedelta64:
 
             # force the dtype if needed
-            if is_datetime64 and dtype != _NS_DTYPE:
+            if is_datetime64 and not is_dtype_equal(dtype,_NS_DTYPE):
                 if dtype.name == 'datetime64[ns]':
                     dtype = _NS_DTYPE
                 else:
                     raise TypeError(
                         "cannot convert datetimelike to dtype [%s]" % dtype)
-            elif is_timedelta64 and dtype != _TD_DTYPE:
+            elif is_datetime64tz:
+                pass
+            elif is_timedelta64 and not is_dtype_equal(dtype,_TD_DTYPE):
                 if dtype.name == 'timedelta64[ns]':
                     dtype = _TD_DTYPE
                 else:
@@ -2026,15 +1995,28 @@ def _possibly_cast_to_datetime(value, dtype, errors='raise'):
                     value = tslib.iNaT
 
                 # we have an array of datetime or timedeltas & nulls
-                elif np.prod(value.shape) and value.dtype != dtype:
+                elif np.prod(value.shape) and not is_dtype_equal(value.dtype, dtype):
                     try:
                         if is_datetime64:
-                            value = to_datetime(value, errors=errors).values
+                            value = to_datetime(value, errors=errors)._values
+                        elif is_datetime64tz:
+
+                            # input has to be UTC at this point, so just localize
+                            value = to_datetime(value, errors=errors).tz_localize(dtype.tz)
                         elif is_timedelta64:
-                            value = to_timedelta(value, errors=errors).values
+                            value = to_timedelta(value, errors=errors)._values
                     except (AttributeError, ValueError):
                         pass
 
+        # coerce datetimelike to object
+        elif is_datetime64_dtype(value) and not is_datetime64_dtype(dtype):
+            if is_object_dtype(dtype):
+                ints = np.asarray(value).view('i8')
+                return tslib.ints_to_pydatetime(ints)
+
+            # we have a non-castable dtype that was passed
+            raise TypeError('Cannot cast datetime64 to %s' % dtype)
+
     else:
 
         is_array = isinstance(value, np.ndarray)
@@ -2073,13 +2055,19 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
 
     Parameters
     ----------
-    value : np.array
+    value : np.array / Series / Index / list-like
     convert_dates : boolean, default False
        if True try really hard to convert dates (such as datetime.date), other
        leave inferred dtype 'date' alone
 
     """
 
+    if isinstance(value, (ABCDatetimeIndex, ABCPeriodIndex)):
+        return value
+    elif isinstance(value, ABCSeries):
+        if isinstance(value._values, ABCDatetimeIndex):
+            return value._values
+
     v = value
     if not is_list_like(v):
         v = [v]
@@ -2093,9 +2081,22 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
         def _try_datetime(v):
             # safe coerce to datetime64
             try:
-                return tslib.array_to_datetime(v, errors='raise').reshape(shape)
+                v = tslib.array_to_datetime(v, errors='raise')
+            except ValueError:
+
+                # we might have a sequence of the same-datetimes with tz's
+                # if so coerce to a DatetimeIndex; if they are not the same, then
+                # these stay as object dtype
+                try:
+                    from pandas import to_datetime
+                    return to_datetime(v)
+                except:
+                    pass
+
             except:
-                return v
+                pass
+
+            return v.reshape(shape)
 
         def _try_timedelta(v):
             # safe coerce to timedelta64
@@ -2103,7 +2104,7 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
             # will try first with a string & object conversion
             from pandas.tseries.timedeltas import to_timedelta
             try:
-                return to_timedelta(v).values.reshape(shape)
+                return to_timedelta(v)._values.reshape(shape)
             except:
                 return v
 
@@ -2112,9 +2113,9 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
         inferred_type = lib.infer_dtype(sample)
 
         if inferred_type in ['datetime', 'datetime64'] or (convert_dates and inferred_type in ['date']):
-            value = _try_datetime(v).reshape(shape)
+            value = _try_datetime(v)
         elif inferred_type in ['timedelta', 'timedelta64']:
-            value = _try_timedelta(v).reshape(shape)
+            value = _try_timedelta(v)
 
         # its possible to have nulls intermixed within the datetime or timedelta
         # these will in general have an inferred_type of 'mixed', so have to try
@@ -2125,9 +2126,9 @@ def _possibly_infer_to_datetimelike(value, convert_dates=False):
         elif inferred_type in ['mixed']:
 
             if lib.is_possible_datetimelike_array(_ensure_object(v)):
-                value = _try_timedelta(v).reshape(shape)
+                value = _try_timedelta(v)
                 if lib.infer_dtype(value) in ['mixed']:
-                    value = _try_datetime(v).reshape(shape)
+                    value = _try_datetime(v)
 
     return value
 
@@ -2448,19 +2449,21 @@ def is_period_arraylike(arr):
 
 def is_datetime_arraylike(arr):
     """ return if we are datetime arraylike / DatetimeIndex """
-    if isinstance(arr, pd.DatetimeIndex):
+    if isinstance(arr, ABCDatetimeIndex):
         return True
     elif isinstance(arr, (np.ndarray, ABCSeries)):
         return arr.dtype == object and lib.infer_dtype(arr) == 'datetime'
     return getattr(arr, 'inferred_type', None) == 'datetime'
 
 def is_datetimelike(arr):
-    return arr.dtype in _DATELIKE_DTYPES or isinstance(arr, ABCPeriodIndex)
+    return arr.dtype in _DATELIKE_DTYPES or isinstance(arr, ABCPeriodIndex) or is_datetimetz(arr)
 
 def _coerce_to_dtype(dtype):
     """ coerce a string / np.dtype to a dtype """
     if is_categorical_dtype(dtype):
         dtype = CategoricalDtype()
+    elif is_datetime64tz_dtype(dtype):
+        dtype = DatetimeTZDtype(dtype)
     else:
         dtype = np.dtype(dtype)
     return dtype
@@ -2471,9 +2474,18 @@ def _get_dtype(arr_or_dtype):
     elif isinstance(arr_or_dtype, type):
         return np.dtype(arr_or_dtype)
     elif isinstance(arr_or_dtype, CategoricalDtype):
-        return CategoricalDtype()
-    return arr_or_dtype.dtype
+        return arr_or_dtype
+    elif isinstance(arr_or_dtype, DatetimeTZDtype):
+        return arr_or_dtype
+    elif isinstance(arr_or_dtype, compat.string_types):
+        if is_categorical_dtype(arr_or_dtype):
+            return CategoricalDtype.construct_from_string(arr_or_dtype)
+        elif is_datetime64tz_dtype(arr_or_dtype):
+            return DatetimeTZDtype.construct_from_string(arr_or_dtype)
 
+    if hasattr(arr_or_dtype, 'dtype'):
+        arr_or_dtype = arr_or_dtype.dtype
+    return np.dtype(arr_or_dtype)
 
 def _get_dtype_type(arr_or_dtype):
     if isinstance(arr_or_dtype, np.dtype):
@@ -2482,23 +2494,26 @@ def _get_dtype_type(arr_or_dtype):
         return np.dtype(arr_or_dtype).type
     elif isinstance(arr_or_dtype, CategoricalDtype):
         return CategoricalDtypeType
+    elif isinstance(arr_or_dtype, DatetimeTZDtype):
+        return DatetimeTZDtypeType
     elif isinstance(arr_or_dtype, compat.string_types):
         if is_categorical_dtype(arr_or_dtype):
             return CategoricalDtypeType
+        elif is_datetime64tz_dtype(arr_or_dtype):
+            return DatetimeTZDtypeType
         return _get_dtype_type(np.dtype(arr_or_dtype))
     try:
         return arr_or_dtype.dtype.type
     except AttributeError:
-        raise ValueError('%r is not a dtype' % arr_or_dtype)
+        return type(None)
 
 def is_dtype_equal(source, target):
     """ return a boolean if the dtypes are equal """
-    source = _get_dtype_type(source)
-    target = _get_dtype_type(target)
-
     try:
+        source = _get_dtype(source)
+        target = _get_dtype(target)
         return source == target
-    except TypeError:
+    except (TypeError, AttributeError):
 
         # invalid comparison
         # object == category will hit this
@@ -2524,14 +2539,24 @@ def is_int_or_datetime_dtype(arr_or_dtype):
     return (issubclass(tipo, np.integer) or
             issubclass(tipo, (np.datetime64, np.timedelta64)))
 
-
 def is_datetime64_dtype(arr_or_dtype):
-    tipo = _get_dtype_type(arr_or_dtype)
+    try:
+        tipo = _get_dtype_type(arr_or_dtype)
+    except TypeError:
+        return False
     return issubclass(tipo, np.datetime64)
 
+def is_datetime64tz_dtype(arr_or_dtype):
+    return DatetimeTZDtype.is_dtype(arr_or_dtype)
+
+def is_datetime64_any_dtype(arr_or_dtype):
+    return is_datetime64_dtype(arr_or_dtype) or is_datetime64tz_dtype(arr_or_dtype)
 
 def is_datetime64_ns_dtype(arr_or_dtype):
-    tipo = _get_dtype(arr_or_dtype)
+    try:
+        tipo = _get_dtype(arr_or_dtype)
+    except TypeError:
+        return False
     return tipo == _NS_DTYPE
 
 def is_timedelta64_dtype(arr_or_dtype):
@@ -2555,9 +2580,10 @@ def is_datetimelike_v_numeric(a, b):
         a = np.asarray(a)
     if not hasattr(b, 'dtype'):
         b = np.asarray(b)
-    f = lambda x: is_integer_dtype(x) or is_float_dtype(x)
-    return (needs_i8_conversion(a) and f(b)) or (
-        needs_i8_conversion(b) and f(a))
+    is_numeric = lambda x: is_integer_dtype(x) or is_float_dtype(x)
+    is_datetimelike = needs_i8_conversion
+    return (is_datetimelike(a) and is_numeric(b)) or (
+        is_datetimelike(b) and is_numeric(a))
 
 def is_datetimelike_v_object(a, b):
     # return if we have an i8 convertible and object comparision
@@ -2566,14 +2592,17 @@ def is_datetimelike_v_object(a, b):
     if not hasattr(b, 'dtype'):
         b = np.asarray(b)
     f = lambda x: is_object_dtype(x)
-    return (needs_i8_conversion(a) and f(b)) or (
-        needs_i8_conversion(b) and f(a))
+    is_object = lambda x: is_integer_dtype(x) or is_float_dtype(x)
+    is_datetimelike = needs_i8_conversion
+    return (is_datetimelike(a) and is_object(b)) or (
+        is_datetimelike(b) and is_object(a))
 
-needs_i8_conversion = is_datetime_or_timedelta_dtype
+needs_i8_conversion = lambda arr_or_dtype: is_datetime_or_timedelta_dtype(arr_or_dtype) or \
+                      is_datetime64tz_dtype(arr_or_dtype)
 
 def i8_boxer(arr_or_dtype):
     """ return the scalar boxer for the dtype """
-    if is_datetime64_dtype(arr_or_dtype):
+    if is_datetime64_dtype(arr_or_dtype) or is_datetime64tz_dtype(arr_or_dtype):
         return lib.Timestamp
     elif is_timedelta64_dtype(arr_or_dtype):
         return lambda x: lib.Timedelta(x,unit='ns')
@@ -2603,20 +2632,33 @@ def is_bool_dtype(arr_or_dtype):
         return False
     return issubclass(tipo, np.bool_)
 
+def is_sparse(array):
+    """ return if we are a sparse array """
+    return isinstance(array, (ABCSparseArray, ABCSparseSeries))
+
+def is_datetimetz(array):
+    """ return if we are a datetime with tz array """
+    return (isinstance(array, ABCDatetimeIndex) and getattr(array,'tz',None) is not None) or is_datetime64tz_dtype(array)
+
+def is_internal_type(value):
+    """
+    if we are a klass that is preserved by the internals
+    these are internal klasses that we represent (and don't use a np.array)
+    """
+    if is_categorical(value):
+        return True
+    elif is_sparse(value):
+        return True
+    elif is_datetimetz(value):
+        return True
+    return False
+
 def is_categorical(array):
     """ return if we are a categorical possibility """
-    return isinstance(array, ABCCategorical) or isinstance(array.dtype, CategoricalDtype)
+    return isinstance(array, ABCCategorical) or is_categorical_dtype(array)
 
 def is_categorical_dtype(arr_or_dtype):
-    if hasattr(arr_or_dtype,'dtype'):
-        arr_or_dtype = arr_or_dtype.dtype
-
-    if isinstance(arr_or_dtype, CategoricalDtype):
-        return True
-    try:
-        return arr_or_dtype == 'category'
-    except:
-        return False
+    return CategoricalDtype.is_dtype(arr_or_dtype)
 
 def is_complex_dtype(arr_or_dtype):
     tipo = _get_dtype_type(arr_or_dtype)
@@ -2979,8 +3021,10 @@ def get_dtype_kinds(l):
         dtype = arr.dtype
         if is_categorical_dtype(dtype):
             typ = 'category'
-        elif isinstance(arr, ABCSparseArray):
+        elif is_sparse(arr):
             typ = 'sparse'
+        elif is_datetimetz(arr):
+            typ = 'datetimetz'
         elif is_datetime64_dtype(dtype):
             typ = 'datetime'
         elif is_timedelta64_dtype(dtype):
@@ -3029,7 +3073,7 @@ def _concat_compat(to_concat, axis=0):
     typs = get_dtype_kinds(to_concat)
 
     # these are mandated to handle empties as well
-    if 'datetime' in typs or 'timedelta' in typs:
+    if 'datetime' in typs or 'datetimetz' in typs or 'timedelta' in typs:
         from pandas.tseries.common import _concat_compat
         return _concat_compat(to_concat, axis=axis)
 
diff --git a/pandas/core/dtypes.py b/pandas/core/dtypes.py
new file mode 100644
index 000000000..b260c2b58
--- /dev/null
+++ b/pandas/core/dtypes.py
@@ -0,0 +1,196 @@
+""" define extension dtypes """
+
+import re
+import numpy as np
+from pandas import compat
+
+class ExtensionDtype(object):
+    """
+    A np.dtype duck-typed class, suitable for holding a custom dtype.
+
+    THIS IS NOT A REAL NUMPY DTYPE
+    """
+    name = None
+    names = None
+    type = None
+    subdtype = None
+    kind = None
+    str = None
+    num = 100
+    shape = tuple()
+    itemsize = 8
+    base = None
+    isbuiltin = 0
+    isnative = 0
+    _metadata = []
+
+    def __unicode__(self):
+        return self.name
+
+    def __str__(self):
+        """
+        Return a string representation for a particular Object
+
+        Invoked by str(df) in both py2/py3.
+        Yields Bytestring in Py2, Unicode String in py3.
+        """
+
+        if compat.PY3:
+            return self.__unicode__()
+        return self.__bytes__()
+
+    def __bytes__(self):
+        """
+        Return a string representation for a particular object.
+
+        Invoked by bytes(obj) in py3 only.
+        Yields a bytestring in both py2/py3.
+        """
+        from pandas.core.config import get_option
+
+        encoding = get_option("display.encoding")
+        return self.__unicode__().encode(encoding, 'replace')
+
+    def __repr__(self):
+        """
+        Return a string representation for a particular object.
+
+        Yields Bytestring in Py2, Unicode String in py3.
+        """
+        return str(self)
+
+    def __hash__(self):
+        raise NotImplementedError("sub-classes should implement an __hash__ method")
+
+    def __eq__(self, other):
+        raise NotImplementedError("sub-classes should implement an __eq__ method")
+
+    @classmethod
+    def is_dtype(cls, dtype):
+        """ Return a boolean if we if the passed type is an actual dtype that we can match (via string or type) """
+        if hasattr(dtype, 'dtype'):
+            dtype = dtype.dtype
+        if isinstance(dtype, cls):
+            return True
+        try:
+            return cls.construct_from_string(dtype) is not None
+        except:
+            return False
+
+class CategoricalDtypeType(type):
+    """
+    the type of CategoricalDtype, this metaclass determines subclass ability
+    """
+    pass
+
+class CategoricalDtype(ExtensionDtype):
+    __metaclass__ = CategoricalDtypeType
+    """
+    A np.dtype duck-typed class, suitable for holding a custom categorical dtype.
+
+    THIS IS NOT A REAL NUMPY DTYPE, but essentially a sub-class of np.object
+    """
+    name = 'category'
+    type = CategoricalDtypeType
+    kind = 'O'
+    str = '|O08'
+    base = np.dtype('O')
+
+    def __hash__(self):
+        # make myself hashable
+        return hash(str(self))
+
+    def __eq__(self, other):
+        if isinstance(other, compat.string_types):
+            return other == self.name
+
+        return isinstance(other, CategoricalDtype)
+
+    @classmethod
+    def construct_from_string(cls, string):
+        """ attempt to construct this type from a string, raise a TypeError if its not possible """
+        try:
+            if string == 'category':
+                return cls()
+        except:
+            pass
+
+        raise TypeError("cannot construct a CategoricalDtype")
+
+class DatetimeTZDtypeType(type):
+    """
+    the type of DatetimeTZDtype, this metaclass determines subclass ability
+    """
+    pass
+
+class DatetimeTZDtype(ExtensionDtype):
+    __metaclass__ = DatetimeTZDtypeType
+    """
+    A np.dtype duck-typed class, suitable for holding a custom datetime with tz dtype.
+
+    THIS IS NOT A REAL NUMPY DTYPE, but essentially a sub-class of np.datetime64[ns]
+    """
+    type = DatetimeTZDtypeType
+    kind = 'M'
+    str = '|M8[ns]'
+    num = 101
+    base = np.dtype('M8[ns]')
+    _metadata = ['unit','tz']
+    _match = re.compile("datetime64\[(?P<unit>.+), (?P<tz>.+)\]")
+
+    def __init__(self, unit, tz=None):
+        """
+        Parameters
+        ----------
+        unit : string unit that this represents, currently must be 'ns'
+        tz : string tz that this represents
+        """
+
+        if isinstance(unit, DatetimeTZDtype):
+            self.unit, self.tz = unit.unit, unit.tz
+            return
+
+        if tz is None:
+
+            # we were passed a string that we can construct
+            try:
+                m = self._match.search(unit)
+                if m is not None:
+                    self.unit = m.groupdict()['unit']
+                    self.tz = m.groupdict()['tz']
+                    return
+            except:
+                raise ValueError("could not construct DatetimeTZDtype")
+
+            raise ValueError("DatetimeTZDtype constructor must have a tz supplied")
+
+        if unit != 'ns':
+            raise ValueError("DatetimeTZDtype only supports ns units")
+        self.unit = unit
+        self.tz = tz
+
+    @classmethod
+    def construct_from_string(cls, string):
+        """ attempt to construct this type from a string, raise a TypeError if its not possible """
+        try:
+            return cls(unit=string)
+        except ValueError:
+            raise TypeError("could not construct DatetimeTZDtype")
+
+    def __unicode__(self):
+        # format the tz
+        return "datetime64[{unit}, {tz}]".format(unit=self.unit,tz=self.tz)
+
+    @property
+    def name(self):
+        return str(self)
+
+    def __hash__(self):
+        # make myself hashable
+        return hash(str(self))
+
+    def __eq__(self, other):
+        if isinstance(other, compat.string_types):
+            return other == self.name
+
+        return isinstance(other, DatetimeTZDtype) and self.unit == other.unit and self.tz == other.tz
diff --git a/pandas/core/format.py b/pandas/core/format.py
index 818391d6e..29f1e1efe 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -19,6 +19,7 @@ import pandas.lib as lib
 from pandas.tslib import iNaT, Timestamp, Timedelta, format_array_from_datetime
 from pandas.tseries.index import DatetimeIndex
 from pandas.tseries.period import PeriodIndex
+import pandas as pd
 import numpy as np
 
 import itertools
@@ -188,7 +189,7 @@ class SeriesFormatter(object):
         # level infos are added to the end and in a new line, like it is done for Categoricals
         # Only added when we request a name
         if name and com.is_categorical_dtype(self.tr_series.dtype):
-            level_info = self.tr_series.values._repr_categories_info()
+            level_info = self.tr_series._values._repr_categories_info()
             if footer:
                 footer += "\n"
             footer += level_info
@@ -208,7 +209,7 @@ class SeriesFormatter(object):
         return fmt_index, have_header
 
     def _get_formatted_values(self):
-        return format_array(self.tr_series.values, None,
+        return format_array(self.tr_series._values, None,
                             float_format=self.float_format,
                             na_rep=self.na_rep)
 
@@ -615,7 +616,7 @@ class DataFrameFormatter(TableFormatter):
                 strcols.insert(i, lev3)
 
         if column_format is None:
-            dtypes = self.frame.dtypes.values
+            dtypes = self.frame.dtypes._values
             column_format = ''.join(map(get_col_type, dtypes))
             if self.index:
                 index_format = 'l' * self.frame.index.nlevels
@@ -681,7 +682,7 @@ class DataFrameFormatter(TableFormatter):
         frame = self.tr_frame
         formatter = self._get_formatter(i)
         return format_array(
-            frame.iloc[:, i].values,
+            frame.iloc[:, i]._values,
             formatter, float_format=self.float_format, na_rep=self.na_rep,
             space=self.col_space
         )
@@ -720,7 +721,7 @@ class DataFrameFormatter(TableFormatter):
         if isinstance(columns, MultiIndex):
             fmt_columns = columns.format(sparsify=False, adjoin=False)
             fmt_columns = lzip(*fmt_columns)
-            dtypes = self.frame.dtypes.values
+            dtypes = self.frame.dtypes._values
 
             # if we have a Float level, they don't use leading space at all
             restrict_formatting = any([l.is_floating for l in columns.levels])
@@ -1401,7 +1402,7 @@ class CSVFormatter(object):
 
         series = {}
         for k, v in compat.iteritems(values._series):
-            series[k] = v.values
+            series[k] = v._values
 
         nlevels = getattr(data_index, 'nlevels', 1)
         for j, idx in enumerate(data_index):
@@ -1919,6 +1920,8 @@ def format_array(values, formatter, float_format=None, na_rep='NaN',
         fmt_klass = PeriodArrayFormatter
     elif com.is_integer_dtype(values.dtype):
         fmt_klass = IntArrayFormatter
+    elif com.is_datetimetz(values):
+        fmt_klass = Datetime64TZFormatter
     elif com.is_datetime64_dtype(values.dtype):
         fmt_klass = Datetime64Formatter
     elif com.is_timedelta64_dtype(values.dtype):
@@ -1975,6 +1978,8 @@ class GenericArrayFormatter(object):
             if self.na_rep is not None and lib.checknull(x):
                 if x is None:
                     return 'None'
+                elif x is pd.NaT:
+                    return 'NaT'
                 return self.na_rep
             elif isinstance(x, PandasObject):
                 return '%s' % x
@@ -1984,7 +1989,7 @@ class GenericArrayFormatter(object):
 
         vals = self.values
         if isinstance(vals, Index):
-            vals = vals.values
+            vals = vals._values
 
         is_float = lib.map_infer(vals, com.is_float) & notnull(vals)
         leading_space = is_float.any()
@@ -2067,9 +2072,7 @@ class IntArrayFormatter(GenericArrayFormatter):
 
     def _format_strings(self):
         formatter = self.formatter or (lambda x: '% d' % x)
-
         fmt_values = [formatter(x) for x in self.values]
-
         return fmt_values
 
 
@@ -2080,27 +2083,16 @@ class Datetime64Formatter(GenericArrayFormatter):
         self.date_format = date_format
 
     def _format_strings(self):
+        """ we by definition have DO NOT have a TZ """
 
-        # we may have a tz, if so, then need to process element-by-element
-        # when DatetimeBlockWithTimezones is a reality this could be fixed
         values = self.values
         if not isinstance(values, DatetimeIndex):
             values = DatetimeIndex(values)
 
-        if values.tz is None:
-            fmt_values = format_array_from_datetime(values.asi8.ravel(),
-                                                    format=_get_format_datetime64_from_values(values, self.date_format),
-                                                    na_rep=self.nat_rep).reshape(values.shape)
-            fmt_values = fmt_values.tolist()
-
-        else:
-
-            values = values.asobject
-            is_dates_only = _is_dates_only(values)
-            formatter = (self.formatter or _get_format_datetime64(is_dates_only, values, date_format=self.date_format))
-            fmt_values = [ formatter(x) for x in values ]
-
-        return fmt_values
+        fmt_values = format_array_from_datetime(values.asi8.ravel(),
+                                                format=_get_format_datetime64_from_values(values, self.date_format),
+                                                na_rep=self.nat_rep).reshape(values.shape)
+        return fmt_values.tolist()
 
 
 class PeriodArrayFormatter(IntArrayFormatter):
@@ -2179,6 +2171,18 @@ def _get_format_datetime64_from_values(values, date_format):
     return date_format
 
 
+class Datetime64TZFormatter(Datetime64Formatter):
+
+    def _format_strings(self):
+        """ we by definition have a TZ """
+
+        values = self.values.asobject
+        is_dates_only = _is_dates_only(values)
+        formatter = (self.formatter or _get_format_datetime64(is_dates_only, date_format=self.date_format))
+        fmt_values = [ formatter(x) for x in values ]
+
+        return fmt_values
+
 class Timedelta64Formatter(GenericArrayFormatter):
 
     def __init__(self, values, nat_rep='NaT', box=False, **kwargs):
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index c5c0f9e82..cb237b93c 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -28,6 +28,7 @@ from pandas.core.common import (isnull, notnull, PandasError, _try_sort, _not_no
                                 _infer_dtype_from_scalar, _values_from_object,
                                 is_list_like, _maybe_box_datetimelike,
                                 is_categorical_dtype, is_object_dtype,
+                                is_internal_type, is_datetimetz,
                                 _possibly_infer_to_datetimelike, _dict_compat)
 from pandas.core.generic import NDFrame, _shared_docs
 from pandas.core.index import Index, MultiIndex, _ensure_index
@@ -116,14 +117,14 @@ suffixes : 2-length sequence (tuple, list, ...)
 copy : boolean, default True
     If False, do not copy data unnecessarily
 indicator : boolean or string, default False
-    If True, adds a column to output DataFrame called "_merge" with 
-    information on the source of each row. 
-    If string, column with information on source of each row will be added to 
-    output DataFrame, and column will be named value of string. 
-    Information column is Categorical-type and takes on a value of "left_only" 
-    for observations whose merge key only appears in 'left' DataFrame, 
-    "right_only" for observations whose merge key only appears in 'right' 
-    DataFrame, and "both" if the observation's merge key is found in both. 
+    If True, adds a column to output DataFrame called "_merge" with
+    information on the source of each row.
+    If string, column with information on source of each row will be added to
+    output DataFrame, and column will be named value of string.
+    Information column is Categorical-type and takes on a value of "left_only"
+    for observations whose merge key only appears in 'left' DataFrame,
+    "right_only" for observations whose merge key only appears in 'right'
+    DataFrame, and "both" if the observation's merge key is found in both.
 
     .. versionadded:: 0.17.0
 
@@ -402,6 +403,9 @@ class DataFrame(NDFrame):
             index, columns = _get_axes(len(values),1)
             return _arrays_to_mgr([ values ], columns, index, columns,
                                   dtype=dtype)
+        elif is_datetimetz(values):
+            return self._init_dict({ 0 : values }, index, columns,
+                                   dtype=dtype)
 
         # by definition an array here
         # the dtypes will be coerced to a single dtype
@@ -871,6 +875,7 @@ class DataFrame(NDFrame):
         -------
         df : DataFrame
         """
+
         # Make a copy of the input columns so we can modify it
         if columns is not None:
             columns = _ensure_index(columns)
@@ -1749,7 +1754,7 @@ class DataFrame(NDFrame):
 
         if takeable:
             series = self._iget_item_cache(col)
-            return _maybe_box_datetimelike(series.values[index])
+            return _maybe_box_datetimelike(series._values[index])
 
         series = self._get_item_cache(col)
         engine = self.index._engine
@@ -1779,7 +1784,7 @@ class DataFrame(NDFrame):
 
             series = self._get_item_cache(col)
             engine = self.index._engine
-            engine.set_value(series.values, index, value)
+            engine.set_value(series._values, index, value)
             return self
         except (KeyError, TypeError):
 
@@ -1831,6 +1836,8 @@ class DataFrame(NDFrame):
                     copy=True
                 else:
                     new_values = self._data.fast_xs(i)
+                    if lib.isscalar(new_values):
+                        return new_values
 
                     # if we are a copy, mark as such
                     copy = isinstance(new_values,np.ndarray) and new_values.base is None
@@ -2423,7 +2430,7 @@ class DataFrame(NDFrame):
             # reindex if necessary
 
             if value.index.equals(self.index) or not len(self.index):
-                value = value.values.copy()
+                value = value._values.copy()
             else:
 
                 # GH 4107
@@ -2475,7 +2482,7 @@ class DataFrame(NDFrame):
 
             # possibly infer to datetimelike
             if is_object_dtype(value.dtype):
-                value = _possibly_infer_to_datetimelike(value.ravel()).reshape(value.shape)
+                value = _possibly_infer_to_datetimelike(value)
 
         else:
             # upcast the scalar
@@ -2483,8 +2490,8 @@ class DataFrame(NDFrame):
             value = np.repeat(value, len(self.index)).astype(dtype)
             value = com._possibly_cast_to_datetime(value, dtype)
 
-        # return unconsolidatables directly
-        if isinstance(value, (Categorical, SparseArray)):
+        # return internal types directly
+        if is_internal_type(value):
             return value
 
         # broadcast across multiple columns if necessary
@@ -2718,7 +2725,7 @@ class DataFrame(NDFrame):
                 level = col
                 names.append(None)
             else:
-                level = frame[col].values
+                level = frame[col]._values
                 names.append(col)
                 if drop:
                     to_remove.append(col)
@@ -2782,7 +2789,7 @@ class DataFrame(NDFrame):
                 values = index.asobject.values
             elif (isinstance(index, DatetimeIndex) and
                   index.tz is not None):
-                values = index.asobject
+                values = index
             else:
                 values = index.values
                 if values.dtype == np.object_:
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index d3a63f9f5..1586bb5df 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -2417,6 +2417,11 @@ class NDFrame(PandasObject):
         """
         return self.as_matrix()
 
+    @property
+    def _values(self):
+        """ internal implementation """
+        return self.values
+
     @property
     def _get_values(self):
         # compat
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 7a5770d39..ce7aaec26 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -2193,9 +2193,9 @@ def _convert_grouper(axis, grouper):
         return grouper.get
     elif isinstance(grouper, Series):
         if grouper.index.equals(axis):
-            return grouper.values
+            return grouper._values
         else:
-            return grouper.reindex(axis).values
+            return grouper.reindex(axis)._values
     elif isinstance(grouper, (list, Series, Index, np.ndarray)):
         if len(grouper) != len(axis):
             raise AssertionError('Grouper and axis must be same length')
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 14ba2dea0..ef1674894 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -19,6 +19,7 @@ from pandas.util.decorators import (Appender, Substitution, cache_readonly,
                                     deprecate, deprecate_kwarg)
 import pandas.core.common as com
 from pandas.core.common import (isnull, array_equivalent, is_dtype_equal, is_object_dtype,
+                                is_datetimetz,
                                 _values_from_object, is_float, is_integer, is_iterator, is_categorical_dtype,
                                 ABCSeries, ABCCategorical, _ensure_object, _ensure_int64, is_bool_indexer,
                                 is_list_like, is_bool_dtype, is_null_slice, is_integer_dtype)
@@ -115,7 +116,7 @@ class Index(IndexOpsMixin, PandasObject):
 
         from pandas.tseries.period import PeriodIndex
         if isinstance(data, (np.ndarray, Index, ABCSeries)):
-            if issubclass(data.dtype.type, np.datetime64):
+            if issubclass(data.dtype.type, np.datetime64) or is_datetimetz(data):
                 from pandas.tseries.index import DatetimeIndex
                 result = DatetimeIndex(data, copy=copy, name=name, **kwargs)
                 if dtype is not None and _o_dtype == dtype:
@@ -207,7 +208,7 @@ class Index(IndexOpsMixin, PandasObject):
         return cls._simple_new(subarr, name)
 
     @classmethod
-    def _simple_new(cls, values, name=None, **kwargs):
+    def _simple_new(cls, values, name=None, dtype=None, **kwargs):
         """
         we require the we have a dtype compat for the values
         if we are passed a non-dtype compat, then coerce using the constructor
@@ -215,9 +216,12 @@ class Index(IndexOpsMixin, PandasObject):
         Must be careful not to recurse.
         """
         if not hasattr(values, 'dtype'):
-            values = np.array(values,copy=False)
-            if is_object_dtype(values):
-                values = cls(values, name=name, **kwargs).values
+            if values is None and dtype is not None:
+                values = np.empty(0, dtype=dtype)
+            else:
+                values = np.array(values,copy=False)
+                if is_object_dtype(values):
+                    values = cls(values, name=name, dtype=dtype, **kwargs)._values
 
         result = object.__new__(cls)
         result._data = values
@@ -305,7 +309,7 @@ class Index(IndexOpsMixin, PandasObject):
         --------
         numpy.ndarray.repeat
         """
-        return self._shallow_copy(self.values.repeat(n))
+        return self._shallow_copy(self._values.repeat(n))
 
     def ravel(self, order='C'):
         """
@@ -315,7 +319,7 @@ class Index(IndexOpsMixin, PandasObject):
         --------
         numpy.ndarray.ravel
         """
-        return self.values.ravel(order=order)
+        return self._values.ravel(order=order)
 
     # construction helpers
     @classmethod
@@ -606,7 +610,7 @@ class Index(IndexOpsMixin, PandasObject):
         return an array repr of this object, potentially casting to object
 
         """
-        return self.values
+        return self.values.copy()
 
     def astype(self, dtype):
         return Index(self.values.astype(dtype), name=self.name,
@@ -1164,7 +1168,7 @@ class Index(IndexOpsMixin, PandasObject):
                 break
 
         to_concat = self._ensure_compat_concat(to_concat)
-        to_concat = [x.values if isinstance(x, Index) else x
+        to_concat = [x._values if isinstance(x, Index) else x
                      for x in to_concat]
         return to_concat, name
 
@@ -1197,10 +1201,15 @@ class Index(IndexOpsMixin, PandasObject):
 
         return indexes
 
-    def take(self, indices, axis=0):
+    def take(self, indices, axis=0, allow_fill=True, fill_value=None):
         """
         return a new Index of the values selected by the indexer
 
+        For internal compatibility with numpy arrays.
+
+        # filling must always be None/nan here
+        # but is passed thru internally
+
         See also
         --------
         numpy.ndarray.take
@@ -1470,20 +1479,20 @@ class Index(IndexOpsMixin, PandasObject):
 
         if self.is_monotonic and other.is_monotonic:
             try:
-                result = self._outer_indexer(self.values, other.values)[0]
+                result = self._outer_indexer(self.values, other._values)[0]
             except TypeError:
                 # incomparable objects
                 result = list(self.values)
 
                 # worth making this faster? a very unusual case
                 value_set = set(self.values)
-                result.extend([x for x in other.values if x not in value_set])
+                result.extend([x for x in other._values if x not in value_set])
         else:
             indexer = self.get_indexer(other)
             indexer, = (indexer == -1).nonzero()
 
             if len(indexer) > 0:
-                other_diff = com.take_nd(other.values, indexer,
+                other_diff = com.take_nd(other._values, indexer,
                                          allow_fill=False)
                 result = com._concat_compat((self.values, other_diff))
 
@@ -1544,17 +1553,17 @@ class Index(IndexOpsMixin, PandasObject):
 
         if self.is_monotonic and other.is_monotonic:
             try:
-                result = self._inner_indexer(self.values, other.values)[0]
+                result = self._inner_indexer(self.values, other._values)[0]
                 return self._wrap_union_result(other, result)
             except TypeError:
                 pass
 
         try:
-            indexer = self.get_indexer(other.values)
+            indexer = Index(self.values).get_indexer(other._values)
             indexer = indexer.take((indexer != -1).nonzero()[0])
         except:
             # duplicates
-            indexer = self.get_indexer_non_unique(other.values)[0].unique()
+            indexer = Index(self.values).get_indexer_non_unique(other._values)[0].unique()
             indexer = indexer[indexer != -1]
 
         taken = self.take(indexer)
@@ -1683,6 +1692,13 @@ class Index(IndexOpsMixin, PandasObject):
         Fast lookup of value from 1-dimensional ndarray. Only use this if you
         know what you're doing
         """
+
+        # if we have something that is Index-like, then
+        # use this, e.g. DatetimeIndex
+        s = getattr(series,'_values',None)
+        if isinstance(s, Index) and lib.isscalar(key):
+            return s[key]
+
         s = _values_from_object(series)
         k = _values_from_object(key)
 
@@ -1808,7 +1824,7 @@ class Index(IndexOpsMixin, PandasObject):
                 raise ValueError('limit argument only valid if doing pad, '
                                  'backfill or nearest reindexing')
 
-            indexer = self._engine.get_indexer(target.values)
+            indexer = self._engine.get_indexer(target._values)
 
         return com._ensure_platform_int(indexer)
 
@@ -1820,12 +1836,12 @@ class Index(IndexOpsMixin, PandasObject):
         if self.is_monotonic_increasing and target.is_monotonic_increasing:
             method = (self._engine.get_pad_indexer if method == 'pad'
                       else self._engine.get_backfill_indexer)
-            indexer = method(target.values, limit)
+            indexer = method(target._values, limit)
         else:
             indexer = self._get_fill_indexer_searchsorted(target, method, limit)
         if tolerance is not None:
             indexer = self._filter_indexer_tolerance(
-                target.values, indexer, tolerance)
+                target._values, indexer, tolerance)
         return indexer
 
     def _get_fill_indexer_searchsorted(self, target, method, limit=None):
@@ -1899,7 +1915,7 @@ class Index(IndexOpsMixin, PandasObject):
             self = Index(self.asi8)
             tgt_values = target.asi8
         else:
-            tgt_values = target.values
+            tgt_values = target._values
 
         indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
         return Index(indexer), missing
@@ -2016,7 +2032,7 @@ class Index(IndexOpsMixin, PandasObject):
         if not isinstance(target, Index) and len(target) == 0:
             attrs = self._get_attributes_dict()
             attrs.pop('freq', None)  # don't preserve freq
-            target = self._simple_new(np.empty(0, dtype=self.dtype), **attrs)
+            target = self._simple_new(None, dtype=self.dtype, **attrs)
         else:
             target = _ensure_index(target)
 
@@ -2077,7 +2093,7 @@ class Index(IndexOpsMixin, PandasObject):
             missing = com._ensure_platform_int(missing)
             missing_labels = target.take(missing)
             missing_indexer = com._ensure_int64(l[~check])
-            cur_labels = self.take(indexer[check]).values
+            cur_labels = self.take(indexer[check])._values
             cur_indexer = com._ensure_int64(l[check])
 
             new_labels = np.empty(tuple([len(indexer)]), dtype=object)
@@ -2097,7 +2113,7 @@ class Index(IndexOpsMixin, PandasObject):
             else:
 
                 # need to retake to have the same size as the indexer
-                indexer = indexer.values
+                indexer = indexer._values
                 indexer[~check] = 0
 
                 # reset the new indexer to account for the new size
@@ -2258,7 +2274,7 @@ class Index(IndexOpsMixin, PandasObject):
     def _join_non_unique(self, other, how='left', return_indexers=False):
         from pandas.tools.merge import _get_join_indexers
 
-        left_idx, right_idx = _get_join_indexers([self.values], [other.values],
+        left_idx, right_idx = _get_join_indexers([self.values], [other._values],
                                                  how=how, sort=True)
 
         left_idx = com._ensure_platform_int(left_idx)
@@ -2266,7 +2282,7 @@ class Index(IndexOpsMixin, PandasObject):
 
         join_index = self.values.take(left_idx)
         mask = left_idx == -1
-        np.putmask(join_index, mask, other.values.take(right_idx))
+        np.putmask(join_index, mask, other._values.take(right_idx))
 
         join_index = self._wrap_joined_index(join_index, other)
 
@@ -2412,7 +2428,7 @@ class Index(IndexOpsMixin, PandasObject):
                 return ret_index
 
         sv = self.values
-        ov = other.values
+        ov = other._values
 
         if self.is_unique and other.is_unique:
             # We can perform much better than the general case
@@ -2679,7 +2695,7 @@ class Index(IndexOpsMixin, PandasObject):
         new_index : Index
         """
         _self = np.asarray(self)
-        item = self._coerce_scalar_to_index(item).values
+        item = self._coerce_scalar_to_index(item)._values
 
         idx = np.concatenate(
             (_self[:loc], item, _self[loc:]))
@@ -3056,7 +3072,7 @@ class CategoricalIndex(Index, PandasDelegate):
 
         if is_categorical_dtype(other):
             if isinstance(other, CategoricalIndex):
-                other = other.values
+                other = other._values
             if not other.is_dtype_equal(self):
                 raise TypeError("categories must match existing categories when appending")
         else:
@@ -3319,9 +3335,13 @@ class CategoricalIndex(Index, PandasDelegate):
 
         return None
 
-    def take(self, indexer, axis=0):
+    def take(self, indexer, axis=0, allow_fill=True, fill_value=None):
         """
-        return a new CategoricalIndex of the values selected by the indexer
+        For internal compatibility with numpy arrays.
+
+        # filling must always be None/nan here
+        # but is passed thru internally
+        assert isnull(fill_value)
 
         See also
         --------
@@ -3401,9 +3421,9 @@ class CategoricalIndex(Index, PandasDelegate):
 
                 # if we have a Categorical type, then must have the same categories
                 if isinstance(other, CategoricalIndex):
-                    other = other.values
+                    other = other._values
                 elif isinstance(other, Index):
-                    other = self._create_categorical(self, other.values, categories=self.categories, ordered=self.ordered)
+                    other = self._create_categorical(self, other._values, categories=self.categories, ordered=self.ordered)
 
                 if isinstance(other, (ABCCategorical, np.ndarray, ABCSeries)):
                     if len(self.values) != len(other):
@@ -3426,8 +3446,8 @@ class CategoricalIndex(Index, PandasDelegate):
 
 
     def _delegate_method(self, name, *args, **kwargs):
-        """ method delegation to the .values """
-        method = getattr(self.values, name)
+        """ method delegation to the ._values """
+        method = getattr(self._values, name)
         if 'inplace' in kwargs:
             raise ValueError("cannot use inplace with CategoricalIndex")
         res = method(*args, **kwargs)
@@ -3680,7 +3700,7 @@ class Float64Index(NumericIndex):
             raise TypeError('Setting %s dtype to anything other than '
                             'float64 or object is not supported' %
                             self.__class__)
-        return Index(self.values, name=self.name, dtype=dtype)
+        return Index(self._values, name=self.name, dtype=dtype)
 
     def _convert_scalar_indexer(self, key, kind=None):
         if kind == 'iloc':
@@ -3743,7 +3763,7 @@ class Float64Index(NumericIndex):
                 other = self._constructor(other)
             if not is_dtype_equal(self.dtype,other.dtype) or self.shape != other.shape:
                 return False
-            left, right = self.values, other.values
+            left, right = self._values, other._values
             return ((left == right) | (self._isnan & other._isnan)).all()
         except TypeError:
             # e.g. fails in numpy 1.6 with DatetimeIndex #1681
@@ -4293,12 +4313,12 @@ class MultiIndex(Index):
             box = hasattr(lev, '_box_values')
             # Try to minimize boxing.
             if box and len(lev) > len(lab):
-                taken = lev._box_values(com.take_1d(lev.values, lab))
+                taken = lev._box_values(com.take_1d(lev._values, lab))
             elif box:
-                taken = com.take_1d(lev._box_values(lev.values), lab,
+                taken = com.take_1d(lev._box_values(lev._values), lab,
                                     fill_value=_get_na_value(lev.dtype.type))
             else:
-                taken = com.take_1d(np.asarray(lev.values), lab)
+                taken = com.take_1d(np.asarray(lev._values), lab)
             values.append(taken)
 
         self._tuples = lib.fast_zip(values)
@@ -4345,7 +4365,7 @@ class MultiIndex(Index):
         def _try_mi(k):
             # TODO: what if a level contains tuples??
             loc = self.get_loc(k)
-            new_values = series.values[loc]
+            new_values = series._values[loc]
             new_index = self[loc]
             new_index = maybe_droplevels(new_index, k)
             return Series(new_values, index=new_index, name=series.name)
@@ -4408,7 +4428,7 @@ class MultiIndex(Index):
         num = self._get_level_number(level)
         unique = self.levels[num]  # .values
         labels = self.labels[num]
-        filled = com.take_1d(unique.values, labels, fill_value=unique._na_value)
+        filled = com.take_1d(unique._values, labels, fill_value=unique._na_value)
         values = unique._simple_new(filled, self.names[num],
                                     freq=getattr(unique, 'freq', None),
                                     tz=getattr(unique, 'tz', None))
@@ -4438,7 +4458,7 @@ class MultiIndex(Index):
                 # weird all NA case
                 formatted = [com.pprint_thing(na if isnull(x) else x,
                                               escape_chars=('\t', '\r', '\n'))
-                             for x in com.take_1d(lev.values, lab)]
+                             for x in com.take_1d(lev._values, lab)]
             stringified_levels.append(formatted)
 
         result_levels = []
@@ -4622,7 +4642,7 @@ class MultiIndex(Index):
 
         if isinstance(tuples, (np.ndarray, Index)):
             if isinstance(tuples, Index):
-                tuples = tuples.values
+                tuples = tuples._values
 
             arrays = list(lib.tuples_to_object_array(tuples).T)
         elif isinstance(tuples, list):
@@ -4777,7 +4797,7 @@ class MultiIndex(Index):
                 arrays.append(label.append(appended))
             return MultiIndex.from_arrays(arrays, names=self.names)
 
-        to_concat = (self.values,) + tuple(k.values for k in other)
+        to_concat = (self.values,) + tuple(k._values for k in other)
         new_tuples = np.concatenate(to_concat)
 
         # if all(isinstance(x, MultiIndex) for x in other):
@@ -5065,7 +5085,7 @@ class MultiIndex(Index):
             raise NotImplementedError("method='nearest' not implemented yet "
                                       'for MultiIndex; see GitHub issue 9365')
         else:
-            indexer = self_index._engine.get_indexer(target.values)
+            indexer = self_index._engine.get_indexer(target._values)
 
         return com._ensure_platform_int(indexer)
 
@@ -5140,7 +5160,7 @@ class MultiIndex(Index):
         -------
         index : Index
         """
-        return Index(self.values)
+        return Index(self._values)
 
     def get_slice_bound(self, label, side, kind):
         if not isinstance(label, tuple):
@@ -5450,7 +5470,7 @@ class MultiIndex(Index):
                 mapper = Series(indexer)
                 indexer = labels.take(com._ensure_platform_int(indexer))
                 result = Series(Index(indexer).isin(r).nonzero()[0])
-                m = result.map(mapper).values
+                m = result.map(mapper)._values
 
             else:
                 m = np.zeros(len(labels),dtype=bool)
@@ -5576,7 +5596,7 @@ class MultiIndex(Index):
                 else:
 
                     # no matches we are done
-                    return Int64Index([]).values
+                    return Int64Index([])._values
 
             elif is_null_slice(k):
                 # empty slice
@@ -5592,8 +5612,8 @@ class MultiIndex(Index):
 
         # empty indexer
         if indexer is None:
-            return Int64Index([]).values
-        return indexer.values
+            return Int64Index([])._values
+        return indexer._values
 
     def truncate(self, before=None, after=None):
         """
@@ -5638,7 +5658,7 @@ class MultiIndex(Index):
             return True
 
         if not isinstance(other, MultiIndex):
-            return array_equivalent(self.values,
+            return array_equivalent(self._values,
                                     _values_from_object(_ensure_index(other)))
 
         if self.nlevels != other.nlevels:
@@ -5648,9 +5668,9 @@ class MultiIndex(Index):
             return False
 
         for i in range(self.nlevels):
-            svalues = com.take_nd(np.asarray(self.levels[i].values), self.labels[i],
+            svalues = com.take_nd(np.asarray(self.levels[i]._values), self.labels[i],
                                   allow_fill=False)
-            ovalues = com.take_nd(np.asarray(other.levels[i].values), other.labels[i],
+            ovalues = com.take_nd(np.asarray(other.levels[i]._values), other.labels[i],
                                   allow_fill=False)
             if not array_equivalent(svalues, ovalues):
                 return False
@@ -5690,7 +5710,7 @@ class MultiIndex(Index):
         if len(other) == 0 or self.equals(other):
             return self
 
-        uniq_tuples = lib.fast_unique_multiple([self.values, other.values])
+        uniq_tuples = lib.fast_unique_multiple([self._values, other._values])
         return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,
                                       names=result_names)
 
@@ -5712,8 +5732,8 @@ class MultiIndex(Index):
         if self.equals(other):
             return self
 
-        self_tuples = self.values
-        other_tuples = other.values
+        self_tuples = self._values
+        other_tuples = other._values
         uniq_tuples = sorted(set(self_tuples) & set(other_tuples))
         if len(uniq_tuples) == 0:
             return MultiIndex(levels=[[]] * self.nlevels,
@@ -5742,7 +5762,7 @@ class MultiIndex(Index):
                               labels=[[]] * self.nlevels,
                               names=result_names, verify_integrity=False)
 
-        difference = sorted(set(self.values) - set(other.values))
+        difference = sorted(set(self._values) - set(other._values))
 
         if len(difference) == 0:
             return MultiIndex(levels=[[]] * self.nlevels,
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index b8ee831cd..7cf194204 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -294,9 +294,9 @@ class _NDFrameIndexer(object):
                     new_index = index.insert(len(index),indexer)
 
                     # this preserves dtype of the value
-                    new_values = Series([value]).values
-                    if len(self.obj.values):
-                        new_values = np.concatenate([self.obj.values,
+                    new_values = Series([value])._values
+                    if len(self.obj._values):
+                        new_values = np.concatenate([self.obj._values,
                                                      new_values])
 
                     self.obj._data = self.obj._constructor(
@@ -548,7 +548,7 @@ class _NDFrameIndexer(object):
             # series, so need to broadcast (see GH5206)
             if (sum_aligners == self.ndim and
                     all([com.is_sequence(_) for _ in indexer])):
-                ser = ser.reindex(obj.axes[0][indexer[0]], copy=True).values
+                ser = ser.reindex(obj.axes[0][indexer[0]], copy=True)._values
 
                 # single indexer
                 if len(indexer) > 1:
@@ -570,9 +570,9 @@ class _NDFrameIndexer(object):
                     else:
                         new_ix = Index(new_ix)
                     if ser.index.equals(new_ix) or not len(new_ix):
-                        return ser.values.copy()
+                        return ser._values.copy()
 
-                    return ser.reindex(new_ix).values
+                    return ser.reindex(new_ix)._values
 
                 # 2 dims
                 elif single_aligner and is_frame:
@@ -580,8 +580,8 @@ class _NDFrameIndexer(object):
                     # reindex along index
                     ax = self.obj.axes[1]
                     if ser.index.equals(ax) or not len(ax):
-                        return ser.values.copy()
-                    return ser.reindex(ax).values
+                        return ser._values.copy()
+                    return ser.reindex(ax)._values
 
                 # >2 dims
                 elif single_aligner:
@@ -596,7 +596,7 @@ class _NDFrameIndexer(object):
                             broadcast.append((n, len(labels)))
 
                     # broadcast along other dims
-                    ser = ser.values.copy()
+                    ser = ser._values.copy()
                     for (axis, l) in broadcast:
                         shape = [-1] * (len(broadcast) + 1)
                         shape[axis] = l
@@ -611,9 +611,9 @@ class _NDFrameIndexer(object):
             ax = self.obj._get_axis(1)
 
             if ser.index.equals(ax):
-                return ser.values.copy()
+                return ser._values.copy()
 
-            return ser.reindex(ax).values
+            return ser.reindex(ax)._values
 
         raise ValueError('Incompatible indexer with Series')
 
@@ -659,16 +659,16 @@ class _NDFrameIndexer(object):
             if idx is not None and cols is not None:
 
                 if df.index.equals(idx) and df.columns.equals(cols):
-                    val = df.copy().values
+                    val = df.copy()._values
                 else:
-                    val = df.reindex(idx, columns=cols).values
+                    val = df.reindex(idx, columns=cols)._values
                 return val
 
         elif ((isinstance(indexer, slice) or is_list_like_indexer(indexer))
               and is_frame):
             ax = self.obj.index[indexer]
             if df.index.equals(ax):
-                val = df.copy().values
+                val = df.copy()._values
             else:
 
                 # we have a multi-index and are trying to align
@@ -677,7 +677,7 @@ class _NDFrameIndexer(object):
                     df.index, MultiIndex) and ax.nlevels != df.index.nlevels:
                     raise TypeError("cannot align on a multi-index with out specifying the join levels")
 
-                val = df.reindex(index=ax).values
+                val = df.reindex(index=ax)._values
             return val
 
         elif np.isscalar(indexer) and is_panel:
@@ -688,9 +688,9 @@ class _NDFrameIndexer(object):
             # a passed in dataframe which is actually a transpose
             # of what is needed
             if idx.equals(df.index) and cols.equals(df.columns):
-                return df.copy().values
+                return df.copy()._values
 
-            return df.reindex(idx, columns=cols).values
+            return df.reindex(idx, columns=cols)._values
 
         raise ValueError('Incompatible indexer with DataFrame')
 
@@ -1660,11 +1660,11 @@ def check_bool_indexer(ax, key):
     result = key
     if isinstance(key, ABCSeries) and not key.index.equals(ax):
         result = result.reindex(ax)
-        mask = com.isnull(result.values)
+        mask = com.isnull(result._values)
         if mask.any():
             raise IndexingError('Unalignable boolean Series key provided')
 
-        result = result.astype(bool).values
+        result = result.astype(bool)._values
 
     else:
         # is_bool_indexer has already checked for nulls in the case of an
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 5366c5a6b..58ee36142 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -11,13 +11,18 @@ from pandas.core.base import PandasObject
 from pandas.core.common import (_possibly_downcast_to_dtype, isnull,
                                 _NS_DTYPE, _TD_DTYPE, ABCSeries, is_list_like,
                                 ABCSparseSeries, _infer_dtype_from_scalar,
+                                is_null_slice, is_dtype_equal,
                                 is_null_datelike_scalar, _maybe_promote,
                                 is_timedelta64_dtype, is_datetime64_dtype,
+                                is_datetime64tz_dtype, is_datetimetz, is_sparse,
                                 array_equivalent, _maybe_convert_string_to_object,
-                                is_categorical, needs_i8_conversion, is_datetimelike_v_numeric)
+                                is_categorical, needs_i8_conversion, is_datetimelike_v_numeric,
+                                is_internal_type)
+
 from pandas.core.index import Index, MultiIndex, _ensure_index
 from pandas.core.indexing import maybe_convert_indices, length_of_indexer
 from pandas.core.categorical import Categorical, maybe_to_categorical
+from pandas.tseries.index import DatetimeIndex
 import pandas.core.common as com
 from pandas.sparse.array import _maybe_to_sparse, SparseArray
 import pandas.lib as lib
@@ -48,11 +53,13 @@ class Block(PandasObject):
     is_integer = False
     is_complex = False
     is_datetime = False
+    is_datetimetz = False
     is_timedelta = False
     is_bool = False
     is_object = False
     is_categorical = False
     is_sparse = False
+    _box_to_block_values = True
     _can_hold_na = False
     _downcast_dtype = None
     _can_consolidate = True
@@ -109,6 +116,22 @@ class Block(PandasObject):
 
         return False
 
+    def external_values(self, dtype=None):
+        """ return an outside world format, currently just the ndarray """
+        return self.values
+
+    def internal_values(self, dtype=None):
+        """ return an internal format, currently just the ndarray
+        this should be the pure internal API format """
+        return self.values
+
+    def get_values(self, dtype=None):
+        """
+        return an internal format, currently just the ndarray
+        this is often overriden to handle to_dense like operations
+        """
+        return self.values
+
     def to_dense(self):
         return self.values.view()
 
@@ -125,6 +148,18 @@ class Block(PandasObject):
         """ the dtype to return if I want to construct this block as an array """
         return self.dtype
 
+    def make_block(self, values, placement=None, ndim=None, **kwargs):
+        """
+        Create a new block, with type inference
+        propogate any values that are not specified
+        """
+        if placement is None:
+            placement = self.mgr_locs
+        if ndim is None:
+            ndim = self.ndim
+
+        return make_block(values, placement=placement, ndim=ndim, **kwargs)
+
     def make_block_same_class(self, values, placement, copy=False, fastpath=True,
                               **kwargs):
         """
@@ -248,9 +283,8 @@ class Block(PandasObject):
 
         new_values = com.take_nd(self.values, indexer, axis,
                                  fill_value=fill_value, mask_info=mask_info)
-        return make_block(new_values,
-                          ndim=self.ndim, fastpath=True,
-                          placement=self.mgr_locs)
+        return self.make_block(new_values,
+                               fastpath=True)
 
     def get(self, item):
         loc = self.items.get_loc(item)
@@ -280,7 +314,7 @@ class Block(PandasObject):
         """ apply the function to my values; return a block if we are not one """
         result = func(self.values, **kwargs)
         if not isinstance(result, Block):
-            result = make_block(values=_block_shape(result), placement=self.mgr_locs,)
+            result = self.make_block(values=_block_shape(result))
 
         return result
 
@@ -334,8 +368,8 @@ class Block(PandasObject):
                 dtypes = 'infer'
 
             nv = _possibly_downcast_to_dtype(values, dtypes)
-            return [make_block(nv, ndim=self.ndim,
-                               fastpath=True, placement=self.mgr_locs)]
+            return [self.make_block(nv,
+                                    fastpath=True)]
 
         # ndim > 1
         if dtypes is None:
@@ -362,9 +396,9 @@ class Block(PandasObject):
                 nv = _possibly_downcast_to_dtype(values[i], dtype)
                 nv = _block_shape(nv, ndim=self.ndim)
 
-            blocks.append(make_block(nv,
-                                     ndim=self.ndim, fastpath=True,
-                                     placement=[rl]))
+            blocks.append(self.make_block(nv,
+                                          fastpath=True,
+                                          placement=[rl]))
 
         return blocks
 
@@ -382,9 +416,7 @@ class Block(PandasObject):
         # may need to convert to categorical
         # this is only called for non-categoricals
         if self.is_categorical_astype(dtype):
-            return make_block(Categorical(self.values, **kwargs),
-                              ndim=self.ndim,
-                              placement=self.mgr_locs)
+            return self.make_block(Categorical(self.values, **kwargs))
 
         # astype processing
         dtype = np.dtype(dtype)
@@ -399,12 +431,20 @@ class Block(PandasObject):
         try:
             # force the copy here
             if values is None:
+
+                if issubclass(dtype.type, (compat.text_type, compat.string_types)):
+                    values = self.to_native_types()
+                else:
+                    values = self.get_values(dtype=dtype)
+
                 # _astype_nansafe works fine with 1-d only
-                values = com._astype_nansafe(self.values.ravel(), dtype, copy=True)
-                values = values.reshape(self.values.shape)
+                values = com._astype_nansafe(values.ravel(), dtype, copy=True)
+                values = values.reshape(self.shape)
+
             newb = make_block(values,
-                              ndim=self.ndim, placement=self.mgr_locs,
-                              fastpath=True, dtype=dtype, klass=klass)
+                              placement=self.mgr_locs,
+                              dtype=dtype,
+                              klass=klass)
         except:
             if raise_on_error is True:
                 raise
@@ -484,7 +524,7 @@ class Block(PandasObject):
     def _try_fill(self, value):
         return value
 
-    def to_native_types(self, slicer=None, na_rep='', quoting=None, **kwargs):
+    def to_native_types(self, slicer=None, na_rep='nan', quoting=None, **kwargs):
         """ convert to our native types format, slicing if desired """
 
         values = self.values
@@ -505,9 +545,9 @@ class Block(PandasObject):
         values = self.values
         if deep:
             values = values.copy()
-        return make_block(values, ndim=self.ndim,
-                          klass=self.__class__, fastpath=True,
-                          placement=self.mgr_locs)
+        return self.make_block(values,
+                               klass=self.__class__,
+                               fastpath=True)
 
     def replace(self, to_replace, value, inplace=False, filter=None,
                 regex=False):
@@ -616,9 +656,8 @@ class Block(PandasObject):
             else:
                 dtype = 'infer'
             values = self._try_coerce_and_cast_result(values, dtype)
-            block = make_block(transf(values),
-                               ndim=self.ndim, placement=self.mgr_locs,
-                               fastpath=True)
+            block = self.make_block(transf(values),
+                                    fastpath=True)
 
             # may have to soft convert_objects here
             if block.is_object and not self.is_object:
@@ -723,17 +762,16 @@ class Block(PandasObject):
 
                     # Put back the dimension that was taken from it and make
                     # a block out of the result.
-                    block = make_block(values=nv[np.newaxis],
-                                       placement=[ref_loc],
-                                       fastpath=True)
+                    block = self.make_block(values=nv[np.newaxis],
+                                            placement=[ref_loc],
+                                            fastpath=True)
 
                     new_blocks.append(block)
 
             else:
                 nv = _putmask_smart(new_values, mask, new)
-                new_blocks.append(make_block(values=nv,
-                                             placement=self.mgr_locs,
-                                             fastpath=True))
+                new_blocks.append(self.make_block(values=nv,
+                                                  fastpath=True))
 
             return new_blocks
 
@@ -743,7 +781,8 @@ class Block(PandasObject):
         if transpose:
             new_values = new_values.T
 
-        return [make_block(new_values, placement=self.mgr_locs, fastpath=True)]
+        return [self.make_block(new_values,
+                                fastpath=True)]
 
     def interpolate(self, method='pad', axis=0, index=None,
                     values=None, inplace=False, limit=None,
@@ -824,9 +863,9 @@ class Block(PandasObject):
                                     dtype=self.dtype)
         values = self._try_coerce_result(values)
 
-        blocks = [make_block(values,
-                             ndim=self.ndim, klass=self.__class__,
-                             fastpath=True, placement=self.mgr_locs)]
+        blocks = [self.make_block(values,
+                                  klass=self.__class__,
+                                  fastpath=True)]
         return self._maybe_downcast(blocks, downcast)
 
     def _interpolate(self, method=None, index=None, values=None,
@@ -865,9 +904,9 @@ class Block(PandasObject):
         # interp each column independently
         interp_values = np.apply_along_axis(func, axis, data)
 
-        blocks = [make_block(interp_values,
-                             ndim=self.ndim, klass=self.__class__,
-                             fastpath=True, placement=self.mgr_locs)]
+        blocks = [self.make_block(interp_values,
+                                  klass=self.__class__,
+                                  fastpath=True)]
         return self._maybe_downcast(blocks, downcast)
 
     def take_nd(self, indexer, axis, new_mgr_locs=None, fill_tuple=None):
@@ -875,13 +914,22 @@ class Block(PandasObject):
         Take values according to indexer and return them as a block.bb
 
         """
+
+        # com.take_nd dispatches for DatetimeTZBlock, CategoricalBlock
+        # so need to preserve types
+        # sparse is treated like an ndarray, but needs .get_values() shaping
+
+        values = self.values
+        if self.is_sparse:
+            values = self.get_values()
+
         if fill_tuple is None:
             fill_value = self.fill_value
-            new_values = com.take_nd(self.get_values(), indexer, axis=axis,
+            new_values = com.take_nd(values, indexer, axis=axis,
                                      allow_fill=False)
         else:
             fill_value = fill_tuple[0]
-            new_values = com.take_nd(self.get_values(), indexer, axis=axis,
+            new_values = com.take_nd(values, indexer, axis=axis,
                                      allow_fill=True, fill_value=fill_value)
 
         if new_mgr_locs is None:
@@ -894,26 +942,24 @@ class Block(PandasObject):
             else:
                 new_mgr_locs = self.mgr_locs
 
-        if new_values.dtype != self.dtype:
-            return make_block(new_values, new_mgr_locs)
+        if not is_dtype_equal(new_values.dtype, self.dtype):
+            return self.make_block(new_values, new_mgr_locs)
         else:
             return self.make_block_same_class(new_values, new_mgr_locs)
 
-    def get_values(self, dtype=None):
-        return self.values
-
     def diff(self, n, axis=1):
         """ return block for the diff of the values """
         new_values = com.diff(self.values, n, axis=axis)
-        return [make_block(values=new_values,
-                           ndim=self.ndim, fastpath=True,
-                           placement=self.mgr_locs)]
+        return [self.make_block(values=new_values,
+                                fastpath=True)]
 
     def shift(self, periods, axis=0):
         """ shift the block by periods, possibly upcast """
+
         # convert integer to float if necessary. need to do a lot more than
         # that, handle boolean etc also
         new_values, fill_value = com._maybe_upcast(self.values)
+
         # make sure array sent to np.roll is c_contiguous
         f_ordered = new_values.flags.f_contiguous
         if f_ordered:
@@ -934,9 +980,8 @@ class Block(PandasObject):
         if f_ordered:
             new_values = new_values.T
 
-        return [make_block(new_values,
-                           ndim=self.ndim, fastpath=True,
-                           placement=self.mgr_locs)]
+        return [self.make_block(new_values,
+                                fastpath=True)]
 
     def eval(self, func, other, raise_on_error=True, try_cast=False):
         """
@@ -1027,8 +1072,8 @@ class Block(PandasObject):
         if try_cast:
             result = self._try_cast_result(result)
 
-        return [make_block(result, ndim=self.ndim,
-                           fastpath=True, placement=self.mgr_locs)]
+        return [self.make_block(result,
+                                fastpath=True,)]
 
     def where(self, other, cond, align=True, raise_on_error=True,
               try_cast=False, axis=0, transpose=False):
@@ -1108,7 +1153,7 @@ class Block(PandasObject):
             if try_cast:
                 result = self._try_cast_result(result)
 
-            return make_block(result, ndim=self.ndim, placement=self.mgr_locs)
+            return self.make_block(result)
 
         # might need to separate out blocks
         axis = cond.ndim - 1
@@ -1121,8 +1166,8 @@ class Block(PandasObject):
             if m.any():
                 r = self._try_cast_result(
                     result.take(m.nonzero()[0], axis=axis))
-                result_blocks.append(make_block(r.T,
-                                                placement=self.mgr_locs[m]))
+                result_blocks.append(self.make_block(r.T,
+                                                     placement=self.mgr_locs[m]))
 
         return result_blocks
 
@@ -1139,7 +1184,7 @@ class NonConsolidatableMixIn(object):
     _holder = None
 
     def __init__(self, values, placement,
-                 ndim=None, fastpath=False,):
+                 ndim=None, fastpath=False, **kwargs):
 
         # Placement must be converted to BlockPlacement via property setter
         # before ndim logic, because placement may be a slice which doesn't
@@ -1159,6 +1204,12 @@ class NonConsolidatableMixIn(object):
 
         self.values = values
 
+    @property
+    def shape(self):
+        if self.ndim == 1:
+            return (len(self.values)),
+        return (len(self.mgr_locs), len(self.values))
+
     def get_values(self, dtype=None):
         """ need to to_dense myself (and always return a ndim sized object) """
         values = self.values.to_dense()
@@ -1170,7 +1221,7 @@ class NonConsolidatableMixIn(object):
 
         if self.ndim == 2 and isinstance(col, tuple):
             col, loc = col
-            if col != 0:
+            if not is_null_slice(col) and col != 0:
                 raise IndexError("{0} only contains one item".format(self))
             return self.values[loc]
         else:
@@ -1450,13 +1501,13 @@ class ObjectBlock(Block):
     _can_hold_na = True
 
     def __init__(self, values, ndim=2, fastpath=False,
-                 placement=None):
+                 placement=None, **kwargs):
         if issubclass(values.dtype.type, compat.string_types):
             values = np.array(values, dtype=object)
 
         super(ObjectBlock, self).__init__(values, ndim=ndim,
                                           fastpath=fastpath,
-                                          placement=placement)
+                                          placement=placement, **kwargs)
 
     @property
     def is_bool(self):
@@ -1490,8 +1541,8 @@ class ObjectBlock(Block):
                     copy=copy
                 ).reshape(values.shape)
                 values = _block_shape(values, ndim=self.ndim)
-                newb = make_block(values,
-                                  ndim=self.ndim, placement=[rl])
+                newb = self.make_block(values,
+                                       placement=[rl])
                 blocks.append(newb)
 
         else:
@@ -1504,8 +1555,7 @@ class ObjectBlock(Block):
                 coerce=coerce,
                 copy=copy
             ).reshape(self.values.shape)
-            blocks.append(make_block(values,
-                                     ndim=self.ndim, placement=self.mgr_locs))
+            blocks.append(self.make_block(values))
 
         return blocks
 
@@ -1559,7 +1609,7 @@ class ObjectBlock(Block):
     def should_store(self, value):
         return not (issubclass(value.dtype.type,
                               (np.integer, np.floating, np.complexfloating,
-                               np.datetime64, np.bool_)) or com.is_categorical_dtype(value))
+                               np.datetime64, np.bool_)) or is_internal_type(value))
 
     def replace(self, to_replace, value, inplace=False, filter=None,
                 regex=False):
@@ -1662,12 +1712,13 @@ class ObjectBlock(Block):
         new_values[filt] = f(new_values[filt])
 
         return [self if inplace else
-                make_block(new_values,
-                           fastpath=True, placement=self.mgr_locs)]
+                self.make_block(new_values,
+                                fastpath=True)]
 
 class CategoricalBlock(NonConsolidatableMixIn, ObjectBlock):
     __slots__ = ()
     is_categorical = True
+    _verify_integrity = True
     _can_hold_na = True
     _holder = Categorical
 
@@ -1690,10 +1741,6 @@ class CategoricalBlock(NonConsolidatableMixIn, ObjectBlock):
     def convert(self, copy=True, **kwargs):
         return [self.copy() if copy else self]
 
-    @property
-    def shape(self):
-        return (len(self.mgr_locs), len(self.values))
-
     @property
     def array_dtype(self):
         """ the dtype to return if I want to construct this block as an array """
@@ -1791,9 +1838,7 @@ class CategoricalBlock(NonConsolidatableMixIn, ObjectBlock):
         if copy:
             values = values.copy()
 
-        return make_block(values,
-                          ndim=self.ndim,
-                          placement=self.mgr_locs)
+        return self.make_block(values)
 
     def to_native_types(self, slicer=None, na_rep='', quoting=None, **kwargs):
         """ convert to our native types format, slicing if desired """
@@ -1893,8 +1938,8 @@ class DatetimeBlock(Block):
 
         np.putmask(values, mask, value)
         return [self if inplace else
-                make_block(values,
-                           fastpath=True, placement=self.mgr_locs)]
+                self.make_block(values,
+                                fastpath=True)]
 
     def to_native_types(self, slicer=None, na_rep=None, date_format=None,
                         quoting=None, **kwargs):
@@ -1902,19 +1947,19 @@ class DatetimeBlock(Block):
 
         values = self.values
         if slicer is not None:
-            values = values[:, slicer]
+            values = values[..., slicer]
 
         from pandas.core.format import _get_format_datetime64_from_values
         format = _get_format_datetime64_from_values(values, date_format)
 
         result = tslib.format_array_from_datetime(values.view('i8').ravel(),
-                                                  tz=None,
+                                                  tz=getattr(self.values,'tz',None),
                                                   format=format,
                                                   na_rep=na_rep).reshape(values.shape)
-        return result
+        return np.atleast_2d(result)
 
     def should_store(self, value):
-        return issubclass(value.dtype.type, np.datetime64)
+        return issubclass(value.dtype.type, np.datetime64) and not is_datetimetz(value)
 
     def set(self, locs, values, check=False):
         """
@@ -1937,12 +1982,102 @@ class DatetimeBlock(Block):
                       .reshape(self.values.shape)
         return self.values
 
+class DatetimeTZBlock(NonConsolidatableMixIn, DatetimeBlock):
+    """ implement a datetime64 block with a tz attribute """
+    __slots__ = ()
+    _holder = DatetimeIndex
+    is_datetimetz = True
+
+    def __init__(self, values, placement, ndim=2,
+                 **kwargs):
+
+        if not isinstance(values, self._holder):
+            values = self._holder(values)
+        if values.tz is None:
+            raise ValueError("cannot create a DatetimeTZBlock without a tz")
+
+        super(DatetimeTZBlock, self).__init__(values,
+                                              placement=placement,
+                                              ndim=ndim,
+                                              **kwargs)
+    def external_values(self):
+        """ we internally represent the data as a DatetimeIndex, but for external
+        compat with ndarray, export as a ndarray of Timestamps """
+        return self.values.astype('datetime64[ns]').values
+
+    def get_values(self, dtype=None):
+        # return object dtype as Timestamps with the zones
+        if dtype == object:
+            return lib.map_infer(self.values.ravel(), lambda x: lib.Timestamp(x,tz=self.values.tz))\
+                      .reshape(self.values.shape)
+        return self.values
+
+    def _slice(self, slicer):
+        """ return a slice of my values """
+        if isinstance(slicer, tuple):
+            col, loc = slicer
+            if not is_null_slice(col) and col != 0:
+                raise IndexError("{0} only contains one item".format(self))
+            return self.values[loc]
+        return self.values[slicer]
+
+    def _try_coerce_args(self, values, other):
+        """ localize and return i8 for the values """
+        values = values.tz_localize(None).asi8
+
+        if is_null_datelike_scalar(other):
+            other = tslib.iNaT
+        elif isinstance(other, self._holder):
+            if other.tz != self.tz:
+                raise ValueError("incompatible or non tz-aware value")
+            other = other.tz_localize(None).asi8
+        else:
+            other = lib.Timestamp(other)
+            if not getattr(other, 'tz', None):
+                raise ValueError("incompatible or non tz-aware value")
+            other = other.value
+
+        return values, other
+
+    def _try_coerce_result(self, result):
+        """ reverse of try_coerce_args """
+        result = super(DatetimeTZBlock, self)._try_coerce_result(result)
+
+        if isinstance(result, np.ndarray):
+            result = self._holder(result, tz=self.values.tz)
+        elif isinstance(result, (np.integer, np.datetime64)):
+            result = lib.Timestamp(result, tz=self.values.tz)
+        return result
+
+    def shift(self, periods, axis=0):
+        """ shift the block by periods """
+
+        ### think about moving this to the DatetimeIndex. This is a non-freq (number of periods) shift ###
+
+        N = len(self)
+        indexer = np.zeros(N, dtype=int)
+        if periods > 0:
+            indexer[periods:] = np.arange(N - periods)
+        else:
+            indexer[:periods] = np.arange(-periods, N)
+
+        # move to UTC & take
+        new_values = self.values.tz_localize(None).asi8.take(indexer)
+
+        if periods > 0:
+            new_values[:periods] = tslib.iNaT
+        else:
+            new_values[periods:] = tslib.iNaT
+
+        new_values = DatetimeIndex(new_values,tz=self.values.tz)
+        return [self.make_block_same_class(new_values, placement=self.mgr_locs)]
 
 class SparseBlock(NonConsolidatableMixIn, Block):
     """ implement as a list of sparse arrays of the same dtype """
     __slots__ = ()
     is_sparse = True
     is_numeric = True
+    _box_to_block_values = False
     _can_hold_na = True
     _ftype = 'sparse'
     _holder = SparseArray
@@ -1967,6 +2102,9 @@ class SparseBlock(NonConsolidatableMixIn, Block):
             v = float(v)
         self.values.fill_value = v
 
+    def to_dense(self):
+        return self.values.to_dense().view()
+
     @property
     def sp_values(self):
         return self.values.sp_values
@@ -2001,7 +2139,7 @@ class SparseBlock(NonConsolidatableMixIn, Block):
 
     def make_block_same_class(self, values, placement,
                               sparse_index=None, kind=None, dtype=None,
-                              fill_value=None, copy=False, fastpath=True):
+                              fill_value=None, copy=False, fastpath=True, **kwargs):
         """ return a new block """
         if dtype is None:
             dtype = self.dtype
@@ -2019,8 +2157,9 @@ class SparseBlock(NonConsolidatableMixIn, Block):
                 # output is 0-item, so let's convert it to a dense block: it
                 # won't take space since there's 0 items, plus it will preserve
                 # the dtype.
-                return make_block(np.empty(values.shape, dtype=dtype),
-                                  placement, fastpath=True,)
+                return self.make_block(np.empty(values.shape, dtype=dtype),
+                                       placement,
+                                       fastpath=True)
             elif nitems > 1:
                 raise ValueError("Only 1-item 2d sparse blocks are supported")
             else:
@@ -2029,8 +2168,9 @@ class SparseBlock(NonConsolidatableMixIn, Block):
         new_values = SparseArray(values, sparse_index=sparse_index,
                                  kind=kind or self.kind, dtype=dtype,
                                  fill_value=fill_value, copy=copy)
-        return make_block(new_values, ndim=self.ndim,
-                          fastpath=fastpath, placement=placement)
+        return self.make_block(new_values,
+                               fastpath=fastpath,
+                               placement=placement)
 
     def interpolate(self, method='pad', axis=0, inplace=False,
                     limit=None, fill_value=None, **kwargs):
@@ -2114,7 +2254,12 @@ def make_block(values, placement, klass=None, ndim=None,
         elif dtype == np.bool_:
             klass = BoolBlock
         elif issubclass(vtype, np.datetime64):
-            klass = DatetimeBlock
+            if hasattr(values,'tz'):
+                klass = DatetimeTZBlock
+            else:
+                klass = DatetimeBlock
+        elif is_datetimetz(values):
+            klass = DatetimeTZBlock
         elif issubclass(vtype, np.complexfloating):
             klass = ComplexBlock
         elif is_categorical(values):
@@ -2410,7 +2555,7 @@ class BlockManager(PandasObject):
         mgr_shape = self.shape
         tot_items = sum(len(x.mgr_locs) for x in self.blocks)
         for block in self.blocks:
-            if not block.is_sparse and block.shape[1:] != mgr_shape[1:]:
+            if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:
                 construction_error(tot_items, block.shape[1:], self.axes)
         if len(self.items) != tot_items:
             raise AssertionError('Number of manager items must equal union of '
@@ -2830,7 +2975,7 @@ class BlockManager(PandasObject):
         single block
         """
         if len(self.blocks) == 1:
-            return self.blocks[0].values[:, loc]
+            return self.blocks[0].iget((slice(None), loc))
 
         items = self.items
 
@@ -2911,10 +3056,9 @@ class BlockManager(PandasObject):
         Otherwise return as a ndarray
 
         """
-
         block = self.blocks[self._blknos[i]]
         values = block.iget(self._blklocs[i])
-        if not fastpath or block.is_sparse or values.ndim != 1:
+        if not fastpath or not block._box_to_block_values or values.ndim != 1:
             return values
 
         # fastpath shortcut for select a single-dim from a 2-dim BM
@@ -2984,18 +3128,10 @@ class BlockManager(PandasObject):
         # FIXME: refactor, clearly separate broadcasting & zip-like assignment
         #        can prob also fix the various if tests for sparse/categorical
 
-        value_is_sparse = isinstance(value, SparseArray)
-        value_is_cat = is_categorical(value)
-        value_is_nonconsolidatable = value_is_sparse or value_is_cat
-
-        if value_is_sparse:
-            # sparse
-            assert self.ndim == 2
+        value_is_internal_type = is_internal_type(value)
 
-            def value_getitem(placement):
-                return value
-        elif value_is_cat:
-            # categorical
+        # categorical/spares/datetimetz
+        if value_is_internal_type:
             def value_getitem(placement):
                 return value
         else:
@@ -3064,7 +3200,7 @@ class BlockManager(PandasObject):
             unfit_count = len(unfit_mgr_locs)
 
             new_blocks = []
-            if value_is_nonconsolidatable:
+            if value_is_internal_type:
                 # This code (ab-)uses the fact that sparse blocks contain only
                 # one item.
                 new_blocks.extend(
@@ -3487,7 +3623,7 @@ class SingleBlockManager(BlockManager):
 
     @property
     def dtype(self):
-        return self._values.dtype
+        return self._block.dtype
 
     @property
     def array_dtype(self):
@@ -3509,9 +3645,11 @@ class SingleBlockManager(BlockManager):
     def get_ftypes(self):
         return np.array([self._block.ftype])
 
-    @property
-    def values(self):
-        return self._values.view()
+    def external_values(self):
+        return self._block.external_values()
+
+    def internal_values(self):
+        return self._block.internal_values()
 
     def get_values(self):
         """ return a dense type view """
@@ -3519,7 +3657,7 @@ class SingleBlockManager(BlockManager):
 
     @property
     def itemsize(self):
-        return self._values.itemsize
+        return self._block.values.itemsize
 
     @property
     def _can_hold_na(self):
@@ -3586,6 +3724,7 @@ def create_block_manager_from_blocks(blocks, axes):
 
 
 def create_block_manager_from_arrays(arrays, names, axes):
+
     try:
         blocks = form_blocks(arrays, names, axes)
         mgr = BlockManager(blocks, axes)
@@ -3605,6 +3744,7 @@ def form_blocks(arrays, names, axes):
     object_items = []
     sparse_items = []
     datetime_items = []
+    datetime_tz_items = []
     cat_items = []
     extra_locs = []
 
@@ -3623,7 +3763,7 @@ def form_blocks(arrays, names, axes):
         k = names[name_idx]
         v = arrays[name_idx]
 
-        if isinstance(v, (SparseArray, ABCSparseSeries)):
+        if is_sparse(v):
             sparse_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.floating):
             float_items.append((i, k, v))
@@ -3633,10 +3773,12 @@ def form_blocks(arrays, names, axes):
             if v.dtype != _NS_DTYPE:
                 v = tslib.cast_to_nanoseconds(v)
 
-            if hasattr(v, 'tz') and v.tz is not None:
-                object_items.append((i, k, v))
+            if is_datetimetz(v):
+                datetime_tz_items.append((i, k, v))
             else:
                 datetime_items.append((i, k, v))
+        elif is_datetimetz(v):
+            datetime_tz_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.integer):
             if v.dtype == np.uint64:
                 # HACK #2355 definite overflow
@@ -3669,6 +3811,14 @@ def form_blocks(arrays, names, axes):
             datetime_items, _NS_DTYPE)
         blocks.extend(datetime_blocks)
 
+    if len(datetime_tz_items):
+        dttz_blocks = [ make_block(array,
+                                   klass=DatetimeTZBlock,
+                                   fastpath=True,
+                                   placement=[i],
+                                   ) for i, names, array in datetime_tz_items ]
+        blocks.extend(dttz_blocks)
+
     if len(bool_items):
         bool_blocks = _simple_blockify(
             bool_items, np.bool_)
@@ -3757,7 +3907,7 @@ def _stack_arrays(tuples, dtype):
     # fml
     def _asarray_compat(x):
         if isinstance(x, ABCSeries):
-            return x.values
+            return x._values
         else:
             return np.asarray(x)
 
@@ -3801,18 +3951,20 @@ def _interleaved_dtype(blocks):
     have_float = len(counts[FloatBlock]) > 0
     have_complex = len(counts[ComplexBlock]) > 0
     have_dt64 = len(counts[DatetimeBlock]) > 0
+    have_dt64_tz = len(counts[DatetimeTZBlock]) > 0
     have_td64 = len(counts[TimeDeltaBlock]) > 0
     have_cat = len(counts[CategoricalBlock]) > 0
     have_sparse = len(counts[SparseBlock]) > 0
     have_numeric = have_float or have_complex or have_int
 
-    has_non_numeric = have_dt64 or have_td64 or have_cat
+    has_non_numeric = have_dt64 or have_dt64_tz or have_td64 or have_cat
 
     if (have_object or
-        (have_bool and (have_numeric or have_dt64 or have_td64)) or
+        (have_bool and (have_numeric or have_dt64 or have_dt64_tz or have_td64)) or
         (have_numeric and has_non_numeric) or
         have_cat or
         have_dt64 or
+        have_dt64_tz or
         have_td64):
         return np.dtype(object)
     elif have_bool:
@@ -4140,6 +4292,8 @@ def get_empty_dtype_and_na(join_units):
 
         if com.is_categorical_dtype(dtype):
             upcast_cls = 'category'
+        elif com.is_datetimetz(dtype):
+            upcast_cls = 'datetimetz'
         elif issubclass(dtype.type, np.bool_):
             upcast_cls = 'bool'
         elif issubclass(dtype.type, np.object_):
@@ -4174,6 +4328,8 @@ def get_empty_dtype_and_na(join_units):
         return np.dtype(np.object_), np.nan
     elif 'float' in upcast_classes:
         return np.dtype(np.float64), np.nan
+    elif 'datetimetz' in upcast_classes:
+        return np.dtype('M8[ns]'), tslib.iNaT
     elif 'datetime' in upcast_classes:
         return np.dtype('M8[ns]'), tslib.iNaT
     elif 'timedelta' in upcast_classes:
@@ -4432,12 +4588,6 @@ class JoinUnit(object):
 
         return True
 
-    @cache_readonly
-    def needs_block_conversion(self):
-        """ we might need to convert the joined values to a suitable block repr """
-        block = self.block
-        return block is not None and (block.is_sparse or block.is_categorical)
-
     def get_reindexed_values(self, empty_dtype, upcasted_na):
         if upcasted_na is None:
             # No upcasting is necessary
@@ -4462,11 +4612,8 @@ class JoinUnit(object):
                 return missing_arr
 
             if not self.indexers:
-                if self.block.is_categorical:
-                    # preserve the categoricals for validation in _concat_compat
-                    return self.block.values
-                elif self.block.is_sparse:
-                    # preserve the sparse array for validation in _concat_compat
+                if not self.block._can_consolidate:
+                    # preserve these for validation in _concat_compat
                     return self.block.values
 
             if self.block.is_bool:
diff --git a/pandas/core/ops.py b/pandas/core/ops.py
index 8e3dd3836..9b0d6e9db 100644
--- a/pandas/core/ops.py
+++ b/pandas/core/ops.py
@@ -9,6 +9,7 @@ import operator
 import warnings
 import numpy as np
 import pandas as pd
+import datetime
 from pandas import compat, lib, tslib
 import pandas.index as _index
 from pandas.util.decorators import Appender
@@ -21,8 +22,10 @@ from pandas.core.common import(is_list_like, notnull, isnull,
                                _values_from_object, _maybe_match_name,
                                needs_i8_conversion, is_datetimelike_v_numeric,
                                is_integer_dtype, is_categorical_dtype, is_object_dtype,
-                               is_timedelta64_dtype, is_datetime64_dtype, is_bool_dtype)
+                               is_timedelta64_dtype, is_datetime64_dtype, is_datetime64tz_dtype,
+                               is_bool_dtype)
 from pandas.io.common import PerformanceWarning
+
 # -----------------------------------------------------------------------------
 # Functions that add arithmetic methods to objects, given arithmetic factory
 # methods
@@ -268,50 +271,61 @@ class _TimeOp(object):
     wrap_results = staticmethod(lambda x: x)
     dtype = None
 
-    def __init__(self, left, right, name):
-        self.name = name
+    def __init__(self, left, right, name, na_op):
 
         # need to make sure that we are aligning the data
         if isinstance(left, pd.Series) and isinstance(right, pd.Series):
             left, right = left.align(right,copy=False)
 
-        self.left = left
-        self.right = right
+        lvalues = self._convert_to_array(left, name=name)
+        rvalues = self._convert_to_array(right, name=name, other=lvalues)
 
-        self.is_offset_lhs = self._is_offset(left)
-        self.is_offset_rhs = self._is_offset(right)
+        self.name = name
+        self.na_op = na_op
 
-        lvalues = self._convert_to_array(left, name=name)
-        self.is_timedelta_lhs = is_timedelta64_dtype(left)
-        self.is_datetime_lhs = is_datetime64_dtype(left)
+        # left
+        self.left = left
+        self.is_offset_lhs = self._is_offset(left)
+        self.is_timedelta_lhs = is_timedelta64_dtype(lvalues)
+        self.is_datetime64_lhs = is_datetime64_dtype(lvalues)
+        self.is_datetime64tz_lhs = is_datetime64tz_dtype(lvalues)
+        self.is_datetime_lhs = self.is_datetime64_lhs or self.is_datetime64tz_lhs
         self.is_integer_lhs = left.dtype.kind in ['i', 'u']
 
-        rvalues = self._convert_to_array(right, name=name, other=lvalues)
-        self.is_datetime_rhs = is_datetime64_dtype(rvalues)
+        # right
+        self.right = right
+        self.is_offset_rhs = self._is_offset(right)
+        self.is_datetime64_rhs = is_datetime64_dtype(rvalues)
+        self.is_datetime64tz_rhs = is_datetime64tz_dtype(rvalues)
+        self.is_datetime_rhs = self.is_datetime64_rhs or self.is_datetime64tz_rhs
         self.is_timedelta_rhs = is_timedelta64_dtype(rvalues)
         self.is_integer_rhs = rvalues.dtype.kind in ('i', 'u')
 
-        self._validate()
+        self._validate(lvalues, rvalues, name)
+        self.lvalues, self.rvalues = self._convert_for_datetime(lvalues, rvalues)
 
-        self._convert_for_datetime(lvalues, rvalues)
-
-    def _validate(self):
+    def _validate(self, lvalues, rvalues, name):
         # timedelta and integer mul/div
 
-        if (self.is_timedelta_lhs and self.is_integer_rhs) or\
-           (self.is_integer_lhs and self.is_timedelta_rhs):
+        if (self.is_timedelta_lhs and self.is_integer_rhs) or (
+            self.is_integer_lhs and self.is_timedelta_rhs):
 
-            if self.name not in ('__truediv__', '__div__', '__mul__'):
+            if name not in ('__div__', '__truediv__', '__mul__'):
                 raise TypeError("can only operate on a timedelta and an "
                                 "integer for division, but the operator [%s]"
-                                "was passed" % self.name)
+                                "was passed" % name)
 
         # 2 datetimes
         elif self.is_datetime_lhs and self.is_datetime_rhs:
-            if self.name != '__sub__':
+
+            if name not in ('__sub__','__rsub__'):
                 raise TypeError("can only operate on a datetimes for"
                                 " subtraction, but the operator [%s] was"
-                                " passed" % self.name)
+                                " passed" % name)
+
+            # if tz's must be equal (same or None)
+            if getattr(lvalues,'tz',None) != getattr(rvalues,'tz',None):
+                raise ValueError("Incompatbile tz's on datetime subtraction ops")
 
         # 2 timedeltas
         elif ((self.is_timedelta_lhs and
@@ -319,29 +333,29 @@ class _TimeOp(object):
               (self.is_timedelta_rhs and
                (self.is_timedelta_lhs or self.is_offset_lhs))):
 
-            if self.name not in ('__div__', '__truediv__', '__add__',
-                                 '__sub__'):
+            if name not in ('__div__', '__rdiv__', '__truediv__', '__rtruediv__',
+                            '__add__', '__radd__', '__sub__', '__rsub__'):
                 raise TypeError("can only operate on a timedeltas for "
                                 "addition, subtraction, and division, but the"
-                                " operator [%s] was passed" % self.name)
+                                " operator [%s] was passed" % name)
 
         # datetime and timedelta/DateOffset
         elif (self.is_datetime_lhs and
               (self.is_timedelta_rhs or self.is_offset_rhs)):
 
-            if self.name not in ('__add__', '__sub__'):
+            if name not in ('__add__', '__radd__', '__sub__'):
                 raise TypeError("can only operate on a datetime with a rhs of"
                                 " a timedelta/DateOffset for addition and subtraction,"
                                 " but the operator [%s] was passed" %
-                                self.name)
+                                name)
 
         elif ((self.is_timedelta_lhs or self.is_offset_lhs)
               and self.is_datetime_rhs):
 
-            if self.name != '__add__':
+            if name not in ('__add__', '__radd__'):
                 raise TypeError("can only operate on a timedelta/DateOffset and"
                                 " a datetime for addition, but the operator"
-                                " [%s] was passed" % self.name)
+                                " [%s] was passed" % name)
         else:
             raise TypeError('cannot operate on a series with out a rhs '
                             'of a series/ndarray of type datetime64[ns] '
@@ -351,8 +365,10 @@ class _TimeOp(object):
         """converts values to ndarray"""
         from pandas.tseries.timedeltas import to_timedelta
 
+        ovalues = values
         if not is_list_like(values):
             values = np.array([values])
+
         inferred_type = lib.infer_dtype(values)
 
         if inferred_type in ('datetime64', 'datetime', 'date', 'time'):
@@ -366,6 +382,13 @@ class _TimeOp(object):
             # a datelike
             elif isinstance(values, pd.DatetimeIndex):
                 values = values.to_series()
+            # datetime with tz
+            elif isinstance(ovalues, datetime.datetime) and hasattr(ovalues,'tz'):
+                values = pd.DatetimeIndex(values)
+            # datetime array with tz
+            elif com.is_datetimetz(values):
+                if isinstance(values, pd.Series):
+                    values = values._values
             elif not (isinstance(values, (np.ndarray, pd.Series)) and
                       is_datetime64_dtype(values)):
                 values = tslib.array_to_datetime(values)
@@ -400,19 +423,25 @@ class _TimeOp(object):
 
     def _convert_for_datetime(self, lvalues, rvalues):
         from pandas.tseries.timedeltas import to_timedelta
-        mask = None
+
+        mask = isnull(lvalues) | isnull(rvalues)
+
         # datetimes require views
         if self.is_datetime_lhs or self.is_datetime_rhs:
+
             # datetime subtraction means timedelta
             if self.is_datetime_lhs and self.is_datetime_rhs:
                 self.dtype = 'timedelta64[ns]'
+            elif self.is_datetime64tz_lhs:
+                self.dtype = lvalues.dtype
+            elif self.is_datetime64tz_rhs:
+                self.dtype = rvalues.dtype
             else:
                 self.dtype = 'datetime64[ns]'
-            mask = isnull(lvalues) | isnull(rvalues)
 
             # if adding single offset try vectorized path
             # in DatetimeIndex; otherwise elementwise apply
-            if self.is_offset_lhs:
+            def _offset(lvalues, rvalues):
                 if len(lvalues) == 1:
                     rvalues = pd.DatetimeIndex(rvalues)
                     lvalues = lvalues[0]
@@ -420,22 +449,31 @@ class _TimeOp(object):
                     warnings.warn("Adding/subtracting array of DateOffsets to Series not vectorized",
                                   PerformanceWarning)
                     rvalues = rvalues.astype('O')
+
+                # pass thru on the na_op
+                self.na_op = lambda x, y: getattr(x,self.name)(y)
+                return lvalues, rvalues
+
+
+            if self.is_offset_lhs:
+                lvalues, rvalues = _offset(lvalues, rvalues)
             elif self.is_offset_rhs:
-                if len(rvalues) == 1:
-                    lvalues = pd.DatetimeIndex(lvalues)
-                    rvalues = rvalues[0]
-                else:
-                    warnings.warn("Adding/subtracting array of DateOffsets to Series not vectorized",
-                                  PerformanceWarning)
-                    lvalues = lvalues.astype('O')
+                rvalues, lvalues = _offset(rvalues, lvalues)
             else:
+
+                # with tz, convert to UTC
+                if self.is_datetime64tz_lhs:
+                    lvalues = lvalues.tz_localize(None)
+                if self.is_datetime64tz_rhs:
+                    rvalues = rvalues.tz_localize(None)
+
                 lvalues = lvalues.view(np.int64)
                 rvalues = rvalues.view(np.int64)
 
         # otherwise it's a timedelta
         else:
+
             self.dtype = 'timedelta64[ns]'
-            mask = isnull(lvalues) | isnull(rvalues)
 
             # convert Tick DateOffset to underlying delta
             if self.is_offset_lhs:
@@ -458,15 +496,20 @@ class _TimeOp(object):
                 rvalues = rvalues.astype(np.float64)
 
         # if we need to mask the results
-        if mask is not None:
-            if mask.any():
-                def f(x):
+        if mask.any():
+            def f(x):
+
+                # datetime64[ns]/timedelta64[ns] masking
+                try:
                     x = np.array(x, dtype=self.dtype)
-                    np.putmask(x, mask, self.fill_value)
-                    return x
-                self.wrap_results = f
-        self.lvalues = lvalues
-        self.rvalues = rvalues
+                except TypeError:
+                    x = np.array(x, dtype='datetime64[ns]')
+
+                np.putmask(x, mask, self.fill_value)
+                return x
+            self.wrap_results = f
+
+        return lvalues, rvalues
 
 
     def _is_offset(self, arr_or_obj):
@@ -479,7 +522,7 @@ class _TimeOp(object):
             return False
 
     @classmethod
-    def maybe_convert_for_time_op(cls, left, right, name):
+    def maybe_convert_for_time_op(cls, left, right, name, na_op):
         """
         if ``left`` and ``right`` are appropriate for datetime arithmetic with
         operation ``name``, processes them and returns a ``_TimeOp`` object
@@ -490,15 +533,12 @@ class _TimeOp(object):
         """
         # decide if we can do it
         is_timedelta_lhs = is_timedelta64_dtype(left)
-        is_datetime_lhs = is_datetime64_dtype(left)
+        is_datetime_lhs = is_datetime64_dtype(left) or is_datetime64tz_dtype(left)
+
         if not (is_datetime_lhs or is_timedelta_lhs):
             return None
 
-        # rops are allowed. No need for special checks, just strip off
-        # r part.
-        if name.startswith('__r'):
-            name = "__" + name[3:]
-        return cls(left, right, name)
+        return cls(left, right, name, na_op)
 
 
 def _arith_method_SERIES(op, name, str_rep, fill_zeros=None,
@@ -529,12 +569,12 @@ def _arith_method_SERIES(op, name, str_rep, fill_zeros=None,
         result = com._fill_zeros(result, x, y, name, fill_zeros)
         return result
 
-    def wrapper(left, right, name=name):
+    def wrapper(left, right, name=name, na_op=na_op):
 
         if isinstance(right, pd.DataFrame):
             return NotImplemented
 
-        time_converted = _TimeOp.maybe_convert_for_time_op(left, right, name)
+        time_converted = _TimeOp.maybe_convert_for_time_op(left, right, name, na_op)
 
         if time_converted is None:
             lvalues, rvalues = left, right
@@ -547,6 +587,7 @@ def _arith_method_SERIES(op, name, str_rep, fill_zeros=None,
             lvalues, rvalues = time_converted.lvalues, time_converted.rvalues
             dtype = time_converted.dtype
             wrap_results = time_converted.wrap_results
+            na_op = time_converted.na_op
 
         if isinstance(rvalues, pd.Series):
             rindex = getattr(rvalues,'index',rvalues)
@@ -616,7 +657,10 @@ def _comp_method_SERIES(op, name, str_rep, masker=False):
 
             # numpy does not like comparisons vs None
             if isscalar(y) and isnull(y):
-                y = np.nan
+                if name == '__ne__':
+                    return np.ones(len(x), dtype=bool)
+                else:
+                    return np.zeros(len(x), dtype=bool)
 
             # we have a datetime/timedelta and may need to convert
             mask = None
@@ -642,7 +686,7 @@ def _comp_method_SERIES(op, name, str_rep, masker=False):
                 result = op(x, y)
 
             if mask is not None and mask.any():
-                result[mask] = False
+                result[mask] = masker
 
         return result
 
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 116ae9f31..f44c23597 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -17,10 +17,12 @@ from pandas.core.common import (isnull, notnull, is_bool_indexer,
                                 _default_index, _maybe_upcast,
                                 _asarray_tuplesafe, _infer_dtype_from_scalar,
                                 is_list_like, _values_from_object,
+                                is_categorical_dtype, is_datetime64tz_dtype,
+                                needs_i8_conversion, i8_boxer,
                                 _possibly_cast_to_datetime, _possibly_castable,
                                 _possibly_convert_platform, _try_sort,
-                                is_int64_dtype,
-                                ABCSparseArray, _maybe_match_name,
+                                is_int64_dtype, is_internal_type, is_datetimetz,
+                                _maybe_match_name, ABCSparseArray,
                                 _coerce_to_dtype, SettingWithCopyError,
                                 _maybe_box_datetimelike, ABCDataFrame,
                                 _dict_compat)
@@ -308,19 +310,43 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
     @property
     def values(self):
         """
-        Return Series as ndarray
+        Return Series as ndarray or ndarray-like
+        depending on the dtype
 
         Returns
         -------
-        arr : numpy.ndarray
+        arr : numpy.ndarray or ndarray-like
+
+        Examples
+        --------
+        >>> pd.Series([1, 2, 3]).values
+        array([1, 2, 3])
+
+        >>> pd.Series(list('aabc')).values
+        array(['a', 'a', 'b', 'c'], dtype=object)
+
+        >>> pd.Series(list('aabc')).astype('category').values
+        [a, a, b, c]
+        Categories (3, object): [a, b, c]
+
+        # this is converted to UTC
+        >>> pd.Series(pd.date_range('20130101',periods=3,tz='US/Eastern')).values
+        array(['2013-01-01T00:00:00.000000000-0500',
+               '2013-01-02T00:00:00.000000000-0500',
+               '2013-01-03T00:00:00.000000000-0500'], dtype='datetime64[ns]')
+
         """
-        return self._data.values
+        return self._data.external_values()
+
+    @property
+    def _values(self):
+        """ return the internal repr of this data """
+        return self._data.internal_values()
 
     def get_values(self):
         """ same as values (but handles sparseness conversions); is a view """
         return self._data.get_values()
 
-
     # ops
     def ravel(self, order='C'):
         """
@@ -330,7 +356,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         --------
         numpy.ndarray.ravel
         """
-        return self.values.ravel(order=order)
+        return self._values.ravel(order=order)
 
     def compress(self, condition, axis=0, out=None, **kwargs):
         """
@@ -366,7 +392,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         --------
         numpy.nonzero
         """
-        return self.values.nonzero()
+        return self._values.nonzero()
 
     def put(self, *args, **kwargs):
         """
@@ -376,7 +402,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         --------
         numpy.ndarray.put
         """
-        self.values.put(*args, **kwargs)
+        self._values.put(*args, **kwargs)
 
     def __len__(self):
         """
@@ -385,7 +411,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         return len(self._data)
 
     def view(self, dtype=None):
-        return self._constructor(self.values.view(dtype),
+        return self._constructor(self._values.view(dtype),
                                  index=self.index).__finalize__(self)
 
     def __array__(self, result=None):
@@ -407,7 +433,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         """
 
         # nice error message for non-ufunc types
-        if context is not None and not isinstance(self.values, np.ndarray):
+        if context is not None and not isinstance(self._values, np.ndarray):
             obj = context[1][0]
             raise TypeError("{obj} with dtype {dtype} cannot perform "
                             "the numpy op {op}".format(obj=type(obj).__name__,
@@ -489,7 +515,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         try:
 
             # dispatch to the values if we need
-            values = self.values
+            values = self._values
             if isinstance(values, np.ndarray):
                 return _index.get_value_at(values, i)
             else:
@@ -619,7 +645,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
 
         # If key is contained, would have returned by now
         indexer, new_index = self.index.get_loc_level(key)
-        return self._constructor(self.values[indexer],
+        return self._constructor(self._values[indexer],
                                  index=new_index).__finalize__(self)
 
     def _get_values(self, indexer):
@@ -627,7 +653,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             return self._constructor(self._data.get_slice(indexer),
                                      fastpath=True).__finalize__(self)
         except Exception:
-            return self.values[indexer]
+            return self._values[indexer]
 
     def __setitem__(self, key, value):
 
@@ -638,7 +664,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             except (SettingWithCopyError):
                 raise
             except (KeyError, ValueError):
-                values = self.values
+                values = self._values
                 if (com.is_integer(key)
                                 and not self.index.inferred_type == 'integer'):
 
@@ -655,7 +681,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                         value = tslib.iNaT
 
                         try:
-                            self.index._engine.set_value(self.values, key, value)
+                            self.index._engine.set_value(self._values, key, value)
                             return
                         except (TypeError):
                             pass
@@ -689,7 +715,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             self._maybe_update_cacher()
 
     def _set_with_engine(self, key, value):
-        values = self.values
+        values = self._values
         try:
             self.index._engine.set_value(values, key, value)
             return
@@ -744,7 +770,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
 
     def _set_values(self, key, value):
         if isinstance(key, Series):
-            key = key.values
+            key = key._values
         self._data = self._data.setitem(indexer=key, value=value)
         self._maybe_update_cacher()
 
@@ -760,7 +786,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         numpy.ndarray.repeat
         """
         new_index = self.index.repeat(reps)
-        new_values = self.values.repeat(reps)
+        new_values = self._values.repeat(reps)
         return self._constructor(new_values,
                                  index=new_index).__finalize__(self)
 
@@ -783,7 +809,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             # XXX ignoring the "order" keyword.
             return self
 
-        return self.values.reshape(shape, **kwargs)
+        return self._values.reshape(shape, **kwargs)
 
     def iget_value(self, i, axis=0):
         """
@@ -824,8 +850,8 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         value : scalar value
         """
         if takeable is True:
-            return _maybe_box_datetimelike(self.values[label])
-        return self.index.get_value(self.values, label)
+            return _maybe_box_datetimelike(self._values[label])
+        return self.index.get_value(self._values, label)
 
     def set_value(self, label, value, takeable=False):
         """
@@ -849,9 +875,9 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         """
         try:
             if takeable:
-                self.values[label] = value
+                self._values[label] = value
             else:
-                self.index._engine.set_value(self.values, label, value)
+                self.index._engine.set_value(self._values, label, value)
             return self
         except KeyError:
 
@@ -894,7 +920,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 # set name if it was passed, otherwise, keep the previous name
                 self.name = name or self.name
             else:
-                return self._constructor(self.values.copy(),
+                return self._constructor(self._values.copy(),
                                          index=new_index).__finalize__(self)
         elif inplace:
             raise TypeError('Cannot reset_index inplace on a Series '
@@ -936,7 +962,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             return u('%s%sLength: %d') % (freqstr, namestr, len(self))
 
         # Categorical
-        if com.is_categorical_dtype(self.dtype):
+        if is_categorical_dtype(self.dtype):
             level_info = self.values._repr_categories_info()
             return u('%sLength: %d, dtype: %s\n%s') % (namestr,
                                                        len(self),
@@ -1021,14 +1047,13 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         return result
 
     def __iter__(self):
-        if  com.is_categorical_dtype(self.dtype):
-            return iter(self.values)
-        elif np.issubdtype(self.dtype, np.datetime64):
-            return (lib.Timestamp(x) for x in self.values)
-        elif np.issubdtype(self.dtype, np.timedelta64):
-            return (lib.Timedelta(x) for x in self.values)
+        """ provide iteration over the values of the Series
+        box values if necessary """
+        if needs_i8_conversion(self.dtype):
+            boxer = i8_boxer(self)
+            return (boxer(x) for x in self._values)
         else:
-            return iter(self.values)
+            return iter(self._values)
 
     def iteritems(self):
         """
@@ -1118,7 +1143,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         nobs : int or Series (if level specified)
         """
         if level is not None:
-            mask = notnull(self.values)
+            mask = notnull(self._values)
 
             if isinstance(level, compat.string_types):
                 level = self.index._get_level_number(level)
@@ -1457,8 +1482,8 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         if sorter is not None:
             sorter = com._ensure_platform_int(sorter)
 
-        return self.values.searchsorted(Series(v).values, side=side,
-                                        sorter=sorter)
+        return self._values.searchsorted(Series(v)._values, side=side,
+                                         sorter=sorter)
 
     #------------------------------------------------------------------------------
     # Combination
@@ -1564,7 +1589,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 new_values[i] = func(lv, rv)
         else:
             new_index = self.index
-            new_values = func(self.values, other)
+            new_values = func(self._values, other)
             new_name = self.name
         return self._constructor(new_values, index=new_index, name=new_name)
 
@@ -1585,7 +1610,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         this = self.reindex(new_index, copy=False)
         other = other.reindex(new_index, copy=False)
         name = _maybe_match_name(self, other)
-        rs_vals = com._where_compat(isnull(this), other.values, this.values)
+        rs_vals = com._where_compat(isnull(this), other._values, this._values)
         return self._constructor(rs_vals, index=new_index).__finalize__(self)
 
     def update(self, other):
@@ -1627,7 +1652,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 # uses the argsort default quicksort
                 return arr.argsort(kind='quicksort')
 
-        arr = self.values
+        arr = self._values
         sortedIdx = np.empty(len(self), dtype=np.int32)
 
         bad = isnull(arr)
@@ -1676,7 +1701,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             new_index, indexer = index.sort_values(return_indexer=True,
                                                    ascending=ascending)
 
-        new_values = self.values.take(indexer)
+        new_values = self._values.take(indexer)
         return self._constructor(new_values,
                                  index=new_index).__finalize__(self)
 
@@ -1772,7 +1797,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         --------
         numpy.ndarray.argsort
         """
-        values = self.values
+        values = self._values
         mask = isnull(values)
 
         if mask.any():
@@ -1813,7 +1838,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         ranks : Series
         """
         from pandas.core.algorithms import rank
-        ranks = rank(self.values, method=method, na_option=na_option,
+        ranks = rank(self._values, method=method, na_option=na_option,
                      ascending=ascending, pct=pct)
         return self._constructor(ranks, index=self.index).__finalize__(self)
 
@@ -1927,7 +1952,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         swapped : Series
         """
         new_index = self.index.swaplevel(i, j)
-        return self._constructor(self.values, index=new_index,
+        return self._constructor(self._values, index=new_index,
                                  copy=copy).__finalize__(self)
 
     def reorder_levels(self, order):
@@ -2023,7 +2048,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         y : Series
             same index as caller
         """
-        values = self.values
+        values = self._values
         if com.is_datetime64_dtype(values.dtype):
             values = lib.map_infer(values, lib.Timestamp)
 
@@ -2040,7 +2065,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 arg = self._constructor(arg, index=arg.keys())
 
             indexer = arg.index.get_indexer(values)
-            new_values = com.take_1d(arg.values, indexer)
+            new_values = com.take_1d(arg._values, indexer)
             return self._constructor(new_values,
                                      index=self.index).__finalize__(self)
         else:
@@ -2176,7 +2201,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         otherwise delegate to the object
 
         """
-        delegate = self.values
+        delegate = self._values
         if isinstance(delegate, np.ndarray):
             # Validate that 'axis' is consistent with Series's single axis.
             self._get_axis_number(axis)
@@ -2200,12 +2225,12 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
 
         """
         if dropna:
-            values = self.dropna().values
+            values = self.dropna()._values
         else:
-            values = self.values
+            values = self._values
 
-        if com.needs_i8_conversion(self):
-            boxer = com.i8_boxer(self)
+        if needs_i8_conversion(self):
+            boxer = i8_boxer(self)
 
             if len(values) == 0:
                 return boxer(tslib.iNaT)
@@ -2303,7 +2328,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
 
         indices = com._ensure_platform_int(indices)
         new_index = self.index.take(indices)
-        new_values = self.values.take(indices)
+        new_values = self._values.take(indices)
         return self._constructor(new_values,
                                  index=new_index).__finalize__(self)
 
@@ -2363,12 +2388,12 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         f = lib.ismember
         if com.is_datetime64_dtype(self):
             from pandas.tseries.tools import to_datetime
-            values = Series(to_datetime(values)).values.view('i8')
+            values = Series(to_datetime(values))._values.view('i8')
             comps = comps.view('i8')
             f = lib.ismember_int64
         elif com.is_timedelta64_dtype(self):
             from pandas.tseries.timedeltas import to_timedelta
-            values = Series(to_timedelta(values)).values.view('i8')
+            values = Series(to_timedelta(values))._values.view('i8')
             comps = comps.view('i8')
             f = lib.ismember_int64
         elif is_int64_dtype(self):
@@ -2541,7 +2566,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         if len(self) == 0:
             return None
 
-        mask = isnull(self.values)
+        mask = isnull(self._values)
         i = mask.argmin()
         if mask[i]:
             return None
@@ -2555,7 +2580,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         if len(self) == 0:
             return None
 
-        mask = isnull(self.values[::-1])
+        mask = isnull(self._values[::-1])
         i = mask.argmin()
         if mask[i]:
             return None
@@ -2587,7 +2612,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         if isinstance(where, compat.string_types):
             where = datetools.to_datetime(where)
 
-        values = self.values
+        values = self._values
 
         if not hasattr(where, '__iter__'):
             start = self.index[0]
@@ -2627,7 +2652,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         -------
         ts : TimeSeries with DatetimeIndex
         """
-        new_values = self.values
+        new_values = self._values
         if copy:
             new_values = new_values.copy()
 
@@ -2648,7 +2673,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         -------
         ts : TimeSeries with PeriodIndex
         """
-        new_values = self.values
+        new_values = self._values
         if copy:
             new_values = new_values.copy()
 
@@ -2672,7 +2697,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
     # Categorical methods
 
     def _make_cat_accessor(self):
-        if not com.is_categorical_dtype(self.dtype):
+        if not is_categorical_dtype(self.dtype):
             raise AttributeError("Can only use .cat accessor with a "
                                  "'category' dtype")
         return CategoricalAccessor(self.values, self.index)
@@ -2713,6 +2738,9 @@ def remove_na(series):
 def _sanitize_index(data, index, copy=False):
     """ sanitize an index type to return an ndarray of the underlying, pass thru a non-Index """
 
+    if index is None:
+        return data
+
     if len(data) != len(index):
         raise ValueError('Length of values does not match length of '
                          'index')
@@ -2754,10 +2782,11 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                 return arr
 
         try:
-            arr = _possibly_cast_to_datetime(arr, dtype)
-            subarr = np.array(arr, dtype=dtype, copy=copy)
+            subarr = _possibly_cast_to_datetime(arr, dtype)
+            if not is_internal_type(subarr):
+                subarr = np.array(subarr, dtype=dtype, copy=copy)
         except (ValueError, TypeError):
-            if com.is_categorical_dtype(dtype):
+            if is_categorical_dtype(dtype):
                 subarr = Categorical(arr)
             elif dtype is not None and raise_cast_failure:
                 raise
@@ -2778,15 +2807,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                 elif copy:
                     subarr = data.copy()
             else:
-                if (com.is_datetime64_dtype(data.dtype) and
-                        not com.is_datetime64_dtype(dtype)):
-                    if dtype == object:
-                        ints = np.asarray(data).view('i8')
-                        subarr = tslib.ints_to_pydatetime(ints)
-                    elif raise_cast_failure:
-                        raise TypeError('Cannot cast datetime64 to %s' % dtype)
-                else:
-                    subarr = _try_cast(data, True)
+                subarr = _try_cast(data, True)
         elif isinstance(data, Index):
             # don't coerce Index types
             # e.g. indexes can have different conversions (so don't fast path them)
@@ -2823,6 +2844,19 @@ def _sanitize_array(data, index, dtype=None, copy=False,
     else:
         subarr = _try_cast(data, False)
 
+    def create_from_value(value, index, dtype):
+        # return a new empty value suitable for the dtype
+
+        if is_datetimetz(dtype):
+            subarr = DatetimeIndex([value]*len(index))
+        else:
+            if not isinstance(dtype, (np.dtype, type(np.dtype))):
+                dtype = dtype.dtype
+            subarr = np.empty(len(index), dtype=dtype)
+            subarr.fill(value)
+
+        return subarr
+
     # scalar like
     if subarr.ndim == 0:
         if isinstance(data, list):  # pragma: no cover
@@ -2837,8 +2871,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
                 # need to possibly convert the value here
                 value = _possibly_cast_to_datetime(value, dtype)
 
-            subarr = np.empty(len(index), dtype=dtype)
-            subarr.fill(value)
+            subarr = create_from_value(value, index, dtype)
 
         else:
             return subarr.item()
@@ -2849,9 +2882,7 @@ def _sanitize_array(data, index, dtype=None, copy=False,
 
             # a 1-element ndarray
             if len(subarr) != len(index) and len(subarr) == 1:
-                value = subarr[0]
-                subarr = np.empty(len(index), dtype=subarr.dtype)
-                subarr.fill(value)
+                subarr = create_from_value(subarr[0], index, subarr)
 
     elif subarr.ndim > 1:
         if isinstance(data, np.ndarray):
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index b5a3577b3..5fdc0ce86 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -1790,6 +1790,8 @@ class DataCol(IndexCol):
         # short-cut certain block types
         if block.is_categorical:
             return self.set_atom_categorical(block, items=block_items, info=info)
+        elif block.is_datetimetz:
+            return self.set_atom_datetime64tz(block, info=info)
         elif block.is_datetime:
             return self.set_atom_datetime64(block)
         elif block.is_timedelta:
@@ -1804,50 +1806,14 @@ class DataCol(IndexCol):
             raise TypeError(
                 "[date] is not implemented as a table column")
         elif inferred_type == 'datetime':
-            rvalues = block.values.ravel()
-            if getattr(rvalues[0], 'tzinfo', None) is not None:
+            # after 8260
+            # this only would be hit for a mutli-timezone dtype
+            # which is an error
 
-                # if this block has more than one timezone, raise
-                try:
-                    # pytz timezones: compare on zone name (to avoid issues with DST being a different zone to STD).
-                    zones = [r.tzinfo.zone for r in rvalues]
-                except:
-                    # dateutil timezones: compare on ==
-                    zones = [r.tzinfo for r in rvalues]
-                    if any(zones[0] != zone_i for zone_i in zones[1:]):
-                        raise TypeError(
-                            "too many timezones in this block, create separate "
-                            "data columns"
-                        )
-                else:
-                    if len(set(zones)) != 1:
-                        raise TypeError(
-                            "too many timezones in this block, create separate "
-                            "data columns"
-                        )
-
-                # convert this column to datetime64[ns] utc, and save the tz
-                index = DatetimeIndex(rvalues)
-                tz = getattr(index, 'tz', None)
-                if tz is None:
-                    raise TypeError(
-                        "invalid timezone specification")
-
-                values = index.tz_convert('UTC').values.view('i8')
-
-                # store a converted timezone
-                zone = tslib.get_timezone(index.tz)
-                if zone is None:
-                    zone = tslib.tot_seconds(index.tz.utcoffset())
-                self.tz = zone
-
-                self.update_info(info)
-                self.set_atom_datetime64(
-                    block, values.reshape(block.values.shape))
-
-            else:
-                raise TypeError(
-                    "[datetime] is not implemented as a table column")
+            raise TypeError(
+                "too many timezones in this block, create separate "
+                "data columns"
+                )
         elif inferred_type == 'unicode':
             raise TypeError(
                 "[unicode] is not implemented as a table column")
@@ -1976,6 +1942,25 @@ class DataCol(IndexCol):
             values = block.values.view('i8')
         self.set_data(values, 'datetime64')
 
+    def set_atom_datetime64tz(self, block, info, values=None):
+
+        if values is None:
+            values = block.values
+
+        # convert this column to datetime64[ns] utc, and save the tz
+        values = values.tz_convert('UTC').values.view('i8').reshape(block.shape)
+
+        # store a converted timezone
+        zone = tslib.get_timezone(block.values.tz)
+        if zone is None:
+            zone = tslib.tot_seconds(block.values.tz.utcoffset())
+        self.tz = zone
+        self.update_info(info)
+
+        self.kind = 'datetime64'
+        self.typ = self.get_atom_datetime64(block)
+        self.set_data(values, 'datetime64')
+
     def get_atom_timedelta64(self, block):
         return _tables().Int64Col(shape=block.shape[0])
 
@@ -2037,9 +2022,8 @@ class DataCol(IndexCol):
                     # we stored as utc, so just set the tz
 
                     index = DatetimeIndex(
-                        self.data.ravel(), tz='UTC').tz_convert(self.tz)
-                    self.data = np.asarray(
-                        index.tolist(), dtype=object).reshape(self.data.shape)
+                        self.data.ravel(), tz='UTC').tz_convert(tslib.maybe_get_tz(self.tz))
+                    self.data = index
 
                 else:
                     self.data = np.asarray(self.data, dtype='M8[ns]')
@@ -4048,7 +4032,7 @@ class AppendableFrameTable(AppendableTable):
                 cols_ = cols
 
             # if we have a DataIndexableCol, its shape will only be 1 dim
-            if values.ndim == 1:
+            if values.ndim == 1 and isinstance(values, np.ndarray):
                 values = values.reshape((1, values.shape[0]))
 
             block = make_block(values, placement=np.arange(len(cols_)))
diff --git a/pandas/io/tests/data/legacy_hdf/datetimetz_object.h5 b/pandas/io/tests/data/legacy_hdf/datetimetz_object.h5
new file mode 100644
index 000000000..8cb4eda47
Binary files /dev/null and b/pandas/io/tests/data/legacy_hdf/datetimetz_object.h5 differ
diff --git a/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_2.7.10.msgpack b/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_2.7.10.msgpack
index 000879f4c..ed606295b 100644
Binary files a/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_2.7.10.msgpack and b/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_2.7.10.msgpack differ
diff --git a/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_3.4.3.msgpack b/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_3.4.3.msgpack
new file mode 100644
index 000000000..7a933b3a9
Binary files /dev/null and b/pandas/io/tests/data/legacy_msgpack/0.16.2/0.16.2_x86_64_darwin_3.4.3.msgpack differ
diff --git a/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_2.7.10.pickle b/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_2.7.10.pickle
index d45936baa..d279403b3 100644
Binary files a/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_2.7.10.pickle and b/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_2.7.10.pickle differ
diff --git a/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_3.4.3.pickle b/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_3.4.3.pickle
new file mode 100644
index 000000000..e480d5ac2
Binary files /dev/null and b/pandas/io/tests/data/legacy_pickle/0.16.2/0.16.2_x86_64_darwin_3.4.3.pickle differ
diff --git a/pandas/io/tests/generate_legacy_storage_files.py b/pandas/io/tests/generate_legacy_storage_files.py
index 0ca5ced1b..91d0333b3 100644
--- a/pandas/io/tests/generate_legacy_storage_files.py
+++ b/pandas/io/tests/generate_legacy_storage_files.py
@@ -83,7 +83,9 @@ def create_data():
                             index=MultiIndex.from_tuples(tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])),
                                                          names=['one', 'two'])),
                   dup=Series(np.arange(5).astype(np.float64), index=['A', 'B', 'C', 'D', 'A']),
-                  cat=Series(Categorical(['foo', 'bar', 'baz'])))
+                  cat=Series(Categorical(['foo', 'bar', 'baz'])),
+                  dt=Series(date_range('20130101',periods=5)),
+                  dt_tz=Series(date_range('20130101',periods=5,tz='US/Eastern')))
     if LooseVersion(pandas.__version__) >= '0.17.0':
         series['period'] = Series([Period('2000Q1')] * 5)
 
@@ -101,7 +103,9 @@ def create_data():
                  cat_onecol=DataFrame(dict(A=Categorical(['foo', 'bar']))),
                  cat_and_float=DataFrame(dict(A=Categorical(['foo', 'bar', 'baz']),
                                               B=np.arange(3).astype(np.int64))),
-                 mixed_dup=mixed_dup_df)
+                 mixed_dup=mixed_dup_df,
+                 dt_mixed_tzs=DataFrame(dict(A=Timestamp('20130102', tz='US/Eastern'), B=Timestamp('20130603', tz='CET')), index=range(5)),
+                 )
 
     mixed_dup_panel = Panel(dict(ItemA=frame['float'], ItemB=frame['int']))
     mixed_dup_panel.items = ['ItemA', 'ItemA']
diff --git a/pandas/io/tests/test_packers.py b/pandas/io/tests/test_packers.py
index 126782108..894b69928 100644
--- a/pandas/io/tests/test_packers.py
+++ b/pandas/io/tests/test_packers.py
@@ -45,7 +45,6 @@ def check_arbitrary(a, b):
     else:
         assert(a == b)
 
-
 class TestPackers(tm.TestCase):
 
     def setUp(self):
@@ -575,7 +574,7 @@ TestPackers
             for kind in v:
                 assert kind in data[typ], '"{0}" not found in data["{1}"]'.format(kind, typ)
 
-    def compare(self, vf):
+    def compare(self, vf, version):
         data = read_msgpack(vf)
         self.check_min_structure(data)
         for typ, dv in data.items():
@@ -586,17 +585,42 @@ TestPackers
                     expected = self.data[typ][dt]
                 except KeyError:
                     continue
-                check_arbitrary(result, expected)
+
+                # use a specific comparator
+                # if available
+                comparator = getattr(self,"compare_{typ}_{dt}".format(typ=typ,dt=dt), None)
+                if comparator is not None:
+                    comparator(result, expected, typ, version)
+                else:
+                    check_arbitrary(result, expected)
 
         return data
 
+    def compare_series_dt_tz(self, result, expected, typ, version):
+        # 8260
+        # dtype is object < 0.17.0
+        if LooseVersion(version) < '0.17.0':
+            expected = expected.astype(object)
+            tm.assert_series_equal(result, expected)
+        else:
+            tm.assert_series_equal(result, expected)
+
+    def compare_frame_dt_mixed_tzs(self, result, expected, typ, version):
+        # 8260
+        # dtype is object < 0.17.0
+        if LooseVersion(version) < '0.17.0':
+            expected = expected.astype(object)
+            tm.assert_frame_equal(result, expected)
+        else:
+            tm.assert_frame_equal(result, expected)
+
     def read_msgpacks(self, version):
 
         pth = tm.get_data_path('legacy_msgpack/{0}'.format(str(version)))
         n = 0
         for f in os.listdir(pth):
             vf = os.path.join(pth, f)
-            self.compare(vf)
+            self.compare(vf, version)
             n += 1
         assert n > 0, 'Msgpack files are not tested'
 
diff --git a/pandas/io/tests/test_pickle.py b/pandas/io/tests/test_pickle.py
index 1ade6ac0f..2a4e429e2 100644
--- a/pandas/io/tests/test_pickle.py
+++ b/pandas/io/tests/test_pickle.py
@@ -8,6 +8,8 @@ import pickle as pkl
 import nose
 import os
 
+from distutils.version import LooseVersion
+
 import numpy as np
 import pandas.util.testing as tm
 import pandas as pd
@@ -41,7 +43,7 @@ class TestPickle():
         self.data = create_pickle_data()
         self.path = u('__%s__.pickle' % tm.rands(10))
 
-    def compare_element(self, typ, result, expected):
+    def compare_element(self, result, expected, typ, version=None):
         if isinstance(expected,Index):
             tm.assert_index_equal(expected, result)
             return
@@ -53,7 +55,7 @@ class TestPickle():
             comparator = getattr(tm,"assert_%s_equal" % typ,tm.assert_almost_equal)
             comparator(result,expected)
 
-    def compare(self, vf):
+    def compare(self, vf, version):
 
         # py3 compat when reading py2 pickle
         try:
@@ -72,9 +74,30 @@ class TestPickle():
                 except (KeyError):
                     continue
 
-                self.compare_element(typ, result, expected)
+                # use a specific comparator
+                # if available
+                comparator = getattr(self,"compare_{typ}_{dt}".format(typ=typ,dt=dt), self.compare_element)
+                comparator(result, expected, typ, version)
         return data
 
+    def compare_series_dt_tz(self, result, expected, typ, version):
+        # 8260
+        # dtype is object < 0.17.0
+        if LooseVersion(version) < '0.17.0':
+            expected = expected.astype(object)
+            tm.assert_series_equal(result, expected)
+        else:
+            tm.assert_series_equal(result, expected)
+
+    def compare_frame_dt_mixed_tzs(self, result, expected, typ, version):
+        # 8260
+        # dtype is object < 0.17.0
+        if LooseVersion(version) < '0.17.0':
+            expected = expected.astype(object)
+            tm.assert_frame_equal(result, expected)
+        else:
+            tm.assert_frame_equal(result, expected)
+
     def read_pickles(self, version):
         if not is_little_endian():
             raise nose.SkipTest("known failure on non-little endian")
@@ -83,7 +106,7 @@ class TestPickle():
         n = 0
         for f in os.listdir(pth):
             vf = os.path.join(pth, f)
-            data = self.compare(vf)
+            data = self.compare(vf, version)
 
             if data is None:
                 continue
@@ -150,14 +173,14 @@ class TestPickle():
 
                         # test reading with each unpickler
                         result = pd.read_pickle(path)
-                        self.compare_element(typ, result, expected)
+                        self.compare_element(result, expected, typ)
 
                         if c_unpickler is not None:
                             result = c_unpickler(path)
-                            self.compare_element(typ, result, expected)
+                            self.compare_element(result, expected, typ)
 
                         result = python_unpickler(path)
-                        self.compare_element(typ, result, expected)
+                        self.compare_element(result, expected, typ)
 
     def _validate_timeseries(self, pickled, current):
         # GH 7748
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index b8536eead..5eef48c51 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -161,7 +161,7 @@ class Base(tm.TestCase):
         pass
 
 
-class TestHDFStore(Base):
+class TestHDFStore(Base, tm.TestCase):
 
     def test_factory_fun(self):
         path = create_tempfile(self.path)
@@ -1980,73 +1980,6 @@ class TestHDFStore(Base):
             # this fails because we have a date in the object block......
             self.assertRaises(TypeError, store.append, 'df_unimplemented', df)
 
-    def test_append_with_timezones_pytz(self):
-
-        from datetime import timedelta
-
-        def compare(a,b):
-            tm.assert_frame_equal(a,b)
-
-            # compare the zones on each element
-            for c in a.columns:
-                for i in a.index:
-                    a_e = a[c][i]
-                    b_e = b[c][i]
-                    if not (a_e == b_e and a_e.tz == b_e.tz):
-                        raise AssertionError("invalid tz comparsion [%s] [%s]" % (a_e,b_e))
-
-        # as columns
-        with ensure_clean_store(self.path) as store:
-
-            _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A = [ Timestamp('20130102 2:00:00',tz='US/Eastern') + timedelta(hours=1)*i for i in range(5) ]))
-            store.append('df_tz',df,data_columns=['A'])
-            result = store['df_tz']
-            compare(result,df)
-            assert_frame_equal(result,df)
-
-            # select with tz aware
-            compare(store.select('df_tz',where=Term('A>=df.A[3]')),df[df.A>=df.A[3]])
-
-            _maybe_remove(store, 'df_tz')
-            # ensure we include dates in DST and STD time here.
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130603',tz='US/Eastern')),index=range(5))
-            store.append('df_tz',df)
-            result = store['df_tz']
-            compare(result,df)
-            assert_frame_equal(result,df)
-
-            _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='EET')),index=range(5))
-            self.assertRaises(TypeError, store.append, 'df_tz', df)
-
-            # this is ok
-            _maybe_remove(store, 'df_tz')
-            store.append('df_tz',df,data_columns=['A','B'])
-            result = store['df_tz']
-            compare(result,df)
-            assert_frame_equal(result,df)
-
-            # can't append with diff timezone
-            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='CET')),index=range(5))
-            self.assertRaises(ValueError, store.append, 'df_tz', df)
-
-        # as index
-        with ensure_clean_store(self.path) as store:
-
-            # GH 4098 example
-            df = DataFrame(dict(A = Series(lrange(3), index=date_range('2000-1-1',periods=3,freq='H', tz='US/Eastern'))))
-
-            _maybe_remove(store, 'df')
-            store.put('df',df)
-            result = store.select('df')
-            assert_frame_equal(result,df)
-
-            _maybe_remove(store, 'df')
-            store.append('df',df)
-            result = store.select('df')
-            assert_frame_equal(result,df)
-
     def test_calendar_roundtrip_issue(self):
 
         # 8591
@@ -2069,128 +2002,6 @@ class TestHDFStore(Base):
             result = store.select('table')
             assert_series_equal(result, s)
 
-    def test_append_with_timezones_dateutil(self):
-
-        from datetime import timedelta
-        tm._skip_if_no_dateutil()
-
-        # use maybe_get_tz instead of dateutil.tz.gettz to handle the windows filename issues.
-        from pandas.tslib import maybe_get_tz
-        gettz = lambda x: maybe_get_tz('dateutil/' + x)
-
-        def compare(a, b):
-            tm.assert_frame_equal(a, b)
-
-            # compare the zones on each element
-            for c in a.columns:
-                for i in a.index:
-                    a_e = a[c][i]
-                    b_e = b[c][i]
-                    if not (a_e == b_e and a_e.tz == b_e.tz):
-                        raise AssertionError("invalid tz comparsion [%s] [%s]" % (a_e, b_e))
-
-        # as columns
-        with ensure_clean_store(self.path) as store:
-
-            _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A=[ Timestamp('20130102 2:00:00', tz=gettz('US/Eastern')) + timedelta(hours=1) * i for i in range(5) ]))
-            store.append('df_tz', df, data_columns=['A'])
-            result = store['df_tz']
-            compare(result, df)
-            assert_frame_equal(result, df)
-
-            # select with tz aware
-            compare(store.select('df_tz', where=Term('A>=df.A[3]')), df[df.A >= df.A[3]])
-
-            _maybe_remove(store, 'df_tz')
-            # ensure we include dates in DST and STD time here.
-            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130603', tz=gettz('US/Eastern'))), index=range(5))
-            store.append('df_tz', df)
-            result = store['df_tz']
-            compare(result, df)
-            assert_frame_equal(result, df)
-
-            _maybe_remove(store, 'df_tz')
-            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130102', tz=gettz('EET'))), index=range(5))
-            self.assertRaises(TypeError, store.append, 'df_tz', df)
-
-            # this is ok
-            _maybe_remove(store, 'df_tz')
-            store.append('df_tz', df, data_columns=['A', 'B'])
-            result = store['df_tz']
-            compare(result, df)
-            assert_frame_equal(result, df)
-
-            # can't append with diff timezone
-            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130102', tz=gettz('CET'))), index=range(5))
-            self.assertRaises(ValueError, store.append, 'df_tz', df)
-
-        # as index
-        with ensure_clean_store(self.path) as store:
-
-            # GH 4098 example
-            df = DataFrame(dict(A=Series(lrange(3), index=date_range('2000-1-1', periods=3, freq='H', tz=gettz('US/Eastern')))))
-
-            _maybe_remove(store, 'df')
-            store.put('df', df)
-            result = store.select('df')
-            assert_frame_equal(result, df)
-
-            _maybe_remove(store, 'df')
-            store.append('df', df)
-            result = store.select('df')
-            assert_frame_equal(result, df)
-
-    def test_store_timezone(self):
-        # GH2852
-        # issue storing datetime.date with a timezone as it resets when read back in a new timezone
-
-        # timezone setting not supported on windows
-        tm._skip_if_windows()
-
-        import datetime
-        import time
-        import os
-
-        # original method
-        with ensure_clean_store(self.path) as store:
-
-            today = datetime.date(2013,9,10)
-            df = DataFrame([1,2,3], index = [today, today, today])
-            store['obj1'] = df
-            result = store['obj1']
-            assert_frame_equal(result, df)
-
-        # with tz setting
-        orig_tz = os.environ.get('TZ')
-
-        def setTZ(tz):
-            if tz is None:
-                try:
-                    del os.environ['TZ']
-                except:
-                    pass
-            else:
-                os.environ['TZ']=tz
-                time.tzset()
-
-        try:
-
-            with ensure_clean_store(self.path) as store:
-
-                setTZ('EST5EDT')
-                today = datetime.date(2013,9,10)
-                df = DataFrame([1,2,3], index = [today, today, today])
-                store['obj1'] = df
-
-                setTZ('CST6CDT')
-                result = store['obj1']
-
-                assert_frame_equal(result, df)
-
-        finally:
-            setTZ(orig_tz)
-
     def test_append_with_timedelta(self):
         # GH 3577
         # append timedelta
@@ -2875,26 +2686,6 @@ class TestHDFStore(Base):
 
         self._check_roundtrip(frame, tm.assert_frame_equal)
 
-    def test_timezones(self):
-        rng = date_range('1/1/2000', '1/30/2000', tz='US/Eastern')
-        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
-
-        with ensure_clean_store(self.path) as store:
-            store['frame'] = frame
-            recons = store['frame']
-            self.assertTrue(recons.index.equals(rng))
-            self.assertEqual(rng.tz, recons.index.tz)
-
-    def test_fixed_offset_tz(self):
-        rng = date_range('1/1/2000 00:00:00-07:00', '1/30/2000 00:00:00-07:00')
-        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
-
-        with ensure_clean_store(self.path) as store:
-            store['frame'] = frame
-            recons = store['frame']
-            self.assertTrue(recons.index.equals(rng))
-            self.assertEqual(rng.tz, recons.index.tz)
-
     def test_store_hierarchical(self):
         index = MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'],
                                    ['one', 'two', 'three']],
@@ -4294,35 +4085,25 @@ class TestHDFStore(Base):
 
     def test_pytables_native_read(self):
 
-        try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/pytables_native.h5'), 'r')
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/pytables_native.h5'), mode='r') as store:
             d2 = store['detector/readout']
-            assert isinstance(d2, DataFrame)
-        finally:
-            safe_close(store)
+            self.assertIsInstance(d2, DataFrame)
 
-        try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/pytables_native2.h5'), 'r')
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/pytables_native2.h5'), mode='r') as store:
             str(store)
             d1 = store['detector']
-            assert isinstance(d1, DataFrame)
-        finally:
-            safe_close(store)
+            self.assertIsInstance(d1, DataFrame)
 
     def test_legacy_read(self):
-        try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/legacy.h5'), 'r')
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/legacy.h5'), mode='r') as store:
             store['a']
             store['b']
             store['c']
             store['d']
-        finally:
-            safe_close(store)
 
     def test_legacy_table_read(self):
         # legacy table types
-        try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/legacy_table.h5'), 'r')
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/legacy_table.h5'), mode='r') as store:
             store.select('df1')
             store.select('df2')
             store.select('wp1')
@@ -4340,24 +4121,17 @@ class TestHDFStore(Base):
                 expected = df2[df2.index > df2.index[2]]
                 assert_frame_equal(expected, result)
 
-        finally:
-            safe_close(store)
-
     def test_legacy_0_10_read(self):
         # legacy from 0.10
-        try:
-            store = HDFStore(tm.get_data_path('legacy_hdf/legacy_0.10.h5'), 'r')
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/legacy_0.10.h5'), mode='r') as store:
             str(store)
             for k in store.keys():
                 store.select(k)
-        finally:
-            safe_close(store)
 
     def test_legacy_0_11_read(self):
         # legacy from 0.11
-        try:
-            path = os.path.join('legacy_hdf', 'legacy_table_0.11.h5')
-            store = HDFStore(tm.get_data_path(path), 'r')
+        path = os.path.join('legacy_hdf', 'legacy_table_0.11.h5')
+        with ensure_clean_store(tm.get_data_path(path), mode='r') as store:
             str(store)
             assert 'df' in store
             assert 'df1' in store
@@ -4368,8 +4142,6 @@ class TestHDFStore(Base):
             assert isinstance(df, DataFrame)
             assert isinstance(df1, DataFrame)
             assert isinstance(mi, DataFrame)
-        finally:
-            safe_close(store)
 
     def test_copy(self):
 
@@ -4506,38 +4278,6 @@ class TestHDFStore(Base):
             self.assertEqual(type(result.index), type(df.index))
             self.assertEqual(result.index.freq, df.index.freq)
 
-    def test_tseries_select_index_column(self):
-        # GH7777
-        # selecting a UTC datetimeindex column did
-        # not preserve UTC tzinfo set before storing
-
-        # check that no tz still works
-        rng = date_range('1/1/2000', '1/30/2000')
-        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
-
-        with ensure_clean_store(self.path) as store:
-            store.append('frame', frame)
-            result = store.select_column('frame', 'index')
-            self.assertEqual(rng.tz, DatetimeIndex(result.values).tz)
-
-        # check utc
-        rng = date_range('1/1/2000', '1/30/2000', tz='UTC')
-        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
-
-        with ensure_clean_store(self.path) as store:
-            store.append('frame', frame)
-            result = store.select_column('frame', 'index')
-            self.assertEqual(rng.tz, DatetimeIndex(result.values).tz)
-
-        # double check non-utc
-        rng = date_range('1/1/2000', '1/30/2000', tz='US/Eastern')
-        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
-
-        with ensure_clean_store(self.path) as store:
-            store.append('frame', frame)
-            result = store.select_column('frame', 'index')
-            self.assertEqual(rng.tz, DatetimeIndex(result.values).tz)
-
     def test_unicode_index(self):
 
         unicode_values = [u('\u03c3'), u('\u03c3\u03c3')]
@@ -4969,6 +4709,249 @@ class TestHDFComplexValues(Base):
             result = store.select('df')
             assert_frame_equal(pd.concat([df, df], 0), result)
 
+class TestTimezones(Base, tm.TestCase):
+
+
+    def _compare_with_tz(self, a, b):
+        tm.assert_frame_equal(a, b)
+
+        # compare the zones on each element
+        for c in a.columns:
+            for i in a.index:
+                a_e = a.loc[i,c]
+                b_e = b.loc[i,c]
+                if not (a_e == b_e and a_e.tz == b_e.tz):
+                    raise AssertionError("invalid tz comparsion [%s] [%s]" % (a_e, b_e))
+
+    def test_append_with_timezones_dateutil(self):
+
+        from datetime import timedelta
+        tm._skip_if_no_dateutil()
+
+        # use maybe_get_tz instead of dateutil.tz.gettz to handle the windows filename issues.
+        from pandas.tslib import maybe_get_tz
+        gettz = lambda x: maybe_get_tz('dateutil/' + x)
+
+        # as columns
+        with ensure_clean_store(self.path) as store:
+
+            _maybe_remove(store, 'df_tz')
+            df = DataFrame(dict(A=[ Timestamp('20130102 2:00:00', tz=gettz('US/Eastern')) + timedelta(hours=1) * i for i in range(5) ]))
+
+            store.append('df_tz', df, data_columns=['A'])
+            result = store['df_tz']
+            self._compare_with_tz(result, df)
+            assert_frame_equal(result, df)
+
+            # select with tz aware
+            expected = df[df.A >= df.A[3]]
+            result = store.select('df_tz', where=Term('A>=df.A[3]'))
+            self._compare_with_tz(result, expected)
+
+            # ensure we include dates in DST and STD time here.
+            _maybe_remove(store, 'df_tz')
+            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130603', tz=gettz('US/Eastern'))), index=range(5))
+            store.append('df_tz', df)
+            result = store['df_tz']
+            self._compare_with_tz(result, df)
+            assert_frame_equal(result, df)
+
+            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130102', tz=gettz('EET'))), index=range(5))
+            self.assertRaises(ValueError, store.append, 'df_tz', df)
+
+            # this is ok
+            _maybe_remove(store, 'df_tz')
+            store.append('df_tz', df, data_columns=['A', 'B'])
+            result = store['df_tz']
+            self._compare_with_tz(result, df)
+            assert_frame_equal(result, df)
+
+            # can't append with diff timezone
+            df = DataFrame(dict(A=Timestamp('20130102', tz=gettz('US/Eastern')), B=Timestamp('20130102', tz=gettz('CET'))), index=range(5))
+            self.assertRaises(ValueError, store.append, 'df_tz', df)
+
+        # as index
+        with ensure_clean_store(self.path) as store:
+
+            # GH 4098 example
+            df = DataFrame(dict(A=Series(lrange(3), index=date_range('2000-1-1', periods=3, freq='H', tz=gettz('US/Eastern')))))
+
+            _maybe_remove(store, 'df')
+            store.put('df', df)
+            result = store.select('df')
+            assert_frame_equal(result, df)
+
+            _maybe_remove(store, 'df')
+            store.append('df', df)
+            result = store.select('df')
+            assert_frame_equal(result, df)
+
+    def test_append_with_timezones_pytz(self):
+
+        from datetime import timedelta
+
+        # as columns
+        with ensure_clean_store(self.path) as store:
+
+            _maybe_remove(store, 'df_tz')
+            df = DataFrame(dict(A = [ Timestamp('20130102 2:00:00',tz='US/Eastern') + timedelta(hours=1)*i for i in range(5) ]))
+            store.append('df_tz',df,data_columns=['A'])
+            result = store['df_tz']
+            self._compare_with_tz(result,df)
+            assert_frame_equal(result,df)
+
+            # select with tz aware
+            self._compare_with_tz(store.select('df_tz',where=Term('A>=df.A[3]')),df[df.A>=df.A[3]])
+
+            _maybe_remove(store, 'df_tz')
+            # ensure we include dates in DST and STD time here.
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130603',tz='US/Eastern')),index=range(5))
+            store.append('df_tz',df)
+            result = store['df_tz']
+            self._compare_with_tz(result,df)
+            assert_frame_equal(result,df)
+
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='EET')),index=range(5))
+            self.assertRaises(ValueError, store.append, 'df_tz', df)
+
+            # this is ok
+            _maybe_remove(store, 'df_tz')
+            store.append('df_tz',df,data_columns=['A','B'])
+            result = store['df_tz']
+            self._compare_with_tz(result,df)
+            assert_frame_equal(result,df)
+
+            # can't append with diff timezone
+            df = DataFrame(dict(A = Timestamp('20130102',tz='US/Eastern'), B = Timestamp('20130102',tz='CET')),index=range(5))
+            self.assertRaises(ValueError, store.append, 'df_tz', df)
+
+        # as index
+        with ensure_clean_store(self.path) as store:
+
+            # GH 4098 example
+            df = DataFrame(dict(A = Series(lrange(3), index=date_range('2000-1-1',periods=3,freq='H', tz='US/Eastern'))))
+
+            _maybe_remove(store, 'df')
+            store.put('df',df)
+            result = store.select('df')
+            assert_frame_equal(result,df)
+
+            _maybe_remove(store, 'df')
+            store.append('df',df)
+            result = store.select('df')
+            assert_frame_equal(result,df)
+
+    def test_tseries_select_index_column(self):
+        # GH7777
+        # selecting a UTC datetimeindex column did
+        # not preserve UTC tzinfo set before storing
+
+        # check that no tz still works
+        rng = date_range('1/1/2000', '1/30/2000')
+        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
+
+        with ensure_clean_store(self.path) as store:
+            store.append('frame', frame)
+            result = store.select_column('frame', 'index')
+            self.assertEqual(rng.tz, DatetimeIndex(result.values).tz)
+
+        # check utc
+        rng = date_range('1/1/2000', '1/30/2000', tz='UTC')
+        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
+
+        with ensure_clean_store(self.path) as store:
+            store.append('frame', frame)
+            result = store.select_column('frame', 'index')
+            self.assertEqual(rng.tz, result.dt.tz)
+
+        # double check non-utc
+        rng = date_range('1/1/2000', '1/30/2000', tz='US/Eastern')
+        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
+
+        with ensure_clean_store(self.path) as store:
+            store.append('frame', frame)
+            result = store.select_column('frame', 'index')
+            self.assertEqual(rng.tz, result.dt.tz)
+
+    def test_timezones(self):
+        rng = date_range('1/1/2000', '1/30/2000', tz='US/Eastern')
+        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
+
+        with ensure_clean_store(self.path) as store:
+            store['frame'] = frame
+            recons = store['frame']
+            self.assertTrue(recons.index.equals(rng))
+            self.assertEqual(rng.tz, recons.index.tz)
+
+    def test_fixed_offset_tz(self):
+        rng = date_range('1/1/2000 00:00:00-07:00', '1/30/2000 00:00:00-07:00')
+        frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
+
+        with ensure_clean_store(self.path) as store:
+            store['frame'] = frame
+            recons = store['frame']
+            self.assertTrue(recons.index.equals(rng))
+            self.assertEqual(rng.tz, recons.index.tz)
+
+    def test_store_timezone(self):
+        # GH2852
+        # issue storing datetime.date with a timezone as it resets when read back in a new timezone
+
+        import platform
+        if platform.system() == "Windows":
+            raise nose.SkipTest("timezone setting not supported on windows")
+
+        import datetime
+        import time
+        import os
+
+        # original method
+        with ensure_clean_store(self.path) as store:
+
+            today = datetime.date(2013,9,10)
+            df = DataFrame([1,2,3], index = [today, today, today])
+            store['obj1'] = df
+            result = store['obj1']
+            assert_frame_equal(result, df)
+
+        # with tz setting
+        orig_tz = os.environ.get('TZ')
+
+        def setTZ(tz):
+            if tz is None:
+                try:
+                    del os.environ['TZ']
+                except:
+                    pass
+            else:
+                os.environ['TZ']=tz
+                time.tzset()
+
+        try:
+
+            with ensure_clean_store(self.path) as store:
+
+                setTZ('EST5EDT')
+                today = datetime.date(2013,9,10)
+                df = DataFrame([1,2,3], index = [today, today, today])
+                store['obj1'] = df
+
+                setTZ('CST6CDT')
+                result = store['obj1']
+
+                assert_frame_equal(result, df)
+
+        finally:
+            setTZ(orig_tz)
+
+    def test_legacy_datetimetz_object(self):
+        # legacy from < 0.17.0
+        # 8260
+        expected = DataFrame(dict(A=Timestamp('20130102', tz='US/Eastern'), B=Timestamp('20130603', tz='CET')), index=range(5))
+        with ensure_clean_store(tm.get_data_path('legacy_hdf/datetimetz_object.h5'), mode='r') as store:
+            result = store['df']
+            assert_frame_equal(result, expected)
+
 def _test_sort(obj):
     if isinstance(obj, DataFrame):
         return obj.reindex(sorted(obj.index))
diff --git a/pandas/io/tests/test_sql.py b/pandas/io/tests/test_sql.py
index 619de8d6b..d61c5f074 100644
--- a/pandas/io/tests/test_sql.py
+++ b/pandas/io/tests/test_sql.py
@@ -1718,12 +1718,14 @@ class _TestPostgreSQLAlchemy(object):
             tm.assert_frame_equal(res1, res2)
 
     def test_datetime_with_time_zone(self):
+
         # Test to see if we read the date column with timezones that
         # the timezone information is converted to utc and into a
         # np.datetime64 (GH #7139)
+
         df = sql.read_sql_table("types_test_data", self.conn)
         self.assertTrue(issubclass(df.DateColWithTz.dtype.type, np.datetime64),
-                        "DateColWithTz loaded with incorrect type")
+                        "DateColWithTz loaded with incorrect type -> {0}".format(df.DateColWithTz.dtype))
 
         # "2000-01-01 00:00:00-08:00" should convert to "2000-01-01 08:00:00"
         self.assertEqual(df.DateColWithTz[0], Timestamp('2000-01-01 08:00:00'))
diff --git a/pandas/lib.pyx b/pandas/lib.pyx
index 07f0c8953..75b25c7a8 100644
--- a/pandas/lib.pyx
+++ b/pandas/lib.pyx
@@ -1732,7 +1732,7 @@ cdef class BlockPlacement:
             self._as_array = arr
             self._has_array = True
 
-    def __unicode__(self):
+    def __str__(self):
         cdef slice s = self._ensure_has_slice()
         if s is not None:
             v = self._as_slice
@@ -1741,6 +1741,8 @@ cdef class BlockPlacement:
 
         return '%s(%r)' % (self.__class__.__name__, v)
 
+    __repr__ = __str__
+
     def __len__(self):
         cdef slice s = self._ensure_has_slice()
         if s is not None:
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index 83278fe12..f1799eb99 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -115,7 +115,7 @@ class SparseDataFrame(DataFrame):
                                           index=index,
                                           kind=self._default_kind,
                                           fill_value=self._default_fill_value)
-            mgr = df_to_manager(data, columns, index)
+            mgr = to_manager(data, columns, index)
             if dtype is not None:
                 mgr = mgr.astype(dtype)
 
@@ -181,7 +181,7 @@ class SparseDataFrame(DataFrame):
             if c not in sdict:
                 sdict[c] = sp_maker(nan_vec)
 
-        return df_to_manager(sdict, columns, index)
+        return to_manager(sdict, columns, index)
 
     def _init_matrix(self, data, index, columns, dtype=None):
         data = _prep_ndarray(data, copy=False)
@@ -233,7 +233,7 @@ class SparseDataFrame(DataFrame):
             series_dict[col] = SparseSeries(sp_values, sparse_index=sp_index,
                                             fill_value=fv)
 
-        self._data = df_to_manager(series_dict, columns, index)
+        self._data = to_manager(series_dict, columns, index)
         self._default_fill_value = fv
         self._default_kind = kind
 
@@ -737,7 +737,7 @@ class SparseDataFrame(DataFrame):
         """
         return self.apply(lambda x: lmap(func, x))
 
-def df_to_manager(sdf, columns, index):
+def to_manager(sdf, columns, index):
     """ create and return the block manager from a dataframe of series, columns, index """
 
     # from BlockManager perspective
diff --git a/pandas/sparse/scipy_sparse.py b/pandas/sparse/scipy_sparse.py
index da079a978..a815ca754 100644
--- a/pandas/sparse/scipy_sparse.py
+++ b/pandas/sparse/scipy_sparse.py
@@ -30,7 +30,7 @@ def _to_ijv(ss, row_levels=(0,), column_levels=(1,), sort_labels=False):
     _check_is_partition([row_levels, column_levels], range(ss.index.nlevels))
 
     # from the SparseSeries: get the labels and data for non-null entries
-    values = ss._data.values._valid_sp_values
+    values = ss._data.internal_values()._valid_sp_values
 
     nonnull_labels = ss.dropna()
 
diff --git a/pandas/sparse/series.py b/pandas/sparse/series.py
index 420cf5093..96d509ed9 100644
--- a/pandas/sparse/series.py
+++ b/pandas/sparse/series.py
@@ -219,15 +219,15 @@ class SparseSeries(Series):
     @property
     def values(self):
         """ return the array """
-        return self._data._values
+        return self.block.values
 
     def __array__(self, result=None):
         """ the array interface, return my values """
-        return self._data._values
+        return self.block.values
 
     def get_values(self):
         """ same as values """
-        return self._data._values.to_dense().view()
+        return self.block.to_dense().view()
 
     @property
     def block(self):
diff --git a/pandas/src/inference.pyx b/pandas/src/inference.pyx
index 9ee5a753a..74bd43737 100644
--- a/pandas/src/inference.pyx
+++ b/pandas/src/inference.pyx
@@ -73,13 +73,11 @@ except AttributeError:
 cdef _try_infer_map(v):
     """ if its in our map, just return the dtype """
     cdef:
-        object val_name, val_kind
-    val_name = v.dtype.name
-    if val_name in _TYPE_MAP:
-        return _TYPE_MAP[val_name]
-    val_kind = v.dtype.kind
-    if val_kind in _TYPE_MAP:
-       return _TYPE_MAP[val_kind]
+        object attr, val
+    for attr in ['name','kind','base']:
+        val = getattr(v.dtype,attr)
+        if val in _TYPE_MAP:
+            return _TYPE_MAP[val]
     return None
 
 def infer_dtype(object _values):
@@ -99,7 +97,7 @@ def infer_dtype(object _values):
         # this will handle ndarray-like
         # e.g. categoricals
         try:
-            values = getattr(_values, 'values', _values)
+            values = getattr(_values, '_values', getattr(_values, 'values', _values))
         except:
             val = _try_infer_map(_values)
             if val is not None:
diff --git a/pandas/tests/test_base.py b/pandas/tests/test_base.py
index 7e8905f4f..fb255f300 100644
--- a/pandas/tests/test_base.py
+++ b/pandas/tests/test_base.py
@@ -7,6 +7,7 @@ import pandas.compat as compat
 import pandas as pd
 from pandas.compat import u, StringIO
 from pandas.core.base import FrozenList, FrozenNDArray, PandasDelegate
+import pandas.core.common as com
 from pandas.tseries.base import DatetimeIndexOpsMixin
 from pandas.util.testing import assertRaisesRegexp, assertIsInstance
 from pandas.tseries.common import is_datetimelike
@@ -315,9 +316,10 @@ class TestIndexOps(Ops):
         for o in self.objs:
 
             # check that we work
-            for p in ['shape', 'dtype', 'base', 'flags', 'T',
+            for p in ['shape', 'dtype', 'flags', 'T',
                       'strides', 'itemsize', 'nbytes']:
                 self.assertIsNotNone(getattr(o, p, None))
+            self.assertTrue(hasattr(o, 'base'))
 
             # if we have a datetimelike dtype then needs a view to work
             # but the user is responsible for that
@@ -401,22 +403,35 @@ class TestIndexOps(Ops):
                 # freq must be specified because repeat makes freq ambiguous
 
                 # resets name from Index
-                expected_index = pd.Index(o[::-1], name=None)
+                expected_index = pd.Index(o[::-1])
 
                 # attach name to klass
-                o = klass(np.repeat(values, range(1, len(o) + 1)), freq=o.freq, name='a')
+                o = o.repeat(range(1, len(o) + 1))
+                o.name = 'a'
+
+            elif isinstance(o, DatetimeIndex):
+
+                # resets name from Index
+                expected_index = pd.Index(o[::-1])
+
+                # attach name to klass
+                o = o.repeat(range(1, len(o) + 1))
+                o.name = 'a'
+
             # don't test boolean
             elif isinstance(o,Index) and o.is_boolean():
                 continue
             elif isinstance(o, Index):
-                expected_index = pd.Index(values[::-1], name=None)
-                o = klass(np.repeat(values, range(1, len(o) + 1)), name='a')
+                expected_index = pd.Index(values[::-1])
+                o = o.repeat(range(1, len(o) + 1))
+                o.name = 'a'
             else:
-                expected_index = pd.Index(values[::-1], name=None)
-                idx = np.repeat(o.index.values, range(1, len(o) + 1))
+                expected_index = pd.Index(values[::-1])
+                idx = o.index.repeat(range(1, len(o) + 1))
                 o = klass(np.repeat(values, range(1, len(o) + 1)), index=idx, name='a')
 
             expected_s = Series(range(10, 0, -1), index=expected_index, dtype='int64', name='a')
+
             result = o.value_counts()
             tm.assert_series_equal(result, expected_s)
             self.assertTrue(result.index.name is None)
@@ -447,7 +462,16 @@ class TestIndexOps(Ops):
                     continue
 
                 # special assign to the numpy array
-                if o.values.dtype == 'datetime64[ns]' or isinstance(o, PeriodIndex):
+                if com.is_datetimetz(o):
+                    if isinstance(o, DatetimeIndex):
+                        v = o.asi8
+                        v[0:2] = pd.tslib.iNaT
+                        values = o._shallow_copy(v)
+                    else:
+                        o = o.copy()
+                        o[0:2] = pd.tslib.iNaT
+                        values = o.values
+                elif o.values.dtype == 'datetime64[ns]' or isinstance(o, PeriodIndex):
                     values[0:2] = pd.tslib.iNaT
                 else:
                     values[0:2] = null_obj
@@ -558,17 +582,19 @@ class TestIndexOps(Ops):
             self.assertEqual(s.nunique(), 0)
 
             # GH 3002, datetime64[ns]
+            # don't test names though
             txt = "\n".join(['xxyyzz20100101PIE', 'xxyyzz20100101GUM', 'xxyyzz20100101EGG',
                              'xxyyww20090101EGG', 'foofoo20080909PIE', 'foofoo20080909GUM'])
             f = StringIO(txt)
             df = pd.read_fwf(f, widths=[6, 8, 3], names=["person_id", "dt", "food"],
                              parse_dates=["dt"])
 
-            s = klass(df['dt'].copy(), name='dt')
+            s = klass(df['dt'].copy())
+            s.name = None
 
             idx = pd.to_datetime(['2010-01-01 00:00:00Z', '2008-09-09 00:00:00Z',
                                   '2009-01-01 00:00:00X'])
-            expected_s = Series([3, 2, 1], index=idx, name='dt')
+            expected_s = Series([3, 2, 1], index=idx)
             tm.assert_series_equal(s.value_counts(), expected_s)
 
             expected = np.array(['2010-01-01 00:00:00Z', '2009-01-01 00:00:00Z',
@@ -583,7 +609,7 @@ class TestIndexOps(Ops):
 
             # with NaT
             s = df['dt'].copy()
-            s = klass([v for v in s.values] + [pd.NaT], name='dt')
+            s = klass([v for v in s.values] + [pd.NaT])
 
             result = s.value_counts()
             self.assertEqual(result.index.dtype, 'datetime64[ns]')
@@ -595,6 +621,7 @@ class TestIndexOps(Ops):
 
             unique = s.unique()
             self.assertEqual(unique.dtype, 'datetime64[ns]')
+
             # numpy_array_equal cannot compare pd.NaT
             self.assert_numpy_array_equal(unique[:3], expected)
             self.assertTrue(unique[3] is pd.NaT or unique[3].astype('int64') == pd.tslib.iNaT)
@@ -753,7 +780,7 @@ class TestIndexOps(Ops):
                 self.assertFalse(result is original)
 
                 idx = original.index[list(range(len(original))) + [5, 3]]
-                values = original.values[list(range(len(original))) + [5, 3]]
+                values = original._values[list(range(len(original))) + [5, 3]]
                 s = Series(values, index=idx, name='a')
 
                 expected = Series([False] * len(original) + [True, True],
diff --git a/pandas/tests/test_categorical.py b/pandas/tests/test_categorical.py
index f687ecbef..9173c0a87 100755
--- a/pandas/tests/test_categorical.py
+++ b/pandas/tests/test_categorical.py
@@ -1284,24 +1284,6 @@ class TestCategoricalAsBlock(tm.TestCase):
 
     def test_dtypes(self):
 
-        dtype = com.CategoricalDtype()
-        hash(dtype)
-        self.assertTrue(com.is_categorical_dtype(dtype))
-
-        s = Series(self.factor,name='A')
-
-        # dtypes
-        self.assertTrue(com.is_categorical_dtype(s.dtype))
-        self.assertTrue(com.is_categorical_dtype(s))
-        self.assertFalse(com.is_categorical_dtype(np.dtype('float64')))
-
-        # np.dtype doesn't know about our new dtype
-        def f():
-            np.dtype(dtype)
-        self.assertRaises(TypeError, f)
-
-        self.assertFalse(dtype == np.str_)
-        self.assertFalse(np.str_ == dtype)
 
         # GH8143
         index = ['cat','obj','num']
@@ -1830,16 +1812,14 @@ Categories (5, datetime64[ns]): [2011-01-01 09:00:00, 2011-01-01 10:00:00, 2011-
         idx = pd.date_range('2011-01-01 09:00', freq='H', periods=5, tz='US/Eastern')
         c = pd.Categorical(idx)
         exp = """[2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00]
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,
-                                 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,\n                                             2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,\n                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(c), exp)
 
         c = pd.Categorical(idx.append(idx), categories=idx)
         exp = """[2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00, 2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00]
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,
-                                 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,
+                                             2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,
+                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(c), exp)
 
     def test_categorical_repr_datetime_ordered(self):
@@ -1859,16 +1839,16 @@ Categories (5, datetime64[ns]): [2011-01-01 09:00:00 < 2011-01-01 10:00:00 < 201
         idx = pd.date_range('2011-01-01 09:00', freq='H', periods=5, tz='US/Eastern')
         c = pd.Categorical(idx, ordered=True)
         exp = """[2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00]
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
-                                 2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
+                                             2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
+                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(c), exp)
 
         c = pd.Categorical(idx.append(idx), categories=idx, ordered=True)
         exp = """[2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00, 2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00, 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00, 2011-01-01 13:00:00-05:00]
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
-                                 2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
+                                             2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
+                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(c), exp)
 
     def test_categorical_repr_period(self):
@@ -2048,9 +2028,9 @@ Categories (5, datetime64[ns]): [2011-01-01 09:00:00, 2011-01-01 10:00:00, 2011-
 3   2011-01-01 12:00:00-05:00
 4   2011-01-01 13:00:00-05:00
 dtype: category
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,
-                                 2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00, 2011-01-01 10:00:00-05:00,
+                                             2011-01-01 11:00:00-05:00, 2011-01-01 12:00:00-05:00,
+                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(s), exp)
 
     def test_categorical_series_repr_datetime_ordered(self):
@@ -2074,9 +2054,9 @@ Categories (5, datetime64[ns]): [2011-01-01 09:00:00 < 2011-01-01 10:00:00 < 201
 3   2011-01-01 12:00:00-05:00
 4   2011-01-01 13:00:00-05:00
 dtype: category
-Categories (5, datetime64[ns]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
-                                 2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
-                                 2011-01-01 13:00:00-05:00]"""
+Categories (5, datetime64[ns, US/Eastern]): [2011-01-01 09:00:00-05:00 < 2011-01-01 10:00:00-05:00 <
+                                             2011-01-01 11:00:00-05:00 < 2011-01-01 12:00:00-05:00 <
+                                             2011-01-01 13:00:00-05:00]"""
         self.assertEqual(repr(s), exp)
 
     def test_categorical_series_repr_period(self):
diff --git a/pandas/tests/test_dtypes.py b/pandas/tests/test_dtypes.py
new file mode 100644
index 000000000..54a49de58
--- /dev/null
+++ b/pandas/tests/test_dtypes.py
@@ -0,0 +1,142 @@
+# -*- coding: utf-8 -*-
+
+import nose
+import numpy as np
+from pandas import Series, Categorical, date_range
+import pandas.core.common as com
+from pandas.core.common import (CategoricalDtype, is_categorical_dtype, is_categorical,
+                                DatetimeTZDtype, is_datetime64tz_dtype, is_datetimetz,
+                                is_dtype_equal, is_datetime64_ns_dtype, is_datetime64_dtype)
+import pandas.util.testing as tm
+
+_multiprocess_can_split_ = True
+
+class Base(object):
+
+    def test_hash(self):
+        hash(self.dtype)
+
+    def test_equality_invalid(self):
+        self.assertRaises(self.dtype == 'foo')
+
+    def test_numpy_informed(self):
+
+        # np.dtype doesn't know about our new dtype
+        def f():
+            np.dtype(self.dtype)
+        self.assertRaises(TypeError, f)
+
+        self.assertNotEqual(self.dtype, np.str_)
+        self.assertNotEqual(np.str_, self.dtype)
+
+    def test_pickle(self):
+        result = self.round_trip_pickle(self.dtype)
+        self.assertEqual(result, self.dtype)
+
+class TestCategoricalDtype(Base, tm.TestCase):
+
+    def setUp(self):
+        self.dtype = CategoricalDtype()
+
+    def test_equality(self):
+        self.assertTrue(is_dtype_equal(self.dtype, 'category'))
+        self.assertTrue(is_dtype_equal(self.dtype, CategoricalDtype()))
+        self.assertFalse(is_dtype_equal(self.dtype, 'foo'))
+
+    def test_construction_from_string(self):
+        result = CategoricalDtype.construct_from_string('category')
+        self.assertTrue(is_dtype_equal(self.dtype, result))
+        self.assertRaises(TypeError, lambda : CategoricalDtype.construct_from_string('foo'))
+
+    def test_is_dtype(self):
+        self.assertTrue(CategoricalDtype.is_dtype(self.dtype))
+        self.assertTrue(CategoricalDtype.is_dtype('category'))
+        self.assertTrue(CategoricalDtype.is_dtype(CategoricalDtype()))
+        self.assertFalse(CategoricalDtype.is_dtype('foo'))
+        self.assertFalse(CategoricalDtype.is_dtype(np.float64))
+
+    def test_basic(self):
+
+        self.assertTrue(is_categorical_dtype(self.dtype))
+
+        factor = Categorical.from_array(['a', 'b', 'b', 'a',
+                                         'a', 'c', 'c', 'c'])
+
+        s = Series(factor,name='A')
+
+        # dtypes
+        self.assertTrue(is_categorical_dtype(s.dtype))
+        self.assertTrue(is_categorical_dtype(s))
+        self.assertFalse(is_categorical_dtype(np.dtype('float64')))
+
+        self.assertTrue(is_categorical(s.dtype))
+        self.assertTrue(is_categorical(s))
+        self.assertFalse(is_categorical(np.dtype('float64')))
+        self.assertFalse(is_categorical(1.0))
+
+class TestDatetimeTZDtype(Base, tm.TestCase):
+
+    def setUp(self):
+        self.dtype = DatetimeTZDtype('ns','US/Eastern')
+
+    def test_construction(self):
+        self.assertRaises(ValueError, lambda : DatetimeTZDtype('ms','US/Eastern'))
+
+    def test_subclass(self):
+        a = DatetimeTZDtype('datetime64[ns, US/Eastern]')
+        b = DatetimeTZDtype('datetime64[ns, CET]')
+
+        self.assertTrue(issubclass(type(a), type(a)))
+        self.assertTrue(issubclass(type(a), type(b)))
+
+    def test_compat(self):
+        self.assertFalse(is_datetime64_ns_dtype(self.dtype))
+        self.assertFalse(is_datetime64_ns_dtype('datetime64[ns, US/Eastern]'))
+        self.assertFalse(is_datetime64_dtype(self.dtype))
+        self.assertFalse(is_datetime64_dtype('datetime64[ns, US/Eastern]'))
+
+    def test_construction_from_string(self):
+        result = DatetimeTZDtype('datetime64[ns, US/Eastern]')
+        self.assertTrue(is_dtype_equal(self.dtype, result))
+        result = DatetimeTZDtype.construct_from_string('datetime64[ns, US/Eastern]')
+        self.assertTrue(is_dtype_equal(self.dtype, result))
+        self.assertRaises(TypeError, lambda : DatetimeTZDtype.construct_from_string('foo'))
+
+    def test_is_dtype(self):
+        self.assertTrue(DatetimeTZDtype.is_dtype(self.dtype))
+        self.assertTrue(DatetimeTZDtype.is_dtype('datetime64[ns, US/Eastern]'))
+        self.assertFalse(DatetimeTZDtype.is_dtype('foo'))
+        self.assertTrue(DatetimeTZDtype.is_dtype(DatetimeTZDtype('ns','US/Pacific')))
+        self.assertFalse(DatetimeTZDtype.is_dtype(np.float64))
+
+    def test_equality(self):
+        self.assertTrue(is_dtype_equal(self.dtype, 'datetime64[ns, US/Eastern]'))
+        self.assertTrue(is_dtype_equal(self.dtype, DatetimeTZDtype('ns','US/Eastern')))
+        self.assertFalse(is_dtype_equal(self.dtype, 'foo'))
+        self.assertFalse(is_dtype_equal(self.dtype, DatetimeTZDtype('ns','CET')))
+        self.assertFalse(is_dtype_equal(DatetimeTZDtype('ns','US/Eastern'), DatetimeTZDtype('ns','US/Pacific')))
+
+        # numpy compat
+        self.assertTrue(is_dtype_equal(np.dtype("M8[ns]"),"datetime64[ns]"))
+
+    def test_basic(self):
+
+        self.assertTrue(is_datetime64tz_dtype(self.dtype))
+
+        dr = date_range('20130101',periods=3,tz='US/Eastern')
+        s = Series(dr,name='A')
+
+        # dtypes
+        self.assertTrue(is_datetime64tz_dtype(s.dtype))
+        self.assertTrue(is_datetime64tz_dtype(s))
+        self.assertFalse(is_datetime64tz_dtype(np.dtype('float64')))
+        self.assertFalse(is_datetime64tz_dtype(1.0))
+
+        self.assertTrue(is_datetimetz(s))
+        self.assertTrue(is_datetimetz(s.dtype))
+        self.assertFalse(is_datetimetz(np.dtype('float64')))
+        self.assertFalse(is_datetimetz(1.0))
+
+if __name__ == '__main__':
+    nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],
+                   exit=False)
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index 5ecb9fc66..24de36d95 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -35,6 +35,7 @@ from pandas import (DataFrame, Index, Series, Panel, notnull, isnull,
                     MultiIndex, DatetimeIndex, Timestamp, date_range,
                     read_csv, timedelta_range, Timedelta, CategoricalIndex,
                     option_context)
+from pandas.core.dtypes import DatetimeTZDtype
 import pandas as pd
 from pandas.parser import CParserError
 from pandas.util.misc import is_little_endian
@@ -2254,6 +2255,11 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
         self.all_mixed    = DataFrame({'a': 1., 'b': 2, 'c': 'foo', 'float32' : np.array([1.]*10,dtype='float32'),
                                        'int32' : np.array([1]*10,dtype='int32'),
                                        }, index=np.arange(10))
+        self.tzframe = DataFrame({'A' : date_range('20130101',periods=3),
+                                  'B' : date_range('20130101',periods=3,tz='US/Eastern'),
+                                  'C' : date_range('20130101',periods=3,tz='CET')})
+        self.tzframe.iloc[1,1] = pd.NaT
+        self.tzframe.iloc[1,2] = pd.NaT
 
         self.ts1 = tm.makeTimeSeries()
         self.ts2 = tm.makeTimeSeries()[5:]
@@ -4080,13 +4086,14 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
         import pytz
         tz = pytz.timezone('US/Eastern')
         dt = tz.localize(datetime(2012, 1, 1))
+
         df = DataFrame({'End Date': dt}, index=[0])
         self.assertEqual(df.iat[0,0],dt)
-        assert_series_equal(df.dtypes,Series({'End Date' : np.dtype('object') }))
+        assert_series_equal(df.dtypes,Series({'End Date' : 'datetime64[ns, US/Eastern]' }))
 
         df = DataFrame([{'End Date': dt}])
         self.assertEqual(df.iat[0,0],dt)
-        assert_series_equal(df.dtypes,Series({'End Date' : np.dtype('object') }))
+        assert_series_equal(df.dtypes,Series({'End Date' : 'datetime64[ns, US/Eastern]' }))
 
         # tz-aware (UTC and other tz's)
         # GH 8411
@@ -4118,6 +4125,183 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
         expected = DataFrame( {'a' : i.to_series(keep_tz=True).reset_index(drop=True), 'b': i_no_tz })
         assert_frame_equal(df, expected)
 
+    def test_constructor_with_datetime_tz(self):
+
+        # 8260
+        # support datetime64 with tz
+
+        idx = Index(date_range('20130101',periods=3,tz='US/Eastern'),
+                    name='foo')
+        dr = date_range('20130110',periods=3)
+
+        # construction
+        df = DataFrame({'A' : idx, 'B' : dr})
+        self.assertTrue(df['A'].dtype,'M8[ns, US/Eastern')
+        self.assertTrue(df['A'].name == 'A')
+        assert_series_equal(df['A'],Series(idx,name='A'))
+        assert_series_equal(df['B'],Series(dr,name='B'))
+
+        # construction from dict
+        df2 = DataFrame(dict(A=Timestamp('20130102', tz='US/Eastern'), B=Timestamp('20130603', tz='CET')), index=range(5))
+        assert_series_equal(df2.dtypes, Series(['datetime64[ns, US/Eastern]', 'datetime64[ns, CET]'], index=['A','B']))
+
+        # dtypes
+        tzframe = DataFrame({'A' : date_range('20130101',periods=3),
+                             'B' : date_range('20130101',periods=3,tz='US/Eastern'),
+                             'C' : date_range('20130101',periods=3,tz='CET')})
+        tzframe.iloc[1,1] = pd.NaT
+        tzframe.iloc[1,2] = pd.NaT
+        result = tzframe.dtypes.sort_index()
+        expected = Series([ np.dtype('datetime64[ns]'),
+                            DatetimeTZDtype('datetime64[ns, US/Eastern]'),
+                            DatetimeTZDtype('datetime64[ns, CET]') ],
+                          ['A','B','C'])
+
+        # concat
+        df3 = pd.concat([df2.A.to_frame(),df2.B.to_frame()],axis=1)
+        assert_frame_equal(df2, df3)
+
+        # select_dtypes
+        result = df3.select_dtypes(include=['datetime64[ns]'])
+        expected = df3.reindex(columns=[])
+        assert_frame_equal(result, expected)
+
+        # this will select based on issubclass, and these are the same class
+        result = df3.select_dtypes(include=['datetime64[ns, CET]'])
+        expected = df3
+        assert_frame_equal(result, expected)
+
+        # from index
+        idx2 = date_range('20130101',periods=3,tz='US/Eastern',name='foo')
+        df2 = DataFrame(idx2)
+        assert_series_equal(df2['foo'],Series(idx2,name='foo'))
+        df2 = DataFrame(Series(idx2))
+        assert_series_equal(df2['foo'],Series(idx2,name='foo'))
+
+        idx2 = date_range('20130101',periods=3,tz='US/Eastern')
+        df2 = DataFrame(idx2)
+        assert_series_equal(df2[0],Series(idx2,name=0))
+        df2 = DataFrame(Series(idx2))
+        assert_series_equal(df2[0],Series(idx2,name=0))
+
+        # interleave with object
+        result = self.tzframe.assign(D = 'foo').values
+        expected = np.array([[Timestamp('2013-01-01 00:00:00'),
+                              Timestamp('2013-01-02 00:00:00'),
+                              Timestamp('2013-01-03 00:00:00')],
+                             [Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00-0500', tz='US/Eastern')],
+                             [Timestamp('2013-01-01 00:00:00+0100', tz='CET'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00+0100', tz='CET')],
+                             ['foo','foo','foo']], dtype=object).T
+        self.assert_numpy_array_equal(result, expected)
+
+        # interleave with only datetime64[ns]
+        result = self.tzframe.values
+        expected = np.array([[Timestamp('2013-01-01 00:00:00'),
+                              Timestamp('2013-01-02 00:00:00'),
+                              Timestamp('2013-01-03 00:00:00')],
+                             [Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00-0500', tz='US/Eastern')],
+                             [Timestamp('2013-01-01 00:00:00+0100', tz='CET'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00+0100', tz='CET')]], dtype=object).T
+        self.assert_numpy_array_equal(result, expected)
+
+        # astype
+        expected = np.array([[Timestamp('2013-01-01 00:00:00'),
+                              Timestamp('2013-01-02 00:00:00'),
+                              Timestamp('2013-01-03 00:00:00')],
+                             [Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00-0500', tz='US/Eastern')],
+                             [Timestamp('2013-01-01 00:00:00+0100', tz='CET'),
+                              pd.NaT,
+                              Timestamp('2013-01-03 00:00:00+0100', tz='CET')]], dtype=object).T
+        result = self.tzframe.astype(object)
+        assert_frame_equal(result, DataFrame(expected, index=self.tzframe.index, columns=self.tzframe.columns))
+
+        result = self.tzframe.astype('datetime64[ns]')
+        expected = DataFrame({'A' : date_range('20130101',periods=3),
+                              'B' : date_range('20130101',periods=3,tz='US/Eastern').tz_convert('UTC').tz_localize(None),
+                              'C' : date_range('20130101',periods=3,tz='CET').tz_convert('UTC').tz_localize(None)})
+        expected.iloc[1,1] = pd.NaT
+        expected.iloc[1,2] = pd.NaT
+        assert_frame_equal(result, expected)
+
+        # str formatting
+        result = self.tzframe.astype(str)
+        expected = np.array([['2013-01-01', '2013-01-01 00:00:00-05:00',
+                              '2013-01-01 00:00:00+01:00'],
+                             ['2013-01-02', 'NaT', 'NaT'],
+                             ['2013-01-03', '2013-01-03 00:00:00-05:00',
+                              '2013-01-03 00:00:00+01:00']], dtype=object)
+        self.assert_numpy_array_equal(result, expected)
+
+        result = str(self.tzframe)
+        self.assertTrue('0 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00+01:00' in result)
+        self.assertTrue('1 2013-01-02                       NaT                       NaT' in result)
+        self.assertTrue('2 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-03 00:00:00+01:00' in result)
+
+        # setitem
+        df['C'] = idx
+        assert_series_equal(df['C'],Series(idx,name='C'))
+
+        df['D'] = 'foo'
+        df['D'] = idx
+        assert_series_equal(df['D'],Series(idx,name='D'))
+        del df['D']
+
+        # assert that A & C are not sharing the same base (e.g. they
+        # are copies)
+        b1 = df._data.blocks[1]
+        b2 = df._data.blocks[2]
+        self.assertTrue(b1.values.equals(b2.values))
+        self.assertFalse(id(b1.values.values.base) == id(b2.values.values.base))
+
+        # with nan
+        df2 = df.copy()
+        df2.iloc[1,1] = pd.NaT
+        df2.iloc[1,2] = pd.NaT
+        result = df2['B']
+        assert_series_equal(notnull(result), Series([True,False,True],name='B'))
+        assert_series_equal(df2.dtypes, df.dtypes)
+
+        # set/reset
+        df = DataFrame({'A' : [0,1,2] }, index=idx)
+        result = df.reset_index()
+        self.assertTrue(result['foo'].dtype,'M8[ns, US/Eastern')
+
+        result = result.set_index('foo')
+        tm.assert_index_equal(df.index,idx)
+
+        # indexing
+        result = df2.iloc[1]
+        expected = Series([Timestamp('2013-01-02 00:00:00-0500', tz='US/Eastern'), np.nan, np.nan],
+                          index=list('ABC'), dtype='object', name=1)
+        assert_series_equal(result, expected)
+        result = df2.loc[1]
+        expected = Series([Timestamp('2013-01-02 00:00:00-0500', tz='US/Eastern'), np.nan, np.nan],
+                          index=list('ABC'), dtype='object', name=1)
+        assert_series_equal(result, expected)
+
+        # indexing - fast_xs
+        df = DataFrame({'a': date_range('2014-01-01', periods=10, tz='UTC')})
+        result = df.iloc[5]
+        expected = Timestamp('2014-01-06 00:00:00+0000', tz='UTC', offset='D')
+        self.assertEqual(result, expected)
+
+        result = df.loc[5]
+        self.assertEqual(result, expected)
+
+        # indexing - boolean
+        result = df[df.a > df.a[3]]
+        expected = df.iloc[4:]
+        assert_frame_equal(result, expected)
+
     def test_constructor_for_list_with_dtypes(self):
         intname = np.dtype(np.int_).name
         floatname = np.dtype(np.float_).name
@@ -4422,11 +4606,11 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
             result = df.astype(tt)
 
             expected = DataFrame({
-                'a' : list(map(tt, a.values)),
-                'b' : list(map(tt, b.values)),
-                'c' : list(map(tt, c.values)),
-                'd' : list(map(tt, d.values)),
-                'e' : list(map(tt, e.values)),
+                'a' : list(map(tt, map(lambda x: Timestamp(x)._date_repr, a._values))),
+                'b' : list(map(tt, map(Timestamp, b._values))),
+                'c' : list(map(tt, map(lambda x: Timedelta(x)._repr_base(format='all'), c._values))),
+                'd' : list(map(tt, d._values)),
+                'e' : list(map(tt, e._values)),
                 })
 
             assert_frame_equal(result, expected)
@@ -4450,6 +4634,10 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
         unpickled = self.round_trip_pickle(self.empty)
         repr(unpickled)
 
+        # tz frame
+        unpickled = self.round_trip_pickle(self.tzframe)
+        assert_frame_equal(self.tzframe, unpickled)
+
     def test_to_dict(self):
         test_data = {
             'A': {'1': 1, '2': 2},
@@ -6382,6 +6570,17 @@ class TestDataFrame(tm.TestCase, CheckIndexing,
 
             assert_frame_equal(df, result, check_index_type=True)
 
+        # tz, 8260
+        with ensure_clean(pname) as path:
+
+            self.tzframe.to_csv(path)
+            result = pd.read_csv(path, index_col=0, parse_dates=['A'])
+
+            converter = lambda c: pd.to_datetime(result[c]).dt.tz_localize('UTC').dt.tz_convert(self.tzframe[c].dt.tz)
+            result['B'] = converter('B')
+            result['C'] = converter('C')
+            assert_frame_equal(result, self.tzframe)
+
     def test_to_csv_cols_reordering(self):
         # GH3454
         import pandas as pd
@@ -15211,8 +15410,10 @@ starting,ending,measure
         self.assertEqual(df[['X']].testattr, 'XXX')
         self.assertEqual(df.loc[['a', 'b'], :].testattr, 'XXX')
         self.assertEqual(df.iloc[[0, 1], :].testattr, 'XXX')
+
         # GH9776
         self.assertEqual(df.iloc[0:1, :].testattr, 'XXX')
+
         # GH10553
         unpickled = self.round_trip_pickle(df)
         assert_frame_equal(df, unpickled)
diff --git a/pandas/tests/test_index.py b/pandas/tests/test_index.py
index 9b2c1bf1a..6e7a72360 100644
--- a/pandas/tests/test_index.py
+++ b/pandas/tests/test_index.py
@@ -573,7 +573,7 @@ class TestIndex(Base, tm.TestCase):
         # corner case
         self.assertRaises(TypeError, Index, 0)
 
-    def test_consruction_list_mixed_tuples(self):
+    def test_construction_list_mixed_tuples(self):
         # 10697
         # if we are constructing from a mixed list of tuples, make sure that we
         # are independent of the sorting order
@@ -2861,9 +2861,7 @@ class DatetimeLike(Base):
 
         if hasattr(idx,'tz'):
             if idx.tz is not None:
-                self.assertTrue("tz='%s'" % idx.tz in str(idx))
-            else:
-                self.assertTrue("tz=None" in str(idx))
+                self.assertTrue(idx.tz in str(idx))
         if hasattr(idx,'freq'):
             self.assertTrue("freq='%s'" % idx.freqstr in str(idx))
 
@@ -2891,6 +2889,24 @@ class TestDatetimeIndex(DatetimeLike, tm.TestCase):
     def create_index(self):
         return date_range('20130101', periods=5)
 
+    def test_construction_with_alt(self):
+
+        i = pd.date_range('20130101',periods=5,freq='H',tz='US/Eastern')
+        i2 = DatetimeIndex(i, dtype=i.dtype)
+        self.assert_index_equal(i, i2)
+
+        i2 = DatetimeIndex(i.tz_localize(None).asi8, tz=i.dtype.tz)
+        self.assert_index_equal(i, i2)
+
+        i2 = DatetimeIndex(i.tz_localize(None).asi8, dtype=i.dtype)
+        self.assert_index_equal(i, i2)
+
+        i2 = DatetimeIndex(i.tz_localize(None).asi8, dtype=i.dtype, tz=i.dtype.tz)
+        self.assert_index_equal(i, i2)
+
+        # incompat tz/dtype
+        self.assertRaises(ValueError, lambda : DatetimeIndex(i.tz_localize(None).asi8, dtype=i.dtype, tz='US/Pacific'))
+
     def test_pickle_compat_construction(self):
         pass
 
diff --git a/pandas/tests/test_internals.py b/pandas/tests/test_internals.py
index 7c51641b8..61966674b 100644
--- a/pandas/tests/test_internals.py
+++ b/pandas/tests/test_internals.py
@@ -6,7 +6,8 @@ from datetime import datetime, date
 import nose
 import numpy as np
 
-from pandas import Index, MultiIndex, DataFrame, Series, Categorical
+import re
+from pandas import Index, MultiIndex, DataFrame, DatetimeIndex, Series, Categorical
 from pandas.compat import OrderedDict, lrange
 from pandas.sparse.array import SparseArray
 from pandas.core.internals import *
@@ -44,7 +45,7 @@ def create_block(typestr, placement, item_shape=None, num_offset=0):
         * complex, c16, c8
         * bool
         * object, string, O
-        * datetime, dt, M8[ns]
+        * datetime, dt, M8[ns], M8[ns, tz]
         * timedelta, td, m8[ns]
         * sparse (SparseArray with fill_value=0.0)
         * sparse_na (SparseArray with fill_value=np.nan)
@@ -74,6 +75,13 @@ def create_block(typestr, placement, item_shape=None, num_offset=0):
         values = np.ones(shape, dtype=np.bool_)
     elif typestr in ('datetime', 'dt', 'M8[ns]'):
         values = (mat * 1e9).astype('M8[ns]')
+    elif typestr.startswith('M8[ns'):
+        # datetime with tz
+        m = re.search('M8\[ns,\s*(\w+\/?\w*)\]', typestr)
+        assert m is not None, "incompatible typestr -> {0}".format(typestr)
+        tz = m.groups()[0]
+        assert num_items == 1, "must have only 1 num items for a tz-aware"
+        values = DatetimeIndex(np.arange(N) * 1e9, tz=tz)
     elif typestr in ('timedelta', 'td', 'm8[ns]'):
         values = (mat * 1).astype('m8[ns]')
     elif typestr in ('category',):
@@ -401,7 +409,7 @@ class TestBlockManager(tm.TestCase):
                 res = self.mgr.get_scalar((item, index))
                 exp = self.mgr.get(item, fastpath=False)[i]
                 assert_almost_equal(res, exp)
-                exp = self.mgr.get(item).values[i]
+                exp = self.mgr.get(item).internal_values()[i]
                 assert_almost_equal(res, exp)
 
     def test_get(self):
@@ -414,19 +422,19 @@ class TestBlockManager(tm.TestCase):
         assert_almost_equal(mgr.get('a', fastpath=False), values[0])
         assert_almost_equal(mgr.get('b', fastpath=False), values[1])
         assert_almost_equal(mgr.get('c', fastpath=False), values[2])
-        assert_almost_equal(mgr.get('a').values, values[0])
-        assert_almost_equal(mgr.get('b').values, values[1])
-        assert_almost_equal(mgr.get('c').values, values[2])
+        assert_almost_equal(mgr.get('a').internal_values(), values[0])
+        assert_almost_equal(mgr.get('b').internal_values(), values[1])
+        assert_almost_equal(mgr.get('c').internal_values(), values[2])
 
     def test_set(self):
         mgr = create_mgr('a,b,c: int', item_shape=(3,))
 
         mgr.set('d', np.array(['foo'] * 3))
         mgr.set('b', np.array(['bar'] * 3))
-        assert_almost_equal(mgr.get('a').values, [0] * 3)
-        assert_almost_equal(mgr.get('b').values, ['bar'] * 3)
-        assert_almost_equal(mgr.get('c').values, [2] * 3)
-        assert_almost_equal(mgr.get('d').values, ['foo'] * 3)
+        assert_almost_equal(mgr.get('a').internal_values(), [0] * 3)
+        assert_almost_equal(mgr.get('b').internal_values(), ['bar'] * 3)
+        assert_almost_equal(mgr.get('c').internal_values(), [2] * 3)
+        assert_almost_equal(mgr.get('d').internal_values(), ['foo'] * 3)
 
     def test_insert(self):
         self.mgr.insert(0, 'inserted', np.arange(N))
@@ -478,7 +486,6 @@ class TestBlockManager(tm.TestCase):
 
     def test_sparse(self):
         mgr = create_mgr('a: sparse-1; b: sparse-2')
-
         # what to test here?
         self.assertEqual(mgr.as_matrix().dtype, np.float64)
 
@@ -510,6 +517,12 @@ class TestBlockManager(tm.TestCase):
         mgr = create_mgr('h: datetime-1; g: datetime-2')
         self.assertEqual(mgr.as_matrix().dtype, 'M8[ns]')
 
+    def test_as_matrix_datetime_tz(self):
+        mgr = create_mgr('h: M8[ns, US/Eastern]; g: M8[ns, CET]')
+        self.assertEqual(mgr.get('h').dtype, 'datetime64[ns, US/Eastern]')
+        self.assertEqual(mgr.get('g').dtype, 'datetime64[ns, CET]')
+        self.assertEqual(mgr.as_matrix().dtype, 'object')
+
     def test_astype(self):
         # coerce all
         mgr = create_mgr('c: f4; d: f2; e: f8')
@@ -692,10 +705,10 @@ class TestBlockManager(tm.TestCase):
         assert_almost_equal(mgr.get('c',fastpath=False), reindexed.get('c',fastpath=False))
         assert_almost_equal(mgr.get('a',fastpath=False), reindexed.get('a',fastpath=False))
         assert_almost_equal(mgr.get('d',fastpath=False), reindexed.get('d',fastpath=False))
-        assert_almost_equal(mgr.get('g').values, reindexed.get('g').values)
-        assert_almost_equal(mgr.get('c').values, reindexed.get('c').values)
-        assert_almost_equal(mgr.get('a').values, reindexed.get('a').values)
-        assert_almost_equal(mgr.get('d').values, reindexed.get('d').values)
+        assert_almost_equal(mgr.get('g').internal_values(), reindexed.get('g').internal_values())
+        assert_almost_equal(mgr.get('c').internal_values(), reindexed.get('c').internal_values())
+        assert_almost_equal(mgr.get('a').internal_values(), reindexed.get('a').internal_values())
+        assert_almost_equal(mgr.get('d').internal_values(), reindexed.get('d').internal_values())
 
     def test_multiindex_xs(self):
         mgr = create_mgr('a,b,c: f8; d,e,f: i8')
@@ -721,18 +734,18 @@ class TestBlockManager(tm.TestCase):
         numeric = mgr.get_numeric_data()
         assert_almost_equal(numeric.items, ['int', 'float', 'complex', 'bool'])
         assert_almost_equal(mgr.get('float',fastpath=False), numeric.get('float',fastpath=False))
-        assert_almost_equal(mgr.get('float').values, numeric.get('float').values)
+        assert_almost_equal(mgr.get('float').internal_values(), numeric.get('float').internal_values())
 
         # Check sharing
         numeric.set('float', np.array([100., 200., 300.]))
         assert_almost_equal(mgr.get('float',fastpath=False), np.array([100., 200., 300.]))
-        assert_almost_equal(mgr.get('float').values, np.array([100., 200., 300.]))
+        assert_almost_equal(mgr.get('float').internal_values(), np.array([100., 200., 300.]))
 
         numeric2 = mgr.get_numeric_data(copy=True)
         assert_almost_equal(numeric.items, ['int', 'float', 'complex', 'bool'])
         numeric2.set('float', np.array([1000., 2000., 3000.]))
         assert_almost_equal(mgr.get('float',fastpath=False), np.array([100., 200., 300.]))
-        assert_almost_equal(mgr.get('float').values, np.array([100., 200., 300.]))
+        assert_almost_equal(mgr.get('float').internal_values(), np.array([100., 200., 300.]))
 
     def test_get_bool_data(self):
         mgr = create_mgr('int: int; float: float; complex: complex;'
@@ -743,17 +756,17 @@ class TestBlockManager(tm.TestCase):
         bools = mgr.get_bool_data()
         assert_almost_equal(bools.items, ['bool'])
         assert_almost_equal(mgr.get('bool',fastpath=False), bools.get('bool',fastpath=False))
-        assert_almost_equal(mgr.get('bool').values, bools.get('bool').values)
+        assert_almost_equal(mgr.get('bool').internal_values(), bools.get('bool').internal_values())
 
         bools.set('bool', np.array([True, False, True]))
         assert_almost_equal(mgr.get('bool',fastpath=False), [True, False, True])
-        assert_almost_equal(mgr.get('bool').values, [True, False, True])
+        assert_almost_equal(mgr.get('bool').internal_values(), [True, False, True])
 
         # Check sharing
         bools2 = mgr.get_bool_data(copy=True)
         bools2.set('bool', np.array([False, True, False]))
         assert_almost_equal(mgr.get('bool',fastpath=False), [True, False, True])
-        assert_almost_equal(mgr.get('bool').values, [True, False, True])
+        assert_almost_equal(mgr.get('bool').internal_values(), [True, False, True])
 
     def test_unicode_repr_doesnt_raise(self):
         str_repr = repr(create_mgr(u('b,\u05d0: object')))
diff --git a/pandas/tests/test_multilevel.py b/pandas/tests/test_multilevel.py
index 1bce047f3..0f55f79b8 100644
--- a/pandas/tests/test_multilevel.py
+++ b/pandas/tests/test_multilevel.py
@@ -2188,6 +2188,21 @@ Thur,Lunch,Yes,51.51,17"""
             self.assertIsInstance(index.levels[0],pd.DatetimeIndex)
             self.assertIsInstance(index.levels[1],pd.DatetimeIndex)
 
+    def test_constructor_with_tz(self):
+
+        index = pd.DatetimeIndex(['2013/01/01 09:00', '2013/01/02 09:00'],
+                                 name='dt1', tz='US/Pacific')
+        columns = pd.DatetimeIndex(['2014/01/01 09:00', '2014/01/02 09:00'],
+                                   name='dt2', tz='Asia/Tokyo')
+
+        result = MultiIndex.from_arrays([index, columns])
+        tm.assert_index_equal(result.levels[0], index)
+        tm.assert_index_equal(result.levels[1], columns)
+
+        result = MultiIndex.from_arrays([Series(index), Series(columns)])
+        tm.assert_index_equal(result.levels[0], index)
+        tm.assert_index_equal(result.levels[1], columns)
+
     def test_set_index_datetime(self):
         # GH 3950
         df = pd.DataFrame({'label':['a', 'a', 'a', 'b', 'b', 'b'],
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index 86eafdf7c..473549d3f 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -1,6 +1,7 @@
 # coding=utf-8
 # pylint: disable-msg=E1101,W0612
 
+import re
 import sys
 from datetime import datetime, timedelta
 import operator
@@ -19,7 +20,7 @@ import numpy.ma as ma
 import pandas as pd
 
 from pandas import (Index, Series, DataFrame, isnull, notnull, bdate_range,
-                    date_range, period_range, timedelta_range)
+                    date_range, period_range, timedelta_range, _np_version_under1p8)
 from pandas.core.index import MultiIndex
 from pandas.core.indexing import IndexingError
 from pandas.tseries.period import PeriodIndex
@@ -92,7 +93,7 @@ class CheckNameIntegration(object):
         ok_for_td_methods = ['components','to_pytimedelta','total_seconds']
 
         def get_expected(s, name):
-            result = getattr(Index(s.values),prop)
+            result = getattr(Index(s._values),prop)
             if isinstance(result, np.ndarray):
                 if com.is_integer_dtype(result):
                     result = result.astype('int64')
@@ -138,6 +139,30 @@ class CheckNameIntegration(object):
             expected = Series(DatetimeIndex(s.values).tz_localize('UTC').tz_convert('US/Eastern'),index=s.index)
             tm.assert_series_equal(result, expected)
 
+        # datetimeindex with tz
+        s = Series(date_range('20130101',periods=5,tz='US/Eastern'))
+        for prop in ok_for_dt:
+
+            # we test freq below
+            if prop != 'freq':
+                compare(s, prop)
+
+        for prop in ok_for_dt_methods:
+            getattr(s.dt,prop)
+
+        result = s.dt.to_pydatetime()
+        self.assertIsInstance(result,np.ndarray)
+        self.assertTrue(result.dtype == object)
+
+        result = s.dt.tz_convert('CET')
+        expected = Series(s._values.tz_convert('CET'),index=s.index)
+        tm.assert_series_equal(result, expected)
+
+        tz_result = result.dt.tz
+        self.assertEqual(str(tz_result), 'CET')
+        freq_result = s.dt.freq
+        self.assertEqual(freq_result, DatetimeIndex(s.values, freq='infer').freq)
+
         # timedeltaindex
         for s in [Series(timedelta_range('1 day',periods=5),index=list('abcde')),
                   Series(timedelta_range('1 day 01:23:45',periods=5,freq='s')),
@@ -157,7 +182,7 @@ class CheckNameIntegration(object):
             result = s.dt.to_pytimedelta()
             self.assertIsInstance(result,np.ndarray)
             self.assertTrue(result.dtype == object)
-            
+
             result = s.dt.total_seconds()
             self.assertIsInstance(result,pd.Series)
             self.assertTrue(result.dtype == 'float64')
@@ -991,6 +1016,86 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         dr = date_range('20130101',periods=3,tz='US/Eastern')
         self.assertTrue(str(Series(dr).iloc[0].tz) == 'US/Eastern')
 
+        # non-convertible
+        s = Series([1479596223000, -1479590, pd.NaT])
+        self.assertTrue(s.dtype == 'object')
+        self.assertTrue(s[2] is pd.NaT)
+        self.assertTrue('NaT' in str(s))
+
+        # if we passed a NaT it remains
+        s = Series([datetime(2010, 1, 1), datetime(2, 1, 1), pd.NaT])
+        self.assertTrue(s.dtype == 'object')
+        self.assertTrue(s[2] is pd.NaT)
+        self.assertTrue('NaT' in str(s))
+
+        # if we passed a nan it remains
+        s = Series([datetime(2010, 1, 1), datetime(2, 1, 1), np.nan])
+        self.assertTrue(s.dtype == 'object')
+        self.assertTrue(s[2] is np.nan)
+        self.assertTrue('NaN' in str(s))
+
+    def test_constructor_with_datetime_tz(self):
+
+        # 8260
+        # support datetime64 with tz
+
+        dr = date_range('20130101',periods=3,tz='US/Eastern')
+        s = Series(dr)
+        self.assertTrue(s.dtype.name == 'datetime64[ns, US/Eastern]')
+        self.assertTrue(s.dtype == 'datetime64[ns, US/Eastern]')
+        self.assertTrue(com.is_datetime64tz_dtype(s.dtype))
+
+        # export
+        result = s.values
+        self.assertIsInstance(result, np.ndarray)
+        self.assertTrue(result.dtype == 'datetime64[ns]')
+        self.assertTrue(dr.equals(pd.DatetimeIndex(result).tz_localize('UTC').tz_convert(tz=s.dt.tz)))
+
+        # indexing
+        result = s.iloc[0]
+        self.assertEqual(result,Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern', offset='D'))
+        result = s[0]
+        self.assertEqual(result,Timestamp('2013-01-01 00:00:00-0500', tz='US/Eastern', offset='D'))
+
+        result = s[Series([True,True,False],index=s.index)]
+        assert_series_equal(result,s[0:2])
+
+        result = s.iloc[0:1]
+        assert_series_equal(result,Series(dr[0:1]))
+
+        # concat
+        result = pd.concat([s.iloc[0:1],s.iloc[1:]])
+        assert_series_equal(result,s)
+
+        # astype
+        result = s.astype(object)
+        expected = Series(DatetimeIndex(s._values).asobject)
+        assert_series_equal(result, expected)
+
+        # short str
+        self.assertTrue('datetime64[ns, US/Eastern]' in str(s))
+
+        # formatting with NaT
+        result = s.shift()
+        self.assertTrue('datetime64[ns, US/Eastern]' in str(result))
+        self.assertTrue('NaT' in str(result))
+
+        # long str
+        t = Series(date_range('20130101',periods=1000,tz='US/Eastern'))
+        self.assertTrue('datetime64[ns, US/Eastern]' in str(t))
+
+        result = pd.DatetimeIndex(s,freq='infer')
+        tm.assert_index_equal(result, dr)
+
+        # inference
+        s = Series([pd.Timestamp('2013-01-01 13:00:00-0800', tz='US/Pacific'),pd.Timestamp('2013-01-02 14:00:00-0800', tz='US/Pacific')])
+        self.assertTrue(s.dtype == 'datetime64[ns, US/Pacific]')
+        self.assertTrue(lib.infer_dtype(s) == 'datetime64')
+
+        s = Series([pd.Timestamp('2013-01-01 13:00:00-0800', tz='US/Pacific'),pd.Timestamp('2013-01-02 14:00:00-0800', tz='US/Eastern')])
+        self.assertTrue(s.dtype == 'object')
+        self.assertTrue(lib.infer_dtype(s) == 'datetime')
+
     def test_constructor_periodindex(self):
         # GH7932
         # converting a PeriodIndex when put in a Series
@@ -3519,16 +3624,17 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
     def test_operators_datetimelike(self):
 
         def run_ops(ops, get_ser, test_ser):
-            for op in ops:
-                try:
-                    op = getattr(get_ser, op, None)
-                    if op is not None:
-                        self.assertRaises(TypeError, op, test_ser)
-                except:
-                    com.pprint_thing("Failed on op %r" % op)
-                    raise
+
+            # check that we are getting a TypeError
+            # with 'operate' (from core/ops.py) for the ops that are not defined
+            for op_str in ops:
+                op = getattr(get_ser, op_str, None)
+                with tm.assertRaisesRegexp(TypeError, 'operate'):
+                    op(test_ser)
+
         ### timedelta64 ###
         td1 = Series([timedelta(minutes=5,seconds=3)]*3)
+        td1.iloc[2] = np.nan
         td2 = timedelta(minutes=5,seconds=4)
         ops = ['__mul__','__floordiv__','__pow__',
                '__rmul__','__rfloordiv__','__rpow__']
@@ -3543,6 +3649,7 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         ### datetime64 ###
         dt1 = Series([Timestamp('20111230'), Timestamp('20120101'),
                       Timestamp('20120103')])
+        dt1.iloc[2] = np.nan
         dt2 = Series([Timestamp('20111231'), Timestamp('20120102'),
                       Timestamp('20120104')])
         ops = ['__add__', '__mul__', '__floordiv__', '__truediv__', '__div__',
@@ -3571,6 +3678,66 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         td1 + dt1
         dt1 + td1
 
+        # 8260, 10763
+        # datetime64 with tz
+        ops = ['__mul__', '__floordiv__', '__truediv__', '__div__', '__pow__',
+               '__rmul__', '__rfloordiv__', '__rtruediv__', '__rdiv__',
+               '__rpow__']
+        dt1 = Series(date_range('2000-01-01 09:00:00',periods=5,tz='US/Eastern'),name='foo')
+        dt2 = dt1.copy()
+        dt2.iloc[2] = np.nan
+        td1 = Series(timedelta_range('1 days 1 min',periods=5, freq='H'))
+        td2 = td1.copy()
+        td2.iloc[1] = np.nan
+        run_ops(ops, dt1, td1)
+
+        result = dt1 + td1[0]
+        expected = (dt1.dt.tz_localize(None) + td1[0]).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        result = dt2 + td2[0]
+        expected = (dt2.dt.tz_localize(None) + td2[0]).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        # odd numpy behavior with scalar timedeltas
+        if not _np_version_under1p8:
+            result = td1[0] + dt1
+            expected = (dt1.dt.tz_localize(None) + td1[0]).dt.tz_localize('US/Eastern')
+            assert_series_equal(result, expected)
+
+            result = td2[0] + dt2
+            expected = (dt2.dt.tz_localize(None) + td2[0]).dt.tz_localize('US/Eastern')
+            assert_series_equal(result, expected)
+
+        result = dt1 - td1[0]
+        expected = (dt1.dt.tz_localize(None) - td1[0]).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+        self.assertRaises(TypeError, lambda: td1[0] - dt1)
+
+        result = dt2 - td2[0]
+        expected = (dt2.dt.tz_localize(None) - td2[0]).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+        self.assertRaises(TypeError, lambda: td2[0] - dt2)
+
+        result = dt1 + td1
+        expected = (dt1.dt.tz_localize(None) + td1).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        result = dt2 + td2
+        expected = (dt2.dt.tz_localize(None) + td2).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        result = dt1 - td1
+        expected = (dt1.dt.tz_localize(None) - td1).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        result = dt2 - td2
+        expected = (dt2.dt.tz_localize(None) - td2).dt.tz_localize('US/Eastern')
+        assert_series_equal(result, expected)
+
+        self.assertRaises(TypeError, lambda: td1 - dt1)
+        self.assertRaises(TypeError, lambda: td2 - dt2)
+
     def test_ops_datetimelike_align(self):
         # GH 7500
         # datetimelike ops need to align
@@ -4842,6 +5009,7 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
             sc = s.copy()
             sc.drop_duplicates(keep='last', inplace=True)
             assert_series_equal(sc, s[~expected])
+
             # deprecate take_last
             with tm.assert_produces_warning(FutureWarning):
                 assert_series_equal(s.duplicated(take_last=True), expected)
@@ -4874,6 +5042,7 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
             sc = s.copy()
             sc.drop_duplicates(keep='last', inplace=True)
             assert_series_equal(sc, s[~expected])
+
             # deprecate take_last
             with tm.assert_produces_warning(FutureWarning):
                 assert_series_equal(s.duplicated(take_last=True), expected)
@@ -5416,6 +5585,16 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
             expected = Series([np.nan,0,1,2,3],index=index)
             assert_series_equal(result,expected)
 
+        # xref 8260
+        # with tz
+        s = Series(date_range('2000-01-01 09:00:00',periods=5,tz='US/Eastern'),name='foo')
+        result = s-s.shift()
+        assert_series_equal(result,Series(TimedeltaIndex(['NaT'] + ['1 days']*4),name='foo'))
+
+        # incompat tz
+        s2 = Series(date_range('2000-01-01 09:00:00',periods=5,tz='CET'),name='foo')
+        self.assertRaises(ValueError, lambda : s-s2)
+
     def test_tshift(self):
         # PeriodIndex
         ps = tm.makePeriodSeries()
@@ -5914,17 +6093,17 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         for tt in set([str, compat.text_type]):
             ts = Series([Timestamp('2010-01-04 00:00:00')])
             s = ts.astype(tt)
-            expected = Series([tt(ts.values[0])])
+            expected = Series([tt('2010-01-04')])
             assert_series_equal(s, expected)
 
             ts = Series([Timestamp('2010-01-04 00:00:00', tz='US/Eastern')])
             s = ts.astype(tt)
-            expected = Series([tt(ts.values[0])])
+            expected = Series([tt('2010-01-04 00:00:00-05:00')])
             assert_series_equal(s, expected)
 
             td = Series([Timedelta(1, unit='d')])
             s = td.astype(tt)
-            expected = Series([tt(td.values[0])])
+            expected = Series([tt('1 days 00:00:00.000000000')])
             assert_series_equal(s, expected)
 
     def test_astype_unicode(self):
@@ -7032,9 +7211,9 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         # test an object with dates + floats + integers + strings
         dr = date_range('1/1/2001', '1/10/2001',
                         freq='D').to_series().reset_index(drop=True)
-        r = dr.astype(object).replace([dr[0],dr[1],dr[2]], [1.0,2,'a'])
-        assert_series_equal(r, Series([1.0,2,'a'] +
-                                      dr[3:].tolist(),dtype=object))
+        result = dr.astype(object).replace([dr[0],dr[1],dr[2]], [1.0,2,'a'])
+        expected = Series([1.0,2,'a'] + dr[3:].tolist(),dtype=object)
+        assert_series_equal(result, expected)
 
     def test_replace_bool_with_string_no_op(self):
         s = Series([True, False, True])
@@ -7113,6 +7292,11 @@ class TestSeries(tm.TestCase, CheckNameIntegration):
         nxp = xp.diff()
         assert_series_equal(nrs, nxp)
 
+        # with tz
+        s = Series(date_range('2000-01-01 09:00:00',periods=5,tz='US/Eastern'), name='foo')
+        result = s.diff()
+        assert_series_equal(result,Series(TimedeltaIndex(['NaT'] + ['1 days']*4),name='foo'))
+
     def test_pct_change(self):
         rs = self.ts.pct_change(fill_method=None)
         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
index 146d558ea..95c68aaa0 100644
--- a/pandas/tools/merge.py
+++ b/pandas/tools/merge.py
@@ -396,11 +396,11 @@ class _MergeOperation(object):
                         right_keys.append(rk)
                         join_names.append(None)  # what to do?
                     else:
-                        right_keys.append(right[rk].values)
+                        right_keys.append(right[rk]._values)
                         join_names.append(rk)
                 else:
                     if not is_rkey(rk):
-                        right_keys.append(right[rk].values)
+                        right_keys.append(right[rk]._values)
                         if lk == rk:
                             # avoid key upcast in corner case (length-0)
                             if len(left) > 0:
@@ -409,7 +409,7 @@ class _MergeOperation(object):
                                 left_drop.append(lk)
                     else:
                         right_keys.append(rk)
-                    left_keys.append(left[lk].values)
+                    left_keys.append(left[lk]._values)
                     join_names.append(lk)
         elif _any(self.left_on):
             for k in self.left_on:
@@ -417,10 +417,10 @@ class _MergeOperation(object):
                     left_keys.append(k)
                     join_names.append(None)
                 else:
-                    left_keys.append(left[k].values)
+                    left_keys.append(left[k]._values)
                     join_names.append(k)
             if isinstance(self.right.index, MultiIndex):
-                right_keys = [lev.values.take(lab)
+                right_keys = [lev._values.take(lab)
                               for lev, lab in zip(self.right.index.levels,
                                                   self.right.index.labels)]
             else:
@@ -431,10 +431,10 @@ class _MergeOperation(object):
                     right_keys.append(k)
                     join_names.append(None)
                 else:
-                    right_keys.append(right[k].values)
+                    right_keys.append(right[k]._values)
                     join_names.append(k)
             if isinstance(self.left.index, MultiIndex):
-                left_keys = [lev.values.take(lab)
+                left_keys = [lev._values.take(lab)
                              for lev, lab in zip(self.left.index.levels,
                                                  self.left.index.labels)]
             else:
@@ -952,7 +952,7 @@ class _Concatenator(object):
 
             # stack blocks
             if self.axis == 0:
-                new_data = com._concat_compat([x.values for x in self.objs])
+                new_data = com._concat_compat([x._values for x in self.objs])
                 name = com._consensus_name_attr(self.objs)
                 return Series(new_data, index=self.new_axes[0], name=name).__finalize__(self, method='concat')
 
diff --git a/pandas/tools/tests/test_pivot.py b/pandas/tools/tests/test_pivot.py
index 34789a3c5..50ae574c0 100644
--- a/pandas/tools/tests/test_pivot.py
+++ b/pandas/tools/tests/test_pivot.py
@@ -579,21 +579,21 @@ class TestPivotTable(tm.TestCase):
 
         exp_idx = Index(['a', 'b'], name='label')
         expected = DataFrame({7: [0, 3], 8: [1, 4], 9:[2, 5]},
-                             index=exp_idx, columns=[7, 8, 9])
+                             index=exp_idx, columns=Index([7, 8, 9],name='dt1'))
         tm.assert_frame_equal(result, expected)
 
         result = pivot_table(df, index=df['dt2'].dt.month, columns=df['dt1'].dt.hour,
                              values='value1')
 
         expected = DataFrame({7: [0, 3], 8: [1, 4], 9:[2, 5]},
-                             index=[1, 2], columns=[7, 8, 9])
+                             index=Index([1, 2],name='dt2'), columns=Index([7, 8, 9],name='dt1'))
         tm.assert_frame_equal(result, expected)
 
-        result = pivot_table(df, index=df['dt2'].dt.year,
+        result = pivot_table(df, index=df['dt2'].dt.year.values,
                              columns=[df['dt1'].dt.hour, df['dt2'].dt.month],
                              values='value1')
 
-        exp_col = MultiIndex.from_arrays([[7, 7, 8, 8, 9, 9], [1, 2] * 3])
+        exp_col = MultiIndex.from_arrays([[7, 7, 8, 8, 9, 9], [1, 2] * 3],names=['dt1','dt2'])
         expected = DataFrame(np.array([[0, 3, 1, 4, 2, 5]],dtype='int64'),
                              index=[2013], columns=exp_col)
         tm.assert_frame_equal(result, expected)
diff --git a/pandas/tseries/base.py b/pandas/tseries/base.py
index a6b289b76..5062b7ead 100644
--- a/pandas/tseries/base.py
+++ b/pandas/tseries/base.py
@@ -178,7 +178,7 @@ class DatetimeIndexOpsMixin(object):
 
             return self._simple_new(sorted_values, **attribs)
 
-    def take(self, indices, axis=0):
+    def take(self, indices, axis=0, **kwargs):
         """
         Analogous to ndarray.take
         """
@@ -343,11 +343,6 @@ class DatetimeIndexOpsMixin(object):
                 if freq is not None:
                     freq = "'%s'" % freq
                 attrs.append(('freq',freq))
-            elif attrib == 'tz':
-                tz = self.tz
-                if tz is not None:
-                    tz = "'%s'" % tz
-                attrs.append(('tz',tz))
         return attrs
 
     @cache_readonly
@@ -451,9 +446,9 @@ class DatetimeIndexOpsMixin(object):
 
         inc = tslib._delta_to_nanoseconds(other)
         mask = self.asi8 == tslib.iNaT
-        new_values = (self.asi8 + inc).view(self.dtype)
+        new_values = (self.asi8 + inc).view('i8')
         new_values[mask] = tslib.iNaT
-        return new_values.view(self.dtype)
+        return new_values.view('i8')
 
     def _add_delta_tdi(self, other):
         # add a delta of a TimedeltaIndex
@@ -547,8 +542,7 @@ class DatetimeIndexOpsMixin(object):
         """
         Analogous to ndarray.repeat
         """
-        return self._simple_new(self.values.repeat(repeats),
-                                name=self.name)
+        return self._shallow_copy(self.values.repeat(repeats), freq=None)
 
     def summary(self, name=None):
         """
diff --git a/pandas/tseries/common.py b/pandas/tseries/common.py
index 9a282bec2..ba9f2b834 100644
--- a/pandas/tseries/common.py
+++ b/pandas/tseries/common.py
@@ -9,6 +9,8 @@ from pandas.tseries.tdi import TimedeltaIndex
 from pandas import tslib
 from pandas.core.common import (_NS_DTYPE, _TD_DTYPE, is_period_arraylike,
                                 is_datetime_arraylike, is_integer_dtype, is_list_like,
+                                is_datetime64_dtype, is_datetime64tz_dtype,
+                                is_timedelta64_dtype,
                                 get_dtype_kinds)
 
 def is_datetimelike(data):
@@ -43,23 +45,24 @@ def maybe_to_datetimelike(data, copy=False):
         raise TypeError("cannot convert an object of type {0} to a datetimelike index".format(type(data)))
 
     index = data.index
-    if issubclass(data.dtype.type, np.datetime64):
-        return DatetimeProperties(DatetimeIndex(data, copy=copy, freq='infer'), index)
-    elif issubclass(data.dtype.type, np.timedelta64):
-        return TimedeltaProperties(TimedeltaIndex(data, copy=copy, freq='infer'), index)
+    if is_datetime64_dtype(data.dtype) or is_datetime64tz_dtype(data.dtype):
+        return DatetimeProperties(DatetimeIndex(data, copy=copy, freq='infer'), index, name=data.name)
+    elif is_timedelta64_dtype(data.dtype):
+        return TimedeltaProperties(TimedeltaIndex(data, copy=copy, freq='infer'), index, name=data.name)
     else:
         if is_period_arraylike(data):
-            return PeriodProperties(PeriodIndex(data, copy=copy), index)
+            return PeriodProperties(PeriodIndex(data, copy=copy), index, name=data.name)
         if is_datetime_arraylike(data):
-            return DatetimeProperties(DatetimeIndex(data, copy=copy, freq='infer'), index)
+            return DatetimeProperties(DatetimeIndex(data, copy=copy, freq='infer'), index, name=data.name)
 
     raise TypeError("cannot convert an object of type {0} to a datetimelike index".format(type(data)))
 
 class Properties(PandasDelegate):
 
-    def __init__(self, values, index):
+    def __init__(self, values, index, name):
         self.values = values
         self.index = index
+        self.name = name
 
     def _delegate_property_get(self, name):
         from pandas import Series
@@ -74,7 +77,7 @@ class Properties(PandasDelegate):
             return result
 
         # return the result as a Series, which is by definition a copy
-        result = Series(result, index=self.index)
+        result = Series(result, index=self.index, name=self.name)
 
         # setting this object will show a SettingWithCopyWarning/Error
         result.is_copy = ("modifications to a property of a datetimelike object are not "
@@ -95,7 +98,7 @@ class Properties(PandasDelegate):
         if not com.is_list_like(result):
             return result
 
-        result = Series(result, index=self.index)
+        result = Series(result, index=self.index, name=self.name)
 
         # setting this object will show a SettingWithCopyWarning/Error
         result.is_copy = ("modifications to a method of a datetimelike object are not "
@@ -196,7 +199,7 @@ class CombinedDatetimelikeProperties(DatetimeProperties, TimedeltaProperties):
 def _concat_compat(to_concat, axis=0):
     """
     provide concatenation of an datetimelike array of arrays each of which is a single
-    M8[ns], or m8[ns] dtype
+    M8[ns], datetimet64[ns, tz] or m8[ns] dtype
 
     Parameters
     ----------
@@ -211,6 +214,10 @@ def _concat_compat(to_concat, axis=0):
     def convert_to_pydatetime(x, axis):
         # coerce to an object dtype
         if x.dtype == _NS_DTYPE:
+
+            if hasattr(x, 'tz'):
+                x = x.asobject
+
             shape = x.shape
             x = tslib.ints_to_pydatetime(x.view(np.int64).ravel())
             x = x.reshape(shape)
@@ -218,10 +225,19 @@ def _concat_compat(to_concat, axis=0):
             shape = x.shape
             x = tslib.ints_to_pytimedelta(x.view(np.int64).ravel())
             x = x.reshape(shape)
+
         return x
 
     typs = get_dtype_kinds(to_concat)
 
+    # datetimetz
+    if 'datetimetz' in typs:
+
+        # we require ALL of the same tz for datetimetz
+        tzs = set([ getattr(x,'tz',None) for x in to_concat ])-set([None])
+        if len(tzs) == 1:
+            return DatetimeIndex(np.concatenate([ x.tz_localize(None).asi8 for x in to_concat ]), tz=list(tzs)[0])
+
     # single dtype
     if len(typs) == 1:
 
diff --git a/pandas/tseries/frequencies.py b/pandas/tseries/frequencies.py
index e471e6661..d7eaab5a5 100644
--- a/pandas/tseries/frequencies.py
+++ b/pandas/tseries/frequencies.py
@@ -15,6 +15,7 @@ import pandas.lib as lib
 import pandas.tslib as tslib
 import pandas._period as period
 from pandas.tslib import Timedelta
+from pytz import AmbiguousTimeError
 
 class FreqGroup(object):
     FR_ANN = 1000
@@ -784,7 +785,7 @@ def _period_str_to_code(freqstr):
     if freqstr in _rule_aliases:
         new = _rule_aliases[freqstr]
         warnings.warn(_LEGACY_FREQ_WARNING.format(freqstr, new),
-                      FutureWarning, stacklevel=6)
+                      FutureWarning, stacklevel=3)
         freqstr = new
     freqstr = _lite_rule_alias.get(freqstr, freqstr)
 
@@ -793,7 +794,7 @@ def _period_str_to_code(freqstr):
         if lower in _rule_aliases:
             new = _rule_aliases[lower]
             warnings.warn(_LEGACY_FREQ_WARNING.format(lower, new),
-                          FutureWarning, stacklevel=6)
+                          FutureWarning, stacklevel=3)
             freqstr = new
         freqstr = _lite_rule_alias.get(lower, freqstr)
 
@@ -833,8 +834,8 @@ def infer_freq(index, warn=True):
     import pandas as pd
 
     if isinstance(index, com.ABCSeries):
-        values = index.values
-        if not (com.is_datetime64_dtype(index.values) or com.is_timedelta64_dtype(index.values) or values.dtype == object):
+        values = index._values
+        if not (com.is_datetime64_dtype(values) or com.is_timedelta64_dtype(values) or values.dtype == object):
             raise TypeError("cannot infer freq from a non-convertible dtype on a Series of {0}".format(index.dtype))
         index = values
 
@@ -850,7 +851,11 @@ def infer_freq(index, warn=True):
             raise TypeError("cannot infer freq from a non-convertible index type {0}".format(type(index)))
         index = index.values
 
-    index = pd.DatetimeIndex(index)
+    try:
+        index = pd.DatetimeIndex(index)
+    except AmbiguousTimeError:
+        index = pd.DatetimeIndex(index.asi8)
+
     inferer = _FrequencyInferer(index, warn=warn)
     return inferer.get_freq()
 
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index c6c66a62b..b1198f975 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -7,8 +7,11 @@ from datetime import timedelta
 import numpy as np
 from pandas.core.common import (_NS_DTYPE, _INT64_DTYPE,
                                 _values_from_object, _maybe_box,
+                                is_object_dtype, is_datetime64_dtype,
+                                is_datetimetz, is_dtype_equal,
                                 ABCSeries, is_integer, is_float,
-                                is_object_dtype, is_datetime64_dtype)
+                                DatetimeTZDtype)
+
 from pandas.io.common import PerformanceWarning
 from pandas.core.index import Index, Int64Index, Float64Index
 import pandas.compat as compat
@@ -114,11 +117,12 @@ def _new_DatetimeIndex(cls, d):
     """ This is called upon unpickling, rather than the default which doesn't have arguments
         and breaks __new__ """
 
-    # simply set the tz
     # data are already in UTC
+    # so need to localize
     tz = d.pop('tz',None)
     result = cls.__new__(cls, **d)
-    result.tz = tz
+    if tz is not None:
+        result = result.tz_localize('UTC').tz_convert(tz)
     return result
 
 class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
@@ -199,7 +203,7 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
                 freq=None, start=None, end=None, periods=None,
                 copy=False, name=None, tz=None,
                 verify_integrity=True, normalize=False,
-                closed=None, ambiguous='raise', **kwargs):
+                closed=None, ambiguous='raise', dtype=None, **kwargs):
 
         dayfirst = kwargs.pop('dayfirst', None)
         yearfirst = kwargs.pop('yearfirst', None)
@@ -264,9 +268,16 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
                                                      dayfirst=dayfirst,
                                                      yearfirst=yearfirst)
 
+        if is_datetimetz(data):
+            # extract the data whether a Series or Index
+            if isinstance(data, ABCSeries):
+                data = data._values
+            tz = data.tz
+            data = data.tz_localize(None, ambiguous='infer').values
+
         if issubclass(data.dtype.type, np.datetime64):
             if isinstance(data, ABCSeries):
-                data = data.values
+                data = data._values
             if isinstance(data, DatetimeIndex):
                 if tz is None:
                     tz = data.tz
@@ -290,7 +301,7 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
                 subarr = data.view(_NS_DTYPE)
         else:
             if isinstance(data, (ABCSeries, Index)):
-                values = data.values
+                values = data._values
             else:
                 values = data
 
@@ -304,7 +315,7 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
 
                     # make sure that we have a index/ndarray like (and not a Series)
                     if isinstance(subarr, ABCSeries):
-                        subarr = subarr.values
+                        subarr = subarr._values
                         if subarr.dtype == np.object_:
                             subarr = tools._to_datetime(subarr, box=False)
 
@@ -312,7 +323,8 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
                     # tz aware
                     subarr = tools._to_datetime(data, box=False, utc=True)
 
-                if not np.issubdtype(subarr.dtype, np.datetime64):
+                # we may not have been able to convert
+                if not (is_datetimetz(subarr) or np.issubdtype(subarr.dtype, np.datetime64)):
                     raise ValueError('Unable to convert %s to datetime dtype'
                                      % str(data))
 
@@ -334,6 +346,16 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
 
         subarr = cls._simple_new(subarr, name=name, freq=freq, tz=tz)
 
+        # if dtype is provided, coerce here
+        if dtype is not None:
+
+            if not is_dtype_equal(subarr.dtype, dtype):
+
+                if subarr.tz is not None:
+                    raise ValueError("cannot localize from non-UTC data")
+                dtype = DatetimeTZDtype.construct_from_string(dtype)
+                subarr = subarr.tz_localize(dtype.tz)
+
         if verify_integrity and len(subarr) > 0:
             if freq is not None and not freq_infer:
                 inferred = subarr.inferred_freq
@@ -498,16 +520,21 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
             return result.take(reverse)
 
     @classmethod
-    def _simple_new(cls, values, name=None, freq=None, tz=None, **kwargs):
+    def _simple_new(cls, values, name=None, freq=None, tz=None, dtype=None, **kwargs):
         """
         we require the we have a dtype compat for the values
         if we are passed a non-dtype compat, then coerce using the constructor
         """
 
         if not getattr(values,'dtype',None):
+            # empty, but with dtype compat
+            if values is None:
+                values = np.empty(0, dtype=_NS_DTYPE)
+                return cls(values, name=name, freq=freq, tz=tz, dtype=dtype, **kwargs)
             values = np.array(values,copy=False)
+
         if is_object_dtype(values):
-            return cls(values, name=name, freq=freq, tz=tz, **kwargs).values
+            return cls(values, name=name, freq=freq, tz=tz, dtype=dtype, **kwargs).values
         elif not is_datetime64_dtype(values):
             values = com._ensure_int64(values).view(_NS_DTYPE)
 
@@ -690,7 +717,15 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
 
     def _add_offset(self, offset):
         try:
-            return offset.apply_index(self)
+            if self.tz is not None:
+                values = self.tz_localize(None)
+            else:
+                values = self
+            result = offset.apply_index(values)
+            if self.tz is not None:
+                result = result.tz_localize(self.tz)
+            return result
+
         except NotImplementedError:
             warnings.warn("Non-vectorized DateOffset being applied to Series or DatetimeIndex",
                            PerformanceWarning)
@@ -716,6 +751,8 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
             return self.asobject
         elif dtype == _INT64_DTYPE:
             return self.asi8.copy()
+        elif dtype == _NS_DTYPE and self.tz is not None:
+            return self.tz_convert('UTC').tz_localize(None)
         else:  # pragma: no cover
             raise ValueError('Cannot cast DatetimeIndex to dtype %s' % dtype)
 
@@ -740,7 +777,8 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
 
                     If the timezone is not set, the resulting
                     Series will have a datetime64[ns] dtype.
-                    Otherwise the Series will have an object dtype; the
+
+                    Otherwise the Series will have an datetime64[ns, tz] dtype; the
                     tz will be preserved.
 
                   If keep_tz is False:
@@ -762,8 +800,11 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
         This is for internal compat
         """
         if keep_tz and self.tz is not None:
-            return self.asobject.values
-        return self.values
+
+            # preserve the tz & copy
+            return self.copy(deep=True)
+
+        return self.values.copy()
 
     def to_pydatetime(self):
         """
@@ -1477,9 +1518,11 @@ class DatetimeIndex(DatelikeOps, DatetimeIndexOpsMixin, Int64Index):
         # sure we can't have ambiguous indexing
         return 'datetime64'
 
-    @property
+    @cache_readonly
     def dtype(self):
-        return _NS_DTYPE
+        if self.tz is None:
+            return _NS_DTYPE
+        return com.DatetimeTZDtype('ns',self.tz)
 
     @property
     def is_all_dates(self):
diff --git a/pandas/tseries/tests/test_base.py b/pandas/tseries/tests/test_base.py
index 22eb1afb7..4c9726bbc 100644
--- a/pandas/tseries/tests/test_base.py
+++ b/pandas/tseries/tests/test_base.py
@@ -111,32 +111,32 @@ class TestDatetimeIndexOps(Ops):
             self.assertTrue(pd.isnull(getattr(obj, op)()))
 
     def test_representation(self):
-        idx1 = DatetimeIndex([], freq='D')
-        idx2 = DatetimeIndex(['2011-01-01'], freq='D')
-        idx3 = DatetimeIndex(['2011-01-01', '2011-01-02'], freq='D')
-        idx4 = DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], freq='D')
-        idx5 = DatetimeIndex(['2011-01-01 09:00', '2011-01-01 10:00', '2011-01-01 11:00'],
-                             freq='H', tz='Asia/Tokyo')
-        idx6 = DatetimeIndex(['2011-01-01 09:00', '2011-01-01 10:00', pd.NaT],
-                             tz='US/Eastern')
 
-        exp1 = """DatetimeIndex([], dtype='datetime64[ns]', freq='D', tz=None)"""
-
-        exp2 = """DatetimeIndex(['2011-01-01'], dtype='datetime64[ns]', freq='D', tz=None)"""
-
-        exp3 = """DatetimeIndex(['2011-01-01', '2011-01-02'], dtype='datetime64[ns]', freq='D', tz=None)"""
-
-        exp4 = """DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq='D', tz=None)"""
-
-        exp5 = """DatetimeIndex(['2011-01-01 09:00:00+09:00', '2011-01-01 10:00:00+09:00', '2011-01-01 11:00:00+09:00'], dtype='datetime64[ns]', freq='H', tz='Asia/Tokyo')"""
-
-        exp6 = """DatetimeIndex(['2011-01-01 09:00:00-05:00', '2011-01-01 10:00:00-05:00', 'NaT'], dtype='datetime64[ns]', freq=None, tz='US/Eastern')"""
+        idx = []
+        idx.append(DatetimeIndex([], freq='D'))
+        idx.append(DatetimeIndex(['2011-01-01'], freq='D'))
+        idx.append(DatetimeIndex(['2011-01-01', '2011-01-02'], freq='D'))
+        idx.append(DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], freq='D'))
+        idx.append(DatetimeIndex(['2011-01-01 09:00', '2011-01-01 10:00', '2011-01-01 11:00'],
+                                 freq='H', tz='Asia/Tokyo'))
+        idx.append(DatetimeIndex(['2011-01-01 09:00', '2011-01-01 10:00', pd.NaT],
+                                 tz='US/Eastern'))
+        idx.append(DatetimeIndex(['2011-01-01 09:00', '2011-01-01 10:00', pd.NaT],
+                                 tz='UTC'))
+
+        exp = []
+        exp.append("""DatetimeIndex([], dtype='datetime64[ns]', freq='D')""")
+        exp.append("""DatetimeIndex(['2011-01-01'], dtype='datetime64[ns]', freq='D')""")
+        exp.append("""DatetimeIndex(['2011-01-01', '2011-01-02'], dtype='datetime64[ns]', freq='D')""")
+        exp.append("""DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], dtype='datetime64[ns]', freq='D')""")
+        exp.append("""DatetimeIndex(['2011-01-01 09:00:00+09:00', '2011-01-01 10:00:00+09:00', '2011-01-01 11:00:00+09:00'], dtype='datetime64[ns, Asia/Tokyo]', freq='H')""")
+        exp.append("""DatetimeIndex(['2011-01-01 09:00:00-05:00', '2011-01-01 10:00:00-05:00', 'NaT'], dtype='datetime64[ns, US/Eastern]', freq=None)""")
+        exp.append("""DatetimeIndex(['2011-01-01 09:00:00+00:00', '2011-01-01 10:00:00+00:00', 'NaT'], dtype='datetime64[ns, UTC]', freq=None)""")
 
         with pd.option_context('display.width', 300):
-            for idx, expected in zip([idx1, idx2, idx3, idx4, idx5, idx6],
-                                     [exp1, exp2, exp3, exp4, exp5, exp6]):
+            for indx, expected in zip(idx, exp):
                 for func in ['__repr__', '__unicode__', '__str__']:
-                    result = getattr(idx, func)()
+                    result = getattr(indx, func)()
                     self.assertEqual(result, expected)
 
     def test_representation_to_series(self):
@@ -164,15 +164,15 @@ dtype: datetime64[ns]"""
 2   2011-01-03
 dtype: datetime64[ns]"""
 
-        exp5 = """0    2011-01-01 09:00:00+09:00
-1    2011-01-01 10:00:00+09:00
-2    2011-01-01 11:00:00+09:00
-dtype: object"""
+        exp5 = """0   2011-01-01 09:00:00+09:00
+1   2011-01-01 10:00:00+09:00
+2   2011-01-01 11:00:00+09:00
+dtype: datetime64[ns, Asia/Tokyo]"""
 
-        exp6 = """0    2011-01-01 09:00:00-05:00
-1    2011-01-01 10:00:00-05:00
-2                          NaN
-dtype: object"""
+        exp6 = """0   2011-01-01 09:00:00-05:00
+1   2011-01-01 10:00:00-05:00
+2                         NaT
+dtype: datetime64[ns, US/Eastern]"""
 
         exp7 = """0   2011-01-01 09:00:00
 1   2011-01-02 10:15:00
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index a9837e279..84a4c3e08 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -1101,6 +1101,60 @@ class TestTimeSeries(tm.TestCase):
             )
         )
 
+    def test_to_datetime_tz(self):
+
+        # xref 8260
+        # uniform returns a DatetimeIndex
+        arr = [pd.Timestamp('2013-01-01 13:00:00-0800', tz='US/Pacific'),pd.Timestamp('2013-01-02 14:00:00-0800', tz='US/Pacific')]
+        result = pd.to_datetime(arr)
+        expected = DatetimeIndex(['2013-01-01 13:00:00','2013-01-02 14:00:00'],tz='US/Pacific')
+        tm.assert_index_equal(result, expected)
+
+        # mixed tzs will raise
+        arr = [pd.Timestamp('2013-01-01 13:00:00', tz='US/Pacific'),pd.Timestamp('2013-01-02 14:00:00', tz='US/Eastern')]
+        self.assertRaises(ValueError, lambda : pd.to_datetime(arr))
+
+    def test_to_datetime_tz_pytz(self):
+
+        # xref 8260
+        tm._skip_if_no_pytz()
+        import pytz
+
+        us_eastern = pytz.timezone('US/Eastern')
+        arr = np.array([us_eastern.localize(datetime(year=2000, month=1, day=1, hour=3, minute=0)),
+                        us_eastern.localize(datetime(year=2000, month=6, day=1, hour=3, minute=0))],dtype=object)
+        result = pd.to_datetime(arr, utc=True)
+        expected = DatetimeIndex(['2000-01-01 08:00:00+00:00', '2000-06-01 07:00:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)
+        tm.assert_index_equal(result, expected)
+
+    def test_to_datetime_tz_psycopg2(self):
+
+        # xref 8260
+        try:
+            import psycopg2
+        except ImportError:
+            raise nose.SkipTest("no psycopg2 installed")
+
+        # misc cases
+        arr = np.array([ datetime(2000, 1, 1, 3, 0, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)),
+                         datetime(2000, 6, 1, 3, 0, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-240, name=None))], dtype=object)
+
+        result = pd.to_datetime(arr, errors='coerce', utc=True)
+        expected = DatetimeIndex(['2000-01-01 08:00:00+00:00', '2000-06-01 07:00:00+00:00'], dtype='datetime64[ns, UTC]', freq=None)
+        tm.assert_index_equal(result, expected)
+
+        # dtype coercion
+        i = pd.DatetimeIndex(['2000-01-01 08:00:00+00:00'],tz=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None))
+        self.assertFalse(com.is_datetime64_ns_dtype(i))
+
+        # tz coerceion
+        result = pd.to_datetime(i, errors='coerce')
+        tm.assert_index_equal(result, i)
+
+        result = pd.to_datetime(i, errors='coerce', utc=True)
+        expected = pd.DatetimeIndex(['2000-01-01 13:00:00'])
+        tm.assert_index_equal(result, expected)
+
     def test_index_to_datetime(self):
         idx = Index(['1/1/2000', '1/2/2000', '1/3/2000'])
 
@@ -2139,6 +2193,12 @@ class TestDatetimeIndex(tm.TestCase):
         result = rng.astype('i8')
         self.assert_numpy_array_equal(result, rng.asi8)
 
+        # with tz
+        rng = date_range('1/1/2000', periods=10, tz='US/Eastern')
+        result = rng.astype('datetime64[ns]')
+        expected = date_range('1/1/2000', periods=10, tz='US/Eastern').tz_convert('UTC').tz_localize(None)
+        tm.assert_index_equal(result, expected)
+
     def test_to_period_nofreq(self):
         idx = DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-04'])
         self.assertRaises(ValueError, idx.to_period)
@@ -2471,6 +2531,15 @@ class TestDatetimeIndex(tm.TestCase):
             exp = klass(date_range('1999-01-01', '1999-01-31'))
             assert_func(result, exp)
 
+            s = klass([Timestamp('2000-01-15 00:15:00', tz='US/Central'),
+                       pd.Timestamp('2000-02-15', tz='US/Central')])
+            result = s + pd.offsets.Day()
+            result2 = pd.offsets.Day() + s
+            exp = klass([Timestamp('2000-01-16 00:15:00', tz='US/Central'),
+                         Timestamp('2000-02-16', tz='US/Central')])
+            assert_func(result, exp)
+            assert_func(result2, exp)
+
             s = klass([Timestamp('2000-01-15 00:15:00', tz='US/Central'),
                        pd.Timestamp('2000-02-15', tz='US/Central')])
             result = s + pd.offsets.MonthEnd()
diff --git a/pandas/tseries/tests/test_timezones.py b/pandas/tseries/tests/test_timezones.py
index b4b5576a5..a6e581215 100644
--- a/pandas/tseries/tests/test_timezones.py
+++ b/pandas/tseries/tests/test_timezones.py
@@ -17,7 +17,7 @@ import pandas.tseries.tools as tools
 from pytz import NonExistentTimeError
 
 import pandas.util.testing as tm
-
+from pandas.core.dtypes import DatetimeTZDtype
 from pandas.util.testing import assert_frame_equal
 from pandas.compat import lrange, zip
 
@@ -669,7 +669,8 @@ class TestTimeZoneSupportPytz(tm.TestCase):
         dr = date_range('2011/1/1', '2012/1/1', freq='W-FRI')
         dr_tz = dr.tz_localize(self.tzstr('US/Eastern'))
         e = DataFrame({'A': 'foo', 'B': dr_tz}, index=dr)
-        self.assertEqual(e['B'].dtype, 'O')
+        tz_expected = DatetimeTZDtype('ns',dr_tz.tzinfo)
+        self.assertEqual(e['B'].dtype, tz_expected)
 
         # GH 2810 (with timezones)
         datetimes_naive   = [ ts.to_pydatetime() for ts in dr ]
@@ -677,8 +678,8 @@ class TestTimeZoneSupportPytz(tm.TestCase):
         df = DataFrame({'dr' : dr, 'dr_tz' : dr_tz,
                         'datetimes_naive': datetimes_naive,
                         'datetimes_with_tz' : datetimes_with_tz })
-        result = df.get_dtype_counts()
-        expected = Series({ 'datetime64[ns]' : 2, 'object' : 2 })
+        result = df.get_dtype_counts().sort_index()
+        expected = Series({ 'datetime64[ns]' : 2, str(tz_expected) : 2 }).sort_index()
         tm.assert_series_equal(result, expected)
 
     def test_hongkong_tz_convert(self):
diff --git a/pandas/tseries/tests/test_tslib.py b/pandas/tseries/tests/test_tslib.py
index 85bae42e7..fadad91e6 100644
--- a/pandas/tseries/tests/test_tslib.py
+++ b/pandas/tseries/tests/test_tslib.py
@@ -878,7 +878,7 @@ class TestTslib(tm.TestCase):
 
     def test_period_ordinal_start_values(self):
         # information for 1.1.1970
-        self.assertEqual(0, period_ordinal(1970, 1, 1, 0, 0, 0, 0, 0, get_freq('Y')))
+        self.assertEqual(0, period_ordinal(1970, 1, 1, 0, 0, 0, 0, 0, get_freq('A')))
         self.assertEqual(0, period_ordinal(1970, 1, 1, 0, 0, 0, 0, 0, get_freq('M')))
         self.assertEqual(1, period_ordinal(1970, 1, 1, 0, 0, 0, 0, 0, get_freq('W')))
         self.assertEqual(0, period_ordinal(1970, 1, 1, 0, 0, 0, 0, 0, get_freq('D')))
@@ -943,6 +943,12 @@ class TestTslib(tm.TestCase):
                                   tslib.maybe_get_tz('Asia/Tokyo'))
         self.assert_numpy_array_equal(result, np.array([], dtype=np.int64))
 
+        # Check all-NaT array
+        result = tslib.tz_convert(np.array([tslib.iNaT], dtype=np.int64),
+                                  tslib.maybe_get_tz('US/Eastern'),
+                                  tslib.maybe_get_tz('Asia/Tokyo'))
+        self.assert_numpy_array_equal(result, np.array([tslib.iNaT], dtype=np.int64))
+
 class TestTimestampOps(tm.TestCase):
     def test_timestamp_and_datetime(self):
         self.assertEqual((Timestamp(datetime.datetime(2013, 10, 13)) - datetime.datetime(2013, 10, 12)).days, 1)
diff --git a/pandas/tseries/timedeltas.py b/pandas/tseries/timedeltas.py
index 282e1d603..11200bb25 100644
--- a/pandas/tseries/timedeltas.py
+++ b/pandas/tseries/timedeltas.py
@@ -58,7 +58,7 @@ def to_timedelta(arg, unit='ns', box=True, errors='raise', coerce=None):
         return arg
     elif isinstance(arg, ABCSeries):
         from pandas import Series
-        values = _convert_listlike(arg.values, box=False, unit=unit)
+        values = _convert_listlike(arg._values, box=False, unit=unit)
         return Series(values, index=arg.index, name=arg.name, dtype='m8[ns]')
     elif isinstance(arg, ABCIndexClass):
         return _convert_listlike(arg, box=box, unit=unit, name=arg.name)
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index 521679f21..5d9808849 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -292,6 +292,14 @@ def _to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False,
                     pass
 
             return arg
+
+        elif com.is_datetime64tz_dtype(arg):
+            if not isinstance(arg, DatetimeIndex):
+                return DatetimeIndex(arg, tz='utc' if utc else None)
+            if utc:
+                arg = arg.tz_convert(None)
+            return arg
+
         elif format is None and com.is_integer_dtype(arg) and unit=='ns':
             result = arg.astype('datetime64[ns]')
             if box:
@@ -371,7 +379,7 @@ def _to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False,
     elif isinstance(arg, tslib.Timestamp):
         return arg
     elif isinstance(arg, Series):
-        values = _convert_listlike(arg.values, False, format)
+        values = _convert_listlike(arg._values, False, format)
         return Series(values, index=arg.index, name=arg.name)
     elif isinstance(arg, ABCIndexClass):
         return _convert_listlike(arg, box, format, name=arg.name)
diff --git a/pandas/tslib.pyx b/pandas/tslib.pyx
index a8b573ab6..774174710 100644
--- a/pandas/tslib.pyx
+++ b/pandas/tslib.pyx
@@ -1979,12 +1979,15 @@ cpdef array_to_datetime(ndarray[object] values, errors='raise',
         for i in range(n):
             val = values[i]
 
-            # set as nan if is even a datetime NaT
+            # set as nan except if its a NaT
             if _checknull_with_nat(val):
-                oresult[i] = np.nan
-            elif util.is_datetime64_object(val):
                 if val is np_NaT or val.view('i8') == iNaT:
+                    oresult[i] = NaT
+                else:
                     oresult[i] = np.nan
+            elif util.is_datetime64_object(val):
+                if val is np_NaT or val.view('i8') == iNaT:
+                    oresult[i] = NaT
                 else:
                     oresult[i] = val.item()
             else:
@@ -3318,7 +3321,7 @@ except:
 
 def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
     cdef:
-        ndarray[int64_t] utc_dates, result, trans, deltas
+        ndarray[int64_t] utc_dates, tt, result, trans, deltas
         Py_ssize_t i, pos, n = len(vals)
         int64_t v, offset
         pandas_datetimestruct dts
@@ -3337,27 +3340,38 @@ def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
         if _is_tzlocal(tz1):
             for i in range(n):
                 v = vals[i]
-                pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
-                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                              dts.min, dts.sec, dts.us, tz1)
-                delta = (int(total_seconds(_get_utcoffset(tz1, dt)))
-                         * 1000000000)
-                utc_dates[i] = v - delta
+                if v == iNaT:
+                    utc_dates[i] = iNaT
+                else:
+                    pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
+                    dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                                  dts.min, dts.sec, dts.us, tz1)
+                    delta = (int(total_seconds(_get_utcoffset(tz1, dt)))
+                             * 1000000000)
+                    utc_dates[i] = v - delta
         else:
             trans, deltas, typ = _get_dst_info(tz1)
 
+            # all-NaT
+            tt = vals[vals!=iNaT]
+            if not len(tt):
+                return vals
+
             trans_len = len(trans)
-            pos = trans.searchsorted(vals[0]) - 1
+            pos = trans.searchsorted(tt[0]) - 1
             if pos < 0:
                 raise ValueError('First time before start of DST info')
 
             offset = deltas[pos]
             for i in range(n):
                 v = vals[i]
-                while pos + 1 < trans_len and v >= trans[pos + 1]:
-                    pos += 1
-                    offset = deltas[pos]
-                utc_dates[i] = v - offset
+                if v == iNaT:
+                    utc_dates[i] = iNaT
+                else:
+                    while pos + 1 < trans_len and v >= trans[pos + 1]:
+                        pos += 1
+                        offset = deltas[pos]
+                    utc_dates[i] = v - offset
     else:
         utc_dates = vals
 
@@ -3368,18 +3382,26 @@ def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
     if _is_tzlocal(tz2):
         for i in range(n):
             v = utc_dates[i]
-            pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                          dts.min, dts.sec, dts.us, tz2)
-            delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
-            result[i] = v + delta
+            if v == iNaT:
+                result[i] = iNaT
+            else:
+                pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
+                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                              dts.min, dts.sec, dts.us, tz2)
+                delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
+                result[i] = v + delta
             return result
 
     # Convert UTC to other timezone
     trans, deltas, typ = _get_dst_info(tz2)
     trans_len = len(trans)
 
-    pos = trans.searchsorted(utc_dates[0]) - 1
+    # use first non-NaT element
+    # if all-NaT, return all-NaT
+    if (result==iNaT).all():
+        return result
+
+    pos = trans.searchsorted(utc_dates[utc_dates!=iNaT][0]) - 1
     if pos < 0:
         raise ValueError('First time before start of DST info')
 
@@ -3387,7 +3409,7 @@ def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
     offset = deltas[pos]
     for i in range(n):
         v = utc_dates[i]
-        if vals[i] == NPY_NAT:
+        if vals[i] == iNaT:
             result[i] = vals[i]
         else:
             while pos + 1 < trans_len and v >= trans[pos + 1]:
@@ -3434,6 +3456,7 @@ def tz_convert_single(int64_t val, object tz1, object tz2):
                       dts.min, dts.sec, dts.us, tz2)
         delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
         return utc_date + delta
+
     # Convert UTC to other timezone
     trans, deltas, typ = _get_dst_info(tz2)
 
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 878bfdf3a..0dad2da4a 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -23,9 +23,11 @@ from numpy.random import randn, rand
 import numpy as np
 
 import pandas as pd
-from pandas.core.common import (is_sequence, array_equivalent, is_list_like,
+from pandas.core.common import (is_sequence, array_equivalent, is_list_like, is_number,
                                 is_datetimelike_v_numeric, is_datetimelike_v_object,
-                                is_number, pprint_thing, take_1d)
+                                is_number, pprint_thing, take_1d,
+                                needs_i8_conversion)
+
 import pandas.compat as compat
 from pandas.compat import(
     filter, map, zip, range, unichr, lrange, lmap, lzip, u, callable, Counter,
@@ -902,7 +904,7 @@ def assert_series_equal(left, right, check_dtype=True,
     elif check_datetimelike_compat:
         # we want to check only if we have compat dtypes
         # e.g. integer and M|m are NOT compat, but we can simply check the values in that case
-        if is_datetimelike_v_numeric(left, right) or is_datetimelike_v_object(left, right):
+        if is_datetimelike_v_numeric(left, right) or is_datetimelike_v_object(left, right) or needs_i8_conversion(left) or needs_i8_conversion(right):
 
             # datetimelike may have different objects (e.g. datetime.datetime vs Timestamp) but will compare equal
             if not Index(left.values).equals(Index(right.values)):
diff --git a/vb_suite/binary_ops.py b/vb_suite/binary_ops.py
index 4c74688ce..7c821374a 100644
--- a/vb_suite/binary_ops.py
+++ b/vb_suite/binary_ops.py
@@ -172,3 +172,28 @@ timestamp_ops_diff1 = Benchmark("s.diff()", setup,
                                 start_date=datetime(2013, 1, 1))
 timestamp_ops_diff2 = Benchmark("s-s.shift()", setup,
                                 start_date=datetime(2013, 1, 1))
+
+#----------------------------------------------------------------------
+# timeseries with tz
+
+setup = common_setup + """
+N = 10000
+halfway = N // 2 - 1
+s = Series(date_range('20010101', periods=N, freq='T', tz='US/Eastern'))
+ts = s[halfway]
+"""
+
+timestamp_tz_series_compare = Benchmark("ts >= s", setup,
+                                        start_date=datetime(2013, 9, 27))
+series_timestamp_tz_compare = Benchmark("s <= ts", setup,
+                                        start_date=datetime(2012, 2, 21))
+
+setup = common_setup + """
+N = 10000
+s = Series(date_range('20010101', periods=N, freq='s', tz='US/Eastern'))
+"""
+
+timestamp_tz_ops_diff1 = Benchmark("s.diff()", setup,
+                                   start_date=datetime(2013, 1, 1))
+timestamp_tz_ops_diff2 = Benchmark("s-s.shift()", setup,
+                                   start_date=datetime(2013, 1, 1))
diff --git a/vb_suite/timeseries.py b/vb_suite/timeseries.py
index 7e10b333d..15bc89d62 100644
--- a/vb_suite/timeseries.py
+++ b/vb_suite/timeseries.py
@@ -110,7 +110,7 @@ timeseries_asof_nan = Benchmark('ts.asof(dates)', setup,
                                 start_date=datetime(2012, 4, 27))
 
 #----------------------------------------------------------------------
-# Time zone stuff
+# Time zone
 
 setup = common_setup + """
 rng = date_range(start='1/1/2000', end='3/1/2000', tz='US/Eastern')
