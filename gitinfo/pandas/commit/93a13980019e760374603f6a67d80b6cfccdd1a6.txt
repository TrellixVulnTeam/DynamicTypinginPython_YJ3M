commit 93a13980019e760374603f6a67d80b6cfccdd1a6
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Dec 20 18:10:21 2011 -0500

    DOC: add whatsnew stuff for 0.6.2, reorg vbench suite. release notes

diff --git a/RELEASE.rst b/RELEASE.rst
index 37e564178..e66f1f053 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -27,6 +27,47 @@ pandas 0.6.2
 
 **Release date:** NOT YET RELEASED
 
+**New features / modules**
+
+  - Handle differently-indexed output values in ``DataFrame.apply`` (GH #498)
+
+**Improvements to existing features**
+
+  - Better error message in DataFrame constructor when passed column labels
+    don't match data (GH #497)
+  - Substantially improve performance of multi-GroupBy aggregation when a
+    Python function is passed, reuse ndarray object in Cython (GH #496)
+  - Can store objects indexed by tuples and floats in HDFStore (GH #492)
+  - Don't print length by default in Series.to_string, add `length` option (GH
+    #489)
+  - Improve Cython code for multi-groupby to aggregate without having to sort
+    the data (GH #93)
+  - Improve MultiIndex reindexing speed by storing tuples in the MultiIndex,
+    test for backwards unpickling compatibility
+  - Improve column reindexing performance by using specialized Cython take
+    function
+  - Further performance tweaking of Series.__getitem__ for standard use cases
+
+**Bug fixes**
+
+  - Raise exception in out-of-bounds indexing of Series instead of
+    seg-faulting, regression from earlier releases (GH #495)
+  - Fix error when joining DataFrames of different dtypes within the same
+    typeclass (e.g. float32 and float64) (GH #486)
+  - Fix bug in Series.min/Series.max on objects like datetime.datetime (GH
+    #487)
+  - Preserve index names in Index.union (GH #501)
+  - Fix bug in Index joining causing subclass information (like DateRange type)
+    to be lost in some cases (GH #500)
+  - Accept empty list as input to DataFrame constructor, regression from 0.6.0
+    (GH #491)
+  - Can output DataFrame and Series with ndarray objects in a dtype=object
+    array (GH #490)
+  - Return empty string from Series.to_string when called on empty Series (GH
+    #488)
+
+Thanks
+------
 
 pandas 0.6.1
 ============
diff --git a/bench/bench_groupby.py b/bench/bench_groupby.py
new file mode 100644
index 000000000..16a7432a1
--- /dev/null
+++ b/bench/bench_groupby.py
@@ -0,0 +1,47 @@
+from pandas import *
+
+import string
+import random
+
+k = 200
+n = 1000
+
+
+foo = np.tile(list(range(k)), n)
+foo2 = list(foo)
+random.shuffle(foo)
+random.shuffle(foo2)
+
+df = DataFrame({'A' : foo,
+                'B' : foo2,
+                'C' : np.random.randn(n * k)})
+
+
+import pandas._tseries as lib
+
+f = np.std
+
+
+grouped = df.groupby(['A', 'B'])
+
+label_list = [ping.labels for ping in grouped.groupings]
+shape = [len(ping.ids) for ping in grouped.groupings]
+
+from pandas.core.groupby import get_group_index
+
+
+group_index = get_group_index(label_list, shape).astype('i4')
+
+ngroups = np.prod(shape)
+
+indexer = lib.groupsort_indexer(group_index, ngroups)
+
+values = df['C'].values.take(indexer)
+group_index = group_index.take(indexer)
+
+f = lambda x: x.std(ddof=1)
+
+grouper = lib.Grouper(df['C'], np.ndarray.std, group_index, ngroups)
+result = grouper.get_result()
+
+expected = grouped.std()
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 86946da81..60e086a0d 100755
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -123,4 +123,5 @@ See the package overview for more detail about what's in the library.
     r_interface
     related
     comparison_with_r
+    benchmarks
     api
diff --git a/doc/source/indexing.rst b/doc/source/indexing.rst
index 34fb68137..002dcdf04 100644
--- a/doc/source/indexing.rst
+++ b/doc/source/indexing.rst
@@ -187,8 +187,10 @@ As we will see later on, the same operation could be accomplished by
 reindexing. However, the syntax would be more verbose; hence, the inclusion of
 this indexing method.
 
-Selecting DataFrame columns
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+.. _indexing.columns.multiple:
+
+Getting and setting multiple DataFrame columns
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 You can pass a list of columns to ``[]`` to select columns in that order:
 
@@ -196,7 +198,17 @@ You can pass a list of columns to ``[]`` to select columns in that order:
 
    df[['C', 'A', 'B']]
 
-If a column is not contained in the DataFrame, an exception will be raised:
+If a column is not contained in the DataFrame, an exception will be
+raised. Multiple columns can also be set in this manner:
+
+.. ipython:: python
+
+   df
+   df[['B', 'A']] = df[['A', 'B']]
+   df
+
+You may find this useful for applying a transform (in-place) to a subset of the
+columns.
 
 .. _indexing.advanced:
 
diff --git a/doc/source/whatsnew.rst b/doc/source/whatsnew.rst
index 8ac15ee6a..fe2aa0d9c 100644
--- a/doc/source/whatsnew.rst
+++ b/doc/source/whatsnew.rst
@@ -2,6 +2,14 @@
 
 .. currentmodule:: pandas
 
+.. ipython:: python
+   :suppress:
+
+   import numpy as np
+   from pandas import *
+   randn = np.random.randn
+   np.set_printoptions(precision=4, suppress=True)
+
 **********
 What's New
 **********
diff --git a/doc/source/whatsnew/v0.6.2.rst b/doc/source/whatsnew/v0.6.2.rst
index 9c3f5f705..29c42ce55 100644
--- a/doc/source/whatsnew/v0.6.2.rst
+++ b/doc/source/whatsnew/v0.6.2.rst
@@ -3,3 +3,94 @@
 
 v.0.6.2 (Not Yet Released)
 --------------------------
+
+These are new features and improvements of note in this release.
+
+New functions or features
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Can set multiple columns in a DataFrame, useful for transformation (Issue342_)
+
+.. ipython:: python
+
+   DataFrame(randn(10, 3), columns=['A', 'B', 'C'])
+
+Improvements to existing features
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+- Handle differently-indexed output values in ``DataFrame.apply`` (Issue498_)
+
+.. ipython:: python
+
+   df = DataFrame(randn(10, 4))
+   df.apply(lambda x: x.describe())
+
+- You can now :ref:`set multiple columns <indexing.columns.multiple>` in a
+  DataFrame via ``__getitem__``.
+
+Performance improvements
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Cythonized GroupBy aggregations no longer presort the data, thus achieving a
+significant speedup (Issue93_)
+
+.. code-block:: ipython
+
+    In [5]: df
+    Out[5]:
+    <class 'pandas.core.frame.DataFrame'>
+    Int64Index: 100000 entries, 0 to 99999
+    Data columns:
+    data    100000  non-null values
+    key1    100000  non-null values
+    key2    100000  non-null values
+    dtypes: float64(1), object(2)
+
+    In [6]: df[:10]
+    Out[6]:
+       data     key1  key2
+    0  2.708    4     1
+    1 -1.945    2     4
+    2 -1.123    2     0
+    3  0.09592  0     0
+    4  2.328    0     3
+    5 -0.6481   0     0
+    6  0.2957   3     2
+    7  0.7336   2     1
+    8  0.371    2     4
+    9  1.009    2     4
+
+    In [6]: df.groupby(['key1', 'key2']).sum()
+    Out[6]:
+               data
+    key1 key2
+    0    0     114
+         1    -14.69
+         2     89.06
+         3    -61.31
+         4    -32.24
+    1    0     57.91
+         1    -16.08
+         2    -46.51
+         3     15.46
+         4     18.96
+    ...
+
+Here's a graph of the performance of this operation over time on a dataset with
+100,000 rows and 10,000 unique groups:
+
+.. .. include:: vbench/groupby_multi_cython.rst
+
+On this similar vein,
+
+GroupBy aggregations with Python functions significantly sped up by clever
+manipulation of the ndarray data type in Cython (Issue496_). Benchmark of a
+similar operation to the above but using a Python function:
+
+.. .. include:: vbench/groupby_multi_python.rst
+
+.. _Issue93: https://github.com/wesm/pandas/issues/93
+.. _Issue342: https://github.com/wesm/pandas/issues/342
+.. _Issue439: https://github.com/wesm/pandas/issues/439
+.. _Issue496: https://github.com/wesm/pandas/issues/496
+.. _Issue498: https://github.com/wesm/pandas/issues/498
diff --git a/vb_suite/.gitignore b/vb_suite/.gitignore
new file mode 100644
index 000000000..126ee4071
--- /dev/null
+++ b/vb_suite/.gitignore
@@ -0,0 +1 @@
+benchmarks.db
\ No newline at end of file
diff --git a/vb_suite/benchmarks.py b/vb_suite/benchmarks.py
index bcba33bfe..e69de29bb 100644
--- a/vb_suite/benchmarks.py
+++ b/vb_suite/benchmarks.py
@@ -1,207 +0,0 @@
-from vbench.benchmark import Benchmark
-from vbench.db import BenchmarkDB
-from vbench.runner import BenchmarkRunner
-from vbench.git import GitRepo
-
-from datetime import datetime
-
-
-common_setup = """
-from pandas import *
-import pandas.util.testing as tm
-import random
-import numpy as np
-"""
-
-#----------------------------------------------------------------------
-# Series.__getitem__, get_value
-
-setup = common_setup + """
-tm.N = 1000
-ts = tm.makeTimeSeries()
-dt = ts.index[500]
-"""
-statement = "ts[dt]"
-
-bm_getitem = Benchmark(statement, setup, ncalls=100000,
-                       name='Series.__getitem__ scalar')
-
-#----------------------------------------------------------------------
-# DataFrame reindex columns
-
-setup = common_setup + """
-index = [tm.rands(10) for _ in xrange(1000)]
-columns = [tm.rands(10) for _ in xrange(30)]
-df = DataFrame(np.random.rand(1000, 30), index=index,
-               columns=columns)
-idx = index[100]
-col = columns[10]
-"""
-statement = "df[col][idx]"
-bm_df_getitem = Benchmark(statement, setup,
-                        name='DataFrame get scalar value')
-
-setup = common_setup + """
-try:
-    klass = DataMatrix
-except:
-    klass = DataFrame
-
-index = [tm.rands(10) for _ in xrange(1000)]
-columns = [tm.rands(10) for _ in xrange(30)]
-df = klass(np.random.rand(1000, 30), index=index,
-               columns=columns)
-idx = index[100]
-col = columns[10]
-"""
-statement = "df[col][idx]"
-bm_df_getitem2 = Benchmark(statement, setup,
-                        name='DataMatrix get scalar value')
-
-setup = common_setup + """
-try:
-    klass = DataMatrix
-except:
-    klass = DataFrame
-
-index = [tm.rands(10) for _ in xrange(1000)]
-columns = [tm.rands(10) for _ in xrange(30)]
-df = klass(np.random.rand(1000, 30), index=index,
-               columns=columns)
-idx = index[100]
-col = columns[10]
-"""
-statement = "df.get_value(idx, col)"
-bm_df_getitem3 = Benchmark(statement, setup, name='DataFrame get_value',
-                           start_date=datetime(2011, 11, 12))
-
-#----------------------------------------------------------------------
-# DataFrame reindex columns
-
-setup = common_setup + """
-df = DataFrame(index=range(10000), data=np.random.rand(10000,30),
-               columns=range(30))
-"""
-statement = "df.reindex(columns=df.columns[1:5])"
-
-bm_reindex1 = Benchmark(statement, setup, name='DataFrame.reindex columns')
-
-#----------------------------------------------------------------------
-
-setup = common_setup + """
-rng = DateRange('1/1/1970', periods=10000, offset=datetools.Minute())
-df = DataFrame(np.random.rand(10000, 10), index=rng,
-               columns=range(10))
-df['foo'] = 'bar'
-rng2 = Index(rng[::2])
-"""
-statement = "df.reindex(rng2)"
-bm_reindex2 = Benchmark(statement, setup,
-                        name='DataFrame.reindex index daterange')
-
-#----------------------------------------------------------------------
-setup = common_setup + """
-
-N = 100000
-ngroups = 100
-
-def get_test_data(ngroups=100, n=N):
-    unique_groups = range(ngroups)
-    arr = np.asarray(np.tile(unique_groups, n / ngroups), dtype=object)
-
-    if len(arr) < n:
-        arr = np.asarray(list(arr) + unique_groups[:n - len(arr)],
-                         dtype=object)
-
-    random.shuffle(arr)
-    return arr
-
-df = DataFrame({'key1' : get_test_data(ngroups=ngroups),
-                'key2' : get_test_data(ngroups=ngroups),
-                'data' : np.random.randn(N)})
-def f():
-    df.groupby(['key1', 'key2']).agg(lambda x: x.values.sum())
-"""
-stmt = "df.groupby(['key1', 'key2'])['data'].agg(lambda x: x.values.sum())"
-
-bm_groupby1 = Benchmark(stmt, setup, name="GroupBy test foo",
-                        start_date=datetime(2011, 7, 1))
-
-stmt2 = "df.groupby(['key1', 'key2']).sum()"
-bm_groupby2 = Benchmark(stmt2, setup, name="GroupBy test 2",
-                        start_date=datetime(2011, 7, 1))
-
-#----------------------------------------------------------------------
-# data alignment
-
-setup = common_setup + """
-from pandas import *
-from pandas.util.testing import rands
-
-n = 1000000
-# indices = Index([rands(10) for _ in xrange(n)])
-def sample(values, k):
-    sampler = np.random.permutation(len(values))
-    return values.take(sampler[:k])
-sz = 500000
-rng = np.arange(0, 10000000000000, 10000000)
-stamps = np.datetime64(datetime.now()).view('i8') + rng
-idx1 = np.sort(sample(stamps, sz))
-idx2 = np.sort(sample(stamps, sz))
-ts1 = Series(np.random.randn(sz), idx1)
-ts2 = Series(np.random.randn(sz), idx2)
-"""
-stmt = "ts1 + ts2"
-bm_align1 = Benchmark(stmt, setup, name="int64 alignment 1",
-                      start_date=datetime(2011, 3, 1))
-
-#----------------------------------------------------------------------
-
-setup = common_setup + """
-from pandas.core.sparse import SparseSeries, SparseDataFrame
-
-K = 50
-N = 50000
-rng = np.asarray(DateRange('1/1/2000', periods=N,
-                           offset=datetools.Minute()))
-
-# rng2 = np.asarray(rng).astype('M8').astype('i8')
-
-series = {}
-for i in range(1, K + 1):
-    data = np.random.randn(N)[:-i]
-    this_rng = rng[:-i]
-    data[100:] = np.nan
-    series[i] = SparseSeries(data, index=this_rng)
-"""
-stmt = "SparseDataFrame(series)"
-
-bm_sparse1 = Benchmark(stmt, setup, name="SparseSeries to SparseDataFrame",
-                      start_date=datetime(2011, 6, 1))
-
-#----------------------------------------------------------------------
-# Actually running the benchmarks
-
-benchmarks = [v for v in locals().values() if isinstance(v, Benchmark)]
-
-REPO_PATH = '/home/wesm/code/pandas'
-REPO_URL = 'git@github.com:wesm/pandas.git'
-DB_PATH = '/home/wesm/code/pandas/gb_suite/benchmarks.db'
-TMP_DIR = '/home/wesm/tmp/gb_pandas'
-PREPARE = """
-python setup.py clean
-"""
-BUILD = """
-python setup.py build_ext --inplace
-"""
-START_DATE = datetime(2011, 3, 1)
-
-repo = GitRepo(REPO_PATH)
-
-to_consider = repo.shas.truncate(START_DATE)
-
-runner = BenchmarkRunner(benchmarks, REPO_PATH, REPO_URL,
-                         BUILD, DB_PATH, TMP_DIR, PREPARE,
-                         run_option='eod', start_date=START_DATE)
-
-runner.run()
diff --git a/vb_suite/binary_ops.py b/vb_suite/binary_ops.py
new file mode 100644
index 000000000..f182b8d5c
--- /dev/null
+++ b/vb_suite/binary_ops.py
@@ -0,0 +1,34 @@
+from vbench.benchmark import Benchmark
+from datetime import datetime
+
+common_setup = """
+from pandas import *
+import pandas.util.testing as tm
+import random
+import numpy as np
+"""
+
+#----------------------------------------------------------------------
+# data alignment
+
+setup = common_setup + """
+from pandas import *
+from pandas.util.testing import rands
+
+n = 1000000
+# indices = Index([rands(10) for _ in xrange(n)])
+def sample(values, k):
+    sampler = np.random.permutation(len(values))
+    return values.take(sampler[:k])
+sz = 500000
+rng = np.arange(0, 10000000000000, 10000000)
+stamps = np.datetime64(datetime.now()).view('i8') + rng
+idx1 = np.sort(sample(stamps, sz))
+idx2 = np.sort(sample(stamps, sz))
+ts1 = Series(np.random.randn(sz), idx1)
+ts2 = Series(np.random.randn(sz), idx2)
+"""
+stmt = "ts1 + ts2"
+bm_align1 = Benchmark(stmt, setup,
+                      name="series_align_int64_index",
+                      start_date=datetime(2011, 3, 1))
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
new file mode 100644
index 000000000..81169f7fd
--- /dev/null
+++ b/vb_suite/groupby.py
@@ -0,0 +1,39 @@
+from vbench.api import Benchmark
+from datetime import datetime
+
+common_setup = """from pandas_vb_common import *
+"""
+
+setup = common_setup + """
+
+N = 100000
+ngroups = 5
+
+def get_test_data(ngroups=100, n=N):
+    unique_groups = range(ngroups)
+    arr = np.asarray(np.tile(unique_groups, n / ngroups), dtype=object)
+
+    if len(arr) < n:
+        arr = np.asarray(list(arr) + unique_groups[:n - len(arr)],
+                         dtype=object)
+
+    random.shuffle(arr)
+    return arr
+
+df = DataFrame({'key1' : get_test_data(ngroups=ngroups),
+                'key2' : get_test_data(ngroups=ngroups),
+                'data' : np.random.randn(N)})
+def f():
+    df.groupby(['key1', 'key2']).agg(lambda x: x.values.sum())
+"""
+
+stmt1 = "df.groupby(['key1', 'key2'])['data'].agg(lambda x: x.values.sum())"
+bm_groupby1 = Benchmark(stmt1, setup,
+                        name="groupby_multi_python",
+                        start_date=datetime(2011, 7, 1))
+
+stmt3 = "df.groupby(['key1', 'key2']).sum()"
+bm_groupby3 = Benchmark(stmt3, setup,
+                        name="groupby_multi_cython",
+                        start_date=datetime(2011, 7, 1))
+
diff --git a/vb_suite/indexing.py b/vb_suite/indexing.py
new file mode 100644
index 000000000..836629b05
--- /dev/null
+++ b/vb_suite/indexing.py
@@ -0,0 +1,80 @@
+from vbench.benchmark import Benchmark
+from datetime import datetime
+
+SECTION = 'Indexing and scalar value access'
+
+common_setup = """from pandas_vb_common import *
+"""
+
+#----------------------------------------------------------------------
+# Series.__getitem__, get_value
+
+setup = common_setup + """
+tm.N = 1000
+ts = tm.makeTimeSeries()
+dt = ts.index[500]
+"""
+statement = "ts[dt]"
+
+bm_getitem = Benchmark(statement, setup, ncalls=100000,
+                       name='series_getitem_scalar')
+
+setup = common_setup + """
+index = [tm.rands(10) for _ in xrange(1000)]
+s = Series(np.random.rand(1000), index=index)
+idx = index[100]
+"""
+statement = "s.get_value(idx)"
+bm_df_getitem3 = Benchmark(statement, setup,
+                           name='series_get_value',
+                           start_date=datetime(2011, 11, 12))
+
+#----------------------------------------------------------------------
+# DataFrame __getitem__
+
+setup = common_setup + """
+index = [tm.rands(10) for _ in xrange(1000)]
+columns = [tm.rands(10) for _ in xrange(30)]
+df = DataFrame(np.random.rand(1000, 30), index=index,
+               columns=columns)
+idx = index[100]
+col = columns[10]
+"""
+statement = "df[col][idx]"
+bm_df_getitem = Benchmark(statement, setup,
+                        name='dataframe_getitem_scalar')
+
+setup = common_setup + """
+try:
+    klass = DataMatrix
+except:
+    klass = DataFrame
+
+index = [tm.rands(10) for _ in xrange(1000)]
+columns = [tm.rands(10) for _ in xrange(30)]
+df = klass(np.random.rand(1000, 30), index=index,
+               columns=columns)
+idx = index[100]
+col = columns[10]
+"""
+statement = "df[col][idx]"
+bm_df_getitem2 = Benchmark(statement, setup,
+                        name='datamatrix_getitem_scalar')
+
+setup = common_setup + """
+try:
+    klass = DataMatrix
+except:
+    klass = DataFrame
+
+index = [tm.rands(10) for _ in xrange(1000)]
+columns = [tm.rands(10) for _ in xrange(30)]
+df = klass(np.random.rand(1000, 30), index=index,
+               columns=columns)
+idx = index[100]
+col = columns[10]
+"""
+statement = "df.get_value(idx, col)"
+bm_df_getitem3 = Benchmark(statement, setup,
+                           name='dataframe_get_value',
+                           start_date=datetime(2011, 11, 12))
diff --git a/vb_suite/pandas_vb_common.py b/vb_suite/pandas_vb_common.py
new file mode 100644
index 000000000..ad601eb53
--- /dev/null
+++ b/vb_suite/pandas_vb_common.py
@@ -0,0 +1,4 @@
+from pandas import *
+import pandas.util.testing as tm
+import random
+import numpy as np
diff --git a/vb_suite/reindex.py b/vb_suite/reindex.py
new file mode 100644
index 000000000..c8f794406
--- /dev/null
+++ b/vb_suite/reindex.py
@@ -0,0 +1,30 @@
+from vbench.benchmark import Benchmark
+from datetime import datetime
+
+common_setup = """from pandas_vb_common import *
+"""
+
+#----------------------------------------------------------------------
+# DataFrame reindex columns
+
+setup = common_setup + """
+df = DataFrame(index=range(10000), data=np.random.rand(10000,30),
+               columns=range(30))
+"""
+statement = "df.reindex(columns=df.columns[1:5])"
+
+bm_reindex1 = Benchmark(statement, setup,
+                        name='dataframe_reindex_columns')
+
+#----------------------------------------------------------------------
+
+setup = common_setup + """
+rng = DateRange('1/1/1970', periods=10000, offset=datetools.Minute())
+df = DataFrame(np.random.rand(10000, 10), index=rng,
+               columns=range(10))
+df['foo'] = 'bar'
+rng2 = Index(rng[::2])
+"""
+statement = "df.reindex(rng2)"
+bm_reindex2 = Benchmark(statement, setup,
+                        name='dataframe_reindex_daterange')
diff --git a/vb_suite/run_suite.py b/vb_suite/run_suite.py
new file mode 100644
index 000000000..211c052d8
--- /dev/null
+++ b/vb_suite/run_suite.py
@@ -0,0 +1,34 @@
+from vbench.api import Benchmark, GitRepo, BenchmarkRunner
+from datetime import datetime
+
+modules = ['groupby', 'indexing', 'reindex', 'binary_ops',
+           'sparse']
+
+all_benchmarks = []
+for modname in modules:
+    ref = __import__(modname)
+    for k, v in ref.__dict__.iteritems():
+        if isinstance(v, Benchmark):
+            all_benchmarks.append(v)
+
+REPO_PATH = '/home/wesm/code/pandas'
+REPO_URL = 'git@github.com:wesm/pandas.git'
+DB_PATH = '/home/wesm/code/pandas/vb_suite/benchmarks.db'
+TMP_DIR = '/home/wesm/tmp/vb_pandas'
+PREPARE = """
+python setup.py clean
+"""
+BUILD = """
+python setup.py build_ext --inplace
+"""
+START_DATE = datetime(2011, 3, 1)
+
+repo = GitRepo(REPO_PATH)
+
+to_consider = repo.shas.truncate(START_DATE)
+
+runner = BenchmarkRunner(all_benchmarks, REPO_PATH, REPO_URL,
+                         BUILD, DB_PATH, TMP_DIR, PREPARE,
+                         run_option='eod', start_date=START_DATE)
+
+runner.run()
diff --git a/vb_suite/sparse.py b/vb_suite/sparse.py
new file mode 100644
index 000000000..3acd48e4f
--- /dev/null
+++ b/vb_suite/sparse.py
@@ -0,0 +1,29 @@
+from vbench.benchmark import Benchmark
+from datetime import datetime
+
+common_setup = """from pandas_vb_common import *
+"""
+
+#----------------------------------------------------------------------
+
+setup = common_setup + """
+from pandas.core.sparse import SparseSeries, SparseDataFrame
+
+K = 50
+N = 50000
+rng = np.asarray(DateRange('1/1/2000', periods=N,
+                           offset=datetools.Minute()))
+
+# rng2 = np.asarray(rng).astype('M8').astype('i8')
+
+series = {}
+for i in range(1, K + 1):
+    data = np.random.randn(N)[:-i]
+    this_rng = rng[:-i]
+    data[100:] = np.nan
+    series[i] = SparseSeries(data, index=this_rng)
+"""
+stmt = "SparseDataFrame(series)"
+
+bm_sparse1 = Benchmark(stmt, setup, name="SparseSeries to SparseDataFrame",
+                      start_date=datetime(2011, 6, 1))
