commit 89fcf7c6ff277eb4f7c5345162fc126c6c7bf617
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Thu Dec 22 14:59:45 2011 -0500

    DOC: more 0.6.1 docs

diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index b9001a5cb..f28cb6e24 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -289,37 +289,6 @@ number of unique values and most frequently occurring values:
    s = Series(['a', 'a', 'b', 'b', 'a', 'a', np.nan, 'c', 'd', 'a'])
    s.describe()
 
-
-Correlations between objects
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-Several handy methods for computing correlations are provided. The only
-behavior available at the moment is to compute "pairwise complete
-observations". In computing correlations in the presence of missing data, one
-must be careful internally to compute the standard deviation of each Series
-over the labels with valid data in both objects.
-
-.. ipython:: python
-
-   # Series with Series
-   frame['a'].corr(frame['b'])
-
-   # Pairwise correlation of DataFrame columns
-   frame.corr()
-
-A related method ``corrwith`` is implemented on DataFrame to compute the
-correlation between like-labeled Series contained in different DataFrame
-objects.
-
-.. ipython:: python
-
-   index = ['a', 'b', 'c', 'd', 'e']
-   columns = ['one', 'two', 'three', 'four']
-   df1 = DataFrame(randn(5, 4), index=index, columns=columns)
-   df2 = DataFrame(randn(4, 4), index=index[:4], columns=columns)
-   df1.corrwith(df2)
-   df2.corrwith(df1, axis=1)
-
 .. _basics.apply:
 
 Function application
@@ -744,6 +713,26 @@ alternately passing the ``dtype`` keyword argument to the object constructor.
    df = DataFrame(np.arange(12).reshape((4, 3)), dtype=float)
    df[0].dtype
 
+.. _basics.cast.infer:
+
+Inferring better types for object columns
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The ``convert_objects`` DataFrame method will attempt to convert
+``dtype=object`` columns to a better NumPy dtype. Occasionally (after
+transposing multiple times, for example), a mixed-type DataFrame will end up
+with everything as ``dtype=object``. This method attempts to fix that:
+
+.. ipython:: python
+
+   df = DataFrame(randn(6, 3), columns=['a', 'b', 'c'])
+   df['d'] = 'foo'
+   df
+   df = df.T.T
+   df.dtypes
+   converted = df.convert_objects()
+   converted.dtypes
+
 .. _basics.serialize:
 
 Pickling and serialization
@@ -791,7 +780,20 @@ For instance:
 
 .. ipython:: python
 
-  set_eng_float_format(precision=3, use_eng_prefix=True)
-  df[0]/1.e3
-  df[0]/1.e6
+   set_eng_float_format(precision=3, use_eng_prefix=True)
+   df[0]/1.e3
+   df[0]/1.e6
+
+.. ipython:: python
+   :suppress:
+
+   set_printoptions(precision=4)
+
 
+The ``set_printoptions`` function has a number of options for controlling how
+floating point numbers are formatted (using hte ``precision`` argument) in the
+console and . The ``max_rows`` and ``max_columns`` control how many rows and
+columns of DataFrame objects are shown by default. If ``max_columns`` is set to
+0 (the default, in fact), the library will attempt to fit the DataFrame's
+string representation into the current terminal width, and defaulting to the
+summary view otherwise.
diff --git a/doc/source/stats.rst b/doc/source/computation.rst
old mode 100755
new mode 100644
similarity index 78%
rename from doc/source/stats.rst
rename to doc/source/computation.rst
index 8ed0de04f..8b0c7c46c
--- a/doc/source/stats.rst
+++ b/doc/source/computation.rst
@@ -1,6 +1,5 @@
-.. currentmodule:: pandas.stats.api
-
-.. _stats:
+.. currentmodule:: pandas
+.. _computation:
 
 .. ipython:: python
    :suppress:
@@ -14,9 +13,85 @@
    import matplotlib.pyplot as plt
    plt.close('all')
 
-**********************************
-Moving window stats and regression
-**********************************
+Computational tools
+===================
+
+Statistical functions
+---------------------
+
+.. _computation.correlation:
+
+Correlation
+~~~~~~~~~~~
+
+Several methods for computing correlations are provided. Several kinds of
+correlation methods are provided:
+
+.. csv-table::
+    :header: "Method name", "Description"
+    :widths: 20, 80
+
+    ``pearson (default)``, Standard correlation coefficient
+    ``kendall``, Kendall Tau correlation coefficient
+    ``spearman``, Spearman rank correlation coefficient
+
+.. \rho = \cov(x, y) / \sigma_x \sigma_y
+
+All of these are currently computed using pariwise complete observations.
+
+.. ipython:: python
+
+   frame = DataFrame(randn(1000, 5), columns=['a', 'b', 'c', 'd', 'e'])
+   frame.ix[::2] = np.nan
+
+   # Series with Series
+   frame['a'].corr(frame['b'])
+   frame['a'].corr(frame['b'], method='spearman')
+
+   # Pairwise correlation of DataFrame columns
+   frame.corr()
+
+A related method ``corrwith`` is implemented on DataFrame to compute the
+correlation between like-labeled Series contained in different DataFrame
+objects.
+
+.. ipython:: python
+
+   index = ['a', 'b', 'c', 'd', 'e']
+   columns = ['one', 'two', 'three', 'four']
+   df1 = DataFrame(randn(5, 4), index=index, columns=columns)
+   df2 = DataFrame(randn(4, 4), index=index[:4], columns=columns)
+   df1.corrwith(df2)
+   df2.corrwith(df1, axis=1)
+
+.. _computation.ranking:
+
+Data ranking
+~~~~~~~~~~~~
+
+The ``rank`` method produces a data ranking with ties being assigned the mean
+of the ranks for the group:
+
+.. ipython:: python
+
+   s = Series(np.random.randn(5), index=list('abcde'))
+   s['d'] = s['b'] # so there's a tie
+   s.rank()
+
+``rank`` is also a DataFrame method and can rank either the rows (``axis=0``)
+or the columns (``axis=1``). ``NaN`` values are excluded from the ranking.
+
+.. ipython:: python
+
+   df = DataFrame(np.random.randn(10, 6))
+   df[4] = df[2][:5] # some ties
+   df
+   df.rank(1)
+
+.. note::
+
+    These methods are significantly faster (around 10-20x) than
+    ``scipy.stats.rankdata``.
 
 .. currentmodule:: pandas
 
@@ -51,6 +126,7 @@ otherwise they can be found in :mod:`pandas.stats.moments`.
     ``rolling_apply``, Generic apply
     ``rolling_cov``, Unbiased covariance (binary)
     ``rolling_corr``, Correlation (binary)
+    ``rolling_corr_pairwise``, Pairwise correlation of DataFrame columns
 
 Generally these methods all have the same interface. The binary operators
 (e.g. ``rolling_corr``) take two Series or DataFrames. Otherwise, they all
@@ -91,6 +167,35 @@ sugar for applying the moving window operator to all of the DataFrame's columns:
    @savefig rolling_mean_frame.png width=4.5in
    rolling_sum(df, 60).plot(subplots=True)
 
+.. _stats.moments.corr_pairwise:
+
+Computing rolling pairwise correlations
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+In financial data analysis and other fields it's common to compute correlation
+matrices for a collection of time series. More difficult is to compute a
+moving-window correlation matrix. This can be done using the
+``rolling_corr_pairwise`` function, which yields a ``Panel`` whose ``items``
+are the dates in question:
+
+.. ipython:: python
+
+   correls = rolling_corr_pairwise(df, 50)
+   correls[df.index[-50]]
+
+You can efficiently retrieve the time series of correlations between two
+columns using ``ix`` indexing:
+
+.. ipython:: python
+   :suppress:
+
+   plt.close('all')
+
+.. ipython:: python
+
+   @savefig rolling_corr_pairwise_ex.png width=4.5in
+   correls.ix[:, 'A', 'C'].plot()
+
 Exponentially weighted moment functions
 ---------------------------------------
 
@@ -332,3 +437,4 @@ Result fields and tests
 
 We'll leave it to the user to explore the docstrings and source, especially as
 we'll be moving this code into statsmodels in the near future.
+
diff --git a/doc/source/dsintro.rst b/doc/source/dsintro.rst
index 8e22aa89d..9a7c5cdc5 100644
--- a/doc/source/dsintro.rst
+++ b/doc/source/dsintro.rst
@@ -222,6 +222,9 @@ not matching up to the passed index.
 If axis labels are not passed, they will be constructed from the input data
 based on common sense rules.
 
+Main constructor
+~~~~~~~~~~~~~~~~
+
 **From dict of Series or dicts**
 
 the result **index** will be the **union** of the indexes of the various
@@ -283,6 +286,40 @@ This case is handled identically to a dict of arrays.
     DataFrame is not intended to work exactly like a 2-dimensional NumPy
     ndarray.
 
+Alternate Constructors
+~~~~~~~~~~~~~~~~~~~~~~
+
+.. _basics.dataframe.from_dict:
+
+**DataFrame.from_dict**
+
+.. _basics.dataframe.from_records:
+
+**DataFrame.from_records**
+
+.. _basics.dataframe.from_items:
+
+**DataFrame.from_items**
+
+``DataFrame.from_items`` works analogously to the form of the ``dict``
+constructor that takes a sequence of ``(key, value)`` pairs, where the keys are
+column (or row, in the case of ``orient='index'``) names, and the value are the
+column values (or row values). This can be useful for constructing a DataFrame
+with the columns in a particular order without having to pass an explicit list
+of columns:
+
+.. ipython:: python
+
+   DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])])
+
+If you pass ``orient='index'``, the keys will be the row labels. But in this
+case you must also pass the desired column names:
+
+.. ipython:: python
+
+   DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5, 6])],
+                        orient='index', columns=['one', 'two', 'three'])
+
 Column selection, addition, deletion
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
diff --git a/doc/source/index.rst b/doc/source/index.rst
index 5e366eee0..51a80e2c5 100755
--- a/doc/source/index.rst
+++ b/doc/source/index.rst
@@ -111,13 +111,13 @@ See the package overview for more detail about what's in the library.
     dsintro
     basics
     indexing
+    computation
     missing_data
     groupby
     merging
     reshaping
     timeseries
     visualization
-    stats
     io
     sparse
     r_interface
diff --git a/doc/source/reshaping.rst b/doc/source/reshaping.rst
index 7f14f6ac0..2c2ac0eb5 100644
--- a/doc/source/reshaping.rst
+++ b/doc/source/reshaping.rst
@@ -201,9 +201,8 @@ some very expressive and fast data manipulations.
    df.mean().unstack(0)
 
 
-**********************************
 Pivot tables and cross-tabulations
-**********************************
+----------------------------------
 
 .. _reshaping.pivot:
 
@@ -220,11 +219,11 @@ Consider a data set like this:
 
 .. ipython:: python
 
-   df = DataFrame({'A' : ['one', 'one', 'two', 'three'] * 3,
-                   'B' : ['A', 'B', 'C'] * 4,
-                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2,
-                   'D' : np.random.randn(12),
-                   'E' : np.random.randn(12)})
+   df = DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,
+                   'B' : ['A', 'B', 'C'] * 8,
+                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,
+                   'D' : np.random.randn(24),
+                   'E' : np.random.randn(24)})
    df
 
 We can produce pivot tables from this data very easily:
@@ -250,3 +249,19 @@ calling ``to_string`` if you wish:
 
    table = pivot_table(df, rows=['A', 'B'], cols=['C'])
    print table.to_string(na_rep='')
+
+Note that ``pivot_table`` is also available as an instance method on DataFrame.
+
+.. _reshaping.pivot.margins:
+
+Adding margins (partial aggregates)
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If you pass ``margins=True`` to ``pivot_table``, special ``All`` columns and
+rows will be added with partial group aggregates across the categories on the
+rows and columns:
+
+.. ipython:: python
+
+   df.pivot_table(rows=['A', 'B'], cols='C', margins=True, aggfunc=np.std)
+
diff --git a/doc/source/sparse.rst b/doc/source/sparse.rst
index 78975119e..fe3e4ead7 100644
--- a/doc/source/sparse.rst
+++ b/doc/source/sparse.rst
@@ -69,7 +69,7 @@ SparseArray
 -----------
 
 ``SparseArray`` is the base layer for all of the sparse indexed data
-structured. It is a 1-dimensional ndarray-like object storing only values
+structures. It is a 1-dimensional ndarray-like object storing only values
 distinct from the ``fill_value``:
 
 .. ipython:: python
diff --git a/doc/source/whatsnew/v0.6.1.rst b/doc/source/whatsnew/v0.6.1.rst
index 3f9979905..a25fed546 100644
--- a/doc/source/whatsnew/v0.6.1.rst
+++ b/doc/source/whatsnew/v0.6.1.rst
@@ -7,8 +7,8 @@ v.0.6.1 (December 13, 2011)
 New features
 ~~~~~~~~~~~~
 - Can :ref:`append single rows <merging.append.row>` (as Series) to a DataFrame
-- Add Spearman and Kendall rank correlation options to Series.corr and
-  DataFrame.corr (GH428_)
+- Add Spearman and Kendall rank :ref:`correlation <computation.correlation>`
+  options to Series.corr and DataFrame.corr (GH428_)
 - :ref:`Added <indexing.basics.get_value>` ``get_value`` and ``set_value`` methods to
   Series, DataFrame, and Panel for very low-overhead access (>2x faster in many
   cases) to scalar elements (GH437_, GH438_). ``set_value`` is capable of
@@ -18,24 +18,51 @@ New features
   and an :ref:`axis option <basics.df_join>` (GH461_)
 - Implement new :ref:`SparseArray <sparse.array>` and :ref:`SparseList <sparse.list>`
   data structures. SparseSeries now derives from SparseArray (GH463_)
-- max_columns / max_rows options in set_printoptions (PR #453)
-- Implement Series.rank and DataFrame.rank, fast versions of
-  scipy.stats.rankdata (GH #428)
-- Implement DataFrame.from_items alternate constructor (GH #444)
-- DataFrame.convert_objects method for inferring better dtypes for object
-  columns (GH #302)
-- Add rolling_corr_pairwise function for computing Panel of correlation
-  matrices (GH #189)
-- Add `margins` option to `pivot_table` for computing subgroup aggregates (GH
-  #114)
+- :ref:`Better console printing options <basics.console_output>` (PR453_)
+- Implement fast :ref:`data ranking <computation.ranking>` for Series and
+  DataFrame, fast versions of scipy.stats.rankdata (GH428_)
+- Implement :ref:`DataFrame.from_items <basics.dataframe.from_items>` alternate
+  constructor (GH444_)
+- DataFrame.convert_objects method for :ref:`inferring better dtypes <basics.cast.infer>`
+  for object columns (GH302_)
+- Add :ref:`rolling_corr_pairwise <stats.moments.corr_pairwise>` function for
+  computing Panel of correlation matrices (GH189_)
+- Add :ref:`margins <reshaping.pivot.margins>` option to :ref:`pivot_table
+  <reshaping.pivot>` for computing subgroup aggregates (GH114_)
 - Add `Series.from_csv` function (PR #482)
+- Improve memory usage of `DataFrame.describe` (do not copy data
+  unnecessarily) (PR #425)
+- Use same formatting function for outputting floating point Series to console
+  as in DataFrame (PR #420)
+- DataFrame.delevel will try to infer better dtype for new columns (GH #440)
+- Exclude non-numeric types in DataFrame.{corr, cov}
+- Override Index.astype to enable dtype casting (GH #412)
+- Use same float formatting function for Series.__repr__ (PR #420)
+- Use available console width to output DataFrame columns (PR #453)
+- Accept ndarrays when setting items in Panel (GH #452)
+- Optimize scalar value lookups in the general case by 25% or more in Series
+  and DataFrame
+- Can pass DataFrame/DataFrame and DataFrame/Series to
+  rolling_corr/rolling_cov (GH #462)
+- Fix performance regression in cross-sectional count in DataFrame, affecting
+  DataFrame.dropna speed
+- Column deletion in DataFrame copies no data (computes views on blocks) (GH
+  #158)
+- MultiIndex.get_level_values can take the level name
+- More helpful error message when DataFrame.plot fails on one of the columns
+  (GH #478)
 
 Performance improvements
 ~~~~~~~~~~~~~~~~~~~~~~~~
 
+.. _GH114: https://github.com/wesm/pandas/issues/114
+.. _GH189: https://github.com/wesm/pandas/issues/302
+.. _GH302: https://github.com/wesm/pandas/issues/302
 .. _GH428: https://github.com/wesm/pandas/issues/428
 .. _GH437: https://github.com/wesm/pandas/issues/437
 .. _GH438: https://github.com/wesm/pandas/issues/438
+.. _GH444: https://github.com/wesm/pandas/issues/444
 .. _GH461: https://github.com/wesm/pandas/issues/461
 .. _GH463: https://github.com/wesm/pandas/issues/463
 .. _PR435: https://github.com/wesm/pandas/pull/435
+.. _PR453: https://github.com/wesm/pandas/pull/453
diff --git a/doc/source/whatsnew/v0.6.2.rst b/doc/source/whatsnew/v0.6.2.rst
index 59c330c83..25fdac6e4 100644
--- a/doc/source/whatsnew/v0.6.2.rst
+++ b/doc/source/whatsnew/v0.6.2.rst
@@ -22,53 +22,53 @@ New features
 Performance improvements
 ~~~~~~~~~~~~~~~~~~~~~~~~
 
-Cythonized GroupBy aggregations no longer presort the data, thus achieving a
-significant speedup (Issue93_)
-
-.. code-block:: ipython
-
-    In [5]: df
-    Out[5]:
-    <class 'pandas.core.frame.DataFrame'>
-    Int64Index: 100000 entries, 0 to 99999
-    Data columns:
-    data    100000  non-null values
-    key1    100000  non-null values
-    key2    100000  non-null values
-    dtypes: float64(1), object(2)
-
-    In [6]: df[:10]
-    Out[6]:
-       data     key1  key2
-    0  2.708    4     1
-    1 -1.945    2     4
-    2 -1.123    2     0
-    3  0.09592  0     0
-    4  2.328    0     3
-    5 -0.6481   0     0
-    6  0.2957   3     2
-    7  0.7336   2     1
-    8  0.371    2     4
-    9  1.009    2     4
-
-    In [6]: df.groupby(['key1', 'key2']).sum()
-    Out[6]:
-               data
-    key1 key2
-    0    0     114
-         1    -14.69
-         2     89.06
-         3    -61.31
-         4    -32.24
-    1    0     57.91
-         1    -16.08
-         2    -46.51
-         3     15.46
-         4     18.96
-    ...
-
-Here's a graph of the performance of this operation over time on a dataset with
-100,000 rows and 10,000 unique groups:
+Cythonized GroupBy aggregations (sum, mean, std) no longer presort the data,
+thus achieving a significant speedup (Issue93_). Here's a graph of the
+performance of this operation over time on a dataset with 100,000 rows and
+10,000 unique groups:
+
+
+.. .. code-block:: ipython
+
+..     In [5]: df
+..     Out[5]:
+..     <class 'pandas.core.frame.DataFrame'>
+..     Int64Index: 100000 entries, 0 to 99999
+..     Data columns:
+..     data    100000  non-null values
+..     key1    100000  non-null values
+..     key2    100000  non-null values
+..     dtypes: float64(1), object(2)
+
+..     In [6]: df[:10]
+..     Out[6]:
+..        data     key1  key2
+..     0  2.708    4     1
+..     1 -1.945    2     4
+..     2 -1.123    2     0
+..     3  0.09592  0     0
+..     4  2.328    0     3
+..     5 -0.6481   0     0
+..     6  0.2957   3     2
+..     7  0.7336   2     1
+..     8  0.371    2     4
+..     9  1.009    2     4
+
+..     In [6]: df.groupby(['key1', 'key2']).sum()
+..     Out[6]:
+..                data
+..     key1 key2
+..     0    0     114
+..          1    -14.69
+..          2     89.06
+..          3    -61.31
+..          4    -32.24
+..     1    0     57.91
+..          1    -16.08
+..          2    -46.51
+..          3     15.46
+..          4     18.96
+..     ...
 
 .. image:: vbench/figures/groupby_multi_cython.png
    :width: 6in
