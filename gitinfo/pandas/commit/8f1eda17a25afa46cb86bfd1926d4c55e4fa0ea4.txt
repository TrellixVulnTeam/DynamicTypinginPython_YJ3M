commit 8f1eda17a25afa46cb86bfd1926d4c55e4fa0ea4
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Dec 11 11:47:17 2012 -0500

    DOC: parser improvements

diff --git a/doc/source/io.rst b/doc/source/io.rst
index d123e8fcb..027db1061 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -9,6 +9,7 @@
    import os
    import csv
    from StringIO import StringIO
+   import pandas as pd
 
    import numpy as np
    np.random.seed(123456)
@@ -27,33 +28,6 @@
 IO Tools (Text, CSV, HDF5, ...)
 *******************************
 
-Clipboard
----------
-
-.. _io.clipboard:
-
-A handy way to grab data is to use the ``read_clipboard`` method, which takes
-the contents of the clipboard buffer and passes them to the ``read_table``
-method described in the next section. For instance, you can copy the following
-text to the clipboard (CTRL-C on many operating systems):
-
-.. code-block:: python
-
-     A B C
-   x 1 4 p
-   y 2 5 q
-   z 3 6 r
-
-And then import the data directly to a DataFrame by calling:
-
-.. code-block:: python
-
-   clipdf = read_clipboard(sep='\s*')
-
-.. ipython:: python
-
-   clipdf
-
 .. _io.read_csv_table:
 
 CSV & Text files
@@ -69,10 +43,15 @@ data into a DataFrame object. They can take a number of arguments:
   - ``sep`` or ``delimiter``: A delimiter / separator to split fields
     on. `read_csv` is capable of inferring the delimiter automatically in some
     cases by "sniffing." The separator may be specified as a regular
-    expression; for instance you may use '\s*' to indicate arbitrary
-    whitespace.
-  - ``dialect``: string or :class:`python:csv.Dialect` instance to expose more ways to specify
-    the file format
+    expression; for instance you may use '\|\s*' to indicate a pipe plus
+    arbitrary whitespace.
+  - ``delim_whitespace``: Parse whitespace-delimited (spaces or tabs) file
+    (much faster than using a regular expression)
+  - ``compression``: decompress ``'gzip'`` and ``'bz2'`` formats on the fly.
+  - ``dialect``: string or :class:`python:csv.Dialect` instance to expose more
+    ways to specify the file format
+  - ``dtype``: A data type name or a dict of column name to data type. If not
+    specified, data types will be inferred.
   - ``header``: row number to use as the column names, and the start of the data.
     Defaults to 0 (first row); specify None if there is no header row.
   - ``skiprows``: A collection of numbers for rows in the file to skip. Can
@@ -86,6 +65,8 @@ data into a DataFrame object. They can take a number of arguments:
     then you should explicitly pass header=None (behavior changed in v0.10.0).
   - ``na_values``: optional list of strings to recognize as NaN (missing
     values), either in addition to or in lieu of the default set.
+  - ``true_values``: list of strings to recognize as ``True``
+  - ``false_values``: list of strings to recognize as ``False``
   - ``keep_default_na``: whether to include the default set of missing values
     in addition to the ones specified in ``na_values``
   - ``parse_dates``: if True then index will be parsed as dates
@@ -120,8 +101,8 @@ data into a DataFrame object. They can take a number of arguments:
   - ``skip_footer``: number of lines to skip at bottom of file (default 0)
   - ``converters``: a dictionary of functions for converting values in certain
     columns, where keys are either integers or column labels
-  - ``encoding``: a string representing the encoding to use if the contents are
-    non-ascii
+  - ``encoding``: a string representing the encoding to use for decoding
+    unicode data, e.g. ``'utf-8``` or ``'latin-1'``.
   - ``verbose``: show number of NA values inserted in non-numeric columns
   - ``squeeze``: if True then output with only one column is turned into Series
 
@@ -142,24 +123,24 @@ The default for `read_csv` is to create a DataFrame with simple numbered rows:
 
 .. ipython:: python
 
-   read_csv('foo.csv')
+   pd.read_csv('foo.csv')
 
 In the case of indexed data, you can pass the column number or column name you
 wish to use as the index:
 
 .. ipython:: python
 
-   read_csv('foo.csv', index_col=0)
+   pd.read_csv('foo.csv', index_col=0)
 
 .. ipython:: python
 
-   read_csv('foo.csv', index_col='date')
+   pd.read_csv('foo.csv', index_col='date')
 
 You can also use a list of columns to create a hierarchical index:
 
 .. ipython:: python
 
-   read_csv('foo.csv', index_col=[0, 'A'])
+   pd.read_csv('foo.csv', index_col=[0, 'A'])
 
 .. _io.dialect:
 
@@ -190,19 +171,143 @@ We can get around this using ``dialect``
 
    dia = csv.excel()
    dia.quoting = csv.QUOTE_NONE
-   read_csv(StringIO(data), dialect=dia)
+   pd.read_csv(StringIO(data), dialect=dia)
+
+All of the dialect options can be specified separately by keyword arguments:
+
+.. ipython:: python
+
+    data = 'a,b,c~1,2,3~4,5,6'
+    pd.read_csv(StringIO(data), lineterminator='~')
 
 The parsers make every attempt to "do the right thing" and not be very
 fragile. Type inference is a pretty big deal. So if a column can be coerced to
 integer dtype without altering the contents, it will do so. Any non-numeric
 columns will come through as object dtype as with the rest of pandas objects.
 
+.. _io.dtypes:
+
+Specifying column data types
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+Starting with v0.10, you can indicate the data type for the whole DataFrame or
+individual columns:
+
+.. ipython:: python
+
+    data = 'a,b,c\n1,2,3\n4,5,6\n7,8,9'
+    print data
+
+    df = pd.read_csv(StringIO(data), dtype=object)
+    df
+    df['a'][0]
+    df = pd.read_csv(StringIO(data), dtype={'b': object, 'c': np.float64})
+    df.dtypes
+
+.. _io.headers:
+
+Handling column names
+~~~~~~~~~~~~~~~~~~~~~
+
+A file may or may not have a header row. pandas assumes the first row should be
+used as the column names:
+
+.. ipython:: python
+
+    from StringIO import StringIO
+    data = 'a,b,c\n1,2,3\n4,5,6\n7,8,9'
+    print data
+    pd.read_csv(StringIO(data))
+
+By specifying the ``names`` argument in conjunction with ``header`` you can
+indicate other names to use and whether or not to throw away the header row (if
+any):
+
+.. ipython:: python
+
+    print data
+    pd.read_csv(StringIO(data), names=['foo', 'bar', 'baz'])
+    pd.read_csv(StringIO(data), names=['foo', 'bar', 'baz'], header=None)
+
+If the header is in a row other than the first, pass the row number to
+``header``. This will skip the preceding rows:
+
+.. ipython:: python
+
+    data = 'skip this skip it\na,b,c\n1,2,3\n4,5,6\n7,8,9'
+    pd.read_csv(StringIO(data), header=1)
+
+.. _io.usecols:
+
+Filtering columns (``usecols``)
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The ``usecols`` argument allows you to select any subset of the columns in a
+file, either using the column names or position numbers:
+
+.. ipython:: python
+
+    data = 'a,b,c,d\n1,2,3,foo\n4,5,6,bar\n7,8,9,baz'
+    pd.read_csv(StringIO(data))
+    pd.read_csv(StringIO(data), usecols=['b', 'd'])
+    pd.read_csv(StringIO(data), usecols=[0, 2, 3])
+
+.. _io.unicode:
+
+Dealing with Unicode Data
+~~~~~~~~~~~~~~~~~~~~~~~~~
+
+The ``encoding`` argument should be used for encoded unicode data, which will
+result in byte strings being decoded to unicode in the result:
+
+.. ipython:: python
+
+   data = 'word,length\nTr\xe4umen,7\nGr\xfc\xdfe,5'
+   df = pd.read_csv(StringIO(data), encoding='latin-1')
+   df
+   df['word'][1]
+
+Some formats which encode all characters as multiple bytes, like UTF-16, won't
+parse correctly at all without specifying the encoding.
+
+.. _io.index_col:
+
+Index columns and trailing delimiters
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+If a file has one more column of data than the number of column names, the
+first column will be used as the DataFrame's row names:
+
+.. ipython:: python
+
+    data = 'a,b,c\n4,apple,bat,5.7\n8,orange,cow,10'
+    pd.read_csv(StringIO(data))
+
+.. ipython:: python
+
+    data = 'index,a,b,c\n4,apple,bat,5.7\n8,orange,cow,10'
+    pd.read_csv(StringIO(data), index_col=0)
+
+Ordinarily, you can achieve this behavior using the ``index_col`` option.
+
+There are some exception cases when a file has been prepared with delimiters at
+the end of each data line, confusing the parser. To explicitly disable the
+index column inference and discard the last column, pass ``index_col=False``:
+
+.. ipython:: python
+
+    data = 'a,b,c\n4,apple,bat,\n8,orange,cow,'
+    print data
+    pd.read_csv(StringIO(data))
+    pd.read_csv(StringIO(data), index_col=False)
+
 .. _io.parse_dates:
 
 Specifying Date Columns
 ~~~~~~~~~~~~~~~~~~~~~~~
 
-To better facilitate working with datetime data, :func:`~pandas.io.parsers.read_csv` and :func:`~pandas.io.parsers.read_table`
+To better facilitate working with datetime data,
+:func:`~pandas.io.parsers.read_csv` and :func:`~pandas.io.parsers.read_table`
 uses the keyword arguments ``parse_dates`` and ``date_parser`` to allow users
 to specify a variety of columns and date/time formats to turn the input text
 data into ``datetime`` objects.
@@ -212,7 +317,7 @@ The simplest case is to just pass in ``parse_dates=True``:
 .. ipython:: python
 
    # Use a column as an index, and parse it as dates.
-   df = read_csv('foo.csv', index_col=0, parse_dates=True)
+   df = pd.read_csv('foo.csv', index_col=0, parse_dates=True)
    df
 
    # These are python datetime objects
@@ -248,7 +353,7 @@ column names:
 .. ipython:: python
 
     print open('tmp.csv').read()
-    df = read_csv('tmp.csv', header=None, parse_dates=[[1, 2], [1, 3]])
+    df = pd.read_csv('tmp.csv', header=None, parse_dates=[[1, 2], [1, 3]])
     df
 
 By default the parser removes the component date columns, but you can choose
@@ -256,8 +361,8 @@ to retain them via the ``keep_date_col`` keyword:
 
 .. ipython:: python
 
-   df = read_csv('tmp.csv', header=None, parse_dates=[[1, 2], [1, 3]],
-                 keep_date_col=True)
+   df = pd.read_csv('tmp.csv', header=None, parse_dates=[[1, 2], [1, 3]],
+                    keep_date_col=True)
    df
 
 Note that if you wish to combine multiple columns into a single date column, a
@@ -271,7 +376,7 @@ You can also use a dict to specify custom name columns:
 .. ipython:: python
 
    date_spec = {'nominal': [1, 2], 'actual': [1, 3]}
-   df = read_csv('tmp.csv', header=None, parse_dates=date_spec)
+   df = pd.read_csv('tmp.csv', header=None, parse_dates=date_spec)
    df
 
 It is important to remember that if multiple text columns are to be parsed into
@@ -283,8 +388,8 @@ data columns:
 .. ipython:: python
 
    date_spec = {'nominal': [1, 2], 'actual': [1, 3]}
-   df = read_csv('tmp.csv', header=None, parse_dates=date_spec,
-                 index_col=0) #index is the nominal column
+   df = pd.read_csv('tmp.csv', header=None, parse_dates=date_spec,
+                    index_col=0) #index is the nominal column
    df
 
 **Note**: When passing a dict as the `parse_dates` argument, the order of
@@ -302,8 +407,8 @@ take full advantage of the flexiblity of the date parsing API:
 .. ipython:: python
 
    import pandas.io.date_converters as conv
-   df = read_csv('tmp.csv', header=None, parse_dates=date_spec,
-                 date_parser=conv.parse_date_time)
+   df = pd.read_csv('tmp.csv', header=None, parse_dates=date_spec,
+                    date_parser=conv.parse_date_time)
    df
 
 You can explore the date parsing functionality in ``date_converters.py`` and
@@ -337,9 +442,8 @@ DD/MM/YYYY instead. For convenience, a ``dayfirst`` keyword is provided:
 
    print open('tmp.csv').read()
 
-   read_csv('tmp.csv', parse_dates=[0])
-
-   read_csv('tmp.csv', dayfirst=True, parse_dates=[0])
+   pd.read_csv('tmp.csv', parse_dates=[0])
+   pd.read_csv('tmp.csv', dayfirst=True, parse_dates=[0])
 
 .. _io.thousands:
 
@@ -365,7 +469,7 @@ By default, integers with a thousands separator will be parsed as strings
 .. ipython:: python
 
     print open('tmp.csv').read()
-    df = read_csv('tmp.csv', sep='|')
+    df = pd.read_csv('tmp.csv', sep='|')
     df
 
     df.level.dtype
@@ -375,7 +479,7 @@ The ``thousands`` keyword allows integers to be parsed correctly
 .. ipython:: python
 
     print open('tmp.csv').read()
-    df = read_csv('tmp.csv', sep='|', thousands=',')
+    df = pd.read_csv('tmp.csv', sep='|', thousands=',')
     df
 
     df.level.dtype
@@ -410,14 +514,14 @@ By default, the parse includes the comments in the output:
 
 .. ipython:: python
 
-   df = read_csv('tmp.csv')
+   df = pd.read_csv('tmp.csv')
    df
 
 We can suppress the comments using the ``comment`` keyword:
 
 .. ipython:: python
 
-   df = read_csv('tmp.csv', comment='#')
+   df = pd.read_csv('tmp.csv', comment='#')
    df
 
 .. ipython:: python
@@ -446,7 +550,7 @@ as a ``Series``:
 
    print open('tmp.csv').read()
 
-   output =  read_csv('tmp.csv', squeeze=True)
+   output =  pd.read_csv('tmp.csv', squeeze=True)
    output
 
    type(output)
@@ -456,6 +560,58 @@ as a ``Series``:
 
    os.remove('tmp.csv')
 
+.. _io.boolean:
+
+Boolean values
+~~~~~~~~~~~~~~
+
+The common values ``True``, ``False``, ``TRUE``, and ``FALSE`` are all
+recognized as boolean. Sometime you would want to recognize some other values
+as being boolean. To do this use the ``true_values`` and ``false_values``
+options:
+
+.. ipython:: python
+
+    data= 'a,b,c\n1,Yes,2\n3,No,4'
+    print data
+    pd.read_csv(StringIO(data))
+    pd.read_csv(StringIO(data), true_values=['Yes'], false_values=['No'])
+
+.. _io.bad_lines:
+
+Handling "bad" lines
+~~~~~~~~~~~~~~~~~~~~
+
+Some files may have malformed lines with too few fields or too many. Lines with
+too few fields will have NA values filled in the trailing fields. Lines with
+too many will cause an error by default:
+
+.. ipython:: python
+   :suppress:
+
+    data = 'a,b,c\n1,2,3\n4,5,6,7\n8,9,10'
+
+.. code-block:: ipython
+
+    In [27]: data = 'a,b,c\n1,2,3\n4,5,6,7\n8,9,10'
+
+    In [28]: pd.read_csv(StringIO(data))
+    ---------------------------------------------------------------------------
+    CParserError                              Traceback (most recent call last)
+    CParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4
+
+You can elect to skip bad lines:
+
+.. code-block:: ipython
+
+    In [29]: pd.read_csv(StringIO(data), error_bad_lines=False)
+    Skipping line 3: expected 3 fields, saw 4
+
+    Out[29]:
+       a  b   c
+    0  1  2   3
+    1  8  9  10
+
 .. _io.fwf:
 
 Files with Fixed Width Columns
@@ -495,7 +651,7 @@ column specifications to the `read_fwf` function along with the file name:
 
    #Column specifications are a list of half-intervals
    colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]
-   df = read_fwf('bar.csv', colspecs=colspecs, header=None, index_col=0)
+   df = pd.read_fwf('bar.csv', colspecs=colspecs, header=None, index_col=0)
    df
 
 Note how the parser automatically picks column names X.<column number> when
@@ -506,7 +662,7 @@ column widths for contiguous columns:
 
    #Widths are a list of integers
    widths = [6, 14, 13, 10]
-   df = read_fwf('bar.csv', widths=widths, header=None)
+   df = pd.read_fwf('bar.csv', widths=widths, header=None)
    df
 
 The parser will take care of extra white spaces around the columns
@@ -539,14 +695,14 @@ as the index of the DataFrame:
 
 .. ipython:: python
 
-   read_csv('foo.csv')
+   pd.read_csv('foo.csv')
 
 Note that the dates weren't automatically parsed. In that case you would need
 to do as before:
 
 .. ipython:: python
 
-   df = read_csv('foo.csv', parse_dates=True)
+   df = pd.read_csv('foo.csv', parse_dates=True)
    df.index
 
 .. ipython:: python
@@ -571,7 +727,7 @@ column numbers to turn multiple columns into a ``MultiIndex``:
 
 .. ipython:: python
 
-   df = read_csv("data/mindex_ex.csv", index_col=[0,1])
+   df = pd.read_csv("data/mindex_ex.csv", index_col=[0,1])
    df
    df.ix[1978]
 
@@ -594,7 +750,7 @@ class of the csv module.
 .. ipython:: python
 
     print open('tmp2.sv').read()
-    read_csv('tmp2.sv')
+    pd.read_csv('tmp2.sv')
 
 .. _io.chunking:
 
@@ -608,7 +764,7 @@ rather than reading the entire file into memory, such as the following:
 .. ipython:: python
 
    print open('tmp.sv').read()
-   table = read_table('tmp.sv', sep='|')
+   table = pd.read_table('tmp.sv', sep='|')
    table
 
 
@@ -617,7 +773,7 @@ value will be an iterable object of type ``TextParser``:
 
 .. ipython:: python
 
-   reader = read_table('tmp.sv', sep='|', chunksize=4)
+   reader = pd.read_table('tmp.sv', sep='|', chunksize=4)
    reader
 
    for chunk in reader:
@@ -628,7 +784,7 @@ Specifying ``iterator=True`` will also return the ``TextParser`` object:
 
 .. ipython:: python
 
-   reader = read_table('tmp.sv', sep='|', iterator=True)
+   reader = pd.read_table('tmp.sv', sep='|', iterator=True)
    reader.get_chunk(5)
 
 .. ipython:: python
@@ -698,9 +854,36 @@ DataFrame object has an instance method ``to_html`` which renders the contents
 of the DataFrame as an html table. The function arguments are as in the method
 ``to_string`` described above.
 
+Clipboard
+---------
+
+.. _io.clipboard:
+
+A handy way to grab data is to use the ``read_clipboard`` method, which takes
+the contents of the clipboard buffer and passes them to the ``read_table``
+method described in the next section. For instance, you can copy the following
+text to the clipboard (CTRL-C on many operating systems):
+
+.. code-block:: python
+
+     A B C
+   x 1 4 p
+   y 2 5 q
+   z 3 6 r
+
+And then import the data directly to a DataFrame by calling:
+
+.. code-block:: python
+
+   clipdf = pd.read_clipboard(delim_whitespace=True)
+
+.. ipython:: python
+
+   clipdf
+
 
 Excel files
-----------------
+-----------
 
 The ``ExcelFile`` class can read an Excel 2003 file using the ``xlrd`` Python
 module and use the same parsing code as the above to convert tabular data into
diff --git a/doc/sphinxext/ipython_directive.py b/doc/sphinxext/ipython_directive.py
index 9b09ff95f..3b19b443a 100644
--- a/doc/sphinxext/ipython_directive.py
+++ b/doc/sphinxext/ipython_directive.py
@@ -352,7 +352,7 @@ class EmbeddedSphinxShell(object):
         self.cout.seek(0)
         output = self.cout.read()
         if not is_suppress and not is_semicolon:
-            ret.append(output)
+            ret.append(output.decode('utf-8'))
 
         if not is_okexcept and "Traceback" in output:
             sys.stdout.write(output)
