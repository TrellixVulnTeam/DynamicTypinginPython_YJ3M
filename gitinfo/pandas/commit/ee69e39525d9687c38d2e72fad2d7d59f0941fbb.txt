commit ee69e39525d9687c38d2e72fad2d7d59f0941fbb
Author: Chang She <changshe@gmail.com>
Date:   Sun Nov 18 15:58:34 2012 -0500

    ENH: google analytics integration using oauth2

diff --git a/.gitignore b/.gitignore
index d17c869c4..320f03a01 100644
--- a/.gitignore
+++ b/.gitignore
@@ -23,3 +23,5 @@ scikits
 pandas.egg-info
 *\#*\#
 .tox
+pandas/io/*.dat
+pandas/io/*.json
\ No newline at end of file
diff --git a/pandas/io/auth.py b/pandas/io/auth.py
new file mode 100644
index 000000000..471436cb1
--- /dev/null
+++ b/pandas/io/auth.py
@@ -0,0 +1,122 @@
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import sys
+import logging
+
+import httplib2
+
+import apiclient.discovery as gapi
+import gflags
+import oauth2client.file as auth_file
+import oauth2client.client as oauth
+import oauth2client.tools as tools
+OOB_CALLBACK_URN = oauth.OOB_CALLBACK_URN
+
+class AuthenticationConfigError(ValueError):
+    pass
+
+FLOWS = {}
+FLAGS = gflags.FLAGS
+DEFAULT_SECRETS = os.path.join(os.path.dirname(__file__), 'client_secrets.json')
+DEFAULT_SCOPE = 'https://www.googleapis.com/auth/analytics.readonly'
+DEFAULT_TOKEN_FILE = os.path.join(os.path.dirname(__file__), 'analytics.dat')
+MISSING_CLIENT_MSG = """
+WARNING: Please configure OAuth 2.0
+
+You need to populate the client_secrets.json file found at:
+
+   %s
+
+with information from the APIs Console <https://code.google.com/apis/console>.
+
+"""
+DOC_URL = ('https://developers.google.com/api-client-library/python/guide/'
+           'aaa_client_secrets')
+
+gflags.DEFINE_enum('logging_level', 'ERROR',
+                   ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
+                   'Set the level of logging detail.')
+
+# Name of file that will store the access and refresh tokens to access
+# the API without having to login each time. Make sure this file is in
+# a secure place.
+
+def process_flags(flags=[]):
+    """Uses the command-line flags to set the logging level.
+
+    Args:
+    argv: List of command line arguments passed to the python script.
+    """
+
+    # Let the gflags module process the command-line arguments.
+    try:
+      FLAGS(flags)
+    except gflags.FlagsError, e:
+      print '%s\nUsage: %s ARGS\n%s' % (e, str(flags), FLAGS)
+      sys.exit(1)
+
+    # Set the logging according to the command-line flag.
+    logging.getLogger().setLevel(getattr(logging, FLAGS.logging_level))
+
+def get_flow(secret, scope, redirect):
+    """
+    Retrieve an authentication flow object based on the given
+    configuration in the secret file name, the authentication scope,
+    and a redirect URN
+    """
+    key = (secret, scope, redirect)
+    flow = FLOWS.get(key, None)
+    if flow is None:
+        msg = MISSING_CLIENT_MSG % secret
+        if not os.path.exists(secret):
+            raise AuthenticationConfigError(msg)
+        flow = oauth.flow_from_clientsecrets(secret, scope,
+                                             redirect_uri=redirect,
+                                             message=msg)
+        FLOWS[key] = flow
+    return flow
+
+def make_token_store(fpath=None):
+    """create token storage from give file name"""
+    if fpath is None:
+        fpath = DEFAULT_TOKEN_FILE
+    return auth_file.Storage(fpath)
+
+def authenticate(flow, storage=None):
+    """
+    Try to retrieve a valid set of credentials from the token store if possible
+    Otherwise use the given authentication flow to obtain new credentials
+    and return an authenticated http object
+
+    Parameters
+    ----------
+    flow : authentication workflow
+    storage: token storage, default None
+    """
+    http = httplib2.Http()
+
+    # Prepare credentials, and authorize HTTP object with them.
+    credentials = storage.get()
+    if credentials is None or credentials.invalid:
+        credentials = tools.run(flow, storage)
+
+    http = credentials.authorize(http)
+    return http
+
+def init_service(http):
+    """
+    Use the given http object to build the analytics service object
+    """
+    return gapi.build('analytics', 'v3', http=http)
diff --git a/pandas/io/ga.py b/pandas/io/ga.py
new file mode 100644
index 000000000..a433a4add
--- /dev/null
+++ b/pandas/io/ga.py
@@ -0,0 +1,429 @@
+"""
+1. Goto https://code.google.com/apis/console
+2. Create new project
+3. Goto APIs and register for OAuth2.0 for installed applications
+4. Download JSON secret file and move into same directory as this file
+"""
+from datetime import datetime
+import numpy as np
+from pandas import DataFrame
+import pandas as pd
+import pandas.io.parsers as psr
+import pandas.lib as lib
+from pandas.io.date_converters import generic_parser
+import pandas.io.auth as auth
+from pandas.util.decorators import Appender, Substitution
+
+from apiclient.errors import HttpError
+from oauth2client.client import AccessTokenRefreshError
+
+TYPE_MAP = {u'INTEGER': int, u'FLOAT': float, u'TIME': int}
+
+NO_CALLBACK = auth.OOB_CALLBACK_URN
+DOC_URL = auth.DOC_URL
+
+_QUERY_PARAMS = """metrics : list of str
+    Un-prefixed metric names (e.g., 'visitors' and not 'ga:visitors')
+dimensions : list of str
+    Un-prefixed dimension variable names
+start_date : str/date/datetime
+end_date : str/date/datetime, optional
+    Defaults to today
+segment : list of str, optional
+filters : list of str, optional
+start_index : int, default 1
+max_results : int, default 10000
+    If >10000, must specify chunksize or ValueError will be raised"""
+
+_QUERY_DOC = """
+Construct a google analytics query using given parameters
+Metrics and dimensions do not need the 'ga:' prefix
+
+Parameters
+----------
+profile_id : str
+%s
+""" % _QUERY_PARAMS
+
+_GA_READER_DOC = """Given query parameters, return a DataFrame with all the data
+or an iterator that returns DataFrames containing chunks of the data
+
+Parameters
+----------
+%s
+sort : bool/list, default True
+    Sort output by index or list of columns
+chunksize : int, optional
+    If max_results >10000, specifies the number of rows per iteration
+index_col : str/list of str/dict, optional
+    If unspecified then dimension variables are set as index
+parse_dates : bool/list/dict, default True
+keep_date_col : boolean, default False
+date_parser : optional
+na_values : optional
+converters : optional
+dayfirst : bool, default False
+    Informs date parsing
+account_name : str, optional
+account_id : str, optional
+property_name : str, optional
+property_id : str, optional
+profile_name : str, optional
+profile_id : str, optional
+%%(extras)s
+Returns
+-------
+data : DataFrame or DataFrame yielding iterator
+""" % _QUERY_PARAMS
+
+_AUTH_PARAMS = """secrets : str, optional
+    File path to the secrets file
+scope : str, optional
+    Authentication scope
+token_file_name : str, optional
+    Path to token storage
+redirect : str, optional
+    Local host redirect if unspecified
+"""
+
+@Substitution(extras=_AUTH_PARAMS)
+@Appender(_GA_READER_DOC)
+def read_ga(metrics, dimensions, start_date, **kwargs):
+    lst = ['secrets', 'scope', 'token_file_name', 'redirect']
+    reader_kwds = dict((p, kwargs.pop(p)) for p in lst if p in kwargs)
+    reader = GAnalytics(**reader_kwds)
+    return reader.get_data(metrics=metrics, start_date=start_date,
+                           dimensions=dimensions, **kwargs)
+
+class OAuthDataReader(object):
+    """
+    Abstract class for handling OAuth2 authentication using the Google
+    oauth2client library
+    """
+    def __init__(self, scope, token_file_name, redirect):
+        """
+        Parameters
+        ----------
+        scope : str
+            Designates the authentication scope
+        token_file_name : str
+            Location of cache for authenticated tokens
+        redirect : str
+            Redirect URL
+        """
+        self.scope = scope
+        self.token_store = auth.make_token_store(token_file_name)
+        self.redirect_url = redirect
+
+    def authenticate(self, secrets):
+        """
+        Run the authentication process and return an authorized
+        http object
+
+        Parameters
+        ----------
+        secrets : str
+            File name for client secrets
+
+        Notes
+        -----
+        See google documention for format of secrets file
+        %s
+        """ % DOC_URL
+        flow = self._create_flow(secrets)
+        return auth.authenticate(flow, self.token_store)
+
+    def _create_flow(self, secrets):
+        """
+        Create an authentication flow based on the secrets file
+
+        Parameters
+        ----------
+        secrets : str
+            File name for client secrets
+
+        Notes
+        -----
+        See google documentation for format of secrets file
+        %s
+        """ % DOC_URL
+        return auth.get_flow(secrets, self.scope, self.redirect_url)
+
+
+class GDataReader(OAuthDataReader):
+    """
+    Abstract class for reading data from google APIs using OAuth2
+    Subclasses must implement create_query method
+    """
+    def __init__(self, scope=auth.DEFAULT_SCOPE,
+                 token_file_name=auth.DEFAULT_TOKEN_FILE,
+                 redirect=NO_CALLBACK, secrets=auth.DEFAULT_SECRETS):
+        super(GDataReader, self).__init__(scope, token_file_name, redirect)
+        self._service = self._init_service(secrets)
+
+    @property
+    def service(self):
+        """The authenticated request service object"""
+        return self._service
+
+    def _init_service(self, secrets):
+        """
+        Build an authenticated google api request service using the given
+        secrets file
+        """
+        http = self.authenticate(secrets)
+        return auth.init_service(http)
+
+    def get_account(self, name=None, id=None, **kwargs):
+        """
+        Retrieve an account that matches the name, id, or some account attribute
+        specified in **kwargs
+
+        Parameters
+        ----------
+        name : str, optional
+        id : str, optional
+        """
+        accounts = self.service.management().accounts().list().execute()
+        return _get_match(accounts, name, id, **kwargs)
+
+    def get_web_property(self, account_id=None, name=None, id=None, **kwargs):
+        """
+        Retrieve a web property given and account and property name, id, or
+        custom attribute
+
+        Parameters
+        ----------
+        account_id : str, optional
+        name : str, optional
+        id : str, optional
+        """
+        prop_store = self.service.management().webproperties()
+        kwds = {}
+        if account_id is not None:
+            kwds['accountId'] = account_id
+        prop_for_acct = prop_store.list(**kwds).execute()
+        return _get_match(prop_for_acct, name, id, **kwargs)
+
+    def get_profile(self, account_id=None, web_property_id=None, name=None,
+                    id=None, **kwargs):
+
+        """
+        Retrieve the right profile for the given account, web property, and
+        profile attribute (name, id, or arbitrary parameter in kwargs)
+
+        Parameters
+        ----------
+        account_id : str, optional
+        web_property_id : str, optional
+        name : str, optional
+        id : str, optional
+        """
+        profile_store = self.service.management().profiles()
+        kwds = {}
+        if account_id is not None:
+            kwds['accountId'] = account_id
+        if web_property_id is not None:
+            kwds['webPropertyId'] = web_property_id
+        profiles = profile_store.list(**kwds).execute()
+        return _get_match(profiles, name, id, **kwargs)
+
+    def create_query(self, *args, **kwargs):
+        raise NotImplementedError()
+
+    @Substitution(extras='')
+    @Appender(_GA_READER_DOC)
+    def get_data(self, metrics, start_date, end_date=None,
+                 dimensions=None, segment=None, filters=None, start_index=1,
+                 max_results=10000, index_col=None, parse_dates=True,
+                 keep_date_col=False, date_parser=None, na_values=None,
+                 converters=None, sort=True, dayfirst=False,
+                 account_name=None, account_id=None, property_name=None,
+                 property_id=None, profile_name=None, profile_id=None,
+                 chunksize=None):
+        if chunksize is None and max_results > 10000:
+            raise ValueError('Google API returns maximum of 10,000 rows, '
+                             'please set chunksize')
+
+        account = self.get_account(account_name, account_id)
+        web_property = self.get_web_property(account.get('id'), property_name,
+                                             property_id)
+        profile = self.get_profile(account.get('id'), web_property.get('id'),
+                                   profile_name, profile_id)
+
+        profile_id = profile.get('id')
+
+        if index_col is None and dimensions is not None:
+            if isinstance(dimensions, basestring):
+                dimensions = [dimensions]
+            index_col = _clean_index(list(dimensions), parse_dates)
+
+        def _read(start, result_size):
+            query = self.create_query(profile_id, metrics, start_date,
+                                      end_date=end_date, dimensions=dimensions,
+                                      segment=segment, filters=filters,
+                                      start_index=start,
+                                      max_results=result_size)
+
+            try:
+                rs = query.execute()
+                rows = rs.get('rows', [])
+                col_info = rs.get('columnHeaders', [])
+                return self._parse_data(rows, col_info, index_col,
+                                        parse_dates=parse_dates,
+                                        keep_date_col=keep_date_col,
+                                        date_parser=date_parser,
+                                        dayfirst=dayfirst,
+                                        na_values=na_values,
+                                        converters=converters, sort=sort)
+            except HttpError, inst:
+                raise ValueError('Google API error %s: %s' % (inst.resp.status,
+                                 inst._get_reason()))
+
+
+        if chunksize is None:
+            return _read(start_index, max_results)
+
+        def iterator():
+            curr_start = start_index
+
+            while curr_start < max_results:
+                yield _read(curr_start, chunksize)
+                curr_start += chunksize
+        return iterator()
+
+    def _parse_data(self, rows, col_info, index_col, parse_dates=True,
+                    keep_date_col=False, date_parser=None, dayfirst=False,
+                    na_values=None, converters=None, sort=True):
+        # TODO use returned column types
+        col_names = _get_col_names(col_info)
+        df = psr._read(rows, dict(index_col=index_col, parse_dates=parse_dates,
+                                  date_parser=date_parser, dayfirst=dayfirst,
+                                  na_values=na_values,
+                                  keep_date_col=keep_date_col,
+                                  converters=converters,
+                                  header=None, names=col_names))
+
+        if isinstance(sort, bool) and sort:
+            return df.sort_index()
+        elif isinstance(sort, (basestring, list, tuple, np.ndarray)):
+            return df.sort_index(by=sort)
+
+        return df
+
+
+class GAnalytics(GDataReader):
+
+    @Appender(_QUERY_DOC)
+    def create_query(self, profile_id, metrics, start_date, end_date=None,
+                     dimensions=None, segment=None, filters=None,
+                     start_index=None, max_results=10000, **kwargs):
+        qry = format_query(profile_id, metrics, start_date, end_date=end_date,
+                           dimensions=dimensions, segment=segment,
+                           filters=filters, start_index=start_index,
+                           max_results=max_results, **kwargs)
+        try:
+            return self.service.data().ga().get(**qry)
+        except TypeError, error:
+            raise ValueError('Error making query: %s' % error)
+
+
+def format_query(ids, metrics, start_date, end_date=None, dimensions=None,
+                 segment=None, filters=None, sort=None, start_index=None,
+                 max_results=10000, **kwargs):
+    if isinstance(metrics, basestring):
+        metrics = [metrics]
+    met =','.join(['ga:%s' % x for x in metrics])
+
+    start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')
+    if end_date is None:
+        end_date = datetime.today()
+    end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')
+
+    qry = dict(ids='ga:%s' % str(ids),
+               metrics=met,
+               start_date=start_date,
+               end_date=end_date)
+    qry.update(kwargs)
+
+    names = ['dimensions', 'segment', 'filters', 'sort']
+    lst = [dimensions, segment, filters, sort]
+    [_maybe_add_arg(qry, n, d) for n, d in zip(names, lst)]
+
+    if start_index is not None:
+        qry['start_index'] = str(start_index)
+
+    if max_results is not None:
+        qry['max_results'] = str(max_results)
+
+    return qry
+
+def _maybe_add_arg(query, field, data):
+    if data is not None:
+        if isinstance(data, basestring):
+            data = [data]
+        data = ','.join(['ga:%s' % x for x in data])
+        query[field] = data
+
+def _get_match(obj_store, name, id, **kwargs):
+    key, val = None, None
+    if len(kwargs) > 0:
+        key = kwargs.keys()[0]
+        val = kwargs.values()[0]
+
+    if name is None and id is None and key is None:
+        return obj_store.get('items')[0]
+
+    name_ok = lambda item: name is not None and item.get('name') == name
+    id_ok = lambda item: id is not None and item.get('id') == id
+    key_ok = lambda item: key is not None and item.get(key) == val
+
+    match = None
+    if obj_store.get('items'):
+        # TODO look up gapi for faster lookup
+        for item in obj_store.get('items'):
+            if name_ok(item) or id_ok(item) or key_ok(item):
+                return item
+
+def _clean_index(index_dims, parse_dates):
+    _should_add = lambda lst: pd.Index(lst).isin(index_dims).all()
+    to_remove = []
+    to_add = []
+
+    if isinstance(parse_dates, (list, tuple, np.ndarray)):
+        for lst in parse_dates:
+            if isinstance(lst, (list, tuple, np.ndarray)):
+                if _should_add(lst):
+                    to_add.append('_'.join(lst))
+                to_remove.extend(lst)
+    elif isinstance(parse_dates, dict):
+        for name, lst in parse_dates.iteritems():
+            if isinstance(lst, (list, tuple, np.ndarray)):
+                if _should_add(lst):
+                    to_add.append(name)
+                to_remove.extend(lst)
+
+    index_dims = pd.Index(index_dims)
+    to_remove = pd.Index(set(to_remove))
+    to_add = pd.Index(set(to_add))
+
+    return index_dims - to_remove + to_add
+
+
+def _get_col_names(header_info):
+    return [x['name'][3:] for x in header_info]
+
+def _get_column_types(header_info):
+    return [(x['name'][3:], x['columnType']) for x in header_info]
+
+def _get_dim_names(header_info):
+    return [x['name'][3:] for x in header_info
+            if x['columnType'] == u'DIMENSION']
+
+def _get_met_names(header_info):
+    return [x['name'][3:] for x in header_info
+            if x['columnType'] == u'METRIC']
+
+def _get_data_types(header_info):
+    return [(x['name'][3:], TYPE_MAP.get(x['dataType'], object))
+            for x in header_info]
diff --git a/pandas/io/tests/test_ga.py b/pandas/io/tests/test_ga.py
new file mode 100644
index 000000000..c3ea62de0
--- /dev/null
+++ b/pandas/io/tests/test_ga.py
@@ -0,0 +1,82 @@
+import unittest
+import nose
+import httplib2
+
+import pandas as pd
+import pandas.core.common as com
+from pandas import DataFrame
+from pandas.io.ga import GAnalytics, read_ga
+from pandas.io.auth import AuthenticationConfigError
+from pandas.util.testing import network, assert_frame_equal
+from numpy.testing.decorators import slow
+
+class TestGoogle(unittest.TestCase):
+
+    @slow
+    @network
+    def test_getdata(self):
+        try:
+            reader = GAnalytics()
+            df = reader.get_data(
+                metrics=['avgTimeOnSite', 'visitors', 'newVisits',
+                         'pageviewsPerVisit'],
+                start_date='2005-1-1',
+                dimensions=['date', 'hour'],
+                parse_dates={'ts' : ['date', 'hour']})
+
+            assert isinstance(df, DataFrame)
+            assert isinstance(df.index, pd.DatetimeIndex)
+            assert len(df) > 1
+            assert 'date' not in df
+            assert 'hour' not in df
+            assert df.index.name == 'ts'
+            assert 'avgTimeOnSite' in df
+            assert 'visitors' in df
+            assert 'newVisits' in df
+            assert 'pageviewsPerVisit' in df
+
+            df2 = read_ga(
+                metrics=['avgTimeOnSite', 'visitors', 'newVisits',
+                         'pageviewsPerVisit'],
+                start_date='2005-1-1',
+                dimensions=['date', 'hour'],
+                parse_dates={'ts' : ['date', 'hour']})
+
+            assert_frame_equal(df, df2)
+
+            it = reader.get_data(
+                metrics='visitors',
+                start_date='2005-1-1',
+                dimensions='date',
+                max_results=10, chunksize=5)
+
+            df1 = it.next()
+            df2 = it.next()
+
+            for df in [df1, df2]:
+                assert isinstance(df, DataFrame)
+                assert isinstance(df.index, pd.DatetimeIndex)
+                assert len(df) == 5
+                assert 'date' not in df
+                assert df.index.name == 'date'
+                assert 'visitors' in df
+
+            assert (df2.index > df1.index).all()
+
+
+
+        except AuthenticationConfigError:
+            raise nose.SkipTest
+        except httplib2.ServerNotFoundError:
+            try:
+                h = httplib2.Http()
+                response, content = h.request("http://www.google.com")
+                raise
+            except httplib2.ServerNotFoundError:
+                raise nose.SkipTest
+
+
+if __name__ == '__main__':
+    import nose
+    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
+                   exit=False)
