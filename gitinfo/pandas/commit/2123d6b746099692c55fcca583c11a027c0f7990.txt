commit 2123d6b746099692c55fcca583c11a027c0f7990
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Feb 7 18:22:47 2012 -0500

    ENH: refactor groupby and accelerate groupby with integer keys, add vbenchmark

diff --git a/RELEASE.rst b/RELEASE.rst
index a1a9343bf..3a8b86b9c 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -261,6 +261,7 @@ pandas 0.7.0
     value (GH #730)
   - Fix Index.format bug causing incorrectly string-formatted Series with
     datetime indexes (# 758)
+  - Fix errors caused by object dtype arrays passed to ols (GH #759)
 
 Thanks
 ------
diff --git a/doc/make.py b/doc/make.py
index 081c2c199..d7bfbed68 100755
--- a/doc/make.py
+++ b/doc/make.py
@@ -29,6 +29,7 @@ def sf():
     'push a copy to the sf site'
     os.system('cd build/html; rsync -avz . wesmckinn,pandas@web.sf.net'
               ':/home/groups/p/pa/pandas/htdocs/ -essh --cvs-exclude')
+
 def sfpdf():
     'push a copy to the sf site'
     os.system('cd build/latex; scp pandas.pdf wesmckinn,pandas@web.sf.net'
diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 475172cc5..17c3a12dc 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -21,14 +21,25 @@ def match(values, index):
     """
     if com.is_float_dtype(index):
         return _match_generic(values, index, lib.Float64HashTable,
-                              _ensure_float64)
+                              com._ensure_float64)
     elif com.is_integer_dtype(index):
         return _match_generic(values, index, lib.Int64HashTable,
-                              _ensure_int64)
+                              com._ensure_int64)
     else:
         return _match_generic(values, index, lib.PyObjectHashTable,
-                              _ensure_object)
-
+                              com._ensure_object)
+
+def _get_hash_table_and_cast(values):
+    if com.is_float_dtype(values):
+        klass = lib.Float64HashTable
+        values = com._ensure_float64(values)
+    elif com.is_integer_dtype(values):
+        klass = lib.Int64HashTable
+        values = com._ensure_int64(values)
+    else:
+        klass = lib.PyObjectHashTable
+        values = com._ensure_object(values)
+    return klass, values
 
 def count(values, uniques=None):
     if uniques is not None:
@@ -36,13 +47,13 @@ def count(values, uniques=None):
     else:
         if com.is_float_dtype(values):
             return _count_generic(values, lib.Float64HashTable,
-                                  _ensure_float64)
+                                  com._ensure_float64)
         elif com.is_integer_dtype(values):
             return _count_generic(values, lib.Int64HashTable,
-                                  _ensure_int64)
+                                  com._ensure_int64)
         else:
             return _count_generic(values, lib.PyObjectHashTable,
-                                  _ensure_object)
+                                  com._ensure_object)
 
 def _count_generic(values, table_type, type_caster):
     values = type_caster(values)
@@ -58,23 +69,42 @@ def _match_generic(values, index, table_type, type_caster):
     table.map_locations(index)
     return table.lookup(values)
 
-def factorize(values):
-    pass
+def factorize(values, sort=False, order=None, na_sentinel=-1):
+    """
+    Encode input values as an enumerated type or categorical variable
 
-def unique(values):
-    pass
+    Parameters
+    ----------
+    values : sequence
+    sort :
+    order :
+
+    Returns
+    -------
+    """
+    hash_klass, values = _get_hash_table_and_cast(values)
+
+    uniques = []
+    table = hash_klass(len(values))
+    labels, counts = table.get_labels(values, uniques, 0, na_sentinel)
+
+    uniques = com._asarray_tuplesafe(uniques)
+    if sort and len(counts) > 0:
+        sorter = uniques.argsort()
+        reverse_indexer = np.empty(len(sorter), dtype=np.int32)
+        reverse_indexer.put(sorter, np.arange(len(sorter)))
+
+        mask = labels < 0
+        labels = reverse_indexer.take(labels)
+        np.putmask(labels, mask, -1)
 
-def _ensure_float64(arr):
-    if arr.dtype != np.float64:
-        arr = arr.astype(np.float64)
-    return arr
+        uniques = uniques.take(sorter)
+        counts = counts.take(sorter)
 
-def _ensure_int64(arr):
-    if arr.dtype != np.int64:
-        arr = arr.astype(np.int64)
-    return arr
+    return labels, uniques, counts
 
-def _ensure_object(arr):
-    if arr.dtype != np.object_:
-        arr = arr.astype('O')
-    return arr
+def unique(values):
+    """
+
+    """
+    pass
diff --git a/pandas/core/common.py b/pandas/core/common.py
index cd109bac2..0b86ce3e9 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -533,10 +533,7 @@ def is_float_dtype(arr_or_dtype):
 
 
 def _ensure_float64(arr):
-    try:
-        if arr.dtype != np.float64:
-            arr = arr.astype(np.float64)
-    except AttributeError:
+    if arr.dtype != np.float64:
         arr = arr.astype(np.float64)
     return arr
 
diff --git a/pandas/core/format.py b/pandas/core/format.py
index 367f031e6..45a575ad2 100644
--- a/pandas/core/format.py
+++ b/pandas/core/format.py
@@ -516,6 +516,9 @@ class IntArrayFormatter(GenericArrayFormatter):
 
 
 def _make_fixed_width(strings, justify='right'):
+    if len(strings) == 0:
+        return strings
+
     max_len = max(len(x) for x in strings)
     if justify == 'left':
         justfunc = lambda self, x: self.ljust(x)
@@ -533,7 +536,7 @@ def _trim_zeros(str_floats):
     """
     # TODO: what if exponential?
     trimmed = str_floats
-    while all([x[-1] == '0' for x in trimmed]):
+    while len(str_floats) > 0 and all([x[-1] == '0' for x in trimmed]):
         trimmed = [x[:-1] for x in trimmed]
 
     # trim decimal points
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index bd79f9af8..4555e11ef 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -10,6 +10,7 @@ from pandas.core.internals import BlockManager, make_block
 from pandas.core.series import Series
 from pandas.core.panel import Panel
 from pandas.util.decorators import cache_readonly, Appender
+import pandas.core.algorithms as algos
 import pandas.core.common as com
 import pandas._tseries as lib
 
@@ -151,10 +152,6 @@ class GroupBy(object):
     def _obj_with_exclusions(self):
         return self.obj
 
-    @property
-    def _group_shape(self):
-        return tuple(ping.ngroups for ping in self.groupings)
-
     def __getattr__(self, attr):
         if hasattr(self.obj, attr) and attr != '_cache':
             return self._make_wrapper(attr)
@@ -226,9 +223,8 @@ class GroupBy(object):
 
     def _multi_iter(self):
         data = self.obj
-        group_index = self._group_index
-        comp_ids, obs_ids = _compress_group_index(group_index)
-        ngroups = len(obs_ids)
+
+        comp_ids, _, ngroups = self._group_info
         label_list = [ping.labels for ping in self.groupings]
         level_list = [ping.group_index for ping in self.groupings]
         mapper = _KeyMapper(comp_ids, ngroups, label_list, level_list)
@@ -343,9 +339,7 @@ class GroupBy(object):
         # TODO: address inefficiencies, like duplicating effort (should
         # aggregate all the columns at once?)
 
-        group_index = self._group_index
-        comp_ids, obs_group_ids = _compress_group_index(group_index)
-        max_group = len(obs_group_ids)
+        comp_ids, obs_group_ids, max_group = self._group_info
 
         output = {}
         for name, obj in self._iterate_slices():
@@ -366,9 +360,7 @@ class GroupBy(object):
     def _python_agg_general(self, func, *args, **kwargs):
         agg_func = lambda x: func(x, *args, **kwargs)
 
-        group_index = self._group_index
-        comp_ids, obs_group_ids = _compress_group_index(group_index)
-        max_group = len(obs_group_ids)
+        comp_ids, obs_group_ids, max_group = self._group_info
 
         # iterate through "columns" ex exclusions to populate output dict
         output = {}
@@ -390,10 +382,23 @@ class GroupBy(object):
         return self._wrap_aggregated_output(output, mask, obs_group_ids)
 
     @property
-    def _group_index(self):
-        result = get_group_index([ping.labels for ping in self.groupings],
-                                 self._group_shape)
-        return com._ensure_int64(result)
+    def _group_info(self):
+        if len(self.groupings) > 1:
+            all_labels = [ping.labels for ping in self.groupings]
+            group_index = get_group_index(all_labels, self._group_shape)
+            comp_ids, obs_group_ids = _compress_group_index(group_index)
+        else:
+            ping = self.groupings[0]
+            group_index = ping.labels
+
+        comp_ids, obs_group_ids = _compress_group_index(group_index)
+        ngroups = len(obs_group_ids)
+        comp_ids = com._ensure_int32(comp_ids)
+        return comp_ids, obs_group_ids, ngroups
+
+    @property
+    def _group_shape(self):
+        return tuple(ping.ngroups for ping in self.groupings)
 
     def _get_multi_index(self, mask, obs_ids):
         masked = [labels for _, labels in
@@ -637,26 +642,10 @@ class Grouping(object):
         if self._was_factor:  # pragma: no cover
             raise Exception('Should not call this method grouping by level')
         else:
-            values = com._ensure_object(self.grouper)
-
-            # khash
-            rizer = lib.Factorizer(len(values))
-            labels, counts = rizer.factorize(values, sort=False)
-
-            uniques = Index(rizer.uniques, name=self.name)
-            if self.sort and len(counts) > 0:
-                sorter = uniques.argsort()
-                reverse_indexer = np.empty(len(sorter), dtype=np.int32)
-                reverse_indexer.put(sorter, np.arange(len(sorter)))
-
-                mask = labels < 0
-                labels = reverse_indexer.take(labels)
-                np.putmask(labels, mask, -1)
-
-                uniques = uniques.take(sorter)
-                counts = counts.take(sorter)
-
-            self._labels = labels
+            labs, uniques, counts = algos.factorize(self.grouper,
+                                                    sort=self.sort)
+            uniques = Index(uniques, name=self.name)
+            self._labels = labs
             self._group_index = uniques
             self._counts = counts
 
@@ -948,12 +937,9 @@ class DataFrameGroupBy(GroupBy):
 
             yield val, slicer(val)
 
-
     def _cython_agg_general(self, how):
 
-        group_index = self._group_index
-        comp_ids, obs_group_ids = _compress_group_index(group_index)
-        max_group = len(obs_group_ids)
+        comp_ids, obs_group_ids, max_group = self._group_info
 
         obj = self._obj_with_exclusions
         if self.axis == 1:
@@ -990,7 +976,7 @@ class DataFrameGroupBy(GroupBy):
                 output_keys.extend(b.items)
             try:
                 output_keys.sort()
-            except TypeError:  # pragma
+            except TypeError:  # pragma: no cover
                 pass
 
             if isinstance(agg_labels, MultiIndex):
@@ -1455,9 +1441,8 @@ def cython_aggregate(values, group_index, ngroups, how='add'):
 
     trans_func = _cython_transforms.get(how, lambda x: x)
 
+    # will be filled in Cython function
     result = np.empty(out_shape, dtype=np.float64)
-    result.fill(np.nan)
-
     counts = np.zeros(ngroups, dtype=np.int32)
 
     agg_func(result, counts, values, group_index)
@@ -1523,3 +1508,13 @@ def _groupby_indices(values):
     if values.dtype != np.object_:
         values = values.astype('O')
     return lib.groupby_indices(values)
+
+def numpy_groupby(data, labels, axis=0):
+    s = np.argsort(labels)
+    keys, inv = np.unique(labels, return_inverse = True)
+    i = inv.take(s)
+    groups_at = np.where(i != np.concatenate(([-1], i[:-1])))[0]
+    ordered_data = data.take(s, axis=axis)
+    group_sums = np.add.reduceat(ordered_data, groups_at, axis=axis)
+
+    return group_sums
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 5109a9110..ff7764f80 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -259,21 +259,29 @@ class Index(np.ndarray):
         """
         Render a string representation of the Index
         """
-        result = []
+        from pandas.core.format import format_array
+
         header = []
         if name:
             header.append(str(self.name) if self.name is not None else '')
 
         if self.is_all_dates:
             zero_time = time(0, 0)
+            result = []
             for dt in self:
                 if dt.time() != zero_time or dt.tzinfo is not None:
                     return header + ['%s' % x for x in self]
                 result.append(dt.strftime("%Y-%m-%d"))
             return header + result
 
-        result.extend(com._stringify(x) for x in self)
+        values = self.values
+        if values.dtype == np.object_:
+            values = lib.maybe_convert_objects(values)
 
+        if values.dtype == np.object_:
+            result = [com._stringify(x) for x in values]
+        else:
+            result = _trim_front(format_array(values, None))
         return header + result
 
     def equals(self, other):
@@ -2160,6 +2168,14 @@ def _union_indexes(indexes):
     else:
         return Index(lib.fast_unique_multiple_list(indexes))
 
+def _trim_front(strings):
+    """
+    Trims zeros and decimal points
+    """
+    trimmed = strings
+    while len(strings) > 0 and all([x[0] == ' ' for x in trimmed]):
+        trimmed = [x[1:] for x in trimmed]
+    return trimmed
 
 def _sanitize_and_check(indexes):
     kinds = list(set([type(index) for index in indexes]))
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
index e88a40e11..270a33347 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/src/hashtable.pyx
@@ -494,7 +494,7 @@ cdef class Int64HashTable:
         return reverse, labels, counts
 
     def get_labels(self, ndarray[int64_t] values, list uniques,
-                   Py_ssize_t count_prior):
+                   Py_ssize_t count_prior, Py_ssize_t na_sentinel):
         cdef:
             Py_ssize_t i, n = len(values)
             ndarray[int32_t] labels
@@ -909,7 +909,7 @@ cdef class Int64Factorizer:
 
     def factorize(self, ndarray[int64_t] values, sort=False):
         labels, counts = self.table.get_labels(values, self.uniques,
-                                               self.count)
+                                               self.count, -1)
 
         # sort on
         if sort:
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index 8e337b013..668819d21 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -923,6 +923,17 @@ class TestGroupBy(unittest.TestCase):
                            'b': ['foo', 'bar'] * 25})
         self.assertRaises(GroupByError, frame.groupby('a')['b'].mean)
 
+        frame = DataFrame({'a': np.random.randint(0, 5, 50),
+                           'b': ['foo', 'bar'] * 25})
+        self.assertRaises(GroupByError, frame[['b']].groupby(frame['a']).mean)
+
+    def test_wrap_aggregated_output_multindex(self):
+        df = self.mframe.T
+        df['baz', 'two'] = 'peekaboo'
+
+        agged = df.groupby([0, 0, 1]).agg(np.mean)
+        self.assert_(isinstance(agged.columns, MultiIndex))
+
     def test_grouping_attrs(self):
         deleveled = self.mframe.reset_index()
         grouped = deleveled.groupby(['first', 'second'])
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 974f01f79..1ae4c72a0 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -57,3 +57,17 @@ df = DataFrame(randn(1000, 1000))
 
 groupby_frame_cython_many_columns = Benchmark('df.groupby(labels).sum()', setup,
                                               start_date=datetime(2011, 8, 1))
+
+#----------------------------------------------------------------------
+# single key, long, integer key
+
+setup = common_setup + """
+data = np.random.randn(100000, 1)
+labels = np.random.randint(0, 1000, size=100000)
+df = DataFrame(data)
+"""
+
+groupby_frame_singlekey_integer = \
+    Benchmark('df.groupby(labels).sum()', setup,
+              start_date=datetime(2011, 8, 1))
+
