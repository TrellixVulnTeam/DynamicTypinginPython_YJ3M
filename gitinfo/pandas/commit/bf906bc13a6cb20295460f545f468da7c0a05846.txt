commit bf906bc13a6cb20295460f545f468da7c0a05846
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Sun Dec 2 17:39:45 2012 -0500

    ENH: more refactoring. add consolidation checks to speed up concatenation

diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 941a2d135..3d77b526a 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -316,7 +316,15 @@ class GroupBy(object):
         -------
         applied : type depending on grouped object and function
         """
-        return self._python_apply_general(func, *args, **kwargs)
+        func = _intercept_function(func)
+        f = lambda g: func(g, *args, **kwargs)
+        return self._python_apply_general(f)
+
+    def _python_apply_general(self, f):
+        keys, values, mutated = self.grouper.apply(f, self.obj, self.axis)
+
+        return self._wrap_applied_output(keys, values,
+                                         not_indexed_same=mutated)
 
     def aggregate(self, func, *args, **kwargs):
         raise NotImplementedError
@@ -433,19 +441,19 @@ class GroupBy(object):
 
     def _python_agg_general(self, func, *args, **kwargs):
         func = _intercept_function(func)
-        agg_func = lambda x: func(x, *args, **kwargs)
+        f = lambda x: func(x, *args, **kwargs)
 
         # iterate through "columns" ex exclusions to populate output dict
         output = {}
         for name, obj in self._iterate_slices():
             try:
-                result, counts = self.grouper.agg_series(obj, agg_func)
+                result, counts = self.grouper.agg_series(obj, f)
                 output[name] = result
             except TypeError:
                 continue
 
         if len(output) == 0:
-            return self._python_apply_general(func, *args, **kwargs)
+            return self._python_apply_general(f)
 
         if self.grouper._filter_empty_groups:
             mask = counts.ravel() > 0
@@ -454,30 +462,6 @@ class GroupBy(object):
 
         return self._wrap_aggregated_output(output)
 
-    def _python_apply_general(self, func, *args, **kwargs):
-        func = _intercept_function(func)
-
-        result_keys = []
-        result_values = []
-
-        not_indexed_same = False
-        for key, group in self:
-            object.__setattr__(group, 'name', key)
-
-            # group might be modified
-            group_axes = _get_axes(group)
-
-            res = func(group, *args, **kwargs)
-
-            if not _is_indexed_like(res, group_axes):
-                not_indexed_same = True
-
-            result_keys.append(key)
-            result_values.append(res)
-
-        return self._wrap_applied_output(result_keys, result_values,
-                                         not_indexed_same=not_indexed_same)
-
     def _wrap_applied_output(self, *args, **kwargs):
         raise NotImplementedError
 
@@ -504,17 +488,6 @@ class GroupBy(object):
         return result
 
 
-def _generate_groups(obj, group_index, ngroups, axis=0):
-    if isinstance(obj, NDFrame) and not isinstance(obj, DataFrame):
-        factory = obj._constructor
-        obj = obj._data
-    else:
-        factory = None
-
-    return generate_groups(obj, group_index, ngroups,
-                           axis=axis, factory=factory)
-
-
 @Appender(GroupBy.__doc__)
 def groupby(obj, by, **kwds):
     if isinstance(obj, Series):
@@ -567,7 +540,7 @@ class Grouper(object):
     def nkeys(self):
         return len(self.groupings)
 
-    def get_iterator(self, data, axis=0):
+    def get_iterator(self, data, axis=0, keep_internal=True):
         """
         Groupby iterator
 
@@ -576,30 +549,44 @@ class Grouper(object):
         Generator yielding sequence of (name, subsetted object)
         for each group
         """
-        if len(self.groupings) == 1:
-            indices = self.indices
-            groups = indices.keys()
-            try:
-                groups = sorted(groups)
-            except Exception:  # pragma: no cover
-                pass
+        comp_ids, _, ngroups = self.group_info
+        splitter = get_splitter(data, comp_ids, ngroups, axis=axis,
+                                keep_internal=keep_internal)
 
-            for name in groups:
-                inds = indices[name]
-                group = data.take(inds, axis=axis)
-                yield name, group
+        if len(self.groupings) == 1:
+            levels = self.groupings[0].group_index
+            for i, group in splitter:
+                yield levels[i], group
         else:
             # provide "flattened" iterator for multi-group setting
-            comp_ids, _, ngroups = self.group_info
-            label_list = self.labels
-            level_list = self.levels
-            mapper = _KeyMapper(comp_ids, ngroups, label_list, level_list)
-
-            for label, group in _generate_groups(data, comp_ids, ngroups,
-                                                 axis=axis):
-                key = mapper.get_key(label)
+            mapper = _KeyMapper(comp_ids, ngroups, self.labels, self.levels)
+            for i, group in splitter:
+                key = mapper.get_key(i)
                 yield key, group
 
+    def apply(self, f, data, axis=0):
+        result_keys = []
+        result_values = []
+
+        mutated = False
+        splitter = self.get_iterator(data, axis=axis)
+
+        for key, group in splitter:
+            object.__setattr__(group, 'name', key)
+
+            # group might be modified
+            group_axes = _get_axes(group)
+
+            res = f(group)
+
+            if not _is_indexed_like(res, group_axes):
+                mutated = True
+
+            result_keys.append(key)
+            result_values.append(res)
+
+        return result_keys, result_values, mutated
+
     @cache_readonly
     def indices(self):
         if len(self.groupings) == 1:
@@ -848,8 +835,9 @@ class Grouper(object):
 
         group_index, _, ngroups = self.group_info
 
-        for label, group in _generate_groups(obj, group_index, ngroups,
-                                             axis=self.axis):
+        splitter = get_splitter(obj, group_index, ngroups, axis=self.axis)
+
+        for label, group in splitter:
             res = func(group)
             if result is None:
                 if isinstance(res, np.ndarray) or isinstance(res, list):
@@ -1925,30 +1913,6 @@ class DataFrameGroupBy(NDFrameGroupBy):
 
         return result
 
-    def _python_apply_general(self, func, *args, **kwargs):
-        func = _intercept_function(func)
-
-        result_keys = []
-        result_values = []
-
-        not_indexed_same = False
-        for key, group in self:
-            object.__setattr__(group, 'name', key)
-
-            # group might be modified
-            group_axes = _get_axes(group)
-
-            res = func(group, *args, **kwargs)
-
-            if not _is_indexed_like(res, group_axes):
-                not_indexed_same = True
-
-            result_keys.append(key)
-            result_values.append(res)
-
-        return self._wrap_applied_output(result_keys, result_values,
-                                         not_indexed_same=not_indexed_same)
-
 
 from pandas.tools.plotting import boxplot_frame_groupby
 DataFrameGroupBy.boxplot = boxplot_frame_groupby
@@ -2038,7 +2002,7 @@ class NDArrayGroupBy(GroupBy):
 
 class DataSplitter(object):
 
-    def __init__(self, data, labels, ngroups, axis=0):
+    def __init__(self, data, labels, ngroups, axis=0, keep_internal=False):
         self.data = data
         self.labels = com._ensure_int64(labels)
         self.ngroups = ngroups
@@ -2051,7 +2015,10 @@ class DataSplitter(object):
     def __iter__(self):
         sdata = self._get_sorted_data()
 
-        starts, ends = lib.generate_slices(self.sort_idx, self.ngroups)
+        if self.ngroups == 0:
+            raise StopIteration
+
+        starts, ends = lib.generate_slices(self.slabels, self.ngroups)
 
         for i, (start, end) in enumerate(zip(starts, ends)):
             # Since I'm now compressing the group ids, it's now not "possible"
@@ -2066,9 +2033,9 @@ class DataSplitter(object):
         return self.data.take(self.sort_idx, axis=self.axis)
 
     def _chop(self, sdata, slice_obj):
-        raise NotImplementedError
+        return sdata[slice_obj]
 
-    def apply(self, f, keep_internal=False):
+    def apply(self, f):
         raise NotImplementedError
 
 
@@ -2079,65 +2046,55 @@ class ArraySplitter(DataSplitter):
 class SeriesSplitter(DataSplitter):
 
     def _chop(self, sdata, slice_obj):
-        return sdata._get_values(slob)
-
+        return sdata._get_values(slice_obj)
 
 class FrameSplitter(DataSplitter):
-    pass
 
-#----------------------------------------------------------------------
-# Grouping generator for BlockManager
+    def __init__(self, data, labels, ngroups, axis=0, keep_internal=False):
+        DataSplitter.__init__(self, data, labels, ngroups, axis=axis,
+                              keep_internal=keep_internal)
 
-def generate_groups(data, group_index, ngroups, axis=0, factory=lambda x: x):
-    """
-    Parameters
-    ----------
-    data : BlockManager
+    def _chop(self, sdata, slice_obj):
+        if self.axis == 0:
+            return sdata[slice_obj]
+        else:
+            return sdata._slice(slice_obj, axis=1) # ix[:, slice_obj]
 
-    Returns
-    -------
-    generator
-    """
-    group_index = com._ensure_int64(group_index)
-    indexer = _algos.groupsort_indexer(group_index, ngroups)[0]
-    group_index = com.ndtake(group_index, indexer)
+class NDFrameSplitter(DataSplitter):
+
+    def __init__(self, data, labels, ngroups, axis=0, keep_internal=False):
+        DataSplitter.__init__(self, data, labels, ngroups, axis=axis,
+                              keep_internal=keep_internal)
+
+        self.factory = data._constructor
+
+    def _get_sorted_data(self):
+        # this is the BlockManager
+        data = self.data._data
 
-    if isinstance(data, BlockManager):
         # this is sort of wasteful but...
-        sorted_axis = data.axes[axis].take(indexer)
-        sorted_data = data.reindex_axis(sorted_axis, axis=axis)
+        sorted_axis = data.axes[self.axis].take(self.sort_idx)
+        sorted_data = data.reindex_axis(sorted_axis, axis=self.axis)
+
+        return sorted_data
+
+    def _chop(self, sdata, slice_obj):
+        return self.factory(sdata.get_slice(slice_obj, axis=self.axis))
+
+
+def get_splitter(data, *args, **kwargs):
     if isinstance(data, Series):
-        sorted_axis = data.index.take(indexer)
-        sorted_data = data.reindex(sorted_axis)
+        klass = SeriesSplitter
     elif isinstance(data, DataFrame):
-        sorted_data = data.take(indexer, axis=axis)
+        klass = FrameSplitter
+    else:
+        klass = NDFrameSplitter
 
-    if isinstance(sorted_data, DataFrame):
-        def _get_slice(slob):
-            if axis == 0:
-                return sorted_data[slob]
-            else:
-                return sorted_data.ix[:, slob]
-    elif isinstance(sorted_data, BlockManager):
-        def _get_slice(slob):
-            return factory(sorted_data.get_slice(slob, axis=axis))
-    elif isinstance(sorted_data, Series):
-        def _get_slice(slob):
-            return sorted_data._get_values(slob)
-    else:  # pragma: no cover
-        def _get_slice(slob):
-            return sorted_data[slob]
+    return klass(data, *args, **kwargs)
 
-    starts, ends = lib.generate_slices(group_index, ngroups)
 
-    for i, (start, end) in enumerate(zip(starts, ends)):
-        # Since I'm now compressing the group ids, it's now not "possible" to
-        # produce empty slices because such groups would not be observed in the
-        # data
-        if start >= end:
-            raise AssertionError('Start %s must be less than end %s'
-                                 % (str(start), str(end)))
-        yield i, _get_slice(slice(start, end))
+#----------------------------------------------------------------------
+# Misc utilities
 
 
 def get_group_index(label_list, shape):
@@ -2254,6 +2211,7 @@ class _KeyMapper(object):
                      for table, level in zip(self.tables, self.levels))
 
 
+
 def _get_indices_dict(label_list, keys):
     shape = [len(x) for x in keys]
     group_index = get_group_index(label_list, shape)
@@ -2268,6 +2226,7 @@ def _get_indices_dict(label_list, keys):
 
     return lib.indices_fast(sorter, group_index, keys, sorted_labels)
 
+
 #----------------------------------------------------------------------
 # sorting levels...cleverly?
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 79a123be1..b5471c479 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -482,7 +482,7 @@ class BlockManager(object):
     -----
     This is *not* a public API class
     """
-    __slots__ = ['axes', 'blocks']
+    __slots__ = ['axes', 'blocks', '_known_consolidated', '_is_consolidated']
 
     def __init__(self, blocks, axes, do_integrity_check=True):
         self.axes = [_ensure_index(ax) for ax in axes]
@@ -499,6 +499,8 @@ class BlockManager(object):
         if do_integrity_check:
             self._verify_integrity()
 
+        self._consolidate_check()
+
     @classmethod
     def make_empty(self):
         return BlockManager([], [[], []])
@@ -602,8 +604,14 @@ class BlockManager(object):
         """
         Return True if more than one block with the same dtype
         """
+        if not self._known_consolidated:
+            self._consolidate_check()
+        return self._is_consolidated
+
+    def _consolidate_check(self):
         dtypes = [blk.dtype.type for blk in self.blocks]
-        return len(dtypes) == len(set(dtypes))
+        self._is_consolidated = len(dtypes) == len(set(dtypes))
+        self._known_consolidated = True
 
     def get_numeric_data(self, copy=False, type_list=None):
         """
@@ -848,6 +856,8 @@ class BlockManager(object):
 
     def _consolidate_inplace(self):
         self.blocks = _consolidate(self.blocks, self.items)
+        self._is_consolidated = true
+        self._known_consolidated = True
 
     def get(self, item):
         _, block = self._find_block(item)
@@ -902,6 +912,7 @@ class BlockManager(object):
         new_items = self.items.delete(loc)
 
         self.set_items_norename(new_items)
+        self._known_consolidated = False
 
     def set(self, item, value):
         """
@@ -937,6 +948,8 @@ class BlockManager(object):
             # insert at end
             self.insert(len(self.items), item, value)
 
+        self._known_consolidated = False
+
     def insert(self, loc, item, value):
         if item in self.items:
             raise Exception('cannot insert %s, already exists' % item)
@@ -950,6 +963,8 @@ class BlockManager(object):
         if len(self.blocks) > 100:
             self._consolidate_inplace()
 
+        self._known_consolidated = False
+
     def set_items_norename(self, value):
         value = _ensure_index(value)
         self.axes[0] = value
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index d8270806c..64e932369 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -3690,13 +3690,12 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         os.remove(path)
 
     def test_to_csv_from_csv_w_some_infs(self):
-        import tempfile
-        path = tempfile.mktemp()
-        path += '__tmp__'
+        path = '__%s__' % tm.rands(10)
 
         # test roundtrip with inf, -inf, nan, as full columns and mix
         self.frame['G'] = np.nan
-        self.frame['H'] = self.frame.index.map(lambda x: [np.inf, np.nan][np.random.rand() < .5])
+        f = lambda x: [np.inf, np.nan][np.random.rand() < .5]
+        self.frame['H'] = self.frame.index.map(f)
 
         self.frame.to_csv(path)
         recons = DataFrame.from_csv(path)
@@ -3704,7 +3703,10 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
         assert_frame_equal(self.frame, recons)
         assert_frame_equal(np.isinf(self.frame), np.isinf(recons))
 
-        os.remove(path)
+        try:
+            os.remove(path)
+        except os.error:
+            pass
 
     def test_to_csv_from_csv_w_all_infs(self):
         import tempfile
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index 68a2bc8a6..439006544 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -226,7 +226,7 @@ groupby_simple_compress_timing = \
 
 setup = common_setup + """
 N = 10000
-labels = np.random.randint(0, 1000, size=N)
+labels = np.random.randint(0, 2000, size=N)
 labels2 = np.random.randint(0, 3, size=N)
 df = DataFrame({'key': labels,
                 'key2': labels2,
diff --git a/vb_suite/join_merge.py b/vb_suite/join_merge.py
index 0830c7b30..d031e78b0 100644
--- a/vb_suite/join_merge.py
+++ b/vb_suite/join_merge.py
@@ -181,6 +181,13 @@ pieces = pieces * 50
 concat_series_axis1 = Benchmark('concat(pieces, axis=1)', setup,
                                 start_date=datetime(2012, 2, 27))
 
+setup = common_setup + """
+df = DataFrame(randn(5, 4))
+"""
+
+concat_small_frames = Benchmark('concat([df] * 1000)', setup,
+                                start_date=datetime(2012, 1, 1))
+
 #----------------------------------------------------------------------
 # Ordered merge
 
