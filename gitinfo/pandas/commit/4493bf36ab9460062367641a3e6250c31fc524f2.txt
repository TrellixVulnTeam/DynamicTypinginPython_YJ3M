commit 4493bf36ab9460062367641a3e6250c31fc524f2
Author: jreback <jeff@reback.net>
Date:   Tue Jul 16 21:41:52 2013 -0400

    CLN: rebase to 0.12
    
    BUG: groupby filter that return a series/ndarray truth testing
    
    BUG: refixed GH3880, prop name index
    
    BUG: not handling sparse block deletes in internals/_delete_from_block
    
    BUG: refix generic/truncate
    
    TST: refixed generic/replace (bug in core/internals/putmask) revealed as well
    
    TST: fix spare_array to put up correct type exceptions rather than Exception
    
    CLN: cleanups
    
    BUG: fix stata dtype inference (error in core/internals/astype)
    
    BUG: fix ujson handling of new series object
    
    BUG: fixed scalar coercion (e.g. calling float(series)) to work
    
    BUG: fixed astyping with and w/o copy
    
    ENH: added _propogate_attributes method to generic.py to allow
         subclasses to automatically propogate things like name
    
    DOC: added v0.13.0.txt feature descriptions
    
    CLN: pep8ish cleanups
    
    BUG: fix 32-bit,numpy 1.6.1 issue with datetimes in astype_nansafe
    
    PERF: speedup for groupby by passing a SNDArray (Series like ndarray) object to evaluation functions
          if allowed, can avoid Series creation overhead
    
    BUG: issue with older numpy (1.6.1) in SeriesGrouper, fallback to passing a Series
         rather than SNDArray
    
    DOC: release notes & doc updates
    
    DOC: fixup doc build failures
    
    DOC: change pasing of direct ndarrays to cython doc functions (enhancedperformance.rst)

diff --git a/doc/source/basics.rst b/doc/source/basics.rst
index c37776b3a..a0818831f 100644
--- a/doc/source/basics.rst
+++ b/doc/source/basics.rst
@@ -478,7 +478,7 @@ maximum value for each column occurred:
 
    tsdf = DataFrame(randn(1000, 3), columns=['A', 'B', 'C'],
                     index=date_range('1/1/2000', periods=1000))
-   tsdf.apply(lambda x: x.index[x.dropna().argmax()])
+   tsdf.apply(lambda x: x[x.idxmax()])
 
 You may also pass additional arguments and keyword arguments to the ``apply``
 method. For instance, consider the following function you would like to apply:
diff --git a/doc/source/dsintro.rst b/doc/source/dsintro.rst
index c1d034d0d..a913bdc35 100644
--- a/doc/source/dsintro.rst
+++ b/doc/source/dsintro.rst
@@ -44,10 +44,15 @@ When using pandas, we recommend the following import convention:
 Series
 ------
 
-:class:`Series` is a one-dimensional labeled array (technically a subclass of
-ndarray) capable of holding any data type (integers, strings, floating point
-numbers, Python objects, etc.). The axis labels are collectively referred to as
-the **index**. The basic method to create a Series is to call:
+.. warning::
+
+   In 0.13.0 ``Series`` has internaly been refactored to no longer sub-class ``ndarray``
+   but instead subclass ``NDFrame``, similarly to the rest of the pandas containers. This should be
+   a transparent change with only very limited API implications (See the :ref:`release notes <release.refactoring_0_13_0>`)
+
+:class:`Series` is a one-dimensional labeled array capable of holding any data
+type (integers, strings, floating point numbers, Python objects, etc.). The axis
+labels are collectively referred to as the **index**. The basic method to create a Series is to call:
 
 ::
 
@@ -109,9 +114,8 @@ provided. The value will be repeated to match the length of **index**
 Series is ndarray-like
 ~~~~~~~~~~~~~~~~~~~~~~
 
-As a subclass of ndarray, Series is a valid argument to most NumPy functions
-and behaves similarly to a NumPy array. However, things like slicing also slice
-the index.
+``Series`` acts very similary to a ``ndarray``, and is a valid argument to most NumPy functions.
+However, things like slicing also slice the index.
 
 .. ipython :: python
 
@@ -177,7 +181,7 @@ labels.
 
 The result of an operation between unaligned Series will have the **union** of
 the indexes involved. If a label is not found in one Series or the other, the
-result will be marked as missing (NaN). Being able to write code without doing
+result will be marked as missing ``NaN``. Being able to write code without doing
 any explicit data alignment grants immense freedom and flexibility in
 interactive data analysis and research. The integrated data alignment features
 of the pandas data structures set pandas apart from the majority of related
@@ -924,11 +928,11 @@ Here we slice to a Panel4D.
     from pandas.core import panelnd
     Panel5D = panelnd.create_nd_panel_factory(
         klass_name   = 'Panel5D',
-        axis_orders  = [ 'cool', 'labels','items','major_axis','minor_axis'],
-        axis_slices  = { 'labels' : 'labels', 'items' : 'items',
-	                 'major_axis' : 'major_axis', 'minor_axis' : 'minor_axis' },
-        slicer       = Panel4D,
-        axis_aliases = { 'major' : 'major_axis', 'minor' : 'minor_axis' },
+        orders  = [ 'cool', 'labels','items','major_axis','minor_axis'],
+        slices  = { 'labels' : 'labels', 'items' : 'items',
+	                'major_axis' : 'major_axis', 'minor_axis' : 'minor_axis' },
+        slicer  = Panel4D,
+        aliases = { 'major' : 'major_axis', 'minor' : 'minor_axis' },
         stat_axis    = 2)
 
     p5d = Panel5D(dict(C1 = p4d))
diff --git a/doc/source/enhancingperf.rst b/doc/source/enhancingperf.rst
index 2fd606daa..95428bd27 100644
--- a/doc/source/enhancingperf.rst
+++ b/doc/source/enhancingperf.rst
@@ -26,7 +26,7 @@ Enhancing Performance
 Cython (Writing C extensions for pandas)
 ----------------------------------------
 
-For many use cases writing pandas in pure python and numpy is sufficient. In some 
+For many use cases writing pandas in pure python and numpy is sufficient. In some
 computationally heavy applications however, it can be possible to achieve sizeable
 speed-ups by offloading work to `cython <http://cython.org/>`__.
 
@@ -68,7 +68,7 @@ Here's the function in pure python:
 We achieve our result by by using ``apply`` (row-wise):
 
 .. ipython:: python
-   
+
    %timeit df.apply(lambda x: integrate_f(x['a'], x['b'], x['N']), axis=1)
 
 But clearly this isn't fast enough for us. Let's take a look and see where the
@@ -83,7 +83,7 @@ By far the majority of time is spend inside either ``integrate_f`` or ``f``,
 hence we'll concentrate our efforts cythonizing these two functions.
 
 .. note::
- 
+
   In python 2 replacing the ``range`` with its generator counterpart (``xrange``)
   would mean the ``range`` line would vanish. In python 3 range is already a generator.
 
@@ -125,7 +125,7 @@ is here to distinguish between function versions):
 
    %timeit df.apply(lambda x: integrate_f_plain(x['a'], x['b'], x['N']), axis=1)
 
-Already this has shaved a third off, not too bad for a simple copy and paste. 
+Already this has shaved a third off, not too bad for a simple copy and paste.
 
 .. _enhancingperf.type:
 
@@ -175,7 +175,7 @@ in python, so maybe we could minimise these by cythonizing the apply part.
   We are now passing ndarrays into the cython function, fortunately cython plays
   very nicely with numpy.
 
-.. ipython:: 
+.. ipython::
 
    In [4]: %%cython
       ...: cimport numpy as np
@@ -205,6 +205,24 @@ The implementation is simple, it creates an array of zeros and loops over
 the rows, applying our ``integrate_f_typed``, and putting this in the zeros array.
 
 
+.. warning::
+
+   In 0.13.0 since ``Series`` has internaly been refactored to no longer sub-class ``ndarray``
+   but instead subclass ``NDFrame``, you can **not pass** a ``Series`` directly as a ``ndarray`` typed parameter
+   to a cython function. Instead pass the actual ``ndarray`` using the ``.values`` attribute of the Series.
+
+   Prior to 0.13.0
+
+   .. code-block:: python
+
+        apply_integrate_f(df['a'], df['b'], df['N'])
+
+   Use ``.values`` to get the underlying ``ndarray``
+
+   .. code-block:: python
+
+        apply_integrate_f(df['a'].values, df['b'].values, df['N'].values)
+
 .. note::
 
     Loop like this would be *extremely* slow in python, but in cython looping over
@@ -212,13 +230,13 @@ the rows, applying our ``integrate_f_typed``, and putting this in the zeros arra
 
 .. ipython:: python
 
-   %timeit apply_integrate_f(df['a'], df['b'], df['N'])
+   %timeit apply_integrate_f(df['a'].values, df['b'].values, df['N'].values)
 
 We've gone another three times faster! Let's check again where the time is spent:
 
 .. ipython:: python
 
-   %prun -l 4 apply_integrate_f(df['a'], df['b'], df['N'])
+   %prun -l 4 apply_integrate_f(df['a'].values, df['b'].values, df['N'].values)
 
 As one might expect, the majority of the time is now spent in ``apply_integrate_f``,
 so if we wanted to make anymore efficiencies we must continue to concentrate our
@@ -261,7 +279,7 @@ advanced cython techniques:
 
 .. ipython:: python
 
-   %timeit apply_integrate_f_wrap(df['a'], df['b'], df['N'])
+   %timeit apply_integrate_f_wrap(df['a'].values, df['b'].values, df['N'].values)
 
 This shaves another third off!
 
diff --git a/doc/source/release.rst b/doc/source/release.rst
index d761f1f00..ddf25c87b 100644
--- a/doc/source/release.rst
+++ b/doc/source/release.rst
@@ -115,6 +115,68 @@ pandas 0.13
     - ``MultiIndex.astype()`` now only allows ``np.object_``-like dtypes and
       now returns a ``MultiIndex`` rather than an ``Index``. (:issue:`4039`)
 
+**Internal Refactoring**
+
+.. _release.refactoring_0_13_0:
+
+In 0.13.0 there is a major refactor primarily to subclass ``Series`` from ``NDFrame``,
+which is the base class currently for ``DataFrame`` and ``Panel``, to unify methods
+and behaviors. Series formerly subclassed directly from ``ndarray``.
+
+- Refactor of series.py/frame.py/panel.py to move common code to generic.py
+  - added _setup_axes to created generic NDFrame structures
+  - moved methods
+
+    - from_axes,_wrap_array,axes,ix,shape,empty,swapaxes,transpose,pop
+    - __iter__,keys,__contains__,__len__,__neg__,__invert__
+    - convert_objects,as_blocks,as_matrix,values
+    - __getstate__,__setstate__ (though compat remains in frame/panel)
+    - __getattr__,__setattr__
+    - _indexed_same,reindex_like,reindex,align,where,mask
+    - filter (also added axis argument to selectively filter on a different axis)
+    - reindex,reindex_axis (which was the biggest change to make generic)
+    - truncate (moved to become part of ``NDFrame``)
+
+- These are API changes which make ``Panel`` more consistent with ``DataFrame``
+  - swapaxes on a Panel with the same axes specified now return a copy
+  - support attribute access for setting
+  - filter supports same api as original DataFrame filter
+
+- Reindex called with no arguments will now return a copy of the input object
+
+- Series now inherits from ``NDFrame`` rather than directly from ``ndarray``.
+  There are several minor changes that affect the API.
+
+  - numpy functions that do not support the array interface will now
+    return ``ndarrays`` rather than series, e.g. ``np.diff`` and ``np.where``
+  - ``Series(0.5)`` would previously return the scalar ``0.5``, this is no
+    longer supported
+  - several methods from frame/series have moved to ``NDFrame``
+    (convert_objects,where,mask)
+  - ``TimeSeries`` is now an alias for ``Series``. the property ``is_time_series``
+    can be used to distinguish (if desired)
+
+- Refactor of Sparse objects to use BlockManager
+
+  - Created a new block type in internals, ``SparseBlock``, which can hold multi-dtypes
+    and is non-consolidatable. ``SparseSeries`` and ``SparseDataFrame`` now inherit
+    more methods from there hierarchy (Series/DataFrame), and no longer inherit
+    from ``SparseArray`` (which instead is the object of the ``SparseBlock``)
+  - Sparse suite now supports integration with non-sparse data. Non-float sparse
+    data is supportable (partially implemented)
+  - Operations on sparse structures within DataFrames should preserve sparseness,
+    merging type operations will convert to dense (and back to sparse), so might
+    be somewhat inefficient
+  - enable setitem on ``SparseSeries`` for boolean/integer/slices
+  - ``SparsePanels`` implementation is unchanged (e.g. not using BlockManager, needs work)
+
+- added ``ftypes`` method to Series/DataFame, similar to ``dtypes``, but indicates
+  if the underlying is sparse/dense (as well as the dtype)
+
+- All ``NDFrame`` objects now have a ``_prop_attributes``, which can be used to indcated various
+  values to propogate to a new object from an existing (e.g. name in ``Series`` will follow
+  more automatically now)
+
 **Experimental Features**
 
 **Bug Fixes**
diff --git a/doc/source/v0.13.0.txt b/doc/source/v0.13.0.txt
index bac8cb319..b64cea0d5 100644
--- a/doc/source/v0.13.0.txt
+++ b/doc/source/v0.13.0.txt
@@ -134,6 +134,67 @@ Enhancements
          from pandas import offsets
          td + offsets.Minute(5) + offsets.Milli(5)
 
+Internal Refactoring
+~~~~~~~~~~~~~~~~~~~~
+
+In 0.13.0 there is a major refactor primarily to subclass ``Series`` from ``NDFrame``,
+which is the base class currently for ``DataFrame`` and ``Panel``, to unify methods
+and behaviors. Series formerly subclassed directly from ``ndarray``. (:issue:`4080`,:issue:`3862`,:issue:`816`)
+
+- Refactor of series.py/frame.py/panel.py to move common code to generic.py
+  - added _setup_axes to created generic NDFrame structures
+  - moved methods
+
+    - from_axes,_wrap_array,axes,ix,shape,empty,swapaxes,transpose,pop
+    - __iter__,keys,__contains__,__len__,__neg__,__invert__
+    - convert_objects,as_blocks,as_matrix,values
+    - __getstate__,__setstate__ (though compat remains in frame/panel)
+    - __getattr__,__setattr__
+    - _indexed_same,reindex_like,reindex,align,where,mask
+    - filter (also added axis argument to selectively filter on a different axis)
+    - reindex,reindex_axis (which was the biggest change to make generic)
+    - truncate (moved to become part of ``NDFrame``)
+
+- These are API changes which make ``Panel`` more consistent with ``DataFrame``
+  - swapaxes on a Panel with the same axes specified now return a copy
+  - support attribute access for setting
+  - filter supports same api as original DataFrame filter
+
+- Reindex called with no arguments will now return a copy of the input object
+
+- Series now inherits from ``NDFrame`` rather than directly from ``ndarray``.
+  There are several minor changes that affect the API.
+
+  - numpy functions that do not support the array interface will now
+    return ``ndarrays`` rather than series, e.g. ``np.diff`` and ``np.where``
+  - ``Series(0.5)`` would previously return the scalar ``0.5``, this is no
+    longer supported
+  - several methods from frame/series have moved to ``NDFrame``
+    (convert_objects,where,mask)
+  - ``TimeSeries`` is now an alias for ``Series``. the property ``is_time_series``
+    can be used to distinguish (if desired)
+
+- Refactor of Sparse objects to use BlockManager
+
+  - Created a new block type in internals, ``SparseBlock``, which can hold multi-dtypes
+    and is non-consolidatable. ``SparseSeries`` and ``SparseDataFrame`` now inherit
+    more methods from there hierarchy (Series/DataFrame), and no longer inherit
+    from ``SparseArray`` (which instead is the object of the ``SparseBlock``)
+  - Sparse suite now supports integration with non-sparse data. Non-float sparse
+    data is supportable (partially implemented)
+  - Operations on sparse structures within DataFrames should preserve sparseness,
+    merging type operations will convert to dense (and back to sparse), so might
+    be somewhat inefficient
+  - enable setitem on ``SparseSeries`` for boolean/integer/slices
+  - ``SparsePanels`` implementation is unchanged (e.g. not using BlockManager, needs work)
+
+- added ``ftypes`` method to Series/DataFame, similar to ``dtypes``, but indicates
+  if the underlying is sparse/dense (as well as the dtype)
+
+- All ``NDFrame`` objects now have a ``_prop_attributes``, which can be used to indcated various
+  values to propogate to a new object from an existing (e.g. name in ``Series`` will follow
+  more automatically now)
+
 Bug Fixes
 ~~~~~~~~~
 
diff --git a/pandas/core/array.py b/pandas/core/array.py
index c9a8a00b7..6847ba073 100644
--- a/pandas/core/array.py
+++ b/pandas/core/array.py
@@ -34,3 +34,19 @@ for _f in _lift_random:
     globals()[_f] = getattr(np.random, _f)
 
 NA = np.nan
+
+#### a series-like ndarray ####
+
+class SNDArray(Array):
+
+    def __new__(cls, data, index=None, name=None):
+        data = data.view(SNDArray)
+        data.index = index
+        data.name = name
+
+        return data
+
+    @property
+    def values(self):
+        return self.view(Array)
+
diff --git a/pandas/core/base.py b/pandas/core/base.py
index a587b18ca..04f48f85f 100644
--- a/pandas/core/base.py
+++ b/pandas/core/base.py
@@ -9,20 +9,6 @@ class StringMixin(object):
     """implements string methods so long as object defines a `__unicode__` method.
     Handles Python2/3 compatibility transparently."""
     # side note - this could be made into a metaclass if more than one object nees
-    def __str__(self):
-
-class PandasObject(object):
-    """ The base class for pandas objects """
-
-    #----------------------------------------------------------------------
-    # Reconstruction
-
-    def save(self, path):
-        com.save(self, path)
-
-    @classmethod
-    def load(cls, path):
-        return com.load(path)
 
     #----------------------------------------------------------------------
     # Formatting
diff --git a/pandas/core/common.py b/pandas/core/common.py
index 787730784..1964aada8 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -45,17 +45,22 @@ _INT64_DTYPE = np.dtype(np.int64)
 _DATELIKE_DTYPES = set([ np.dtype(t) for t in ['M8[ns]','m8[ns]'] ])
 
 def is_series(obj):
-    return getattr(obj,'_typ',None) == 'series'
+    return getattr(obj, '_typ' ,None) == 'series'
+
 def is_sparse_series(obj):
-    return getattr(obj,'_subtyp',None) in ('sparse_series','sparse_time_series')
+    return getattr(obj, '_subtyp', None) in ('sparse_series','sparse_time_series')
+
 def is_sparse_array_like(obj):
-    return getattr(obj,'_subtyp',None) in ['sparse_array','sparse_series','sparse_array']
+    return getattr(obj, '_subtyp', None) in ['sparse_array','sparse_series','sparse_array']
+
 def is_dataframe(obj):
-    return getattr(obj,'_typ',None) == 'dataframe'
+    return getattr(obj, '_typ', None) == 'dataframe'
+
 def is_panel(obj):
-    return getattr(obj,'_typ',None) == 'panel'
+    return getattr(obj, '_typ', None) == 'panel'
+
 def is_generic(obj):
-    return getattr(obj,'_data',None) is not None
+    return getattr(obj, '_data', None) is not None
 
 def isnull(obj):
     """Detect missing values (NaN in numeric arrays, None/NaN in object arrays)
@@ -1155,7 +1160,10 @@ def _maybe_box(indexer, values, obj, key):
 
 def _values_from_object(o):
     """ return my values or the object if we are say an ndarray """
-    return o.get_values() if hasattr(o,'get_values') else o
+    f = getattr(o,'get_values',None)
+    if f is not None:
+        o = f()
+    return o
 
 def _possibly_convert_objects(values, convert_dates=True, convert_numeric=True):
     """ if we have an object dtype, try to coerce dates and/or numers """
@@ -1733,7 +1741,8 @@ _ensure_object = algos.ensure_object
 
 
 def _astype_nansafe(arr, dtype, copy=True):
-    """ return a view if copy is False """
+    """ return a view if copy is False, but
+        need to be very careful as the result shape could change! """
     if not isinstance(dtype, np.dtype):
         dtype = np.dtype(dtype)
 
diff --git a/pandas/core/expressions.py b/pandas/core/expressions.py
index 9ada495f3..b1bd104ce 100644
--- a/pandas/core/expressions.py
+++ b/pandas/core/expressions.py
@@ -6,6 +6,7 @@ Offer fast expression evaluation thru numexpr
 
 """
 import numpy as np
+from pandas.core.common import _values_from_object
 
 try:
     import numexpr as ne
@@ -106,7 +107,7 @@ def _evaluate_numexpr(op, op_str, a, b, raise_on_error = False, **eval_kwargs):
     return result
 
 def _where_standard(cond, a, b, raise_on_error=True):
-    return np.where(cond, a, b)
+    return np.where(_values_from_object(cond), _values_from_object(a), _values_from_object(b))
 
 def _where_numexpr(cond, a, b, raise_on_error = False):
     result = None
diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index e86a3f6a5..5c943a7eb 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -552,10 +552,6 @@ class DataFrame(NDFrame):
     def shape(self):
         return (len(self.index), len(self.columns))
 
-    # Class behavior
-    def __nonzero__(self):
-        raise ValueError("Cannot call bool() on DataFrame.")
-
     def _repr_fits_vertical_(self):
         """
         Check length against max_rows.
@@ -1874,7 +1870,6 @@ class DataFrame(NDFrame):
 
     def _slice(self, slobj, axis=0, raise_on_error=False):
         axis = self._get_block_manager_axis(axis)
-        new_data = self._data.get_slice(slobj, axis=axis)
         new_data = self._data.get_slice(slobj, axis=axis, raise_on_error=raise_on_error)
         return self._constructor(new_data)
 
@@ -2222,32 +2217,34 @@ class DataFrame(NDFrame):
     #----------------------------------------------------------------------
     # Reindexing and alignment
 
-    def _reindex_axes(self, axes, level, limit, method, fill_value, copy):
+    def _reindex_axes(self, axes, level, limit, method, fill_value, copy, takeable=False):
       frame = self
 
       columns = axes['columns']
       if columns is not None:
           frame = frame._reindex_columns(columns, copy, level,
-                                         fill_value, limit)
+                                         fill_value, limit, takeable=takeable)
 
       index = axes['index']
       if index is not None:
           frame = frame._reindex_index(index, method, copy, level,
-                                       fill_value, limit)
+                                       fill_value, limit, takeable=takeable)
 
       return frame
 
     def _reindex_index(self, new_index, method, copy, level, fill_value=NA,
-                       limit=None):
+                       limit=None, takeable=False):
         new_index, indexer = self.index.reindex(new_index, method, level,
-                                                limit=limit)
+                                                limit=limit, copy_if_needed=True,
+                                                takeable=takeable)
         return self._reindex_with_indexers({ 0 : [ new_index, indexer ] },
                                            copy=copy, fill_value=fill_value)
 
     def _reindex_columns(self, new_columns, copy, level, fill_value=NA,
-                         limit=None):
+                         limit=None, takeable=False):
         new_columns, indexer = self.columns.reindex(new_columns, level=level,
-                                                    limit=limit)
+                                                    limit=limit, copy_if_needed=True,
+                                                    takeable=takeable)
         return self._reindex_with_indexers({ 1 : [ new_columns, indexer ] },
                                            copy=copy, fill_value=fill_value)
 
@@ -2270,47 +2267,6 @@ class DataFrame(NDFrame):
         else:
             return self.copy() if copy else self
 
-    def _reindex_index(self, new_index, method, copy, level, fill_value=NA,
-                       limit=None, takeable=False):
-        new_index, indexer = self.index.reindex(new_index, method, level,
-                                                limit=limit, copy_if_needed=True,
-                                                takeable=takeable)
-        return self._reindex_with_indexers(new_index, indexer, None, None,
-                                           copy, fill_value)
-
-    def _reindex_columns(self, new_columns, copy, level, fill_value=NA,
-                         limit=None, takeable=False):
-        new_columns, indexer = self.columns.reindex(new_columns, level=level,
-                                                    limit=limit, copy_if_needed=True,
-                                                    takeable=takeable)
-        return self._reindex_with_indexers(None, None, new_columns, indexer,
-                                           copy, fill_value)
-
-    def _reindex_with_indexers(self, index, row_indexer, columns, col_indexer,
-                               copy, fill_value):
-        new_data = self._data
-        if row_indexer is not None:
-            row_indexer = com._ensure_int64(row_indexer)
-            new_data = new_data.reindex_indexer(index, row_indexer, axis=1,
-                                                fill_value=fill_value)
-        elif index is not None and index is not new_data.axes[1]:
-            new_data = new_data.copy(deep=copy)
-            new_data.axes[1] = index
-
-        if col_indexer is not None:
-            # TODO: speed up on homogeneous DataFrame objects
-            col_indexer = com._ensure_int64(col_indexer)
-            new_data = new_data.reindex_indexer(columns, col_indexer, axis=0,
-                                                fill_value=fill_value)
-        elif columns is not None and columns is not new_data.axes[0]:
-            new_data = new_data.reindex_items(columns, copy=copy,
-                                              fill_value=fill_value)
-
-        if copy and new_data is self._data:
-            new_data = new_data.copy()
-
-        return DataFrame(new_data)
-
     def reindex_like(self, other, method=None, copy=True, limit=None,
                      fill_value=NA):
         """
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 9aab6e48f..3397e2fdd 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1,6 +1,7 @@
 # pylint: disable=W0231,E1101
 import warnings
 from pandas import compat
+import itertools
 import operator
 import numpy as np
 import pandas.lib as lib
@@ -20,9 +21,6 @@ from pandas.core.common import (isnull, notnull, is_list_like,
                                 _infer_dtype_from_scalar, _maybe_promote)
 from pandas.core.base import PandasObject
 
-_internal_names     = ['_data','name','_subtyp','_index','_default_kind','_default_fill_value']
-_internal_names_set = set(_internal_names)
-
 class NDFrame(PandasObject):
     """
     N-dimensional analogue of DataFrame. Store multi-dimensional in a
@@ -34,6 +32,9 @@ class NDFrame(PandasObject):
     axes : list
     copy : boolean, default False
     """
+    _internal_names     = ['_data','name','_subtyp','_index','_default_kind','_default_fill_value']
+    _internal_names_set = set(_internal_names)
+    _prop_attributes    = []
 
     def __init__(self, data, axes=None, copy=False, dtype=None, fastpath=False):
 
@@ -191,7 +192,7 @@ class NDFrame(PandasObject):
                     kwargs[a] = args.pop(0)
                 except (IndexError):
                     if require_all:
-                        raise ValueError(
+                        raise AssertionError(
                             "not enough arguments specified!")
 
         axes = dict([ (a,kwargs.get(a)) for a in self._AXIS_ORDERS])
@@ -524,13 +525,13 @@ class NDFrame(PandasObject):
                 # to avoid definitional recursion
                 # e.g. say fill_value needing _data to be
                 # defined
-                for k in _internal_names:
+                for k in self._internal_names:
                     if k in state:
                         v = state[k]
                         object.__setattr__(self,k,v)
 
                 for k, v in state.items():
-                    if k not in _internal_names:
+                    if k not in self._internal_names:
                         object.__setattr__(self,k,v)
 
             else:
@@ -550,6 +551,101 @@ class NDFrame(PandasObject):
 
         self._item_cache = {}
 
+    #----------------------------------------------------------------------
+    # IO
+
+    def to_pickle(self, path):
+        """
+        Pickle (serialize) object to input file path
+
+        Parameters
+        ----------
+        path : string
+            File path
+        """
+        from pandas.io.pickle import to_pickle
+        return to_pickle(self, path)
+
+    def save(self, path):  # TODO remove in 0.13
+        import warnings
+        from pandas.io.pickle import to_pickle
+        warnings.warn("save is deprecated, use to_pickle", FutureWarning)
+        return to_pickle(self, path)
+
+    def load(self, path):  # TODO remove in 0.13
+        import warnings
+        from pandas.io.pickle import read_pickle
+        warnings.warn("load is deprecated, use pd.read_pickle", FutureWarning)
+        return read_pickle(path)
+
+    def to_hdf(self, path_or_buf, key, **kwargs):
+        """ activate the HDFStore """
+        from pandas.io import pytables
+        return pytables.to_hdf(path_or_buf, key, self, **kwargs)
+
+    def to_clipboard(self):
+        """
+        Attempt to write text representation of object to the system clipboard
+
+        Notes
+        -----
+        Requirements for your platform
+          - Linux: xclip, or xsel (with gtk or PyQt4 modules)
+          - Windows:
+          - OS X:
+        """
+        from pandas.io import clipboard
+        clipboard.to_clipboard(self)
+
+    def to_json(self, path_or_buf=None, orient=None, date_format='epoch',
+                double_precision=10, force_ascii=True):
+        """
+        Convert the object to a JSON string.
+
+        Note NaN's and None will be converted to null and datetime objects
+        will be converted to UNIX timestamps.
+
+        Parameters
+        ----------
+        path_or_buf : the path or buffer to write the result string
+            if this is None, return a StringIO of the converted string
+        orient : string
+
+            * Series
+
+              - default is 'index'
+              - allowed values are: {'split','records','index'}
+
+            * DataFrame
+
+              - default is 'columns'
+              - allowed values are: {'split','records','index','columns','values'}
+
+            * The format of the JSON string
+
+              - split : dict like {index -> [index], columns -> [columns], data -> [values]}
+              - records : list like [{column -> value}, ... , {column -> value}]
+              - index : dict like {index -> {column -> value}}
+              - columns : dict like {column -> {index -> value}}
+              - values : just the values array
+
+        date_format : type of date conversion (epoch = epoch milliseconds, iso = ISO8601)
+            default is epoch
+        double_precision : The number of decimal places to use when encoding
+            floating point values, default 10.
+        force_ascii : force encoded string to be ASCII, default True.
+
+        Returns
+        -------
+        result : a JSON compatible string written to the path_or_buf;
+                 if the path_or_buf is none, return a StringIO of the result
+
+        """
+
+        from pandas.io import json
+        return json.to_json(path_or_buf=path_or_buf, obj=self, orient=orient, date_format=date_format,
+                            double_precision=double_precision, force_ascii=force_ascii)
+
     #----------------------------------------------------------------------
     # Fancy Indexing
 
@@ -843,6 +939,8 @@ class NDFrame(PandasObject):
             "compatible" value
         limit : int, default None
             Maximum size gap to forward or backward fill
+        takeable : boolean, default False
+            treat the passed as positional values
 
         Examples
         --------
@@ -860,6 +958,7 @@ class NDFrame(PandasObject):
         copy       = kwargs.get('copy',True)
         limit      = kwargs.get('limit')
         fill_value = kwargs.get('fill_value',np.nan)
+        takeable   = kwargs.get('takeable',False)
 
         self._consolidate_inplace()
 
@@ -874,9 +973,9 @@ class NDFrame(PandasObject):
         if copy and not com._count_not_none(*axes.values()):
             return self.copy()
 
-        return self._reindex_axes(axes, level, limit, method, fill_value, copy)
+        return self._reindex_axes(axes, level, limit, method, fill_value, copy, takeable=takeable)
 
-    def _reindex_axes(self, axes, level, limit, method, fill_value, copy):
+    def _reindex_axes(self, axes, level, limit, method, fill_value, copy, takeable=False):
         """ perform the reinxed for all the axes """
         obj = self
         for a in self._AXIS_ORDERS:
@@ -888,7 +987,7 @@ class NDFrame(PandasObject):
                 labels = _ensure_index(labels)
 
             axis   = self._get_axis_number(a)
-            new_index, indexer = self._get_axis(a).reindex(labels, level=level, limit=limit)
+            new_index, indexer = self._get_axis(a).reindex(labels, level=level, limit=limit, takeable=takeable)
             obj    = obj._reindex_with_indexers({ axis : [ labels, indexer ] }, method, fill_value, copy)
 
         return obj
@@ -942,7 +1041,7 @@ class NDFrame(PandasObject):
         axis_name   = self._get_axis_name(axis)
         axis_values = self._get_axis(axis_name)
         new_index, indexer = axis_values.reindex(labels, method, level,
-                                                 limit=limit)
+                                                 limit=limit, copy_if_needed=True)
         return self._reindex_with_indexers({ axis : [ new_index, indexer ] }, method, fill_value, copy)
 
     def _reindex_with_indexers(self, reindexers, method=None, fill_value=np.nan, copy=False):
@@ -1026,6 +1125,12 @@ class NDFrame(PandasObject):
     #----------------------------------------------------------------------
     # Attribute access
 
+    def _propogate_attributes(self, other):
+        """ propogate attributes from other to self"""
+        for name in self._prop_attributes:
+            object.__setattr__(self,name,getattr(other,name,None))
+        return self
+
     def __getattr__(self, name):
         """After regular attribute access, try looking up the name of a the info
         This allows simpler access to columns for interactive use."""
@@ -1037,7 +1142,7 @@ class NDFrame(PandasObject):
     def __setattr__(self, name, value):
         """After regular attribute access, try looking up the name of the info
         This allows simpler access to columns for interactive use."""
-        if name in _internal_names_set:
+        if name in self._internal_names_set:
             object.__setattr__(self, name, value)
         else:
             try:
@@ -1198,7 +1303,7 @@ class NDFrame(PandasObject):
         """
 
         mgr = self._data.astype(dtype, copy = copy, raise_on_error = raise_on_error)
-        return self._constructor(mgr)
+        return self._constructor(mgr)._propogate_attributes(self)
 
     def copy(self, deep=True):
         """
@@ -1218,21 +1323,21 @@ class NDFrame(PandasObject):
             data = data.copy()
         return self._constructor(data)
 
-    def convert_objects(self, convert_dates=True, convert_numeric=False):
+    def convert_objects(self, convert_dates=True, convert_numeric=False, copy=True):
         """
         Attempt to infer better dtype for object columns
-        Always returns a copy (even if no object columns)
 
         Parameters
         ----------
         convert_dates : if True, attempt to soft convert_dates, if 'coerce', force conversion (and non-convertibles get NaT)
         convert_numeric : if True attempt to coerce to numerbers (including strings), non-convertibles get NaN
+        copy : Boolean, if True, return copy, default is True
 
         Returns
         -------
         converted : asm as input object
         """
-        return self._constructor(self._data.convert(convert_dates=convert_dates, convert_numeric=convert_numeric))
+        return self._constructor(self._data.convert(convert_dates=convert_dates, convert_numeric=convert_numeric, copy=copy))
 
     #----------------------------------------------------------------------
     # Filling NA's
@@ -1279,6 +1384,9 @@ class NDFrame(PandasObject):
         self._consolidate_inplace()
 
         axis = self._get_axis_number(axis)
+        if axis+1 > self._AXIS_LEN:
+            raise ValueError("invalid axis passed for object type {0}".format(type(self)))
+
         if value is None:
             if method is None:
                 raise ValueError('must specify a fill method or value')
@@ -1296,10 +1404,10 @@ class NDFrame(PandasObject):
         else:
             if method is not None:
                 raise ValueError('cannot specify both a fill method and value')
-            # Float type values
-            if len(self.columns) == 0:
+
+            if len(self._get_axis(axis)) == 0:
                 return self
-            if isinstance(value, (dict, Series)):
+            if isinstance(value, dict) or com.is_series(value):
                 if axis == 1:
                     raise NotImplementedError('Currently only can fill '
                                               'with dict/Series column '
@@ -1441,9 +1549,12 @@ class NDFrame(PandasObject):
 
         self._consolidate_inplace()
 
+        def is_dictlike(x):
+            return isinstance(x, dict) or com.is_series(x)
+
         if value is None:
-            if not isinstance(to_replace, (dict, Series)):
-                if not isinstance(regex, (dict, Series)):
+            if not is_dictlike(to_replace):
+                if not is_dictlike(regex):
                     raise TypeError('If "to_replace" and "value" are both None'
                                     ' then regex must be a mapping')
                 to_replace = regex
@@ -1452,7 +1563,7 @@ class NDFrame(PandasObject):
             items = to_replace.items()
             keys, values = itertools.izip(*items)
 
-            are_mappings = [isinstance(v, (dict, Series)) for v in values]
+            are_mappings = [ is_dictlike(v) for v in values]
 
             if any(are_mappings):
                 if not all(are_mappings):
@@ -1478,8 +1589,8 @@ class NDFrame(PandasObject):
                 return self
 
             new_data = self._data
-            if isinstance(to_replace, (dict, Series)):
-                if isinstance(value, (dict, Series)):  # {'A' : NA} -> {'A' : 0}
+            if is_dictlike(to_replace):
+                if is_dictlike(value):  # {'A' : NA} -> {'A' : 0}
                     new_data = self._data
                     for c, src in to_replace.iteritems():
                         if c in value and c in self:
@@ -1517,7 +1628,7 @@ class NDFrame(PandasObject):
                                                   inplace=inplace, regex=regex)
             elif to_replace is None:
                 if not (com.is_re_compilable(regex) or
-                        isinstance(regex, (list, dict, np.ndarray, Series))):
+                        isinstance(regex, (list, np.ndarray)) or is_dictlike(regex)):
                     raise TypeError("'regex' must be a string or a compiled "
                                     "regular expression or a list or dict of "
                                     "strings or regular expressions, you "
@@ -1527,7 +1638,7 @@ class NDFrame(PandasObject):
             else:
 
                 # dest iterable dict-like
-                if isinstance(value, (dict, Series)):  # NA -> {'A' : 0, 'B' : -1}
+                if is_dictlike(value):  # NA -> {'A' : 0, 'B' : -1}
                     new_data = self._data
 
                     for k, v in value.iteritems():
@@ -1621,7 +1732,7 @@ class NDFrame(PandasObject):
         return obj
 
     def groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,
-                group_keys=True):
+                group_keys=True, squeeze=False):
         """
         Group series using mapper (dict or key function, apply given function
         to group, return result as series) or by a series of columns
@@ -1645,6 +1756,9 @@ class NDFrame(PandasObject):
             Sort group keys. Get better performance by turning this off
         group_keys : boolean, default True
             When calling apply, add group keys to index to identify pieces
+        squeeze : boolean, default False
+            reduce the dimensionaility of the return type if possible, otherwise
+            return a consistent type
 
         Examples
         --------
@@ -1660,11 +1774,13 @@ class NDFrame(PandasObject):
         Returns
         -------
         GroupBy object
+
         """
+
         from pandas.core.groupby import groupby
         axis = self._get_axis_number(axis)
         return groupby(self, by, axis=axis, level=level, as_index=as_index,
-                       sort=sort, group_keys=group_keys)
+                       sort=sort, group_keys=group_keys, squeeze=squeeze)
 
     def asfreq(self, freq, method=None, how=None, normalize=False):
         """
@@ -2032,10 +2148,10 @@ class NDFrame(PandasObject):
 
                     icond = cond.values
 
-                    # GH 2745
+                    # GH 2745 / GH 4192
                     # treat like a scalar
                     if len(other) == 1:
-                        other = np.array(other[0]*len(self))
+                        other = np.array(other[0])
 
                     # GH 3235
                     # match True cond to other
@@ -2317,9 +2433,13 @@ class NDFrame(PandasObject):
         -------
         truncated : type of caller
         """
-        from pandas.tseries.tools import to_datetime
-        before = to_datetime(before)
-        after = to_datetime(after)
+
+        # if we have a date index, convert to dates, otherwise
+        # treat like a slice
+        if self.index.is_all_dates:
+            from pandas.tseries.tools import to_datetime
+            before = to_datetime(before)
+            after = to_datetime(after)
 
         if before is not None and after is not None:
             if before > after:
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 15f3e9650..698af6804 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -87,7 +87,8 @@ class Index(FrozenNDArray):
 
     _engine_type = _index.ObjectEngine
 
-    def __new__(cls, data, dtype=None, copy=False, name=None, fastpath=False):
+    def __new__(cls, data, dtype=None, copy=False, name=None, fastpath=False,
+                **kwargs):
 
         # no class inference!
         if fastpath:
@@ -99,7 +100,7 @@ class Index(FrozenNDArray):
         if isinstance(data, np.ndarray):
             if issubclass(data.dtype.type, np.datetime64):
                 from pandas.tseries.index import DatetimeIndex
-                result = DatetimeIndex(data, copy=copy, name=name)
+                result = DatetimeIndex(data, copy=copy, name=name, **kwargs)
                 if dtype is not None and _o_dtype == dtype:
                     return Index(result.to_pydatetime(), dtype=_o_dtype)
                 else:
@@ -113,7 +114,7 @@ class Index(FrozenNDArray):
                 except TypeError:
                     pass
             elif isinstance(data, PeriodIndex):
-                return PeriodIndex(data, copy=copy, name=name)
+                return PeriodIndex(data, copy=copy, name=name, **kwargs)
 
             if issubclass(data.dtype.type, np.integer):
                 return Int64Index(data, copy=copy, dtype=dtype, name=name)
@@ -140,45 +141,9 @@ class Index(FrozenNDArray):
                 if (inferred.startswith('datetime') or
                     tslib.is_timestamp_array(subarr)):
                     from pandas.tseries.index import DatetimeIndex
-                    result = DatetimeIndex(data, copy=copy, name=name)
-                    if dtype is not None and _o_dtype == dtype:
-                        return Index(result.to_pydatetime(), dtype=_o_dtype)
-                    else:
-                        return result
-                elif issubclass(data.dtype.type, np.timedelta64):
-                    return Int64Index(data, copy=copy, name=name)
-
-                if dtype is not None:
-                    try:
-                        data = np.array(data, dtype=dtype, copy=copy)
-                    except TypeError:
-                        pass
-                elif isinstance(data, PeriodIndex):
-                    return PeriodIndex(data, copy=copy, name=name)
-
-                if issubclass(data.dtype.type, np.integer):
-                    return Int64Index(data, copy=copy, dtype=dtype, name=name)
-
-                subarr = com._asarray_tuplesafe(data, dtype=object)
-            elif np.isscalar(data):
-                raise ValueError('Index(...) must be called with a collection '
-                                 'of some kind, %s was passed' % repr(data))
-            else:
-                # other iterable of some kind
-                subarr = com._asarray_tuplesafe(data, dtype=object)
-
-            if dtype is None:
-                inferred = lib.infer_dtype(subarr)
-                if inferred == 'integer':
-                    return Int64Index(subarr.astype('i8'), name=name)
-                elif inferred != 'string':
-                    if (inferred.startswith('datetime') or
-                        tslib.is_timestamp_array(subarr)):
-                        from pandas.tseries.index import DatetimeIndex
-                        return DatetimeIndex(subarr, copy=copy, name=name)
-
-                    elif inferred == 'period':
-                        return PeriodIndex(subarr, name=name)
+                    return DatetimeIndex(data, copy=copy, name=name, **kwargs)
+                elif inferred == 'period':
+                    return PeriodIndex(subarr, name=name, **kwargs)
 
         subarr = subarr.view(cls)
         # could also have a _set_name, but I don't think it's really necessary
@@ -1472,6 +1437,33 @@ class Int64Index(Index):
                 raise ValueError('Index(...) must be called with a collection '
                                  'of some kind, %s was passed' % repr(data))
 
+            if not isinstance(data, np.ndarray):
+                if np.isscalar(data):
+                    raise ValueError('Index(...) must be called with a collection '
+                                     'of some kind, %s was passed' % repr(data))
+
+                # other iterable of some kind
+                if not isinstance(data, (list, tuple)):
+                    data = list(data)
+                data = np.asarray(data)
+
+            if issubclass(data.dtype.type, basestring):
+                raise TypeError('String dtype not supported, you may need '
+                                'to explicitly cast to int')
+            elif issubclass(data.dtype.type, np.integer):
+                # don't force the upcast as we may be dealing
+                # with a platform int
+                if dtype is None or not issubclass(np.dtype(dtype).type, np.integer):
+                    dtype = np.int64
+
+                subarr = np.array(data, dtype=dtype, copy=copy)
+            else:
+                subarr = np.array(data, dtype=np.int64, copy=copy)
+                if len(data) > 0:
+                    if (subarr != data).any():
+                        raise TypeError('Unsafe NumPy casting, you must '
+                                        'explicitly cast')
+
             # other iterable of some kind
             if not isinstance(data, (list, tuple)):
                 data = list(data)
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index cc85d4e32..c6d7d9563 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -507,11 +507,7 @@ class _NDFrameIndexer(object):
                     if axis+1 > ndim:
                         raise AssertionError("invalid indexing error with non-unique index")
 
-                    args = [None] * (2*ndim)
-                    args[2*axis] = new_labels
-                    args[2*axis+1] = new_indexer
-
-                    result = result._reindex_with_indexers(*args, copy=False, fill_value=np.nan)
+                    result = result._reindex_with_indexers({ axis : [ new_labels, new_indexer ] }, copy=True)
 
                 return result
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index e44a2914f..3811cdfa6 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -125,6 +125,7 @@ class Block(PandasObject):
     def __unicode__(self):
 
         # don't want to print out all of the items here
+        name = com.pprint_thing(self.__class__.__name__)
         if self._is_single_block:
 
             result = '%s: %s dtype: %s' % (
@@ -325,11 +326,11 @@ class Block(PandasObject):
 
         return blocks
 
-    def astype(self, dtype, copy=True, raise_on_error=True, values=None):
+    def astype(self, dtype, copy=False, raise_on_error=True, values=None):
         return self._astype(dtype, copy=copy, raise_on_error=raise_on_error,
                             values=values)
 
-    def _astype(self, dtype, copy=True, raise_on_error=True, values=None,
+    def _astype(self, dtype, copy=False, raise_on_error=True, values=None,
                 klass=None):
         """
         Coerce to the new type (if copy=True, return a new copy)
@@ -342,8 +343,9 @@ class Block(PandasObject):
             return self
 
         try:
+            # force the copy here
             if values is None:
-                values = com._astype_nansafe(self.values, dtype, copy=copy)
+                values = com._astype_nansafe(self.values, dtype, copy=True)
             newb = make_block(values, self.items, self.ref_items, ndim=self.ndim,
                               fastpath=True, dtype=dtype, klass=klass)
         except:
@@ -352,12 +354,11 @@ class Block(PandasObject):
             newb = self.copy() if copy else self
 
         if newb.is_numeric and self.is_numeric:
-            if (newb.shape != self.shape or
-                    (not copy and newb.itemsize < self.itemsize)):
+            if newb.shape != self.shape:
                 raise TypeError("cannot set astype for copy = [%s] for dtype "
                                 "(%s [%s]) with smaller itemsize that current "
                                 "(%s [%s])" % (copy, self.dtype.name,
-                                self.itemsize, newb.dtype.name, newb.itemsize))
+                                               self.itemsize, newb.dtype.name, newb.itemsize))
         return newb
 
     def convert(self, copy = True, **kwargs):
@@ -507,7 +508,14 @@ class Block(PandasObject):
                     # need a new block
                     if m.any():
 
-                        n = new[i] if isinstance(new, np.ndarray) else new
+                        n = new[i] if isinstance(new, np.ndarray) else np.array(new)
+
+                        # type of the new block
+                        dtype, _ = com._maybe_promote(n.dtype)
+
+                        # we need to exiplicty astype here to make a copy
+                        n = n.astype(dtype)
+
                         block = create_block(v,m,n,item)
 
                     else:
@@ -834,7 +842,7 @@ class ObjectBlock(Block):
         """ we can be a bool if we have only bool values but are of type object """
         return lib.is_bool_array(self.values.ravel())
 
-    def convert(self, convert_dates = True, convert_numeric = True, copy = True, by_item = True):
+    def convert(self, convert_dates=True, convert_numeric=True, copy=True, by_item=True):
         """ attempt to coerce any object types to better types
             return a copy of the block (if copy = True)
             by definition we ARE an ObjectBlock!!!!!
@@ -853,7 +861,8 @@ class ObjectBlock(Block):
                 values = com._possibly_convert_objects(values, convert_dates=convert_dates, convert_numeric=convert_numeric)
                 values = _block_shape(values)
                 items = self.items.take([i])
-                newb = make_block(values, items, self.ref_items, ndim = self.ndim)
+                placement = None if is_unique else [i]
+                newb = make_block(values, items, self.ref_items, ndim=self.ndim, placement=placement)
                 blocks.append(newb)
 
         else:
@@ -938,9 +947,12 @@ class ObjectBlock(Block):
         else:
             # if the thing to replace is not a string or compiled regex call
             # the superclass method -> to_replace is some kind of object
-            return super(ObjectBlock, self).replace(to_replace, value,
-                                                    inplace=inplace,
-                                                    filter=filter, regex=regex)
+            result = super(ObjectBlock, self).replace(to_replace, value,
+                                                      inplace=inplace,
+                                                      filter=filter, regex=regex)
+            if not isinstance(result, list):
+                result = [ result]
+            return result
 
         new_values = self.values if inplace else self.values.copy()
 
@@ -1047,7 +1059,7 @@ class DatetimeBlock(Block):
     def should_store(self, value):
         return issubclass(value.dtype.type, np.datetime64)
 
-    def astype(self, dtype, copy = True, raise_on_error=True):
+    def astype(self, dtype, copy=False, raise_on_error=True):
         """
         handle convert to object as a special case
         """
@@ -1088,7 +1100,7 @@ class SparseBlock(Block):
     _verify_integrity = False
     _ftype = 'sparse'
 
-    def __init__(self, values, items, ref_items, ndim=None, fastpath=False):
+    def __init__(self, values, items, ref_items, ndim=None, fastpath=False, placement=None):
 
         # kludgetastic
         if ndim is not None:
@@ -1285,7 +1297,7 @@ class SparseBlock(Block):
             return []
         return super(SparseBlock, self).split_block_at(self, item)
 
-def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fastpath=False):
+def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fastpath=False, placement=None):
 
     if klass is None:
         dtype = dtype or values.dtype
@@ -1323,7 +1335,7 @@ def make_block(values, items, ref_items, klass=None, ndim=None, dtype=None, fast
             if klass is None:
                 klass = ObjectBlock
 
-    return klass(values, items, ref_items, ndim=ndim, fastpath=fastpath)
+    return klass(values, items, ref_items, ndim=ndim, fastpath=fastpath, placement=placement)
 
 # TODO: flexible with index=None and/or items=None
 
@@ -1711,8 +1723,12 @@ class BlockManager(PandasObject):
                 new_rb = []
                 for b in rb:
                     if b.dtype == np.object_:
-                        new_rb.extend(b.replace(s, d, inplace=inplace,
-                                                regex=regex))
+                        result = b.replace(s, d, inplace=inplace,
+                                           regex=regex)
+                        if isinstance(result, list):
+                            new_rb.extend(result)
+                        else:
+                            new_rb.append(result)
                     else:
                         # get our mask for this element, sized to this
                         # particular block
@@ -1889,6 +1905,7 @@ class BlockManager(PandasObject):
             if len(self.blocks) == 1:
                 blk = self.blocks[0]
                 newb = make_block(blk._slice(slobj),
+                                  new_items,
                                   new_items,
                                   klass=blk.__class__,
                                   fastpath=True,
@@ -2302,47 +2319,50 @@ class BlockManager(PandasObject):
         ref_locs  = self._set_ref_locs()
         prev_items_map = self._items_map.pop(block) if ref_locs is not None else None
 
-        # compute the split mask
-        loc = block.items.get_loc(item)
-        if type(loc) == slice or com.is_integer(loc):
-            mask = np.array([True] * len(block))
-            mask[loc] = False
-        else:  # already a mask, inverted
-            mask = -loc
+        # if we can't consolidate, then we are removing this block in its entirey
+        if block._can_consolidate:
 
-        # split the block
-        counter = 0
-        for s, e in com.split_ranges(mask):
+            # compute the split mask
+            loc = block.items.get_loc(item)
+            if type(loc) == slice or com.is_integer(loc):
+                mask = np.array([True] * len(block))
+                mask[loc] = False
+            else:  # already a mask, inverted
+                mask = -loc
 
-            sblock = make_block(block.values[s:e],
-                                block.items[s:e].copy(),
-                                block.ref_items,
-                                klass=block.__class__,
-                                fastpath=True)
+            # split the block
+            counter = 0
+            for s, e in com.split_ranges(mask):
 
-            self.blocks.append(sblock)
+                sblock = make_block(block.values[s:e],
+                                    block.items[s:e].copy(),
+                                    block.ref_items,
+                                    klass=block.__class__,
+                                    fastpath=True)
 
-            # update the _ref_locs/_items_map
-            if ref_locs is not None:
+                self.blocks.append(sblock)
 
-                # fill the item_map out for this sub-block
-                m = maybe_create_block_in_items_map(self._items_map,sblock)
-                for j, itm in enumerate(sblock.items):
+                # update the _ref_locs/_items_map
+                if ref_locs is not None:
 
-                    # is this item masked (e.g. was deleted)?
-                    while (True):
+                    # fill the item_map out for this sub-block
+                    m = maybe_create_block_in_items_map(self._items_map,sblock)
+                    for j, itm in enumerate(sblock.items):
 
-                        if counter > len(mask) or mask[counter]:
-                            break
-                        else:
-                            counter += 1
+                        # is this item masked (e.g. was deleted)?
+                        while (True):
+
+                            if counter > len(mask) or mask[counter]:
+                                break
+                            else:
+                                counter += 1
 
-                    # find my mapping location
-                    m[j] = prev_items_map[counter]
-                    counter += 1
+                        # find my mapping location
+                        m[j] = prev_items_map[counter]
+                        counter += 1
 
-                # set the ref_locs in this block
-                sblock.set_ref_locs(m)
+                    # set the ref_locs in this block
+                    sblock.set_ref_locs(m)
 
         # reset the ref_locs to the new structure
         if ref_locs is not None:
@@ -2619,7 +2639,7 @@ class BlockManager(PandasObject):
             new_items = MultiIndex.from_tuples(items, names=self.items.names)
         else:
             items = [mapper(x) for x in self.items]
-            new_items = Index(items, names=self.items.names)
+            new_items = Index(items, name=self.items.name)
 
         new_blocks = []
         for block in self.blocks:
@@ -2863,7 +2883,7 @@ def form_blocks(arrays, names, axes):
 
     for i, (k, v) in enumerate(zip(names, arrays)):
         if isinstance(v, SparseArray) or is_sparse_series(v):
-            sparse_items.append((i, k,v))
+            sparse_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.floating):
             float_items.append((i, k, v))
         elif issubclass(v.dtype.type, np.complexfloating):
@@ -2966,7 +2986,7 @@ def _sparse_blockify(tuples, ref_items, dtype = None):
     """ return an array of blocks that potentially have different dtypes (and are sparse) """
 
     new_blocks = []
-    for names, array in tuples:
+    for i, names, array in tuples:
 
         if not isinstance(names, (list,tuple)):
             names = [ names ]
diff --git a/pandas/core/series.py b/pandas/core/series.py
index c7d50ea43..4836747ce 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -379,6 +379,14 @@ def _radd_compat(left, right):
 
     return output
 
+def _coerce_method(converter):
+    """ install the scalar coercion methods """
+
+    def wrapper(self):
+        if len(self) == 1:
+            return converter(self.iloc[0])
+        raise TypeError("cannot convert the series to {0}".format(str(converter)))
+    return wrapper
 
 def _maybe_match_name(a, b):
     name = None
@@ -500,6 +508,7 @@ class Series(generic.NDFrame):
         If None, dtype will be inferred
     copy : boolean, default False, copy input data
     """
+    _prop_attributes    = ['name']
 
     def __init__(self, data=None, index=None, dtype=None, name=None,
                  copy=False, fastpath=False):
@@ -572,12 +581,14 @@ class Series(generic.NDFrame):
                     data = data.to_dense()
 
             if index is None:
+                if not is_list_like(data):
+                    data = [ data ]
                 index = _default_index(len(data))
 
             # create/copy the manager
             if isinstance(data, SingleBlockManager):
                 if dtype is not None:
-                    data = data.astype(dtype,copy=copy)
+                    data = data.astype(dtype, raise_on_error=False)
                 elif copy:
                     data = data.copy()
             else:
@@ -709,6 +720,18 @@ class Series(generic.NDFrame):
     def __contains__(self, key):
         return key in self.index
 
+    # coercion
+    __float__ = _coerce_method(float)
+    __long__ = _coerce_method(int)
+    __int__ = _coerce_method(int)
+    __bool__ = _coerce_method(bool)
+
+    def __nonzero__(self):
+        # special case of a single element bool series degenerating to a scalar
+        if self.dtype == np.bool_ and len(self) == 1:
+            return bool(self.iloc[0])
+        return not self.empty
+
     # we are preserving name here
     def __getstate__(self):
         return dict(_data = self._data, name = self.name)
@@ -1732,6 +1755,10 @@ class Series(generic.NDFrame):
             return pa.NA
         return self.index[i]
 
+    # ndarray compat
+    argmin = idxmin
+    argmax = idxmax
+
     def cumsum(self, axis=0, dtype=None, out=None, skipna=True):
         """
         Cumulative sum of values. Preserves locations of NaN values
@@ -2848,12 +2875,12 @@ class Series(generic.NDFrame):
 
         # GH4246 (dispatch to a common method with frame to handle possibly
         # duplicate index)
-        return self._reindex_with_indexers(new_index, indexer, copy=copy,
-                                           fill_value=fill_value)
+        return self._reindex_with_indexers({ 0 : [new_index, indexer] }, copy=copy, fill_value=fill_value)
 
-    def _reindex_with_indexers(self, index, indexer, copy, fill_value):
+    def _reindex_with_indexers(self, reindexers, copy, fill_value=None):
+        index, indexer = reindexers[0]
         new_values = com.take_1d(self.values, indexer, fill_value=fill_value)
-        return self._constructor(new_values, index=new_index, name=self.name)
+        return self._constructor(new_values, index=index, name=self.name)
 
     def reindex_axis(self, labels, axis=0, **kwargs):
         """ for compatibility with higher dims """
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index 21cf6d40d..9d21e10d6 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -551,7 +551,7 @@ class StataReader(StataParser):
                 labeled_data = np.copy(data[col])
                 labeled_data = labeled_data.astype(object)
                 for k, v in compat.iteritems(self.value_label_dict[self.lbllist[i]]):
-                    labeled_data[data[col] == k] = v
+                    labeled_data[(data[col] == k).values] = v
                 data[col] = Categorical.from_array(labeled_data)
 
         return data
diff --git a/pandas/io/tests/test_stata.py b/pandas/io/tests/test_stata.py
index d75de149d..31472dc66 100644
--- a/pandas/io/tests/test_stata.py
+++ b/pandas/io/tests/test_stata.py
@@ -48,11 +48,10 @@ class StataTests(unittest.TestCase):
                              columns=['float_miss', 'double_miss', 'byte_miss',
                                       'int_miss', 'long_miss'])
 
-        for i, col in enumerate(parsed.columns):
-            np.testing.assert_almost_equal(
-                parsed[col],
-                expected[expected.columns[i]]
-            )
+        # this is an oddity as really the nan should be float64, but
+        # the casting doesn't fail so need to match stata here
+        expected['float_miss'] = expected['float_miss'].astype(np.float32)
+        tm.assert_frame_equal(parsed, expected)
 
     def test_read_dta2(self):
         expected = DataFrame.from_records(
@@ -101,14 +100,16 @@ class StataTests(unittest.TestCase):
         tm.assert_frame_equal(parsed, expected)
 
     def test_read_dta3(self):
+
         parsed = self.read_dta(self.dta3)
+
+        # match stata here
         expected = self.read_csv(self.csv3)
-        for i, col in enumerate(parsed.columns):
-            np.testing.assert_almost_equal(
-                parsed[col],
-                expected[expected.columns[i]],
-                decimal=3
-            )
+        expected = expected.astype(np.float32)
+        expected['year'] = expected['year'].astype(np.int32)
+        expected['quarter']= expected['quarter'].astype(np.int16)
+
+        tm.assert_frame_equal(parsed,expected)
 
     def test_read_dta4(self):
         parsed = self.read_dta(self.dta4)
@@ -164,37 +165,19 @@ class StataTests(unittest.TestCase):
     def test_read_dta7(self):
         expected = read_csv(self.csv7, parse_dates=True, sep='\t')
         parsed = self.read_dta(self.dta7)
-
-        for i, col in enumerate(parsed.columns):
-            np.testing.assert_almost_equal(
-                parsed[col],
-                expected[expected.columns[i]],
-                decimal=3
-            )
+        tm.assert_frame_equal(parsed, expected)
 
     @nose.tools.nottest
     def test_read_dta8(self):
         expected = read_csv(self.csv8, parse_dates=True, sep='\t')
         parsed = self.read_dta(self.dta8)
-
-        for i, col in enumerate(parsed.columns):
-            np.testing.assert_almost_equal(
-                parsed[col],
-                expected[expected.columns[i]],
-                decimal=3
-            )
+        tm.assert_frame_equal(parsed, expected)
 
     @nose.tools.nottest
     def test_read_dta9(self):
         expected = read_csv(self.csv9, parse_dates=True, sep='\t')
         parsed = self.read_dta(self.dta9)
-
-        for i, col in enumerate(parsed.columns):
-            np.testing.assert_equal(
-                parsed[col],
-                expected[expected.columns[i]],
-                decimal=3
-            )
+        assert_frame_equal(parsed, expected)
 
     def test_read_write_dta10(self):
         if not is_little_endian():
diff --git a/pandas/lib.pyx b/pandas/lib.pyx
index 7c4ba1cda..f5205ae0c 100644
--- a/pandas/lib.pyx
+++ b/pandas/lib.pyx
@@ -714,11 +714,20 @@ def vec_binop(ndarray[object] left, ndarray[object] right, object op):
 def astype_intsafe(ndarray[object] arr, new_dtype):
     cdef:
         Py_ssize_t i, n = len(arr)
+        object v
+        bint is_datelike
         ndarray result
 
+    # on 32-bit, 1.6.2 numpy M8[ns] is a subdtype of integer, which is weird
+    is_datelike = new_dtype in ['M8[ns]','m8[ns]']
+
     result = np.empty(n, dtype=new_dtype)
     for i in range(n):
-        util.set_value_at(result, i, arr[i])
+        v = arr[i]
+        if is_datelike and checknull(v):
+           result[i] = NPY_NAT
+        else:
+           util.set_value_at(result, i, v)
 
     return result
 
diff --git a/pandas/sparse/array.py b/pandas/sparse/array.py
index c4abfddd1..10adb8245 100644
--- a/pandas/sparse/array.py
+++ b/pandas/sparse/array.py
@@ -261,7 +261,7 @@ to sparse
     def sp_values(self):
         # caching not an option, leaks memory
         return self.view(np.ndarray)
-    
+
     def get_values(self, fill=None):
         """ return a dense representation """
         return self.to_dense(fill=fill)
@@ -309,7 +309,7 @@ to sparse
             loc += n
 
         if loc >= n or loc < 0:
-            raise Exception('Out of bounds access')
+            raise IndexError('Out of bounds access')
 
         sp_loc = self.sp_index.lookup(loc)
         if sp_loc == -1:
@@ -327,11 +327,12 @@ to sparse
         """
         if not ((axis == 0)):
             raise AssertionError()
-        indices = np.asarray(indices, dtype=int)
+        indices = np.atleast_1d(np.asarray(indices, dtype=int))
 
+        # allow -1 to indicate missing values
         n = len(self)
-        if (indices >= n).any():
-            raise Exception('out of bounds access')
+        if ((indices >= n) | (indices < -1)).any():
+            raise IndexError('out of bounds access')
 
         if self.sp_index.npoints > 0:
             locs = np.array([self.sp_index.lookup(loc) if loc > -1 else -1 for loc in indices ])
@@ -356,7 +357,7 @@ to sparse
         #    self.values[key] = value
         #else:
         #    raise Exception("SparseArray does not support seting non-scalars via setitem")
-        raise Exception("SparseArray does not support setting via setitem")
+        raise TypeError("SparseArray does not support item assignment via setitem")
 
     def __setslice__(self, i, j, value):
         if i < 0:
@@ -371,7 +372,7 @@ to sparse
         #x = self.values
         #x[slobj] = value
         #self.values = x
-        raise Exception("SparseArray does not support seting via slices")
+        raise TypeError("SparseArray does not support item assignment via slices")
 
     def astype(self, dtype=None):
         """
diff --git a/pandas/sparse/frame.py b/pandas/sparse/frame.py
index 0ff08b0ae..e282a89f8 100644
--- a/pandas/sparse/frame.py
+++ b/pandas/sparse/frame.py
@@ -20,7 +20,8 @@ from pandas.core.frame import (DataFrame, extract_index, _prep_ndarray,
 from pandas.util.decorators import cache_readonly
 import pandas.core.common as com
 import pandas.core.datetools as datetools
-from pandas.core.internals import BlockManager, form_blocks
+from pandas.core.internals import BlockManager, create_block_manager_from_arrays
+
 from pandas.core.generic import NDFrame
 from pandas.sparse.series import SparseSeries,SparseArray
 from pandas.util.decorators import Appender
@@ -727,12 +728,7 @@ def dict_to_manager(sdict, columns, index):
     # from BlockManager perspective
     axes = [_ensure_index(columns), _ensure_index(index)]
 
-    # segregates dtypes and forms blocks matching to columns
-    blocks = form_blocks([ sdict[c] for c in columns ], columns, axes)
-
-    # consolidate for now
-    mgr = BlockManager(blocks, axes)
-    return mgr.consolidate()
+    return create_block_manager_from_arrays([ sdict[c] for c in columns ], columns, axes)
 
 def stack_sparse_frame(frame):
     """
diff --git a/pandas/sparse/series.py b/pandas/sparse/series.py
index 3092dc6fd..f321fcd48 100644
--- a/pandas/sparse/series.py
+++ b/pandas/sparse/series.py
@@ -105,9 +105,6 @@ class SparseSeries(Series):
                 data = data.copy()
         else:
 
-            if index is None:
-                raise TypeError('must pass index!')
-
             is_sparse_array = isinstance(data, SparseArray)
             if fill_value is None:
                 if is_sparse_array:
@@ -156,8 +153,6 @@ class SparseSeries(Series):
                     data = data.reindex(index,copy=False)
 
             else:
-                if index is None:
-                    raise Exception('must pass index!')
 
                 length = len(index)
 
@@ -331,8 +326,6 @@ class SparseSeries(Series):
                     fill_value = self.fill_value,
                     name       = self.name)
 
-
-
     def _unpickle_series_compat(self, state):
 
         nd_state, own_state = state
diff --git a/pandas/src/reduce.pyx b/pandas/src/reduce.pyx
index 4d18bc71c..d59c28a30 100644
--- a/pandas/src/reduce.pyx
+++ b/pandas/src/reduce.pyx
@@ -1,6 +1,11 @@
+#cython=False
 from numpy cimport *
 import numpy as np
 
+from pandas.core.array import SNDArray
+from distutils.version import LooseVersion
+
+is_numpy_prior_1_6_2 = LooseVersion(np.__version__) < '1.6.2'
 
 cdef class Reducer:
     '''
@@ -35,7 +40,7 @@ cdef class Reducer:
         self.typ = None
         self.labels = labels
         self.dummy, index = self._check_dummy(dummy)
-  
+
         if axis == 0:
              self.labels = index
              self.index  = labels
@@ -58,10 +63,10 @@ cdef class Reducer:
 
             # we passed a series-like
             if hasattr(dummy,'values'):
-                
+
                 self.typ = type(dummy)
                 index = getattr(dummy,'index',None)
-                dummy = dummy.values                
+                dummy = dummy.values
 
         return dummy, index
 
@@ -93,11 +98,9 @@ cdef class Reducer:
 
                          # recreate with the index if supplied
                          if index is not None:
-                              tchunk = typ(chunk,
-                                 index = index,
-                                 name  = name)
+                              tchunk = typ(chunk, index=index, name=name, fastpath=True)
                          else:
-                             tchunk = typ(chunk, name=name)    
+                             tchunk = typ(chunk, name=name)
 
                      except:
                          tchunk = chunk
@@ -196,7 +199,7 @@ cdef class SeriesBinGrouper:
             ndarray arr, result
             ndarray[int64_t] counts
             Py_ssize_t i, n, group_size
-            object res, chunk
+            object res
             bint initialized = 0, needs_typ = 1, try_typ = 0
             Slider vslider, islider
             object gin, typ, ityp, name
@@ -211,10 +214,9 @@ cdef class SeriesBinGrouper:
                 else:
                     counts[i] = self.bins[i] - self.bins[i-1]
 
-        chunk = self.dummy_arr
         group_size = 0
         n = len(self.arr)
-        typ = self.typ	
+        typ = self.typ
         ityp = self.ityp
         name = self.name
 
@@ -223,6 +225,11 @@ cdef class SeriesBinGrouper:
 
         gin = self.dummy_index._engine
 
+        # old numpy issue, need to always create and pass the Series
+        if is_numpy_prior_1_6_2:
+            try_typ = 1
+            needs_typ = 1
+
         try:
             for i in range(self.ngroups):
                 group_size = counts[i]
@@ -231,28 +238,24 @@ cdef class SeriesBinGrouper:
                 vslider.set_length(group_size)
 
                 # see if we need to create the object proper
-                if not try_typ:
+                if try_typ:
+                    if needs_typ:
+                          res = self.f(typ(vslider.buf, index=islider.buf,
+                                           name=name, fastpath=True))
+                    else:
+                          res = self.f(SNDArray(vslider.buf,islider.buf,name=name))
+                else:
                      try:
-                          chunk.name = name
-                          res = self.f(chunk)
+                          res = self.f(SNDArray(vslider.buf,islider.buf,name=name))
                           needs_typ = 0
                      except:
                           res = self.f(typ(vslider.buf, index=islider.buf,
                                            name=name, fastpath=True))
                           needs_typ = 1
 
-                     try_typ = 0
-                else:
-                    if needs_typ:
-                          res = self.f(typ(vslider.buf, index=islider.buf,
-                                           name=name, fastpath=True))
-                    else:                              
-                          chunk.name = name
-                          res = self.f(chunk)
-
-                if hasattr(res,'values'):
-                        res = res.values
+                     try_typ = 1
 
+                res = _extract_result(res)
                 if not initialized:
                     result = self._get_result_array(res)
                     initialized = 1
@@ -337,17 +340,16 @@ cdef class SeriesGrouper:
             ndarray arr, result
             ndarray[int64_t] labels, counts
             Py_ssize_t i, n, group_size, lab
-            object res, chunk
+            object res
             bint initialized = 0, needs_typ = 1, try_typ = 0
             Slider vslider, islider
             object gin, typ, ityp, name
 
         labels = self.labels
         counts = np.zeros(self.ngroups, dtype=np.int64)
-        chunk = self.dummy_arr
         group_size = 0
         n = len(self.arr)
-        typ = self.typ	
+        typ = self.typ
         ityp = self.ityp
         name = self.name
 
@@ -355,6 +357,12 @@ cdef class SeriesGrouper:
         islider = Slider(self.index, self.dummy_index)
 
         gin = self.dummy_index._engine
+
+        # old numpy issue, need to always create and pass the Series
+        if is_numpy_prior_1_6_2:
+            try_typ = 1
+            needs_typ = 1
+
         try:
             for i in range(n):
                 group_size += 1
@@ -372,28 +380,27 @@ cdef class SeriesGrouper:
                     vslider.set_length(group_size)
 
                     # see if we need to create the object proper
-                    if not try_typ:
-                         try:
-                              chunk.name = name
-                              res = self.f(chunk)
-                              needs_typ = 0
-                         except:
+                    # try on the first go around
+                    if try_typ:
+                        if needs_typ:
                               res = self.f(typ(vslider.buf, index=islider.buf,
                                                name=name, fastpath=True))
-                              needs_typ = 1
-
-                         try_typ = 0
+                        else:
+                              res = self.f(SNDArray(vslider.buf,islider.buf,name=name))
                     else:
-                        if needs_typ:
+
+                         # try with a numpy array directly
+                         try:
+                              res = self.f(SNDArray(vslider.buf,islider.buf,name=name))
+                              needs_typ = 0
+                         except (Exception), detail:
                               res = self.f(typ(vslider.buf, index=islider.buf,
                                                name=name, fastpath=True))
-                        else:                              
-                              chunk.name = name
-                              res = self.f(chunk)
+                              needs_typ = 1
 
-                    if hasattr(res,'values'):
-                        res = res.values
+                         try_typ = 1
 
+                    res = _extract_result(res)
                     if not initialized:
                         result = self._get_result_array(res)
                         initialized = 1
@@ -429,6 +436,18 @@ cdef class SeriesGrouper:
             raise ValueError('function does not reduce')
         return result
 
+cdef inline _extract_result(object res):
+    ''' extract the result object, it might be a 0-dim ndarray
+        or a len-1 0-dim, or a scalar '''
+    if hasattr(res,'values'):
+       res = res.values
+    if not np.isscalar(res):
+       if isinstance(res, np.ndarray):
+          if res.ndim == 0:
+             res = res.item()
+          elif res.ndim == 1 and len(res) == 1:
+             res = res[0]
+    return res
 
 cdef class Slider:
     '''
diff --git a/pandas/src/ujson/python/objToJSON.c b/pandas/src/ujson/python/objToJSON.c
index f28ed1373..d413ece44 100644
--- a/pandas/src/ujson/python/objToJSON.c
+++ b/pandas/src/ujson/python/objToJSON.c
@@ -260,7 +260,7 @@ static void *PandasDateTimeStructToJSON(pandas_datetimestruct *dts, JSONTypeCont
       return NULL;
     }
   }
-  else 
+  else
   {
     PRINTMARK();
     *((JSINT64*)outValue) = pandas_datetimestruct_to_datetime(base, dts);
@@ -283,7 +283,7 @@ static void *PyDateTimeToJSON(JSOBJ _obj, JSONTypeContext *tc, void *outValue, s
   pandas_datetimestruct dts;
   PyObject *obj = (PyObject *) _obj;
 
-  
+
   if (!convert_pydatetime_to_datetimestruct(obj, &dts, NULL, 1))
   {
     PRINTMARK();
@@ -453,7 +453,7 @@ int NpyArr_iterNext(JSOBJ _obj, JSONTypeContext *tc)
   PRINTMARK();
   npyarr = GET_TC(tc)->npyarr;
 
-  if (PyErr_Occurred()) 
+  if (PyErr_Occurred())
   {
     PRINTMARK();
     return 0;
@@ -1234,7 +1234,7 @@ void Object_beginTypeContext (JSOBJ _obj, JSONTypeContext *tc)
 
     PRINTMARK();
     pc->PyTypeToJSON = NpyDateTimeToJSON;
-    if (enc->datetimeIso) 
+    if (enc->datetimeIso)
     {
       tc->type = JT_UTF8;
     }
@@ -1311,7 +1311,7 @@ void Object_beginTypeContext (JSOBJ _obj, JSONTypeContext *tc)
     }
     return;
   }
-  else 
+  else
   if (PyObject_IsInstance(obj, type_decimal))
   {
     PRINTMARK();
@@ -1337,7 +1337,7 @@ void Object_beginTypeContext (JSOBJ _obj, JSONTypeContext *tc)
 
     PRINTMARK();
     pc->PyTypeToJSON = PyDateTimeToJSON;
-    if (enc->datetimeIso) 
+    if (enc->datetimeIso)
     {
       PRINTMARK();
       tc->type = JT_UTF8;
@@ -1397,7 +1397,7 @@ ISITERABLE:
       pc->iterGetValue = Tuple_iterGetValue;
       pc->iterGetName = Tuple_iterGetName;
       return;
-  }       
+  }
   else
   if (PyAnySet_Check(obj))
   {
@@ -1450,6 +1450,8 @@ ISITERABLE:
       return;
     }
 
+    pc->newObj = PyObject_GetAttrString(obj, "values");
+
     if (enc->outputFormat == INDEX || enc->outputFormat == COLUMNS)
     {
       PRINTMARK();
@@ -1466,7 +1468,6 @@ ISITERABLE:
       PRINTMARK();
       tc->type = JT_ARRAY;
     }
-    pc->newObj = PyObject_GetAttrString(obj, "values");
     pc->iterBegin = NpyArr_iterBegin;
     pc->iterEnd = NpyArr_iterEnd;
     pc->iterNext = NpyArr_iterNext;
@@ -1715,7 +1716,7 @@ PyObject* objToJSON(PyObject* self, PyObject *args, PyObject *kwargs)
   PyObject *oencodeHTMLChars = NULL;
   char *sOrient = NULL;
   char *sdateFormat = NULL;
-  PyObject *oisoDates = 0; 
+  PyObject *oisoDates = 0;
 
   PyObjectEncoder pyEncoder =
   {
@@ -1765,11 +1766,11 @@ PyObject* objToJSON(PyObject* self, PyObject *args, PyObject *kwargs)
     encoder->encodeHTMLChars = 1;
   }
 
-  if (idoublePrecision > JSON_DOUBLE_MAX_DECIMALS || idoublePrecision < 0) 
+  if (idoublePrecision > JSON_DOUBLE_MAX_DECIMALS || idoublePrecision < 0)
   {
       PyErr_Format (
-          PyExc_ValueError, 
-          "Invalid value '%d' for option 'double_precision', max is '%u'", 
+          PyExc_ValueError,
+          "Invalid value '%d' for option 'double_precision', max is '%u'",
           idoublePrecision,
           JSON_DOUBLE_MAX_DECIMALS);
       return NULL;
@@ -1821,7 +1822,7 @@ PyObject* objToJSON(PyObject* self, PyObject *args, PyObject *kwargs)
     {
       pyEncoder.datetimeUnit = PANDAS_FR_us;
     }
-    else 
+    else
     if (strcmp(sdateFormat, "ns") == 0)
     {
       pyEncoder.datetimeUnit = PANDAS_FR_ns;
diff --git a/pandas/tests/test_frame.py b/pandas/tests/test_frame.py
index 04f59d8e5..c4d74e4af 100644
--- a/pandas/tests/test_frame.py
+++ b/pandas/tests/test_frame.py
@@ -3329,27 +3329,17 @@ class TestDataFrame(unittest.TestCase, CheckIndexing,
     def test_astype_with_view(self):
 
         tf = self.mixed_float.reindex(columns = ['A','B','C'])
-        self.assertRaises(TypeError, self.frame.astype, np.int32, copy = False)
 
-        self.assertRaises(TypeError, tf, np.int32, copy = False)
-
-        self.assertRaises(TypeError, tf, np.int64, copy = False)
         casted = tf.astype(np.int64)
 
-        self.assertRaises(TypeError, tf, np.float32, copy = False)
         casted = tf.astype(np.float32)
 
         # this is the only real reason to do it this way
         tf = np.round(self.frame).astype(np.int32)
         casted = tf.astype(np.float32, copy = False)
-        #self.assert_(casted.values.data == tf.values.data)
 
         tf = self.frame.astype(np.float64)
         casted = tf.astype(np.int64, copy = False)
-        #self.assert_(casted.values.data == tf.values.data)
-
-        # can't view to an object array
-        self.assertRaises(Exception, self.frame.astype, 'O', copy = False)
 
     def test_astype_cast_nan_int(self):
         df = DataFrame(data={"Values": [1.0, 2.0, 3.0, np.nan]})
diff --git a/pandas/tests/test_panel.py b/pandas/tests/test_panel.py
index 1112f4013..430e5df83 100644
--- a/pandas/tests/test_panel.py
+++ b/pandas/tests/test_panel.py
@@ -1196,7 +1196,7 @@ class TestPanel(unittest.TestCase, PanelTests, CheckIndexing,
                           maj='major', majo='items')
 
         # test invalid kwargs
-        self.assertRaises(KeyError, self.panel.transpose, 'minor',
+        self.assertRaises(AssertionError, self.panel.transpose, 'minor',
                           maj='major', minor='items')
 
         result = self.panel.transpose(2, 1, 0)
diff --git a/pandas/tests/test_series.py b/pandas/tests/test_series.py
index c7a2005fa..003f237eb 100644
--- a/pandas/tests/test_series.py
+++ b/pandas/tests/test_series.py
@@ -282,6 +282,31 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
 
         self.empty = Series([], index=[])
 
+    def test_scalar_conversion(self):
+
+        # Pass in scalar is disabled
+        scalar = Series(0.5)
+        self.assert_(not isinstance(scalar, float))
+
+        # coercion
+        self.assert_(float(Series([1.])) == 1.0)
+        self.assert_(int(Series([1.])) == 1)
+        self.assert_(long(Series([1.])) == 1)
+
+        self.assert_(bool(Series([True])) == True)
+        self.assert_(bool(Series([False])) == False)
+
+        self.assert_(bool(Series([True,True])) == True)
+        self.assert_(bool(Series([False,True])) == True)
+
+    def test_astype(self):
+        s = Series(np.random.randn(5),name='foo')
+
+        for dtype in ['float32','float64','int64','int32']:
+            astyped = s.astype(dtype)
+            self.assert_(astyped.dtype == dtype)
+            self.assert_(astyped.name == s.name)
+
     def test_constructor(self):
         # Recognize TimeSeries
         self.assert_(self.ts.is_time_series == True)
@@ -294,10 +319,6 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         # Ensure new index is not created
         self.assertEquals(id(self.ts.index), id(derived.index))
 
-        # Pass in scalar (now disabled)
-        #scalar = Series(0.5)
-        #self.assert_(isinstance(scalar, float))
-
         # Mixed type Series
         mixed = Series(['hello', np.NaN], index=[0, 1])
         self.assert_(mixed.dtype == np.object_)
@@ -2517,6 +2538,18 @@ class TestSeries(unittest.TestCase, CheckNameIntegration):
         result = s.idxmax()
         self.assert_(result == 4)
 
+    def test_ndarray_compat(self):
+
+        # test numpy compat with Series as sub-class of NDFrame
+        tsdf = DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'],
+                         index=date_range('1/1/2000', periods=1000))
+
+        def f(x):
+            return x[x.argmax()]
+        result = tsdf.apply(f)
+        expected = tsdf.max()
+        assert_series_equal(result,expected)
+
     def test_operators_corner(self):
         series = self.ts
 
diff --git a/pandas/tools/pivot.py b/pandas/tools/pivot.py
index d33563968..9bca698cd 100644
--- a/pandas/tools/pivot.py
+++ b/pandas/tools/pivot.py
@@ -291,11 +291,9 @@ def _generate_marginal_results_without_values(table, data, rows, cols, aggfunc):
 def _convert_by(by):
     if by is None:
         by = []
-    elif (np.isscalar(by) or isinstance(by, np.ndarray)
+    elif (np.isscalar(by) or isinstance(by, (np.ndarray, Series))
           or hasattr(by, '__call__')):
         by = [by]
-    elif isinstance(by, Series):
-        by = [by]
     else:
         by = list(by)
     return by
diff --git a/pandas/tools/rplot.py b/pandas/tools/rplot.py
index 5928472df..1c3d17ee9 100644
--- a/pandas/tools/rplot.py
+++ b/pandas/tools/rplot.py
@@ -1,5 +1,6 @@
 import random
 from copy import deepcopy
+from pandas.core.common import _values_from_object
 
 import numpy as np
 from pandas.compat import range, zip
@@ -498,7 +499,7 @@ class GeomHistogram(Layer):
             else:
                 ax = fig.gca()
         x = self.data[self.aes['x']]
-        ax.hist(x, self.bins, facecolor=self.colour)
+        ax.hist(_values_from_object(x), self.bins, facecolor=self.colour)
         ax.set_xlabel(self.aes['x'])
         return fig, ax
 
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 20290086a..7af1dd657 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -519,8 +519,6 @@ class DatetimeIndex(Int64Index):
 
         return summary
 
-    __str__ = __repr__
-
     def __reduce__(self):
         """Necessary for making this object picklable"""
         object_state = list(np.ndarray.__reduce__(self))
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index f38f42c89..4b2f097c2 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -261,7 +261,7 @@ def f(g):
 groupby_frame_apply_overhead = Benchmark("df.groupby('key').apply(f)", setup,
                                          start_date=datetime(2011, 10, 1))
 
-groupbym_frame_apply = Benchmark("df.groupby(['key', 'key2']).apply(f)", setup,
+groupby_frame_apply = Benchmark("df.groupby(['key', 'key2']).apply(f)", setup,
                                  start_date=datetime(2011, 10, 1))
 
 #----------------------------------------------------------------------
