commit 6c58bf7991a3f46f73e8d8ef81b7efb245fe5d36
Author: jreback <jeff@reback.net>
Date:   Thu Dec 27 11:32:00 2012 -0500

    CLN: removed keywork 'compression' from put (replaced by complib), to make nomenclature consistent
         for compression.
         doc updates for compression

diff --git a/RELEASE.rst b/RELEASE.rst
index ccd27685f..d2b995282 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -51,6 +51,11 @@ pandas 0.10.1
     - raise correctly on non-implemented column types (unicode/date)
     - handle correctly ``Term`` passed types (e.g. ``index<1000``, when index is ``Int64``), (closes GH512_)
 
+**API Changes**
+
+  - ``HDFStore``
+    - removed keyword ``compression`` from ``put`` (replaced by keyword ``complib`` to be consistent across library)
+
 .. _GH512: https://github.com/pydata/pandas/issues/512
 .. _GH1277: https://github.com/pydata/pandas/issues/1277
 
diff --git a/doc/source/io.rst b/doc/source/io.rst
index 73822b387..bf9c91390 100644
--- a/doc/source/io.rst
+++ b/doc/source/io.rst
@@ -1327,16 +1327,27 @@ Please note that HDF5 **DOES NOT RECLAIM SPACE** in the h5 files automatically.
 
 Compression
 ~~~~~~~~~~~
-``PyTables`` allows the stored data to be compressed (this applies to all kinds of stores, not just tables). You can pass ``complevel=int`` for a compression level (1-9, with 0 being no compression, and the default), ``complib=lib`` where lib is any of ``zlib, bzip2, lzo, blosc`` for whichever compression library you prefer. ``blosc`` offers very fast compression (its level defaults to 9), and is my most used. 
+``PyTables`` allows the stored data to be compressed. Tthis applies to all kinds of stores, not just tables.
 
-``PyTables`` offer better write performance when compressed after writing them, as opposed to turning on compression at the very beginning. You can use the supplied ``PyTables`` utility ``ptrepack``. In addition, ``ptrepack`` can change compression levels after the fact.
+   - Pass ``complevel=int`` for a compression level (1-9, with 0 being no compression, and the default)
+   - Pass ``complib=lib`` where lib is any of ``zlib, bzip2, lzo, blosc`` for whichever compression library you prefer.
 
-   - ``ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5``
+``HDFStore`` will use the file based compression scheme if no overriding ``complib`` or ``complevel`` options are provided. ``blosc`` offers very fast compression, and is my most used. Note that ``lzo`` and ``bzip2`` may not be installed (by Python) by default.
 
-Or on-the-fly compression
+Compression for all objects within the file
 
    - ``store_compressed = HDFStore('store_compressed.h5', complevel=9, complib='blosc')``
 
+Or on-the-fly compression (this only applies to tables). You can turn off file compression for a specific table by passing ``complevel=0``
+
+   - ``store.append('df', df, complib='zlib', complevel=5)``
+
+**ptrepack**
+
+``PyTables`` offer better write performance when compressed after writing them, as opposed to turning on compression at the very beginning. You can use the supplied ``PyTables`` utility ``ptrepack``. In addition, ``ptrepack`` can change compression levels after the fact.
+
+   - ``ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5``
+
 Furthermore ``ptrepack in.h5 out.h5`` will *repack* the file to allow you to reuse previously deleted space (alternatively, one can simply remove the file and write again).
 
 Notes & Caveats
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index 4df28b59f..346dfb7c8 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -114,28 +114,7 @@ def get_store(path, mode='a', complevel=None, complib=None,
 
     Parameters
     ----------
-    path : string
-        File path to HDF5 file
-    mode : {'a', 'w', 'r', 'r+'}, default 'a'
-
-        ``'r'``
-            Read-only; no data can be modified.
-        ``'w'``
-            Write; a new file is created (an existing file with the same
-            name would be deleted).
-        ``'a'``
-            Append; an existing file is opened for reading and writing,
-            and if the file does not exist it is created.
-        ``'r+'``
-            It is similar to ``'a'``, but the file must already exist.
-    complevel : int, 1-9, default 0
-            If a complib is specified compression will be applied
-            where possible
-    complib : {'zlib', 'bzip2', 'lzo', 'blosc', None}, default None
-            If complevel is > 0 apply compression to objects written
-            in the store wherever possible
-    fletcher32 : bool, default False
-            If applying compression use the fletcher32 checksum
+    same as HDFStore
 
     Examples
     --------
@@ -445,8 +424,7 @@ class HDFStore(object):
         # concat and return
         return concat(objs, axis = axis, verify_integrity = True)
 
-    def put(self, key, value, table=False, append=False,
-            compression=None, **kwargs):
+    def put(self, key, value, table=False, append=False, **kwargs):
         """
         Store object in HDFStore
 
@@ -461,13 +439,8 @@ class HDFStore(object):
         append : boolean, default False
             For table data structures, append the input data to the existing
             table
-        compression : {None, 'blosc', 'lzo', 'zlib'}, default None
-            Use a compression algorithm to compress the data
-            If None, the compression settings specified in the ctor will
-            be used.
         """
-        self._write_to_group(key, value, table=table, append=append,
-                             comp=compression, **kwargs)
+        self._write_to_group(key, value, table=table, append=append, **kwargs)
 
     def remove(self, key, where=None, start=None, stop=None):
         """
@@ -645,7 +618,7 @@ class HDFStore(object):
         return getattr(self, '_%s_%s' % (op, kind))
 
     def _write_to_group(self, key, value, table=False, append=False,
-                        comp=None, **kwargs):
+                        complib=None, **kwargs):
         group = self.get_node(key)
         if group is None:
             paths = key.split('/')
@@ -669,11 +642,11 @@ class HDFStore(object):
             kind = '%s_table' % kind
             handler = self._get_handler(op='write', kind=kind)
             wrapper = lambda value: handler(group, value, append=append,
-                                            comp=comp, **kwargs)
+                                            complib=complib, **kwargs)
         else:
             if append:
                 raise ValueError('Can only append to Tables')
-            if comp:
+            if complib:
                 raise ValueError('Compression only supported on Tables')
 
             handler = self._get_handler(op='write', kind=kind)
@@ -808,12 +781,11 @@ class HDFStore(object):
     def _read_wide(self, group, where=None, **kwargs):
         return Panel(self._read_block_manager(group))
 
-    def _write_ndim_table(self, group, obj, append=False, comp=None, axes=None, index=True, **kwargs):
+    def _write_ndim_table(self, group, obj, append=False, axes=None, index=True, **kwargs):
         if axes is None:
             axes = _AXES_MAP[type(obj)]
         t = create_table(self, group, typ = 'appendable_ndim')
-        t.write(axes=axes, obj=obj,
-                append=append, compression=comp, **kwargs)
+        t.write(axes=axes, obj=obj, append=append, **kwargs)
         if index:
             t.create_index(columns = index)
 
@@ -821,23 +793,22 @@ class HDFStore(object):
         t = create_table(self, group, **kwargs)
         return t.read(where, **kwargs)
 
-    def _write_frame_table(self, group, df, append=False, comp=None, axes=None, index=True, **kwargs):
+    def _write_frame_table(self, group, df, append=False, axes=None, index=True, **kwargs):
         if axes is None:
             axes = _AXES_MAP[type(df)]
 
         t = create_table(self, group, typ = 'appendable_frame' if df.index.nlevels == 1 else 'appendable_multiframe')
-        t.write(axes=axes, obj=df, append=append, compression=comp, **kwargs)
+        t.write(axes=axes, obj=df, append=append, **kwargs)
         if index:
             t.create_index(columns = index)
 
     _read_frame_table = _read_ndim_table
 
-    def _write_wide_table(self, group, panel, append=False, comp=None, axes=None, index=True, **kwargs):
+    def _write_wide_table(self, group, panel, append=False, axes=None, index=True, **kwargs):
         if axes is None:
             axes = _AXES_MAP[type(panel)]
         t = create_table(self, group, typ = 'appendable_panel')
-        t.write(axes=axes, obj=panel,
-                append=append, compression=comp, **kwargs)
+        t.write(axes=axes, obj=panel, append=append, **kwargs)
         if index:
             t.create_index(columns = index)
 
@@ -1902,7 +1873,7 @@ class Table(object):
 
         return obj
 
-    def create_description(self, compression = None, complevel = None, expectedrows = None):
+    def create_description(self, complib = None, complevel = None, fletcher32 = False, expectedrows = None):
         """ create the description of the table from the axes & values """
 
         # expected rows estimate
@@ -1913,13 +1884,12 @@ class Table(object):
         # description from the axes & values
         d['description'] = dict([ (a.cname,a.typ) for a in self.axes ])
 
-        if compression:
-            complevel = self.complevel
+        if complib:
             if complevel is None:
-                complevel = 9
-            filters = _tables().Filters(complevel=complevel,
-                                        complib=compression,
-                                        fletcher32=self.fletcher32)
+                complevel = self.complevel or 9
+            filters = _tables().Filters(complevel  = complevel,
+                                        complib    = complib,
+                                        fletcher32 = fletcher32 or self.fletcher32)
             d['filters'] = filters
         elif self.filters is not None:
             d['filters'] = self.filters
@@ -2104,8 +2074,8 @@ class AppendableTable(LegacyTable):
     _indexables = None
     table_type = 'appendable'
 
-    def write(self, axes, obj, append=False, compression=None,
-              complevel=None, min_itemsize = None, chunksize = 50000,
+    def write(self, axes, obj, append=False, complib=None,
+              complevel=None, fletcher32=None, min_itemsize = None, chunksize = 50000,
               expectedrows = None, **kwargs):
 
         # create the table if it doesn't exist (or get it if it does)
@@ -2119,7 +2089,10 @@ class AppendableTable(LegacyTable):
         if 'table' not in self.group:
 
             # create the table
-            options = self.create_description(compression = compression, complevel = complevel, expectedrows = expectedrows)
+            options = self.create_description(complib      = complib, 
+                                              complevel    = complevel, 
+                                              fletcher32   = fletcher32,
+                                              expectedrows = expectedrows)
 
             # set the table attributes
             self.set_attrs()
diff --git a/pandas/io/tests/test_pytables.py b/pandas/io/tests/test_pytables.py
index a594da99a..6f11ebdaa 100644
--- a/pandas/io/tests/test_pytables.py
+++ b/pandas/io/tests/test_pytables.py
@@ -203,12 +203,12 @@ class TestHDFStore(unittest.TestCase):
     def test_put_compression(self):
         df = tm.makeTimeDataFrame()
 
-        self.store.put('c', df, table=True, compression='zlib')
+        self.store.put('c', df, table=True, complib='zlib')
         tm.assert_frame_equal(self.store['c'], df)
 
         # can't compress if table=False
         self.assertRaises(ValueError, self.store.put, 'b', df,
-                          table=False, compression='zlib')
+                          table=False, complib='zlib')
 
     def test_put_compression_blosc(self):
         tm.skip_if_no_package('tables', '2.2', app='blosc support')
@@ -216,9 +216,9 @@ class TestHDFStore(unittest.TestCase):
 
         # can't compress if table=False
         self.assertRaises(ValueError, self.store.put, 'b', df,
-                          table=False, compression='blosc')
+                          table=False, complib='blosc')
 
-        self.store.put('c', df, table=True, compression='blosc')
+        self.store.put('c', df, table=True, complib='blosc')
         tm.assert_frame_equal(self.store['c'], df)
 
     def test_put_integer(self):
