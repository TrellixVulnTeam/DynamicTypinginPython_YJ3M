commit ff534d103dcda4e81f38a41281041e3f27c2543f
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Mon Sep 3 19:57:48 2012 -0400

    ENH: fix segfaulting issue in to_boolean, start on header parsing support etc.

diff --git a/pandas/io/tests/test1.csv b/pandas/io/tests/test1.csv
index 8774b3bdf..4bdb62943 100644
--- a/pandas/io/tests/test1.csv
+++ b/pandas/io/tests/test1.csv
@@ -5,4 +5,4 @@ index,A,B,C,D
 2000-01-06 00:00:00,1.12020151869,1.56762092543,0.00364077397681,0.67525259227
 2000-01-07 00:00:00,-0.487094399463,0.571454623474,-1.6116394093,0.103468562917
 2000-01-10 00:00:00,0.836648671666,0.246461918642,0.588542635376,1.0627820613
-2000-01-11 00:00:00,-0.157160753327,1.34030689438,1.19577795622,-1.09700699751
+2000-01-11 00:00:00,-0.157160753327,1.34030689438,1.19577795622,-1.09700699751
\ No newline at end of file
diff --git a/pandas/io/tests/test_cparser.py b/pandas/io/tests/test_cparser.py
index 084ccb20f..f1027f34d 100644
--- a/pandas/io/tests/test_cparser.py
+++ b/pandas/io/tests/test_cparser.py
@@ -43,10 +43,6 @@ class TestCParser(unittest.TestCase):
         self.csv2 = os.path.join(self.dirpath, 'test2.csv')
         self.xls1 = os.path.join(self.dirpath, 'test.xls')
 
-    def test_string_filename(self):
-        reader = TextReader(self.csv1)
-        result = reader.read()
-
     def test_file_handle(self):
         try:
             f = open(self.csv1, 'rb')
@@ -55,23 +51,27 @@ class TestCParser(unittest.TestCase):
         finally:
             f.close()
 
+    def test_string_filename(self):
+        reader = TextReader(self.csv1, header=None)
+        result = reader.read()
+
     def test_file_handle_mmap(self):
         try:
             f = open(self.csv1, 'rb')
-            reader = TextReader(f, memory_map=True)
+            reader = TextReader(f, memory_map=True, header=None)
             result = reader.read()
         finally:
             f.close()
 
     def test_StringIO(self):
         text = open(self.csv1, 'rb').read()
-        reader = TextReader(BytesIO(text))
+        reader = TextReader(BytesIO(text), header=None)
         result = reader.read()
 
     def test_string_factorize(self):
         # should this be optional?
         data = 'a\nb\na\nb\na'
-        reader = TextReader(StringIO(data))
+        reader = TextReader(StringIO(data), header=None)
         result = reader.read()
         self.assert_(len(set(map(id, result[0]))) == 2)
 
@@ -81,7 +81,8 @@ class TestCParser(unittest.TestCase):
                 'a,   b\n'
                 'a,   b')
 
-        reader = TextReader(StringIO(data), skipinitialspace=True)
+        reader = TextReader(StringIO(data), skipinitialspace=True,
+                            header=None)
         result = reader.read()
 
         self.assert_(np.array_equal(result[0], ['a', 'a', 'a', 'a']))
@@ -90,7 +91,7 @@ class TestCParser(unittest.TestCase):
     def test_parse_booleans(self):
         data = 'True\nFalse\nTrue\nTrue'
 
-        reader = TextReader(StringIO(data))
+        reader = TextReader(StringIO(data), header=None)
         result = reader.read()
 
         self.assert_(result[0].dtype == np.bool_)
@@ -98,7 +99,8 @@ class TestCParser(unittest.TestCase):
     def test_delimit_whitespace(self):
         data = 'a  b\na\t\t "b"\n"a"\t \t b'
 
-        reader = TextReader(StringIO(data), delim_whitespace=True)
+        reader = TextReader(StringIO(data), delim_whitespace=True,
+                            header=None)
         result = reader.read()
 
         self.assert_(np.array_equal(result[0], ['a', 'a', 'a']))
@@ -107,7 +109,7 @@ class TestCParser(unittest.TestCase):
     def test_embedded_newline(self):
         data = 'a\n"hello\nthere"\nthis'
 
-        reader = TextReader(StringIO(data))
+        reader = TextReader(StringIO(data), header=None)
         result = reader.read()
 
         expected = ['a', 'hello\nthere', 'this']
@@ -117,7 +119,7 @@ class TestCParser(unittest.TestCase):
         data = '12345,67\n345,678'
 
         reader = TextReader(StringIO(data), delimiter=':',
-                                   decimal=',')
+                            decimal=',', header=None)
         result = reader.read()
 
         expected = [12345.67, 345.678]
@@ -127,7 +129,7 @@ class TestCParser(unittest.TestCase):
         data = '123,456\n12,500'
 
         reader = TextReader(StringIO(data), delimiter=':',
-                                   thousands=',')
+                            thousands=',', header=None)
         result = reader.read()
 
         expected = [123456, 12500]
@@ -140,10 +142,12 @@ class TestCParser(unittest.TestCase):
                 'j:k\n'
                 'l:m:n')
 
-        reader = TextReader(StringIO(data), delimiter=':')
+        reader = TextReader(StringIO(data), delimiter=':',
+                            header=None)
         self.assertRaises(parser.CParserError, reader.read)
 
         reader = TextReader(StringIO(data), delimiter=':',
+                            header=None,
                             error_bad_lines=False,
                             warn_bad_lines=False)
         result = reader.read()
@@ -152,6 +156,39 @@ class TestCParser(unittest.TestCase):
                     2: ['c', 'f', 'i', 'n']}
         assert_array_dicts_equal(result, expected)
 
+    def test_header_not_enough_lines(self):
+        data = ('skip this\n'
+                'skip this\n'
+                'a,b,c\n'
+                '1,2,3\n'
+                '4,5,6')
+
+        reader = TextReader(StringIO(data), delimiter=',', header=2,
+                            as_recarray=True)
+        header = reader.get_header()
+        expected = ['a', 'b', 'c']
+        self.assertEquals(header, expected)
+
+        recs = reader.read()
+        expected = {'a': [1, 4], 'b': [2, 5], 'c': [3, 6]}
+        assert_array_dicts_equal(expected, recs)
+
+        # not enough rows
+        reader = TextReader(StringIO(data), delimiter=',', header=5,
+                            as_recarray=True)
+        self.assertRaises(parser.CParserError, reader.get_header)
+
+    def test_escapechar(self):
+        data = ('\\"hello world\"\n'
+                '\\"hello world\"\n'
+                '\\"hello world\"')
+
+        reader = TextReader(StringIO(data), delimiter=',', header=None,
+                            escapechar='\\')
+        result = reader.read()
+        expected = {0: ['"hello world"'] * 3}
+        assert_array_dicts_equal(result, expected)
+
     def test_eof_has_eol(self):
         # handling of new line at EOF
         pass
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index e5f12d577..2c4ff647e 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -62,6 +62,7 @@ cdef extern from "parser/parser.h":
         QUOTE_IN_QUOTED_FIELD
         EAT_CRNL
         EAT_WHITESPACE
+        FINISHED
 
     ctypedef struct table_chunk:
         void **columns
@@ -142,9 +143,8 @@ cdef extern from "parser/parser.h":
         char **words
         int *line_start
         int col
-        int line
 
-    void coliter_setup(coliter_t *it, parser_t *parser, int i)
+    void coliter_setup(coliter_t *it, parser_t *parser, int i, int start)
     char* COLITER_NEXT(coliter_t it)
 
     parser_t* parser_new()
@@ -169,7 +169,7 @@ cdef extern from "parser/parser.h":
 
 
 
-DEFAULT_CHUNKSIZE = 256 * 1024
+DEFAULT_CHUNKSIZE = 1024 * 1024
 
 cdef class TextReader:
     '''
@@ -182,13 +182,16 @@ cdef class TextReader:
         parser_t *parser
         object file_handle, should_close
         bint factorize, na_filter
+        int parser_start
 
     cdef public:
         object delimiter, na_values, converters, delim_whitespace
         object memory_map
         object as_recarray
+        object header
 
-    def __cinit__(self, source, delimiter=b',', header=0,
+    def __cinit__(self, source, delimiter=b',',
+                  header=0,
                   memory_map=False,
                   tokenize_chunksize=DEFAULT_CHUNKSIZE,
                   delim_whitespace=False,
@@ -198,6 +201,7 @@ cdef class TextReader:
                   factorize=True,
                   as_recarray=False,
                   skipinitialspace=False,
+                  escapechar=None,
                   decimal=b'.',
                   error_bad_lines=True,
                   warn_bad_lines=True,
@@ -219,8 +223,16 @@ cdef class TextReader:
 
         self.factorize = factorize
 
+        self.header = None
         # TODO: no header vs. header is not the first row
-        self.parser.header = header
+        if header is None:
+            # sentinel value
+            self.parser.header = -1
+            self.parser_start = 0
+        else:
+            self.parser.header = header
+            self.parser_start = header + 1
+
         self.parser.skipinitialspace = skipinitialspace
 
         if len(decimal) != 1:
@@ -232,6 +244,11 @@ cdef class TextReader:
                 raise ValueError('Only length-1 decimal markers supported')
             self.parser.thousands = ord(thousands)
 
+        if escapechar is not None:
+            if len(escapechar) != 1:
+                raise ValueError('Only length-1 escapes  supported')
+            self.parser.escapechar = ord(escapechar)
+
         # error handling of bad lines
         self.parser.error_bad_lines = int(error_bad_lines)
         self.parser.warn_bad_lines = int(warn_bad_lines)
@@ -247,6 +264,7 @@ cdef class TextReader:
 
         self.na_filter = na_filter
         self.as_recarray = as_recarray
+        self.header = None
 
     def __init__(self, *args, **kwards):
         pass
@@ -295,8 +313,38 @@ cdef class TextReader:
                 raise Exception('Initializing parser from file-like '
                                 'object failed')
 
-    def _parse_table_header(self):
-        pass
+    def get_header(self):
+        if self.parser.header >= 0:
+            self.header = self._parse_table_header()
+
+        return self.header
+
+    cdef _parse_table_header(self):
+        cdef:
+            size_t i, start, fields
+            char *word
+            # ndarray[object] header
+
+        if self.parser.lines < self.parser.header + 1:
+            tokenize_nrows(self.parser, self.parser.header + 1)
+
+        # e.g., if header=3 and file only has 2 lines
+        if self.parser.lines < self.parser.header + 1:
+            raise CParserError('Passed header=%d but only %d lines in file'
+                               % (self.parser.header, self.parser.lines))
+
+        fields = self.parser.line_fields[self.parser.header]
+        start = self.parser.line_start[self.parser.header]
+
+        # TODO: Py3 vs. Py2
+
+        # np.empty(fields, dtype=np.object_)
+        header = []
+        for i in range(fields):
+            word = self.parser.words[start + i]
+            header.append(PyString_FromString(word))
+
+        return header
 
     def read(self, rows=None):
         """
@@ -321,7 +369,13 @@ cdef class TextReader:
             raise_parser_error('Error tokenizing data', self.parser)
 
         # start = time.clock()
+
         columns, names = self._convert_column_data()
+
+        header = self.get_header()
+        if header is not None:
+            names = header
+
         # end = time.clock()
         # print 'Type conversion took %.4f sec' % (end - start)
 
@@ -344,14 +398,14 @@ cdef class TextReader:
             kh_str_t *table
             int start, end
 
-        ncols = self.parser.line_fields[0]
-
         na_values = ['NA', 'nan', 'NaN']
         table = kset_from_list(na_values)
 
-        start = 0
+        start = self.parser_start
         end = self.parser.lines
 
+        ncols = self.parser.line_fields[start]
+
         names = []
 
         results = {}
@@ -436,7 +490,7 @@ cdef _string_box_factorize(parser_t *parser, int col,
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.object_)
 
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
 
     for i in range(lines):
         word = COLITER_NEXT(it)
@@ -485,7 +539,7 @@ cdef _try_double(parser_t *parser, int col, int line_start, int line_end,
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.float64)
     data = <double *> result.data
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
 
     if na_filter:
         for i in range(lines):
@@ -530,7 +584,7 @@ cdef _try_int64(parser_t *parser, int col, int line_start, int line_end,
 
     data = <int64_t *> result.data
 
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
 
     if na_filter:
         for i in range(lines):
@@ -574,7 +628,7 @@ cdef _try_bool(parser_t *parser, int col, int line_start, int line_end,
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.uint8)
     data = <uint8_t *> result.data
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
 
     if na_filter:
         for i in range(lines):
@@ -617,7 +671,7 @@ cdef _get_na_mask(parser_t *parser, int col, int line_start, int line_end,
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.bool_)
 
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
     for i in range(lines):
         word = COLITER_NEXT(it)
 
@@ -700,7 +754,7 @@ cdef _apply_converter(object f, parser_t *parser, int col,
     lines = line_end - line_start
     result = np.empty(lines, dtype=np.object_)
 
-    coliter_setup(&it, parser, col)
+    coliter_setup(&it, parser, col, line_start)
     for i in range(lines):
         word = COLITER_NEXT(it)
         val = PyString_FromString(word)
@@ -708,7 +762,7 @@ cdef _apply_converter(object f, parser_t *parser, int col,
 
     return lib.maybe_convert_objects(result)
 
-def _to_structured_array(dict columns, object colnames):
+def _to_structured_array(dict columns, object names):
     cdef:
         ndarray recs, column
         cnp.dtype dt
@@ -719,7 +773,8 @@ def _to_structured_array(dict columns, object colnames):
         int stride, elsize
         char *buf
 
-    dt = np.dtype([(str(name), columns[name].dtype) for name in colnames])
+    dt = np.dtype([(str(name), columns[i].dtype)
+                   for i, name in enumerate(names)])
     fnames = dt.names
     fields = dt.fields
 
@@ -742,7 +797,7 @@ def _to_structured_array(dict columns, object colnames):
 
     for i in range(nfields):
         # start = time.clock()
-        name = colnames[i]
+        # name = names[i]
 
         # XXX
         field_type = fields[fnames[i]]
@@ -750,7 +805,7 @@ def _to_structured_array(dict columns, object colnames):
         # (dtype, stride) tuple
         offset = field_type[1]
         elsize = field_type[0].itemsize
-        column = columns[name]
+        column = columns[i]
 
         _fill_structured_column(buf + offset, <char*> column.data,
                                 elsize, stride, length,
diff --git a/pandas/src/parser/Makefile b/pandas/src/parser/Makefile
index f23da130a..ec88eaf44 100644
--- a/pandas/src/parser/Makefile
+++ b/pandas/src/parser/Makefile
@@ -3,7 +3,7 @@ NUMPY_INC = /Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/site-
 PYTHON_INC = -I$(PYTHONBASE)/include/python2.7 -I$(NUMPY_INC)
 PYTHON_LINK = -L$(PYTHONBASE)/lib -lpython
 
-SOURCES = common.c conversions.c rows.c str_to.c
+SOURCES = conversions.c parser.c str_to.c
 
 check-syntax:
 	gcc -g $(PYTHON_INC) -o /dev/null -S ${CHK_SOURCES}
diff --git a/pandas/src/parser/conversions.c b/pandas/src/parser/conversions.c
index 9e020d700..d938f9f46 100644
--- a/pandas/src/parser/conversions.c
+++ b/pandas/src/parser/conversions.c
@@ -138,7 +138,7 @@ int inline to_boolean(char *item, uint8_t *val) {
 	char *tstr = "TRUE";
 	char *fstr = "FALSE";
 
-	tmp = malloc(sizeof(char) * sizeof(item));
+	tmp = malloc(sizeof(char) * strlen(item));
 	strcpy(tmp, item);
 	uppercase(tmp);
 
diff --git a/pandas/src/parser/parser.c b/pandas/src/parser/parser.c
index 1b3253ef2..b45d88efb 100644
--- a/pandas/src/parser/parser.c
+++ b/pandas/src/parser/parser.c
@@ -61,12 +61,11 @@ void *safe_realloc(void *buffer, size_t size) {
 }
 
 
-void coliter_setup(coliter_t *self, parser_t *parser, int i) {
+void coliter_setup(coliter_t *self, parser_t *parser, int i, int start) {
     // column i, starting at 0
     self->words = parser->words;
     self->col = i;
-    self->line = 0;
-    self->line_start = parser->line_start;
+    self->line_start = parser->line_start + start;
 }
 
 coliter_t *coliter_new(parser_t *self, int i) {
@@ -77,7 +76,7 @@ coliter_t *coliter_new(parser_t *self, int i) {
         return NULL;
     }
 
-    coliter_setup(iter, self, i);
+    coliter_setup(iter, self, i, 0);
     return iter;
 }
 
@@ -124,6 +123,7 @@ coliter_t *coliter_new(parser_t *self, int i) {
 
  #define FS(source) ((file_source *)source)
 
+
  void *new_file_source(FILE *fp) {
      file_source *fs = (file_source *) malloc(sizeof(file_source));
      fs->fp = fp;
@@ -142,6 +142,23 @@ coliter_t *coliter_new(parser_t *self, int i) {
      free(fs);
  }
 
+
+int parser_file_source_init(parser_t *self, FILE* fp) {
+    self->sourcetype = 'F';
+    self->source = new_file_source(fp);
+
+    // Only allocate this heap memory if we are not memory-mapping the file
+    self->data = (char*) malloc((self->chunksize + 1) * sizeof(char));
+    memset(self->data, 0, self->chunksize + 1);
+    self->data[self->chunksize] = '\0';
+
+    if (self->data == NULL) {
+        return PARSER_OUT_OF_MEMORY;
+    }
+
+    return 0;
+}
+
  /*
 
    In-memory bytes
@@ -626,7 +643,7 @@ void parser_set_default_options(parser_t *self) {
 
     self->doublequote = 0;
     self->quotechar = '"';
-    self->escapechar = '\\';
+    self->escapechar = 0;
     self->skipinitialspace = 0;
     self->quoting = QUOTE_MINIMAL;
     self->allow_embedded_newline = 1;
@@ -650,20 +667,6 @@ parser_t* parser_new() {
     return (parser_t*) calloc(1, sizeof(parser_t));
 }
 
-int parser_file_source_init(parser_t *self, FILE* fp) {
-    self->sourcetype = 'F';
-    self->source = new_file_source(fp);
-
-    // Only allocate this heap memory if we are not memory-mapping the file
-    self->data = (char*) malloc((self->chunksize + 1) * sizeof(char));
-
-    if (self->data == NULL) {
-        return PARSER_OUT_OF_MEMORY;
-    }
-
-    return 0;
-}
-
 // XXX handle on systems without the capability
 
 #include <sys/stat.h>
@@ -864,7 +867,7 @@ int make_stream_space(parser_t *self, size_t nbytes) {
 
     // Can we fit potentially nbytes tokens (+ null terminators) in the stream?
 
-    TRACE(("maybe growing buffers\n"));
+    /* TRACE(("maybe growing buffers\n")); */
 
     /*
       TOKEN STREAM
@@ -883,7 +886,7 @@ int make_stream_space(parser_t *self, size_t nbytes) {
     // realloc sets errno when moving buffer?
     if (self->stream != orig_ptr) {
         // uff
-        TRACE(("Moving word pointers\n"))
+        /* TRACE(("Moving word pointers\n")) */
 
         self->pword_start = self->stream + self->word_start;
 
@@ -942,7 +945,7 @@ int make_stream_space(parser_t *self, size_t nbytes) {
     }
 
 
-    TRACE(("finished growing buffers\n"));
+    /* TRACE(("finished growing buffers\n")); */
 
     return 0;
 }
@@ -982,71 +985,60 @@ int inline end_field(parser_t *self) {
 }
 
 int inline end_line(parser_t *self) {
-    int fields, ex_fields;
+    int fields;
+    int ex_fields = -1;
 
     fields = self->line_fields[self->lines];
 
-    // TODO: header line handling
-
-    if (self->lines == 0) {
-        // Nothing to check here
-        self->file_lines++;
-        self->lines++;
-        self->line_fields[self->lines] = 0;
-        self->line_start[self->lines] = fields;
-    } else {
+    if (self->lines > 0) {
         ex_fields = self->line_fields[self->lines - 1];
+    }
 
-        // TODO: better check here
-        if (fields != ex_fields) {
-            // increment file line count
-            self->file_lines++;
+    if (!(self->lines <= self->header + 1) && fields != ex_fields) {
+        // increment file line count
+        self->file_lines++;
 
-            // skip the tokens from this bad line
-            self->line_start[self->lines] += fields;
+        // skip the tokens from this bad line
+        self->line_start[self->lines] += fields;
 
-            // reset field count
-            self->line_fields[self->lines] = 0;
+        // reset field count
+        self->line_fields[self->lines] = 0;
 
-            // file_lines is now the _actual_ file line number (starting at 1)
+        // file_lines is now the _actual_ file line number (starting at 1)
 
-            if (self->error_bad_lines) {
-                self->error_msg = (char*) malloc(100);
-                sprintf(self->error_msg, "Expected %d fields in line %d, saw %d\n",
-                        ex_fields, self->file_lines, fields);
+        if (self->error_bad_lines) {
+            self->error_msg = (char*) malloc(100);
+            sprintf(self->error_msg, "Expected %d fields in line %d, saw %d\n",
+                    ex_fields, self->file_lines, fields);
 
-                return -1;
-            } else {
-                // simply skip bad lines
-                if (self->warn_bad_lines) {
-                    // print error message
-                    printf("Skipping line %d: expected %d fields, saw %d\n",
-                           self->file_lines, ex_fields, fields);
-                }
-            }
+            return -1;
         } else {
-            // increment both line counts
-            self->lines++;
-            self->file_lines++;
+            // simply skip bad lines
+            if (self->warn_bad_lines) {
+                // print error message
+                printf("Skipping line %d: expected %d fields, saw %d\n",
+                       self->file_lines, ex_fields, fields);
+            }
+        }
+    } else {
+        // increment both line counts
+        self->lines++;
+        self->file_lines++;
 
-            // good line, set new start point
-            self->line_start[self->lines] = (self->line_start[self->lines - 1]
-                                             + fields);
+        // good line, set new start point
+        self->line_start[self->lines] = (self->line_start[self->lines - 1] +
+                                         fields);
 
-            // new line start with 0 fields
-            self->line_fields[self->lines] = 0;
-        }
+        // new line start with 0 fields
+        self->line_fields[self->lines] = 0;
     }
 
+    TRACE(("Finished line, at %d\n", self->lines));
 
     return 0;
 }
 
 int parser_clear_data_buffers(parser_t *self) {
-    if (self->sourcetype == 'F') {
-        free_if_not_null(self->data);
-    }
-
     free_if_not_null(self->stream);
     free_if_not_null(self->words);
     free_if_not_null(self->word_starts);
@@ -1148,7 +1140,7 @@ int _buffer_array_bytes(parser_t *self, size_t nbytes) {
 
     TRACE(("datalen: %d\n", self->datalen));
 
-    TRACE(("pos: %d, length: %d", src->position, src->length));
+    TRACE(("pos: %d, length: %d", (int) src->position, (int) src->length));
     return 0;
 }
 
@@ -1157,6 +1149,7 @@ int parser_cleanup_filebuffers(parser_t *self) {
     switch(self->sourcetype) {
 
         case 'F':
+            free(self->data);
             del_file_source(self->source);
             break;
 
@@ -1239,8 +1232,8 @@ int tokenize_delimited(parser_t *self)
         // Next character in file
         c = *buf++;
 
-        TRACE(("Iter: %d Char: %c Line %d field_count %d\n",
-               i, c, self->file_lines + 1, self->line_fields[self->file_lines]));
+        /* TRACE(("Iter: %d Char: %c Line %d field_count %d\n", */
+        /*        i, c, self->file_lines + 1, self->line_fields[self->file_lines])); */
 
         switch(self->state) {
         case START_RECORD:
@@ -1699,6 +1692,10 @@ int _tokenize_helper(parser_t *self, size_t nrows, int all) {
         tokenize_bytes = tokenize_delimited;
     }
 
+    if (self->state == FINISHED) {
+        return 0;
+    }
+
     while (1) {
         if (!all && self->lines - start_lines >= nrows)
             break;
@@ -1712,6 +1709,7 @@ int _tokenize_helper(parser_t *self, size_t nrows, int all) {
         if (status == REACHED_EOF) {
             // close out last line
             status = parser_handle_eof(self);
+            self->state = FINISHED;
             break;
         }
 
@@ -1752,7 +1750,8 @@ int test_tokenize(char *fname) {
 
     self = parser_new();
     self->chunksize = nbytes;
-    self->source = malloc(sizeof(file_source));
+
+    // self->source = malloc(sizeof(file_source));
 
     FILE* fp = fopen(fname, "rb");
     parser_file_source_init(self, fp);
@@ -1763,6 +1762,8 @@ int test_tokenize(char *fname) {
         return -1;
     }
 
+    self->header = 0;
+
     status = tokenize_all_rows(self);
 
     if (status != 0) {
@@ -1802,7 +1803,7 @@ int test_tokenize(char *fname) {
 
     for (j = 0; j < columns; ++j)
     {
-        coliter_setup(&citer, self, j);
+        coliter_setup(&citer, self, j, 0);
 
         for (i = 0; i < self->lines; ++i)
         {
@@ -1871,6 +1872,8 @@ int test_tokenize(char *fname) {
 
     parser_free(self);
 
+    fclose(fp);
+
     /* if (parser_cleanup(self) < 0) { */
     /*     return -1; */
     /* } */
@@ -1925,7 +1928,7 @@ int main(int argc, char *argv[])
     int i;
     TRACE(("hello: %s\n", "Wes"));
 
-    test_tokenize("/Users/wesm/code/textreader/foo.csv");
+    test_tokenize("/Users/wesm/code/pandas/pandas/io/tests/test1.csv");
 
     // char *msg = (char*) malloc(50);
     // sprintf(msg, "Hello: %s\n", "wes");
diff --git a/pandas/src/parser/parser.h b/pandas/src/parser/parser.h
index fc699e79f..1c59fb2f9 100644
--- a/pandas/src/parser/parser.h
+++ b/pandas/src/parser/parser.h
@@ -87,7 +87,8 @@ typedef enum {
     ESCAPE_IN_QUOTED_FIELD,
     QUOTE_IN_QUOTED_FIELD,
     EAT_CRNL,
-    EAT_WHITESPACE
+    EAT_WHITESPACE,
+    FINISHED
 } ParserState;
 
 typedef enum {
@@ -184,14 +185,15 @@ typedef struct coliter_t {
     char **words;
     int *line_start;
     int col;
-    int line;
 } coliter_t;
 
-void coliter_setup(coliter_t *self, parser_t *parser, int i);
+void coliter_setup(coliter_t *self, parser_t *parser, int i, int start);
 coliter_t *coliter_new(parser_t *self, int i);
 
 /* #define COLITER_NEXT(iter) iter->words[iter->line_start[iter->line++] + iter->col] */
-#define COLITER_NEXT(iter) iter.words[iter.line_start[iter.line++] + iter.col]
+// #define COLITER_NEXT(iter) iter.words[iter.line_start[iter.line++] + iter.col]
+
+#define COLITER_NEXT(iter) iter.words[*iter.line_start++ + iter.col]
 
 parser_t* parser_new();
 
