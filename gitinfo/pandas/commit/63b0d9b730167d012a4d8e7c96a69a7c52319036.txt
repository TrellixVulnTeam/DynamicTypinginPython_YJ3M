commit 63b0d9b730167d012a4d8e7c96a69a7c52319036
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue May 15 18:16:20 2012 -0400

    REF: microsecond -> nanosecond migration, most of the way there #1238

diff --git a/pandas/core/common.py b/pandas/core/common.py
index 6e92e55f2..2da212cbd 100644
--- a/pandas/core/common.py
+++ b/pandas/core/common.py
@@ -67,7 +67,7 @@ def isnull(obj):
 
             if isinstance(obj, Series):
                 result = Series(result, index=obj.index, copy=False)
-        elif obj.dtype == np.datetime64:
+        elif obj.dtype == np.dtype('M8[ns]'):
             # this is the NaT pattern
             result = np.array(obj).view('i8') == lib.NaT
         else:
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index cbd1ccfab..7e8e67274 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -1125,7 +1125,7 @@ def form_blocks(data, axes):
 
     if len(datetime_dict):
         datetime_block = _simple_blockify(datetime_dict, items,
-                                          np.dtype('M8[us]'))
+                                          np.dtype('M8[ns]'))
         blocks.append(datetime_block)
 
     if len(bool_dict):
diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index 8fb01d1a8..e742bdb55 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -406,7 +406,7 @@ def unique1d(values):
                            dtype=np.int64)
 
         if values.dtype == np.datetime64:
-            uniques = uniques.view('M8[us]')
+            uniques = uniques.view('M8[ns]')
     else:
         table = lib.PyObjectHashTable(len(values))
         uniques = table.unique(com._ensure_object(values))
diff --git a/pandas/io/pytables.py b/pandas/io/pytables.py
index f41952d39..7ac5ad901 100644
--- a/pandas/io/pytables.py
+++ b/pandas/io/pytables.py
@@ -963,7 +963,7 @@ def _read_array(group, key):
 
 def _unconvert_index(data, kind):
     if kind == 'datetime64':
-        index = np.array(data, dtype='M8[us]')
+        index = np.array(data, dtype='M8[ns]')
     elif kind == 'datetime':
         index = np.array([datetime.fromtimestamp(v) for v in data],
                          dtype=object)
diff --git a/pandas/src/datetime.pyx b/pandas/src/datetime.pyx
index 4627e0bd8..f623376bd 100644
--- a/pandas/src/datetime.pyx
+++ b/pandas/src/datetime.pyx
@@ -47,10 +47,9 @@ except NameError: # py3
 # This serves as the box for datetime64
 class Timestamp(_Timestamp):
 
-    __slots__ = ['value', 'offset']
-
     def __new__(cls, object ts_input, object offset=None, tz=None):
         cdef _TSObject ts
+        cdef _Timestamp ts_base
 
         if isinstance(ts_input, float):
             # to do, do we want to support this, ie with fractional seconds?
@@ -72,6 +71,7 @@ class Timestamp(_Timestamp):
         # fill out rest of data
         ts_base.value = ts.value
         ts_base.offset = offset
+        ts_base.nanosecond = ts.dts.ps / 1000
 
         return ts_base
 
@@ -185,7 +185,7 @@ def apply_offset(ndarray[object] values, object offset):
         ndarray[int64_t] new_values
         object boxed
 
-    result = np.empty(n, dtype='M8[us]')
+    result = np.empty(n, dtype='M8[ns]')
     new_values = result.view('i8')
     pass
 
@@ -194,8 +194,8 @@ def apply_offset(ndarray[object] values, object offset):
 # (see Timestamp class above). This will serve as a C extension type that
 # shadows the python class, where we do any heavy lifting.
 cdef class _Timestamp(datetime):
-    cdef:
-        int64_t value       # numpy int64
+    cdef public:
+        int64_t value, nanosecond
         object offset       # frequency reference
 
     def __add__(self, other):
@@ -250,13 +250,13 @@ cpdef convert_to_tsobject(object ts, object tz=None):
 
     if is_datetime64_object(ts):
         obj.value = unbox_datetime64_scalar(ts)
-        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_us, &obj.dts)
+        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns, &obj.dts)
     elif is_integer_object(ts):
         obj.value = ts
-        pandas_datetime_to_datetimestruct(ts, PANDAS_FR_us, &obj.dts)
+        pandas_datetime_to_datetimestruct(ts, PANDAS_FR_ns, &obj.dts)
     elif util.is_string_object(ts):
         _string_to_dts(ts, &obj.dts)
-        obj.value = pandas_datetimestruct_to_datetime(PANDAS_FR_us, &obj.dts)
+        obj.value = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &obj.dts)
     elif PyDateTime_Check(ts):
         obj.value = _pydatetime_to_dts(ts, &obj.dts)
         obj.tzinfo = ts.tzinfo
@@ -280,7 +280,7 @@ cpdef convert_to_tsobject(object ts, object tz=None):
             obj.value = obj.value + deltas[pos]
 
             if utc_convert:
-                pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_us,
+                pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns,
                                                  &obj.dts)
                 obj.tzinfo = tz._tzinfos[inf]
 
@@ -297,7 +297,7 @@ cpdef convert_to_tsobject(object ts, object tz=None):
 
 cdef inline object _datetime64_to_datetime(int64_t val):
     cdef pandas_datetimestruct dts
-    pandas_datetime_to_datetimestruct(val, PANDAS_FR_us, &dts)
+    pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
     return _dts_to_pydatetime(&dts)
 
 cdef inline object _dts_to_pydatetime(pandas_datetimestruct *dts):
@@ -313,7 +313,7 @@ cdef inline int64_t _pydatetime_to_dts(object val, pandas_datetimestruct *dts):
     dts.min = PyDateTime_DATE_GET_MINUTE(val)
     dts.sec = PyDateTime_DATE_GET_SECOND(val)
     dts.us = PyDateTime_DATE_GET_MICROSECOND(val)
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_us, dts)
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
 
 cdef inline int64_t _dtlike_to_datetime64(object val,
                                           pandas_datetimestruct *dts):
@@ -324,7 +324,7 @@ cdef inline int64_t _dtlike_to_datetime64(object val,
     dts.min = val.minute
     dts.sec = val.second
     dts.us = val.microsecond
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_us, dts)
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
 
 cdef inline int64_t _date_to_datetime64(object val,
                                         pandas_datetimestruct *dts):
@@ -335,7 +335,7 @@ cdef inline int64_t _date_to_datetime64(object val,
     dts.min = 0
     dts.sec = 0
     dts.us = 0
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_us, dts)
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
 
 
 cdef inline int _string_to_dts(object val, pandas_datetimestruct* dts) except -1:
@@ -345,7 +345,7 @@ cdef inline int _string_to_dts(object val, pandas_datetimestruct* dts) except -1
 
     if PyUnicode_Check(val):
         val = PyUnicode_AsASCIIString(val);
-    parse_iso_8601_datetime(val, len(val), PANDAS_FR_us, NPY_UNSAFE_CASTING,
+    parse_iso_8601_datetime(val, len(val), PANDAS_FR_ns, NPY_UNSAFE_CASTING,
                             dts, &islocal, &out_bestunit, &special)
     return 0
 
@@ -738,7 +738,7 @@ def string_to_datetime(ndarray[object] strings, raise_=False, dayfirst=False):
     from dateutil.parser import parse
 
     try:
-        result = np.empty(n, dtype='M8[us]')
+        result = np.empty(n, dtype='M8[ns]')
         iresult = result.view('i8')
         for i in range(n):
             val = strings[i]
@@ -903,7 +903,7 @@ def _get_transitions(tz):
     Get UTC times of DST transitions
     """
     if tz not in trans_cache:
-        arr = np.array(tz._utc_transition_times, dtype='M8[us]')
+        arr = np.array(tz._utc_transition_times, dtype='M8[ns]')
         trans_cache[tz] = arr.view('i8')
     return trans_cache[tz]
 
@@ -1009,7 +1009,7 @@ def build_field_sarray(ndarray[int64_t] dtindex):
     mus = out['u']
 
     for i in range(count):
-        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
         years[i] = dts.year
         months[i] = dts.month
         days[i] = dts.day
@@ -1044,49 +1044,49 @@ def fast_field_accessor(ndarray[int64_t] dtindex, object field):
 
     if field == 'Y':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.year
         return out
 
     elif field == 'M':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.month
         return out
 
     elif field == 'D':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.day
         return out
 
     elif field == 'h':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.hour
         return out
 
     elif field == 'm':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.min
         return out
 
     elif field == 's':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.sec
         return out
 
     elif field == 'us':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.us
         return out
 
     elif field == 'doy':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             isleap = is_leapyear(dts.year)
             out[i] = _month_offset[isleap, dts.month-1] + dts.day
         return out
@@ -1099,7 +1099,7 @@ def fast_field_accessor(ndarray[int64_t] dtindex, object field):
 
     elif field == 'woy':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             isleap = is_leapyear(dts.year)
             out[i] = _month_offset[isleap, dts.month - 1] + dts.day
             out[i] = ((out[i] - 1) / 7) + 1
@@ -1107,7 +1107,7 @@ def fast_field_accessor(ndarray[int64_t] dtindex, object field):
 
     elif field == 'q':
         for i in range(count):
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_us, &dts)
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
             out[i] = dts.month
             out[i] = ((out[i] - 1) / 3) + 1
         return out
@@ -1119,7 +1119,7 @@ cdef inline int m8_weekday(int64_t val):
     ts = convert_to_tsobject(val)
     return ts_dayofweek(ts)
 
-cdef int64_t DAY_US = 86400000000LL
+cdef int64_t DAY_NS = 86400000000000LL
 
 def values_at_time(ndarray[int64_t] stamps, int64_t time):
     cdef:
@@ -1133,18 +1133,14 @@ def values_at_time(ndarray[int64_t] stamps, int64_t time):
         return np.empty(0, dtype=np.int64)
 
     # is this OK?
-    # days = stamps // DAY_US
-    times = stamps % DAY_US
+    # days = stamps // DAY_NS
+    times = stamps % DAY_NS
 
-    # Microsecond resolution
+    # Nanosecond resolution
     count = 0
     for i in range(1, n):
         if times[i] == time:
             count += 1
-        # cur = days[i]
-        # if cur > last:
-        #     count += 1
-        #     last = cur
 
     indexer = np.empty(count, dtype=np.int64)
 
@@ -1155,11 +1151,6 @@ def values_at_time(ndarray[int64_t] stamps, int64_t time):
             indexer[j] = i
             j += 1
 
-        # cur = days[i]
-        # if cur > last:
-        #     j += 1
-        #     last = cur
-
     return indexer
 
 
@@ -1170,12 +1161,12 @@ def date_normalize(ndarray[int64_t] stamps):
         pandas_datetimestruct dts
 
     for i in range(n):
-        pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_us, &dts)
+        pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
         dts.hour = 0
         dts.min = 0
         dts.sec = 0
         dts.us = 0
-        result[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_us, &dts)
+        result[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
 
     return result
 
@@ -1185,7 +1176,7 @@ def dates_normalized(ndarray[int64_t] stamps):
         pandas_datetimestruct dts
 
     for i in range(n):
-        pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_us, &dts)
+        pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
         if (dts.hour + dts.min + dts.sec + dts.us) > 0:
             return False
 
@@ -1250,7 +1241,7 @@ def dt64arr_to_periodarr(ndarray[int64_t] dtarr, int freq):
     out = np.empty(l, dtype='i8')
 
     for i in range(l):
-        pandas_datetime_to_datetimestruct(dtarr[i], PANDAS_FR_us, &dts)
+        pandas_datetime_to_datetimestruct(dtarr[i], PANDAS_FR_ns, &dts)
         out[i] = get_period_ordinal(dts.year, dts.month, dts.day,
                                   dts.hour, dts.min, dts.sec, freq)
     return out
@@ -1349,7 +1340,7 @@ cpdef int64_t period_ordinal_to_dt64(int64_t ordinal, int freq):
     dts.sec = int(dinfo.second)
     dts.us = 0
 
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_us, &dts)
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
 
 def period_ordinal_to_string(int64_t value, int freq):
     cdef:
diff --git a/pandas/src/engines.pyx b/pandas/src/engines.pyx
index b465dc370..5c16ebb5f 100644
--- a/pandas/src/engines.pyx
+++ b/pandas/src/engines.pyx
@@ -415,20 +415,20 @@ cdef class DatetimeEngine(Int64Engine):
 
     def get_indexer(self, values):
         self._ensure_mapping_populated()
-        if values.dtype != 'M8':
+        if values.dtype != 'M8[ns]':
             return np.repeat(-1, len(values)).astype('i4')
         values = np.asarray(values).view('i8')
         return self.mapping.lookup(values)
 
     def get_pad_indexer(self, other, limit=None):
-        if other.dtype != 'M8':
+        if other.dtype != 'M8[ns]':
             return np.repeat(-1, len(other)).astype('i4')
         other = np.asarray(other).view('i8')
         return _algos.pad_int64(self._get_index_values(), other,
                                 limit=limit)
 
     def get_backfill_indexer(self, other, limit=None):
-        if other.dtype != 'M8':
+        if other.dtype != 'M8[ns]':
             return np.repeat(-1, len(other)).astype('i4')
         other = np.asarray(other).view('i8')
         return _algos.backfill_int64(self._get_index_values(), other,
diff --git a/pandas/tests/test_tseries.py b/pandas/tests/test_tseries.py
index 318f78237..57f154384 100644
--- a/pandas/tests/test_tseries.py
+++ b/pandas/tests/test_tseries.py
@@ -197,6 +197,8 @@ def test_maybe_booleans_to_slice():
     result = lib.maybe_booleans_to_slice(arr)
     assert(result.dtype == np.bool_)
 
+    result = lib.maybe_booleans_to_slice(arr[:0])
+    assert(result == slice(0, 0))
 
 def test_convert_objects():
     arr = np.array(['a', 'b', nan, nan, 'd', 'e', 'f'], dtype='O')
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
index 701acfddf..8253ad4e1 100644
--- a/pandas/tools/tests/test_merge.py
+++ b/pandas/tools/tests/test_merge.py
@@ -1198,7 +1198,7 @@ class TestConcatenate(unittest.TestCase):
         result = concat(pieces, keys=[0, 1, 2])
         expected = ts.copy()
 
-        ts.index = DatetimeIndex(np.array(ts.index.values, dtype='M8[us]'))
+        ts.index = DatetimeIndex(np.array(ts.index.values, dtype='M8[ns]'))
 
         exp_labels = [np.repeat([0, 1, 2], [len(x) for x in pieces]),
                       np.arange(len(ts))]
diff --git a/pandas/tseries/frequencies.py b/pandas/tseries/frequencies.py
index fe198b101..4501e1d6a 100644
--- a/pandas/tseries/frequencies.py
+++ b/pandas/tseries/frequencies.py
@@ -696,6 +696,12 @@ def infer_freq(index, warn=True):
     inferer = _FrequencyInferer(index, warn=warn)
     return inferer.get_freq()
 
+_ONE_MICRO = 1000L
+_ONE_MILLI = _ONE_MICRO * 1000
+_ONE_SECOND = _ONE_MILLI * 1000
+_ONE_MINUTE = 60 * _ONE_SECOND
+_ONE_HOUR = 60 * _ONE_MINUTE
+_ONE_DAY = 24 * _ONE_HOUR
 
 class _FrequencyInferer(object):
     """
@@ -727,31 +733,34 @@ class _FrequencyInferer(object):
     def get_freq(self):
 
         delta = self.deltas[0]
-        if _is_multiple(delta, _day_us):
+        if _is_multiple(delta, _ONE_DAY):
             return self._infer_daily_rule()
         else:
             # Possibly intraday frequency
             if not self.is_unique:
                 return None
-            if _is_multiple(delta, 60 * 60 * 1000000):
+            if _is_multiple(delta, _ONE_HOUR):
                 # Hours
-                return _maybe_add_count('H', delta / (60 * 60 * 1000000))
-            elif _is_multiple(delta, 60 * 1000000):
+                return _maybe_add_count('H', delta / _ONE_HOUR)
+            elif _is_multiple(delta, _ONE_MINUTE):
                 # Minutes
-                return _maybe_add_count('T', delta / (60 * 1000000))
-            elif _is_multiple(delta, 1000000):
+                return _maybe_add_count('T', delta / _ONE_MINUTE)
+            elif _is_multiple(delta, _ONE_SECOND):
                 # Seconds
-                return _maybe_add_count('S', delta / 1000000)
-            elif _is_multiple(delta, 1000):
+                return _maybe_add_count('S', delta / _ONE_SECOND)
+            elif _is_multiple(delta, _ONE_MILLI):
                 # Milliseconds
-                return _maybe_add_count('L', delta / 1000)
-            else:
+                return _maybe_add_count('L', delta / _ONE_MILLI)
+            elif _is_multiple(delta, _ONE_MICRO):
                 # Microseconds
+                return _maybe_add_count('L', delta / _ONE_MICRO)
+            else:
+                # Nanoseconds
                 return _maybe_add_count('U', delta)
 
     @cache_readonly
     def day_deltas(self):
-        return [x / _day_us for x in self.deltas]
+        return [x / _ONE_DAY for x in self.deltas]
 
     @cache_readonly
     def fields(self):
@@ -828,7 +837,7 @@ class _FrequencyInferer(object):
             return monthly_rule
 
         if self.is_unique:
-            days = self.deltas[0] / _day_us
+            days = self.deltas[0] / _ONE_DAY
             if days % 7 == 0:
                 # Weekly
                 alias = _weekday_rule_aliases[self.rep_stamp.weekday()]
@@ -990,5 +999,3 @@ _month_aliases = dict((k + 1, v) for k, v in enumerate(MONTHS))
 
 def _is_multiple(us, mult):
     return us % mult == 0
-
-_day_us = 24 * 60 * 60 * 1000000
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 36814876f..4b3e63990 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -1,3 +1,5 @@
+# pylint: disable=E1101
+
 from datetime import time, datetime
 from datetime import timedelta
 
@@ -64,7 +66,7 @@ def _join_i8_wrapper(joinf, with_indexers=True):
         results = joinf(left, right)
         if with_indexers:
             join_index, left_indexer, right_indexer = results
-            join_index = join_index.view('M8')
+            join_index = join_index.view('M8[ns]')
             return join_index, left_indexer, right_indexer
         return results
     return wrapper
@@ -128,7 +130,6 @@ class DatetimeIndex(Int64Index):
     ----------
     data  : array-like (1-dimensional), optional
         Optional datetime-like data to construct index with
-    dtype : NumPy dtype (default: M8[us])
     copy  : bool
         Make a copy of input ndarray
     freq : string or pandas offset object, optional
@@ -169,7 +170,7 @@ class DatetimeIndex(Int64Index):
 
     def __new__(cls, data=None,
                 freq=None, start=None, end=None, periods=None,
-                dtype=None, copy=False, name=None, tz=None,
+                copy=False, name=None, tz=None,
                 verify_integrity=True, normalize=False, **kwds):
 
         warn = False
@@ -225,7 +226,7 @@ class DatetimeIndex(Int64Index):
             if lib.is_string_array(data):
                 data = _str_to_dt_array(data, offset)
             else:
-                data = np.asarray(data, dtype='M8[us]')
+                data = np.asarray(data, dtype='M8[ns]')
 
         if issubclass(data.dtype.type, basestring):
             subarr = _str_to_dt_array(data, offset)
@@ -235,11 +236,11 @@ class DatetimeIndex(Int64Index):
                 offset = data.offset
                 verify_integrity = False
             else:
-                subarr = np.array(data, dtype='M8[us]', copy=copy)
+                subarr = np.array(data, dtype='M8[ns]', copy=copy)
         elif issubclass(data.dtype.type, np.integer):
-            subarr = np.array(data, dtype='M8[us]', copy=copy)
+            subarr = np.array(data, dtype='M8[ns]', copy=copy)
         else:
-            subarr = np.array(data, dtype='M8[us]', copy=copy)
+            subarr = np.array(data, dtype='M8[ns]', copy=copy)
 
         if tz is not None:
             tz = tools._maybe_get_tz(tz)
@@ -247,7 +248,7 @@ class DatetimeIndex(Int64Index):
             ints = subarr.view('i8')
             lib.tz_localize_check(ints, tz)
             subarr = lib.tz_convert(ints, tz, _utc())
-            subarr = subarr.view('M8[us]')
+            subarr = subarr.view('M8[ns]')
 
         subarr = subarr.view(cls)
         subarr.name = name
@@ -312,7 +313,7 @@ class DatetimeIndex(Int64Index):
             ints = index.view('i8')
             lib.tz_localize_check(ints, tz)
             index = lib.tz_convert(ints, tz, _utc())
-            index = index.view('M8[us]')
+            index = index.view('M8[ns]')
 
         index = index.view(cls)
         index.name = name
@@ -354,7 +355,7 @@ class DatetimeIndex(Int64Index):
                                  end=_CACHE_END)
 
             arr = np.array(_to_m8_array(list(xdr)),
-                           dtype='M8[us]', copy=False)
+                           dtype='M8[ns]', copy=False)
 
             cachedRange = arr.view(DatetimeIndex)
             cachedRange.offset = offset
@@ -448,7 +449,7 @@ class DatetimeIndex(Int64Index):
             # extract the raw datetime data, turn into datetime64
             index_state = state[0]
             raw_data = index_state[0][4]
-            raw_data = np.array(raw_data, dtype='M8[us]')
+            raw_data = np.array(raw_data, dtype='M8[ns]')
             new_state = raw_data.__reduce__()
             np.ndarray.__setstate__(self, new_state[2])
         else:  # pragma: no cover
@@ -476,8 +477,8 @@ class DatetimeIndex(Int64Index):
 
     def _add_delta(self, delta):
         if isinstance(delta, (Tick, timedelta)):
-            inc = offsets._delta_to_microseconds(delta)
-            new_values = (self.asi8 + inc).view('M8[us]')
+            inc = offsets._delta_to_nanoseconds(delta)
+            new_values = (self.asi8 + inc).view('M8[ns]')
         else:
             new_values = self.astype('O') + delta
         return DatetimeIndex(new_values, tz=self.tz, freq='infer')
@@ -496,6 +497,13 @@ class DatetimeIndex(Int64Index):
 
         return result
 
+    def astype(self, dtype):
+        dtype = np.dtype(dtype)
+
+        if dtype == np.object_:
+            return self.asobject
+        return Index.astype(self, dtype)
+
     @property
     def asi8(self):
         # do not cache or you'll create a memory leak
@@ -545,7 +553,6 @@ class DatetimeIndex(Int64Index):
             return self._simple_new(sorted_values, self.name, None,
                                     self.tz)
 
-
     def snap(self, freq='S'):
         """
         Snap time stamps to nearest occuring frequency
@@ -554,7 +561,7 @@ class DatetimeIndex(Int64Index):
         # Superdumb, punting on any optimizing
         freq = to_offset(freq)
 
-        snapped = np.empty(len(self), dtype='M8[us]')
+        snapped = np.empty(len(self), dtype='M8[ns]')
 
         for i, v in enumerate(self):
             s = v
@@ -565,7 +572,7 @@ class DatetimeIndex(Int64Index):
                     s = t0
                 else:
                     s = t1
-            snapped[i] = np.datetime64(s)
+            snapped[i] = s
 
         # we know it conforms; skip check
         return DatetimeIndex(snapped, freq=freq, verify_integrity=False)
@@ -633,6 +640,12 @@ class DatetimeIndex(Int64Index):
         -------
         y : Index or DatetimeIndex
         """
+        if not isinstance(other, DatetimeIndex):
+            try:
+                other = DatetimeIndex(other)
+            except TypeError:
+                pass
+
         this, other = self._maybe_utc_convert(other)
 
         if this._can_fast_union(other):
@@ -879,8 +892,8 @@ class DatetimeIndex(Int64Index):
 
         # TODO: time object with tzinfo?
 
-        mus = _time_to_microsecond(key)
-        indexer = lib.values_at_time(self.asi8, mus)
+        nanos = _time_to_nanosecond(key)
+        indexer = lib.values_at_time(self.asi8, nanos)
         return com._ensure_platform_int(indexer)
 
     def _get_string_slice(self, key):
@@ -990,7 +1003,7 @@ class DatetimeIndex(Int64Index):
 
     def searchsorted(self, key, side='left'):
         if isinstance(key, np.ndarray):
-            key = np.array(key, dtype='M8[us]', copy=False)
+            key = np.array(key, dtype='M8[ns]', copy=False)
         else:
             key = _to_m8(key)
 
@@ -1015,7 +1028,7 @@ class DatetimeIndex(Int64Index):
 
     @property
     def dtype(self):
-        return np.dtype('M8')
+        return np.dtype('M8[ns]')
 
     @property
     def is_all_dates(self):
@@ -1107,7 +1120,7 @@ class DatetimeIndex(Int64Index):
 
         # Convert to UTC
         new_dates = lib.tz_convert(self.asi8, tz, _utc())
-        new_dates = new_dates.view('M8[us]')
+        new_dates = new_dates.view('M8[ns]')
         return self._simple_new(new_dates, self.name, self.offset, tz)
 
     def tz_validate(self):
@@ -1138,7 +1151,7 @@ def _generate_regular_range(start, end, periods, offset):
         raise ValueError('Must specify two of start, end, or periods')
 
     if isinstance(offset, Tick):
-        stride = offset.micros
+        stride = offset.nanos
         if periods is None:
             b = Timestamp(start).value
             e = Timestamp(end).value
@@ -1153,12 +1166,12 @@ def _generate_regular_range(start, end, periods, offset):
             raise NotImplementedError
 
         data = np.arange(b, e, stride, dtype=np.int64)
-        data = data.view('M8[us]')
+        data = data.view('M8[ns]')
     else:
         xdr = generate_range(start=start, end=end,
             periods=periods, offset=offset)
 
-        data = np.array(list(xdr), dtype='M8[us]')
+        data = np.array(list(xdr), dtype='M8[ns]')
 
     return data
 
@@ -1247,7 +1260,7 @@ def _str_to_dt_array(arr, offset=None):
 
     p_ufunc = np.frompyfunc(parser, 1, 1)
     data = p_ufunc(arr)
-    return np.array(data, dtype='M8[us]')
+    return np.array(data, dtype='M8[ns]')
 
 
 _CACHE_START = Timestamp(datetime(1950, 1, 1))
@@ -1265,6 +1278,6 @@ def _naive_in_cache_range(start, end):
 def _in_range(start, end, rng_start, rng_end):
     return start > rng_start and end < rng_end
 
-def _time_to_microsecond(time):
+def _time_to_nanosecond(time):
     seconds = time.hour * 60 * 60 + 60 * time.minute + time.second
-    return 1000000 * seconds + time.microsecond
+    return (1000000 * seconds + time.microsecond) * 1000
diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 98716ed1f..e9c2628f6 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -963,8 +963,8 @@ class Tick(DateOffset):
         return self._delta
 
     @property
-    def micros(self):
-        return _delta_to_microseconds(self.delta)
+    def nanos(self):
+        return _delta_to_nanoseconds(self.delta)
 
     def apply(self, other):
         if isinstance(other, (datetime, timedelta)):
@@ -990,18 +990,18 @@ def _delta_to_tick(delta):
             else:
                 return Second(seconds)
     else:
-        mus = _delta_to_microseconds(delta)
+        mus = _delta_to_nanoseconds(delta)
         if mus % 1000 == 0:
             return Milli(mus // 1000)
         else:
             return Micro(mus)
 
-def _delta_to_microseconds(delta):
+def _delta_to_nanoseconds(delta):
     if isinstance(delta, Tick):
         delta = delta.delta
     return (delta.days * 24 * 60 * 60 * 1000000
             + delta.seconds * 1000000
-            + delta.microseconds)
+            + delta.microseconds) * 1000
 
 class Day(Tick, CacheableOffset):
     _inc = timedelta(1)
diff --git a/pandas/tseries/resample.py b/pandas/tseries/resample.py
index 97025eafa..f1109dd52 100644
--- a/pandas/tseries/resample.py
+++ b/pandas/tseries/resample.py
@@ -237,7 +237,7 @@ def _make_period_bins(axis, freq, begin=None, end=None,
 
 def _get_range_edges(axis, begin, end, offset, closed='left',
                      base=0):
-    from pandas.tseries.offsets import Tick, _delta_to_microseconds
+    from pandas.tseries.offsets import Tick, _delta_to_nanoseconds
     if isinstance(offset, basestring):
         offset = to_offset(offset)
 
@@ -245,9 +245,9 @@ def _get_range_edges(axis, begin, end, offset, closed='left',
         raise ValueError("Rule not a recognized offset")
 
     if isinstance(offset, Tick):
-        day_micros = _delta_to_microseconds(timedelta(1))
+        day_nanos = _delta_to_nanoseconds(timedelta(1))
         # #1165
-        if ((day_micros % offset.micros) == 0 and begin is None
+        if ((day_nanos % offset.nanos) == 0 and begin is None
             and end is None):
             return _adjust_dates_anchored(axis[0], axis[-1], offset,
                                           closed=closed, base=base)
@@ -271,26 +271,26 @@ def _get_range_edges(axis, begin, end, offset, closed='left',
 def _adjust_dates_anchored(first, last, offset, closed='right', base=0):
     from pandas.tseries.tools import normalize_date
 
-    start_day_micros = Timestamp(normalize_date(first)).value
-    last_day_micros = Timestamp(normalize_date(last)).value
+    start_day_nanos = Timestamp(normalize_date(first)).value
+    last_day_nanos = Timestamp(normalize_date(last)).value
 
-    base_micros = (base % offset.n) * offset.micros // offset.n
-    start_day_micros += base_micros
-    last_day_micros += base_micros
+    base_nanos = (base % offset.n) * offset.nanos // offset.n
+    start_day_nanos += base_nanos
+    last_day_nanos += base_nanos
 
-    foffset = (first.value - start_day_micros) % offset.micros
-    loffset = (last.value - last_day_micros) % offset.micros
+    foffset = (first.value - start_day_nanos) % offset.nanos
+    loffset = (last.value - last_day_nanos) % offset.nanos
 
     if closed == 'right':
         if foffset > 0:
             # roll back
             fresult = first.value - foffset
         else:
-            fresult = first.value - offset.micros
+            fresult = first.value - offset.nanos
 
         if loffset > 0:
             # roll forward
-            lresult = last.value + (offset.micros - loffset)
+            lresult = last.value + (offset.nanos - loffset)
         else:
             # already the end of the road
             lresult = last.value
@@ -303,9 +303,9 @@ def _adjust_dates_anchored(first, last, offset, closed='right', base=0):
 
         if loffset > 0:
             # roll forward
-            lresult = last.value + (offset.micros - loffset)
+            lresult = last.value + (offset.nanos - loffset)
         else:
-            lresult = last.value + offset.micros
+            lresult = last.value + offset.nanos
 
     return Timestamp(fresult), Timestamp(lresult)
 
@@ -361,11 +361,11 @@ def values_at_time(obj, time, tz=None, asof=False):
 
     # TODO: time object with tzinfo?
 
-    mus = _time_to_microsecond(time)
+    mus = _time_to_nanosecond(time)
     indexer = lib.values_at_time(obj.index.asi8, mus)
     indexer = com._ensure_platform_int(indexer)
     return obj.take(indexer)
 
-def _time_to_microsecond(time):
+def _time_to_nanosecond(time):
     seconds = time.hour * 60 * 60 + 60 * time.minute + time.second
-    return 1000000 * seconds + time.microsecond
+    return 1000000000L * seconds + time.microsecond * 1000
diff --git a/pandas/tseries/tests/test_timeseries.py b/pandas/tseries/tests/test_timeseries.py
index c6f5c39cd..5fae73c72 100644
--- a/pandas/tseries/tests/test_timeseries.py
+++ b/pandas/tseries/tests/test_timeseries.py
@@ -54,7 +54,7 @@ class TestTimeSeriesDuplicates(unittest.TestCase):
 
     def test_index_unique(self):
         uniques = self.dups.index.unique()
-        self.assert_(uniques.dtype == 'M8') # sanity
+        self.assert_(uniques.dtype == 'M8[ns]') # sanity
 
     def test_duplicate_dates_indexing(self):
         ts = self.dups
@@ -310,7 +310,7 @@ class TestTimeSeries(unittest.TestCase):
         dates = np.asarray(rng)
 
         df = DataFrame({'A': np.random.randn(len(rng)), 'B': dates})
-        self.assert_(np.issubdtype(df['B'].dtype, np.datetime64))
+        self.assert_(np.issubdtype(df['B'].dtype, np.dtype('M8[ns]')))
 
     def test_frame_add_datetime64_column(self):
         rng = date_range('1/1/2000 00:00:00', '1/1/2000 1:59:50',
@@ -318,7 +318,7 @@ class TestTimeSeries(unittest.TestCase):
         df = DataFrame(index=np.arange(len(rng)))
 
         df['A'] = rng
-        self.assert_(np.issubdtype(df['A'].dtype, np.datetime64))
+        self.assert_(np.issubdtype(df['A'].dtype, np.dtype('M8[ns]')))
 
     def test_series_ctor_datetime64(self):
         rng = date_range('1/1/2000 00:00:00', '1/1/2000 1:59:50',
@@ -326,14 +326,14 @@ class TestTimeSeries(unittest.TestCase):
         dates = np.asarray(rng)
 
         series = Series(dates)
-        self.assert_(np.issubdtype(series.dtype, np.datetime64))
+        self.assert_(np.issubdtype(series.dtype, np.dtype('M8[ns]')))
 
     def test_reindex_series_add_nat(self):
         rng = date_range('1/1/2000 00:00:00', periods=10, freq='10s')
         series = Series(rng)
 
         result = series.reindex(range(15))
-        self.assert_(np.issubdtype(result.dtype, np.datetime64))
+        self.assert_(np.issubdtype(result.dtype, np.dtype('M8[ns]')))
 
         mask = result.isnull()
         self.assert_(mask[-5:].all())
@@ -344,14 +344,14 @@ class TestTimeSeries(unittest.TestCase):
         df = DataFrame({'A': np.random.randn(len(rng)), 'B': rng})
 
         result = df.reindex(range(15))
-        self.assert_(np.issubdtype(result['B'].dtype, np.datetime64))
+        self.assert_(np.issubdtype(result['B'].dtype, np.dtype('M8[ns]')))
 
         mask = com.isnull(result)['B']
         self.assert_(mask[-5:].all())
         self.assert_(not mask[:-5].any())
 
     def test_series_repr_nat(self):
-        series = Series([0, 1, 2, NaT], dtype='M8[us]')
+        series = Series([0, 1, 2, NaT], dtype='M8[ns]')
 
         result = repr(series)
         expected = ('0          1970-01-01 00:00:00\n'
@@ -361,20 +361,20 @@ class TestTimeSeries(unittest.TestCase):
         self.assertEquals(result, expected)
 
     def test_fillna_nat(self):
-        series = Series([0, 1, 2, NaT], dtype='M8[us]')
+        series = Series([0, 1, 2, NaT], dtype='M8[ns]')
 
         filled = series.fillna(method='pad')
-        filled2 = series.fillna(value=series[2])
+        filled2 = series.fillna(value=series.values[2])
 
         expected = series.copy()
-        expected[3] = expected[2]
+        expected.values[3] = expected.values[2]
 
         assert_series_equal(filled, expected)
         assert_series_equal(filled2, expected)
 
         df = DataFrame({'A': series})
         filled = df.fillna(method='pad')
-        filled2 = df.fillna(value=series[2])
+        filled2 = df.fillna(value=series.values[2])
         expected = DataFrame({'A': expected})
         assert_frame_equal(filled, expected)
         assert_frame_equal(filled2, expected)
@@ -387,7 +387,7 @@ class TestTimeSeries(unittest.TestCase):
         strings = np.array(['1/1/2000', '1/2/2000', np.nan,
                             '1/4/2000, 12:34:56'], dtype=object)
 
-        expected = np.empty(4, dtype='M8')
+        expected = np.empty(4, dtype='M8[ns]')
         for i, val in enumerate(strings):
             if com.isnull(val):
                 expected[i] = NaT
@@ -417,7 +417,7 @@ class TestTimeSeries(unittest.TestCase):
         result = to_datetime(series)
         dresult = to_datetime(dseries)
 
-        expected = Series(np.empty(5, dtype='M8[us]'), index=idx)
+        expected = Series(np.empty(5, dtype='M8[ns]'), index=idx)
         for i in range(5):
             x = series[i]
             if isnull(x):
@@ -659,6 +659,22 @@ class TestTimeSeries(unittest.TestCase):
         expected = rng.shift(-5)
         self.assert_(result.equals(expected))
 
+    def test_astype_object(self):
+        # NumPy 1.6.1 weak ns support
+        rng = date_range('1/1/2000', periods=20)
+
+        casted = rng.astype('O')
+        exp_values = list(rng)
+
+        self.assert_(np.array_equal(casted, exp_values))
+
+
+    def test_catch_infinite_loop(self):
+        offset = datetools.DateOffset(minute=5)
+        # blow up, don't loop forever
+        self.assertRaises(Exception, date_range, datetime(2011,11,11),
+                          datetime(2011,11,12), freq=offset)
+
 
 def _simple_ts(start, end, freq='D'):
     rng = date_range(start, end, freq=freq)
@@ -881,7 +897,7 @@ class TestLegacySupport(unittest.TestCase):
 
         offset = timedelta(2)
         values = np.array([snap + i * offset for i in range(n)],
-                          dtype='M8[us]')
+                          dtype='M8[ns]')
 
         self.assert_(np.array_equal(rng, values))
 
@@ -982,8 +998,7 @@ class TestDatetime64(unittest.TestCase):
         self.series = Series(rand(len(dti)), dti)
 
     def test_datetimeindex_accessors(self):
-        dti = DatetimeIndex(freq='Q-JAN', start=datetime(1997,12,31),
-                            periods=100)
+        dti = DatetimeIndex(freq='Q-JAN', start=datetime(1997,12,31), periods=100)
 
         self.assertEquals(dti.year[0], 1998)
         self.assertEquals(dti.month[0], 1)
@@ -1069,11 +1084,11 @@ class TestDatetime64(unittest.TestCase):
         idx4 = DatetimeIndex(arr)
 
         arr = np.array(['1/1/2005', '1/2/2005', '1/3/2005',
-                        '2005-01-04'], dtype='M8[us]')
+                        '2005-01-04'], dtype='M8[ns]')
         idx5 = DatetimeIndex(arr)
 
         arr = np.array(['1/1/2005', '1/2/2005', 'Jan 3, 2005',
-                        '2005-01-04'], dtype='M8[us]')
+                        '2005-01-04'], dtype='M8[ns]')
         idx6 = DatetimeIndex(arr)
 
         for other in [idx2, idx3, idx4, idx5, idx6]:
@@ -1116,7 +1131,7 @@ class TestDatetime64(unittest.TestCase):
         dti = DatetimeIndex(start='1/1/2001', end='6/1/2001', freq='D')
         d1 = DataFrame({'v' : np.random.rand(len(dti))}, index=dti)
         d2 = d1.reset_index()
-        self.assert_(d2.dtypes[0] == np.datetime64)
+        self.assert_(d2.dtypes[0] == np.dtype('M8[ns]'))
         d3 = d2.set_index('index')
         assert_frame_equal(d1, d3)
 
@@ -1134,6 +1149,27 @@ class TestDatetime64(unittest.TestCase):
     # TODO: test merge & concat with datetime64 block
 
 
+class TestTimestamp(unittest.TestCase):
+
+    def test_basics_nanos(self):
+        arr = np.array(['1/1/2000'], dtype='M8[ns]')
+        stamp = Timestamp(arr[0].view('i8') + 500)
+        self.assert_(stamp.year == 2000)
+        self.assert_(stamp.month == 1)
+        self.assert_(stamp.microsecond == 0)
+        self.assert_(stamp.nanosecond == 500)
+
+    def test_comparison(self):
+        arr = np.array(['1/1/2000'], dtype='M8[ns]')
+
+        x = Timestamp(arr[0].view('i8') + 500)
+        y = Timestamp(arr[0].view('i8'))
+
+        self.assert_(arr[0].astype('O') == x)
+        self.assert_(x != y)
+
+"""
+
 class TestNewOffsets(unittest.TestCase):
 
     def test_yearoffset(self):
@@ -1326,13 +1362,7 @@ class TestNewOffsets(unittest.TestCase):
                     self.assert_(t.weekday() == day)
 
 
-    def test_catch_infinite_loop(self):
-        offset = datetools.DateOffset(minute=5)
-        # blow up, don't loop forever
-        self.assertRaises(Exception, date_range, datetime(2011,11,11),
-                          datetime(2011,11,12), freq=offset)
-
-
+"""
 
 if __name__ == '__main__':
     nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
diff --git a/vb_suite/sparse.py b/vb_suite/sparse.py
index 3c068e743..18cd71fb4 100644
--- a/vb_suite/sparse.py
+++ b/vb_suite/sparse.py
@@ -14,7 +14,7 @@ N = 50000
 rng = np.asarray(DateRange('1/1/2000', periods=N,
                            offset=datetools.Minute()))
 
-# rng2 = np.asarray(rng).astype('M8[us]').astype('i8')
+# rng2 = np.asarray(rng).astype('M8[ns]').astype('i8')
 
 series = {}
 for i in range(1, K + 1):
