commit 51cbf8bb928bf7bb0d798f374cbf3080b23cf9da
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Tue Oct 2 17:17:07 2012 -0400

    BUG: tokenization error reporting, work on parameter hell simplification

diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index 65ee3872f..8775968a2 100644
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -182,9 +182,8 @@ def _read(filepath_or_buffer, kwds):
             kwds['parse_dates'] = True
 
     # Extract some of the arguments (pass chunksize on).
-    kwds.pop('filepath_or_buffer')
-    iterator = kwds.pop('iterator')
-    nrows = kwds.pop('nrows')
+    iterator = kwds.pop('iterator', False)
+    nrows = kwds.pop('nrows', None)
     chunksize = kwds.get('chunksize', None)
 
     # Create the parser.
@@ -197,162 +196,147 @@ def _read(filepath_or_buffer, kwds):
 
     return parser.read()
 
-@Appender(_read_csv_doc)
-def read_csv(filepath_or_buffer,
-             sep=',',
-             engine='c',
-             as_recarray=False,
-             dialect=None,
-             header=0,
-             index_col=None,
-             names=None,
-             skiprows=None,
-             na_values=None,
-             na_filter=True,
-             compact_ints=False,
-             use_unsigned=False,
-             keep_default_na=True,
-             thousands=None,
-             comment=None,
-             parse_dates=False,
-             keep_date_col=False,
-             dayfirst=False,
-             date_parser=None,
-             nrows=None,
-             iterator=False,
-             chunksize=None,
-             skip_footer=0,
-             converters=None,
-             verbose=False,
-             delimiter=None,
-             encoding=None,
-             squeeze=False):
-    kwds = dict(filepath_or_buffer=filepath_or_buffer,
-                sep=sep, dialect=dialect,
-                header=header, index_col=index_col,
-                names=names, skiprows=skiprows,
-                na_values=na_values,
-                na_filter=na_filter,
-                compact_ints=compact_ints, use_unsigned=use_unsigned,
-                keep_default_na=keep_default_na,
-                thousands=thousands,
-                comment=comment, parse_dates=parse_dates,
-                keep_date_col=keep_date_col,
-                dayfirst=dayfirst, date_parser=date_parser,
-                nrows=nrows, iterator=iterator,
-                chunksize=chunksize, skip_footer=skip_footer,
-                converters=converters, verbose=verbose,
-                delimiter=delimiter, encoding=encoding,
-                squeeze=squeeze,
-                engine=engine, as_recarray=as_recarray)
-
-    # Alias sep -> delimiter.
-    sep = kwds.pop('sep')
-    if kwds.get('delimiter', None) is None:
-        kwds['delimiter'] = sep
+_parser_defaults = {
+    'engine': 'c',
+    'sep': ',',
+    'dialect': None,
+    'header': 0,
+    'index_col': None,
+    'names': None,
+    'skiprows': None,
+    'na_values': None,
+    'delimiter': None,
+    'skip_footer': 0,
+    'converters': None,
+
+    'keep_default_na': True,
+    'thousands': None,
+    'comment': None,
+
+    'parse_dates': False,
+    'keep_date_col': False,
+    'dayfirst': False,
+    'date_parser': None,
+
+    'nrows': None,
+    'iterator': False,
+    'chunksize': None,
+    'verbose': False,
+    'encoding': None,
+    'squeeze': False
+}
+
+_c_parser_defaults = {
+    'delim_whitespace': False,
+    'as_recarray': False,
+    'na_filter': True,
+    'compact_ints': False,
+    'use_unsigned': False,
+    'low_memory': True,
+    'buffer_lines': 2**14,
+    'error_bad_lines': True,
+    'warn_bad_lines': True
+}
+
+_c_unsupported = set(['comment', 'skip_footer', 'encoding'])
+
+def _make_parser_function(name, sep=','):
+
+    def parser_f(filepath_or_buffer,
+                 sep=sep,
+                 dialect=None,
+                 header=0,
+                 index_col=None,
+                 names=None,
+                 skiprows=None,
+                 na_values=None,
+                 delimiter=None,
+                 skip_footer=0,
+                 converters=None,
 
-    return _read(filepath_or_buffer, kwds)
+                 engine='c',
+                 delim_whitespace=False,
+                 as_recarray=False,
+                 na_filter=True,
+                 compact_ints=False,
+                 use_unsigned=False,
+                 low_memory=False,
+                 buffer_lines=2**14,
+                 warn_bad_lines=True,
+                 error_bad_lines=True,
 
-@Appender(_read_table_doc)
-def read_table(filepath_or_buffer,
-               sep='\t',
-               engine='c',
-               dialect=None,
-               header=0,
-               index_col=None,
-               names=None,
-               skiprows=None,
-               na_values=None,
-               na_filter=True,
-               keep_default_na=True,
-               thousands=None,
-               comment=None,
-               parse_dates=False,
-               keep_date_col=False,
-               dayfirst=False,
-               date_parser=None,
-               nrows=None,
-               iterator=False,
-               chunksize=None,
-               skip_footer=0,
-               converters=None,
-               verbose=False,
-               delimiter=None,
-               encoding=None,
-               squeeze=False):
-    kwds = dict(filepath_or_buffer=filepath_or_buffer,
-                sep=sep, dialect=dialect,
-                header=header, index_col=index_col,
-                names=names, skiprows=skiprows,
-                na_values=na_values,
-                na_filter=na_filter,
-                keep_default_na=keep_default_na,
-                thousands=thousands,
-                comment=comment, parse_dates=parse_dates,
-                keep_date_col=keep_date_col,
-                dayfirst=dayfirst, date_parser=date_parser,
-                nrows=nrows, iterator=iterator,
-                chunksize=chunksize, skip_footer=skip_footer,
-                converters=converters, verbose=verbose,
-                delimiter=delimiter, encoding=encoding,
-                squeeze=squeeze)
-
-    # Alias sep -> delimiter.
-    sep = kwds.pop('sep')
-    if kwds.get('delimiter', None) is None:
-        kwds['delimiter'] = sep
-
-    # Override as default encoding.
-    kwds['encoding'] = None
+                 keep_default_na=True,
+                 thousands=None,
+                 comment=None,
 
-    return _read(filepath_or_buffer, kwds)
+                 parse_dates=False,
+                 keep_date_col=False,
+                 dayfirst=False,
+                 date_parser=None,
 
-@Appender(_read_fwf_doc)
-def read_fwf(filepath_or_buffer,
-             colspecs=None,
-             widths=None,
-             header=0,
-             index_col=None,
-             names=None,
-             skiprows=None,
-             na_values=None,
-             na_filter=True,
-             keep_default_na=True,
-             thousands=None,
-             comment=None,
-             parse_dates=False,
-             keep_date_col=False,
-             dayfirst=False,
-             date_parser=None,
-             nrows=None,
-             iterator=False,
-             chunksize=None,
-             skip_footer=0,
-             converters=None,
-             delimiter=None,
-             verbose=False,
-             encoding=None,
-             squeeze=False):
-    kwds = dict(filepath_or_buffer=filepath_or_buffer,
-                colspecs=colspecs, widths=widths,
-                header=header, index_col=index_col,
-                names=names, skiprows=skiprows,
-                na_values=na_values,
-                na_filter=na_filter,
-                keep_default_na=keep_default_na,
-                thousands=thousands,
-                comment=comment, parse_dates=parse_dates,
-                keep_date_col=keep_date_col,
-                dayfirst=dayfirst, date_parser=date_parser,
-                nrows=nrows, iterator=iterator,
-                chunksize=chunksize, skip_footer=skip_footer,
-                converters=converters, verbose=verbose,
-                delimiter=delimiter, encoding=encoding,
-                squeeze=squeeze)
+                 nrows=None,
+                 iterator=False,
+                 chunksize=None,
 
+                 verbose=False,
+                 encoding=None,
+                 squeeze=False):
+        kwds = dict(sep=sep,
+                    delimiter=delimiter,
+                    engine=engine,
+                    dialect=dialect,
+                    header=header,
+                    index_col=index_col,
+                    names=names,
+                    skiprows=skiprows,
+                    na_values=na_values,
+                    keep_default_na=keep_default_na,
+                    thousands=thousands,
+                    comment=comment,
+
+                    parse_dates=parse_dates,
+                    keep_date_col=keep_date_col,
+                    dayfirst=dayfirst,
+                    date_parser=date_parser,
+
+                    nrows=nrows,
+                    iterator=iterator,
+                    chunksize=chunksize,
+                    skip_footer=skip_footer,
+                    converters=converters,
+                    verbose=verbose,
+                    encoding=encoding,
+                    squeeze=squeeze,
+
+                    na_filter=na_filter,
+                    compact_ints=compact_ints,
+                    use_unsigned=use_unsigned,
+                    delim_whitespace=delim_whitespace,
+                    as_recarray=as_recarray,
+                    warn_bad_lines=warn_bad_lines,
+                    error_bad_lines=error_bad_lines)
+
+        # Alias sep -> delimiter.
+        sep = kwds.pop('sep')
+        if kwds.get('delimiter', None) is None:
+            kwds['delimiter'] = sep
+
+        return _read(filepath_or_buffer, kwds)
+
+    parser_f.__name__ = name
+
+    return parser_f
+
+read_csv = _make_parser_function('read_csv', sep=',')
+read_csv = Appender(_read_csv_doc)(read_csv)
+
+read_table = _make_parser_function('read_table', sep='\t')
+read_table = Appender(_read_table_doc)(read_table)
+
+
+@Appender(_read_fwf_doc)
+def read_fwf(filepath_or_buffer, colspecs=None, widths=None, **kwds):
     # Check input arguments.
-    colspecs = kwds.get('colspecs', None)
-    widths = kwds.pop('widths', None)
     if bool(colspecs is None) == bool(widths is None):
         raise ValueError("You must specify only one of 'widths' and "
                          "'colspecs'")
@@ -363,13 +347,12 @@ def read_fwf(filepath_or_buffer,
         for w in widths:
             colspecs.append( (col, col+w) )
             col += w
-        kwds['colspecs'] = colspecs
 
-    kwds['thousands'] = thousands
+    kwds['colspecs'] = colspecs
     kwds['engine'] = 'python-fwf'
-
     return _read(filepath_or_buffer, kwds)
 
+
 def read_clipboard(**kwargs):  # pragma: no cover
     """
     Read text from clipboard and pass to read_table. See read_table for the
@@ -416,41 +399,50 @@ class TextFileReader(object):
     def __init__(self, f,
                  engine='python',
                  delimiter=None,
-                 delim_whitespace=False,
                  dialect=None,
-                 names=None, header=0, index_col=None,
+                 names=None,
+                 header=0,
+                 index_col=None,
                  thousands=None,
                  comment=None,
-                 parse_dates=False, keep_date_col=False,
-                 date_parser=None, dayfirst=False,
+
                  chunksize=None,
-                 skiprows=None, skip_footer=0,
+                 skiprows=None,
+                 skip_footer=0,
                  converters=None,
                  verbose=False,
                  encoding=None,
-                 na_filter=True, na_values=None, keep_default_na=True,
-                 warn_bad_lines=True, error_bad_lines=True,
+                 na_values=None,
+                 keep_default_na=True,
+
+                 parse_dates=False,
+                 keep_date_col=False,
+                 date_parser=None,
+                 dayfirst=False,
+
                  doublequote=True,
                  escapechar=None,
                  quotechar='"',
                  quoting=csv.QUOTE_MINIMAL,
                  skipinitialspace=False,
+
                  squeeze=False,
+
+                 delim_whitespace=False,
+                 na_filter=True,
+                 warn_bad_lines=True,
+                 error_bad_lines=True,
                  as_recarray=False,
                  compact_ints=False,
                  use_unsigned=False,
                  factorize=True,
+
                  **kwds):
 
         # Tokenization options
         self.delimiter = delimiter
         self.delim_whitespace = delim_whitespace
 
-        if delimiter is None and not delim_whitespace:
-            if engine == 'c':
-                print 'Using Python parser to sniff delimiter'
-                engine = 'python'
-
         self.doublequote = doublequote
         self.escapechar = escapechar
         self.skipinitialspace = skipinitialspace
@@ -467,7 +459,6 @@ class TextFileReader(object):
             if not isinstance(index_col, (list, tuple, np.ndarray)):
                 index_col = [index_col]
         self.index_col = index_col
-
         self.names = list(names) if names is not None else names
 
         # type conversion-related
@@ -476,7 +467,6 @@ class TextFileReader(object):
             self.converters = converters
         else:
             self.converters = {}
-
         self.thousands = thousands
 
         self.parse_dates = parse_dates
@@ -492,10 +482,6 @@ class TextFileReader(object):
 
         self.skip_footer = skip_footer
 
-        # C engine not supported yet
-        if skip_footer > 0 and engine == 'c':
-            engine = 'python'
-
         if com.is_integer(skiprows):
             skiprows = range(skiprows)
         self.skiprows = set() if skiprows is None else set(skiprows)
@@ -518,15 +504,8 @@ class TextFileReader(object):
         # file options
         self.f = f
         self.encoding = encoding
-
-        # can't handle it
-        if encoding is not None and engine == 'c':
-            engine = 'python'
-
         self.engine_kind = engine
-
         self.factorize = factorize
-
         self.chunksize = chunksize
 
         self._engine = None
@@ -565,6 +544,23 @@ class TextFileReader(object):
         return na_values
 
     def _make_engine(self, kind='c'):
+
+        if self.delimiter is None and not self.delim_whitespace:
+            if kind == 'c':
+                print 'Using Python parser to sniff delimiter'
+                kind = 'python'
+        elif len(self.delimiter) > 1:
+            # wait until regex engine integrated
+            kind = 'python'
+
+        # can't handle it
+        if self.encoding is not None and kind == 'c':
+            kind = 'python'
+
+        # C engine not supported yet
+        if self.skip_footer > 0 and kind == 'c':
+            kind = 'python'
+
         params = dict(delimiter=self.delimiter,
                       names=self.names,
                       index_col=self.index_col,
@@ -1315,8 +1311,8 @@ class PythonParser(ParserBase):
 
             row_num = self.pos - (len(content) - i + footers)
 
-            msg = ('Expecting %d columns, got %d in row %d' %
-                   (col_len, zip_len, row_num))
+            msg = ('Expected %d fields in line %d, saw %d' %
+                   (col_len, row_num + 1, zip_len))
             raise ValueError(msg)
 
         return zipped_content
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index f36f048ab..80ea11c7a 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -359,8 +359,8 @@ A,B,C
         try:
             df = self.read_table(StringIO(data), sep=',', header=1, comment='#')
             self.assert_(False)
-        except ValueError, inst:
-            self.assert_('Expecting 3 columns, got 5 in row 3' in str(inst))
+        except Exception, inst:
+            self.assert_('Expected 3 fields in line 4, saw 5' in str(inst))
 
         #skip_footer
         data = """ignore
@@ -376,7 +376,7 @@ footer
                             skip_footer=1)
             self.assert_(False)
         except ValueError, inst:
-            self.assert_('Expecting 3 columns, got 5 in row 3' in str(inst))
+            self.assert_('Expected 3 fields in line 4, saw 5' in str(inst))
 
         # first chunk
         data = """ignore
@@ -393,8 +393,8 @@ skip
                             skiprows=[2])
             df = it.read(5)
             self.assert_(False)
-        except ValueError, inst:
-            self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
+        except Exception, inst:
+            self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
 
         # middle chunk
@@ -414,7 +414,7 @@ skip
             it.read(2)
             self.assert_(False)
         except ValueError, inst:
-            self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
+            self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
 
         # last chunk
@@ -434,7 +434,7 @@ skip
             it.read()
             self.assert_(False)
         except ValueError, inst:
-            self.assert_('Expecting 3 columns, got 5 in row 5' in str(inst))
+            self.assert_('Expected 3 fields in line 6, saw 5' in str(inst))
 
     def test_quoting(self):
         bad_line_small = """printer\tresult\tvariant_name
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index 4f34ccf92..d6e581fb1 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -414,14 +414,14 @@ cdef class TextReader:
         cdef:
             size_t i, start, data_line, field_count, passed_count
             char *word
+            object name
+            int status
 
         if self.parser.header >= 0:
             # Header is in the file
 
             if self.parser.lines < self.parser.header + 1:
-                # print 'tokenizing %d rows' % (self.parser.header + 2)
-
-                tokenize_nrows(self.parser, self.parser.header + 2)
+                self._tokenize_rows(self.parser.header + 2)
 
             # e.g., if header=3 and file only has 2 lines
             if self.parser.lines < self.parser.header + 1:
@@ -433,23 +433,34 @@ cdef class TextReader:
 
             # TODO: Py3 vs. Py2
             header = []
+            counts = {}
             for i in range(field_count):
                 word = self.parser.words[start + i]
-                header.append(PyString_FromString(word))
+                name = PyString_FromString(word)
+
+                if name == '':
+                    name = 'Unnamed: %d' % i
+
+                count = counts.get(name, 0)
+                if count > 0:
+                    header.append('%s.%d' % (name, count))
+                else:
+                    header.append(name)
+                counts[name] = count + 1
 
             data_line = self.parser.header + 1
 
         elif self.names is not None:
             # Names passed
             if self.parser.lines < 1:
-                tokenize_nrows(self.parser, 1)
+                self._tokenize_rows(1)
 
             header = self.names
             data_line = 0
         else:
             # No header passed nor to be found in the file
             if self.parser.lines < 1:
-                tokenize_nrows(self.parser, 1)
+                self._tokenize_rows(1)
 
             return None, self.parser.line_fields[0]
 
@@ -522,9 +533,14 @@ cdef class TextReader:
                     break
 
         # destructive to chunks
-        result = _concatenate_chunks(chunks)
+        return _concatenate_chunks(chunks)
 
-        return result
+    cdef _tokenize_rows(self, size_t nrows):
+        cdef int status
+        with nogil:
+            status = tokenize_nrows(self.parser, nrows)
+        if status < 0:
+            raise_parser_error('Error tokenizing data', self.parser)
 
     cdef _read_high_memory(self, rows, bint trim):
         cdef:
@@ -537,27 +553,24 @@ cdef class TextReader:
             irows = rows
             buffered_lines = self.parser.lines - self.parser_start
             if buffered_lines < irows:
-                with nogil:
-                    status = tokenize_nrows(self.parser,
-                                            irows - buffered_lines)
+                self._tokenize_rows(irows - buffered_lines)
+
             if self.skip_footer > 0:
                 raise ValueError('skip_footer can only be used to read '
                                  'the whole file')
         else:
             with nogil:
                 status = tokenize_all_rows(self.parser)
+            if status < 0:
+                raise_parser_error('Error tokenizing data', self.parser)
             footer = self.skip_footer
 
         if self.parser_start == self.parser.lines:
             raise StopIteration
-
         self._end_clock('Tokenization')
 
-        if status < 0:
-            raise_parser_error('Error tokenizing data', self.parser)
 
         self._start_clock()
-
         columns = self._convert_column_data(rows=rows,
                                             footer=footer,
                                             upcast_na=not self.as_recarray)
@@ -1056,7 +1069,14 @@ def downcast_int64(ndarray[int64_t] arr, bint use_unsigned=0):
 
 
 def _concatenate_chunks(list chunks):
-    pass
+    cdef:
+        Py_ssize_t i, j, ncols = len(chunks[0])
+
+    result = {}
+    for i in range(ncols):
+        arrs = [chunk.pop(i) for chunk in chunks]
+        result[i] = np.concatenate(arrs)
+    return result
 
 #----------------------------------------------------------------------
 
diff --git a/pandas/src/parser/parser.c b/pandas/src/parser/parser.c
index 7fe6904d4..6925a8056 100644
--- a/pandas/src/parser/parser.c
+++ b/pandas/src/parser/parser.c
@@ -969,7 +969,7 @@ int inline end_field(parser_t *self) {
     // set pointer and metadata
     self->words[self->words_len] = self->pword_start;
 
-    TRACE(("Saw word at: %d\n", self->word_start))
+    TRACE(("Saw word %s at: %d\n", self->pword_start, self->word_start))
 
     self->word_starts[self->words_len] = self->word_start;
     self->words_len++;
@@ -1029,6 +1029,8 @@ int inline end_line(parser_t *self) {
             sprintf(self->error_msg, "Expected %d fields in line %d, saw %d\n",
                     ex_fields, self->file_lines, fields);
 
+            TRACE(("Error at line %d, %d fields\n", self->file_lines, fields));
+
             return -1;
         } else {
             // simply skip bad lines
@@ -1828,6 +1830,8 @@ int _tokenize_helper(parser_t *self, size_t nrows, int all) {
         return 0;
     }
 
+    TRACE(("Asked to tokenize %d rows\n", (int) nrows));
+
     while (1) {
         if (!all && self->lines - start_lines >= nrows)
             break;
@@ -1849,11 +1853,13 @@ int _tokenize_helper(parser_t *self, size_t nrows, int all) {
 
         if (status < 0) {
             // XXX
+            TRACE(("Status %d returned from tokenize_bytes, breaking\n",
+                   status));
             status = -1;
             break;
         }
     }
-
+    TRACE(("leaving tokenize_helper\n"));
     return status;
 }
 
