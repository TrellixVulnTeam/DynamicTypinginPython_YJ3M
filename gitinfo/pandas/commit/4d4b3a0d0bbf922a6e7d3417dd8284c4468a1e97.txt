commit 4d4b3a0d0bbf922a6e7d3417dd8284c4468a1e97
Author: Phillip Cloud <cpcloud@gmail.com>
Date:   Mon Oct 7 18:31:23 2013 -0400

    TST: only test Google Finance if there is at least one US English locale

diff --git a/.travis.yml b/.travis.yml
index 1402f1b48..818278eeb 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -10,9 +10,9 @@ matrix:
     - python: 2.7
       env: NOSE_ARGS="slow and not network" LOCALE_OVERRIDE="zh_CN.GB18030" FULL_DEPS=true JOB_TAG=_LOCALE
     - python: 2.7
-      env: NOSE_ARGS="not slow" FULL_DEPS=true GUI=gtk2
+      env: NOSE_ARGS="not slow" FULL_DEPS=true CLIPBOARD_GUI=gtk2
     - python: 3.2
-      env: NOSE_ARGS="not slow" FULL_DEPS=true GUI=qt4
+      env: NOSE_ARGS="not slow" FULL_DEPS=true CLIPBOARD_GUI=qt4
     - python: 3.3
       env: NOSE_ARGS="not slow" FULL_DEPS=true CLIPBOARD=xsel
   exclude:
@@ -25,28 +25,25 @@ virtualenv:
   system_site_packages: true
 
 before_install:
-  - echo "Waldo1"
+  - echo "before_install"
   - echo $VIRTUAL_ENV
   - df -h
   - date
-  # - export PIP_ARGS=-q # comment this this to debug travis install issues
-  # - export APT_ARGS=-qq # comment this to debug travis install issues
-  # - set -x # enable this to see bash commands
-  - export ZIP_FLAGS=-q # comment this to debug travis install issues
   - ci/before_install.sh
   - python -V
+  # Xvfb stuff for clipboard functionality; see the travis-ci documentation
   - export DISPLAY=:99.0
   - sh -e /etc/init.d/xvfb start
 
 install:
-  - echo "Waldo2"
+  - echo "install"
   - ci/install.sh
 
 before_script:
   - mysql -e 'create database pandas_nosetest;'
 
 script:
-  - echo "Waldo3"
+  - echo "script"
   - ci/script.sh
 
 after_script:
diff --git a/ci/install.sh b/ci/install.sh
index f6ad18a95..528d669ae 100755
--- a/ci/install.sh
+++ b/ci/install.sh
@@ -13,32 +13,37 @@
 #    (no compiling needed), then directly goto script and collect 200$.
 #
 
-function edit_init() {
+function edit_init()
+{
     if [ -n "$LOCALE_OVERRIDE" ]; then
-        pandas_dir=pandas
-        echo "Adding locale to the first line of $pandas_dir/__init__.py"
-        rm -f $pandas_dir/__init__.pyc
-        sedc="1iimport locale; locale.setlocale(locale.LC_ALL, '$LOCALE_OVERRIDE')"
-        sed -i "$sedc" $pandas_dir/__init__.py
-        echo "First line of $pandas_dir/__init__.py"
-        head $pandas_dir/__init__.py
+        echo "Adding locale to the first line of pandas/__init__.py"
+        rm -f pandas/__init__.pyc
+        sedc="3iimport locale\nlocale.setlocale(locale.LC_ALL, '$LOCALE_OVERRIDE')\n"
+        sed -i "$sedc" pandas/__init__.py
+        echo "head -4 pandas/__init__.py"
+        head -4 pandas/__init__.py
+        echo
     fi
 }
 
-echo "inside $0"
+edit_init
 
 # Install Dependencies
-# as of pip 1.4rc2, wheel files are still being broken regularly, this is a known good
-# commit. should revert to pypi when a final release is out
-pip install -I git+https://github.com/pypa/pip@42102e9deaea99db08b681d06906c2945f6f95e2#egg=pip
-pv="${TRAVIS_PYTHON_VERSION:0:1}"
-[ "$pv" == "2" ] && pv=""
+# as of pip 1.4rc2, wheel files are still being broken regularly, this is a
+# known good commit. should revert to pypi when a final release is out
+pip_commit=42102e9deaea99db08b681d06906c2945f6f95e2
+pip install -I git+https://github.com/pypa/pip@$pip_commit#egg=pip
+
+python_major_version="${TRAVIS_PYTHON_VERSION:0:1}"
+[ "$python_major_version" == "2" ] && python_major_version=""
 
 pip install -I -U setuptools
 pip install wheel
 
 # comment this line to disable the fetching of wheel files
-PIP_ARGS+=" -I --use-wheel --find-links=http://cache27diy-cpycloud.rhcloud.com/${TRAVIS_PYTHON_VERSION}${JOB_TAG}/"
+base_url=http://cache27diy-cpycloud.rhcloud.com
+wheel_box=${TRAVIS_PYTHON_VERSION}${JOB_TAG}
+PIP_ARGS+=" -I --use-wheel --find-links=$base_url/$wheel_box/"
 
 # Force virtualenv to accpet system_site_packages
 rm -f $VIRTUAL_ENV/lib/python$TRAVIS_PYTHON_VERSION/no-global-site-packages.txt
@@ -47,30 +52,37 @@ rm -f $VIRTUAL_ENV/lib/python$TRAVIS_PYTHON_VERSION/no-global-site-packages.txt
 if [ -n "$LOCALE_OVERRIDE" ]; then
     # make sure the locale is available
     # probably useless, since you would need to relogin
-    sudo locale-gen "$LOCALE_OVERRIDE"
+    time sudo locale-gen "$LOCALE_OVERRIDE"
 fi
 
-
 # show-skipped is working at this particular commit
-time pip install git+git://github.com/cpcloud/nose-show-skipped.git@fa4ff84e53c09247753a155b428c1bf2c69cb6c3
-time pip install $PIP_ARGS -r ci/requirements-${TRAVIS_PYTHON_VERSION}${JOB_TAG}.txt
-time sudo apt-get install libatlas-base-dev gfortran
+show_skipped_commit=fa4ff84e53c09247753a155b428c1bf2c69cb6c3
+time pip install git+git://github.com/cpcloud/nose-show-skipped.git@$show_skipped_commit
+time pip install $PIP_ARGS -r ci/requirements-${wheel_box}.txt
+
+# we need these for numpy
+time sudo apt-get $APT_ARGS install libatlas-base-dev gfortran
 
 
-# need to enable for locale testing
+# Need to enable for locale testing. The location of the locale file(s) is
+# distro specific. For example, on Arch Linux all of the locales are in a
+# commented file--/etc/locale.gen--that must be commented in to be used
+# whereas Ubuntu looks in /var/lib/locales/supported.d/* and generates locales
+# based on what's in the files in that folder
 time echo 'it_CH.UTF-8 UTF-8' | sudo tee -a /var/lib/locales/supported.d/it
 time sudo locale-gen
 
 
 # install gui for clipboard testing
-if [ -n "$GUI" ]; then
-    echo "Using GUI clipboard: $GUI"
-    [ -n "$pv" ] && py="py"
-    time sudo apt-get $APT_ARGS install python${pv}-${py}${GUI}
+if [ -n "$CLIPBOARD_GUI" ]; then
+    echo "Using CLIPBOARD_GUI: $CLIPBOARD_GUI"
+    [ -n "$python_major_version" ] && py="py"
+    python_cb_gui_pkg=python${python_major_version}-${py}${CLIPBOARD_GUI}
+    time sudo apt-get $APT_ARGS install $python_cb_gui_pkg
 fi
 
 
-# install a clipboard
+# install a clipboard if $CLIPBOARD is not empty
 if [ -n "$CLIPBOARD" ]; then
     echo "Using clipboard: $CLIPBOARD"
     time sudo apt-get $APT_ARGS install $CLIPBOARD
@@ -78,16 +90,15 @@ fi
 
 
 # Optional Deps
-if [ x"$FULL_DEPS" == x"true" ]; then
+if [ -n "$FULL_DEPS" ]; then
     echo "Installing FULL_DEPS"
-    # for pytables gets the lib as well
+
+    # need libhdf5 for PyTables
     time sudo apt-get $APT_ARGS install libhdf5-serial-dev
 fi
 
 
-edit_init
-
-# build pandas
+# build and install pandas
 time python setup.py build_ext install
 
 true
diff --git a/pandas/io/tests/test_data.py b/pandas/io/tests/test_data.py
index 02598ba05..4e2331f05 100644
--- a/pandas/io/tests/test_data.py
+++ b/pandas/io/tests/test_data.py
@@ -36,6 +36,15 @@ def assert_n_failed_equals_n_null_columns(wngs, obj, cls=SymbolWarning):
 
 
 class TestGoogle(unittest.TestCase):
+    @classmethod
+    def setUpClass(cls):
+        cls.locales = tm.get_locales(prefix='en_US')
+        if not cls.locales:
+            raise nose.SkipTest("US English locale not available for testing")
+
+    @classmethod
+    def tearDownClass(cls):
+        del cls.locales
 
     @network
     def test_google(self):
@@ -45,8 +54,9 @@ class TestGoogle(unittest.TestCase):
         start = datetime(2010, 1, 1)
         end = datetime(2013, 1, 27)
 
-        with tm.set_locale('en_US.UTF-8'):
-            panel = web.DataReader("F", 'google', start, end)
+        for locale in self.locales:
+            with tm.set_locale(locale):
+                panel = web.DataReader("F", 'google', start, end)
             self.assertEquals(panel.Close[-1], 13.68)
 
         self.assertRaises(Exception, web.DataReader, "NON EXISTENT TICKER",
@@ -59,41 +69,40 @@ class TestGoogle(unittest.TestCase):
 
     @network
     def test_get_goog_volume(self):
-        with tm.set_locale('en_US.UTF-8'):
-            df = web.get_data_google('GOOG').sort_index()
-        self.assertEqual(df.Volume.ix['OCT-08-2010'], 2863473)
+        for locale in self.locales:
+            with tm.set_locale(locale):
+                df = web.get_data_google('GOOG').sort_index()
+            self.assertEqual(df.Volume.ix['OCT-08-2010'], 2863473)
 
     @network
     def test_get_multi1(self):
-        sl = ['AAPL', 'AMZN', 'GOOG']
-        with tm.set_locale('en_US.UTF-8'):
-            pan = web.get_data_google(sl, '2012')
-
-        def testit():
+        for locale in self.locales:
+            sl = ['AAPL', 'AMZN', 'GOOG']
+            with tm.set_locale(locale):
+                pan = web.get_data_google(sl, '2012')
             ts = pan.Close.GOOG.index[pan.Close.AAPL > pan.Close.GOOG]
-            self.assertEquals(ts[0].dayofyear, 96)
-
-        if (hasattr(pan, 'Close') and hasattr(pan.Close, 'GOOG') and
-            hasattr(pan.Close, 'AAPL')):
-            testit()
-        else:
-            self.assertRaises(AttributeError, testit)
+            if (hasattr(pan, 'Close') and hasattr(pan.Close, 'GOOG') and
+                hasattr(pan.Close, 'AAPL')):
+                self.assertEquals(ts[0].dayofyear, 96)
+            else:
+                self.assertRaises(AttributeError, lambda: pan.Close)
 
     @network
     def test_get_multi2(self):
         with warnings.catch_warnings(record=True) as w:
-            with tm.set_locale('en_US.UTF-8'):
-                pan = web.get_data_google(['GE', 'MSFT', 'INTC'], 'JAN-01-12',
-                                        'JAN-31-12')
-            result = pan.Close.ix['01-18-12']
-            assert_n_failed_equals_n_null_columns(w, result)
-
-            # sanity checking
-
-            assert np.issubdtype(result.dtype, np.floating)
-            result = pan.Open.ix['Jan-15-12':'Jan-20-12']
-            self.assertEqual((4, 3), result.shape)
-            assert_n_failed_equals_n_null_columns(w, result)
+            for locale in self.locales:
+                with tm.set_locale(locale):
+                    pan = web.get_data_google(['GE', 'MSFT', 'INTC'],
+                                              'JAN-01-12', 'JAN-31-12')
+                result = pan.Close.ix['01-18-12']
+                assert_n_failed_equals_n_null_columns(w, result)
+
+                # sanity checking
+
+                assert np.issubdtype(result.dtype, np.floating)
+                result = pan.Open.ix['Jan-15-12':'Jan-20-12']
+                self.assertEqual((4, 3), result.shape)
+                assert_n_failed_equals_n_null_columns(w, result)
 
 
 class TestYahoo(unittest.TestCase):
diff --git a/pandas/io/tests/test_json/test_pandas.py b/pandas/io/tests/test_json/test_pandas.py
index 8c7d89641..6d392eb26 100644
--- a/pandas/io/tests/test_json/test_pandas.py
+++ b/pandas/io/tests/test_json/test_pandas.py
@@ -1,11 +1,9 @@
 # pylint: disable-msg=W0612,E1101
 from pandas.compat import range, lrange, StringIO
 from pandas import compat
-from pandas.io.common import URLError
 import os
 import unittest
 
-import nose
 import numpy as np
 
 from pandas import Series, DataFrame, DatetimeIndex, Timestamp
@@ -16,7 +14,6 @@ from pandas.util.testing import (assert_almost_equal, assert_frame_equal,
                                  assert_series_equal, network,
                                  ensure_clean, assert_index_equal)
 import pandas.util.testing as tm
-from numpy.testing.decorators import slow
 
 _seriesd = tm.getSeriesData()
 _tsd = tm.getTimeSeriesData()
@@ -53,17 +50,35 @@ class TestPandasContainer(unittest.TestCase):
         self.tsframe = _tsframe.copy()
         self.mixed_frame = _mixed_frame.copy()
 
+    def tearDown(self):
+        del self.dirpath
+
+        del self.ts
+
+        del self.series
+
+        del self.objSeries
+
+        del self.empty_series
+        del self.empty_frame
+
+        del self.frame
+        del self.frame2
+        del self.intframe
+        del self.tsframe
+        del self.mixed_frame
+
     def test_frame_double_encoded_labels(self):
         df = DataFrame([['a', 'b'], ['c', 'd']],
                        index=['index " 1', 'index / 2'],
                        columns=['a \\ b', 'y / z'])
 
-        assert_frame_equal(
-            df, read_json(df.to_json(orient='split'), orient='split'))
-        assert_frame_equal(
-            df, read_json(df.to_json(orient='columns'), orient='columns'))
-        assert_frame_equal(
-            df, read_json(df.to_json(orient='index'), orient='index'))
+        assert_frame_equal(df, read_json(df.to_json(orient='split'),
+                                         orient='split'))
+        assert_frame_equal(df, read_json(df.to_json(orient='columns'),
+                                         orient='columns'))
+        assert_frame_equal(df, read_json(df.to_json(orient='index'),
+                                         orient='index'))
         df_unser = read_json(df.to_json(orient='records'), orient='records')
         assert_index_equal(df.columns, df_unser.columns)
         np.testing.assert_equal(df.values, df_unser.values)
@@ -75,10 +90,10 @@ class TestPandasContainer(unittest.TestCase):
         self.assertRaises(ValueError, df.to_json, orient='index')
         self.assertRaises(ValueError, df.to_json, orient='columns')
 
-        assert_frame_equal(
-            df, read_json(df.to_json(orient='split'), orient='split'))
+        assert_frame_equal(df, read_json(df.to_json(orient='split'),
+                                         orient='split'))
         unser = read_json(df.to_json(orient='records'), orient='records')
-        self.assert_(df.columns.equals(unser.columns))
+        self.assertTrue(df.columns.equals(unser.columns))
         np.testing.assert_equal(df.values, unser.values)
         unser = read_json(df.to_json(orient='values'), orient='values')
         np.testing.assert_equal(df.values, unser.values)
@@ -102,7 +117,8 @@ class TestPandasContainer(unittest.TestCase):
         assert_frame_equal(result, df)
 
         def _check(df):
-            result = read_json(df.to_json(orient='split'), orient='split', convert_dates=['x'])
+            result = read_json(df.to_json(orient='split'), orient='split',
+                               convert_dates=['x'])
             assert_frame_equal(result, df)
 
         for o in [[['a','b'],['c','d']],
@@ -112,15 +128,15 @@ class TestPandasContainer(unittest.TestCase):
             _check(DataFrame(o, index=[1,2], columns=['x','x']))
 
     def test_frame_from_json_to_json(self):
-
-        def _check_orient(df, orient, dtype=None, numpy=False, convert_axes=True, check_dtype=True, raise_ok=None):
+        def _check_orient(df, orient, dtype=None, numpy=False,
+                          convert_axes=True, check_dtype=True, raise_ok=None):
             df = df.sort()
             dfjson = df.to_json(orient=orient)
 
             try:
                 unser = read_json(dfjson, orient=orient, dtype=dtype,
                                   numpy=numpy, convert_axes=convert_axes)
-            except (Exception) as detail:
+            except Exception as detail:
                 if raise_ok is not None:
                     if isinstance(detail, raise_ok):
                         return
@@ -151,7 +167,8 @@ class TestPandasContainer(unittest.TestCase):
                 if convert_axes:
                     assert_frame_equal(df, unser, check_dtype=check_dtype)
                 else:
-                    assert_frame_equal(df, unser, check_less_precise=False, check_dtype=check_dtype)
+                    assert_frame_equal(df, unser, check_less_precise=False,
+                                       check_dtype=check_dtype)
 
         def _check_all_orients(df, dtype=None, convert_axes=True, raise_ok=None):
 
@@ -171,17 +188,27 @@ class TestPandasContainer(unittest.TestCase):
 
             # numpy=True and raise_ok might be not None, so ignore the error
             if convert_axes:
-                _check_orient(df, "columns", dtype=dtype, numpy=True, raise_ok=raise_ok)
-                _check_orient(df, "records", dtype=dtype, numpy=True, raise_ok=raise_ok)
-                _check_orient(df, "split", dtype=dtype, numpy=True, raise_ok=raise_ok)
-                _check_orient(df, "index", dtype=dtype, numpy=True, raise_ok=raise_ok)
-                _check_orient(df, "values", dtype=dtype, numpy=True, raise_ok=raise_ok)
-
-            _check_orient(df, "columns", dtype=dtype, numpy=True, convert_axes=False, raise_ok=raise_ok)
-            _check_orient(df, "records", dtype=dtype, numpy=True, convert_axes=False, raise_ok=raise_ok)
-            _check_orient(df, "split", dtype=dtype, numpy=True, convert_axes=False, raise_ok=raise_ok)
-            _check_orient(df, "index", dtype=dtype, numpy=True, convert_axes=False, raise_ok=raise_ok)
-            _check_orient(df, "values", dtype=dtype, numpy=True, convert_axes=False, raise_ok=raise_ok)
+                _check_orient(df, "columns", dtype=dtype, numpy=True,
+                              raise_ok=raise_ok)
+                _check_orient(df, "records", dtype=dtype, numpy=True,
+                              raise_ok=raise_ok)
+                _check_orient(df, "split", dtype=dtype, numpy=True,
+                              raise_ok=raise_ok)
+                _check_orient(df, "index", dtype=dtype, numpy=True,
+                              raise_ok=raise_ok)
+                _check_orient(df, "values", dtype=dtype, numpy=True,
+                              raise_ok=raise_ok)
+
+            _check_orient(df, "columns", dtype=dtype, numpy=True,
+                          convert_axes=False, raise_ok=raise_ok)
+            _check_orient(df, "records", dtype=dtype, numpy=True,
+                          convert_axes=False, raise_ok=raise_ok)
+            _check_orient(df, "split", dtype=dtype, numpy=True,
+                          convert_axes=False, raise_ok=raise_ok)
+            _check_orient(df, "index", dtype=dtype, numpy=True,
+                          convert_axes=False, raise_ok=raise_ok)
+            _check_orient(df, "values", dtype=dtype, numpy=True,
+                          convert_axes=False, raise_ok=raise_ok)
 
         # basic
         _check_all_orients(self.frame)
@@ -202,9 +229,10 @@ class TestPandasContainer(unittest.TestCase):
         # dtypes
         _check_all_orients(DataFrame(biggie, dtype=np.float64),
                            dtype=np.float64, convert_axes=False)
-        _check_all_orients(DataFrame(biggie, dtype=np.int), dtype=np.int, convert_axes=False)
-        _check_all_orients(DataFrame(biggie, dtype='U3'), dtype='U3', convert_axes=False,
-                           raise_ok=ValueError)
+        _check_all_orients(DataFrame(biggie, dtype=np.int), dtype=np.int,
+                           convert_axes=False)
+        _check_all_orients(DataFrame(biggie, dtype='U3'), dtype='U3',
+                           convert_axes=False, raise_ok=ValueError)
 
         # empty
         _check_all_orients(self.empty_frame)
@@ -258,37 +286,37 @@ class TestPandasContainer(unittest.TestCase):
     def test_frame_from_json_nones(self):
         df = DataFrame([[1, 2], [4, 5, 6]])
         unser = read_json(df.to_json())
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
 
         df = DataFrame([['1', '2'], ['4', '5', '6']])
         unser = read_json(df.to_json())
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
         unser = read_json(df.to_json(),dtype=False)
-        self.assert_(unser[2][0] is None)
+        self.assertTrue(unser[2][0] is None)
         unser = read_json(df.to_json(),convert_axes=False,dtype=False)
-        self.assert_(unser['2']['0'] is None)
+        self.assertTrue(unser['2']['0'] is None)
 
         unser = read_json(df.to_json(), numpy=False)
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
         unser = read_json(df.to_json(), numpy=False, dtype=False)
-        self.assert_(unser[2][0] is None)
+        self.assertTrue(unser[2][0] is None)
         unser = read_json(df.to_json(), numpy=False, convert_axes=False, dtype=False)
-        self.assert_(unser['2']['0'] is None)
+        self.assertTrue(unser['2']['0'] is None)
 
         # infinities get mapped to nulls which get mapped to NaNs during
         # deserialisation
         df = DataFrame([[1, 2], [4, 5, 6]])
         df[2][0] = np.inf
         unser = read_json(df.to_json())
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
         unser = read_json(df.to_json(), dtype=False)
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
 
         df[2][0] = np.NINF
         unser = read_json(df.to_json())
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
         unser = read_json(df.to_json(),dtype=False)
-        self.assert_(np.isnan(unser[2][0]))
+        self.assertTrue(np.isnan(unser[2][0]))
 
     def test_frame_to_json_except(self):
         df = DataFrame([1, 2, 3])
@@ -345,7 +373,7 @@ class TestPandasContainer(unittest.TestCase):
                 except:
                     raise
                 if orient == "split":
-                    self.assert_(series.name == unser.name)
+                    self.assertEqual(series.name, unser.name)
 
         def _check_all_orients(series, dtype=None):
             _check_orient(series, "columns", dtype=dtype)
@@ -403,12 +431,12 @@ class TestPandasContainer(unittest.TestCase):
         result = read_json(df.to_json())
 
         # the index is serialized as strings....correct?
-        #assert_frame_equal(result,df)
+        assert_frame_equal(result, df)
 
     def test_path(self):
         with ensure_clean('test.json') as path:
-
-            for df in [ self.frame, self.frame2, self.intframe, self.tsframe, self.mixed_frame ]:
+            for df in [self.frame, self.frame2, self.intframe, self.tsframe,
+                       self.mixed_frame]:
                 df.to_json(path)
                 read_json(path)
 
@@ -512,7 +540,6 @@ class TestPandasContainer(unittest.TestCase):
             assert_frame_equal(result, df)
 
     def test_weird_nested_json(self):
-
         # this used to core dump the parser
         s = r'''{
         "status": "success",
@@ -528,9 +555,9 @@ class TestPandasContainer(unittest.TestCase):
             "title": "Another blog post",
             "body": "More content"
             }
-        ]
-    }
-}'''
+           ]
+          }
+        }'''
 
         read_json(s)
 
@@ -550,18 +577,19 @@ class TestPandasContainer(unittest.TestCase):
         # parsing unordered input fails
         result = read_json('[{"a": 1, "b": 2}, {"b":2, "a" :1}]',numpy=True)
         expected = DataFrame([[1,2],[1,2]],columns=['a','b'])
-        #assert_frame_equal(result,expected)
+        with tm.assertRaisesRegexp(AssertionError,
+                                   '\[index\] left \[.+\], right \[.+\]'):
+            assert_frame_equal(result, expected)
 
         result = read_json('[{"a": 1, "b": 2}, {"b":2, "a" :1}]')
         expected = DataFrame([[1,2],[1,2]],columns=['a','b'])
         assert_frame_equal(result,expected)
 
     @network
-    @slow
     def test_round_trip_exception_(self):
         # GH 3867
-
-        df = pd.read_csv('https://raw.github.com/hayd/lahman2012/master/csvs/Teams.csv')
+        csv = 'https://raw.github.com/hayd/lahman2012/master/csvs/Teams.csv'
+        df = pd.read_csv(csv)
         s = df.to_json()
         result = pd.read_json(s)
         assert_frame_equal(result.reindex(index=df.index,columns=df.columns),df)
@@ -569,12 +597,9 @@ class TestPandasContainer(unittest.TestCase):
     @network
     def test_url(self):
         url = 'https://api.github.com/repos/pydata/pandas/issues?per_page=5'
-        result = read_json(url,convert_dates=True)
-        for c in ['created_at','closed_at','updated_at']:
-            self.assert_(result[c].dtype == 'datetime64[ns]')
-
-        url = 'http://search.twitter.com/search.json?q=pandas%20python'
-        result = read_json(url)
+        result = read_json(url, convert_dates=True)
+        for c in ['created_at', 'closed_at', 'updated_at']:
+            self.assertEqual(result[c].dtype, 'datetime64[ns]')
 
     def test_default_handler(self):
         from datetime import timedelta
@@ -585,6 +610,6 @@ class TestPandasContainer(unittest.TestCase):
             expected, pd.read_json(frame.to_json(default_handler=str)))
 
         def my_handler_raises(obj):
-            raise TypeError
-        self.assertRaises(
-            TypeError, frame.to_json, default_handler=my_handler_raises)
+            raise TypeError("raisin")
+        self.assertRaises(TypeError, frame.to_json,
+                          default_handler=my_handler_raises)
diff --git a/pandas/io/tests/test_json/test_ujson.py b/pandas/io/tests/test_json/test_ujson.py
index 0b3bff7a1..06ff5abf7 100644
--- a/pandas/io/tests/test_json/test_ujson.py
+++ b/pandas/io/tests/test_json/test_ujson.py
@@ -32,6 +32,7 @@ def _skip_if_python_ver(skip_major, skip_minor=None):
     if major == skip_major and (skip_minor is None or minor == skip_minor):
         raise nose.SkipTest("skipping Python version %d.%d" % (major, minor))
 
+
 json_unicode = (json.dumps if sys.version_info[0] >= 3
                 else partial(json.dumps, encoding="utf-8"))
 
@@ -194,7 +195,6 @@ class UltraJSONTests(TestCase):
         # will throw typeError
         self.assertRaises(TypeError, ujson.encode, input, double_precision = None)
 
-
     def test_encodeStringConversion(self):
         input = "A string \\ / \b \f \n \r \t"
         output = ujson.encode(input)
@@ -220,7 +220,6 @@ class UltraJSONTests(TestCase):
         self.assertEquals(input, dec)
         self.assertEquals(enc, json_unicode(input))
 
-
     def test_encodeUnicodeConversion2(self):
         input = "\xe6\x97\xa5\xd1\x88"
         enc = ujson.encode(input)
@@ -259,7 +258,6 @@ class UltraJSONTests(TestCase):
         self.assertEquals(enc, json_unicode(input))
         self.assertEquals(dec, json.loads(enc))
 
-
     def test_encodeArrayInArray(self):
         input = [[[[]]]]
         output = ujson.encode(input)
@@ -286,7 +284,6 @@ class UltraJSONTests(TestCase):
         self.assertEquals(input, ujson.decode(output))
         pass
 
-
     def test_encodeLongNegConversion(self):
         input = -9223372036854775808
         output = ujson.encode(input)
@@ -448,7 +445,6 @@ class UltraJSONTests(TestCase):
         input = -np.inf
         assert ujson.encode(input) == 'null', "Expected null"
 
-
     def test_decodeJibberish(self):
         input = "fdsa sda v9sa fdsa"
         try:
@@ -566,7 +562,6 @@ class UltraJSONTests(TestCase):
             return
         assert False, "Wrong exception"
 
-
     def test_decodeBrokenDictKeyTypeLeakTest(self):
         input = '{{1337:""}}'
         for x in range(1000):
@@ -667,7 +662,6 @@ class UltraJSONTests(TestCase):
         input = "\"31337 \\u0000 31337\""
         self.assertEquals(ujson.decode(input), json.loads(input))
 
-
     def test_encodeListLongConversion(self):
         input = [9223372036854775807, 9223372036854775807, 9223372036854775807,
                  9223372036854775807, 9223372036854775807, 9223372036854775807 ]
@@ -1147,6 +1141,7 @@ class NumpyJSONTests(TestCase):
             self.assertTrue((np.array(['1','2','3']) == output[1]).all())
             self.assertTrue((np.array(['a', 'b']) == output[2]).all())
 
+
 class PandasJSONTests(TestCase):
 
     def testDataFrame(self):
@@ -1178,7 +1173,6 @@ class PandasJSONTests(TestCase):
         assert_array_equal(df.transpose().columns, outp.columns)
         assert_array_equal(df.transpose().index, outp.index)
 
-
     def testDataFrameNumpy(self):
         df = DataFrame([[1,2,3], [4,5,6]], index=['a', 'b'], columns=['x', 'y', 'z'])
 
@@ -1486,7 +1480,6 @@ class PandasJSONTests(TestCase):
         else:
             assert False, "expected ValueError"
 
-
     def test_decodeFloatingPointAdditionalTests(self):
         places = 15
 
@@ -1529,39 +1522,10 @@ class PandasJSONTests(TestCase):
             self.assertTrue(v in s)
 
 
-"""
-def test_decodeNumericIntFrcOverflow(self):
-input = "X.Y"
-raise NotImplementedError("Implement this test!")
-
-
-def test_decodeStringUnicodeEscape(self):
-input = "\u3131"
-raise NotImplementedError("Implement this test!")
-
-def test_decodeStringUnicodeBrokenEscape(self):
-input = "\u3131"
-raise NotImplementedError("Implement this test!")
-
-def test_decodeStringUnicodeInvalidEscape(self):
-input = "\u3131"
-raise NotImplementedError("Implement this test!")
-
-def test_decodeStringUTF8(self):
-input = "someutfcharacters"
-raise NotImplementedError("Implement this test!")
-
-
-
-"""
-
 def _clean_dict(d):
     return dict((str(k), v) for k, v in compat.iteritems(d))
 
+
 if __name__ == '__main__':
-    # unittest.main()
-    import nose
-    # nose.runmodule(argv=[__file__,'-vvs','-x', '--ipdb-failure'],
-    #                exit=False)
     nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
                    exit=False)
diff --git a/pandas/tools/tests/test_util.py b/pandas/tools/tests/test_util.py
index 99094d7f4..614f5ecc3 100644
--- a/pandas/tools/tests/test_util.py
+++ b/pandas/tools/tests/test_util.py
@@ -8,8 +8,8 @@ import nose
 import numpy as np
 from numpy.testing import assert_equal
 
+import pandas.util.testing as tm
 from pandas.tools.util import cartesian_product
-from pandas.util.testing import get_locales, set_locale
 
 
 CURRENT_LOCALE = locale.getlocale()
@@ -27,15 +27,35 @@ class TestCartesianProduct(unittest.TestCase):
 
 
 class TestLocaleUtils(unittest.TestCase):
+    @classmethod
+    def setUpClass(cls):
+        cls.locales = tm.get_locales()
+
+        if not cls.locales:
+            raise nose.SkipTest("No locales found")
+
+        if os.name == 'nt':  # we're on windows
+            raise nose.SkipTest("Running on Windows")
+
+    @classmethod
+    def tearDownClass(cls):
+        del cls.locales
+
     def test_get_locales(self):
-        assert len(get_locales(prefix='en')) > 0
+        # all systems should have at least a single locale
+        assert len(tm.get_locales()) > 0
 
     def test_get_locales_prefix(self):
-        with set_locale('en_US.UTF-8'):
-            assert len(get_locales(prefix='en')) > 0
+        if len(self.locales) == 1:
+            raise nose.SkipTest("Only a single locale found, no point in "
+                                "trying to test filtering locale prefixes")
+        first_locale = self.locales[0]
+        assert len(tm.get_locales(prefix=first_locale[:2])) > 0
 
     def test_set_locale(self):
-        old_locale = CURRENT_LOCALE
+        if len(self.locales) == 1:
+            raise nose.SkipTest("Only a single locale found, no point in "
+                                "trying to test setting another locale")
 
         if LOCALE_OVERRIDE is not None:
             lang, enc = LOCALE_OVERRIDE.split('.')
@@ -45,14 +65,19 @@ class TestLocaleUtils(unittest.TestCase):
         enc = codecs.lookup(enc).name
         new_locale = lang, enc
 
-        with set_locale(new_locale) as normalized_locale:
-            new_lang, new_enc = normalized_locale.split('.')
-            new_enc = codecs.lookup(enc).name
-            normalized_locale = new_lang, new_enc
-            self.assertEqual(normalized_locale, new_locale)
+        if not tm._can_set_locale('.'.join(new_locale)):
+            with tm.assertRaises(locale.Error):
+                with tm.set_locale(new_locale):
+                    pass
+        else:
+            with tm.set_locale(new_locale) as normalized_locale:
+                new_lang, new_enc = normalized_locale.split('.')
+                new_enc = codecs.lookup(enc).name
+                normalized_locale = new_lang, new_enc
+                self.assertEqual(normalized_locale, new_locale)
 
         current_locale = locale.getlocale()
-        self.assertEqual(current_locale, old_locale)
+        self.assertEqual(current_locale, CURRENT_LOCALE)
 
 
 if __name__ == '__main__':
