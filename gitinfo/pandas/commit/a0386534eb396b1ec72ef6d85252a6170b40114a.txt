commit a0386534eb396b1ec72ef6d85252a6170b40114a
Author: Jeffrey Tratner <jeffrey.tratner@gmail.com>
Date:   Sat Jul 27 15:02:37 2013 -0400

    CLN/ENH: Refactor url methods to be Py2/3 compatible

diff --git a/doc/sphinxext/docscrape.py b/doc/sphinxext/docscrape.py
index 384a6db2c..5d27810a1 100755
--- a/doc/sphinxext/docscrape.py
+++ b/doc/sphinxext/docscrape.py
@@ -491,7 +491,7 @@ class ClassDoc(NumpyDocString):
         if self._cls is None:
             return []
         return [name for name,func in inspect.getmembers(self._cls)
-                if not name.startswith('_') and callable(func)]
+                if not name.startswith('_') and six.callable(func)]
 
     @property
     def properties(self):
diff --git a/doc/sphinxext/docscrape_sphinx.py b/doc/sphinxext/docscrape_sphinx.py
index a5b53eb09..896ae070d 100755
--- a/doc/sphinxext/docscrape_sphinx.py
+++ b/doc/sphinxext/docscrape_sphinx.py
@@ -212,7 +212,7 @@ def get_doc_object(obj, what=None, doc=None, config={}):
             what = 'class'
         elif inspect.ismodule(obj):
             what = 'module'
-        elif callable(obj):
+        elif six.callable(obj):
             what = 'function'
         else:
             what = 'object'
diff --git a/doc/sphinxext/numpydoc.py b/doc/sphinxext/numpydoc.py
index f32d778b6..4ddc12e4c 100755
--- a/doc/sphinxext/numpydoc.py
+++ b/doc/sphinxext/numpydoc.py
@@ -83,7 +83,7 @@ def mangle_signature(app, what, name, obj, options, sig, retann):
         'initializes x; see ' in pydoc.getdoc(obj.__init__))):
         return '', ''
 
-    if not (callable(obj) or hasattr(obj, '__argspec_is_invalid_')): return
+    if not (six.callable(obj) or hasattr(obj, '__argspec_is_invalid_')): return
     if not hasattr(obj, '__doc__'): return
 
     doc = SphinxDocString(pydoc.getdoc(obj))
diff --git a/doc/sphinxext/traitsdoc.py b/doc/sphinxext/traitsdoc.py
index 952206c44..0298a441e 100755
--- a/doc/sphinxext/traitsdoc.py
+++ b/doc/sphinxext/traitsdoc.py
@@ -117,7 +117,7 @@ def get_doc_object(obj, what=None, config=None):
             what = 'class'
         elif inspect.ismodule(obj):
             what = 'module'
-        elif callable(obj):
+        elif six.callable(obj):
             what = 'function'
         else:
             what = 'object'
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 9bd4f24ee..ccbec5e9f 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -1283,7 +1283,7 @@ def _get_grouper(obj, key=None, axis=0, level=None, sort=True):
 
     # what are we after, exactly?
     match_axis_length = len(keys) == len(group_axis)
-    any_callable = any(callable(g) or isinstance(g, dict) for g in keys)
+    any_callable = any(six.callable(g) or isinstance(g, dict) for g in keys)
     any_arraylike = any(isinstance(g, (list, tuple, np.ndarray))
                         for g in keys)
 
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 0ff462ce2..37aa4e4ca 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -1266,7 +1266,7 @@ class BlockManager(PandasObject):
                 if not blk.items.isin(filter).any():
                     result_blocks.append(blk)
                     continue
-            if callable(f):
+            if six.callable(f):
                 applied = f(blk, *args, **kwargs)
             else:
                 applied = getattr(blk,f)(*args, **kwargs)
diff --git a/pandas/io/common.py b/pandas/io/common.py
index 3ad181c3d..786153183 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -1,19 +1,39 @@
 """Common IO api utilities"""
 
 import sys
-import urlparse
-import urllib2
 import zipfile
 from contextlib import contextmanager, closing
-from pandas.util.py3compat import StringIO
 
+from pandas.util.py3compat import StringIO
 from pandas.util import py3compat
 
-_VALID_URLS = set(urlparse.uses_relative + urlparse.uses_netloc +
-                  urlparse.uses_params)
-_VALID_URLS.discard('')
+
+if py3compat.PY3:
+    from urllib.request import urlopen
+    from urllib.parse import urlparse as parse_url
+    import urllib.parse as compat_parse
+    from urllib.parse import uses_relative, uses_netloc, uses_params
+    from urllib.error import URLError
+    from http.client import HTTPException
+else:
+    from urllib2 import urlopen as _urlopen
+    from urlparse import urlparse as parse_url
+    from urlparse import uses_relative, uses_netloc, uses_params
+    from urllib2 import URLError
+    from httplib import HTTPException
+    from contextlib import contextmanager, closing
+    from functools import wraps
+
+    @wraps(_urlopen)
+    @contextmanager
+    def urlopen(*args, **kwargs):
+        with closing(_urlopen(*args, **kwargs)) as f:
+            yield f
 
 
+_VALID_URLS = set(uses_relative + uses_netloc + uses_params)
+_VALID_URLS.discard('')
+
 class PerformanceWarning(Warning):
     pass
 
@@ -31,7 +51,7 @@ def _is_url(url):
         If `url` has a valid protocol return True otherwise False.
     """
     try:
-        return urlparse.urlparse(url).scheme in _VALID_URLS
+        return parse_url(url).scheme in _VALID_URLS
     except:
         return False
 
@@ -68,10 +88,11 @@ def get_filepath_or_buffer(filepath_or_buffer, encoding=None):
             else:
                 errors = 'replace'
                 encoding = 'utf-8'
-            bytes = filepath_or_buffer.read().decode(encoding, errors)
-            filepath_or_buffer = StringIO(bytes)
-            return filepath_or_buffer, encoding
-        return filepath_or_buffer, None
+            out = StringIO(req.read().decode(encoding, errors))
+        else:
+            encoding = None
+            out = req
+        return out, encoding
 
     if _is_s3_url(filepath_or_buffer):
         try:
@@ -91,16 +112,6 @@ def get_filepath_or_buffer(filepath_or_buffer, encoding=None):
     return filepath_or_buffer, None
 
 
-# ----------------------
-# Prevent double closing
-if py3compat.PY3:
-    urlopen = urllib2.urlopen
-else:
-    @contextmanager
-    def urlopen(*args, **kwargs):
-        with closing(urllib2.urlopen(*args, **kwargs)) as f:
-            yield f
-
 # ZipFile is not a context manager for <= 2.6
 # must be tuple index here since 2.6 doesn't use namedtuple for version_info
 if sys.version_info[1] <= 6:
diff --git a/pandas/io/data.py b/pandas/io/data.py
index afec82627..febf5c049 100644
--- a/pandas/io/data.py
+++ b/pandas/io/data.py
@@ -3,7 +3,6 @@ Module contains tools for collecting data from various remote sources
 
 
 """
-from pandas.util.py3compat import range
 import warnings
 import tempfile
 import datetime as dt
@@ -14,7 +13,7 @@ from collections import defaultdict
 
 import numpy as np
 
-from pandas.util.py3compat import StringIO, bytes_to_str
+from pandas.util.py3compat import StringIO, bytes_to_str, range
 from pandas import Panel, DataFrame, Series, read_csv, concat
 from pandas.core.common import PandasError
 from pandas.io.parsers import TextParser
diff --git a/pandas/io/html.py b/pandas/io/html.py
index 3fee071cd..dd58e8068 100644
--- a/pandas/io/html.py
+++ b/pandas/io/html.py
@@ -3,13 +3,9 @@ HTML IO.
 
 """
 
-from pandas.util.py3compat import range
-from pandas.util import compat
 import os
 import re
 import numbers
-import urllib2
-import urlparse
 import collections
 
 from distutils.version import LooseVersion
@@ -17,7 +13,9 @@ from distutils.version import LooseVersion
 import numpy as np
 
 from pandas import DataFrame, MultiIndex, isnull
-from pandas.io.common import _is_url, urlopen
+from pandas.io.common import _is_url, urlopen, parse_url
+from pandas.util.py3compat import range
+from pandas.util import compat
 import six
 from six.moves import map
 
@@ -553,7 +551,7 @@ class _LxmlFrameParser(_HtmlFrameParser):
                     pass
             else:
                 # not a url
-                scheme = urlparse.urlparse(self.io).scheme
+                scheme = parse_url(self.io).scheme
                 if scheme not in _valid_schemes:
                     # lxml can't parse it
                     msg = ('{0} is not a valid url scheme, valid schemes are '
diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py
index 765c0cd46..a81c77d87 100644
--- a/pandas/io/pickle.py
+++ b/pandas/io/pickle.py
@@ -1,4 +1,7 @@
-import cPickle as pkl
+try:
+    import cPickle as pkl
+except ImportError:
+    import pickle as pkl
 
 
 def to_pickle(obj, path):
diff --git a/pandas/io/tests/test_html.py b/pandas/io/tests/test_html.py
index 2f7c6092d..fc78d630b 100644
--- a/pandas/io/tests/test_html.py
+++ b/pandas/io/tests/test_html.py
@@ -6,7 +6,7 @@ from unittest import TestCase
 import warnings
 import six
 from distutils.version import LooseVersion
-import urllib2
+from pandas.io.common import URLError
 
 import nose
 from nose.tools import assert_raises
@@ -291,12 +291,12 @@ class TestReadHtmlBase(TestCase):
 
     @network
     def test_bad_url_protocol(self):
-        self.assertRaises(urllib2.URLError, self.run_read_html,
+        self.assertRaises(URLError, self.run_read_html,
                           'git://github.com', '.*Water.*')
 
     @network
     def test_invalid_url(self):
-        self.assertRaises(urllib2.URLError, self.run_read_html,
+        self.assertRaises(URLError, self.run_read_html,
                           'http://www.a23950sdfa908sd.com')
 
     @slow
diff --git a/pandas/io/tests/test_json/test_pandas.py b/pandas/io/tests/test_json/test_pandas.py
index b739181b7..4bb73dc76 100644
--- a/pandas/io/tests/test_json/test_pandas.py
+++ b/pandas/io/tests/test_json/test_pandas.py
@@ -5,6 +5,7 @@ from datetime import datetime, timedelta
 from pandas.util.py3compat import StringIO
 from pandas.util.py3compat import range
 from pandas.util import compat
+from pandas.io.common import URLError
 import cPickle as pickle
 import operator
 import os
@@ -473,7 +474,6 @@ class TestPandasContainer(unittest.TestCase):
     @network
     @slow
     def test_url(self):
-        import urllib2
         try:
 
             url = 'https://api.github.com/repos/pydata/pandas/issues?per_page=5'
@@ -484,5 +484,5 @@ class TestPandasContainer(unittest.TestCase):
             url = 'http://search.twitter.com/search.json?q=pandas%20python'
             result = read_json(url)
 
-        except urllib2.URLError:
+        except URLError:
             raise nose.SkipTest
diff --git a/pandas/io/tests/test_parsers.py b/pandas/io/tests/test_parsers.py
index 08b6a40df..5796ea577 100644
--- a/pandas/io/tests/test_parsers.py
+++ b/pandas/io/tests/test_parsers.py
@@ -1,22 +1,19 @@
 # pylint: disable=E1101
 
-from pandas.util.py3compat import StringIO, BytesIO, PY3
 from datetime import datetime
-from pandas.util.py3compat import range, long
 import csv
 import os
 import sys
 import re
 import unittest
-from contextlib import closing
-from urllib2 import urlopen
-
 import nose
 
 from numpy import nan
 import numpy as np
 
 from pandas import DataFrame, Series, Index, MultiIndex, DatetimeIndex
+from pandas.util.py3compat import StringIO, BytesIO, PY3, range, long
+from pandas.io.common import urlopen, URLError
 import pandas.io.parsers as parsers
 from pandas.io.parsers import (read_csv, read_table, read_fwf,
                                TextFileReader, TextParser)
@@ -1393,7 +1390,6 @@ a,b,c,d
     @slow
     @network
     def test_url(self):
-        import urllib2
         try:
             # HTTP(S)
             url = ('https://raw.github.com/pydata/pandas/master/'
@@ -1405,18 +1401,17 @@ a,b,c,d
             tm.assert_frame_equal(url_table, local_table)
             # TODO: ftp testing
 
-        except urllib2.URLError:
+        except URLError:
             try:
                 with closing(urlopen('http://www.google.com')) as resp:
                     pass
-            except urllib2.URLError:
+            except URLError:
                 raise nose.SkipTest
             else:
                 raise
 
     @slow
     def test_file(self):
-        import urllib2
 
         # FILE
         if sys.version_info[:2] < (2, 6):
@@ -1427,7 +1422,7 @@ a,b,c,d
 
         try:
             url_table = self.read_table('file://localhost/' + localtable)
-        except urllib2.URLError:
+        except URLError:
             # fails on some systems
             raise nose.SkipTest
 
diff --git a/pandas/io/wb.py b/pandas/io/wb.py
index 5048551cf..59e3c211a 100644
--- a/pandas/io/wb.py
+++ b/pandas/io/wb.py
@@ -1,8 +1,7 @@
 from __future__ import print_function
-from urllib2 import urlopen
 from pandas.util.py3compat import range
-import json
-from contextlib import closing
+from pandas.io.common import urlopen
+from pandas.io import json
 import pandas
 import numpy as np
 from six.moves import map, reduce
@@ -89,7 +88,7 @@ def _get_data(indicator="NY.GNS.ICTR.GN.ZS", country='US',
         indicator + "?date=" + str(start) + ":" + str(end) + "&per_page=25000" + \
         "&format=json"
     # Download
-    with closing(urlopen(url)) as response:
+    with urlopen(url) as response:
         data = response.read()
     # Parse JSON file
     data = json.loads(data)[1]
@@ -106,7 +105,7 @@ def get_countries():
     '''Query information about countries
     '''
     url = 'http://api.worldbank.org/countries/all?format=json'
-    with closing(urlopen(url)) as response:
+    with urlopen(url) as response:
         data = response.read()
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
@@ -122,7 +121,7 @@ def get_indicators():
     '''Download information about all World Bank data series
     '''
     url = 'http://api.worldbank.org/indicators?per_page=50000&format=json'
-    with closing(urlopen(url)) as response:
+    with urlopen(url) as response:
         data = response.read()
     data = json.loads(data)[1]
     data = pandas.DataFrame(data)
diff --git a/pandas/tseries/tools.py b/pandas/tseries/tools.py
index c56fa192b..f043dcb87 100644
--- a/pandas/tseries/tools.py
+++ b/pandas/tseries/tools.py
@@ -272,8 +272,8 @@ def dateutil_parse(timestr, default,
     if res.weekday is not None and not res.day:
         ret = ret + relativedelta.relativedelta(weekday=res.weekday)
     if not ignoretz:
-        if callable(tzinfos) or tzinfos and res.tzname in tzinfos:
-            if callable(tzinfos):
+        if six.callable(tzinfos) or tzinfos and res.tzname in tzinfos:
+            if six.callable(tzinfos):
                 tzdata = tzinfos(res.tzname, res.tzoffset)
             else:
                 tzdata = tzinfos.get(res.tzname)
diff --git a/pandas/util/compat.py b/pandas/util/compat.py
index 10fb2b107..8b9148456 100644
--- a/pandas/util/compat.py
+++ b/pandas/util/compat.py
@@ -485,7 +485,7 @@ class OrderedDefaultdict(OrderedDict):
         newargs = ()
         if args:
             newdefault = args[0]
-            if not (newdefault is None or callable(newdefault)):
+            if not (newdefault is None or six.callable(newdefault)):
                 raise TypeError('first argument must be callable or None')
             newargs = args[1:]
         self.default_factory = newdefault
diff --git a/pandas/util/decorators.py b/pandas/util/decorators.py
index 4a8762dcb..a5f4cc7e1 100644
--- a/pandas/util/decorators.py
+++ b/pandas/util/decorators.py
@@ -1,5 +1,6 @@
 from pandas.util.py3compat import StringIO
 from pandas.lib import cache_readonly
+import six
 import sys
 import warnings
 
@@ -163,7 +164,7 @@ def knownfailureif(fail_condition, msg=None):
         msg = 'Test skipped due to known failure'
 
     # Allow for both boolean or callable known failure conditions.
-    if callable(fail_condition):
+    if six.callable(fail_condition):
         fail_val = fail_condition
     else:
         fail_val = lambda: fail_condition
diff --git a/pandas/util/py3compat.py b/pandas/util/py3compat.py
index ad13d913b..f4d262e45 100644
--- a/pandas/util/py3compat.py
+++ b/pandas/util/py3compat.py
@@ -30,6 +30,7 @@ else:
         return b
 
     range = xrange
+    # have to explicitly put builtins into the namespace
     long = long
     unichr = unichr
 
diff --git a/pandas/util/testing.py b/pandas/util/testing.py
index 16e8c649e..d235298e8 100644
--- a/pandas/util/testing.py
+++ b/pandas/util/testing.py
@@ -2,7 +2,7 @@ from __future__ import division
 
 # pylint: disable-msg=W0402
 
-from pandas.util.py3compat import range
+from pandas.util.py3compat import range, unichr
 from six.moves import zip
 import random
 import string
@@ -14,9 +14,7 @@ import os
 
 from datetime import datetime
 from functools import wraps
-from contextlib import contextmanager
-from httplib import HTTPException
-from urllib2 import urlopen
+from contextlib import contextmanager, closing
 from distutils.version import LooseVersion
 
 from numpy.random import randn
@@ -34,7 +32,7 @@ from pandas import bdate_range
 from pandas.tseries.index import DatetimeIndex
 from pandas.tseries.period import PeriodIndex
 
-from pandas.io.common import urlopen
+from pandas.io.common import urlopen, HTTPException
 import six
 from six.moves import map
 
@@ -696,7 +694,7 @@ def optional_args(decorator):
         def dec(f):
             return decorator(f, *args, **kwargs)
 
-        is_decorating = not kwargs and len(args) == 1 and callable(args[0])
+        is_decorating = not kwargs and len(args) == 1 and six.callable(args[0])
         if is_decorating:
             f = args[0]
             args = []
@@ -746,11 +744,12 @@ def network(t, raise_on_error=_RAISE_NETWORK_ERROR_DEFAULT,
     A test can be decorated as requiring network like this::
 
       >>> from pandas.util.testing import network
-      >>> import urllib2
+      >>> from pandas.io.common import urlopen
       >>> import nose
       >>> @network
       ... def test_network():
-      ...   urllib2.urlopen("rabbit://bonanza.com")
+      ...   with urlopen("rabbit://bonanza.com") as f:
+      ...     pass
       ...
       >>> try:
       ...   test_network()
@@ -764,7 +763,8 @@ def network(t, raise_on_error=_RAISE_NETWORK_ERROR_DEFAULT,
 
       >>> @network(raise_on_error=True)
       ... def test_network():
-      ...   urllib2.urlopen("complaint://deadparrot.com")
+      ...   with urlopen("complaint://deadparrot.com") as f:
+      ...     pass
       ...
       >>> test_network()
       Traceback (most recent call last):
@@ -852,7 +852,7 @@ def with_connectivity_check(t, url="http://www.google.com",
     t : callable
         The test requiring network connectivity.
     url : path
-        The url to test via ``urllib2.urlopen`` to check for connectivity.
+        The url to test via ``pandas.io.common.urlopen`` to check for connectivity.
         Defaults to 'http://www.google.com'.
     raise_on_error : bool
         If True, never catches errors.
diff --git a/scripts/gen_release_notes.py b/scripts/gen_release_notes.py
index 905240fcf..02ba4f57c 100644
--- a/scripts/gen_release_notes.py
+++ b/scripts/gen_release_notes.py
@@ -1,8 +1,7 @@
 from __future__ import print_function
 import sys
-import urllib2
 import json
-from contextlib import closing
+from pandas.io.common import urlopen
 from datetime import datetime
 
 
@@ -49,8 +48,7 @@ def get_issues():
 def _get_page(page_number):
     gh_url = ('https://api.github.com/repos/pydata/pandas/issues?'
               'milestone=*&state=closed&assignee=*&page=%d') % page_number
-    req = urllib2.Request(gh_url)
-    with closing(urllib2.urlopen(req)) as resp:
+    with urlopen(gh_url) as resp:
         rs = resp.readlines()[0]
     jsondata = json.loads(rs)
     issues = [Issue(x['title'], x['labels'], x['number'],
diff --git a/scripts/json_manip.py b/scripts/json_manip.py
index 0b2ac8ff6..ccdf02b3f 100644
--- a/scripts/json_manip.py
+++ b/scripts/json_manip.py
@@ -274,7 +274,7 @@ def flatten(*stack):
         except StopIteration:
             stack.pop(0)
             continue
-        if hasattr(x,'next') and callable(getattr(x,'next')):
+        if hasattr(x,'next') and six.callable(getattr(x,'next')):
             stack.insert(0, x)
 
         #if isinstance(x, (GeneratorType,listerator)):
diff --git a/vb_suite/perf_HEAD.py b/vb_suite/perf_HEAD.py
index cd0a51d02..4ecaa3b9c 100755
--- a/vb_suite/perf_HEAD.py
+++ b/vb_suite/perf_HEAD.py
@@ -7,9 +7,7 @@ from __future__ import print_function
 
 """
 
-import urllib2
-from contextlib import closing
-from urllib2 import urlopen
+from pandas.io.common import urlopen
 import json
 
 import pandas as pd
@@ -26,7 +24,7 @@ def get_travis_data():
     if not jobid:
         return None, None
 
-    with closing(urlopen("https://api.travis-ci.org/workers/")) as resp:
+    with urlopen("https://api.travis-ci.org/workers/") as resp:
         workers = json.loads(resp.read())
 
     host = njobs = None
@@ -134,7 +132,7 @@ if __name__ == "__main__":
 
 
 def get_vbench_log(build_url):
-    with closing(urllib2.urlopen(build_url)) as r:
+    with urlopen(build_url) as r:
         if not (200 <= r.getcode() < 300):
             return
 
@@ -145,7 +143,7 @@ def get_vbench_log(build_url):
         if not s:
             return
         id = s[0]['id']  # should be just one for now
-        with closing(urllib2.urlopen("https://api.travis-ci.org/jobs/%s" % id)) as r2:
+        with urlopen("https://api.travis-ci.org/jobs/%s" % id) as r2:
             if not 200 <= r.getcode() < 300:
                 return
             s2 = json.loads(r2.read())
@@ -173,7 +171,7 @@ def convert_json_to_df(results_url):
     df contains timings for all successful vbenchmarks
     """
 
-    with closing(urlopen(results_url)) as resp:
+    with urlopen(results_url) as resp:
         res = json.loads(resp.read())
     timings = res.get("timings")
     if not timings:
@@ -217,7 +215,7 @@ def get_all_results(repo_id=53976):  # travis pydata/pandas id
     dfs = OrderedDict()
 
     while True:
-        with closing(urlopen(url)) as r:
+        with urlopen(url) as r:
             if not (200 <= r.getcode() < 300):
                 break
             builds = json.loads(r.read())
