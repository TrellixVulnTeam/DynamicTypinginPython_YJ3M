commit 20446016fef2c76f47b597b74c72d7bfbabfea06
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Thu Jul 12 13:45:42 2012 -0400

    ENH: Cython group_median method close #1358

diff --git a/RELEASE.rst b/RELEASE.rst
index 4894a795e..8caa85248 100644
--- a/RELEASE.rst
+++ b/RELEASE.rst
@@ -40,6 +40,7 @@ pandas 0.8.1
 
   - Use moving min/max algorithms from Bottleneck in rolling_min/rolling_max
     for > 100x speedup. (#1504, #50)
+  - Add Cython group median method for >15x speedup (#1358)
   - Drastically improve ``to_datetime`` performance on ISO8601 datetime strings
     (with no time zones) (#1571)
   - Add ability to append hierarchical index levels with ``set_index`` and to
diff --git a/pandas/core/groupby.py b/pandas/core/groupby.py
index 3ac141d2c..cb28f11c5 100644
--- a/pandas/core/groupby.py
+++ b/pandas/core/groupby.py
@@ -296,6 +296,20 @@ class GroupBy(object):
             f = lambda x: x.mean(axis=self.axis)
             return self._python_agg_general(f)
 
+    def median(self):
+        """
+        Compute mean of groups, excluding missing values
+
+        For multiple groupings, the result index will be a MultiIndex
+        """
+        try:
+            return self._cython_agg_general('median')
+        except GroupByError:
+            raise
+        except Exception:  # pragma: no cover
+            f = lambda x: x.median(axis=self.axis)
+            return self._python_agg_general(f)
+
     def std(self, ddof=1):
         """
         Compute standard deviation of groups, excluding missing values
@@ -631,6 +645,7 @@ class Grouper(object):
         'min' : lib.group_min,
         'max' : lib.group_max,
         'mean' : lib.group_mean,
+        'median' : lib.group_median,
         'var' : lib.group_var,
         'std' : lib.group_var,
         'first': lambda a, b, c, d: lib.group_nth(a, b, c, d, 1),
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index 7bb22393c..552f781e6 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -276,6 +276,7 @@ def group_add(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = sumx[i, j]
 
+
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def group_prod(ndarray[float64_t, ndim=2] out,
@@ -681,6 +682,80 @@ def group_mean(ndarray[float64_t, ndim=2] out,
             else:
                 out[i, j] = sumx[i, j] / count
 
+
+def group_median(ndarray[float64_t, ndim=2] out,
+                 ndarray[int64_t] counts,
+                 ndarray[float64_t, ndim=2] values,
+                 ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, size
+        ndarray[int64_t] _counts
+        ndarray data
+        float64_t* ptr
+
+    from pandas._algos import take_2d_axis1_float64
+
+    ngroups = len(counts)
+    N, K = (<object> values).shape
+
+    indexer, _counts = groupsort_indexer(labels, ngroups)
+    counts[:] = _counts[1:]
+
+    data = np.empty((K, N), dtype=np.float64)
+    ptr = <float64_t*> data.data
+
+    take_2d_axis1_float64(values.T, indexer, out=data)
+
+    for i in range(K):
+        # exclude NA group
+        ptr += _counts[0]
+        for j in range(ngroups):
+            size = _counts[j + 1]
+            out[j, i] = _median_linear(ptr, size)
+            ptr += size
+
+
+cdef inline float64_t _median_linear(float64_t* a, int n):
+    cdef int i, j, na_count = 0
+    cdef float64_t result
+    cdef float64_t* tmp
+
+    # count NAs
+    for i in range(n):
+        if a[i] != a[i]:
+            na_count += 1
+
+    if na_count:
+        if na_count == n:
+            return NaN
+
+        tmp = <float64_t*> malloc((n - na_count) * sizeof(float64_t))
+
+        j = 0
+        for i in range(n):
+            if a[i] == a[i]:
+                tmp[j] = a[i]
+                j += 1
+
+        a = tmp
+        n -= na_count
+
+
+    if n % 2:
+        result = kth_smallest_c(a, n / 2, n)
+    else:
+        result = (kth_smallest_c(a, n / 2, n) +
+                  kth_smallest_c(a, n / 2 - 1, n)) / 2
+
+    if na_count:
+        free(a)
+
+    return result
+
+
 @cython.boundscheck(False)
 @cython.wraparound(False)
 def group_var(ndarray[float64_t, ndim=2] out,
diff --git a/pandas/src/moments.pyx b/pandas/src/moments.pyx
index 702066171..6e37b6807 100644
--- a/pandas/src/moments.pyx
+++ b/pandas/src/moments.pyx
@@ -54,6 +54,33 @@ def kth_smallest(ndarray[double_t] a, Py_ssize_t k):
         if k < i: m = j
     return a[k]
 
+cdef inline kth_smallest_c(float64_t* a, Py_ssize_t k, Py_ssize_t n):
+    cdef:
+        Py_ssize_t i,j,l,m
+        double_t x, t
+
+    l = 0
+    m = n-1
+    while (l<m):
+        x = a[k]
+        i = l
+        j = m
+
+        while 1:
+            while a[i] < x: i += 1
+            while x < a[j]: j -= 1
+            if i <= j:
+                t = a[i]
+                a[i] = a[j]
+                a[j] = t
+                i += 1; j -= 1
+
+            if i > j: break
+
+        if j < k: l = i
+        if k < i: m = j
+    return a[k]
+
 
 def median(ndarray arr):
     '''
@@ -72,6 +99,7 @@ def median(ndarray arr):
         return (kth_smallest(arr, n / 2) +
                 kth_smallest(arr, n / 2 - 1)) / 2
 
+
 # -------------- Min, Max subsequence
 
 def max_subseq(ndarray[double_t] arr):
diff --git a/pandas/tests/test_groupby.py b/pandas/tests/test_groupby.py
index eb4fc3ff0..01a75961f 100644
--- a/pandas/tests/test_groupby.py
+++ b/pandas/tests/test_groupby.py
@@ -20,6 +20,8 @@ import pandas.core.datetools as dt
 import numpy as np
 from numpy.testing import assert_equal
 
+import pandas.core.nanops as nanops
+
 import pandas.util.testing as tm
 
 def commonSetUp(self):
@@ -2015,6 +2017,16 @@ class TestGroupBy(unittest.TestCase):
         result = df.groupby([('to filter', '')]).groups
         self.assertEquals(result, expected)
 
+    def test_cython_median(self):
+        df = DataFrame(np.random.randn(1000))
+        df.values[::2] = np.nan
+
+        labels = np.random.randint(0, 50, size=1000).astype(float)
+        labels[::17] = np.nan
+
+        result = df.groupby(labels).median()
+        exp = df.groupby(labels).agg(nanops.nanmedian)
+        assert_frame_equal(result, exp)
 
 def _check_groupby(df, result, keys, field, f=lambda x: x.sum()):
     tups = map(tuple, df[keys].values)
diff --git a/vb_suite/groupby.py b/vb_suite/groupby.py
index f690135e0..7db790a58 100644
--- a/vb_suite/groupby.py
+++ b/vb_suite/groupby.py
@@ -192,3 +192,19 @@ ts = Series(np.random.randn(len(rng)), index=rng)
 
 groupby_indices = Benchmark('len(ts.groupby([year, month, day]))',
                             setup, start_date=datetime(2012, 1, 1))
+
+#----------------------------------------------------------------------
+# median
+
+#----------------------------------------------------------------------
+# single key, long, integer key
+
+setup = common_setup + """
+data = np.random.randn(100000, 2)
+labels = np.random.randint(0, 1000, size=100000)
+df = DataFrame(data)
+"""
+
+groupby_frame_median = \
+    Benchmark('df.groupby(labels).median()', setup,
+              start_date=datetime(2011, 8, 1), logy=True)
