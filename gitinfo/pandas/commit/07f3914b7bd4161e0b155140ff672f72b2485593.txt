commit 07f3914b7bd4161e0b155140ff672f72b2485593
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Thu Dec 29 16:45:05 2011 -0500

    ENH: first cuts on many-to-many joining, #249, #267

diff --git a/TODO.rst b/TODO.rst
index 84a2ecd71..bb9560f9e 100644
--- a/TODO.rst
+++ b/TODO.rst
@@ -1,10 +1,7 @@
-LongPanel removal
-=================
-
-- DONE level to flex methods
-- DONE level to reindex
-- ?? fast take for items
-
+Join methods todo
+-----------------
+- Joint factorizer
+- NA group handling
 
 DONE
 ----
diff --git a/bench/bench_merge.py b/bench/bench_merge.py
new file mode 100644
index 000000000..9c6900526
--- /dev/null
+++ b/bench/bench_merge.py
@@ -0,0 +1,36 @@
+from pandas import *
+import random
+
+N = 10000
+ngroups = 3
+
+def get_test_data(ngroups=100, n=N):
+    unique_groups = range(ngroups)
+    arr = np.asarray(np.tile(unique_groups, n / ngroups), dtype=object)
+
+    if len(arr) < n:
+        arr = np.asarray(list(arr) + unique_groups[:n - len(arr)],
+                         dtype=object)
+
+    random.shuffle(arr)
+    return arr
+
+# aggregate multiple columns
+df = DataFrame({'key1' : get_test_data(ngroups=ngroups),
+                'key2' : get_test_data(ngroups=ngroups),
+                'data1' : np.random.randn(N),
+                'data2' : np.random.randn(N)})
+
+df2 = DataFrame({'key1'  : [0, 1, 2, 0, 1, 2],
+                 'key2'  : [0, 1, 2, 0, 1, 2],
+                 'value' : list('abcdef')})
+
+
+import pandas.tools.merge as merge
+reload(merge)
+
+left, right = merge._get_group_keys([df['key1'], df['key2']],
+                                        [df2['key1'], df2['key2']])
+
+left, right = merge._get_group_keys([df['key1']], [df2['key1']])
+
diff --git a/pandas/core/index.py b/pandas/core/index.py
index 32fea98c3..57bcd6579 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -1790,7 +1790,8 @@ class MultiIndex(Index):
             new_levels.append(level)
             new_labels.append(np.insert(labels, loc, lev_loc))
 
-        return MultiIndex(levels=new_levels, labels=new_labels, names=self.names)
+        return MultiIndex(levels=new_levels, labels=new_labels,
+                          names=self.names)
 
     def delete(self, loc):
         """
diff --git a/pandas/core/internals.py b/pandas/core/internals.py
index 3c82c90a0..5e5926c35 100644
--- a/pandas/core/internals.py
+++ b/pandas/core/internals.py
@@ -1081,7 +1081,10 @@ def _union_items_slow(all_items):
     return seen
 
 def join_managers(left, right, axis=1, how='left', copy=True):
-    op = _JoinOperation(left, right, axis=axis, how=how)
+    join_index, left_indexer, right_indexer = \
+        left.axes[axis].join(right.axes[axis], how=how, return_indexers=True)
+    op = _JoinOperation(left, right, join_index, left_indexer,
+                        right_indexer, axis=axis)
     return op.get_result(copy=copy)
 
 class _JoinOperation(object):
@@ -1089,7 +1092,10 @@ class _JoinOperation(object):
     Object responsible for orchestrating efficient join operation between two
     BlockManager data structures
     """
-    def __init__(self, left, right, axis=1, how='left'):
+    def __init__(self, left, right, join_index, left_indexer, right_indexer,
+                 axis=1):
+        assert(axis > 0)
+
         if not left.is_consolidated():
             left = left.consolidate()
         if not right.is_consolidated():
@@ -1098,14 +1104,10 @@ class _JoinOperation(object):
         self.left = left
         self.right = right
         self.axis = axis
-        self.how = how
-
-        laxis = left.axes[axis]
-        raxis = right.axes[axis]
 
-        (self.join_index,
-         self.lindexer,
-         self.rindexer) = laxis.join(raxis, how=how, return_indexers=True)
+        self.join_index = join_index
+        self.lindexer = left_indexer
+        self.rindexer = right_indexer
 
         # do NOT sort
         self.result_items = left.items.append(right.items)
@@ -1284,17 +1286,3 @@ class _JoinOperation(object):
 
         # use any ref_items
         return _consolidate(new_blocks, newb.ref_items)
-
-def _make_block_indexers(blocks, indexer, block_ids, block_locs, block_dtypes,
-                         ref_items):
-    counts = defaultdict(int)
-    for dtype_name in block_dtypes.take(indexer):
-        counts[dtype_name] += 1
-
-    findexer = np.empty(counts['float64'], dtype='i4')
-    bindexer = np.empty(counts['bool'], dtype='i4')
-    oindexer = np.empty(counts['object'], dtype='i4')
-    iindexer = np.empty(counts['int64'], dtype='i4')
-
-    for idx in indexer:
-        pass
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
new file mode 100644
index 000000000..5e95d87e8
--- /dev/null
+++ b/pandas/src/hashtable.pyx
@@ -0,0 +1,534 @@
+from khash cimport *
+from cpython cimport PyString_Check, PyString_AsString
+
+
+def test(ndarray arr, Py_ssize_t size_hint):
+    cdef:
+        kh_pymap_t *table
+        int ret
+        khiter_t k
+        PyObject **data
+        Py_ssize_t i, n
+        ndarray[Py_ssize_t] indexer
+
+    table = kh_init_pymap()
+    kh_resize_pymap(table, size_hint)
+
+    data = <PyObject**> arr.data
+    n = len(arr)
+
+    indexer = np.empty(n, dtype=np.int_)
+
+    for i in range(n):
+        k = kh_put_pymap(table, data[i], &ret)
+
+        # if not ret:
+        #     kh_del_pymap(table, k)
+
+        table.vals[k] = i
+
+    for i in range(n):
+        k = kh_get_pymap(table, data[i])
+        indexer[i] = table.vals[k]
+
+    kh_destroy_pymap(table)
+
+    return indexer
+
+
+def test_str(ndarray arr, Py_ssize_t size_hint):
+    cdef:
+        kh_str_t *table
+        kh_cstr_t val
+        int ret
+        khiter_t k
+        PyObject **data
+        Py_ssize_t i, n
+        ndarray[Py_ssize_t] indexer
+
+    table = kh_init_str()
+    kh_resize_str(table, size_hint)
+
+    data = <PyObject**> arr.data
+    n = len(arr)
+
+    indexer = np.empty(n, dtype=np.int_)
+
+    for i in range(n):
+        k = kh_put_str(table, PyString_AsString(<object> data[i]), &ret)
+
+        # if not ret:
+        #     kh_del_str(table, k)
+
+        table.vals[k] = i
+
+    # for i in range(n):
+    #     k = kh_get_str(table, PyString_AsString(<object> data[i]))
+    #     indexer[i] = table.vals[k]
+
+    kh_destroy_str(table)
+
+    return indexer
+
+# def test2(ndarray[object] arr):
+#     cdef:
+#         dict table
+#         object obj
+#         Py_ssize_t i, loc, n
+#         ndarray[Py_ssize_t] indexer
+
+#     n = len(arr)
+#     indexer = np.empty(n, dtype=np.int_)
+
+#     table = {}
+#     for i in range(n):
+#         table[arr[i]] = i
+
+#     for i in range(n):
+#         indexer[i] =  table[arr[i]]
+
+#     return indexer
+
+def obj_unique(ndarray[object] arr):
+    cdef:
+        kh_pyset_t *table
+        # PyObject *obj
+        object obj
+        PyObject **data
+        int ret
+        khiter_t k
+        Py_ssize_t i, n
+        list uniques
+
+    n = len(arr)
+    uniques = []
+
+    table = kh_init_pyset()
+
+    data = <PyObject**> arr.data
+
+    # size hint
+    kh_resize_pyset(table, n // 10)
+
+    for i in range(n):
+        obj = arr[i]
+
+        k = kh_get_pyset(table, <PyObject*> obj)
+        if not kh_exist_pyset(table, k):
+            k = kh_put_pyset(table, <PyObject*> obj, &ret)
+            # uniques.append(obj)
+            # Py_INCREF(<object> obj)
+
+    kh_destroy_pyset(table)
+
+    return None
+
+def int64_unique(ndarray[int64_t] arr):
+    cdef:
+        kh_int64_t *table
+        # PyObject *obj
+        int64_t obj
+        PyObject **data
+        int ret
+        khiter_t k
+        Py_ssize_t i, j, n
+        ndarray[int64_t] uniques
+
+    n = len(arr)
+    uniques = np.empty(n, dtype='i8')
+
+    table = kh_init_int64()
+    kh_resize_int64(table, n)
+
+    j = 0
+
+    for i in range(n):
+        obj = arr[i]
+
+        k = kh_get_int64(table, obj)
+        if not kh_exist_int64(table, k):
+            k = kh_put_int64(table, obj, &ret)
+            uniques[j] = obj
+            j += 1
+            # Py_INCREF(<object> obj)
+
+    kh_destroy_int64(table)
+
+    return np.sort(uniques[:j])
+
+cdef class StringHashTable:
+
+    cdef:
+        kh_str_t *table
+
+    def __init__(self, size_hint=1):
+        if size_hint is not None:
+            kh_resize_str(self.table, size_hint)
+
+    def __cinit__(self):
+        self.table = kh_init_str()
+
+    def __dealloc__(self):
+        kh_destroy_str(self.table)
+
+    cdef inline int check_type(self, object val):
+        return PyString_Check(val)
+
+    cpdef get_item(self, object val):
+        cdef khiter_t k
+        k = kh_get_str(self.table, PyString_AsString(val))
+        if k != self.table.n_buckets:
+            return self.table.vals[k]
+        else:
+            raise KeyError(val)
+
+    def get_iter_test(self, object key, Py_ssize_t iterations):
+        cdef Py_ssize_t i, val
+        for i in range(iterations):
+            k = kh_get_str(self.table, PyString_AsString(key))
+            if k != self.table.n_buckets:
+                val = self.table.vals[k]
+
+    cpdef set_item(self, object key, Py_ssize_t val):
+        cdef:
+            khiter_t k
+            int ret
+            char* buf
+
+        buf = PyString_AsString(key)
+
+        k = kh_put_str(self.table, buf, &ret)
+        self.table.keys[k] = key
+        if kh_exist_str(self.table, k):
+            self.table.vals[k] = val
+        else:
+            raise KeyError(key)
+
+    def factorize(self, ndarray[object] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
+            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
+            dict reverse = {}
+            Py_ssize_t idx, count = 0
+            int ret
+            object val
+            char *buf
+            khiter_t k
+
+        for i in range(n):
+            val = values[i]
+            buf = PyString_AsString(val)
+            k = kh_get_str(self.table, buf)
+            if k != self.table.n_buckets:
+                idx = self.table.vals[k]
+                labels[i] = idx
+                counts[idx] = counts[idx] + 1
+            else:
+                k = kh_put_str(self.table, buf, &ret)
+                # print 'putting %s, %s' % (val, count)
+                if not ret:
+                    kh_del_str(self.table, k)
+
+                self.table.vals[k] = count
+                reverse[count] = val
+                labels[i] = count
+                counts[count] = 1
+                count += 1
+
+        # return None
+        return reverse, labels, counts[:count].copy()
+
+cdef class Int64HashTable:
+
+    cdef:
+        kh_int64_t *table
+
+    def __init__(self, size_hint=1):
+        if size_hint is not None:
+            kh_resize_int64(self.table, size_hint)
+
+    def __cinit__(self):
+        self.table = kh_init_int64()
+
+    def __dealloc__(self):
+        kh_destroy_int64(self.table)
+
+    cdef inline int check_type(self, object val):
+        return PyString_Check(val)
+
+    cpdef get_item(self, int64_t val):
+        cdef khiter_t k
+        k = kh_get_int64(self.table, val)
+        if k != self.table.n_buckets:
+            return self.table.vals[k]
+        else:
+            raise KeyError(val)
+
+    def get_iter_test(self, int64_t key, Py_ssize_t iterations):
+        cdef Py_ssize_t i, val
+        for i in range(iterations):
+            k = kh_get_int64(self.table, val)
+            if k != self.table.n_buckets:
+                val = self.table.vals[k]
+
+    cpdef set_item(self, int64_t key, Py_ssize_t val):
+        cdef:
+            khiter_t k
+            int ret
+
+        k = kh_put_int64(self.table, key, &ret)
+        self.table.keys[k] = key
+        if kh_exist_int64(self.table, k):
+            self.table.vals[k] = val
+        else:
+            raise KeyError(key)
+
+    def map_locations(self, ndarray[int64_t] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            int64_t val
+            khiter_t k
+
+        for i in range(n):
+            val = values[i]
+            k = kh_put_int64(self.table, val, &ret)
+            # print 'putting %s, %s' % (val, count)
+            self.table.vals[k] = i
+
+    def lookup_locations(self, ndarray[int64_t] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            int64_t val
+            khiter_t k
+            ndarray[int32_t] locs = np.empty(n, dtype='i4')
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_int64(self.table, val)
+            if k != self.table.n_buckets:
+                locs[i] = self.table.vals[k]
+            else:
+                locs[i] = -1
+
+        return locs
+
+    def factorize(self, ndarray[int64_t] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
+            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
+            dict reverse = {}
+            Py_ssize_t idx, count = 0
+            int ret
+            int64_t val
+            khiter_t k
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_int64(self.table, val)
+            if k != self.table.n_buckets:
+                idx = self.table.vals[k]
+                labels[i] = idx
+                counts[idx] = counts[idx] + 1
+            else:
+                k = kh_put_int64(self.table, val, &ret)
+                if not ret:
+                    kh_del_int64(self.table, k)
+                self.table.vals[k] = count
+                reverse[count] = val
+                labels[i] = count
+                counts[count] = 1
+                count += 1
+
+        # return None
+        return reverse, labels, counts[:count].copy()
+
+cdef class PyObjectHashTable:
+
+    cdef:
+        kh_pymap_t *table
+
+    def __init__(self, size_hint=1):
+        self.table = kh_init_pymap()
+        kh_resize_pymap(self.table, size_hint)
+
+    def __dealloc__(self):
+        if self.table is not NULL:
+            self.destroy()
+
+    cpdef destroy(self):
+        kh_destroy_pymap(self.table)
+        self.table = NULL
+
+    cpdef get_item(self, object val):
+        cdef khiter_t k
+        k = kh_get_pymap(self.table, <PyObject*>val)
+        if k != self.table.n_buckets:
+            return self.table.vals[k]
+        else:
+            raise KeyError(val)
+
+    def get_iter_test(self, object key, Py_ssize_t iterations):
+        cdef Py_ssize_t i, val
+        for i in range(iterations):
+            k = kh_get_pymap(self.table, <PyObject*>key)
+            if k != self.table.n_buckets:
+                val = self.table.vals[k]
+
+    cpdef set_item(self, object key, Py_ssize_t val):
+        cdef:
+            khiter_t k
+            int ret
+            char* buf
+
+        k = kh_put_pymap(self.table, <PyObject*>key, &ret)
+        # self.table.keys[k] = key
+        if kh_exist_pymap(self.table, k):
+            self.table.vals[k] = val
+        else:
+            raise KeyError(key)
+
+    def map_locations(self, ndarray[object] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            object val
+            khiter_t k
+
+        for i in range(n):
+            val = values[i]
+            k = kh_put_pymap(self.table, <PyObject*>val, &ret)
+            # print 'putting %s, %s' % (val, count)
+            self.table.vals[k] = i
+
+    def lookup_locations(self, ndarray[object] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            object val
+            khiter_t k
+            ndarray[int32_t] locs = np.empty(n, dtype='i4')
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_pymap(self.table, <PyObject*>val)
+            if k != self.table.n_buckets:
+                locs[i] = self.table.vals[k]
+            else:
+                locs[i] = -1
+
+        return locs
+
+    def lookup_locations2(self, ndarray[object] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            int ret
+            object val
+            khiter_t k
+            long hval
+            ndarray[int32_t] locs = np.empty(n, dtype='i4')
+
+        # for i in range(n):
+        #     val = values[i]
+            # hval = PyObject_Hash(val)
+            # k = kh_get_pymap(self.table, <PyObject*>val)
+
+        return locs
+
+    def unique(self, ndarray[object] values):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
+            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
+            dict reverse = {}
+            Py_ssize_t idx, count = 0
+            int ret
+            object val
+            khiter_t k
+            list uniques = []
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_pymap(self.table, <PyObject*>val)
+            if k == self.table.n_buckets:
+                k = kh_put_pymap(self.table, <PyObject*>val, &ret)
+                uniques.append(val)
+
+        return uniques
+
+    def factorize(self, ndarray[object] values):
+        reverse = {}
+        labels, counts = self.get_labels(values, reverse, 0)
+        return reverse, labels, counts
+
+    cpdef get_labels(self, ndarray[object] values, dict reverse,
+                     Py_ssize_t count_prior):
+        cdef:
+            Py_ssize_t i, n = len(values)
+            ndarray[int32_t] labels
+            ndarray[int32_t] counts
+            Py_ssize_t idx, count = count_prior
+            int ret
+            object val
+            khiter_t k
+
+        labels = np.empty(n, dtype=np.int32)
+        counts = np.empty(count_prior + n, dtype=np.int32)
+
+        for i in range(n):
+            val = values[i]
+            k = kh_get_pymap(self.table, <PyObject*>val)
+            if k != self.table.n_buckets:
+                idx = self.table.vals[k]
+                labels[i] = idx
+                counts[idx] = counts[idx] + 1
+            else:
+                k = kh_put_pymap(self.table, <PyObject*>val, &ret)
+                self.table.vals[k] = count
+                reverse[count] = val
+                labels[i] = count
+                counts[count] = 1
+                count += 1
+
+        return labels, counts[:count].copy()
+
+cdef class Factorizer:
+
+    cdef public:
+        PyObjectHashTable table
+        dict id_table
+        Py_ssize_t count
+
+    def __init__(self, size_hint):
+        self.table = PyObjectHashTable(size_hint)
+        self.id_table = {}
+        self.count = 0
+
+    def get_count(self):
+        return self.count
+
+    def factorize(self, ndarray[object] values):
+        labels, counts = self.table.get_labels(values, self.id_table,
+                                               self.count)
+        self.count = len(counts)
+        return labels, counts
+
+def lookup_locations2(ndarray[object] values):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        int ret
+        object val
+        khiter_t k
+        long hval
+        ndarray[int32_t] locs = np.empty(n, dtype='i4')
+
+    # for i in range(n):
+    #     val = values[i]
+        # hval = PyObject_Hash(val)
+        # k = kh_get_pymap(self.table, <PyObject*>val)
+
+    return locs
+
diff --git a/pandas/src/inference.pyx b/pandas/src/inference.pyx
index 065b7a131..b64e88607 100644
--- a/pandas/src/inference.pyx
+++ b/pandas/src/inference.pyx
@@ -1,6 +1,27 @@
+cpdef is_array(object o):
+    return np.PyArray_Check(o)
+
+
 def is_bool_array(ndarray[object] values):
     cdef Py_ssize_t i, n = len(values)
     for i in range(n):
         if not util.is_bool_object(values[i]):
             return False
     return True
+
+
+
+def isAllDates(ndarray[object, ndim=1] arr):
+    cdef int i, size = len(arr)
+    cdef object date
+
+    if size == 0:
+        return False
+
+    for i from 0 <= i < size:
+        date = arr[i]
+
+        if not PyDateTime_Check(date):
+            return False
+
+    return True
diff --git a/pandas/src/join.pyx b/pandas/src/join.pyx
new file mode 100644
index 000000000..f970e0d7d
--- /dev/null
+++ b/pandas/src/join.pyx
@@ -0,0 +1,93 @@
+
+def inner_join(ndarray[int32_t] left, ndarray[int32_t] right):
+    cdef:
+        Py_ssize_t count
+
+def left_outer_join(ndarray[int32_t] left, ndarray[int32_t] right,
+                    Py_ssize_t max_groups):
+    cdef:
+        Py_ssize_t i, j, k, count = 0
+        ndarray[int32_t] left_count, right_count, left_sorter, right_sorter
+        ndarray[int32_t] left_indexer, right_indexer
+        int32_t lc, rc
+
+    # NA group in location 0
+
+    left_sorter, left_count = groupsort_indexer(left, max_groups)
+    right_sorter, right_count = groupsort_indexer(right, max_groups)
+
+    # First pass, determine size of result set, do not use the NA group
+    for i in range(1, max_groups + 1):
+        if right_count[i] > 0:
+            count += left_count[i] * right_count[i]
+        else:
+            count += left_count[i]
+
+    left_indexer = np.empty(count, dtype='i4')
+    right_indexer = np.empty(count, dtype='i4')
+
+    left = left.take(left_sorter)
+    right = right.take(right_sorter)
+
+    # group 0 is the NA group
+    cdef:
+        Py_ssize_t loc, left_pos = 0, right_pos = 0, position = 0
+        Py_ssize_t offset
+
+    # exclude the NA group
+    left_pos = left_count[0]
+    right_pos = right_count[0]
+
+    for i in range(1, max_groups + 1):
+        lc = left_count[i]
+        rc = right_count[i]
+
+        if rc == 0:
+            for j in range(lc):
+                left_indexer[position + j] = left_pos + j
+                right_indexer[position + j] = -1
+            left_pos += lc
+            position += lc
+        else:
+            for j in range(lc):
+                offset = position + j * rc
+                for k in range(rc):
+                    left_indexer[offset + k] = left_pos + j
+                    right_indexer[offset + k] = right_pos + k
+            left_pos += lc
+            right_pos += rc
+            position += lc * rc
+
+    return left_sorter, left_indexer, right_sorter, right_indexer
+
+
+def full_outer_join(ndarray[int32_t] left, ndarray[int32_t] right):
+    pass
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def groupsort_indexer(ndarray[int32_t] index, Py_ssize_t ngroups):
+    cdef:
+        Py_ssize_t i, loc, label, n
+        ndarray[int32_t] counts, where, result
+
+    # count group sizes, location 0 for NA
+    counts = np.zeros(ngroups + 1, dtype='i4')
+    n = len(index)
+    for i from 0 <= i < n:
+        counts[index[i] + 1] += 1
+
+    # mark the start of each contiguous group of like-indexed data
+    where = np.zeros(ngroups + 1, dtype='i4')
+    for i from 1 <= i < ngroups + 1:
+        where[i] = where[i - 1] + counts[i - 1]
+
+    # this is our indexer
+    result = np.zeros(n, dtype='i4')
+    for i from 0 <= i < n:
+        label = index[i] + 1
+        result[where[label]] = i
+        where[label] += 1
+
+    return result, counts
diff --git a/pandas/src/sandbox.pyx b/pandas/src/sandbox.pyx
index 976c50f27..96d827a95 100644
--- a/pandas/src/sandbox.pyx
+++ b/pandas/src/sandbox.pyx
@@ -76,521 +76,6 @@ def foo(object _chunk, object _arr):
     # chunk.shape[0] = 100
     return chunk
 
-from khash cimport *
-
-def test(ndarray arr, Py_ssize_t size_hint):
-    cdef:
-        kh_pymap_t *table
-        int ret
-        khiter_t k
-        PyObject **data
-        Py_ssize_t i, n
-        ndarray[Py_ssize_t] indexer
-
-    table = kh_init_pymap()
-    kh_resize_pymap(table, size_hint)
-
-    data = <PyObject**> arr.data
-    n = len(arr)
-
-    indexer = np.empty(n, dtype=np.int_)
-
-    for i in range(n):
-        k = kh_put_pymap(table, data[i], &ret)
-
-        # if not ret:
-        #     kh_del_pymap(table, k)
-
-        table.vals[k] = i
-
-    for i in range(n):
-        k = kh_get_pymap(table, data[i])
-        indexer[i] = table.vals[k]
-
-    kh_destroy_pymap(table)
-
-    return indexer
-
-
-from cpython cimport PyString_AsString
-
-def test_str(ndarray arr, Py_ssize_t size_hint):
-    cdef:
-        kh_str_t *table
-        kh_cstr_t val
-        int ret
-        khiter_t k
-        PyObject **data
-        Py_ssize_t i, n
-        ndarray[Py_ssize_t] indexer
-
-    table = kh_init_str()
-    kh_resize_str(table, size_hint)
-
-    data = <PyObject**> arr.data
-    n = len(arr)
-
-    indexer = np.empty(n, dtype=np.int_)
-
-    for i in range(n):
-        k = kh_put_str(table, PyString_AsString(<object> data[i]), &ret)
-
-        # if not ret:
-        #     kh_del_str(table, k)
-
-        table.vals[k] = i
-
-    # for i in range(n):
-    #     k = kh_get_str(table, PyString_AsString(<object> data[i]))
-    #     indexer[i] = table.vals[k]
-
-    kh_destroy_str(table)
-
-    return indexer
-
-# def test2(ndarray[object] arr):
-#     cdef:
-#         dict table
-#         object obj
-#         Py_ssize_t i, loc, n
-#         ndarray[Py_ssize_t] indexer
-
-#     n = len(arr)
-#     indexer = np.empty(n, dtype=np.int_)
-
-#     table = {}
-#     for i in range(n):
-#         table[arr[i]] = i
-
-#     for i in range(n):
-#         indexer[i] =  table[arr[i]]
-
-#     return indexer
-
-from cpython cimport Py_INCREF
-
-def obj_unique(ndarray[object] arr):
-    cdef:
-        kh_pyset_t *table
-        # PyObject *obj
-        object obj
-        PyObject **data
-        int ret
-        khiter_t k
-        Py_ssize_t i, n
-        list uniques
-
-    n = len(arr)
-    uniques = []
-
-    table = kh_init_pyset()
-
-    data = <PyObject**> arr.data
-
-    # size hint
-    kh_resize_pyset(table, n // 10)
-
-    for i in range(n):
-        obj = arr[i]
-
-        k = kh_get_pyset(table, <PyObject*> obj)
-        if not kh_exist_pyset(table, k):
-            k = kh_put_pyset(table, <PyObject*> obj, &ret)
-            # uniques.append(obj)
-            # Py_INCREF(<object> obj)
-
-    kh_destroy_pyset(table)
-
-    return None
-
-def int64_unique(ndarray[int64_t] arr):
-    cdef:
-        kh_int64_t *table
-        # PyObject *obj
-        int64_t obj
-        PyObject **data
-        int ret
-        khiter_t k
-        Py_ssize_t i, j, n
-        ndarray[int64_t] uniques
-
-    n = len(arr)
-    uniques = np.empty(n, dtype='i8')
-
-    table = kh_init_int64()
-    kh_resize_int64(table, n)
-
-    j = 0
-
-    for i in range(n):
-        obj = arr[i]
-
-        k = kh_get_int64(table, obj)
-        if not kh_exist_int64(table, k):
-            k = kh_put_int64(table, obj, &ret)
-            uniques[j] = obj
-            j += 1
-            # Py_INCREF(<object> obj)
-
-    kh_destroy_int64(table)
-
-    return np.sort(uniques[:j])
-
-from cpython cimport PyString_Check, PyString_AsString
-
-cdef class StringHashTable:
-
-    cdef:
-        kh_str_t *table
-
-    def __init__(self, size_hint=1):
-        if size_hint is not None:
-            kh_resize_str(self.table, size_hint)
-
-    def __cinit__(self):
-        self.table = kh_init_str()
-
-    def __dealloc__(self):
-        kh_destroy_str(self.table)
-
-    cdef inline int check_type(self, object val):
-        return PyString_Check(val)
-
-    cpdef get_item(self, object val):
-        cdef khiter_t k
-        k = kh_get_str(self.table, PyString_AsString(val))
-        if k != self.table.n_buckets:
-            return self.table.vals[k]
-        else:
-            raise KeyError(val)
-
-    def get_iter_test(self, object key, Py_ssize_t iterations):
-        cdef Py_ssize_t i, val
-        for i in range(iterations):
-            k = kh_get_str(self.table, PyString_AsString(key))
-            if k != self.table.n_buckets:
-                val = self.table.vals[k]
-
-    cpdef set_item(self, object key, Py_ssize_t val):
-        cdef:
-            khiter_t k
-            int ret
-            char* buf
-
-        buf = PyString_AsString(key)
-
-        k = kh_put_str(self.table, buf, &ret)
-        self.table.keys[k] = key
-        if kh_exist_str(self.table, k):
-            self.table.vals[k] = val
-        else:
-            raise KeyError(key)
-
-    def factorize(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
-            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
-            dict reverse = {}
-            Py_ssize_t idx, count = 0
-            int ret
-            object val
-            char *buf
-            khiter_t k
-
-        for i in range(n):
-            val = values[i]
-            buf = PyString_AsString(val)
-            k = kh_get_str(self.table, buf)
-            if k != self.table.n_buckets:
-                idx = self.table.vals[k]
-                labels[i] = idx
-                counts[idx] = counts[idx] + 1
-            else:
-                k = kh_put_str(self.table, buf, &ret)
-                # print 'putting %s, %s' % (val, count)
-                if not ret:
-                    kh_del_str(self.table, k)
-
-                self.table.vals[k] = count
-                reverse[count] = val
-                labels[i] = count
-                counts[count] = 1
-                count += 1
-
-        # return None
-        return reverse, labels, counts[:count].copy()
-
-cdef class Int64HashTable:
-
-    cdef:
-        kh_int64_t *table
-
-    def __init__(self, size_hint=1):
-        if size_hint is not None:
-            kh_resize_int64(self.table, size_hint)
-
-    def __cinit__(self):
-        self.table = kh_init_int64()
-
-    def __dealloc__(self):
-        kh_destroy_int64(self.table)
-
-    cdef inline int check_type(self, object val):
-        return PyString_Check(val)
-
-    cpdef get_item(self, int64_t val):
-        cdef khiter_t k
-        k = kh_get_int64(self.table, val)
-        if k != self.table.n_buckets:
-            return self.table.vals[k]
-        else:
-            raise KeyError(val)
-
-    def get_iter_test(self, int64_t key, Py_ssize_t iterations):
-        cdef Py_ssize_t i, val
-        for i in range(iterations):
-            k = kh_get_int64(self.table, val)
-            if k != self.table.n_buckets:
-                val = self.table.vals[k]
-
-    cpdef set_item(self, int64_t key, Py_ssize_t val):
-        cdef:
-            khiter_t k
-            int ret
-
-        k = kh_put_int64(self.table, key, &ret)
-        self.table.keys[k] = key
-        if kh_exist_int64(self.table, k):
-            self.table.vals[k] = val
-        else:
-            raise KeyError(key)
-
-    def map_locations(self, ndarray[int64_t] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            int ret
-            int64_t val
-            khiter_t k
-
-        for i in range(n):
-            val = values[i]
-            k = kh_put_int64(self.table, val, &ret)
-            # print 'putting %s, %s' % (val, count)
-            self.table.vals[k] = i
-
-    def lookup_locations(self, ndarray[int64_t] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            int ret
-            int64_t val
-            khiter_t k
-            ndarray[int32_t] locs = np.empty(n, dtype='i4')
-
-        for i in range(n):
-            val = values[i]
-            k = kh_get_int64(self.table, val)
-            if k != self.table.n_buckets:
-                locs[i] = self.table.vals[k]
-            else:
-                locs[i] = -1
-
-        return locs
-
-    def factorize(self, ndarray[int64_t] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
-            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
-            dict reverse = {}
-            Py_ssize_t idx, count = 0
-            int ret
-            int64_t val
-            khiter_t k
-
-        for i in range(n):
-            val = values[i]
-            k = kh_get_int64(self.table, val)
-            if k != self.table.n_buckets:
-                idx = self.table.vals[k]
-                labels[i] = idx
-                counts[idx] = counts[idx] + 1
-            else:
-                k = kh_put_int64(self.table, val, &ret)
-                if not ret:
-                    kh_del_int64(self.table, k)
-                self.table.vals[k] = count
-                reverse[count] = val
-                labels[i] = count
-                counts[count] = 1
-                count += 1
-
-        # return None
-        return reverse, labels, counts[:count].copy()
-
-from libc.stdlib cimport free
-
-cdef class PyObjectHashTable:
-
-    cdef:
-        kh_pymap_t *table
-
-    def __init__(self, size_hint=1):
-        self.table = kh_init_pymap()
-        kh_resize_pymap(self.table, size_hint)
-
-    def __dealloc__(self):
-        if self.table is not NULL:
-            self.destroy()
-
-    cpdef destroy(self):
-        kh_destroy_pymap(self.table)
-        self.table = NULL
-
-    cpdef get_item(self, object val):
-        cdef khiter_t k
-        k = kh_get_pymap(self.table, <PyObject*>val)
-        if k != self.table.n_buckets:
-            return self.table.vals[k]
-        else:
-            raise KeyError(val)
-
-    def get_iter_test(self, object key, Py_ssize_t iterations):
-        cdef Py_ssize_t i, val
-        for i in range(iterations):
-            k = kh_get_pymap(self.table, <PyObject*>key)
-            if k != self.table.n_buckets:
-                val = self.table.vals[k]
-
-    cpdef set_item(self, object key, Py_ssize_t val):
-        cdef:
-            khiter_t k
-            int ret
-            char* buf
-
-        k = kh_put_pymap(self.table, <PyObject*>key, &ret)
-        # self.table.keys[k] = key
-        if kh_exist_pymap(self.table, k):
-            self.table.vals[k] = val
-        else:
-            raise KeyError(key)
-
-    def map_locations(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            int ret
-            object val
-            khiter_t k
-
-        for i in range(n):
-            val = values[i]
-            k = kh_put_pymap(self.table, <PyObject*>val, &ret)
-            # print 'putting %s, %s' % (val, count)
-            self.table.vals[k] = i
-
-    def lookup_locations(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            int ret
-            object val
-            khiter_t k
-            ndarray[int32_t] locs = np.empty(n, dtype='i4')
-
-        for i in range(n):
-            val = values[i]
-            k = kh_get_pymap(self.table, <PyObject*>val)
-            if k != self.table.n_buckets:
-                locs[i] = self.table.vals[k]
-            else:
-                locs[i] = -1
-
-        return locs
-
-    def lookup_locations2(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            int ret
-            object val
-            khiter_t k
-            long hval
-            ndarray[int32_t] locs = np.empty(n, dtype='i4')
-
-        # for i in range(n):
-        #     val = values[i]
-            # hval = PyObject_Hash(val)
-            # k = kh_get_pymap(self.table, <PyObject*>val)
-
-        return locs
-
-    def unique(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
-            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
-            dict reverse = {}
-            Py_ssize_t idx, count = 0
-            int ret
-            object val
-            khiter_t k
-            list uniques = []
-
-        for i in range(n):
-            val = values[i]
-            k = kh_get_pymap(self.table, <PyObject*>val)
-            if k == self.table.n_buckets:
-                k = kh_put_pymap(self.table, <PyObject*>val, &ret)
-                uniques.append(val)
-
-        return uniques
-
-    def factorize(self, ndarray[object] values):
-        cdef:
-            Py_ssize_t i, n = len(values)
-            ndarray[int32_t] labels = np.empty(n, dtype=np.int32)
-            ndarray[int32_t] counts = np.empty(n, dtype=np.int32)
-            dict reverse = {}
-            Py_ssize_t idx, count = 0
-            int ret
-            object val
-            khiter_t k
-
-        for i in range(n):
-            val = values[i]
-            k = kh_get_pymap(self.table, <PyObject*>val)
-            if k != self.table.n_buckets:
-                idx = self.table.vals[k]
-                labels[i] = idx
-                counts[idx] = counts[idx] + 1
-            else:
-                k = kh_put_pymap(self.table, <PyObject*>val, &ret)
-                # print 'putting %s, %s' % (val, count)
-                # if not ret:
-                #     kh_del_pymap(self.table, k)
-
-                self.table.vals[k] = count
-                reverse[count] = val
-                labels[i] = count
-                counts[count] = 1
-                count += 1
-
-        return reverse, labels, counts[:count].copy()
-
-def lookup_locations2(ndarray[object] values):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        int ret
-        object val
-        khiter_t k
-        long hval
-        ndarray[int32_t] locs = np.empty(n, dtype='i4')
-
-    # for i in range(n):
-    #     val = values[i]
-        # hval = PyObject_Hash(val)
-        # k = kh_get_pymap(self.table, <PyObject*>val)
-
-    return locs
-
 
 from skiplist cimport *
 
@@ -689,3 +174,5 @@ def roll_median(ndarray[float64_t] arg, int win, int minp):
 
     return output
 
+include "hashtable.pyx"
+include "join.pyx"
diff --git a/pandas/src/tseries.pyx b/pandas/src/tseries.pyx
index 4875215d7..04369c8ef 100644
--- a/pandas/src/tseries.pyx
+++ b/pandas/src/tseries.pyx
@@ -129,21 +129,6 @@ cdef class MultiMap:
         raise KeyError(key)
 
 
-def isAllDates(ndarray[object, ndim=1] arr):
-    cdef int i, size = len(arr)
-    cdef object date
-
-    if size == 0:
-        return False
-
-    for i from 0 <= i < size:
-        date = arr[i]
-
-        if not PyDateTime_Check(date):
-            return False
-
-    return True
-
 def ismember(ndarray arr, set values):
     '''
     Checks whether
@@ -468,10 +453,6 @@ def fast_zip(list ndarrays):
 
     return result
 
-cpdef is_array(object o):
-    return np.PyArray_Check(o)
-
-
 # cdef class TypeConverter:
 #     cdef:
 #         cpython.PyTypeObject* klass_type
diff --git a/pandas/tools/merge.py b/pandas/tools/merge.py
new file mode 100644
index 000000000..44256b5c2
--- /dev/null
+++ b/pandas/tools/merge.py
@@ -0,0 +1,135 @@
+"""
+SQL-style merge routines
+"""
+
+import numpy as np
+
+from pandas.core.frame import DataFrame
+from pandas.core.index import Index
+from pandas.core.internals import _JoinOperation
+
+import pandas._tseries as lib
+from pandas._sandbox import Factorizer
+
+def merge(left, right, how='inner', cols=None, left_cols=None, right_cols=None,
+          left_index=False, right_index=False, sort=True,
+          suffixes=('.x', '.y'), copy=True):
+    """
+    Merge DataFrame objects by performing a database-style join operation by
+    columns or indexes
+
+    Parameters
+    ----------
+    left : DataFrame
+    right : DataFrame
+    how : {'left', 'right', 'outer', 'inner'}
+        How to handle indexes of the two objects. Default: 'left'
+        for joining on index, None otherwise
+        * left: use only keys from left frame
+        * right: use only keys from right frame
+        * outer: use union of keys from both frames
+        * inner: use intersection of keys from both frames
+    cols
+    left_cols
+    right_cols
+    left_index
+    right_index
+    sort
+    suffixes
+    copy : boolean, default True
+        If False, do not copy data unnecessarily
+
+    Examples
+    --------
+
+    Returns
+    -------
+    merged : DataFrame
+    """
+    left_join_keys, right_join_keys = _get_merge_keys(left, right, cols,
+                                                      left_cols, right_cols,
+                                                      left_index, right_index)
+
+    # max groups = largest possible number of distinct groups
+    left_key, right_key, max_groups = _get_group_keys(left_join_keys,
+                                                      right_join_keys)
+
+    join_func = _join_functions[how]
+    left_indexer, right_indexer = join_func(left_key, right_key, max_groups)
+    new_axis = Index(np.arange(len(left_indexer)))
+
+    join_op = _JoinOperation(left, right, new_axis, left_indexer,
+                             right_indexer, axis=1)
+    result_data = join_op.get_result(copy=copy)
+    return DataFrame(result_data)
+
+class _MergeOperation(object):
+
+    def __init__(self, left, right, how='inner', cols=None,
+                 left_cols=None, right_cols=None,
+                 left_index=False, right_index=False, sort=True,
+                 suffixes=('.x', '.y'), copy=True):
+        pass
+
+def _get_merge_keys(left, right, cols, left_cols, right_cols,
+                    left_index=False, right_index=False):
+    """
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
+    """
+    if on is None:
+        pass
+    else:
+        pass
+
+def _get_group_keys(left_keys, right_keys):
+    """
+
+    Parameters
+    ----------
+
+    Returns
+    -------
+
+    """
+    from pandas.core.groupby import get_group_index
+
+    assert(len(left_keys) == len(right_keys))
+
+    left_labels = []
+    right_labels = []
+    group_sizes = []
+
+    for lk, rk in zip(left_keys, right_keys):
+        rizer = Factorizer(max(len(lk), len(rk)))
+
+        llab, _ = rizer.factorize(lk.astype('O'))
+        rlab, _ = rizer.factorize(rk.astype('O'))
+
+        left_labels.append(llab)
+        right_labels.append(rlab)
+        group_sizes.append(rizer.get_count())
+
+    left_group_key = get_group_index(left_labels, group_sizes)
+    right_group_key = get_group_index(right_labels, group_sizes)
+    max_groups = np.prod(group_sizes)
+
+    return left_group_key, right_group_key, max_groups
+
+import pandas._sandbox as sbx
+
+def _right_outer_join(x, y):
+    right_indexer, left_indexer = sbx.left_outer_join(y, x)
+    return left_indexer, right_indexer
+
+_join_functions = {
+    'inner' : sbx.inner_join,
+    'left' : sbx.left_outer_join,
+    'right' : _right_outer_join,
+    'outer' : sbx.full_outer_join,
+}
diff --git a/pandas/tools/tests/test_merge.py b/pandas/tools/tests/test_merge.py
new file mode 100644
index 000000000..40aa0646e
--- /dev/null
+++ b/pandas/tools/tests/test_merge.py
@@ -0,0 +1,98 @@
+import nose
+import unittest
+
+import numpy as np
+
+from pandas.tools.merge import merge
+import pandas._sandbox as sbx
+
+a_ = np.array
+
+class TestMerge(unittest.TestCase):
+
+    def setUp(self):
+        pass
+
+    def test_cython_left_outer_join(self):
+        left = a_([0, 1, 2, 1, 2, 0, 0, 1, 2, 3, 3], dtype='i4')
+        right = a_([1, 1, 0, 4, 2, 2, 1], dtype='i4')
+        max_group = 5
+
+        ls, li, rs, ri = sbx.left_outer_join(left, right, max_group)
+
+        exp_ls = left.argsort(kind='mergesort')
+        exp_rs = right.argsort(kind='mergesort')
+
+        exp_li = a_([0, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5,
+                     6, 6, 7, 7, 8, 8, 9, 10])
+        exp_ri = a_([0, 0, 0, 1, 2, 3, 1, 2, 3, 1, 2, 3,
+                     4, 5, 4, 5, 4, 5, -1, -1])
+
+        self.assert_(np.array_equal(ls, exp_ls))
+        self.assert_(np.array_equal(rs, exp_rs))
+        self.assert_(np.array_equal(li, exp_li))
+        self.assert_(np.array_equal(ri, exp_ri))
+
+    def test_cython_right_outer_join(self):
+        left = a_([0, 1, 2, 1, 2, 0, 0, 1, 2, 3, 3], dtype='i4')
+        right = a_([1, 1, 0, 4, 2, 2, 1], dtype='i4')
+        max_group = 5
+
+        rs, ri, ls, li  = sbx.left_outer_join(right, left, max_group)
+
+        exp_ls = left.argsort(kind='mergesort')
+        exp_rs = right.argsort(kind='mergesort')
+
+        #            0        1        1        1
+        exp_li = a_([0, 1, 2, 3, 4, 5, 3, 4, 5, 3, 4, 5,
+#                    2        2        4
+                     6, 7, 8, 6, 7, 8, -1])
+        exp_ri = a_([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3,
+                     4, 4, 4, 5, 5, 5, 6])
+
+        self.assert_(np.array_equal(ls, exp_ls))
+        self.assert_(np.array_equal(rs, exp_rs))
+        self.assert_(np.array_equal(li, exp_li))
+        self.assert_(np.array_equal(ri, exp_ri))
+
+    def test_cython_inner_join(self):
+        raise nose.SkipTest
+
+        left = a_([0, 1, 2, 1, 2, 0, 0, 1, 2, 3, 3], dtype='i4')
+        right = a_([1, 1, 0, 4, 2, 2, 1, 4], dtype='i4')
+        max_group = 5
+
+        ls, li, rs, ri = sbx.inner_join(left, right, max_group)
+
+        exp_ls = left.argsort(kind='mergesort')
+        exp_rs = right.argsort(kind='mergesort')
+
+        exp_li = a_([0, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5,
+                     6, 6, 7, 7, 8, 8])
+        exp_ri = a_([0, 0, 0, 1, 2, 3, 1, 2, 3, 1, 2, 3,
+                     4, 5, 4, 5, 4, 5])
+
+        self.assert_(np.array_equal(ls, exp_ls))
+        self.assert_(np.array_equal(rs, exp_rs))
+        self.assert_(np.array_equal(li, exp_li))
+        self.assert_(np.array_equal(ri, exp_ri))
+
+    def test_cython_full_outer_join(self):
+        pass
+
+    def test_left_join(self):
+        pass
+
+    def test_merge_common(self):
+        pass
+
+    def test_merge_index(self):
+        pass
+
+if __name__ == '__main__':
+    import nose
+    nose.runmodule(argv=[__file__,'-vvs','-x','--pdb', '--pdb-failure'],
+                   exit=False)
+
+
+
