commit 7b4492de9d7f4e3e09fa454fae3353ef5ab16f47
Author: Wes McKinney <wesmckinn@gmail.com>
Date:   Fri Nov 30 12:01:03 2012 -0500

    REF: reorganizing cython code. sharing ext types still not working

diff --git a/pandas/core/index.py b/pandas/core/index.py
index 02580d2e1..fb68d1505 100644
--- a/pandas/core/index.py
+++ b/pandas/core/index.py
@@ -14,6 +14,7 @@ import pandas._algos as _algos
 from pandas.lib import Timestamp
 from pandas.util import py3compat
 from pandas.core.config import get_option
+import pandas._hash as _hash
 
 __all__ = ['Index']
 
@@ -78,7 +79,7 @@ class Index(np.ndarray):
     name = None
     asi8 = None
 
-    _engine_type = lib.ObjectEngine
+    _engine_type = _hash.ObjectEngine
 
     def __new__(cls, data, dtype=None, copy=False, name=None):
         if isinstance(data, np.ndarray):
@@ -1201,7 +1202,7 @@ class Int64Index(Index):
     _inner_indexer = _algos.inner_join_indexer_int64
     _outer_indexer = _algos.outer_join_indexer_int64
 
-    _engine_type = lib.Int64Engine
+    _engine_type = _hash.Int64Engine
 
     def __new__(cls, data, dtype=None, copy=False, name=None):
         if not isinstance(data, np.ndarray):
diff --git a/pandas/src/datetime.pxd b/pandas/src/datetime.pxd
index 09073dcd4..1a977aab4 100644
--- a/pandas/src/datetime.pxd
+++ b/pandas/src/datetime.pxd
@@ -4,7 +4,7 @@ from cpython cimport PyObject
 from cpython cimport PyUnicode_Check, PyUnicode_AsASCIIString
 
 
-cdef extern from "stdint.h":
+cdef extern from "headers/stdint.h":
     enum: INT64_MIN
     enum: INT32_MIN
 
@@ -116,6 +116,9 @@ cdef extern from "datetime/np_datetime_strings.h":
 
     # int parse_python_string(object obj, pandas_datetimestruct *out) except -1
 
+
+
+
 cdef inline _string_to_dts(object val, pandas_datetimestruct* dts):
     cdef int result
     cdef char *tmp
@@ -140,3 +143,47 @@ cdef inline int _cstring_to_dts(char *val, int length,
                                      NPY_UNSAFE_CASTING,
                                      dts, &islocal, &out_bestunit, &special)
     return result
+
+
+cdef inline object _datetime64_to_datetime(int64_t val):
+    cdef pandas_datetimestruct dts
+    pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
+    return _dts_to_pydatetime(&dts)
+
+cdef inline object _dts_to_pydatetime(pandas_datetimestruct *dts):
+    return <object> PyDateTime_FromDateAndTime(dts.year, dts.month,
+                                               dts.day, dts.hour,
+                                               dts.min, dts.sec, dts.us)
+
+cdef inline int64_t _pydatetime_to_dts(object val, pandas_datetimestruct *dts):
+    dts.year = PyDateTime_GET_YEAR(val)
+    dts.month = PyDateTime_GET_MONTH(val)
+    dts.day = PyDateTime_GET_DAY(val)
+    dts.hour = PyDateTime_DATE_GET_HOUR(val)
+    dts.min = PyDateTime_DATE_GET_MINUTE(val)
+    dts.sec = PyDateTime_DATE_GET_SECOND(val)
+    dts.us = PyDateTime_DATE_GET_MICROSECOND(val)
+    dts.ps = dts.as = 0
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
+
+cdef inline int64_t _dtlike_to_datetime64(object val,
+                                          pandas_datetimestruct *dts):
+    dts.year = val.year
+    dts.month = val.month
+    dts.day = val.day
+    dts.hour = val.hour
+    dts.min = val.minute
+    dts.sec = val.second
+    dts.us = val.microsecond
+    dts.ps = dts.as = 0
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
+
+cdef inline int64_t _date_to_datetime64(object val,
+                                        pandas_datetimestruct *dts):
+    dts.year = PyDateTime_GET_YEAR(val)
+    dts.month = PyDateTime_GET_MONTH(val)
+    dts.day = PyDateTime_GET_DAY(val)
+    dts.hour = dts.min = dts.sec = dts.us = 0
+    dts.ps = dts.as = 0
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
+
diff --git a/pandas/src/datetime.pyx b/pandas/src/datetime.pyx
deleted file mode 100644
index ed9561492..000000000
--- a/pandas/src/datetime.pyx
+++ /dev/null
@@ -1,2004 +0,0 @@
-# cython: profile=False
-cimport numpy as np
-import numpy as np
-
-from numpy cimport int32_t, int64_t, import_array, ndarray
-from cpython cimport *
-
-from libc.stdlib cimport free
-# this is our datetime.pxd
-from datetime cimport *
-from util cimport is_integer_object, is_datetime64_object
-
-from datetime import timedelta
-from dateutil.parser import parse as parse_date
-cimport util
-
-from khash cimport *
-import cython
-
-# initialize numpy
-import_array()
-#import_ufunc()
-
-# import datetime C API
-PyDateTime_IMPORT
-
-# in numpy 1.7, will prob need the following:
-# numpy_pydatetime_import
-
-try:
-    basestring
-except NameError: # py3
-    basestring = str
-
-def ints_to_pydatetime(ndarray[int64_t] arr, tz=None):
-    cdef:
-        Py_ssize_t i, n = len(arr)
-        pandas_datetimestruct dts
-        ndarray[object] result = np.empty(n, dtype=object)
-
-    if tz is not None:
-        if _is_utc(tz):
-            for i in range(n):
-                pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
-                result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
-                                     dts.min, dts.sec, dts.us, tz)
-        elif _is_tzlocal(tz):
-            for i in range(n):
-                pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
-                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                              dts.min, dts.sec, dts.us, tz)
-                result[i] = dt + tz.utcoffset(dt)
-        else:
-            trans = _get_transitions(tz)
-            deltas = _get_deltas(tz)
-            for i in range(n):
-                # Adjust datetime64 timestamp, recompute datetimestruct
-                pos = trans.searchsorted(arr[i]) - 1
-                inf = tz._transition_info[pos]
-
-                pandas_datetime_to_datetimestruct(arr[i] + deltas[pos],
-                                                  PANDAS_FR_ns, &dts)
-                result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
-                                     dts.min, dts.sec, dts.us,
-                                     tz._tzinfos[inf])
-    else:
-        for i in range(n):
-            pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
-            result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
-                                 dts.min, dts.sec, dts.us)
-
-    return result
-
-from dateutil.tz import tzlocal
-
-def _is_tzlocal(tz):
-    return isinstance(tz, tzlocal)
-
-# Python front end to C extension type _Timestamp
-# This serves as the box for datetime64
-class Timestamp(_Timestamp):
-
-    def __new__(cls, object ts_input, object offset=None, tz=None):
-        cdef _TSObject ts
-        cdef _Timestamp ts_base
-
-        if isinstance(ts_input, float):
-            # to do, do we want to support this, ie with fractional seconds?
-            raise TypeError("Cannot convert a float to datetime")
-
-        if isinstance(ts_input, basestring):
-            try:
-                ts_input = parse_date(ts_input)
-            except Exception:
-                pass
-
-        ts = convert_to_tsobject(ts_input, tz)
-
-        if ts.value == NPY_NAT:
-            return NaT
-
-        # make datetime happy
-        ts_base = _Timestamp.__new__(cls, ts.dts.year, ts.dts.month,
-                                     ts.dts.day, ts.dts.hour, ts.dts.min,
-                                     ts.dts.sec, ts.dts.us, ts.tzinfo)
-
-        # fill out rest of data
-        ts_base.value = ts.value
-        ts_base.offset = offset
-        ts_base.nanosecond = ts.dts.ps / 1000
-
-        return ts_base
-
-    def __repr__(self):
-        result = self._repr_base
-
-        try:
-            result += self.strftime('%z')
-            if self.tzinfo:
-                zone = _get_zone(self.tzinfo)
-                result += self.strftime(' %%Z, tz=%s' % zone)
-        except ValueError:
-            year2000 = self.replace(year=2000)
-            result += year2000.strftime('%z')
-            if self.tzinfo:
-                zone = _get_zone(self.tzinfo)
-                result += year2000.strftime(' %%Z, tz=%s' % zone)
-
-        return '<Timestamp: %s>' % result
-
-    @property
-    def _repr_base(self):
-        result = '%d-%.2d-%.2d %.2d:%.2d:%.2d' % (self.year, self.month,
-                                                  self.day, self.hour,
-                                                  self.minute, self.second)
-
-        if self.nanosecond != 0:
-            nanos = self.nanosecond + 1000 * self.microsecond
-            result += '.%.9d' % nanos
-        elif self.microsecond != 0:
-            result += '.%.6d' % self.microsecond
-
-        return result
-
-    @property
-    def tz(self):
-        """
-        Alias for tzinfo
-        """
-        return self.tzinfo
-
-    @property
-    def freq(self):
-        return self.offset
-
-    def __setstate__(self, state):
-        self.value = state[0]
-        self.offset = state[1]
-        self.tzinfo = state[2]
-
-    def __reduce__(self):
-        object_state = self.value, self.offset, self.tzinfo
-        return (Timestamp, object_state)
-
-    def to_period(self, freq=None):
-        """
-        Return an period of which this timestamp is an observation.
-        """
-        from pandas.tseries.period import Period
-
-        if freq is None:
-            freq = self.freq
-
-        return Period(self, freq=freq)
-
-    @property
-    def dayofweek(self):
-        return self.weekday()
-
-    @property
-    def dayofyear(self):
-        return self._get_field('doy')
-
-    @property
-    def week(self):
-        return self._get_field('woy')
-
-    weekofyear = week
-
-    @property
-    def quarter(self):
-        return self._get_field('q')
-
-    @property
-    def freqstr(self):
-        return getattr(self.offset, 'freqstr', self.offset)
-
-    @property
-    def asm8(self):
-        return np.int64(self.value).view('M8[ns]')
-
-    def tz_localize(self, tz):
-        """
-        Convert naive Timestamp to local time zone
-
-        Parameters
-        ----------
-        tz : pytz.timezone
-
-        Returns
-        -------
-        localized : Timestamp
-        """
-        if self.tzinfo is None:
-            # tz naive, localize
-            return Timestamp(self.to_pydatetime(), tz=tz)
-        else:
-            raise Exception('Cannot localize tz-aware Timestamp, use '
-                            'tz_convert for conversions')
-
-    def tz_convert(self, tz):
-        """
-        Convert Timestamp to another time zone or localize to requested time
-        zone
-
-        Parameters
-        ----------
-        tz : pytz.timezone
-
-        Returns
-        -------
-        converted : Timestamp
-        """
-        if self.tzinfo is None:
-            # tz naive, use tz_localize
-            raise Exception('Cannot convert tz-naive Timestamp, use '
-                            'tz_localize to localize')
-        else:
-            # Same UTC timestamp, different time zone
-            return Timestamp(self.value, tz=tz)
-
-    astimezone = tz_convert
-
-    def replace(self, **kwds):
-        return Timestamp(datetime.replace(self, **kwds),
-                         offset=self.offset)
-
-    def to_pydatetime(self, warn=True):
-        """
-        If warn=True, issue warning if nanoseconds is nonzero
-        """
-        cdef:
-            pandas_datetimestruct dts
-            _TSObject ts
-
-        if self.nanosecond != 0 and warn:
-            print 'Warning: discarding nonzero nanoseconds'
-        ts = convert_to_tsobject(self, self.tzinfo)
-
-        return datetime(ts.dts.year, ts.dts.month, ts.dts.day,
-                        ts.dts.hour, ts.dts.min, ts.dts.sec,
-                        ts.dts.us, ts.tzinfo)
-
-
-class NaTType(_NaT):
-
-    def __new__(cls):
-        cdef _NaT base
-
-        base = _NaT.__new__(cls, 1, 1, 1)
-        mangle_nat(base)
-        base.value = NPY_NAT
-
-        return base
-
-    def __repr__(self):
-        return 'NaT'
-
-    def weekday(self):
-        return -1
-
-    def toordinal(self):
-        return -1
-
-fields = ['year', 'quarter', 'month', 'day', 'hour',
-          'minute', 'second', 'microsecond', 'nanosecond',
-          'week', 'dayofyear']
-for field in fields:
-    prop = property(fget=lambda self: -1)
-    setattr(NaTType, field, prop)
-
-
-NaT = NaTType()
-
-iNaT = util.get_nat()
-
-
-cdef inline bint is_timestamp(object o):
-    return isinstance(o, Timestamp)
-
-def is_timestamp_array(ndarray[object] values):
-    cdef int i, n = len(values)
-    if n == 0:
-        return False
-    for i in range(n):
-        if not is_timestamp(values[i]):
-            return False
-    return True
-
-
-cpdef object get_value_box(ndarray arr, object loc):
-    cdef:
-        Py_ssize_t i, sz
-        void* data_ptr
-    if util.is_float_object(loc):
-        casted = int(loc)
-        if casted == loc:
-            loc = casted
-    i = <Py_ssize_t> loc
-    sz = cnp.PyArray_SIZE(arr)
-
-    if i < 0 and sz > 0:
-        i += sz
-    elif i >= sz or sz == 0:
-        raise IndexError('index out of bounds')
-
-    if arr.descr.type_num == NPY_DATETIME:
-        return Timestamp(util.get_value_1d(arr, i))
-    else:
-        return util.get_value_1d(arr, i)
-
-
-#----------------------------------------------------------------------
-# Frequency inference
-
-def unique_deltas(ndarray[int64_t] arr):
-    cdef:
-        Py_ssize_t i, n = len(arr)
-        int64_t val
-        khiter_t k
-        kh_int64_t *table
-        int ret = 0
-        list uniques = []
-
-    table = kh_init_int64()
-    kh_resize_int64(table, 10)
-    for i in range(n - 1):
-        val = arr[i + 1] - arr[i]
-        k = kh_get_int64(table, val)
-        if k == table.n_buckets:
-            kh_put_int64(table, val, &ret)
-            uniques.append(val)
-    kh_destroy_int64(table)
-
-    result = np.array(uniques, dtype=np.int64)
-    result.sort()
-    return result
-
-
-cdef inline bint _is_multiple(int64_t us, int64_t mult):
-    return us % mult == 0
-
-
-def apply_offset(ndarray[object] values, object offset):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[int64_t] new_values
-        object boxed
-
-    result = np.empty(n, dtype='M8[ns]')
-    new_values = result.view('i8')
-    pass
-
-
-# This is PITA. Because we inherit from datetime, which has very specific
-# construction requirements, we need to do object instantiation in python
-# (see Timestamp class above). This will serve as a C extension type that
-# shadows the python class, where we do any heavy lifting.
-cdef class _Timestamp(datetime):
-    cdef readonly:
-        int64_t value, nanosecond
-        object offset       # frequency reference
-
-    def __hash__(self):
-        if self.nanosecond:
-            return hash(self.value)
-        else:
-            return datetime.__hash__(self)
-
-    def __richcmp__(_Timestamp self, object other, int op):
-        cdef _Timestamp ots
-
-        if isinstance(other, _Timestamp):
-            ots = other
-        elif type(other) is datetime:
-            if self.nanosecond == 0:
-                val = self.to_datetime()
-                return PyObject_RichCompareBool(val, other, op)
-
-            try:
-                ots = Timestamp(other)
-            except ValueError:
-                return self._compare_outside_nanorange(other, op)
-        else:
-            if op == 2:
-                return False
-            elif op == 3:
-                return True
-            else:
-                raise TypeError('Cannot compare Timestamp with %s' % str(other))
-
-        self._assert_tzawareness_compat(other)
-
-        if op == 2: # ==
-            return self.value == ots.value
-        elif op == 3: # !=
-            return self.value != ots.value
-        elif op == 0: # <
-            return self.value < ots.value
-        elif op == 1: # <=
-            return self.value <= ots.value
-        elif op == 4: # >
-            return self.value > ots.value
-        elif op == 5: # >=
-            return self.value >= ots.value
-
-    cdef _compare_outside_nanorange(self, object other, int op):
-        dtval = self.to_datetime()
-
-        self._assert_tzawareness_compat(other)
-
-        if self.nanosecond == 0:
-            if op == 2: # ==
-                return dtval == other
-            elif op == 3: # !=
-                return dtval != other
-            elif op == 0: # <
-                return dtval < other
-            elif op == 1: # <=
-                return dtval <= other
-            elif op == 4: # >
-                return dtval > other
-            elif op == 5: # >=
-                return dtval >= other
-        else:
-            if op == 2: # ==
-                return False
-            elif op == 3: # !=
-                return True
-            elif op == 0: # <
-                return dtval < other
-            elif op == 1: # <=
-                return dtval < other
-            elif op == 4: # >
-                return dtval >= other
-            elif op == 5: # >=
-                return dtval >= other
-
-    cdef _assert_tzawareness_compat(self, object other):
-        if self.tzinfo is None:
-            if other.tzinfo is not None:
-                raise Exception('Cannot compare tz-naive and '
-                                'tz-aware timestamps')
-        elif other.tzinfo is None:
-            raise Exception('Cannot compare tz-naive and tz-aware timestamps')
-
-    cpdef to_datetime(self):
-        cdef:
-            pandas_datetimestruct dts
-            _TSObject ts
-        ts = convert_to_tsobject(self, self.tzinfo)
-        dts = ts.dts
-        return datetime(dts.year, dts.month, dts.day,
-                        dts.hour, dts.min, dts.sec,
-                        dts.us, ts.tzinfo)
-
-    def __add__(self, other):
-        if is_integer_object(other):
-            if self.offset is None:
-                msg = ("Cannot add integral value to Timestamp "
-                       "without offset.")
-                raise ValueError(msg)
-            else:
-                return Timestamp((self.offset.__mul__(other)).apply(self))
-        else:
-            if isinstance(other, timedelta) or hasattr(other, 'delta'):
-                nanos = _delta_to_nanoseconds(other)
-                return Timestamp(self.value + nanos, tz=self.tzinfo)
-            else:
-                result = datetime.__add__(self, other)
-                if isinstance(result, datetime):
-                    result = Timestamp(result)
-                    result.nanosecond = self.nanosecond
-                return result
-
-    def __sub__(self, other):
-        if is_integer_object(other):
-            return self.__add__(-other)
-        else:
-            return datetime.__sub__(self, other)
-
-    cpdef _get_field(self, field):
-        out = get_date_field(np.array([self.value], dtype=np.int64), field)
-        return out[0]
-
-
-cdef class _NaT(_Timestamp):
-
-    def __richcmp__(_NaT self, object other, int op):
-        # if not isinstance(other, (_NaT, _Timestamp)):
-        #     raise TypeError('Cannot compare %s with NaT' % type(other))
-
-        if op == 2: # ==
-            return False
-        elif op == 3: # !=
-            return True
-        elif op == 0: # <
-            return False
-        elif op == 1: # <=
-            return False
-        elif op == 4: # >
-            return False
-        elif op == 5: # >=
-            return False
-
-
-
-
-def _delta_to_nanoseconds(delta):
-    try:
-        delta = delta.delta
-    except:
-        pass
-    return (delta.days * 24 * 60 * 60 * 1000000
-            + delta.seconds * 1000000
-            + delta.microseconds) * 1000
-
-
-# lightweight C object to hold datetime & int64 pair
-cdef class _TSObject:
-    cdef:
-        pandas_datetimestruct dts      # pandas_datetimestruct
-        int64_t value               # numpy dt64
-        object tzinfo
-
-    property value:
-        def __get__(self):
-            return self.value
-
-cpdef _get_utcoffset(tzinfo, obj):
-    try:
-        return tzinfo._utcoffset
-    except AttributeError:
-        return tzinfo.utcoffset(obj)
-
-# helper to extract datetime and int64 from several different possibilities
-cpdef convert_to_tsobject(object ts, object tz=None):
-    """
-    Extract datetime and int64 from any of:
-        - np.int64
-        - np.datetime64
-        - python int or long object
-        - iso8601 string object
-        - python datetime object
-        - another timestamp object
-    """
-    cdef:
-        _TSObject obj
-        bint utc_convert = 1
-
-    if tz is not None:
-        if isinstance(tz, basestring):
-            tz = pytz.timezone(tz)
-
-    obj = _TSObject()
-
-    if is_datetime64_object(ts):
-        obj.value = _get_datetime64_nanos(ts)
-        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns, &obj.dts)
-    elif is_integer_object(ts):
-        obj.value = ts
-        pandas_datetime_to_datetimestruct(ts, PANDAS_FR_ns, &obj.dts)
-    elif util.is_string_object(ts):
-        _string_to_dts(ts, &obj.dts)
-        obj.value = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &obj.dts)
-    elif PyDateTime_Check(ts):
-        if tz is not None:
-            # sort of a temporary hack
-            if ts.tzinfo is not None:
-                ts = tz.normalize(ts)
-                obj.value = _pydatetime_to_dts(ts, &obj.dts)
-                obj.tzinfo = ts.tzinfo
-            elif not _is_utc(tz):
-                ts = tz.localize(ts)
-                obj.value = _pydatetime_to_dts(ts, &obj.dts)
-                offset = _get_utcoffset(ts.tzinfo, ts)
-                obj.value -= _delta_to_nanoseconds(offset)
-                obj.tzinfo = ts.tzinfo
-            else:
-                # UTC
-                obj.value = _pydatetime_to_dts(ts, &obj.dts)
-                obj.tzinfo = pytz.utc
-        else:
-            obj.value = _pydatetime_to_dts(ts, &obj.dts)
-            obj.tzinfo = ts.tzinfo
-            if obj.tzinfo is not None and not _is_utc(obj.tzinfo):
-                offset = _get_utcoffset(obj.tzinfo, ts)
-                obj.value -= _delta_to_nanoseconds(offset)
-
-        if is_timestamp(ts):
-            obj.value += ts.nanosecond
-        _check_dts_bounds(obj.value, &obj.dts)
-        return obj
-    elif PyDate_Check(ts):
-        obj.value  = _date_to_datetime64(ts, &obj.dts)
-    else:
-        raise ValueError("Could not construct Timestamp from argument %s" %
-                         type(ts))
-
-    if obj.value != NPY_NAT:
-        _check_dts_bounds(obj.value, &obj.dts)
-
-    if tz is not None:
-        _localize_tso(obj, tz)
-
-    return obj
-
-cdef inline void _localize_tso(_TSObject obj, object tz):
-    if _is_utc(tz):
-        obj.tzinfo = tz
-    elif _is_tzlocal(tz):
-        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns, &obj.dts)
-        dt = datetime(obj.dts.year, obj.dts.month, obj.dts.day, obj.dts.hour,
-                      obj.dts.min, obj.dts.sec, obj.dts.us, tz)
-        delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
-        pandas_datetime_to_datetimestruct(obj.value + delta,
-                                          PANDAS_FR_ns, &obj.dts)
-        obj.tzinfo = tz
-    else:
-        # Adjust datetime64 timestamp, recompute datetimestruct
-        trans = _get_transitions(tz)
-        deltas = _get_deltas(tz)
-        pos = trans.searchsorted(obj.value, side='right') - 1
-
-        # statictzinfo
-        if not hasattr(tz, '_transition_info'):
-            pandas_datetime_to_datetimestruct(obj.value + deltas[0],
-                                              PANDAS_FR_ns, &obj.dts)
-            obj.tzinfo = tz
-        else:
-            inf = tz._transition_info[pos]
-            pandas_datetime_to_datetimestruct(obj.value + deltas[pos],
-                                              PANDAS_FR_ns, &obj.dts)
-            obj.tzinfo = tz._tzinfos[inf]
-
-
-def get_timezone(tz):
-    return _get_zone(tz)
-
-cdef inline bint _is_utc(object tz):
-    return tz is UTC or isinstance(tz, _du_utc)
-
-cdef inline object _get_zone(object tz):
-    if _is_utc(tz):
-        return 'UTC'
-    else:
-        try:
-            return tz.zone
-        except AttributeError:
-            return tz
-
-# cdef int64_t _NS_LOWER_BOUND = -9223285636854775809LL
-# cdef int64_t _NS_UPPER_BOUND = -9223372036854775807LL
-
-cdef inline _check_dts_bounds(int64_t value, pandas_datetimestruct *dts):
-    cdef pandas_datetimestruct dts2
-    if dts.year <= 1677 or dts.year >= 2262:
-        pandas_datetime_to_datetimestruct(value, PANDAS_FR_ns, &dts2)
-        if dts2.year != dts.year:
-            fmt = '%d-%.2d-%.2d %.2d:%.2d:%.2d' % (dts.year, dts.month,
-                                                   dts.day, dts.hour,
-                                                   dts.min, dts.sec)
-
-            raise ValueError('Out of bounds nanosecond timestamp: %s' % fmt)
-
-# elif isinstance(ts, _Timestamp):
-#     tmp = ts
-#     obj.value = (<_Timestamp> ts).value
-#     obj.dtval =
-# elif isinstance(ts, object):
-#     # If all else fails
-#     obj.value = _dtlike_to_datetime64(ts, &obj.dts)
-#     obj.dtval = _dts_to_pydatetime(&obj.dts)
-
-cdef inline object _datetime64_to_datetime(int64_t val):
-    cdef pandas_datetimestruct dts
-    pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
-    return _dts_to_pydatetime(&dts)
-
-cdef inline object _dts_to_pydatetime(pandas_datetimestruct *dts):
-    return <object> PyDateTime_FromDateAndTime(dts.year, dts.month,
-                                               dts.day, dts.hour,
-                                               dts.min, dts.sec, dts.us)
-
-cdef inline int64_t _pydatetime_to_dts(object val, pandas_datetimestruct *dts):
-    dts.year = PyDateTime_GET_YEAR(val)
-    dts.month = PyDateTime_GET_MONTH(val)
-    dts.day = PyDateTime_GET_DAY(val)
-    dts.hour = PyDateTime_DATE_GET_HOUR(val)
-    dts.min = PyDateTime_DATE_GET_MINUTE(val)
-    dts.sec = PyDateTime_DATE_GET_SECOND(val)
-    dts.us = PyDateTime_DATE_GET_MICROSECOND(val)
-    dts.ps = dts.as = 0
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
-
-cdef inline int64_t _dtlike_to_datetime64(object val,
-                                          pandas_datetimestruct *dts):
-    dts.year = val.year
-    dts.month = val.month
-    dts.day = val.day
-    dts.hour = val.hour
-    dts.min = val.minute
-    dts.sec = val.second
-    dts.us = val.microsecond
-    dts.ps = dts.as = 0
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
-
-cdef inline int64_t _date_to_datetime64(object val,
-                                        pandas_datetimestruct *dts):
-    dts.year = PyDateTime_GET_YEAR(val)
-    dts.month = PyDateTime_GET_MONTH(val)
-    dts.day = PyDateTime_GET_DAY(val)
-    dts.hour = dts.min = dts.sec = dts.us = 0
-    dts.ps = dts.as = 0
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
-
-
-def datetime_to_datetime64(ndarray[object] values):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        object val, inferred_tz = None
-        ndarray[int64_t] iresult
-        pandas_datetimestruct dts
-        _TSObject _ts
-
-    result = np.empty(n, dtype='M8[ns]')
-    iresult = result.view('i8')
-    for i in range(n):
-        val = values[i]
-        if util._checknull(val):
-            iresult[i] = iNaT
-        elif PyDateTime_Check(val):
-            if val.tzinfo is not None:
-                if inferred_tz is not None:
-                    if _get_zone(val.tzinfo) != inferred_tz:
-                        raise ValueError('Array must be all same time zone')
-                else:
-                    inferred_tz = _get_zone(val.tzinfo)
-
-                _ts = convert_to_tsobject(val)
-                iresult[i] = _ts.value
-                _check_dts_bounds(iresult[i], &_ts.dts)
-            else:
-                if inferred_tz is not None:
-                    raise ValueError('Cannot mix tz-aware with tz-naive values')
-                iresult[i] = _pydatetime_to_dts(val, &dts)
-                _check_dts_bounds(iresult[i], &dts)
-        else:
-            raise TypeError('Unrecognized value type: %s' % type(val))
-
-    return result, inferred_tz
-
-
-def array_to_datetime(ndarray[object] values, raise_=False, dayfirst=False,
-                      format=None, utc=None):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        object val
-        ndarray[int64_t] iresult
-        ndarray[object] oresult
-        pandas_datetimestruct dts
-        bint utc_convert = bool(utc)
-        _TSObject _ts
-
-    from dateutil.parser import parse
-
-    try:
-        result = np.empty(n, dtype='M8[ns]')
-        iresult = result.view('i8')
-        for i in range(n):
-            val = values[i]
-            if util._checknull(val) or val is NaT:
-                iresult[i] = iNaT
-            elif PyDateTime_Check(val):
-                if val.tzinfo is not None:
-                    if utc_convert:
-                        _ts = convert_to_tsobject(val)
-                        iresult[i] = _ts.value
-                        _check_dts_bounds(iresult[i], &_ts.dts)
-                    else:
-                        raise ValueError('Tz-aware datetime.datetime cannot '
-                                         'be converted to datetime64 unless '
-                                         'utc=True')
-                else:
-                    iresult[i] = _pydatetime_to_dts(val, &dts)
-                    if isinstance(val, _Timestamp):
-                        iresult[i] += (<_Timestamp>val).nanosecond
-                    _check_dts_bounds(iresult[i], &dts)
-            elif PyDate_Check(val):
-                iresult[i] = _date_to_datetime64(val, &dts)
-                _check_dts_bounds(iresult[i], &dts)
-            elif util.is_datetime64_object(val):
-                iresult[i] = _get_datetime64_nanos(val)
-            elif util.is_integer_object(val):
-                iresult[i] = val
-            else:
-                if len(val) == 0:
-                    iresult[i] = iNaT
-                    continue
-
-                try:
-                    _string_to_dts(val, &dts)
-                    iresult[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_ns,
-                                                                   &dts)
-                    _check_dts_bounds(iresult[i], &dts)
-                except ValueError:
-                    try:
-                        result[i] = parse(val, dayfirst=dayfirst)
-                    except Exception:
-                        raise TypeError
-                    pandas_datetime_to_datetimestruct(iresult[i], PANDAS_FR_ns,
-                                                      &dts)
-                    _check_dts_bounds(iresult[i], &dts)
-        return result
-    except TypeError:
-        oresult = np.empty(n, dtype=object)
-
-        for i in range(n):
-            val = values[i]
-            if util._checknull(val):
-                oresult[i] = val
-            else:
-                if len(val) == 0:
-                    # TODO: ??
-                    oresult[i] = 'NaT'
-                    continue
-                try:
-                    oresult[i] = parse(val, dayfirst=dayfirst)
-                except Exception:
-                    if raise_:
-                        raise
-                    return values
-                    # oresult[i] = val
-
-        return oresult
-
-cdef inline _get_datetime64_nanos(object val):
-    cdef:
-        pandas_datetimestruct dts
-        PANDAS_DATETIMEUNIT unit
-        npy_datetime ival
-
-    unit = get_datetime64_unit(val)
-    if unit == 3:
-        raise ValueError('NumPy 1.6.1 business freq not supported')
-
-    ival = get_datetime64_value(val)
-
-    if unit != PANDAS_FR_ns:
-        pandas_datetime_to_datetimestruct(ival, unit, &dts)
-        return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
-    else:
-        return ival
-
-
-def cast_to_nanoseconds(ndarray arr):
-    cdef:
-        Py_ssize_t i, n = arr.size
-        ndarray[int64_t] ivalues, iresult
-        PANDAS_DATETIMEUNIT unit
-        pandas_datetimestruct dts
-
-    shape = (<object> arr).shape
-
-    ivalues = arr.view(np.int64).ravel()
-
-    result = np.empty(shape, dtype='M8[ns]')
-    iresult = result.ravel().view(np.int64)
-
-    unit = get_datetime64_unit(arr.flat[0])
-    if unit == 3:
-        raise ValueError('NumPy 1.6.1 business freq not supported')
-
-    for i in range(n):
-        pandas_datetime_to_datetimestruct(ivalues[i], unit, &dts)
-        iresult[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
-
-    return result
-
-#----------------------------------------------------------------------
-# Conversion routines
-
-
-def pydt_to_i8(object pydt):
-    '''
-    Convert to int64 representation compatible with numpy datetime64; converts
-    to UTC
-    '''
-    cdef:
-        _TSObject ts
-
-    ts = convert_to_tsobject(pydt)
-
-    return ts.value
-
-def i8_to_pydt(int64_t i8, object tzinfo = None):
-    '''
-    Inverse of pydt_to_i8
-    '''
-    return Timestamp(i8)
-
-#----------------------------------------------------------------------
-# time zone conversion helpers
-
-try:
-    from dateutil.tz import tzutc as _du_utc
-    import pytz
-    UTC = pytz.utc
-    have_pytz = True
-except:
-    have_pytz = False
-
-def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
-    cdef:
-        ndarray[int64_t] utc_dates, result, trans, deltas
-        Py_ssize_t i, pos, n = len(vals)
-        int64_t v, offset
-        pandas_datetimestruct dts
-
-    if not have_pytz:
-        import pytz
-
-    # Convert to UTC
-
-    if _get_zone(tz1) != 'UTC':
-        utc_dates = np.empty(n, dtype=np.int64)
-        if _is_tzlocal(tz1):
-            for i in range(n):
-                v = vals[i]
-                pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
-                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                              dts.min, dts.sec, dts.us, tz1)
-                delta = int(total_seconds(_get_utcoffset(tz1, dt))) * 1000000000
-                utc_dates[i] = v - delta
-        else:
-            deltas = _get_deltas(tz1)
-            trans = _get_transitions(tz1)
-            pos = trans.searchsorted(vals[0]) - 1
-            if pos < 0:
-                raise ValueError('First time before start of DST info')
-
-            offset = deltas[pos]
-            for i in range(n):
-                v = vals[i]
-                if v >= [pos + 1]:
-                    pos += 1
-                    offset = deltas[pos]
-                utc_dates[i] = v - offset
-    else:
-        utc_dates = vals
-
-    if _get_zone(tz2) == 'UTC':
-        return utc_dates
-
-    result = np.empty(n, dtype=np.int64)
-    if _is_tzlocal(tz2):
-        for i in range(n):
-            v = utc_dates[i]
-            pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                          dts.min, dts.sec, dts.us, tz2)
-            delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
-            result[i] = v + delta
-            return result
-
-    # Convert UTC to other timezone
-    trans = _get_transitions(tz2)
-    deltas = _get_deltas(tz2)
-    pos = trans.searchsorted(utc_dates[0])
-    if pos == 0:
-        raise ValueError('First time before start of DST info')
-    elif pos == len(trans):
-        return utc_dates + deltas[-1]
-
-    # TODO: this assumed sortedness :/
-    pos -= 1
-
-    offset = deltas[pos]
-    for i in range(n):
-        v = utc_dates[i]
-        if v >= trans[pos + 1]:
-            pos += 1
-            offset = deltas[pos]
-        result[i] = v + offset
-
-    return result
-
-def tz_convert_single(int64_t val, object tz1, object tz2):
-    cdef:
-        ndarray[int64_t] trans, deltas
-        Py_ssize_t pos
-        int64_t v, offset, utc_date
-        pandas_datetimestruct dts
-
-    if not have_pytz:
-        import pytz
-
-    # Convert to UTC
-    if _is_tzlocal(tz1):
-        pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
-        dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                      dts.min, dts.sec, dts.us, tz1)
-        delta = int(total_seconds(_get_utcoffset(tz1, dt))) * 1000000000
-        utc_date = val - delta
-    elif _get_zone(tz1) != 'UTC':
-        deltas = _get_deltas(tz1)
-        trans = _get_transitions(tz1)
-        pos = trans.searchsorted(val) - 1
-        if pos < 0:
-            raise ValueError('First time before start of DST info')
-        offset = deltas[pos]
-        utc_date = val - offset
-    else:
-        utc_date = val
-
-    if _get_zone(tz2) == 'UTC':
-        return utc_date
-    if _is_tzlocal(tz2):
-        pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
-        dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                      dts.min, dts.sec, dts.us, tz2)
-        delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
-        return utc_date + delta
-    # Convert UTC to other timezone
-    trans = _get_transitions(tz2)
-    deltas = _get_deltas(tz2)
-    pos = trans.searchsorted(utc_date) - 1
-    if pos < 0:
-        raise ValueError('First time before start of DST info')
-
-    offset = deltas[pos]
-    return utc_date + offset
-
-
-trans_cache = {}
-utc_offset_cache = {}
-
-def _get_transitions(tz):
-    """
-    Get UTC times of DST transitions
-    """
-    try:
-        # tzoffset not hashable in Python 3
-        hash(tz)
-    except TypeError:
-        return np.array([NPY_NAT + 1], dtype=np.int64)
-
-    if tz not in trans_cache:
-        if hasattr(tz, '_utc_transition_times'):
-            arr = np.array(tz._utc_transition_times, dtype='M8[ns]')
-            arr = arr.view('i8')
-            try:
-                if tz._utc_transition_times[0].year == 1:
-                    arr[0] = NPY_NAT + 1
-            except Exception:
-                pass
-        else:
-            arr = np.array([NPY_NAT + 1], dtype=np.int64)
-        trans_cache[tz] = arr
-    return trans_cache[tz]
-
-def _get_deltas(tz):
-    """
-    Get UTC offsets in microseconds corresponding to DST transitions
-    """
-    try:
-        # tzoffset not hashable in Python 3
-        hash(tz)
-    except TypeError:
-        num = int(total_seconds(_get_utcoffset(tz, None))) * 1000000000
-        return np.array([num], dtype=np.int64)
-
-    if tz not in utc_offset_cache:
-        if hasattr(tz, '_utc_transition_times'):
-            utc_offset_cache[tz] = _unbox_utcoffsets(tz._transition_info)
-        else:
-            # static tzinfo
-            num = int(total_seconds(_get_utcoffset(tz, None))) * 1000000000
-            utc_offset_cache[tz] = np.array([num], dtype=np.int64)
-
-    return utc_offset_cache[tz]
-
-cdef double total_seconds(object td): # Python 2.6 compat
-    return ((td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) //
-            10**6)
-
-def tot_seconds(td):
-    return total_seconds(td)
-
-cpdef ndarray _unbox_utcoffsets(object transinfo):
-    cdef:
-        Py_ssize_t i, sz
-        ndarray[int64_t] arr
-
-    sz = len(transinfo)
-    arr = np.empty(sz, dtype='i8')
-
-    for i in range(sz):
-        arr[i] = int(total_seconds(transinfo[i][0])) * 1000000000
-
-    return arr
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def tz_localize_to_utc(ndarray[int64_t] vals, object tz):
-    """
-    Localize tzinfo-naive DateRange to given time zone (using pytz). If
-    there are ambiguities in the values, raise AmbiguousTimeError.
-
-    Returns
-    -------
-    localized : DatetimeIndex
-    """
-    cdef:
-        ndarray[int64_t] trans, deltas, idx_shifted
-        Py_ssize_t i, idx, pos, ntrans, n = len(vals)
-        int64_t *tdata
-        int64_t v, left, right
-        ndarray[int64_t] result, result_a, result_b
-        pandas_datetimestruct dts
-
-    # Vectorized version of DstTzInfo.localize
-
-    if not have_pytz:
-        raise Exception("Could not find pytz module")
-
-    if tz == UTC or tz is None:
-        return vals
-
-    result = np.empty(n, dtype=np.int64)
-
-    if _is_tzlocal(tz):
-        for i in range(n):
-            v = vals[i]
-            pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                          dts.min, dts.sec, dts.us, tz)
-            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
-            result[i] = v - delta
-        return result
-
-    trans = _get_transitions(tz)  # transition dates
-    deltas = _get_deltas(tz)      # utc offsets
-
-    tdata = <int64_t*> trans.data
-    ntrans = len(trans)
-
-    result_a = np.empty(n, dtype=np.int64)
-    result_b = np.empty(n, dtype=np.int64)
-    result_a.fill(NPY_NAT)
-    result_b.fill(NPY_NAT)
-
-    # left side
-    idx_shifted = _ensure_int64(
-        np.maximum(0, trans.searchsorted(vals - DAY_NS, side='right') - 1))
-
-    for i in range(n):
-        v = vals[i] - deltas[idx_shifted[i]]
-        pos = bisect_right_i8(tdata, v, ntrans) - 1
-
-        # timestamp falls to the left side of the DST transition
-        if v + deltas[pos] == vals[i]:
-            result_a[i] = v
-
-    # right side
-    idx_shifted = _ensure_int64(
-        np.maximum(0, trans.searchsorted(vals + DAY_NS, side='right') - 1))
-
-    for i in range(n):
-        v = vals[i] - deltas[idx_shifted[i]]
-        pos = bisect_right_i8(tdata, v, ntrans) - 1
-
-        # timestamp falls to the right side of the DST transition
-        if v + deltas[pos] == vals[i]:
-            result_b[i] = v
-
-    for i in range(n):
-        left = result_a[i]
-        right = result_b[i]
-        if left != NPY_NAT and right != NPY_NAT:
-            if left == right:
-                result[i] = left
-            else:
-                stamp = Timestamp(vals[i])
-                raise pytz.AmbiguousTimeError(stamp)
-        elif left != NPY_NAT:
-            result[i] = left
-        elif right != NPY_NAT:
-            result[i] = right
-        else:
-            stamp = Timestamp(vals[i])
-            raise pytz.NonExistentTimeError(stamp)
-
-    return result
-
-cdef _ensure_int64(object arr):
-    if util.is_array(arr):
-        if (<ndarray> arr).descr.type_num == NPY_INT64:
-            return arr
-        else:
-            return arr.astype(np.int64)
-    else:
-        return np.array(arr, dtype=np.int64)
-
-
-cdef inline bisect_right_i8(int64_t *data, int64_t val, Py_ssize_t n):
-    cdef Py_ssize_t pivot, left = 0, right = n
-
-    # edge cases
-    if val > data[n - 1]:
-        return n
-
-    if val < data[0]:
-        return 0
-
-    while left < right:
-        pivot = left + (right - left) // 2
-
-        if data[pivot] <= val:
-            left = pivot + 1
-        else:
-            right = pivot
-
-    return left
-
-
-# Accessors
-#----------------------------------------------------------------------
-
-def build_field_sarray(ndarray[int64_t] dtindex):
-    '''
-    Datetime as int64 representation to a structured array of fields
-    '''
-    cdef:
-        Py_ssize_t i, count = 0
-        int isleap
-        pandas_datetimestruct dts
-        ndarray[int32_t] years, months, days, hours, minutes, seconds, mus
-
-    count = len(dtindex)
-
-    sa_dtype = [('Y', 'i4'), # year
-                ('M', 'i4'), # month
-                ('D', 'i4'), # day
-                ('h', 'i4'), # hour
-                ('m', 'i4'), # min
-                ('s', 'i4'), # second
-                ('u', 'i4')] # microsecond
-
-    out = np.empty(count, dtype=sa_dtype)
-
-    years = out['Y']
-    months = out['M']
-    days = out['D']
-    hours = out['h']
-    minutes = out['m']
-    seconds = out['s']
-    mus = out['u']
-
-    for i in range(count):
-        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-        years[i] = dts.year
-        months[i] = dts.month
-        days[i] = dts.day
-        hours[i] = dts.hour
-        minutes[i] = dts.min
-        seconds[i] = dts.sec
-        mus[i] = dts.us
-
-    return out
-
-def get_time_micros(ndarray[int64_t] dtindex):
-    '''
-    Datetime as int64 representation to a structured array of fields
-    '''
-    cdef:
-        Py_ssize_t i, n = len(dtindex)
-        pandas_datetimestruct dts
-        ndarray[int64_t] micros
-
-    micros = np.empty(n, dtype=np.int64)
-
-    for i in range(n):
-        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-        micros[i] = 1000000LL * (dts.hour * 60 * 60 +
-                                 60 * dts.min + dts.sec) + dts.us
-
-    return micros
-
-@cython.wraparound(False)
-def get_date_field(ndarray[int64_t] dtindex, object field):
-    '''
-    Given a int64-based datetime index, extract the year, month, etc.,
-    field and return an array of these values.
-    '''
-    cdef:
-        _TSObject ts
-        Py_ssize_t i, count = 0
-        ndarray[int32_t] out
-        ndarray[int32_t, ndim=2] _month_offset
-        int isleap
-        pandas_datetimestruct dts
-
-    _month_offset = np.array(
-        [[ 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365 ],
-         [ 0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366 ]],
-         dtype=np.int32 )
-
-    count = len(dtindex)
-    out = np.empty(count, dtype='i4')
-
-    if field == 'Y':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.year
-        return out
-
-    elif field == 'M':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.month
-        return out
-
-    elif field == 'D':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.day
-        return out
-
-    elif field == 'h':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.hour
-        return out
-
-    elif field == 'm':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.min
-        return out
-
-    elif field == 's':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.sec
-        return out
-
-    elif field == 'us':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.us
-        return out
-    elif field == 'ns':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.ps / 1000
-        return out
-    elif field == 'doy':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            isleap = is_leapyear(dts.year)
-            out[i] = _month_offset[isleap, dts.month-1] + dts.day
-        return out
-
-    elif field == 'dow':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            ts = convert_to_tsobject(dtindex[i])
-            out[i] = ts_dayofweek(ts)
-        return out
-
-    elif field == 'woy':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            isleap = is_leapyear(dts.year)
-            out[i] = _month_offset[isleap, dts.month - 1] + dts.day
-            out[i] = ((out[i] - 1) / 7) + 1
-        return out
-
-    elif field == 'q':
-        for i in range(count):
-            if dtindex[i] == NPY_NAT: out[i] = -1; continue
-
-            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
-            out[i] = dts.month
-            out[i] = ((out[i] - 1) / 3) + 1
-        return out
-
-    raise ValueError("Field %s not supported" % field)
-
-
-cdef inline int m8_weekday(int64_t val):
-    ts = convert_to_tsobject(val)
-    return ts_dayofweek(ts)
-
-cdef int64_t DAY_NS = 86400000000000LL
-
-
-def date_normalize(ndarray[int64_t] stamps, tz=None):
-    cdef:
-        Py_ssize_t i, n = len(stamps)
-        pandas_datetimestruct dts
-        _TSObject tso
-        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
-
-    if tz is not None:
-        tso = _TSObject()
-        if isinstance(tz, basestring):
-            tz = pytz.timezone(tz)
-        result = _normalize_local(stamps, tz)
-    else:
-        for i in range(n):
-            if stamps[i] == NPY_NAT:
-                result[i] = NPY_NAT
-                continue
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
-            result[i] = _normalized_stamp(&dts)
-
-    return result
-
-cdef _normalize_local(ndarray[int64_t] stamps, object tz):
-    cdef:
-        Py_ssize_t n = len(stamps)
-        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
-        ndarray[int64_t] trans, deltas, pos
-        pandas_datetimestruct dts
-
-    if _is_utc(tz):
-        for i in range(n):
-            if stamps[i] == NPY_NAT:
-                result[i] = NPY_NAT
-                continue
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
-            result[i] = _normalized_stamp(&dts)
-    elif _is_tzlocal(tz):
-        for i in range(n):
-            if stamps[i] == NPY_NAT:
-                result[i] = NPY_NAT
-                continue
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns,
-                                              &dts)
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                          dts.min, dts.sec, dts.us, tz)
-            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
-            pandas_datetime_to_datetimestruct(stamps[i] + delta,
-                                              PANDAS_FR_ns, &dts)
-            result[i] = _normalized_stamp(&dts)
-    else:
-        # Adjust datetime64 timestamp, recompute datetimestruct
-        trans = _get_transitions(tz)
-        deltas = _get_deltas(tz)
-        _pos = trans.searchsorted(stamps, side='right') - 1
-        if _pos.dtype != np.int64:
-            _pos = _pos.astype(np.int64)
-        pos = _pos
-
-        # statictzinfo
-        if not hasattr(tz, '_transition_info'):
-            for i in range(n):
-                if stamps[i] == NPY_NAT:
-                    result[i] = NPY_NAT
-                    continue
-                pandas_datetime_to_datetimestruct(stamps[i] + deltas[0],
-                                                  PANDAS_FR_ns, &dts)
-                result[i] = _normalized_stamp(&dts)
-        else:
-            for i in range(n):
-                if stamps[i] == NPY_NAT:
-                    result[i] = NPY_NAT
-                    continue
-                pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos[i]],
-                                                  PANDAS_FR_ns, &dts)
-                result[i] = _normalized_stamp(&dts)
-
-    return result
-
-cdef inline int64_t _normalized_stamp(pandas_datetimestruct *dts):
-    dts.hour = 0
-    dts.min = 0
-    dts.sec = 0
-    dts.us = 0
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
-
-
-cdef inline void m8_populate_tsobject(int64_t stamp, _TSObject tso, object tz):
-    tso.value = stamp
-    pandas_datetime_to_datetimestruct(tso.value, PANDAS_FR_ns, &tso.dts)
-
-    if tz is not None:
-        _localize_tso(tso, tz)
-
-
-def dates_normalized(ndarray[int64_t] stamps, tz=None):
-    cdef:
-        Py_ssize_t i, n = len(stamps)
-        pandas_datetimestruct dts
-
-    if tz is None or _is_utc(tz):
-        for i in range(n):
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
-            if (dts.hour + dts.min + dts.sec + dts.us) > 0:
-                return False
-    elif _is_tzlocal(tz):
-        for i in range(n):
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
-            if (dts.min + dts.sec + dts.us) > 0:
-                return False
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour, dts.min,
-                          dts.sec, dts.us, tz)
-            dt = dt + tz.utcoffset(dt)
-            if dt.hour > 0:
-                return False
-    else:
-        trans = _get_transitions(tz)
-        deltas = _get_deltas(tz)
-        for i in range(n):
-            # Adjust datetime64 timestamp, recompute datetimestruct
-            pos = trans.searchsorted(stamps[i]) - 1
-            inf = tz._transition_info[pos]
-
-            pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos],
-                                              PANDAS_FR_ns, &dts)
-            if (dts.hour + dts.min + dts.sec + dts.us) > 0:
-                return False
-
-    return True
-
-# Some general helper functions
-#----------------------------------------------------------------------
-
-def isleapyear(int64_t year):
-    return is_leapyear(year)
-
-def monthrange(int64_t year, int64_t month):
-    cdef:
-        int64_t days
-        int64_t day_of_week
-
-    if month < 1 or month > 12:
-        raise ValueError("bad month number 0; must be 1-12")
-
-    days = days_per_month_table[is_leapyear(year)][month-1]
-
-    return (dayofweek(year, month, 1), days)
-
-cdef inline int64_t ts_dayofweek(_TSObject ts):
-    return dayofweek(ts.dts.year, ts.dts.month, ts.dts.day)
-
-
-cpdef normalize_date(object dt):
-    '''
-    Normalize datetime.datetime value to midnight. Returns datetime.date as a
-    datetime.datetime at midnight
-
-    Returns
-    -------
-    normalized : datetime.datetime or Timestamp
-    '''
-    if PyDateTime_Check(dt):
-        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
-    elif PyDate_Check(dt):
-        return datetime(dt.year, dt.month, dt.day)
-    else:
-        raise TypeError('Unrecognized type: %s' % type(dt))
-
-cdef ndarray[int64_t] localize_dt64arr_to_period(ndarray[int64_t] stamps,
-                                                 int freq, object tz):
-    cdef:
-        Py_ssize_t n = len(stamps)
-        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
-        ndarray[int64_t] trans, deltas, pos
-        pandas_datetimestruct dts
-
-    if not have_pytz:
-        raise Exception('Could not find pytz module')
-
-    if _is_utc(tz):
-        for i in range(n):
-            if stamps[i] == NPY_NAT:
-                result[i] = NPY_NAT
-                continue
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
-            result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
-                                           dts.hour, dts.min, dts.sec, freq)
-
-    elif _is_tzlocal(tz):
-        for i in range(n):
-            if stamps[i] == NPY_NAT:
-                result[i] = NPY_NAT
-                continue
-            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns,
-                                              &dts)
-            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
-                          dts.min, dts.sec, dts.us, tz)
-            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
-            pandas_datetime_to_datetimestruct(stamps[i] + delta,
-                                              PANDAS_FR_ns, &dts)
-            result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
-                                           dts.hour, dts.min, dts.sec, freq)
-    else:
-        # Adjust datetime64 timestamp, recompute datetimestruct
-        trans = _get_transitions(tz)
-        deltas = _get_deltas(tz)
-        _pos = trans.searchsorted(stamps, side='right') - 1
-        if _pos.dtype != np.int64:
-            _pos = _pos.astype(np.int64)
-        pos = _pos
-
-        # statictzinfo
-        if not hasattr(tz, '_transition_info'):
-            for i in range(n):
-                if stamps[i] == NPY_NAT:
-                    result[i] = NPY_NAT
-                    continue
-                pandas_datetime_to_datetimestruct(stamps[i] + deltas[0],
-                                                  PANDAS_FR_ns, &dts)
-                result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
-                                               dts.hour, dts.min, dts.sec, freq)
-        else:
-            for i in range(n):
-                if stamps[i] == NPY_NAT:
-                    result[i] = NPY_NAT
-                    continue
-                pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos[i]],
-                                                  PANDAS_FR_ns, &dts)
-                result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
-                                               dts.hour, dts.min, dts.sec, freq)
-
-    return result
-
-
-cdef extern from "period.h":
-    ctypedef struct date_info:
-        int64_t absdate
-        double abstime
-        double second
-        int minute
-        int hour
-        int day
-        int month
-        int quarter
-        int year
-        int day_of_week
-        int day_of_year
-        int calendar
-
-    ctypedef struct asfreq_info:
-        int from_week_end
-        int to_week_end
-
-        int from_a_year_end
-        int to_a_year_end
-
-        int from_q_year_end
-        int to_q_year_end
-
-    ctypedef int64_t (*freq_conv_func)(int64_t, char, asfreq_info*)
-
-    int64_t asfreq(int64_t dtordinal, int freq1, int freq2, char relation) except INT32_MIN
-    freq_conv_func get_asfreq_func(int fromFreq, int toFreq)
-    void get_asfreq_info(int fromFreq, int toFreq, asfreq_info *af_info)
-
-    int64_t get_period_ordinal(int year, int month, int day,
-                          int hour, int minute, int second,
-                          int freq) except INT32_MIN
-
-    int64_t get_python_ordinal(int64_t period_ordinal, int freq) except INT32_MIN
-
-    int get_date_info(int64_t ordinal, int freq, date_info *dinfo) except INT32_MIN
-    double getAbsTime(int, int64_t, int64_t)
-
-    int pyear(int64_t ordinal, int freq) except INT32_MIN
-    int pqyear(int64_t ordinal, int freq) except INT32_MIN
-    int pquarter(int64_t ordinal, int freq) except INT32_MIN
-    int pmonth(int64_t ordinal, int freq) except INT32_MIN
-    int pday(int64_t ordinal, int freq) except INT32_MIN
-    int pweekday(int64_t ordinal, int freq) except INT32_MIN
-    int pday_of_week(int64_t ordinal, int freq) except INT32_MIN
-    int pday_of_year(int64_t ordinal, int freq) except INT32_MIN
-    int pweek(int64_t ordinal, int freq) except INT32_MIN
-    int phour(int64_t ordinal, int freq) except INT32_MIN
-    int pminute(int64_t ordinal, int freq) except INT32_MIN
-    int psecond(int64_t ordinal, int freq) except INT32_MIN
-    char *c_strftime(date_info *dinfo, char *fmt)
-    int get_yq(int64_t ordinal, int freq, int *quarter, int *year)
-
-# Period logic
-#----------------------------------------------------------------------
-
-cdef inline int64_t apply_mult(int64_t period_ord, int64_t mult):
-    """
-    Get freq+multiple ordinal value from corresponding freq-only ordinal value.
-    For example, 5min ordinal will be 1/5th the 1min ordinal (rounding down to
-    integer).
-    """
-    if mult == 1:
-        return period_ord
-
-    return (period_ord - 1) // mult
-
-cdef inline int64_t remove_mult(int64_t period_ord_w_mult, int64_t mult):
-    """
-    Get freq-only ordinal value from corresponding freq+multiple ordinal.
-    """
-    if mult == 1:
-        return period_ord_w_mult
-
-    return period_ord_w_mult * mult + 1;
-
-def dt64arr_to_periodarr(ndarray[int64_t] dtarr, int freq, tz=None):
-    """
-    Convert array of datetime64 values (passed in as 'i8' dtype) to a set of
-    periods corresponding to desired frequency, per period convention.
-    """
-    cdef:
-        ndarray[int64_t] out
-        Py_ssize_t i, l
-        pandas_datetimestruct dts
-
-    l = len(dtarr)
-
-    out = np.empty(l, dtype='i8')
-
-    if tz is None:
-        for i in range(l):
-            pandas_datetime_to_datetimestruct(dtarr[i], PANDAS_FR_ns, &dts)
-            out[i] = get_period_ordinal(dts.year, dts.month, dts.day,
-                                        dts.hour, dts.min, dts.sec, freq)
-    else:
-        out = localize_dt64arr_to_period(dtarr, freq, tz)
-    return out
-
-def periodarr_to_dt64arr(ndarray[int64_t] periodarr, int freq):
-    """
-    Convert array to datetime64 values from a set of ordinals corresponding to
-    periods per period convention.
-    """
-    cdef:
-        ndarray[int64_t] out
-        Py_ssize_t i, l
-
-    l = len(periodarr)
-
-    out = np.empty(l, dtype='i8')
-
-    for i in range(l):
-        out[i] = period_ordinal_to_dt64(periodarr[i], freq)
-
-    return out
-
-cdef char START = 'S'
-cdef char END = 'E'
-
-cpdef int64_t period_asfreq(int64_t period_ordinal, int freq1, int freq2,
-                            bint end):
-    """
-    Convert period ordinal from one frequency to another, and if upsampling,
-    choose to use start ('S') or end ('E') of period.
-    """
-    cdef:
-        int64_t retval
-
-    if end:
-        retval = asfreq(period_ordinal, freq1, freq2, END)
-    else:
-        retval = asfreq(period_ordinal, freq1, freq2, START)
-
-    if retval == INT32_MIN:
-        raise ValueError('Frequency conversion failed')
-
-    return retval
-
-def period_asfreq_arr(ndarray[int64_t] arr, int freq1, int freq2, bint end):
-    """
-    Convert int64-array of period ordinals from one frequency to another, and
-    if upsampling, choose to use start ('S') or end ('E') of period.
-    """
-    cdef:
-        ndarray[int64_t] result
-        Py_ssize_t i, n
-        freq_conv_func func
-        asfreq_info finfo
-        int64_t val, ordinal
-        char relation
-
-    n = len(arr)
-    result = np.empty(n, dtype=np.int64)
-
-    func = get_asfreq_func(freq1, freq2)
-    get_asfreq_info(freq1, freq2, &finfo)
-
-    if end:
-        relation = END
-    else:
-        relation = START
-
-    for i in range(n):
-        val = func(arr[i], relation, &finfo)
-        if val == INT32_MIN:
-            raise ValueError("Unable to convert to desired frequency.")
-        result[i] = val
-
-    return result
-
-def period_ordinal(int y, int m, int d, int h, int min, int s, int freq):
-    cdef:
-        int64_t ordinal
-
-    return get_period_ordinal(y, m, d, h, min, s, freq)
-
-
-cpdef int64_t period_ordinal_to_dt64(int64_t ordinal, int freq):
-    cdef:
-        pandas_datetimestruct dts
-        date_info dinfo
-
-    get_date_info(ordinal, freq, &dinfo)
-
-    dts.year = dinfo.year
-    dts.month = dinfo.month
-    dts.day = dinfo.day
-    dts.hour = dinfo.hour
-    dts.min = dinfo.minute
-    dts.sec = int(dinfo.second)
-    dts.us = dts.ps = 0
-
-    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
-
-def period_format(int64_t value, int freq, object fmt=None):
-    cdef:
-        int freq_group
-
-    if fmt is None:
-        freq_group = (freq // 1000) * 1000
-        if freq_group == 1000: # FR_ANN
-            fmt = b'%Y'
-        elif freq_group == 2000: # FR_QTR
-            fmt = b'%FQ%q'
-        elif freq_group == 3000: # FR_MTH
-            fmt = b'%Y-%m'
-        elif freq_group == 4000: # WK
-            left = period_asfreq(value, freq, 6000, 0)
-            right = period_asfreq(value, freq, 6000, 1)
-            return '%s/%s' % (period_format(left, 6000),
-                              period_format(right, 6000))
-        elif (freq_group == 5000 # BUS
-              or freq_group == 6000): # DAY
-            fmt = b'%Y-%m-%d'
-        elif freq_group == 7000: # HR
-            fmt = b'%Y-%m-%d %H:00'
-        elif freq_group == 8000: # MIN
-            fmt = b'%Y-%m-%d %H:%M'
-        elif freq_group == 9000: # SEC
-            fmt = b'%Y-%m-%d %H:%M:%S'
-        else:
-            raise ValueError('Unknown freq: %d' % freq)
-
-    return _period_strftime(value, freq, fmt)
-
-
-cdef list extra_fmts = [(b"%q", b"^`AB`^"),
-                        (b"%f", b"^`CD`^"),
-                        (b"%F", b"^`EF`^")]
-
-cdef list str_extra_fmts = ["^`AB`^", "^`CD`^", "^`EF`^"]
-
-cdef _period_strftime(int64_t value, int freq, object fmt):
-    cdef:
-        Py_ssize_t i
-        date_info dinfo
-        char *formatted
-        object pat, repl, result
-        list found_pat = [False] * len(extra_fmts)
-        int year, quarter
-
-    if PyUnicode_Check(fmt):
-        fmt = fmt.encode('utf-8')
-
-    get_date_info(value, freq, &dinfo)
-    for i in range(len(extra_fmts)):
-        pat = extra_fmts[i][0]
-        repl = extra_fmts[i][1]
-        if pat in fmt:
-            fmt = fmt.replace(pat, repl)
-            found_pat[i] = True
-
-    formatted = c_strftime(&dinfo, <char*> fmt)
-
-    result = util.char_to_string(formatted)
-    free(formatted)
-
-    for i in range(len(extra_fmts)):
-        if found_pat[i]:
-            if get_yq(value, freq, &quarter, &year) < 0:
-                raise ValueError('Unable to get quarter and year')
-
-            if i == 0:
-                repl = '%d' % quarter
-            elif i == 1:  # %f, 2-digit year
-                repl = '%.2d' % (year % 100)
-            elif i == 2:
-                repl = '%d' % year
-
-            result = result.replace(str_extra_fmts[i], repl)
-
-    # Py3?
-    if not PyString_Check(result):
-        result = str(result)
-
-    return result
-
-# period accessors
-
-ctypedef int (*accessor)(int64_t ordinal, int freq) except INT32_MIN
-
-def get_period_field(int code, int64_t value, int freq):
-    cdef accessor f = _get_accessor_func(code)
-    return f(value, freq)
-
-def get_period_field_arr(int code, ndarray[int64_t] arr, int freq):
-    cdef:
-        Py_ssize_t i, sz
-        ndarray[int64_t] out
-        accessor f
-
-    f = _get_accessor_func(code)
-
-    sz = len(arr)
-    out = np.empty(sz, dtype=np.int64)
-
-    for i in range(sz):
-        out[i] = f(arr[i], freq)
-
-    return out
-
-
-
-cdef accessor _get_accessor_func(int code):
-    if code == 0:
-        return &pyear
-    elif code == 1:
-        return &pqyear
-    elif code == 2:
-        return &pquarter
-    elif code == 3:
-        return &pmonth
-    elif code == 4:
-        return &pday
-    elif code == 5:
-        return &phour
-    elif code == 6:
-        return &pminute
-    elif code == 7:
-        return &psecond
-    elif code == 8:
-        return &pweek
-    elif code == 9:
-        return &pday_of_year
-    elif code == 10:
-        return &pweekday
-    else:
-        raise ValueError('Unrecognized code: %s' % code)
diff --git a/pandas/src/engines.pyx b/pandas/src/engines.pyx
index 9f2ec4edb..a21824ff6 100644
--- a/pandas/src/engines.pyx
+++ b/pandas/src/engines.pyx
@@ -1,6 +1,7 @@
 from numpy cimport ndarray
 
-from numpy cimport float64_t, int32_t, int64_t, uint8_t
+from numpy cimport (float64_t, int32_t, int64_t, uint8_t,
+                    NPY_DATETIME)
 cimport cython
 
 cimport numpy as cnp
@@ -14,12 +15,25 @@ import numpy as np
 
 import _algos
 
-# include "hashtable.pyx"
+cimport _tseries
+from _tseries import Timestamp
+import _tseries
+
+from hashtable cimport *
+import hashtable as _hash
+
+from datetime cimport (get_datetime64_value, _pydatetime_to_dts,
+                       pandas_datetimestruct)
+
+from cpython cimport PyTuple_Check, PyList_Check
 
 cdef extern from "datetime.h":
     bint PyDateTime_Check(object o)
     void PyDateTime_IMPORT()
 
+cdef int64_t iNaT = util.get_nat()
+
+
 PyDateTime_IMPORT
 
 cdef extern from "Python.h":
@@ -252,23 +266,11 @@ cdef class IndexEngine:
 
 
 
-# @cache_readonly
-# def _monotonicity_check(self):
-#     try:
-#         f = self._algos['is_monotonic']
-#         # wrong buffer type raises ValueError
-#         return f(self.values)
-#     except TypeError:
-#         return False, None
-
-
 
 cdef class Int64Engine(IndexEngine):
 
-    # cdef Int64HashTable mapping
-
     cdef _make_hash_table(self, n):
-        return Int64HashTable(n)
+        return _hash.Int64HashTable(n)
 
     def _call_monotonic(self, values):
         return _algos.is_monotonic_int64(values)
@@ -323,10 +325,8 @@ cdef class Int64Engine(IndexEngine):
 
 cdef class Float64Engine(IndexEngine):
 
-    # cdef Float64HashTable mapping
-
     cdef _make_hash_table(self, n):
-        return Float64HashTable(n)
+        return _hash.Float64HashTable(n)
 
     def _call_monotonic(self, values):
         return _algos.is_monotonic_float64(values)
@@ -379,10 +379,8 @@ _backfill_functions = {
 
 cdef class ObjectEngine(IndexEngine):
 
-    # cdef PyObjectHashTable mapping
-
     cdef _make_hash_table(self, n):
-        return PyObjectHashTable(n)
+        return _hash.PyObjectHashTable(n)
 
     def _call_monotonic(self, values):
         return _algos.is_monotonic_object(values)
@@ -479,10 +477,10 @@ cdef class DatetimeEngine(Int64Engine):
                                      limit=limit)
 
 
-cpdef convert_scalar(ndarray arr, object value):
+cdef convert_scalar(ndarray arr, object value):
     if arr.descr.type_num == NPY_DATETIME:
-        if isinstance(value, _Timestamp):
-            return (<_Timestamp> value).value
+        if isinstance(value, Timestamp):
+            return value.value
         elif value is None or value != value:
             return iNaT
         else:
@@ -505,64 +503,3 @@ cdef inline _to_i8(object val):
             return _pydatetime_to_dts(val, &dts)
         return val
 
-
-# ctypedef fused idxvalue_t:
-#     object
-#     int
-#     float64_t
-#     int32_t
-#     int64_t
-
-# @cython.boundscheck(False)
-# @cython.wraparound(False)
-# def is_monotonic(ndarray[idxvalue_t] arr):
-#     '''
-#     Returns
-#     -------
-#     is_monotonic, is_unique
-#     '''
-#     cdef:
-#         Py_ssize_t i, n
-#         idxvalue_t prev, cur
-#         bint is_unique = 1
-
-#     n = len(arr)
-
-#     if n < 2:
-#         return True, True
-
-#     prev = arr[0]
-#     for i in range(1, n):
-#         cur = arr[i]
-#         if cur < prev:
-#             return False, None
-#         elif cur == prev:
-#             is_unique = 0
-#         prev = cur
-#     return True, is_unique
-
-
-# @cython.wraparound(False)
-# @cython.boundscheck(False)
-# def groupby_index(ndarray[idxvalue_t] index, ndarray labels):
-#     cdef dict result = {}
-#     cdef Py_ssize_t i, length
-#     cdef list members
-#     cdef object idx, key
-
-#     length = len(index)
-
-#     for i in range(length):
-#         key = util.get_value_1d(labels, i)
-
-#         if util._checknull(key):
-#             continue
-
-#         idx = index[i]
-#         if key in result:
-#             members = result[key]
-#             members.append(idx)
-#         else:
-#             result[key] = [idx]
-
-#     return result
diff --git a/pandas/src/groupby.pyx b/pandas/src/groupby.pyx
index 739897ea8..fcf5f0b3a 100644
--- a/pandas/src/groupby.pyx
+++ b/pandas/src/groupby.pyx
@@ -13,97 +13,6 @@ def arrmap(ndarray[object] index, object func):
 
     return result
 
-@cython.boundscheck(False)
-def groupby_func(object index, object mapper):
-    cdef dict result = {}
-    cdef ndarray[object] mapped_index
-    cdef ndarray[object] index_buf
-    cdef ndarray[int8_t] mask
-    cdef int i, length
-    cdef list members
-    cdef object idx, key
-
-    length = len(index)
-
-    index_buf = np.asarray(index)
-    mapped_index = arrmap(index_buf, mapper)
-    mask = isnullobj(mapped_index)
-
-    for i from 0 <= i < length:
-        if mask[i]:
-            continue
-
-        key = mapped_index[i]
-        idx = index_buf[i]
-        if key in result:
-            members = result[key]
-            members.append(idx)
-        else:
-            result[key] = [idx]
-
-    return result
-
-
-def func_groupby_indices(object index, object mapper):
-    return groupby_indices_naive(arrmap(index, mapper))
-
-@cython.boundscheck(False)
-cpdef groupby_indices_naive(ndarray[object] values):
-    cdef dict result
-    cdef ndarray[int8_t] mask
-    cdef Py_ssize_t i, length = len(values)
-    cdef object key
-
-    result = {}
-    mask = isnullobj(values)
-    for i from 0 <= i < length:
-        if mask[i]:
-            continue
-
-        key = values[i]
-        if key in result:
-            (<list> result[key]).append(i)
-        else:
-            result[key] = [i]
-
-    return result
-
-@cython.boundscheck(False)
-def groupby_indices(ndarray values):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[int64_t] labels, counts, arr, seen
-        int64_t loc
-        dict ids = {}
-        object val
-        int64_t k
-
-    ids, labels, counts = group_labels(values)
-    seen = np.zeros_like(counts)
-
-    # try not to get in trouble here...
-    cdef int64_t **vecs = <int64_t **> malloc(len(ids) * sizeof(int64_t*))
-    result = {}
-    for i from 0 <= i < len(counts):
-        arr = np.empty(counts[i], dtype=np.int64)
-        result[ids[i]] = arr
-        vecs[i] = <int64_t *> arr.data
-
-    for i from 0 <= i < n:
-        k = labels[i]
-
-        # was NaN
-        if k == -1:
-            continue
-
-        loc = seen[k]
-        vecs[k][loc] = i
-        seen[k] = loc + 1
-
-    free(vecs)
-
-    return result
-
 @cython.wraparound(False)
 @cython.boundscheck(False)
 def is_lexsorted(list list_of_arrays):
@@ -137,864 +46,8 @@ def is_lexsorted(list list_of_arrays):
     free(vecs)
     return True
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def group_labels(ndarray[object] values):
-    '''
-    Compute label vector from input values and associated useful data
-
-    Returns
-    -------
-    '''
-    cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[int64_t] labels = np.empty(n, dtype=np.int64)
-        ndarray[int64_t] counts = np.empty(n, dtype=np.int64)
-        dict ids = {}, reverse = {}
-        int64_t idx
-        object val
-        int64_t count = 0
-
-    for i from 0 <= i < n:
-        val = values[i]
-
-        # is NaN
-        if val != val:
-            labels[i] = -1
-            continue
-
-        # for large number of groups, not doing try: except: makes a big
-        # difference
-        if val in ids:
-            idx = ids[val]
-            labels[i] = idx
-            counts[idx] = counts[idx] + 1
-        else:
-            ids[val] = count
-            reverse[count] = val
-            labels[i] = count
-            counts[count] = 1
-            count += 1
-
-    return reverse, labels, counts[:count].copy()
-
-
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def get_unique_labels(ndarray[object] values, dict idMap):
-    cdef int i, length
-    cdef object idx
-    cdef ndarray[int64_t] fillVec
-    length = len(values)
-    fillVec = np.empty(length, dtype=np.int64)
-    for i from 0 <= i < length:
-        idx = values[i]
-        fillVec[i] = idMap[idx]
-
-    return fillVec
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def groupsort_indexer(ndarray[int64_t] index, Py_ssize_t ngroups):
-    cdef:
-        Py_ssize_t i, loc, label, n
-        ndarray[int64_t] counts, where, result
-
-    # count group sizes, location 0 for NA
-    counts = np.zeros(ngroups + 1, dtype=np.int64)
-    n = len(index)
-    for i from 0 <= i < n:
-        counts[index[i] + 1] += 1
-
-    # mark the start of each contiguous group of like-indexed data
-    where = np.zeros(ngroups + 1, dtype=np.int64)
-    for i from 1 <= i < ngroups + 1:
-        where[i] = where[i - 1] + counts[i - 1]
-
-    # this is our indexer
-    result = np.zeros(n, dtype=np.int64)
-    for i from 0 <= i < n:
-        label = index[i] + 1
-        result[where[label]] = i
-        where[label] += 1
-
-    return result, counts
-
-# TODO: aggregate multiple columns in single pass
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_add(ndarray[float64_t, ndim=2] out,
-              ndarray[int64_t] counts,
-              ndarray[float64_t, ndim=2] values,
-              ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] sumx, nobs
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    sumx[lab, j] += val
-    else:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                sumx[lab, 0] += val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = sumx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_prod(ndarray[float64_t, ndim=2] out,
-               ndarray[int64_t] counts,
-               ndarray[float64_t, ndim=2] values,
-               ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] prodx, nobs
-
-    nobs = np.zeros_like(out)
-    prodx = np.ones_like(out)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    prodx[lab, j] *= val
-    else:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                prodx[lab, 0] *= val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = prodx[i, j]
-
-#----------------------------------------------------------------------
-# first, nth, last
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_nth(ndarray[float64_t, ndim=2] out,
-              ndarray[int64_t] counts,
-              ndarray[float64_t, ndim=2] values,
-              ndarray[int64_t] labels, int64_t rank):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] resx
-        ndarray[int64_t, ndim=2] nobs
-
-    nobs = np.zeros((<object> out).shape, dtype=np.int64)
-    resx = np.empty_like(out)
-
-    N, K = (<object> values).shape
-
-    for i in range(N):
-        lab = labels[i]
-        if lab < 0:
-            continue
-
-        counts[lab] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[lab, j] += 1
-                if nobs[lab, j] == rank:
-                    resx[lab, j] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_nth_object(ndarray[object, ndim=2] out,
-                     ndarray[int64_t] counts,
-                     ndarray[object, ndim=2] values,
-                     ndarray[int64_t] labels,
-                     int64_t rank):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        object val
-        float64_t count
-        ndarray[int64_t, ndim=2] nobs
-        ndarray[object, ndim=2] resx
-
-    nobs = np.zeros((<object> out).shape, dtype=np.int64)
-    resx = np.empty((<object> out).shape, dtype=object)
-
-    N, K = (<object> values).shape
-
-    for i in range(N):
-        lab = labels[i]
-        if lab < 0:
-            continue
-
-        counts[lab] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[lab, j] += 1
-                if nobs[lab, j] == rank:
-                    resx[lab, j] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = <object> nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_nth_bin(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins, int64_t rank):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] resx, nobs
-
-    nobs = np.zeros_like(out)
-    resx = np.empty_like(out)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    for i in range(N):
-        while b < ngroups - 1 and i >= bins[b]:
-            b += 1
-
-        counts[b] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[b, j] += 1
-                if nobs[b, j] == rank:
-                    resx[b, j] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_nth_bin_object(ndarray[object, ndim=2] out,
-                         ndarray[int64_t] counts,
-                         ndarray[object, ndim=2] values,
-                         ndarray[int64_t] bins, int64_t rank):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        object val
-        float64_t count
-        ndarray[object, ndim=2] resx
-        ndarray[float64_t, ndim=2] nobs
-
-    nobs = np.zeros((<object> out).shape, dtype=np.float64)
-    resx = np.empty((<object> out).shape, dtype=object)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    for i in range(N):
-        while b < ngroups - 1 and i >= bins[b]:
-            b += 1
-
-        counts[b] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[b, j] += 1
-                if nobs[b, j] == rank:
-                    resx[b, j] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_last(ndarray[float64_t, ndim=2] out,
-               ndarray[int64_t] counts,
-               ndarray[float64_t, ndim=2] values,
-               ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] resx
-        ndarray[int64_t, ndim=2] nobs
-
-    nobs = np.zeros((<object> out).shape, dtype=np.int64)
-    resx = np.empty_like(out)
-
-    N, K = (<object> values).shape
-
-    for i in range(N):
-        lab = labels[i]
-        if lab < 0:
-            continue
-
-        counts[lab] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[lab, j] += 1
-                resx[lab, j] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_last_object(ndarray[object, ndim=2] out,
-                      ndarray[int64_t] counts,
-                      ndarray[object, ndim=2] values,
-                      ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        object val
-        float64_t count
-        ndarray[object, ndim=2] resx
-        ndarray[int64_t, ndim=2] nobs
-
-    nobs = np.zeros((<object> out).shape, dtype=np.int64)
-    resx = np.empty((<object> out).shape, dtype=object)
-
-    N, K = (<object> values).shape
-
-    for i in range(N):
-        lab = labels[i]
-        if lab < 0:
-            continue
-
-        counts[lab] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[lab, j] += 1
-                resx[lab, j] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_last_bin(ndarray[float64_t, ndim=2] out,
-                   ndarray[int64_t] counts,
-                   ndarray[float64_t, ndim=2] values,
-                   ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] resx, nobs
-
-    nobs = np.zeros_like(out)
-    resx = np.empty_like(out)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    for i in range(N):
-        while b < ngroups - 1 and i >= bins[b]:
-            b += 1
-
-        counts[b] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[b, j] += 1
-                resx[b, j] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_last_bin_object(ndarray[object, ndim=2] out,
-                          ndarray[int64_t] counts,
-                          ndarray[object, ndim=2] values,
-                          ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        object val
-        float64_t count
-        ndarray[object, ndim=2] resx
-        ndarray[float64_t, ndim=2] nobs
-
-    nobs = np.zeros((<object> out).shape, dtype=np.float64)
-    resx = np.empty((<object> out).shape, dtype=object)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    for i in range(N):
-        while b < ngroups - 1 and i >= bins[b]:
-            b += 1
-
-        counts[b] += 1
-        for j in range(K):
-            val = values[i, j]
-
-            # not nan
-            if val == val:
-                nobs[b, j] += 1
-                resx[b, j] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = resx[i, j]
-
-#----------------------------------------------------------------------
-# group_min, group_max
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_min(ndarray[float64_t, ndim=2] out,
-              ndarray[int64_t] counts,
-              ndarray[float64_t, ndim=2] values,
-              ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] minx, nobs
-
-    nobs = np.zeros_like(out)
-
-    minx = np.empty_like(out)
-    minx.fill(np.inf)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    if val < minx[lab, j]:
-                        minx[lab, j] = val
-    else:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                if val < minx[lab, 0]:
-                    minx[lab, 0] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = minx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_max(ndarray[float64_t, ndim=2] out,
-              ndarray[int64_t] counts,
-              ndarray[float64_t, ndim=2] values,
-              ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] maxx, nobs
-
-    nobs = np.zeros_like(out)
-
-    maxx = np.empty_like(out)
-    maxx.fill(-np.inf)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    if val > maxx[lab, j]:
-                        maxx[lab, j] = val
-    else:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                if val > maxx[lab, 0]:
-                    maxx[lab, 0] = val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = maxx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_mean(ndarray[float64_t, ndim=2] out,
-               ndarray[int64_t] counts,
-               ndarray[float64_t, ndim=2] values,
-               ndarray[int64_t] labels):
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, count
-        ndarray[float64_t, ndim=2] sumx, nobs
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            for j in range(K):
-                val = values[i, j]
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    sumx[lab, j] += val
-    else:
-        for i in range(N):
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                sumx[lab, 0] += val
-
-    for i in range(len(counts)):
-        for j in range(K):
-            count = nobs[i, j]
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = sumx[i, j] / count
 
 
-def group_median(ndarray[float64_t, ndim=2] out,
-                 ndarray[int64_t] counts,
-                 ndarray[float64_t, ndim=2] values,
-                 ndarray[int64_t] labels):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, size
-        ndarray[int64_t] _counts
-        ndarray data
-        float64_t* ptr
-
-    from pandas._algos import take_2d_axis1_float64
-
-    ngroups = len(counts)
-    N, K = (<object> values).shape
-
-    indexer, _counts = groupsort_indexer(labels, ngroups)
-    counts[:] = _counts[1:]
-
-    data = np.empty((K, N), dtype=np.float64)
-    ptr = <float64_t*> data.data
-
-    take_2d_axis1_float64(values.T, indexer, out=data)
-
-    for i in range(K):
-        # exclude NA group
-        ptr += _counts[0]
-        for j in range(ngroups):
-            size = _counts[j + 1]
-            out[j, i] = _median_linear(ptr, size)
-            ptr += size
-
-
-cdef inline float64_t _median_linear(float64_t* a, int n):
-    cdef int i, j, na_count = 0
-    cdef float64_t result
-    cdef float64_t* tmp
-
-    # count NAs
-    for i in range(n):
-        if a[i] != a[i]:
-            na_count += 1
-
-    if na_count:
-        if na_count == n:
-            return NaN
-
-        tmp = <float64_t*> malloc((n - na_count) * sizeof(float64_t))
-
-        j = 0
-        for i in range(n):
-            if a[i] == a[i]:
-                tmp[j] = a[i]
-                j += 1
-
-        a = tmp
-        n -= na_count
-
-
-    if n % 2:
-        result = kth_smallest_c(a, n / 2, n)
-    else:
-        result = (kth_smallest_c(a, n / 2, n) +
-                  kth_smallest_c(a, n / 2 - 1, n)) / 2
-
-    if na_count:
-        free(a)
-
-    return result
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_var(ndarray[float64_t, ndim=2] out,
-              ndarray[int64_t] counts,
-              ndarray[float64_t, ndim=2] values,
-              ndarray[int64_t] labels):
-    cdef:
-        Py_ssize_t i, j, N, K, lab
-        float64_t val, ct
-        ndarray[float64_t, ndim=2] nobs, sumx, sumxx
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-    sumxx = np.zeros_like(out)
-
-    N, K = (<object> values).shape
-
-    if K > 1:
-        for i in range(N):
-
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[lab, j] += 1
-                    sumx[lab, j] += val
-                    sumxx[lab, j] += val * val
-    else:
-        for i in range(N):
-
-            lab = labels[i]
-            if lab < 0:
-                continue
-
-            counts[lab] += 1
-            val = values[i, 0]
-            # not nan
-            if val == val:
-                nobs[lab, 0] += 1
-                sumx[lab, 0] += val
-                sumxx[lab, 0] += val * val
-
-
-    for i in range(len(counts)):
-        for j in range(K):
-            ct = nobs[i, j]
-            if ct < 2:
-                out[i, j] = nan
-            else:
-                out[i, j] = ((ct * sumxx[i, j] - sumx[i, j] * sumx[i, j]) /
-                             (ct * ct - ct))
-
 # TODO: could do even better if we know something about the data. eg, index has
 # 1-min data, binner has 5-min data, then  bins are just strides in index. This
 # is a general, O(max(len(values), len(binner))) method.
@@ -1045,438 +98,6 @@ def generate_bins_dt64(ndarray[int64_t] values, ndarray[int64_t] binner,
 
     return bins
 
-# add passing bin edges, instead of labels
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_add_bin(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b, nbins
-        float64_t val, count
-        ndarray[float64_t, ndim=2] sumx, nobs
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-    N, K = (<object> values).shape
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    sumx[b, j] += val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                sumx[b, 0] += val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = sumx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_prod_bin(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] prodx, nobs
-
-    nobs = np.zeros_like(out)
-    prodx = np.ones_like(out)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-    N, K = (<object> values).shape
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    prodx[b, j] *= val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                prodx[b, 0] *= val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = prodx[i, j]
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_min_bin(ndarray[float64_t, ndim=2] out,
-                   ndarray[int64_t] counts,
-                   ndarray[float64_t, ndim=2] values,
-                   ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] minx, nobs
-
-    nobs = np.zeros_like(out)
-
-    minx = np.empty_like(out)
-    minx.fill(np.inf)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    if val < minx[b, j]:
-                        minx[b, j] = val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                if val < minx[b, 0]:
-                    minx[b, 0] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = minx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_max_bin(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] maxx, nobs
-
-    nobs = np.zeros_like(out)
-    maxx = np.empty_like(out)
-    maxx.fill(-np.inf)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    if val > maxx[b, j]:
-                        maxx[b, j] = val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                if val > maxx[b, 0]:
-                    maxx[b, 0] = val
-
-    for i in range(ngroups):
-        for j in range(K):
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = maxx[i, j]
-
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_ohlc(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins):
-    '''
-    Only aggregates on axis=0
-    '''
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        float64_t vopen, vhigh, vlow, vclose, NA
-        bint got_first = 0
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    if out.shape[1] != 4:
-        raise ValueError('Output array must have 4 columns')
-
-    NA = np.nan
-
-    b = 0
-    if K > 1:
-        raise NotImplementedError
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                if not got_first:
-                    out[b, 0] = NA
-                    out[b, 1] = NA
-                    out[b, 2] = NA
-                    out[b, 3] = NA
-                else:
-                    out[b, 0] = vopen
-                    out[b, 1] = vhigh
-                    out[b, 2] = vlow
-                    out[b, 3] = vclose
-                b += 1
-                got_first = 0
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                if not got_first:
-                    got_first = 1
-                    vopen = val
-                    vlow = val
-                    vhigh = val
-                else:
-                    if val < vlow:
-                        vlow = val
-                    if val > vhigh:
-                        vhigh = val
-                vclose = val
-
-        if not got_first:
-            out[b, 0] = NA
-            out[b, 1] = NA
-            out[b, 2] = NA
-            out[b, 3] = NA
-        else:
-            out[b, 0] = vopen
-            out[b, 1] = vhigh
-            out[b, 2] = vlow
-            out[b, 3] = vclose
-
-
-# @cython.boundscheck(False)
-# @cython.wraparound(False)
-def group_mean_bin(ndarray[float64_t, ndim=2] out,
-                   ndarray[int64_t] counts,
-                   ndarray[float64_t, ndim=2] values,
-                   ndarray[int64_t] bins):
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, count
-        ndarray[float64_t, ndim=2] sumx, nobs
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-
-    N, K = (<object> values).shape
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    sumx[b, j] += val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                sumx[b, 0] += val
-
-    for i in range(ngroups):
-        for j in range(K):
-            count = nobs[i, j]
-            if nobs[i, j] == 0:
-                out[i, j] = nan
-            else:
-                out[i, j] = sumx[i, j] / count
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def group_var_bin(ndarray[float64_t, ndim=2] out,
-                  ndarray[int64_t] counts,
-                  ndarray[float64_t, ndim=2] values,
-                  ndarray[int64_t] bins):
-
-    cdef:
-        Py_ssize_t i, j, N, K, ngroups, b
-        float64_t val, ct
-        ndarray[float64_t, ndim=2] nobs, sumx, sumxx
-
-    nobs = np.zeros_like(out)
-    sumx = np.zeros_like(out)
-    sumxx = np.zeros_like(out)
-
-    if bins[len(bins) - 1] == len(values):
-        ngroups = len(bins)
-    else:
-        ngroups = len(bins) + 1
-
-    N, K = (<object> values).shape
-
-    b = 0
-    if K > 1:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-
-            for j in range(K):
-                val = values[i, j]
-
-                # not nan
-                if val == val:
-                    nobs[b, j] += 1
-                    sumx[b, j] += val
-                    sumxx[b, j] += val * val
-    else:
-        for i in range(N):
-            while b < ngroups - 1 and i >= bins[b]:
-                b += 1
-
-            counts[b] += 1
-            val = values[i, 0]
-
-            # not nan
-            if val == val:
-                nobs[b, 0] += 1
-                sumx[b, 0] += val
-                sumxx[b, 0] += val * val
-
-    for i in range(ngroups):
-        for j in range(K):
-            ct = nobs[i, j]
-            if ct < 2:
-                out[i, j] = nan
-            else:
-                out[i, j] = ((ct * sumxx[i, j] - sumx[i, j] * sumx[i, j]) /
-                             (ct * ct - ct))
 
 
 
@@ -1692,37 +313,6 @@ def generate_slices(ndarray[int64_t] labels, Py_ssize_t ngroups):
     return starts, ends
 
 
-def groupby_arrays(ndarray index, ndarray[int64_t] labels, sort=True):
-    cdef:
-        Py_ssize_t i, lab, cur, start, n = len(index)
-        dict result = {}
-
-    index = np.asarray(index)
-
-    # this is N log N. If this is a bottleneck may we worth fixing someday
-    if sort:
-        indexer = labels.argsort(kind='mergesort')
-
-        labels = labels.take(indexer)
-        index = index.take(indexer)
-
-    if n == 0:
-        return result
-
-    start = 0
-    cur = labels[0]
-    for i in range(1, n):
-        lab = labels[i]
-
-        if lab != cur:
-            if lab != -1:
-                result[cur] = index[start:i]
-            start = i
-        cur = lab
-
-    result[cur] = index[start:]
-    return result
-
 def indices_fast(object index, ndarray[int64_t] labels, list keys,
                  list sorted_labels):
     cdef:
diff --git a/pandas/src/hashtable.pxd b/pandas/src/hashtable.pxd
index 951e93839..ac68dadd8 100644
--- a/pandas/src/hashtable.pxd
+++ b/pandas/src/hashtable.pxd
@@ -2,50 +2,24 @@ from khash cimport *
 
 # prototypes for sharing
 
-# cdef class StringHashTable:
-#     cdef kh_str_t *table
+cdef class HashTable:
+    pass
 
-#     cdef inline int check_type(self, object)
-#     cpdef get_item(self, object)
-#     cpdef set_item(self, object, Py_ssize_t)
+cdef class Int64HashTable(HashTable):
+    cdef kh_int64_t *table
 
-# cdef class Int32HashTable:
-#     cdef kh_int32_t *table
+    cpdef get_item(self, int64_t val)
+    cpdef set_item(self, int64_t key, Py_ssize_t val)
 
-#     cdef inline int check_type(self, object)
-#     cpdef get_item(self, int32_t)
-#     cpdef set_item(self, int32_t, Py_ssize_t)
 
-# cdef class Int64HashTable:
-#     cdef kh_int64_t *table
+cdef class Float64HashTable(HashTable):
+    cdef kh_float64_t *table
 
-#     cdef inline bint has_key(self, int64_t)
-#     cpdef get_item(self, int64_t)
-#     cpdef set_item(self, int64_t, Py_ssize_t)
+    # cpdef get_item(self, float64_t val)
+    # cpdef set_item(self, float64_t key, Py_ssize_t val)
 
+cdef class PyObjectHashTable(HashTable):
+    cdef kh_pymap_t *table
 
-# cdef class Float64HashTable:
-#     cdef kh_float64_t *table
-
-#     cpdef get_labels(self, ndarray, list, Py_ssize_t, int32_t)
-
-
-# cdef class PyObjectHashTable:
-#     cdef kh_pymap_t *table
-
-#     cdef destroy(self)
-#     cpdef get_item(self, object)
-#     cpdef set_item(self, object, Py_ssize_t)
-#     cpdef get_labels(self, ndarray, list, Py_ssize_t, int32_t)
-
-
-# cdef class Factorizer:
-#     cdef public PyObjectHashTable table
-#     cdef public uniques
-#     cdef public Py_ssize_t count
-
-
-# cdef class Int64Factorizer:
-#     cdef public Int64HashTable table
-#     cdef public list uniques
-#     cdef public Py_ssize_t count
+    cpdef get_item(self, object val)
+    cpdef set_item(self, object key, Py_ssize_t val)
diff --git a/pandas/src/hashtable.pyx b/pandas/src/hashtable.pyx
index 8e53e4cb6..2fd366b22 100644
--- a/pandas/src/hashtable.pyx
+++ b/pandas/src/hashtable.pyx
@@ -1,4 +1,4 @@
-from cpython cimport PyObject, Py_INCREF
+from cpython cimport PyObject, Py_INCREF, PyList_Check, PyTuple_Check
 
 from khash cimport *
 from numpy cimport *
@@ -10,6 +10,25 @@ import numpy as np
 
 ONAN = np.nan
 
+cimport cython
+cimport numpy as cnp
+
+cnp.import_array()
+cnp.import_ufunc()
+
+cdef int64_t iNaT = util.get_nat()
+
+import _algos
+
+cdef extern from "datetime.h":
+    bint PyDateTime_Check(object o)
+    void PyDateTime_IMPORT()
+
+PyDateTime_IMPORT
+
+cdef extern from "Python.h":
+    int PySlice_Check(object)
+
 
 def list_to_object_array(list obj):
     '''
@@ -349,16 +368,14 @@ cdef class Int32HashTable(HashTable):
         # return None
         return reverse, labels
 
-cdef class Int64HashTable(HashTable):
-    cdef kh_int64_t *table
+cdef class Int64HashTable: #(HashTable):
+    # cdef kh_int64_t *table
 
-    def __init__(self, size_hint=1):
+    def __cinit__(self, size_hint=1):
+        self.table = kh_init_int64()
         if size_hint is not None:
             kh_resize_int64(self.table, size_hint)
 
-    def __cinit__(self):
-        self.table = kh_init_int64()
-
     def __dealloc__(self):
         kh_destroy_int64(self.table)
 
@@ -370,11 +387,6 @@ cdef class Int64HashTable(HashTable):
     def __len__(self):
         return self.table.size
 
-    cdef inline bint has_key(self, int64_t val):
-        cdef khiter_t k
-        k = kh_get_int64(self.table, val)
-        return k != self.table.n_buckets
-
     cpdef get_item(self, int64_t val):
         cdef khiter_t k
         k = kh_get_int64(self.table, val)
@@ -535,15 +547,13 @@ cdef class Int64HashTable(HashTable):
 
 
 cdef class Float64HashTable(HashTable):
-    cdef kh_float64_t *table
+    # cdef kh_float64_t *table
 
-    def __init__(self, size_hint=1):
+    def __cinit__(self, size_hint=1):
+        self.table = kh_init_float64()
         if size_hint is not None:
             kh_resize_float64(self.table, size_hint)
 
-    def __cinit__(self):
-        self.table = kh_init_float64()
-
     def __len__(self):
         return self.table.size
 
@@ -555,7 +565,7 @@ cdef class Float64HashTable(HashTable):
         labels = self.get_labels(values, uniques, 0, -1)
         return uniques.to_array(), labels
 
-    cpdef get_labels(self, ndarray[float64_t] values,
+    def get_lables(self, ndarray[float64_t] values,
                      Float64Vector uniques,
                      Py_ssize_t count_prior, int64_t na_sentinel):
         cdef:
@@ -642,7 +652,7 @@ cdef class Float64HashTable(HashTable):
         return uniques.to_array()
 
 cdef class PyObjectHashTable(HashTable):
-    cdef kh_pymap_t *table
+    # cdef kh_pymap_t *table
 
     def __init__(self, size_hint=1):
         self.table = kh_init_pymap()
@@ -661,7 +671,7 @@ cdef class PyObjectHashTable(HashTable):
         k = kh_get_pymap(self.table, <PyObject*>key)
         return k != self.table.n_buckets
 
-    cpdef destroy(self):
+    def destroy(self):
         kh_destroy_pymap(self.table)
         self.table = NULL
 
@@ -769,7 +779,7 @@ cdef class PyObjectHashTable(HashTable):
 
         return result
 
-    cpdef get_labels(self, ndarray[object] values, ObjectVector uniques,
+    def get_lables(self, ndarray[object] values, ObjectVector uniques,
                      Py_ssize_t count_prior, int64_t na_sentinel):
         cdef:
             Py_ssize_t i, n = len(values)
@@ -872,3 +882,36 @@ cdef class Int64Factorizer:
         return labels
 
 
+
+def value_count_int64(ndarray[int64_t] values):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        kh_int64_t *table
+        int ret = 0
+        list uniques = []
+
+    table = kh_init_int64()
+    kh_resize_int64(table, n)
+
+    for i in range(n):
+        val = values[i]
+        k = kh_get_int64(table, val)
+        if k != table.n_buckets:
+            table.vals[k] += 1
+        else:
+            k = kh_put_int64(table, val, &ret)
+            table.vals[k] = 1
+
+    # for (k = kh_begin(h); k != kh_end(h); ++k)
+    # 	if (kh_exist(h, k)) kh_value(h, k) = 1;
+    i = 0
+    result_keys = np.empty(table.n_occupied, dtype=np.int64)
+    result_counts = np.zeros(table.n_occupied, dtype=np.int64)
+    for k in range(table.n_buckets):
+        if kh_exist_int64(table, k):
+            result_keys[i] = table.keys[k]
+            result_counts[i] = table.vals[k]
+            i += 1
+    kh_destroy_int64(table)
+
+    return result_keys, result_counts
diff --git a/pandas/src/ms_inttypes.h b/pandas/src/headers/ms_inttypes.h
similarity index 100%
rename from pandas/src/ms_inttypes.h
rename to pandas/src/headers/ms_inttypes.h
diff --git a/pandas/src/ms_stdint.h b/pandas/src/headers/ms_stdint.h
similarity index 100%
rename from pandas/src/ms_stdint.h
rename to pandas/src/headers/ms_stdint.h
diff --git a/pandas/src/stdint.h b/pandas/src/headers/stdint.h
similarity index 100%
rename from pandas/src/stdint.h
rename to pandas/src/headers/stdint.h
diff --git a/pandas/src/inference.pyx b/pandas/src/inference.pyx
index 7337d572f..7327e5586 100644
--- a/pandas/src/inference.pyx
+++ b/pandas/src/inference.pyx
@@ -421,7 +421,7 @@ def maybe_convert_objects(ndarray[object] objects, bint try_float=0,
             seen_float = 1
         elif util.is_datetime64_object(val):
             if convert_datetime:
-                idatetimes[i] = convert_to_tsobject(val).value
+                idatetimes[i] = convert_to_tsobject(val, None).value
                 seen_datetime = 1
             else:
                 seen_object = 1
@@ -438,7 +438,7 @@ def maybe_convert_objects(ndarray[object] objects, bint try_float=0,
         elif PyDateTime_Check(val) or util.is_datetime64_object(val):
             if convert_datetime:
                 seen_datetime = 1
-                idatetimes[i] = convert_to_tsobject(val).value
+                idatetimes[i] = convert_to_tsobject(val, None).value
             else:
                 seen_object = 1
         elif try_float and not util.is_string_object(val):
diff --git a/pandas/src/join.pyx b/pandas/src/join.pyx
index 94ecfbf84..91102a2fa 100644
--- a/pandas/src/join.pyx
+++ b/pandas/src/join.pyx
@@ -1,5 +1,3 @@
-import time
-
 def inner_join(ndarray[int64_t] left, ndarray[int64_t] right,
                Py_ssize_t max_groups):
     cdef:
@@ -241,33 +239,3 @@ def ffill_by_group(ndarray[int64_t] indexer, ndarray[int64_t] group_ids,
 
     return result
 
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def join_sorter(ndarray[int64_t] index, Py_ssize_t ngroups):
-    cdef:
-        Py_ssize_t i, loc, label, n
-        ndarray[int64_t] counts, where, result
-
-    # count group sizes, location 0 for NA
-    counts = np.zeros(ngroups + 1, dtype=np.int64)
-    n = len(index)
-    for i from 0 <= i < n:
-        counts[index[i] + 1] += 1
-
-    # mark the start of each contiguous group of like-indexed data
-    where = np.zeros(ngroups + 1, dtype=np.int64)
-    for i from 1 <= i < ngroups + 1:
-        where[i] = where[i - 1] + counts[i - 1]
-
-    # this is our indexer
-    result = np.zeros(n, dtype=np.int64)
-    for i from 0 <= i < n:
-        label = index[i] + 1
-        result[where[label]] = i
-        where[label] += 1
-
-    return result, counts
-
-def _big_join_sorter(index):
-    pass
diff --git a/pandas/src/lib.pyx b/pandas/src/lib.pyx
new file mode 100644
index 000000000..89ab6cf59
--- /dev/null
+++ b/pandas/src/lib.pyx
@@ -0,0 +1,693 @@
+cimport numpy as np
+cimport cython
+import numpy as np
+
+from numpy cimport *
+
+
+cdef extern from "numpy/arrayobject.h":
+    cdef enum NPY_TYPES:
+        NPY_intp "NPY_INTP"
+
+from cpython cimport (PyDict_New, PyDict_GetItem, PyDict_SetItem,
+                      PyDict_Contains, PyDict_Keys,
+                      Py_INCREF, PyTuple_SET_ITEM,
+                      PyList_Check, PyFloat_Check,
+                      PyString_Check,
+                      PyTuple_SetItem,
+                      PyTuple_New)
+cimport cpython
+
+isnan = np.isnan
+cdef double NaN = <double> np.NaN
+cdef double nan = NaN
+cdef double NAN = nan
+
+from datetime import datetime as pydatetime
+
+# this is our tseries.pxd
+from datetime cimport *
+
+from _tseries cimport convert_to_tsobject
+import _tseries
+
+cdef int64_t NPY_NAT = util.get_nat()
+
+ctypedef unsigned char UChar
+
+cimport util
+from util cimport is_array, _checknull, _checknan
+
+cdef extern from "headers/stdint.h":
+    enum: UINT8_MAX
+
+
+cdef extern from "math.h":
+    double sqrt(double x)
+    double fabs(double)
+
+# import datetime C API
+PyDateTime_IMPORT
+
+# initialize numpy
+import_array()
+import_ufunc()
+
+cpdef map_indices_list(list index):
+    '''
+    Produce a dict mapping the values of the input array to their respective
+    locations.
+
+    Example:
+        array(['hi', 'there']) --> {'hi' : 0 , 'there' : 1}
+
+    Better to do this with Cython because of the enormous speed boost.
+    '''
+    cdef Py_ssize_t i, length
+    cdef dict result = {}
+
+    length = len(index)
+
+    for i from 0 <= i < length:
+        result[index[i]] = i
+
+    return result
+
+
+from libc.stdlib cimport malloc, free
+
+def ismember(ndarray arr, set values):
+    '''
+    Checks whether
+
+    Parameters
+    ----------
+    arr : ndarray
+    values : set
+
+    Returns
+    -------
+    ismember : ndarray (boolean dtype)
+    '''
+    cdef:
+        Py_ssize_t i, n
+        ndarray[uint8_t] result
+        object val
+
+    n = len(arr)
+    result = np.empty(n, dtype=np.uint8)
+    for i in range(n):
+        val = util.get_value_at(arr, i)
+        if val in values:
+            result[i] = 1
+        else:
+            result[i] = 0
+
+    return result.view(np.bool_)
+
+#----------------------------------------------------------------------
+# datetime / io related
+
+cdef int _EPOCH_ORD = 719163
+
+from datetime import date as pydate
+
+cdef inline int64_t gmtime(object date):
+    cdef int y, m, d, h, mn, s, days
+
+    y = PyDateTime_GET_YEAR(date)
+    m = PyDateTime_GET_MONTH(date)
+    d = PyDateTime_GET_DAY(date)
+    h = PyDateTime_DATE_GET_HOUR(date)
+    mn = PyDateTime_DATE_GET_MINUTE(date)
+    s = PyDateTime_DATE_GET_SECOND(date)
+
+    days = pydate(y, m, 1).toordinal() - _EPOCH_ORD + d - 1
+    return ((<int64_t> (((days * 24 + h) * 60 + mn))) * 60 + s) * 1000
+
+cpdef object to_datetime(int64_t timestamp):
+    return pydatetime.utcfromtimestamp(timestamp / 1000.0)
+
+cpdef object to_timestamp(object dt):
+    return gmtime(dt)
+
+def array_to_timestamp(ndarray[object, ndim=1] arr):
+    cdef int i, n
+    cdef ndarray[int64_t, ndim=1] result
+
+    n = len(arr)
+    result = np.empty(n, dtype=np.int64)
+
+    for i from 0 <= i < n:
+        result[i] = gmtime(arr[i])
+
+    return result
+
+def time64_to_datetime(ndarray[int64_t, ndim=1] arr):
+    cdef int i, n
+    cdef ndarray[object, ndim=1] result
+
+    n = len(arr)
+    result = np.empty(n, dtype=object)
+
+    for i from 0 <= i < n:
+        result[i] = to_datetime(arr[i])
+
+    return result
+
+#----------------------------------------------------------------------
+# isnull / notnull related
+
+cdef double INF = <double> np.inf
+cdef double NEGINF = -INF
+
+from _tseries import NaT
+
+cpdef checknull(object val):
+    if util.is_float_object(val) or util.is_complex_object(val):
+        return val != val or val == INF or val == NEGINF
+    elif util.is_datetime64_object(val):
+        return get_datetime64_value(val) == NPY_NAT
+    elif val is NaT:
+        return True
+    elif is_array(val):
+        return False
+    else:
+        return util._checknull(val)
+
+def isscalar(object val):
+    return np.isscalar(val) or val is None or PyDateTime_Check(val)
+
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def isnullobj(ndarray[object] arr):
+    cdef Py_ssize_t i, n
+    cdef object val
+    cdef ndarray[uint8_t] result
+
+    n = len(arr)
+    result = np.zeros(n, dtype=np.uint8)
+    for i from 0 <= i < n:
+        result[i] = util._checknull(arr[i])
+    return result.view(np.bool_)
+
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def isnullobj2d(ndarray[object, ndim=2] arr):
+    cdef Py_ssize_t i, j, n, m
+    cdef object val
+    cdef ndarray[uint8_t, ndim=2] result
+
+    n, m = (<object> arr).shape
+    result = np.zeros((n, m), dtype=np.uint8)
+    for i from 0 <= i < n:
+        for j from 0 <= j < m:
+            val = arr[i, j]
+            if checknull(val):
+                result[i, j] = 1
+    return result.view(np.bool_)
+
+def list_to_object_array(list obj):
+    '''
+    Convert list to object ndarray. Seriously can't believe I had to write this
+    function
+    '''
+    cdef:
+        Py_ssize_t i, n
+        ndarray[object] arr
+
+    n = len(obj)
+    arr = np.empty(n, dtype=object)
+
+    for i from 0 <= i < n:
+        arr[i] = obj[i]
+
+    return arr
+
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def fast_unique(ndarray[object] values):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        list uniques = []
+        dict table = {}
+        object val, stub = 0
+
+    for i from 0 <= i < n:
+        val = values[i]
+        if val not in table:
+            table[val] = stub
+            uniques.append(val)
+    try:
+        uniques.sort()
+    except Exception:
+        pass
+
+    return uniques
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def fast_unique_multiple(list arrays):
+    cdef:
+        ndarray[object] buf
+        Py_ssize_t k = len(arrays)
+        Py_ssize_t i, j, n
+        list uniques = []
+        dict table = {}
+        object val, stub = 0
+
+    for i from 0 <= i < k:
+        buf = arrays[i]
+        n = len(buf)
+        for j from 0 <= j < n:
+            val = buf[j]
+            if val not in table:
+                table[val] = stub
+                uniques.append(val)
+    try:
+        uniques.sort()
+    except Exception:
+        pass
+
+    return uniques
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def fast_unique_multiple_list(list lists):
+    cdef:
+        list buf
+        Py_ssize_t k = len(lists)
+        Py_ssize_t i, j, n
+        list uniques = []
+        dict table = {}
+        object val, stub = 0
+
+    for i from 0 <= i < k:
+        buf = lists[i]
+        n = len(buf)
+        for j from 0 <= j < n:
+            val = buf[j]
+            if val not in table:
+                table[val] = stub
+                uniques.append(val)
+    try:
+        uniques.sort()
+    except Exception:
+        pass
+
+    return uniques
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def fast_unique_multiple_list_gen(object gen):
+    cdef:
+        list buf
+        Py_ssize_t j, n
+        list uniques = []
+        dict table = {}
+        object val, stub = 0
+
+    for buf in gen:
+        n = len(buf)
+        for j from 0 <= j < n:
+            val = buf[j]
+            if val not in table:
+                table[val] = stub
+                uniques.append(val)
+
+    try:
+        uniques.sort()
+    except Exception:
+        pass
+
+    return uniques
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def dicts_to_array(list dicts, list columns):
+    cdef:
+        Py_ssize_t i, j, k, n
+        ndarray[object, ndim=2] result
+        dict row
+        object col, onan = np.nan
+
+    k = len(columns)
+    n = len(dicts)
+
+    result = np.empty((n, k), dtype='O')
+
+    for i in range(n):
+        row = dicts[i]
+        for j in range(k):
+            col = columns[j]
+            if col in row:
+                result[i, j] = row[col]
+            else:
+                result[i, j] = onan
+
+    return result
+
+
+def fast_zip(list ndarrays):
+    '''
+    For zipping multiple ndarrays into an ndarray of tuples
+    '''
+    cdef:
+        Py_ssize_t i, j, k, n
+        ndarray[object] result
+        flatiter it
+        object val, tup
+
+    k = len(ndarrays)
+    n = len(ndarrays[0])
+
+    result = np.empty(n, dtype=object)
+
+    # initialize tuples on first pass
+    arr = ndarrays[0]
+    it = <flatiter> PyArray_IterNew(arr)
+    for i in range(n):
+        val = PyArray_GETITEM(arr, PyArray_ITER_DATA(it))
+        tup = PyTuple_New(k)
+
+        PyTuple_SET_ITEM(tup, 0, val)
+        Py_INCREF(val)
+        result[i] = tup
+        PyArray_ITER_NEXT(it)
+
+    for j in range(1, k):
+        arr = ndarrays[j]
+        it = <flatiter> PyArray_IterNew(arr)
+        if len(arr) != n:
+            raise ValueError('all arrays must be same length')
+
+        for i in range(n):
+            val = PyArray_GETITEM(arr, PyArray_ITER_DATA(it))
+            PyTuple_SET_ITEM(result[i], j, val)
+            Py_INCREF(val)
+            PyArray_ITER_NEXT(it)
+
+    return result
+
+def get_reverse_indexer(ndarray[int64_t] indexer, Py_ssize_t length):
+    cdef:
+        Py_ssize_t i, n = len(indexer)
+        ndarray[int64_t] rev_indexer
+        int64_t idx
+
+    rev_indexer = np.empty(length, dtype=np.int64)
+    rev_indexer.fill(-1)
+    for i in range(n):
+        idx = indexer[i]
+        if idx != -1:
+            rev_indexer[idx] = i
+
+    return rev_indexer
+
+
+def has_infs_f4(ndarray[float32_t] arr):
+    cdef:
+        Py_ssize_t i, n = len(arr)
+        float32_t inf, neginf, val
+
+    inf = np.inf
+    neginf = -inf
+
+    for i in range(n):
+        val = arr[i]
+        if val == inf or val == neginf:
+            return True
+    return False
+
+def has_infs_f8(ndarray[float64_t] arr):
+    cdef:
+        Py_ssize_t i, n = len(arr)
+        float64_t inf, neginf, val
+
+    inf = np.inf
+    neginf = -inf
+
+    for i in range(n):
+        val = arr[i]
+        if val == inf or val == neginf:
+            return True
+    return False
+
+def convert_timestamps(ndarray values):
+    cdef:
+        object val, f, result
+        dict cache = {}
+        Py_ssize_t i, n = len(values)
+        ndarray[object] out
+
+    # for HDFStore, a bit temporary but...
+
+    from datetime import datetime
+    f = datetime.fromtimestamp
+
+    out = np.empty(n, dtype='O')
+
+    for i in range(n):
+        val = util.get_value_1d(values, i)
+        if val in cache:
+            out[i] = cache[val]
+        else:
+            cache[val] = out[i] = f(val)
+
+    return out
+
+def maybe_indices_to_slice(ndarray[int64_t] indices):
+    cdef:
+        Py_ssize_t i, n = len(indices)
+
+    if n == 0:
+        return indices
+
+    for i in range(1, n):
+        if indices[i] - indices[i - 1] != 1:
+            return indices
+    return slice(indices[0], indices[n - 1] + 1)
+
+
+def maybe_booleans_to_slice(ndarray[uint8_t] mask):
+    cdef:
+        Py_ssize_t i, n = len(mask)
+        Py_ssize_t start, end
+        bint started = 0, finished = 0
+
+    for i in range(n):
+        if mask[i]:
+            if finished:
+                return mask.view(np.bool_)
+            if not started:
+                started = 1
+                start = i
+        else:
+            if finished:
+                continue
+
+            if started:
+                end = i
+                finished = 1
+
+    if not started:
+        return slice(0, 0)
+    if not finished:
+        return slice(start, None)
+    else:
+        return slice(start, end)
+
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def scalar_compare(ndarray[object] values, object val, object op):
+    import operator
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[uint8_t, cast=True] result
+        int flag
+        object x
+
+    if op is operator.lt:
+        flag = cpython.Py_LT
+    elif op is operator.le:
+        flag = cpython.Py_LE
+    elif op is operator.gt:
+        flag = cpython.Py_GT
+    elif op is operator.ge:
+        flag = cpython.Py_GE
+    elif op is operator.eq:
+        flag = cpython.Py_EQ
+    elif op is operator.ne:
+        flag = cpython.Py_NE
+    else:
+        raise ValueError('Unrecognized operator')
+
+    result = np.empty(n, dtype=bool).view(np.uint8)
+
+    if flag == cpython.Py_NE:
+        for i in range(n):
+            x = values[i]
+            if _checknull(x):
+                result[i] = True
+            else:
+                result[i] = cpython.PyObject_RichCompareBool(x, val, flag)
+    else:
+        for i in range(n):
+            x = values[i]
+            if _checknull(x):
+                result[i] = False
+            else:
+                result[i] = cpython.PyObject_RichCompareBool(x, val, flag)
+
+    return result.view(bool)
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def vec_compare(ndarray[object] left, ndarray[object] right, object op):
+    import operator
+    cdef:
+        Py_ssize_t i, n = len(left)
+        ndarray[uint8_t, cast=True] result
+        int flag
+
+    if n != len(right):
+        raise ValueError('Arrays were different lengths: %d vs %d'
+                         % (n, len(right)))
+
+    if op is operator.lt:
+        flag = cpython.Py_LT
+    elif op is operator.le:
+        flag = cpython.Py_LE
+    elif op is operator.gt:
+        flag = cpython.Py_GT
+    elif op is operator.ge:
+        flag = cpython.Py_GE
+    elif op is operator.eq:
+        flag = cpython.Py_EQ
+    elif op is operator.ne:
+        flag = cpython.Py_NE
+    else:
+        raise ValueError('Unrecognized operator')
+
+    result = np.empty(n, dtype=bool).view(np.uint8)
+
+    if flag == cpython.Py_NE:
+        for i in range(n):
+            x = left[i]
+            y = right[i]
+
+            if _checknull(x) or _checknull(y):
+                result[i] = True
+            else:
+                result[i] = cpython.PyObject_RichCompareBool(x, y, flag)
+    else:
+        for i in range(n):
+            x = left[i]
+            y = right[i]
+
+            if _checknull(x) or _checknull(y):
+                result[i] = False
+            else:
+                result[i] = cpython.PyObject_RichCompareBool(x, y, flag)
+
+    return result.view(bool)
+
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def scalar_binop(ndarray[object] values, object val, object op):
+    cdef:
+        Py_ssize_t i, n = len(values)
+        ndarray[object] result
+        object x
+
+    result = np.empty(n, dtype=object)
+
+    for i in range(n):
+        x = values[i]
+        if util._checknull(x):
+            result[i] = x
+        else:
+            result[i] = op(x, val)
+
+    return maybe_convert_bool(result)
+
+@cython.wraparound(False)
+@cython.boundscheck(False)
+def vec_binop(ndarray[object] left, ndarray[object] right, object op):
+    cdef:
+        Py_ssize_t i, n = len(left)
+        ndarray[object] result
+
+    if n != len(right):
+        raise ValueError('Arrays were different lengths: %d vs %d'
+                         % (n, len(right)))
+
+    result = np.empty(n, dtype=object)
+
+    for i in range(n):
+        x = left[i]
+        y = right[i]
+        try:
+            result[i] = op(x, y)
+        except TypeError:
+            if util._checknull(x):
+                result[i] = x
+            elif util._checknull(y):
+                result[i] = y
+            else:
+                raise
+
+    return maybe_convert_bool(result)
+
+
+def astype_intsafe(ndarray[object] arr, new_dtype):
+    cdef:
+        Py_ssize_t i, n = len(arr)
+        ndarray result
+
+    result = np.empty(n, dtype=new_dtype)
+    for i in range(n):
+        util.set_value_at(result, i, arr[i])
+
+    return result
+
+def clean_index_list(list obj):
+    '''
+    Utility used in pandas.core.index._ensure_index
+    '''
+    cdef:
+        ndarray[object] converted
+        Py_ssize_t i, n = len(obj)
+        object v
+        bint all_arrays = 1
+
+    for i in range(n):
+        v = obj[i]
+        if not (PyList_Check(v) or np.PyArray_Check(v)):
+            all_arrays = 0
+            break
+
+    if all_arrays:
+        return obj, all_arrays
+
+    converted = np.empty(n, dtype=object)
+    for i in range(n):
+        v = obj[i]
+        if PyList_Check(v) or np.PyArray_Check(v):
+            converted[i] = tuple(v)
+        else:
+            converted[i] = v
+
+    return maybe_convert_objects(converted), 0
+
+
+include "groupby.pyx"
+include "reindex.pyx"
+include "reduce.pyx"
+include "properties.pyx"
+include "inference.pyx"
diff --git a/pandas/src/moments.pyx b/pandas/src/moments.pyx
deleted file mode 100644
index efe42108d..000000000
--- a/pandas/src/moments.pyx
+++ /dev/null
@@ -1,954 +0,0 @@
-# Cython implementations of rolling sum, mean, variance, skewness,
-# other statistical moment functions
-#
-# Misc implementation notes
-# -------------------------
-#
-# - In Cython x * x is faster than x ** 2 for C types, this should be
-#   periodically revisited to see if it's still true.
-#
-# -
-
-def _check_minp(win, minp, N):
-    if minp > win:
-        raise ValueError('min_periods (%d) must be <= window (%d)'
-                        % (minp, win))
-    elif minp > N:
-        minp = N + 1
-    elif minp == 0:
-        minp = 1
-    elif minp < 0:
-        raise ValueError('min_periods must be >= 0')
-    return minp
-
-# original C implementation by N. Devillard.
-# This code in public domain.
-# Function :   kth_smallest()
-# In       :   array of elements, # of elements in the array, rank k
-# Out      :   one element
-# Job      :   find the kth smallest element in the array
-
-#             Reference:
-
-#               Author: Wirth, Niklaus
-#                Title: Algorithms + data structures = programs
-#            Publisher: Englewood Cliffs: Prentice-Hall, 1976
-# Physical description: 366 p.
-#               Series: Prentice-Hall Series in Automatic Computation
-
-def kth_smallest(ndarray[double_t] a, Py_ssize_t k):
-    cdef:
-        Py_ssize_t i,j,l,m,n
-        double_t x, t
-
-    n = len(a)
-
-    l = 0
-    m = n-1
-    while (l<m):
-        x = a[k]
-        i = l
-        j = m
-
-        while 1:
-            while a[i] < x: i += 1
-            while x < a[j]: j -= 1
-            if i <= j:
-                t = a[i]
-                a[i] = a[j]
-                a[j] = t
-                i += 1; j -= 1
-
-            if i > j: break
-
-        if j < k: l = i
-        if k < i: m = j
-    return a[k]
-
-cdef inline kth_smallest_c(float64_t* a, Py_ssize_t k, Py_ssize_t n):
-    cdef:
-        Py_ssize_t i,j,l,m
-        double_t x, t
-
-    l = 0
-    m = n-1
-    while (l<m):
-        x = a[k]
-        i = l
-        j = m
-
-        while 1:
-            while a[i] < x: i += 1
-            while x < a[j]: j -= 1
-            if i <= j:
-                t = a[i]
-                a[i] = a[j]
-                a[j] = t
-                i += 1; j -= 1
-
-            if i > j: break
-
-        if j < k: l = i
-        if k < i: m = j
-    return a[k]
-
-
-def median(ndarray arr):
-    '''
-    A faster median
-    '''
-    cdef int n = len(arr)
-
-    if len(arr) == 0:
-        return np.NaN
-
-    arr = arr.copy()
-
-    if n % 2:
-        return kth_smallest(arr, n / 2)
-    else:
-        return (kth_smallest(arr, n / 2) +
-                kth_smallest(arr, n / 2 - 1)) / 2
-
-
-# -------------- Min, Max subsequence
-
-def max_subseq(ndarray[double_t] arr):
-    cdef:
-        Py_ssize_t i=0,s=0,e=0,T,n
-        double m, S
-
-    n = len(arr)
-
-    if len(arr) == 0:
-        return (-1,-1,None)
-
-    m = arr[0]
-    S = m
-    T = 0
-
-    for i in range(1, n):
-        # S = max { S + A[i], A[i] )
-        if (S > 0):
-            S = S + arr[i]
-        else:
-            S = arr[i]
-            T = i
-        if S > m:
-            s = T
-            e = i
-            m = S
-
-    return (s, e, m)
-
-def min_subseq(ndarray[double_t] arr):
-    cdef:
-        Py_ssize_t s, e
-        double m
-
-    (s, e, m) = max_subseq(-arr)
-
-    return (s, e, -m)
-
-#-------------------------------------------------------------------------------
-# Rolling sum
-
-def roll_sum(ndarray[double_t] input, int win, int minp):
-    cdef double val, prev, sum_x = 0
-    cdef int nobs = 0, i
-    cdef int N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            sum_x += val
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if val == val:
-            nobs += 1
-            sum_x += val
-
-        if i > win - 1:
-            prev = input[i - win]
-            if prev == prev:
-                sum_x -= prev
-                nobs -= 1
-
-        if nobs >= minp:
-            output[i] = sum_x
-        else:
-            output[i] = NaN
-
-    return output
-
-#-------------------------------------------------------------------------------
-# Rolling mean
-
-def roll_mean(ndarray[double_t] input,
-               int win, int minp):
-    cdef double val, prev, sum_x = 0
-    cdef Py_ssize_t nobs = 0, i
-    cdef Py_ssize_t N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            sum_x += val
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if val == val:
-            nobs += 1
-            sum_x += val
-
-        if i > win - 1:
-            prev = input[i - win]
-            if prev == prev:
-                sum_x -= prev
-                nobs -= 1
-
-        if nobs >= minp:
-            output[i] = sum_x / nobs
-        else:
-            output[i] = NaN
-
-    return output
-
-#-------------------------------------------------------------------------------
-# Exponentially weighted moving average
-
-def ewma(ndarray[double_t] input, double_t com, int adjust):
-    '''
-    Compute exponentially-weighted moving average using center-of-mass.
-
-    Parameters
-    ----------
-    input : ndarray (float64 type)
-    com : float64
-
-    Returns
-    -------
-    y : ndarray
-    '''
-
-    cdef double cur, prev, neww, oldw, adj
-    cdef Py_ssize_t i
-    cdef Py_ssize_t N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    if N == 0:
-        return output
-
-    neww = 1. / (1. + com)
-    oldw = 1. - neww
-    adj = oldw
-
-    if adjust:
-        output[0] = neww * input[0]
-    else:
-        output[0] = input[0]
-
-    for i from 1 <= i < N:
-        cur = input[i]
-        prev = output[i - 1]
-
-        if cur == cur:
-            if prev == prev:
-                output[i] = oldw * prev + neww * cur
-            else:
-                output[i] = neww * cur
-        else:
-            output[i] = prev
-
-    if adjust:
-        for i from 0 <= i < N:
-            cur = input[i]
-
-            if cur == cur:
-                output[i] = output[i] / (1. - adj)
-                adj *= oldw
-            else:
-                if i >= 1:
-                    output[i] = output[i - 1]
-
-    return output
-
-#----------------------------------------------------------------------
-# Pairwise correlation/covariance
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def nancorr(ndarray[float64_t, ndim=2] mat, cov=False, minp=None):
-    cdef:
-        Py_ssize_t i, j, xi, yi, N, K
-        ndarray[float64_t, ndim=2] result
-        ndarray[uint8_t, ndim=2] mask
-        int64_t nobs = 0
-        float64_t vx, vy, sumx, sumy, sumxx, sumyy, meanx, meany, divisor
-
-    N, K = (<object> mat).shape
-
-    if minp is None:
-        minp = 1
-
-    result = np.empty((K, K), dtype=np.float64)
-    mask = np.isfinite(mat).view(np.uint8)
-
-    for xi in range(K):
-        for yi in range(xi + 1):
-            nobs = sumxx = sumyy = sumx = sumy = 0
-            for i in range(N):
-                if mask[i, xi] and mask[i, yi]:
-                    vx = mat[i, xi]
-                    vy = mat[i, yi]
-                    nobs += 1
-                    sumx += vx
-                    sumy += vy
-
-            if nobs < minp:
-                result[xi, yi] = result[yi, xi] = np.NaN
-            else:
-                meanx = sumx / nobs
-                meany = sumy / nobs
-
-                # now the cov numerator
-                sumx = 0
-
-                for i in range(N):
-                    if mask[i, xi] and mask[i, yi]:
-                        vx = mat[i, xi] - meanx
-                        vy = mat[i, yi] - meany
-
-                        sumx += vx * vy
-                        sumxx += vx * vx
-                        sumyy += vy * vy
-
-                divisor = (nobs - 1.0) if cov else sqrt(sumxx * sumyy)
-
-                if divisor != 0:
-                    result[xi, yi] = result[yi, xi] = sumx / divisor
-                else:
-                    result[xi, yi] = result[yi, xi] = np.NaN
-
-    return result
-
-#----------------------------------------------------------------------
-# Rolling variance
-
-def roll_var(ndarray[double_t] input, int win, int minp, int ddof=1):
-    cdef double val, prev, sum_x = 0, sum_xx = 0, nobs = 0
-    cdef Py_ssize_t i
-    cdef Py_ssize_t N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            sum_x += val
-            sum_xx += val * val
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if val == val:
-            nobs += 1
-            sum_x += val
-            sum_xx += val * val
-
-        if i > win - 1:
-            prev = input[i - win]
-            if prev == prev:
-                sum_x -= prev
-                sum_xx -= prev * prev
-                nobs -= 1
-
-        if nobs >= minp:
-            # pathological case
-            if nobs == 1:
-                output[i] = 0
-                continue
-
-            val = (nobs * sum_xx - sum_x * sum_x) / (nobs * (nobs - ddof))
-            if val < 0:
-                val = 0
-
-            output[i] = val
-        else:
-            output[i] = NaN
-
-    return output
-
-#-------------------------------------------------------------------------------
-# Rolling skewness
-
-def roll_skew(ndarray[double_t] input, int win, int minp):
-    cdef double val, prev
-    cdef double x = 0, xx = 0, xxx = 0
-    cdef Py_ssize_t nobs = 0, i
-    cdef Py_ssize_t N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    # 3 components of the skewness equation
-    cdef double A, B, C, R
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            x += val
-            xx += val * val
-            xxx += val * val * val
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if val == val:
-            nobs += 1
-            x += val
-            xx += val * val
-            xxx += val * val * val
-
-        if i > win - 1:
-            prev = input[i - win]
-            if prev == prev:
-                x -= prev
-                xx -= prev * prev
-                xxx -= prev * prev * prev
-
-                nobs -= 1
-
-        if nobs >= minp:
-            A = x / nobs
-            B = xx / nobs - A * A
-            C = xxx / nobs - A * A * A - 3 * A * B
-
-            R = sqrt(B)
-
-            output[i] = ((sqrt(nobs * (nobs - 1.)) * C) /
-                         ((nobs-2) * R * R * R))
-        else:
-            output[i] = NaN
-
-    return output
-
-#-------------------------------------------------------------------------------
-# Rolling kurtosis
-
-
-def roll_kurt(ndarray[double_t] input,
-               int win, int minp):
-    cdef double val, prev
-    cdef double x = 0, xx = 0, xxx = 0, xxxx = 0
-    cdef Py_ssize_t nobs = 0, i
-    cdef Py_ssize_t N = len(input)
-
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    # 5 components of the kurtosis equation
-    cdef double A, B, C, D, R, K
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-
-            # seriously don't ask me why this is faster
-            x += val
-            xx += val * val
-            xxx += val * val * val
-            xxxx += val * val * val * val
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if val == val:
-            nobs += 1
-            x += val
-            xx += val * val
-            xxx += val * val * val
-            xxxx += val * val * val * val
-
-        if i > win - 1:
-            prev = input[i - win]
-            if prev == prev:
-                x -= prev
-                xx -= prev * prev
-                xxx -= prev * prev * prev
-                xxxx -= prev * prev * prev * prev
-
-                nobs -= 1
-
-        if nobs >= minp:
-            A = x / nobs
-            R = A * A
-            B = xx / nobs - R
-            R = R * A
-            C = xxx / nobs - R - 3 * A * B
-            R = R * A
-            D = xxxx / nobs - R - 6*B*A*A - 4*C*A
-
-            K = (nobs * nobs - 1.)*D/(B*B) - 3*((nobs-1.)**2)
-            K = K / ((nobs - 2.)*(nobs-3.))
-
-            output[i] = K
-        else:
-            output[i] = NaN
-
-    return output
-
-#-------------------------------------------------------------------------------
-# Rolling median, min, max
-
-ctypedef double_t (* skiplist_f)(object sl, int n, int p)
-
-cdef _roll_skiplist_op(ndarray arg, int win, int minp, skiplist_f op):
-    cdef ndarray[double_t] input = arg
-    cdef double val, prev, midpoint
-    cdef IndexableSkiplist skiplist
-    cdef Py_ssize_t nobs = 0, i
-
-    cdef Py_ssize_t N = len(input)
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    skiplist = IndexableSkiplist(win)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            skiplist.insert(val)
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if i > win - 1:
-            prev = input[i - win]
-
-            if prev == prev:
-                skiplist.remove(prev)
-                nobs -= 1
-
-        if val == val:
-            nobs += 1
-            skiplist.insert(val)
-
-        output[i] = op(skiplist, nobs, minp)
-
-    return output
-
-from skiplist cimport *
-
-def roll_median_c(ndarray[float64_t] arg, int win, int minp):
-    cdef double val, res, prev
-    cdef:
-        int ret=0
-        skiplist_t *sl
-        Py_ssize_t midpoint, nobs = 0, i
-
-
-    cdef Py_ssize_t N = len(arg)
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    sl = skiplist_init(win)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = arg[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            skiplist_insert(sl, val)
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = arg[i]
-
-        if i > win - 1:
-            prev = arg[i - win]
-
-            if prev == prev:
-                skiplist_remove(sl, prev)
-                nobs -= 1
-
-        if val == val:
-            nobs += 1
-            skiplist_insert(sl, val)
-
-        if nobs >= minp:
-            midpoint = nobs / 2
-            if nobs % 2:
-                res = skiplist_get(sl, midpoint, &ret)
-            else:
-                res = (skiplist_get(sl, midpoint, &ret) +
-                       skiplist_get(sl, (midpoint - 1), &ret)) / 2
-        else:
-            res = NaN
-
-        output[i] = res
-
-    skiplist_destroy(sl)
-
-    return output
-
-def roll_median_cython(ndarray input, int win, int minp):
-    '''
-    O(N log(window)) implementation using skip list
-    '''
-    return _roll_skiplist_op(input, win, minp, _get_median)
-
-# Unfortunately had to resort to some hackery here, would like for
-# Cython to be able to get this right.
-
-cdef double_t _get_median(object sl, int nobs, int minp):
-    cdef Py_ssize_t midpoint
-    cdef IndexableSkiplist skiplist = <IndexableSkiplist> sl
-    if nobs >= minp:
-        midpoint = nobs / 2
-        if nobs % 2:
-            return skiplist.get(midpoint)
-        else:
-            return (skiplist.get(midpoint) +
-                    skiplist.get(midpoint - 1)) / 2
-    else:
-        return NaN
-
-#----------------------------------------------------------------------
-
-# Moving maximum / minimum code taken from Bottleneck under the terms
-# of its Simplified BSD license
-# https://github.com/kwgoodman/bottleneck
-
-cdef struct pairs:
-    double value
-    int death
-
-from libc cimport stdlib
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def roll_max2(ndarray[float64_t] a, int window, int minp):
-    "Moving max of 1d array of dtype=float64 along axis=0 ignoring NaNs."
-    cdef np.float64_t ai, aold
-    cdef Py_ssize_t count
-    cdef pairs* ring
-    cdef pairs* minpair
-    cdef pairs* end
-    cdef pairs* last
-    cdef Py_ssize_t i0
-    cdef np.npy_intp *dim
-    dim = PyArray_DIMS(a)
-    cdef Py_ssize_t n0 = dim[0]
-    cdef np.npy_intp *dims = [n0]
-    cdef np.ndarray[np.float64_t, ndim=1] y = PyArray_EMPTY(1, dims,
-		NPY_float64, 0)
-
-    if window < 1:
-        raise ValueError('Invalid window size %d'
-                         % (window))
-
-    if minp > window:
-        raise ValueError('Invalid min_periods size %d greater than window %d'
-                        % (minp, window))
-
-    minp = _check_minp(window, minp, n0)
-
-    window = min(window, n0)
-
-    ring = <pairs*>stdlib.malloc(window * sizeof(pairs))
-    end = ring + window
-    last = ring
-
-    minpair = ring
-    ai = a[0]
-    if ai == ai:
-        minpair.value = ai
-    else:
-        minpair.value = MINfloat64
-    minpair.death = window
-
-    count = 0
-    for i0 in range(n0):
-        ai = a[i0]
-        if ai == ai:
-            count += 1
-        else:
-            ai = MINfloat64
-        if i0 >= window:
-            aold = a[i0 - window]
-            if aold == aold:
-                count -= 1
-        if minpair.death == i0:
-            minpair += 1
-            if minpair >= end:
-                minpair = ring
-        if ai >= minpair.value:
-            minpair.value = ai
-            minpair.death = i0 + window
-            last = minpair
-        else:
-            while last.value <= ai:
-                if last == ring:
-                    last = end
-                last -= 1
-            last += 1
-            if last == end:
-                last = ring
-            last.value = ai
-            last.death = i0 + window
-        if count >= minp:
-            y[i0] = minpair.value
-        else:
-            y[i0] = NaN
-
-    for i0 in range(minp - 1):
-        y[i0] = NaN
-
-    stdlib.free(ring)
-    return y
-
-def roll_max(ndarray input, int win, int minp):
-    '''
-    O(N log(window)) implementation using skip list
-    '''
-    return _roll_skiplist_op(input, win, minp, _get_max)
-
-
-cdef double_t _get_max(object skiplist, int nobs, int minp):
-    if nobs >= minp:
-        return <IndexableSkiplist> skiplist.get(nobs - 1)
-    else:
-        return NaN
-
-def roll_min(ndarray input, int win, int minp):
-    '''
-    O(N log(window)) implementation using skip list
-    '''
-    return _roll_skiplist_op(input, win, minp, _get_min)
-
-@cython.boundscheck(False)
-@cython.wraparound(False)
-def roll_min2(np.ndarray[np.float64_t, ndim=1] a, int window, int minp):
-    "Moving min of 1d array of dtype=float64 along axis=0 ignoring NaNs."
-    cdef np.float64_t ai, aold
-    cdef Py_ssize_t count
-    cdef pairs* ring
-    cdef pairs* minpair
-    cdef pairs* end
-    cdef pairs* last
-    cdef Py_ssize_t i0
-    cdef np.npy_intp *dim
-    dim = PyArray_DIMS(a)
-    cdef Py_ssize_t n0 = dim[0]
-    cdef np.npy_intp *dims = [n0]
-    cdef np.ndarray[np.float64_t, ndim=1] y = PyArray_EMPTY(1, dims,
-		NPY_float64, 0)
-
-    if window < 1:
-        raise ValueError('Invalid window size %d'
-                         % (window))
-
-    if minp > window:
-        raise ValueError('Invalid min_periods size %d greater than window %d'
-                        % (minp, window))
-
-    window = min(window, n0)
-
-    minp = _check_minp(window, minp, n0)
-
-    ring = <pairs*>stdlib.malloc(window * sizeof(pairs))
-    end = ring + window
-    last = ring
-
-    minpair = ring
-    ai = a[0]
-    if ai == ai:
-        minpair.value = ai
-    else:
-        minpair.value = MAXfloat64
-    minpair.death = window
-
-    count = 0
-    for i0 in range(n0):
-        ai = a[i0]
-        if ai == ai:
-            count += 1
-        else:
-            ai = MAXfloat64
-        if i0 >= window:
-            aold = a[i0 - window]
-            if aold == aold:
-                count -= 1
-        if minpair.death == i0:
-            minpair += 1
-            if minpair >= end:
-                minpair = ring
-        if ai <= minpair.value:
-            minpair.value = ai
-            minpair.death = i0 + window
-            last = minpair
-        else:
-            while last.value >= ai:
-                if last == ring:
-                    last = end
-                last -= 1
-            last += 1
-            if last == end:
-                last = ring
-            last.value = ai
-            last.death = i0 + window
-        if count >= minp:
-            y[i0] = minpair.value
-        else:
-            y[i0] = NaN
-
-    for i0 in range(minp - 1):
-        y[i0] = NaN
-
-    stdlib.free(ring)
-    return y
-
-cdef double_t _get_min(object skiplist, int nobs, int minp):
-    if nobs >= minp:
-        return <IndexableSkiplist> skiplist.get(0)
-    else:
-        return NaN
-
-def roll_quantile(ndarray[float64_t, cast=True] input, int win,
-                  int minp, double quantile):
-    '''
-    O(N log(window)) implementation using skip list
-    '''
-    cdef double val, prev, midpoint
-    cdef IndexableSkiplist skiplist
-    cdef Py_ssize_t nobs = 0, i
-    cdef Py_ssize_t N = len(input)
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
-
-    skiplist = IndexableSkiplist(win)
-
-    minp = _check_minp(win, minp, N)
-
-    for i from 0 <= i < minp - 1:
-        val = input[i]
-
-        # Not NaN
-        if val == val:
-            nobs += 1
-            skiplist.insert(val)
-
-        output[i] = NaN
-
-    for i from minp - 1 <= i < N:
-        val = input[i]
-
-        if i > win - 1:
-            prev = input[i - win]
-
-            if prev == prev:
-                skiplist.remove(prev)
-                nobs -= 1
-
-        if val == val:
-            nobs += 1
-            skiplist.insert(val)
-
-        if nobs >= minp:
-            idx = int((quantile / 1.) * (nobs - 1))
-            output[i] = skiplist.get(idx)
-        else:
-            output[i] = NaN
-
-    return output
-
-def roll_generic(ndarray[float64_t, cast=True] input, int win,
-                 int minp, object func):
-    cdef ndarray[double_t] output, counts, bufarr
-    cdef Py_ssize_t i, n
-    cdef float64_t *buf, *oldbuf
-
-    if not input.flags.c_contiguous:
-        input = input.copy('C')
-
-    buf = <float64_t*> input.data
-
-    n = len(input)
-    if n == 0:
-        return input
-
-    minp = _check_minp(win, minp, n)
-    output = np.empty(n, dtype=float)
-    counts = roll_sum(np.isfinite(input).astype(float), win, minp)
-
-    bufarr = np.empty(win, dtype=float)
-    oldbuf = <float64_t*> bufarr.data
-
-    n = len(input)
-    for i from 0 <= i < int_min(win, n):
-        if counts[i] >= minp:
-            output[i] = func(input[int_max(i - win + 1, 0) : i + 1])
-        else:
-            output[i] = NaN
-
-    for i from win <= i < n:
-        buf = buf + 1
-        bufarr.data = <char*> buf
-        if counts[i] >= minp:
-            output[i] = func(bufarr)
-        else:
-            output[i] = NaN
-
-    bufarr.data = <char*> oldbuf
-
-    return output
diff --git a/pandas/src/parser.pyx b/pandas/src/parser.pyx
index 81f874e70..b6518701e 100644
--- a/pandas/src/parser.pyx
+++ b/pandas/src/parser.pyx
@@ -42,7 +42,7 @@ cdef bint PY3 = (sys.version_info[0] >= 3)
 cdef double INF = <double> np.inf
 cdef double NEGINF = -INF
 
-cdef extern from "stdint.h":
+cdef extern from "headers/stdint.h":
     enum: UINT8_MAX
     enum: UINT16_MAX
     enum: UINT32_MAX
diff --git a/pandas/src/properties.pyx b/pandas/src/properties.pyx
index 30184979d..67a480d3b 100644
--- a/pandas/src/properties.pyx
+++ b/pandas/src/properties.pyx
@@ -1,5 +1,6 @@
 from cpython cimport PyDict_Contains, PyDict_GetItem, PyDict_GetItem
 
+
 cdef class cache_readonly(object):
 
     cdef readonly:
diff --git a/pandas/src/reduce.pyx b/pandas/src/reduce.pyx
index 752aef2a3..54430740b 100644
--- a/pandas/src/reduce.pyx
+++ b/pandas/src/reduce.pyx
@@ -1,6 +1,7 @@
 from numpy cimport *
 import numpy as np
 
+
 cdef class Reducer:
     '''
     Performs generic reduction operation on a C or Fortran-contiguous ndarray
@@ -106,6 +107,7 @@ cdef class Reducer:
             raise ValueError('function does not reduce')
         return result
 
+
 cdef class SeriesBinGrouper:
     '''
     Performs grouping operation according to bin edges, rather than labels
@@ -155,7 +157,7 @@ cdef class SeriesBinGrouper:
             object res, chunk
             bint initialized = 0
             Slider vslider, islider
-            IndexEngine gin
+            object gin
 
         counts = np.zeros(self.ngroups, dtype=np.int64)
 
@@ -174,7 +176,7 @@ cdef class SeriesBinGrouper:
         vslider = Slider(self.arr, self.dummy)
         islider = Slider(self.index, self.dummy.index)
 
-        gin = <IndexEngine> self.dummy.index._engine
+        gin = self.dummy.index._engine
 
         try:
             for i in range(self.ngroups):
@@ -217,6 +219,7 @@ cdef class SeriesBinGrouper:
             raise ValueError('function does not reduce')
         return result
 
+
 cdef class SeriesGrouper:
     '''
     Performs generic grouping operation while avoiding ndarray construction
@@ -263,7 +266,7 @@ cdef class SeriesGrouper:
             object res, chunk
             bint initialized = 0
             Slider vslider, islider
-            IndexEngine gin
+            object gin
 
         labels = self.labels
         counts = np.zeros(self.ngroups, dtype=np.int64)
@@ -274,7 +277,7 @@ cdef class SeriesGrouper:
         vslider = Slider(self.arr, self.dummy)
         islider = Slider(self.index, self.dummy.index)
 
-        gin = <IndexEngine> self.dummy.index._engine
+        gin = self.dummy.index._engine
         try:
             for i in range(n):
                 group_size += 1
@@ -328,6 +331,7 @@ cdef class SeriesGrouper:
             raise ValueError('function does not reduce')
         return result
 
+
 cdef class Slider:
     '''
     Only handles contiguous data for now
@@ -365,6 +369,7 @@ cdef class Slider:
         self.buf.data = self.orig_data
         self.buf.strides[0] = self.orig_stride
 
+
 def reduce(arr, f, axis=0, dummy=None, labels=None):
     if labels._has_complex_internals:
         raise Exception('Cannot use shortcut')
diff --git a/pandas/src/stats.pyx b/pandas/src/stats.pyx
index 84bef827d..7f30606af 100644
--- a/pandas/src/stats.pyx
+++ b/pandas/src/stats.pyx
@@ -1,7 +1,53 @@
+from numpy cimport *
+cimport numpy as np
+
+cimport cython
+
+import_array()
+
 cdef float64_t FP_ERR = 1e-13
 
 cimport util
 
+from libc.stdlib cimport malloc, free
+
+from numpy cimport NPY_INT32 as NPY_int32
+from numpy cimport NPY_INT64 as NPY_int64
+from numpy cimport NPY_FLOAT32 as NPY_float32
+from numpy cimport NPY_FLOAT64 as NPY_float64
+
+int32 = np.dtype(np.int32)
+int64 = np.dtype(np.int64)
+float32 = np.dtype(np.float32)
+float64 = np.dtype(np.float64)
+
+cdef np.int32_t MINint32 = np.iinfo(np.int32).min
+cdef np.int64_t MINint64 = np.iinfo(np.int64).min
+cdef np.float32_t MINfloat32 = np.NINF
+cdef np.float64_t MINfloat64 = np.NINF
+
+cdef np.int32_t MAXint32 = np.iinfo(np.int32).max
+cdef np.int64_t MAXint64 = np.iinfo(np.int64).max
+cdef np.float32_t MAXfloat32 = np.inf
+cdef np.float64_t MAXfloat64 = np.inf
+
+cdef double NaN = <double> np.NaN
+cdef double nan = NaN
+
+
+cdef inline int int_max(int a, int b): return a if a >= b else b
+cdef inline int int_min(int a, int b): return a if a <= b else b
+
+
+cdef extern from "math.h":
+    double sqrt(double x)
+    double fabs(double)
+
+import lib
+
+include "skiplist.pyx"
+
+
 cdef:
     int TIEBREAK_AVERAGE = 0
     int TIEBREAK_MIN = 1
@@ -384,7 +430,7 @@ def rank_1d_generic(object in_arr, bint retry=1, ties_method='average',
     else:
         nan_value = NegInfinity()
 
-    mask = isnullobj(values)
+    mask = lib.isnullobj(values)
     np.putmask(values, mask, nan_value)
 
     n = len(values)
@@ -498,7 +544,7 @@ def rank_2d_generic(object in_arr, axis=0, ties_method='average',
     else:
         nan_value = NegInfinity()
 
-    mask = isnullobj2d(values)
+    mask = lib.isnullobj2d(values)
     np.putmask(values, mask, nan_value)
 
     n, k = (<object> values).shape
@@ -700,3 +746,2200 @@ def diff_2d_int32(ndarray[int64_t, ndim=2] arr,
             for i in range(sx):
                 for j in range(start, stop):
                     out[i, j] = arr[i, j] - arr[i, j - periods]
+
+
+# Cython implementations of rolling sum, mean, variance, skewness,
+# other statistical moment functions
+#
+# Misc implementation notes
+# -------------------------
+#
+# - In Cython x * x is faster than x ** 2 for C types, this should be
+#   periodically revisited to see if it's still true.
+#
+# -
+
+def _check_minp(win, minp, N):
+    if minp > win:
+        raise ValueError('min_periods (%d) must be <= window (%d)'
+                        % (minp, win))
+    elif minp > N:
+        minp = N + 1
+    elif minp == 0:
+        minp = 1
+    elif minp < 0:
+        raise ValueError('min_periods must be >= 0')
+    return minp
+
+# original C implementation by N. Devillard.
+# This code in public domain.
+# Function :   kth_smallest()
+# In       :   array of elements, # of elements in the array, rank k
+# Out      :   one element
+# Job      :   find the kth smallest element in the array
+
+#             Reference:
+
+#               Author: Wirth, Niklaus
+#                Title: Algorithms + data structures = programs
+#            Publisher: Englewood Cliffs: Prentice-Hall, 1976
+# Physical description: 366 p.
+#               Series: Prentice-Hall Series in Automatic Computation
+
+def kth_smallest(ndarray[double_t] a, Py_ssize_t k):
+    cdef:
+        Py_ssize_t i,j,l,m,n
+        double_t x, t
+
+    n = len(a)
+
+    l = 0
+    m = n-1
+    while (l<m):
+        x = a[k]
+        i = l
+        j = m
+
+        while 1:
+            while a[i] < x: i += 1
+            while x < a[j]: j -= 1
+            if i <= j:
+                t = a[i]
+                a[i] = a[j]
+                a[j] = t
+                i += 1; j -= 1
+
+            if i > j: break
+
+        if j < k: l = i
+        if k < i: m = j
+    return a[k]
+
+cdef inline kth_smallest_c(float64_t* a, Py_ssize_t k, Py_ssize_t n):
+    cdef:
+        Py_ssize_t i,j,l,m
+        double_t x, t
+
+    l = 0
+    m = n-1
+    while (l<m):
+        x = a[k]
+        i = l
+        j = m
+
+        while 1:
+            while a[i] < x: i += 1
+            while x < a[j]: j -= 1
+            if i <= j:
+                t = a[i]
+                a[i] = a[j]
+                a[j] = t
+                i += 1; j -= 1
+
+            if i > j: break
+
+        if j < k: l = i
+        if k < i: m = j
+    return a[k]
+
+
+def median(ndarray arr):
+    '''
+    A faster median
+    '''
+    cdef int n = len(arr)
+
+    if len(arr) == 0:
+        return np.NaN
+
+    arr = arr.copy()
+
+    if n % 2:
+        return kth_smallest(arr, n / 2)
+    else:
+        return (kth_smallest(arr, n / 2) +
+                kth_smallest(arr, n / 2 - 1)) / 2
+
+
+# -------------- Min, Max subsequence
+
+def max_subseq(ndarray[double_t] arr):
+    cdef:
+        Py_ssize_t i=0,s=0,e=0,T,n
+        double m, S
+
+    n = len(arr)
+
+    if len(arr) == 0:
+        return (-1,-1,None)
+
+    m = arr[0]
+    S = m
+    T = 0
+
+    for i in range(1, n):
+        # S = max { S + A[i], A[i] )
+        if (S > 0):
+            S = S + arr[i]
+        else:
+            S = arr[i]
+            T = i
+        if S > m:
+            s = T
+            e = i
+            m = S
+
+    return (s, e, m)
+
+def min_subseq(ndarray[double_t] arr):
+    cdef:
+        Py_ssize_t s, e
+        double m
+
+    (s, e, m) = max_subseq(-arr)
+
+    return (s, e, -m)
+
+#-------------------------------------------------------------------------------
+# Rolling sum
+
+def roll_sum(ndarray[double_t] input, int win, int minp):
+    cdef double val, prev, sum_x = 0
+    cdef int nobs = 0, i
+    cdef int N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            sum_x += val
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if val == val:
+            nobs += 1
+            sum_x += val
+
+        if i > win - 1:
+            prev = input[i - win]
+            if prev == prev:
+                sum_x -= prev
+                nobs -= 1
+
+        if nobs >= minp:
+            output[i] = sum_x
+        else:
+            output[i] = NaN
+
+    return output
+
+#-------------------------------------------------------------------------------
+# Rolling mean
+
+def roll_mean(ndarray[double_t] input,
+               int win, int minp):
+    cdef double val, prev, sum_x = 0
+    cdef Py_ssize_t nobs = 0, i
+    cdef Py_ssize_t N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            sum_x += val
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if val == val:
+            nobs += 1
+            sum_x += val
+
+        if i > win - 1:
+            prev = input[i - win]
+            if prev == prev:
+                sum_x -= prev
+                nobs -= 1
+
+        if nobs >= minp:
+            output[i] = sum_x / nobs
+        else:
+            output[i] = NaN
+
+    return output
+
+#-------------------------------------------------------------------------------
+# Exponentially weighted moving average
+
+def ewma(ndarray[double_t] input, double_t com, int adjust):
+    '''
+    Compute exponentially-weighted moving average using center-of-mass.
+
+    Parameters
+    ----------
+    input : ndarray (float64 type)
+    com : float64
+
+    Returns
+    -------
+    y : ndarray
+    '''
+
+    cdef double cur, prev, neww, oldw, adj
+    cdef Py_ssize_t i
+    cdef Py_ssize_t N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    if N == 0:
+        return output
+
+    neww = 1. / (1. + com)
+    oldw = 1. - neww
+    adj = oldw
+
+    if adjust:
+        output[0] = neww * input[0]
+    else:
+        output[0] = input[0]
+
+    for i from 1 <= i < N:
+        cur = input[i]
+        prev = output[i - 1]
+
+        if cur == cur:
+            if prev == prev:
+                output[i] = oldw * prev + neww * cur
+            else:
+                output[i] = neww * cur
+        else:
+            output[i] = prev
+
+    if adjust:
+        for i from 0 <= i < N:
+            cur = input[i]
+
+            if cur == cur:
+                output[i] = output[i] / (1. - adj)
+                adj *= oldw
+            else:
+                if i >= 1:
+                    output[i] = output[i - 1]
+
+    return output
+
+#----------------------------------------------------------------------
+# Pairwise correlation/covariance
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def nancorr(ndarray[float64_t, ndim=2] mat, cov=False, minp=None):
+    cdef:
+        Py_ssize_t i, j, xi, yi, N, K
+        ndarray[float64_t, ndim=2] result
+        ndarray[uint8_t, ndim=2] mask
+        int64_t nobs = 0
+        float64_t vx, vy, sumx, sumy, sumxx, sumyy, meanx, meany, divisor
+
+    N, K = (<object> mat).shape
+
+    if minp is None:
+        minp = 1
+
+    result = np.empty((K, K), dtype=np.float64)
+    mask = np.isfinite(mat).view(np.uint8)
+
+    for xi in range(K):
+        for yi in range(xi + 1):
+            nobs = sumxx = sumyy = sumx = sumy = 0
+            for i in range(N):
+                if mask[i, xi] and mask[i, yi]:
+                    vx = mat[i, xi]
+                    vy = mat[i, yi]
+                    nobs += 1
+                    sumx += vx
+                    sumy += vy
+
+            if nobs < minp:
+                result[xi, yi] = result[yi, xi] = np.NaN
+            else:
+                meanx = sumx / nobs
+                meany = sumy / nobs
+
+                # now the cov numerator
+                sumx = 0
+
+                for i in range(N):
+                    if mask[i, xi] and mask[i, yi]:
+                        vx = mat[i, xi] - meanx
+                        vy = mat[i, yi] - meany
+
+                        sumx += vx * vy
+                        sumxx += vx * vx
+                        sumyy += vy * vy
+
+                divisor = (nobs - 1.0) if cov else sqrt(sumxx * sumyy)
+
+                if divisor != 0:
+                    result[xi, yi] = result[yi, xi] = sumx / divisor
+                else:
+                    result[xi, yi] = result[yi, xi] = np.NaN
+
+    return result
+
+#----------------------------------------------------------------------
+# Rolling variance
+
+def roll_var(ndarray[double_t] input, int win, int minp, int ddof=1):
+    cdef double val, prev, sum_x = 0, sum_xx = 0, nobs = 0
+    cdef Py_ssize_t i
+    cdef Py_ssize_t N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            sum_x += val
+            sum_xx += val * val
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if val == val:
+            nobs += 1
+            sum_x += val
+            sum_xx += val * val
+
+        if i > win - 1:
+            prev = input[i - win]
+            if prev == prev:
+                sum_x -= prev
+                sum_xx -= prev * prev
+                nobs -= 1
+
+        if nobs >= minp:
+            # pathological case
+            if nobs == 1:
+                output[i] = 0
+                continue
+
+            val = (nobs * sum_xx - sum_x * sum_x) / (nobs * (nobs - ddof))
+            if val < 0:
+                val = 0
+
+            output[i] = val
+        else:
+            output[i] = NaN
+
+    return output
+
+#-------------------------------------------------------------------------------
+# Rolling skewness
+
+def roll_skew(ndarray[double_t] input, int win, int minp):
+    cdef double val, prev
+    cdef double x = 0, xx = 0, xxx = 0
+    cdef Py_ssize_t nobs = 0, i
+    cdef Py_ssize_t N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    # 3 components of the skewness equation
+    cdef double A, B, C, R
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            x += val
+            xx += val * val
+            xxx += val * val * val
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if val == val:
+            nobs += 1
+            x += val
+            xx += val * val
+            xxx += val * val * val
+
+        if i > win - 1:
+            prev = input[i - win]
+            if prev == prev:
+                x -= prev
+                xx -= prev * prev
+                xxx -= prev * prev * prev
+
+                nobs -= 1
+
+        if nobs >= minp:
+            A = x / nobs
+            B = xx / nobs - A * A
+            C = xxx / nobs - A * A * A - 3 * A * B
+
+            R = sqrt(B)
+
+            output[i] = ((sqrt(nobs * (nobs - 1.)) * C) /
+                         ((nobs-2) * R * R * R))
+        else:
+            output[i] = NaN
+
+    return output
+
+#-------------------------------------------------------------------------------
+# Rolling kurtosis
+
+
+def roll_kurt(ndarray[double_t] input,
+               int win, int minp):
+    cdef double val, prev
+    cdef double x = 0, xx = 0, xxx = 0, xxxx = 0
+    cdef Py_ssize_t nobs = 0, i
+    cdef Py_ssize_t N = len(input)
+
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    # 5 components of the kurtosis equation
+    cdef double A, B, C, D, R, K
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+
+            # seriously don't ask me why this is faster
+            x += val
+            xx += val * val
+            xxx += val * val * val
+            xxxx += val * val * val * val
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if val == val:
+            nobs += 1
+            x += val
+            xx += val * val
+            xxx += val * val * val
+            xxxx += val * val * val * val
+
+        if i > win - 1:
+            prev = input[i - win]
+            if prev == prev:
+                x -= prev
+                xx -= prev * prev
+                xxx -= prev * prev * prev
+                xxxx -= prev * prev * prev * prev
+
+                nobs -= 1
+
+        if nobs >= minp:
+            A = x / nobs
+            R = A * A
+            B = xx / nobs - R
+            R = R * A
+            C = xxx / nobs - R - 3 * A * B
+            R = R * A
+            D = xxxx / nobs - R - 6*B*A*A - 4*C*A
+
+            K = (nobs * nobs - 1.)*D/(B*B) - 3*((nobs-1.)**2)
+            K = K / ((nobs - 2.)*(nobs-3.))
+
+            output[i] = K
+        else:
+            output[i] = NaN
+
+    return output
+
+#-------------------------------------------------------------------------------
+# Rolling median, min, max
+
+ctypedef double_t (* skiplist_f)(object sl, int n, int p)
+
+cdef _roll_skiplist_op(ndarray arg, int win, int minp, skiplist_f op):
+    cdef ndarray[double_t] input = arg
+    cdef double val, prev, midpoint
+    cdef IndexableSkiplist skiplist
+    cdef Py_ssize_t nobs = 0, i
+
+    cdef Py_ssize_t N = len(input)
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    skiplist = IndexableSkiplist(win)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            skiplist.insert(val)
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if i > win - 1:
+            prev = input[i - win]
+
+            if prev == prev:
+                skiplist.remove(prev)
+                nobs -= 1
+
+        if val == val:
+            nobs += 1
+            skiplist.insert(val)
+
+        output[i] = op(skiplist, nobs, minp)
+
+    return output
+
+from skiplist cimport *
+
+def roll_median_c(ndarray[float64_t] arg, int win, int minp):
+    cdef double val, res, prev
+    cdef:
+        int ret=0
+        skiplist_t *sl
+        Py_ssize_t midpoint, nobs = 0, i
+
+
+    cdef Py_ssize_t N = len(arg)
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    sl = skiplist_init(win)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = arg[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            skiplist_insert(sl, val)
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = arg[i]
+
+        if i > win - 1:
+            prev = arg[i - win]
+
+            if prev == prev:
+                skiplist_remove(sl, prev)
+                nobs -= 1
+
+        if val == val:
+            nobs += 1
+            skiplist_insert(sl, val)
+
+        if nobs >= minp:
+            midpoint = nobs / 2
+            if nobs % 2:
+                res = skiplist_get(sl, midpoint, &ret)
+            else:
+                res = (skiplist_get(sl, midpoint, &ret) +
+                       skiplist_get(sl, (midpoint - 1), &ret)) / 2
+        else:
+            res = NaN
+
+        output[i] = res
+
+    skiplist_destroy(sl)
+
+    return output
+
+def roll_median_cython(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
+    return _roll_skiplist_op(input, win, minp, _get_median)
+
+# Unfortunately had to resort to some hackery here, would like for
+# Cython to be able to get this right.
+
+cdef double_t _get_median(object sl, int nobs, int minp):
+    cdef Py_ssize_t midpoint
+    cdef IndexableSkiplist skiplist = <IndexableSkiplist> sl
+    if nobs >= minp:
+        midpoint = nobs / 2
+        if nobs % 2:
+            return skiplist.get(midpoint)
+        else:
+            return (skiplist.get(midpoint) +
+                    skiplist.get(midpoint - 1)) / 2
+    else:
+        return NaN
+
+#----------------------------------------------------------------------
+
+# Moving maximum / minimum code taken from Bottleneck under the terms
+# of its Simplified BSD license
+# https://github.com/kwgoodman/bottleneck
+
+cdef struct pairs:
+    double value
+    int death
+
+from libc cimport stdlib
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def roll_max2(ndarray[float64_t] a, int window, int minp):
+    "Moving max of 1d array of dtype=float64 along axis=0 ignoring NaNs."
+    cdef np.float64_t ai, aold
+    cdef Py_ssize_t count
+    cdef pairs* ring
+    cdef pairs* minpair
+    cdef pairs* end
+    cdef pairs* last
+    cdef Py_ssize_t i0
+    cdef np.npy_intp *dim
+    dim = PyArray_DIMS(a)
+    cdef Py_ssize_t n0 = dim[0]
+    cdef np.npy_intp *dims = [n0]
+    cdef np.ndarray[np.float64_t, ndim=1] y = PyArray_EMPTY(1, dims,
+		NPY_float64, 0)
+
+    if window < 1:
+        raise ValueError('Invalid window size %d'
+                         % (window))
+
+    if minp > window:
+        raise ValueError('Invalid min_periods size %d greater than window %d'
+                        % (minp, window))
+
+    minp = _check_minp(window, minp, n0)
+
+    window = min(window, n0)
+
+    ring = <pairs*>stdlib.malloc(window * sizeof(pairs))
+    end = ring + window
+    last = ring
+
+    minpair = ring
+    ai = a[0]
+    if ai == ai:
+        minpair.value = ai
+    else:
+        minpair.value = MINfloat64
+    minpair.death = window
+
+    count = 0
+    for i0 in range(n0):
+        ai = a[i0]
+        if ai == ai:
+            count += 1
+        else:
+            ai = MINfloat64
+        if i0 >= window:
+            aold = a[i0 - window]
+            if aold == aold:
+                count -= 1
+        if minpair.death == i0:
+            minpair += 1
+            if minpair >= end:
+                minpair = ring
+        if ai >= minpair.value:
+            minpair.value = ai
+            minpair.death = i0 + window
+            last = minpair
+        else:
+            while last.value <= ai:
+                if last == ring:
+                    last = end
+                last -= 1
+            last += 1
+            if last == end:
+                last = ring
+            last.value = ai
+            last.death = i0 + window
+        if count >= minp:
+            y[i0] = minpair.value
+        else:
+            y[i0] = NaN
+
+    for i0 in range(minp - 1):
+        y[i0] = NaN
+
+    stdlib.free(ring)
+    return y
+
+def roll_max(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
+    return _roll_skiplist_op(input, win, minp, _get_max)
+
+
+cdef double_t _get_max(object skiplist, int nobs, int minp):
+    if nobs >= minp:
+        return <IndexableSkiplist> skiplist.get(nobs - 1)
+    else:
+        return NaN
+
+def roll_min(ndarray input, int win, int minp):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
+    return _roll_skiplist_op(input, win, minp, _get_min)
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def roll_min2(np.ndarray[np.float64_t, ndim=1] a, int window, int minp):
+    "Moving min of 1d array of dtype=float64 along axis=0 ignoring NaNs."
+    cdef np.float64_t ai, aold
+    cdef Py_ssize_t count
+    cdef pairs* ring
+    cdef pairs* minpair
+    cdef pairs* end
+    cdef pairs* last
+    cdef Py_ssize_t i0
+    cdef np.npy_intp *dim
+    dim = PyArray_DIMS(a)
+    cdef Py_ssize_t n0 = dim[0]
+    cdef np.npy_intp *dims = [n0]
+    cdef np.ndarray[np.float64_t, ndim=1] y = PyArray_EMPTY(1, dims,
+		NPY_float64, 0)
+
+    if window < 1:
+        raise ValueError('Invalid window size %d'
+                         % (window))
+
+    if minp > window:
+        raise ValueError('Invalid min_periods size %d greater than window %d'
+                        % (minp, window))
+
+    window = min(window, n0)
+
+    minp = _check_minp(window, minp, n0)
+
+    ring = <pairs*>stdlib.malloc(window * sizeof(pairs))
+    end = ring + window
+    last = ring
+
+    minpair = ring
+    ai = a[0]
+    if ai == ai:
+        minpair.value = ai
+    else:
+        minpair.value = MAXfloat64
+    minpair.death = window
+
+    count = 0
+    for i0 in range(n0):
+        ai = a[i0]
+        if ai == ai:
+            count += 1
+        else:
+            ai = MAXfloat64
+        if i0 >= window:
+            aold = a[i0 - window]
+            if aold == aold:
+                count -= 1
+        if minpair.death == i0:
+            minpair += 1
+            if minpair >= end:
+                minpair = ring
+        if ai <= minpair.value:
+            minpair.value = ai
+            minpair.death = i0 + window
+            last = minpair
+        else:
+            while last.value >= ai:
+                if last == ring:
+                    last = end
+                last -= 1
+            last += 1
+            if last == end:
+                last = ring
+            last.value = ai
+            last.death = i0 + window
+        if count >= minp:
+            y[i0] = minpair.value
+        else:
+            y[i0] = NaN
+
+    for i0 in range(minp - 1):
+        y[i0] = NaN
+
+    stdlib.free(ring)
+    return y
+
+cdef double_t _get_min(object skiplist, int nobs, int minp):
+    if nobs >= minp:
+        return <IndexableSkiplist> skiplist.get(0)
+    else:
+        return NaN
+
+def roll_quantile(ndarray[float64_t, cast=True] input, int win,
+                  int minp, double quantile):
+    '''
+    O(N log(window)) implementation using skip list
+    '''
+    cdef double val, prev, midpoint
+    cdef IndexableSkiplist skiplist
+    cdef Py_ssize_t nobs = 0, i
+    cdef Py_ssize_t N = len(input)
+    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+
+    skiplist = IndexableSkiplist(win)
+
+    minp = _check_minp(win, minp, N)
+
+    for i from 0 <= i < minp - 1:
+        val = input[i]
+
+        # Not NaN
+        if val == val:
+            nobs += 1
+            skiplist.insert(val)
+
+        output[i] = NaN
+
+    for i from minp - 1 <= i < N:
+        val = input[i]
+
+        if i > win - 1:
+            prev = input[i - win]
+
+            if prev == prev:
+                skiplist.remove(prev)
+                nobs -= 1
+
+        if val == val:
+            nobs += 1
+            skiplist.insert(val)
+
+        if nobs >= minp:
+            idx = int((quantile / 1.) * (nobs - 1))
+            output[i] = skiplist.get(idx)
+        else:
+            output[i] = NaN
+
+    return output
+
+def roll_generic(ndarray[float64_t, cast=True] input, int win,
+                 int minp, object func):
+    cdef ndarray[double_t] output, counts, bufarr
+    cdef Py_ssize_t i, n
+    cdef float64_t *buf, *oldbuf
+
+    if not input.flags.c_contiguous:
+        input = input.copy('C')
+
+    buf = <float64_t*> input.data
+
+    n = len(input)
+    if n == 0:
+        return input
+
+    minp = _check_minp(win, minp, n)
+    output = np.empty(n, dtype=float)
+    counts = roll_sum(np.isfinite(input).astype(float), win, minp)
+
+    bufarr = np.empty(win, dtype=float)
+    oldbuf = <float64_t*> bufarr.data
+
+    n = len(input)
+    for i from 0 <= i < int_min(win, n):
+        if counts[i] >= minp:
+            output[i] = func(input[int_max(i - win + 1, 0) : i + 1])
+        else:
+            output[i] = NaN
+
+    for i from win <= i < n:
+        buf = buf + 1
+        bufarr.data = <char*> buf
+        if counts[i] >= minp:
+            output[i] = func(bufarr)
+        else:
+            output[i] = NaN
+
+    bufarr.data = <char*> oldbuf
+
+    return output
+
+
+#----------------------------------------------------------------------
+# group operations
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def groupsort_indexer(ndarray[int64_t] index, Py_ssize_t ngroups):
+    cdef:
+        Py_ssize_t i, loc, label, n
+        ndarray[int64_t] counts, where, result
+
+    # count group sizes, location 0 for NA
+    counts = np.zeros(ngroups + 1, dtype=np.int64)
+    n = len(index)
+    for i from 0 <= i < n:
+        counts[index[i] + 1] += 1
+
+    # mark the start of each contiguous group of like-indexed data
+    where = np.zeros(ngroups + 1, dtype=np.int64)
+    for i from 1 <= i < ngroups + 1:
+        where[i] = where[i - 1] + counts[i - 1]
+
+    # this is our indexer
+    result = np.zeros(n, dtype=np.int64)
+    for i from 0 <= i < n:
+        label = index[i] + 1
+        result[where[label]] = i
+        where[label] += 1
+
+    return result, counts
+
+# TODO: aggregate multiple columns in single pass
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_add(ndarray[float64_t, ndim=2] out,
+              ndarray[int64_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    sumx[lab, j] += val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                sumx[lab, 0] += val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_prod(ndarray[float64_t, ndim=2] out,
+               ndarray[int64_t] counts,
+               ndarray[float64_t, ndim=2] values,
+               ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] prodx, nobs
+
+    nobs = np.zeros_like(out)
+    prodx = np.ones_like(out)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    prodx[lab, j] *= val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                prodx[lab, 0] *= val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = prodx[i, j]
+
+#----------------------------------------------------------------------
+# first, nth, last
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_nth(ndarray[float64_t, ndim=2] out,
+              ndarray[int64_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int64_t] labels, int64_t rank):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] resx
+        ndarray[int64_t, ndim=2] nobs
+
+    nobs = np.zeros((<object> out).shape, dtype=np.int64)
+    resx = np.empty_like(out)
+
+    N, K = (<object> values).shape
+
+    for i in range(N):
+        lab = labels[i]
+        if lab < 0:
+            continue
+
+        counts[lab] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[lab, j] += 1
+                if nobs[lab, j] == rank:
+                    resx[lab, j] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_nth_object(ndarray[object, ndim=2] out,
+                     ndarray[int64_t] counts,
+                     ndarray[object, ndim=2] values,
+                     ndarray[int64_t] labels,
+                     int64_t rank):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        object val
+        float64_t count
+        ndarray[int64_t, ndim=2] nobs
+        ndarray[object, ndim=2] resx
+
+    nobs = np.zeros((<object> out).shape, dtype=np.int64)
+    resx = np.empty((<object> out).shape, dtype=object)
+
+    N, K = (<object> values).shape
+
+    for i in range(N):
+        lab = labels[i]
+        if lab < 0:
+            continue
+
+        counts[lab] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[lab, j] += 1
+                if nobs[lab, j] == rank:
+                    resx[lab, j] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = <object> nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_nth_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins, int64_t rank):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] resx, nobs
+
+    nobs = np.zeros_like(out)
+    resx = np.empty_like(out)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    for i in range(N):
+        while b < ngroups - 1 and i >= bins[b]:
+            b += 1
+
+        counts[b] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[b, j] += 1
+                if nobs[b, j] == rank:
+                    resx[b, j] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_nth_bin_object(ndarray[object, ndim=2] out,
+                         ndarray[int64_t] counts,
+                         ndarray[object, ndim=2] values,
+                         ndarray[int64_t] bins, int64_t rank):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        object val
+        float64_t count
+        ndarray[object, ndim=2] resx
+        ndarray[float64_t, ndim=2] nobs
+
+    nobs = np.zeros((<object> out).shape, dtype=np.float64)
+    resx = np.empty((<object> out).shape, dtype=object)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    for i in range(N):
+        while b < ngroups - 1 and i >= bins[b]:
+            b += 1
+
+        counts[b] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[b, j] += 1
+                if nobs[b, j] == rank:
+                    resx[b, j] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_last(ndarray[float64_t, ndim=2] out,
+               ndarray[int64_t] counts,
+               ndarray[float64_t, ndim=2] values,
+               ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] resx
+        ndarray[int64_t, ndim=2] nobs
+
+    nobs = np.zeros((<object> out).shape, dtype=np.int64)
+    resx = np.empty_like(out)
+
+    N, K = (<object> values).shape
+
+    for i in range(N):
+        lab = labels[i]
+        if lab < 0:
+            continue
+
+        counts[lab] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[lab, j] += 1
+                resx[lab, j] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_last_object(ndarray[object, ndim=2] out,
+                      ndarray[int64_t] counts,
+                      ndarray[object, ndim=2] values,
+                      ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        object val
+        float64_t count
+        ndarray[object, ndim=2] resx
+        ndarray[int64_t, ndim=2] nobs
+
+    nobs = np.zeros((<object> out).shape, dtype=np.int64)
+    resx = np.empty((<object> out).shape, dtype=object)
+
+    N, K = (<object> values).shape
+
+    for i in range(N):
+        lab = labels[i]
+        if lab < 0:
+            continue
+
+        counts[lab] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[lab, j] += 1
+                resx[lab, j] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_last_bin(ndarray[float64_t, ndim=2] out,
+                   ndarray[int64_t] counts,
+                   ndarray[float64_t, ndim=2] values,
+                   ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] resx, nobs
+
+    nobs = np.zeros_like(out)
+    resx = np.empty_like(out)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    for i in range(N):
+        while b < ngroups - 1 and i >= bins[b]:
+            b += 1
+
+        counts[b] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[b, j] += 1
+                resx[b, j] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_last_bin_object(ndarray[object, ndim=2] out,
+                          ndarray[int64_t] counts,
+                          ndarray[object, ndim=2] values,
+                          ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        object val
+        float64_t count
+        ndarray[object, ndim=2] resx
+        ndarray[float64_t, ndim=2] nobs
+
+    nobs = np.zeros((<object> out).shape, dtype=np.float64)
+    resx = np.empty((<object> out).shape, dtype=object)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    for i in range(N):
+        while b < ngroups - 1 and i >= bins[b]:
+            b += 1
+
+        counts[b] += 1
+        for j in range(K):
+            val = values[i, j]
+
+            # not nan
+            if val == val:
+                nobs[b, j] += 1
+                resx[b, j] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = resx[i, j]
+
+#----------------------------------------------------------------------
+# group_min, group_max
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_min(ndarray[float64_t, ndim=2] out,
+              ndarray[int64_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] minx, nobs
+
+    nobs = np.zeros_like(out)
+
+    minx = np.empty_like(out)
+    minx.fill(np.inf)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    if val < minx[lab, j]:
+                        minx[lab, j] = val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                if val < minx[lab, 0]:
+                    minx[lab, 0] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = minx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_max(ndarray[float64_t, ndim=2] out,
+              ndarray[int64_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] maxx, nobs
+
+    nobs = np.zeros_like(out)
+
+    maxx = np.empty_like(out)
+    maxx.fill(-np.inf)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    if val > maxx[lab, j]:
+                        maxx[lab, j] = val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                if val > maxx[lab, 0]:
+                    maxx[lab, 0] = val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = maxx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_mean(ndarray[float64_t, ndim=2] out,
+               ndarray[int64_t] counts,
+               ndarray[float64_t, ndim=2] values,
+               ndarray[int64_t] labels):
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            for j in range(K):
+                val = values[i, j]
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    sumx[lab, j] += val
+    else:
+        for i in range(N):
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                sumx[lab, 0] += val
+
+    for i in range(len(counts)):
+        for j in range(K):
+            count = nobs[i, j]
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j] / count
+
+
+def group_median(ndarray[float64_t, ndim=2] out,
+                 ndarray[int64_t] counts,
+                 ndarray[float64_t, ndim=2] values,
+                 ndarray[int64_t] labels):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, size
+        ndarray[int64_t] _counts
+        ndarray data
+        float64_t* ptr
+
+    from pandas._algos import take_2d_axis1_float64
+
+    ngroups = len(counts)
+    N, K = (<object> values).shape
+
+    indexer, _counts = groupsort_indexer(labels, ngroups)
+    counts[:] = _counts[1:]
+
+    data = np.empty((K, N), dtype=np.float64)
+    ptr = <float64_t*> data.data
+
+    take_2d_axis1_float64(values.T, indexer, out=data)
+
+    for i in range(K):
+        # exclude NA group
+        ptr += _counts[0]
+        for j in range(ngroups):
+            size = _counts[j + 1]
+            out[j, i] = _median_linear(ptr, size)
+            ptr += size
+
+
+cdef inline float64_t _median_linear(float64_t* a, int n):
+    cdef int i, j, na_count = 0
+    cdef float64_t result
+    cdef float64_t* tmp
+
+    # count NAs
+    for i in range(n):
+        if a[i] != a[i]:
+            na_count += 1
+
+    if na_count:
+        if na_count == n:
+            return NaN
+
+        tmp = <float64_t*> malloc((n - na_count) * sizeof(float64_t))
+
+        j = 0
+        for i in range(n):
+            if a[i] == a[i]:
+                tmp[j] = a[i]
+                j += 1
+
+        a = tmp
+        n -= na_count
+
+
+    if n % 2:
+        result = kth_smallest_c(a, n / 2, n)
+    else:
+        result = (kth_smallest_c(a, n / 2, n) +
+                  kth_smallest_c(a, n / 2 - 1, n)) / 2
+
+    if na_count:
+        free(a)
+
+    return result
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_var(ndarray[float64_t, ndim=2] out,
+              ndarray[int64_t] counts,
+              ndarray[float64_t, ndim=2] values,
+              ndarray[int64_t] labels):
+    cdef:
+        Py_ssize_t i, j, N, K, lab
+        float64_t val, ct
+        ndarray[float64_t, ndim=2] nobs, sumx, sumxx
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+    sumxx = np.zeros_like(out)
+
+    N, K = (<object> values).shape
+
+    if K > 1:
+        for i in range(N):
+
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[lab, j] += 1
+                    sumx[lab, j] += val
+                    sumxx[lab, j] += val * val
+    else:
+        for i in range(N):
+
+            lab = labels[i]
+            if lab < 0:
+                continue
+
+            counts[lab] += 1
+            val = values[i, 0]
+            # not nan
+            if val == val:
+                nobs[lab, 0] += 1
+                sumx[lab, 0] += val
+                sumxx[lab, 0] += val * val
+
+
+    for i in range(len(counts)):
+        for j in range(K):
+            ct = nobs[i, j]
+            if ct < 2:
+                out[i, j] = nan
+            else:
+                out[i, j] = ((ct * sumxx[i, j] - sumx[i, j] * sumx[i, j]) /
+                             (ct * ct - ct))
+# add passing bin edges, instead of labels
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_add_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b, nbins
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+    N, K = (<object> values).shape
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    sumx[b, j] += val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                sumx[b, 0] += val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_prod_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] prodx, nobs
+
+    nobs = np.zeros_like(out)
+    prodx = np.ones_like(out)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+    N, K = (<object> values).shape
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    prodx[b, j] *= val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                prodx[b, 0] *= val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = prodx[i, j]
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_min_bin(ndarray[float64_t, ndim=2] out,
+                   ndarray[int64_t] counts,
+                   ndarray[float64_t, ndim=2] values,
+                   ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] minx, nobs
+
+    nobs = np.zeros_like(out)
+
+    minx = np.empty_like(out)
+    minx.fill(np.inf)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    if val < minx[b, j]:
+                        minx[b, j] = val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                if val < minx[b, 0]:
+                    minx[b, 0] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = minx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_max_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] maxx, nobs
+
+    nobs = np.zeros_like(out)
+    maxx = np.empty_like(out)
+    maxx.fill(-np.inf)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    if val > maxx[b, j]:
+                        maxx[b, j] = val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                if val > maxx[b, 0]:
+                    maxx[b, 0] = val
+
+    for i in range(ngroups):
+        for j in range(K):
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = maxx[i, j]
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_ohlc(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins):
+    '''
+    Only aggregates on axis=0
+    '''
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        float64_t vopen, vhigh, vlow, vclose, NA
+        bint got_first = 0
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    if out.shape[1] != 4:
+        raise ValueError('Output array must have 4 columns')
+
+    NA = np.nan
+
+    b = 0
+    if K > 1:
+        raise NotImplementedError
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                if not got_first:
+                    out[b, 0] = NA
+                    out[b, 1] = NA
+                    out[b, 2] = NA
+                    out[b, 3] = NA
+                else:
+                    out[b, 0] = vopen
+                    out[b, 1] = vhigh
+                    out[b, 2] = vlow
+                    out[b, 3] = vclose
+                b += 1
+                got_first = 0
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                if not got_first:
+                    got_first = 1
+                    vopen = val
+                    vlow = val
+                    vhigh = val
+                else:
+                    if val < vlow:
+                        vlow = val
+                    if val > vhigh:
+                        vhigh = val
+                vclose = val
+
+        if not got_first:
+            out[b, 0] = NA
+            out[b, 1] = NA
+            out[b, 2] = NA
+            out[b, 3] = NA
+        else:
+            out[b, 0] = vopen
+            out[b, 1] = vhigh
+            out[b, 2] = vlow
+            out[b, 3] = vclose
+
+
+# @cython.boundscheck(False)
+# @cython.wraparound(False)
+def group_mean_bin(ndarray[float64_t, ndim=2] out,
+                   ndarray[int64_t] counts,
+                   ndarray[float64_t, ndim=2] values,
+                   ndarray[int64_t] bins):
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, count
+        ndarray[float64_t, ndim=2] sumx, nobs
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+
+    N, K = (<object> values).shape
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    sumx[b, j] += val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                sumx[b, 0] += val
+
+    for i in range(ngroups):
+        for j in range(K):
+            count = nobs[i, j]
+            if nobs[i, j] == 0:
+                out[i, j] = nan
+            else:
+                out[i, j] = sumx[i, j] / count
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def group_var_bin(ndarray[float64_t, ndim=2] out,
+                  ndarray[int64_t] counts,
+                  ndarray[float64_t, ndim=2] values,
+                  ndarray[int64_t] bins):
+
+    cdef:
+        Py_ssize_t i, j, N, K, ngroups, b
+        float64_t val, ct
+        ndarray[float64_t, ndim=2] nobs, sumx, sumxx
+
+    nobs = np.zeros_like(out)
+    sumx = np.zeros_like(out)
+    sumxx = np.zeros_like(out)
+
+    if bins[len(bins) - 1] == len(values):
+        ngroups = len(bins)
+    else:
+        ngroups = len(bins) + 1
+
+    N, K = (<object> values).shape
+
+    b = 0
+    if K > 1:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+
+            for j in range(K):
+                val = values[i, j]
+
+                # not nan
+                if val == val:
+                    nobs[b, j] += 1
+                    sumx[b, j] += val
+                    sumxx[b, j] += val * val
+    else:
+        for i in range(N):
+            while b < ngroups - 1 and i >= bins[b]:
+                b += 1
+
+            counts[b] += 1
+            val = values[i, 0]
+
+            # not nan
+            if val == val:
+                nobs[b, 0] += 1
+                sumx[b, 0] += val
+                sumxx[b, 0] += val * val
+
+    for i in range(ngroups):
+        for j in range(K):
+            ct = nobs[i, j]
+            if ct < 2:
+                out[i, j] = nan
+            else:
+                out[i, j] = ((ct * sumxx[i, j] - sumx[i, j] * sumx[i, j]) /
+                             (ct * ct - ct))
+
+include "join.pyx"
diff --git a/pandas/src/tseries.pyx b/pandas/src/tseries.pyx
index 1c733664d..69be22892 100644
--- a/pandas/src/tseries.pyx
+++ b/pandas/src/tseries.pyx
@@ -1,751 +1,1971 @@
+# cython: profile=False
 cimport numpy as np
-cimport cython
-import numpy as np
 
-from numpy cimport *
-from numpy cimport NPY_INT32 as NPY_int32
-from numpy cimport NPY_INT64 as NPY_int64
-from numpy cimport NPY_FLOAT32 as NPY_float32
-from numpy cimport NPY_FLOAT64 as NPY_float64
+from numpy cimport (int32_t, int64_t, import_array, ndarray,
+                    NPY_INT64, NPY_DATETIME)
+from cpython cimport *
 
-int32 = np.dtype(np.int32)
-int64 = np.dtype(np.int64)
-float32 = np.dtype(np.float32)
-float64 = np.dtype(np.float64)
+from libc.stdlib cimport free
 
-cdef np.int32_t MINint32 = np.iinfo(np.int32).min
-cdef np.int64_t MINint64 = np.iinfo(np.int64).min
-cdef np.float32_t MINfloat32 = np.NINF
-cdef np.float64_t MINfloat64 = np.NINF
+from util cimport is_integer_object, is_datetime64_object
+cimport util
 
-cdef np.int32_t MAXint32 = np.iinfo(np.int32).max
-cdef np.int64_t MAXint64 = np.iinfo(np.int64).max
-cdef np.float32_t MAXfloat32 = np.inf
-cdef np.float64_t MAXfloat64 = np.inf
+from datetime cimport *
+from khash cimport *
+cimport cython
 
+import _algos
 
-cdef extern from "numpy/arrayobject.h":
-    cdef enum NPY_TYPES:
-        NPY_intp "NPY_INTP"
+import numpy as np
+from datetime import timedelta, datetime
+from dateutil.parser import parse as parse_date
 
-from cpython cimport (PyDict_New, PyDict_GetItem, PyDict_SetItem,
-                      PyDict_Contains, PyDict_Keys,
-                      Py_INCREF, PyTuple_SET_ITEM,
-                      PyTuple_SetItem,
-                      PyTuple_New)
-from cpython cimport PyFloat_Check
-cimport cpython
+cdef extern from "Python.h":
+    int PySlice_Check(object)
 
-isnan = np.isnan
-cdef double NaN = <double> np.NaN
-cdef double nan = NaN
-cdef double NAN = nan
+# initialize numpy
+import_array()
+#import_ufunc()
 
-from datetime import datetime as pydatetime
+# import datetime C API
+PyDateTime_IMPORT
 
-# this is our datetime.pxd
-from datetime cimport *
+# in numpy 1.7, will prob need the following:
+# numpy_pydatetime_import
 
 cdef int64_t NPY_NAT = util.get_nat()
 
-from khash cimport *
 
-cdef inline int int_max(int a, int b): return a if a >= b else b
-cdef inline int int_min(int a, int b): return a if a <= b else b
+try:
+    basestring
+except NameError: # py3
+    basestring = str
 
-ctypedef unsigned char UChar
+def ints_to_pydatetime(ndarray[int64_t] arr, tz=None):
+    cdef:
+        Py_ssize_t i, n = len(arr)
+        pandas_datetimestruct dts
+        ndarray[object] result = np.empty(n, dtype=object)
+
+    if tz is not None:
+        if _is_utc(tz):
+            for i in range(n):
+                pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
+                result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
+                                     dts.min, dts.sec, dts.us, tz)
+        elif _is_tzlocal(tz):
+            for i in range(n):
+                pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
+                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                              dts.min, dts.sec, dts.us, tz)
+                result[i] = dt + tz.utcoffset(dt)
+        else:
+            trans = _get_transitions(tz)
+            deltas = _get_deltas(tz)
+            for i in range(n):
+                # Adjust datetime64 timestamp, recompute datetimestruct
+                pos = trans.searchsorted(arr[i]) - 1
+                inf = tz._transition_info[pos]
+
+                pandas_datetime_to_datetimestruct(arr[i] + deltas[pos],
+                                                  PANDAS_FR_ns, &dts)
+                result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
+                                     dts.min, dts.sec, dts.us,
+                                     tz._tzinfos[inf])
+    else:
+        for i in range(n):
+            pandas_datetime_to_datetimestruct(arr[i], PANDAS_FR_ns, &dts)
+            result[i] = datetime(dts.year, dts.month, dts.day, dts.hour,
+                                 dts.min, dts.sec, dts.us)
 
-cimport util
-from util cimport is_array, _checknull, _checknan
+    return result
 
-cdef extern from "stdint.h":
-    enum: UINT8_MAX
+from dateutil.tz import tzlocal
 
+def _is_tzlocal(tz):
+    return isinstance(tz, tzlocal)
 
-cdef extern from "math.h":
-    double sqrt(double x)
-    double fabs(double)
+# Python front end to C extension type _Timestamp
+# This serves as the box for datetime64
+class Timestamp(_Timestamp):
 
-# import datetime C API
-PyDateTime_IMPORT
+    def __new__(cls, object ts_input, object offset=None, tz=None):
+        cdef _TSObject ts
+        cdef _Timestamp ts_base
 
-# initialize numpy
-import_array()
-import_ufunc()
+        if isinstance(ts_input, float):
+            # to do, do we want to support this, ie with fractional seconds?
+            raise TypeError("Cannot convert a float to datetime")
 
-cpdef map_indices_list(list index):
-    '''
-    Produce a dict mapping the values of the input array to their respective
-    locations.
+        if isinstance(ts_input, basestring):
+            try:
+                ts_input = parse_date(ts_input)
+            except Exception:
+                pass
 
-    Example:
-        array(['hi', 'there']) --> {'hi' : 0 , 'there' : 1}
+        ts = convert_to_tsobject(ts_input, tz)
 
-    Better to do this with Cython because of the enormous speed boost.
-    '''
-    cdef Py_ssize_t i, length
-    cdef dict result = {}
+        if ts.value == NPY_NAT:
+            return NaT
 
-    length = len(index)
+        # make datetime happy
+        ts_base = _Timestamp.__new__(cls, ts.dts.year, ts.dts.month,
+                                     ts.dts.day, ts.dts.hour, ts.dts.min,
+                                     ts.dts.sec, ts.dts.us, ts.tzinfo)
 
-    for i from 0 <= i < length:
-        result[index[i]] = i
+        # fill out rest of data
+        ts_base.value = ts.value
+        ts_base.offset = offset
+        ts_base.nanosecond = ts.dts.ps / 1000
 
-    return result
+        return ts_base
 
+    def __repr__(self):
+        result = self._repr_base
 
-from libc.stdlib cimport malloc, free
+        try:
+            result += self.strftime('%z')
+            if self.tzinfo:
+                zone = _get_zone(self.tzinfo)
+                result += self.strftime(' %%Z, tz=%s' % zone)
+        except ValueError:
+            year2000 = self.replace(year=2000)
+            result += year2000.strftime('%z')
+            if self.tzinfo:
+                zone = _get_zone(self.tzinfo)
+                result += year2000.strftime(' %%Z, tz=%s' % zone)
+
+        return '<Timestamp: %s>' % result
+
+    @property
+    def _repr_base(self):
+        result = '%d-%.2d-%.2d %.2d:%.2d:%.2d' % (self.year, self.month,
+                                                  self.day, self.hour,
+                                                  self.minute, self.second)
+
+        if self.nanosecond != 0:
+            nanos = self.nanosecond + 1000 * self.microsecond
+            result += '.%.9d' % nanos
+        elif self.microsecond != 0:
+            result += '.%.6d' % self.microsecond
+
+        return result
+
+    @property
+    def tz(self):
+        """
+        Alias for tzinfo
+        """
+        return self.tzinfo
+
+    @property
+    def freq(self):
+        return self.offset
+
+    def __setstate__(self, state):
+        self.value = state[0]
+        self.offset = state[1]
+        self.tzinfo = state[2]
+
+    def __reduce__(self):
+        object_state = self.value, self.offset, self.tzinfo
+        return (Timestamp, object_state)
+
+    def to_period(self, freq=None):
+        """
+        Return an period of which this timestamp is an observation.
+        """
+        from pandas.tseries.period import Period
+
+        if freq is None:
+            freq = self.freq
+
+        return Period(self, freq=freq)
+
+    @property
+    def dayofweek(self):
+        return self.weekday()
+
+    @property
+    def dayofyear(self):
+        return self._get_field('doy')
+
+    @property
+    def week(self):
+        return self._get_field('woy')
+
+    weekofyear = week
+
+    @property
+    def quarter(self):
+        return self._get_field('q')
+
+    @property
+    def freqstr(self):
+        return getattr(self.offset, 'freqstr', self.offset)
+
+    @property
+    def asm8(self):
+        return np.int64(self.value).view('M8[ns]')
+
+    def tz_localize(self, tz):
+        """
+        Convert naive Timestamp to local time zone
+
+        Parameters
+        ----------
+        tz : pytz.timezone
+
+        Returns
+        -------
+        localized : Timestamp
+        """
+        if self.tzinfo is None:
+            # tz naive, localize
+            return Timestamp(self.to_pydatetime(), tz=tz)
+        else:
+            raise Exception('Cannot localize tz-aware Timestamp, use '
+                            'tz_convert for conversions')
+
+    def tz_convert(self, tz):
+        """
+        Convert Timestamp to another time zone or localize to requested time
+        zone
+
+        Parameters
+        ----------
+        tz : pytz.timezone
+
+        Returns
+        -------
+        converted : Timestamp
+        """
+        if self.tzinfo is None:
+            # tz naive, use tz_localize
+            raise Exception('Cannot convert tz-naive Timestamp, use '
+                            'tz_localize to localize')
+        else:
+            # Same UTC timestamp, different time zone
+            return Timestamp(self.value, tz=tz)
 
-def ismember(ndarray arr, set values):
-    '''
-    Checks whether
+    astimezone = tz_convert
 
-    Parameters
-    ----------
-    arr : ndarray
-    values : set
+    def replace(self, **kwds):
+        return Timestamp(datetime.replace(self, **kwds),
+                         offset=self.offset)
 
-    Returns
-    -------
-    ismember : ndarray (boolean dtype)
-    '''
-    cdef:
-        Py_ssize_t i, n
-        ndarray[uint8_t] result
-        object val
+    def to_pydatetime(self, warn=True):
+        """
+        If warn=True, issue warning if nanoseconds is nonzero
+        """
+        cdef:
+            pandas_datetimestruct dts
+            _TSObject ts
 
-    n = len(arr)
-    result = np.empty(n, dtype=np.uint8)
-    for i in range(n):
-        val = util.get_value_at(arr, i)
-        if val in values:
-            result[i] = 1
-        else:
-            result[i] = 0
+        if self.nanosecond != 0 and warn:
+            print 'Warning: discarding nonzero nanoseconds'
+        ts = convert_to_tsobject(self, self.tzinfo)
 
-    return result.view(np.bool_)
+        return datetime(ts.dts.year, ts.dts.month, ts.dts.day,
+                        ts.dts.hour, ts.dts.min, ts.dts.sec,
+                        ts.dts.us, ts.tzinfo)
 
-#----------------------------------------------------------------------
-# datetime / io related
 
-cdef int _EPOCH_ORD = 719163
+class NaTType(_NaT):
 
-from datetime import date as pydate
+    def __new__(cls):
+        cdef _NaT base
 
-cdef inline int64_t gmtime(object date):
-    cdef int y, m, d, h, mn, s, days
+        base = _NaT.__new__(cls, 1, 1, 1)
+        mangle_nat(base)
+        base.value = NPY_NAT
 
-    y = PyDateTime_GET_YEAR(date)
-    m = PyDateTime_GET_MONTH(date)
-    d = PyDateTime_GET_DAY(date)
-    h = PyDateTime_DATE_GET_HOUR(date)
-    mn = PyDateTime_DATE_GET_MINUTE(date)
-    s = PyDateTime_DATE_GET_SECOND(date)
+        return base
 
-    days = pydate(y, m, 1).toordinal() - _EPOCH_ORD + d - 1
-    return ((<int64_t> (((days * 24 + h) * 60 + mn))) * 60 + s) * 1000
+    def __repr__(self):
+        return 'NaT'
 
-cpdef object to_datetime(int64_t timestamp):
-    return pydatetime.utcfromtimestamp(timestamp / 1000.0)
+    def weekday(self):
+        return -1
 
-cpdef object to_timestamp(object dt):
-    return gmtime(dt)
+    def toordinal(self):
+        return -1
 
-def array_to_timestamp(ndarray[object, ndim=1] arr):
-    cdef int i, n
-    cdef ndarray[int64_t, ndim=1] result
+fields = ['year', 'quarter', 'month', 'day', 'hour',
+          'minute', 'second', 'microsecond', 'nanosecond',
+          'week', 'dayofyear']
+for field in fields:
+    prop = property(fget=lambda self: -1)
+    setattr(NaTType, field, prop)
 
-    n = len(arr)
-    result = np.empty(n, dtype=np.int64)
 
-    for i from 0 <= i < n:
-        result[i] = gmtime(arr[i])
+NaT = NaTType()
 
-    return result
+iNaT = util.get_nat()
 
-def time64_to_datetime(ndarray[int64_t, ndim=1] arr):
-    cdef int i, n
-    cdef ndarray[object, ndim=1] result
 
-    n = len(arr)
-    result = np.empty(n, dtype=object)
+cdef inline bint is_timestamp(object o):
+    return isinstance(o, Timestamp)
 
-    for i from 0 <= i < n:
-        result[i] = to_datetime(arr[i])
+def is_timestamp_array(ndarray[object] values):
+    cdef int i, n = len(values)
+    if n == 0:
+        return False
+    for i in range(n):
+        if not is_timestamp(values[i]):
+            return False
+    return True
 
-    return result
 
-#----------------------------------------------------------------------
-# isnull / notnull related
-
-cdef double INF = <double> np.inf
-cdef double NEGINF = -INF
-
-cpdef checknull(object val):
-    if util.is_float_object(val) or util.is_complex_object(val):
-        return val != val or val == INF or val == NEGINF
-    elif util.is_datetime64_object(val):
-        return get_datetime64_value(val) == NPY_NAT
-    elif isinstance(val, _NaT):
-        return True
-    elif is_array(val):
-        return False
+cpdef object get_value_box(ndarray arr, object loc):
+    cdef:
+        Py_ssize_t i, sz
+        void* data_ptr
+    if util.is_float_object(loc):
+        casted = int(loc)
+        if casted == loc:
+            loc = casted
+    i = <Py_ssize_t> loc
+    sz = np.PyArray_SIZE(arr)
+
+    if i < 0 and sz > 0:
+        i += sz
+    elif i >= sz or sz == 0:
+        raise IndexError('index out of bounds')
+
+    if arr.descr.type_num == NPY_DATETIME:
+        return Timestamp(util.get_value_1d(arr, i))
     else:
-        return util._checknull(val)
+        return util.get_value_1d(arr, i)
 
-def isscalar(object val):
-    return np.isscalar(val) or val is None or isinstance(val, _Timestamp)
 
+#----------------------------------------------------------------------
+# Frequency inference
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def isnullobj(ndarray[object] arr):
-    cdef Py_ssize_t i, n
-    cdef object val
-    cdef ndarray[uint8_t] result
+def unique_deltas(ndarray[int64_t] arr):
+    cdef:
+        Py_ssize_t i, n = len(arr)
+        int64_t val
+        khiter_t k
+        kh_int64_t *table
+        int ret = 0
+        list uniques = []
 
-    n = len(arr)
-    result = np.zeros(n, dtype=np.uint8)
-    for i from 0 <= i < n:
-        result[i] = util._checknull(arr[i])
-    return result.view(np.bool_)
+    table = kh_init_int64()
+    kh_resize_int64(table, 10)
+    for i in range(n - 1):
+        val = arr[i + 1] - arr[i]
+        k = kh_get_int64(table, val)
+        if k == table.n_buckets:
+            kh_put_int64(table, val, &ret)
+            uniques.append(val)
+    kh_destroy_int64(table)
 
+    result = np.array(uniques, dtype=np.int64)
+    result.sort()
+    return result
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def isnullobj2d(ndarray[object, ndim=2] arr):
-    cdef Py_ssize_t i, j, n, m
-    cdef object val
-    cdef ndarray[uint8_t, ndim=2] result
-
-    n, m = (<object> arr).shape
-    result = np.zeros((n, m), dtype=np.uint8)
-    for i from 0 <= i < n:
-        for j from 0 <= j < m:
-            val = arr[i, j]
-            if checknull(val):
-                result[i, j] = 1
-    return result.view(np.bool_)
-
-def list_to_object_array(list obj):
-    '''
-    Convert list to object ndarray. Seriously can't believe I had to write this
-    function
-    '''
+
+cdef inline bint _is_multiple(int64_t us, int64_t mult):
+    return us % mult == 0
+
+
+def apply_offset(ndarray[object] values, object offset):
     cdef:
-        Py_ssize_t i, n
-        ndarray[object] arr
+        Py_ssize_t i, n = len(values)
+        ndarray[int64_t] new_values
+        object boxed
+
+    result = np.empty(n, dtype='M8[ns]')
+    new_values = result.view('i8')
+    pass
+
+
+# This is PITA. Because we inherit from datetime, which has very specific
+# construction requirements, we need to do object instantiation in python
+# (see Timestamp class above). This will serve as a C extension type that
+# shadows the python class, where we do any heavy lifting.
+cdef class _Timestamp(datetime):
+    cdef readonly:
+        int64_t value, nanosecond
+        object offset       # frequency reference
+
+    def __hash__(self):
+        if self.nanosecond:
+            return hash(self.value)
+        else:
+            return datetime.__hash__(self)
+
+    def __richcmp__(_Timestamp self, object other, int op):
+        cdef _Timestamp ots
+
+        if isinstance(other, _Timestamp):
+            ots = other
+        elif type(other) is datetime:
+            if self.nanosecond == 0:
+                val = self.to_datetime()
+                return PyObject_RichCompareBool(val, other, op)
+
+            try:
+                ots = Timestamp(other)
+            except ValueError:
+                return self._compare_outside_nanorange(other, op)
+        else:
+            if op == 2:
+                return False
+            elif op == 3:
+                return True
+            else:
+                raise TypeError('Cannot compare Timestamp with %s' % str(other))
+
+        self._assert_tzawareness_compat(other)
+
+        if op == 2: # ==
+            return self.value == ots.value
+        elif op == 3: # !=
+            return self.value != ots.value
+        elif op == 0: # <
+            return self.value < ots.value
+        elif op == 1: # <=
+            return self.value <= ots.value
+        elif op == 4: # >
+            return self.value > ots.value
+        elif op == 5: # >=
+            return self.value >= ots.value
+
+    cdef _compare_outside_nanorange(self, object other, int op):
+        dtval = self.to_datetime()
+
+        self._assert_tzawareness_compat(other)
+
+        if self.nanosecond == 0:
+            if op == 2: # ==
+                return dtval == other
+            elif op == 3: # !=
+                return dtval != other
+            elif op == 0: # <
+                return dtval < other
+            elif op == 1: # <=
+                return dtval <= other
+            elif op == 4: # >
+                return dtval > other
+            elif op == 5: # >=
+                return dtval >= other
+        else:
+            if op == 2: # ==
+                return False
+            elif op == 3: # !=
+                return True
+            elif op == 0: # <
+                return dtval < other
+            elif op == 1: # <=
+                return dtval < other
+            elif op == 4: # >
+                return dtval >= other
+            elif op == 5: # >=
+                return dtval >= other
+
+    cdef _assert_tzawareness_compat(self, object other):
+        if self.tzinfo is None:
+            if other.tzinfo is not None:
+                raise Exception('Cannot compare tz-naive and '
+                                'tz-aware timestamps')
+        elif other.tzinfo is None:
+            raise Exception('Cannot compare tz-naive and tz-aware timestamps')
+
+    cpdef to_datetime(self):
+        cdef:
+            pandas_datetimestruct dts
+            _TSObject ts
+        ts = convert_to_tsobject(self, self.tzinfo)
+        dts = ts.dts
+        return datetime(dts.year, dts.month, dts.day,
+                        dts.hour, dts.min, dts.sec,
+                        dts.us, ts.tzinfo)
+
+    def __add__(self, other):
+        if is_integer_object(other):
+            if self.offset is None:
+                msg = ("Cannot add integral value to Timestamp "
+                       "without offset.")
+                raise ValueError(msg)
+            else:
+                return Timestamp((self.offset.__mul__(other)).apply(self))
+        else:
+            if isinstance(other, timedelta) or hasattr(other, 'delta'):
+                nanos = _delta_to_nanoseconds(other)
+                return Timestamp(self.value + nanos, tz=self.tzinfo)
+            else:
+                result = datetime.__add__(self, other)
+                if isinstance(result, datetime):
+                    result = Timestamp(result)
+                    result.nanosecond = self.nanosecond
+                return result
+
+    def __sub__(self, other):
+        if is_integer_object(other):
+            return self.__add__(-other)
+        else:
+            return datetime.__sub__(self, other)
 
-    n = len(obj)
-    arr = np.empty(n, dtype=object)
+    cpdef _get_field(self, field):
+        out = get_date_field(np.array([self.value], dtype=np.int64), field)
+        return out[0]
 
-    for i from 0 <= i < n:
-        arr[i] = obj[i]
 
-    return arr
+cdef class _NaT(_Timestamp):
 
+    def __richcmp__(_NaT self, object other, int op):
+        # if not isinstance(other, (_NaT, _Timestamp)):
+        #     raise TypeError('Cannot compare %s with NaT' % type(other))
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def fast_unique(ndarray[object] values):
-    cdef:
-        Py_ssize_t i, n = len(values)
-        list uniques = []
-        dict table = {}
-        object val, stub = 0
+        if op == 2: # ==
+            return False
+        elif op == 3: # !=
+            return True
+        elif op == 0: # <
+            return False
+        elif op == 1: # <=
+            return False
+        elif op == 4: # >
+            return False
+        elif op == 5: # >=
+            return False
 
-    for i from 0 <= i < n:
-        val = values[i]
-        if val not in table:
-            table[val] = stub
-            uniques.append(val)
+
+
+
+def _delta_to_nanoseconds(delta):
     try:
-        uniques.sort()
-    except Exception:
+        delta = delta.delta
+    except:
         pass
+    return (delta.days * 24 * 60 * 60 * 1000000
+            + delta.seconds * 1000000
+            + delta.microseconds) * 1000
 
-    return uniques
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def fast_unique_multiple(list arrays):
+# lightweight C object to hold datetime & int64 pair
+cdef class _TSObject:
     cdef:
-        ndarray[object] buf
-        Py_ssize_t k = len(arrays)
-        Py_ssize_t i, j, n
-        list uniques = []
-        dict table = {}
-        object val, stub = 0
-
-    for i from 0 <= i < k:
-        buf = arrays[i]
-        n = len(buf)
-        for j from 0 <= j < n:
-            val = buf[j]
-            if val not in table:
-                table[val] = stub
-                uniques.append(val)
+        pandas_datetimestruct dts      # pandas_datetimestruct
+        int64_t value               # numpy dt64
+        object tzinfo
+
+    property value:
+        def __get__(self):
+            return self.value
+
+cpdef _get_utcoffset(tzinfo, obj):
     try:
-        uniques.sort()
-    except Exception:
-        pass
+        return tzinfo._utcoffset
+    except AttributeError:
+        return tzinfo.utcoffset(obj)
+
+# helper to extract datetime and int64 from several different possibilities
+cdef convert_to_tsobject(object ts, object tz):
+    """
+    Extract datetime and int64 from any of:
+        - np.int64
+        - np.datetime64
+        - python int or long object
+        - iso8601 string object
+        - python datetime object
+        - another timestamp object
+    """
+    cdef:
+        _TSObject obj
+        bint utc_convert = 1
+
+    if tz is not None:
+        if isinstance(tz, basestring):
+            tz = pytz.timezone(tz)
+
+    obj = _TSObject()
+
+    if is_datetime64_object(ts):
+        obj.value = _get_datetime64_nanos(ts)
+        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns, &obj.dts)
+    elif is_integer_object(ts):
+        obj.value = ts
+        pandas_datetime_to_datetimestruct(ts, PANDAS_FR_ns, &obj.dts)
+    elif util.is_string_object(ts):
+        _string_to_dts(ts, &obj.dts)
+        obj.value = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &obj.dts)
+    elif PyDateTime_Check(ts):
+        if tz is not None:
+            # sort of a temporary hack
+            if ts.tzinfo is not None:
+                ts = tz.normalize(ts)
+                obj.value = _pydatetime_to_dts(ts, &obj.dts)
+                obj.tzinfo = ts.tzinfo
+            elif not _is_utc(tz):
+                ts = tz.localize(ts)
+                obj.value = _pydatetime_to_dts(ts, &obj.dts)
+                offset = _get_utcoffset(ts.tzinfo, ts)
+                obj.value -= _delta_to_nanoseconds(offset)
+                obj.tzinfo = ts.tzinfo
+            else:
+                # UTC
+                obj.value = _pydatetime_to_dts(ts, &obj.dts)
+                obj.tzinfo = pytz.utc
+        else:
+            obj.value = _pydatetime_to_dts(ts, &obj.dts)
+            obj.tzinfo = ts.tzinfo
+            if obj.tzinfo is not None and not _is_utc(obj.tzinfo):
+                offset = _get_utcoffset(obj.tzinfo, ts)
+                obj.value -= _delta_to_nanoseconds(offset)
+
+        if is_timestamp(ts):
+            obj.value += ts.nanosecond
+        _check_dts_bounds(obj.value, &obj.dts)
+        return obj
+    elif PyDate_Check(ts):
+        obj.value  = _date_to_datetime64(ts, &obj.dts)
+    else:
+        raise ValueError("Could not construct Timestamp from argument %s" %
+                         type(ts))
+
+    if obj.value != NPY_NAT:
+        _check_dts_bounds(obj.value, &obj.dts)
+
+    if tz is not None:
+        _localize_tso(obj, tz)
+
+    return obj
+
+cdef inline void _localize_tso(_TSObject obj, object tz):
+    if _is_utc(tz):
+        obj.tzinfo = tz
+    elif _is_tzlocal(tz):
+        pandas_datetime_to_datetimestruct(obj.value, PANDAS_FR_ns, &obj.dts)
+        dt = datetime(obj.dts.year, obj.dts.month, obj.dts.day, obj.dts.hour,
+                      obj.dts.min, obj.dts.sec, obj.dts.us, tz)
+        delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
+        pandas_datetime_to_datetimestruct(obj.value + delta,
+                                          PANDAS_FR_ns, &obj.dts)
+        obj.tzinfo = tz
+    else:
+        # Adjust datetime64 timestamp, recompute datetimestruct
+        trans = _get_transitions(tz)
+        deltas = _get_deltas(tz)
+        pos = trans.searchsorted(obj.value, side='right') - 1
+
+        # statictzinfo
+        if not hasattr(tz, '_transition_info'):
+            pandas_datetime_to_datetimestruct(obj.value + deltas[0],
+                                              PANDAS_FR_ns, &obj.dts)
+            obj.tzinfo = tz
+        else:
+            inf = tz._transition_info[pos]
+            pandas_datetime_to_datetimestruct(obj.value + deltas[pos],
+                                              PANDAS_FR_ns, &obj.dts)
+            obj.tzinfo = tz._tzinfos[inf]
 
-    return uniques
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def fast_unique_multiple_list(list lists):
+def get_timezone(tz):
+    return _get_zone(tz)
+
+cdef inline bint _is_utc(object tz):
+    return tz is UTC or isinstance(tz, _du_utc)
+
+cdef inline object _get_zone(object tz):
+    if _is_utc(tz):
+        return 'UTC'
+    else:
+        try:
+            return tz.zone
+        except AttributeError:
+            return tz
+
+# cdef int64_t _NS_LOWER_BOUND = -9223285636854775809LL
+# cdef int64_t _NS_UPPER_BOUND = -9223372036854775807LL
+
+cdef inline _check_dts_bounds(int64_t value, pandas_datetimestruct *dts):
+    cdef pandas_datetimestruct dts2
+    if dts.year <= 1677 or dts.year >= 2262:
+        pandas_datetime_to_datetimestruct(value, PANDAS_FR_ns, &dts2)
+        if dts2.year != dts.year:
+            fmt = '%d-%.2d-%.2d %.2d:%.2d:%.2d' % (dts.year, dts.month,
+                                                   dts.day, dts.hour,
+                                                   dts.min, dts.sec)
+
+            raise ValueError('Out of bounds nanosecond timestamp: %s' % fmt)
+
+# elif isinstance(ts, _Timestamp):
+#     tmp = ts
+#     obj.value = (<_Timestamp> ts).value
+#     obj.dtval =
+# elif isinstance(ts, object):
+#     # If all else fails
+#     obj.value = _dtlike_to_datetime64(ts, &obj.dts)
+#     obj.dtval = _dts_to_pydatetime(&obj.dts)
+
+def datetime_to_datetime64(ndarray[object] values):
     cdef:
-        list buf
-        Py_ssize_t k = len(lists)
-        Py_ssize_t i, j, n
-        list uniques = []
-        dict table = {}
-        object val, stub = 0
-
-    for i from 0 <= i < k:
-        buf = lists[i]
-        n = len(buf)
-        for j from 0 <= j < n:
-            val = buf[j]
-            if val not in table:
-                table[val] = stub
-                uniques.append(val)
-    try:
-        uniques.sort()
-    except Exception:
-        pass
+        Py_ssize_t i, n = len(values)
+        object val, inferred_tz = None
+        ndarray[int64_t] iresult
+        pandas_datetimestruct dts
+        _TSObject _ts
 
-    return uniques
+    result = np.empty(n, dtype='M8[ns]')
+    iresult = result.view('i8')
+    for i in range(n):
+        val = values[i]
+        if util._checknull(val):
+            iresult[i] = iNaT
+        elif PyDateTime_Check(val):
+            if val.tzinfo is not None:
+                if inferred_tz is not None:
+                    if _get_zone(val.tzinfo) != inferred_tz:
+                        raise ValueError('Array must be all same time zone')
+                else:
+                    inferred_tz = _get_zone(val.tzinfo)
+
+                _ts = convert_to_tsobject(val, None)
+                iresult[i] = _ts.value
+                _check_dts_bounds(iresult[i], &_ts.dts)
+            else:
+                if inferred_tz is not None:
+                    raise ValueError('Cannot mix tz-aware with tz-naive values')
+                iresult[i] = _pydatetime_to_dts(val, &dts)
+                _check_dts_bounds(iresult[i], &dts)
+        else:
+            raise TypeError('Unrecognized value type: %s' % type(val))
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def fast_unique_multiple_list_gen(object gen):
+    return result, inferred_tz
+
+
+def array_to_datetime(ndarray[object] values, raise_=False, dayfirst=False,
+                      format=None, utc=None):
     cdef:
-        list buf
-        Py_ssize_t j, n
-        list uniques = []
-        dict table = {}
-        object val, stub = 0
+        Py_ssize_t i, n = len(values)
+        object val
+        ndarray[int64_t] iresult
+        ndarray[object] oresult
+        pandas_datetimestruct dts
+        bint utc_convert = bool(utc)
+        _TSObject _ts
 
-    for buf in gen:
-        n = len(buf)
-        for j from 0 <= j < n:
-            val = buf[j]
-            if val not in table:
-                table[val] = stub
-                uniques.append(val)
+    from dateutil.parser import parse
 
     try:
-        uniques.sort()
-    except Exception:
-        pass
+        result = np.empty(n, dtype='M8[ns]')
+        iresult = result.view('i8')
+        for i in range(n):
+            val = values[i]
+            if util._checknull(val) or val is NaT:
+                iresult[i] = iNaT
+            elif PyDateTime_Check(val):
+                if val.tzinfo is not None:
+                    if utc_convert:
+                        _ts = convert_to_tsobject(val, None)
+                        iresult[i] = _ts.value
+                        _check_dts_bounds(iresult[i], &_ts.dts)
+                    else:
+                        raise ValueError('Tz-aware datetime.datetime cannot '
+                                         'be converted to datetime64 unless '
+                                         'utc=True')
+                else:
+                    iresult[i] = _pydatetime_to_dts(val, &dts)
+                    if isinstance(val, _Timestamp):
+                        iresult[i] += (<_Timestamp>val).nanosecond
+                    _check_dts_bounds(iresult[i], &dts)
+            elif PyDate_Check(val):
+                iresult[i] = _date_to_datetime64(val, &dts)
+                _check_dts_bounds(iresult[i], &dts)
+            elif util.is_datetime64_object(val):
+                iresult[i] = _get_datetime64_nanos(val)
+            elif util.is_integer_object(val):
+                iresult[i] = val
+            else:
+                if len(val) == 0:
+                    iresult[i] = iNaT
+                    continue
+
+                try:
+                    _string_to_dts(val, &dts)
+                    iresult[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_ns,
+                                                                   &dts)
+                    _check_dts_bounds(iresult[i], &dts)
+                except ValueError:
+                    try:
+                        result[i] = parse(val, dayfirst=dayfirst)
+                    except Exception:
+                        raise TypeError
+                    pandas_datetime_to_datetimestruct(iresult[i], PANDAS_FR_ns,
+                                                      &dts)
+                    _check_dts_bounds(iresult[i], &dts)
+        return result
+    except TypeError:
+        oresult = np.empty(n, dtype=object)
 
-    return uniques
+        for i in range(n):
+            val = values[i]
+            if util._checknull(val):
+                oresult[i] = val
+            else:
+                if len(val) == 0:
+                    # TODO: ??
+                    oresult[i] = 'NaT'
+                    continue
+                try:
+                    oresult[i] = parse(val, dayfirst=dayfirst)
+                except Exception:
+                    if raise_:
+                        raise
+                    return values
+                    # oresult[i] = val
+
+        return oresult
+
+cdef inline _get_datetime64_nanos(object val):
+    cdef:
+        pandas_datetimestruct dts
+        PANDAS_DATETIMEUNIT unit
+        npy_datetime ival
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def dicts_to_array(list dicts, list columns):
+    unit = get_datetime64_unit(val)
+    if unit == 3:
+        raise ValueError('NumPy 1.6.1 business freq not supported')
+
+    ival = get_datetime64_value(val)
+
+    if unit != PANDAS_FR_ns:
+        pandas_datetime_to_datetimestruct(ival, unit, &dts)
+        return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
+    else:
+        return ival
+
+
+def cast_to_nanoseconds(ndarray arr):
     cdef:
-        Py_ssize_t i, j, k, n
-        ndarray[object, ndim=2] result
-        dict row
-        object col, onan = np.nan
+        Py_ssize_t i, n = arr.size
+        ndarray[int64_t] ivalues, iresult
+        PANDAS_DATETIMEUNIT unit
+        pandas_datetimestruct dts
+
+    shape = (<object> arr).shape
+
+    ivalues = arr.view(np.int64).ravel()
 
-    k = len(columns)
-    n = len(dicts)
+    result = np.empty(shape, dtype='M8[ns]')
+    iresult = result.ravel().view(np.int64)
 
-    result = np.empty((n, k), dtype='O')
+    unit = get_datetime64_unit(arr.flat[0])
+    if unit == 3:
+        raise ValueError('NumPy 1.6.1 business freq not supported')
 
     for i in range(n):
-        row = dicts[i]
-        for j in range(k):
-            col = columns[j]
-            if col in row:
-                result[i, j] = row[col]
-            else:
-                result[i, j] = onan
+        pandas_datetime_to_datetimestruct(ivalues[i], unit, &dts)
+        iresult[i] = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
 
     return result
 
+#----------------------------------------------------------------------
+# Conversion routines
+
 
-def fast_zip(list ndarrays):
+def pydt_to_i8(object pydt):
     '''
-    For zipping multiple ndarrays into an ndarray of tuples
+    Convert to int64 representation compatible with numpy datetime64; converts
+    to UTC
     '''
     cdef:
-        Py_ssize_t i, j, k, n
-        ndarray[object] result
-        flatiter it
-        object val, tup
+        _TSObject ts
 
-    k = len(ndarrays)
-    n = len(ndarrays[0])
+    ts = convert_to_tsobject(pydt, None)
 
-    result = np.empty(n, dtype=object)
+    return ts.value
 
-    # initialize tuples on first pass
-    arr = ndarrays[0]
-    it = <flatiter> PyArray_IterNew(arr)
-    for i in range(n):
-        val = PyArray_GETITEM(arr, PyArray_ITER_DATA(it))
-        tup = PyTuple_New(k)
+def i8_to_pydt(int64_t i8, object tzinfo = None):
+    '''
+    Inverse of pydt_to_i8
+    '''
+    return Timestamp(i8)
 
-        PyTuple_SET_ITEM(tup, 0, val)
-        Py_INCREF(val)
-        result[i] = tup
-        PyArray_ITER_NEXT(it)
+#----------------------------------------------------------------------
+# time zone conversion helpers
 
-    for j in range(1, k):
-        arr = ndarrays[j]
-        it = <flatiter> PyArray_IterNew(arr)
-        if len(arr) != n:
-            raise ValueError('all arrays must be same length')
+try:
+    from dateutil.tz import tzutc as _du_utc
+    import pytz
+    UTC = pytz.utc
+    have_pytz = True
+except:
+    have_pytz = False
 
+def tz_convert(ndarray[int64_t] vals, object tz1, object tz2):
+    cdef:
+        ndarray[int64_t] utc_dates, result, trans, deltas
+        Py_ssize_t i, pos, n = len(vals)
+        int64_t v, offset
+        pandas_datetimestruct dts
+
+    if not have_pytz:
+        import pytz
+
+    # Convert to UTC
+
+    if _get_zone(tz1) != 'UTC':
+        utc_dates = np.empty(n, dtype=np.int64)
+        if _is_tzlocal(tz1):
+            for i in range(n):
+                v = vals[i]
+                pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
+                dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                              dts.min, dts.sec, dts.us, tz1)
+                delta = int(total_seconds(_get_utcoffset(tz1, dt))) * 1000000000
+                utc_dates[i] = v - delta
+        else:
+            deltas = _get_deltas(tz1)
+            trans = _get_transitions(tz1)
+            pos = trans.searchsorted(vals[0]) - 1
+            if pos < 0:
+                raise ValueError('First time before start of DST info')
+
+            offset = deltas[pos]
+            for i in range(n):
+                v = vals[i]
+                if v >= [pos + 1]:
+                    pos += 1
+                    offset = deltas[pos]
+                utc_dates[i] = v - offset
+    else:
+        utc_dates = vals
+
+    if _get_zone(tz2) == 'UTC':
+        return utc_dates
+
+    result = np.empty(n, dtype=np.int64)
+    if _is_tzlocal(tz2):
         for i in range(n):
-            val = PyArray_GETITEM(arr, PyArray_ITER_DATA(it))
-            PyTuple_SET_ITEM(result[i], j, val)
-            Py_INCREF(val)
-            PyArray_ITER_NEXT(it)
+            v = utc_dates[i]
+            pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
+            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                          dts.min, dts.sec, dts.us, tz2)
+            delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
+            result[i] = v + delta
+            return result
+
+    # Convert UTC to other timezone
+    trans = _get_transitions(tz2)
+    deltas = _get_deltas(tz2)
+    pos = trans.searchsorted(utc_dates[0])
+    if pos == 0:
+        raise ValueError('First time before start of DST info')
+    elif pos == len(trans):
+        return utc_dates + deltas[-1]
+
+    # TODO: this assumed sortedness :/
+    pos -= 1
+
+    offset = deltas[pos]
+    for i in range(n):
+        v = utc_dates[i]
+        if v >= trans[pos + 1]:
+            pos += 1
+            offset = deltas[pos]
+        result[i] = v + offset
 
     return result
 
-def get_reverse_indexer(ndarray[int64_t] indexer, Py_ssize_t length):
+def tz_convert_single(int64_t val, object tz1, object tz2):
     cdef:
-        Py_ssize_t i, n = len(indexer)
-        ndarray[int64_t] rev_indexer
-        int64_t idx
+        ndarray[int64_t] trans, deltas
+        Py_ssize_t pos
+        int64_t v, offset, utc_date
+        pandas_datetimestruct dts
+
+    if not have_pytz:
+        import pytz
+
+    # Convert to UTC
+    if _is_tzlocal(tz1):
+        pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
+        dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                      dts.min, dts.sec, dts.us, tz1)
+        delta = int(total_seconds(_get_utcoffset(tz1, dt))) * 1000000000
+        utc_date = val - delta
+    elif _get_zone(tz1) != 'UTC':
+        deltas = _get_deltas(tz1)
+        trans = _get_transitions(tz1)
+        pos = trans.searchsorted(val) - 1
+        if pos < 0:
+            raise ValueError('First time before start of DST info')
+        offset = deltas[pos]
+        utc_date = val - offset
+    else:
+        utc_date = val
+
+    if _get_zone(tz2) == 'UTC':
+        return utc_date
+    if _is_tzlocal(tz2):
+        pandas_datetime_to_datetimestruct(val, PANDAS_FR_ns, &dts)
+        dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                      dts.min, dts.sec, dts.us, tz2)
+        delta = int(total_seconds(_get_utcoffset(tz2, dt))) * 1000000000
+        return utc_date + delta
+    # Convert UTC to other timezone
+    trans = _get_transitions(tz2)
+    deltas = _get_deltas(tz2)
+    pos = trans.searchsorted(utc_date) - 1
+    if pos < 0:
+        raise ValueError('First time before start of DST info')
+
+    offset = deltas[pos]
+    return utc_date + offset
+
+
+trans_cache = {}
+utc_offset_cache = {}
+
+def _get_transitions(tz):
+    """
+    Get UTC times of DST transitions
+    """
+    try:
+        # tzoffset not hashable in Python 3
+        hash(tz)
+    except TypeError:
+        return np.array([NPY_NAT + 1], dtype=np.int64)
+
+    if tz not in trans_cache:
+        if hasattr(tz, '_utc_transition_times'):
+            arr = np.array(tz._utc_transition_times, dtype='M8[ns]')
+            arr = arr.view('i8')
+            try:
+                if tz._utc_transition_times[0].year == 1:
+                    arr[0] = NPY_NAT + 1
+            except Exception:
+                pass
+        else:
+            arr = np.array([NPY_NAT + 1], dtype=np.int64)
+        trans_cache[tz] = arr
+    return trans_cache[tz]
+
+def _get_deltas(tz):
+    """
+    Get UTC offsets in microseconds corresponding to DST transitions
+    """
+    try:
+        # tzoffset not hashable in Python 3
+        hash(tz)
+    except TypeError:
+        num = int(total_seconds(_get_utcoffset(tz, None))) * 1000000000
+        return np.array([num], dtype=np.int64)
+
+    if tz not in utc_offset_cache:
+        if hasattr(tz, '_utc_transition_times'):
+            utc_offset_cache[tz] = _unbox_utcoffsets(tz._transition_info)
+        else:
+            # static tzinfo
+            num = int(total_seconds(_get_utcoffset(tz, None))) * 1000000000
+            utc_offset_cache[tz] = np.array([num], dtype=np.int64)
 
-    rev_indexer = np.empty(length, dtype=np.int64)
-    rev_indexer.fill(-1)
-    for i in range(n):
-        idx = indexer[i]
-        if idx != -1:
-            rev_indexer[idx] = i
+    return utc_offset_cache[tz]
 
-    return rev_indexer
+cdef double total_seconds(object td): # Python 2.6 compat
+    return ((td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) //
+            10**6)
 
+def tot_seconds(td):
+    return total_seconds(td)
 
-def has_infs_f4(ndarray[float32_t] arr):
+cpdef ndarray _unbox_utcoffsets(object transinfo):
     cdef:
-        Py_ssize_t i, n = len(arr)
-        float32_t inf, neginf, val
+        Py_ssize_t i, sz
+        ndarray[int64_t] arr
 
-    inf = np.inf
-    neginf = -inf
+    sz = len(transinfo)
+    arr = np.empty(sz, dtype='i8')
 
-    for i in range(n):
-        val = arr[i]
-        if val == inf or val == neginf:
-            return True
-    return False
+    for i in range(sz):
+        arr[i] = int(total_seconds(transinfo[i][0])) * 1000000000
+
+    return arr
+
+
+@cython.boundscheck(False)
+@cython.wraparound(False)
+def tz_localize_to_utc(ndarray[int64_t] vals, object tz):
+    """
+    Localize tzinfo-naive DateRange to given time zone (using pytz). If
+    there are ambiguities in the values, raise AmbiguousTimeError.
 
-def has_infs_f8(ndarray[float64_t] arr):
+    Returns
+    -------
+    localized : DatetimeIndex
+    """
     cdef:
-        Py_ssize_t i, n = len(arr)
-        float64_t inf, neginf, val
+        ndarray[int64_t] trans, deltas, idx_shifted
+        Py_ssize_t i, idx, pos, ntrans, n = len(vals)
+        int64_t *tdata
+        int64_t v, left, right
+        ndarray[int64_t] result, result_a, result_b
+        pandas_datetimestruct dts
+
+    # Vectorized version of DstTzInfo.localize
+
+    if not have_pytz:
+        raise Exception("Could not find pytz module")
+
+    if tz == UTC or tz is None:
+        return vals
+
+    result = np.empty(n, dtype=np.int64)
+
+    if _is_tzlocal(tz):
+        for i in range(n):
+            v = vals[i]
+            pandas_datetime_to_datetimestruct(v, PANDAS_FR_ns, &dts)
+            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                          dts.min, dts.sec, dts.us, tz)
+            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
+            result[i] = v - delta
+        return result
+
+    trans = _get_transitions(tz)  # transition dates
+    deltas = _get_deltas(tz)      # utc offsets
 
-    inf = np.inf
-    neginf = -inf
+    tdata = <int64_t*> trans.data
+    ntrans = len(trans)
+
+    result_a = np.empty(n, dtype=np.int64)
+    result_b = np.empty(n, dtype=np.int64)
+    result_a.fill(NPY_NAT)
+    result_b.fill(NPY_NAT)
+
+    # left side
+    idx_shifted = _ensure_int64(
+        np.maximum(0, trans.searchsorted(vals - DAY_NS, side='right') - 1))
 
     for i in range(n):
-        val = arr[i]
-        if val == inf or val == neginf:
-            return True
-    return False
+        v = vals[i] - deltas[idx_shifted[i]]
+        pos = bisect_right_i8(tdata, v, ntrans) - 1
 
-def convert_timestamps(ndarray values):
-    cdef:
-        object val, f, result
-        dict cache = {}
-        Py_ssize_t i, n = len(values)
-        ndarray[object] out
+        # timestamp falls to the left side of the DST transition
+        if v + deltas[pos] == vals[i]:
+            result_a[i] = v
 
-    # for HDFStore, a bit temporary but...
+    # right side
+    idx_shifted = _ensure_int64(
+        np.maximum(0, trans.searchsorted(vals + DAY_NS, side='right') - 1))
 
-    from datetime import datetime
-    f = datetime.fromtimestamp
+    for i in range(n):
+        v = vals[i] - deltas[idx_shifted[i]]
+        pos = bisect_right_i8(tdata, v, ntrans) - 1
 
-    out = np.empty(n, dtype='O')
+        # timestamp falls to the right side of the DST transition
+        if v + deltas[pos] == vals[i]:
+            result_b[i] = v
 
     for i in range(n):
-        val = util.get_value_1d(values, i)
-        if val in cache:
-            out[i] = cache[val]
+        left = result_a[i]
+        right = result_b[i]
+        if left != NPY_NAT and right != NPY_NAT:
+            if left == right:
+                result[i] = left
+            else:
+                stamp = Timestamp(vals[i])
+                raise pytz.AmbiguousTimeError(stamp)
+        elif left != NPY_NAT:
+            result[i] = left
+        elif right != NPY_NAT:
+            result[i] = right
         else:
-            cache[val] = out[i] = f(val)
+            stamp = Timestamp(vals[i])
+            raise pytz.NonExistentTimeError(stamp)
+
+    return result
+
+cdef _ensure_int64(object arr):
+    if util.is_array(arr):
+        if (<ndarray> arr).descr.type_num == NPY_INT64:
+            return arr
+        else:
+            return arr.astype(np.int64)
+    else:
+        return np.array(arr, dtype=np.int64)
+
+
+cdef inline bisect_right_i8(int64_t *data, int64_t val, Py_ssize_t n):
+    cdef Py_ssize_t pivot, left = 0, right = n
+
+    # edge cases
+    if val > data[n - 1]:
+        return n
+
+    if val < data[0]:
+        return 0
+
+    while left < right:
+        pivot = left + (right - left) // 2
+
+        if data[pivot] <= val:
+            left = pivot + 1
+        else:
+            right = pivot
+
+    return left
+
+
+# Accessors
+#----------------------------------------------------------------------
+
+def build_field_sarray(ndarray[int64_t] dtindex):
+    '''
+    Datetime as int64 representation to a structured array of fields
+    '''
+    cdef:
+        Py_ssize_t i, count = 0
+        int isleap
+        pandas_datetimestruct dts
+        ndarray[int32_t] years, months, days, hours, minutes, seconds, mus
+
+    count = len(dtindex)
+
+    sa_dtype = [('Y', 'i4'), # year
+                ('M', 'i4'), # month
+                ('D', 'i4'), # day
+                ('h', 'i4'), # hour
+                ('m', 'i4'), # min
+                ('s', 'i4'), # second
+                ('u', 'i4')] # microsecond
+
+    out = np.empty(count, dtype=sa_dtype)
+
+    years = out['Y']
+    months = out['M']
+    days = out['D']
+    hours = out['h']
+    minutes = out['m']
+    seconds = out['s']
+    mus = out['u']
+
+    for i in range(count):
+        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+        years[i] = dts.year
+        months[i] = dts.month
+        days[i] = dts.day
+        hours[i] = dts.hour
+        minutes[i] = dts.min
+        seconds[i] = dts.sec
+        mus[i] = dts.us
 
     return out
 
-def maybe_indices_to_slice(ndarray[int64_t] indices):
+def get_time_micros(ndarray[int64_t] dtindex):
+    '''
+    Datetime as int64 representation to a structured array of fields
+    '''
     cdef:
-        Py_ssize_t i, n = len(indices)
+        Py_ssize_t i, n = len(dtindex)
+        pandas_datetimestruct dts
+        ndarray[int64_t] micros
 
-    if n == 0:
-        return indices
+    micros = np.empty(n, dtype=np.int64)
 
-    for i in range(1, n):
-        if indices[i] - indices[i - 1] != 1:
-            return indices
-    return slice(indices[0], indices[n - 1] + 1)
+    for i in range(n):
+        pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+        micros[i] = 1000000LL * (dts.hour * 60 * 60 +
+                                 60 * dts.min + dts.sec) + dts.us
 
+    return micros
 
-def maybe_booleans_to_slice(ndarray[uint8_t] mask):
+@cython.wraparound(False)
+def get_date_field(ndarray[int64_t] dtindex, object field):
+    '''
+    Given a int64-based datetime index, extract the year, month, etc.,
+    field and return an array of these values.
+    '''
     cdef:
-        Py_ssize_t i, n = len(mask)
-        Py_ssize_t start, end
-        bint started = 0, finished = 0
+        _TSObject ts
+        Py_ssize_t i, count = 0
+        ndarray[int32_t] out
+        ndarray[int32_t, ndim=2] _month_offset
+        int isleap
+        pandas_datetimestruct dts
+
+    _month_offset = np.array(
+        [[ 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334, 365 ],
+         [ 0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335, 366 ]],
+         dtype=np.int32 )
+
+    count = len(dtindex)
+    out = np.empty(count, dtype='i4')
+
+    if field == 'Y':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.year
+        return out
 
-    for i in range(n):
-        if mask[i]:
-            if finished:
-                return mask.view(np.bool_)
-            if not started:
-                started = 1
-                start = i
-        else:
-            if finished:
-                continue
+    elif field == 'M':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.month
+        return out
+
+    elif field == 'D':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.day
+        return out
+
+    elif field == 'h':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.hour
+        return out
+
+    elif field == 'm':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.min
+        return out
+
+    elif field == 's':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.sec
+        return out
+
+    elif field == 'us':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.us
+        return out
+    elif field == 'ns':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.ps / 1000
+        return out
+    elif field == 'doy':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            isleap = is_leapyear(dts.year)
+            out[i] = _month_offset[isleap, dts.month-1] + dts.day
+        return out
+
+    elif field == 'dow':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            ts = convert_to_tsobject(dtindex[i], None)
+            out[i] = ts_dayofweek(ts)
+        return out
+
+    elif field == 'woy':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            isleap = is_leapyear(dts.year)
+            out[i] = _month_offset[isleap, dts.month - 1] + dts.day
+            out[i] = ((out[i] - 1) / 7) + 1
+        return out
+
+    elif field == 'q':
+        for i in range(count):
+            if dtindex[i] == NPY_NAT: out[i] = -1; continue
+
+            pandas_datetime_to_datetimestruct(dtindex[i], PANDAS_FR_ns, &dts)
+            out[i] = dts.month
+            out[i] = ((out[i] - 1) / 3) + 1
+        return out
+
+    raise ValueError("Field %s not supported" % field)
+
+
+cdef inline int m8_weekday(int64_t val):
+    ts = convert_to_tsobject(val, None)
+    return ts_dayofweek(ts)
+
+cdef int64_t DAY_NS = 86400000000000LL
 
-            if started:
-                end = i
-                finished = 1
 
-    if not started:
-        return slice(0, 0)
-    if not finished:
-        return slice(start, None)
+def date_normalize(ndarray[int64_t] stamps, tz=None):
+    cdef:
+        Py_ssize_t i, n = len(stamps)
+        pandas_datetimestruct dts
+        _TSObject tso
+        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
+
+    if tz is not None:
+        tso = _TSObject()
+        if isinstance(tz, basestring):
+            tz = pytz.timezone(tz)
+        result = _normalize_local(stamps, tz)
     else:
-        return slice(start, end)
+        for i in range(n):
+            if stamps[i] == NPY_NAT:
+                result[i] = NPY_NAT
+                continue
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
+            result[i] = _normalized_stamp(&dts)
 
+    return result
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def scalar_compare(ndarray[object] values, object val, object op):
-    import operator
+cdef _normalize_local(ndarray[int64_t] stamps, object tz):
     cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[uint8_t, cast=True] result
-        int flag
-        object x
-
-    if op is operator.lt:
-        flag = cpython.Py_LT
-    elif op is operator.le:
-        flag = cpython.Py_LE
-    elif op is operator.gt:
-        flag = cpython.Py_GT
-    elif op is operator.ge:
-        flag = cpython.Py_GE
-    elif op is operator.eq:
-        flag = cpython.Py_EQ
-    elif op is operator.ne:
-        flag = cpython.Py_NE
+        Py_ssize_t n = len(stamps)
+        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
+        ndarray[int64_t] trans, deltas, pos
+        pandas_datetimestruct dts
+
+    if _is_utc(tz):
+        for i in range(n):
+            if stamps[i] == NPY_NAT:
+                result[i] = NPY_NAT
+                continue
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
+            result[i] = _normalized_stamp(&dts)
+    elif _is_tzlocal(tz):
+        for i in range(n):
+            if stamps[i] == NPY_NAT:
+                result[i] = NPY_NAT
+                continue
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns,
+                                              &dts)
+            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                          dts.min, dts.sec, dts.us, tz)
+            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
+            pandas_datetime_to_datetimestruct(stamps[i] + delta,
+                                              PANDAS_FR_ns, &dts)
+            result[i] = _normalized_stamp(&dts)
     else:
-        raise ValueError('Unrecognized operator')
+        # Adjust datetime64 timestamp, recompute datetimestruct
+        trans = _get_transitions(tz)
+        deltas = _get_deltas(tz)
+        _pos = trans.searchsorted(stamps, side='right') - 1
+        if _pos.dtype != np.int64:
+            _pos = _pos.astype(np.int64)
+        pos = _pos
+
+        # statictzinfo
+        if not hasattr(tz, '_transition_info'):
+            for i in range(n):
+                if stamps[i] == NPY_NAT:
+                    result[i] = NPY_NAT
+                    continue
+                pandas_datetime_to_datetimestruct(stamps[i] + deltas[0],
+                                                  PANDAS_FR_ns, &dts)
+                result[i] = _normalized_stamp(&dts)
+        else:
+            for i in range(n):
+                if stamps[i] == NPY_NAT:
+                    result[i] = NPY_NAT
+                    continue
+                pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos[i]],
+                                                  PANDAS_FR_ns, &dts)
+                result[i] = _normalized_stamp(&dts)
+
+    return result
+
+cdef inline int64_t _normalized_stamp(pandas_datetimestruct *dts):
+    dts.hour = 0
+    dts.min = 0
+    dts.sec = 0
+    dts.us = 0
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, dts)
+
 
-    result = np.empty(n, dtype=bool).view(np.uint8)
+cdef inline void m8_populate_tsobject(int64_t stamp, _TSObject tso, object tz):
+    tso.value = stamp
+    pandas_datetime_to_datetimestruct(tso.value, PANDAS_FR_ns, &tso.dts)
 
-    if flag == cpython.Py_NE:
+    if tz is not None:
+        _localize_tso(tso, tz)
+
+
+def dates_normalized(ndarray[int64_t] stamps, tz=None):
+    cdef:
+        Py_ssize_t i, n = len(stamps)
+        pandas_datetimestruct dts
+
+    if tz is None or _is_utc(tz):
         for i in range(n):
-            x = values[i]
-            if _checknull(x):
-                result[i] = True
-            else:
-                result[i] = cpython.PyObject_RichCompareBool(x, val, flag)
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
+            if (dts.hour + dts.min + dts.sec + dts.us) > 0:
+                return False
+    elif _is_tzlocal(tz):
+        for i in range(n):
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
+            if (dts.min + dts.sec + dts.us) > 0:
+                return False
+            dt = datetime(dts.year, dts.month, dts.day, dts.hour, dts.min,
+                          dts.sec, dts.us, tz)
+            dt = dt + tz.utcoffset(dt)
+            if dt.hour > 0:
+                return False
     else:
+        trans = _get_transitions(tz)
+        deltas = _get_deltas(tz)
         for i in range(n):
-            x = values[i]
-            if _checknull(x):
-                result[i] = False
-            else:
-                result[i] = cpython.PyObject_RichCompareBool(x, val, flag)
+            # Adjust datetime64 timestamp, recompute datetimestruct
+            pos = trans.searchsorted(stamps[i]) - 1
+            inf = tz._transition_info[pos]
 
-    return result.view(bool)
+            pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos],
+                                              PANDAS_FR_ns, &dts)
+            if (dts.hour + dts.min + dts.sec + dts.us) > 0:
+                return False
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def vec_compare(ndarray[object] left, ndarray[object] right, object op):
-    import operator
-    cdef:
-        Py_ssize_t i, n = len(left)
-        ndarray[uint8_t, cast=True] result
-        int flag
-
-    if n != len(right):
-        raise ValueError('Arrays were different lengths: %d vs %d'
-                         % (n, len(right)))
-
-    if op is operator.lt:
-        flag = cpython.Py_LT
-    elif op is operator.le:
-        flag = cpython.Py_LE
-    elif op is operator.gt:
-        flag = cpython.Py_GT
-    elif op is operator.ge:
-        flag = cpython.Py_GE
-    elif op is operator.eq:
-        flag = cpython.Py_EQ
-    elif op is operator.ne:
-        flag = cpython.Py_NE
+    return True
+
+# Some general helper functions
+#----------------------------------------------------------------------
+
+def isleapyear(int64_t year):
+    return is_leapyear(year)
+
+def monthrange(int64_t year, int64_t month):
+    cdef:
+        int64_t days
+        int64_t day_of_week
+
+    if month < 1 or month > 12:
+        raise ValueError("bad month number 0; must be 1-12")
+
+    days = days_per_month_table[is_leapyear(year)][month-1]
+
+    return (dayofweek(year, month, 1), days)
+
+cdef inline int64_t ts_dayofweek(_TSObject ts):
+    return dayofweek(ts.dts.year, ts.dts.month, ts.dts.day)
+
+
+cpdef normalize_date(object dt):
+    '''
+    Normalize datetime.datetime value to midnight. Returns datetime.date as a
+    datetime.datetime at midnight
+
+    Returns
+    -------
+    normalized : datetime.datetime or Timestamp
+    '''
+    if PyDateTime_Check(dt):
+        return dt.replace(hour=0, minute=0, second=0, microsecond=0)
+    elif PyDate_Check(dt):
+        return datetime(dt.year, dt.month, dt.day)
     else:
-        raise ValueError('Unrecognized operator')
+        raise TypeError('Unrecognized type: %s' % type(dt))
+
+cdef ndarray[int64_t] localize_dt64arr_to_period(ndarray[int64_t] stamps,
+                                                 int freq, object tz):
+    cdef:
+        Py_ssize_t n = len(stamps)
+        ndarray[int64_t] result = np.empty(n, dtype=np.int64)
+        ndarray[int64_t] trans, deltas, pos
+        pandas_datetimestruct dts
 
-    result = np.empty(n, dtype=bool).view(np.uint8)
+    if not have_pytz:
+        raise Exception('Could not find pytz module')
 
-    if flag == cpython.Py_NE:
+    if _is_utc(tz):
         for i in range(n):
-            x = left[i]
-            y = right[i]
+            if stamps[i] == NPY_NAT:
+                result[i] = NPY_NAT
+                continue
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns, &dts)
+            result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
+                                           dts.hour, dts.min, dts.sec, freq)
 
-            if _checknull(x) or _checknull(y):
-                result[i] = True
-            else:
-                result[i] = cpython.PyObject_RichCompareBool(x, y, flag)
-    else:
+    elif _is_tzlocal(tz):
         for i in range(n):
-            x = left[i]
-            y = right[i]
+            if stamps[i] == NPY_NAT:
+                result[i] = NPY_NAT
+                continue
+            pandas_datetime_to_datetimestruct(stamps[i], PANDAS_FR_ns,
+                                              &dts)
+            dt = datetime(dts.year, dts.month, dts.day, dts.hour,
+                          dts.min, dts.sec, dts.us, tz)
+            delta = int(total_seconds(_get_utcoffset(tz, dt))) * 1000000000
+            pandas_datetime_to_datetimestruct(stamps[i] + delta,
+                                              PANDAS_FR_ns, &dts)
+            result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
+                                           dts.hour, dts.min, dts.sec, freq)
+    else:
+        # Adjust datetime64 timestamp, recompute datetimestruct
+        trans = _get_transitions(tz)
+        deltas = _get_deltas(tz)
+        _pos = trans.searchsorted(stamps, side='right') - 1
+        if _pos.dtype != np.int64:
+            _pos = _pos.astype(np.int64)
+        pos = _pos
+
+        # statictzinfo
+        if not hasattr(tz, '_transition_info'):
+            for i in range(n):
+                if stamps[i] == NPY_NAT:
+                    result[i] = NPY_NAT
+                    continue
+                pandas_datetime_to_datetimestruct(stamps[i] + deltas[0],
+                                                  PANDAS_FR_ns, &dts)
+                result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
+                                               dts.hour, dts.min, dts.sec, freq)
+        else:
+            for i in range(n):
+                if stamps[i] == NPY_NAT:
+                    result[i] = NPY_NAT
+                    continue
+                pandas_datetime_to_datetimestruct(stamps[i] + deltas[pos[i]],
+                                                  PANDAS_FR_ns, &dts)
+                result[i] = get_period_ordinal(dts.year, dts.month, dts.day,
+                                               dts.hour, dts.min, dts.sec, freq)
 
-            if _checknull(x) or _checknull(y):
-                result[i] = False
-            else:
-                result[i] = cpython.PyObject_RichCompareBool(x, y, flag)
+    return result
 
-    return result.view(bool)
 
+cdef extern from "period.h":
+    ctypedef struct date_info:
+        int64_t absdate
+        double abstime
+        double second
+        int minute
+        int hour
+        int day
+        int month
+        int quarter
+        int year
+        int day_of_week
+        int day_of_year
+        int calendar
+
+    ctypedef struct asfreq_info:
+        int from_week_end
+        int to_week_end
+
+        int from_a_year_end
+        int to_a_year_end
+
+        int from_q_year_end
+        int to_q_year_end
+
+    ctypedef int64_t (*freq_conv_func)(int64_t, char, asfreq_info*)
+
+    int64_t asfreq(int64_t dtordinal, int freq1, int freq2, char relation) except INT32_MIN
+    freq_conv_func get_asfreq_func(int fromFreq, int toFreq)
+    void get_asfreq_info(int fromFreq, int toFreq, asfreq_info *af_info)
+
+    int64_t get_period_ordinal(int year, int month, int day,
+                          int hour, int minute, int second,
+                          int freq) except INT32_MIN
+
+    int64_t get_python_ordinal(int64_t period_ordinal, int freq) except INT32_MIN
+
+    int get_date_info(int64_t ordinal, int freq, date_info *dinfo) except INT32_MIN
+    double getAbsTime(int, int64_t, int64_t)
+
+    int pyear(int64_t ordinal, int freq) except INT32_MIN
+    int pqyear(int64_t ordinal, int freq) except INT32_MIN
+    int pquarter(int64_t ordinal, int freq) except INT32_MIN
+    int pmonth(int64_t ordinal, int freq) except INT32_MIN
+    int pday(int64_t ordinal, int freq) except INT32_MIN
+    int pweekday(int64_t ordinal, int freq) except INT32_MIN
+    int pday_of_week(int64_t ordinal, int freq) except INT32_MIN
+    int pday_of_year(int64_t ordinal, int freq) except INT32_MIN
+    int pweek(int64_t ordinal, int freq) except INT32_MIN
+    int phour(int64_t ordinal, int freq) except INT32_MIN
+    int pminute(int64_t ordinal, int freq) except INT32_MIN
+    int psecond(int64_t ordinal, int freq) except INT32_MIN
+    char *c_strftime(date_info *dinfo, char *fmt)
+    int get_yq(int64_t ordinal, int freq, int *quarter, int *year)
+
+# Period logic
+#----------------------------------------------------------------------
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def scalar_binop(ndarray[object] values, object val, object op):
+cdef inline int64_t apply_mult(int64_t period_ord, int64_t mult):
+    """
+    Get freq+multiple ordinal value from corresponding freq-only ordinal value.
+    For example, 5min ordinal will be 1/5th the 1min ordinal (rounding down to
+    integer).
+    """
+    if mult == 1:
+        return period_ord
+
+    return (period_ord - 1) // mult
+
+cdef inline int64_t remove_mult(int64_t period_ord_w_mult, int64_t mult):
+    """
+    Get freq-only ordinal value from corresponding freq+multiple ordinal.
+    """
+    if mult == 1:
+        return period_ord_w_mult
+
+    return period_ord_w_mult * mult + 1;
+
+def dt64arr_to_periodarr(ndarray[int64_t] dtarr, int freq, tz=None):
+    """
+    Convert array of datetime64 values (passed in as 'i8' dtype) to a set of
+    periods corresponding to desired frequency, per period convention.
+    """
     cdef:
-        Py_ssize_t i, n = len(values)
-        ndarray[object] result
-        object x
+        ndarray[int64_t] out
+        Py_ssize_t i, l
+        pandas_datetimestruct dts
 
-    result = np.empty(n, dtype=object)
+    l = len(dtarr)
 
-    for i in range(n):
-        x = values[i]
-        if util._checknull(x):
-            result[i] = x
-        else:
-            result[i] = op(x, val)
+    out = np.empty(l, dtype='i8')
 
-    return maybe_convert_bool(result)
+    if tz is None:
+        for i in range(l):
+            pandas_datetime_to_datetimestruct(dtarr[i], PANDAS_FR_ns, &dts)
+            out[i] = get_period_ordinal(dts.year, dts.month, dts.day,
+                                        dts.hour, dts.min, dts.sec, freq)
+    else:
+        out = localize_dt64arr_to_period(dtarr, freq, tz)
+    return out
 
-@cython.wraparound(False)
-@cython.boundscheck(False)
-def vec_binop(ndarray[object] left, ndarray[object] right, object op):
+def periodarr_to_dt64arr(ndarray[int64_t] periodarr, int freq):
+    """
+    Convert array to datetime64 values from a set of ordinals corresponding to
+    periods per period convention.
+    """
     cdef:
-        Py_ssize_t i, n = len(left)
-        ndarray[object] result
+        ndarray[int64_t] out
+        Py_ssize_t i, l
 
-    if n != len(right):
-        raise ValueError('Arrays were different lengths: %d vs %d'
-                         % (n, len(right)))
+    l = len(periodarr)
 
-    result = np.empty(n, dtype=object)
+    out = np.empty(l, dtype='i8')
 
-    for i in range(n):
-        x = left[i]
-        y = right[i]
-        try:
-            result[i] = op(x, y)
-        except TypeError:
-            if util._checknull(x):
-                result[i] = x
-            elif util._checknull(y):
-                result[i] = y
-            else:
-                raise
+    for i in range(l):
+        out[i] = period_ordinal_to_dt64(periodarr[i], freq)
 
-    return maybe_convert_bool(result)
+    return out
 
+cdef char START = 'S'
+cdef char END = 'E'
 
-def value_count_int64(ndarray[int64_t] values):
+cpdef int64_t period_asfreq(int64_t period_ordinal, int freq1, int freq2,
+                            bint end):
+    """
+    Convert period ordinal from one frequency to another, and if upsampling,
+    choose to use start ('S') or end ('E') of period.
+    """
     cdef:
-        Py_ssize_t i, n = len(values)
-        kh_int64_t *table
-        int ret = 0
-        list uniques = []
+        int64_t retval
 
-    table = kh_init_int64()
-    kh_resize_int64(table, n)
+    if end:
+        retval = asfreq(period_ordinal, freq1, freq2, END)
+    else:
+        retval = asfreq(period_ordinal, freq1, freq2, START)
 
-    for i in range(n):
-        val = values[i]
-        k = kh_get_int64(table, val)
-        if k != table.n_buckets:
-            table.vals[k] += 1
-        else:
-            k = kh_put_int64(table, val, &ret)
-            table.vals[k] = 1
-
-    # for (k = kh_begin(h); k != kh_end(h); ++k)
-    # 	if (kh_exist(h, k)) kh_value(h, k) = 1;
-    i = 0
-    result_keys = np.empty(table.n_occupied, dtype=np.int64)
-    result_counts = np.zeros(table.n_occupied, dtype=np.int64)
-    for k in range(table.n_buckets):
-        if kh_exist_int64(table, k):
-            result_keys[i] = table.keys[k]
-            result_counts[i] = table.vals[k]
-            i += 1
-    kh_destroy_int64(table)
+    if retval == INT32_MIN:
+        raise ValueError('Frequency conversion failed')
 
-    return result_keys, result_counts
+    return retval
 
-def astype_intsafe(ndarray[object] arr, new_dtype):
+def period_asfreq_arr(ndarray[int64_t] arr, int freq1, int freq2, bint end):
+    """
+    Convert int64-array of period ordinals from one frequency to another, and
+    if upsampling, choose to use start ('S') or end ('E') of period.
+    """
     cdef:
-        Py_ssize_t i, n = len(arr)
-        ndarray result
+        ndarray[int64_t] result
+        Py_ssize_t i, n
+        freq_conv_func func
+        asfreq_info finfo
+        int64_t val, ordinal
+        char relation
+
+    n = len(arr)
+    result = np.empty(n, dtype=np.int64)
+
+    func = get_asfreq_func(freq1, freq2)
+    get_asfreq_info(freq1, freq2, &finfo)
+
+    if end:
+        relation = END
+    else:
+        relation = START
 
-    result = np.empty(n, dtype=new_dtype)
     for i in range(n):
-        util.set_value_at(result, i, arr[i])
+        val = func(arr[i], relation, &finfo)
+        if val == INT32_MIN:
+            raise ValueError("Unable to convert to desired frequency.")
+        result[i] = val
 
     return result
 
-def clean_index_list(list obj):
-    '''
-    Utility used in pandas.core.index._ensure_index
-    '''
+def period_ordinal(int y, int m, int d, int h, int min, int s, int freq):
     cdef:
-        ndarray[object] converted
-        Py_ssize_t i, n = len(obj)
-        object v
-        bint all_arrays = 1
+        int64_t ordinal
 
-    for i in range(n):
-        v = obj[i]
-        if not (PyList_Check(v) or cnp.PyArray_Check(v)):
-            all_arrays = 0
-            break
+    return get_period_ordinal(y, m, d, h, min, s, freq)
 
-    if all_arrays:
-        return obj, all_arrays
 
-    converted = np.empty(n, dtype=object)
-    for i in range(n):
-        v = obj[i]
-        if PyList_Check(v) or cnp.PyArray_Check(v):
-            converted[i] = tuple(v)
+cpdef int64_t period_ordinal_to_dt64(int64_t ordinal, int freq):
+    cdef:
+        pandas_datetimestruct dts
+        date_info dinfo
+
+    get_date_info(ordinal, freq, &dinfo)
+
+    dts.year = dinfo.year
+    dts.month = dinfo.month
+    dts.day = dinfo.day
+    dts.hour = dinfo.hour
+    dts.min = dinfo.minute
+    dts.sec = int(dinfo.second)
+    dts.us = dts.ps = 0
+
+    return pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)
+
+def period_format(int64_t value, int freq, object fmt=None):
+    cdef:
+        int freq_group
+
+    if fmt is None:
+        freq_group = (freq // 1000) * 1000
+        if freq_group == 1000: # FR_ANN
+            fmt = b'%Y'
+        elif freq_group == 2000: # FR_QTR
+            fmt = b'%FQ%q'
+        elif freq_group == 3000: # FR_MTH
+            fmt = b'%Y-%m'
+        elif freq_group == 4000: # WK
+            left = period_asfreq(value, freq, 6000, 0)
+            right = period_asfreq(value, freq, 6000, 1)
+            return '%s/%s' % (period_format(left, 6000),
+                              period_format(right, 6000))
+        elif (freq_group == 5000 # BUS
+              or freq_group == 6000): # DAY
+            fmt = b'%Y-%m-%d'
+        elif freq_group == 7000: # HR
+            fmt = b'%Y-%m-%d %H:00'
+        elif freq_group == 8000: # MIN
+            fmt = b'%Y-%m-%d %H:%M'
+        elif freq_group == 9000: # SEC
+            fmt = b'%Y-%m-%d %H:%M:%S'
         else:
-            converted[i] = v
-
-    return maybe_convert_objects(converted), 0
-
-
-include "hashtable.pyx"
-include "datetime.pyx"
-include "skiplist.pyx"
-include "groupby.pyx"
-include "moments.pyx"
-include "reindex.pyx"
-include "reduce.pyx"
-include "stats.pyx"
-include "properties.pyx"
-include "inference.pyx"
-include "join.pyx"
-include "engines.pyx"
+            raise ValueError('Unknown freq: %d' % freq)
+
+    return _period_strftime(value, freq, fmt)
+
+
+cdef list extra_fmts = [(b"%q", b"^`AB`^"),
+                        (b"%f", b"^`CD`^"),
+                        (b"%F", b"^`EF`^")]
+
+cdef list str_extra_fmts = ["^`AB`^", "^`CD`^", "^`EF`^"]
+
+cdef _period_strftime(int64_t value, int freq, object fmt):
+    cdef:
+        Py_ssize_t i
+        date_info dinfo
+        char *formatted
+        object pat, repl, result
+        list found_pat = [False] * len(extra_fmts)
+        int year, quarter
+
+    if PyUnicode_Check(fmt):
+        fmt = fmt.encode('utf-8')
+
+    get_date_info(value, freq, &dinfo)
+    for i in range(len(extra_fmts)):
+        pat = extra_fmts[i][0]
+        repl = extra_fmts[i][1]
+        if pat in fmt:
+            fmt = fmt.replace(pat, repl)
+            found_pat[i] = True
+
+    formatted = c_strftime(&dinfo, <char*> fmt)
+
+    result = util.char_to_string(formatted)
+    free(formatted)
+
+    for i in range(len(extra_fmts)):
+        if found_pat[i]:
+            if get_yq(value, freq, &quarter, &year) < 0:
+                raise ValueError('Unable to get quarter and year')
+
+            if i == 0:
+                repl = '%d' % quarter
+            elif i == 1:  # %f, 2-digit year
+                repl = '%.2d' % (year % 100)
+            elif i == 2:
+                repl = '%d' % year
+
+            result = result.replace(str_extra_fmts[i], repl)
+
+    # Py3?
+    if not PyString_Check(result):
+        result = str(result)
+
+    return result
+
+# period accessors
+
+ctypedef int (*accessor)(int64_t ordinal, int freq) except INT32_MIN
+
+def get_period_field(int code, int64_t value, int freq):
+    cdef accessor f = _get_accessor_func(code)
+    return f(value, freq)
+
+def get_period_field_arr(int code, ndarray[int64_t] arr, int freq):
+    cdef:
+        Py_ssize_t i, sz
+        ndarray[int64_t] out
+        accessor f
+
+    f = _get_accessor_func(code)
+
+    sz = len(arr)
+    out = np.empty(sz, dtype=np.int64)
+
+    for i in range(sz):
+        out[i] = f(arr[i], freq)
+
+    return out
+
+
+
+cdef accessor _get_accessor_func(int code):
+    if code == 0:
+        return &pyear
+    elif code == 1:
+        return &pqyear
+    elif code == 2:
+        return &pquarter
+    elif code == 3:
+        return &pmonth
+    elif code == 4:
+        return &pday
+    elif code == 5:
+        return &phour
+    elif code == 6:
+        return &pminute
+    elif code == 7:
+        return &psecond
+    elif code == 8:
+        return &pweek
+    elif code == 9:
+        return &pday_of_year
+    elif code == 10:
+        return &pweekday
+    else:
+        raise ValueError('Unrecognized code: %s' % code)
+
diff --git a/pandas/tseries/index.py b/pandas/tseries/index.py
index 9d7d11c8d..6572b6c47 100644
--- a/pandas/tseries/index.py
+++ b/pandas/tseries/index.py
@@ -19,6 +19,7 @@ import pandas.tseries.tools as tools
 from pandas.lib import Timestamp
 import pandas.lib as lib
 import pandas._algos as _algos
+import pandas._hash as _hash
 
 
 def _utc():
@@ -134,7 +135,7 @@ class DatetimeIndex(Int64Index):
     # structured array cache for datetime fields
     _sarr_cache = None
 
-    _engine_type = lib.DatetimeEngine
+    _engine_type = _hash.DatetimeEngine
 
     offset = None
 
diff --git a/setup.py b/setup.py
index 07a172570..a056440c6 100755
--- a/setup.py
+++ b/setup.py
@@ -547,53 +547,90 @@ else:
     cmdclass['build_src'] = DummyBuildSrc
     cmdclass['build_ext'] =  build_ext
 
-tseries_depends = ['reindex', 'groupby', 'skiplist', 'moments',
-                   'reduce', 'stats', 'datetime',
-                   'hashtable', 'inference', 'properties', 'join', 'engines']
+lib_depends = ['reindex', 'groupby',
+               'reduce', 'inference', 'properties']
 
 def srcpath(name=None, suffix='.pyx', subdir='src'):
     return pjoin('pandas', subdir, name+suffix)
 
 if suffix == '.pyx':
-    tseries_depends = [srcpath(f, suffix='.pyx')
-                       for f in tseries_depends]
-    tseries_depends.append('pandas/src/util.pxd')
+    lib_depends = [srcpath(f, suffix='.pyx') for f in lib_depends]
+    lib_depends.append('pandas/src/util.pxd')
 else:
-    tseries_depends = []
+    lib_depends = []
     plib_depends = []
 
-common_include = [np.get_include(), 'pandas/src/klib']
+common_include = [np.get_include(), 'pandas/src/klib', 'pandas/src']
 
 algos_ext = Extension('pandas._algos',
                       sources=[srcpath('generated', suffix=suffix)],
                       include_dirs=common_include,
                       )
 
-lib_depends = tseries_depends + ['pandas/src/numpy_helper.h',
-                                 'pandas/src/parse_helper.h',
-                                 'pandas/src/datetime/np_datetime.h',
-                                 'pandas/src/datetime/np_datetime_strings.h',
-                                 'pandas/src/period.h']
+def pxd(name):
+    return os.path.abspath(pjoin('pandas/src', name+'.pxd'))
+
+
+lib_depends = lib_depends + ['pandas/src/numpy_helper.h',
+                             'pandas/src/parse_helper.h']
+
+
+tseries_depends = ['pandas/src/datetime/np_datetime.h',
+                   'pandas/src/datetime/np_datetime_strings.h',
+                   'pandas/src/period.h']
+
 
 # some linux distros require it
 libraries = ['m'] if 'win32' not in sys.platform else []
 
-lib_ext = Extension('pandas.lib',
-                    depends=lib_depends,
-                    sources=[srcpath('tseries', suffix=suffix),
-                             'pandas/src/datetime/np_datetime.c',
-                             'pandas/src/datetime/np_datetime_strings.c',
-                             'pandas/src/period.c'],
-                    include_dirs=common_include,
-                    # pyrex_gdb=True,
-                    # extra_compile_args=['-Wconversion']
-                    )
+ext_data = dict(
+    lib={'pyxfile': 'lib',
+         'pxdfiles': [],
+         'depends': lib_depends},
+    hashtable={'pyxfile': 'hashtable',
+               'pxdfiles': ['hashtable']},
+    _stats={'pyxfile': 'stats'}
+)
+
+extensions = []
+
+for name, data in ext_data.iteritems():
+    sources = [srcpath(data['pyxfile'], suffix=suffix)]
+    pxds = [pxd(x) for x in data.get('pxdfiles', [])]
+    if suffix == '.pyx' and pxds:
+        sources.extend(pxds)
+
+    include = data.get('include', common_include)
+
+    obj = Extension('pandas.%s' % name,
+                    sources=sources,
+                    depends=data.get('depends', []),
+                    include_dirs=include)
+
+    extensions.append(obj)
+
+
+index_ext = Extension('pandas._index',
+                     sources=[srcpath('engines', suffix=suffix),
+                              'pandas/src/datetime/np_datetime.c',
+                              'pandas/src/datetime/np_datetime_strings.c'],
+                     include_dirs=common_include)
+
+
+tseries_ext = Extension('pandas._tseries',
+                        depends=tseries_depends,
+                        sources=[srcpath('tseries', suffix=suffix),
+                                 'pandas/src/datetime/np_datetime.c',
+                                 'pandas/src/datetime/np_datetime_strings.c',
+                                 'pandas/src/period.c'],
+                        include_dirs=common_include)
 
 sparse_ext = Extension('pandas._sparse',
                        sources=[srcpath('sparse', suffix=suffix)],
                        include_dirs=[np.get_include()],
                        libraries=libraries)
 
+
 parser_ext = Extension('pandas._parser',
                        depends=['pandas/src/parser/tokenizer.h',
                                 'pandas/src/parser/io.h',
@@ -602,28 +639,30 @@ parser_ext = Extension('pandas._parser',
                                 'pandas/src/parser/tokenizer.c',
                                 'pandas/src/parser/io.c',
                                 ],
-                       #extra_compile_args=['-O3'],
                        include_dirs=common_include)
 
 sandbox_ext = Extension('pandas._sandbox',
                         sources=[srcpath('sandbox', suffix=suffix)],
                         include_dirs=common_include)
 
+
 pytables_ext = Extension('pandas._pytables',
                          sources=[srcpath('pytables', suffix=suffix)],
                          include_dirs=[np.get_include()],
                          libraries=libraries)
 
+
 cppsandbox_ext = Extension('pandas._cppsandbox',
                            language='c++',
                            sources=[srcpath('cppsandbox', suffix=suffix)],
                            include_dirs=[np.get_include()])
 
-extensions = [algos_ext,
-              lib_ext,
-              sparse_ext,
-              pytables_ext,
-              parser_ext]
+extensions.extend([algos_ext,
+                   tseries_ext,
+                   index_ext,
+                   sparse_ext,
+                   pytables_ext,
+                   parser_ext])
 
 # if not ISRELEASED:
 #     extensions.extend([sandbox_ext])
