commit b6e35ff277800d14d399b6fd0652d9d6cbcad730
Author: jbrockmendel <jbrockmendel@gmail.com>
Date:   Mon Aug 20 04:13:10 2018 -0700

    [CLN] More Misc Cleanups in _libs (#22287)

diff --git a/pandas/_libs/algos.pxd b/pandas/_libs/algos.pxd
index a535872ff..0888cf3c8 100644
--- a/pandas/_libs/algos.pxd
+++ b/pandas/_libs/algos.pxd
@@ -1,10 +1,12 @@
 from util cimport numeric
-from numpy cimport float64_t, double_t
+
 
 cpdef numeric kth_smallest(numeric[:] a, Py_ssize_t k) nogil
 
+
 cdef inline Py_ssize_t swap(numeric *a, numeric *b) nogil:
-    cdef numeric t
+    cdef:
+        numeric t
 
     # cython doesn't allow pointer dereference so use array syntax
     t = a[0]
@@ -12,6 +14,7 @@ cdef inline Py_ssize_t swap(numeric *a, numeric *b) nogil:
     b[0] = t
     return 0
 
+
 cdef enum TiebreakEnumType:
     TIEBREAK_AVERAGE
     TIEBREAK_MIN,
diff --git a/pandas/_libs/algos.pyx b/pandas/_libs/algos.pyx
index 908bf5998..249033b86 100644
--- a/pandas/_libs/algos.pyx
+++ b/pandas/_libs/algos.pyx
@@ -45,7 +45,7 @@ tiebreakers = {
 }
 
 
-cdef inline are_diff(object left, object right):
+cdef inline bint are_diff(object left, object right):
     try:
         return fabs(left - right) > FP_ERR
     except TypeError:
diff --git a/pandas/_libs/algos_common_helper.pxi.in b/pandas/_libs/algos_common_helper.pxi.in
index 97b7196da..42dda15ea 100644
--- a/pandas/_libs/algos_common_helper.pxi.in
+++ b/pandas/_libs/algos_common_helper.pxi.in
@@ -68,12 +68,12 @@ cpdef map_indices_{{name}}(ndarray[{{c_type}}] index):
 
 @cython.boundscheck(False)
 @cython.wraparound(False)
-def pad_{{name}}(ndarray[{{c_type}}] old, ndarray[{{c_type}}] new,
-                 limit=None):
-    cdef Py_ssize_t i, j, nleft, nright
-    cdef ndarray[int64_t, ndim=1] indexer
-    cdef {{c_type}} cur, next
-    cdef int lim, fill_count = 0
+def pad_{{name}}(ndarray[{{c_type}}] old, ndarray[{{c_type}}] new, limit=None):
+    cdef:
+        Py_ssize_t i, j, nleft, nright
+        ndarray[int64_t, ndim=1] indexer
+        {{c_type}} cur, next
+        int lim, fill_count = 0
 
     nleft = len(old)
     nright = len(new)
@@ -135,9 +135,10 @@ def pad_{{name}}(ndarray[{{c_type}}] old, ndarray[{{c_type}}] new,
 def pad_inplace_{{name}}(ndarray[{{c_type}}] values,
                          ndarray[uint8_t, cast=True] mask,
                          limit=None):
-    cdef Py_ssize_t i, N
-    cdef {{c_type}} val
-    cdef int lim, fill_count = 0
+    cdef:
+        Py_ssize_t i, N
+        {{c_type}} val
+        int lim, fill_count = 0
 
     N = len(values)
 
@@ -171,9 +172,10 @@ def pad_inplace_{{name}}(ndarray[{{c_type}}] values,
 def pad_2d_inplace_{{name}}(ndarray[{{c_type}}, ndim=2] values,
                             ndarray[uint8_t, ndim=2] mask,
                             limit=None):
-    cdef Py_ssize_t i, j, N, K
-    cdef {{c_type}} val
-    cdef int lim, fill_count = 0
+    cdef:
+        Py_ssize_t i, j, N, K
+        {{c_type}} val
+        int lim, fill_count = 0
 
     K, N = (<object> values).shape
 
@@ -233,10 +235,11 @@ D
 @cython.wraparound(False)
 def backfill_{{name}}(ndarray[{{c_type}}] old, ndarray[{{c_type}}] new,
                       limit=None):
-    cdef Py_ssize_t i, j, nleft, nright
-    cdef ndarray[int64_t, ndim=1] indexer
-    cdef {{c_type}} cur, prev
-    cdef int lim, fill_count = 0
+    cdef:
+        Py_ssize_t i, j, nleft, nright
+        ndarray[int64_t, ndim=1] indexer
+        {{c_type}} cur, prev
+        int lim, fill_count = 0
 
     nleft = len(old)
     nright = len(new)
@@ -299,9 +302,10 @@ def backfill_{{name}}(ndarray[{{c_type}}] old, ndarray[{{c_type}}] new,
 def backfill_inplace_{{name}}(ndarray[{{c_type}}] values,
                               ndarray[uint8_t, cast=True] mask,
                               limit=None):
-    cdef Py_ssize_t i, N
-    cdef {{c_type}} val
-    cdef int lim, fill_count = 0
+    cdef:
+        Py_ssize_t i, N
+        {{c_type}} val
+        int lim, fill_count = 0
 
     N = len(values)
 
@@ -335,9 +339,10 @@ def backfill_inplace_{{name}}(ndarray[{{c_type}}] values,
 def backfill_2d_inplace_{{name}}(ndarray[{{c_type}}, ndim=2] values,
                                  ndarray[uint8_t, ndim=2] mask,
                                  limit=None):
-    cdef Py_ssize_t i, j, N, K
-    cdef {{c_type}} val
-    cdef int lim, fill_count = 0
+    cdef:
+        Py_ssize_t i, j, N, K
+        {{c_type}} val
+        int lim, fill_count = 0
 
     K, N = (<object> values).shape
 
@@ -428,10 +433,10 @@ def is_monotonic_{{name}}(ndarray[{{c_type}}] arr, bint timelike):
 @cython.wraparound(False)
 @cython.boundscheck(False)
 def arrmap_{{name}}(ndarray[{{c_type}}] index, object func):
-    cdef Py_ssize_t length = index.shape[0]
-    cdef Py_ssize_t i = 0
-
-    cdef ndarray[object] result = np.empty(length, dtype=np.object_)
+    cdef:
+        Py_ssize_t length = index.shape[0]
+        Py_ssize_t i = 0
+        ndarray[object] result = np.empty(length, dtype=np.object_)
 
     from pandas._libs.lib import maybe_convert_objects
 
@@ -535,6 +540,7 @@ def put2d_{{name}}_{{dest_type}}(ndarray[{{c_type}}, ndim=2, cast=True] values,
 
 cdef int PLATFORM_INT = (<ndarray> np.arange(0, dtype=np.intp)).descr.type_num
 
+
 cpdef ensure_platform_int(object arr):
     # GH3033, GH1392
     # platform int is the size of the int pointer, e.g. np.intp
@@ -546,6 +552,7 @@ cpdef ensure_platform_int(object arr):
     else:
         return np.array(arr, dtype=np.intp)
 
+
 cpdef ensure_object(object arr):
     if util.is_array(arr):
         if (<ndarray> arr).descr.type_num == NPY_OBJECT:
diff --git a/pandas/_libs/groupby.pyx b/pandas/_libs/groupby.pyx
index 077ef925a..d8feda9ef 100644
--- a/pandas/_libs/groupby.pyx
+++ b/pandas/_libs/groupby.pyx
@@ -27,9 +27,10 @@ cdef double nan = NaN
 
 
 cdef inline float64_t median_linear(float64_t* a, int n) nogil:
-    cdef int i, j, na_count = 0
-    cdef float64_t result
-    cdef float64_t* tmp
+    cdef:
+        int i, j, na_count = 0
+        float64_t result
+        float64_t* tmp
 
     if n == 0:
         return NaN
@@ -318,7 +319,7 @@ def group_fillna_indexer(ndarray[int64_t] out, ndarray[int64_t] labels,
 
             # If we move to the next group, reset
             # the fill_idx and counter
-            if i == N - 1 or labels[idx] != labels[sorted_labels[i+1]]:
+            if i == N - 1 or labels[idx] != labels[sorted_labels[i + 1]]:
                 curr_fill_idx = -1
                 filled_vals = 0
 
diff --git a/pandas/_libs/hashing.pyx b/pandas/_libs/hashing.pyx
index 65fdeb8e3..557e3e34a 100644
--- a/pandas/_libs/hashing.pyx
+++ b/pandas/_libs/hashing.pyx
@@ -48,9 +48,8 @@ def hash_object_array(object[:] arr, object key, object encoding='utf8'):
     k = <bytes>key.encode(encoding)
     kb = <uint8_t *>k
     if len(k) != 16:
-        raise ValueError(
-            'key should be a 16-byte string encoded, got {!r} (len {})'.format(
-                k, len(k)))
+        raise ValueError("key should be a 16-byte string encoded, "
+                         "got {key} (len {klen})".format(key=k, klen=len(k)))
 
     n = len(arr)
 
@@ -70,8 +69,9 @@ def hash_object_array(object[:] arr, object key, object encoding='utf8'):
             data = <bytes>str(val).encode(encoding)
 
         else:
-            raise TypeError("{} of type {} is not a valid type for hashing, "
-                            "must be string or null".format(val, type(val)))
+            raise TypeError("{val} of type {typ} is not a valid type "
+                            "for hashing, must be string or null"
+                            .format(val=val, typ=type(val)))
 
         l = len(data)
         lens[i] = l
@@ -134,9 +134,9 @@ cdef inline void _sipround(uint64_t* v0, uint64_t* v1,
 
 cpdef uint64_t siphash(bytes data, bytes key) except? 0:
     if len(key) != 16:
-        raise ValueError(
-            'key should be a 16-byte bytestring, got {!r} (len {})'.format(
-                key, len(key)))
+        raise ValueError("key should be a 16-byte bytestring, "
+                         "got {key} (len {klen})"
+                         .format(key=key, klen=len(key)))
     return low_level_siphash(data, len(data), key)
 
 
diff --git a/pandas/_libs/hashtable_class_helper.pxi.in b/pandas/_libs/hashtable_class_helper.pxi.in
index 7f4a2eeaf..550cabd5e 100644
--- a/pandas/_libs/hashtable_class_helper.pxi.in
+++ b/pandas/_libs/hashtable_class_helper.pxi.in
@@ -115,7 +115,8 @@ cdef class {{name}}Vector:
 
         if needs_resize(self.data):
             if self.external_view_exists:
-                raise ValueError("external reference but Vector.resize() needed")
+                raise ValueError("external reference but "
+                                 "Vector.resize() needed")
             self.resize()
 
         append_data_{{dtype}}(self.data, x)
@@ -194,6 +195,7 @@ cdef class StringVector:
         for i in range(len(x)):
             self.append(x[i])
 
+
 cdef class ObjectVector:
 
     cdef:
@@ -215,7 +217,8 @@ cdef class ObjectVector:
     cdef inline append(self, object o):
         if self.n == self.m:
             if self.external_view_exists:
-                raise ValueError("external reference but Vector.resize() needed")
+                raise ValueError("external reference but "
+                                 "Vector.resize() needed")
             self.m = max(self.m * 2, _INIT_VEC_CAP)
             self.ao.resize(self.m, refcheck=False)
             self.data = <PyObject**> self.ao.data
@@ -405,8 +408,9 @@ cdef class {{name}}HashTable(HashTable):
                     if needs_resize(ud):
                         with gil:
                             if uniques.external_view_exists:
-                                raise ValueError("external reference to uniques held, "
-                                        "but Vector.resize() needed")
+                                raise ValueError("external reference to "
+                                                 "uniques held, but "
+                                                 "Vector.resize() needed")
                             uniques.resize()
                     append_data_{{dtype}}(ud, val)
                     labels[i] = count
@@ -742,8 +746,10 @@ cdef class StringHashTable(HashTable):
 
         return np.asarray(labels)
 
+
 na_sentinel = object
 
+
 cdef class PyObjectHashTable(HashTable):
 
     def __init__(self, size_hint=1):
diff --git a/pandas/_libs/hashtable_func_helper.pxi.in b/pandas/_libs/hashtable_func_helper.pxi.in
index f5cd8d056..45a69b613 100644
--- a/pandas/_libs/hashtable_func_helper.pxi.in
+++ b/pandas/_libs/hashtable_func_helper.pxi.in
@@ -273,7 +273,6 @@ def ismember_{{dtype}}({{scalar}}[:] arr, {{scalar}}[:] values):
 {{endfor}}
 
 
-
 #----------------------------------------------------------------------
 # Mode Computations
 #----------------------------------------------------------------------
diff --git a/pandas/_libs/reduction.pyx b/pandas/_libs/reduction.pyx
index 2ccb58dd6..fe993ecc0 100644
--- a/pandas/_libs/reduction.pyx
+++ b/pandas/_libs/reduction.pyx
@@ -24,7 +24,7 @@ is_numpy_prior_1_6_2 = LooseVersion(np.__version__) < '1.6.2'
 cdef _get_result_array(object obj, Py_ssize_t size, Py_ssize_t cnt):
 
     if (util.is_array(obj) or
-            isinstance(obj, list) and len(obj) == cnt or
+            (isinstance(obj, list) and len(obj) == cnt) or
             getattr(obj, 'shape', None) == (cnt,)):
         raise ValueError('function does not reduce')
 
diff --git a/pandas/_libs/tslib.pyx b/pandas/_libs/tslib.pyx
index 7b938d027..be695eabb 100644
--- a/pandas/_libs/tslib.pyx
+++ b/pandas/_libs/tslib.pyx
@@ -61,12 +61,14 @@ cdef inline object create_datetime_from_ts(
     return datetime(dts.year, dts.month, dts.day, dts.hour,
                     dts.min, dts.sec, dts.us, tz)
 
+
 cdef inline object create_date_from_ts(
         int64_t value, npy_datetimestruct dts,
         object tz, object freq):
     """ convenience routine to construct a datetime.date from its parts """
     return date(dts.year, dts.month, dts.day)
 
+
 cdef inline object create_time_from_ts(
         int64_t value, npy_datetimestruct dts,
         object tz, object freq):
@@ -350,8 +352,8 @@ cpdef array_with_unit_to_datetime(ndarray values, unit, errors='coerce'):
 
             if ((fvalues < _NS_LOWER_BOUND).any()
                     or (fvalues > _NS_UPPER_BOUND).any()):
-                raise OutOfBoundsDatetime(
-                    "cannot convert input with unit '{0}'".format(unit))
+                raise OutOfBoundsDatetime("cannot convert input with unit "
+                                          "'{unit}'".format(unit=unit))
             result = (iresult * m).astype('M8[ns]')
             iresult = result.view('i8')
             iresult[mask] = iNaT
@@ -377,8 +379,8 @@ cpdef array_with_unit_to_datetime(ndarray values, unit, errors='coerce'):
                     except OverflowError:
                         if is_raise:
                             raise OutOfBoundsDatetime(
-                                "cannot convert input {0} with the unit "
-                                "'{1}'".format(val, unit))
+                                "cannot convert input {val} with the unit "
+                                "'{unit}'".format(val=val, unit=unit))
                         elif is_ignore:
                             raise AssertionError
                         iresult[i] = NPY_NAT
@@ -393,16 +395,16 @@ cpdef array_with_unit_to_datetime(ndarray values, unit, errors='coerce'):
                     except ValueError:
                         if is_raise:
                             raise ValueError(
-                                "non convertible value {0} with the unit "
-                                "'{1}'".format(val, unit))
+                                "non convertible value {val} with the unit "
+                                "'{unit}'".format(val=val, unit=unit))
                         elif is_ignore:
                             raise AssertionError
                         iresult[i] = NPY_NAT
                     except:
                         if is_raise:
                             raise OutOfBoundsDatetime(
-                                "cannot convert input {0} with the unit "
-                                "'{1}'".format(val, unit))
+                                "cannot convert input {val} with the unit "
+                                "'{unit}'".format(val=val, unit=unit))
                         elif is_ignore:
                             raise AssertionError
                         iresult[i] = NPY_NAT
@@ -695,8 +697,8 @@ cpdef array_to_datetime(ndarray[object] values, errors='raise',
                 if is_coerce:
                     iresult[i] = NPY_NAT
                 else:
-                    raise TypeError("{0} is not convertible to datetime"
-                                    .format(type(val)))
+                    raise TypeError("{typ} is not convertible to datetime"
+                                    .format(typ=type(val)))
 
         if seen_datetime and seen_integer:
             # we have mixed datetimes & integers
diff --git a/pandas/_libs/tslibs/timezones.pxd b/pandas/_libs/tslibs/timezones.pxd
index e8a10a072..8965b46f7 100644
--- a/pandas/_libs/tslibs/timezones.pxd
+++ b/pandas/_libs/tslibs/timezones.pxd
@@ -10,7 +10,7 @@ cpdef bint tz_compare(object start, object end)
 cpdef object get_timezone(object tz)
 cpdef object maybe_get_tz(object tz)
 
-cpdef get_utcoffset(tzinfo, obj)
+cdef get_utcoffset(tzinfo, obj)
 cdef bint is_fixed_offset(object tz)
 
 cdef object get_dst_info(object tz)
diff --git a/pandas/_libs/tslibs/timezones.pyx b/pandas/_libs/tslibs/timezones.pyx
index 4d87a3786..36ec499c7 100644
--- a/pandas/_libs/tslibs/timezones.pyx
+++ b/pandas/_libs/tslibs/timezones.pyx
@@ -149,7 +149,7 @@ cdef inline object tz_cache_key(object tz):
 # UTC Offsets
 
 
-cpdef get_utcoffset(tzinfo, obj):
+cdef get_utcoffset(tzinfo, obj):
     try:
         return tzinfo._utcoffset
     except AttributeError:
@@ -186,7 +186,7 @@ cdef object get_utc_trans_times_from_dateutil_tz(object tz):
     return new_trans
 
 
-cpdef int64_t[:] unbox_utcoffsets(object transinfo):
+cdef int64_t[:] unbox_utcoffsets(object transinfo):
     cdef:
         Py_ssize_t i, sz
         int64_t[:] arr
diff --git a/pandas/_libs/window.pyx b/pandas/_libs/window.pyx
index c43750c75..b25fb4706 100644
--- a/pandas/_libs/window.pyx
+++ b/pandas/_libs/window.pyx
@@ -609,12 +609,12 @@ def roll_mean(ndarray[double_t] input, int64_t win, int64_t minp,
     else:
 
         with nogil:
-            for i from 0 <= i < minp - 1:
+            for i in range(minp - 1):
                 val = input[i]
                 add_mean(val, &nobs, &sum_x, &neg_ct)
                 output[i] = NaN
 
-            for i from minp - 1 <= i < N:
+            for i in range(minp - 1, N):
                 val = input[i]
                 add_mean(val, &nobs, &sum_x, &neg_ct)
 
@@ -747,7 +747,7 @@ def roll_var(ndarray[double_t] input, int64_t win, int64_t minp,
 
             # Over the first window, observations can only be added, never
             # removed
-            for i from 0 <= i < win:
+            for i in range(win):
                 add_var(input[i], &nobs, &mean_x, &ssqdm_x)
                 output[i] = calc_var(minp, ddof, nobs, ssqdm_x)
 
@@ -756,7 +756,7 @@ def roll_var(ndarray[double_t] input, int64_t win, int64_t minp,
 
             # After the first window, observations can both be added and
             # removed
-            for i from win <= i < N:
+            for i in range(win, N):
                 val = input[i]
                 prev = input[i - win]
 
@@ -816,6 +816,7 @@ cdef inline double calc_skew(int64_t minp, int64_t nobs, double x, double xx,
 
     return result
 
+
 cdef inline void add_skew(double val, int64_t *nobs, double *x, double *xx,
                           double *xxx) nogil:
     """ add a value from the skew calc """
@@ -829,6 +830,7 @@ cdef inline void add_skew(double val, int64_t *nobs, double *x, double *xx,
         xx[0] = xx[0] + val * val
         xxx[0] = xxx[0] + val * val * val
 
+
 cdef inline void remove_skew(double val, int64_t *nobs, double *x, double *xx,
                              double *xxx) nogil:
     """ remove a value from the skew calc """
@@ -896,12 +898,12 @@ def roll_skew(ndarray[double_t] input, int64_t win, int64_t minp,
     else:
 
         with nogil:
-            for i from 0 <= i < minp - 1:
+            for i in range(minp - 1):
                 val = input[i]
                 add_skew(val, &nobs, &x, &xx, &xxx)
                 output[i] = NaN
 
-            for i from minp - 1 <= i < N:
+            for i in range(minp - 1, N):
                 val = input[i]
                 add_skew(val, &nobs, &x, &xx, &xxx)
 
@@ -951,6 +953,7 @@ cdef inline double calc_kurt(int64_t minp, int64_t nobs, double x, double xx,
 
     return result
 
+
 cdef inline void add_kurt(double val, int64_t *nobs, double *x, double *xx,
                           double *xxx, double *xxxx) nogil:
     """ add a value from the kurotic calc """
@@ -965,6 +968,7 @@ cdef inline void add_kurt(double val, int64_t *nobs, double *x, double *xx,
         xxx[0] = xxx[0] + val * val * val
         xxxx[0] = xxxx[0] + val * val * val * val
 
+
 cdef inline void remove_kurt(double val, int64_t *nobs, double *x, double *xx,
                              double *xxx, double *xxxx) nogil:
     """ remove a value from the kurotic calc """
@@ -1031,11 +1035,11 @@ def roll_kurt(ndarray[double_t] input, int64_t win, int64_t minp,
 
         with nogil:
 
-            for i from 0 <= i < minp - 1:
+            for i in range(minp - 1):
                 add_kurt(input[i], &nobs, &x, &xx, &xxx, &xxxx)
                 output[i] = NaN
 
-            for i from minp - 1 <= i < N:
+            for i in range(minp - 1, N):
                 add_kurt(input[i], &nobs, &x, &xx, &xxx, &xxxx)
 
                 if i > win - 1:
@@ -1589,7 +1593,7 @@ def roll_generic(object obj,
 
     elif not raw:
         # series
-        for i from 0 <= i < N:
+        for i in range(N):
             if counts[i] >= minp:
                 sl = slice(int_max(i + offset - win + 1, 0),
                            int_min(i + offset + 1, N))
@@ -1652,7 +1656,7 @@ def roll_window(ndarray[float64_t, ndim=1, cast=True] input,
     minp = _check_minp(len(weights), minp, in_n)
 
     if avg:
-        for win_i from 0 <= win_i < win_n:
+        for win_i in range(win_n):
             val_win = weights[win_i]
             if val_win != val_win:
                 continue
@@ -1664,7 +1668,7 @@ def roll_window(ndarray[float64_t, ndim=1, cast=True] input,
                     counts[in_i + (win_n - win_i) - 1] += 1
                     tot_wgt[in_i + (win_n - win_i) - 1] += val_win
 
-        for in_i from 0 <= in_i < in_n:
+        for in_i in range(in_n):
             c = counts[in_i]
             if c < minp:
                 output[in_i] = NaN
@@ -1676,7 +1680,7 @@ def roll_window(ndarray[float64_t, ndim=1, cast=True] input,
                     output[in_i] /= tot_wgt[in_i]
 
     else:
-        for win_i from 0 <= win_i < win_n:
+        for win_i in range(win_n):
             val_win = weights[win_i]
             if val_win != val_win:
                 continue
@@ -1688,7 +1692,7 @@ def roll_window(ndarray[float64_t, ndim=1, cast=True] input,
                     output[in_i + (win_n - win_i) - 1] += val_in * val_win
                     counts[in_i + (win_n - win_i) - 1] += 1
 
-        for in_i from 0 <= in_i < in_n:
+        for in_i in range(in_n):
             c = counts[in_i]
             if c < minp:
                 output[in_i] = NaN
@@ -1699,14 +1703,13 @@ def roll_window(ndarray[float64_t, ndim=1, cast=True] input,
 # Exponentially weighted moving average
 
 
-def ewma(ndarray[double_t] input, double_t com, int adjust, int ignore_na,
-         int minp):
+def ewma(double_t[:] vals, double_t com, int adjust, int ignore_na, int minp):
     """
     Compute exponentially-weighted moving average using center-of-mass.
 
     Parameters
     ----------
-    input : ndarray (float64 type)
+    vals : ndarray (float64 type)
     com : float64
     adjust: int
     ignore_na: int
@@ -1717,28 +1720,29 @@ def ewma(ndarray[double_t] input, double_t com, int adjust, int ignore_na,
     y : ndarray
     """
 
-    cdef Py_ssize_t N = len(input)
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+    cdef:
+        Py_ssize_t N = len(vals)
+        ndarray[double_t] output = np.empty(N, dtype=float)
+        double alpha, old_wt_factor, new_wt, weighted_avg, old_wt, cur
+        Py_ssize_t i, nobs
+
     if N == 0:
         return output
 
     minp = max(minp, 1)
 
-    cdef double alpha, old_wt_factor, new_wt, weighted_avg, old_wt, cur
-    cdef Py_ssize_t i, nobs
-
     alpha = 1. / (1. + com)
     old_wt_factor = 1. - alpha
     new_wt = 1. if adjust else alpha
 
-    weighted_avg = input[0]
+    weighted_avg = vals[0]
     is_observation = (weighted_avg == weighted_avg)
     nobs = int(is_observation)
     output[0] = weighted_avg if (nobs >= minp) else NaN
     old_wt = 1.
 
-    for i from 1 <= i < N:
-        cur = input[i]
+    for i in range(1, N):
+        cur = vals[i]
         is_observation = (cur == cur)
         nobs += int(is_observation)
         if weighted_avg == weighted_avg:
@@ -1767,7 +1771,7 @@ def ewma(ndarray[double_t] input, double_t com, int adjust, int ignore_na,
 # Exponentially weighted moving covariance
 
 
-def ewmcov(ndarray[double_t] input_x, ndarray[double_t] input_y,
+def ewmcov(double_t[:] input_x, double_t[:] input_y,
            double_t com, int adjust, int ignore_na, int minp, int bias):
     """
     Compute exponentially-weighted moving variance using center-of-mass.
@@ -1787,20 +1791,23 @@ def ewmcov(ndarray[double_t] input_x, ndarray[double_t] input_y,
     y : ndarray
     """
 
-    cdef Py_ssize_t N = len(input_x)
+    cdef:
+        Py_ssize_t N = len(input_x)
+        double alpha, old_wt_factor, new_wt, mean_x, mean_y, cov
+        double sum_wt, sum_wt2, old_wt, cur_x, cur_y, old_mean_x, old_mean_y
+        Py_ssize_t i, nobs
+        ndarray[double_t] output
+
     if len(input_y) != N:
         raise ValueError("arrays are of different lengths "
-                         "(%d and %d)" % (N, len(input_y)))
-    cdef ndarray[double_t] output = np.empty(N, dtype=float)
+                         "({N} and {len_y})".format(N=N, len_y=len(input_y)))
+
+    output = np.empty(N, dtype=float)
     if N == 0:
         return output
 
     minp = max(minp, 1)
 
-    cdef double alpha, old_wt_factor, new_wt, mean_x, mean_y, cov
-    cdef double sum_wt, sum_wt2, old_wt, cur_x, cur_y, old_mean_x, old_mean_y
-    cdef Py_ssize_t i, nobs
-
     alpha = 1. / (1. + com)
     old_wt_factor = 1. - alpha
     new_wt = 1. if adjust else alpha
@@ -1818,7 +1825,7 @@ def ewmcov(ndarray[double_t] input_x, ndarray[double_t] input_y,
     sum_wt2 = 1.
     old_wt = 1.
 
-    for i from 1 <= i < N:
+    for i in range(1, N):
         cur_x = input_x[i]
         cur_y = input_y[i]
         is_observation = ((cur_x == cur_x) and (cur_y == cur_y))
