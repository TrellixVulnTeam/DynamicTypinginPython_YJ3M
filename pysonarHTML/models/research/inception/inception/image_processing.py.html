<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/research/inception/inception/image_processing.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS'>FLAGS</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs'>inputs</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs'>distorted_inputs</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg'>decode_jpeg</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color'>distort_color</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image'>distort_image</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image'>eval_image</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing'>image_preprocessing</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto'>parse_example_proto</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', xid='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs'>batch_inputs</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2016 Google Inc. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Read and preprocess image data.
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span>  Image processing occurs on a single image at a time. Image are read and
<span class='lineno'>  18</span>  preprocessed in parallel across multiple threads. The resulting images
<span class='lineno'>  19</span>  are concatenated together to form a single batch for training or evaluation.
<span class='lineno'>  20</span> 
<span class='lineno'>  21</span>  -- Provide processed image data for a network:
<span class='lineno'>  22</span>  inputs: Construct batches of evaluation examples of images.
<span class='lineno'>  23</span>  distorted_inputs: Construct batches of training examples of images.
<span class='lineno'>  24</span>  batch_inputs: Construct batches of training or evaluation examples of images.
<span class='lineno'>  25</span> 
<span class='lineno'>  26</span>  -- Data processing:
<span class='lineno'>  27</span>  parse_example_proto: Parses an Example proto containing a training example
<span class='lineno'>  28</span>    of an image.
<span class='lineno'>  29</span> 
<span class='lineno'>  30</span>  -- Image decoding:
<span class='lineno'>  31</span>  decode_jpeg: Decode a JPEG encoded string into a 3-D float32 Tensor.
<span class='lineno'>  32</span> 
<span class='lineno'>  33</span>  -- Image preprocessing:
<span class='lineno'>  34</span>  image_preprocessing: Decode and preprocess one image for evaluation or training
<span class='lineno'>  35</span>  distort_image: Distort one image for training a network.
<span class='lineno'>  36</span>  eval_image: Prepare one image for evaluation.
<span class='lineno'>  37</span>  distort_color: Distort the color in one image for training.
<span class='lineno'>  38</span> &quot;&quot;&quot;
<span class='lineno'>  39</span> from __future__ import absolute_import
<span class='lineno'>  40</span> from __future__ import division
<span class='lineno'>  41</span> from __future__ import print_function
<span class='lineno'>  42</span> 
<span class='lineno'>  43</span> import tensorflow as tf
<span class='lineno'>  44</span> 
<span class='lineno'>  45</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a> = tf.app.flags.FLAGS
<span class='lineno'>  46</span> 
<span class='lineno'>  47</span> tf.app.flags.DEFINE_integer(&#39;batch_size&#39;, 32,
<span class='lineno'>  48</span>                             &quot;&quot;&quot;Number of images to process in a batch.&quot;&quot;&quot;)
<span class='lineno'>  49</span> tf.app.flags.DEFINE_integer(&#39;image_size&#39;, 299,
<span class='lineno'>  50</span>                             &quot;&quot;&quot;Provide square images of this size.&quot;&quot;&quot;)
<span class='lineno'>  51</span> tf.app.flags.DEFINE_integer(&#39;num_preprocess_threads&#39;, 4,
<span class='lineno'>  52</span>                             &quot;&quot;&quot;Number of preprocessing threads per tower. &quot;&quot;&quot;
<span class='lineno'>  53</span>                             &quot;&quot;&quot;Please make this a multiple of 4.&quot;&quot;&quot;)
<span class='lineno'>  54</span> tf.app.flags.DEFINE_integer(&#39;num_readers&#39;, 4,
<span class='lineno'>  55</span>                             &quot;&quot;&quot;Number of parallel readers during train.&quot;&quot;&quot;)
<span class='lineno'>  56</span> 
<span class='lineno'>  57</span> # Images are preprocessed asynchronously using multiple threads specified by
<span class='lineno'>  58</span> # --num_preprocss_threads and the resulting processed images are stored in a
<span class='lineno'>  59</span> # random shuffling queue. The shuffling queue dequeues --batch_size images
<span class='lineno'>  60</span> # for processing on a given Inception tower. A larger shuffling queue guarantees
<span class='lineno'>  61</span> # better mixing across examples within a batch and results in slightly higher
<span class='lineno'>  62</span> # predictive performance in a trained model. Empirically,
<span class='lineno'>  63</span> # --input_queue_memory_factor=16 works well. A value of 16 implies a queue size
<span class='lineno'>  64</span> # of 1024*16 images. Assuming RGB 299x299 images, this implies a queue size of
<span class='lineno'>  65</span> # 16GB. If the machine is memory limited, then decrease this factor to
<span class='lineno'>  66</span> # decrease the CPU memory footprint, accordingly.
<span class='lineno'>  67</span> tf.app.flags.DEFINE_integer(&#39;input_queue_memory_factor&#39;, 16,
<span class='lineno'>  68</span>                             &quot;&quot;&quot;Size of the queue of preprocessed images. &quot;&quot;&quot;
<span class='lineno'>  69</span>                             &quot;&quot;&quot;Default is ideal but try smaller values, e.g. &quot;&quot;&quot;
<span class='lineno'>  70</span>                             &quot;&quot;&quot;4, 2 or 1, if host memory is constrained. See &quot;&quot;&quot;
<span class='lineno'>  71</span>                             &quot;&quot;&quot;comments in code for more details.&quot;&quot;&quot;)
<span class='lineno'>  72</span> 
<span class='lineno'>  73</span> 
<span class='lineno'>  74</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs', title='(?, None, None) -> (?, ?)'>inputs</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.dataset', title='?'>dataset</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', title='None'>batch_size</a>=None, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>=None):
<span class='lineno'>  75</span>   &quot;&quot;&quot;Generate batches of ImageNet images for evaluation.
<span class='lineno'>  76</span> 
<span class='lineno'>  77</span>   Use this function as the inputs for evaluating a network.
<span class='lineno'>  78</span> 
<span class='lineno'>  79</span>   Note that some (minimal) image preprocessing occurs during evaluation
<span class='lineno'>  80</span>   including central cropping and resizing of the image to fit the network.
<span class='lineno'>  81</span> 
<span class='lineno'>  82</span>   Args:
<span class='lineno'>  83</span>     dataset: instance of Dataset class specifying the dataset.
<span class='lineno'>  84</span>     batch_size: integer, number of examples in batch
<span class='lineno'>  85</span>     num_preprocess_threads: integer, total number of preprocessing threads but
<span class='lineno'>  86</span>       None defaults to FLAGS.num_preprocess_threads.
<span class='lineno'>  87</span> 
<span class='lineno'>  88</span>   Returns:
<span class='lineno'>  89</span>     images: Images. 4D tensor of size [batch_size, FLAGS.image_size,
<span class='lineno'>  90</span>                                        image_size, 3].
<span class='lineno'>  91</span>     labels: 1-D integer Tensor of [FLAGS.batch_size].
<span class='lineno'>  92</span>   &quot;&quot;&quot;
<span class='lineno'>  93</span>   if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', title='None'>batch_size</a>:
<span class='lineno'>  94</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', title='?'>batch_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.batch_size
<span class='lineno'>  95</span> 
<span class='lineno'>  96</span>   # Force all input processing onto CPU in order to reserve the GPU for
<span class='lineno'>  97</span>   # the forward inference and back-propagation.
<span class='lineno'>  98</span>   with tf.device(&#39;/cpu:0&#39;):
<span class='lineno'>  99</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.images', title='?'>images</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.labels', title='?'>labels</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', title='(?, ?, ?, None, int) -> (?, ?) / (?, None, bool, None, int) -> (?, ?)'>batch_inputs</a>(
<span class='lineno'> 100</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.dataset', title='?'>dataset</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.batch_size', title='None'>batch_size</a>, train=False,
<span class='lineno'> 101</span>         num_preprocess_threads=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>,
<span class='lineno'> 102</span>         num_readers=1)
<span class='lineno'> 103</span> 
<span class='lineno'> 104</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.images', title='?'>images</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.inputs.labels', title='?'>labels</a>
<span class='lineno'> 105</span> 
<span class='lineno'> 106</span> 
<span class='lineno'> 107</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs', title='(?, None, None) -> (?, ?)'>distorted_inputs</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.dataset', title='?'>dataset</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', title='None'>batch_size</a>=None, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>=None):
<span class='lineno'> 108</span>   &quot;&quot;&quot;Generate batches of distorted versions of ImageNet images.
<span class='lineno'> 109</span> 
<span class='lineno'> 110</span>   Use this function as the inputs for training a network.
<span class='lineno'> 111</span> 
<span class='lineno'> 112</span>   Distorting images provides a useful technique for augmenting the data
<span class='lineno'> 113</span>   set during training in order to make the network invariant to aspects
<span class='lineno'> 114</span>   of the image that do not effect the label.
<span class='lineno'> 115</span> 
<span class='lineno'> 116</span>   Args:
<span class='lineno'> 117</span>     dataset: instance of Dataset class specifying the dataset.
<span class='lineno'> 118</span>     batch_size: integer, number of examples in batch
<span class='lineno'> 119</span>     num_preprocess_threads: integer, total number of preprocessing threads but
<span class='lineno'> 120</span>       None defaults to FLAGS.num_preprocess_threads.
<span class='lineno'> 121</span> 
<span class='lineno'> 122</span>   Returns:
<span class='lineno'> 123</span>     images: Images. 4D tensor of size [batch_size, FLAGS.image_size,
<span class='lineno'> 124</span>                                        FLAGS.image_size, 3].
<span class='lineno'> 125</span>     labels: 1-D integer Tensor of [batch_size].
<span class='lineno'> 126</span>   &quot;&quot;&quot;
<span class='lineno'> 127</span>   if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', title='None'>batch_size</a>:
<span class='lineno'> 128</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', title='?'>batch_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.batch_size
<span class='lineno'> 129</span> 
<span class='lineno'> 130</span>   # Force all input processing onto CPU in order to reserve the GPU for
<span class='lineno'> 131</span>   # the forward inference and back-propagation.
<span class='lineno'> 132</span>   with tf.device(&#39;/cpu:0&#39;):
<span class='lineno'> 133</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.images', title='?'>images</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.labels', title='?'>labels</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', title='(?, ?, ?, None, int) -> (?, ?) / (?, None, bool, None, int) -> (?, ?)'>batch_inputs</a>(
<span class='lineno'> 134</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.dataset', title='?'>dataset</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.batch_size', title='None'>batch_size</a>, train=True,
<span class='lineno'> 135</span>         num_preprocess_threads=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>,
<span class='lineno'> 136</span>         num_readers=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.num_readers)
<span class='lineno'> 137</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.images', title='?'>images</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distorted_inputs.labels', title='?'>labels</a>
<span class='lineno'> 138</span> 
<span class='lineno'> 139</span> 
<span class='lineno'> 140</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg', title='(?, None) -> None'>decode_jpeg</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', title='?'>image_buffer</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.scope', title='None'>scope</a>=None):
<span class='lineno'> 141</span>   &quot;&quot;&quot;Decode a JPEG string into one 3-D float image Tensor.
<span class='lineno'> 142</span> 
<span class='lineno'> 143</span>   Args:
<span class='lineno'> 144</span>     image_buffer: scalar string Tensor.
<span class='lineno'> 145</span>     scope: Optional scope for name_scope.
<span class='lineno'> 146</span>   Returns:
<span class='lineno'> 147</span>     3-D float Tensor with values ranging from [0, 1).
<span class='lineno'> 148</span>   &quot;&quot;&quot;
<span class='lineno'> 149</span>   with tf.name_scope(values=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', title='?'>image_buffer</a>], name=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.scope', title='None'>scope</a>,
<span class='lineno'> 150</span>                      default_name=&#39;decode_jpeg&#39;):
<span class='lineno'> 151</span>     # Decode the string as an RGB JPEG.
<span class='lineno'> 152</span>     # Note that the resulting image contains an unknown height and width
<span class='lineno'> 153</span>     # that is set dynamically by decode_jpeg. In other words, the height
<span class='lineno'> 154</span>     # and width of image is unknown at compile-time.
<span class='lineno'> 155</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', title='?'>image</a> = tf.image.decode_jpeg(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image_buffer', title='?'>image_buffer</a>, channels=3)
<span class='lineno'> 156</span> 
<span class='lineno'> 157</span>     # After this point, all image pixels reside in [0,1)
<span class='lineno'> 158</span>     # until the very end, when they&#39;re rescaled to (-1, 1).  The various
<span class='lineno'> 159</span>     # adjust_* ops all require this range for dtype float.
<span class='lineno'> 160</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', title='?'>image</a> = tf.image.convert_image_dtype(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', title='?'>image</a>, dtype=tf.float32)
<span class='lineno'> 161</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg.image', title='?'>image</a>
<span class='lineno'> 162</span> 
<span class='lineno'> 163</span> 
<span class='lineno'> 164</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color', title='(?, int, None) -> None'>distort_color</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.thread_id', title='int'>thread_id</a>=0, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.scope', title='None'>scope</a>=None):
<span class='lineno'> 165</span>   &quot;&quot;&quot;Distort the color of the image.
<span class='lineno'> 166</span> 
<span class='lineno'> 167</span>   Each color distortion is non-commutative and thus ordering of the color ops
<span class='lineno'> 168</span>   matters. Ideally we would randomly permute the ordering of the color ops.
<span class='lineno'> 169</span>   Rather than adding that level of complication, we select a distinct ordering
<span class='lineno'> 170</span>   of color ops for each preprocessing thread.
<span class='lineno'> 171</span> 
<span class='lineno'> 172</span>   Args:
<span class='lineno'> 173</span>     image: Tensor containing single image.
<span class='lineno'> 174</span>     thread_id: preprocessing thread ID.
<span class='lineno'> 175</span>     scope: Optional scope for name_scope.
<span class='lineno'> 176</span>   Returns:
<span class='lineno'> 177</span>     color-distorted image
<span class='lineno'> 178</span>   &quot;&quot;&quot;
<span class='lineno'> 179</span>   with tf.name_scope(values=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>], name=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.scope', title='None'>scope</a>, default_name=&#39;distort_color&#39;):
<span class='lineno'> 180</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', title='int'>color_ordering</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.thread_id', title='int'>thread_id</a> % 2
<span class='lineno'> 181</span> 
<span class='lineno'> 182</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', title='int'>color_ordering</a> == 0:
<span class='lineno'> 183</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'> 184</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'> 185</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'> 186</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'> 187</span>     elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.color_ordering', title='int'>color_ordering</a> == 1:
<span class='lineno'> 188</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'> 189</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'> 190</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'> 191</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'> 192</span> 
<span class='lineno'> 193</span>     # The random_* ops do not necessarily clamp.
<span class='lineno'> 194</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a> = tf.clip_by_value(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>, 0.0, 1.0)
<span class='lineno'> 195</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color.image', title='?'>image</a>
<span class='lineno'> 196</span> 
<span class='lineno'> 197</span> 
<span class='lineno'> 198</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image', title='(?, ?, ?, ?, int, None) -> None / (None, ?, ?, ?, int, None) -> None'>distort_image</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', title='?'>height</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', title='?'>width</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', title='?'>bbox</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>=0, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.scope', title='None'>scope</a>=None):
<span class='lineno'> 199</span>   &quot;&quot;&quot;Distort one image for training a network.
<span class='lineno'> 200</span> 
<span class='lineno'> 201</span>   Distorting images provides a useful technique for augmenting the data
<span class='lineno'> 202</span>   set during training in order to make the network invariant to aspects
<span class='lineno'> 203</span>   of the image that do not effect the label.
<span class='lineno'> 204</span> 
<span class='lineno'> 205</span>   Args:
<span class='lineno'> 206</span>     image: 3-D float Tensor of image
<span class='lineno'> 207</span>     height: integer
<span class='lineno'> 208</span>     width: integer
<span class='lineno'> 209</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 210</span>       where each coordinate is [0, 1) and the coordinates are arranged
<span class='lineno'> 211</span>       as [ymin, xmin, ymax, xmax].
<span class='lineno'> 212</span>     thread_id: integer indicating the preprocessing thread.
<span class='lineno'> 213</span>     scope: Optional scope for name_scope.
<span class='lineno'> 214</span>   Returns:
<span class='lineno'> 215</span>     3-D float Tensor of distorted image used for training.
<span class='lineno'> 216</span>   &quot;&quot;&quot;
<span class='lineno'> 217</span>   with tf.name_scope(values=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', title='?'>width</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', title='?'>bbox</a>], name=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.scope', title='None'>scope</a>,
<span class='lineno'> 218</span>                      default_name=&#39;distort_image&#39;):
<span class='lineno'> 219</span>     # Each bounding box has shape [1, num_boxes, box coords] and
<span class='lineno'> 220</span>     # the coordinates are ordered [ymin, xmin, ymax, xmax].
<span class='lineno'> 221</span> 
<span class='lineno'> 222</span>     # Display the bounding box in the first thread only.
<span class='lineno'> 223</span>     if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>:
<span class='lineno'> 224</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_box', title='?'>image_with_box</a> = tf.image.draw_bounding_boxes(tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>, 0),
<span class='lineno'> 225</span>                                                     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', title='?'>bbox</a>)
<span class='lineno'> 226</span>       tf.summary.image(&#39;image_with_bounding_boxes&#39;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_box', title='?'>image_with_box</a>)
<span class='lineno'> 227</span> 
<span class='lineno'> 228</span>   # A large fraction of image datasets contain a human-annotated bounding
<span class='lineno'> 229</span>   # box delineating the region of the image containing the object of interest.
<span class='lineno'> 230</span>   # We choose to create a new bounding box for the object which is a randomly
<span class='lineno'> 231</span>   # distorted version of the human-annotated bounding box that obeys an allowed
<span class='lineno'> 232</span>   # range of aspect ratios, sizes and overlap with the human-annotated
<span class='lineno'> 233</span>   # bounding box. If no box is supplied, then we assume the bounding box is
<span class='lineno'> 234</span>   # the entire image.
<span class='lineno'> 235</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.sample_distorted_bounding_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.sample_distorted_bounding_box', title='?'>sample_distorted_bounding_box</a> = tf.image.sample_distorted_bounding_box(
<span class='lineno'> 236</span>         tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>),
<span class='lineno'> 237</span>         bounding_boxes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox', title='?'>bbox</a>,
<span class='lineno'> 238</span>         min_object_covered=0.1,
<span class='lineno'> 239</span>         aspect_ratio_range=[0.75, 1.33],
<span class='lineno'> 240</span>         area_range=[0.05, 1.0],
<span class='lineno'> 241</span>         max_attempts=100,
<span class='lineno'> 242</span>         use_image_if_no_bounding_boxes=True)
<span class='lineno'> 243</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_begin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_begin', title='?'>bbox_begin</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_size', title='?'>bbox_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distort_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distort_bbox', title='?'>distort_bbox</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.sample_distorted_bounding_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.sample_distorted_bounding_box', title='?'>sample_distorted_bounding_box</a>
<span class='lineno'> 244</span>     if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>:
<span class='lineno'> 245</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_distorted_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_distorted_box', title='?'>image_with_distorted_box</a> = tf.image.draw_bounding_boxes(
<span class='lineno'> 246</span>           tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>, 0), <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distort_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distort_bbox', title='?'>distort_bbox</a>)
<span class='lineno'> 247</span>       tf.summary.image(&#39;images_with_distorted_bounding_box&#39;,
<span class='lineno'> 248</span>                        <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_distorted_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image_with_distorted_box', title='?'>image_with_distorted_box</a>)
<span class='lineno'> 249</span> 
<span class='lineno'> 250</span>     # Crop the image to the specified bounding box.
<span class='lineno'> 251</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a> = tf.slice(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_begin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_begin', title='?'>bbox_begin</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.bbox_size', title='?'>bbox_size</a>)
<span class='lineno'> 252</span> 
<span class='lineno'> 253</span>     # This resizing operation may distort the images because the aspect
<span class='lineno'> 254</span>     # ratio is not respected. We select a resize method in a round robin
<span class='lineno'> 255</span>     # fashion based on the thread number.
<span class='lineno'> 256</span>     # Note that ResizeMethod contains 4 enumerated resizing methods.
<span class='lineno'> 257</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.resize_method', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.resize_method', title='int'>resize_method</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a> % 4
<span class='lineno'> 258</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a> = tf.image.resize_images(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', title='?'>width</a>],
<span class='lineno'> 259</span>                                              method=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.resize_method', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.resize_method', title='int'>resize_method</a>)
<span class='lineno'> 260</span>     # Restore the shape since the dynamic slice based upon the bbox_size loses
<span class='lineno'> 261</span>     # the third dimension.
<span class='lineno'> 262</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a>.set_shape([<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.width', title='?'>width</a>, 3])
<span class='lineno'> 263</span>     if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>:
<span class='lineno'> 264</span>       tf.summary.image(&#39;cropped_resized_image&#39;,
<span class='lineno'> 265</span>                        tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a>, 0))
<span class='lineno'> 266</span> 
<span class='lineno'> 267</span>     # Randomly flip the image horizontally.
<span class='lineno'> 268</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a> = tf.image.random_flip_left_right(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a>)
<span class='lineno'> 269</span> 
<span class='lineno'> 270</span>     # Randomly distort the colors.
<span class='lineno'> 271</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='None'>distorted_image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_color', title='(?, int, None) -> None'>distort_color</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='?'>distorted_image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>)
<span class='lineno'> 272</span> 
<span class='lineno'> 273</span>     if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.thread_id', title='int'>thread_id</a>:
<span class='lineno'> 274</span>       tf.summary.image(&#39;final_distorted_image&#39;,
<span class='lineno'> 275</span>                        tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='None'>distorted_image</a>, 0))
<span class='lineno'> 276</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image.distorted_image', title='None'>distorted_image</a>
<span class='lineno'> 277</span> 
<span class='lineno'> 278</span> 
<span class='lineno'> 279</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image', title='(None, ?, ?, None) -> None / (?, ?, ?, None) -> None'>eval_image</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='None'>image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', title='?'>height</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', title='?'>width</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.scope', title='None'>scope</a>=None):
<span class='lineno'> 280</span>   &quot;&quot;&quot;Prepare one image for evaluation.
<span class='lineno'> 281</span> 
<span class='lineno'> 282</span>   Args:
<span class='lineno'> 283</span>     image: 3-D float Tensor
<span class='lineno'> 284</span>     height: integer
<span class='lineno'> 285</span>     width: integer
<span class='lineno'> 286</span>     scope: Optional scope for name_scope.
<span class='lineno'> 287</span>   Returns:
<span class='lineno'> 288</span>     3-D float Tensor of prepared image.
<span class='lineno'> 289</span>   &quot;&quot;&quot;
<span class='lineno'> 290</span>   with tf.name_scope(values=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', title='?'>width</a>], name=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.scope', title='None'>scope</a>,
<span class='lineno'> 291</span>                      default_name=&#39;eval_image&#39;):
<span class='lineno'> 292</span>     # Crop the central region of the image with an area containing 87.5% of
<span class='lineno'> 293</span>     # the original image.
<span class='lineno'> 294</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a> = tf.image.central_crop(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='None'>image</a>, central_fraction=0.875)
<span class='lineno'> 295</span> 
<span class='lineno'> 296</span>     # Resize the image to the original height and width.
<span class='lineno'> 297</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a>, 0)
<span class='lineno'> 298</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a> = tf.image.resize_bilinear(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.width', title='?'>width</a>],
<span class='lineno'> 299</span>                                      align_corners=False)
<span class='lineno'> 300</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a> = tf.squeeze(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a>, [0])
<span class='lineno'> 301</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image.image', title='?'>image</a>
<span class='lineno'> 302</span> 
<span class='lineno'> 303</span> 
<span class='lineno'> 304</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing', title='(?, ?, bool, int) -> None / (?, ?, ?, int) -> None'>image_preprocessing</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image_buffer', title='?'>image_buffer</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', title='?'>bbox</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.train', title='bool'>train</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.thread_id', title='int'>thread_id</a>=0):
<span class='lineno'> 305</span>   &quot;&quot;&quot;Decode and preprocess one image for evaluation or training.
<span class='lineno'> 306</span> 
<span class='lineno'> 307</span>   Args:
<span class='lineno'> 308</span>     image_buffer: JPEG encoded string Tensor
<span class='lineno'> 309</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 310</span>       where each coordinate is [0, 1) and the coordinates are arranged as
<span class='lineno'> 311</span>       [ymin, xmin, ymax, xmax].
<span class='lineno'> 312</span>     train: boolean
<span class='lineno'> 313</span>     thread_id: integer indicating preprocessing thread
<span class='lineno'> 314</span> 
<span class='lineno'> 315</span>   Returns:
<span class='lineno'> 316</span>     3-D float Tensor containing an appropriately scaled image
<span class='lineno'> 317</span> 
<span class='lineno'> 318</span>   Raises:
<span class='lineno'> 319</span>     ValueError: if user does not provide bounding box
<span class='lineno'> 320</span>   &quot;&quot;&quot;
<span class='lineno'> 321</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', title='?'>bbox</a> is None:
<span class='lineno'> 322</span>     raise ValueError(&#39;Please supply a bounding box.&#39;)
<span class='lineno'> 323</span> 
<span class='lineno'> 324</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.decode_jpeg', title='(?, None) -> None'>decode_jpeg</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image_buffer', title='?'>image_buffer</a>)
<span class='lineno'> 325</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', title='?'>height</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.image_size
<span class='lineno'> 326</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', title='?'>width</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.image_size
<span class='lineno'> 327</span> 
<span class='lineno'> 328</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.train', title='bool'>train</a>:
<span class='lineno'> 329</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.distort_image', title='(?, ?, ?, ?, int, None) -> None / (None, ?, ?, ?, int, None) -> None'>distort_image</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', title='?'>width</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.bbox', title='?'>bbox</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.thread_id', title='int'>thread_id</a>)
<span class='lineno'> 330</span>   else:
<span class='lineno'> 331</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.eval_image', title='(None, ?, ?, None) -> None / (?, ?, ?, None) -> None'>eval_image</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.width', title='?'>width</a>)
<span class='lineno'> 332</span> 
<span class='lineno'> 333</span>   # Finally, rescale to [-1,1] instead of [0, 1)
<span class='lineno'> 334</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='?'>image</a> = tf.subtract(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='None'>image</a>, 0.5)
<span class='lineno'> 335</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='?'>image</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='?'>image</a>, 2.0)
<span class='lineno'> 336</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing.image', title='?'>image</a>
<span class='lineno'> 337</span> 
<span class='lineno'> 338</span> 
<span class='lineno'> 339</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto', title='? -> (?, ?, ?, ?)'>parse_example_proto</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.example_serialized', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.example_serialized', title='?'>example_serialized</a>):
<span class='lineno'> 340</span>   &quot;&quot;&quot;Parses an Example proto containing a training example of an image.
<span class='lineno'> 341</span> 
<span class='lineno'> 342</span>   The output of the build_image_data.py image preprocessing script is a dataset
<span class='lineno'> 343</span>   containing serialized Example protocol buffers. Each Example proto contains
<span class='lineno'> 344</span>   the following fields:
<span class='lineno'> 345</span> 
<span class='lineno'> 346</span>     image/height: 462
<span class='lineno'> 347</span>     image/width: 581
<span class='lineno'> 348</span>     image/colorspace: &#39;RGB&#39;
<span class='lineno'> 349</span>     image/channels: 3
<span class='lineno'> 350</span>     image/class/label: 615
<span class='lineno'> 351</span>     image/class/synset: &#39;n03623198&#39;
<span class='lineno'> 352</span>     image/class/text: &#39;knee pad&#39;
<span class='lineno'> 353</span>     image/object/bbox/xmin: 0.1
<span class='lineno'> 354</span>     image/object/bbox/xmax: 0.9
<span class='lineno'> 355</span>     image/object/bbox/ymin: 0.2
<span class='lineno'> 356</span>     image/object/bbox/ymax: 0.6
<span class='lineno'> 357</span>     image/object/bbox/label: 615
<span class='lineno'> 358</span>     image/format: &#39;JPEG&#39;
<span class='lineno'> 359</span>     image/filename: &#39;ILSVRC2012_val_00041207.JPEG&#39;
<span class='lineno'> 360</span>     image/encoded: &lt;JPEG encoded string&gt;
<span class='lineno'> 361</span> 
<span class='lineno'> 362</span>   Args:
<span class='lineno'> 363</span>     example_serialized: scalar Tensor tf.string containing a serialized
<span class='lineno'> 364</span>       Example protocol buffer.
<span class='lineno'> 365</span> 
<span class='lineno'> 366</span>   Returns:
<span class='lineno'> 367</span>     image_buffer: Tensor tf.string containing the contents of a JPEG file.
<span class='lineno'> 368</span>     label: Tensor tf.int32 containing the label.
<span class='lineno'> 369</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 370</span>       where each coordinate is [0, 1) and the coordinates are arranged as
<span class='lineno'> 371</span>       [ymin, xmin, ymax, xmax].
<span class='lineno'> 372</span>     text: Tensor tf.string containing the human-readable label.
<span class='lineno'> 373</span>   &quot;&quot;&quot;
<span class='lineno'> 374</span>   # Dense features in Example proto.
<span class='lineno'> 375</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', title='dict'>feature_map</a> = {
<span class='lineno'> 376</span>       &#39;image/encoded&#39;: tf.FixedLenFeature([], dtype=tf.string,
<span class='lineno'> 377</span>                                           default_value=&#39;&#39;),
<span class='lineno'> 378</span>       &#39;image/class/label&#39;: tf.FixedLenFeature([1], dtype=tf.int64,
<span class='lineno'> 379</span>                                               default_value=-1),
<span class='lineno'> 380</span>       &#39;image/class/text&#39;: tf.FixedLenFeature([], dtype=tf.string,
<span class='lineno'> 381</span>                                              default_value=&#39;&#39;),
<span class='lineno'> 382</span>   }
<span class='lineno'> 383</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.sparse_float32', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.sparse_float32', title='?'>sparse_float32</a> = tf.VarLenFeature(dtype=tf.float32)
<span class='lineno'> 384</span>   # Sparse features in Example proto.
<span class='lineno'> 385</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', title='dict'>feature_map</a>.update(
<span class='lineno'> 386</span>       {<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', title='str'>k</a>: <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.sparse_float32', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.sparse_float32', title='?'>sparse_float32</a> for <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', title='str'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.k', title='str'>k</a></a> in [&#39;image/object/bbox/xmin&#39;,
<span class='lineno'> 387</span>                                    &#39;image/object/bbox/ymin&#39;,
<span class='lineno'> 388</span>                                    &#39;image/object/bbox/xmax&#39;,
<span class='lineno'> 389</span>                                    &#39;image/object/bbox/ymax&#39;]})
<span class='lineno'> 390</span> 
<span class='lineno'> 391</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a> = tf.parse_single_example(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.example_serialized', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.example_serialized', title='?'>example_serialized</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.feature_map', title='dict'>feature_map</a>)
<span class='lineno'> 392</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.label', title='?'>label</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/class/label&#39;], dtype=tf.int32)
<span class='lineno'> 393</span> 
<span class='lineno'> 394</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmin', title='?'>xmin</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/object/bbox/xmin&#39;].values, 0)
<span class='lineno'> 395</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymin', title='?'>ymin</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/object/bbox/ymin&#39;].values, 0)
<span class='lineno'> 396</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmax', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmax', title='?'>xmax</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/object/bbox/xmax&#39;].values, 0)
<span class='lineno'> 397</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymax', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymax', title='?'>ymax</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/object/bbox/ymax&#39;].values, 0)
<span class='lineno'> 398</span> 
<span class='lineno'> 399</span>   # Note that we impose an ordering of (y, x) just to make life difficult.
<span class='lineno'> 400</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a> = tf.concat(axis=0, values=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymin', title='?'>ymin</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmin', title='?'>xmin</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymax', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.ymax', title='?'>ymax</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmax', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.xmax', title='?'>xmax</a>])
<span class='lineno'> 401</span> 
<span class='lineno'> 402</span>   # Force the variable number of bounding boxes into the shape
<span class='lineno'> 403</span>   # [1, num_boxes, coords].
<span class='lineno'> 404</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a>, 0)
<span class='lineno'> 405</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a> = tf.transpose(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a>, [0, 2, 1])
<span class='lineno'> 406</span> 
<span class='lineno'> 407</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/encoded&#39;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.label', title='?'>label</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.bbox', title='?'>bbox</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto.features', title='?'>features</a>[&#39;image/class/text&#39;]
<span class='lineno'> 408</span> 
<span class='lineno'> 409</span> 
<span class='lineno'> 410</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs', title='(?, ?, ?, None, int) -> (?, ?) / (?, None, bool, None, int) -> (?, ?)'>batch_inputs</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', title='?'>dataset</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', title='bool'>train</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>=None,
<span class='lineno'> 411</span>                  <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='int'>num_readers</a>=1):
<span class='lineno'> 412</span>   &quot;&quot;&quot;Contruct batches of training or evaluation examples from the image dataset.
<span class='lineno'> 413</span> 
<span class='lineno'> 414</span>   Args:
<span class='lineno'> 415</span>     dataset: instance of Dataset class specifying the dataset.
<span class='lineno'> 416</span>       See dataset.py for details.
<span class='lineno'> 417</span>     batch_size: integer
<span class='lineno'> 418</span>     train: boolean
<span class='lineno'> 419</span>     num_preprocess_threads: integer, total number of preprocessing threads
<span class='lineno'> 420</span>     num_readers: integer, number of parallel readers
<span class='lineno'> 421</span> 
<span class='lineno'> 422</span>   Returns:
<span class='lineno'> 423</span>     images: 4-D float Tensor of a batch of images
<span class='lineno'> 424</span>     labels: 1-D integer Tensor of [batch_size].
<span class='lineno'> 425</span> 
<span class='lineno'> 426</span>   Raises:
<span class='lineno'> 427</span>     ValueError: if data is not found
<span class='lineno'> 428</span>   &quot;&quot;&quot;
<span class='lineno'> 429</span>   with tf.name_scope(&#39;batch_processing&#39;):
<span class='lineno'> 430</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', title='?'>data_files</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', title='?'>dataset</a>.data_files()
<span class='lineno'> 431</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', title='?'>data_files</a> is None:
<span class='lineno'> 432</span>       raise ValueError(&#39;No data files found for this dataset&#39;)
<span class='lineno'> 433</span> 
<span class='lineno'> 434</span>     # Create filename_queue
<span class='lineno'> 435</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', title='bool'>train</a>:
<span class='lineno'> 436</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', title='?'>filename_queue</a> = tf.train.string_input_producer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', title='?'>data_files</a>,
<span class='lineno'> 437</span>                                                       shuffle=True,
<span class='lineno'> 438</span>                                                       capacity=16)
<span class='lineno'> 439</span>     else:
<span class='lineno'> 440</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', title='?'>filename_queue</a> = tf.train.string_input_producer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.data_files', title='?'>data_files</a>,
<span class='lineno'> 441</span>                                                       shuffle=False,
<span class='lineno'> 442</span>                                                       capacity=1)
<span class='lineno'> 443</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a> is None:
<span class='lineno'> 444</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='?'>num_preprocess_threads</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.num_preprocess_threads
<span class='lineno'> 445</span> 
<span class='lineno'> 446</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a> % 4:
<span class='lineno'> 447</span>       raise ValueError(&#39;Please make num_preprocess_threads a multiple &#39;
<span class='lineno'> 448</span>                        &#39;of 4 (%d % 4 != 0).&#39;, num_preprocess_threads)
<span class='lineno'> 449</span> 
<span class='lineno'> 450</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='int'>num_readers</a> is None:
<span class='lineno'> 451</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='?'>num_readers</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.num_readers
<span class='lineno'> 452</span> 
<span class='lineno'> 453</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='int'>num_readers</a> &lt; 1:
<span class='lineno'> 454</span>       raise ValueError(&#39;Please make num_readers at least 1&#39;)
<span class='lineno'> 455</span> 
<span class='lineno'> 456</span>     # Approximate number of examples per shard.
<span class='lineno'> 457</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', title='int'>examples_per_shard</a> = 1024
<span class='lineno'> 458</span>     # Size the random shuffle queue to balance between good global
<span class='lineno'> 459</span>     # mixing (more examples) and memory use (fewer examples).
<span class='lineno'> 460</span>     # 1 image uses 299*299*3*4 bytes = 1MB
<span class='lineno'> 461</span>     # The default input_queue_memory_factor is 16 implying a shuffling queue
<span class='lineno'> 462</span>     # size: examples_per_shard * 16 * 1MB = 17.6GB
<span class='lineno'> 463</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', title='int'>min_queue_examples</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', title='int'>examples_per_shard</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.input_queue_memory_factor
<span class='lineno'> 464</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', title='bool'>train</a>:
<span class='lineno'> 465</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', title='?'>examples_queue</a> = tf.RandomShuffleQueue(
<span class='lineno'> 466</span>           capacity=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', title='int'>min_queue_examples</a> + 3 * <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>,
<span class='lineno'> 467</span>           min_after_dequeue=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.min_queue_examples', title='int'>min_queue_examples</a>,
<span class='lineno'> 468</span>           dtypes=[tf.string])
<span class='lineno'> 469</span>     else:
<span class='lineno'> 470</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', title='?'>examples_queue</a> = tf.FIFOQueue(
<span class='lineno'> 471</span>           capacity=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_per_shard', title='int'>examples_per_shard</a> + 3 * <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>,
<span class='lineno'> 472</span>           dtypes=[tf.string])
<span class='lineno'> 473</span> 
<span class='lineno'> 474</span>     # Create multiple readers to populate the queue of examples.
<span class='lineno'> 475</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='int'>num_readers</a> &gt; 1:
<span class='lineno'> 476</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', title='[?]'>enqueue_ops</a> = []
<span class='lineno'> 477</span>       for <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', title='int'>_</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_readers', title='int'>num_readers</a>):
<span class='lineno'> 478</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', title='?'>reader</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', title='?'>dataset</a>.reader()
<span class='lineno'> 479</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', title='?'>_</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.value', title='?'>value</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', title='?'>reader</a>.read(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', title='?'>filename_queue</a>)
<span class='lineno'> 480</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', title='[?]'>enqueue_ops</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', title='?'>examples_queue</a>.enqueue([<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.value', title='?'>value</a>]))
<span class='lineno'> 481</span> 
<span class='lineno'> 482</span>       tf.train.queue_runner.add_queue_runner(
<span class='lineno'> 483</span>           tf.train.queue_runner.QueueRunner(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', title='?'>examples_queue</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.enqueue_ops', title='[?]'>enqueue_ops</a>))
<span class='lineno'> 484</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', title='?'>example_serialized</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.examples_queue', title='?'>examples_queue</a>.dequeue()
<span class='lineno'> 485</span>     else:
<span class='lineno'> 486</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', title='?'>reader</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.dataset', title='?'>dataset</a>.reader()
<span class='lineno'> 487</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', title='?'>_</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', title='?'>example_serialized</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.reader', title='?'>reader</a>.read(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.filename_queue', title='?'>filename_queue</a>)
<span class='lineno'> 488</span> 
<span class='lineno'> 489</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', title='[[None]]'>images_and_labels</a> = []
<span class='lineno'> 490</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.thread_id', title='int'>thread_id</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a>):
<span class='lineno'> 491</span>       # Parse a serialized Example proto to extract the image and metadata.
<span class='lineno'> 492</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image_buffer', title='?'>image_buffer</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index', title='?'>label_index</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.bbox', title='?'>bbox</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs._', title='?'>_</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.parse_example_proto', title='? -> (?, ?, ?, ?)'>parse_example_proto</a>(
<span class='lineno'> 493</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.example_serialized', title='?'>example_serialized</a>)
<span class='lineno'> 494</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image', title='None'>image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.image_preprocessing', title='(?, ?, bool, int) -> None / (?, ?, ?, int) -> None'>image_preprocessing</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image_buffer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image_buffer', title='?'>image_buffer</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.bbox', title='?'>bbox</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.train', title='bool'>train</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.thread_id', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.thread_id', title='int'>thread_id</a>)
<span class='lineno'> 495</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', title='[[None]]'>images_and_labels</a>.append([<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.image', title='None'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index', title='?'>label_index</a>])
<span class='lineno'> 496</span> 
<span class='lineno'> 497</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index_batch', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index_batch', title='?'>label_index_batch</a> = tf.train.batch_join(
<span class='lineno'> 498</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images_and_labels', title='[[None]]'>images_and_labels</a>,
<span class='lineno'> 499</span>         batch_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>,
<span class='lineno'> 500</span>         capacity=2 * <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.num_preprocess_threads', title='None'>num_preprocess_threads</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>)
<span class='lineno'> 501</span> 
<span class='lineno'> 502</span>     # Reshape images into these desired dimensions.
<span class='lineno'> 503</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.height', title='?'>height</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.image_size
<span class='lineno'> 504</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.width', title='?'>width</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.FLAGS', title='?'>FLAGS</a>.image_size
<span class='lineno'> 505</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.depth', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.depth', title='int'>depth</a> = 3
<span class='lineno'> 506</span> 
<span class='lineno'> 507</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a>, tf.float32)
<span class='lineno'> 508</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a> = tf.reshape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a>, shape=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.width', title='?'>width</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.depth', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.depth', title='int'>depth</a>])
<span class='lineno'> 509</span> 
<span class='lineno'> 510</span>     # Display the training images in the visualizer.
<span class='lineno'> 511</span>     tf.summary.image(&#39;images&#39;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a>)
<span class='lineno'> 512</span> 
<span class='lineno'> 513</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.images', title='?'>images</a>, tf.reshape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index_batch', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.label_index_batch', title='?'>label_index_batch</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.inception.inception.image_processing.batch_inputs.batch_size', title='None'>batch_size</a>])
</pre></td></tr></table></body></html>