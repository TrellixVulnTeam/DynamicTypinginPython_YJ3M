<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/research/attention_ocr/python/sequence_layers.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer'>orthogonal_initializer</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerParams', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerParams'>SequenceLayerParams</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase'>SequenceLayerBase</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__metaclass__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__metaclass__'>__metaclass__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input'>get_train_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input'>get_eval_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell'>unroll_cell</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training'>is_training</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit'>char_logit</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot'>char_one_hot</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input'>get_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits'>create_logits</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice'>NetSlice</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature'>get_image_feature</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input'>get_eval_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input'>get_train_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell'>unroll_cell</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression'>NetSliceWithAutoregression</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input'>get_eval_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input'>get_train_input</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention'>Attention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input'>get_eval_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input'>get_train_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell'>unroll_cell</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression'>AttentionWithAutoregression</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input'>get_train_input</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input'>get_eval_input</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class'>get_layer_class</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2017 The TensorFlow Authors All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> 
<span class='lineno'>  16</span> &quot;&quot;&quot;Various implementations of sequence layers for character prediction.
<span class='lineno'>  17</span> 
<span class='lineno'>  18</span> A &#39;sequence layer&#39; is a part of a computation graph which is responsible of
<span class='lineno'>  19</span> producing a sequence of characters using extracted image features. There are
<span class='lineno'>  20</span> many reasonable ways to implement such layers. All of them are using RNNs.
<span class='lineno'>  21</span> This module provides implementations which uses &#39;attention&#39; mechanism to
<span class='lineno'>  22</span> spatially &#39;pool&#39; image features and also can use a previously predicted
<span class='lineno'>  23</span> character to predict the next (aka auto regression).
<span class='lineno'>  24</span> 
<span class='lineno'>  25</span> Usage:
<span class='lineno'>  26</span>   Select one of available classes, e.g. Attention or use a wrapper function to
<span class='lineno'>  27</span>   pick one based on your requirements:
<span class='lineno'>  28</span>   layer_class = sequence_layers.get_layer_class(use_attention=True,
<span class='lineno'>  29</span>                                                 use_autoregression=True)
<span class='lineno'>  30</span>   layer = layer_class(net, labels_one_hot, model_params, method_params)
<span class='lineno'>  31</span>   char_logits = layer.create_logits()
<span class='lineno'>  32</span> &quot;&quot;&quot;
<span class='lineno'>  33</span> 
<span class='lineno'>  34</span> from __future__ import absolute_import
<span class='lineno'>  35</span> from __future__ import division
<span class='lineno'>  36</span> from __future__ import print_function
<span class='lineno'>  37</span> 
<span class='lineno'>  38</span> import collections
<span class='lineno'>  39</span> import abc
<span class='lineno'>  40</span> import logging
<span class='lineno'>  41</span> import numpy as np
<span class='lineno'>  42</span> 
<span class='lineno'>  43</span> import tensorflow as tf
<span class='lineno'>  44</span> 
<span class='lineno'>  45</span> from tensorflow.contrib import slim
<span class='lineno'>  46</span> 
<span class='lineno'>  47</span> 
<span class='lineno'>  48</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', title='(?, ?) -> None'>orthogonal_initializer</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', title='?'>shape</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.dtype', title='?'>dtype</a>=tf.float32, *args, **kwargs):
<span class='lineno'>  49</span>   &quot;&quot;&quot;Generates orthonormal matrices with random values.
<span class='lineno'>  50</span> 
<span class='lineno'>  51</span>   Orthonormal initialization is important for RNNs:
<span class='lineno'>  52</span>     http://arxiv.org/abs/1312.6120
<span class='lineno'>  53</span>     http://smerity.com/articles/2016/orthogonal_init.html
<span class='lineno'>  54</span> 
<span class='lineno'>  55</span>   For non-square shapes the returned matrix will be semi-orthonormal: if the
<span class='lineno'>  56</span>   number of columns exceeds the number of rows, then the rows are orthonormal
<span class='lineno'>  57</span>   vectors; but if the number of rows exceeds the number of columns, then the
<span class='lineno'>  58</span>   columns are orthonormal vectors.
<span class='lineno'>  59</span> 
<span class='lineno'>  60</span>   We use SVD decomposition to generate an orthonormal matrix with random
<span class='lineno'>  61</span>   values. The same way as it is done in the Lasagne library for Theano. Note
<span class='lineno'>  62</span>   that both u and v returned by the svd are orthogonal and random. We just need
<span class='lineno'>  63</span>   to pick one with the right shape.
<span class='lineno'>  64</span> 
<span class='lineno'>  65</span>   Args:
<span class='lineno'>  66</span>     shape: a shape of the tensor matrix to initialize.
<span class='lineno'>  67</span>     dtype: a dtype of the initialized tensor.
<span class='lineno'>  68</span>     *args: not used.
<span class='lineno'>  69</span>     **kwargs: not used.
<span class='lineno'>  70</span> 
<span class='lineno'>  71</span>   Returns:
<span class='lineno'>  72</span>     An initialized tensor.
<span class='lineno'>  73</span>   &quot;&quot;&quot;
<span class='lineno'>  74</span>   del args
<span class='lineno'>  75</span>   del kwargs
<span class='lineno'>  76</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', title='(?, ?)'>flat_shape</a> = (<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', title='?'>shape</a>[0], np.prod(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', title='?'>shape</a>[1:]))
<span class='lineno'>  77</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', title='?'>w</a> = np.random.randn(*<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', title='(?, ?)'>flat_shape</a>)
<span class='lineno'>  78</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', title='?'>u</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer._', title='?'>_</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.v', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.v', title='?'>v</a> = np.linalg.svd(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', title='?'>w</a>, full_matrices=False)
<span class='lineno'>  79</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', title='?'>w</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', title='?'>u</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.u', title='?'>u</a>.shape == <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.flat_shape', title='(?, ?)'>flat_shape</a> else <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.v', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.v', title='?'>v</a>
<span class='lineno'>  80</span>   return tf.constant(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.w', title='?'>w</a>.reshape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.shape', title='?'>shape</a>), dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer.dtype', title='?'>dtype</a>)
<span class='lineno'>  81</span> 
<span class='lineno'>  82</span> 
<span class='lineno'>  83</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerParams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerParams', title='<(namedtuple)>'>SequenceLayerParams</a> = collections.namedtuple(&#39;SequenceLogitsParams&#39;, [
<span class='lineno'>  84</span>     &#39;num_lstm_units&#39;, &#39;weight_decay&#39;, &#39;lstm_state_clip_value&#39;
<span class='lineno'>  85</span> ])
<span class='lineno'>  86</span> 
<span class='lineno'>  87</span> 
<span class='lineno'>  88</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', title='<SequenceLayerBase>'>SequenceLayerBase</a>(object):
<span class='lineno'>  89</span>   &quot;&quot;&quot;A base abstruct class for all sequence layers.
<span class='lineno'>  90</span> 
<span class='lineno'>  91</span>   A child class has to define following methods:
<span class='lineno'>  92</span>     get_train_input
<span class='lineno'>  93</span>     get_eval_input
<span class='lineno'>  94</span>     unroll_cell
<span class='lineno'>  95</span>   &quot;&quot;&quot;
<span class='lineno'>  96</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__metaclass__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__metaclass__', title='?'>__metaclass__</a> = abc.ABCMeta
<span class='lineno'>  97</span> 
<span class='lineno'>  98</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', title='?'>net</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.labels_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.labels_one_hot', title='?'>labels_one_hot</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.model_params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.model_params', title='?'>model_params</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.method_params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.method_params', title='?'>method_params</a>):
<span class='lineno'>  99</span>     &quot;&quot;&quot;Stores argument in member variable for further use.
<span class='lineno'> 100</span> 
<span class='lineno'> 101</span>     Args:
<span class='lineno'> 102</span>       net: A tensor with shape [batch_size, num_features, feature_size] which
<span class='lineno'> 103</span>         contains some extracted image features.
<span class='lineno'> 104</span>       labels_one_hot: An optional (can be None) ground truth labels for the
<span class='lineno'> 105</span>         input features. Is a tensor with shape
<span class='lineno'> 106</span>         [batch_size, seq_length, num_char_classes]
<span class='lineno'> 107</span>       model_params: A namedtuple with model parameters (model.ModelParams).
<span class='lineno'> 108</span>       method_params: A SequenceLayerParams instance.
<span class='lineno'> 109</span>     &quot;&quot;&quot;
<span class='lineno'> 110</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'>_params</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.model_params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.model_params', title='?'>model_params</a>
<span class='lineno'> 111</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'>_mparams</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.method_params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.method_params', title='?'>method_params</a>
<span class='lineno'> 112</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._net', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._net', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._net', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._net', title='?'>_net</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', title='?'>net</a>
<span class='lineno'> 113</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', title='?'>_labels_one_hot</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.labels_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.labels_one_hot', title='?'>labels_one_hot</a>
<span class='lineno'> 114</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', title='?'>_batch_size</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.net', title='?'>net</a>.get_shape().dims[0].value
<span class='lineno'> 115</span> 
<span class='lineno'> 116</span>     # Initialize parameters for char logits which will be computed on the fly
<span class='lineno'> 117</span>     # inside an LSTM decoder.
<span class='lineno'> 118</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', title='dict'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', title='dict'>_char_logits</a></a> = {}
<span class='lineno'> 119</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', title='?'>regularizer</a> = slim.l2_regularizer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'>_mparams</a>.weight_decay)
<span class='lineno'> 120</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', title='?'>_softmax_w</a></a> = slim.model_variable(
<span class='lineno'> 121</span>         &#39;softmax_w&#39;,
<span class='lineno'> 122</span>         [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'>_mparams</a>.num_lstm_units, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'>_params</a>.num_char_classes],
<span class='lineno'> 123</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', title='(?, ?) -> None'>orthogonal_initializer</a>,
<span class='lineno'> 124</span>         regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', title='?'>regularizer</a>)
<span class='lineno'> 125</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', title='?'>_softmax_b</a></a> = slim.model_variable(
<span class='lineno'> 126</span>         &#39;softmax_b&#39;, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.self', title='SequenceLayerBase'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'>_params</a>.num_char_classes],
<span class='lineno'> 127</span>         initializer=tf.zeros_initializer(),
<span class='lineno'> 128</span>         regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.__init__.regularizer', title='?'>regularizer</a>)
<span class='lineno'> 129</span> 
<span class='lineno'> 130</span>   @abc.abstractmethod
<span class='lineno'> 131</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input', title='(SequenceLayerBase, None, int) -> None / (SequenceLayerBase, ?, ?) -> None'>get_train_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.self', title='SequenceLayerBase'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input.i', title='int'>i</a>):
<span class='lineno'> 132</span>     &quot;&quot;&quot;Returns a sample to be used to predict a character during training.
<span class='lineno'> 133</span> 
<span class='lineno'> 134</span>     This function is used as a loop_function for an RNN decoder.
<span class='lineno'> 135</span> 
<span class='lineno'> 136</span>     Args:
<span class='lineno'> 137</span>       prev: output tensor from previous step of the RNN. A tensor with shape:
<span class='lineno'> 138</span>         [batch_size, num_char_classes].
<span class='lineno'> 139</span>       i: index of a character in the output sequence.
<span class='lineno'> 140</span> 
<span class='lineno'> 141</span>     Returns:
<span class='lineno'> 142</span>       A tensor with shape [batch_size, ?] - depth depends on implementation
<span class='lineno'> 143</span>       details.
<span class='lineno'> 144</span>     &quot;&quot;&quot;
<span class='lineno'> 145</span>     pass
<span class='lineno'> 146</span> 
<span class='lineno'> 147</span>   @abc.abstractmethod
<span class='lineno'> 148</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input', title='(SequenceLayerBase, None, int) -> None / (SequenceLayerBase, ?, ?) -> None'>get_eval_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.self', title='SequenceLayerBase'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input.i', title='int'>i</a>):
<span class='lineno'> 149</span>     &quot;&quot;&quot;Returns a sample to be used to predict a character during inference.
<span class='lineno'> 150</span> 
<span class='lineno'> 151</span>     This function is used as a loop_function for an RNN decoder.
<span class='lineno'> 152</span> 
<span class='lineno'> 153</span>     Args:
<span class='lineno'> 154</span>       prev: output tensor from previous step of the RNN. A tensor with shape:
<span class='lineno'> 155</span>         [batch_size, num_char_classes].
<span class='lineno'> 156</span>       i: index of a character in the output sequence.
<span class='lineno'> 157</span> 
<span class='lineno'> 158</span>     Returns:
<span class='lineno'> 159</span>       A tensor with shape [batch_size, ?] - depth depends on implementation
<span class='lineno'> 160</span>       details.
<span class='lineno'> 161</span>     &quot;&quot;&quot;
<span class='lineno'> 162</span>     raise AssertionError(&#39;Not implemented&#39;)
<span class='lineno'> 163</span> 
<span class='lineno'> 164</span>   @abc.abstractmethod
<span class='lineno'> 165</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell', title='(SequenceLayerBase, ?, ?, ?, ?) -> None / (SequenceLayerBase, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None'>unroll_cell</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.self', title='SequenceLayerBase'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.decoder_inputs', title='[None]'>decoder_inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.initial_state', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.initial_state', title='?'>initial_state</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.loop_function', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.loop_function', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>loop_function</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.unroll_cell.cell', title='?'>cell</a>):
<span class='lineno'> 166</span>     &quot;&quot;&quot;Unrolls an RNN cell for all inputs.
<span class='lineno'> 167</span> 
<span class='lineno'> 168</span>     This is a placeholder to call some RNN decoder. It has a similar to
<span class='lineno'> 169</span>     tf.seq2seq.rnn_decode interface.
<span class='lineno'> 170</span> 
<span class='lineno'> 171</span>     Args:
<span class='lineno'> 172</span>       decoder_inputs: A list of 2D Tensors* [batch_size x input_size]. In fact,
<span class='lineno'> 173</span>         most of existing decoders in presence of a loop_function use only the
<span class='lineno'> 174</span>         first element to determine batch_size and length of the list to
<span class='lineno'> 175</span>         determine number of steps.
<span class='lineno'> 176</span>       initial_state: 2D Tensor with shape [batch_size x cell.state_size].
<span class='lineno'> 177</span>       loop_function: function will be applied to the i-th output in order to
<span class='lineno'> 178</span>         generate the i+1-st input (see self.get_input).
<span class='lineno'> 179</span>       cell: rnn_cell.RNNCell defining the cell function and size.
<span class='lineno'> 180</span> 
<span class='lineno'> 181</span>     Returns:
<span class='lineno'> 182</span>       A tuple of the form (outputs, state), where:
<span class='lineno'> 183</span>         outputs: A list of character logits of the same length as
<span class='lineno'> 184</span>         decoder_inputs of 2D Tensors with shape [batch_size x num_characters].
<span class='lineno'> 185</span>         state: The state of each cell at the final time-step.
<span class='lineno'> 186</span>           It is a 2D Tensor of shape [batch_size x cell.state_size].
<span class='lineno'> 187</span>     &quot;&quot;&quot;
<span class='lineno'> 188</span>     pass
<span class='lineno'> 189</span> 
<span class='lineno'> 190</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training', title='AttentionWithAutoregression -> bool / NetSlice -> bool / NetSliceWithAutoregression -> bool / SequenceLayerBase -> bool / Attention -> bool'>is_training</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>):
<span class='lineno'> 191</span>     &quot;&quot;&quot;Returns True if the layer is created for training stage.&quot;&quot;&quot;
<span class='lineno'> 192</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._labels_one_hot', title='?'>_labels_one_hot</a> is not None
<span class='lineno'> 193</span> 
<span class='lineno'> 194</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', title='(AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None / (NetSlice, ?, ?) -> None / (Attention, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None / (SequenceLayerBase, ?, ?) -> None / (AttentionWithAutoregression, ?, int) -> None / (NetSliceWithAutoregression, ?, int) -> None'>char_logit</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.inputs', title='None'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', title='int'>char_index</a>):
<span class='lineno'> 195</span>     &quot;&quot;&quot;Creates logits for a character if required.
<span class='lineno'> 196</span> 
<span class='lineno'> 197</span>     Args:
<span class='lineno'> 198</span>       inputs: A tensor with shape [batch_size, ?] (depth is implementation
<span class='lineno'> 199</span>         dependent).
<span class='lineno'> 200</span>       char_index: A integer index of a character in the output sequence.
<span class='lineno'> 201</span> 
<span class='lineno'> 202</span>     Returns:
<span class='lineno'> 203</span>       A tensor with shape [batch_size, num_char_classes]
<span class='lineno'> 204</span>     &quot;&quot;&quot;
<span class='lineno'> 205</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', title='int'>char_index</a> not in <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', title='dict'>_char_logits</a>:
<span class='lineno'> 206</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', title='dict'>_char_logits</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', title='int'>char_index</a>] = tf.nn.xw_plus_b(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.inputs', title='None'>inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_w', title='?'>_softmax_w</a>,
<span class='lineno'> 207</span>                                                       <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._softmax_b', title='?'>_softmax_b</a>)
<span class='lineno'> 208</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._char_logits', title='dict'>_char_logits</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit.char_index', title='int'>char_index</a>]
<span class='lineno'> 209</span> 
<span class='lineno'> 210</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', title='(SequenceLayerBase, ?) -> None / (NetSliceWithAutoregression, None) -> None / (AttentionWithAutoregression, None) -> None'>char_one_hot</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.self', title='{AttentionWithAutoregression | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.logit', title='None'>logit</a>):
<span class='lineno'> 211</span>     &quot;&quot;&quot;Creates one hot encoding for a logit of a character.
<span class='lineno'> 212</span> 
<span class='lineno'> 213</span>     Args:
<span class='lineno'> 214</span>       logit: A tensor with shape [batch_size, num_char_classes].
<span class='lineno'> 215</span> 
<span class='lineno'> 216</span>     Returns:
<span class='lineno'> 217</span>       A tensor with shape [batch_size, num_char_classes]
<span class='lineno'> 218</span>     &quot;&quot;&quot;
<span class='lineno'> 219</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.prediction', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.prediction', title='?'>prediction</a> = tf.argmax(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.logit', title='None'>logit</a>, axis=1)
<span class='lineno'> 220</span>     return slim.one_hot_encoding(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.prediction', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.prediction', title='?'>prediction</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot.self', title='{AttentionWithAutoregression | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'>_params</a>.num_char_classes)
<span class='lineno'> 221</span> 
<span class='lineno'> 222</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>get_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', title='int'>i</a>):
<span class='lineno'> 223</span>     &quot;&quot;&quot;A wrapper for get_train_input and get_eval_input.
<span class='lineno'> 224</span> 
<span class='lineno'> 225</span>     Args:
<span class='lineno'> 226</span>       prev: output tensor from previous step of the RNN. A tensor with shape:
<span class='lineno'> 227</span>         [batch_size, num_char_classes].
<span class='lineno'> 228</span>       i: index of a character in the output sequence.
<span class='lineno'> 229</span> 
<span class='lineno'> 230</span>     Returns:
<span class='lineno'> 231</span>       A tensor with shape [batch_size, ?] - depth depends on implementation
<span class='lineno'> 232</span>       details.
<span class='lineno'> 233</span>     &quot;&quot;&quot;
<span class='lineno'> 234</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.is_training', title='AttentionWithAutoregression -> bool / NetSlice -> bool / NetSliceWithAutoregression -> bool / SequenceLayerBase -> bool / Attention -> bool'>is_training</a>():
<span class='lineno'> 235</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_train_input', title='{(Attention, None, int) -> None / (Attention, ?, ?) -> None | (AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None | (NetSlice, None, int) -> None / (NetSlice, ?, ?) -> None | (NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None | (SequenceLayerBase, None, int) -> None / (SequenceLayerBase, ?, ?) -> None}'>get_train_input</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', title='None'>prev</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', title='int'>i</a>)
<span class='lineno'> 236</span>     else:
<span class='lineno'> 237</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_eval_input', title='{(Attention, None, int) -> None / (Attention, ?, ?) -> None | (AttentionWithAutoregression, None, int) -> None / (AttentionWithAutoregression, ?, ?) -> None | (NetSlice, None, int) -> None / (NetSlice, ?, ?) -> None | (NetSliceWithAutoregression, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None | (SequenceLayerBase, None, int) -> None / (SequenceLayerBase, ?, ?) -> None}'>get_eval_input</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.prev', title='None'>prev</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input.i', title='int'>i</a>)
<span class='lineno'> 238</span> 
<span class='lineno'> 239</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits', title='AttentionWithAutoregression -> None / NetSlice -> None / Attention -> None / NetSliceWithAutoregression -> None / SequenceLayerBase -> None'>create_logits</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>):
<span class='lineno'> 240</span>     &quot;&quot;&quot;Creates character sequence logits for a net specified in the constructor.
<span class='lineno'> 241</span> 
<span class='lineno'> 242</span>     A &quot;main&quot; method for the sequence layer which glues together all pieces.
<span class='lineno'> 243</span> 
<span class='lineno'> 244</span>     Returns:
<span class='lineno'> 245</span>       A tensor with shape [batch_size, seq_length, num_char_classes].
<span class='lineno'> 246</span>     &quot;&quot;&quot;
<span class='lineno'> 247</span>     with tf.variable_scope(&#39;LSTM&#39;):
<span class='lineno'> 248</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.first_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.first_label', title='None'>first_label</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>get_input</a>(prev=None, i=0)
<span class='lineno'> 249</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.decoder_inputs', title='[None]'>decoder_inputs</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.first_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.first_label', title='None'>first_label</a>] + [None] * (<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._params', title='?'>_params</a>.seq_length - 1)
<span class='lineno'> 250</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', title='?'>lstm_cell</a> = tf.contrib.rnn.LSTMCell(
<span class='lineno'> 251</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'>_mparams</a>.num_lstm_units,
<span class='lineno'> 252</span>           use_peepholes=False,
<span class='lineno'> 253</span>           cell_clip=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._mparams', title='?'>_mparams</a>.lstm_state_clip_value,
<span class='lineno'> 254</span>           state_is_tuple=True,
<span class='lineno'> 255</span>           initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.orthogonal_initializer', title='(?, ?) -> None'>orthogonal_initializer</a>)
<span class='lineno'> 256</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_outputs', title='?'>lstm_outputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits._', title='?'>_</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell', title='{(AttentionWithAutoregression, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> ? / (Attention, ?, ?, ?, ?) -> ? / (Attention, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> ? | (NetSlice, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None / (NetSlice, ?, ?, ?, ?) -> None / (NetSliceWithAutoregression, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None | (SequenceLayerBase, ?, ?, ?, ?) -> None / (SequenceLayerBase, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None}'>unroll_cell</a>(
<span class='lineno'> 257</span>           decoder_inputs=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.decoder_inputs', title='[None]'>decoder_inputs</a>,
<span class='lineno'> 258</span>           initial_state=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', title='?'>lstm_cell</a>.zero_state(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase._batch_size', title='?'>_batch_size</a>, tf.float32),
<span class='lineno'> 259</span>           loop_function=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>get_input</a>,
<span class='lineno'> 260</span>           cell=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_cell', title='?'>lstm_cell</a>)
<span class='lineno'> 261</span> 
<span class='lineno'> 262</span>     with tf.variable_scope(&#39;logits&#39;):
<span class='lineno'> 263</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logits_list', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logits_list', title='[?]'>logits_list</a> = [
<span class='lineno'> 264</span>           tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.self', title='{Attention | AttentionWithAutoregression | NetSlice | NetSliceWithAutoregression | SequenceLayerBase}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', title='(AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None / (NetSlice, ?, ?) -> None / (Attention, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None / (SequenceLayerBase, ?, ?) -> None / (AttentionWithAutoregression, ?, int) -> None / (NetSliceWithAutoregression, ?, int) -> None'>char_logit</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', title='?'>logit</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', title='?'>i</a>), dim=1)
<span class='lineno'> 265</span>           for <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.i', title='?'>i</a></a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logit', title='?'>logit</a></a> in enumerate(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.lstm_outputs', title='?'>lstm_outputs</a>)
<span class='lineno'> 266</span>       ]
<span class='lineno'> 267</span> 
<span class='lineno'> 268</span>     return tf.concat(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logits_list', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.create_logits.logits_list', title='[?]'>logits_list</a>, 1)
<span class='lineno'> 269</span> 
<span class='lineno'> 270</span> 
<span class='lineno'> 271</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', title='<NetSlice>'>NetSlice</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', title='<SequenceLayerBase>'>SequenceLayerBase</a>):
<span class='lineno'> 272</span>   &quot;&quot;&quot;A layer which uses a subset of image features to predict each character.
<span class='lineno'> 273</span>   &quot;&quot;&quot;
<span class='lineno'> 274</span> 
<span class='lineno'> 275</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', title='NetSlice'>self</a>, *args, **kwargs):
<span class='lineno'> 276</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', title='<NetSlice>'>NetSlice</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', title='NetSlice'>self</a>).__init__(*args, **kwargs)
<span class='lineno'> 277</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', title='NetSlice'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice._zero_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice._zero_label', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice._zero_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice._zero_label', title='?'>_zero_label</a></a> = tf.zeros(
<span class='lineno'> 278</span>         [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', title='NetSlice'>self</a>._batch_size, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.__init__.self', title='NetSlice'>self</a>._params.num_char_classes])
<span class='lineno'> 279</span> 
<span class='lineno'> 280</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', title='(NetSlice, int) -> None / (NetSliceWithAutoregression, ?) -> None / (NetSlice, ?) -> None / (NetSliceWithAutoregression, int) -> None'>get_image_feature</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', title='int'>char_index</a>):
<span class='lineno'> 281</span>     &quot;&quot;&quot;Returns a subset of image features for a character.
<span class='lineno'> 282</span> 
<span class='lineno'> 283</span>     Args:
<span class='lineno'> 284</span>       char_index: an index of a character.
<span class='lineno'> 285</span> 
<span class='lineno'> 286</span>     Returns:
<span class='lineno'> 287</span>       A tensor with shape [batch_size, ?]. The output depth depends on the
<span class='lineno'> 288</span>       depth of input net.
<span class='lineno'> 289</span>     &quot;&quot;&quot;
<span class='lineno'> 290</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.batch_size', title='?'>batch_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.features_num', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.features_num', title='?'>features_num</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature._', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature._', title='?'>_</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', title='?'>d</a>.value for <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.d', title='?'>d</a></a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>._net.get_shape()]
<span class='lineno'> 291</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.slice_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.slice_len', title='int'>slice_len</a> = int(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.features_num', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.features_num', title='?'>features_num</a> / <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>._params.seq_length)
<span class='lineno'> 292</span>     # In case when features_num != seq_length, we just pick a subset of image
<span class='lineno'> 293</span>     # features, this choice is arbitrary and there is no intuitive geometrical
<span class='lineno'> 294</span>     # interpretation. If features_num is not dividable by seq_length there will
<span class='lineno'> 295</span>     # be unused image features.
<span class='lineno'> 296</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.net_slice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.net_slice', title='?'>net_slice</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>._net[:, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', title='int'>char_index</a>:<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.char_index', title='int'>char_index</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.slice_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.slice_len', title='int'>slice_len</a>, :]
<span class='lineno'> 297</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', title='?'>feature</a> = tf.reshape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.net_slice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.net_slice', title='?'>net_slice</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.batch_size', title='?'>batch_size</a>, -1])
<span class='lineno'> 298</span>     logging.debug(&#39;Image feature: %s&#39;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', title='?'>feature</a>)
<span class='lineno'> 299</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature.feature', title='?'>feature</a>
<span class='lineno'> 300</span> 
<span class='lineno'> 301</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input', title='(NetSlice, None, int) -> None / (NetSlice, ?, ?) -> None'>get_eval_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.self', title='NetSlice'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.i', title='int'>i</a>):
<span class='lineno'> 302</span>     &quot;&quot;&quot;See SequenceLayerBase.get_eval_input for details.&quot;&quot;&quot;
<span class='lineno'> 303</span>     del <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.prev', title='None'>prev</a>
<span class='lineno'> 304</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.self', title='NetSlice'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', title='(NetSlice, int) -> None / (NetSliceWithAutoregression, ?) -> None / (NetSlice, ?) -> None / (NetSliceWithAutoregression, int) -> None'>get_image_feature</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input.i', title='int'>i</a>)
<span class='lineno'> 305</span> 
<span class='lineno'> 306</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input', title='(NetSlice, None, int) -> None / (NetSlice, ?, ?) -> None'>get_train_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.self', title='NetSlice'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.i', title='int'>i</a>):
<span class='lineno'> 307</span>     &quot;&quot;&quot;See SequenceLayerBase.get_train_input for details.&quot;&quot;&quot;
<span class='lineno'> 308</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.self', title='NetSlice'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_eval_input', title='(NetSlice, None, int) -> None / (NetSlice, ?, ?) -> None'>get_eval_input</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.prev', title='None'>prev</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_train_input.i', title='int'>i</a>)
<span class='lineno'> 309</span> 
<span class='lineno'> 310</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell', title='(NetSlice, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None / (NetSlice, ?, ?, ?, ?) -> None / (NetSliceWithAutoregression, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> None'>unroll_cell</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.decoder_inputs', title='[None]'>decoder_inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.initial_state', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.initial_state', title='?'>initial_state</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.loop_function', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.loop_function', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>loop_function</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.cell', title='?'>cell</a>):
<span class='lineno'> 311</span>     &quot;&quot;&quot;See SequenceLayerBase.unroll_cell for details.&quot;&quot;&quot;
<span class='lineno'> 312</span>     return tf.contrib.legacy_seq2seq.rnn_decoder(
<span class='lineno'> 313</span>         decoder_inputs=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.decoder_inputs', title='[None]'>decoder_inputs</a>,
<span class='lineno'> 314</span>         initial_state=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.initial_state', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.initial_state', title='?'>initial_state</a>,
<span class='lineno'> 315</span>         cell=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.cell', title='?'>cell</a>,
<span class='lineno'> 316</span>         loop_function=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.unroll_cell.self', title='{NetSlice | NetSliceWithAutoregression}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>get_input</a>)
<span class='lineno'> 317</span> 
<span class='lineno'> 318</span> 
<span class='lineno'> 319</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', title='<NetSliceWithAutoregression>'>NetSliceWithAutoregression</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', title='<NetSlice>'>NetSlice</a>):
<span class='lineno'> 320</span>   &quot;&quot;&quot;A layer similar to NetSlice, but it also uses auto regression.
<span class='lineno'> 321</span> 
<span class='lineno'> 322</span>   The &quot;auto regression&quot; means that we use network output for previous character
<span class='lineno'> 323</span>   as a part of input for the current character.
<span class='lineno'> 324</span>   &quot;&quot;&quot;
<span class='lineno'> 325</span> 
<span class='lineno'> 326</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__.self', title='NetSliceWithAutoregression'>self</a>, *args, **kwargs):
<span class='lineno'> 327</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', title='<NetSliceWithAutoregression>'>NetSliceWithAutoregression</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.__init__.self', title='NetSliceWithAutoregression'>self</a>).__init__(*args, **kwargs)
<span class='lineno'> 328</span> 
<span class='lineno'> 329</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input', title='(NetSliceWithAutoregression, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None'>get_eval_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', title='NetSliceWithAutoregression'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', title='int'>i</a>):
<span class='lineno'> 330</span>     &quot;&quot;&quot;See SequenceLayerBase.get_eval_input for details.&quot;&quot;&quot;
<span class='lineno'> 331</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', title='int'>i</a> == 0:
<span class='lineno'> 332</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', title='?'>prev</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', title='NetSliceWithAutoregression'>self</a>._zero_label
<span class='lineno'> 333</span>     else:
<span class='lineno'> 334</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.logit', title='None'>logit</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', title='NetSliceWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', title='(AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None / (NetSlice, ?, ?) -> None / (Attention, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None / (SequenceLayerBase, ?, ?) -> None / (AttentionWithAutoregression, ?, int) -> None / (NetSliceWithAutoregression, ?, int) -> None'>char_logit</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', title='None'>prev</a>, char_index=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', title='int'>i</a> - 1)
<span class='lineno'> 335</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', title='None'>prev</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', title='NetSliceWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', title='(SequenceLayerBase, ?) -> None / (NetSliceWithAutoregression, None) -> None / (AttentionWithAutoregression, None) -> None'>char_one_hot</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.logit', title='None'>logit</a>)
<span class='lineno'> 336</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.image_feature', title='None'>image_feature</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.self', title='NetSliceWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', title='(NetSlice, int) -> None / (NetSliceWithAutoregression, ?) -> None / (NetSlice, ?) -> None / (NetSliceWithAutoregression, int) -> None'>get_image_feature</a>(char_index=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.i', title='int'>i</a>)
<span class='lineno'> 337</span>     return tf.concat([<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.image_feature', title='None'>image_feature</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_eval_input.prev', title='None'>prev</a>], 1)
<span class='lineno'> 338</span> 
<span class='lineno'> 339</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input', title='(NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None'>get_train_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', title='NetSliceWithAutoregression'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', title='int'>i</a>):
<span class='lineno'> 340</span>     &quot;&quot;&quot;See SequenceLayerBase.get_train_input for details.&quot;&quot;&quot;
<span class='lineno'> 341</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', title='int'>i</a> == 0:
<span class='lineno'> 342</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', title='?'>prev</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', title='NetSliceWithAutoregression'>self</a>._zero_label
<span class='lineno'> 343</span>     else:
<span class='lineno'> 344</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', title='?'>prev</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', title='NetSliceWithAutoregression'>self</a>._labels_one_hot[:, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', title='int'>i</a> - 1, :]
<span class='lineno'> 345</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.image_feature', title='None'>image_feature</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.self', title='NetSliceWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice.get_image_feature', title='(NetSlice, int) -> None / (NetSliceWithAutoregression, ?) -> None / (NetSlice, ?) -> None / (NetSliceWithAutoregression, int) -> None'>get_image_feature</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.i', title='int'>i</a>)
<span class='lineno'> 346</span>     return tf.concat([<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.image_feature', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.image_feature', title='None'>image_feature</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression.get_train_input.prev', title='?'>prev</a>], 1)
<span class='lineno'> 347</span> 
<span class='lineno'> 348</span> 
<span class='lineno'> 349</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', title='<Attention>'>Attention</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase', title='<SequenceLayerBase>'>SequenceLayerBase</a>):
<span class='lineno'> 350</span>   &quot;&quot;&quot;A layer which uses attention mechanism to select image features.&quot;&quot;&quot;
<span class='lineno'> 351</span> 
<span class='lineno'> 352</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', title='Attention'>self</a>, *args, **kwargs):
<span class='lineno'> 353</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', title='<Attention>'>Attention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', title='Attention'>self</a>).__init__(*args, **kwargs)
<span class='lineno'> 354</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', title='Attention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', title='?'>_zero_label</a></a> = tf.zeros(
<span class='lineno'> 355</span>         [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', title='Attention'>self</a>._batch_size, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.__init__.self', title='Attention'>self</a>._params.num_char_classes])
<span class='lineno'> 356</span> 
<span class='lineno'> 357</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input', title='(Attention, None, int) -> None / (Attention, ?, ?) -> None'>get_eval_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.self', title='Attention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.i', title='int'>i</a>):
<span class='lineno'> 358</span>     &quot;&quot;&quot;See SequenceLayerBase.get_eval_input for details.&quot;&quot;&quot;
<span class='lineno'> 359</span>     del <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.prev', title='None'>prev</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.i', title='int'>i</a>
<span class='lineno'> 360</span>     # The attention_decoder will fetch image features from the net, no need for
<span class='lineno'> 361</span>     # extra inputs.
<span class='lineno'> 362</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention._zero_label', title='?'>_zero_label</a>
<span class='lineno'> 363</span> 
<span class='lineno'> 364</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input', title='(Attention, None, int) -> None / (Attention, ?, ?) -> None'>get_train_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.self', title='Attention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.i', title='int'>i</a>):
<span class='lineno'> 365</span>     &quot;&quot;&quot;See SequenceLayerBase.get_train_input for details.&quot;&quot;&quot;
<span class='lineno'> 366</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_eval_input', title='(Attention, None, int) -> None / (Attention, ?, ?) -> None'>get_eval_input</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.prev', title='None'>prev</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.get_train_input.i', title='int'>i</a>)
<span class='lineno'> 367</span> 
<span class='lineno'> 368</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell', title='(AttentionWithAutoregression, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> ? / (Attention, ?, ?, ?, ?) -> ? / (Attention, [None], ?, (AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None, ?) -> ?'>unroll_cell</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', title='{Attention | AttentionWithAutoregression}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.decoder_inputs', title='[None]'>decoder_inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.initial_state', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.initial_state', title='?'>initial_state</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.loop_function', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.loop_function', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>loop_function</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.cell', title='?'>cell</a>):
<span class='lineno'> 369</span>     return tf.contrib.legacy_seq2seq.attention_decoder(
<span class='lineno'> 370</span>         decoder_inputs=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.decoder_inputs', title='[None]'>decoder_inputs</a>,
<span class='lineno'> 371</span>         initial_state=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.initial_state', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.initial_state', title='?'>initial_state</a>,
<span class='lineno'> 372</span>         attention_states=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', title='{Attention | AttentionWithAutoregression}'>self</a>._net,
<span class='lineno'> 373</span>         cell=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.cell', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.cell', title='?'>cell</a>,
<span class='lineno'> 374</span>         loop_function=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention.unroll_cell.self', title='{Attention | AttentionWithAutoregression}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.get_input', title='(AttentionWithAutoregression, None, int) -> None / (NetSlice, None, int) -> None / (SequenceLayerBase, None, int) -> None / (NetSliceWithAutoregression, None, int) -> None / (SequenceLayerBase, ?, ?) -> None / (Attention, None, int) -> None'>get_input</a>)
<span class='lineno'> 375</span> 
<span class='lineno'> 376</span> 
<span class='lineno'> 377</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', title='<AttentionWithAutoregression>'>AttentionWithAutoregression</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', title='<Attention>'>Attention</a>):
<span class='lineno'> 378</span>   &quot;&quot;&quot;A layer which uses both attention and auto regression.&quot;&quot;&quot;
<span class='lineno'> 379</span> 
<span class='lineno'> 380</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__.self', title='AttentionWithAutoregression'>self</a>, *args, **kwargs):
<span class='lineno'> 381</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', title='<AttentionWithAutoregression>'>AttentionWithAutoregression</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.__init__.self', title='AttentionWithAutoregression'>self</a>).__init__(*args, **kwargs)
<span class='lineno'> 382</span> 
<span class='lineno'> 383</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input', title='(AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None'>get_train_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', title='AttentionWithAutoregression'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', title='int'>i</a>):
<span class='lineno'> 384</span>     &quot;&quot;&quot;See SequenceLayerBase.get_train_input for details.&quot;&quot;&quot;
<span class='lineno'> 385</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', title='int'>i</a> == 0:
<span class='lineno'> 386</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', title='AttentionWithAutoregression'>self</a>._zero_label
<span class='lineno'> 387</span>     else:
<span class='lineno'> 388</span>       # TODO(gorban): update to gradually introduce gt labels.
<span class='lineno'> 389</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.self', title='AttentionWithAutoregression'>self</a>._labels_one_hot[:, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_train_input.i', title='int'>i</a> - 1, :]
<span class='lineno'> 390</span> 
<span class='lineno'> 391</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input', title='(AttentionWithAutoregression, None, int) -> None / (AttentionWithAutoregression, ?, ?) -> None'>get_eval_input</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', title='AttentionWithAutoregression'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.prev', title='None'>prev</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', title='int'>i</a>):
<span class='lineno'> 392</span>     &quot;&quot;&quot;See SequenceLayerBase.get_eval_input for details.&quot;&quot;&quot;
<span class='lineno'> 393</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', title='int'>i</a> == 0:
<span class='lineno'> 394</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', title='AttentionWithAutoregression'>self</a>._zero_label
<span class='lineno'> 395</span>     else:
<span class='lineno'> 396</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.logit', title='None'>logit</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', title='AttentionWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_logit', title='(AttentionWithAutoregression, ?, ?) -> None / (AttentionWithAutoregression, None, int) -> None / (NetSlice, ?, ?) -> None / (Attention, ?, ?) -> None / (NetSliceWithAutoregression, None, int) -> None / (NetSliceWithAutoregression, ?, ?) -> None / (SequenceLayerBase, ?, ?) -> None / (AttentionWithAutoregression, ?, int) -> None / (NetSliceWithAutoregression, ?, int) -> None'>char_logit</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.prev', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.prev', title='None'>prev</a>, char_index=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.i', title='int'>i</a> - 1)
<span class='lineno'> 397</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.self', title='AttentionWithAutoregression'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.SequenceLayerBase.char_one_hot', title='(SequenceLayerBase, ?) -> None / (NetSliceWithAutoregression, None) -> None / (AttentionWithAutoregression, None) -> None'>char_one_hot</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.logit', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression.get_eval_input.logit', title='None'>logit</a>)
<span class='lineno'> 398</span> 
<span class='lineno'> 399</span> 
<span class='lineno'> 400</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class', title='(?, ?) -> {<Attention> | <AttentionWithAutoregression> | <NetSlice> | <NetSliceWithAutoregression>}'>get_layer_class</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', title='?'>use_attention</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', title='?'>use_autoregression</a>):
<span class='lineno'> 401</span>   &quot;&quot;&quot;A convenience function to get a layer class based on requirements.
<span class='lineno'> 402</span> 
<span class='lineno'> 403</span>   Args:
<span class='lineno'> 404</span>     use_attention: if True a returned class will use attention.
<span class='lineno'> 405</span>     use_autoregression: if True a returned class will use auto regression.
<span class='lineno'> 406</span> 
<span class='lineno'> 407</span>   Returns:
<span class='lineno'> 408</span>     One of available sequence layers (child classes for SequenceLayerBase).
<span class='lineno'> 409</span>   &quot;&quot;&quot;
<span class='lineno'> 410</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', title='?'>use_attention</a> and <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', title='?'>use_autoregression</a>:
<span class='lineno'> 411</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='<AttentionWithAutoregression>'>layer_class</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.AttentionWithAutoregression', title='<AttentionWithAutoregression>'>AttentionWithAutoregression</a>
<span class='lineno'> 412</span>   elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', title='?'>use_attention</a> and not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', title='?'>use_autoregression</a>:
<span class='lineno'> 413</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='<Attention>'>layer_class</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.Attention', title='<Attention>'>Attention</a>
<span class='lineno'> 414</span>   elif not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', title='?'>use_attention</a> and not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', title='?'>use_autoregression</a>:
<span class='lineno'> 415</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='<NetSlice>'>layer_class</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSlice', title='<NetSlice>'>NetSlice</a>
<span class='lineno'> 416</span>   elif not <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_attention', title='?'>use_attention</a> and <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.use_autoregression', title='?'>use_autoregression</a>:
<span class='lineno'> 417</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='<NetSliceWithAutoregression>'>layer_class</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.NetSliceWithAutoregression', title='<NetSliceWithAutoregression>'>NetSliceWithAutoregression</a>
<span class='lineno'> 418</span>   else:
<span class='lineno'> 419</span>     raise AssertionError(&#39;Unsupported sequence layer class&#39;)
<span class='lineno'> 420</span> 
<span class='lineno'> 421</span>   logging.debug(&#39;Use %s as a layer class&#39;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='{<Attention> | <AttentionWithAutoregression> | <NetSlice> | <NetSliceWithAutoregression>}'>layer_class</a>.__name__)
<span class='lineno'> 422</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.sequence_layers.get_layer_class.layer_class', title='{<Attention> | <AttentionWithAutoregression> | <NetSlice> | <NetSliceWithAutoregression>}'>layer_class</a>
</pre></td></tr></table></body></html>