<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/research/attention_ocr/python/inception_preprocessing.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector'>apply_with_random_selector</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color'>distort_color</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop'>distorted_bounding_box_crop</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train'>preprocess_for_train</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval'>preprocess_for_eval</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image', xid='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image'>preprocess_image</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Provides utilities to preprocess images for the Inception networks.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> # TODO(gorban): add as a dependency, when slim or tensorflow/models are pipfied
<span class='lineno'>  18</span> # Source:
<span class='lineno'>  19</span> # https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py
<span class='lineno'>  20</span> from __future__ import absolute_import
<span class='lineno'>  21</span> from __future__ import division
<span class='lineno'>  22</span> from __future__ import print_function
<span class='lineno'>  23</span> 
<span class='lineno'>  24</span> import tensorflow as tf
<span class='lineno'>  25</span> 
<span class='lineno'>  26</span> from tensorflow.python.ops import control_flow_ops
<span class='lineno'>  27</span> 
<span class='lineno'>  28</span> 
<span class='lineno'>  29</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', title='(?, (?, int) -> None, int) -> None / (?, (?, int) -> ?, int) -> None / (None, ?, int) -> None / (?, ?, ?) -> None'>apply_with_random_selector</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.x', title='None'>x</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.func', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.func', title='{(?, int) -> ? | (?, int) -> ? | (?, int) -> None}'>func</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', title='int'>num_cases</a>):
<span class='lineno'>  30</span>   &quot;&quot;&quot;Computes func(x, sel), with sel sampled from [0...num_cases-1].
<span class='lineno'>  31</span> 
<span class='lineno'>  32</span>   Args:
<span class='lineno'>  33</span>     x: input Tensor.
<span class='lineno'>  34</span>     func: Python function to apply.
<span class='lineno'>  35</span>     num_cases: Python int32, number of cases to sample sel from.
<span class='lineno'>  36</span> 
<span class='lineno'>  37</span>   Returns:
<span class='lineno'>  38</span>     The result of func(x, sel), where func receives the value of the
<span class='lineno'>  39</span>     selector as a python integer, but sel is sampled dynamically.
<span class='lineno'>  40</span>   &quot;&quot;&quot;
<span class='lineno'>  41</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.sel', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.sel', title='?'>sel</a> = tf.random_uniform([], maxval=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', title='int'>num_cases</a>, dtype=tf.int32)
<span class='lineno'>  42</span>   # Pass the real x only to one of the func calls.
<span class='lineno'>  43</span>   return control_flow_ops.merge([
<span class='lineno'>  44</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.func', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.func', title='{(?, int) -> ? | (?, int) -> ? | (?, int) -> None}'>func</a>(control_flow_ops.switch(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.x', title='None'>x</a>, tf.equal(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.sel', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.sel', title='?'>sel</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', title='int'>case</a>))[1], <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', title='int'>case</a>)
<span class='lineno'>  45</span>       for <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', title='int'><a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.case', title='int'>case</a></a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector.num_cases', title='int'>num_cases</a>)
<span class='lineno'>  46</span>   ])[0]
<span class='lineno'>  47</span> 
<span class='lineno'>  48</span> 
<span class='lineno'>  49</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color', title='(?, int, bool, None) -> None'>distort_color</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a>=0, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.fast_mode', title='bool'>fast_mode</a>=True, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.scope', title='None'>scope</a>=None):
<span class='lineno'>  50</span>   &quot;&quot;&quot;Distort the color of a Tensor image.
<span class='lineno'>  51</span> 
<span class='lineno'>  52</span>   Each color distortion is non-commutative and thus ordering of the color ops
<span class='lineno'>  53</span>   matters. Ideally we would randomly permute the ordering of the color ops.
<span class='lineno'>  54</span>   Rather than adding that level of complication, we select a distinct ordering
<span class='lineno'>  55</span>   of color ops for each preprocessing thread.
<span class='lineno'>  56</span> 
<span class='lineno'>  57</span>   Args:
<span class='lineno'>  58</span>     image: 3-D Tensor containing single image in [0, 1].
<span class='lineno'>  59</span>     color_ordering: Python int, a type of distortion (valid values: 0-3).
<span class='lineno'>  60</span>     fast_mode: Avoids slower ops (random_hue and random_contrast)
<span class='lineno'>  61</span>     scope: Optional scope for name_scope.
<span class='lineno'>  62</span>   Returns:
<span class='lineno'>  63</span>     3-D Tensor color-distorted image on range [0, 1]
<span class='lineno'>  64</span>   Raises:
<span class='lineno'>  65</span>     ValueError: if color_ordering not in [0, 3]
<span class='lineno'>  66</span>   &quot;&quot;&quot;
<span class='lineno'>  67</span>   with tf.name_scope(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.scope', title='None'>scope</a>, &#39;distort_color&#39;, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>]):
<span class='lineno'>  68</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.fast_mode', title='bool'>fast_mode</a>:
<span class='lineno'>  69</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a> == 0:
<span class='lineno'>  70</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  71</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  72</span>       else:
<span class='lineno'>  73</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  74</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  75</span>     else:
<span class='lineno'>  76</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a> == 0:
<span class='lineno'>  77</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  78</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  79</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'>  80</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  81</span>       elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a> == 1:
<span class='lineno'>  82</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  83</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  84</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  85</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'>  86</span>       elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a> == 2:
<span class='lineno'>  87</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  88</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'>  89</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  90</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  91</span>       elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.color_ordering', title='int'>color_ordering</a> == 3:
<span class='lineno'>  92</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_hue(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=0.2)
<span class='lineno'>  93</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_saturation(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  94</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_contrast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, lower=0.5, upper=1.5)
<span class='lineno'>  95</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a> = tf.image.random_brightness(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, max_delta=32. / 255.)
<span class='lineno'>  96</span>       else:
<span class='lineno'>  97</span>         raise ValueError(&#39;color_ordering must be in [0, 3]&#39;)
<span class='lineno'>  98</span> 
<span class='lineno'>  99</span>     # The random_* ops do not necessarily clamp.
<span class='lineno'> 100</span>     return tf.clip_by_value(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color.image', title='?'>image</a>, 0.0, 1.0)
<span class='lineno'> 101</span> 
<span class='lineno'> 102</span> 
<span class='lineno'> 103</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop', title='(?, ?, float, (float, float), (float, float), int, None) -> (?, ?) / (?, None, float, (float, float), (float, float), int, None) -> (?, ?)'>distorted_bounding_box_crop</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', title='?'>image</a>,
<span class='lineno'> 104</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', title='None'>bbox</a>,
<span class='lineno'> 105</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.min_object_covered', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.min_object_covered', title='float'>min_object_covered</a>=0.1,
<span class='lineno'> 106</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.aspect_ratio_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.aspect_ratio_range', title='(float, float)'>aspect_ratio_range</a>=(0.75, 1.33),
<span class='lineno'> 107</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.area_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.area_range', title='(float, float)'>area_range</a>=(0.05, 1.0),
<span class='lineno'> 108</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.max_attempts', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.max_attempts', title='int'>max_attempts</a>=100,
<span class='lineno'> 109</span>                                 <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.scope', title='None'>scope</a>=None):
<span class='lineno'> 110</span>   &quot;&quot;&quot;Generates cropped_image using a one of the bboxes randomly distorted.
<span class='lineno'> 111</span> 
<span class='lineno'> 112</span>   See `tf.image.sample_distorted_bounding_box` for more documentation.
<span class='lineno'> 113</span> 
<span class='lineno'> 114</span>   Args:
<span class='lineno'> 115</span>     image: 3-D Tensor of image (it will be converted to floats in [0, 1]).
<span class='lineno'> 116</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 117</span>       where each coordinate is [0, 1) and the coordinates are arranged
<span class='lineno'> 118</span>       as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the
<span class='lineno'> 119</span>       whole image.
<span class='lineno'> 120</span>     min_object_covered: An optional `float`. Defaults to `0.1`. The cropped
<span class='lineno'> 121</span>       area of the image must contain at least this fraction of any bounding box
<span class='lineno'> 122</span>       supplied.
<span class='lineno'> 123</span>     aspect_ratio_range: An optional list of `floats`. The cropped area of the
<span class='lineno'> 124</span>       image must have an aspect ratio = width / height within this range.
<span class='lineno'> 125</span>     area_range: An optional list of `floats`. The cropped area of the image
<span class='lineno'> 126</span>       must contain a fraction of the supplied image within in this range.
<span class='lineno'> 127</span>     max_attempts: An optional `int`. Number of attempts at generating a cropped
<span class='lineno'> 128</span>       region of the image of the specified constraints. After `max_attempts`
<span class='lineno'> 129</span>       failures, return the entire image.
<span class='lineno'> 130</span>     scope: Optional scope for name_scope.
<span class='lineno'> 131</span>   Returns:
<span class='lineno'> 132</span>     A tuple, a 3-D Tensor cropped_image and the distorted bbox
<span class='lineno'> 133</span>   &quot;&quot;&quot;
<span class='lineno'> 134</span>   with tf.name_scope(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.scope', title='None'>scope</a>, &#39;distorted_bounding_box_crop&#39;, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', title='None'>bbox</a>]):
<span class='lineno'> 135</span>     # Each bounding box has shape [1, num_boxes, box coords] and
<span class='lineno'> 136</span>     # the coordinates are ordered [ymin, xmin, ymax, xmax].
<span class='lineno'> 137</span> 
<span class='lineno'> 138</span>     # A large fraction of image datasets contain a human-annotated bounding
<span class='lineno'> 139</span>     # box delineating the region of the image containing the object of interest.
<span class='lineno'> 140</span>     # We choose to create a new bounding box for the object which is a randomly
<span class='lineno'> 141</span>     # distorted version of the human-annotated bounding box that obeys an
<span class='lineno'> 142</span>     # allowed range of aspect ratios, sizes and overlap with the human-annotated
<span class='lineno'> 143</span>     # bounding box. If no box is supplied, then we assume the bounding box is
<span class='lineno'> 144</span>     # the entire image.
<span class='lineno'> 145</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.sample_distorted_bounding_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.sample_distorted_bounding_box', title='?'>sample_distorted_bounding_box</a> = tf.image.sample_distorted_bounding_box(
<span class='lineno'> 146</span>         tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', title='?'>image</a>),
<span class='lineno'> 147</span>         bounding_boxes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox', title='None'>bbox</a>,
<span class='lineno'> 148</span>         min_object_covered=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.min_object_covered', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.min_object_covered', title='float'>min_object_covered</a>,
<span class='lineno'> 149</span>         aspect_ratio_range=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.aspect_ratio_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.aspect_ratio_range', title='(float, float)'>aspect_ratio_range</a>,
<span class='lineno'> 150</span>         area_range=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.area_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.area_range', title='(float, float)'>area_range</a>,
<span class='lineno'> 151</span>         max_attempts=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.max_attempts', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.max_attempts', title='int'>max_attempts</a>,
<span class='lineno'> 152</span>         use_image_if_no_bounding_boxes=True)
<span class='lineno'> 153</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_begin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_begin', title='?'>bbox_begin</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_size', title='?'>bbox_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.distort_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.distort_bbox', title='?'>distort_bbox</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.sample_distorted_bounding_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.sample_distorted_bounding_box', title='?'>sample_distorted_bounding_box</a>
<span class='lineno'> 154</span> 
<span class='lineno'> 155</span>     # Crop the image to the specified bounding box.
<span class='lineno'> 156</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.cropped_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.cropped_image', title='?'>cropped_image</a> = tf.slice(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_begin', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_begin', title='?'>bbox_begin</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.bbox_size', title='?'>bbox_size</a>)
<span class='lineno'> 157</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.cropped_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.cropped_image', title='?'>cropped_image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.distort_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop.distort_bbox', title='?'>distort_bbox</a>
<span class='lineno'> 158</span> 
<span class='lineno'> 159</span> 
<span class='lineno'> 160</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train', title='(?, ?, ?, ?, bool, None) -> None / (?, ?, ?, None, bool, None) -> None'>preprocess_for_train</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>,
<span class='lineno'> 161</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', title='?'>height</a>,
<span class='lineno'> 162</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', title='?'>width</a>,
<span class='lineno'> 163</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='None'>bbox</a>,
<span class='lineno'> 164</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', title='bool'>fast_mode</a>=True,
<span class='lineno'> 165</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.scope', title='None'>scope</a>=None):
<span class='lineno'> 166</span>   &quot;&quot;&quot;Distort one image for training a network.
<span class='lineno'> 167</span> 
<span class='lineno'> 168</span>   Distorting images provides a useful technique for augmenting the data
<span class='lineno'> 169</span>   set during training in order to make the network invariant to aspects
<span class='lineno'> 170</span>   of the image that do not effect the label.
<span class='lineno'> 171</span> 
<span class='lineno'> 172</span>   Additionally it would create image_summaries to display the different
<span class='lineno'> 173</span>   transformations applied to the image.
<span class='lineno'> 174</span> 
<span class='lineno'> 175</span>   Args:
<span class='lineno'> 176</span>     image: 3-D Tensor of image. If dtype is tf.float32 then the range should be
<span class='lineno'> 177</span>       [0, 1], otherwise it would converted to tf.float32 assuming that the range
<span class='lineno'> 178</span>       is [0, MAX], where MAX is largest positive representable number for
<span class='lineno'> 179</span>       int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).
<span class='lineno'> 180</span>     height: integer
<span class='lineno'> 181</span>     width: integer
<span class='lineno'> 182</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 183</span>       where each coordinate is [0, 1) and the coordinates are arranged
<span class='lineno'> 184</span>       as [ymin, xmin, ymax, xmax].
<span class='lineno'> 185</span>     fast_mode: Optional boolean, if True avoids slower transformations (i.e.
<span class='lineno'> 186</span>       bi-cubic resizing, random_hue or random_contrast).
<span class='lineno'> 187</span>     scope: Optional scope for name_scope.
<span class='lineno'> 188</span>   Returns:
<span class='lineno'> 189</span>     3-D float Tensor of distorted image used for training with range [-1, 1].
<span class='lineno'> 190</span>   &quot;&quot;&quot;
<span class='lineno'> 191</span>   with tf.name_scope(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.scope', title='None'>scope</a>, &#39;distort_image&#39;, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', title='?'>width</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='None'>bbox</a>]):
<span class='lineno'> 192</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='None'>bbox</a> is None:
<span class='lineno'> 193</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='?'>bbox</a> = tf.constant(
<span class='lineno'> 194</span>           [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
<span class='lineno'> 195</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>.dtype != tf.float32:
<span class='lineno'> 196</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a> = tf.image.convert_image_dtype(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>, dtype=tf.float32)
<span class='lineno'> 197</span>     # Each bounding box has shape [1, num_boxes, box coords] and
<span class='lineno'> 198</span>     # the coordinates are ordered [ymin, xmin, ymax, xmax].
<span class='lineno'> 199</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_box', title='?'>image_with_box</a> = tf.image.draw_bounding_boxes(
<span class='lineno'> 200</span>         tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>, 0), <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='None'>bbox</a>)
<span class='lineno'> 201</span>     tf.summary.image(&#39;image_with_bounding_boxes&#39;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_box', title='?'>image_with_box</a>)
<span class='lineno'> 202</span> 
<span class='lineno'> 203</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_bbox', title='?'>distorted_bbox</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distorted_bounding_box_crop', title='(?, ?, float, (float, float), (float, float), int, None) -> (?, ?) / (?, None, float, (float, float), (float, float), int, None) -> (?, ?)'>distorted_bounding_box_crop</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.bbox', title='None'>bbox</a>)
<span class='lineno'> 204</span>     # Restore the shape since the dynamic slice based upon the bbox_size loses
<span class='lineno'> 205</span>     # the third dimension.
<span class='lineno'> 206</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>.set_shape([None, None, 3])
<span class='lineno'> 207</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_distorted_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_distorted_box', title='?'>image_with_distorted_box</a> = tf.image.draw_bounding_boxes(
<span class='lineno'> 208</span>         tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image', title='?'>image</a>, 0), <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_bbox', title='?'>distorted_bbox</a>)
<span class='lineno'> 209</span>     tf.summary.image(&#39;images_with_distorted_bounding_box&#39;,
<span class='lineno'> 210</span>                      <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_distorted_box', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.image_with_distorted_box', title='?'>image_with_distorted_box</a>)
<span class='lineno'> 211</span> 
<span class='lineno'> 212</span>     # This resizing operation may distort the images because the aspect
<span class='lineno'> 213</span>     # ratio is not respected. We select a resize method in a round robin
<span class='lineno'> 214</span>     # fashion based on the thread number.
<span class='lineno'> 215</span>     # Note that ResizeMethod contains 4 enumerated resizing methods.
<span class='lineno'> 216</span> 
<span class='lineno'> 217</span>     # We select only 1 case for fast_mode bilinear.
<span class='lineno'> 218</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.num_resize_cases', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.num_resize_cases', title='int'>num_resize_cases</a> = 1 if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', title='bool'>fast_mode</a> else 4
<span class='lineno'> 219</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', title='(?, (?, int) -> None, int) -> None / (?, (?, int) -> ?, int) -> None / (None, ?, int) -> None / (?, ?, ?) -> None'>apply_with_random_selector</a>(
<span class='lineno'> 220</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>,
<span class='lineno'> 221</span>         lambda <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.x', title='?'>x</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.method', title='int'>method</a>: tf.image.resize_images(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.x', title='?'>x</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.width', title='?'>width</a>], method=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%94.method', title='int'>method</a>),
<span class='lineno'> 222</span>         num_cases=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.num_resize_cases', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.num_resize_cases', title='int'>num_resize_cases</a>)
<span class='lineno'> 223</span> 
<span class='lineno'> 224</span>     tf.summary.image(&#39;cropped_resized_image&#39;,
<span class='lineno'> 225</span>                      tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a>, 0))
<span class='lineno'> 226</span> 
<span class='lineno'> 227</span>     # Randomly flip the image horizontally.
<span class='lineno'> 228</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a> = tf.image.random_flip_left_right(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a>)
<span class='lineno'> 229</span> 
<span class='lineno'> 230</span>     # Randomly distort the colors. There are 4 ways to do it.
<span class='lineno'> 231</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.apply_with_random_selector', title='(?, (?, int) -> None, int) -> None / (?, (?, int) -> ?, int) -> None / (None, ?, int) -> None / (?, ?, ?) -> None'>apply_with_random_selector</a>(
<span class='lineno'> 232</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>,
<span class='lineno'> 233</span>         lambda <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.x', title='?'>x</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.ordering', title='int'>ordering</a>: <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.distort_color', title='(?, int, bool, None) -> None'>distort_color</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.x', title='?'>x</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.ordering', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.lambda%95.ordering', title='int'>ordering</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.fast_mode', title='bool'>fast_mode</a>),
<span class='lineno'> 234</span>         num_cases=4)
<span class='lineno'> 235</span> 
<span class='lineno'> 236</span>     tf.summary.image(&#39;final_distorted_image&#39;,
<span class='lineno'> 237</span>                      tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a>, 0))
<span class='lineno'> 238</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a> = tf.subtract(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='None'>distorted_image</a>, 0.5)
<span class='lineno'> 239</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>, 2.0)
<span class='lineno'> 240</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train.distorted_image', title='?'>distorted_image</a>
<span class='lineno'> 241</span> 
<span class='lineno'> 242</span> 
<span class='lineno'> 243</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval', title='(?, ?, ?, float, None) -> None'>preprocess_for_eval</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>,
<span class='lineno'> 244</span>                         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', title='?'>height</a>,
<span class='lineno'> 245</span>                         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', title='?'>width</a>,
<span class='lineno'> 246</span>                         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', title='float'>central_fraction</a>=0.875,
<span class='lineno'> 247</span>                         <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.scope', title='None'>scope</a>=None):
<span class='lineno'> 248</span>   &quot;&quot;&quot;Prepare one image for evaluation.
<span class='lineno'> 249</span> 
<span class='lineno'> 250</span>   If height and width are specified it would output an image with that size by
<span class='lineno'> 251</span>   applying resize_bilinear.
<span class='lineno'> 252</span> 
<span class='lineno'> 253</span>   If central_fraction is specified it would cropt the central fraction of the
<span class='lineno'> 254</span>   input image.
<span class='lineno'> 255</span> 
<span class='lineno'> 256</span>   Args:
<span class='lineno'> 257</span>     image: 3-D Tensor of image. If dtype is tf.float32 then the range should be
<span class='lineno'> 258</span>       [0, 1], otherwise it would converted to tf.float32 assuming that the range
<span class='lineno'> 259</span>       is [0, MAX], where MAX is largest positive representable number for
<span class='lineno'> 260</span>       int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)
<span class='lineno'> 261</span>     height: integer
<span class='lineno'> 262</span>     width: integer
<span class='lineno'> 263</span>     central_fraction: Optional Float, fraction of the image to crop.
<span class='lineno'> 264</span>     scope: Optional scope for name_scope.
<span class='lineno'> 265</span>   Returns:
<span class='lineno'> 266</span>     3-D float Tensor of prepared image.
<span class='lineno'> 267</span>   &quot;&quot;&quot;
<span class='lineno'> 268</span>   with tf.name_scope(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.scope', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.scope', title='None'>scope</a>, &#39;eval_image&#39;, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', title='?'>width</a>]):
<span class='lineno'> 269</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>.dtype != tf.float32:
<span class='lineno'> 270</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.image.convert_image_dtype(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, dtype=tf.float32)
<span class='lineno'> 271</span>     # Crop the central region of the image with an area containing 87.5% of
<span class='lineno'> 272</span>     # the original image.
<span class='lineno'> 273</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', title='float'>central_fraction</a>:
<span class='lineno'> 274</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.image.central_crop(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, central_fraction=<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.central_fraction', title='float'>central_fraction</a>)
<span class='lineno'> 275</span> 
<span class='lineno'> 276</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', title='?'>height</a> and <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', title='?'>width</a>:
<span class='lineno'> 277</span>       # Resize the image to the specified height and width.
<span class='lineno'> 278</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, 0)
<span class='lineno'> 279</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.image.resize_bilinear(
<span class='lineno'> 280</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.width', title='?'>width</a>], align_corners=False)
<span class='lineno'> 281</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.squeeze(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, [0])
<span class='lineno'> 282</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.subtract(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, 0.5)
<span class='lineno'> 283</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>, 2.0)
<span class='lineno'> 284</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval.image', title='?'>image</a>
<span class='lineno'> 285</span> 
<span class='lineno'> 286</span> 
<span class='lineno'> 287</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image', title='(?, ?, ?, bool, None, bool) -> None'>preprocess_image</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', title='?'>image</a>,
<span class='lineno'> 288</span>                      <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', title='?'>height</a>,
<span class='lineno'> 289</span>                      <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', title='?'>width</a>,
<span class='lineno'> 290</span>                      <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.is_training', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.is_training', title='bool'>is_training</a>=False,
<span class='lineno'> 291</span>                      <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.bbox', title='None'>bbox</a>=None,
<span class='lineno'> 292</span>                      <a name='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.fast_mode', title='bool'>fast_mode</a>=True):
<span class='lineno'> 293</span>   &quot;&quot;&quot;Pre-process one image for training or evaluation.
<span class='lineno'> 294</span> 
<span class='lineno'> 295</span>   Args:
<span class='lineno'> 296</span>     image: 3-D Tensor [height, width, channels] with the image.
<span class='lineno'> 297</span>     height: integer, image expected height.
<span class='lineno'> 298</span>     width: integer, image expected width.
<span class='lineno'> 299</span>     is_training: Boolean. If true it would transform an image for train,
<span class='lineno'> 300</span>       otherwise it would transform it for evaluation.
<span class='lineno'> 301</span>     bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
<span class='lineno'> 302</span>       where each coordinate is [0, 1) and the coordinates are arranged as
<span class='lineno'> 303</span>       [ymin, xmin, ymax, xmax].
<span class='lineno'> 304</span>     fast_mode: Optional boolean, if True avoids slower transformations.
<span class='lineno'> 305</span> 
<span class='lineno'> 306</span>   Returns:
<span class='lineno'> 307</span>     3-D float Tensor containing an appropriately scaled image
<span class='lineno'> 308</span> 
<span class='lineno'> 309</span>   Raises:
<span class='lineno'> 310</span>     ValueError: if user does not provide bounding box
<span class='lineno'> 311</span>   &quot;&quot;&quot;
<span class='lineno'> 312</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.is_training', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.is_training', title='bool'>is_training</a>:
<span class='lineno'> 313</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_train', title='(?, ?, ?, ?, bool, None) -> None / (?, ?, ?, None, bool, None) -> None'>preprocess_for_train</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', title='?'>width</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.bbox', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.bbox', title='None'>bbox</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.fast_mode', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.fast_mode', title='bool'>fast_mode</a>)
<span class='lineno'> 314</span>   else:
<span class='lineno'> 315</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_for_eval', title='(?, ?, ?, float, None) -> None'>preprocess_for_eval</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.height', title='?'>height</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', xid ='.home.xxm.Desktop.EMSE.dataset.models.research.attention_ocr.python.inception_preprocessing.preprocess_image.width', title='?'>width</a>)
</pre></td></tr></table></body></html>