<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/transformer/model_utils.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32'>_NEG_INF_FP32</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16'>_NEG_INF_FP16</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding'>get_position_encoding</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias'>get_decoder_self_attention_bias</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding'>get_padding</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias'>get_padding_bias</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Transformer model helper methods.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> from __future__ import absolute_import
<span class='lineno'>  18</span> from __future__ import division
<span class='lineno'>  19</span> from __future__ import print_function
<span class='lineno'>  20</span> 
<span class='lineno'>  21</span> import math
<span class='lineno'>  22</span> 
<span class='lineno'>  23</span> import numpy as np
<span class='lineno'>  24</span> import tensorflow as tf
<span class='lineno'>  25</span> 
<span class='lineno'>  26</span> # Very low numbers to represent -infinity. We do not actually use -Inf, since we
<span class='lineno'>  27</span> # want to be able to multiply these values by zero to get zero. (-Inf * 0 = NaN)
<span class='lineno'>  28</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', title='float'>_NEG_INF_FP32</a> = -1e9
<span class='lineno'>  29</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16', title='?'>_NEG_INF_FP16</a> = np.finfo(np.float16).min
<span class='lineno'>  30</span> 
<span class='lineno'>  31</span> 
<span class='lineno'>  32</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding', title='(?, ?, float, float) -> None'>get_position_encoding</a>(
<span class='lineno'>  33</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.length', title='?'>length</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.hidden_size', title='?'>hidden_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', title='float'>min_timescale</a>=1.0, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.max_timescale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.max_timescale', title='float'>max_timescale</a>=1.0e4):
<span class='lineno'>  34</span>   &quot;&quot;&quot;Return positional encoding.
<span class='lineno'>  35</span> 
<span class='lineno'>  36</span>   Calculates the position encoding as a mix of sine and cosine functions with
<span class='lineno'>  37</span>   geometrically increasing wavelengths.
<span class='lineno'>  38</span>   Defined and formulized in Attention is All You Need, section 3.5.
<span class='lineno'>  39</span> 
<span class='lineno'>  40</span>   Args:
<span class='lineno'>  41</span>     length: Sequence length.
<span class='lineno'>  42</span>     hidden_size: Size of the
<span class='lineno'>  43</span>     min_timescale: Minimum scale that will be applied at each position
<span class='lineno'>  44</span>     max_timescale: Maximum scale that will be applied at each position
<span class='lineno'>  45</span> 
<span class='lineno'>  46</span>   Returns:
<span class='lineno'>  47</span>     Tensor with shape [length, hidden_size]
<span class='lineno'>  48</span>   &quot;&quot;&quot;
<span class='lineno'>  49</span>   # We compute the positional encoding in float32 even if the model uses
<span class='lineno'>  50</span>   # float16, as many of the ops used, like log and exp, are numerically unstable
<span class='lineno'>  51</span>   # in float16.
<span class='lineno'>  52</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.position', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.position', title='?'>position</a> = tf.cast(tf.range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.length', title='?'>length</a>), tf.float32)
<span class='lineno'>  53</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', title='int'>num_timescales</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.hidden_size', title='?'>hidden_size</a> // 2
<span class='lineno'>  54</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.log_timescale_increment', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.log_timescale_increment', title='int'>log_timescale_increment</a> = (
<span class='lineno'>  55</span>       math.log(float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.max_timescale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.max_timescale', title='float'>max_timescale</a>) / float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', title='float'>min_timescale</a>)) /
<span class='lineno'>  56</span>       (tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', title='int'>num_timescales</a>, tf.float32) - 1))
<span class='lineno'>  57</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.inv_timescales', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.inv_timescales', title='float'>inv_timescales</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.min_timescale', title='float'>min_timescale</a> * tf.exp(
<span class='lineno'>  58</span>       tf.cast(tf.range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.num_timescales', title='int'>num_timescales</a>), tf.float32) * -<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.log_timescale_increment', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.log_timescale_increment', title='int'>log_timescale_increment</a>)
<span class='lineno'>  59</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', title='?'>scaled_time</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.position', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.position', title='?'>position</a>, 1) * tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.inv_timescales', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.inv_timescales', title='float'>inv_timescales</a>, 0)
<span class='lineno'>  60</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.signal', title='?'>signal</a> = tf.concat([tf.sin(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', title='?'>scaled_time</a>), tf.cos(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.scaled_time', title='?'>scaled_time</a>)], axis=1)
<span class='lineno'>  61</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_position_encoding.signal', title='?'>signal</a>
<span class='lineno'>  62</span> 
<span class='lineno'>  63</span> 
<span class='lineno'>  64</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias', title='(?, ?) -> float'>get_decoder_self_attention_bias</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', title='?'>length</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', title='?'>dtype</a>=tf.float32):
<span class='lineno'>  65</span>   &quot;&quot;&quot;Calculate bias for decoder that maintains model&#39;s autoregressive property.
<span class='lineno'>  66</span> 
<span class='lineno'>  67</span>   Creates a tensor that masks out locations that correspond to illegal
<span class='lineno'>  68</span>   connections, so prediction at position i cannot draw information from future
<span class='lineno'>  69</span>   positions.
<span class='lineno'>  70</span> 
<span class='lineno'>  71</span>   Args:
<span class='lineno'>  72</span>     length: int length of sequences in batch.
<span class='lineno'>  73</span>     dtype: The dtype of the return value.
<span class='lineno'>  74</span> 
<span class='lineno'>  75</span>   Returns:
<span class='lineno'>  76</span>     float tensor of shape [1, 1, length, length]
<span class='lineno'>  77</span>   &quot;&quot;&quot;
<span class='lineno'>  78</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.neg_inf', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.neg_inf', title='float'>neg_inf</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP16', title='?'>_NEG_INF_FP16</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', title='?'>dtype</a> == tf.float16 else <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', title='float'>_NEG_INF_FP32</a>
<span class='lineno'>  79</span>   with tf.name_scope(&quot;decoder_self_attention_bias&quot;):
<span class='lineno'>  80</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', title='?'>valid_locs</a> = tf.linalg.band_part(tf.ones([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', title='?'>length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', title='?'>length</a>], dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.dtype', title='?'>dtype</a>),
<span class='lineno'>  81</span>                                      -1, 0)
<span class='lineno'>  82</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', title='?'>valid_locs</a> = tf.reshape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', title='?'>valid_locs</a>, [1, 1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', title='?'>length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.length', title='?'>length</a>])
<span class='lineno'>  83</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.decoder_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.decoder_bias', title='float'>decoder_bias</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.neg_inf', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.neg_inf', title='float'>neg_inf</a> * (1.0 - <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.valid_locs', title='?'>valid_locs</a>)
<span class='lineno'>  84</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.decoder_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_decoder_self_attention_bias.decoder_bias', title='float'>decoder_bias</a>
<span class='lineno'>  85</span> 
<span class='lineno'>  86</span> 
<span class='lineno'>  87</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding', title='(?, int, ?) -> None'>get_padding</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.x', title='?'>x</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.padding_value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.padding_value', title='int'>padding_value</a>=0, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.dtype', title='?'>dtype</a>=tf.float32):
<span class='lineno'>  88</span>   &quot;&quot;&quot;Return float tensor representing the padding values in x.
<span class='lineno'>  89</span> 
<span class='lineno'>  90</span>   Args:
<span class='lineno'>  91</span>     x: int tensor with any shape
<span class='lineno'>  92</span>     padding_value: int which represents padded values in input
<span class='lineno'>  93</span>     dtype: The dtype of the return value.
<span class='lineno'>  94</span> 
<span class='lineno'>  95</span>   Returns:
<span class='lineno'>  96</span>     float tensor with same shape as x containing values 0 or 1.
<span class='lineno'>  97</span>       0 -&gt; non-padding, 1 -&gt; padding
<span class='lineno'>  98</span>   &quot;&quot;&quot;
<span class='lineno'>  99</span>   with tf.name_scope(&quot;padding&quot;):
<span class='lineno'> 100</span>     return tf.cast(tf.equal(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.x', title='?'>x</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.padding_value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.padding_value', title='int'>padding_value</a>), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding.dtype', title='?'>dtype</a>)
<span class='lineno'> 101</span> 
<span class='lineno'> 102</span> 
<span class='lineno'> 103</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias', title='(?, int, ?) -> None'>get_padding_bias</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.x', title='?'>x</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding_value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding_value', title='int'>padding_value</a>=0, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.dtype', title='?'>dtype</a>=tf.float32):
<span class='lineno'> 104</span>   &quot;&quot;&quot;Calculate bias tensor from padding values in tensor.
<span class='lineno'> 105</span> 
<span class='lineno'> 106</span>   Bias tensor that is added to the pre-softmax multi-headed attention logits,
<span class='lineno'> 107</span>   which has shape [batch_size, num_heads, length, length]. The tensor is zero at
<span class='lineno'> 108</span>   non-padding locations, and -1e9 (negative infinity) at padding locations.
<span class='lineno'> 109</span> 
<span class='lineno'> 110</span>   Args:
<span class='lineno'> 111</span>     x: int tensor with shape [batch_size, length]
<span class='lineno'> 112</span>     padding_value: int which represents padded values in input
<span class='lineno'> 113</span>     dtype: The dtype of the return value
<span class='lineno'> 114</span> 
<span class='lineno'> 115</span>   Returns:
<span class='lineno'> 116</span>     Attention bias tensor of shape [batch_size, 1, 1, length].
<span class='lineno'> 117</span>   &quot;&quot;&quot;
<span class='lineno'> 118</span>   with tf.name_scope(&quot;attention_bias&quot;):
<span class='lineno'> 119</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding', title='None'>padding</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding', title='(?, int, ?) -> None'>get_padding</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.x', title='?'>x</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding_value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding_value', title='int'>padding_value</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.dtype', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.dtype', title='?'>dtype</a>)
<span class='lineno'> 120</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', title='?'>attention_bias</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.padding', title='None'>padding</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils._NEG_INF_FP32', title='float'>_NEG_INF_FP32</a>
<span class='lineno'> 121</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', title='?'>attention_bias</a> = tf.expand_dims(
<span class='lineno'> 122</span>         tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', title='?'>attention_bias</a>, axis=1), axis=1)
<span class='lineno'> 123</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.model_utils.get_padding_bias.attention_bias', title='?'>attention_bias</a>
</pre></td></tr></table></body></html>