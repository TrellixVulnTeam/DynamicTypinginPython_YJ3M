<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/transformer/transformer.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model'>create_model</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer'>Transformer</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call'>call</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode'>encode</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode'>decode</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn'>_get_symbols_to_logits_fn</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict'>predict</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper'>PrePostProcessingWrapper</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build'>build</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call'>call</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack'>EncoderStack</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build'>build</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call'>call</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack'>DecoderStack</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build'>build</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call'>call</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Defines the Transformer model in TF 2.0.
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> Model paper: https://arxiv.org/pdf/1706.03762.pdf
<span class='lineno'>  18</span> Transformer model code source: https://github.com/tensorflow/tensor2tensor
<span class='lineno'>  19</span> &quot;&quot;&quot;
<span class='lineno'>  20</span> from __future__ import absolute_import
<span class='lineno'>  21</span> from __future__ import division
<span class='lineno'>  22</span> from __future__ import print_function
<span class='lineno'>  23</span> 
<span class='lineno'>  24</span> import tensorflow as tf
<span class='lineno'>  25</span> from official.nlp.modeling.layers import position_embedding
<span class='lineno'>  26</span> from official.nlp.transformer import attention_layer
<span class='lineno'>  27</span> from official.nlp.transformer import beam_search
<span class='lineno'>  28</span> from official.nlp.transformer import embedding_layer
<span class='lineno'>  29</span> from official.nlp.transformer import ffn_layer
<span class='lineno'>  30</span> from official.nlp.transformer import metrics
<span class='lineno'>  31</span> from official.nlp.transformer import model_utils
<span class='lineno'>  32</span> from official.nlp.transformer.utils.tokenizer import EOS_ID
<span class='lineno'>  33</span> 
<span class='lineno'>  34</span> 
<span class='lineno'>  35</span> # Disable the not-callable lint error, since it claims many objects are not
<span class='lineno'>  36</span> # callable when they actually are.
<span class='lineno'>  37</span> # pylint: disable=not-callable
<span class='lineno'>  38</span> 
<span class='lineno'>  39</span> 
<span class='lineno'>  40</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model', title='(?, ?) -> None'>create_model</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', title='?'>is_train</a>):
<span class='lineno'>  41</span>   &quot;&quot;&quot;Creates transformer model.&quot;&quot;&quot;
<span class='lineno'>  42</span>   with tf.name_scope(&quot;model&quot;):
<span class='lineno'>  43</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', title='?'>is_train</a>:
<span class='lineno'>  44</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a> = tf.keras.layers.Input((None,), dtype=&quot;int64&quot;, name=&quot;inputs&quot;)
<span class='lineno'>  45</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', title='?'>targets</a> = tf.keras.layers.Input((None,), dtype=&quot;int64&quot;, name=&quot;targets&quot;)
<span class='lineno'>  46</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', title='Transformer'>internal_model</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', title='<Transformer>'>Transformer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>, name=&quot;transformer_v2&quot;)
<span class='lineno'>  47</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', title='Transformer'>internal_model</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', title='?'>targets</a>], training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', title='?'>is_train</a>)
<span class='lineno'>  48</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', title='?'>vocab_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>[&quot;vocab_size&quot;]
<span class='lineno'>  49</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.label_smoothing', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.label_smoothing', title='?'>label_smoothing</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>[&quot;label_smoothing&quot;]
<span class='lineno'>  50</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>[&quot;enable_metrics_in_training&quot;]:
<span class='lineno'>  51</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a> = metrics.MetricLayer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', title='?'>vocab_size</a>)([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', title='?'>targets</a>])
<span class='lineno'>  52</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a> = tf.keras.layers.Lambda(lambda <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.lambda%114.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.lambda%114.x', title='?'>x</a>: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.lambda%114.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.lambda%114.x', title='?'>x</a>, name=&quot;logits&quot;,
<span class='lineno'>  53</span>                                       dtype=tf.float32)(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a>)
<span class='lineno'>  54</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', title='?'>model</a> = tf.keras.Model([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', title='?'>targets</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a>)
<span class='lineno'>  55</span>       # TODO(reedwm): Can we do this loss in float16 instead of float32?
<span class='lineno'>  56</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.loss', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.loss', title='?'>loss</a> = metrics.transformer_loss(
<span class='lineno'>  57</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.logits', title='?'>logits</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.targets', title='?'>targets</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.label_smoothing', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.label_smoothing', title='?'>label_smoothing</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.vocab_size', title='?'>vocab_size</a>)
<span class='lineno'>  58</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', title='?'>model</a>.add_loss(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.loss', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.loss', title='?'>loss</a>)
<span class='lineno'>  59</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.model', title='?'>model</a>
<span class='lineno'>  60</span> 
<span class='lineno'>  61</span>     else:
<span class='lineno'>  62</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a> = tf.keras.layers.Input((None,), dtype=&quot;int64&quot;, name=&quot;inputs&quot;)
<span class='lineno'>  63</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', title='Transformer'>internal_model</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', title='<Transformer>'>Transformer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.params', title='?'>params</a>, name=&quot;transformer_v2&quot;)
<span class='lineno'>  64</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', title='?'>ret</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.internal_model', title='Transformer'>internal_model</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a>], training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.is_train', title='?'>is_train</a>)
<span class='lineno'>  65</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.outputs', title='?'>outputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.scores', title='?'>scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', title='?'>ret</a>[&quot;outputs&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.ret', title='?'>ret</a>[&quot;scores&quot;]
<span class='lineno'>  66</span>       return tf.keras.Model(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.inputs', title='?'>inputs</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.outputs', title='?'>outputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.create_model.scores', title='?'>scores</a>])
<span class='lineno'>  67</span> 
<span class='lineno'>  68</span> 
<span class='lineno'>  69</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', title='<Transformer>'>Transformer</a>(tf.keras.Model):
<span class='lineno'>  70</span>   &quot;&quot;&quot;Transformer model with Keras.
<span class='lineno'>  71</span> 
<span class='lineno'>  72</span>   Implemented as described in: https://arxiv.org/pdf/1706.03762.pdf
<span class='lineno'>  73</span> 
<span class='lineno'>  74</span>   The Transformer model consists of an encoder and decoder. The input is an int
<span class='lineno'>  75</span>   sequence (or a batch of sequences). The encoder produces a continuous
<span class='lineno'>  76</span>   representation, and the decoder uses the encoder output to generate
<span class='lineno'>  77</span>   probabilities for the output sequence.
<span class='lineno'>  78</span>   &quot;&quot;&quot;
<span class='lineno'>  79</span> 
<span class='lineno'>  80</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.name', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.name', title='None'>name</a>=None):
<span class='lineno'>  81</span>     &quot;&quot;&quot;Initialize layers to build Transformer model.
<span class='lineno'>  82</span> 
<span class='lineno'>  83</span>     Args:
<span class='lineno'>  84</span>       params: hyperparameter object defining layer sizes, dropout values, etc.
<span class='lineno'>  85</span>       name: name of the model.
<span class='lineno'>  86</span>     &quot;&quot;&quot;
<span class='lineno'>  87</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer', title='<Transformer>'>Transformer</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>).__init__(name=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.name', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.name', title='None'>name</a>)
<span class='lineno'>  88</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>
<span class='lineno'>  89</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a></a> = embedding_layer.EmbeddingSharedWeights(
<span class='lineno'>  90</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>[&quot;vocab_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>[&quot;hidden_size&quot;])
<span class='lineno'>  91</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', title='EncoderStack'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', title='EncoderStack'>encoder_stack</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', title='<EncoderStack>'>EncoderStack</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>)
<span class='lineno'>  92</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', title='DecoderStack'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', title='DecoderStack'>decoder_stack</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', title='<DecoderStack>'>DecoderStack</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.params', title='?'>params</a>)
<span class='lineno'>  93</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', title='?'>position_embedding</a></a> = position_embedding.RelativePositionEmbedding(
<span class='lineno'>  94</span>         hidden_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.__init__.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;hidden_size&quot;])
<span class='lineno'>  95</span> 
<span class='lineno'>  96</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config', title='Transformer -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config.self', title='Transformer'>self</a>):
<span class='lineno'>  97</span>     return {
<span class='lineno'>  98</span>         &quot;params&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.get_config.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>,
<span class='lineno'>  99</span>     }
<span class='lineno'> 100</span> 
<span class='lineno'> 101</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call', title='(Transformer, ?, ?) -> dict'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', title='?'>training</a>):
<span class='lineno'> 102</span>     &quot;&quot;&quot;Calculate target logits or inferred target sequences.
<span class='lineno'> 103</span> 
<span class='lineno'> 104</span>     Args:
<span class='lineno'> 105</span>       inputs: input tensor list of size 1 or 2.
<span class='lineno'> 106</span>         First item, inputs: int tensor with shape [batch_size, input_length].
<span class='lineno'> 107</span>         Second item (optional), targets: None or int tensor with shape
<span class='lineno'> 108</span>           [batch_size, target_length].
<span class='lineno'> 109</span>       training: boolean, whether in training mode or not.
<span class='lineno'> 110</span> 
<span class='lineno'> 111</span>     Returns:
<span class='lineno'> 112</span>       If targets is defined, then return logits for each word in the target
<span class='lineno'> 113</span>       sequence. float tensor with shape [batch_size, target_length, vocab_size]
<span class='lineno'> 114</span>       If target is none, then generate output sequence one token at a time.
<span class='lineno'> 115</span>         returns a dictionary {
<span class='lineno'> 116</span>           outputs: [batch_size, decoded length]
<span class='lineno'> 117</span>           scores: [batch_size, float]}
<span class='lineno'> 118</span>       Even when float16 is used, the output tensor(s) are always float32.
<span class='lineno'> 119</span> 
<span class='lineno'> 120</span>     Raises:
<span class='lineno'> 121</span>       NotImplementedError: If try to use padded decode method on CPU/GPUs.
<span class='lineno'> 122</span>     &quot;&quot;&quot;
<span class='lineno'> 123</span>     if len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>) == 2:
<span class='lineno'> 124</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', title='?'>targets</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>[1]
<span class='lineno'> 125</span>     else:
<span class='lineno'> 126</span>       # Decoding path.
<span class='lineno'> 127</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', title='None'>targets</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>[0], None
<span class='lineno'> 128</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;]:
<span class='lineno'> 129</span>         if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;num_replicas&quot;]:
<span class='lineno'> 130</span>           raise NotImplementedError(
<span class='lineno'> 131</span>               &quot;Padded decoding on CPU/GPUs is not supported.&quot;)
<span class='lineno'> 132</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.decode_batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.decode_batch_size', title='int'>decode_batch_size</a> = int(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;decode_batch_size&quot;] /
<span class='lineno'> 133</span>                                 <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;num_replicas&quot;])
<span class='lineno'> 134</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>.set_shape([
<span class='lineno'> 135</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.decode_batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.decode_batch_size', title='int'>decode_batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;decode_max_length&quot;]
<span class='lineno'> 136</span>         ])
<span class='lineno'> 137</span> 
<span class='lineno'> 138</span>     # Variance scaling is used here because it seems to work in many problems.
<span class='lineno'> 139</span>     # Other reasonable initializers may also work just as well.
<span class='lineno'> 140</span>     with tf.name_scope(&quot;Transformer&quot;):
<span class='lineno'> 141</span>       # Calculate attention bias for encoder self-attention and decoder
<span class='lineno'> 142</span>       # multi-headed attention layers.
<span class='lineno'> 143</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', title='?'>attention_bias</a> = model_utils.get_padding_bias(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>)
<span class='lineno'> 144</span> 
<span class='lineno'> 145</span>       # Run the inputs through the encoder layer to map the symbol
<span class='lineno'> 146</span>       # representations to continuous representations.
<span class='lineno'> 147</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', title='None'>encoder_outputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode', title='(Transformer, ?, ?, ?) -> None'>encode</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.inputs', title='?'>inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', title='?'>training</a>)
<span class='lineno'> 148</span>       # Generate output sequence if targets is None, or return logits if target
<span class='lineno'> 149</span>       # sequence is known.
<span class='lineno'> 150</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', title='None'>targets</a> is None:
<span class='lineno'> 151</span>         return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict', title='(Transformer, None, ?, ?) -> dict / (Transformer, ?, ?, ?) -> dict'>predict</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', title='None'>encoder_outputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', title='?'>training</a>)
<span class='lineno'> 152</span>       else:
<span class='lineno'> 153</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.logits', title='None'>logits</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode', title='(Transformer, None, None, ?, ?) -> None / (Transformer, ?, ?, ?, ?) -> None'>decode</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.targets', title='None'>targets</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.encoder_outputs', title='None'>encoder_outputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.training', title='?'>training</a>)
<span class='lineno'> 154</span>         return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.call.logits', title='None'>logits</a>
<span class='lineno'> 155</span> 
<span class='lineno'> 156</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode', title='(Transformer, ?, ?, ?) -> None'>encode</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', title='?'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', title='?'>attention_bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', title='?'>training</a>):
<span class='lineno'> 157</span>     &quot;&quot;&quot;Generate continuous representation for inputs.
<span class='lineno'> 158</span> 
<span class='lineno'> 159</span>     Args:
<span class='lineno'> 160</span>       inputs: int tensor with shape [batch_size, input_length].
<span class='lineno'> 161</span>       attention_bias: float tensor with shape [batch_size, 1, 1, input_length].
<span class='lineno'> 162</span>       training: boolean, whether in training mode or not.
<span class='lineno'> 163</span> 
<span class='lineno'> 164</span>     Returns:
<span class='lineno'> 165</span>       float tensor with shape [batch_size, input_length, hidden_size]
<span class='lineno'> 166</span>     &quot;&quot;&quot;
<span class='lineno'> 167</span>     with tf.name_scope(&quot;encode&quot;):
<span class='lineno'> 168</span>       # Prepare inputs to the layer stack by adding positional encodings and
<span class='lineno'> 169</span>       # applying dropout.
<span class='lineno'> 170</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', title='?'>embedded_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', title='?'>inputs</a>)
<span class='lineno'> 171</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', title='?'>embedded_inputs</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', title='?'>embedded_inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 172</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs_padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs_padding', title='?'>inputs_padding</a> = model_utils.get_padding(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs', title='?'>inputs</a>)
<span class='lineno'> 173</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', title='?'>attention_bias</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 174</span> 
<span class='lineno'> 175</span>       with tf.name_scope(&quot;add_pos_encoding&quot;):
<span class='lineno'> 176</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', title='?'>pos_encoding</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', title='?'>position_embedding</a>(inputs=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', title='?'>embedded_inputs</a>)
<span class='lineno'> 177</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', title='?'>pos_encoding</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', title='?'>pos_encoding</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 178</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', title='?'>encoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.embedded_inputs', title='?'>embedded_inputs</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.pos_encoding', title='?'>pos_encoding</a>
<span class='lineno'> 179</span> 
<span class='lineno'> 180</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', title='?'>training</a>:
<span class='lineno'> 181</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', title='?'>encoder_inputs</a> = tf.nn.dropout(
<span class='lineno'> 182</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', title='?'>encoder_inputs</a>, rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;layer_postprocess_dropout&quot;])
<span class='lineno'> 183</span> 
<span class='lineno'> 184</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encoder_stack', title='EncoderStack'>encoder_stack</a>(
<span class='lineno'> 185</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.encoder_inputs', title='?'>encoder_inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs_padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.inputs_padding', title='?'>inputs_padding</a>, training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.encode.training', title='?'>training</a>)
<span class='lineno'> 186</span> 
<span class='lineno'> 187</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode', title='(Transformer, None, None, ?, ?) -> None / (Transformer, ?, ?, ?, ?) -> None'>decode</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.targets', title='None'>targets</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.encoder_outputs', title='None'>encoder_outputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', title='?'>attention_bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', title='?'>training</a>):
<span class='lineno'> 188</span>     &quot;&quot;&quot;Generate logits for each value in the target sequence.
<span class='lineno'> 189</span> 
<span class='lineno'> 190</span>     Args:
<span class='lineno'> 191</span>       targets: target values for the output sequence. int tensor with shape
<span class='lineno'> 192</span>         [batch_size, target_length]
<span class='lineno'> 193</span>       encoder_outputs: continuous representation of input sequence. float tensor
<span class='lineno'> 194</span>         with shape [batch_size, input_length, hidden_size]
<span class='lineno'> 195</span>       attention_bias: float tensor with shape [batch_size, 1, 1, input_length]
<span class='lineno'> 196</span>       training: boolean, whether in training mode or not.
<span class='lineno'> 197</span> 
<span class='lineno'> 198</span>     Returns:
<span class='lineno'> 199</span>       float32 tensor with shape [batch_size, target_length, vocab_size]
<span class='lineno'> 200</span>     &quot;&quot;&quot;
<span class='lineno'> 201</span>     with tf.name_scope(&quot;decode&quot;):
<span class='lineno'> 202</span>       # Prepare inputs to decoder layers by shifting targets, adding positional
<span class='lineno'> 203</span>       # encoding and applying dropout.
<span class='lineno'> 204</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.targets', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.targets', title='None'>targets</a>)
<span class='lineno'> 205</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 206</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', title='?'>attention_bias</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', title='?'>attention_bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 207</span>       with tf.name_scope(&quot;shift_targets&quot;):
<span class='lineno'> 208</span>         # Shift targets to the right, and remove the last element
<span class='lineno'> 209</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a> = tf.pad(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>,
<span class='lineno'> 210</span>                                 [[0, 0], [1, 0], [0, 0]])[:, :-1, :]
<span class='lineno'> 211</span>       with tf.name_scope(&quot;add_pos_encoding&quot;):
<span class='lineno'> 212</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.length', title='?'>length</a> = tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>)[1]
<span class='lineno'> 213</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', title='?'>pos_encoding</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', title='?'>position_embedding</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>)
<span class='lineno'> 214</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', title='?'>pos_encoding</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', title='?'>pos_encoding</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 215</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.pos_encoding', title='?'>pos_encoding</a>
<span class='lineno'> 216</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', title='?'>training</a>:
<span class='lineno'> 217</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a> = tf.nn.dropout(
<span class='lineno'> 218</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>, rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;layer_postprocess_dropout&quot;])
<span class='lineno'> 219</span> 
<span class='lineno'> 220</span>       # Run values
<span class='lineno'> 221</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a> = model_utils.get_decoder_self_attention_bias(
<span class='lineno'> 222</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.length', title='?'>length</a>, dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 223</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.outputs', title='?'>outputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', title='DecoderStack'>decoder_stack</a>(
<span class='lineno'> 224</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_inputs', title='?'>decoder_inputs</a>,
<span class='lineno'> 225</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.encoder_outputs', title='None'>encoder_outputs</a>,
<span class='lineno'> 226</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>,
<span class='lineno'> 227</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.attention_bias', title='?'>attention_bias</a>,
<span class='lineno'> 228</span>           training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.training', title='?'>training</a>)
<span class='lineno'> 229</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', title='?'>logits</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.outputs', title='?'>outputs</a>, mode=&quot;linear&quot;)
<span class='lineno'> 230</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', title='?'>logits</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', title='?'>logits</a>, tf.float32)
<span class='lineno'> 231</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decode.logits', title='?'>logits</a>
<span class='lineno'> 232</span> 
<span class='lineno'> 233</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn', title='(Transformer, ?, ?) -> (?, ?, ?) -> (?, ?)'>_get_symbols_to_logits_fn</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', title='?'>max_decode_length</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.training', title='?'>training</a>):
<span class='lineno'> 234</span>     &quot;&quot;&quot;Returns a decoding function that calculates logits of the next tokens.&quot;&quot;&quot;
<span class='lineno'> 235</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.position_embedding', title='?'>position_embedding</a>(
<span class='lineno'> 236</span>         inputs=None, length=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', title='?'>max_decode_length</a> + 1)
<span class='lineno'> 237</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 238</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a> = model_utils.get_decoder_self_attention_bias(
<span class='lineno'> 239</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.max_decode_length', title='?'>max_decode_length</a>, dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 240</span> 
<span class='lineno'> 241</span>     # TODO(b/139770046): Refactor code with better naming of i.
<span class='lineno'> 242</span>     def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn', title='(?, ?, ?) -> (?, ?)'>symbols_to_logits_fn</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.ids', title='?'>ids</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', title='?'>cache</a>):
<span class='lineno'> 243</span>       &quot;&quot;&quot;Generate logits for next potential IDs.
<span class='lineno'> 244</span> 
<span class='lineno'> 245</span>       Args:
<span class='lineno'> 246</span>         ids: Current decoded sequences. int tensor with shape [batch_size *
<span class='lineno'> 247</span>           beam_size, i + 1].
<span class='lineno'> 248</span>         i: Loop index.
<span class='lineno'> 249</span>         cache: dictionary of values storing the encoder output, encoder-decoder
<span class='lineno'> 250</span>           attention bias, and previous decoder attention values.
<span class='lineno'> 251</span> 
<span class='lineno'> 252</span>       Returns:
<span class='lineno'> 253</span>         Tuple of
<span class='lineno'> 254</span>           (logits with shape [batch_size * beam_size, vocab_size],
<span class='lineno'> 255</span>            updated cache values)
<span class='lineno'> 256</span>       &quot;&quot;&quot;
<span class='lineno'> 257</span>       # Set decoder input to the last generated IDs
<span class='lineno'> 258</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.ids', title='?'>ids</a>[:, -1:]
<span class='lineno'> 259</span> 
<span class='lineno'> 260</span>       # Preprocess decoder input by getting embeddings and adding timing signal.
<span class='lineno'> 261</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a>)
<span class='lineno'> 262</span> 
<span class='lineno'> 263</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;]:
<span class='lineno'> 264</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.timing_signal_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.timing_signal_shape', title='?'>timing_signal_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a>.shape.as_list()
<span class='lineno'> 265</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a></a> += tf.slice(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a>, [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a>, 0],
<span class='lineno'> 266</span>                                   [1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.timing_signal_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.timing_signal_shape', title='?'>timing_signal_shape</a>[1]])
<span class='lineno'> 267</span> 
<span class='lineno'> 268</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', title='?'>bias_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>.shape.as_list()
<span class='lineno'> 269</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', title='?'>self_attention_bias</a> = tf.slice(
<span class='lineno'> 270</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>, [0, 0, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a>, 0],
<span class='lineno'> 271</span>             [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', title='?'>bias_shape</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', title='?'>bias_shape</a>[1], 1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.bias_shape', title='?'>bias_shape</a>[3]])
<span class='lineno'> 272</span>       else:
<span class='lineno'> 273</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.timing_signal', title='?'>timing_signal</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a>:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a> + 1]
<span class='lineno'> 274</span> 
<span class='lineno'> 275</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', title='?'>self_attention_bias</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>[:, :, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a>:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a> + 1, :<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a> + 1]
<span class='lineno'> 276</span> 
<span class='lineno'> 277</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_outputs', title='?'>decoder_outputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.decoder_stack', title='DecoderStack'>decoder_stack</a>(
<span class='lineno'> 278</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_input', title='?'>decoder_input</a>,
<span class='lineno'> 279</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', title='?'>cache</a>.get(&quot;encoder_outputs&quot;),
<span class='lineno'> 280</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.self_attention_bias', title='?'>self_attention_bias</a>,
<span class='lineno'> 281</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', title='?'>cache</a>.get(&quot;encoder_decoder_attention_bias&quot;),
<span class='lineno'> 282</span>           training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.training', title='?'>training</a>,
<span class='lineno'> 283</span>           cache=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', title='?'>cache</a>,
<span class='lineno'> 284</span>           decode_loop_step=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.i', title='?'>i</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;] else None)
<span class='lineno'> 285</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', title='?'>logits</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.embedding_softmax_layer', title='?'>embedding_softmax_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.decoder_outputs', title='?'>decoder_outputs</a>, mode=&quot;linear&quot;)
<span class='lineno'> 286</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', title='?'>logits</a> = tf.squeeze(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', title='?'>logits</a>, axis=[1])
<span class='lineno'> 287</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.logits', title='?'>logits</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn.cache', title='?'>cache</a>
<span class='lineno'> 288</span> 
<span class='lineno'> 289</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn.symbols_to_logits_fn', title='(?, ?, ?) -> (?, ?)'>symbols_to_logits_fn</a>
<span class='lineno'> 290</span> 
<span class='lineno'> 291</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict', title='(Transformer, None, ?, ?) -> dict / (Transformer, ?, ?, ?) -> dict'>predict</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='None'>encoder_outputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', title='?'>encoder_decoder_attention_bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.training', title='?'>training</a>):
<span class='lineno'> 292</span>     &quot;&quot;&quot;Return predicted sequence.&quot;&quot;&quot;
<span class='lineno'> 293</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='None'>encoder_outputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 294</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;]:
<span class='lineno'> 295</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', title='?'>batch_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a>.shape.as_list()[0]
<span class='lineno'> 296</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', title='?'>input_length</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a>.shape.as_list()[1]
<span class='lineno'> 297</span>     else:
<span class='lineno'> 298</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', title='?'>batch_size</a> = tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a>)[0]
<span class='lineno'> 299</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', title='?'>input_length</a> = tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a>)[1]
<span class='lineno'> 300</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', title='?'>max_decode_length</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.input_length', title='?'>input_length</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;extra_decode_length&quot;]
<span class='lineno'> 301</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', title='?'>encoder_decoder_attention_bias</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', title='?'>encoder_decoder_attention_bias</a>,
<span class='lineno'> 302</span>                                              <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 303</span> 
<span class='lineno'> 304</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.symbols_to_logits_fn', title='(?, ?, ?) -> (?, ?)'>symbols_to_logits_fn</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer._get_symbols_to_logits_fn', title='(Transformer, ?, ?) -> (?, ?, ?) -> (?, ?)'>_get_symbols_to_logits_fn</a>(
<span class='lineno'> 305</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', title='?'>max_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.training', title='?'>training</a>)
<span class='lineno'> 306</span> 
<span class='lineno'> 307</span>     # Create initial set of IDs that will be passed into symbols_to_logits_fn.
<span class='lineno'> 308</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.initial_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.initial_ids', title='?'>initial_ids</a> = tf.zeros([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', title='?'>batch_size</a>], dtype=tf.int32)
<span class='lineno'> 309</span> 
<span class='lineno'> 310</span>     # Create cache storing decoder attention values for each layer.
<span class='lineno'> 311</span>     # pylint: disable=g-complex-comprehension
<span class='lineno'> 312</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', title='int'>init_decode_length</a> = (
<span class='lineno'> 313</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', title='?'>max_decode_length</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;] else 0)
<span class='lineno'> 314</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', title='?'>num_heads</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;num_heads&quot;]
<span class='lineno'> 315</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', title='?'>dim_per_head</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;hidden_size&quot;] // <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', title='?'>num_heads</a>
<span class='lineno'> 316</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', title='dict'>cache</a> = {
<span class='lineno'> 317</span>         &quot;layer_%d&quot; % <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', title='int'>layer</a>: {
<span class='lineno'> 318</span>             &quot;k&quot;:
<span class='lineno'> 319</span>                 tf.zeros([
<span class='lineno'> 320</span>                     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', title='?'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', title='?'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', title='?'>dim_per_head</a>
<span class='lineno'> 321</span>                 ],
<span class='lineno'> 322</span>                          dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;]),
<span class='lineno'> 323</span>             &quot;v&quot;:
<span class='lineno'> 324</span>                 tf.zeros([
<span class='lineno'> 325</span>                     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.batch_size', title='?'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.num_heads', title='?'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.dim_per_head', title='?'>dim_per_head</a>
<span class='lineno'> 326</span>                 ],
<span class='lineno'> 327</span>                          dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 328</span>         } for <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', title='int'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.layer', title='int'>layer</a></a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;num_hidden_layers&quot;])
<span class='lineno'> 329</span>     }
<span class='lineno'> 330</span>     # pylint: enable=g-complex-comprehension
<span class='lineno'> 331</span> 
<span class='lineno'> 332</span>     # Add encoder output and attention bias to the cache.
<span class='lineno'> 333</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', title='dict'>cache</a>[&quot;encoder_outputs&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_outputs', title='?'>encoder_outputs</a>
<span class='lineno'> 334</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', title='dict'>cache</a>[&quot;encoder_decoder_attention_bias&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.encoder_decoder_attention_bias', title='?'>encoder_decoder_attention_bias</a>
<span class='lineno'> 335</span> 
<span class='lineno'> 336</span>     # Use beam search to find the top beam_size sequences and scores.
<span class='lineno'> 337</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.decoded_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.decoded_ids', title='?'>decoded_ids</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.scores', title='?'>scores</a> = beam_search.sequence_beam_search(
<span class='lineno'> 338</span>         symbols_to_logits_fn=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.symbols_to_logits_fn', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.symbols_to_logits_fn', title='(?, ?, ?) -> (?, ?)'>symbols_to_logits_fn</a>,
<span class='lineno'> 339</span>         initial_ids=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.initial_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.initial_ids', title='?'>initial_ids</a>,
<span class='lineno'> 340</span>         initial_cache=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.cache', title='dict'>cache</a>,
<span class='lineno'> 341</span>         vocab_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;vocab_size&quot;],
<span class='lineno'> 342</span>         beam_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;beam_size&quot;],
<span class='lineno'> 343</span>         alpha=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;alpha&quot;],
<span class='lineno'> 344</span>         max_decode_length=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.max_decode_length', title='?'>max_decode_length</a>,
<span class='lineno'> 345</span>         eos_id=EOS_ID,
<span class='lineno'> 346</span>         padded_decode=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;padded_decode&quot;],
<span class='lineno'> 347</span>         dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.self', title='Transformer'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.params', title='?'>params</a>[&quot;dtype&quot;])
<span class='lineno'> 348</span> 
<span class='lineno'> 349</span>     # Get the top sequence for each batch element
<span class='lineno'> 350</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_decoded_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_decoded_ids', title='?'>top_decoded_ids</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.decoded_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.decoded_ids', title='?'>decoded_ids</a>[:, 0, 1:]
<span class='lineno'> 351</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_scores', title='?'>top_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.scores', title='?'>scores</a>[:, 0]
<span class='lineno'> 352</span> 
<span class='lineno'> 353</span>     return {&quot;outputs&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_decoded_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_decoded_ids', title='?'>top_decoded_ids</a>, &quot;scores&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.Transformer.predict.top_scores', title='?'>top_scores</a>}
<span class='lineno'> 354</span> 
<span class='lineno'> 355</span> 
<span class='lineno'> 356</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(tf.keras.layers.Layer):
<span class='lineno'> 357</span>   &quot;&quot;&quot;Wrapper class that applies layer pre-processing and post-processing.&quot;&quot;&quot;
<span class='lineno'> 358</span> 
<span class='lineno'> 359</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', title='PrePostProcessingWrapper'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.layer', title='?'>layer</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', title='?'>params</a>):
<span class='lineno'> 360</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', title='PrePostProcessingWrapper'>self</a>).__init__()
<span class='lineno'> 361</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', title='PrePostProcessingWrapper'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', title='?'>layer</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.layer', title='?'>layer</a>
<span class='lineno'> 362</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', title='PrePostProcessingWrapper'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', title='?'>params</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', title='?'>params</a>
<span class='lineno'> 363</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.self', title='PrePostProcessingWrapper'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', title='?'>postprocess_dropout</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.__init__.params', title='?'>params</a>[&quot;layer_postprocess_dropout&quot;]
<span class='lineno'> 364</span> 
<span class='lineno'> 365</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build', title='(PrePostProcessingWrapper, ?) -> None'>build</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', title='PrePostProcessingWrapper'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.input_shape', title='?'>input_shape</a>):
<span class='lineno'> 366</span>     # Create normalization layer
<span class='lineno'> 367</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', title='PrePostProcessingWrapper'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer_norm', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer_norm', title='?'>layer_norm</a> = tf.keras.layers.LayerNormalization(
<span class='lineno'> 368</span>         epsilon=1e-6, dtype=&quot;float32&quot;)
<span class='lineno'> 369</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.self', title='PrePostProcessingWrapper'>self</a>).build(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 370</span> 
<span class='lineno'> 371</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config', title='PrePostProcessingWrapper -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config.self', title='PrePostProcessingWrapper'>self</a>):
<span class='lineno'> 372</span>     return {
<span class='lineno'> 373</span>         &quot;params&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.get_config.self', title='PrePostProcessingWrapper'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.params', title='?'>params</a>,
<span class='lineno'> 374</span>     }
<span class='lineno'> 375</span> 
<span class='lineno'> 376</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call', title='(PrePostProcessingWrapper, ?) -> None'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', title='PrePostProcessingWrapper'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', title='?'>x</a>, *args, **kwargs):
<span class='lineno'> 377</span>     &quot;&quot;&quot;Calls wrapped layer with same parameters.&quot;&quot;&quot;
<span class='lineno'> 378</span>     # Preprocessing: apply layer normalization
<span class='lineno'> 379</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.training', title='?'>training</a> = kwargs[&quot;training&quot;]
<span class='lineno'> 380</span> 
<span class='lineno'> 381</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', title='PrePostProcessingWrapper'>self</a>.layer_norm(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', title='?'>x</a>)
<span class='lineno'> 382</span> 
<span class='lineno'> 383</span>     # Get layer output
<span class='lineno'> 384</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', title='PrePostProcessingWrapper'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.layer', title='?'>layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a>, *args, **kwargs)
<span class='lineno'> 385</span> 
<span class='lineno'> 386</span>     # Postprocessing: apply dropout and residual connection
<span class='lineno'> 387</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.training', title='?'>training</a>:
<span class='lineno'> 388</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a> = tf.nn.dropout(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a>, rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.self', title='PrePostProcessingWrapper'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.postprocess_dropout', title='?'>postprocess_dropout</a>)
<span class='lineno'> 389</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.x', title='?'>x</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper.call.y', title='?'>y</a>
<span class='lineno'> 390</span> 
<span class='lineno'> 391</span> 
<span class='lineno'> 392</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', title='<EncoderStack>'>EncoderStack</a>(tf.keras.layers.Layer):
<span class='lineno'> 393</span>   &quot;&quot;&quot;Transformer encoder stack.
<span class='lineno'> 394</span> 
<span class='lineno'> 395</span>   The encoder stack is made up of N identical layers. Each layer is composed
<span class='lineno'> 396</span>   of the sublayers:
<span class='lineno'> 397</span>     1. Self-attention layer
<span class='lineno'> 398</span>     2. Feedforward network (which is 2 fully-connected layers)
<span class='lineno'> 399</span>   &quot;&quot;&quot;
<span class='lineno'> 400</span> 
<span class='lineno'> 401</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', title='EncoderStack'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.params', title='None'>params</a>):
<span class='lineno'> 402</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', title='<EncoderStack>'>EncoderStack</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', title='EncoderStack'>self</a>).__init__()
<span class='lineno'> 403</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', title='?'>params</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.params', title='?'>params</a>
<span class='lineno'> 404</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.__init__.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'>layers</a></a> = []
<span class='lineno'> 405</span> 
<span class='lineno'> 406</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build', title='(EncoderStack, ?) -> None'>build</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', title='EncoderStack'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.input_shape', title='?'>input_shape</a>):
<span class='lineno'> 407</span>     &quot;&quot;&quot;Builds the encoder stack.&quot;&quot;&quot;
<span class='lineno'> 408</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', title='?'>params</a>
<span class='lineno'> 409</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build._', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build._', title='int'>_</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;num_hidden_layers&quot;]):
<span class='lineno'> 410</span>       # Create sublayers for each layer.
<span class='lineno'> 411</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self_attention_layer', title='?'>self_attention_layer</a> = attention_layer.SelfAttention(
<span class='lineno'> 412</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;hidden_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;num_heads&quot;],
<span class='lineno'> 413</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;attention_dropout&quot;])
<span class='lineno'> 414</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.feed_forward_network', title='?'>feed_forward_network</a> = ffn_layer.FeedForwardNetwork(
<span class='lineno'> 415</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;hidden_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;filter_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>[&quot;relu_dropout&quot;])
<span class='lineno'> 416</span> 
<span class='lineno'> 417</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'>layers</a>.append([
<span class='lineno'> 418</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self_attention_layer', title='?'>self_attention_layer</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>),
<span class='lineno'> 419</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.feed_forward_network', title='?'>feed_forward_network</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.params', title='?'>params</a>)
<span class='lineno'> 420</span>       ])
<span class='lineno'> 421</span> 
<span class='lineno'> 422</span>     # Create final layer normalization layer.
<span class='lineno'> 423</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', title='EncoderStack'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.output_normalization', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.output_normalization', title='?'>output_normalization</a> = tf.keras.layers.LayerNormalization(
<span class='lineno'> 424</span>         epsilon=1e-6, dtype=&quot;float32&quot;)
<span class='lineno'> 425</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack', title='<EncoderStack>'>EncoderStack</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.self', title='EncoderStack'>self</a>).build(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 426</span> 
<span class='lineno'> 427</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config', title='EncoderStack -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config.self', title='EncoderStack'>self</a>):
<span class='lineno'> 428</span>     return {
<span class='lineno'> 429</span>         &quot;params&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.get_config.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.params', title='?'>params</a>,
<span class='lineno'> 430</span>     }
<span class='lineno'> 431</span> 
<span class='lineno'> 432</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call', title='(EncoderStack, ?, ?, ?, ?) -> None'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', title='EncoderStack'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.attention_bias', title='?'>attention_bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.inputs_padding', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.inputs_padding', title='?'>inputs_padding</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', title='?'>training</a>):
<span class='lineno'> 433</span>     &quot;&quot;&quot;Return the output of the encoder layer stacks.
<span class='lineno'> 434</span> 
<span class='lineno'> 435</span>     Args:
<span class='lineno'> 436</span>       encoder_inputs: tensor with shape [batch_size, input_length, hidden_size]
<span class='lineno'> 437</span>       attention_bias: bias for the encoder self-attention layer. [batch_size, 1,
<span class='lineno'> 438</span>         1, input_length]
<span class='lineno'> 439</span>       inputs_padding: tensor with shape [batch_size, input_length], inputs with
<span class='lineno'> 440</span>         zero paddings.
<span class='lineno'> 441</span>       training: boolean, whether in training mode or not.
<span class='lineno'> 442</span> 
<span class='lineno'> 443</span>     Returns:
<span class='lineno'> 444</span>       Output of encoder layer stack.
<span class='lineno'> 445</span>       float32 tensor with shape [batch_size, input_length, hidden_size]
<span class='lineno'> 446</span>     &quot;&quot;&quot;
<span class='lineno'> 447</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.n', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.n', title='?'>n</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', title='?'>layer</a> in enumerate(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', title='EncoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.layers', title='[?]'>layers</a>):
<span class='lineno'> 448</span>       # Run inputs through the sublayers.
<span class='lineno'> 449</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self_attention_layer', title='?'>self_attention_layer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', title='?'>layer</a>[0]
<span class='lineno'> 450</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.feed_forward_network', title='?'>feed_forward_network</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.layer', title='?'>layer</a>[1]
<span class='lineno'> 451</span> 
<span class='lineno'> 452</span>       with tf.name_scope(&quot;layer_%d&quot; % <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.n', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.n', title='?'>n</a>):
<span class='lineno'> 453</span>         with tf.name_scope(&quot;self_attention&quot;):
<span class='lineno'> 454</span>           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self_attention_layer', title='?'>self_attention_layer</a>(
<span class='lineno'> 455</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.attention_bias', title='?'>attention_bias</a>, training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', title='?'>training</a>)
<span class='lineno'> 456</span>         with tf.name_scope(&quot;ffn&quot;):
<span class='lineno'> 457</span>           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.feed_forward_network', title='?'>feed_forward_network</a>(
<span class='lineno'> 458</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a>, training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.training', title='?'>training</a>)
<span class='lineno'> 459</span> 
<span class='lineno'> 460</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.self', title='EncoderStack'>self</a>.output_normalization(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.EncoderStack.call.encoder_inputs', title='?'>encoder_inputs</a>)
<span class='lineno'> 461</span> 
<span class='lineno'> 462</span> 
<span class='lineno'> 463</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', title='<DecoderStack>'>DecoderStack</a>(tf.keras.layers.Layer):
<span class='lineno'> 464</span>   &quot;&quot;&quot;Transformer decoder stack.
<span class='lineno'> 465</span> 
<span class='lineno'> 466</span>   Like the encoder stack, the decoder stack is made up of N identical layers.
<span class='lineno'> 467</span>   Each layer is composed of the sublayers:
<span class='lineno'> 468</span>     1. Self-attention layer
<span class='lineno'> 469</span>     2. Multi-headed attention layer combining encoder outputs with results from
<span class='lineno'> 470</span>        the previous self-attention layer.
<span class='lineno'> 471</span>     3. Feedforward network (2 fully-connected layers)
<span class='lineno'> 472</span>   &quot;&quot;&quot;
<span class='lineno'> 473</span> 
<span class='lineno'> 474</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', title='DecoderStack'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.params', title='?'>params</a>):
<span class='lineno'> 475</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', title='<DecoderStack>'>DecoderStack</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', title='DecoderStack'>self</a>).__init__()
<span class='lineno'> 476</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', title='DecoderStack'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', title='?'>params</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.params', title='?'>params</a>
<span class='lineno'> 477</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.__init__.self', title='DecoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'>layers</a></a> = []
<span class='lineno'> 478</span> 
<span class='lineno'> 479</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build', title='(DecoderStack, ?) -> None'>build</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', title='DecoderStack'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.input_shape', title='?'>input_shape</a>):
<span class='lineno'> 480</span>     &quot;&quot;&quot;Builds the decoder stack.&quot;&quot;&quot;
<span class='lineno'> 481</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', title='DecoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', title='?'>params</a>
<span class='lineno'> 482</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build._', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build._', title='int'>_</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;num_hidden_layers&quot;]):
<span class='lineno'> 483</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self_attention_layer', title='?'>self_attention_layer</a> = attention_layer.SelfAttention(
<span class='lineno'> 484</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;hidden_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;num_heads&quot;],
<span class='lineno'> 485</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;attention_dropout&quot;])
<span class='lineno'> 486</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.enc_dec_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.enc_dec_attention_layer', title='?'>enc_dec_attention_layer</a> = attention_layer.Attention(
<span class='lineno'> 487</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;hidden_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;num_heads&quot;],
<span class='lineno'> 488</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;attention_dropout&quot;])
<span class='lineno'> 489</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.feed_forward_network', title='?'>feed_forward_network</a> = ffn_layer.FeedForwardNetwork(
<span class='lineno'> 490</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;hidden_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;filter_size&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>[&quot;relu_dropout&quot;])
<span class='lineno'> 491</span> 
<span class='lineno'> 492</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', title='DecoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', title='{[?] | [[PrePostProcessingWrapper]]}'>layers</a>.append([
<span class='lineno'> 493</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self_attention_layer', title='?'>self_attention_layer</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>),
<span class='lineno'> 494</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.enc_dec_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.enc_dec_attention_layer', title='?'>enc_dec_attention_layer</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>),
<span class='lineno'> 495</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.PrePostProcessingWrapper', title='<PrePostProcessingWrapper>'>PrePostProcessingWrapper</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.feed_forward_network', title='?'>feed_forward_network</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.params', title='?'>params</a>)
<span class='lineno'> 496</span>       ])
<span class='lineno'> 497</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', title='DecoderStack'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.output_normalization', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.output_normalization', title='?'>output_normalization</a> = tf.keras.layers.LayerNormalization(
<span class='lineno'> 498</span>         epsilon=1e-6, dtype=&quot;float32&quot;)
<span class='lineno'> 499</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack', title='<DecoderStack>'>DecoderStack</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.self', title='DecoderStack'>self</a>).build(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 500</span> 
<span class='lineno'> 501</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config', title='DecoderStack -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config.self', title='DecoderStack'>self</a>):
<span class='lineno'> 502</span>     return {
<span class='lineno'> 503</span>         &quot;params&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.get_config.self', title='DecoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.params', title='?'>params</a>,
<span class='lineno'> 504</span>     }
<span class='lineno'> 505</span> 
<span class='lineno'> 506</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call', title='(DecoderStack, ?, ?, ?, ?, ?, None, None) -> None'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', title='DecoderStack'>self</a>,
<span class='lineno'> 507</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a>,
<span class='lineno'> 508</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.encoder_outputs', title='?'>encoder_outputs</a>,
<span class='lineno'> 509</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>,
<span class='lineno'> 510</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.attention_bias', title='?'>attention_bias</a>,
<span class='lineno'> 511</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', title='?'>training</a>,
<span class='lineno'> 512</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', title='None'>cache</a>=None,
<span class='lineno'> 513</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decode_loop_step', title='None'>decode_loop_step</a>=None):
<span class='lineno'> 514</span>     &quot;&quot;&quot;Return the output of the decoder layer stacks.
<span class='lineno'> 515</span> 
<span class='lineno'> 516</span>     Args:
<span class='lineno'> 517</span>       decoder_inputs: A tensor with shape
<span class='lineno'> 518</span>         [batch_size, target_length, hidden_size].
<span class='lineno'> 519</span>       encoder_outputs: A tensor with shape
<span class='lineno'> 520</span>         [batch_size, input_length, hidden_size]
<span class='lineno'> 521</span>       decoder_self_attention_bias: A tensor with shape
<span class='lineno'> 522</span>         [1, 1, target_len, target_length], the bias for decoder self-attention
<span class='lineno'> 523</span>         layer.
<span class='lineno'> 524</span>       attention_bias: A tensor with shape [batch_size, 1, 1, input_length],
<span class='lineno'> 525</span>         the bias for encoder-decoder attention layer.
<span class='lineno'> 526</span>       training: A bool, whether in training mode or not.
<span class='lineno'> 527</span>       cache: (Used for fast decoding) A nested dictionary storing previous
<span class='lineno'> 528</span>         decoder self-attention values. The items are:
<span class='lineno'> 529</span>           {layer_n: {&quot;k&quot;: A tensor with shape [batch_size, i, key_channels],
<span class='lineno'> 530</span>                      &quot;v&quot;: A tensor with shape [batch_size, i, value_channels]},
<span class='lineno'> 531</span>                        ...}
<span class='lineno'> 532</span>       decode_loop_step: An integer, the step number of the decoding loop. Used
<span class='lineno'> 533</span>         only for autoregressive inference on TPU.
<span class='lineno'> 534</span> 
<span class='lineno'> 535</span>     Returns:
<span class='lineno'> 536</span>       Output of decoder layer stack.
<span class='lineno'> 537</span>       float32 tensor with shape [batch_size, target_length, hidden_size]
<span class='lineno'> 538</span>     &quot;&quot;&quot;
<span class='lineno'> 539</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.n', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.n', title='?'>n</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', title='?'>layer</a> in enumerate(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', title='DecoderStack'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.layers', title='[?]'>layers</a>):
<span class='lineno'> 540</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self_attention_layer', title='?'>self_attention_layer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', title='?'>layer</a>[0]
<span class='lineno'> 541</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.enc_dec_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.enc_dec_attention_layer', title='?'>enc_dec_attention_layer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', title='?'>layer</a>[1]
<span class='lineno'> 542</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.feed_forward_network', title='?'>feed_forward_network</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer', title='?'>layer</a>[2]
<span class='lineno'> 543</span> 
<span class='lineno'> 544</span>       # Run inputs through the sublayers.
<span class='lineno'> 545</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', title='str'>layer_name</a> = &quot;layer_%d&quot; % <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.n', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.n', title='?'>n</a>
<span class='lineno'> 546</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_cache', title='None'>layer_cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', title='None'>cache</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', title='str'>layer_name</a>] if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.cache', title='None'>cache</a> is not None else None
<span class='lineno'> 547</span>       with tf.name_scope(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_name', title='str'>layer_name</a>):
<span class='lineno'> 548</span>         with tf.name_scope(&quot;self_attention&quot;):
<span class='lineno'> 549</span>           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self_attention_layer', title='?'>self_attention_layer</a>(
<span class='lineno'> 550</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a>,
<span class='lineno'> 551</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_self_attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_self_attention_bias', title='?'>decoder_self_attention_bias</a>,
<span class='lineno'> 552</span>               training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', title='?'>training</a>,
<span class='lineno'> 553</span>               cache=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.layer_cache', title='None'>layer_cache</a>,
<span class='lineno'> 554</span>               decode_loop_step=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decode_loop_step', title='None'>decode_loop_step</a>)
<span class='lineno'> 555</span>         with tf.name_scope(&quot;encdec_attention&quot;):
<span class='lineno'> 556</span>           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.enc_dec_attention_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.enc_dec_attention_layer', title='?'>enc_dec_attention_layer</a>(
<span class='lineno'> 557</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a>,
<span class='lineno'> 558</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.encoder_outputs', title='?'>encoder_outputs</a>,
<span class='lineno'> 559</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.attention_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.attention_bias', title='?'>attention_bias</a>,
<span class='lineno'> 560</span>               training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', title='?'>training</a>)
<span class='lineno'> 561</span>         with tf.name_scope(&quot;ffn&quot;):
<span class='lineno'> 562</span>           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.feed_forward_network', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.feed_forward_network', title='?'>feed_forward_network</a>(
<span class='lineno'> 563</span>               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a>, training=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.training', title='?'>training</a>)
<span class='lineno'> 564</span> 
<span class='lineno'> 565</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.self', title='DecoderStack'>self</a>.output_normalization(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.transformer.DecoderStack.call.decoder_inputs', title='?'>decoder_inputs</a>)
</pre></td></tr></table></body></html>