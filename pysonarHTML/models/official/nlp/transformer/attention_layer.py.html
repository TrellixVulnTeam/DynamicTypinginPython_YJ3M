<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/transformer/attention_layer.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention'>Attention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build'>build</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call'>call</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention'>SelfAttention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call'>call</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2018 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Implementation of multiheaded attention and self-attention layers.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> from __future__ import absolute_import
<span class='lineno'>  18</span> from __future__ import division
<span class='lineno'>  19</span> from __future__ import print_function
<span class='lineno'>  20</span> 
<span class='lineno'>  21</span> import math
<span class='lineno'>  22</span> 
<span class='lineno'>  23</span> import tensorflow as tf
<span class='lineno'>  24</span> from official.nlp.modeling import layers
<span class='lineno'>  25</span> 
<span class='lineno'>  26</span> 
<span class='lineno'>  27</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', title='<Attention>'>Attention</a>(tf.keras.layers.Layer):
<span class='lineno'>  28</span>   &quot;&quot;&quot;Multi-headed attention layer.&quot;&quot;&quot;
<span class='lineno'>  29</span> 
<span class='lineno'>  30</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', title='{Attention | SelfAttention}'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', title='?'>hidden_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', title='?'>num_heads</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.attention_dropout', title='?'>attention_dropout</a>):
<span class='lineno'>  31</span>     &quot;&quot;&quot;Initialize Attention.
<span class='lineno'>  32</span> 
<span class='lineno'>  33</span>     Args:
<span class='lineno'>  34</span>       hidden_size: int, output dim of hidden layer.
<span class='lineno'>  35</span>       num_heads: int, number of heads to repeat the same attention structure.
<span class='lineno'>  36</span>       attention_dropout: float, dropout rate inside attention for training.
<span class='lineno'>  37</span>     &quot;&quot;&quot;
<span class='lineno'>  38</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', title='?'>hidden_size</a> % <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', title='?'>num_heads</a>:
<span class='lineno'>  39</span>       raise ValueError(
<span class='lineno'>  40</span>           &quot;Hidden size ({}) must be divisible by the number of heads ({}).&quot;
<span class='lineno'>  41</span>           .format(hidden_size, num_heads))
<span class='lineno'>  42</span> 
<span class='lineno'>  43</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', title='<Attention>'>Attention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', title='{Attention | SelfAttention}'>self</a>).__init__()
<span class='lineno'>  44</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', title='{Attention | SelfAttention}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.hidden_size', title='?'>hidden_size</a>
<span class='lineno'>  45</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', title='{Attention | SelfAttention}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.num_heads', title='?'>num_heads</a>
<span class='lineno'>  46</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.self', title='{Attention | SelfAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', title='?'>attention_dropout</a></a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.__init__.attention_dropout', title='?'>attention_dropout</a>
<span class='lineno'>  47</span> 
<span class='lineno'>  48</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build', title='(Attention, ?) -> None'>build</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', title='?'>input_shape</a>):
<span class='lineno'>  49</span>     &quot;&quot;&quot;Builds the layer.&quot;&quot;&quot;
<span class='lineno'>  50</span>     # Layers for linearly projecting the queries, keys, and values.
<span class='lineno'>  51</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', title='?'>size_per_head</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a> // <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>
<span class='lineno'>  52</span> 
<span class='lineno'>  53</span>     def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', title='(?, ?) -> None'>_glorot_initializer</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_in', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_in', title='?'>fan_in</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_out', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_out', title='?'>fan_out</a>):
<span class='lineno'>  54</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', title='int'>limit</a> = math.sqrt(6.0 / (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_in', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_in', title='?'>fan_in</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_out', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.fan_out', title='?'>fan_out</a>))
<span class='lineno'>  55</span>       return tf.keras.initializers.RandomUniform(minval=-<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', title='int'>limit</a>, maxval=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer.limit', title='int'>limit</a>)
<span class='lineno'>  56</span> 
<span class='lineno'>  57</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', title='None'>attention_initializer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', title='(?, ?) -> None'>_glorot_initializer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', title='?'>input_shape</a>.as_list()[-1],
<span class='lineno'>  58</span>                                                 <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a>)
<span class='lineno'>  59</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.query_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.query_dense_layer', title='?'>query_dense_layer</a> = layers.DenseEinsum(
<span class='lineno'>  60</span>         output_shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', title='?'>size_per_head</a>),
<span class='lineno'>  61</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', title='None'>attention_initializer</a>,
<span class='lineno'>  62</span>         use_bias=False,
<span class='lineno'>  63</span>         name=&quot;query&quot;)
<span class='lineno'>  64</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.key_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.key_dense_layer', title='?'>key_dense_layer</a> = layers.DenseEinsum(
<span class='lineno'>  65</span>         output_shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', title='?'>size_per_head</a>),
<span class='lineno'>  66</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', title='None'>attention_initializer</a>,
<span class='lineno'>  67</span>         use_bias=False,
<span class='lineno'>  68</span>         name=&quot;key&quot;)
<span class='lineno'>  69</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.value_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.value_dense_layer', title='?'>value_dense_layer</a> = layers.DenseEinsum(
<span class='lineno'>  70</span>         output_shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.size_per_head', title='?'>size_per_head</a>),
<span class='lineno'>  71</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.attention_initializer', title='None'>attention_initializer</a>,
<span class='lineno'>  72</span>         use_bias=False,
<span class='lineno'>  73</span>         name=&quot;value&quot;)
<span class='lineno'>  74</span> 
<span class='lineno'>  75</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.output_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.output_initializer', title='None'>output_initializer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build._glorot_initializer', title='(?, ?) -> None'>_glorot_initializer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a>)
<span class='lineno'>  76</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.output_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.output_dense_layer', title='?'>output_dense_layer</a> = layers.DenseEinsum(
<span class='lineno'>  77</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a>,
<span class='lineno'>  78</span>         num_summed_dimensions=2,
<span class='lineno'>  79</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.output_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.output_initializer', title='None'>output_initializer</a>,
<span class='lineno'>  80</span>         use_bias=False,
<span class='lineno'>  81</span>         name=&quot;output_transform&quot;)
<span class='lineno'>  82</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', title='<Attention>'>Attention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.self', title='Attention'>self</a>).build(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'>  83</span> 
<span class='lineno'>  84</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config', title='Attention -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', title='Attention'>self</a>):
<span class='lineno'>  85</span>     return {
<span class='lineno'>  86</span>         &quot;hidden_size&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a>,
<span class='lineno'>  87</span>         &quot;num_heads&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>,
<span class='lineno'>  88</span>         &quot;attention_dropout&quot;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.get_config.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', title='?'>attention_dropout</a>,
<span class='lineno'>  89</span>     }
<span class='lineno'>  90</span> 
<span class='lineno'>  91</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call', title='(Attention, ?, ?, ?, ?, None, None) -> None'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query_input', title='?'>query_input</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', title='?'>source_input</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.bias', title='?'>bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.training', title='?'>training</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>=None,
<span class='lineno'>  92</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', title='None'>decode_loop_step</a>=None):
<span class='lineno'>  93</span>     &quot;&quot;&quot;Apply attention mechanism to query_input and source_input.
<span class='lineno'>  94</span> 
<span class='lineno'>  95</span>     Args:
<span class='lineno'>  96</span>       query_input: A tensor with shape [batch_size, length_query, hidden_size].
<span class='lineno'>  97</span>       source_input: A tensor with shape [batch_size, length_source,
<span class='lineno'>  98</span>         hidden_size].
<span class='lineno'>  99</span>       bias: A tensor with shape [batch_size, 1, length_query, length_source],
<span class='lineno'> 100</span>         the attention bias that will be added to the result of the dot product.
<span class='lineno'> 101</span>       training: A bool, whether in training mode or not.
<span class='lineno'> 102</span>       cache: (Used during prediction) A dictionary with tensors containing
<span class='lineno'> 103</span>         results of previous attentions. The dictionary must have the items:
<span class='lineno'> 104</span>             {&quot;k&quot;: tensor with shape [batch_size, i, heads, dim_per_head],
<span class='lineno'> 105</span>              &quot;v&quot;: tensor with shape [batch_size, i, heads, dim_per_head]}
<span class='lineno'> 106</span>         where i is the current decoded length for non-padded decode, or max
<span class='lineno'> 107</span>         sequence length for padded decode.
<span class='lineno'> 108</span>       decode_loop_step: An integer, step number of the decoding loop. Used only
<span class='lineno'> 109</span>         for autoregressive inference on TPU.
<span class='lineno'> 110</span> 
<span class='lineno'> 111</span>     Returns:
<span class='lineno'> 112</span>       Attention layer output with shape [batch_size, length_query, hidden_size]
<span class='lineno'> 113</span>     &quot;&quot;&quot;
<span class='lineno'> 114</span>     # Linearly project the query, key and value using different learned
<span class='lineno'> 115</span>     # projections. Splitting heads is automatically done during the linear
<span class='lineno'> 116</span>     # projections --&gt; [batch_size, length, num_heads, dim_per_head].
<span class='lineno'> 117</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', title='?'>query</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.query_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.query_dense_layer', title='?'>query_dense_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query_input', title='?'>query_input</a>)
<span class='lineno'> 118</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.key_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.key_dense_layer', title='?'>key_dense_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', title='?'>source_input</a>)
<span class='lineno'> 119</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.value_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.value_dense_layer', title='?'>value_dense_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.source_input', title='?'>source_input</a>)
<span class='lineno'> 120</span> 
<span class='lineno'> 121</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a> is not None:
<span class='lineno'> 122</span>       # Combine cached keys and values with new keys and values.
<span class='lineno'> 123</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', title='None'>decode_loop_step</a> is not None:
<span class='lineno'> 124</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', title='?'>cache_k_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;k&quot;].shape.as_list()
<span class='lineno'> 125</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', title='?'>indices</a> = tf.reshape(
<span class='lineno'> 126</span>             tf.one_hot(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', title='None'>decode_loop_step</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', title='?'>cache_k_shape</a>[1], dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a>.dtype),
<span class='lineno'> 127</span>             [1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_k_shape', title='?'>cache_k_shape</a>[1], 1, 1])
<span class='lineno'> 128</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;k&quot;] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', title='?'>indices</a>
<span class='lineno'> 129</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', title='?'>cache_v_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;v&quot;].shape.as_list()
<span class='lineno'> 130</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', title='?'>indices</a> = tf.reshape(
<span class='lineno'> 131</span>             tf.one_hot(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.decode_loop_step', title='None'>decode_loop_step</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', title='?'>cache_v_shape</a>[1], dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a>.dtype),
<span class='lineno'> 132</span>             [1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache_v_shape', title='?'>cache_v_shape</a>[1], 1, 1])
<span class='lineno'> 133</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;v&quot;] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.indices', title='?'>indices</a>
<span class='lineno'> 134</span>       else:
<span class='lineno'> 135</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a> = tf.concat([tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;k&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a>.dtype), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a>], axis=1)
<span class='lineno'> 136</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a> = tf.concat([tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;v&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a>.dtype), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a>], axis=1)
<span class='lineno'> 137</span> 
<span class='lineno'> 138</span>       # Update cache
<span class='lineno'> 139</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;k&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a>
<span class='lineno'> 140</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.cache', title='None'>cache</a>[&quot;v&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a>
<span class='lineno'> 141</span> 
<span class='lineno'> 142</span>     # Scale query to prevent the dot product between query and key from growing
<span class='lineno'> 143</span>     # too large.
<span class='lineno'> 144</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.depth', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.depth', title='?'>depth</a> = (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.hidden_size', title='?'>hidden_size</a> // <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.num_heads', title='?'>num_heads</a>)
<span class='lineno'> 145</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', title='float'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', title='?'>query</a></a> *= <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.depth', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.depth', title='?'>depth</a> ** -0.5
<span class='lineno'> 146</span> 
<span class='lineno'> 147</span>     # Calculate dot product attention
<span class='lineno'> 148</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', title='?'>logits</a> = tf.einsum(&quot;BTNH,BFNH-&gt;BNFT&quot;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.key', title='?'>key</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.query', title='float'>query</a>)
<span class='lineno'> 149</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', title='?'>logits</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.bias', title='?'>bias</a>
<span class='lineno'> 150</span>     # Note that softmax internally performs math operations using float32
<span class='lineno'> 151</span>     # for numeric stability. When training with float16, we keep the input
<span class='lineno'> 152</span>     # and output in float16 for better performance.
<span class='lineno'> 153</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', title='?'>weights</a> = tf.nn.softmax(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.logits', title='?'>logits</a>, name=&quot;attention_weights&quot;)
<span class='lineno'> 154</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.training', title='?'>training</a>:
<span class='lineno'> 155</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', title='?'>weights</a> = tf.nn.dropout(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', title='?'>weights</a>, rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.attention_dropout', title='?'>attention_dropout</a>)
<span class='lineno'> 156</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', title='?'>attention_output</a> = tf.einsum(&quot;BNFT,BTNH-&gt;BFNH&quot;, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.weights', title='?'>weights</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.value', title='?'>value</a>)
<span class='lineno'> 157</span> 
<span class='lineno'> 158</span>     # Run the outputs through another linear projection layer. Recombining heads
<span class='lineno'> 159</span>     # is automatically done --&gt; [batch_size, length, hidden_size]
<span class='lineno'> 160</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', title='?'>attention_output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.self', title='Attention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.output_dense_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.output_dense_layer', title='?'>output_dense_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', title='?'>attention_output</a>)
<span class='lineno'> 161</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention.call.attention_output', title='?'>attention_output</a>
<span class='lineno'> 162</span> 
<span class='lineno'> 163</span> 
<span class='lineno'> 164</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention', title='<SelfAttention>'>SelfAttention</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.Attention', title='<Attention>'>Attention</a>):
<span class='lineno'> 165</span>   &quot;&quot;&quot;Multiheaded self-attention layer.&quot;&quot;&quot;
<span class='lineno'> 166</span> 
<span class='lineno'> 167</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call', title='(SelfAttention, ?, ?, ?, None, None) -> ?'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.self', title='SelfAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', title='?'>query_input</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.bias', title='?'>bias</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.training', title='?'>training</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.cache', title='None'>cache</a>=None,
<span class='lineno'> 168</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.decode_loop_step', title='None'>decode_loop_step</a>=None):
<span class='lineno'> 169</span>     return super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention', title='<SelfAttention>'>SelfAttention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.self', title='SelfAttention'>self</a>).call(
<span class='lineno'> 170</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', title='?'>query_input</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.query_input', title='?'>query_input</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.bias', title='?'>bias</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.training', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.training', title='?'>training</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.cache', title='None'>cache</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.attention_layer.SelfAttention.call.decode_loop_step', title='None'>decode_loop_step</a>)
</pre></td></tr></table></body></html>