<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/transformer/misc.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS'>FLAGS</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP'>PARAMS_MAP</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params'>get_model_params</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags'>define_transformer_flags</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks'>get_callbacks</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats'>update_stats</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &#39;License&#39;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &#39;AS IS&#39; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Misc for Transformer.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> from __future__ import absolute_import
<span class='lineno'>  18</span> from __future__ import division
<span class='lineno'>  19</span> from __future__ import print_function
<span class='lineno'>  20</span> 
<span class='lineno'>  21</span> # pylint: disable=g-bad-import-order
<span class='lineno'>  22</span> from absl import flags
<span class='lineno'>  23</span> import tensorflow as tf
<span class='lineno'>  24</span> 
<span class='lineno'>  25</span> from official.nlp.transformer import model_params
<span class='lineno'>  26</span> from official.utils.flags import core as flags_core
<span class='lineno'>  27</span> from official.utils.misc import keras_utils
<span class='lineno'>  28</span> 
<span class='lineno'>  29</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a> = flags.FLAGS
<span class='lineno'>  30</span> 
<span class='lineno'>  31</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', title='dict'>PARAMS_MAP</a> = {
<span class='lineno'>  32</span>     &#39;tiny&#39;: model_params.TINY_PARAMS,
<span class='lineno'>  33</span>     &#39;base&#39;: model_params.BASE_PARAMS,
<span class='lineno'>  34</span>     &#39;big&#39;: model_params.BIG_PARAMS,
<span class='lineno'>  35</span> }
<span class='lineno'>  36</span> 
<span class='lineno'>  37</span> 
<span class='lineno'>  38</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params', title='(?, ?) -> None'>get_model_params</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', title='?'>param_set</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.num_gpus', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.num_gpus', title='?'>num_gpus</a>):
<span class='lineno'>  39</span>   &quot;&quot;&quot;Gets predefined model params.&quot;&quot;&quot;
<span class='lineno'>  40</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.num_gpus', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.num_gpus', title='?'>num_gpus</a> &gt; 1:
<span class='lineno'>  41</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', title='?'>param_set</a> == &#39;big&#39;:
<span class='lineno'>  42</span>       return model_params.BIG_MULTI_GPU_PARAMS.copy()
<span class='lineno'>  43</span>     elif <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', title='?'>param_set</a> == &#39;base&#39;:
<span class='lineno'>  44</span>       return model_params.BASE_MULTI_GPU_PARAMS.copy()
<span class='lineno'>  45</span>     else:
<span class='lineno'>  46</span>       raise ValueError(&#39;Not valid params: param_set={} num_gpus={}&#39;.format(
<span class='lineno'>  47</span>           param_set, num_gpus))
<span class='lineno'>  48</span> 
<span class='lineno'>  49</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', title='dict'>PARAMS_MAP</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_model_params.param_set', title='?'>param_set</a>].copy()
<span class='lineno'>  50</span> 
<span class='lineno'>  51</span> 
<span class='lineno'>  52</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags', title='() -> None'>define_transformer_flags</a>():
<span class='lineno'>  53</span>   &quot;&quot;&quot;Add flags and flag validators for running transformer_main.&quot;&quot;&quot;
<span class='lineno'>  54</span>   # Add common flags (data_dir, model_dir, etc.).
<span class='lineno'>  55</span>   flags_core.define_base(num_gpu=True, distribution_strategy=True)
<span class='lineno'>  56</span>   flags_core.define_performance(
<span class='lineno'>  57</span>       num_parallel_calls=True,
<span class='lineno'>  58</span>       inter_op=False,
<span class='lineno'>  59</span>       intra_op=False,
<span class='lineno'>  60</span>       synthetic_data=True,
<span class='lineno'>  61</span>       max_train_steps=False,
<span class='lineno'>  62</span>       dtype=True,
<span class='lineno'>  63</span>       loss_scale=True,
<span class='lineno'>  64</span>       all_reduce_alg=True,
<span class='lineno'>  65</span>       num_packs=True,
<span class='lineno'>  66</span>       tf_gpu_thread_mode=True,
<span class='lineno'>  67</span>       datasets_num_private_threads=True,
<span class='lineno'>  68</span>       enable_xla=True,
<span class='lineno'>  69</span>       fp16_implementation=True
<span class='lineno'>  70</span>   )
<span class='lineno'>  71</span> 
<span class='lineno'>  72</span>   flags_core.define_benchmark()
<span class='lineno'>  73</span>   flags_core.define_device(tpu=True)
<span class='lineno'>  74</span> 
<span class='lineno'>  75</span>   flags.DEFINE_integer(
<span class='lineno'>  76</span>       name=&#39;train_steps&#39;, short_name=&#39;ts&#39;, default=300000,
<span class='lineno'>  77</span>       help=flags_core.help_wrap(&#39;The number of steps used to train.&#39;))
<span class='lineno'>  78</span>   flags.DEFINE_integer(
<span class='lineno'>  79</span>       name=&#39;steps_between_evals&#39;, short_name=&#39;sbe&#39;, default=5000,
<span class='lineno'>  80</span>       help=flags_core.help_wrap(
<span class='lineno'>  81</span>           &#39;The Number of training steps to run between evaluations. This is &#39;
<span class='lineno'>  82</span>           &#39;used if --train_steps is defined.&#39;))
<span class='lineno'>  83</span>   flags.DEFINE_boolean(
<span class='lineno'>  84</span>       name=&#39;enable_time_history&#39;, default=True,
<span class='lineno'>  85</span>       help=&#39;Whether to enable TimeHistory callback.&#39;)
<span class='lineno'>  86</span>   flags.DEFINE_boolean(
<span class='lineno'>  87</span>       name=&#39;enable_tensorboard&#39;, default=False,
<span class='lineno'>  88</span>       help=&#39;Whether to enable Tensorboard callback.&#39;)
<span class='lineno'>  89</span>   flags.DEFINE_boolean(
<span class='lineno'>  90</span>       name=&#39;enable_metrics_in_training&#39;, default=False,
<span class='lineno'>  91</span>       help=&#39;Whether to enable metrics during training.&#39;)
<span class='lineno'>  92</span>   flags.DEFINE_boolean(
<span class='lineno'>  93</span>       name=&#39;enable_mlir_bridge&#39;,
<span class='lineno'>  94</span>       default=False,
<span class='lineno'>  95</span>       help=&#39;Whether to enable the TF to XLA bridge.&#39;)
<span class='lineno'>  96</span>   # Set flags from the flags_core module as &#39;key flags&#39; so they&#39;re listed when
<span class='lineno'>  97</span>   # the &#39;-h&#39; flag is used. Without this line, the flags defined above are
<span class='lineno'>  98</span>   # only shown in the full `--helpful` help text.
<span class='lineno'>  99</span>   flags.adopt_module_key_flags(flags_core)
<span class='lineno'> 100</span> 
<span class='lineno'> 101</span>   # Add transformer-specific flags
<span class='lineno'> 102</span>   flags.DEFINE_enum(
<span class='lineno'> 103</span>       name=&#39;param_set&#39;, short_name=&#39;mp&#39;, default=&#39;big&#39;,
<span class='lineno'> 104</span>       enum_values=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.PARAMS_MAP', title='dict'>PARAMS_MAP</a>.keys(),
<span class='lineno'> 105</span>       help=flags_core.help_wrap(
<span class='lineno'> 106</span>           &#39;Parameter set to use when creating and training the model. The &#39;
<span class='lineno'> 107</span>           &#39;parameters define the input shape (batch size and max length), &#39;
<span class='lineno'> 108</span>           &#39;model configuration (size of embedding, # of hidden layers, etc.), &#39;
<span class='lineno'> 109</span>           &#39;and various other settings. The big parameter set increases the &#39;
<span class='lineno'> 110</span>           &#39;default batch size, embedding/hidden size, and filter size. For a &#39;
<span class='lineno'> 111</span>           &#39;complete list of parameters, please see model/model_params.py.&#39;))
<span class='lineno'> 112</span> 
<span class='lineno'> 113</span>   flags.DEFINE_bool(
<span class='lineno'> 114</span>       name=&#39;static_batch&#39;, short_name=&#39;sb&#39;, default=False,
<span class='lineno'> 115</span>       help=flags_core.help_wrap(
<span class='lineno'> 116</span>           &#39;Whether the batches in the dataset should have static shapes. In &#39;
<span class='lineno'> 117</span>           &#39;general, this setting should be False. Dynamic shapes allow the &#39;
<span class='lineno'> 118</span>           &#39;inputs to be grouped so that the number of padding tokens is &#39;
<span class='lineno'> 119</span>           &#39;minimized, and helps model training. In cases where the input shape &#39;
<span class='lineno'> 120</span>           &#39;must be static (e.g. running on TPU), this setting will be ignored &#39;
<span class='lineno'> 121</span>           &#39;and static batching will always be used.&#39;))
<span class='lineno'> 122</span>   flags.DEFINE_integer(
<span class='lineno'> 123</span>       name=&#39;max_length&#39;, short_name=&#39;ml&#39;, default=256,
<span class='lineno'> 124</span>       help=flags_core.help_wrap(
<span class='lineno'> 125</span>           &#39;Max sentence length for Transformer. Default is 256. Note: Usually &#39;
<span class='lineno'> 126</span>           &#39;it is more effective to use a smaller max length if static_batch is &#39;
<span class='lineno'> 127</span>           &#39;enabled, e.g. 64.&#39;))
<span class='lineno'> 128</span> 
<span class='lineno'> 129</span>   # Flags for training with steps (may be used for debugging)
<span class='lineno'> 130</span>   flags.DEFINE_integer(
<span class='lineno'> 131</span>       name=&#39;validation_steps&#39;, short_name=&#39;vs&#39;, default=64,
<span class='lineno'> 132</span>       help=flags_core.help_wrap(&#39;The number of steps used in validation.&#39;))
<span class='lineno'> 133</span> 
<span class='lineno'> 134</span>   # BLEU score computation
<span class='lineno'> 135</span>   flags.DEFINE_string(
<span class='lineno'> 136</span>       name=&#39;bleu_source&#39;, short_name=&#39;bls&#39;, default=None,
<span class='lineno'> 137</span>       help=flags_core.help_wrap(
<span class='lineno'> 138</span>           &#39;Path to source file containing text translate when calculating the &#39;
<span class='lineno'> 139</span>           &#39;official BLEU score. Both --bleu_source and --bleu_ref must be set. &#39;
<span class='lineno'> 140</span>           ))
<span class='lineno'> 141</span>   flags.DEFINE_string(
<span class='lineno'> 142</span>       name=&#39;bleu_ref&#39;, short_name=&#39;blr&#39;, default=None,
<span class='lineno'> 143</span>       help=flags_core.help_wrap(
<span class='lineno'> 144</span>           &#39;Path to source file containing text translate when calculating the &#39;
<span class='lineno'> 145</span>           &#39;official BLEU score. Both --bleu_source and --bleu_ref must be set. &#39;
<span class='lineno'> 146</span>           ))
<span class='lineno'> 147</span>   flags.DEFINE_string(
<span class='lineno'> 148</span>       name=&#39;vocab_file&#39;, short_name=&#39;vf&#39;, default=None,
<span class='lineno'> 149</span>       help=flags_core.help_wrap(
<span class='lineno'> 150</span>           &#39;Path to subtoken vocabulary file. If data_download.py was used to &#39;
<span class='lineno'> 151</span>           &#39;download and encode the training data, look in the data_dir to find &#39;
<span class='lineno'> 152</span>           &#39;the vocab file.&#39;))
<span class='lineno'> 153</span>   flags.DEFINE_string(
<span class='lineno'> 154</span>       name=&#39;mode&#39;, default=&#39;train&#39;,
<span class='lineno'> 155</span>       help=flags_core.help_wrap(&#39;mode: train, eval, or predict&#39;))
<span class='lineno'> 156</span>   flags.DEFINE_bool(
<span class='lineno'> 157</span>       name=&#39;use_ctl&#39;,
<span class='lineno'> 158</span>       default=False,
<span class='lineno'> 159</span>       help=flags_core.help_wrap(
<span class='lineno'> 160</span>           &#39;Whether the model runs with custom training loop.&#39;))
<span class='lineno'> 161</span>   flags.DEFINE_integer(
<span class='lineno'> 162</span>       name=&#39;decode_batch_size&#39;,
<span class='lineno'> 163</span>       default=32,
<span class='lineno'> 164</span>       help=flags_core.help_wrap(
<span class='lineno'> 165</span>           &#39;Global batch size used for Transformer autoregressive decoding on &#39;
<span class='lineno'> 166</span>           &#39;TPU.&#39;))
<span class='lineno'> 167</span>   flags.DEFINE_integer(
<span class='lineno'> 168</span>       name=&#39;decode_max_length&#39;,
<span class='lineno'> 169</span>       default=97,
<span class='lineno'> 170</span>       help=flags_core.help_wrap(
<span class='lineno'> 171</span>           &#39;Max sequence length of the decode/eval data. This is used by &#39;
<span class='lineno'> 172</span>           &#39;Transformer autoregressive decoding on TPU to have minimum &#39;
<span class='lineno'> 173</span>           &#39;paddings.&#39;))
<span class='lineno'> 174</span>   flags.DEFINE_bool(
<span class='lineno'> 175</span>       name=&#39;padded_decode&#39;,
<span class='lineno'> 176</span>       default=False,
<span class='lineno'> 177</span>       help=flags_core.help_wrap(
<span class='lineno'> 178</span>           &#39;Whether the autoregressive decoding runs with input data padded to &#39;
<span class='lineno'> 179</span>           &#39;the decode_max_length. For TPU/XLA-GPU runs, this flag has to be &#39;
<span class='lineno'> 180</span>           &#39;set due the static shape requirement. Although CPU/GPU could also &#39;
<span class='lineno'> 181</span>           &#39;use padded_decode, it has not been tested. In addition, this method &#39;
<span class='lineno'> 182</span>           &#39;will introduce unnecessary overheads which grow quadratically with &#39;
<span class='lineno'> 183</span>           &#39;the max sequence length.&#39;))
<span class='lineno'> 184</span>   flags.DEFINE_bool(
<span class='lineno'> 185</span>       name=&#39;enable_checkpointing&#39;,
<span class='lineno'> 186</span>       default=True,
<span class='lineno'> 187</span>       help=flags_core.help_wrap(
<span class='lineno'> 188</span>           &#39;Whether to do checkpointing during training. When running under &#39;
<span class='lineno'> 189</span>           &#39;benchmark harness, we will avoid checkpointing.&#39;))
<span class='lineno'> 190</span> 
<span class='lineno'> 191</span>   flags_core.set_defaults(data_dir=&#39;/tmp/translate_ende&#39;,
<span class='lineno'> 192</span>                           model_dir=&#39;/tmp/transformer_model&#39;,
<span class='lineno'> 193</span>                           batch_size=None)
<span class='lineno'> 194</span> 
<span class='lineno'> 195</span>   # pylint: disable=unused-variable
<span class='lineno'> 196</span>   @flags.multi_flags_validator(
<span class='lineno'> 197</span>       [&#39;bleu_source&#39;, &#39;bleu_ref&#39;],
<span class='lineno'> 198</span>       message=&#39;Both or neither --bleu_source and --bleu_ref must be def<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined', title='? -> bool'>ined</a>.&#39;)
<span class='lineno'> 199</span>   def _check_bleu_files(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', title='?'>flags_dict</a>):
<span class='lineno'> 200</span>     return (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', title='?'>flags_dict</a>[&#39;bleu_source&#39;] is None) == (
<span class='lineno'> 201</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined.flags_dict', title='?'>flags_dict</a>[&#39;bleu_ref&#39;] is None)
<span class='lineno'> 202</span> 
<span class='lineno'> 203</span>   @flags.multi_flags_validator(
<span class='lineno'> 204</span>       [&#39;bleu_source&#39;, &#39;bleu_ref&#39;, &#39;vocab_file&#39;],
<span class='lineno'> 205</span>       message=&#39;--vocab_file must be def<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.define_transformer_flags.ined', title='? -> ?'>ined</a> if --bleu_source and --bleu_ref &#39;
<span class='lineno'> 206</span>               &#39;are defined.&#39;)
<span class='lineno'> 207</span>   def _check_bleu_vocab_file(flags_dict):
<span class='lineno'> 208</span>     if flags_dict[&#39;bleu_source&#39;] and flags_dict[&#39;bleu_ref&#39;]:
<span class='lineno'> 209</span>       return flags_dict[&#39;vocab_file&#39;] is not None
<span class='lineno'> 210</span>     return True
<span class='lineno'> 211</span>   # pylint: enable=unused-variable
<span class='lineno'> 212</span> 
<span class='lineno'> 213</span> 
<span class='lineno'> 214</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks', title='() -> [?]'>get_callbacks</a>():
<span class='lineno'> 215</span>   &quot;&quot;&quot;Returns common callbacks.&quot;&quot;&quot;
<span class='lineno'> 216</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', title='[?]'>callbacks</a> = []
<span class='lineno'> 217</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.enable_time_history:
<span class='lineno'> 218</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.time_callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.time_callback', title='?'>time_callback</a> = keras_utils.TimeHistory(
<span class='lineno'> 219</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.batch_size,
<span class='lineno'> 220</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.log_steps,
<span class='lineno'> 221</span>         logdir=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.model_dir if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.enable_tensorboard else None)
<span class='lineno'> 222</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', title='[?]'>callbacks</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.time_callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.time_callback', title='?'>time_callback</a>)
<span class='lineno'> 223</span> 
<span class='lineno'> 224</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.enable_tensorboard:
<span class='lineno'> 225</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.tensorboard_callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.tensorboard_callback', title='?'>tensorboard_callback</a> = tf.keras.callbacks.TensorBoard(
<span class='lineno'> 226</span>         log_dir=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.FLAGS', title='?'>FLAGS</a>.model_dir)
<span class='lineno'> 227</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', title='[?]'>callbacks</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.tensorboard_callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.tensorboard_callback', title='?'>tensorboard_callback</a>)
<span class='lineno'> 228</span> 
<span class='lineno'> 229</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.get_callbacks.callbacks', title='[?]'>callbacks</a>
<span class='lineno'> 230</span> 
<span class='lineno'> 231</span> 
<span class='lineno'> 232</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats', title='(?, ?, ?) -> None'>update_stats</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', title='?'>history</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', title='?'>stats</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', title='?'>callbacks</a>):
<span class='lineno'> 233</span>   &quot;&quot;&quot;Normalizes and updates dictionary of stats.
<span class='lineno'> 234</span> 
<span class='lineno'> 235</span>   Args:
<span class='lineno'> 236</span>     history: Results of the training step.
<span class='lineno'> 237</span>     stats: Dict with pre-existing training stats.
<span class='lineno'> 238</span>     callbacks: a list of callbacks which might include a time history callback
<span class='lineno'> 239</span>       used during keras.fit.
<span class='lineno'> 240</span>   &quot;&quot;&quot;
<span class='lineno'> 241</span> 
<span class='lineno'> 242</span>   if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', title='?'>history</a> and <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', title='?'>history</a>.history:
<span class='lineno'> 243</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.train_hist', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.train_hist', title='?'>train_hist</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.history', title='?'>history</a>.history
<span class='lineno'> 244</span>     # Gets final loss from training.
<span class='lineno'> 245</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', title='?'>stats</a>[&#39;loss&#39;] = float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.train_hist', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.train_hist', title='?'>train_hist</a>[&#39;loss&#39;][-1])
<span class='lineno'> 246</span> 
<span class='lineno'> 247</span>   if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', title='?'>callbacks</a>:
<span class='lineno'> 248</span>     return
<span class='lineno'> 249</span> 
<span class='lineno'> 250</span>   # Look for the time history callback which was used during keras.fit
<span class='lineno'> 251</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callbacks', title='?'>callbacks</a>:
<span class='lineno'> 252</span>     if isinstance(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a></a>, keras_utils.TimeHistory):
<span class='lineno'> 253</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', title='?'>timestamp_log</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a>.timestamp_log
<span class='lineno'> 254</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', title='?'>stats</a>[&#39;step_timestamp_log&#39;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', title='?'>timestamp_log</a>
<span class='lineno'> 255</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', title='?'>stats</a>[&#39;train_finish_time&#39;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a>.train_finish_time
<span class='lineno'> 256</span>       if len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', title='?'>timestamp_log</a>) &gt; 1:
<span class='lineno'> 257</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.stats', title='?'>stats</a>[&#39;avg_exp_per_second&#39;] = (
<span class='lineno'> 258</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a>.batch_size * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a>.log_steps *
<span class='lineno'> 259</span>             (len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.callback', title='?'>callback</a>.timestamp_log)-1) /
<span class='lineno'> 260</span>             (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', title='?'>timestamp_log</a>[-1].timestamp - <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.transformer.misc.update_stats.timestamp_log', title='?'>timestamp_log</a>[0].timestamp))
</pre></td></tr></table></body></html>