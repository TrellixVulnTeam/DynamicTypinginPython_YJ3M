<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/modeling/layers/talking_heads_attention.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX'>_CHR_IDX</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention'>TalkingHeadsAttention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention'>_build_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention'>_compute_attention</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Talking Head Attention layer.&quot;&quot;&quot;
<span class='lineno'>  16</span> # pylint: disable=g-classes-have-attributes
<span class='lineno'>  17</span> import math
<span class='lineno'>  18</span> import string
<span class='lineno'>  19</span> 
<span class='lineno'>  20</span> import gin
<span class='lineno'>  21</span> import tensorflow as tf
<span class='lineno'>  22</span> 
<span class='lineno'>  23</span> from official.nlp.modeling.layers import attention
<span class='lineno'>  24</span> 
<span class='lineno'>  25</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', title='?'>_CHR_IDX</a> = string.ascii_lowercase
<span class='lineno'>  26</span> 
<span class='lineno'>  27</span> 
<span class='lineno'>  28</span> @tf.keras.utils.register_keras_serializable(package=&quot;Text&quot;)
<span class='lineno'>  29</span> @gin.configurable
<span class='lineno'>  30</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention', title='<TalkingHeadsAttention>'>TalkingHeadsAttention</a>(attention.MultiHeadAttention):
<span class='lineno'>  31</span>   &quot;&quot;&quot;Implements Talking-Heads Attention.
<span class='lineno'>  32</span> 
<span class='lineno'>  33</span>   This is an implementation of Talking-Heads Attention based on the paper
<span class='lineno'>  34</span>   Talking-Heads Attention (https://arxiv.org/abs/2003.02436): it enhanced
<span class='lineno'>  35</span>   multi-head attention by including linearprojections across the attention-heads
<span class='lineno'>  36</span>   dimension, immediately before and after the softmax operation.
<span class='lineno'>  37</span> 
<span class='lineno'>  38</span>   See the base class `MultiHeadAttention` for more details.
<span class='lineno'>  39</span> 
<span class='lineno'>  40</span>   Arguments:
<span class='lineno'>  41</span>     num_heads: Number of attention heads.
<span class='lineno'>  42</span>     key_size: Size of each attention head for query and key.
<span class='lineno'>  43</span>     value_size:  Size of each attention head for value.
<span class='lineno'>  44</span>     dropout: Dropout probability.
<span class='lineno'>  45</span>     use_bias: Boolean, whether the dense layers use bias vectors/matrices.
<span class='lineno'>  46</span>     output_shape: The expected shape of an output tensor, besides the batch and
<span class='lineno'>  47</span>       sequence dims. If not specified, projects back to the key feature dim.
<span class='lineno'>  48</span>     attention_axes: axes over which the attention is applied. `None` means
<span class='lineno'>  49</span>       attention over all axes, but batch, heads, and features.
<span class='lineno'>  50</span>     return_attention_scores: bool, if `True`, returns the multi-head attention
<span class='lineno'>  51</span>       scores as an additional output argument.
<span class='lineno'>  52</span>     kernel_initializer: Initializer for dense layer kernels.
<span class='lineno'>  53</span>     bias_initializer: Initializer for dense layer biases.
<span class='lineno'>  54</span>     kernel_regularizer: Regularizer for dense layer kernels.
<span class='lineno'>  55</span>     bias_regularizer: Regularizer for dense layer biases.
<span class='lineno'>  56</span>     activity_regularizer: Regularizer for dense layer activity.
<span class='lineno'>  57</span>     kernel_constraint: Constraint for dense layer kernels.
<span class='lineno'>  58</span>     bias_constraint: Constraint for dense layer kernels.
<span class='lineno'>  59</span>   &quot;&quot;&quot;
<span class='lineno'>  60</span> 
<span class='lineno'>  61</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention', title='(TalkingHeadsAttention, ?) -> None'>_build_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', title='?'>qkv_rank</a>):
<span class='lineno'>  62</span>     &quot;&quot;&quot;Builds multi-head dot-product attention computations.
<span class='lineno'>  63</span> 
<span class='lineno'>  64</span>     This function overrides base class to create additional linear projection
<span class='lineno'>  65</span>     that will be applied on attention scores before and after softmax.
<span class='lineno'>  66</span> 
<span class='lineno'>  67</span>     Args:
<span class='lineno'>  68</span>       qkv_rank: the rank of query, key, value tensors after projection.
<span class='lineno'>  69</span>     &quot;&quot;&quot;
<span class='lineno'>  70</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention', title='<TalkingHeadsAttention>'>TalkingHeadsAttention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>)._build_attention(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', title='?'>qkv_rank</a>)
<span class='lineno'>  71</span> 
<span class='lineno'>  72</span>     # Build an equation:
<span class='lineno'>  73</span>     # (&lt;batch_dims&gt;, num_heads_a, ...),(num_heads_a, num_heads_b) -&gt;
<span class='lineno'>  74</span>     # (&lt;batch_dims&gt;, num_heads_b, ...)
<span class='lineno'>  75</span>     # qkv_ranks has `batch_dims`, `attention_dims`, `num_heads` and `channels`.
<span class='lineno'>  76</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', title='int'>num_batch_dims</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.qkv_rank', title='?'>qkv_rank</a> - len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._attention_axes) - 2
<span class='lineno'>  77</span> 
<span class='lineno'>  78</span>     # The shape of attn_scores is:
<span class='lineno'>  79</span>     # (&lt;batch_dims&gt;, num_heads, &lt;query_attn_dims&gt;, &lt;key_attn_dims&gt;)
<span class='lineno'>  80</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', title='int'>num_batch_dims</a> + 1 + len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._attention_axes) * 2
<span class='lineno'>  81</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', title='?'>scores_notation</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', title='?'>_CHR_IDX</a>[:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a>]
<span class='lineno'>  82</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projection_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projection_notation', title='?'>projection_notation</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', title='?'>scores_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', title='int'>num_batch_dims</a>] + (
<span class='lineno'>  83</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a>])
<span class='lineno'>  84</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projected_scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projected_scores_notation', title='?'>projected_scores_notation</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', title='?'>scores_notation</a>[:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', title='int'>num_batch_dims</a>] + (
<span class='lineno'>  85</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a>] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', title='?'>scores_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.num_batch_dims', title='int'>num_batch_dims</a> + 1:])
<span class='lineno'>  86</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', title='?'>_talking_heads_equation</a> = &quot;%s,%s-&gt;%s&quot; % (
<span class='lineno'>  87</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.scores_notation', title='?'>scores_notation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projection_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projection_notation', title='?'>projection_notation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projected_scores_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.projected_scores_notation', title='?'>projected_scores_notation</a>)
<span class='lineno'>  88</span> 
<span class='lineno'>  89</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._pre_softmax_weight', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._pre_softmax_weight', title='?'>_pre_softmax_weight</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.add_weight(
<span class='lineno'>  90</span>         &quot;pre_softmax_weight&quot;,
<span class='lineno'>  91</span>         shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._num_heads, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._num_heads),
<span class='lineno'>  92</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_initializer,
<span class='lineno'>  93</span>         regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_regularizer,
<span class='lineno'>  94</span>         constraint=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_constraint,
<span class='lineno'>  95</span>         dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.dtype,
<span class='lineno'>  96</span>         trainable=True)
<span class='lineno'>  97</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._post_softmax_weight', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._post_softmax_weight', title='?'>_post_softmax_weight</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.add_weight(
<span class='lineno'>  98</span>         &quot;post_softmax_weight&quot;,
<span class='lineno'>  99</span>         shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._num_heads, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._num_heads),
<span class='lineno'> 100</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_initializer,
<span class='lineno'> 101</span>         regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_regularizer,
<span class='lineno'> 102</span>         constraint=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>._kernel_constraint,
<span class='lineno'> 103</span>         dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._build_attention.self', title='TalkingHeadsAttention'>self</a>.dtype,
<span class='lineno'> 104</span>         trainable=True)
<span class='lineno'> 105</span> 
<span class='lineno'> 106</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention', title='(TalkingHeadsAttention, ?, ?, ?, None) -> (?, ?)'>_compute_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>,
<span class='lineno'> 107</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.query_tensor', title='?'>query_tensor</a>,
<span class='lineno'> 108</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 109</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.value_tensor', title='?'>value_tensor</a>,
<span class='lineno'> 110</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_mask', title='None'>attention_mask</a>=None):
<span class='lineno'> 111</span>     &quot;&quot;&quot;Applies Dot-product attention with query, key, value tensors.
<span class='lineno'> 112</span> 
<span class='lineno'> 113</span>     This function overrides base class to apply additional linear projection
<span class='lineno'> 114</span>     on attention scores before and after softmax.
<span class='lineno'> 115</span> 
<span class='lineno'> 116</span>     Args:
<span class='lineno'> 117</span>       query_tensor: Projected query `Tensor` of shape `[B, T, N, key_size]`.
<span class='lineno'> 118</span>       key_tensor: Projected key `Tensor` of shape `[B, T, N, key_size]`.
<span class='lineno'> 119</span>       value_tensor: Projected value `Tensor` of shape `[B, T, N, value_size]`.
<span class='lineno'> 120</span>       attention_mask: a boolean mask of shape `[B, T, S]`, that prevents
<span class='lineno'> 121</span>         attention to certain positions.
<span class='lineno'> 122</span> 
<span class='lineno'> 123</span>     Returns:
<span class='lineno'> 124</span>       attention_output: Multi-headed outputs of attention computation.
<span class='lineno'> 125</span>       attention_scores: Multi-headed attention weights.
<span class='lineno'> 126</span>     &quot;&quot;&quot;
<span class='lineno'> 127</span>     # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw
<span class='lineno'> 128</span>     # attention scores.
<span class='lineno'> 129</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>._dot_product_equation, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 130</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.query_tensor', title='?'>query_tensor</a>)
<span class='lineno'> 131</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 132</span>                                    1.0 / math.sqrt(float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>._key_size)))
<span class='lineno'> 133</span> 
<span class='lineno'> 134</span>     # Apply linear projection before softmax
<span class='lineno'> 135</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', title='?'>_talking_heads_equation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 136</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._pre_softmax_weight', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._pre_softmax_weight', title='?'>_pre_softmax_weight</a>)
<span class='lineno'> 137</span> 
<span class='lineno'> 138</span>     # Normalize the attention scores to probabilities.
<span class='lineno'> 139</span>     # `attention_scores` = [B, N, T, S]
<span class='lineno'> 140</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>._masked_softmax(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_mask', title='None'>attention_mask</a>)
<span class='lineno'> 141</span> 
<span class='lineno'> 142</span>     # Apply linear projection after softmax
<span class='lineno'> 143</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._talking_heads_equation', title='?'>_talking_heads_equation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 144</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._post_softmax_weight', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._post_softmax_weight', title='?'>_post_softmax_weight</a>)
<span class='lineno'> 145</span> 
<span class='lineno'> 146</span>     # This is actually dropping out entire tokens to attend to, which might
<span class='lineno'> 147</span>     # seem a bit unusual, but is taken from the original Transformer paper.
<span class='lineno'> 148</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores_dropout', title='?'>attention_scores_dropout</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>._dropout_layer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>)
<span class='lineno'> 149</span> 
<span class='lineno'> 150</span>     # `context_layer` = [B, T, N, H]
<span class='lineno'> 151</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_output', title='?'>attention_output</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.self', title='TalkingHeadsAttention'>self</a>._combine_equation,
<span class='lineno'> 152</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores_dropout', title='?'>attention_scores_dropout</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.value_tensor', title='?'>value_tensor</a>)
<span class='lineno'> 153</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_output', title='?'>attention_output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.talking_heads_attention.TalkingHeadsAttention._compute_attention.attention_scores', title='?'>attention_scores</a>
</pre></td></tr></table></body></html>