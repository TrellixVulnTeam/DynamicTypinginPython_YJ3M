<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/modeling/layers/attention.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense'>EinsumDense</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX'>_CHR_IDX</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation'>_build_attention_equation</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation'>_build_proj_equation</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape'>_get_output_shape</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention'>MultiHeadAttention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build'>build</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention'>_build_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention'>_compute_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call'>call</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention'>CachedAttention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache'>_update_cache</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call'>call</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Lint as: python3
<span class='lineno'>   2</span> # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   3</span> #
<span class='lineno'>   4</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   5</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   6</span> # You may obtain a copy of the License at
<span class='lineno'>   7</span> #
<span class='lineno'>   8</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   9</span> #
<span class='lineno'>  10</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  11</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  12</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  13</span> # See the License for the specific language governing permissions and
<span class='lineno'>  14</span> # limitations under the License.
<span class='lineno'>  15</span> # ==============================================================================
<span class='lineno'>  16</span> &quot;&quot;&quot;Keras-based attention layer.&quot;&quot;&quot;
<span class='lineno'>  17</span> # pylint: disable=g-classes-have-attributes
<span class='lineno'>  18</span> from __future__ import absolute_import
<span class='lineno'>  19</span> from __future__ import division
<span class='lineno'>  20</span> # from __future__ import google_type_annotations
<span class='lineno'>  21</span> from __future__ import print_function
<span class='lineno'>  22</span> 
<span class='lineno'>  23</span> import collections
<span class='lineno'>  24</span> import math
<span class='lineno'>  25</span> import string
<span class='lineno'>  26</span> 
<span class='lineno'>  27</span> import numpy as np
<span class='lineno'>  28</span> import tensorflow as tf
<span class='lineno'>  29</span> 
<span class='lineno'>  30</span> from official.nlp.modeling.layers import masked_softmax
<span class='lineno'>  31</span> 
<span class='lineno'>  32</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', title='?'>EinsumDense</a> = tf.keras.layers.experimental.EinsumDense
<span class='lineno'>  33</span> <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a> = string.ascii_lowercase
<span class='lineno'>  34</span> 
<span class='lineno'>  35</span> 
<span class='lineno'>  36</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation', title='(int, {None | tuple}) -> (?, ?, int) / (?, {None | tuple}) -> (?, ?, int) / (?, ?) -> (?, ?, int)'>_build_attention_equation</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', title='{None | tuple}'>attn_axes</a>):
<span class='lineno'>  37</span>   &quot;&quot;&quot;Builds einsum equations for the attention computation.
<span class='lineno'>  38</span> 
<span class='lineno'>  39</span>   Query, key, value inputs after projection are expected to have the shape as:
<span class='lineno'>  40</span>   (bs, &lt;non-attention dims&gt;, &lt;attention dims&gt;, num_heads, channels).
<span class='lineno'>  41</span>   bs and &lt;non-attention dims&gt; are treated as &lt;batch dims&gt;.
<span class='lineno'>  42</span>   The attention operations can be generalized:
<span class='lineno'>  43</span>   (1) Query-key dot product:
<span class='lineno'>  44</span>   (&lt;batch dims&gt;, &lt;query attention dims&gt;, num_heads, channels), (&lt;batch dims&gt;,
<span class='lineno'>  45</span>   &lt;key attention dims&gt;, num_heads, channels) -&gt; (&lt;batch dims&gt;,
<span class='lineno'>  46</span>   num_heads, &lt;query attention dims&gt;, &lt;key attention dims&gt;)
<span class='lineno'>  47</span>   (2) Combination:
<span class='lineno'>  48</span>   (&lt;batch dims&gt;, num_heads, &lt;query attention dims&gt;, &lt;key attention dims&gt;),
<span class='lineno'>  49</span>   (&lt;batch dims&gt;, &lt;value attention dims&gt;, num_heads, channels) -&gt; (&lt;batch dims&gt;,
<span class='lineno'>  50</span>   &lt;query attention dims&gt;, num_heads, channels)
<span class='lineno'>  51</span> 
<span class='lineno'>  52</span>   Args:
<span class='lineno'>  53</span>     qkv_rank: the rank of query, key, value tensors.
<span class='lineno'>  54</span>     attn_axes: a list/tuple of axes, [1, rank), that will do attention.
<span class='lineno'>  55</span> 
<span class='lineno'>  56</span>   Returns:
<span class='lineno'>  57</span>     Einsum equations.
<span class='lineno'>  58</span>   &quot;&quot;&quot;
<span class='lineno'>  59</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a>[:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a>]
<span class='lineno'>  60</span>   # `batch_dims` includes the head dim.
<span class='lineno'>  61</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', title='tuple'>batch_dims</a> = tuple(np.delete(range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a>), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', title='{None | tuple}'>attn_axes</a> + (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a> - 1,)))
<span class='lineno'>  62</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', title='int'>letter_offset</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a>
<span class='lineno'>  63</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a> = &quot;&quot;
<span class='lineno'>  64</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='int'>i</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a>):
<span class='lineno'>  65</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='int'>i</a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', title='tuple'>batch_dims</a> or <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='int'>i</a> == <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.qkv_rank', title='int'>qkv_rank</a> - 1:
<span class='lineno'>  66</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='int'>i</a>]
<span class='lineno'>  67</span>     else:
<span class='lineno'>  68</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', title='int'>letter_offset</a>]
<span class='lineno'>  69</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', title='int'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.letter_offset', title='int'>letter_offset</a></a> += 1
<span class='lineno'>  70</span> 
<span class='lineno'>  71</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', title='str'>product_notation</a> = &quot;&quot;.join([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a>] for <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a></a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.batch_dims', title='tuple'>batch_dims</a>] +
<span class='lineno'>  72</span>                              [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a>] for <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a></a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', title='{None | tuple}'>attn_axes</a>] +
<span class='lineno'>  73</span>                              [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a>] for <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.i', title='?'>i</a></a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_axes', title='{None | tuple}'>attn_axes</a>])
<span class='lineno'>  74</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.dot_product_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.dot_product_equation', title='?'>dot_product_equation</a> = &quot;%s,%s-&gt;%s&quot; % (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a>,
<span class='lineno'>  75</span>                                         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', title='str'>product_notation</a>)
<span class='lineno'>  76</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_scores_rank', title='int'>attn_scores_rank</a> = len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', title='str'>product_notation</a>)
<span class='lineno'>  77</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.combine_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.combine_equation', title='?'>combine_equation</a> = &quot;%s,%s-&gt;%s&quot; % (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.product_notation', title='str'>product_notation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.source_notation', title='str'>source_notation</a>,
<span class='lineno'>  78</span>                                     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.target_notation', title='?'>target_notation</a>)
<span class='lineno'>  79</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.dot_product_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.dot_product_equation', title='?'>dot_product_equation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.combine_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.combine_equation', title='?'>combine_equation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation.attn_scores_rank', title='int'>attn_scores_rank</a>
<span class='lineno'>  80</span> 
<span class='lineno'>  81</span> 
<span class='lineno'>  82</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', title='(int, int, int) -> (?, str, int) / (?, ?, ?) -> (?, str, int)'>_build_proj_equation</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', title='int'>free_dims</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', title='int'>bound_dims</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_dims', title='int'>output_dims</a>):
<span class='lineno'>  83</span>   &quot;&quot;&quot;Builds an einsum equation for projections inside multi-head attention.&quot;&quot;&quot;
<span class='lineno'>  84</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'>input_str</a> = &quot;&quot;
<span class='lineno'>  85</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'>kernel_str</a> = &quot;&quot;
<span class='lineno'>  86</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'>output_str</a> = &quot;&quot;
<span class='lineno'>  87</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', title='str'>bias_axes</a> = &quot;&quot;
<span class='lineno'>  88</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a> = 0
<span class='lineno'>  89</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', title='int'>free_dims</a>):
<span class='lineno'>  90</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a>]
<span class='lineno'>  91</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'>input_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'>  92</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'>output_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'>  93</span> 
<span class='lineno'>  94</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.free_dims', title='int'>free_dims</a>
<span class='lineno'>  95</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', title='int'>bound_dims</a>):
<span class='lineno'>  96</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a>]
<span class='lineno'>  97</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'>input_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'>  98</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'>kernel_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'>  99</span> 
<span class='lineno'> 100</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bound_dims', title='int'>bound_dims</a>
<span class='lineno'> 101</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_dims', title='int'>output_dims</a>):
<span class='lineno'> 102</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._CHR_IDX', title='?'>_CHR_IDX</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.i', title='int'>i</a> + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.letter_offset', title='int'>letter_offset</a>]
<span class='lineno'> 103</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'>kernel_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'> 104</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'>output_str</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'> 105</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', title='str'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', title='str'>bias_axes</a></a> += <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.char', title='?'>char</a>
<span class='lineno'> 106</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.equation', title='?'>equation</a> = &quot;%s,%s-&gt;%s&quot; % (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.input_str', title='str'>input_str</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.kernel_str', title='str'>kernel_str</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'>output_str</a>)
<span class='lineno'> 107</span> 
<span class='lineno'> 108</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.equation', title='?'>equation</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.bias_axes', title='str'>bias_axes</a>, len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation.output_str', title='str'>output_str</a>)
<span class='lineno'> 109</span> 
<span class='lineno'> 110</span> 
<span class='lineno'> 111</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', title='(?, ?) -> list / (int, [None]) -> list / (int, [?]) -> list / (int, {[?] | [None]}) -> list'>_get_output_shape</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.output_rank', title='int'>output_rank</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', title='{[?] | [None]}'>known_last_dims</a>):
<span class='lineno'> 112</span>   return [None] * (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.output_rank', title='int'>output_rank</a> - len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', title='{[?] | [None]}'>known_last_dims</a>)) + list(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape.known_last_dims', title='{[?] | [None]}'>known_last_dims</a>)
<span class='lineno'> 113</span> 
<span class='lineno'> 114</span> 
<span class='lineno'> 115</span> @tf.keras.utils.register_keras_serializable(package=&quot;Text&quot;)
<span class='lineno'> 116</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', title='<MultiHeadAttention>'>MultiHeadAttention</a>(tf.keras.layers.Layer):
<span class='lineno'> 117</span>   &quot;&quot;&quot;MultiHeadAttention layer.
<span class='lineno'> 118</span> 
<span class='lineno'> 119</span>   This is an implementation of multi-headed attention based on &quot;Attention
<span class='lineno'> 120</span>   is all you Need&quot;. If `query`, `key,` `value` are the same, then
<span class='lineno'> 121</span>   this is self-attention. Each timestep in `query` attends to the
<span class='lineno'> 122</span>   corresponding sequence in `key`, and returns a fixed-width vector.
<span class='lineno'> 123</span> 
<span class='lineno'> 124</span>   This layer first projects `query`, `key` and `value`. These are
<span class='lineno'> 125</span>   (effectively) a list of tensors of length `num_attention_heads`, where the
<span class='lineno'> 126</span>   corresponding shapes are [batch_size, &lt;query dimensions&gt;, key_size],
<span class='lineno'> 127</span>   [batch_size, &lt;key/value dimensions&gt;, key_size],
<span class='lineno'> 128</span>   [batch_size, &lt;key/value dimensions&gt;, value_size].
<span class='lineno'> 129</span> 
<span class='lineno'> 130</span>   Then, the query and key tensors are dot-producted and scaled. These are
<span class='lineno'> 131</span>   softmaxed to obtain attention probabilities. The value tensors are then
<span class='lineno'> 132</span>   interpolated by these probabilities, then concatenated back to a single
<span class='lineno'> 133</span>   tensor.
<span class='lineno'> 134</span> 
<span class='lineno'> 135</span>   Finally, the result tensor with the last dimension as value_size can take an
<span class='lineno'> 136</span>   linear projection and return.
<span class='lineno'> 137</span> 
<span class='lineno'> 138</span>   Examples:
<span class='lineno'> 139</span> 
<span class='lineno'> 140</span>   Performs 1D cross-attention over two sequence inputs with an attention mask.
<span class='lineno'> 141</span>   Returns the additional attention weights over heads.
<span class='lineno'> 142</span> 
<span class='lineno'> 143</span>   &gt;&gt;&gt; layer = MultiHeadAttention(num_heads=2, key_size=2,
<span class='lineno'> 144</span>   ...                            return_attention_scores=True)
<span class='lineno'> 145</span>   &gt;&gt;&gt; target = tf.keras.Input(shape=[8, 16])
<span class='lineno'> 146</span>   &gt;&gt;&gt; source = tf.keras.Input(shape=[4, 16])
<span class='lineno'> 147</span>   &gt;&gt;&gt; mask_tensor = tf.keras.Input(shape=[8, 4])
<span class='lineno'> 148</span>   &gt;&gt;&gt; output_tensor, weights = layer([target, source])
<span class='lineno'> 149</span>   &gt;&gt;&gt; print(output_tensor.shape), print(weights.shape)
<span class='lineno'> 150</span>   (None, 8, 16)  (None, 2, 8, 4)
<span class='lineno'> 151</span> 
<span class='lineno'> 152</span>   Performs 2D self-attention over a 5D input tensor on axes 2 and 3.
<span class='lineno'> 153</span> 
<span class='lineno'> 154</span>   &gt;&gt;&gt; layer = MultiHeadAttention(num_heads=2, key_size=2, attention_axes=(2, 3))
<span class='lineno'> 155</span>   &gt;&gt;&gt; input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])
<span class='lineno'> 156</span>   &gt;&gt;&gt; output_tensor = layer([input_tensor, input_tensor])
<span class='lineno'> 157</span>   &gt;&gt;&gt; print(output_tensor.shape)
<span class='lineno'> 158</span>   (None, 5, 3, 4, 16)
<span class='lineno'> 159</span> 
<span class='lineno'> 160</span>   Arguments:
<span class='lineno'> 161</span>     num_heads: Number of attention heads.
<span class='lineno'> 162</span>     key_size: Size of each attention head for query and key.
<span class='lineno'> 163</span>     value_size:  Size of each attention head for value.
<span class='lineno'> 164</span>     dropout: Dropout probability.
<span class='lineno'> 165</span>     use_bias: Boolean, whether the dense layers use bias vectors/matrices.
<span class='lineno'> 166</span>     output_shape: The expected shape of an output tensor, besides the batch and
<span class='lineno'> 167</span>       sequence dims. If not specified, projects back to the key feature dim.
<span class='lineno'> 168</span>     attention_axes: axes over which the attention is applied. `None` means
<span class='lineno'> 169</span>       attention over all axes, but batch, heads, and features.
<span class='lineno'> 170</span>     return_attention_scores: bool, if `True`, returns the multi-head
<span class='lineno'> 171</span>       attention scores as an additional output argument.
<span class='lineno'> 172</span>     kernel_initializer: Initializer for dense layer kernels.
<span class='lineno'> 173</span>     bias_initializer: Initializer for dense layer biases.
<span class='lineno'> 174</span>     kernel_regularizer: Regularizer for dense layer kernels.
<span class='lineno'> 175</span>     bias_regularizer: Regularizer for dense layer biases.
<span class='lineno'> 176</span>     activity_regularizer: Regularizer for dense layer activity.
<span class='lineno'> 177</span>     kernel_constraint: Constraint for dense layer kernels.
<span class='lineno'> 178</span>     bias_constraint: Constraint for dense layer kernels.
<span class='lineno'> 179</span>   &quot;&quot;&quot;
<span class='lineno'> 180</span> 
<span class='lineno'> 181</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>,
<span class='lineno'> 182</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.num_heads', title='?'>num_heads</a>,
<span class='lineno'> 183</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', title='?'>key_size</a>,
<span class='lineno'> 184</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', title='None'>value_size</a>=None,
<span class='lineno'> 185</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.dropout', title='float'>dropout</a>=0.0,
<span class='lineno'> 186</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.use_bias', title='bool'>use_bias</a>=True,
<span class='lineno'> 187</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.output_shape', title='None'>output_shape</a>=None,
<span class='lineno'> 188</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', title='None'>attention_axes</a>=None,
<span class='lineno'> 189</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.return_attention_scores', title='bool'>return_attention_scores</a>=False,
<span class='lineno'> 190</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_initializer', title='str'>kernel_initializer</a>=&quot;glorot_uniform&quot;,
<span class='lineno'> 191</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_initializer', title='str'>bias_initializer</a>=&quot;zeros&quot;,
<span class='lineno'> 192</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_regularizer', title='None'>kernel_regularizer</a>=None,
<span class='lineno'> 193</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_regularizer', title='None'>bias_regularizer</a>=None,
<span class='lineno'> 194</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.activity_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.activity_regularizer', title='None'>activity_regularizer</a>=None,
<span class='lineno'> 195</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_constraint', title='None'>kernel_constraint</a>=None,
<span class='lineno'> 196</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_constraint', title='None'>bias_constraint</a>=None,
<span class='lineno'> 197</span>                **kwargs):
<span class='lineno'> 198</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', title='<MultiHeadAttention>'>MultiHeadAttention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>).__init__(**kwargs)
<span class='lineno'> 199</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', title='?'>_num_heads</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.num_heads', title='?'>num_heads</a>
<span class='lineno'> 200</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', title='?'>_key_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', title='?'>key_size</a>
<span class='lineno'> 201</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', title='None'>_value_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', title='None'>value_size</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.value_size', title='None'>value_size</a> else <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.key_size', title='?'>key_size</a>
<span class='lineno'> 202</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', title='float'>_dropout</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.dropout', title='float'>dropout</a>
<span class='lineno'> 203</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.use_bias', title='bool'>use_bias</a>
<span class='lineno'> 204</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.output_shape', title='None'>output_shape</a>
<span class='lineno'> 205</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', title='bool'>_return_attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.return_attention_scores', title='bool'>return_attention_scores</a>
<span class='lineno'> 206</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', title='?'>_kernel_initializer</a> = tf.keras.initializers.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_initializer', title='str'>kernel_initializer</a>)
<span class='lineno'> 207</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', title='?'>_bias_initializer</a> = tf.keras.initializers.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_initializer', title='str'>bias_initializer</a>)
<span class='lineno'> 208</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', title='?'>_kernel_regularizer</a> = tf.keras.regularizers.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_regularizer', title='None'>kernel_regularizer</a>)
<span class='lineno'> 209</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', title='?'>_bias_regularizer</a> = tf.keras.regularizers.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_regularizer', title='None'>bias_regularizer</a>)
<span class='lineno'> 210</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', title='?'>_kernel_constraint</a> = tf.keras.constraints.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.kernel_constraint', title='None'>kernel_constraint</a>)
<span class='lineno'> 211</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', title='?'>_bias_constraint</a> = tf.keras.constraints.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.bias_constraint', title='None'>bias_constraint</a>)
<span class='lineno'> 212</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', title='None'>attention_axes</a> is not None and not isinstance(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', title='None'>attention_axes</a>,
<span class='lineno'> 213</span>                                                      collections.abc.Sized):
<span class='lineno'> 214</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a> = (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', title='None'>attention_axes</a>,)
<span class='lineno'> 215</span>     else:
<span class='lineno'> 216</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.self', title='{CachedAttention | MultiHeadAttention}'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.__init__.attention_axes', title='None'>attention_axes</a>
<span class='lineno'> 217</span> 
<span class='lineno'> 218</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config', title='MultiHeadAttention -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>):
<span class='lineno'> 219</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.config', title='dict'>config</a> = {
<span class='lineno'> 220</span>         &quot;num_heads&quot;:
<span class='lineno'> 221</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', title='?'>_num_heads</a>,
<span class='lineno'> 222</span>         &quot;key_size&quot;:
<span class='lineno'> 223</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', title='?'>_key_size</a>,
<span class='lineno'> 224</span>         &quot;value_size&quot;:
<span class='lineno'> 225</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', title='None'>_value_size</a>,
<span class='lineno'> 226</span>         &quot;dropout&quot;:
<span class='lineno'> 227</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', title='float'>_dropout</a>,
<span class='lineno'> 228</span>         &quot;use_bias&quot;:
<span class='lineno'> 229</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a>,
<span class='lineno'> 230</span>         &quot;output_shape&quot;:
<span class='lineno'> 231</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a>,
<span class='lineno'> 232</span>         &quot;attention_axes&quot;:
<span class='lineno'> 233</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a>,
<span class='lineno'> 234</span>         &quot;return_attention_scores&quot;:
<span class='lineno'> 235</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', title='bool'>_return_attention_scores</a>,
<span class='lineno'> 236</span>         &quot;kernel_initializer&quot;:
<span class='lineno'> 237</span>             tf.keras.initializers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', title='?'>_kernel_initializer</a>),
<span class='lineno'> 238</span>         &quot;bias_initializer&quot;:
<span class='lineno'> 239</span>             tf.keras.initializers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', title='?'>_bias_initializer</a>),
<span class='lineno'> 240</span>         &quot;kernel_regularizer&quot;:
<span class='lineno'> 241</span>             tf.keras.regularizers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', title='?'>_kernel_regularizer</a>),
<span class='lineno'> 242</span>         &quot;bias_regularizer&quot;:
<span class='lineno'> 243</span>             tf.keras.regularizers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', title='?'>_bias_regularizer</a>),
<span class='lineno'> 244</span>         &quot;activity_regularizer&quot;:
<span class='lineno'> 245</span>             tf.keras.regularizers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>._activity_regularizer),
<span class='lineno'> 246</span>         &quot;kernel_constraint&quot;:
<span class='lineno'> 247</span>             tf.keras.constraints.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', title='?'>_kernel_constraint</a>),
<span class='lineno'> 248</span>         &quot;bias_constraint&quot;:
<span class='lineno'> 249</span>             tf.keras.constraints.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', title='?'>_bias_constraint</a>)
<span class='lineno'> 250</span>     }
<span class='lineno'> 251</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.base_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.base_config', title='?'>base_config</a> = super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', title='<MultiHeadAttention>'>MultiHeadAttention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.self', title='MultiHeadAttention'>self</a>).get_config()
<span class='lineno'> 252</span>     return dict(list(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.base_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.base_config', title='?'>base_config</a>.items()) + list(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.get_config.config', title='dict'>config</a>.items()))
<span class='lineno'> 253</span> 
<span class='lineno'> 254</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build', title='(MultiHeadAttention, ?) -> None'>build</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', title='?'>input_shape</a>):
<span class='lineno'> 255</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', title='int'>inputs_len</a> = len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 256</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', title='int'>inputs_len</a> &gt; 3 or <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', title='int'>inputs_len</a> &lt; 2:
<span class='lineno'> 257</span>       raise ValueError(
<span class='lineno'> 258</span>           &quot;Expects inputs list of length 2 or 3, namely [query, value] or &quot;
<span class='lineno'> 259</span>           &quot;[query, value, key]. &quot;
<span class='lineno'> 260</span>           &quot;Given length: %d&quot; % inputs_len)
<span class='lineno'> 261</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', title='?'>tensor_shapes</a> = tf.nest.map_structure(tf.TensorShape, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 262</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', title='?'>query_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', title='?'>tensor_shapes</a>[0]
<span class='lineno'> 263</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', title='?'>value_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', title='?'>tensor_shapes</a>[1]
<span class='lineno'> 264</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.key_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.key_shape', title='?'>key_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.tensor_shapes', title='?'>tensor_shapes</a>[2] if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.inputs_len', title='int'>inputs_len</a> == 3 else <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', title='?'>value_shape</a>
<span class='lineno'> 265</span> 
<span class='lineno'> 266</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.common_kwargs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.common_kwargs', title='dict'>common_kwargs</a> = dict(
<span class='lineno'> 267</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_initializer', title='?'>_kernel_initializer</a>,
<span class='lineno'> 268</span>         bias_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_initializer', title='?'>_bias_initializer</a>,
<span class='lineno'> 269</span>         kernel_regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_regularizer', title='?'>_kernel_regularizer</a>,
<span class='lineno'> 270</span>         bias_regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_regularizer', title='?'>_bias_regularizer</a>,
<span class='lineno'> 271</span>         activity_regularizer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>._activity_regularizer,
<span class='lineno'> 272</span>         kernel_constraint=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._kernel_constraint', title='?'>_kernel_constraint</a>,
<span class='lineno'> 273</span>         bias_constraint=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._bias_constraint', title='?'>_bias_constraint</a>)
<span class='lineno'> 274</span> 
<span class='lineno'> 275</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', title='int'>free_dims</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', title='?'>query_shape</a>.rank - 1
<span class='lineno'> 276</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', title='(int, int, int) -> (?, str, int) / (?, ?, ?) -> (?, str, int)'>_build_proj_equation</a>(
<span class='lineno'> 277</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', title='int'>free_dims</a>, bound_dims=1, output_dims=2)
<span class='lineno'> 278</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._query_dense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._query_dense', title='?'>_query_dense</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', title='?'>EinsumDense</a>(
<span class='lineno'> 279</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>,
<span class='lineno'> 280</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', title='(?, ?) -> list / (int, [None]) -> list / (int, [?]) -> list / (int, {[?] | [None]}) -> list'>_get_output_shape</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> - 1,
<span class='lineno'> 281</span>                                        [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', title='?'>_num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', title='?'>_key_size</a>]),
<span class='lineno'> 282</span>         bias_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a> else None,
<span class='lineno'> 283</span>         name=&quot;query&quot;,
<span class='lineno'> 284</span>         **common_kwargs)
<span class='lineno'> 285</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', title='(int, int, int) -> (?, str, int) / (?, ?, ?) -> (?, str, int)'>_build_proj_equation</a>(
<span class='lineno'> 286</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.key_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.key_shape', title='?'>key_shape</a>.rank - 1, bound_dims=1, output_dims=2)
<span class='lineno'> 287</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_dense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_dense', title='?'>_key_dense</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', title='?'>EinsumDense</a>(
<span class='lineno'> 288</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>,
<span class='lineno'> 289</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', title='(?, ?) -> list / (int, [None]) -> list / (int, [?]) -> list / (int, {[?] | [None]}) -> list'>_get_output_shape</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> - 1,
<span class='lineno'> 290</span>                                        [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', title='?'>_num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', title='?'>_key_size</a>]),
<span class='lineno'> 291</span>         bias_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a> else None,
<span class='lineno'> 292</span>         name=&quot;key&quot;,
<span class='lineno'> 293</span>         **common_kwargs)
<span class='lineno'> 294</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', title='(int, int, int) -> (?, str, int) / (?, ?, ?) -> (?, str, int)'>_build_proj_equation</a>(
<span class='lineno'> 295</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.value_shape', title='?'>value_shape</a>.rank - 1, bound_dims=1, output_dims=2)
<span class='lineno'> 296</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_dense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_dense', title='?'>_value_dense</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', title='?'>EinsumDense</a>(
<span class='lineno'> 297</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>,
<span class='lineno'> 298</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', title='(?, ?) -> list / (int, [None]) -> list / (int, [?]) -> list / (int, {[?] | [None]}) -> list'>_get_output_shape</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> - 1,
<span class='lineno'> 299</span>                                        [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._num_heads', title='?'>_num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._value_size', title='None'>_value_size</a>]),
<span class='lineno'> 300</span>         bias_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a> else None,
<span class='lineno'> 301</span>         name=&quot;value&quot;,
<span class='lineno'> 302</span>         **common_kwargs)
<span class='lineno'> 303</span> 
<span class='lineno'> 304</span>     # Builds the attention computations for multi-head dot product attention.
<span class='lineno'> 305</span>     # These computations could be wrapped into the keras attention layer once it
<span class='lineno'> 306</span>     # support mult-head einsum computations.
<span class='lineno'> 307</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention', title='(MultiHeadAttention, int) -> None / (MultiHeadAttention, ?) -> None'>_build_attention</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a>)
<span class='lineno'> 308</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a>:
<span class='lineno'> 309</span>       if not isinstance(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a>, collections.abc.Sized):
<span class='lineno'> 310</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', title='[None]'>output_shape</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a>]
<span class='lineno'> 311</span>       else:
<span class='lineno'> 312</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', title='None'>output_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_shape', title='None'>_output_shape</a>
<span class='lineno'> 313</span>     else:
<span class='lineno'> 314</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', title='[?]'>output_shape</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.query_shape', title='?'>query_shape</a>[-1]]
<span class='lineno'> 315</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_proj_equation', title='(int, int, int) -> (?, str, int) / (?, ?, ?) -> (?, str, int)'>_build_proj_equation</a>(
<span class='lineno'> 316</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.free_dims', title='int'>free_dims</a>, bound_dims=2, output_dims=len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', title='{[?] | [None]}'>output_shape</a>))
<span class='lineno'> 317</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_dense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._output_dense', title='?'>_output_dense</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.EinsumDense', title='?'>EinsumDense</a>(
<span class='lineno'> 318</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.einsum_equation', title='?'>einsum_equation</a>,
<span class='lineno'> 319</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._get_output_shape', title='(?, ?) -> list / (int, [None]) -> list / (int, [?]) -> list / (int, {[?] | [None]}) -> list'>_get_output_shape</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_rank', title='int'>output_rank</a> - 1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.output_shape', title='{[?] | [None]}'>output_shape</a>),
<span class='lineno'> 320</span>         bias_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.bias_axes', title='str'>bias_axes</a> if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._use_bias', title='bool'>_use_bias</a> else None,
<span class='lineno'> 321</span>         name=&quot;attention_output&quot;,
<span class='lineno'> 322</span>         **common_kwargs)
<span class='lineno'> 323</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', title='<MultiHeadAttention>'>MultiHeadAttention</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.self', title='MultiHeadAttention'>self</a>).build(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.build.input_shape', title='?'>input_shape</a>)
<span class='lineno'> 324</span> 
<span class='lineno'> 325</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention', title='(MultiHeadAttention, int) -> None / (MultiHeadAttention, ?) -> None'>_build_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', title='int'>qkv_rank</a>):
<span class='lineno'> 326</span>     &quot;&quot;&quot;Builds multi-head dot-product attention computations.
<span class='lineno'> 327</span> 
<span class='lineno'> 328</span>     This function builds attributes necessary for `_compute_attention` to
<span class='lineno'> 329</span>     costomize attention computation to replace the default dot-product
<span class='lineno'> 330</span>     attention.
<span class='lineno'> 331</span> 
<span class='lineno'> 332</span>     Args:
<span class='lineno'> 333</span>       qkv_rank: the rank of query, key, value tensors.
<span class='lineno'> 334</span>     &quot;&quot;&quot;
<span class='lineno'> 335</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a> is None:
<span class='lineno'> 336</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a> = tuple(range(1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', title='int'>qkv_rank</a> - 2))
<span class='lineno'> 337</span>     else:
<span class='lineno'> 338</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a> = tuple(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a>)
<span class='lineno'> 339</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dot_product_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dot_product_equation', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dot_product_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dot_product_equation', title='?'>_dot_product_equation</a></a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._combine_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._combine_equation', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._combine_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._combine_equation', title='?'>_combine_equation</a></a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a> = (
<span class='lineno'> 340</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention._build_attention_equation', title='(int, {None | tuple}) -> (?, ?, int) / (?, {None | tuple}) -> (?, ?, int) / (?, ?) -> (?, ?, int)'>_build_attention_equation</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.qkv_rank', title='int'>qkv_rank</a>, attn_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a>))
<span class='lineno'> 341</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.norm_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.norm_axes', title='tuple'>norm_axes</a> = tuple(
<span class='lineno'> 342</span>         range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a> - len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._attention_axes', title='{None | tuple}'>_attention_axes</a>), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.attn_scores_rank', title='int'>attn_scores_rank</a>))
<span class='lineno'> 343</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._masked_softmax', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._masked_softmax', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._masked_softmax', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._masked_softmax', title='?'>_masked_softmax</a></a> = masked_softmax.MaskedSoftmax(
<span class='lineno'> 344</span>         mask_expansion_axes=[1], normalization_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.norm_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.norm_axes', title='tuple'>norm_axes</a>)
<span class='lineno'> 345</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout_layer', title='?'><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout_layer', title='?'>_dropout_layer</a></a> = tf.keras.layers.Dropout(rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._build_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._dropout', title='float'>_dropout</a>)
<span class='lineno'> 346</span> 
<span class='lineno'> 347</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention', title='(MultiHeadAttention, ?, ?, ?, None) -> (?, ?)'>_compute_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>,
<span class='lineno'> 348</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.query_tensor', title='?'>query_tensor</a>,
<span class='lineno'> 349</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 350</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.value_tensor', title='?'>value_tensor</a>,
<span class='lineno'> 351</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_mask', title='None'>attention_mask</a>=None):
<span class='lineno'> 352</span>     &quot;&quot;&quot;Applies Dot-product attention with query, key, value tensors.
<span class='lineno'> 353</span> 
<span class='lineno'> 354</span>     This function defines the computation inside `call` with projected
<span class='lineno'> 355</span>     multi-head Q, K, V inputs. Users can override this function for customized
<span class='lineno'> 356</span>     attention implementation.
<span class='lineno'> 357</span> 
<span class='lineno'> 358</span>     Args:
<span class='lineno'> 359</span>       query_tensor: Projected query `Tensor` of shape `[B, T, N, key_size]`.
<span class='lineno'> 360</span>       key_tensor: Projected key `Tensor` of shape `[B, T, N, key_size]`.
<span class='lineno'> 361</span>       value_tensor: Projected value `Tensor` of shape `[B, T, N, value_size]`.
<span class='lineno'> 362</span>       attention_mask: a boolean mask of shape `[B, T, S]`, that prevents
<span class='lineno'> 363</span>         attention to certain positions.
<span class='lineno'> 364</span> 
<span class='lineno'> 365</span>     Returns:
<span class='lineno'> 366</span>       attention_output: Multi-headed outputs of attention computation.
<span class='lineno'> 367</span>       attention_scores: Multi-headed attention weights.
<span class='lineno'> 368</span>     &quot;&quot;&quot;
<span class='lineno'> 369</span>     # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw
<span class='lineno'> 370</span>     # attention scores.
<span class='lineno'> 371</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>._dot_product_equation, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 372</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.query_tensor', title='?'>query_tensor</a>)
<span class='lineno'> 373</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 374</span>                                    1.0 / math.sqrt(float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._key_size', title='?'>_key_size</a>)))
<span class='lineno'> 375</span> 
<span class='lineno'> 376</span>     # Normalize the attention scores to probabilities.
<span class='lineno'> 377</span>     # `attention_scores` = [B, N, T, S]
<span class='lineno'> 378</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>._masked_softmax(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_mask', title='None'>attention_mask</a>)
<span class='lineno'> 379</span> 
<span class='lineno'> 380</span>     # This is actually dropping out entire tokens to attend to, which might
<span class='lineno'> 381</span>     # seem a bit unusual, but is taken from the original Transformer paper.
<span class='lineno'> 382</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores_dropout', title='?'>attention_scores_dropout</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>._dropout_layer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a>)
<span class='lineno'> 383</span> 
<span class='lineno'> 384</span>     # `context_layer` = [B, T, N, H]
<span class='lineno'> 385</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_output', title='?'>attention_output</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.self', title='MultiHeadAttention'>self</a>._combine_equation,
<span class='lineno'> 386</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores_dropout', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores_dropout', title='?'>attention_scores_dropout</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.value_tensor', title='?'>value_tensor</a>)
<span class='lineno'> 387</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_output', title='?'>attention_output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention.attention_scores', title='?'>attention_scores</a>
<span class='lineno'> 388</span> 
<span class='lineno'> 389</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call', title='(MultiHeadAttention, ?, None) -> (?, ?)'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', title='?'>inputs</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_mask', title='None'>attention_mask</a>=None):
<span class='lineno'> 390</span>     &quot;&quot;&quot;Implements the forward pass.
<span class='lineno'> 391</span> 
<span class='lineno'> 392</span>     Size glossary:
<span class='lineno'> 393</span>       * Number of heads (H): the number of attention heads.
<span class='lineno'> 394</span>       * Value size (V): the size of each value embedding per head.
<span class='lineno'> 395</span>       * Key size (K): the size of each key embedding per head. Equally, the size
<span class='lineno'> 396</span>           of each query embedding per head. Typically K &lt;= V.
<span class='lineno'> 397</span>       * Batch dimensions (B).
<span class='lineno'> 398</span>       * Query (target) attention axes shape (T).
<span class='lineno'> 399</span>       * Value (source) attention axes shape (S), the rank must match the target.
<span class='lineno'> 400</span> 
<span class='lineno'> 401</span>     Args:
<span class='lineno'> 402</span>       inputs: List of the following tensors:
<span class='lineno'> 403</span>         * query: Query `Tensor` of shape `[B, T, dim]`.
<span class='lineno'> 404</span>         * value: Value `Tensor` of shape `[B, S, dim]`.
<span class='lineno'> 405</span>         * key: Optional key `Tensor` of shape `[B, S, dim]`. If not given, will
<span class='lineno'> 406</span>           use `value` for both `key` and `value`, which is the most common case.
<span class='lineno'> 407</span>       attention_mask: a boolean mask of shape `[B, T, S]`, that prevents
<span class='lineno'> 408</span>         attention to certain positions.
<span class='lineno'> 409</span> 
<span class='lineno'> 410</span>     Returns:
<span class='lineno'> 411</span>       attention_output: The result of the computation, of shape [B, T, E],
<span class='lineno'> 412</span>         where `T` is for target sequence shapes and `E` is the query input last
<span class='lineno'> 413</span>         dimension if `output_shape` is `None`. Otherwise, the multi-head outputs
<span class='lineno'> 414</span>         are project to the shape specified by `output_shape`.
<span class='lineno'> 415</span>       attention_scores: [Optional] multi-head attention coeffients over
<span class='lineno'> 416</span>       attention
<span class='lineno'> 417</span>         axes.
<span class='lineno'> 418</span>     &quot;&quot;&quot;
<span class='lineno'> 419</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', title='int'>inputs_len</a> = len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', title='?'>inputs</a>)
<span class='lineno'> 420</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', title='int'>inputs_len</a> &gt; 3 or <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', title='int'>inputs_len</a> &lt; 2:
<span class='lineno'> 421</span>       raise ValueError(
<span class='lineno'> 422</span>           &quot;Expects inputs list of length 2 or 3, namely [query, value] or &quot;
<span class='lineno'> 423</span>           &quot;[query, value, key]. &quot;
<span class='lineno'> 424</span>           &quot;Given length: %d&quot; % inputs_len)
<span class='lineno'> 425</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query', title='?'>query</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', title='?'>inputs</a>[0]
<span class='lineno'> 426</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', title='?'>value</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', title='?'>inputs</a>[1]
<span class='lineno'> 427</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key', title='?'>key</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs', title='?'>inputs</a>[2] if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.inputs_len', title='int'>inputs_len</a> == 3 else <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', title='?'>value</a>
<span class='lineno'> 428</span> 
<span class='lineno'> 429</span>     #   N = `num_attention_heads`
<span class='lineno'> 430</span>     #   H = `size_per_head`
<span class='lineno'> 431</span>     # `query_tensor` = [B, T, N ,H]
<span class='lineno'> 432</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query_tensor', title='?'>query_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>._query_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query', title='?'>query</a>)
<span class='lineno'> 433</span> 
<span class='lineno'> 434</span>     # `key_tensor` = [B, S, N, H]
<span class='lineno'> 435</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key_tensor', title='?'>key_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>._key_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key', title='?'>key</a>)
<span class='lineno'> 436</span> 
<span class='lineno'> 437</span>     # `value_tensor` = [B, S, N, H]
<span class='lineno'> 438</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value_tensor', title='?'>value_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>._value_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value', title='?'>value</a>)
<span class='lineno'> 439</span> 
<span class='lineno'> 440</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', title='?'>attention_output</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_scores', title='?'>attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._compute_attention', title='(MultiHeadAttention, ?, ?, ?, None) -> (?, ?)'>_compute_attention</a>(
<span class='lineno'> 441</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.query_tensor', title='?'>query_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.key_tensor', title='?'>key_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.value_tensor', title='?'>value_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_mask', title='None'>attention_mask</a>)
<span class='lineno'> 442</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', title='?'>attention_output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>._output_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', title='?'>attention_output</a>)
<span class='lineno'> 443</span> 
<span class='lineno'> 444</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.self', title='MultiHeadAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention._return_attention_scores', title='bool'>_return_attention_scores</a>:
<span class='lineno'> 445</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', title='?'>attention_output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_scores', title='?'>attention_scores</a>
<span class='lineno'> 446</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention.call.attention_output', title='?'>attention_output</a>
<span class='lineno'> 447</span> 
<span class='lineno'> 448</span> 
<span class='lineno'> 449</span> @tf.keras.utils.register_keras_serializable(package=&quot;Text&quot;)
<span class='lineno'> 450</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention', title='<CachedAttention>'>CachedAttention</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.MultiHeadAttention', title='<MultiHeadAttention>'>MultiHeadAttention</a>):
<span class='lineno'> 451</span>   &quot;&quot;&quot;Attention layer with cache used for auto-agressive decoding.
<span class='lineno'> 452</span> 
<span class='lineno'> 453</span>   Arguments are the same as `MultiHeadAttention` layer.
<span class='lineno'> 454</span>   &quot;&quot;&quot;
<span class='lineno'> 455</span> 
<span class='lineno'> 456</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache', title='(CachedAttention, ?, ?, None, None) -> (?, ?) / (CachedAttention, ?, ?, ?, ?) -> (?, ?)'>_update_cache</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.self', title='CachedAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', title='None'>decode_loop_step</a>):
<span class='lineno'> 457</span>     &quot;&quot;&quot;Updates cache states and gets full-length key/value tensors.&quot;&quot;&quot;
<span class='lineno'> 458</span>     # Combines cached keys and values with new keys and values.
<span class='lineno'> 459</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', title='None'>decode_loop_step</a> is not None:
<span class='lineno'> 460</span>       # TPU special case.
<span class='lineno'> 461</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', title='?'>key_seq_dim</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;key&quot;].shape.as_list()[1]
<span class='lineno'> 462</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', title='?'>indices</a> = tf.reshape(
<span class='lineno'> 463</span>           tf.one_hot(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', title='None'>decode_loop_step</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', title='?'>key_seq_dim</a>, dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>.dtype),
<span class='lineno'> 464</span>           [1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_seq_dim', title='?'>key_seq_dim</a>, 1, 1])
<span class='lineno'> 465</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;key&quot;] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', title='?'>indices</a>
<span class='lineno'> 466</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', title='?'>value_seq_dim</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;value&quot;].shape.as_list()[1]
<span class='lineno'> 467</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', title='?'>indices</a> = tf.reshape(
<span class='lineno'> 468</span>           tf.one_hot(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.decode_loop_step', title='None'>decode_loop_step</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', title='?'>value_seq_dim</a>, dtype=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>.dtype),
<span class='lineno'> 469</span>           [1, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_seq_dim', title='?'>value_seq_dim</a>, 1, 1])
<span class='lineno'> 470</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;value&quot;] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.indices', title='?'>indices</a>
<span class='lineno'> 471</span>     else:
<span class='lineno'> 472</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a> = tf.concat(
<span class='lineno'> 473</span>           [tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;key&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>.dtype), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>], axis=1)
<span class='lineno'> 474</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a> = tf.concat(
<span class='lineno'> 475</span>           [tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;value&quot;], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>.dtype), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>], axis=1)
<span class='lineno'> 476</span> 
<span class='lineno'> 477</span>     # Update cache
<span class='lineno'> 478</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;key&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>
<span class='lineno'> 479</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.cache', title='None'>cache</a>[&quot;value&quot;] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>
<span class='lineno'> 480</span> 
<span class='lineno'> 481</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.key_tensor', title='?'>key_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache.value_tensor', title='?'>value_tensor</a>
<span class='lineno'> 482</span> 
<span class='lineno'> 483</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call', title='(CachedAttention, ?, None, None, None) -> {(?, ?, None) | (?, None)}'>call</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>,
<span class='lineno'> 484</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', title='?'>inputs</a>,
<span class='lineno'> 485</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_mask', title='None'>attention_mask</a>=None,
<span class='lineno'> 486</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', title='None'>cache</a>=None,
<span class='lineno'> 487</span>            <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.decode_loop_step', title='None'>decode_loop_step</a>=None):
<span class='lineno'> 488</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.from_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.from_tensor', title='?'>from_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', title='?'>inputs</a>[0]
<span class='lineno'> 489</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', title='?'>to_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.inputs', title='?'>inputs</a>[1]
<span class='lineno'> 490</span> 
<span class='lineno'> 491</span>     # Scalar dimensions referenced here:
<span class='lineno'> 492</span>     #   B = batch size (number of sequences)
<span class='lineno'> 493</span>     #   F = `from_tensor` sequence length
<span class='lineno'> 494</span>     #   T = `to_tensor` sequence length
<span class='lineno'> 495</span>     #   N = `num_attention_heads`
<span class='lineno'> 496</span>     #   H = `size_per_head`
<span class='lineno'> 497</span>     # `query_tensor` = [B, F, N ,H]
<span class='lineno'> 498</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.query_tensor', title='?'>query_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._query_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.from_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.from_tensor', title='?'>from_tensor</a>)
<span class='lineno'> 499</span> 
<span class='lineno'> 500</span>     # `key_tensor` = [B, T, N, H]
<span class='lineno'> 501</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', title='?'>key_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._key_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', title='?'>to_tensor</a>)
<span class='lineno'> 502</span> 
<span class='lineno'> 503</span>     # `value_tensor` = [B, T, N, H]
<span class='lineno'> 504</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', title='?'>value_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._value_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.to_tensor', title='?'>to_tensor</a>)
<span class='lineno'> 505</span> 
<span class='lineno'> 506</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', title='None'>cache</a>:
<span class='lineno'> 507</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', title='?'>key_tensor</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', title='?'>value_tensor</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._update_cache', title='(CachedAttention, ?, ?, None, None) -> (?, ?) / (CachedAttention, ?, ?, ?, ?) -> (?, ?)'>_update_cache</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', title='?'>key_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', title='?'>value_tensor</a>,
<span class='lineno'> 508</span>                                                     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', title='None'>cache</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.decode_loop_step', title='None'>decode_loop_step</a>)
<span class='lineno'> 509</span> 
<span class='lineno'> 510</span>     # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw
<span class='lineno'> 511</span>     # attention scores.
<span class='lineno'> 512</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._dot_product_equation, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 513</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.query_tensor', title='?'>query_tensor</a>)
<span class='lineno'> 514</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a> = tf.multiply(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 515</span>                                    1.0 / math.sqrt(float(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._key_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._key_size', title='?'>_key_size</a>)))
<span class='lineno'> 516</span> 
<span class='lineno'> 517</span>     # Normalize the attention scores to probabilities.
<span class='lineno'> 518</span>     # `attention_scores` = [B, N, F, T]
<span class='lineno'> 519</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._masked_softmax(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_mask', title='None'>attention_mask</a>)
<span class='lineno'> 520</span> 
<span class='lineno'> 521</span>     # This is actually dropping out entire tokens to attend to, which might
<span class='lineno'> 522</span>     # seem a bit unusual, but is taken from the original Transformer paper.
<span class='lineno'> 523</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._dropout_layer(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a>)
<span class='lineno'> 524</span>     # `context_layer` = [B, F, N, H]
<span class='lineno'> 525</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', title='?'>attention_output</a> = tf.einsum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._combine_equation, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a>,
<span class='lineno'> 526</span>                                  <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.value_tensor', title='?'>value_tensor</a>)
<span class='lineno'> 527</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', title='?'>attention_output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>._output_dense(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', title='?'>attention_output</a>)
<span class='lineno'> 528</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.self', title='CachedAttention'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._return_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention._return_attention_scores', title='bool'>_return_attention_scores</a>:
<span class='lineno'> 529</span>       return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', title='?'>attention_output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_scores', title='?'>attention_scores</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', title='None'>cache</a>
<span class='lineno'> 530</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.attention_output', title='?'>attention_output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention.CachedAttention.call.cache', title='None'>cache</a>
</pre></td></tr></table></body></html>