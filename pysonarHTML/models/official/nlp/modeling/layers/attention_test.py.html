<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/modeling/layers/attention_test.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest'>MultiHeadAttentionTest</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention'>test_non_masked_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention'>test_non_masked_self_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores'>test_attention_scores</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention'>test_masked_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer'>test_initializer</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention'>test_high_dim_attention</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention'>SubclassAttention</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention'>_build_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention'>_compute_attention</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest'>AttentionSubclassTest</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer'>test_initializer</a></li></ul>
</li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache'>_create_cache</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest'>CachedAttentionTest</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention'>test_masked_attention</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode'>test_padded_decode</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Tests for the attention layer.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> from __future__ import absolute_import
<span class='lineno'>  18</span> from __future__ import division
<span class='lineno'>  19</span> from __future__ import print_function
<span class='lineno'>  20</span> 
<span class='lineno'>  21</span> from absl.testing import parameterized
<span class='lineno'>  22</span> import numpy as np
<span class='lineno'>  23</span> import tensorflow as tf
<span class='lineno'>  24</span> 
<span class='lineno'>  25</span> from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
<span class='lineno'>  26</span> from official.nlp.modeling.layers import attention
<span class='lineno'>  27</span> 
<span class='lineno'>  28</span> 
<span class='lineno'>  29</span> # This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
<span class='lineno'>  30</span> # guarantees forward compatibility of this code for the V2 switchover.
<span class='lineno'>  31</span> @keras_parameterized.run_all_keras_modes
<span class='lineno'>  32</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest', title='<MultiHeadAttentionTest>'>MultiHeadAttentionTest</a>(keras_parameterized.TestCase):
<span class='lineno'>  33</span> 
<span class='lineno'>  34</span>   @parameterized.named_parameters(
<span class='lineno'>  35</span>       (&quot;key_value_same_proj&quot;, None, None, [40, 80]),
<span class='lineno'>  36</span>       (&quot;key_value_different_proj&quot;, 32, 60, [40, 60]),
<span class='lineno'>  37</span>   )
<span class='lineno'>  38</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention', title='(MultiHeadAttentionTest, ?, ?, ?) -> None'>test_non_masked_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.self', title='MultiHeadAttentionTest'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value_size', title='?'>value_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_shape', title='?'>output_shape</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_dims', title='?'>output_dims</a>):
<span class='lineno'>  39</span>     &quot;&quot;&quot;Test that the attention layer can be created without a mask tensor.&quot;&quot;&quot;
<span class='lineno'>  40</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(
<span class='lineno'>  41</span>         num_heads=12,
<span class='lineno'>  42</span>         key_size=64,
<span class='lineno'>  43</span>         value_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value_size', title='?'>value_size</a>,
<span class='lineno'>  44</span>         output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_shape', title='?'>output_shape</a>)
<span class='lineno'>  45</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'>  46</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.query', title='?'>query</a> = tf.keras.Input(shape=(40, 80))
<span class='lineno'>  47</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value', title='?'>value</a> = tf.keras.Input(shape=(20, 80))
<span class='lineno'>  48</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.value', title='?'>value</a>])
<span class='lineno'>  49</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output', title='?'>output</a>.shape.as_list(), [None] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_attention.output_dims', title='?'>output_dims</a>)
<span class='lineno'>  50</span> 
<span class='lineno'>  51</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention', title='MultiHeadAttentionTest -> None'>test_non_masked_self_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.self', title='MultiHeadAttentionTest'>self</a>):
<span class='lineno'>  52</span>     &quot;&quot;&quot;Test with one input (self-attenntion) and no mask tensor.&quot;&quot;&quot;
<span class='lineno'>  53</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(num_heads=12, key_size=64)
<span class='lineno'>  54</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'>  55</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', title='?'>query</a> = tf.keras.Input(shape=(40, 80))
<span class='lineno'>  56</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.query', title='?'>query</a>])
<span class='lineno'>  57</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.self', title='MultiHeadAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_non_masked_self_attention.output', title='?'>output</a>.shape.as_list(), [None, 40, 80])
<span class='lineno'>  58</span> 
<span class='lineno'>  59</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores', title='MultiHeadAttentionTest -> None'>test_attention_scores</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', title='MultiHeadAttentionTest'>self</a>):
<span class='lineno'>  60</span>     &quot;&quot;&quot;Test attention outputs with coefficients.&quot;&quot;&quot;
<span class='lineno'>  61</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(
<span class='lineno'>  62</span>         num_heads=12, key_size=64, return_attention_scores=True)
<span class='lineno'>  63</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'>  64</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', title='?'>query</a> = tf.keras.Input(shape=(40, 80))
<span class='lineno'>  65</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.output', title='?'>output</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.coef', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.coef', title='?'>coef</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.query', title='?'>query</a>])
<span class='lineno'>  66</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', title='MultiHeadAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.output', title='?'>output</a>.shape.as_list(), [None, 40, 80])
<span class='lineno'>  67</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.self', title='MultiHeadAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.coef', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_attention_scores.coef', title='?'>coef</a>.shape.as_list(), [None, 12, 40, 40])
<span class='lineno'>  68</span> 
<span class='lineno'>  69</span>   @parameterized.named_parameters((&quot;with_bias&quot;, True), (&quot;no_bias&quot;, False))
<span class='lineno'>  70</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention', title='(MultiHeadAttentionTest, ?) -> None'>test_masked_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', title='?'>use_bias</a>):
<span class='lineno'>  71</span>     &quot;&quot;&quot;Test with a mask tensor.&quot;&quot;&quot;
<span class='lineno'>  72</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(
<span class='lineno'>  73</span>         num_heads=2, key_size=2, use_bias=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', title='?'>use_bias</a>)
<span class='lineno'>  74</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'>  75</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a> = 3
<span class='lineno'>  76</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', title='?'>query</a> = tf.keras.Input(shape=(4, 8))
<span class='lineno'>  77</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', title='?'>value</a> = tf.keras.Input(shape=(2, 8))
<span class='lineno'>  78</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', title='?'>mask_tensor</a> = tf.keras.Input(shape=(4, 2))
<span class='lineno'>  79</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', title='?'>value</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', title='?'>mask_tensor</a>)
<span class='lineno'>  80</span> 
<span class='lineno'>  81</span>     # Create a model containing the test layer.
<span class='lineno'>  82</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a> = tf.keras.Model([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', title='?'>value</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', title='?'>mask_tensor</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', title='?'>output</a>)
<span class='lineno'>  83</span> 
<span class='lineno'>  84</span>     # Generate data for the input (non-mask) tensors.
<span class='lineno'>  85</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', title='int'>from_data</a> = 10 * np.random.random_sample((<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, 4, 8))
<span class='lineno'>  86</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a> = 10 * np.random.random_sample((<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, 2, 8))
<span class='lineno'>  87</span> 
<span class='lineno'>  88</span>     # Invoke the data with a random set of mask data. This should mask at least
<span class='lineno'>  89</span>     # one element.
<span class='lineno'>  90</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a> = np.random.randint(2, size=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, 4, 2))
<span class='lineno'>  91</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a>.predict([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', title='int'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a>])
<span class='lineno'>  92</span> 
<span class='lineno'>  93</span>     # Invoke the same data, but with a null mask (where no elements are masked).
<span class='lineno'>  94</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', title='?'>null_mask_data</a> = np.ones((<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, 4, 2))
<span class='lineno'>  95</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', title='?'>unmasked_output_data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a>.predict([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', title='int'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', title='?'>null_mask_data</a>])
<span class='lineno'>  96</span> 
<span class='lineno'>  97</span>     # Because one data is masked and one is not, the outputs should not be the
<span class='lineno'>  98</span>     # same.
<span class='lineno'>  99</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertNotAllClose(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', title='?'>unmasked_output_data</a>)
<span class='lineno'> 100</span> 
<span class='lineno'> 101</span>     # Tests the layer with three inputs: Q, K, V.
<span class='lineno'> 102</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', title='?'>key</a> = tf.keras.Input(shape=(2, 8))
<span class='lineno'> 103</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', title='?'>value</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', title='?'>key</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', title='?'>mask_tensor</a>)
<span class='lineno'> 104</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a> = tf.keras.Model([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.value', title='?'>value</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.key', title='?'>key</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_tensor', title='?'>mask_tensor</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.output', title='?'>output</a>)
<span class='lineno'> 105</span> 
<span class='lineno'> 106</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a>.predict([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', title='int'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a>])
<span class='lineno'> 107</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', title='?'>unmasked_output_data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.model', title='?'>model</a>.predict(
<span class='lineno'> 108</span>         [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.from_data', title='int'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.to_data', title='int'>to_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.null_mask_data', title='?'>null_mask_data</a>])
<span class='lineno'> 109</span>     # Because one data is masked and one is not, the outputs should not be the
<span class='lineno'> 110</span>     # same.
<span class='lineno'> 111</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertNotAllClose(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.unmasked_output_data', title='?'>unmasked_output_data</a>)
<span class='lineno'> 112</span> 
<span class='lineno'> 113</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.use_bias', title='?'>use_bias</a>:
<span class='lineno'> 114</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertLen(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>._query_dense.trainable_variables, 2)
<span class='lineno'> 115</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertLen(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>._output_dense.trainable_variables, 2)
<span class='lineno'> 116</span>     else:
<span class='lineno'> 117</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertLen(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>._query_dense.trainable_variables, 1)
<span class='lineno'> 118</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.self', title='MultiHeadAttentionTest'>self</a>.assertLen(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_masked_attention.test_layer', title='?'>test_layer</a>._output_dense.trainable_variables, 1)
<span class='lineno'> 119</span> 
<span class='lineno'> 120</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer', title='MultiHeadAttentionTest -> None'>test_initializer</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.self', title='MultiHeadAttentionTest'>self</a>):
<span class='lineno'> 121</span>     &quot;&quot;&quot;Test with a specified initializer.&quot;&quot;&quot;
<span class='lineno'> 122</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(
<span class='lineno'> 123</span>         num_heads=12,
<span class='lineno'> 124</span>         key_size=64,
<span class='lineno'> 125</span>         kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))
<span class='lineno'> 126</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'> 127</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', title='?'>query</a> = tf.keras.Input(shape=(40, 80))
<span class='lineno'> 128</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.query', title='?'>query</a>])
<span class='lineno'> 129</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.self', title='MultiHeadAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_initializer.output', title='?'>output</a>.shape.as_list(), [None, 40, 80])
<span class='lineno'> 130</span> 
<span class='lineno'> 131</span>   @parameterized.named_parameters(
<span class='lineno'> 132</span>       (&quot;4d_inputs_one_free_batch&quot;, [3, 4], [3, 2], [4, 2], (2,)),
<span class='lineno'> 133</span>       (&quot;4D_inputs_2D_attention&quot;, [3, 4], [3, 2], [3, 4, 3, 2], (1, 2)),
<span class='lineno'> 134</span>       (&quot;5D_inputs_2D_attention&quot;, [5, 3, 4], [5, 3, 2], [3, 4, 3, 2], (2, 3)))
<span class='lineno'> 135</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention', title='(MultiHeadAttentionTest, ?, ?, ?, ?) -> None'>test_high_dim_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.self', title='MultiHeadAttentionTest'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.q_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.q_dims', title='?'>q_dims</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.v_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.v_dims', title='?'>v_dims</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_dims', title='?'>mask_dims</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.attention_axes', title='?'>attention_axes</a>):
<span class='lineno'> 136</span>     &quot;&quot;&quot;Test with a mask tensor.&quot;&quot;&quot;
<span class='lineno'> 137</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', title='?'>test_layer</a> = attention.MultiHeadAttention(
<span class='lineno'> 138</span>         num_heads=2, key_size=2, attention_axes=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.attention_axes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.attention_axes', title='?'>attention_axes</a>)
<span class='lineno'> 139</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', title='int'>batch_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', title='int'>hidden_size</a> = 3, 8
<span class='lineno'> 140</span>     # Generate data for the input (non-mask) tensors.
<span class='lineno'> 141</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query_shape', title='[int]'>query_shape</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', title='int'>batch_size</a>] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.q_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.q_dims', title='?'>q_dims</a> + [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', title='int'>hidden_size</a>]
<span class='lineno'> 142</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value_shape', title='[int]'>value_shape</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', title='int'>batch_size</a>] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.v_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.v_dims', title='?'>v_dims</a> + [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.hidden_size', title='int'>hidden_size</a>]
<span class='lineno'> 143</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', title='[int]'>mask_shape</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.batch_size', title='int'>batch_size</a>] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_dims', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_dims', title='?'>mask_dims</a>
<span class='lineno'> 144</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', title='int'>query</a> = 10 * np.random.random_sample(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query_shape', title='[int]'>query_shape</a>)
<span class='lineno'> 145</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', title='int'>value</a> = 10 * np.random.random_sample(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value_shape', title='[int]'>value_shape</a>)
<span class='lineno'> 146</span> 
<span class='lineno'> 147</span>     # Invoke the data with a random set of mask data. This should mask at least
<span class='lineno'> 148</span>     # one element.
<span class='lineno'> 149</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_data', title='?'>mask_data</a> = np.random.randint(2, size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', title='[int]'>mask_shape</a>).astype(&quot;bool&quot;)
<span class='lineno'> 150</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', title='int'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', title='int'>value</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_data', title='?'>mask_data</a>)
<span class='lineno'> 151</span> 
<span class='lineno'> 152</span>     # Invoke the same data, but with a null mask (where no elements are masked).
<span class='lineno'> 153</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.null_mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.null_mask_data', title='?'>null_mask_data</a> = np.ones(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.mask_shape', title='[int]'>mask_shape</a>)
<span class='lineno'> 154</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.unmasked_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.unmasked_output', title='?'>unmasked_output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.test_layer', title='?'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.query', title='int'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.value', title='int'>value</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.null_mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.null_mask_data', title='?'>null_mask_data</a>)
<span class='lineno'> 155</span>     # Because one data is masked and one is not, the outputs should not be the
<span class='lineno'> 156</span>     # same.
<span class='lineno'> 157</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.self', title='MultiHeadAttentionTest'>self</a>.assertNotAllClose(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.output', title='?'>output</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.unmasked_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.MultiHeadAttentionTest.test_high_dim_attention.unmasked_output', title='?'>unmasked_output</a>)
<span class='lineno'> 158</span> 
<span class='lineno'> 159</span> 
<span class='lineno'> 160</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention', title='<SubclassAttention>'>SubclassAttention</a>(attention.MultiHeadAttention):
<span class='lineno'> 161</span> 
<span class='lineno'> 162</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention', title='(SubclassAttention, ?) -> None'>_build_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention.self', title='SubclassAttention'>self</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention.qkv_rank', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._build_attention.qkv_rank', title='?'>qkv_rank</a>):
<span class='lineno'> 163</span>     pass
<span class='lineno'> 164</span> 
<span class='lineno'> 165</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention', title='(SubclassAttention, ?, ?, ?, None) -> (?, None)'>_compute_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.self', title='SubclassAttention'>self</a>,
<span class='lineno'> 166</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.query_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.query_tensor', title='?'>query_tensor</a>,
<span class='lineno'> 167</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.key_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.key_tensor', title='?'>key_tensor</a>,
<span class='lineno'> 168</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.value_tensor', title='?'>value_tensor</a>,
<span class='lineno'> 169</span>                          <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.attention_mask', title='None'>attention_mask</a>=None):
<span class='lineno'> 170</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.value_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention._compute_attention.value_tensor', title='?'>value_tensor</a>, None
<span class='lineno'> 171</span> 
<span class='lineno'> 172</span> 
<span class='lineno'> 173</span> @keras_parameterized.run_all_keras_modes
<span class='lineno'> 174</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest', title='<AttentionSubclassTest>'>AttentionSubclassTest</a>(keras_parameterized.TestCase):
<span class='lineno'> 175</span> 
<span class='lineno'> 176</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer', title='AttentionSubclassTest -> None'>test_initializer</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.self', title='AttentionSubclassTest'>self</a>):
<span class='lineno'> 177</span>     &quot;&quot;&quot;Test with a specified initializer.&quot;&quot;&quot;
<span class='lineno'> 178</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.test_layer', title='SubclassAttention'>test_layer</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.SubclassAttention', title='<SubclassAttention>'>SubclassAttention</a>(
<span class='lineno'> 179</span>         num_heads=12,
<span class='lineno'> 180</span>         key_size=64)
<span class='lineno'> 181</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'> 182</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', title='?'>query</a> = tf.keras.Input(shape=(40, 80))
<span class='lineno'> 183</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.output', title='?'>output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.test_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.test_layer', title='SubclassAttention'>test_layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', title='?'>query</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.query', title='?'>query</a>])
<span class='lineno'> 184</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.self', title='AttentionSubclassTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.AttentionSubclassTest.test_initializer.output', title='?'>output</a>.shape.as_list(), [None, 40, 80])
<span class='lineno'> 185</span> 
<span class='lineno'> 186</span> 
<span class='lineno'> 187</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', title='(?, ?, ?, ?) -> dict / (int, int, int, int) -> dict'>_create_cache</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', title='int'>batch_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', title='int'>init_decode_length</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', title='int'>num_heads</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', title='int'>head_size</a>):
<span class='lineno'> 188</span>   return {
<span class='lineno'> 189</span>       &quot;key&quot;:
<span class='lineno'> 190</span>           tf.zeros([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', title='int'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', title='int'>head_size</a>],
<span class='lineno'> 191</span>                    dtype=tf.float32),
<span class='lineno'> 192</span>       &quot;value&quot;:
<span class='lineno'> 193</span>           tf.zeros([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.num_heads', title='int'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache.head_size', title='int'>head_size</a>],
<span class='lineno'> 194</span>                    dtype=tf.float32)
<span class='lineno'> 195</span>   }
<span class='lineno'> 196</span> 
<span class='lineno'> 197</span> 
<span class='lineno'> 198</span> @keras_parameterized.run_all_keras_modes
<span class='lineno'> 199</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest', title='<CachedAttentionTest>'>CachedAttentionTest</a>(keras_parameterized.TestCase):
<span class='lineno'> 200</span> 
<span class='lineno'> 201</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention', title='CachedAttentionTest -> None'>test_masked_attention</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', title='CachedAttentionTest'>self</a>):
<span class='lineno'> 202</span>     &quot;&quot;&quot;Test with a mask tensor.&quot;&quot;&quot;
<span class='lineno'> 203</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', title='int'>num_heads</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', title='int'>head_size</a> = 2, 2
<span class='lineno'> 204</span>     # Create a 3-dimensional input (the first dimension is implicit).
<span class='lineno'> 205</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', title='int'>from_seq_length</a> = 4
<span class='lineno'> 206</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a> = 3
<span class='lineno'> 207</span>     # GPU/CPU case.
<span class='lineno'> 208</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.init_decode_length', title='int'>init_decode_length</a> = 0
<span class='lineno'> 209</span>     # Directly tests the keras layer.
<span class='lineno'> 210</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='dict'>cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', title='(?, ?, ?, ?) -> dict / (int, int, int, int) -> dict'>_create_cache</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', title='int'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', title='int'>head_size</a>)
<span class='lineno'> 211</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', title='?'>layer</a> = attention.CachedAttention(num_heads=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.num_heads', title='int'>num_heads</a>, key_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.head_size', title='int'>head_size</a>)
<span class='lineno'> 212</span> 
<span class='lineno'> 213</span>     # Generate data for the input (non-mask) tensors.
<span class='lineno'> 214</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', title='?'>from_data</a> = tf.zeros((<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', title='int'>from_seq_length</a>, 8), dtype=np.float32)
<span class='lineno'> 215</span>     # Invoke the data with a random set of mask data. This should mask at least
<span class='lineno'> 216</span>     # one element.
<span class='lineno'> 217</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a> = np.random.randint(
<span class='lineno'> 218</span>         2, size=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', title='int'>from_seq_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_seq_length', title='int'>from_seq_length</a>))
<span class='lineno'> 219</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='?'>cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', title='?'>layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', title='?'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', title='?'>from_data</a>], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='dict'>cache</a>)
<span class='lineno'> 220</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', title='CachedAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>.shape, (3, 4, 8))
<span class='lineno'> 221</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', title='CachedAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='?'>cache</a>[&quot;value&quot;].shape, (3, 4, 2, 2))
<span class='lineno'> 222</span> 
<span class='lineno'> 223</span>     # Tests inputs without cache.
<span class='lineno'> 224</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='?'>cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.layer', title='?'>layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', title='?'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.from_data', title='?'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.mask_data', title='?'>mask_data</a>])
<span class='lineno'> 225</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', title='CachedAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.masked_output_data', title='?'>masked_output_data</a>.shape, (3, 4, 8))
<span class='lineno'> 226</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.self', title='CachedAttentionTest'>self</a>.assertIsNone(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_masked_attention.cache', title='?'>cache</a>)
<span class='lineno'> 227</span> 
<span class='lineno'> 228</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode', title='CachedAttentionTest -> None'>test_padded_decode</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', title='CachedAttentionTest'>self</a>):
<span class='lineno'> 229</span>     &quot;&quot;&quot;Test with a mask tensor.&quot;&quot;&quot;
<span class='lineno'> 230</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', title='int'>num_heads</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', title='int'>head_size</a> = 2, 2
<span class='lineno'> 231</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', title='int'>from_seq_length</a> = 4
<span class='lineno'> 232</span>     # TPU decoding should pre-allocate the entire sequence.
<span class='lineno'> 233</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', title='int'>batch_size</a> = 3
<span class='lineno'> 234</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.init_decode_length', title='int'>init_decode_length</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', title='int'>from_seq_length</a>
<span class='lineno'> 235</span> 
<span class='lineno'> 236</span>     # Directly tests the keras layer.
<span class='lineno'> 237</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', title='dict'>cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test._create_cache', title='(?, ?, ?, ?) -> dict / (int, int, int, int) -> dict'>_create_cache</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.init_decode_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.init_decode_length', title='int'>init_decode_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', title='int'>num_heads</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', title='int'>head_size</a>)
<span class='lineno'> 238</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.layer', title='?'>layer</a> = attention.CachedAttention(num_heads=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.num_heads', title='int'>num_heads</a>, key_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.head_size', title='int'>head_size</a>)
<span class='lineno'> 239</span> 
<span class='lineno'> 240</span>     # Generate data for the input (non-mask) tensors.
<span class='lineno'> 241</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', title='?'>from_data</a> = tf.zeros((<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', title='int'>from_seq_length</a>, 8), dtype=np.float32)
<span class='lineno'> 242</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.decode_loop_step', title='int'>decode_loop_step</a> = 2
<span class='lineno'> 243</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.mask_data', title='?'>mask_data</a> = np.random.randint(
<span class='lineno'> 244</span>         2, size=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.batch_size', title='int'>batch_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', title='int'>from_seq_length</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_seq_length', title='int'>from_seq_length</a>), dtype=np.int32)
<span class='lineno'> 245</span>     # Testing the invocation directly as Keras cannot consume inputs correctly.
<span class='lineno'> 246</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.masked_output_data', title='?'>masked_output_data</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', title='?'>cache</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.layer', title='?'>layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', title='?'>from_data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.from_data', title='?'>from_data</a>],
<span class='lineno'> 247</span>                                       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.mask_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.mask_data', title='?'>mask_data</a>,
<span class='lineno'> 248</span>                                       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', title='dict'>cache</a>,
<span class='lineno'> 249</span>                                       decode_loop_step=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.decode_loop_step', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.decode_loop_step', title='int'>decode_loop_step</a>)
<span class='lineno'> 250</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', title='CachedAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.masked_output_data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.masked_output_data', title='?'>masked_output_data</a>.shape, (3, 4, 8))
<span class='lineno'> 251</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.self', title='CachedAttentionTest'>self</a>.assertEqual(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.layers.attention_test.CachedAttentionTest.test_padded_decode.cache', title='?'>cache</a>[&quot;value&quot;].shape, (3, 4, 2, 2))
<span class='lineno'> 252</span> 
<span class='lineno'> 253</span> 
<span class='lineno'> 254</span> if __name__ == &quot;__main__&quot;:
<span class='lineno'> 255</span>   tf.test.main()
</pre></td></tr></table></body></html>