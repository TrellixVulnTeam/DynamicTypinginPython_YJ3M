<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/nlp/modeling/networks/transformer_encoder.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder'>TransformerEncoder</a><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__'>__init__</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table'>get_embedding_table</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config'>get_config</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers'>transformer_layers</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer'>pooler_layer</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config', xid='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config'>from_config</a></li></ul>
</li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Transformer-based text encoder network.&quot;&quot;&quot;
<span class='lineno'>  16</span> # pylint: disable=g-classes-have-attributes
<span class='lineno'>  17</span> from __future__ import absolute_import
<span class='lineno'>  18</span> from __future__ import division
<span class='lineno'>  19</span> # from __future__ import google_type_annotations
<span class='lineno'>  20</span> from __future__ import print_function
<span class='lineno'>  21</span> 
<span class='lineno'>  22</span> import tensorflow as tf
<span class='lineno'>  23</span> 
<span class='lineno'>  24</span> from official.modeling import activations
<span class='lineno'>  25</span> from official.nlp.modeling import layers
<span class='lineno'>  26</span> 
<span class='lineno'>  27</span> 
<span class='lineno'>  28</span> @tf.keras.utils.register_keras_serializable(package=&#39;Text&#39;)
<span class='lineno'>  29</span> class <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder', title='<TransformerEncoder>'>TransformerEncoder</a>(tf.keras.Model):
<span class='lineno'>  30</span>   &quot;&quot;&quot;Bi-directional Transformer-based encoder network.
<span class='lineno'>  31</span> 
<span class='lineno'>  32</span>   This network implements a bi-directional Transformer-based encoder as
<span class='lineno'>  33</span>   described in &quot;BERT: Pre-training of Deep Bidirectional Transformers for
<span class='lineno'>  34</span>   Language Understanding&quot; (https://arxiv.org/abs/1810.04805). It includes the
<span class='lineno'>  35</span>   embedding lookups and transformer layers, but not the masked language model
<span class='lineno'>  36</span>   or classification task networks.
<span class='lineno'>  37</span> 
<span class='lineno'>  38</span>   The default values for this object are taken from the BERT-Base implementation
<span class='lineno'>  39</span>   in &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language
<span class='lineno'>  40</span>   Understanding&quot;.
<span class='lineno'>  41</span> 
<span class='lineno'>  42</span>   Arguments:
<span class='lineno'>  43</span>     vocab_size: The size of the token vocabulary.
<span class='lineno'>  44</span>     hidden_size: The size of the transformer hidden layers.
<span class='lineno'>  45</span>     num_layers: The number of transformer layers.
<span class='lineno'>  46</span>     num_attention_heads: The number of attention heads for each transformer. The
<span class='lineno'>  47</span>       hidden size must be divisible by the number of attention heads.
<span class='lineno'>  48</span>     sequence_length: The sequence length that this encoder expects. If None, the
<span class='lineno'>  49</span>       sequence length is dynamic; if an integer, the encoder will require
<span class='lineno'>  50</span>       sequences padded to this length.
<span class='lineno'>  51</span>     max_sequence_length: The maximum sequence length that this encoder can
<span class='lineno'>  52</span>       consume. If None, max_sequence_length uses the value from sequence length.
<span class='lineno'>  53</span>       This determines the variable shape for positional embeddings.
<span class='lineno'>  54</span>     type_vocab_size: The number of types that the &#39;type_ids&#39; input can take.
<span class='lineno'>  55</span>     intermediate_size: The intermediate size for the transformer layers.
<span class='lineno'>  56</span>     activation: The activation to use for the transformer layers.
<span class='lineno'>  57</span>     dropout_rate: The dropout rate to use for the transformer layers.
<span class='lineno'>  58</span>     attention_dropout_rate: The dropout rate to use for the attention layers
<span class='lineno'>  59</span>       within the transformer layers.
<span class='lineno'>  60</span>     initializer: The initialzer to use for all weights in this encoder.
<span class='lineno'>  61</span>     return_all_encoder_outputs: Whether to output sequence embedding outputs of
<span class='lineno'>  62</span>       all encoder transformer layers.
<span class='lineno'>  63</span>     output_range: the sequence output range, [0, output_range), by slicing the
<span class='lineno'>  64</span>       target sequence of the last transformer layer. `None` means the entire
<span class='lineno'>  65</span>       target sequence will attend to the source sequence, which yeilds the full
<span class='lineno'>  66</span>       output.
<span class='lineno'>  67</span>     embedding_width: The width of the word embeddings. If the embedding width
<span class='lineno'>  68</span>       is not equal to hidden size, embedding parameters will be factorized into
<span class='lineno'>  69</span>       two matrices in the shape of [&#39;vocab_size&#39;, &#39;embedding_width&#39;] and
<span class='lineno'>  70</span>       [&#39;embedding_width&#39;, &#39;hidden_size&#39;] (&#39;embedding_width&#39; is usually much
<span class='lineno'>  71</span>       smaller than &#39;hidden_size&#39;).
<span class='lineno'>  72</span>   &quot;&quot;&quot;
<span class='lineno'>  73</span> 
<span class='lineno'>  74</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__', title='? -> ?'>__init__</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>,
<span class='lineno'>  75</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', title='?'>vocab_size</a>,
<span class='lineno'>  76</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>=768,
<span class='lineno'>  77</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', title='int'>num_layers</a>=12,
<span class='lineno'>  78</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', title='int'>num_attention_heads</a>=12,
<span class='lineno'>  79</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>=512,
<span class='lineno'>  80</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', title='None'>max_sequence_length</a>=None,
<span class='lineno'>  81</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', title='int'>type_vocab_size</a>=16,
<span class='lineno'>  82</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', title='int'>intermediate_size</a>=3072,
<span class='lineno'>  83</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', title='?'>activation</a>=activations.gelu,
<span class='lineno'>  84</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', title='float'>dropout_rate</a>=0.1,
<span class='lineno'>  85</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', title='float'>attention_dropout_rate</a>=0.1,
<span class='lineno'>  86</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>=tf.keras.initializers.TruncatedNormal(stddev=0.02),
<span class='lineno'>  87</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', title='bool'>return_all_encoder_outputs</a>=False,
<span class='lineno'>  88</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', title='None'>output_range</a>=None,
<span class='lineno'>  89</span>                <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='None'>embedding_width</a>=None,
<span class='lineno'>  90</span>                **kwargs):
<span class='lineno'>  91</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', title='?'>activation</a> = tf.keras.activations.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', title='?'>activation</a>)
<span class='lineno'>  92</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a> = tf.keras.initializers.get(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>)
<span class='lineno'>  93</span> 
<span class='lineno'>  94</span>     if not <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', title='None'>max_sequence_length</a>:
<span class='lineno'>  95</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', title='int'>max_sequence_length</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>
<span class='lineno'>  96</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._self_setattr_tracking', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._self_setattr_tracking', title='bool'>_self_setattr_tracking</a> = False
<span class='lineno'>  97</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._config_dict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._config_dict', title='dict'>_config_dict</a> = {
<span class='lineno'>  98</span>         &#39;vocab_size&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', title='?'>vocab_size</a>,
<span class='lineno'>  99</span>         &#39;hidden_size&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>,
<span class='lineno'> 100</span>         &#39;num_layers&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', title='int'>num_layers</a>,
<span class='lineno'> 101</span>         &#39;num_attention_heads&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', title='int'>num_attention_heads</a>,
<span class='lineno'> 102</span>         &#39;sequence_length&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>,
<span class='lineno'> 103</span>         &#39;max_sequence_length&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', title='int'>max_sequence_length</a>,
<span class='lineno'> 104</span>         &#39;type_vocab_size&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', title='int'>type_vocab_size</a>,
<span class='lineno'> 105</span>         &#39;intermediate_size&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', title='int'>intermediate_size</a>,
<span class='lineno'> 106</span>         &#39;activation&#39;: tf.keras.activations.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', title='?'>activation</a>),
<span class='lineno'> 107</span>         &#39;dropout_rate&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', title='float'>dropout_rate</a>,
<span class='lineno'> 108</span>         &#39;attention_dropout_rate&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', title='float'>attention_dropout_rate</a>,
<span class='lineno'> 109</span>         &#39;initializer&#39;: tf.keras.initializers.serialize(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>),
<span class='lineno'> 110</span>         &#39;return_all_encoder_outputs&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', title='bool'>return_all_encoder_outputs</a>,
<span class='lineno'> 111</span>         &#39;output_range&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', title='None'>output_range</a>,
<span class='lineno'> 112</span>         &#39;embedding_width&#39;: <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='None'>embedding_width</a>,
<span class='lineno'> 113</span>     }
<span class='lineno'> 114</span> 
<span class='lineno'> 115</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', title='?'>word_ids</a> = tf.keras.layers.Input(
<span class='lineno'> 116</span>         shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>,), dtype=tf.int32, name=&#39;input_word_ids&#39;)
<span class='lineno'> 117</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', title='?'>mask</a> = tf.keras.layers.Input(
<span class='lineno'> 118</span>         shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>,), dtype=tf.int32, name=&#39;input_mask&#39;)
<span class='lineno'> 119</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', title='?'>type_ids</a> = tf.keras.layers.Input(
<span class='lineno'> 120</span>         shape=(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.sequence_length', title='int'>sequence_length</a>,), dtype=tf.int32, name=&#39;input_type_ids&#39;)
<span class='lineno'> 121</span> 
<span class='lineno'> 122</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='None'>embedding_width</a> is None:
<span class='lineno'> 123</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='int'>embedding_width</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>
<span class='lineno'> 124</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', title='?'>_embedding_layer</a> = layers.OnDeviceEmbedding(
<span class='lineno'> 125</span>         vocab_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.vocab_size', title='?'>vocab_size</a>,
<span class='lineno'> 126</span>         embedding_width=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='int'>embedding_width</a>,
<span class='lineno'> 127</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 128</span>         name=&#39;word_embeddings&#39;)
<span class='lineno'> 129</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', title='?'>word_embeddings</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', title='?'>_embedding_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', title='?'>word_ids</a>)
<span class='lineno'> 130</span> 
<span class='lineno'> 131</span>     # Always uses dynamic slicing for simplicity.
<span class='lineno'> 132</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._position_embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._position_embedding_layer', title='?'>_position_embedding_layer</a> = layers.PositionEmbedding(
<span class='lineno'> 133</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 134</span>         use_dynamic_slicing=True,
<span class='lineno'> 135</span>         max_sequence_length=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.max_sequence_length', title='int'>max_sequence_length</a>,
<span class='lineno'> 136</span>         name=&#39;position_embedding&#39;)
<span class='lineno'> 137</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.position_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.position_embeddings', title='?'>position_embeddings</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._position_embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._position_embedding_layer', title='?'>_position_embedding_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', title='?'>word_embeddings</a>)
<span class='lineno'> 138</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._type_embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._type_embedding_layer', title='?'>_type_embedding_layer</a> = layers.OnDeviceEmbedding(
<span class='lineno'> 139</span>         vocab_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_vocab_size', title='int'>type_vocab_size</a>,
<span class='lineno'> 140</span>         embedding_width=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='int'>embedding_width</a>,
<span class='lineno'> 141</span>         initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 142</span>         use_one_hot=True,
<span class='lineno'> 143</span>         name=&#39;type_embeddings&#39;)
<span class='lineno'> 144</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_embeddings', title='?'>type_embeddings</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._type_embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._type_embedding_layer', title='?'>_type_embedding_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', title='?'>type_ids</a>)
<span class='lineno'> 145</span> 
<span class='lineno'> 146</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a> = tf.keras.layers.Add()(
<span class='lineno'> 147</span>         [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_embeddings', title='?'>word_embeddings</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.position_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.position_embeddings', title='?'>position_embeddings</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_embeddings', title='?'>type_embeddings</a>])
<span class='lineno'> 148</span> 
<span class='lineno'> 149</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a> = (
<span class='lineno'> 150</span>         tf.keras.layers.LayerNormalization(
<span class='lineno'> 151</span>             name=&#39;embeddings/layer_norm&#39;,
<span class='lineno'> 152</span>             axis=-1,
<span class='lineno'> 153</span>             epsilon=1e-12,
<span class='lineno'> 154</span>             dtype=tf.float32)(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a>))
<span class='lineno'> 155</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a> = (
<span class='lineno'> 156</span>         tf.keras.layers.Dropout(rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', title='float'>dropout_rate</a>)(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a>))
<span class='lineno'> 157</span> 
<span class='lineno'> 158</span>     # We project the &#39;embedding&#39; output to &#39;hidden_size&#39; if it is not already
<span class='lineno'> 159</span>     # &#39;hidden_size&#39;.
<span class='lineno'> 160</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embedding_width', title='int'>embedding_width</a> != <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>:
<span class='lineno'> 161</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_projection', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_projection', title='?'>_embedding_projection</a> = tf.keras.layers.experimental.EinsumDense(
<span class='lineno'> 162</span>           &#39;...x,xy-&gt;...y&#39;,
<span class='lineno'> 163</span>           output_shape=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>,
<span class='lineno'> 164</span>           bias_axes=&#39;y&#39;,
<span class='lineno'> 165</span>           kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 166</span>           name=&#39;embedding_projection&#39;)
<span class='lineno'> 167</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_projection', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_projection', title='?'>_embedding_projection</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a>)
<span class='lineno'> 168</span> 
<span class='lineno'> 169</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', title='[?]'>_transformer_layers</a> = []
<span class='lineno'> 170</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', title='?'>data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.embeddings', title='?'>embeddings</a>
<span class='lineno'> 171</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_mask', title='?'>attention_mask</a> = layers.SelfAttentionMask()([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', title='?'>data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', title='?'>mask</a>])
<span class='lineno'> 172</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', title='[?]'>encoder_outputs</a> = []
<span class='lineno'> 173</span>     for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', title='int'>i</a> in range(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', title='int'>num_layers</a>):
<span class='lineno'> 174</span>       if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', title='int'>i</a> == <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_layers', title='int'>num_layers</a> - 1 and <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', title='None'>output_range</a> is not None:
<span class='lineno'> 175</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', title='None'>transformer_output_range</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.output_range', title='None'>output_range</a>
<span class='lineno'> 176</span>       else:
<span class='lineno'> 177</span>         <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', title='None'>transformer_output_range</a> = None
<span class='lineno'> 178</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', title='?'>layer</a> = layers.Transformer(
<span class='lineno'> 179</span>           num_attention_heads=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.num_attention_heads', title='int'>num_attention_heads</a>,
<span class='lineno'> 180</span>           intermediate_size=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.intermediate_size', title='int'>intermediate_size</a>,
<span class='lineno'> 181</span>           intermediate_activation=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.activation', title='?'>activation</a>,
<span class='lineno'> 182</span>           dropout_rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.dropout_rate', title='float'>dropout_rate</a>,
<span class='lineno'> 183</span>           attention_dropout_rate=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_dropout_rate', title='float'>attention_dropout_rate</a>,
<span class='lineno'> 184</span>           output_range=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.transformer_output_range', title='None'>transformer_output_range</a>,
<span class='lineno'> 185</span>           kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 186</span>           name=&#39;transformer/layer_%d&#39; % <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.i', title='int'>i</a>)
<span class='lineno'> 187</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', title='[?]'>_transformer_layers</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', title='?'>layer</a>)
<span class='lineno'> 188</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', title='?'>data</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.layer', title='?'>layer</a>([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', title='?'>data</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.attention_mask', title='?'>attention_mask</a>])
<span class='lineno'> 189</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', title='[?]'>encoder_outputs</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.data', title='?'>data</a>)
<span class='lineno'> 190</span> 
<span class='lineno'> 191</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.first_token_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.first_token_tensor', title='?'>first_token_tensor</a> = (
<span class='lineno'> 192</span>         tf.keras.layers.Lambda(lambda <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.lambda%90.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.lambda%90.x', title='?'>x</a>: tf.squeeze(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.lambda%90.x', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.lambda%90.x', title='?'>x</a>[:, 0:1, :], axis=1))(
<span class='lineno'> 193</span>             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', title='[?]'>encoder_outputs</a>[-1]))
<span class='lineno'> 194</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', title='?'>_pooler_layer</a> = tf.keras.layers.Dense(
<span class='lineno'> 195</span>         units=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.hidden_size', title='int'>hidden_size</a>,
<span class='lineno'> 196</span>         activation=&#39;tanh&#39;,
<span class='lineno'> 197</span>         kernel_initializer=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.initializer', title='?'>initializer</a>,
<span class='lineno'> 198</span>         name=&#39;pooler_transform&#39;)
<span class='lineno'> 199</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', title='?'>cls_output</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', title='?'>_pooler_layer</a>(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.first_token_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.first_token_tensor', title='?'>first_token_tensor</a>)
<span class='lineno'> 200</span> 
<span class='lineno'> 201</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.return_all_encoder_outputs', title='bool'>return_all_encoder_outputs</a>:
<span class='lineno'> 202</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', title='[[?]]'>outputs</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', title='[?]'>encoder_outputs</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', title='?'>cls_output</a>]
<span class='lineno'> 203</span>     else:
<span class='lineno'> 204</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', title='[?]'>outputs</a> = [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.encoder_outputs', title='[?]'>encoder_outputs</a>[-1], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.cls_output', title='?'>cls_output</a>]
<span class='lineno'> 205</span> 
<span class='lineno'> 206</span>     super(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder', title='<TransformerEncoder>'>TransformerEncoder</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.self', title='TransformerEncoder'>self</a>).__init__(
<span class='lineno'> 207</span>         inputs=[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.word_ids', title='?'>word_ids</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.mask', title='?'>mask</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.type_ids', title='?'>type_ids</a>], outputs=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.__init__.outputs', title='{[?] | [[?]]}'>outputs</a>, **kwargs)
<span class='lineno'> 208</span> 
<span class='lineno'> 209</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table', title='TransformerEncoder -> ?'>get_embedding_table</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table.self', title='TransformerEncoder'>self</a>):
<span class='lineno'> 210</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_embedding_table.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._embedding_layer', title='?'>_embedding_layer</a>.embeddings
<span class='lineno'> 211</span> 
<span class='lineno'> 212</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config', title='TransformerEncoder -> dict'>get_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config.self', title='TransformerEncoder'>self</a>):
<span class='lineno'> 213</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.get_config.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._config_dict', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._config_dict', title='dict'>_config_dict</a>
<span class='lineno'> 214</span> 
<span class='lineno'> 215</span>   @property
<span class='lineno'> 216</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers', title='TransformerEncoder -> [?]'>transformer_layers</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers.self', title='TransformerEncoder'>self</a>):
<span class='lineno'> 217</span>     &quot;&quot;&quot;List of Transformer layers in the encoder.&quot;&quot;&quot;
<span class='lineno'> 218</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.transformer_layers.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._transformer_layers', title='[?]'>_transformer_layers</a>
<span class='lineno'> 219</span> 
<span class='lineno'> 220</span>   @property
<span class='lineno'> 221</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer', title='TransformerEncoder -> None'>pooler_layer</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer.self', title='TransformerEncoder'>self</a>):
<span class='lineno'> 222</span>     &quot;&quot;&quot;The pooler dense layer after the transformer layers.&quot;&quot;&quot;
<span class='lineno'> 223</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer.self', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.pooler_layer.self', title='TransformerEncoder'>self</a>.<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder._pooler_layer', title='?'>_pooler_layer</a>
<span class='lineno'> 224</span> 
<span class='lineno'> 225</span>   @classmethod
<span class='lineno'> 226</span>   def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config', title='(<TransformerEncoder>, ?, None) -> TransformerEncoder'>from_config</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.cls', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.cls', title='<TransformerEncoder>'>cls</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.config', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.config', title='?'>config</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.custom_objects', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.custom_objects', title='None'>custom_objects</a>=None):
<span class='lineno'> 227</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.cls', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.nlp.modeling.networks.transformer_encoder.TransformerEncoder.from_config.cls', title='<TransformerEncoder>'>cls</a>(**config)
</pre></td></tr></table></body></html>