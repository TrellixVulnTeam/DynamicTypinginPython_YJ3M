<html>
<head>
<meta charset="utf-8">
<title>/home/xxm/Desktop/EMSE/dataset/models/official/vision/detection/utils/input_utils.py</title>
<style type='text/css'>
body { color: #666666; }
a {
    text-decoration: none; color: #5AA2A7;
    border: solid 1px rgba(255,255,255,0);
}
a.active {
    background: -webkit-linear-gradient(top,rgba(255, 255, 200, 0.35) 0,rgba(255, 255, 200, 0.55) 100%);
    border: solid 1px #E5E600;
}
table, th, td { border: 1px solid lightgrey; padding: 5px; corner: rounded; }
.builtin {color: #B17E41;}
.comment, .block-comment {color: #aaaaaa; font-style: italic;}
.constant {color: #888888;}
.decorator {color: #778899;}
.doc-string {color: #aaaaaa;}
.error {border-bottom: 1px solid red;}
.field-name {color: #2e8b57;}
.function {color: #4682b4;}
.identifier {color: #8b7765;}
.info {border-bottom: 1px dotted RoyalBlue;}
.keyword {color: #0000cd;}
.lineno {color: #cccccc;}
.number {color: #483d8b;}
.parameter {color: #777777;}
.string {color: #999999;}
.type-name {color: #4682b4;}
.warning {border-bottom: 1px solid orange; padding-bottom: 1px}
</style>
<script language="JavaScript" type="text/javascript">
var highlighted;

function highlight(xid)
{
    var elms = document.querySelectorAll('[xid="' + xid + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "active";
    }
    highlighted = xid;
}

function clearHighlight() {
    var elms = document.querySelectorAll('[xid="' + highlighted + '"]');
    for (k in elms) {
        v = elms[k]
        v.className = "";
    }
}

window.onload =
    function (e) {
        var tags = document.getElementsByTagName("A")
        for (var i = 0; i < tags.length; i++) {
            tags[i].onmouseover =
                function (e) {
                    clearHighlight();
                    var xid = e.toElement.getAttribute('xid');
                    highlight(xid);
                }
        }
    }</script>
</head>
<body>
<table width=100% border='1px solid gray'><tr><td valign='top'><ul>
<li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size'>pad_to_fixed_size</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image'>normalize_image</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size'>compute_padded_size</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image'>resize_and_crop_image</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2'>resize_and_crop_image_v2</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes'>resize_and_crop_boxes</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks'>resize_and_crop_masks</a></li><li><a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip', xid='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip'>random_horizontal_flip</a></li></ul>
</td><td><pre><span class='lineno'>   1</span> # Copyright 2019 The TensorFlow Authors. All Rights Reserved.
<span class='lineno'>   2</span> #
<span class='lineno'>   3</span> # Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
<span class='lineno'>   4</span> # you may not use this file except in compliance with the License.
<span class='lineno'>   5</span> # You may obtain a copy of the License at
<span class='lineno'>   6</span> #
<span class='lineno'>   7</span> #     http://www.apache.org/licenses/LICENSE-2.0
<span class='lineno'>   8</span> #
<span class='lineno'>   9</span> # Unless required by applicable law or agreed to in writing, software
<span class='lineno'>  10</span> # distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
<span class='lineno'>  11</span> # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<span class='lineno'>  12</span> # See the License for the specific language governing permissions and
<span class='lineno'>  13</span> # limitations under the License.
<span class='lineno'>  14</span> # ==============================================================================
<span class='lineno'>  15</span> &quot;&quot;&quot;Utility functions for input processing.&quot;&quot;&quot;
<span class='lineno'>  16</span> 
<span class='lineno'>  17</span> import math
<span class='lineno'>  18</span> import tensorflow as tf
<span class='lineno'>  19</span> 
<span class='lineno'>  20</span> from official.vision.detection.utils import box_utils
<span class='lineno'>  21</span> from official.vision.detection.utils.object_detection import preprocessor
<span class='lineno'>  22</span> 
<span class='lineno'>  23</span> 
<span class='lineno'>  24</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size', title='(?, ?, int) -> None'>pad_to_fixed_size</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', title='?'>size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.constant_values', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.constant_values', title='int'>constant_values</a>=0):
<span class='lineno'>  25</span>   &quot;&quot;&quot;Pads data to a fixed length at the first dimension.
<span class='lineno'>  26</span> 
<span class='lineno'>  27</span>   Args:
<span class='lineno'>  28</span>     input_tensor: `Tensor` with any dimension.
<span class='lineno'>  29</span>     size: `int` number for the first dimension of output Tensor.
<span class='lineno'>  30</span>     constant_values: `int` value assigned to the paddings.
<span class='lineno'>  31</span> 
<span class='lineno'>  32</span>   Returns:
<span class='lineno'>  33</span>     `Tensor` with the first dimension padded to `size`.
<span class='lineno'>  34</span>   &quot;&quot;&quot;
<span class='lineno'>  35</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', title='?'>input_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>.get_shape().as_list()
<span class='lineno'>  36</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', title='[?]'>padding_shape</a> = []
<span class='lineno'>  37</span> 
<span class='lineno'>  38</span>   # Computes the padding length on the first dimension.
<span class='lineno'>  39</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', title='?'>padding_length</a> = tf.maximum(0, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', title='?'>size</a> - tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>)[0])
<span class='lineno'>  40</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.assert_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.assert_length', title='?'>assert_length</a> = tf.Assert(
<span class='lineno'>  41</span>       tf.greater_equal(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', title='?'>padding_length</a>, 0), [<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', title='?'>padding_length</a>])
<span class='lineno'>  42</span>   with tf.control_dependencies([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.assert_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.assert_length', title='?'>assert_length</a>]):
<span class='lineno'>  43</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', title='[?]'>padding_shape</a>.append(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_length', title='?'>padding_length</a>)
<span class='lineno'>  44</span> 
<span class='lineno'>  45</span>   # Copies shapes of the rest of input shape dimensions.
<span class='lineno'>  46</span>   for <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.i', title='int'>i</a> in range(1, len(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', title='?'>input_shape</a>)):
<span class='lineno'>  47</span>     <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', title='[?]'>padding_shape</a>.append(tf.shape(input=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>)[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.i', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.i', title='int'>i</a>])
<span class='lineno'>  48</span> 
<span class='lineno'>  49</span>   # Pads input tensor to the fixed first dimension.
<span class='lineno'>  50</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.paddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.paddings', title='?'>paddings</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.constant_values', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.constant_values', title='int'>constant_values</a> * tf.ones(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padding_shape', title='[?]'>padding_shape</a>),
<span class='lineno'>  51</span>                      <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>.dtype)
<span class='lineno'>  52</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', title='?'>padded_tensor</a> = tf.concat([<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_tensor', title='?'>input_tensor</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.paddings', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.paddings', title='?'>paddings</a>], axis=0)
<span class='lineno'>  53</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', title='?'>output_shape</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.input_shape', title='?'>input_shape</a>
<span class='lineno'>  54</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', title='?'>output_shape</a>[0] = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.size', title='?'>size</a>
<span class='lineno'>  55</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', title='?'>padded_tensor</a>.set_shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.output_shape', title='?'>output_shape</a>)
<span class='lineno'>  56</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.pad_to_fixed_size.padded_tensor', title='?'>padded_tensor</a>
<span class='lineno'>  57</span> 
<span class='lineno'>  58</span> 
<span class='lineno'>  59</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image', title='(?, (float, float, float), (float, float, float)) -> None'>normalize_image</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a>,
<span class='lineno'>  60</span>                     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='(float, float, float)'>offset</a>=(0.485, 0.456, 0.406),
<span class='lineno'>  61</span>                     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='(float, float, float)'>scale</a>=(0.229, 0.224, 0.225)):
<span class='lineno'>  62</span>   &quot;&quot;&quot;Normalizes the image to zero mean and unit variance.&quot;&quot;&quot;
<span class='lineno'>  63</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a> = tf.image.convert_image_dtype(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a>, dtype=tf.float32)
<span class='lineno'>  64</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a> = tf.constant(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='(float, float, float)'>offset</a>)
<span class='lineno'>  65</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a>, axis=0)
<span class='lineno'>  66</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a>, axis=0)
<span class='lineno'>  67</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a></a> -= <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.offset', title='?'>offset</a>
<span class='lineno'>  68</span> 
<span class='lineno'>  69</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a> = tf.constant(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='(float, float, float)'>scale</a>)
<span class='lineno'>  70</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a>, axis=0)
<span class='lineno'>  71</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a> = tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a>, axis=0)
<span class='lineno'>  72</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a></a> /= <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.scale', title='?'>scale</a>
<span class='lineno'>  73</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.normalize_image.image', title='?'>image</a>
<span class='lineno'>  74</span> 
<span class='lineno'>  75</span> 
<span class='lineno'>  76</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size', title='(?, ?) -> [int]'>compute_padded_size</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', title='?'>desired_size</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', title='?'>stride</a>):
<span class='lineno'>  77</span>   &quot;&quot;&quot;Compute the padded size given the desired size and the stride.
<span class='lineno'>  78</span> 
<span class='lineno'>  79</span>   The padded size will be the smallest rectangle, such that each dimension is
<span class='lineno'>  80</span>   the smallest multiple of the stride which is larger than the desired
<span class='lineno'>  81</span>   dimension. For example, if desired_size = (100, 200) and stride = 32,
<span class='lineno'>  82</span>   the output padded_size = (128, 224).
<span class='lineno'>  83</span> 
<span class='lineno'>  84</span>   Args:
<span class='lineno'>  85</span>     desired_size: a `Tensor` or `int` list/tuple of two elements representing
<span class='lineno'>  86</span>       [height, width] of the target output image size.
<span class='lineno'>  87</span>     stride: an integer, the stride of the backbone network.
<span class='lineno'>  88</span> 
<span class='lineno'>  89</span>   Returns:
<span class='lineno'>  90</span>     padded_size: a `Tensor` or `int` list/tuple of two elements representing
<span class='lineno'>  91</span>       [height, width] of the padded output image size.
<span class='lineno'>  92</span>   &quot;&quot;&quot;
<span class='lineno'>  93</span>   if isinstance(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', title='?'>desired_size</a>, list) or isinstance(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', title='?'>desired_size</a>, tuple):
<span class='lineno'>  94</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', title='[int]'>padded_size</a> = [int(math.ceil(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', title='?'>d</a> * 1.0 / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', title='?'>stride</a>) * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', title='?'>stride</a>)
<span class='lineno'>  95</span>                    for <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.d', title='?'>d</a></a> in <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', title='?'>desired_size</a>]
<span class='lineno'>  96</span>   else:
<span class='lineno'>  97</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', title='?'>padded_size</a> = tf.cast(
<span class='lineno'>  98</span>         tf.math.ceil(
<span class='lineno'>  99</span>             tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.desired_size', title='?'>desired_size</a>, dtype=tf.float32) / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', title='?'>stride</a>) * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.stride', title='?'>stride</a>,
<span class='lineno'> 100</span>         tf.int32)
<span class='lineno'> 101</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.compute_padded_size.padded_size', title='[int]'>padded_size</a>
<span class='lineno'> 102</span> 
<span class='lineno'> 103</span> 
<span class='lineno'> 104</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image', title='(?, ?, ?, float, float, int, ?) -> (?, ?)'>resize_and_crop_image</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', title='?'>image</a>,
<span class='lineno'> 105</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>,
<span class='lineno'> 106</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', title='?'>padded_size</a>,
<span class='lineno'> 107</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', title='float'>aug_scale_min</a>=1.0,
<span class='lineno'> 108</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', title='float'>aug_scale_max</a>=1.0,
<span class='lineno'> 109</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', title='int'>seed</a>=1,
<span class='lineno'> 110</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.method', title='?'>method</a>=tf.image.ResizeMethod.BILINEAR):
<span class='lineno'> 111</span>   &quot;&quot;&quot;Resizes the input image to output size.
<span class='lineno'> 112</span> 
<span class='lineno'> 113</span>   Resize and pad images given the desired output size of the image and
<span class='lineno'> 114</span>   stride size.
<span class='lineno'> 115</span> 
<span class='lineno'> 116</span>   Here are the preprocessing steps.
<span class='lineno'> 117</span>   1. For a given image, keep its aspect ratio and rescale the image to make it
<span class='lineno'> 118</span>      the largest rectangle to be bounded by the rectangle specified by the
<span class='lineno'> 119</span>      `desired_size`.
<span class='lineno'> 120</span>   2. Pad the rescaled image to the padded_size.
<span class='lineno'> 121</span> 
<span class='lineno'> 122</span>   Args:
<span class='lineno'> 123</span>     image: a `Tensor` of shape [height, width, 3] representing an image.
<span class='lineno'> 124</span>     desired_size: a `Tensor` or `int` list/tuple of two elements representing
<span class='lineno'> 125</span>       [height, width] of the desired actual output image size.
<span class='lineno'> 126</span>     padded_size: a `Tensor` or `int` list/tuple of two elements representing
<span class='lineno'> 127</span>       [height, width] of the padded output image size. Padding will be applied
<span class='lineno'> 128</span>       after scaling the image to the desired_size.
<span class='lineno'> 129</span>     aug_scale_min: a `float` with range between [0, 1.0] representing minimum
<span class='lineno'> 130</span>       random scale applied to desired_size for training scale jittering.
<span class='lineno'> 131</span>     aug_scale_max: a `float` with range between [1.0, inf] representing maximum
<span class='lineno'> 132</span>       random scale applied to desired_size for training scale jittering.
<span class='lineno'> 133</span>     seed: seed for random scale jittering.
<span class='lineno'> 134</span>     method: function to resize input image to scaled image.
<span class='lineno'> 135</span> 
<span class='lineno'> 136</span>   Returns:
<span class='lineno'> 137</span>     output_image: `Tensor` of shape [height, width, 3] where [height, width]
<span class='lineno'> 138</span>       equals to `output_size`.
<span class='lineno'> 139</span>     image_info: a 2D `Tensor` that encodes the information of the image and the
<span class='lineno'> 140</span>       applied preprocessing. It is in the format of
<span class='lineno'> 141</span>       [[original_height, original_width], [desired_height, desired_width],
<span class='lineno'> 142</span>        [y_scale, x_scale], [y_offset, x_offset]], where [desired_height,
<span class='lineno'> 143</span>       desireed_width] is the actual scaled image size, and [y_scale, x_scale] is
<span class='lineno'> 144</span>       the scaling factory, which is the ratio of
<span class='lineno'> 145</span>       scaled dimension / original dimension.
<span class='lineno'> 146</span>   &quot;&quot;&quot;
<span class='lineno'> 147</span>   with tf.name_scope(&#39;resize_and_crop_image&#39;):
<span class='lineno'> 148</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a> = tf.cast(tf.shape(input=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', title='?'>image</a>)[0:2], tf.float32)
<span class='lineno'> 149</span> 
<span class='lineno'> 150</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', title='bool'>random_jittering</a> = (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', title='float'>aug_scale_min</a> != 1.0 or <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', title='float'>aug_scale_max</a> != 1.0)
<span class='lineno'> 151</span> 
<span class='lineno'> 152</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 153</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_scale', title='?'>random_scale</a> = tf.random.uniform([],
<span class='lineno'> 154</span>                                        <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_min', title='float'>aug_scale_min</a>,
<span class='lineno'> 155</span>                                        <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.aug_scale_max', title='float'>aug_scale_max</a>,
<span class='lineno'> 156</span>                                        seed=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', title='int'>seed</a>)
<span class='lineno'> 157</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a> = tf.round(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_scale', title='?'>random_scale</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>)
<span class='lineno'> 158</span>     else:
<span class='lineno'> 159</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>
<span class='lineno'> 160</span> 
<span class='lineno'> 161</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scale', title='?'>scale</a> = tf.minimum(
<span class='lineno'> 162</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a>[0] / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a>[1] / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a>[1])
<span class='lineno'> 163</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a> = tf.round(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scale', title='?'>scale</a>)
<span class='lineno'> 164</span> 
<span class='lineno'> 165</span>     # Computes 2D image_scale.
<span class='lineno'> 166</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_scale', title='?'>image_scale</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a> / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a>
<span class='lineno'> 167</span> 
<span class='lineno'> 168</span>     # Selects non-zero random offset (x, y) if scaled image is larger than
<span class='lineno'> 169</span>     # desired_size.
<span class='lineno'> 170</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 171</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a> - <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>
<span class='lineno'> 172</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a> = tf.where(tf.less(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a>, 0),
<span class='lineno'> 173</span>                             tf.zeros_like(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a>),
<span class='lineno'> 174</span>                             <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a>)
<span class='lineno'> 175</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.max_offset', title='?'>max_offset</a> * tf.random.uniform([
<span class='lineno'> 176</span>           2,
<span class='lineno'> 177</span>       ], 0, 1, seed=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.seed', title='int'>seed</a>)
<span class='lineno'> 178</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>, tf.int32)
<span class='lineno'> 179</span>     else:
<span class='lineno'> 180</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a> = tf.zeros((2,), tf.int32)
<span class='lineno'> 181</span> 
<span class='lineno'> 182</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', title='?'>scaled_image</a> = tf.image.resize(
<span class='lineno'> 183</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image', title='?'>image</a>, tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_size', title='?'>scaled_size</a>, tf.int32), method=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.method', title='?'>method</a>)
<span class='lineno'> 184</span> 
<span class='lineno'> 185</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 186</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', title='?'>scaled_image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', title='?'>scaled_image</a>[<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>[0]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>[0] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>[0],
<span class='lineno'> 187</span>                                   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>[1]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>[1] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>[1], :]
<span class='lineno'> 188</span> 
<span class='lineno'> 189</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.output_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.output_image', title='?'>output_image</a> = tf.image.pad_to_bounding_box(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.scaled_image', title='?'>scaled_image</a>, 0, 0,
<span class='lineno'> 190</span>                                                 <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', title='?'>padded_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.padded_size', title='?'>padded_size</a>[1])
<span class='lineno'> 191</span> 
<span class='lineno'> 192</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_info', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_info', title='?'>image_info</a> = tf.stack([
<span class='lineno'> 193</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_size', title='?'>image_size</a>,
<span class='lineno'> 194</span>         tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.desired_size', title='?'>desired_size</a>, dtype=tf.float32),
<span class='lineno'> 195</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_scale', title='?'>image_scale</a>,
<span class='lineno'> 196</span>         tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.offset', title='?'>offset</a>, tf.float32)])
<span class='lineno'> 197</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.output_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.output_image', title='?'>output_image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_info', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image.image_info', title='?'>image_info</a>
<span class='lineno'> 198</span> 
<span class='lineno'> 199</span> 
<span class='lineno'> 200</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2', title='(?, ?, ?, ?, float, float, int, ?) -> (?, ?)'>resize_and_crop_image_v2</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', title='?'>image</a>,
<span class='lineno'> 201</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.short_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.short_side', title='?'>short_side</a>,
<span class='lineno'> 202</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', title='?'>long_side</a>,
<span class='lineno'> 203</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', title='?'>padded_size</a>,
<span class='lineno'> 204</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', title='float'>aug_scale_min</a>=1.0,
<span class='lineno'> 205</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', title='float'>aug_scale_max</a>=1.0,
<span class='lineno'> 206</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', title='int'>seed</a>=1,
<span class='lineno'> 207</span>                              <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.method', title='?'>method</a>=tf.image.ResizeMethod.BILINEAR):
<span class='lineno'> 208</span>   &quot;&quot;&quot;Resizes the input image to output size (Faster R-CNN style).
<span class='lineno'> 209</span> 
<span class='lineno'> 210</span>   Resize and pad images given the specified short / long side length and the
<span class='lineno'> 211</span>   stride size.
<span class='lineno'> 212</span> 
<span class='lineno'> 213</span>   Here are the preprocessing steps.
<span class='lineno'> 214</span>   1. For a given image, keep its aspect ratio and first try to rescale the short
<span class='lineno'> 215</span>      side of the original image to `short_side`.
<span class='lineno'> 216</span>   2. If the scaled image after 1 has a long side that exceeds `long_side`, keep
<span class='lineno'> 217</span>      the aspect ratio and rescal the long side of the image to `long_side`.
<span class='lineno'> 218</span>   2. Pad the rescaled image to the padded_size.
<span class='lineno'> 219</span> 
<span class='lineno'> 220</span>   Args:
<span class='lineno'> 221</span>     image: a `Tensor` of shape [height, width, 3] representing an image.
<span class='lineno'> 222</span>     short_side: a scalar `Tensor` or `int` representing the desired short side
<span class='lineno'> 223</span>       to be rescaled to.
<span class='lineno'> 224</span>     long_side: a scalar `Tensor` or `int` representing the desired long side to
<span class='lineno'> 225</span>       be rescaled to.
<span class='lineno'> 226</span>     padded_size: a `Tensor` or `int` list/tuple of two elements representing
<span class='lineno'> 227</span>       [height, width] of the padded output image size. Padding will be applied
<span class='lineno'> 228</span>       after scaling the image to the desired_size.
<span class='lineno'> 229</span>     aug_scale_min: a `float` with range between [0, 1.0] representing minimum
<span class='lineno'> 230</span>       random scale applied to desired_size for training scale jittering.
<span class='lineno'> 231</span>     aug_scale_max: a `float` with range between [1.0, inf] representing maximum
<span class='lineno'> 232</span>       random scale applied to desired_size for training scale jittering.
<span class='lineno'> 233</span>     seed: seed for random scale jittering.
<span class='lineno'> 234</span>     method: function to resize input image to scaled image.
<span class='lineno'> 235</span> 
<span class='lineno'> 236</span>   Returns:
<span class='lineno'> 237</span>     output_image: `Tensor` of shape [height, width, 3] where [height, width]
<span class='lineno'> 238</span>       equals to `output_size`.
<span class='lineno'> 239</span>     image_info: a 2D `Tensor` that encodes the information of the image and the
<span class='lineno'> 240</span>       applied preprocessing. It is in the format of
<span class='lineno'> 241</span>       [[original_height, original_width], [desired_height, desired_width],
<span class='lineno'> 242</span>        [y_scale, x_scale], [y_offset, x_offset]], where [desired_height,
<span class='lineno'> 243</span>       desired_width] is the actual scaled image size, and [y_scale, x_scale] is
<span class='lineno'> 244</span>       the scaling factor, which is the ratio of
<span class='lineno'> 245</span>       scaled dimension / original dimension.
<span class='lineno'> 246</span>   &quot;&quot;&quot;
<span class='lineno'> 247</span>   with tf.name_scope(&#39;resize_and_crop_image_v2&#39;):
<span class='lineno'> 248</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a> = tf.cast(tf.shape(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', title='?'>image</a>)[0:2], tf.float32)
<span class='lineno'> 249</span> 
<span class='lineno'> 250</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_short_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_short_side', title='?'>scale_using_short_side</a> = (
<span class='lineno'> 251</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.short_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.short_side', title='?'>short_side</a> / tf.math.minimum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>[1]))
<span class='lineno'> 252</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_long_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_long_side', title='?'>scale_using_long_side</a> = (
<span class='lineno'> 253</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', title='?'>long_side</a> / tf.math.maximum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>[1]))
<span class='lineno'> 254</span> 
<span class='lineno'> 255</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a> = tf.math.round(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_short_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_short_side', title='?'>scale_using_short_side</a>)
<span class='lineno'> 256</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a> = tf.where(
<span class='lineno'> 257</span>         tf.math.greater(
<span class='lineno'> 258</span>             tf.math.maximum(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>[1]), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.long_side', title='?'>long_side</a>),
<span class='lineno'> 259</span>         tf.math.round(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_long_side', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scale_using_long_side', title='?'>scale_using_long_side</a>), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>)
<span class='lineno'> 260</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', title='?'>desired_size</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>
<span class='lineno'> 261</span> 
<span class='lineno'> 262</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', title='bool'>random_jittering</a> = (<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', title='float'>aug_scale_min</a> != 1.0 or <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', title='float'>aug_scale_max</a> != 1.0)
<span class='lineno'> 263</span> 
<span class='lineno'> 264</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 265</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_scale', title='?'>random_scale</a> = tf.random.uniform([],
<span class='lineno'> 266</span>                                        <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_min', title='float'>aug_scale_min</a>,
<span class='lineno'> 267</span>                                        <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.aug_scale_max', title='float'>aug_scale_max</a>,
<span class='lineno'> 268</span>                                        seed=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', title='int'>seed</a>)
<span class='lineno'> 269</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a> = tf.math.round(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_scale', title='?'>random_scale</a> * <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>)
<span class='lineno'> 270</span> 
<span class='lineno'> 271</span>     # Computes 2D image_scale.
<span class='lineno'> 272</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_scale', title='?'>image_scale</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a> / <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>
<span class='lineno'> 273</span> 
<span class='lineno'> 274</span>     # Selects non-zero random offset (x, y) if scaled image is larger than
<span class='lineno'> 275</span>     # desired_size.
<span class='lineno'> 276</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 277</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a> - <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', title='?'>desired_size</a>
<span class='lineno'> 278</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a> = tf.where(
<span class='lineno'> 279</span>           tf.math.less(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a>, 0), tf.zeros_like(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a>), <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a>)
<span class='lineno'> 280</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.max_offset', title='?'>max_offset</a> * tf.random.uniform([
<span class='lineno'> 281</span>           2,
<span class='lineno'> 282</span>       ], 0, 1, seed=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.seed', title='int'>seed</a>)
<span class='lineno'> 283</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>, tf.int32)
<span class='lineno'> 284</span>     else:
<span class='lineno'> 285</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a> = tf.zeros((2,), tf.int32)
<span class='lineno'> 286</span> 
<span class='lineno'> 287</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', title='?'>scaled_image</a> = tf.image.resize(
<span class='lineno'> 288</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image', title='?'>image</a>, tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_size', title='?'>scaled_size</a>, tf.int32), method=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.method', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.method', title='?'>method</a>)
<span class='lineno'> 289</span> 
<span class='lineno'> 290</span>     if <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.random_jittering', title='bool'>random_jittering</a>:
<span class='lineno'> 291</span>       <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', title='?'>scaled_image</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', title='?'>scaled_image</a>[
<span class='lineno'> 292</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>[0]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>[0] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', title='?'>desired_size</a>[0],
<span class='lineno'> 293</span>           <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>[1]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>[1] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', title='?'>desired_size</a>[1], :]
<span class='lineno'> 294</span> 
<span class='lineno'> 295</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.output_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.output_image', title='?'>output_image</a> = tf.image.pad_to_bounding_box(
<span class='lineno'> 296</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.scaled_image', title='?'>scaled_image</a>, 0, 0, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', title='?'>padded_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.padded_size', title='?'>padded_size</a>[1])
<span class='lineno'> 297</span> 
<span class='lineno'> 298</span>     <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_info', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_info', title='?'>image_info</a> = tf.stack([
<span class='lineno'> 299</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_size', title='?'>image_size</a>,
<span class='lineno'> 300</span>         tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.desired_size', title='?'>desired_size</a>, dtype=tf.float32),
<span class='lineno'> 301</span>         <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_scale', title='?'>image_scale</a>,
<span class='lineno'> 302</span>         tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.offset', title='?'>offset</a>, tf.float32)])
<span class='lineno'> 303</span>     return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.output_image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.output_image', title='?'>output_image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_info', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_image_v2.image_info', title='?'>image_info</a>
<span class='lineno'> 304</span> 
<span class='lineno'> 305</span> 
<span class='lineno'> 306</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes', title='(?, ?, ?, ?) -> None'>resize_and_crop_boxes</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a>,
<span class='lineno'> 307</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.image_scale', title='?'>image_scale</a>,
<span class='lineno'> 308</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.output_size', title='?'>output_size</a>,
<span class='lineno'> 309</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.offset', title='?'>offset</a>):
<span class='lineno'> 310</span>   &quot;&quot;&quot;Resizes boxes to output size with scale and offset.
<span class='lineno'> 311</span> 
<span class='lineno'> 312</span>   Args:
<span class='lineno'> 313</span>     boxes: `Tensor` of shape [N, 4] representing ground truth boxes.
<span class='lineno'> 314</span>     image_scale: 2D float `Tensor` representing scale factors that apply to
<span class='lineno'> 315</span>       [height, width] of input image.
<span class='lineno'> 316</span>     output_size: 2D `Tensor` or `int` representing [height, width] of target
<span class='lineno'> 317</span>       output image size.
<span class='lineno'> 318</span>     offset: 2D `Tensor` representing top-left corner [y0, x0] to crop scaled
<span class='lineno'> 319</span>       boxes.
<span class='lineno'> 320</span> 
<span class='lineno'> 321</span>   Returns:
<span class='lineno'> 322</span>     boxes: `Tensor` of shape [N, 4] representing the scaled boxes.
<span class='lineno'> 323</span>   &quot;&quot;&quot;
<span class='lineno'> 324</span>   # Adjusts box coordinates based on image_scale and offset.
<span class='lineno'> 325</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a></a> *= tf.tile(tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.image_scale', title='?'>image_scale</a>, axis=0), [1, 2])
<span class='lineno'> 326</span>   <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'><a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a></a> -= tf.tile(tf.expand_dims(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.offset', title='?'>offset</a>, axis=0), [1, 2])
<span class='lineno'> 327</span>   # Clips the boxes.
<span class='lineno'> 328</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a> = box_utils.clip_boxes(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.output_size', title='?'>output_size</a>)
<span class='lineno'> 329</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_boxes.boxes', title='?'>boxes</a>
<span class='lineno'> 330</span> 
<span class='lineno'> 331</span> 
<span class='lineno'> 332</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks', title='(?, ?, ?, ?) -> None'>resize_and_crop_masks</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', title='?'>masks</a>,
<span class='lineno'> 333</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', title='?'>image_scale</a>,
<span class='lineno'> 334</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', title='?'>output_size</a>,
<span class='lineno'> 335</span>                           <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>):
<span class='lineno'> 336</span>   &quot;&quot;&quot;Resizes boxes to output size with scale and offset.
<span class='lineno'> 337</span> 
<span class='lineno'> 338</span>   Args:
<span class='lineno'> 339</span>     masks: `Tensor` of shape [N, H, W, 1] representing ground truth masks.
<span class='lineno'> 340</span>     image_scale: 2D float `Tensor` representing scale factors that apply to
<span class='lineno'> 341</span>       [height, width] of input image.
<span class='lineno'> 342</span>     output_size: 2D `Tensor` or `int` representing [height, width] of target
<span class='lineno'> 343</span>       output image size.
<span class='lineno'> 344</span>     offset: 2D `Tensor` representing top-left corner [y0, x0] to crop scaled
<span class='lineno'> 345</span>       boxes.
<span class='lineno'> 346</span> 
<span class='lineno'> 347</span>   Returns:
<span class='lineno'> 348</span>     masks: `Tensor` of shape [N, H, W, 1] representing the scaled masks.
<span class='lineno'> 349</span>   &quot;&quot;&quot;
<span class='lineno'> 350</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.mask_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.mask_size', title='?'>mask_size</a> = tf.shape(input=<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', title='?'>masks</a>)[1:3]
<span class='lineno'> 351</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_size', title='?'>scaled_size</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', title='?'>image_scale</a> * tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.mask_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.mask_size', title='?'>mask_size</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.image_scale', title='?'>image_scale</a>.dtype),
<span class='lineno'> 352</span>                         tf.int32)
<span class='lineno'> 353</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', title='?'>scaled_masks</a> = tf.image.resize(
<span class='lineno'> 354</span>       <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.masks', title='?'>masks</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_size', title='?'>scaled_size</a>, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
<span class='lineno'> 355</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a> = tf.cast(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>, tf.int32)
<span class='lineno'> 356</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', title='?'>scaled_masks</a> = <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', title='?'>scaled_masks</a>[:, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>[0]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>[0] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', title='?'>output_size</a>[0],
<span class='lineno'> 357</span>                               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>[1]:<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.offset', title='?'>offset</a>[1] + <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', title='?'>output_size</a>[1], :]
<span class='lineno'> 358</span> 
<span class='lineno'> 359</span>   <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_masks', title='?'>output_masks</a> = tf.image.pad_to_bounding_box(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.scaled_masks', title='?'>scaled_masks</a>, 0, 0,
<span class='lineno'> 360</span>                                               <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', title='?'>output_size</a>[0], <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_size', title='?'>output_size</a>[1])
<span class='lineno'> 361</span>   return <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.resize_and_crop_masks.output_masks', title='?'>output_masks</a>
<span class='lineno'> 362</span> 
<span class='lineno'> 363</span> 
<span class='lineno'> 364</span> def <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip', title='(?, None, None) -> None'>random_horizontal_flip</a>(<a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.image', title='?'>image</a>, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.boxes', title='None'>boxes</a>=None, <a name='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.masks', title='None'>masks</a>=None):
<span class='lineno'> 365</span>   &quot;&quot;&quot;Randomly flips input image and bounding boxes.&quot;&quot;&quot;
<span class='lineno'> 366</span>   return preprocessor.random_horizontal_flip(<a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.image', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.image', title='?'>image</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.boxes', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.boxes', title='None'>boxes</a>, <a href='#.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.masks', xid ='.home.xxm.Desktop.EMSE.dataset.models.official.vision.detection.utils.input_utils.random_horizontal_flip.masks', title='None'>masks</a>)
</pre></td></tr></table></body></html>